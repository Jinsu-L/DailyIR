# Daily Papers Report - 2025-11-26

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. $\text{R}^2\text{R}$: A Route-to-Rerank Post-Training Framework for Multi-Domain Decoder-Only Rerankers

- **LLM Score**: 8
- **Keyword Score**: 14
- **Authors**: Xinyu Wang, Hanwei Wu, Qingchen Hu, Zhenghan Tai, Jingrui Tian, Lei Ding, Jijun Chi, Hailin He, Tung Sum Thomas Kwok, Yufei Cui, Sicheng Lyu, Muzhi Li, Mingze Li, Xinyue Yu, Ling Zhou, Peng Lu
- **URL**: <http://arxiv.org/abs/2511.19987v1>
- **Submitted**: 2025-11-25 06:54:51
- **Comment**: 13 pages, including 3 figures and 3 tables
- **Topic Keywords**: query, rerank, relevance, rag, retrieval, rank
- **Reason**: This paper is highly relevant to your interests in Information Retrieval, particularly in the area of ranking models and query understanding. The use of a post-training framework for rerankers and the introduction of a domain-aware approach aligns with your focus on deep semantic understanding and real-time relevance optimization. However, the specific application to Retrieval-Augmented Generation (RAG) and the use of decoder-only rerankers may not be directly related to your core research themes.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: R2R (Retrieval‚Äëto‚ÄëReranker): A Post‚ÄëTraining Framework for Multi‚ÄëDomain Decoder‚ÄëOnly Rerankers
- **Aim**: Develop a single frozen decoder‚Äëonly reranker backbone with lightweight per‚Äëdomain LoRA adapters and a latent‚Äësemantic router to deliver domain‚Äëspecific relevance scoring while preserving general ranking ability and avoiding catastrophic forgetting.
- **Rationale**: Generalist rerankers struggle with domain terminology, fine‚Äëgrained intents, and distribution shift; naive fine‚Äëtuning overfits to surface entities and forgets general patterns. A curriculum that first abstracts entities and then injects domain knowledge can learn invariant relevance patterns before specialization.
- **Ground**: 1) Two‚Äëstage training: Stage‚ÄØ1 (Entity‚ÄëAbstraction‚Äëfor‚ÄëGeneralization) replaces named entities with placeholders and trains with contrastive loss to capture structural relevance. Stage‚ÄØ2 fine‚Äëtunes on unmasked data per domain, producing a LoRA expert. 2) Architecture: frozen transformer backbone, next‚Äëtoken relevance scoring, LoRA adapters (rank‚ÄØ32, Œ±‚ÄØ64), dynamic parameterization via latent domain inference. 3) Latent Semantic Router (LSR) predicts domain from last‚Äëtoken embedding using a small MLP (‚âà0.2‚ÄØB params) and activates the corresponding LoRA module.
- **Experiment**: Cross‚Äëdomain benchmarks (legal, medical, financial) show R2R outperforms generalist and single‚Äëdomain fine‚Äëtuned baselines: e.g., LexRAG NDCG‚ÄØ+‚ÄØ11.4‚ÄØpts, MRR‚ÄØ+‚ÄØ10.2‚ÄØpts; ChatDoctor NDCG‚ÄØ+‚ÄØ2.1‚ÄØpts. Routing variants compared; LSR achieves highest accuracy and macro‚ÄëF1. Catastrophic forgetting mitigated: direct fine‚Äëtuning drops NDCG@5 from 81.1‚ÄØ‚Üí‚ÄØ77.7, whereas EAG preserves performance. Parameter overhead <‚ÄØ0.5‚ÄØ% per domain; LSR adds ‚âà0.2‚ÄØB params; no extra inference latency.
- **Takeaway**: R2R‚Äôs contrastive two‚Äëstage fine‚Äëtuning plus lightweight latent‚Äësemantic routing yields domain‚Äëspecialised decoder‚Äëonly rerankers that outperform direct fine‚Äëtuning, avoid catastrophic forgetting, and maintain minimal parameter and latency overhead, making it a practical, scalable solution for diverse retrieval domains.

#### Abstract
> Decoder-only rerankers are central to Retrieval-Augmented Generation (RAG). However, generalist models miss domain-specific nuances in high-stakes fields like finance and law, and naive fine-tuning causes surface-form overfitting and catastrophic forgetting. To address this challenge, we introduce R2R, a domain-aware framework that combines dynamic expert routing with a two-stage training strategy, Entity Abstraction for Generalization (EAG). EAG introduces a counter-shortcut mechanism by masking the most predictive surface cues, forcing the reranker to learn domain-invariant relevance patterns rather than memorizing dataset-specific entities. To efficiently activate domain experts, R2R employs a lightweight Latent Semantic Router that probes internal representations from the frozen backbone decoder to select the optimal LoRA expert per query. Extensive experiments across different reranker backbones and diverse domains (legal, medical, and financial) demonstrate that R2R consistently surpasses generalist and single-domain fine-tuned baselines. Our results confirm that R2R is a model-agnostic and modular approach to domain specialization with strong cross-domain robustness.

---

### 2. HKRAG: Holistic Knowledge Retrieval-Augmented Generation Over Visually-Rich Documents

- **LLM Score**: 8
- **Keyword Score**: 13
- **Authors**: Anyang Tong, Xiang Niu, ZhiPing Liu, Chang Tian, Yanyan Wei, Zenglin Shi, Meng Wang
- **URL**: <http://arxiv.org/abs/2511.20227v1>
- **Submitted**: 2025-11-25 11:59:52
- **Topic Keywords**: information retrieval, retriever, query, rag, retrieval
- **Reason**: This paper focuses on multimodal retrieval-augmented generation for visually rich documents, which is somewhat related to your interests in Information Retrieval and Search technologies. Although it doesn't specifically address query understanding, ranking models, or user behavior modeling, it does involve deep semantic understanding and real-time relevance optimization, making it a useful contribution to the field.

#### Abstract
> Existing multimodal Retrieval-Augmented Generation (RAG) methods for visually rich documents (VRD) are often biased towards retrieving salient knowledge(e.g., prominent text and visual elements), while largely neglecting the critical fine-print knowledge(e.g., small text, contextual details). This limitation leads to incomplete retrieval and compromises the generator's ability to produce accurate and comprehensive answers. To bridge this gap, we propose HKRAG, a new holistic RAG framework designed to explicitly capture and integrate both knowledge types. Our framework features two key components: (1) a Hybrid Masking-based Holistic Retriever that employs explicit masking strategies to separately model salient and fine-print knowledge, ensuring a query-relevant holistic information retrieval; and (2) an Uncertainty-guided Agentic Generator that dynamically assesses the uncertainty of initial answers and actively decides how to integrate the two distinct knowledge streams for optimal response generation. Extensive experiments on open-domain visual question answering benchmarks show that HKRAG consistently outperforms existing methods in both zero-shot and supervised settings, demonstrating the critical importance of holistic knowledge retrieval for VRD understanding.

---

### 3. On Evaluating LLM Alignment by Evaluating LLMs as Judges

- **LLM Score**: 8
- **Keyword Score**: 6
- **Authors**: Yixin Liu, Pengfei Liu, Arman Cohan
- **URL**: <http://arxiv.org/abs/2511.20604v1>
- **Submitted**: 2025-11-25 18:33:24
- **Comment**: NeurIPS 2025 Camera Ready
- **Topic Keywords**: ranking, rank, acl
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of query understanding and ranking models. Although it focuses on large language models (LLMs), the evaluation of LLMs as judges aligns with your interest in user behavior modeling and click models. The paper's emphasis on alignment with human preferences and the introduction of a new benchmarking paradigm, AlignEval, also relates to your work in real-time relevance optimization.

#### Abstract
> Alignment with human preferences is an important evaluation aspect of LLMs, requiring them to be helpful, honest, safe, and to precisely follow human instructions. Evaluating large language models' (LLMs) alignment typically involves directly assessing their open-ended responses, requiring human annotators or strong LLM judges. Conversely, LLMs themselves have also been extensively evaluated as judges for assessing alignment. In this work, we examine the relationship between LLMs' generation and evaluation capabilities in aligning with human preferences. To this end, we first conduct a comprehensive analysis of the generation-evaluation consistency (GE-consistency) among various LLMs, revealing a strong correlation between their generation and evaluation capabilities when evaluated by a strong LLM preference oracle. Utilizing this finding, we propose a benchmarking paradigm that measures LLM alignment with human preferences without directly evaluating their generated outputs, instead assessing LLMs in their role as evaluators. Our evaluation shows that our proposed benchmark, AlignEval, matches or surpasses widely used automatic LLM evaluation benchmarks, such as AlpacaEval and Arena-Hard, in capturing human preferences when ranking LLMs. Our study offers valuable insights into the connection between LLMs' generation and evaluation capabilities, and introduces a benchmark that assesses alignment without directly evaluating model outputs.

---

### 4. Efficient Multi-Hop Question Answering over Knowledge Graphs via LLM Planning and Embedding-Guided Search

- **LLM Score**: 8
- **Keyword Score**: 5
- **Authors**: Manil Shrestha, Edward Kim
- **URL**: <http://arxiv.org/abs/2511.19648v1>
- **Submitted**: 2025-11-24 19:27:56
- **Topic Keywords**: ranking, rank, search
- **Reason**: This paper is highly relevant to your interests in Information Retrieval, particularly in query understanding and ranking models. The focus on multi-hop question answering over knowledge graphs and the use of Large Language Models (LLMs) for planning and search are directly related to your research themes. Although the paper's primary focus is on NLP and knowledge graphs, the underlying concepts and techniques are applicable to broader IR and search technologies.

#### Abstract
> Multi-hop question answering over knowledge graphs remains computationally challenging due to the combinatorial explosion of possible reasoning paths. Recent approaches rely on expensive Large Language Model (LLM) inference for both entity linking and path ranking, limiting their practical deployment. Additionally, LLM-generated answers often lack verifiable grounding in structured knowledge. We present two complementary hybrid algorithms that address both efficiency and verifiability: (1) LLM-Guided Planning that uses a single LLM call to predict relation sequences executed via breadth-first search, achieving near-perfect accuracy (micro-F1 > 0.90) while ensuring all answers are grounded in the knowledge graph, and (2) Embedding-Guided Neural Search that eliminates LLM calls entirely by fusing text and graph embeddings through a lightweight 6.7M-parameter edge scorer, achieving over 100 times speedup with competitive accuracy. Through knowledge distillation, we compress planning capability into a 4B-parameter model that matches large-model performance at zero API cost. Evaluation on MetaQA demonstrates that grounded reasoning consistently outperforms ungrounded generation, with structured planning proving more transferable than direct answer generation. Our results show that verifiable multi-hop reasoning does not require massive models at inference time, but rather the right architectural inductive biases combining symbolic structure with learned representations.

---

### 5. DesignPref: Capturing Personal Preferences in Visual Design Generation

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Yi-Hao Peng, Jeffrey P. Bigham, Jason Wu
- **URL**: <http://arxiv.org/abs/2511.20513v1>
- **Submitted**: 2025-11-25 17:19:10
- **Topic Keywords**: pairwise, rag, personalization, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, as it involves understanding and modeling user preferences. However, the focus on visual design generation and preference modeling in the e-commerce domain is not a central match to your primary research themes. The paper's emphasis on dataset creation and personalized models may be of interest, but it does not directly address your core areas of query understanding, ranking models, or user behavior modeling.

#### Abstract
> Generative models, such as large language models and text-to-image diffusion models, are increasingly used to create visual designs like user interfaces (UIs) and presentation slides. Finetuning and benchmarking these generative models have often relied on datasets of human-annotated design preferences. Yet, due to the subjective and highly personalized nature of visual design, preference varies widely among individuals. In this paper, we study this problem by introducing DesignPref, a dataset of 12k pairwise comparisons of UI design generation annotated by 20 professional designers with multi-level preference ratings. We found that among trained designers, substantial levels of disagreement exist (Krippendorff's alpha = 0.25 for binary preferences). Natural language rationales provided by these designers indicate that disagreements stem from differing perceptions of various design aspect importance and individual preferences. With DesignPref, we demonstrate that traditional majority-voting methods for training aggregated judge models often do not accurately reflect individual preferences. To address this challenge, we investigate multiple personalization strategies, particularly fine-tuning or incorporating designer-specific annotations into RAG pipelines. Our results show that personalized models consistently outperform aggregated baseline models in predicting individual designers' preferences, even when using 20 times fewer examples. Our work provides the first dataset to study personalized visual design evaluation and support future research into modeling individual design taste.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. MTA: A Merge-then-Adapt Framework for Personalized Large Language Model

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Xiaopeng Li, Yuanjin Zheng, Wanyu Wang, wenlin zhang, Pengyue Jia, Yiqi Wang, Maolin Wang, Xuetao Wei, Xiangyu Zhao
- **URL**: <http://arxiv.org/abs/2511.20072v2>
- **Submitted**: 2025-11-25 08:46:09
- **Topic Keywords**: ltr, rag, personalization, rank
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, particularly in the area of personalized models. However, the focus on Large Language Models and personalization for user-centric applications is not directly aligned with your primary focus on query understanding, ranking models, and user behavior modeling. The paper's relevance is mostly in the broader context of NLP and data mining.

#### Abstract
> Personalized Large Language Models (PLLMs) aim to align model outputs with individual user preferences, a crucial capability for user-centric applications. However, the prevalent approach of fine-tuning a separate module for each user faces two major limitations: (1) storage costs scale linearly with the number of users, rendering the method unscalable; and (2) fine-tuning a static model from scratch often yields suboptimal performance for users with sparse data. To address these challenges, we propose MTA, a Merge-then-Adapt framework for PLLMs. MTA comprises three key stages. First, we construct a shared Meta-LoRA Bank by selecting anchor users and pre-training meta-personalization traits within meta-LoRA modules. Second, to ensure scalability and enable dynamic personalization combination beyond static models, we introduce an Adaptive LoRA Fusion stage. This stage retrieves and dynamically merges the most relevant anchor meta-LoRAs to synthesize a user-specific one, thereby eliminating the need for user-specific storage and supporting more flexible personalization. Third, we propose a LoRA Stacking for Few-Shot Personalization stage, which applies an additional ultra-low-rank, lightweight LoRA module on top of the merged LoRA. Fine-tuning this module enables effective personalization under few-shot settings. Extensive experiments on the LaMP benchmark demonstrate that our approach outperforms existing SOTA methods across multiple tasks.

### 7. SSA: Sparse Sparse Attention by Aligning Full and Sparse Attention Outputs in Feature Space

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Zhenyi Shen, Junru Lu, Lin Gui, Jiazheng Li, Yulan He, Di Yin, Xing Sun
- **URL**: <http://arxiv.org/abs/2511.20102v1>
- **Submitted**: 2025-11-25 09:21:57
- **Comment**: 28 pages
- **Topic Keywords**: query, rag, rank
- **Reason**: This paper focuses on sparse attention in large language models, which is somewhat related to information retrieval and search technologies. However, the primary focus on sparse attention and its applications in language models does not directly align with the user's core research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> The quadratic complexity of full attention limits efficient long-context processing in large language models (LLMs). Sparse attention mitigates this cost by restricting each query to attend to a subset of previous tokens; however, training-free approaches often lead to severe performance degradation. Native sparse-attention methods (e.g., NSA, MoBA) alleviate this issue, yet exhibit a critical paradox: they produce lower attention sparsity than full-attention models, despite aiming to approximate full attention, which may constrain their effectiveness. We attribute this paradox to gradient update deficiency: low-ranked key-value pairs excluded during sparse training receive neither forward contribution nor backward gradients, and thus never learn proper suppression. To overcome this limitation, we propose SSA (Sparse Sparse Attention), a unified training framework that considers both sparse and full attention and enforces bidirectional alignment at every layer. This design preserves gradient flow to all tokens while explicitly encouraging sparse-attention outputs to align with their full-attention counterparts, thereby promoting stronger sparsity. As a result, SSA achieves state-of-the-art performance under both sparse and full attention inference across multiple commonsense benchmarks. Furthermore, SSA enables models to adapt smoothly to varying sparsity budgets; performance improves consistently as more tokens are allowed to attend, supporting flexible compute-performance trade-offs at inference time. Finally, we show that native sparse-attention training surprisingly improves long-context extrapolation by mitigating the over-allocation of attention values in sink areas, with SSA demonstrating the strongest extrapolation capability.

### 8. Online-PVLM: Advancing Personalized VLMs with Online Concept Learning

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Huiyu Bai, Runze Wang, Zhuoyun Du, Yiyang Zhao, Fengji Zhang, Haoyu Chen, Xiaoyong Zhu, Bo Zheng, Xuejiao Zhao
- **URL**: <http://arxiv.org/abs/2511.20056v1>
- **Submitted**: 2025-11-25 08:25:30
- **Comment**: Work in Progress
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper Online-PVLM explores personalized visual language models, which is somewhat related to information retrieval and query understanding. However, the focus on visual language models and concept learning is not a central match to the user's core research themes in IR and NLP. The paper's emphasis on scalability and efficiency is also not directly aligned with the user's interests in deep semantic understanding and real-time relevance optimization.

#### Abstract
> Personalized Visual Language Models (VLMs) are gaining increasing attention for their formidable ability in user-specific concepts aligned interactions (e.g., identifying a user's bike). Existing methods typically require the learning of separate embeddings for each new concept, which fails to support real-time adaptation during testing. This limitation becomes particularly pronounced in large-scale scenarios, where efficient retrieval of concept embeddings is not achievable. To alleviate this gap, we propose Online-PVLM, a framework for online concept learning by leveraging hyperbolic representations. Our approach makes a train-free paradigm for concept embeddings generation at test time, making the use of personalized VLMs both scalable and efficient. In addition, we develop OP-Eval, a comprehensive and large-scale benchmark comprising 1,292 concepts and over 30K high-quality instances with diverse question types, designed to rigorously assess online concept learning in realistic scenarios. Extensive experiments demonstrate the state-of-the-art performance of our proposed framework. Our source code and dataset will be made available.

### 9. REWA: Witness-Overlap Theory -- Foundations for Composable Binary Similarity Systems

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Nikit Phadke
- **URL**: <http://arxiv.org/abs/2511.19998v1>
- **Submitted**: 2025-11-25 07:04:44
- **Topic Keywords**: ranking, rank
- **Reason**: The paper introduces a theory of similarity based on witness-overlap structures, which is related to information retrieval and ranking models. However, the focus is on similarity systems and their encoding complexity, which is somewhat tangential to the user's core research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> REWA introduces a general theory of similarity based on witness-overlap structures. We show that whenever similarity between concepts can be expressed as monotone witness overlap -- whether arising from graph neighborhoods, causal relations, temporal structure, topological features, symbolic patterns, or embedding-based neighborhoods -- it admits a reduction to compact encodings with provable ranking preservation guarantees. REWA systems consist of: (1) finite witness sets $W(v)$, (2) semi-random bit assignments generated from each witness, and (3) monotonicity of expected similarity in the overlap $Œî(u, v) = |W(u) \cap W(v)|$. We prove that under an overlap-gap condition on the final witness sets -- independent of how they were constructed -- top-$k$ rankings are preserved using $m = O(\log(|V|/Œ¥))$ bits. The witness-set formulation is compositional: any sequence of structural, temporal, causal, topological, information-theoretic, or learned transformations can be combined into pipelines that terminate in discrete witness sets. The theory applies to the final witness overlap, enabling modular construction of similarity systems from reusable primitives. This yields a vast design space: millions of composable similarity definitions inherit logarithmic encoding complexity. REWA subsumes and unifies Bloom filters, minhash, LSH bitmaps, random projections, sketches, and hierarchical filters as special cases. It provides a principled foundation for similarity systems whose behavior is governed by witness overlap rather than hash-function engineering. This manuscript presents the axioms, the main reducibility theorem, complete proofs with explicit constants, and a detailed discussion of compositional design, limitations, and future extensions including multi-bit encodings, weighted witnesses, and non-set representations.

### 10. A Systematic Analysis of Large Language Models with RAG-enabled Dynamic Prompting for Medical Error Detection and Correction

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Farzad Ahmed, Joniel Augustine Jerome, Meliha Yetisgen, √ñzlem Uzuner
- **URL**: <http://arxiv.org/abs/2511.19858v2>
- **Submitted**: 2025-11-25 02:40:49
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and query understanding, as it explores the application of large language models with dynamic prompting for medical error detection and correction. However, the focus on medical error detection and correction is not directly aligned with your primary research themes in Information Retrieval (IR) and Search technologies. The paper's use of retrieval-augmented dynamic prompting (RDP) is an interesting aspect, but its relevance to your core research areas is limited.

#### Abstract
> Objective: Clinical documentation contains factual, diagnostic, and management errors that can compromise patient safety. Large language models (LLMs) may help detect and correct such errors, but their behavior under different prompting strategies remains unclear. We evaluate zero-shot prompting, static prompting with random exemplars (SPR), and retrieval-augmented dynamic prompting (RDP) for three subtasks of medical error processing: error flag detection, error sentence detection, and error correction.
  Methods: Using the MEDEC dataset, we evaluated nine instruction-tuned LLMs (GPT, Claude, Gemini, and OpenAI o-series models). We measured performance using accuracy, recall, false-positive rate (FPR), and an aggregate score of ROUGE-1, BLEURT, and BERTScore for error correction. We also analyzed example outputs to identify failure modes and differences between LLM and clinician reasoning.
  Results: Zero-shot prompting showed low recall in both detection tasks, often missing abbreviation-heavy or atypical errors. SPR improved recall but increased FPR. Across all nine LLMs, RDP reduced FPR by about 15 percent, improved recall by 5 to 10 percent in error sentence detection, and generated more contextually accurate corrections.
  Conclusion: Across diverse LLMs, RDP outperforms zero-shot and SPR prompting. Using retrieved exemplars improves detection accuracy, reduces false positives, and enhances the reliability of medical error correction.

### 11. Enhancing Sequential Recommendation with World Knowledge from Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Tianjie Dai, Xu Chen, Yunmeng Shu, Jinsong Lan, Xiaoyong Zhu, Jiangchao Yao, Bo Zheng
- **URL**: <http://arxiv.org/abs/2511.20177v1>
- **Submitted**: 2025-11-25 10:59:38
- **Topic Keywords**: retrieval, recommend
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, as it involves the use of Large Language Models (LLMs) and attention mechanisms. However, the focus on sequential recommendation and recommender systems is not a central match to your primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Sequential Recommendation System~(SRS) has become pivotal in modern society, which predicts subsequent actions based on the user's historical behavior. However, traditional collaborative filtering-based sequential recommendation models often lead to suboptimal performance due to the limited information of their collaborative signals. With the rapid development of LLMs, an increasing number of works have incorporated LLMs' world knowledge into sequential recommendation. Although they achieve considerable gains, these approaches typically assume the correctness of LLM-generated results and remain susceptible to noise induced by LLM hallucinations. To overcome these limitations, we propose GRASP (Generation Augmented Retrieval with Holistic Attention for Sequential Prediction), a flexible framework that integrates generation augmented retrieval for descriptive synthesis and similarity retrieval, and holistic attention enhancement which employs multi-level attention to effectively employ LLM's world knowledge even with hallucinations and better capture users' dynamic interests. The retrieved similar users/items serve as auxiliary contextual information for the later holistic attention enhancement module, effectively mitigating the noisy guidance of supervision-based methods. Comprehensive evaluations on two public benchmarks and one industrial dataset reveal that GRASP consistently achieves state-of-the-art performance when integrated with diverse backbones. The code is available at: https://anonymous.4open.science/r/GRASP-SRS.

### 12. A Task-Oriented Evaluation Framework for Text Normalization in Modern NLP Pipelines

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Md Abdullah Al Kafi, Raka Moni, Sumit Kumar Banshal
- **URL**: <http://arxiv.org/abs/2511.20409v1>
- **Submitted**: 2025-11-25 15:35:42
- **Topic Keywords**: rag
- **Reason**: This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and text preprocessing, but it does not directly align with their focus on query understanding, ranking models, or user behavior modeling in the context of information retrieval. The paper's emphasis on text normalization and stemming methods is relevant to NLP, but it does not address the user's core research themes.

#### Abstract
> Text normalization is an essential preprocessing step in many natural language processing (NLP) tasks, and stemming is one such normalization technique that reduces words to their base or root form. However, evaluating stemming methods is challenging because current evaluation approaches are limited and do not capture the potential harm caused by excessive stemming; therefore, it is essential to develop new approaches to evaluate stemming methods. To address this issue, this study propose a novel, task-oriented approach to evaluate stemming methods, which considers three aspects: (1) the utility of stemming using Stemming Effectiveness Score (SES), (2) the impact of stemming on downstream tasks using Model Performance Delta (MPD), and (3) the semantic similarity between stemmed and original words using Average Normalized Levenshtein Distance (ANLD), thus providing a comprehensive evaluation framework. We apply our evaluation framework to compare two stemmers for Bangla (BNLTK) and English (Snowball), and our results reveal a significant issue, prompting us to analyze their performance in detail. While the Bangla stemmer achieves the highest SES (1.67) due to effective word reduction (CR = 1.90), SES alone is insufficient because our proposed safety measure, ANLD, reveals that this high SES is due to harmful over-stemming (ANLD = 0.26), which correlates with the observed decrease in downstream performance.In contrast, the English stemmer achieves a moderate SES (1.31) with a safe meaning distance (ANLD = 0.14), allowing its word reduction to contribute positively to downstream performance; therefore, it is a more reliable stemmer. Our study provides a valuable tool for distinguishing between potential efficiency gains (high SES) and meaning preservation (low ANLD).

### 13. REFLEX: Self-Refining Explainable Fact-Checking via Disentangling Truth into Style and Substance

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Chuyi Kong, Gao Wei, Jing Ma, Hongzhan Lin, Zhiyuan Fan
- **URL**: <http://arxiv.org/abs/2511.20233v1>
- **Submitted**: 2025-11-25 12:06:23
- **Topic Keywords**: rag
- **Reason**: The paper explores fact-checking and explainable AI, which is somewhat related to information retrieval and NLP. However, the focus on fact-checking and its specific challenges, such as latency and hallucinations, does not directly align with the user's core research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> The prevalence of misinformation on social media threatens public trust, demanding automated fact-checking systems that provide accurate verdicts with interpretable explanations. However, existing large language model-based (LLM-based) approaches often rely heavily on external knowledge sources, introducing substantial latency and even hallucinations that undermine reliability, interpretability, and responsiveness, which is crucial for real-time use. To address these challenges, we propose REason-guided Fact-checking with Latent EXplanations REFLEX paradigm, a plug-and-play, self-refining paradigm that leverages the internal knowledge in backbone model to improve both verdict accuracy and explanation quality. REFLEX reformulates fact-checking as a role-play dialogue and jointly trains verdict prediction and explanation generation. It adaptively extracts contrastive activation pairs between the backbone model and its fine-tuned variant to construct steering vectors that disentangle truth into style and substance naturally. These activation-level signals guide inference and suppress noisy explanations, enabling more faithful and efficient reasoning. Experiments on real-world datasets show that REFLEX outperforms previous methods that steer toward a single truth direction and underscores the challenge traditional approaches face when handling the subtle, human-unknown truth in fact-checking tasks. Remarkably, with only 465 self-refined training samples, RELFEX achieves state-of-the-art performance. Furthermore, models trained with explanatory objectives can effectively guide those without them, yielding up to a 7.57% improvement, highlighting that internal explanation signals play a dual role in both interpreting and enhancing factual reasoning.

### 14. Language-Independent Sentiment Labelling with Distant Supervision: A Case Study for English, Sepedi and Setswana

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Koena Ronny Mabokela, Tim Schlippe, Mpho Raborife, Turgay Celik
- **URL**: <http://arxiv.org/abs/2511.19818v1>
- **Submitted**: 2025-11-25 01:15:54
- **Comment**: Published in the The Fourth Workshop on Processing Emotions, Decisions and Opinions (EDO 2023) at 10th Language & Technology Conference: Human Language Technologies as a Challenge for Computer Science and Linguistics (LTC 2023), Pozna≈Ñ, Poland, 21-23 April 2023. ISBN: 978-83-232-4176-8
- **Topic Keywords**: rag
- **Reason**: This paper is somewhat related to your interests in Natural Language Processing (NLP) and sentiment analysis, but it doesn't directly align with your core focus on Information Retrieval (IR), query understanding, and ranking models. The paper's use of distant supervision and emojis for sentiment labelling is an interesting application, but it doesn't seem to contribute significantly to your primary research areas.

#### Abstract
> Sentiment analysis is a helpful task to automatically analyse opinions and emotions on various topics in areas such as AI for Social Good, AI in Education or marketing. While many of the sentiment analysis systems are developed for English, many African languages are classified as low-resource languages due to the lack of digital language resources like text labelled with corresponding sentiment classes. One reason for that is that manually labelling text data is time-consuming and expensive. Consequently, automatic and rapid processes are needed to reduce the manual effort as much as possible making the labelling process as efficient as possible. In this paper, we present and analyze an automatic language-independent sentiment labelling method that leverages information from sentiment-bearing emojis and words. Our experiments are conducted with tweets in the languages English, Sepedi and Setswana from SAfriSenti, a multilingual sentiment corpus for South African languages. We show that our sentiment labelling approach is able to label the English tweets with an accuracy of 66%, the Sepedi tweets with 69%, and the Setswana tweets with 63%, so that on average only 34% of the automatically generated labels remain to be corrected.

### 15. Comparative Analysis of LoRA-Adapted Embedding Models for Clinical Cardiology Text Representation

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Richard J. Young, Alice M. Matthews
- **URL**: <http://arxiv.org/abs/2511.19739v1>
- **Submitted**: 2025-11-24 21:57:09
- **Comment**: 25 pages, 13 figures, 5 tables
- **Topic Keywords**: rank, search
- **Reason**: This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and clinical applications, but it focuses on clinical cardiology text representation and domain-specific embeddings, which is a specific area that may not align with the user's broader interests in Information Retrieval and Search technologies.

#### Abstract
> Domain-specific text embeddings are critical for clinical natural language processing, yet systematic comparisons across model architectures remain limited. This study evaluates ten transformer-based embedding models adapted for cardiology through Low-Rank Adaptation (LoRA) fine-tuning on 106,535 cardiology text pairs derived from authoritative medical textbooks. Results demonstrate that encoder-only architectures, particularly BioLinkBERT, achieve superior domain-specific performance (separation score: 0.510) compared to larger decoder-based models, while requiring significantly fewer computational resources. The findings challenge the assumption that larger language models necessarily produce better domain-specific embeddings and provide practical guidance for clinical NLP system development. All models, training code, and evaluation datasets are publicly available to support reproducible research in medical informatics.

### 16. Towards A Tri-View Diffusion Framework for Recommendation

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Ximing Chen, Pui Ieng Lei, Yijun Sheng, Yanyan Liu, Zhiguo Gong
- **URL**: <http://arxiv.org/abs/2511.20122v1>
- **Submitted**: 2025-11-25 09:43:00
- **Comment**: 13 pages, 11 figures, accepted by KDD2026 (First Cycle)
- **Topic Keywords**: recommend
- **Reason**: This paper explores a diffusion framework for recommendation, which is somewhat related to the user's interests in Information Retrieval and recommendation systems. However, it focuses more on the recommendation aspect and does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The use of diffusion models and thermodynamic views is an interesting approach, but it does not align with the user's primary focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> Diffusion models (DMs) have recently gained significant interest for their exceptional potential in recommendation tasks. This stems primarily from their prominent capability in distilling, modeling, and generating comprehensive user preferences. However, previous work fails to examine DMs in recommendation tasks through a rigorous lens. In this paper, we first experimentally investigate the completeness of recommender models from a thermodynamic view. We reveal that existing DM-based recommender models operate by maximizing the energy, while classic recommender models operate by reducing the entropy. Based on this finding, we propose a minimalistic diffusion framework that incorporates both factors via the maximization of Helmholtz free energy. Meanwhile, to foster the optimization, our reverse process is armed with a well-designed denoiser to maintain the inherent anisotropy, which measures the user-item cross-correlation in the context of bipartite graphs. Finally, we adopt an Acceptance-Rejection Gumbel Sampling Process (AR-GSP) to prioritize the far-outnumbered unobserved interactions for model robustness. AR-GSP integrates an acceptance-rejection sampling to ensure high-quality hard negative samples for general recommendation tasks, and a timestep-dependent Gumbel Softmax to handle an adaptive sampling strategy for diffusion models. Theoretical analyses and extensive experiments demonstrate that our proposed framework has distinct superiority over baselines in terms of accuracy and efficiency.

### 17. HHFT: Hierarchical Heterogeneous Feature Transformer for Recommendation Systems

- **LLM Score**: 3
- **Keyword Score**: 3
- **Authors**: Liren Yu, Wenming Zhang, Silu Zhou, Zhixuan Zhang, Dan Ou
- **URL**: <http://arxiv.org/abs/2511.20235v1>
- **Submitted**: 2025-11-25 12:07:56
- **Topic Keywords**: ctr, recommend
- **Reason**: This paper focuses on recommendation systems and proposes a novel architecture for CTR prediction, but it does not directly relate to information retrieval, query understanding, or ranking models, which are core areas of your research interests.

#### Abstract
> We propose HHFT (Hierarchical Heterogeneous Feature Transformer), a Transformer-based architecture tailored for industrial CTR prediction. HHFT addresses the limitations of DNN through three key designs: (1) Semantic Feature Partitioning: Grouping heterogeneous features (e.g. user profile, item information, behaviour sequennce) into semantically coherent blocks to preserve domain-specific information; (2) Heterogeneous Transformer Encoder: Adopting block-specific QKV projections and FFNs to avoid semantic confusion between distinct feature types; (3) Hiformer Layer: Capturing high-order interactions across features. Our findings reveal that Transformers significantly outperform DNN baselines, achieving a +0.4% improvement in CTR AUC at scale. We have successfully deployed the model on Taobao's production platform, observing a significant uplift in key business metrics, including a +0.6% increase in Gross Merchandise Value (GMV).

### 18. Invisible in Search? Auditing Aesthetic Bias in the Visual Representation of Holocaust Victims on Google

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Mykola Makhortykh, Tobias Rohrbach, Maryna Sydorova
- **URL**: <http://arxiv.org/abs/2511.20036v1>
- **Submitted**: 2025-11-25 08:04:14
- **Comment**: 22 pages
- **Topic Keywords**: information retrieval, retrieval, search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The paper's focus on aesthetic bias in search engine representation of historical events is outside your primary areas of interest.

#### Abstract
> Information retrieval systems, such as search engines, increasingly shape the representation of the past and present states of social reality. Despite their importance, these systems face challenges in dealing with the ethical aspects of representation due to various forms of bias, including aesthetic bias that perpetuates hegemonic patterns of representation. While most research on aesthetic bias has examined it in the context of current societal issues, it is also crucial for historical representation, particularly of sensitive subjects such as historical atrocities. To address this gap, we conduct a comparative audit of the visual representation of Holocaust victims on Google. We find that Google tends to propagate a male-dominated representation of Holocaust victims with an emphasis on atrocity context, risking rendering invisible gender-specific suffering and decreasing potential for nurturing empathy. We also observe a variation in representation across geographic locations, suggesting that search algorithms may produce their own aesthetic of victimhood.

### 19. "When Data is Scarce, Prompt Smarter"... Approaches to Grammatical Error Correction in Low-Resource Settings

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Somsubhra De, Harsh Kumar, Arun Prakash A
- **URL**: <http://arxiv.org/abs/2511.20120v1>
- **Submitted**: 2025-11-25 09:40:57
- **Comment**: 10 pages, 5 figures, 5 tables; Accept-demonstration at BHASHA Workshop, IJCNLP-AACL 2025
- **Topic Keywords**: ranking, rank
- **Reason**: This paper is primarily focused on Grammatical Error Correction in low-resource settings using large language models, which is outside the user's core research themes of Information Retrieval, Search technologies, and Natural Language Processing. While it touches on NLP, it doesn't align with the user's specific interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Grammatical error correction (GEC) is an important task in Natural Language Processing that aims to automatically detect and correct grammatical mistakes in text. While recent advances in transformer-based models and large annotated datasets have greatly improved GEC performance for high-resource languages such as English, the progress has not extended equally. For most Indic languages, GEC remains a challenging task due to limited resources, linguistic diversity and complex morphology. In this work, we explore prompting-based approaches using state-of-the-art large language models (LLMs), such as GPT-4.1, Gemini-2.5 and LLaMA-4, combined with few-shot strategy to adapt them to low-resource settings. We observe that even basic prompting strategies, such as zero-shot and few-shot approaches, enable these LLMs to substantially outperform fine-tuned Indic-language models like Sarvam-22B, thereby illustrating the exceptional multilingual generalization capabilities of contemporary LLMs for GEC. Our experiments show that carefully designed prompts and lightweight adaptation significantly enhance correction quality across multiple Indic languages. We achieved leading results in the shared task--ranking 1st in Tamil (GLEU: 91.57) and Hindi (GLEU: 85.69), 2nd in Telugu (GLEU: 85.22), 4th in Bangla (GLEU: 92.86), and 5th in Malayalam (GLEU: 92.97). These findings highlight the effectiveness of prompt-driven NLP techniques and underscore the potential of large-scale LLMs to bridge resource gaps in multilingual GEC.

### 20. Mispronunciation Detection and Diagnosis Without Model Training: A Retrieval-Based Approach

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Huu Tuong Tu, Ha Viet Khanh, Tran Tien Dat, Vu Huan, Thien Van Luong, Nguyen Tien Cuong, Nguyen Thi Thu Trang
- **URL**: <http://arxiv.org/abs/2511.20107v1>
- **Submitted**: 2025-11-25 09:26:34
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing, as it focuses on speech recognition and mispronunciation detection, which is outside your primary areas of interest.

#### Abstract
> Mispronunciation Detection and Diagnosis (MDD) is crucial for language learning and speech therapy. Unlike conventional methods that require scoring models or training phoneme-level models, we propose a novel training-free framework that leverages retrieval techniques with a pretrained Automatic Speech Recognition model. Our method avoids phoneme-specific modeling or additional task-specific training, while still achieving accurate detection and diagnosis of pronunciation errors. Experiments on the L2-ARCTIC dataset show that our method achieves a superior F1 score of 69.60% while avoiding the complexity of model training.

### 21. The 2nd Workshop on Human-Centered Recommender Systems

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Kaike Zhang, Jiakai Tang, Du Su, Shuchang Liu, Julian McAuley, Lina Yao, Qi Cao, Yue Feng, Fei Sun
- **URL**: <http://arxiv.org/abs/2511.19979v1>
- **Submitted**: 2025-11-25 06:49:14
- **Topic Keywords**: click, recommend, search
- **Reason**: This paper is about recommender systems, but it focuses on human-centered aspects and does not align with your primary research interests in Information Retrieval, especially query understanding, ranking models, and user behavior modeling. While it touches on related topics, it does not demonstrate a deep semantic understanding or real-time relevance optimization, which are key areas of interest for you.

#### Abstract
> Recommender systems shape how people discover information, form opinions, and connect with society. Yet, as their influence grows, traditional metrics, e.g., accuracy, clicks, and engagement, no longer capture what truly matters to humans. The workshop on Human-Centered Recommender Systems (HCRS) calls for a paradigm shift from optimizing engagement toward designing systems that truly understand, involve, and benefit people. It brings together researchers in recommender systems, human-computer interaction, AI safety, and social computing to explore how human values, e.g., trust, safety, fairness, transparency, and well-being, can be integrated into recommendation processes. Centered around three thematic axes-Human Understanding, Human Involvement, and Human Impact-HCRS features keynotes, panels, and papers covering topics from LLM-based interactive recommenders to societal welfare optimization. By fostering interdisciplinary collaboration, HCRS aims to shape the next decade of responsible and human-aligned recommendation research.

### 22. Does Understanding Inform Generation in Unified Multimodal Models? From Analysis to Path Forward

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yuwei Niu, Weiyang Jin, Jiaqi Liao, Chaoran Feng, Peng Jin, Bin Lin, Zongjian Li, Bin Zhu, Weihao Yu, Li Yuan
- **URL**: <http://arxiv.org/abs/2511.20561v1>
- **Submitted**: 2025-11-25 17:58:48
- **Topic Keywords**: query
- **Reason**: This paper focuses on Unified Multimodal Models and the relationship between understanding and generation, which is not directly related to your core research themes in Information Retrieval and Search technologies. While it touches on aspects of query-based architectures, the primary focus is on NLP and generative models, making it somewhat tangential to your interests.

#### Abstract
> Recent years have witnessed significant progress in Unified Multimodal Models, yet a fundamental question remains: Does understanding truly inform generation? To investigate this, we introduce UniSandbox, a decoupled evaluation framework paired with controlled, synthetic datasets to avoid data leakage and enable detailed analysis. Our findings reveal a significant understanding-generation gap, which is mainly reflected in two key dimensions: reasoning generation and knowledge transfer. Specifically, for reasoning generation tasks, we observe that explicit Chain-of-Thought (CoT) in the understanding module effectively bridges the gap, and further demonstrate that a self-training approach can successfully internalize this ability, enabling implicit reasoning during generation. Additionally, for knowledge transfer tasks, we find that CoT assists the generative process by helping retrieve newly learned knowledge, and also discover that query-based architectures inherently exhibit latent CoT-like properties that affect this transfer. UniSandbox provides preliminary insights for designing future unified architectures and training strategies that truly bridge the gap between understanding and generation. Code and data are available at https://github.com/PKU-YuanGroup/UniSandBox

### 23. From Words to Wisdom: Discourse Annotation and Baseline Models for Student Dialogue Understanding

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Farjana Sultana Mim, Shuchin Aeron, Eric Miller, Kristen Wendell
- **URL**: <http://arxiv.org/abs/2511.20547v1>
- **Submitted**: 2025-11-25 17:46:00
- **Topic Keywords**: rag, search
- **Reason**: This paper is not directly related to your core research themes in Information Retrieval, Search technologies, or query understanding. While it involves Natural Language Processing (NLP), the focus is on discourse annotation and dialogue understanding in an educational context, which is not a primary area of interest for you.

#### Abstract
> Identifying discourse features in student conversations is quite important for educational researchers to recognize the curricular and pedagogical variables that cause students to engage in constructing knowledge rather than merely completing tasks. The manual analysis of student conversations to identify these discourse features is time-consuming and labor-intensive, which limits the scale and scope of studies. Leveraging natural language processing (NLP) techniques can facilitate the automatic detection of these discourse features, offering educational researchers scalable and data-driven insights. However, existing studies in NLP that focus on discourse in dialogue rarely address educational data. In this work, we address this gap by introducing an annotated educational dialogue dataset of student conversations featuring knowledge construction and task production discourse. We also establish baseline models for automatically predicting these discourse properties for each turn of talk within conversations, using pre-trained large language models GPT-3.5 and Llama-3.1. Experimental results indicate that these state-of-the-art models perform suboptimally on this task, indicating the potential for future research.

### 24. Profile-LLM: Dynamic Profile Optimization for Realistic Personality Expression in LLMs

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Shi-Wei Dai, Yan-Wei Shie, Tsung-Huan Yang, Lun-Wei Ku, Yung-Hui Li
- **URL**: <http://arxiv.org/abs/2511.19852v1>
- **Submitted**: 2025-11-25 02:31:40
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on optimizing personality expression in Large Language Models (LLMs) using a framework called PersonaPulse. While it touches on the idea of user-AI interactions, it doesn't directly relate to information retrieval, search technologies, or query understanding, which are core areas of your research interests.

#### Abstract
> Personalized Large Language Models (LLMs) have been shown to be an effective way to create more engaging and enjoyable user-AI interactions. While previous studies have explored using prompts to elicit specific personality traits in LLMs, they have not optimized these prompts to maximize personality expression. To address this limitation, we propose PersonaPulse: Dynamic Profile Optimization for Realistic Personality Expression in LLMs, a framework that leverages LLMs' inherent knowledge of personality traits to iteratively enhance role-play prompts while integrating a situational response benchmark as a scoring tool, ensuring a more realistic and contextually grounded evaluation to guide the optimization process. Quantitative evaluations demonstrate that the prompts generated by PersonaPulse outperform those of prior work, which were designed based on personality descriptions from psychological studies. Additionally, we explore the relationship between model size and personality modeling through extensive experiments. Finally, we find that, for certain personality traits, the extent of personality evocation can be partially controlled by pausing the optimization process. These findings underscore the importance of prompt optimization in shaping personality expression within LLMs, offering valuable insights for future research on adaptive AI interactions.

### 25. Breaking Bad: Norms for Valence, Arousal, and Dominance for over 10k English Multiword Expressions

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Saif M. Mohammad
- **URL**: <http://arxiv.org/abs/2511.19816v1>
- **Submitted**: 2025-11-25 01:14:25
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on the creation of a lexicon for valence, arousal, and dominance ratings of multiword expressions, which is a topic in Natural Language Processing. However, it does not appear to be directly related to information retrieval, query understanding, ranking models, or user behavior modeling, which are the core areas of your research interests.

#### Abstract
> Factor analysis studies have shown that the primary dimensions of word meaning are Valence (V), Arousal (A), and Dominance (D). Existing lexicons such as the NRC VAD Lexicon, published in 2018, include VAD association ratings for words. Here, we present a complement to it, which has human ratings of valence, arousal, and dominance for 10k English Multiword Expressions (MWEs) and their constituent words. We also increase the coverage of unigrams, especially words that have become more common since 2018. In all, the new NRC VAD Lexicon v2 now has entries for 10k MWEs and 25k words, in addition to the entries in v1. We show that the associations are highly reliable. We use the lexicon to examine emotional characteristics of MWEs, including: 1. The degree to which MWEs (idioms, noun compounds, and verb particle constructions) exhibit strong emotionality; 2. The degree of emotional compositionality in MWEs. The lexicon enables a wide variety of research in NLP, Psychology, Public Health, Digital Humanities, and Social Sciences. The NRC VAD Lexicon v2 is freely available through the project webpage: http://saifmohammad.com/WebPages/nrc-vad.html

### 26. The Text Aphasia Battery (TAB): A Clinically-Grounded Benchmark for Aphasia-Like Deficits in Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Nathan Roll, Jill Kries, Flora Jin, Catherine Wang, Ann Marie Finley, Meghan Sumner, Cory Shain, Laura Gwilliams
- **URL**: <http://arxiv.org/abs/2511.20507v1>
- **Submitted**: 2025-11-25 17:16:38
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests as it focuses on assessing language deficits in artificial systems using a clinically-grounded benchmark, which is outside the scope of Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Large language models (LLMs) have emerged as a candidate "model organism" for human language, offering an unprecedented opportunity to study the computational basis of linguistic disorders like aphasia. However, traditional clinical assessments are ill-suited for LLMs, as they presuppose human-like pragmatic pressures and probe cognitive processes not inherent to artificial architectures. We introduce the Text Aphasia Battery (TAB), a text-only benchmark adapted from the Quick Aphasia Battery (QAB) to assess aphasic-like deficits in LLMs. The TAB comprises four subtests: Connected Text, Word Comprehension, Sentence Comprehension, and Repetition. This paper details the TAB's design, subtests, and scoring criteria. To facilitate large-scale use, we validate an automated evaluation protocol using Gemini 2.5 Flash, which achieves reliability comparable to expert human raters (prevalence-weighted Cohen's kappa = 0.255 for model--consensus agreement vs. 0.286 for human--human agreement). We release TAB as a clinically-grounded, scalable framework for analyzing language deficits in artificial systems.

### 27. QiMeng-Kernel: Macro-Thinking Micro-Coding Paradigm for LLM-Based High-Performance GPU Kernel Generation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Xinguo Zhu, Shaohui Peng, Jiaming Guo, Yunji Chen, Qi Guo, Yuanbo Wen, Hang Qin, Ruizhi Chen, Qirui Zhou, Ke Gao, Yanjun Wu, Chen Zhao, Ling Li
- **URL**: <http://arxiv.org/abs/2511.20100v1>
- **Submitted**: 2025-11-25 09:17:47
- **Comment**: 9 pages, 2 figures, accepted by AAAI 2026
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests as it focuses on GPU kernel generation using LLMs, which is outside the scope of Information Retrieval, Search technologies, and related topics. While it involves optimization and efficiency, the context is different from your areas of interest.

#### Abstract
> Developing high-performance GPU kernels is critical for AI and scientific computing, but remains challenging due to its reliance on expert crafting and poor portability. While LLMs offer promise for automation, both general-purpose and finetuned LLMs suffer from two fundamental and conflicting limitations: correctness and efficiency. The key reason is that existing LLM-based approaches directly generate the entire optimized low-level programs, requiring exploration of an extremely vast space encompassing both optimization policies and implementation codes. To address the challenge of exploring an intractable space, we propose Macro Thinking Micro Coding (MTMC), a hierarchical framework inspired by the staged optimization strategy of human experts. It decouples optimization strategy from implementation details, ensuring efficiency through high-level strategy and correctness through low-level implementation. Specifically, Macro Thinking employs reinforcement learning to guide lightweight LLMs in efficiently exploring and learning semantic optimization strategies that maximize hardware utilization. Micro Coding leverages general-purpose LLMs to incrementally implement the stepwise optimization proposals from Macro Thinking, avoiding full-kernel generation errors. Together, they effectively navigate the vast optimization space and intricate implementation details, enabling LLMs for high-performance GPU kernel generation. Comprehensive results on widely adopted benchmarks demonstrate the superior performance of MTMC on GPU kernel generation in both accuracy and running time. On KernelBench, MTMC achieves near 100% and 70% accuracy at Levels 1-2 and 3, over 50% than SOTA general-purpose and domain-finetuned LLMs, with up to 7.3x speedup over LLMs, and 2.2x over expert-optimized PyTorch Eager kernels. On the more challenging TritonBench, MTMC attains up to 59.64% accuracy and 34x speedup.

### 28. AppSelectBench: Application-Level Tool Selection Benchmark

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Tianyi Chen, Michael Solodko, Sen Wang, Jongwoo Ko, Junheng Hao, Colby Banbury, Sara Abdali, Saeed Amizadeh, Qing Xiao, Yinheng Li, Tianyu Ding, Kamran Ghasedi Dizaji, Suzhen Zheng, Hao Fan, Justin Wagle, Pashmina Cameron, Kazuhito Koishida
- **URL**: <http://arxiv.org/abs/2511.19957v1>
- **Submitted**: 2025-11-25 06:06:17
- **Topic Keywords**: retrieval
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling. While it involves large language models and user tasks, the focus is on application selection in Computer Using Agents, which is not a central theme in your research.

#### Abstract
> Computer Using Agents (CUAs) are increasingly equipped with external tools, enabling them to perform complex and realistic tasks. For CUAs to operate effectively, application selection, which refers to deciding which application to use before invoking fine-grained tools such as APIs, is a fundamental capability. It determines whether the agent initializes the correct environment, avoids orchestration confusion, and efficiently focuses on relevant context. However, existing benchmarks primarily assess fine-grained API selection, offering limited insight into whether models can reason across and choose between different applications. To fill this gap, we introduce AppSelectBench, a comprehensive benchmark for evaluating application selection in CUAs. AppSelectBench contains a novel user task generation pipeline that produces realistic, diverse, and semantically grounded user intents at scale, together with unified evaluation protocols covering random, heuristic, zero-shot, few-shot, and retrieval-augmented-settings. AppSelectBench covers one hundred widely used desktop applications and includes more than one hundred thousand realistic, diverse, and semantically grounded user tasks. Extensive experiments across both closed-source and open-source large language models reveal systematic strengths and weaknesses in inter-application reasoning, showing that even the most capable models still struggle to make consistent application choices. Together, these results establish AppSelectBench as a foundation for studying and advancing application level reasoning, an essential yet underexplored capability of intelligent CUAs. The source is available at https://github.com/microsoft/appselectbench.

### 29. CropVLM: Learning to Zoom for Fine-Grained Vision-Language Perception

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Miguel Carvalho, Helder Dias, Bruno Martins
- **URL**: <http://arxiv.org/abs/2511.19820v1>
- **Submitted**: 2025-11-25 01:21:26
- **Topic Keywords**: rag
- **Reason**: This paper focuses on fine-grained vision-language perception, specifically in the context of computer vision and image understanding, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Vision-Language Models (VLMs) often struggle with tasks that require fine-grained image understanding, such as scene-text recognition or document analysis, due to perception limitations and visual fragmentation. To address these challenges, we introduce CropVLM as an external low-cost method for boosting performance, enabling VLMs to dynamically ''zoom in'' on relevant image regions, enhancing their ability to capture fine details. CropVLM is trained using reinforcement learning, without using human-labeled bounding boxes as a supervision signal, and without expensive synthetic evaluations. The model is trained once and can be paired with both open-source and proprietary VLMs to improve their performance. Our approach delivers significant improvements on tasks that require high-resolution image understanding, notably for benchmarks that are out-of-domain for the target VLM, without modifying or fine-tuning the VLM, thus avoiding catastrophic forgetting.

### 30. Fara-7B: An Efficient Agentic Model for Computer Use

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ahmed Awadallah, Yash Lara, Raghav Magazine, Hussein Mozannar, Akshay Nambi, Yash Pandya, Aravind Rajeswaran, Corby Rosset, Alexey Taymanov, Vibhav Vineet, Spencer Whitehead, Andrew Zhao
- **URL**: <http://arxiv.org/abs/2511.19663v1>
- **Submitted**: 2025-11-24 19:56:28
- **Topic Keywords**: rag
- **Reason**: This paper focuses on computer use agents and synthetic data generation for multi-step web tasks, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Progress in computer use agents (CUAs) has been constrained by the absence of large and high-quality datasets that capture how humans interact with a computer. While LLMs have thrived on abundant textual data, no comparable corpus exists for CUA trajectories. To address these gaps, we introduce FaraGen, a novel synthetic data generation system for multi-step web tasks. FaraGen can propose diverse tasks from frequently used websites, generate multiple solution attempts, and filter successful trajectories using multiple verifiers. It achieves high throughput, yield, and diversity for multi-step web tasks, producing verified trajectories at approximately $1 each. We use this data to train Fara-7B, a native CUA model that perceives the computer using only screenshots, executes actions via predicted coordinates, and is small enough to run on-device. We find that Fara-7B outperforms other CUA models of comparable size on benchmarks like WebVoyager, Online-Mind2Web, and WebTailBench -- our novel benchmark that better captures under-represented web tasks in pre-existing benchmarks. Furthermore, Fara-7B is competitive with much larger frontier models, illustrating key benefits of scalable data generation systems in advancing small efficient agentic models. We are making Fara-7B open-weight on Microsoft Foundry and HuggingFace, and we are releasing WebTailBench.

### 31. Scaling LLM Speculative Decoding: Non-Autoregressive Forecasting in Large-Batch Scenarios

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Luohe Shi, Zuchao Li, Lefei Zhang, Baoyuan Qi, Guoming Liu, Hai Zhao
- **URL**: <http://arxiv.org/abs/2511.20340v1>
- **Submitted**: 2025-11-25 14:20:08
- **Comment**: accepted by AAAI-2026
- **Topic Keywords**: search
- **Reason**: This paper focuses on scaling Large Language Model (LLM) inference using speculative decoding, which is not directly related to Information Retrieval (IR) or Search technologies. Although it involves deep learning models, the context is more aligned with NLP and model optimization rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> Speculative decoding accelerates LLM inference by utilizing otherwise idle computational resources during memory-to-chip data transfer. Current speculative decoding methods typically assume a considerable amount of available computing power, then generate a complex and massive draft tree using a small autoregressive language model to improve overall prediction accuracy. However, methods like batching have been widely applied in mainstream model inference systems as a superior alternative to speculative decoding, as they compress the available idle computing power. Therefore, performing speculative decoding with low verification resources and low scheduling costs has become an important research problem. We believe that more capable models that allow for parallel generation on draft sequences are what we truly need. Recognizing the fundamental nature of draft models to only generate sequences of limited length, we propose SpecFormer, a novel architecture that integrates unidirectional and bidirectional attention mechanisms. SpecFormer combines the autoregressive model's ability to extract information from the entire input sequence with the parallel generation benefits of non-autoregressive models. This design eliminates the reliance on large prefix trees and achieves consistent acceleration, even in large-batch scenarios. Through lossless speculative decoding experiments across models of various scales, we demonstrate that SpecFormer sets a new standard for scaling LLM inference with lower training demands and reduced computational costs.

### 32. InvisibleBench: A Deployment Gate for Caregiving Relationship AI

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Ali Madad
- **URL**: <http://arxiv.org/abs/2511.20733v1>
- **Submitted**: 2025-11-25 14:09:45
- **Comment**: 29 pages, 3 figures
- **Topic Keywords**: rank
- **Reason**: This paper appears to be focused on evaluating AI models for caregiving-relationship interactions, which is outside the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves AI models, the context and evaluation metrics are not directly related to the user's core themes of query understanding, ranking models, or user behavior modeling.

#### Abstract
> InvisibleBench is a deployment gate for caregiving-relationship AI, evaluating 3-20+ turn interactions across five dimensions: Safety, Compliance, Trauma-Informed Design, Belonging/Cultural Fitness, and Memory. The benchmark includes autofail conditions for missed crises, medical advice (WOPR Act), harmful information, and attachment engineering. We evaluate four frontier models across 17 scenarios (N=68) spanning three complexity tiers. All models show significant safety gaps (11.8-44.8 percent crisis detection), indicating the necessity of deterministic crisis routing in production systems. DeepSeek Chat v3 achieves the highest overall score (75.9 percent), while strengths differ by dimension: GPT-4o Mini leads Compliance (88.2 percent), Gemini leads Trauma-Informed Design (85.0 percent), and Claude Sonnet 4.5 ranks highest in crisis detection (44.8 percent). We release all scenarios, judge prompts, and scoring configurations with code. InvisibleBench extends single-turn safety tests by evaluating longitudinal risk, where real harms emerge. No clinical claims; this is a deployment-readiness evaluation.

### 33. Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Areeb Ahmad, Abhinav Joshi, Ashutosh Modi
- **URL**: <http://arxiv.org/abs/2511.20273v1>
- **Submitted**: 2025-11-25 12:59:15
- **Comment**: Accepted at NeurIPS 2025
- **Topic Keywords**: rank
- **Reason**: This paper focuses on interpretability of transformer-based language models, which is a topic related to NLP but not directly related to information retrieval, query understanding, or ranking models, which are the core areas of your research interests.

#### Abstract
> Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.

### 34. KyrgyzBERT: A Compact, Efficient Language Model for Kyrgyz NLP

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Adilet Metinov, Gulida M. Kudakeeva, Gulnara D. Kabaeva
- **URL**: <http://arxiv.org/abs/2511.20182v1>
- **Submitted**: 2025-11-25 11:05:53
- **Comment**: 3 pages, 1 figure, 2 tables. Preprint
- **Topic Keywords**: search
- **Reason**: This paper focuses on developing a language model for the Kyrgyz language, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the specific context and application are limited to a low-resource language, making it less relevant to the user's broader interests.

#### Abstract
> Kyrgyz remains a low-resource language with limited foundational NLP tools. To address this gap, we introduce KyrgyzBERT, the first publicly available monolingual BERT-based language model for Kyrgyz. The model has 35.9M parameters and uses a custom tokenizer designed for the language's morphological structure. To evaluate performance, we create kyrgyz-sst2, a sentiment analysis benchmark built by translating the Stanford Sentiment Treebank and manually annotating the full test set. KyrgyzBERT fine-tuned on this dataset achieves an F1-score of 0.8280, competitive with a fine-tuned mBERT model five times larger. All models, data, and code are released to support future research in Kyrgyz NLP.

### 35. MAPS: Preserving Vision-Language Representations via Module-Wise Proximity Scheduling for Better Vision-Language-Action Generalization

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Chengyue Huang, Mellon M. Zhang, Robert Azarcon, Glen Chou, Zsolt Kira
- **URL**: <http://arxiv.org/abs/2511.19878v1>
- **Submitted**: 2025-11-25 03:39:37
- **Topic Keywords**: rank
- **Reason**: This paper focuses on Vision-Language-Action models and their generalization, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves fine-tuning and generalization, the context and application are quite different from your areas of expertise.

#### Abstract
> Vision-Language-Action (VLA) models inherit strong priors from pretrained Vision-Language Models (VLMs), but naive fine-tuning often disrupts these representations and harms generalization. Existing fixes -- freezing modules or applying uniform regularization -- either overconstrain adaptation or ignore the differing roles of VLA components. We present MAPS (Module-Wise Proximity Scheduling), the first robust fine-tuning framework for VLAs. Through systematic analysis, we uncover an empirical order in which proximity constraints should be relaxed to balance stability and flexibility. MAPS linearly schedules this relaxation, enabling visual encoders to stay close to their pretrained priors while action-oriented language layers adapt more freely. MAPS introduces no additional parameters or data, and can be seamlessly integrated into existing VLAs. Across MiniVLA-VQ, MiniVLA-OFT, OpenVLA-OFT, and challenging benchmarks such as SimplerEnv, CALVIN, LIBERO, as well as real-world evaluations on the Franka Emika Panda platform, MAPS consistently boosts both in-distribution and out-of-distribution performance (up to +30%). Our findings highlight empirically guided proximity to pretrained VLMs as a simple yet powerful principle for preserving broad generalization in VLM-to-VLA transfer.

### 36. Kleinkram: Open Robotic Data Management

- **LLM Score**: 0
- **Keyword Score**: 3
- **Authors**: Cyrill P√ºntener, Johann Schwabe, Dominique Garmier, Jonas Frey, Marco Hutter
- **URL**: <http://arxiv.org/abs/2511.20492v1>
- **Submitted**: 2025-11-25 16:59:29
- **Comment**: for associated source code, see https://github.com/leggedrobotics/kleinkram
- **Topic Keywords**: rag, search
- **Reason**: This paper appears to be unrelated to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, Natural Language Processing, and data mining. The focus on robotic data management and storage solutions does not align with your areas of expertise.

#### Abstract
> We introduce Kleinkram, a free and open-source system designed to solve the challenge of managing massive, unstructured robotic datasets. Designed as a modular, on-premises cloud solution, Kleinkram enables scalable storage, indexing, and sharing of datasets, ranging from individual experiments to large-scale research collections. Kleinkram natively integrates with standard formats such as ROS bags and MCAP and utilises S3-compatible storage for flexibility. Beyond storage, Kleinkram features an integrated "Action Runner" that executes customizable Docker-based workflows for data validation, curation, and benchmarking. Kleinkram has successfully managed over 30 TB of data from diverse robotic systems, streamlining the research lifecycle through a modern web interface and a robust Command Line Interface (CLI).

### 37. ST-PPO: Stabilized Off-Policy Proximal Policy Optimization for Multi-Turn Agents Training

- **LLM Score**: 0
- **Keyword Score**: 1
- **Authors**: Chenliang Li, Adel Elmahdy, Alex Boyd, Zhongruo Wang, Alfredo Garcia, Parminder Bhatia, Taha Kass-Hout, Cao Xiao, Mingyi Hong
- **URL**: <http://arxiv.org/abs/2511.20718v1>
- **Submitted**: 2025-11-25 05:54:02
- **Topic Keywords**: search
- **Reason**: LLM scoring failed.

#### Abstract
> PPO has been widely adopted for training large language models (LLMs) at the token level in multi-turn dialogue and reasoning tasks. However, its performance is often unstable and prone to collapse. Through empirical analysis, we identify two main sources of instability in this setting: (1)~token-level importance sampling, which is misaligned with the natural granularity of multi-turn environments that have distinct turn-level stages, and (2) inaccurate advantage estimates from off-policy samples, where the critic has not learned to evaluate certain state-action pairs, resulting in high-variance gradients and unstable updates. To address these challenges, we introduce two complementary stabilization techniques: (1) turn-level importance sampling, which aligns optimization with the natural structure of multi-turn reasoning, and (2) clipping-bias correction, which normalizes gradients by downweighting unreliable, highly off-policy samples. Depending on how these components are combined, we obtain three variants: Turn-PPO (turn-level sampling only), S-PPO (clipping-bias correction applied to token-level PPO), and ST-PPO (turn-level sampling combined with clipping-bias correction). In our experiments, we primarily study ST-PPO and S-PPO, which together demonstrate how the two stabilization mechanisms address complementary sources of instability. Experiments on multi-turn search tasks across general QA, multi-hop QA, and medical multiple-choice QA benchmarks show that ST-PPO and S-PPO consistently prevent the performance collapses observed in large-model training, maintain lower clipping ratios throughout optimization, and achieve higher task performance than standard token-level PPO. These results demonstrate that combining turn-level importance sampling with clipping-bias correction provides a practical and scalable solution for stabilizing multi-turn LLM agent training.

### 38. LLM-EDT: Large Language Model Enhanced Cross-domain Sequential Recommendation with Dual-phase Training

- **LLM Score**: 0
- **Keyword Score**: 1
- **Authors**: Ziwei Liu, Qidong Liu, Wanyu Wang, Yejing Wang, Tong Xu, Wei Huang, Chong Chen, Peng Chuan, Xiangyu Zhao
- **URL**: <http://arxiv.org/abs/2511.19931v1>
- **Submitted**: 2025-11-25 05:18:04
- **Topic Keywords**: recommend
- **Reason**: LLM scoring failed.

#### Abstract
> Cross-domain Sequential Recommendation (CDSR) has been proposed to enrich user-item interactions by incorporating information from various domains. Despite current progress, the imbalance issue and transition issue hinder further development of CDSR. The former one presents a phenomenon that the interactions in one domain dominate the entire behavior, leading to difficulty in capturing the domain-specific features in the other domain. The latter points to the difficulty in capturing users' cross-domain preferences within the mixed interaction sequence, resulting in poor next-item prediction performance for specific domains. With world knowledge and powerful reasoning ability, Large Language Models (LLMs) partially alleviate the above issues by performing as a generator and an encoder. However, current LLMs-enhanced CDSR methods are still under exploration, which fail to recognize the irrelevant noise and rough profiling problems. Thus, to make peace with the aforementioned challenges, we proposed an LLMs Enhanced Cross-domain Sequential Recommendation with Dual-phase Training ({LLM-EDT}). To address the imbalance issue while introducing less irrelevant noise, we first propose the transferable item augmenter to adaptively generate possible cross-domain behaviors for users. Then, to alleviate the transition issue, we introduce a dual-phase training strategy to empower the domain-specific thread with a domain-shared background. As for the rough profiling problem, we devise a domain-aware profiling module to summarize the user's preference in each domain and adaptively aggregate them to generate comprehensive user profiles. The experiments on three public datasets validate the effectiveness of our proposed LLM-EDT. To ease reproducibility, we have released the detailed code online at {https://anonymous.4open.science/r/LLM-EDT-583F}.

---

