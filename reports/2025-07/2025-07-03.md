# Daily Papers Report - 2025-07-03

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Confidence and Stability of Global and Pairwise Scores in NLP Evaluation

- **LLM Score**: 6
- **Keyword Score**: 10
- **Authors**: Georgii Levtsov, Dmitry Ustalov
- **URL**: <http://arxiv.org/abs/2507.01633v1>
- **Submitted**: 2025-07-02 12:05:22
- **Comment**: 8 pages, accepted at ACL SRW 2025
- **Topic Keywords**: ranking, pointwise, pairwise, rank
- **Reason**: The paper explores the strengths and weaknesses of global and pairwise scores in NLP evaluation, which is related to my interest in Information Retrieval and Search technologies. However, the focus is on NLP evaluation rather than query understanding, ranking models, or user behavior modeling, which are my core research themes.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Evaluation of global and pairwise scores in natural language processing (NLP) evaluation
- **Aim**: To aid decision-making in selecting the appropriate model evaluation approach by empirically examining the strengths and limitations of global and pairwise evaluation criteria
- **Rationale**: To address the need for a comprehensive evaluation of global and pairwise scores in NLP evaluation, particularly in text generation tasks
- **Ground**: The study is grounded in the use of synthetic and real-world datasets, including Jigsaw, SST-5, and CEval, with standard global metrics and the Bradley-Terry model for pairwise comparisons
- **Experiment**: The authors conducted computational experiments to compare global and pairwise evaluation criteria, examining their strengths and limitations, and exploring the impact of decision value distributions and model confidence on model rankings
- **Takeaway**: The study provides insights into the selection of appropriate evaluation approaches for different task types, highlighting the strengths and limitations of global and pairwise scores in NLP evaluation

#### Abstract
> With the advent of highly capable instruction-tuned neural language models,
benchmarking in natural language processing (NLP) is increasingly shifting
towards pairwise comparison leaderboards, such as LMSYS Arena, from traditional
global pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper
empirically investigates the strengths and weaknesses of both global scores and
pairwise comparisons to aid decision-making in selecting appropriate model
evaluation strategies. Through computational experiments on synthetic and
real-world datasets using standard global metrics and the popular Bradley-Terry
model for pairwise comparisons, we found that while global scores provide more
reliable overall rankings, they can underestimate strong models with rare,
significant errors or low confidence. Conversely, pairwise comparisons are
particularly effective for identifying strong contenders among models with
lower global scores, especially where quality metrics are hard to define (e.g.,
text generation), though they require more comparisons to converge if ties are
frequent. Our code and data are available at
https://github.com/HSPyroblast/srw-ranking under a permissive license.

---

### 2. Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks

- **LLM Score**: 6
- **Keyword Score**: 6
- **Authors**: Xinxi Lyu, Michael Duan, Rulin Shao, Pang Wei Koh, Sewon Min
- **URL**: <http://arxiv.org/abs/2507.01297v1>
- **Submitted**: 2025-07-02 02:35:47
- **Comment**: 33 pages, 2 figures, 27 tables
- **Topic Keywords**: rag, retrieval, web search, search
- **Reason**: The paper explores Retrieval-Augmented Generation (RAG) in challenging, reasoning-intensive benchmarks, which aligns with your interest in Information Retrieval and Search technologies. The focus on query understanding and ranking models is also relevant. However, the paper's primary focus is on RAG and its applications, rather than query understanding and ranking models, which limits its relevance to your core research themes.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: Retrieval-Augmented Generation (RAG) and CompactDS
- **Aim**: Challenge the prevailing view that RAG has limited success on reasoning-intensive benchmarks
- **Rationale**: Design a web-scale datastore that achieves high retrieval accuracy and subsecond latency on a single node
- **Ground**: Importance of diversity in data sources, including web crawls, curated math, academic papers, and textbooks
- **Experiment**: Combining in-memory approximate nearest neighbor (ANN) retrieval with on-disk exact search to balance speed and recall
- **Takeaway**: CompactDS achieves consistent accuracy improvements across all benchmarks and model sizes, with relative gains ranging from 10% to 33%

#### Abstract
> Retrieval-augmented Generation (RAG) has primarily been studied in limited
settings, such as factoid question answering; more challenging,
reasoning-intensive benchmarks have seen limited success from minimal RAG. In
this work, we challenge this prevailing view on established,
reasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We
identify a key missing component in prior work: a usable, web-scale datastore
aligned with the breadth of pretraining data. To this end, we introduce
CompactDS: a diverse, high-quality, web-scale datastore that achieves high
retrieval accuracy and subsecond latency on a single-node. The key insights are
(1) most web content can be filtered out without sacrificing coverage, and a
compact, high-quality subset is sufficient; and (2) combining in-memory
approximate nearest neighbor (ANN) retrieval and on-disk exact search balances
speed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves
consistent accuracy improvements across all benchmarks and model sizes
(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,
and 19% on MATH. No single data source suffices alone, highlighting the
importance of diversity of sources (web crawls, curated math, academic papers,
textbooks). Finally, we show that our carefully designed in-house datastore
matches or outperforms web search engines such as Google Search, as well as
recently proposed, complex agent-based RAG systems--all while maintaining
simplicity, reproducibility, and self-containment. We release CompactDS and our
retrieval pipeline, supporting future research exploring retrieval-based AI
systems.

---

### 3. LEDOM: An Open and Fundamental Reverse Language Model

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Xunjian Yin, Sitao Cheng, Yuxi Xie, Xinyu Hu, Li Lin, Xinyi Wang, Liangming Pan, William Yang Wang, Xiaojun Wan
- **URL**: <http://arxiv.org/abs/2507.01335v1>
- **Submitted**: 2025-07-02 03:52:00
- **Comment**: Work in progress
- **Topic Keywords**: ranking, rerank, rag, rank, search
- **Reason**: The paper introduces a novel language model, LEDOM, which is trained autoregressively on a large dataset. While it shows promising results in refining generation quality, the focus is on language modeling rather than information retrieval or search technologies. The connection to user behavior modeling or ranking models is not immediately apparent, making it only loosely relevant to the user's research interests.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Reverse Language Models for Reasoning and Generation
- **Aim**: To introduce LEDOM, a reverse language model that processes sequences in reverse temporal order, and explore its potential as a foundational model for various NLP tasks
- **Rationale**: Traditional forward language models have limitations in assessing the plausibility of a sequence leading up to a given output, and LEDOM's reverse-causal perspective can provide a more comprehensive understanding of the reasoning process
- **Ground**: LEDOM is trained autoregressively on a diverse dataset of 435 billion tokens and evaluated on eight diverse benchmarks from the OpenCompass suite
- **Experiment**: The authors fine-tune the LEDOM model on domain-specific mathematical reasoning datasets and evaluate its performance on various tasks, including reasoning, coding, and math
- **Takeaway**: LEDOM demonstrates potential as a foundational model, and its integration with forward language models can improve generative performance, particularly in tasks involving sophisticated reasoning and logical inference

#### Abstract
> We introduce LEDOM, the first purely reverse language model, trained
autoregressively on 435B tokens with 2B and 7B parameter variants, which
processes sequences in reverse temporal order through previous token
prediction. For the first time, we present the reverse language model as a
potential foundational model across general tasks, accompanied by a set of
intriguing examples and insights. Based on LEDOM, we further introduce a novel
application: Reverse Reward, where LEDOM-guided reranking of forward language
model outputs leads to substantial performance improvements on mathematical
reasoning tasks. This approach leverages LEDOM's unique backward reasoning
capability to refine generation quality through posterior evaluation. Our
findings suggest that LEDOM exhibits unique characteristics with broad
application potential. We will release all models, training code, and
pre-training data to facilitate future research.

---

### 4. GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Micha≈Ç Matak, Jaros≈Çaw A. Chudziak
- **URL**: <http://arxiv.org/abs/2507.01259v1>
- **Submitted**: 2025-07-02 00:36:27
- **Comment**: 8 pages, 2 figures, presented at ICAART 2025, in proceedings of the
  17th International Conference on Agents and Artificial Intelligence - Volume
  3: ICAART
- **Topic Keywords**: information retrieval, rag, retrieval, search
- **Reason**: The paper discusses the application of large language models in legal information retrieval, which is not directly related to the user's primary focus on information retrieval, search technologies, and query understanding. While the paper touches on the topic of retrieval mechanisms, it is more focused on the legal domain and does not address the user's specific interests in e-commerce, user behavior modeling, or real-time relevance optimization.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Large Language Models in Legal Matters
- **Aim**: To develop an explainable, human-friendly, and high-performing cognitive LLM-based agent for legal information retrieval and democratize law
- **Rationale**: To address the limitations of existing LLMs in legal information retrieval and provide better access to legal information for all individuals
- **Ground**: The current state of research on augmenting LLMs with domain-specific knowledge and the idea of AI agents, including strengths and limitations in various applications
- **Experiment**: Evaluating the proposed architecture using a dataset based on single-choice questions from entrance exams for law apprenticeships in Poland, comparing with other approaches such as LegalBert, LawGPT, and CaseGPT
- **Takeaway**: The proposed architecture improves the performance of the used LLM by 419%, and future directions include expanding the system to cover additional legal acts, incorporating case retrieval, and developing the system to solve more complex legal cases

#### Abstract
> In this paper we discuss the capability of large language models to base
their answer and provide proper references when dealing with legal matters of
non-english and non-chinese speaking country. We discuss the history of legal
information retrieval, the difference between case law and statute law, its
impact on the legal tasks and analyze the latest research in this field. Basing
on that background we introduce gAIus, the architecture of the cognitive
LLM-based agent, whose responses are based on the knowledge retrieved from
certain legal act, which is Polish Civil Code. We propose a retrieval mechanism
which is more explainable, human-friendly and achieves better results than
embedding-based approaches. To evaluate our method we create special dataset
based on single-choice questions from entrance exams for law apprenticeships
conducted in Poland. The proposed architecture critically leveraged the
abilities of used large language models, improving the gpt-3.5-turbo-0125 by
419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.
At the end of our paper we show the possible future path of research and
potential applications of our findings.

---

### 5. Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: √Ålvaro Zaera, Diana Nicoleta Popa, Ivan Sekulic, Paolo Rosso
- **URL**: <http://arxiv.org/abs/2507.01541v1>
- **Submitted**: 2025-07-02 09:51:41
- **Topic Keywords**: queries, rag
- **Reason**: The paper focuses on out-of-scope detection in dialogue systems, which is not directly related to information retrieval or search technologies. While it mentions large language models, the primary application is in dialogue systems, and the paper's emphasis is on uncertainty modeling and routing, which is not a key area of interest for the user.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Uncertainty-Driven Large Language Models Routing (UDRIL) for Out-of-Scope Intent Detection in Task-Oriented Dialogue Systems
- **Aim**: To propose a novel framework for efficient and accurate out-of-scope intent detection in task-oriented dialogue systems
- **Rationale**: The need for a scalable solution for reliable out-of-scope detection without disrupting existing intent classification models
- **Ground**: The authors' approach combines the strengths of traditional intent classification, uncertainty modeling, and large language models, achieving state-of-the-art results on key out-of-scope detection benchmarks
- **Experiment**: The authors evaluate their approach on two internal and public benchmarks for intent detection in a deployed task-oriented dialogue system, showing significant improvements in out-of-scope detection and in-scope intent detection
- **Takeaway**: The UDRIL framework provides a modular, scalable, and adaptable solution for out-of-scope intent detection, balancing computational efficiency and performance, and enhancing the robustness of deployed task-oriented dialogue systems

#### Abstract
> Out-of-scope (OOS) intent detection is a critical challenge in task-oriented
dialogue systems (TODS), as it ensures robustness to unseen and ambiguous
queries. In this work, we propose a novel but simple modular framework that
combines uncertainty modeling with fine-tuned large language models (LLMs) for
efficient and accurate OOS detection. The first step applies uncertainty
estimation to the output of an in-scope intent detection classifier, which is
currently deployed in a real-world TODS handling tens of thousands of user
interactions daily. The second step then leverages an emerging LLM-based
approach, where a fine-tuned LLM is triggered to make a final decision on
instances with high uncertainty. Unlike prior approaches, our method
effectively balances computational efficiency and performance, combining
traditional approaches with LLMs and yielding state-of-the-art results on key
OOS detection benchmarks, including real-world OOS data acquired from a
deployed TODS.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Is External Information Useful for Stance Detection with LLMs?

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Quang Minh Nguyen, Taegyoon Kim
- **URL**: <http://arxiv.org/abs/2507.01543v1>
- **Submitted**: 2025-07-02 09:53:41
- **Comment**: ACL Findings 2025
- **Topic Keywords**: web search, search, acl
- **Reason**: The paper explores the use of external information in stance detection with large language models (LLMs), which is related to query understanding and ranking models in Information Retrieval. However, the focus on stance detection and LLMs is not directly aligned with the user's primary interests in search technologies and user behavior modeling.

#### Abstract
> In the stance detection task, a text is classified as either favorable,
opposing, or neutral towards a target. Prior work suggests that the use of
external information, e.g., excerpts from Wikipedia, improves stance detection
performance. However, whether or not such information can benefit large
language models (LLMs) remains an unanswered question, despite their wide
adoption in many reasoning tasks. In this study, we conduct a systematic
evaluation on how Wikipedia and web search external information can affect
stance detection across eight LLMs and in three datasets with 12 targets.
Surprisingly, we find that such information degrades performance in most cases,
with macro F1 scores dropping by up to 27.9\%. We explain this through
experiments showing LLMs' tendency to align their predictions with the stance
and sentiment of the provided information rather than the ground truth stance
of the given text. We also find that performance degradation persists with
chain-of-thought prompting, while fine-tuning mitigates but does not fully
eliminate it. Our findings, in contrast to previous literature on BERT-based
systems which suggests that external information enhances performance,
highlight the risks of information biases in LLM-based stance classifiers. Code
is available at https://github.com/ngqm/acl2025-stance-detection.

### 7. Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Juan Chen, Baolong Bi, Wei Zhang, Jingyan Sui, Xiaofei Zhu, Yuanzhuo Wang, Lingrui Mei, Shenghua Liu
- **URL**: <http://arxiv.org/abs/2507.01281v1>
- **Submitted**: 2025-07-02 01:39:49
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on Retrieval-Augmented Generation (RAG) and proposes a novel framework, CARE-RAG, to improve trustworthiness through Conflict-Driven Summarization. While it touches on information retrieval and external content, the primary focus is on language models and generation, which is not directly related to the user's interests in query understanding, ranking models, and user behavior modeling in the context of search technologies.

#### Abstract
> Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating their parametric knowledge with external retrieved content.
However, knowledge conflicts caused by internal inconsistencies or noisy
retrieved content can severely undermine the generation reliability of RAG
systems.In this work, we argue that LLMs should rethink all evidence, including
both retrieved content and internal knowledge, before generating responses.We
propose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel
framework that improves trustworthiness through Conflict-Driven Summarization
of all available evidence.CARE-RAG first derives parameter-aware evidence by
comparing parameter records to identify diverse internal perspectives. It then
refines retrieved evidences to produce context-aware evidence, removing
irrelevant or misleading content. To detect and summarize conflicts, we distill
a 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable
synthesis across multiple sources.To further ensure evaluation integrity, we
introduce a QA Repair step to correct outdated or ambiguous benchmark
answers.Experiments on revised QA datasets with retrieval data show that
CARE-RAG consistently outperforms strong RAG baselines, especially in scenarios
with noisy or conflicting evidence.

### 8. Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Gopichand Kanumolu, Ashok Urlana, Charaka Vinayak Kumar, Bala Mallikarjunarao Garlapati
- **URL**: <http://arxiv.org/abs/2507.01717v1>
- **Submitted**: 2025-07-02 13:47:17
- **Comment**: AgentScen Workshop, IJCAI 2025
- **Topic Keywords**: relevance
- **Reason**: The paper explores the use of Large Language Models and autonomous agents to generate product concepts from patents, which is related to information retrieval and NLP. However, the focus on patent data and business idea generation is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat tangential, but the use of LLMs and agent-based architectures is of interest.

#### Abstract
> Patents contain rich technical knowledge that can inspire innovative product
ideas, yet accessing and interpreting this information remains a challenge.
This work explores the use of Large Language Models (LLMs) and autonomous
agents to mine and generate product concepts from a given patent. In this work,
we design Agent Ideate, a framework for automatically generating product-based
business ideas from patents. We experimented with open-source LLMs and
agent-based architectures across three domains: Computer Science, Natural
Language Processing, and Material Chemistry. Evaluation results show that the
agentic approach consistently outperformed standalone LLMs in terms of idea
quality, relevance, and novelty. These findings suggest that combining LLMs
with agentic workflows can significantly enhance the innovation pipeline by
unlocking the untapped potential of business idea generation from patent data.

### 9. Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Zhaoyan Sun, Jiayi Wang, Xinyang Zhao, Jiachi Wang, Guoliang Li
- **URL**: <http://arxiv.org/abs/2507.01599v1>
- **Submitted**: 2025-07-02 11:04:49
- **Topic Keywords**: queries
- **Reason**: The paper proposes a Data Agent architecture for orchestrating Data+AI ecosystems, focusing on integrating knowledge comprehension, reasoning, and planning capabilities. While it touches on semantic understanding, it does not specifically address query understanding, ranking models, or user behavior modeling, which are core areas of interest for you. The paper's focus on data science and analytics agents is somewhat related to your background in e-commerce, but it does not directly align with your primary research interests in Information Retrieval and Search technologies.

#### Abstract
> Traditional Data+AI systems utilize data-driven techniques to optimize
performance, but they rely heavily on human experts to orchestrate system
pipelines, enabling them to adapt to changes in data, queries, tasks, and
environments. For instance, while there are numerous data science tools
available, developing a pipeline planning system to coordinate these tools
remains challenging. This difficulty arises because existing Data+AI systems
have limited capabilities in semantic understanding, reasoning, and planning.
Fortunately, we have witnessed the success of large language models (LLMs) in
enhancing semantic understanding, reasoning, and planning abilities. It is
crucial to incorporate LLM techniques to revolutionize data systems for
orchestrating Data+AI applications effectively.
  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive
architecture designed to orchestrate Data+AI ecosystems, which focuses on
tackling data-related tasks by integrating knowledge comprehension, reasoning,
and planning capabilities. We delve into the challenges involved in designing
data agents, such as understanding data/queries/environments/tools,
orchestrating pipelines/workflows, optimizing and executing pipelines, and
fostering pipeline self-reflection. Furthermore, we present examples of data
agent systems, including a data science agent, data analytics agents (such as
unstructured data analytics agent, semantic structured data analytics agent,
data lake analytics agent, and multi-modal data analytics agent), and a
database administrator (DBA) agent. We also outline several open challenges
associated with designing data agent systems.

### 10. The Thin Line Between Comprehension and Persuasion in LLMs

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Adrian de Wynter, Tangming Yuan
- **URL**: <http://arxiv.org/abs/2507.01936v1>
- **Submitted**: 2025-07-02 17:46:56
- **Topic Keywords**: rag
- **Reason**: The paper explores the capabilities of Large Language Models (LLMs) in maintaining debates and understanding dialogue structures, but it does not directly relate to the user's research interests in Information Retrieval, Search technologies, and query understanding. While the paper touches on the topic of comprehension, it is more focused on the pragmatic context and coherence of dialogue, which is not a central match for the user's interests.

#### Abstract
> Large language models (LLMs) are excellent at maintaining high-level,
convincing dialogues. They are being fast deployed as chatbots and evaluators
in sensitive areas, such as peer review and mental health applications. This,
along with the disparate accounts on their reasoning capabilities, calls for a
closer examination of LLMs and their comprehension of dialogue. In this work we
begin by evaluating LLMs' ability to maintain a debate--one of the purest yet
most complex forms of human communication. Then we measure how this capability
relates to their understanding of what is being talked about, namely, their
comprehension of dialogical structures and the pragmatic context. We find that
LLMs are capable of maintaining coherent, persuasive debates, often swaying the
beliefs of participants and audiences alike. We also note that awareness or
suspicion of AI involvement encourage people to be more critical of the
arguments made. When polling LLMs on their comprehension of deeper structures
of dialogue, however, they cannot demonstrate said understanding. Our findings
tie the shortcomings of LLMs-as-evaluators to their (in)ability to understand
the context. More broadly, for the field of argumentation theory we posit that,
if an agent can convincingly maintain a dialogue, it is not necessary for it to
know what it is talking about. Hence, the modelling of pragmatic context and
coherence are secondary to effectiveness.

### 11. Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Aditya Tomar, Rudra Murthy, Pushpak Bhattacharyya
- **URL**: <http://arxiv.org/abs/2507.01715v1>
- **Submitted**: 2025-07-02 13:46:00
- **Topic Keywords**: rag
- **Reason**: The paper focuses on bias and stereotype detection in language models, which is a topic related to Natural Language Processing (NLP). While it explores multi-task learning, it does not directly address query understanding, ranking models, or user behavior modeling, which are core interests in Information Retrieval (IR). The paper's findings on leveraging stereotype information to build fairer AI systems are interesting but not directly applicable to the user's research areas.

#### Abstract
> Bias and stereotypes in language models can cause harm, especially in
sensitive areas like content moderation and decision-making. This paper
addresses bias and stereotype detection by exploring how jointly learning these
tasks enhances model performance. We introduce StereoBias, a unique dataset
labeled for bias and stereotype detection across five categories: religion,
gender, socio-economic status, race, profession, and others, enabling a deeper
study of their relationship. Our experiments compare encoder-only models and
fine-tuned decoder-only models using QLoRA. While encoder-only models perform
well, decoder-only models also show competitive results. Crucially, joint
training on bias and stereotype detection significantly improves bias detection
compared to training them separately. Additional experiments with sentiment
analysis confirm that the improvements stem from the connection between bias
and stereotypes, not multi-task learning alone. These findings highlight the
value of leveraging stereotype information to build fairer and more effective
AI systems.

### 12. LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Tianyu Liu, Qitan Lv, Hao Li, Xing Gao, Xiao Sun
- **URL**: <http://arxiv.org/abs/2507.01449v1>
- **Submitted**: 2025-07-02 08:08:30
- **Topic Keywords**: retrieval
- **Reason**: The paper proposes a technique for accelerating retrieval-based speculative decoding in language models, which is related to query understanding and ranking models in Information Retrieval. However, the focus is on language model inference acceleration rather than deep semantic understanding and real-time relevance optimization, which are core aspects of your research interests.

#### Abstract
> Speculative decoding (SD), where a small draft model is employed to propose
draft tokens in advance and then the target model validates them in parallel,
has emerged as a promising technique for LLM inference acceleration. Many
endeavors to improve SD are to eliminate the need for a draft model and
generate draft tokens in a retrieval-based manner in order to further alleviate
the drafting overhead and significantly reduce the difficulty in deployment and
applications. However, retrieval-based SD relies on a matching paradigm to
retrieval the most relevant reference as the draft tokens, where these methods
often fail to find matched and accurate draft tokens. To address this
challenge, we propose LogitSpec to effectively expand the retrieval range and
find the most relevant reference as drafts. Our LogitSpec is motivated by the
observation that the logit of the last token can not only predict the next
token, but also speculate the next next token. Specifically, LogitSpec
generates draft tokens in two steps: (1) utilizing the last logit to speculate
the next next token; (2) retrieving relevant reference for both the next token
and the next next token. LogitSpec is training-free and plug-and-play, which
can be easily integrated into existing LLM inference frameworks. Extensive
experiments on a wide range of text generation benchmarks demonstrate that
LogitSpec can achieve up to 2.61 $\times$ speedup and 3.28 mean accepted tokens
per decoding step. Our code is available at
https://github.com/smart-lty/LogitSpec.

### 13. Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Ting Xu, Xiaoxiao Deng, Xiandong Meng, Haifeng Yang, Yan Wu
- **URL**: <http://arxiv.org/abs/2507.01437v1>
- **Submitted**: 2025-07-02 07:45:22
- **Topic Keywords**: ctr
- **Reason**: The paper is somewhat related to information retrieval and natural language processing, as it deals with processing and analyzing clinical text. However, the focus on clinical NLP and disease prediction is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> This paper addresses the challenges posed by the unstructured nature and
high-dimensional semantic complexity of electronic health record texts. A deep
learning method based on attention mechanisms is proposed to achieve unified
modeling for information extraction and multi-label disease prediction. The
study is conducted on the MIMIC-IV dataset. A Transformer-based architecture is
used to perform representation learning over clinical text. Multi-layer
self-attention mechanisms are employed to capture key medical entities and
their contextual relationships. A Sigmoid-based multi-label classifier is then
applied to predict multiple disease labels. The model incorporates a
context-aware semantic alignment mechanism, enhancing its representational
capacity in typical medical scenarios such as label co-occurrence and sparse
information. To comprehensively evaluate model performance, a series of
experiments were conducted, including baseline comparisons, hyperparameter
sensitivity analysis, data perturbation studies, and noise injection tests.
Results demonstrate that the proposed method consistently outperforms
representative existing approaches across multiple performance metrics. The
model maintains strong generalization under varying data scales, interference
levels, and model depth configurations. The framework developed in this study
offers an efficient algorithmic foundation for processing real-world clinical
texts and presents practical significance for multi-label medical text modeling
tasks.

### 14. LLMs for Legal Subsumption in German Employment Contracts

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Oliver Wardas, Florian Matthes
- **URL**: <http://arxiv.org/abs/2507.01734v1>
- **Submitted**: 2025-07-02 14:07:54
- **Comment**: PrePrint - ICAIL25, Chicago
- **Topic Keywords**: search
- **Reason**: The paper explores the application of Large Language Models (LLMs) in legal subsumption, specifically in German employment contracts. While it touches on NLP and data-driven approaches, the focus is on legal domain and LLMs, which is not directly related to the user's primary interests in Information Retrieval, Search technologies, and query understanding. The paper's relevance is somewhat limited to the user's broader interests in NLP and data mining.

#### Abstract
> Legal work, characterized by its text-heavy and resource-intensive nature,
presents unique challenges and opportunities for NLP research. While
data-driven approaches have advanced the field, their lack of interpretability
and trustworthiness limits their applicability in dynamic legal environments.
To address these issues, we collaborated with legal experts to extend an
existing dataset and explored the use of Large Language Models (LLMs) and
in-context learning to evaluate the legality of clauses in German employment
contracts. Our work evaluates the ability of different LLMs to classify clauses
as "valid," "unfair," or "void" under three legal context variants: no legal
context, full-text sources of laws and court rulings, and distilled versions of
these (referred to as examination guidelines). Results show that full-text
sources moderately improve performance, while examination guidelines
significantly enhance recall for void clauses and weighted F1-Score, reaching
80\%. Despite these advancements, LLMs' performance when using full-text
sources remains substantially below that of human lawyers. We contribute an
extended dataset, including examination guidelines, referenced legal sources,
and corresponding annotations, alongside our code and all log files. Our
findings highlight the potential of LLMs to assist lawyers in contract legality
review while also underscoring the limitations of the methods presented.

### 15. ManifoldMind: Dynamic Hyperbolic Reasoning for Trustworthy Recommendations

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Anoushka Harit, Zhongtian Sun, Suncica Hadzidedic
- **URL**: <http://arxiv.org/abs/2507.02014v1>
- **Submitted**: 2025-07-02 08:42:11
- **Topic Keywords**: recommend
- **Reason**: The paper introduces a probabilistic geometric recommender system, ManifoldMind, which explores semantic hierarchies in hyperbolic space. While it touches on some aspects of information retrieval, such as personalized uncertainty modeling and geometry-aware semantic exploration, its primary focus is on recommender systems, which is only loosely related to the user's interests in information retrieval and search technologies.

#### Abstract
> We introduce ManifoldMind, a probabilistic geometric recommender system for
exploratory reasoning over semantic hierarchies in hyperbolic space. Unlike
prior methods with fixed curvature and rigid embeddings, ManifoldMind
represents users, items, and tags as adaptive-curvature probabilistic spheres,
enabling personalised uncertainty modeling and geometry-aware semantic
exploration. A curvature-aware semantic kernel supports soft, multi-hop
inference, allowing the model to explore diverse conceptual paths instead of
overfitting to shallow or direct interactions. Experiments on four public
benchmarks show superior NDCG, calibration, and diversity compared to strong
baselines. ManifoldMind produces explicit reasoning traces, enabling
transparent, trustworthy, and exploration-driven recommendations in sparse or
abstract domains.

### 16. Towards a Signal Detection Based Measure for Assessing Information Quality of Explainable Recommender Systems

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Yeonbin Son, Matthew L. Bolton
- **URL**: <http://arxiv.org/abs/2507.01168v1>
- **Submitted**: 2025-07-01 20:11:17
- **Comment**: Accepted to IEEE CAI 2025
- **Topic Keywords**: recommend
- **Reason**: The paper focuses on explainable recommender systems, which is somewhat related to information retrieval and search technologies. However, the emphasis on recommender systems and explanation quality is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling. The paper's use of signal detection theory and objective metrics is interesting, but the topic is not a central match for the user's research themes.

#### Abstract
> There is growing interest in explainable recommender systems that provide
recommendations along with explanations for the reasoning behind them. When
evaluating recommender systems, most studies focus on overall recommendation
performance. Only a few assess the quality of the explanations. Explanation
quality is often evaluated through user studies that subjectively gather users'
opinions on representative explanatory factors that shape end-users'
perspective towards the results, not about the explanation contents itself. We
aim to fill this gap by developing an objective metric to evaluate Veracity:
the information quality of explanations. Specifically, we decompose Veracity
into two dimensions: Fidelity and Attunement. Fidelity refers to whether the
explanation includes accurate information about the recommended item.
Attunement evaluates whether the explanation reflects the target user's
preferences. By applying signal detection theory, we first determine decision
outcomes for each dimension and then combine them to calculate a sensitivity,
which serves as the final Veracity value. To assess the effectiveness of the
proposed metric, we set up four cases with varying levels of information
quality to validate whether our metric can accurately capture differences in
quality. The results provided meaningful insights into the effectiveness of our
proposed metric.

### 17. MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Zhixun Chen, Ping Guo, Wenhan Han, Yifan Zhang, Binbin Liu, Haobin Lin, Fengze Liu, Yan Zhao, Bingni Zhang, Taifeng Wang, Yin Zheng, Meng Fang
- **URL**: <http://arxiv.org/abs/2507.01785v1>
- **Submitted**: 2025-07-02 15:11:12
- **Topic Keywords**: pairwise, rag
- **Reason**: The paper focuses on large language model pretraining, data quality, and multilingual evaluation, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it mentions NLP, the context is different from the user's focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Data quality is a critical driver of large language model performance, yet
existing model-based selection methods focus almost exclusively on English. We
introduce MuRating, a scalable framework that transfers high-quality English
data-quality signals into a single rater for 17 target languages. MuRating
aggregates multiple English "raters" via pairwise comparisons to learn unified
document-quality scores,then projects these judgments through translation to
train a multilingual evaluator on monolingual, cross-lingual, and parallel text
pairs. Applied to web data, MuRating selects balanced subsets of English and
multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to
strong baselines, including QuRater, AskLLM, DCLM and so on, our approach
boosts average accuracy on both English benchmarks and multilingual
evaluations, with especially large gains on knowledge-intensive tasks. We
further analyze translation fidelity, selection biases, and underrepresentation
of narrative material, outlining directions for future work.

### 18. Enhanced Influence-aware Group Recommendation for Online Media Propagation

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Chengkun He, Xiangmin Zhou, Chen Wang, Longbing Cao, Jie Shao, Xiaodong Li, Guang Xu, Carrie Jinqiu Hu, Zahir Tari
- **URL**: <http://arxiv.org/abs/2507.01616v1>
- **Submitted**: 2025-07-02 11:34:17
- **Topic Keywords**: rag, recommend, commerce, e-commerce
- **Reason**: The paper focuses on group recommendation and influence-aware recommendation, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and query understanding. Although it mentions social media and online media propagation, the context is different from the user's background in e-commerce and NLP.

#### Abstract
> Group recommendation over social media streams has attracted significant
attention due to its wide applications in domains such as e-commerce,
entertainment, and online news broadcasting. By leveraging social connections
and group behaviours, group recommendation (GR) aims to provide more accurate
and engaging content to a set of users rather than individuals. Recently,
influence-aware GR has emerged as a promising direction, as it considers the
impact of social influence on group decision-making. In earlier work, we
proposed Influence-aware Group Recommendation (IGR) to solve this task.
However, this task remains challenging due to three key factors: the large and
ever-growing scale of social graphs, the inherently dynamic nature of influence
propagation within user groups, and the high computational overhead of
real-time group-item matching.
  To tackle these issues, we propose an Enhanced Influence-aware Group
Recommendation (EIGR) framework. First, we introduce a Graph Extraction-based
Sampling (GES) strategy to minimise redundancy across multiple temporal social
graphs and effectively capture the evolving dynamics of both groups and items.
Second, we design a novel DYnamic Independent Cascade (DYIC) model to predict
how influence propagates over time across social items and user groups.
Finally, we develop a two-level hash-based User Group Index (UG-Index) to
efficiently organise user groups and enable real-time recommendation
generation. Extensive experiments on real-world datasets demonstrate that our
proposed framework, EIGR, consistently outperforms state-of-the-art baselines
in both effectiveness and efficiency.

### 19. Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Aymen Rayane Khouas, Mohamed Reda Bouadjenek, Hakim Hacid, Sunil Aryal
- **URL**: <http://arxiv.org/abs/2507.01285v1>
- **Submitted**: 2025-07-02 01:57:58
- **Comment**: 17 pages, 5 figures
- **Topic Keywords**: relevance, recommend, personalization
- **Reason**: The paper focuses on graph federated recommendation systems, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper mentions aggregation methods, it does not address query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user.

#### Abstract
> Graph federated recommendation systems offer a privacy-preserving alternative
to traditional centralized recommendation architectures, which often raise
concerns about data security. While federated learning enables personalized
recommendations without exposing raw user data, existing aggregation methods
overlook the unique properties of user embeddings in this setting. Indeed,
traditional aggregation methods fail to account for their complexity and the
critical role of user similarity in recommendation effectiveness. Moreover,
evolving user interactions require adaptive aggregation while preserving the
influence of high-relevance anchor users (the primary users before expansion in
graph-based frameworks). To address these limitations, we introduce
Dist-FedAvg, a novel distance-based aggregation method designed to enhance
personalization and aggregation efficiency in graph federated learning. Our
method assigns higher aggregation weights to users with similar embeddings,
while ensuring that anchor users retain significant influence in local updates.
Empirical evaluations on multiple datasets demonstrate that Dist-FedAvg
consistently outperforms baseline aggregation techniques, improving
recommendation accuracy while maintaining seamless integration into existing
federated learning frameworks.

### 20. Deep Recommender Models Inference: Automatic Asymmetric Data Flow Optimization

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Giuseppe Ruggeri, Renzo Andri, Daniele Jahier Pagliari, Lukas Cavigelli
- **URL**: <http://arxiv.org/abs/2507.01676v1>
- **Submitted**: 2025-07-02 13:00:39
- **Comment**: 5 pages, 4 figures, conference: IEEE ICCD24
- **Topic Keywords**: query, recommend
- **Reason**: The paper focuses on optimizing deep recommender models for inference, which is not directly related to the user's interests in information retrieval, query understanding, ranking models, and user behavior modeling. While the paper mentions embedding layers, it does not explore the semantic understanding and real-time relevance optimization aspects that are central to the user's research.

#### Abstract
> Deep Recommender Models (DLRMs) inference is a fundamental AI workload
accounting for more than 79% of the total AI workload in Meta's data centers.
DLRMs' performance bottleneck is found in the embedding layers, which perform
many random memory accesses to retrieve small embedding vectors from tables of
various sizes. We propose the design of tailored data flows to speedup
embedding look-ups. Namely, we propose four strategies to look up an embedding
table effectively on one core, and a framework to automatically map the tables
asymmetrically to the multiple cores of a SoC. We assess the effectiveness of
our method using the Huawei Ascend AI accelerators, comparing it with the
default Ascend compiler, and we perform high-level comparisons with Nvidia
A100. Results show a speed-up varying from 1.5x up to 6.5x for real workload
distributions, and more than 20x for extremely unbalanced distributions.
Furthermore, the method proves to be much more independent of the query
distribution than the baseline.

### 21. Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Chris Yuhao Liu, Liang Zeng, Yuzhen Xiao, Jujie He, Jiacai Liu, Chaojie Wang, Rui Yan, Wei Shen, Fuxiang Zhang, Jiacheng Xu, Yang Liu, Yahui Zhou
- **URL**: <http://arxiv.org/abs/2507.01352v2>
- **Submitted**: 2025-07-02 04:40:29
- **Topic Keywords**: rag, ctr
- **Reason**: This paper focuses on reward models and preference datasets, which is not directly related to the user's interests in Information Retrieval, Search technologies, and query understanding. While it mentions human-AI synergy, it does not seem to be relevant to the user's specific areas of interest, such as ranking models, user behavior modeling, and deep semantic understanding.

#### Abstract
> Despite the critical role of reward models (RMs) in reinforcement learning
from human feedback (RLHF), current state-of-the-art open RMs perform poorly on
most existing evaluation benchmarks, failing to capture the spectrum of nuanced
and sophisticated human preferences. Even approaches that incorporate advanced
training techniques have not yielded meaningful performance improvements. We
hypothesize that this brittleness stems primarily from limitations in
preference datasets, which are often narrowly scoped, synthetically labeled, or
lack rigorous quality control. To address these challenges, we present a
large-scale preference dataset comprising 40 million preference pairs, named
SynPref-40M. To enable data curation at scale, we design a human-AI synergistic
two-stage pipeline that leverages the complementary strengths of human
annotation quality and AI scalability. In this pipeline, humans provide
verified annotations, while large language models perform automatic curation
based on human guidance. Training on this preference mixture, we introduce
Skywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B
parameters, trained on a carefully curated subset of 26 million preference
pairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile
across a wide range of capabilities, including alignment with human
preferences, objective correctness, safety, resistance to stylistic biases, and
best-of-N scaling, achieving state-of-the-art performance across seven major
reward model benchmarks. Ablation studies confirm that the effectiveness of our
approach stems not only from data scale but also from high-quality curation.
The Skywork-Reward-V2 series represents substantial progress in open reward
models, highlighting the untapped potential of existing preference datasets and
demonstrating how human-AI curation synergy can unlock significantly higher
data quality.

### 22. MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Dongyi Ding, Tiannan Wang, Chenghao Zhu, Meiling Tao, Yuchen Eleanor Jiang, Wangchunshu Zhou
- **URL**: <http://arxiv.org/abs/2507.01887v1>
- **Submitted**: 2025-07-02 16:57:01
- **Comment**: Work in progress
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on large and small language models, distillation, and teacher assistants, which is not directly related to information retrieval, search technologies, or query understanding. The concepts and techniques discussed in the paper are primarily from the natural language processing domain, but they do not seem to be applicable to the user's research interests.

#### Abstract
> Large language models (LLMs) excel at reasoning tasks requiring long thought
sequences for planning, reflection, and refinement. However, their substantial
model size and high computational demands are impractical for widespread
deployment. Yet, small language models (SLMs) often struggle to learn long-form
CoT reasoning due to their limited capacity, a phenomenon we refer to as the
"SLMs Learnability Gap". To address this, we introduce
\textbf{Mi}d-\textbf{Co}T \textbf{T}eacher \textbf{A}ssistant Distillation
(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA
employs intermediate-sized models as teacher assistants and utilizes
intermediate-length CoT sequences to bridge both the capacity and reasoning
length gaps. Our experiments on downstream tasks demonstrate that although SLMs
distilled from large teachers can perform poorly, by applying MiCoTA, they
achieve significant improvements in reasoning performance. Specifically,
Qwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and
3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and
GSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform
a quantitative experiment demonstrating that our method produces data more
closely aligned with base SLM distributions. Our insights pave the way for
future research into long-CoT data distillation for SLMs.

### 23. LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Reza Arabpour, Haitz S√°ez de Oc√°riz Borde, Anastasis Kratsios
- **URL**: <http://arxiv.org/abs/2507.01806v1>
- **Submitted**: 2025-07-02 15:24:47
- **Comment**: 5-page main paper (excluding references) + 11-page appendix, 3
  tables, 1 figure. Accepted to ICML 2025 Workshop on Efficient Systems for
  Foundation Models
- **Topic Keywords**: rag, rank
- **Reason**: This paper focuses on LoRA fine-tuning for Large Language Models, which is not directly related to Information Retrieval, Search technologies, or query understanding. The paper's abstract does not mention any relevance to user behavior modeling, ranking models, or real-time relevance optimization, making it an off-topic paper for the user's research interests.

#### Abstract
> Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language
Models (LLMs) by enabling parameter-efficient updates. However, their
widespread adoption remains limited by the reliance on GPU-based training. In
this work, we propose a theoretically grounded approach to LoRA fine-tuning
designed specifically for users with limited computational resources,
particularly those restricted to standard laptop CPUs. Our method learns a
meta-operator that maps any input dataset, represented as a probability
distribution, to a set of LoRA weights by leveraging a large bank of
pre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of
performing new gradient-based updates, our pipeline constructs adapters via
lightweight combinations of existing LoRAs directly on CPU. While the resulting
adapters do not match the performance of GPU-trained counterparts, they
consistently outperform the base Mistral model on downstream tasks, offering a
practical and accessible alternative to traditional GPU-based fine-tuning.

### 24. Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Robert Aufschl√§ger, Youssef Shoeb, Azarm Nowzad, Michael Heigl, Fabian Bally, Martin Schramm
- **URL**: <http://arxiv.org/abs/2507.01504v1>
- **Submitted**: 2025-07-02 09:10:33
- **Comment**: accepted for publication at the 2025 IEEE 28th International
  Conference on Intelligent Transportation Systems (ITSC 2025), taking place
  during November 18-21, 2025 in Gold Coast, Australia
- **Topic Keywords**: rag, search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The topic of person re-identification using cross-modal intelligence is outside your areas of focus, and the paper does not mention query understanding, ranking models, or user behavior modeling.

#### Abstract
> The collection and release of street-level recordings as Open Data play a
vital role in advancing autonomous driving systems and AI research. However,
these datasets pose significant privacy risks, particularly for pedestrians,
due to the presence of Personally Identifiable Information (PII) that extends
beyond biometric traits such as faces. In this paper, we present cRID, a novel
cross-modal framework combining Large Vision-Language Models, Graph Attention
Networks, and representation learning to detect textual describable clues of
PII and enhance person re-identification (Re-ID). Our approach focuses on
identifying and leveraging interpretable features, enabling the detection of
semantically meaningful PII beyond low-level appearance cues. We conduct a
systematic evaluation of PII presence in person image datasets. Our experiments
show improved performance in practical cross-dataset Re-ID scenarios, notably
from Market-1501 to CUHK03-np (detected), highlighting the framework's
practical utility. Code is available at https://github.com/RAufschlaeger/cRID.

### 25. Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yingqiang Gao, Kaede Johnson, David Froehlich, Luisa Carrer, Sarah Ebling
- **URL**: <http://arxiv.org/abs/2507.01479v1>
- **Submitted**: 2025-07-02 08:43:06
- **Topic Keywords**: rag, personalization
- **Reason**: The paper focuses on automatic text simplification for persons with intellectual disabilities, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions large language models, the application is not in the context of search or retrieval, and the paper does not address ranking models or user behavior modeling.

#### Abstract
> Automatic text simplification (ATS) aims to enhance language accessibility
for various target groups, particularly persons with intellectual disabilities.
Recent advancements in generative AI, especially large language models (LLMs),
have substantially improved the quality of machine-generated text
simplifications, thereby mitigating information barriers for the target group.
However, existing LLM-based ATS systems do not incorporate preference feedback
on text simplifications during training, resulting in a lack of personalization
tailored to the specific needs of target group representatives.
  In this work, we extend the standard supervised fine-tuning (SFT) approach
for adapting LLM-based ATS models by leveraging a computationally efficient LLM
alignment technique -- direct preference optimization (DPO). Specifically, we
post-train LLM-based ATS models using human feedback collected from persons
with intellectual disabilities, reflecting their preferences on paired text
simplifications generated by mainstream LLMs. Furthermore, we propose a
pipeline for developing personalized LLM-based ATS systems, encompassing data
collection, model selection, SFT and DPO post-training, and evaluation. Our
findings underscore the necessity of active participation of target group
persons in designing personalized AI accessibility solutions aligned with human
expectations. This work represents a step towards personalizing inclusive AI
systems at the target-group level, incorporating insights not only from text
simplification experts but also from target group persons themselves.

### 26. Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yoonseok Yang, Minjune Kim, Marlon Rondinelli, Keren Shao
- **URL**: <http://arxiv.org/abs/2507.01431v1>
- **Submitted**: 2025-07-02 07:33:19
- **Comment**: 7 pages, 5 figues, 1 table
- **Topic Keywords**: rag, www
- **Reason**: The paper focuses on handwritten STEM grading, which is not directly related to information retrieval, search technologies, or natural language processing. While it uses AI-powered language models, the application is specific to grading and does not address query understanding, ranking models, or user behavior modeling, making it only loosely relevant to the user's research interests.

#### Abstract
> Grading handwritten, open-ended responses remains a major bottleneck in large
university STEM courses. We introduce Pensieve (https://www.pensieve.co), an
AI-assisted grading platform that leverages large language models (LLMs) to
transcribe and evaluate student work, providing instructors with rubric-aligned
scores, transcriptions, and confidence ratings. Unlike prior tools that focus
narrowly on specific tasks like transcription or rubric generation, Pensieve
supports the entire grading pipeline-from scanned student submissions to final
feedback-within a human-in-the-loop interface.
  Pensieve has been deployed in real-world courses at over 20 institutions and
has graded more than 300,000 student responses. We present system details and
empirical results across four core STEM disciplines: Computer Science,
Mathematics, Physics, and Chemistry. Our findings show that Pensieve reduces
grading time by an average of 65%, while maintaining a 95.4% agreement rate
with instructor-assigned grades for high-confidence predictions.

### 27. MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Adamu Lawan, Juhua Pu, Haruna Yunusa, Jawad Muhammad, Muhammad Lawan
- **URL**: <http://arxiv.org/abs/2507.01213v1>
- **Submitted**: 2025-07-01 22:21:33
- **Comment**: 6, 1 figure
- **Topic Keywords**: query
- **Reason**: The paper focuses on Aspect-based Sentiment Analysis, which is a topic in Natural Language Processing (NLP), but it does not directly relate to Information Retrieval (IR) or Search technologies, which are the user's primary research interests. The paper's emphasis on deep learning models and their application in ABSA does not align with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language
Processing (NLP) task that extracts aspects from text and determines their
associated sentiments, enabling fine-grained analysis of user opinions.
Existing ABSA methods struggle to balance computational efficiency with high
performance: deep learning models often lack global context, transformers
demand significant computational resources, and Mamba-based approaches face
CUDA dependency and diminished local correlations. Recent advancements in
Extended Long Short-Term Memory (xLSTM) models, particularly their efficient
modeling of long-range dependencies, have significantly advanced the NLP
community. However, their potential in ABSA remains untapped. To this end, we
propose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework
integrating a bi-directional mLSTM architecture with forward and partially
flipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context
modeling by processing the initial sequence segment in reverse with dedicated
parameters, preserving critical short-range patterns. We further introduce an
mLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that
dynamically combines forward mLSTM outputs as query and key with PF-mLSTM
outputs as value, optimizing short-range dependency capture while maintaining
global context and efficiency. Experimental results on three benchmark datasets
demonstrate that MEGA outperforms state-of-the-art baselines, achieving
superior accuracy and efficiency in ABSA tasks.

### 28. STELLA: Self-Evolving LLM Agent for Biomedical Research

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Ruofan Jin, Zaixi Zhang, Mengdi Wang, Le Cong
- **URL**: <http://arxiv.org/abs/2507.02004v1>
- **Submitted**: 2025-07-01 20:52:01
- **Topic Keywords**: rag, search
- **Reason**: This paper is not relevant to your research interests, as it focuses on biomedical research and AI agents, which is outside your primary area of interest in Information Retrieval and Search technologies. The paper's emphasis on bioinformatics tools and biomedical benchmarks is also not aligned with your background in e-commerce and interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

### 29. DIY-MKG: An LLM-Based Polyglot Language Learning System

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Kenan Tang, Yanhong Li, Yao Qin
- **URL**: <http://arxiv.org/abs/2507.01872v1>
- **Submitted**: 2025-07-02 16:38:51
- **Comment**: Submitted to EMNLP 2025 System Demonstration
- **Topic Keywords**: rag
- **Reason**: The paper focuses on language learning and knowledge graph construction, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it mentions Large Language Models, the application is in a different domain and does not address query understanding, ranking models, or user behavior modeling.

#### Abstract
> Existing language learning tools, even those powered by Large Language Models
(LLMs), often lack support for polyglot learners to build linguistic
connections across vocabularies in multiple languages, provide limited
customization for individual learning paces or needs, and suffer from
detrimental cognitive offloading. To address these limitations, we design
Do-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system
that supports polyglot language learning. DIY-MKG allows the user to build
personalized vocabulary knowledge graphs, which are constructed by selective
expansion with related words suggested by an LLM. The system further enhances
learning through rich annotation capabilities and an adaptive review module
that leverages LLMs for dynamic, personalized quiz generation. In addition,
DIY-MKG allows users to flag incorrect quiz questions, simultaneously
increasing user engagement and providing a feedback loop for prompt refinement.
Our evaluation of LLM-based components in DIY-MKG shows that vocabulary
expansion is reliable and fair across multiple languages, and that the
generated quizzes are highly accurate, validating the robustness of DIY-MKG.

### 30. Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Samridhi Raj Sinha, Rajvee Sheth, Abhishek Upperwal, Mayank Singh
- **URL**: <http://arxiv.org/abs/2507.01853v1>
- **Submitted**: 2025-07-02 16:07:54
- **Topic Keywords**: rag
- **Reason**: The paper focuses on evaluating large language models in Indian languages, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the topic of language models, the scope is limited to evaluation frameworks and does not explore ranking models, user behavior modeling, or real-time relevance optimization, which are core areas of interest.

#### Abstract
> The rapid advancement of Large Language Models (LLMs) has intensified the
need for evaluation frameworks that go beyond English centric benchmarks and
address the requirements of linguistically diverse regions such as India. We
present EKA-EVAL, a unified and production-ready evaluation framework that
integrates over 35 benchmarks, including 10 Indic-specific datasets, spanning
categories like reasoning, mathematics, tool use, long-context understanding,
and reading comprehension. Compared to existing Indian language evaluation
tools, EKA-EVAL offers broader benchmark coverage, with built-in support for
distributed inference, quantization, and multi-GPU usage. Our systematic
comparison positions EKA-EVAL as the first end-to-end, extensible evaluation
suite tailored for both global and Indic LLMs, significantly lowering the
barrier to multilingual benchmarking. The framework is open-source and publicly
available at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA
initiative (https://eka.soket.ai), which aims to scale up to over 100
benchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.

### 31. Low-Perplexity LLM-Generated Sequences and Where To Find Them

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Arthur Wuhrmann, Anastasiia Kucherenko, Andrei Kucharavy
- **URL**: <http://arxiv.org/abs/2507.01844v1>
- **Submitted**: 2025-07-02 15:58:51
- **Comment**: Camera-ready version. Accepted to ACL 2025. 10 pages, 4 figures, 6
  tables
- **Topic Keywords**: rag
- **Reason**: The paper focuses on Large Language Models (LLMs) and their training data, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on the topic of model behavior, it does not address ranking models, user behavior modeling, or real-time relevance optimization, making it only loosely relevant to your research interests.

#### Abstract
> As Large Language Models (LLMs) become increasingly widespread, understanding
how specific training data shapes their outputs is crucial for transparency,
accountability, privacy, and fairness. To explore how LLMs leverage and
replicate their training data, we introduce a systematic approach centered on
analyzing low-perplexity sequences - high-probability text spans generated by
the model. Our pipeline reliably extracts such long sequences across diverse
topics while avoiding degeneration, then traces them back to their sources in
the training data. Surprisingly, we find that a substantial portion of these
low-perplexity spans cannot be mapped to the corpus. For those that do match,
we quantify the distribution of occurrences across source documents,
highlighting the scope and nature of verbatim recall and paving a way toward
better understanding of how LLMs training data impacts their behavior.

### 32. Probing Evaluation Awareness of Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jord Nguyen, Khiem Hoang, Carlo Leonardo Attubato, Felix Hofst√§tter
- **URL**: <http://arxiv.org/abs/2507.01786v1>
- **Submitted**: 2025-07-02 15:12:43
- **Comment**: Technical AI Governance Workshop, ICML (Poster)
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus is on language models, evaluation awareness, and safety evaluations, which are outside the scope of the user's primary research interests.

#### Abstract
> Language models can distinguish between testing and deployment phases -- a
capability known as evaluation awareness. This has significant safety and
policy implications, potentially undermining the reliability of evaluations
that are central to AI governance frameworks and voluntary industry
commitments. In this paper, we study evaluation awareness in
Llama-3.3-70B-Instruct. We show that linear probes can separate real-world
evaluation and deployment prompts, suggesting that current models internally
represent this distinction. We also find that current safety evaluations are
correctly classified by the probes, suggesting that they already appear
artificial or inauthentic to models. Our findings underscore the importance of
ensuring trustworthy evaluations and understanding deceptive capabilities. More
broadly, our work showcases how model internals may be leveraged to support
blackbox methods in safety audits, especially for future models more competent
at evaluation awareness and deception.

### 33. Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ismail Labiad, Mathurin Videau, Matthieu Kowalski, Marc Schoenauer, Alessandro Leite, Julia Kempe, Olivier Teytaud
- **URL**: <http://arxiv.org/abs/2507.01752v1>
- **Submitted**: 2025-07-02 14:29:30
- **Topic Keywords**: rag
- **Reason**: This paper focuses on black box optimization methods for large language models, which is not directly related to information retrieval, search technologies, or query understanding. The paper's emphasis on privacy and security concerns is also not a primary focus of your research interests.

#### Abstract
> Gradient-based optimization is the workhorse of deep learning, offering
efficient and scalable training via backpropagation. However, its reliance on
large volumes of labeled data raises privacy and security concerns such as
susceptibility to data poisoning attacks and the risk of overfitting. In
contrast, black box optimization methods, which treat the model as an opaque
function, relying solely on function evaluations to guide optimization, offer a
promising alternative in scenarios where data access is restricted, adversarial
risks are high, or overfitting is a concern. However, black box methods also
pose significant challenges, including poor scalability to high-dimensional
parameter spaces, as prevalent in large language models (LLMs), and high
computational costs due to reliance on numerous model evaluations. This paper
introduces BBoxER, an evolutionary black-box method for LLM post-training that
induces an information bottleneck via implicit compression of the training
data. Leveraging the tractability of information flow, we provide strong
theoretical bounds on generalization, differential privacy, susceptibility to
data poisoning attacks, and robustness to extraction attacks. BBoxER operates
on top of pre-trained LLMs, offering a lightweight and modular enhancement
suitable for deployment in restricted or privacy-sensitive environments, in
addition to non-vacuous generalization guarantees. In experiments with LLMs, we
demonstrate empirically that Retrofitting methods are able to learn, showing
how a few iterations of BBoxER improve performance and generalize well on a
benchmark of reasoning datasets. This positions BBoxER as an attractive add-on
on top of gradient-based optimization.

### 34. Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Rifki Afina Putri
- **URL**: <http://arxiv.org/abs/2507.01645v1>
- **Submitted**: 2025-07-02 12:17:55
- **Comment**: AMLDS 2025
- **Topic Keywords**: rag
- **Reason**: The paper focuses on adapting language models to Indonesian local languages, which is not directly related to Information Retrieval, Search technologies, or query understanding. Although it touches on language transferability, it does not explore ranking models, user behavior modeling, or real-time relevance optimization, which are core aspects of your research interests.

#### Abstract
> In this paper, we investigate the transferability of pre-trained language
models to low-resource Indonesian local languages through the task of sentiment
analysis. We evaluate both zero-shot performance and adapter-based transfer on
ten local languages using models of different types: a monolingual Indonesian
BERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based
approach called MAD-X. To better understand model behavior, we group the target
languages into three categories: seen (included during pre-training), partially
seen (not included but linguistically related to seen languages), and unseen
(absent and unrelated in pre-training data). Our results reveal clear
performance disparities across these groups: multilingual models perform best
on seen languages, moderately on partially seen ones, and poorly on unseen
languages. We find that MAD-X significantly improves performance, especially
for seen and partially seen languages, without requiring labeled data in the
target language. Additionally, we conduct a further analysis on tokenization
and show that while subword fragmentation and vocabulary overlap with
Indonesian correlate weakly with prediction quality, they do not fully explain
the observed performance. Instead, the most consistent predictor of transfer
success is the model's prior exposure to the language, either directly or
through a related language.

### 35. Self-Guided Process Reward Optimization with Redefined Step-wise Advantage for Process Reinforcement Learning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Wu Fei, Hao Kong, Shuxian Liang, Yang Lin, Yibo Yang, Jing Tang, Lei Chen, Xiansheng Hua
- **URL**: <http://arxiv.org/abs/2507.01551v2>
- **Submitted**: 2025-07-02 10:05:14
- **Topic Keywords**: rag
- **Reason**: The paper focuses on Process Reinforcement Learning, which is not directly related to Information Retrieval or Search technologies. Although it mentions Large Language Models, the context is not relevant to query understanding, ranking models, or user behavior modeling. The paper's topics, such as process reward optimization and masked step advantage, are not aligned with the user's research interests.

#### Abstract
> Process Reinforcement Learning~(PRL) has demonstrated considerable potential
in enhancing the reasoning capabilities of Large Language Models~(LLMs).
However, introducing additional process reward models incurs substantial
computational overhead, and there is no unified theoretical framework for
process-level advantage estimation. To bridge this gap, we propose
\textbf{S}elf-Guided \textbf{P}rocess \textbf{R}eward
\textbf{O}ptimization~(\textbf{SPRO}), a novel framework that enables
process-aware RL through two key innovations: (1) we first theoretically
demonstrate that process rewards can be derived intrinsically from the policy
model itself, and (2) we introduce well-defined cumulative process rewards and
\textbf{M}asked \textbf{S}tep \textbf{A}dvantage (\textbf{MSA}), which
facilitates rigorous step-wise action advantage estimation within shared-prompt
sampling groups. Our experimental results demonstrate that SPRO outperforms
vaniila GRPO with 3.4x higher training efficiency and a 17.5\% test accuracy
improvement. Furthermore, SPRO maintains a stable and elevated policy entropy
throughout training while reducing the average response length by approximately
$1/3$, evidencing sufficient exploration and prevention of reward hacking.
Notably, SPRO incurs no additional computational overhead compared to
outcome-supervised RL methods such as GRPO, which benefit industrial
implementation.

### 36. Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Wen Zhan, Ziqun Hua, Peiyue Lin, Yunfei Chen
- **URL**: <http://arxiv.org/abs/2507.01548v2>
- **Submitted**: 2025-07-02 10:00:12
- **Comment**: A version of this manuscript has been submitted to the [IASDR 2025
  Conference](https://iasdr2025.org/) and is currently under review
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The focus on AI-assisted co-creation, narrative bridges, and elderly migrants is outside the scope of your expertise and interests.

#### Abstract
> This paper explores how older adults, particularly aging migrants in urban
China, can engage AI-assisted co-creation to express personal narratives that
are often fragmented, underrepresented, or difficult to verbalize. Through a
pilot workshop combining oral storytelling and the symbolic reconstruction of
Hanzi, participants shared memories of migration and recreated new character
forms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),
together with physical materials. Supported by human facilitation and a soft AI
presence, participants transformed lived experience into visual and tactile
expressions without requiring digital literacy. This approach offers new
perspectives on human-AI collaboration and aging by repositioning AI not as a
content producer but as a supportive mechanism, and by supporting narrative
agency within sociotechnical systems.

### 37. DARTS: A Dual-View Attack Framework for Targeted Manipulation in Federated Sequential Recommendation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Qitao Qin, Yucong Luo, Zhibo Chu
- **URL**: <http://arxiv.org/abs/2507.01383v1>
- **Submitted**: 2025-07-02 05:57:09
- **Comment**: 10 pages. arXiv admin note: substantial text overlap with
  arXiv:2409.07500; text overlap with arXiv:2212.05399 by other authors
- **Topic Keywords**: recommend, search
- **Reason**: The paper focuses on federated sequential recommendation and targeted attacks, which is not directly related to the user's interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions recommendation models, the context is different from the user's focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Federated recommendation (FedRec) preserves user privacy by enabling
decentralized training of personalized models, but this architecture is
inherently vulnerable to adversarial attacks. Significant research has been
conducted on targeted attacks in FedRec systems, motivated by commercial and
social influence considerations. However, much of this work has largely
overlooked the differential robustness of recommendation models. Moreover, our
empirical findings indicate that existing targeted attack methods achieve only
limited effectiveness in Federated Sequential Recommendation(FSR) tasks. Driven
by these observations, we focus on investigating targeted attacks in FSR and
propose a novel dualview attack framework, named DV-FSR. This attack method
uniquely combines a sampling-based explicit strategy with a contrastive
learning-based implicit gradient strategy to orchestrate a coordinated attack.
Additionally, we introduce a specific defense mechanism tailored for targeted
attacks in FSR, aiming to evaluate the mitigation effects of the attack method
we proposed. Extensive experiments validate the effectiveness of our proposed
approach on representative sequential models. Our codes are publicly available.

### 38. Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Nifu Dan, Yujun Cai, Yiwei Wang
- **URL**: <http://arxiv.org/abs/2507.01334v2>
- **Submitted**: 2025-07-02 03:51:16
- **Topic Keywords**: ctr
- **Reason**: The paper focuses on the application of Large Language Models (LLMs) to physics problem-solving, which is not directly related to Information Retrieval, Search technologies, or Natural Language Processing. While it explores advanced instruction-tuned reasoning models, the topic is not aligned with the user's primary research interests.

#### Abstract
> Navigating the complexities of physics reasoning has long been a difficult
task for Large Language Models (LLMs), requiring a synthesis of profound
conceptual understanding and adept problem-solving techniques. In this study,
we investigate the application of advanced instruction-tuned reasoning models,
such as Deepseek-R1, to address a diverse spectrum of physics problems curated
from the challenging SciBench benchmark. Our comprehensive experimental
evaluation reveals the remarkable capabilities of reasoning models. Not only do
they achieve state-of-the-art accuracy in answering intricate physics
questions, but they also generate distinctive reasoning patterns that emphasize
on symbolic derivation. Furthermore, our findings indicate that even for these
highly sophisticated reasoning models, the strategic incorporation of few-shot
prompting can still yield measurable improvements in overall accuracy,
highlighting the potential for continued performance gains.

### 39. La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Kai Liu, Bowen Xu, Shaoyu Wu, Xin Chen, Hao Zhou, Yongliang Tao, Lulu Hu
- **URL**: <http://arxiv.org/abs/2507.01299v1>
- **Submitted**: 2025-07-02 02:36:03
- **Comment**: ICML 2025 Acceptance
- **Topic Keywords**: rag
- **Reason**: The paper focuses on Large Language Model (LLM) efficiency and activation sparsity, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper mentions NLP, it is primarily concerned with improving LLM efficiency, which is not a central theme in the user's research.

#### Abstract
> Activation sparsity can reduce the computational overhead and memory
transfers during the forward pass of Large Language Model (LLM) inference.
Existing methods face limitations, either demanding time-consuming recovery
training that hinders real-world adoption, or relying on empirical
magnitude-based pruning, which causes fluctuating sparsity and unstable
inference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse
Activation), a novel method for activation sparsification designed to improve
LLM efficiency without requiring additional training or magnitude-based
pruning. We leverage layerwise orthogonal rotations to transform input
activations into rotated forms that are more suitable for sparsification. By
employing a Top-K selection approach within the rotated activations, we achieve
consistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA
is effective across various sizes and types of LLMs, demonstrating minimal
performance degradation and robust inference acceleration. Specifically, for
LLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a
consistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in
zero-shot tasks compared to the dense model to just 0.54%, while surpassing
TEAL by 1.77% and CATS by 17.14%.

### 40. Test-Time Scaling with Reflective Generative Model

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Zixiao Wang, Yuxin Wang, Xiaorui Wang, Mengting Xing, Jie Gao, Jianjun Xu, Guangcan Liu, Chenhui Jin, Zhuo Wang, Shengzhuo Zhang, Hongtao Xie
- **URL**: <http://arxiv.org/abs/2507.01951v1>
- **Submitted**: 2025-07-02 17:58:01
- **Topic Keywords**: search
- **Reason**: The paper focuses on a reflective generative model for test-time scaling, which is not directly related to information retrieval, search technologies, or query understanding. While it involves some NLP concepts, the primary focus is on generative models and process reward models, which are not within the user's core research themes.

#### Abstract
> We introduce our first reflective generative model MetaStone-S1, which
obtains OpenAI o3's performance via the self-supervised process reward model
(SPRM). Through sharing the backbone network and using task-specific heads for
next token prediction and process scoring respectively, SPRM successfully
integrates the policy model and process reward model(PRM) into a unified
interface without extra process annotation, reducing over 99% PRM parameters
for efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable
for test time scaling (TTS), and we provide three reasoning effort modes (low,
medium, and high), based on the controllable thinking length. Moreover, we
empirically establish a scaling law that reveals the relationship between total
thinking computation and TTS performance. Experiments demonstrate that our
MetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with
only 32B parameter size. To support the research community, we have
open-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.

### 41. AI4Research: A Survey of Artificial Intelligence for Scientific Research

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Qiguang Chen, Mingda Yang, Libo Qin, Jinhao Liu, Zheng Yan, Jiannan Guan, Dengyun Peng, Yiyan Ji, Hanjing Li, Mengkang Hu, Yimeng Zhang, Yihao Liang, Yuhang Zhou, Jiaqi Wang, Zhi Chen, Wanxiang Che
- **URL**: <http://arxiv.org/abs/2507.01903v1>
- **Submitted**: 2025-07-02 17:19:20
- **Comment**: Preprint
- **Topic Keywords**: search
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The focus is on Artificial Intelligence for Scientific Research, which is a broader and more general topic. While the paper mentions language models, it does not specifically discuss query understanding, ranking models, or user behavior modeling, which are key areas of interest for you.

#### Abstract
> Recent advancements in artificial intelligence (AI), particularly in large
language models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated
remarkable capabilities in complex domains such as logical reasoning and
experimental coding. Motivated by these advancements, numerous studies have
explored the application of AI in the innovation process, particularly in the
context of scientific research. These AI technologies primarily aim to develop
systems that can autonomously conduct research processes across a wide range of
scientific disciplines. Despite these significant strides, a comprehensive
survey on AI for Research (AI4Research) remains absent, which hampers our
understanding and impedes further development in this field. To address this
gap, we present a comprehensive survey and offer a unified perspective on
AI4Research. Specifically, the main contributions of our work are as follows:
(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify
five mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key
research gaps and highlight promising future directions, focusing on the rigor
and scalability of automated experiments, as well as the societal impact. (3)
Abundant applications and resources: Finally, we compile a wealth of resources,
including relevant multidisciplinary applications, data corpora, and tools. We
hope our work will provide the research community with quick access to these
resources and stimulate innovative breakthroughs in AI4Research.

### 42. The Anatomy of Evidence: An Investigation Into Explainable ICD Coding

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Katharina Beckh, Elisa Studeny, Sujan Sai Gannamaneni, Dario Antweiler, Stefan R√ºping
- **URL**: <http://arxiv.org/abs/2507.01802v1>
- **Submitted**: 2025-07-02 15:21:29
- **Comment**: Accepted to ACL 2025 Findings
- **Topic Keywords**: recommend
- **Reason**: The paper is not relevant to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus is on medical coding and evidence extraction, which is outside the user's primary areas of interest.

#### Abstract
> Automatic medical coding has the potential to ease documentation and billing
processes. For this task, transparency plays an important role for medical
coders and regulatory bodies, which can be achieved using explainability
methods. However, the evaluation of these approaches has been mostly limited to
short text and binary settings due to a scarcity of annotated data. Recent
efforts by Cheng et al. (2023) have introduced the MDACE dataset, which
provides a valuable resource containing code evidence in clinical records. In
this work, we conduct an in-depth analysis of the MDACE dataset and perform
plausibility evaluation of current explainable medical coding systems from an
applied perspective. With this, we contribute to a deeper understanding of
automatic medical coding and evidence extraction. Our findings reveal that
ground truth evidence aligns with code descriptions to a certain degree. An
investigation into state-of-the-art approaches shows a high overlap with ground
truth evidence. We propose match measures and highlight success and failure
cases. Based on our findings, we provide recommendations for developing and
evaluating explainable medical coding systems.

### 43. Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Matteo Di Cristofaro
- **URL**: <http://arxiv.org/abs/2507.01764v1>
- **Submitted**: 2025-07-02 14:46:26
- **Comment**: Author submitted manuscript
- **Topic Keywords**: search
- **Reason**: This paper focuses on tokenization and corpus linguistics, which is not directly related to Information Retrieval, Search technologies, or query understanding. Although it touches on preprocessing and data fidelity, the context is linguistic analysis rather than search or ranking models.

#### Abstract
> Tokenisation - "the process of splitting text into atomic parts" (Brezina &
Timperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides
the basis for any applicable quantitative method (e.g. collocations) while
ensuring the reliability of qualitative approaches. This paper examines how
discrepancies in tokenisation affect the representation of language data and
the validity of analytical findings: investigating the challenges posed by
emojis and homoglyphs, the study highlights the necessity of preprocessing
these elements to maintain corpus fidelity to the source data. The research
presents methods for ensuring that digital texts are accurately represented in
corpora, thereby supporting reliable linguistic analysis and guaranteeing the
repeatability of linguistic interpretations. The findings emphasise the
necessity of a detailed understanding of both linguistic and technical aspects
involved in digital textual data to enhance the accuracy of corpus analysis,
and have significant implications for both quantitative and qualitative
approaches in corpus-based research.

### 44. ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Kai Chen, Ruiyuan Gao, Lanqing Hong, Hang Xu, Xu Jia, Holger Caesar, Dengxin Dai, Bingbing Liu, Dzmitry Tsishkou, Songcen Xu, Chunjing Xu, Qiang Xu, Huchuan Lu, Dit-Yan Yeung
- **URL**: <http://arxiv.org/abs/2507.01735v1>
- **Submitted**: 2025-07-02 14:10:25
- **Comment**: ECCV 2024. Workshop page: https://coda-dataset.github.io/w-coda2024/
- **Topic Keywords**: search
- **Reason**: The paper is not relevant to the user's research interests as it focuses on autonomous driving, multimodal perception, and comprehension, which are not directly related to information retrieval, search technologies, or natural language processing. The topics of corner cases, scene understanding, and generation are also not aligned with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> In this paper, we present details of the 1st W-CODA workshop, held in
conjunction with the ECCV 2024. W-CODA aims to explore next-generation
solutions for autonomous driving corner cases, empowered by state-of-the-art
multimodal perception and comprehension techniques. 5 Speakers from both
academia and industry are invited to share their latest progress and opinions.
We collect research papers and hold a dual-track challenge, including both
corner case scene understanding and generation. As the pioneering effort, we
will continuously bridge the gap between frontier autonomous driving techniques
and fully intelligent, reliable self-driving agents robust towards corner
cases.

### 45. Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Zeyu Huang, Tianhao Cheng, Zihan Qiu, Zili Wang, Yinghui Xu, Edoardo M. Ponti, Ivan Titov
- **URL**: <http://arxiv.org/abs/2507.01679v1>
- **Submitted**: 2025-07-02 13:04:09
- **Comment**: Work in progress
- **Topic Keywords**: search
- **Reason**: The paper focuses on post-training techniques for large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions reinforcement learning, it does not specifically address ranking models or user behavior modeling, making it only loosely relevant to the user's research interests.

#### Abstract
> Existing post-training techniques for large language models are broadly
categorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning
(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking
demonstration data but can lead to problematic generalization as a form of
behavior cloning. Conversely, RFT can significantly enhance a model's
performance but is prone to learn unexpected behaviors, and its performance is
highly sensitive to the initial policy. In this paper, we propose a unified
view of these methods and introduce Prefix-RFT, a hybrid approach that
synergizes learning from both demonstration and exploration. Using mathematical
reasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is
both simple and effective. It not only surpasses the performance of standalone
SFT and RFT but also outperforms parallel mixed-policy RFT methods. A key
advantage is its seamless integration into existing open-source frameworks,
requiring only minimal modifications to the standard RFT pipeline. Our analysis
highlights the complementary nature of SFT and RFT, and validates that
Prefix-RFT effectively harmonizes these two learning paradigms. Furthermore,
ablation studies confirm the method's robustness to variations in the quality
and quantity of demonstration data. We hope this work offers a new perspective
on LLM post-training, suggesting that a unified paradigm that judiciously
integrates demonstration and exploration could be a promising direction for
future research.

### 46. T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Yuehang Si, Zefan Zeng, Jincai Huang, Qing Cheng
- **URL**: <http://arxiv.org/abs/2507.01597v1>
- **Submitted**: 2025-07-02 11:02:37
- **Topic Keywords**: search
- **Reason**: The paper focuses on Temporal Knowledge Graph Reasoning, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it involves modeling and training, the context is different from the user's primary interests in IR and NLP.

#### Abstract
> Temporal Knowledge Graph (TKG) is an efficient method for describing the
dynamic development of facts along a timeline. Most research on TKG reasoning
(TKGR) focuses on modelling the repetition of global facts and designing
patterns of local historical facts. However, they face two significant
challenges: inadequate modeling of the event distribution shift between
training and test samples, and reliance on random entity substitution for
generating negative samples, which often results in low-quality sampling. To
this end, we propose a novel distributional feature modeling approach for
training TKGR models, Test-Time Training-guided Distribution shift Modelling
(T3DM), to adjust the model based on distribution shift and ensure the global
consistency of model reasoning. In addition, we design a negative-sampling
strategy to generate higher-quality negative quadruples based on adversarial
training. Extensive experiments show that T3DM provides better and more robust
results than the state-of-the-art baselines in most cases.

### 47. Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Cindy Lie Tabuse, David Restepo, Carolina Gracitelli, Fernando Korn Malerbi, Caio Regatieri, Luis Filipe Nakayama
- **URL**: <http://arxiv.org/abs/2507.01278v1>
- **Submitted**: 2025-07-02 01:35:59
- **Topic Keywords**: recommend
- **Reason**: This paper is not relevant to your research interests as it focuses on evaluating large language models for ophthalmic decision-making, which is outside the scope of information retrieval, search technologies, and natural language processing. The paper's application in education, documentation, or image annotation workflows in ophthalmology is also not directly related to your interests.

#### Abstract
> Large language models (LLMs) can simulate clinical reasoning based on natural
language prompts, but their utility in ophthalmology is largely unexplored.
This study evaluated GPT-4's ability to interpret structured textual
descriptions of retinal fundus photographs and simulate clinical decisions for
diabetic retinopathy (DR) and glaucoma screening, including the impact of
adding real or synthetic clinical metadata. We conducted a retrospective
diagnostic validation study using 300 annotated fundus images. GPT-4 received
structured prompts describing each image, with or without patient metadata. The
model was tasked with assigning an ICDR severity score, recommending DR
referral, and estimating the cup-to-disc ratio for glaucoma referral.
Performance was evaluated using accuracy, macro and weighted F1 scores, and
Cohen's kappa. McNemar's test and change rate analysis were used to assess the
influence of metadata. GPT-4 showed moderate performance for ICDR
classification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),
driven mainly by correct identification of normal cases. Performance improved
in the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For
glaucoma referral, performance was poor across all settings (accuracy ~78%, F1
<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes
(McNemar p > 0.05), and predictions remained consistent across conditions.
GPT-4 can simulate basic ophthalmic decision-making from structured prompts but
lacks precision for complex tasks. While not suitable for clinical use, LLMs
may assist in education, documentation, or image annotation workflows in
ophthalmology.

---

