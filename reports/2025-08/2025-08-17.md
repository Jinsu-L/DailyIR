# Daily Papers Report - 2025-08-17

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity

- **LLM Score**: 8
- **Keyword Score**: 11
- **Authors**: Bowen Zhang, Zixin Song, Chunquan Chen, Qian-Wen Zhang, Di Yin, Xing Sun
- **URL**: <http://arxiv.org/abs/2508.11442v1>
- **Submitted**: 2025-08-15 12:46:35
- **Topic Keywords**: information retrieval, ranking, retrieval, rank, acl
- **Reason**: The paper focuses on representation learning for Information Retrieval (IR) and Semantic Textual Similarity (STS), which aligns with your interest in IR and NLP. The proposed framework, CoDiEmb, addresses the challenge of negative transfer between IR and STS, which is a relevant topic in query understanding and ranking models. While the paper does not specifically mention user behavior modeling or click models, its contributions to unified representation learning and joint optimization are likely to be useful in your research.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Representation Learning in Information Retrieval and Semantic Textual Similarity
- **Aim**: To develop a unified framework for joint training of IR and STS tasks, addressing performance trade-offs and improving embedding space geometry
- **Rationale**: CoDiEmb integrates task-specialized objectives, delta-guided model fusion, and efficient single-stage training to decouple task-specific learning signals and mitigate cross-task trade-offs
- **Ground**: The framework employs specialized loss functions, dynamic sampling, and hierarchical model fusion to learn robust and generalizable text representations
- **Experiment**: CoDiEmb is evaluated on the CMTEB leaderboard, achieving superior performance on both IR and STS tasks, outperforming baseline methods and demonstrating a more expressive and isotropic embedding space
- **Takeaway**: The paper presents a novel framework that advances the state-of-the-art in text representation learning, highlighting the importance of well-designed loss functions and synergistic model design for achieving robust and generalizable text representations

#### Abstract
> Learning unified text embeddings that excel across diverse downstream tasks
is a central goal in representation learning, yet negative transfer remains a
persistent obstacle. This challenge is particularly pronounced when jointly
training a single encoder for Information Retrieval (IR) and Semantic Textual
Similarity (STS), two essential but fundamentally disparate tasks for which
naive co-training typically yields steep performance trade-offs. We argue that
resolving this conflict requires systematically decoupling task-specific
learning signals throughout the training pipeline. To this end, we introduce
CoDiEmb, a unified framework that reconciles the divergent requirements of IR
and STS in a collaborative yet distinct manner. CoDiEmb integrates three key
innovations for effective joint optimization: (1) Task-specialized objectives
paired with a dynamic sampler that forms single-task batches and balances
per-task updates, thereby preventing gradient interference. For IR, we employ a
contrastive loss with multiple positives and hard negatives, augmented by
cross-device sampling. For STS, we adopt order-aware objectives that directly
optimize correlation and ranking consistency. (2) A delta-guided model fusion
strategy that computes fine-grained merging weights for checkpoints by
analyzing each parameter's deviation from its pre-trained initialization,
proving more effective than traditional Model Soups. (3) An efficient,
single-stage training pipeline that is simple to implement and converges
stably. Extensive experiments on 15 standard IR and STS benchmarks across three
base encoders validate CoDiEmb. Our results and analysis demonstrate that the
framework not only mitigates cross-task trade-offs but also measurably improves
the geometric properties of the embedding space.

---

### 2. Role-Augmented Intent-Driven Generative Search Engine Optimization

- **LLM Score**: 8
- **Keyword Score**: 11
- **Authors**: Xiaolu Chen, Haojie Wu, Jie Bao, Zhen Chen, Yong Liao, Hu Huang
- **URL**: <http://arxiv.org/abs/2508.11158v1>
- **Submitted**: 2025-08-15 02:08:55
- **Comment**: 7 pages, 5 figures
- **Topic Keywords**: information retrieval, query, rag, retrieval, search
- **Reason**: The paper's focus on Generative Search Engines, Large Language Models, and Retrieval-Augmented Generation aligns with your interest in Information Retrieval and Search technologies. The paper's emphasis on understanding search intent and optimizing content for generative retrieval contexts is also relevant to your research on query understanding and ranking models. However, the paper's primary focus on generative search engines and content optimization may not be directly related to your work on user behavior modeling and click models.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Generative Search Engine Optimization (G-SEO)
- **Aim**: To propose a novel approach to G-SEO called Role-Augmented Intent-Driven Generative Search Engine Optimization (RAID G-SEO) that addresses the limitations of traditional SEO strategies in the context of Generative Search Engines (GSEs)
- **Rationale**: Traditional SEO strategies are limited in GSEs, and search intent modeling is crucial for targeted content enhancement
- **Ground**: The RAID G-SEO framework consists of a four-stage pipeline: content summarization, intent inference and refinement, step planning, and content rewriting, with a key component being the 4W multi-role deep reflection module
- **Experiment**: Experimental results demonstrate that search intent serves as an effective signal for guiding content optimization, yielding significant improvements over single-aspect baseline approaches in both subjective impressions and objective content visibility within GSE responses
- **Takeaway**: The proposed RAID G-SEO method provides a principled and structured optimization path for content creators to improve the visibility of their content in GSE-generated outputs, while preserving semantic integrity and enhancing user experience

#### Abstract
> Generative Search Engines (GSEs), powered by Large Language Models (LLMs) and
Retrieval-Augmented Generation (RAG), are reshaping information retrieval.
While commercial systems (e.g., BingChat, Perplexity.ai) demonstrate impressive
semantic synthesis capabilities, their black-box nature fundamentally
undermines established Search Engine Optimization (SEO) practices. Content
creators face a critical challenge: their optimization strategies, effective in
traditional search engines, are misaligned with generative retrieval contexts,
resulting in diminished visibility. To bridge this gap, we propose a
Role-Augmented Intent-Driven Generative Search Engine Optimization (G-SEO)
method, providing a structured optimization pathway tailored for GSE scenarios.
Our method models search intent through reflective refinement across diverse
informational roles, enabling targeted content enhancement. To better evaluate
the method under realistic settings, we address the benchmarking limitations of
prior work by: (1) extending the GEO dataset with diversified query variations
reflecting real-world search scenarios and (2) introducing G-Eval 2.0, a
6-level LLM-augmented evaluation rubric for fine-grained human-aligned
assessment. Experimental results demonstrate that search intent serves as an
effective signal for guiding content optimization, yielding significant
improvements over single-aspect baseline approaches in both subjective
impressions and objective content visibility within GSE responses.

---

### 3. Retrieval-augmented reasoning with lean language models

- **LLM Score**: 7
- **Keyword Score**: 15
- **Authors**: Ryan Sze-Yin Chan, Federico Nanni, Tomas Lazauskas, Rosie Wood, Penelope Yong, Lionel Tarassenko, Mark Girolami, James Geddes, Andrew Duncan
- **URL**: <http://arxiv.org/abs/2508.11386v1>
- **Submitted**: 2025-08-15 10:38:15
- **Topic Keywords**: retriever, query, queries, rag, retrieval augmented generation, retrieval
- **Reason**: The paper explores retrieval-augmented reasoning with lean language models, which is related to query understanding and ranking models in Information Retrieval. The use of lightweight backbone models and fine-tuned Qwen2.5-Instruct models is also relevant to Learning to Rank. However, the focus on domain-specific fine-tuning and summarisation-based document compression is not directly aligned with the user's primary interests in real-time relevance optimization and deep semantic understanding.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: Reasoning and Retrieval Augmented Generation (RAG) in Language Models
- **Aim**: Develop a performant and privacy-preserving RAG solution for resource-constrained or secure environments
- **Rationale**: Integrate dense retriever with fine-tuned Qwen2.5-Instruct models, using synthetic query generation and reasoning traces over a curated corpus
- **Ground**: Evaluate the impact of summarization-based document compression, synthetic data design, and reasoning-aware fine-tuning on model performance
- **Experiment**: Domain-specific fine-tuning approach yields substantial gains in answer accuracy and consistency, approaching frontier-level performance
- **Takeaway**: The proposed approach achieves substantial gains in answer accuracy and consistency, making it feasible for local deployment in resource-constrained or secure environments

#### Abstract
> This technical report details a novel approach to combining reasoning and
retrieval augmented generation (RAG) within a single, lean language model
architecture. While existing RAG systems typically rely on large-scale models
and external APIs, our work addresses the increasing demand for performant and
privacy-preserving solutions deployable in resource-constrained or secure
environments. Building on recent developments in test-time scaling and
small-scale reasoning models, we develop a retrieval augmented conversational
agent capable of interpreting complex, domain-specific queries using a
lightweight backbone model. Our system integrates a dense retriever with
fine-tuned Qwen2.5-Instruct models, using synthetic query generation and
reasoning traces derived from frontier models (e.g., DeepSeek-R1) over a
curated corpus, in this case, the NHS A-to-Z condition pages. We explore the
impact of summarisation-based document compression, synthetic data design, and
reasoning-aware fine-tuning on model performance. Evaluation against both
non-reasoning and general-purpose lean models demonstrates that our
domain-specific fine-tuning approach yields substantial gains in answer
accuracy and consistency, approaching frontier-level performance while
remaining feasible for local deployment. All implementation details and code
are publicly released to support reproducibility and adaptation across domains.

---

### 4. +VeriRel: Verification Feedback to Enhance Document Retrieval for Scientific Fact Checking

- **LLM Score**: 7
- **Keyword Score**: 12
- **Authors**: Xingyu Deng, Xi Wang, Mark Stevenson
- **URL**: <http://arxiv.org/abs/2508.11122v1>
- **Submitted**: 2025-08-14 23:57:40
- **Comment**: Accpeted for the 34th ACM International Conference on Information and
  Knowledge Management (CIKM'25)
- **Topic Keywords**: information retrieval, ranking, relevance, retrieval, rank
- **Reason**: The paper proposes a novel approach to document retrieval for scientific fact checking, incorporating verification feedback to enhance relevance assessment. While not directly focused on query understanding, ranking models, or user behavior modeling, the paper's emphasis on relevance optimization and evidence-based ranking aligns with the user's interests in Information Retrieval. The paper's scope is somewhat narrow, focusing on scientific fact checking, but its innovative approach to document retrieval makes it relevant to the user's research themes.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Scientific Fact Checking
- **Aim**: To develop a system that integrates verification feedback into document ranking for accurate scientific fact checking
- **Rationale**: Existing approaches rely on off-the-shelf Information Retrieval algorithms that rank documents based on relevance rather than the evidence they provide
- **Ground**: The proposed system, +VeriRel, uses a trainable reranker that integrates verification feedback for document ranking, enabling broad applicability and scalable training
- **Experiment**: The authors experiment with three publicly available datasets: SciFact, SciFact-Open, and Check-COVID, and evaluate the approach using metrics such as Recall@k, precision, recall, and F1-score
- **Takeaway**: The results demonstrate the effectiveness of +VeriRel in retrieving relevant evidence and improving downstream verification, highlighting the benefits of integrating verification feedback into document relevance estimation

#### Abstract
> Identification of appropriate supporting evidence is critical to the success
of scientific fact checking. However, existing approaches rely on off-the-shelf
Information Retrieval algorithms that rank documents based on relevance rather
than the evidence they provide to support or refute the claim being checked.
This paper proposes +VeriRel which includes verification success in the
document ranking. Experimental results on three scientific fact checking
datasets (SciFact, SciFact-Open and Check-Covid) demonstrate consistently
leading performance by +VeriRel for document evidence retrieval and a positive
impact on downstream verification. This study highlights the potential of
integrating verification feedback to document relevance assessment for
effective scientific fact checking systems. It shows promising future work to
evaluate fine-grained relevance when examining complex documents for advanced
scientific fact checking.

---

### 5. Reference Points in LLM Sentiment Analysis: The Role of Structured Context

- **LLM Score**: 6
- **Keyword Score**: 1
- **Authors**: Junichiro Niimi
- **URL**: <http://arxiv.org/abs/2508.11454v1>
- **Submitted**: 2025-08-15 13:04:32
- **Topic Keywords**: search
- **Reason**: The paper explores the role of structured context in sentiment analysis using Large Language Models (LLMs), which is related to query understanding and ranking models in Information Retrieval. However, the focus on sentiment analysis and marketing applications is not directly aligned with the user's primary research interests in Information Retrieval and Search technologies. The paper's relevance is somewhat related, but not a central match.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Improving Sentiment Analysis in Marketing Research using Large Language Models
- **Aim**: To address the limitations of large language models in sentiment analysis by incorporating rich contextual data and reference points in customer evaluations
- **Rationale**: The importance of considering reference points and contextual factors in sentiment analysis to improve model performance
- **Ground**: Experimental studies comparing model performance across different prompt formats and contextual factors using the Yelp Open Dataset
- **Experiment**: Three experimental studies comparing model performance with and without supplementary information, and with different prompt formats and contextual factors
- **Takeaway**: The proposed framework improves the performance of LLM-based sentiment analysis in marketing research and provides insights for practical deployment strategies

#### Abstract
> Large language models (LLMs) are now widely used across many fields,
including marketing research. Sentiment analysis, in particular, helps firms
understand consumer preferences. While most NLP studies classify sentiment from
review text alone, marketing theories, such as prospect theory and
expectation--disconfirmation theory, point out that customer evaluations are
shaped not only by the actual experience but also by additional reference
points. This study therefore investigates how the content and format of such
supplementary information affect sentiment analysis using LLMs. We compare
natural language (NL) and JSON-formatted prompts using a lightweight 3B
parameter model suitable for practical marketing applications. Experiments on
two Yelp categories (Restaurant and Nightlife) show that the JSON prompt with
additional information outperforms all baselines without fine-tuning: Macro-F1
rises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it
deployable in resource-constrained edge devices. Furthermore, a follow-up
analysis confirms that performance gains stem from genuine contextual reasoning
rather than label proxying. This work demonstrates that structured prompting
can enable smaller models to achieve competitive performance, offering a
practical alternative to large-scale model deployment.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. INFNet: A Task-aware Information Flow Network for Large-Scale Recommendation Systems

- **LLM Score**: 4
- **Keyword Score**: 11
- **Authors**: Kaiyuan Li, Dongdong Mao, Yongxiang Tang, Yanhua Cheng, Yanxiang Zeng, Chao Wang, Xialong Liu, Peng Jiang
- **URL**: <http://arxiv.org/abs/2508.11565v1>
- **Submitted**: 2025-08-15 16:18:32
- **Topic Keywords**: ranking, click, ctr, click-through rate, recommend, rank
- **Reason**: The paper proposes a novel architecture for large-scale recommendation systems, focusing on feature interaction and task-aware modeling. While it touches on ranking models, the primary focus is on recommender systems, which is not directly related to the user's core research themes in Information Retrieval and Search technologies. The paper's emphasis on feature interaction and multi-task learning is somewhat related to the user's interests in query understanding and ranking models, but the context and application are different.

#### Abstract
> Feature interaction has long been a cornerstone of ranking models in
large-scale recommender systems due to its proven effectiveness in capturing
complex dependencies among features. However, existing feature interaction
strategies face two critical challenges in industrial applications: (1) The
vast number of categorical and sequential features makes exhaustive interaction
computationally prohibitive, often resulting in optimization difficulties. (2)
Real-world recommender systems typically involve multiple prediction
objectives, yet most current approaches apply feature interaction modules prior
to the multi-task learning layers. This late-fusion design overlooks
task-specific feature dependencies and inherently limits the capacity of
multi-task modeling. To address these limitations, we propose the Information
Flow Network (INFNet), a task-aware architecture designed for large-scale
recommendation scenarios. INFNet distinguishes features into three token types,
categorical tokens, sequence tokens, and task tokens, and introduces a novel
dual-flow design comprising heterogeneous and homogeneous alternating
information blocks. For heterogeneous information flow, we employ a
cross-attention mechanism with proxy that facilitates efficient cross-modal
token interaction with balanced computational cost. For homogeneous flow, we
design type-specific Proxy Gated Units (PGUs) to enable fine-grained intra-type
feature processing. Extensive experiments on multiple offline benchmarks
confirm that INFNet achieves state-of-the-art performance. Moreover, INFNet has
been successfully deployed in a commercial online advertising system, yielding
significant gains of +1.587% in Revenue (REV) and +1.155% in Click-Through Rate
(CTR).

### 7. PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Zhuoqun Li, Xuanang Chen, Hongyu Lin, Yaojie Lu, Xianpei Han, Le Sun
- **URL**: <http://arxiv.org/abs/2508.11116v1>
- **Submitted**: 2025-08-14 23:43:46
- **Topic Keywords**: query, queries, retrieval, search
- **Reason**: The paper focuses on paper search, proposing a hierarchical indexing and adaptive retrieval approach to support flexible-grained queries. While it's related to information retrieval, the context is specific to paper search and lacks direct connection to query understanding, ranking models, or user behavior modeling, which are core interests in your research.

#### Abstract
> Paper search is an important activity for researchers, typically involving
using a query with description of a topic to find relevant papers. As research
deepens, paper search requirements may become more flexible, sometimes
involving specific details such as module configuration rather than being
limited to coarse-grained topics. However, previous paper search systems are
unable to meet these flexible-grained requirements, as these systems mainly
collect paper abstracts to construct index of corpus, which lack detailed
information to support retrieval by finer-grained queries. In this work, we
propose PaperRegister, consisted of offline hierarchical indexing and online
adaptive retrieval, transforming traditional abstract-based index into
hierarchical index tree for paper search, thereby supporting queries at
flexible granularity. Experiments on paper search tasks across a range of
granularity demonstrate that PaperRegister achieves the state-of-the-art
performance, and particularly excels in fine-grained scenarios, highlighting
the good potential as an effective solution for flexible-grained paper search
in real-world applications. Code for this work is in
https://github.com/Li-Z-Q/PaperRegister.

### 8. Hybrid-Hierarchical Fashion Graph Attention Network for Compatibility-Oriented and Personalized Outfit Recommendation

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Sajjad Saed, Babak Teimourpour
- **URL**: <http://arxiv.org/abs/2508.11105v1>
- **Submitted**: 2025-08-14 23:09:57
- **Topic Keywords**: rag, recommend, commerce, e-commerce, search
- **Reason**: The paper focuses on personalized outfit recommendation, which is somewhat related to my interests in Information Retrieval and Search technologies. However, the approach is more geared towards recommender systems and does not directly address query understanding, ranking models, or user behavior modeling, which are my primary research areas.

#### Abstract
> The rapid expansion of the fashion industry and the growing variety of
products have made it challenging for users to find compatible items on
e-commerce platforms. Effective fashion recommendation systems are crucial for
filtering irrelevant items and suggesting suitable ones. However,
simultaneously addressing outfit compatibility and personalized recommendations
remains a significant challenge, as these aspects are often treated
independently in existing studies, often overlooking the complex interactions
between items and user preferences. This research introduces a new framework
named FGAT, inspired by the HFGN model, which leverages graph neural networks
and graph attention mechanisms to tackle this issue. The proposed framework
constructs a three-tier hierarchical graph of users, outfits, and items,
integrating visual and textual features to simultaneously model outfit
compatibility and user preferences. A graph attention mechanism dynamically
weights node importance during representation propagation, enabling the capture
of key interactions and generating precise representations for both user
preferences and outfit compatibility. Evaluated on the POG dataset, FGAT
outperforms baseline models such as HFGN, achieving improved results in
precision, HR, recall, NDCG, and accuracy.These results demonstrate that
combining multimodal visual-textual features with a hierarchical graph
structure and attention mechanisms significantly enhances the accuracy and
efficiency of personalized fashion recommendation systems.

### 9. LLM Compression: How Far Can We Go in Balancing Size and Performance?

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Sahil Sk, Debasish Dhal, Sonal Khosla, Sk Shahid, Sambit Shekhar, Akash Dhaka, Shantipriya Parida, Dilip K. Prasad, Ond≈ôej Bojar
- **URL**: <http://arxiv.org/abs/2508.11318v1>
- **Submitted**: 2025-08-15 08:41:20
- **Comment**: This paper has been accepted for presentation at the RANLP 2025
  conference
- **Topic Keywords**: information retrieval, retrieval
- **Reason**: The paper discusses model compression techniques for large language models, which is related to information retrieval and search technologies. However, the focus is on NLP tasks and model performance, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user.

#### Abstract
> Quantization is an essential and popular technique for improving the
accessibility of large language models (LLMs) by reducing memory usage and
computational costs while maintaining performance. In this study, we apply
4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer
Quantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their
impact across multiple NLP tasks. We benchmark these models on MS MARCO
(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K
(Mathematical Reasoning) datasets, assessing both accuracy and efficiency
across various tasks. The study measures the trade-offs between model
compression and task performance, analyzing key evaluation metrics, namely
accuracy, inference latency, and throughput (total output tokens generated per
second), providing insights into the suitability of low-bit quantization for
real-world deployment. Using the results, users can then make suitable
decisions based on the specifications that need to be met. We discuss the pros
and cons of GSQ and GPTQ techniques on models of different sizes, which also
serve as a benchmark for future experiments.

### 10. ORFuzz: Fuzzing the "Other Side" of LLM Safety -- Testing Over-Refusal

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Haonan Zhang, Dongxia Wang, Yi Liu, Kexin Chen, Jiashui Wang, Xinlei Ying, Long Liu, Wenhai Wang
- **URL**: <http://arxiv.org/abs/2508.11222v1>
- **Submitted**: 2025-08-15 05:03:26
- **Topic Keywords**: queries, rag
- **Reason**: The paper focuses on testing the reliability and usability of Large Language Models (LLMs) by detecting over-refusal behavior, which is not directly related to my research interests in Information Retrieval, Search technologies, and query understanding. While it touches on NLP, the specific application and methodology are not aligned with my primary focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously
rejecting benign queries due to overly conservative safety measures - a
critical functional flaw that undermines their reliability and usability.
Current methods for testing this behavior are demonstrably inadequate,
suffering from flawed benchmarks and limited test generation capabilities, as
highlighted by our empirical user study. To the best of our knowledge, this
paper introduces the first evolutionary testing framework, ORFuzz, for the
systematic detection and analysis of LLM over-refusals. ORFuzz uniquely
integrates three core components: (1) safety category-aware seed selection for
comprehensive test coverage, (2) adaptive mutator optimization using reasoning
LLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge
model validated to accurately reflect user perception of toxicity and refusal.
Our extensive evaluations demonstrate that ORFuzz generates diverse, validated
over-refusal instances at a rate (6.98% average) more than double that of
leading baselines, effectively uncovering vulnerabilities. Furthermore,
ORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly
transferable test cases that achieves a superior 63.56% average over-refusal
rate across 10 diverse LLMs, significantly outperforming existing datasets.
ORFuzz and ORFuzzSet provide a robust automated testing framework and a
valuable community resource, paving the way for developing more reliable and
trustworthy LLM-based software systems.

### 11. Expressive Speech Retrieval using Natural Language Descriptions of Speaking Style

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Wonjune Kang, Deb Roy
- **URL**: <http://arxiv.org/abs/2508.11187v1>
- **Submitted**: 2025-08-15 03:38:21
- **Comment**: Accepted to ASRU 2025
- **Topic Keywords**: queries, retrieval
- **Reason**: The paper's focus on expressive speech retrieval using natural language descriptions of speaking style is somewhat related to information retrieval, but it does not directly align with the user's interests in query understanding, ranking models, and user behavior modeling. While the paper explores cross-modal alignment and prompt augmentation, it does not specifically address ranking models or user behavior modeling, making it only loosely relevant to the user's research themes.

#### Abstract
> We introduce the task of expressive speech retrieval, where the goal is to
retrieve speech utterances spoken in a given style based on a natural language
description of that style. While prior work has primarily focused on performing
speech retrieval based on what was said in an utterance, we aim to do so based
on how something was said. We train speech and text encoders to embed speech
and text descriptions of speaking styles into a joint latent space, which
enables using free-form text prompts describing emotions or styles as queries
to retrieve matching expressive speech segments. We perform detailed analyses
of various aspects of our proposed framework, including encoder architectures,
training criteria for effective cross-modal alignment, and prompt augmentation
for improved generalization to arbitrary text queries. Experiments on multiple
datasets encompassing 22 speaking styles demonstrate that our approach achieves
strong retrieval performance as measured by Recall@k.

### 12. Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Changjian Wang, Weihong Deng, Weili Guan, Quan Lu, Ning Jiang
- **URL**: <http://arxiv.org/abs/2508.11247v1>
- **Submitted**: 2025-08-15 06:36:13
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper proposes a novel approach for multi-hop question answering, leveraging hypergraphs to integrate structural and semantic information. While it touches on retrieval and generation, the focus is on question answering and knowledge graph-based methods, which are not directly related to my core research interests in information retrieval, search technologies, and query understanding.

#### Abstract
> Multi-hop question answering (MHQA) requires integrating knowledge scattered
across multiple passages to derive the correct answer. Traditional
retrieval-augmented generation (RAG) methods primarily focus on coarse-grained
textual semantic similarity and ignore structural associations among dispersed
knowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods
address this by leveraging knowledge graphs (KGs) to capture structural
associations, but they tend to overly rely on structural information and
fine-grained word- or phrase-level retrieval, resulting in an underutilization
of textual semantics. In this paper, we propose a novel RAG approach called
HGRAG for MHQA that achieves cross-granularity integration of structural and
semantic information via hypergraphs. Structurally, we construct an entity
hypergraph where fine-grained entities serve as nodes and coarse-grained
passages as hyperedges, and establish knowledge association through shared
entities. Semantically, we design a hypergraph retrieval method that integrates
fine-grained entity similarity and coarse-grained passage similarity via
hypergraph diffusion. Finally, we employ a retrieval enhancement module, which
further refines the retrieved results both semantically and structurally, to
obtain the most relevant passages as context for answer generation with the
LLM. Experimental results on benchmark datasets demonstrate that our approach
outperforms state-of-the-art methods in QA performance, and achieves a
6$\times$ speedup in retrieval efficiency.

### 13. MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Hikaru Asano, Hiroki Ouchi, Akira Kasuga, Ryo Yonetani
- **URL**: <http://arxiv.org/abs/2508.11163v1>
- **Submitted**: 2025-08-15 02:30:20
- **Comment**: 23 pages, 12 figures
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper presents a benchmark dataset for evaluating the semantic understanding of human mobility data through question answering, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on natural language processing, the focus is on large language models and their capabilities for interpreting human movement patterns, which is not a primary interest area for you.

#### Abstract
> This paper presents MobQA, a benchmark dataset designed to evaluate the
semantic understanding capabilities of large language models (LLMs) for human
mobility data through natural language question answering.
  While existing models excel at predicting human movement patterns, it remains
unobvious how much they can interpret the underlying reasons or semantic
meaning of those patterns. MobQA provides a comprehensive evaluation framework
for LLMs to answer questions about diverse human GPS trajectories spanning
daily to weekly granularities. It comprises 5,800 high-quality question-answer
pairs across three complementary question types: factual retrieval (precise
data extraction), multiple-choice reasoning (semantic inference), and free-form
explanation (interpretive description), which all require spatial, temporal,
and semantic reasoning. Our evaluation of major LLMs reveals strong performance
on factual retrieval but significant limitations in semantic reasoning and
explanation question answering, with trajectory length substantially impacting
model effectiveness. These findings demonstrate the achievements and
limitations of state-of-the-art LLMs for semantic mobility
understanding.\footnote{MobQA dataset is available at
https://github.com/CyberAgentAILab/mobqa.}

### 14. AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Jinpeng Hu, Ao Wang, Qianqian Xie, Hui Ma, Zhuo Li, Dan Guo
- **URL**: <http://arxiv.org/abs/2508.11567v1>
- **Submitted**: 2025-08-15 16:20:45
- **Topic Keywords**: queries
- **Reason**: The paper proposes a multi-agent framework for mental health assessment, which involves dynamic interaction and iterative questioning. While it employs some relevant concepts like adaptive questioning and memory updating, the focus is on mental health assessment rather than information retrieval or search technologies. The paper's connection to the user's interests is limited to the use of questioning and memory updating, which are also relevant in IR and NLP, but the overall topic and approach are not directly aligned with the user's research themes.

#### Abstract
> Mental health assessment is crucial for early intervention and effective
treatment, yet traditional clinician-based approaches are limited by the
shortage of qualified professionals. Recent advances in artificial intelligence
have sparked growing interest in automated psychological assessment, yet most
existing approaches are constrained by their reliance on static text analysis,
limiting their ability to capture deeper and more informative insights that
emerge through dynamic interaction and iterative questioning. Therefore, in
this paper, we propose a multi-agent framework for mental health evaluation
that simulates clinical doctor-patient dialogues, with specialized agents
assigned to questioning, adequacy evaluation, scoring, and updating. We
introduce an adaptive questioning mechanism in which an evaluation agent
assesses the adequacy of user responses to determine the necessity of
generating targeted follow-up queries to address ambiguity and missing
information. Additionally, we employ a tree-structured memory in which the root
node encodes the user's basic information, while child nodes (e.g., topic and
statement) organize key information according to distinct symptom categories
and interaction turns. This memory is dynamically updated throughout the
interaction to reduce redundant questioning and further enhance the information
extraction and contextual tracking capabilities. Experimental results on the
DAIC-WOZ dataset illustrate the effectiveness of our proposed method, which
achieves better performance than existing approaches.

### 15. Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Marc Brinner, Sina Zarrie√ü
- **URL**: <http://arxiv.org/abs/2508.11393v1>
- **Submitted**: 2025-08-15 10:51:58
- **Topic Keywords**: relevance
- **Reason**: The paper focuses on rationalizing transformer predictions, which is related to query understanding and ranking models. However, the context is more focused on NLP and classification, rather than information retrieval and search technologies. The paper's emphasis on rationalized models and class-wise rationales is not directly applicable to my research interests.

#### Abstract
> We propose an end-to-end differentiable training paradigm for stable training
of a rationalized transformer classifier. Our approach results in a single
model that simultaneously classifies a sample and scores input tokens based on
their relevance to the classification. To this end, we build on the widely-used
three-player-game for training rationalized models, which typically relies on
training a rationale selector, a classifier and a complement classifier. We
simplify this approach by making a single model fulfill all three roles,
leading to a more efficient training paradigm that is not susceptible to the
common training instabilities that plague existing approaches. Further, we
extend this paradigm to produce class-wise rationales while incorporating
recent advances in parameterizing and regularizing the resulting rationales,
thus leading to substantially improved and state-of-the-art alignment with
human annotations without any explicit supervision.

### 16. MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Tomer Wolfson, Harsh Trivedi, Mor Geva, Yoav Goldberg, Dan Roth, Tushar Khot, Ashish Sabharwal, Reut Tsarfaty
- **URL**: <http://arxiv.org/abs/2508.11133v1>
- **Submitted**: 2025-08-15 00:58:10
- **Comment**: Accepted for publication in Transactions of the Association for
  Computational Linguistics (TACL), 2025. Authors pre-print
- **Topic Keywords**: query
- **Reason**: The paper introduces a new benchmark for natural language processing, focusing on complex and time-consuming questions. While it touches on query understanding and information retrieval, the primary focus is on language models and question answering, which is not directly related to my research interests in search technologies and ranking models.

#### Abstract
> Large language models (LLMs) are emerging as a go-to tool for querying
information. However, current LLM benchmarks rarely feature natural questions
that are both information-seeking as well as genuinely time-consuming for
humans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural
and complex questions that require dozens, and at times hundreds, of
intermediate steps to solve -- far more than any existing QA benchmark. To
build MoNaCo, we developed a decomposed annotation pipeline to elicit and
manually answer natural time-consuming questions at scale. Frontier LLMs
evaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and
hallucinations. Our results underscore the need for reasoning models that
better handle the complexity and sheer breadth of real-world
information-seeking questions -- with MoNaCo providing an effective resource
for tracking such progress. The MONACO benchmark, codebase, prompts and models
predictions are publicly available at: https://tomerwolgithub.github.io/monaco

### 17. Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Lorenzo Jaime Yu Flores, Junyi Shen, Xiaoyuan Gu
- **URL**: <http://arxiv.org/abs/2508.11120v1>
- **Submitted**: 2025-08-14 23:52:39
- **Topic Keywords**: queries
- **Reason**: The paper discusses the development of a multi-agent framework for marketing applications, leveraging large language models and long-term memory. While it touches on planning and verification, the focus is on marketing and audience curation, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Recent advances in large language models (LLMs) enabled the development of AI
agents that can plan and interact with tools to complete complex tasks.
However, literature on their reliability in real-world applications remains
limited. In this paper, we introduce a multi-agent framework for a marketing
task: audience curation. To solve this, we introduce a framework called RAMP
that iteratively plans, calls tools, verifies the output, and generates
suggestions to improve the quality of the audience generated. Additionally, we
equip the model with a long-term memory store, which is a knowledge base of
client-specific facts and past queries. Overall, we demonstrate the use of LLM
planning and memory, which increases accuracy by 28 percentage points on a set
of 88 evaluation queries. Moreover, we show the impact of iterative
verification and reflection on more ambiguous queries, showing progressively
better recall (roughly +20 percentage points) with more verify/reflect
iterations on a smaller challenge set, and higher user satisfaction. Our
results provide practical insights for deploying reliable LLM-based systems in
dynamic, industry-facing environments.

### 18. Relative Advantage Debiasing for Watch-Time Prediction in Short-Video Recommendation

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Emily Liu, Kuan Han, Minfeng Zhan, Bocheng Zhao, Guanyu Mu, Yang Song
- **URL**: <http://arxiv.org/abs/2508.11086v1>
- **Submitted**: 2025-08-14 21:52:00
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on recommender systems, specifically video recommendation, which is related to the user's background in e-commerce. However, the topic is not directly aligned with the user's primary interests in Information Retrieval, query understanding, ranking models, and user behavior modeling. The paper's emphasis on debiasing watch-time prediction and distributional embeddings is not directly relevant to the user's research themes.

#### Abstract
> Watch time is widely used as a proxy for user satisfaction in video
recommendation platforms. However, raw watch times are influenced by
confounding factors such as video duration, popularity, and individual user
behaviors, potentially distorting preference signals and resulting in biased
recommendation models. We propose a novel relative advantage debiasing
framework that corrects watch time by comparing it to empirically derived
reference distributions conditioned on user and item groups. This approach
yields a quantile-based preference signal and introduces a two-stage
architecture that explicitly separates distribution estimation from preference
learning. Additionally, we present distributional embeddings to efficiently
parameterize watch-time quantiles without requiring online sampling or storage
of historical data. Both offline and online experiments demonstrate significant
improvements in recommendation accuracy and robustness compared to existing
baseline methods.

### 19. Controlling Multimodal LLMs via Reward-guided Decoding

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Oscar Ma√±as, Pierluca D'Oro, Koustuv Sinha, Adriana Romero-Soriano, Michal Drozdzal, Aishwarya Agrawal
- **URL**: <http://arxiv.org/abs/2508.11616v1>
- **Submitted**: 2025-08-15 17:29:06
- **Comment**: Published at ICCV 2025
- **Topic Keywords**: search
- **Reason**: The paper explores Multimodal Large Language Models (MLLMs) and controlled decoding, which is not directly related to Information Retrieval (IR) or Search technologies. While it touches on topics like relevance optimization, the focus is on visual grounding and object precision/recall, which is not a primary interest in IR. The paper's relevance to NLP and data mining is higher, but it does not specifically address query understanding, ranking models, or user behavior modeling.

#### Abstract
> As Multimodal Large Language Models (MLLMs) gain widespread applicability, it
is becoming increasingly desirable to adapt them for diverse user needs. In
this paper, we study the adaptation of MLLMs through controlled decoding. To
achieve this, we introduce the first method for reward-guided decoding of MLLMs
and demonstrate its application in improving their visual grounding. Our method
involves building reward models for visual grounding and using them to guide
the MLLM's decoding process. Concretely, we build two separate reward models to
independently control the degree of object precision and recall in the model's
output. Our approach enables on-the-fly controllability of an MLLM's inference
process in two ways: first, by giving control over the relative importance of
each reward function during decoding, allowing a user to dynamically trade off
object precision for recall in image captioning tasks; second, by giving
control over the breadth of the search during decoding, allowing the user to
control the trade-off between the amount of test-time compute and the degree of
visual grounding. We evaluate our method on standard object hallucination
benchmarks, showing that it provides significant controllability over MLLM
inference, while consistently outperforming existing hallucination mitigation
methods.

### 20. When Algorithms Mirror Minds: A Confirmation-Aware Social Dynamic Model of Echo Chamber and Homogenization Traps

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Ming Tang, Xiaowen Huang, Jitao Sang
- **URL**: <http://arxiv.org/abs/2508.11516v1>
- **Submitted**: 2025-08-15 14:55:55
- **Topic Keywords**: recommend
- **Reason**: The paper explores echo chambers and homogenization in recommender systems, which is related to information retrieval and search technologies. However, the focus is on the psychological mechanisms of users and the closed-loop interaction between users and recommenders, which is not directly aligned with my interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat related but not a central match.

#### Abstract
> Recommender systems increasingly suffer from echo chambers and user
homogenization, systemic distortions arising from the dynamic interplay between
algorithmic recommendations and human behavior. While prior work has studied
these phenomena through the lens of algorithmic bias or social network
structure, we argue that the psychological mechanisms of users and the
closed-loop interaction between users and recommenders are critical yet
understudied drivers of these emergent effects. To bridge this gap, we propose
the Confirmation-Aware Social Dynamic Model which incorporates user psychology
and social relationships to simulate the actual user and recommender
interaction process. Our theoretical analysis proves that echo chambers and
homogenization traps, defined respectively as reduced recommendation diversity
and homogenized user representations, will inevitably occur. We also conduct
extensive empirical simulations on two real-world datasets and one synthetic
dataset with five well-designed metrics, exploring the root factors influencing
the aforementioned phenomena from three level perspectives: the stochasticity
and social integration degree of recommender (system-level), the psychological
mechanisms of users (user-level), and the dataset scale (platform-level).
Furthermore, we demonstrate four practical mitigation strategies that help
alleviate echo chambers and user homogenization at the cost of some
recommendation accuracy. Our findings provide both theoretical and empirical
insights into the emergence and drivers of echo chambers and user
homogenization, as well as actionable guidelines for human-centered recommender
design.

### 21. Mitigating Filter Bubble from the Perspective of Community Detection: A Universal Framework

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Ming Tang, Xiaowen Huang, Jitao Sang
- **URL**: <http://arxiv.org/abs/2508.11239v1>
- **Submitted**: 2025-08-15 05:57:38
- **Topic Keywords**: recommend
- **Reason**: The paper focuses on recommender systems and the filter bubble effect, which is somewhat related to my interests in Information Retrieval and Search technologies. However, the paper's emphasis on community detection and graph convolutional networks is not directly aligned with my research themes, such as query understanding, ranking models, and user behavior modeling.

#### Abstract
> In recent years, recommender systems have primarily focused on improving
accuracy at the expense of diversity, which exacerbates the well-known filter
bubble effect. This paper proposes a universal framework called CD-CGCN to
address the filter bubble issue in recommender systems from a community
detection perspective. By analyzing user-item interaction histories with a
community detection algorithm, we reveal that state-of-the-art recommendations
often focus on intra-community items, worsening the filter bubble effect.
CD-CGCN, a model-agnostic framework, integrates a Conditional Discriminator and
a Community-reweighted Graph Convolutional Network which can be plugged into
most recommender models. Using adversarial learning based on community labels,
it counteracts the extracted community attributes and incorporates an inference
strategy tailored to the user's specific filter bubble state. Extensive
experiments on real-world datasets with multiple base models validate its
effectiveness in mitigating filter bubbles while preserving recommendation
quality. Additionally, by applying community debiasing to the original test set
to construct an unbiased test set, we observe that CD-CGCN demonstrates
superior performance in capturing users' inter-community preferences.

### 22. Can Multi-modal (reasoning) LLMs detect document manipulation?

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Zisheng Liang, Kidus Zewde, Rudra Pratap Singh, Disha Patil, Zexi Chen, Jiayu Xue, Yao Yao, Yifei Chen, Qinzhe Liu, Simiao Ren
- **URL**: <http://arxiv.org/abs/2508.11021v1>
- **Submitted**: 2025-08-14 18:57:07
- **Comment**: arXiv admin note: text overlap with arXiv:2503.20084
- **Topic Keywords**: search
- **Reason**: The paper explores the application of multi-modal large language models in detecting document manipulation, which is a specific problem in the domain of information retrieval. While it touches on the topic of query understanding and ranking models, the focus is more on the capabilities of the models rather than their application in search technologies. The paper's relevance to the user's interests is limited, but it does provide some insights into the potential of LLMs in enhancing document fraud detection systems.

#### Abstract
> Document fraud poses a significant threat to industries reliant on secure and
verifiable documentation, necessitating robust detection mechanisms. This study
investigates the efficacy of state-of-the-art multi-modal large language models
(LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus,
Grok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and
3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against
each other and prior work on document fraud detection techniques using a
standard dataset with real transactional documents. Through prompt optimization
and detailed analysis of the models' reasoning processes, we evaluate their
ability to identify subtle indicators of fraud, such as tampered text,
misaligned formatting, and inconsistent transactional sums. Our results reveal
that top-performing multi-modal LLMs demonstrate superior zero-shot
generalization, outperforming conventional methods on out-of-distribution
datasets, while several vision LLMs exhibit inconsistent or subpar performance.
Notably, model size and advanced reasoning capabilities show limited
correlation with detection accuracy, suggesting task-specific fine-tuning is
critical. This study underscores the potential of multi-modal LLMs in enhancing
document fraud detection systems and provides a foundation for future research
into interpretable and scalable fraud mitigation strategies.

### 23. Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Kangyu Wang, Hongliang He, Lin Liu, Ruiqi Liang, Zhenzhong Lan, Jianguo Li
- **URL**: <http://arxiv.org/abs/2508.11452v1>
- **Submitted**: 2025-08-15 13:00:07
- **Comment**: Our platform is publicly accessible at
  https://doraemon.alipay.com/model-ranking
- **Topic Keywords**: ranking, pairwise, rank
- **Reason**: The paper focuses on evaluating large language models and multimodal large language models, which is not directly related to information retrieval, search technologies, or query understanding. The paper's emphasis on real-world applications and user-centric deployments is also not aligned with the user's research interests.

#### Abstract
> Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)
have ushered in a new era of AI capabilities, demonstrating near-human-level
performance across diverse scenarios. While numerous benchmarks (e.g., MMLU)
and leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the
development of LLMs and MLLMs, most rely on static datasets or crowdsourced
general-domain prompts, often falling short of reflecting performance in
real-world applications. To bridge this critical gap, we present Inclusion
Arena, a live leaderboard that ranks models based on human feedback collected
directly from AI-powered applications. Our platform integrates pairwise model
comparisons into natural user interactions, ensuring evaluations reflect
practical usage scenarios. For robust model ranking, we employ the
Bradley-Terry model augmented with two key innovations: (1) Placement Matches,
a cold-start mechanism to quickly estimate initial ratings for newly integrated
models, and (2) Proximity Sampling, an intelligent comparison strategy that
prioritizes battles between models of similar capabilities to maximize
information gain and enhance rating stability. Extensive empirical analyses and
simulations demonstrate that Inclusion Arena yields reliable and stable
rankings, exhibits higher data transitivity compared to general crowdsourced
datasets, and significantly mitigates the risk of malicious manipulation. By
fostering an open alliance between foundation models and real-world
applications, Inclusion Arena aims to accelerate the development of LLMs and
MLLMs truly optimized for practical, user-centric deployments. The platform is
publicly accessible at https://doraemon.alipay.com/model-ranking.

### 24. HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Shivam Dubey
- **URL**: <http://arxiv.org/abs/2508.11429v1>
- **Submitted**: 2025-08-15 12:07:56
- **Topic Keywords**: pairwise, relevance, search
- **Reason**: The paper focuses on humor generation using Large Language Models, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it mentions context modeling, it is not applicable to the user's interests in IR and NLP.

#### Abstract
> Automated humor generation with Large Language Models (LLMs) often yields
jokes that feel generic, repetitive, or tone-deaf because humor is deeply
situated and hinges on the listener's cultural background, mindset, and
immediate context. We introduce HumorPlanSearch, a modular pipeline that
explicitly models context through: (1) Plan-Search for diverse, topic-tailored
strategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and
stylistic reasoning; (3) a Knowledge Graph to retrieve and adapt
high-performing historical strategies; (4) novelty filtering via semantic
embeddings; and (5) an iterative judge-driven revision loop. To evaluate
context sensitivity and comedic quality, we propose the Humor Generation Score
(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,
and topic relevance. In experiments across nine topics with feedback from 13
human judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent
(p < 0.05) over a strong baseline. By foregrounding context at every stage from
strategy planning to multi-signal evaluation, HumorPlanSearch advances
AI-driven humor toward more coherent, adaptive, and culturally attuned comedy.

### 25. AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Arya VarastehNezhad, Reza Tavasoli, Soroush Elyasi, MohammadHossein LotfiNia, Hamed Farbeh
- **URL**: <http://arxiv.org/abs/2508.11285v1>
- **Submitted**: 2025-08-15 07:47:10
- **Topic Keywords**: queries, rag
- **Reason**: The paper focuses on the emotional and sentiment analysis of Large Language Models' responses to depression, anxiety, and stress queries, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on mental health applications and emotional signatures of language models is also outside the user's primary focus.

#### Abstract
> Depression, anxiety, and stress are widespread mental health concerns that
increasingly drive individuals to seek information from Large Language Models
(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini
Pro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty
pragmatic questions about depression, anxiety, and stress when those questions
are framed for six user profiles (baseline, woman, man, young, old, and
university student). The models generated 2,880 answers, which we scored for
sentiment and emotions using state-of-the-art tools. Our analysis revealed that
optimism, fear, and sadness dominated the emotional landscape across all
outputs, with neutral sentiment maintaining consistently high values.
Gratitude, joy, and trust appeared at moderate levels, while emotions such as
anger, disgust, and love were rarely expressed. The choice of LLM significantly
influenced emotional expression patterns. Mixtral exhibited the highest levels
of negative emotions including disapproval, annoyance, and sadness, while Llama
demonstrated the most optimistic and joyful responses. The type of mental
health condition dramatically shaped emotional responses: anxiety prompts
elicited extraordinarily high fear scores (0.974), depression prompts generated
elevated sadness (0.686) and the highest negative sentiment, while
stress-related queries produced the most optimistic responses (0.755) with
elevated joy and trust. In contrast, demographic framing of queries produced
only marginal variations in emotional tone. Statistical analyses confirmed
significant model-specific and condition-specific differences, while
demographic influences remained minimal. These findings highlight the critical
importance of model selection in mental health applications, as each LLM
exhibits a distinct emotional signature that could significantly impact user
experience and outcomes.

### 26. SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Beichen Guo, Zhiyuan Wen, Yu Yang, Peng Gao, Ruosong Yang, Jiaxing Shen
- **URL**: <http://arxiv.org/abs/2508.11310v1>
- **Submitted**: 2025-08-15 08:27:58
- **Comment**: Accepted to The 21st International Conference on Advanced Data Mining
  and Applications (ADMA2025)
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on automatic survey generation, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions large language models, which are relevant to NLP, the paper's primary focus is on evaluation methods for survey generation, which is not a central match for the user's research interests.

#### Abstract
> The growing interest in automatic survey generation (ASG), a task that
traditionally required considerable time and effort, has been spurred by recent
advances in large language models (LLMs). With advancements in
retrieval-augmented generation (RAG) and the rising popularity of multi-agent
systems (MASs), synthesizing academic surveys using LLMs has become a viable
approach, thereby elevating the need for robust evaluation methods in this
domain. However, existing evaluation methods suffer from several limitations,
including biased metrics, a lack of human preference, and an over-reliance on
LLMs-as-judges. To address these challenges, we propose SGSimEval, a
comprehensive benchmark for Survey Generation with Similarity-Enhanced
Evaluation that evaluates automatic survey generation systems by integrating
assessments of the outline, content, and references, and also combines
LLM-based scoring with quantitative metrics to provide a multifaceted
evaluation framework. In SGSimEval, we also introduce human preference metrics
that emphasize both inherent quality and similarity to humans. Extensive
experiments reveal that current ASG systems demonstrate human-comparable
superiority in outline generation, while showing significant room for
improvement in content and reference generation, and our evaluation metrics
maintain strong consistency with human assessments.

### 27. RAG for Geoscience: What We Expect, Gaps and Opportunities

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Runlong Yu, Shiyuan Luo, Rahul Ghosh, Lingyao Li, Yiqun Xie, Xiaowei Jia
- **URL**: <http://arxiv.org/abs/2508.11246v1>
- **Submitted**: 2025-08-15 06:33:27
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper discusses Retrieval-Augmented Generation (RAG) in the context of geoscience, which is unrelated to the user's primary focus on Information Retrieval, Search technologies, and Natural Language Processing. While the paper touches on retrieval and generation, the domain and application are distinct from the user's interests.

#### Abstract
> Retrieval-Augmented Generation (RAG) enhances language models by combining
retrieval with generation. However, its current workflow remains largely
text-centric, limiting its applicability in geoscience. Many geoscientific
tasks are inherently evidence-hungry. Typical examples involve imputing missing
observations using analog scenes, retrieving equations and parameters to
calibrate models, geolocating field photos based on visual cues, or surfacing
historical case studies to support policy analyses. A simple
``retrieve-then-generate'' pipeline is insufficient for these needs. We
envision Geo-RAG, a next-generation paradigm that reimagines RAG as a modular
retrieve $\rightarrow$ reason $\rightarrow$ generate $\rightarrow$ verify loop.
Geo-RAG supports four core capabilities: (i) retrieval of multi-modal Earth
data; (ii) reasoning under physical and domain constraints; (iii) generation of
science-grade artifacts; and (iv) verification of generated hypotheses against
numerical models, ground measurements, and expert assessments. This shift opens
new opportunities for more trustworthy and transparent geoscience workflows.

### 28. Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Tao Wu, Jingyuan Chen, Wang Lin, Jian Zhan, Mengze Li, Kun Kuang, Fei Wu
- **URL**: <http://arxiv.org/abs/2508.11184v1>
- **Submitted**: 2025-08-15 03:20:37
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on personalized distractor generation for educational assessment, using Monte Carlo Tree Search and reasoning reconstruction. While it involves some NLP and data mining aspects, the primary goal is not information retrieval or search, and the techniques are not directly applicable to query understanding, ranking models, or user behavior modeling.

#### Abstract
> Distractors, incorrect but plausible answer choices in multiple-choice
questions (MCQs), play a critical role in educational assessment by diagnosing
student misconceptions. Recent work has leveraged large language models (LLMs)
to generate shared, group-level distractors by learning common error patterns
across large student populations. However, such distractors often fail to
capture the diverse reasoning errors of individual students, limiting their
diagnostic effectiveness. To address this limitation, we introduce the task of
personalized distractor generation, which aims to generate tailored distractors
based on individual misconceptions inferred from each student's past
question-answering (QA) records, ensuring every student receives options that
effectively exposes their specific reasoning errors. While promising, this task
is challenging because each student typically has only a few QA records, which
often lack the student's underlying reasoning processes, making training-based
group-level approaches infeasible. To overcome this, we propose a training-free
two-stage framework. In the first stage, we construct a student-specific
misconception prototype by applying Monte Carlo Tree Search (MCTS) to recover
the student's reasoning trajectories from past incorrect answers. In the second
stage, this prototype guides the simulation of the student's reasoning on new
questions, enabling the generation of personalized distractors that align with
the student's recurring misconceptions. Experiments show that our approach
achieves the best performance in generating plausible, personalized distractors
for 140 students, and also effectively generalizes to group-level settings,
highlighting its robustness and adaptability.

### 29. A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Bin Ma, Yifei Zhang, Yongjin Xian, Qi Li, Linna Zhou, Gongxun Miao
- **URL**: <http://arxiv.org/abs/2508.11141v1>
- **Submitted**: 2025-08-15 01:13:50
- **Topic Keywords**: relevance
- **Reason**: The paper focuses on cross-modal rumor detection using contrastive learning, which is not directly related to information retrieval, search technologies, or query understanding. While it involves text and image processing, the primary goal is not to improve search or ranking models, but rather to detect rumors, which is outside the scope of the user's research interests.

#### Abstract
> Existing rumor detection methods often neglect the content within images as
well as the inherent relationships between contexts and images across different
visual scales, thereby resulting in the loss of critical information pertinent
to rumor identification. To address these issues, this paper presents a novel
cross-modal rumor detection scheme based on contrastive learning, namely the
Multi-scale Image and Context Correlation exploration algorithm (MICC).
Specifically, we design an SCLIP encoder to generate unified semantic
embeddings for text and multi-scale image patches through contrastive
pretraining, enabling their relevance to be measured via dot-product
similarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is
introduced to identify image regions most relevant to the textual semantics,
guided by mutual information maximization and the information bottleneck
principle, through a Top-K selection strategy based on a cross-modal relevance
matrix constructed between the text and multi-scale image patches. Moreover, a
scale-aware fusion network is designed to integrate the highly correlated
multi-scale image features with global text features by assigning adaptive
weights to image regions based on their semantic importance and cross-modal
relevance. The proposed methodology has been extensively evaluated on two
real-world datasets. The experimental results demonstrate that it achieves a
substantial performance improvement over existing state-of-the-art approaches
in rumor detection, highlighting its effectiveness and potential for practical
applications.

### 30. Pretrained Conformers for Audio Fingerprinting and Retrieval

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Kemal Altwlkany, Elmedin Selmanovic, Sead Delalic
- **URL**: <http://arxiv.org/abs/2508.11609v1>
- **Submitted**: 2025-08-15 17:19:09
- **Topic Keywords**: retrieval
- **Reason**: The paper focuses on audio fingerprinting and retrieval using conformer-based encoders, which is not directly related to information retrieval, search technologies, or query understanding. While it involves deep learning and embeddings, the context is specific to audio processing and does not align with the user's primary research interests in IR and NLP.

#### Abstract
> Conformers have shown great results in speech processing due to their ability
to capture both local and global interactions. In this work, we utilize a
self-supervised contrastive learning framework to train conformer-based
encoders that are capable of generating unique embeddings for small segments of
audio, generalizing well to previously unseen data. We achieve state-of-the-art
results for audio retrieval tasks while using only 3 seconds of audio to
generate embeddings. Our models are almost completely immune to temporal
misalignments and achieve state-of-the-art results in cases of other audio
distortions such as noise, reverb or extreme temporal stretching. Code and
models are made publicly available and the results are easy to reproduce as we
train and test using popular and freely available datasets of different sizes.

### 31. Representing Speech Through Autoregressive Prediction of Cochlear Tokens

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Greta Tuckute, Klemen Kotar, Evelina Fedorenko, Daniel L. K. Yamins
- **URL**: <http://arxiv.org/abs/2508.11598v1>
- **Submitted**: 2025-08-15 17:06:04
- **Topic Keywords**: ctr
- **Reason**: The paper focuses on speech processing and representation learning, which is outside the scope of Information Retrieval and Search technologies. Although it uses sequence models, the context is not related to query understanding, ranking models, or user behavior modeling, making it irrelevant to the user's primary research interests.

#### Abstract
> We introduce AuriStream, a biologically inspired model for encoding speech
via a two-stage framework inspired by the human auditory processing hierarchy.
The first stage transforms raw audio into a time-frequency representation based
on the human cochlea, from which we extract discrete \textbf{cochlear tokens}.
The second stage applies an autoregressive sequence model over the cochlear
tokens. AuriStream learns meaningful phoneme and word representations, and
state-of-the-art lexical semantics. AuriStream shows competitive performance on
diverse downstream SUPERB speech tasks. Complementing AuriStream's strong
representational capabilities, it generates continuations of audio which can be
visualized in a spectrogram space and decoded back into audio, providing
insights into the model's predictions. In summary, we present a two-stage
framework for speech representation learning to advance the development of more
human-like models that efficiently handle a range of speech-based tasks.

### 32. TrajSV: A Trajectory-based Model for Sports Video Representations and Applications

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zheng Wang, Shihao Xu, Wei Shi
- **URL**: <http://arxiv.org/abs/2508.11569v1>
- **Submitted**: 2025-08-15 16:23:36
- **Comment**: This paper has been accepted by TCSVT
- **Topic Keywords**: retrieval
- **Reason**: This paper focuses on sports video representations and applications, which is not directly related to information retrieval, search technologies, or query understanding. The techniques and models presented are specific to the sports analytics domain and do not align with the user's primary research interests in IR and NLP.

#### Abstract
> Sports analytics has received significant attention from both academia and
industry in recent years. Despite the growing interest and efforts in this
field, several issues remain unresolved, including (1) data unavailability, (2)
lack of an effective trajectory-based framework, and (3) requirement for
sufficient supervision labels. In this paper, we present TrajSV, a
trajectory-based framework that addresses various issues in existing studies.
TrajSV comprises three components: data preprocessing, Clip Representation
Network (CRNet), and Video Representation Network (VRNet). The data
preprocessing module extracts player and ball trajectories from sports
broadcast videos. CRNet utilizes a trajectory-enhanced Transformer module to
learn clip representations based on these trajectories. Additionally, VRNet
learns video representations by aggregating clip representations and visual
features with an encoder-decoder architecture. Finally, a triple contrastive
loss is introduced to optimize both video and clip representations in an
unsupervised manner. The experiments are conducted on three broadcast video
datasets to verify the effectiveness of TrajSV for three types of sports (i.e.,
soccer, basketball, and volleyball) with three downstream applications (i.e.,
sports video retrieval, action spotting, and video captioning). The results
demonstrate that TrajSV achieves state-of-the-art performance in sports video
retrieval, showcasing a nearly 70% improvement. It outperforms baselines in
action spotting, achieving state-of-the-art results in 9 out of 17 action
categories, and demonstrates a nearly 20% improvement in video captioning.
Additionally, we introduce a deployed system along with the three applications
based on TrajSV.

### 33. SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Haitong Luo, Weiyao Zhang, Suhang Wang, Wenji Zou, Chungang Lin, Xuying Meng, Yujun Zhang
- **URL**: <http://arxiv.org/abs/2508.11343v1>
- **Submitted**: 2025-08-15 09:13:42
- **Comment**: Under Review
- **Topic Keywords**: ctr
- **Reason**: The paper focuses on detecting LLM-generated text, which is not directly related to information retrieval, search technologies, or query understanding. While it uses signal processing techniques, the application is specific to text detection and does not align with the user's primary research interests.

#### Abstract
> The proliferation of high-quality text from Large Language Models (LLMs)
demands reliable and efficient detection methods. While existing training-free
approaches show promise, they often rely on surface-level statistics and
overlook fundamental signal properties of the text generation process. In this
work, we reframe detection as a signal processing problem, introducing a novel
paradigm that analyzes the sequence of token log-probabilities in the frequency
domain. By systematically analyzing the signal's spectral properties using the
global Discrete Fourier Transform (DFT) and the local Short-Time Fourier
Transform (STFT), we find that human-written text consistently exhibits
significantly higher spectral energy. This higher energy reflects the
larger-amplitude fluctuations inherent in human writing compared to the
suppressed dynamics of LLM-generated text. Based on this key insight, we
construct SpecDetect, a detector built on a single, robust feature from the
global DFT: DFT total energy. We also propose an enhanced version,
SpecDetect++, which incorporates a sampling discrepancy mechanism to further
boost robustness. Extensive experiments demonstrate that our approach
outperforms the state-of-the-art model while running in nearly half the time.
Our work introduces a new, efficient, and interpretable pathway for
LLM-generated text detection, showing that classical signal processing
techniques offer a surprisingly powerful solution to this modern challenge.

### 34. Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Haitong Luo, Suhang Wang, Weiyao Zhang, Ruiqi Meng, Xuying Meng, Yujun Zhang
- **URL**: <http://arxiv.org/abs/2508.11328v1>
- **Submitted**: 2025-08-15 08:55:57
- **Comment**: Under Review
- **Topic Keywords**: ctr
- **Reason**: This paper focuses on graph pre-training and prompt-tuning, which is not directly related to Information Retrieval, Search technologies, or Natural Language Processing. The concepts of homophily and heterophily are not relevant to query understanding, ranking models, or user behavior modeling, and the paper's emphasis on spectral graph theory and contrastive learning does not align with the user's research interests.

#### Abstract
> Graph ``pre-training and prompt-tuning'' aligns downstream tasks with
pre-trained objectives to enable efficient knowledge transfer under limited
supervision. However, existing methods rely on homophily-based low-frequency
knowledge, failing to handle diverse spectral distributions in real-world
graphs with varying homophily. Our theoretical analysis reveals a spectral
specificity principle: optimal knowledge transfer requires alignment between
pre-trained spectral filters and the intrinsic spectrum of downstream graphs.
Under limited supervision, large spectral gaps between pre-training and
downstream tasks impede effective adaptation. To bridge this gap, we propose
the HS-GPPT model, a novel framework that ensures spectral alignment throughout
both pre-training and prompt-tuning. We utilize a hybrid spectral filter
backbone and local-global contrastive learning to acquire abundant spectral
knowledge. Then we design prompt graphs to align the spectral distribution with
pretexts, facilitating spectral knowledge transfer across homophily and
heterophily. Extensive experiments validate the effectiveness under both
transductive and inductive learning settings. Our code is available at
https://anonymous.4open.science/r/HS-GPPT-62D2/.

### 35. LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ruiyan Qi, Congding Wen, Weibo Zhou, Shangsong Liang, Lingbo Li
- **URL**: <http://arxiv.org/abs/2508.11280v1>
- **Submitted**: 2025-08-15 07:37:12
- **Topic Keywords**: rag
- **Reason**: The paper focuses on evaluating large language models in the tourism domain, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on label-free evaluation and expert-derived reasoning structures is also not aligned with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Evaluating large language models (LLMs) in specific domain like tourism
remains challenging due to the prohibitive cost of annotated benchmarks and
persistent issues like hallucinations. We propose $\textbf{L}$able-Free
$\textbf{E}$valuation of LLM on $\textbf{T}$ourism using Expert
$\textbf{T}$ree-$\textbf{o}$f-$\textbf{T}$hought (LETToT), a framework that
leverages expert-derived reasoning structures-instead of labeled data-to access
LLMs in tourism. First, we iteratively refine and validate hierarchical ToT
components through alignment with generic quality dimensions and expert
feedback. Results demonstrate the effectiveness of our systematically optimized
expert ToT with 4.99-14.15\% relative quality gains over baselines. Second, we
apply LETToT's optimized expert ToT to evaluate models of varying scales
(32B-671B parameters), revealing: (1) Scaling laws persist in specialized
domains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,
DeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit
reasoning architectures outperform counterparts in accuracy and conciseness
($p<0.05$). Our work established a scalable, label-free paradigm for
domain-specific LLM evaluation, offering a robust alternative to conventional
annotated benchmarks.

### 36. Diffusion is a code repair operator and generator

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Mukul Singh, Gust Verbruggen, Vu Le, Sumit Gulwani
- **URL**: <http://arxiv.org/abs/2508.11110v1>
- **Submitted**: 2025-08-14 23:27:09
- **Comment**: 12 pages
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests as it focuses on code diffusion models and last-mile repair in the context of programming languages, which is outside the scope of Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Code diffusion models generate code by iteratively removing noise from the
latent representation of a code snippet. During later steps of the diffusion
process, when the code snippet has almost converged, differences between
discrete representations of these snippets look like last-mile repairs applied
to broken or incomplete code. We evaluate the extent to which this resemblance
can be exploited to leverage pre-trained code diffusion models for the problem
of last-mile repair by considering two applications with significant potential.
First, we can leverage the diffusion model for last-mile repair by adding noise
to a broken code snippet and resuming the diffusion process. Second, we can
leverage the diffusion model to generate arbitrary amount of training data for
last-mile repair tasks (that are computationally more efficient) by sampling an
intermediate program (input) and the final program (output) from the diffusion
process. We perform experiments on 3 domains (Python, Excel and PowerShell) to
evaluate applications, as well as analyze properties.

### 37. SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Wenpeng Xing, Lanyi Wei, Haixiao Hu, Rongchang Li, Mohan Li, Changting Lin, Meng Han
- **URL**: <http://arxiv.org/abs/2508.11009v1>
- **Submitted**: 2025-08-14 18:21:39
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on large language models for youth, AI safety, and child-centric AI design, which are outside your primary areas of interest.

#### Abstract
> The rapid proliferation of large language models (LLMs) in applications
targeting children and adolescents necessitates a fundamental reassessment of
prevailing AI safety frameworks, which are largely tailored to adult users and
neglect the distinct developmental vulnerabilities of minors. This paper
highlights key deficiencies in existing LLM safety benchmarks, including their
inadequate coverage of age-specific cognitive, emotional, and social risks
spanning early childhood (ages 0--6), middle childhood (7--12), and adolescence
(13--18). To bridge these gaps, we introduce SproutBench, an innovative
evaluation suite comprising 1,283 developmentally grounded adversarial prompts
designed to probe risks such as emotional dependency, privacy violations, and
imitation of hazardous behaviors. Through rigorous empirical evaluation of 47
diverse LLMs, we uncover substantial safety vulnerabilities, corroborated by
robust inter-dimensional correlations (e.g., between Safety and Risk
Prevention) and a notable inverse relationship between Interactivity and Age
Appropriateness. These insights yield practical guidelines for advancing
child-centric AI design and deployment.

### 38. Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Tejomay Kishor Padole, Suyash P Awate, Pushpak Bhattacharyya
- **URL**: <http://arxiv.org/abs/2508.10995v1>
- **Submitted**: 2025-08-14 18:01:22
- **Comment**: Accepted as a main conference submission in the European Conference
  on Artificial Intelligence (ECAI 2025)
- **Topic Keywords**: rag
- **Reason**: This paper focuses on text style transfer using masked diffusion language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on language models, the primary focus is on generative models rather than ranking models or user behavior modeling, making it only loosely relevant to the user's research interests.

#### Abstract
> Masked diffusion language models (MDMs) have recently gained traction as a
viable generative framework for natural language. This can be attributed to its
scalability and ease of training compared to other diffusion model paradigms
for discrete data, establishing itself as the state-of-the-art
non-autoregressive generator for discrete data. Diffusion models, in general,
have shown excellent ability to improve the generation quality by leveraging
inference-time scaling either by increasing the number of denoising steps or by
using external verifiers on top of the outputs of each step to guide the
generation. In this work, we propose a verifier-based inference-time scaling
method that aids in finding a better candidate generation during the denoising
process of the MDM. Our experiments demonstrate the application of MDMs for
standard text-style transfer tasks and establish MDMs as a better alternative
to autoregressive language models. Additionally, we show that a simple
soft-value-based verifier setup for MDMs using off-the-shelf pre-trained
embedding models leads to significant gains in generation quality even when
used on top of typical classifier-free guidance setups in the existing
literature.

### 39. Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Monika Jotautaitƒó, Lucius Caviola, David A. Brewster, Thilo Hagendorff
- **URL**: <http://arxiv.org/abs/2508.11534v1>
- **Submitted**: 2025-08-15 15:22:00
- **Topic Keywords**: search
- **Reason**: The paper focuses on the ethical implications of large language models in relation to animal species, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on fairness and alignment frameworks is also not a central concern for the user's research.

#### Abstract
> As large language models (LLMs) become more widely deployed, it is crucial to
examine their ethical tendencies. Building on research on fairness and
discrimination in AI, we investigate whether LLMs exhibit speciesist bias --
discrimination based on species membership -- and how they value non-human
animals. We systematically examine this issue across three paradigms: (1)
SpeciesismBench, a 1,003-item benchmark assessing recognition and moral
evaluation of speciesist statements; (2) established psychological measures
comparing model responses with those of human participants; (3) text-generation
tasks probing elaboration on, or resistance to, speciesist rationalizations. In
our benchmark, LLMs reliably detected speciesist statements but rarely
condemned them, often treating speciesist attitudes as morally acceptable. On
psychological measures, results were mixed: LLMs expressed slightly lower
explicit speciesism than people, yet in direct trade-offs they more often chose
to save one human over multiple animals. A tentative interpretation is that
LLMs may weight cognitive capacity rather than species per se: when capacities
were equal, they showed no species preference, and when an animal was described
as more capable, they tended to prioritize it over a less capable human. In
open-ended text generation tasks, LLMs frequently normalized or rationalized
harm toward farmed animals while refusing to do so for non-farmed animals.
These findings suggest that while LLMs reflect a mixture of progressive and
mainstream human views, they nonetheless reproduce entrenched cultural norms
around animal exploitation. We argue that expanding AI fairness and alignment
frameworks to explicitly include non-human moral patients is essential for
reducing these biases and preventing the entrenchment of speciesist attitudes
in AI systems and the societies they influence.

### 40. Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Sylvio R√ºdian, Yassin Elsir, Marvin Kretschmer, Sabine Cayrou, Niels Pinkwart
- **URL**: <http://arxiv.org/abs/2508.11364v1>
- **Submitted**: 2025-08-15 09:59:22
- **Comment**: 11 pages, one table
- **Topic Keywords**: search
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. While it involves language models, the focus is on extracting indicators for language learning feedback, which is not a core area of interest for you.

#### Abstract
> Automated feedback generation has the potential to enhance students' learning
progress by providing timely and targeted feedback. Moreover, it can assist
teachers in optimizing their time, allowing them to focus on more strategic and
personalized aspects of teaching. To generate high-quality, information-rich
formative feedback, it is essential first to extract relevant indicators, as
these serve as the foundation upon which the feedback is constructed. Teachers
often employ feedback criteria grids composed of various indicators that they
evaluate systematically. This study examines the initial phase of extracting
such indicators from students' submissions of a language learning course using
the large language model Llama 3.1. Accordingly, the alignment between
indicators generated by the LLM and human ratings across various feedback
criteria is investigated. The findings demonstrate statistically significant
strong correlations, even in cases involving unanticipated combinations of
indicators and criteria. The methodology employed in this paper offers a
promising foundation for extracting indicators from students' submissions using
LLMs. Such indicators can potentially be utilized to auto-generate explainable
and transparent formative feedback in future research.

### 41. Benchmarking Prosody Encoding in Discrete Speech Tokens

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Kentaro Onda, Satoru Fukayama, Daisuke Saito, Nobuaki Minematsu
- **URL**: <http://arxiv.org/abs/2508.11224v1>
- **Submitted**: 2025-08-15 05:11:16
- **Comment**: Accepted by ASRU2025
- **Topic Keywords**: search
- **Reason**: This paper focuses on prosody encoding in discrete speech tokens, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it touches on language models, the primary focus is on speech language models and prosodic features, which is outside the scope of the user's research interests.

#### Abstract
> Recently, discrete tokens derived from self-supervised learning (SSL) models
via k-means clustering have been actively studied as pseudo-text in speech
language models and as efficient intermediate representations for various
tasks. However, these discrete tokens are typically learned in advance,
separately from the training of language models or downstream tasks. As a
result, choices related to discretization, such as the SSL model used or the
number of clusters, must be made heuristically. In particular, speech language
models are expected to understand and generate responses that reflect not only
the semantic content but also prosodic features. Yet, there has been limited
research on the ability of discrete tokens to capture prosodic information. To
address this gap, this study conducts a comprehensive analysis focusing on
prosodic encoding based on their sensitivity to the artificially modified
prosody, aiming to provide practical guidelines for designing discrete tokens.

### 42. Representation Quantization for Collaborative Filtering Augmentation

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Yunze Luo, Yinjie Jiang, Gaode Chen, Jingchi Wang, Shicheng Wang, Ruina Sun, Jiang Yuezihan, Jun Zhang, Jian Liang, Han Li, Kun Gai, Kaigui Bian
- **URL**: <http://arxiv.org/abs/2508.11194v1>
- **Submitted**: 2025-08-15 04:00:50
- **Comment**: 11 pages, 4 figures
- **Topic Keywords**: recommend
- **Reason**: The paper focuses on collaborative filtering and recommendation systems, which is outside the user's primary research interest in Information Retrieval and Search technologies. Although it mentions feature and linkage augmentation, the approach is not related to query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user.

#### Abstract
> As the core algorithm in recommendation systems, collaborative filtering (CF)
algorithms inevitably face the problem of data sparsity. Since CF captures
similar users and items for recommendations, it is effective to augment the
lacking user-user and item-item homogeneous linkages. However, existing methods
are typically limited to connecting through overlapping interacted neighbors or
through similar attributes and contents. These approaches are constrained by
coarse-grained, sparse attributes and fail to effectively extract behavioral
characteristics jointly from interaction sequences and attributes. To address
these challenges, we propose a novel two-stage collaborative recommendation
algorithm, DQRec: Decomposition-based Quantized Variational AutoEncoder
(DQ-VAE) for Recommendation. DQRec augments features and homogeneous linkages
by extracting the behavior characteristics jointly from interaction sequences
and attributes, namely patterns, such as user multi-aspect interests. Inspired
by vector quantization (VQ) technology, we propose a new VQ algorithm, DQ-VAE,
which decomposes the pre-trained representation embeddings into distinct
dimensions, and quantize them to generates semantic IDs. We utilize the
generated semantic IDs as the extracted patterns mentioned above. By
integrating these semantic ID patterns into the recommendation process through
feature and linkage augmentation, the system enriches both latent and explicit
user and item features, identifies pattern-similar neighbors, and thereby
improves the efficiency of information diffusion. Experimental comparisons with
baselines across multiple datasets demonstrate the superior performance of the
proposed DQRec method.

### 43. Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Anusha M D, Deepthi Vikram, Bharathi Raja Chakravarthi, Parameshwar R Hegde
- **URL**: <http://arxiv.org/abs/2508.11166v1>
- **Submitted**: 2025-08-15 02:34:22
- **Comment**: 20 pages, 3 tables, 3 figures. Submitted to Language Resources and
  Evaluation (Springer)
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests as it focuses on a specific language (Tulu) and a topic (Offensive Language Identification) that is not directly related to Information Retrieval, Search technologies, or Natural Language Processing. The paper's emphasis on deep learning models and corpus creation is also not aligned with your interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Tulu, a low-resource Dravidian language predominantly spoken in southern
India, has limited computational resources despite its growing digital
presence. This study presents the first benchmark dataset for Offensive
Language Identification (OLI) in code-mixed Tulu social media content,
collected from YouTube comments across various domains. The dataset, annotated
with high inter-annotator agreement (Krippendorff's alpha = 0.984), includes
3,845 comments categorized into four classes: Not Offensive, Not Tulu,
Offensive Untargeted, and Offensive Targeted. We evaluate a suite of deep
learning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based
variants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU
model with self-attention achieves the best performance with 82% accuracy and a
0.81 macro F1-score. Transformer models underperform, highlighting the
limitations of multilingual pretraining in code-mixed, under-resourced
contexts. This work lays the foundation for further NLP research in Tulu and
similar low-resource, code-mixed languages.

### 44. Hell or High Water: Evaluating Agentic Recovery from External Failures

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Andrew Wang, Sophia Hager, Adi Asija, Daniel Khashabi, Nicholas Andrews
- **URL**: <http://arxiv.org/abs/2508.11027v1>
- **Submitted**: 2025-08-14 19:21:09
- **Comment**: Accepted to COLM 2025
- **Topic Keywords**: search
- **Reason**: The paper focuses on evaluating language model agents' ability to adapt to external failures in planning tasks, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it touches on search spaces and function calls, the context is quite different from the user's core research themes.

#### Abstract
> As language model agents are applied to real world problems of increasing
complexity, they will be expected to formulate plans across large search
spaces. If those plans fail for reasons beyond their control, how well do
language agents search for alternative ways to achieve their goals? We devise a
specialized agentic planning benchmark to study this question. Each planning
problem is solved via combinations of function calls. The agent searches for
relevant functions from a set of over four thousand possibilities, and observes
environmental feedback in the form of function outputs or error messages. Our
benchmark confronts the agent with external failures in its workflow, such as
functions that suddenly become unavailable. At the same time, even with the
introduction of these failures, we guarantee that the task remains solvable.
Ideally, an agent's performance on the planning task should not be affected by
the presence of external failures. Overall, we find that language agents
struggle to formulate and execute backup plans in response to environment
feedback. While state-of-the-art models are often able to identify the correct
function to use in the right context, they struggle to adapt to feedback from
the environment and often fail to pursue alternate courses of action, even when
the search space is artificially restricted. We provide a systematic analysis
of the failures of both open-source and commercial models, examining the
effects of search space size, as well as the benefits of scaling model size in
our setting. Our analysis identifies key challenges for current generative
models as well as promising directions for future work.

---

