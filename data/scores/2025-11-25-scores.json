[
    {
        "title": "Revisiting Feedback Models for HyDE",
        "abstract": "Recent approaches that leverage large language models (LLMs) for pseudo-relevance feedback (PRF) have generally not utilized well-established feedback models like Rocchio and RM3 when expanding queries for sparse retrievers like BM25. Instead, they often opt for a simple string concatenation of the query and LLM-generated expansion content. But is this optimal? To answer this question, we revisit and systematically evaluate traditional feedback models in the context of HyDE, a popular method that enriches query representations with LLM-generated hypothetical answer documents. Our experiments show that HyDE's effectiveness can be substantially improved when leveraging feedback algorithms such as Rocchio to extract and weight expansion terms, providing a simple way to further enhance the accuracy of LLM-based PRF methods.",
        "url": "http://arxiv.org/abs/2511.19349v1",
        "pdf_url": "https://arxiv.org/pdf/2511.19349v1",
        "arxiv_id": "2511.19349v1",
        "authors": [
            "Nour Jedidi",
            "Jimmy Lin"
        ],
        "submitted": "2025-11-24 17:50:18",
        "source": "arxiv",
        "comment": null,
        "score": 17,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'relevance feedback' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, specifically query understanding and ranking models. The use of pseudo-relevance feedback and traditional feedback models like Rocchio is closely related to your work on Learning to Rank and user behavior modeling. The focus on improving the accuracy of LLM-based methods is also aligned with your interests in deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "What Drives Cross-lingual Ranking? Retrieval Approaches with Multilingual Language Models",
        "abstract": "Cross-lingual information retrieval (CLIR) enables access to multilingual knowledge but remains challenging due to disparities in resources, scripts, and weak cross-lingual semantic alignment in embedding models. Existing pipelines often rely on translation and monolingual retrieval heuristics, which add computational overhead and noise, degrading performance. This work systematically evaluates four intervention types, namely document translation, multilingual dense retrieval with pretrained encoders, contrastive learning at word, phrase, and query-document levels, and cross-encoder re-ranking, across three benchmark datasets. We find that dense retrieval models trained specifically for CLIR consistently outperform lexical matching methods and derive little benefit from document translation. Contrastive learning mitigates language biases and yields substantial improvements for encoders with weak initial alignment, and re-ranking can be effective, but depends on the quality of the cross-encoder training data. Although high-resource languages still dominate overall performance, gains over lexical and document-translated baselines are most pronounced for low-resource and cross-script pairs. These findings indicate that cross-lingual search systems should prioritise semantic multilingual embeddings and targeted learning-based alignment over translation-based pipelines, particularly for cross-script and under-resourced languages.",
        "url": "http://arxiv.org/abs/2511.19324v1",
        "pdf_url": "https://arxiv.org/pdf/2511.19324v1",
        "arxiv_id": "2511.19324v1",
        "authors": [
            "Roksana Goworek",
            "Olivia Macmillan-Scott",
            "Eda B. Özyiğit"
        ],
        "submitted": "2025-11-24 17:17:40",
        "source": "arxiv",
        "comment": null,
        "score": 16,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'dense retrieval' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in cross-lingual ranking and retrieval approaches. The use of multilingual language models and dense retrieval models aligns with your focus on query understanding and ranking models. The paper's emphasis on semantic multilingual embeddings and targeted learning-based alignment also resonates with your interest in deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "Generative Query Expansion with Multilingual LLMs for Cross-Lingual Information Retrieval",
        "abstract": "Query expansion is the reformulation of a user query by adding semantically related information, and is an essential component of monolingual and cross-lingual information retrieval used to ensure that relevant documents are not missed. Recently, multilingual large language models (mLLMs) have shifted query expansion from semantic augmentation with synonyms and related words to pseudo-document generation. Pseudo-documents both introduce additional relevant terms and bridge the gap between short queries and long documents, which is particularly beneficial in dense retrieval. This study evaluates recent mLLMs and fine-tuned variants across several generative expansion strategies to identify factors that drive cross-lingual retrieval performance. Results show that query length largely determines which prompting technique is effective, and that more elaborate prompts often do not yield further gains. Substantial linguistic disparities persist: cross-lingual query expansion can produce the largest improvements for languages with the weakest baselines, yet retrieval is especially poor between languages written in different scripts. Fine-tuning is found to lead to performance gains only when the training and test data are of similar format. These outcomes underline the need for more balanced multilingual and cross-lingual training and evaluation resources.",
        "url": "http://arxiv.org/abs/2511.19325v1",
        "pdf_url": "https://arxiv.org/pdf/2511.19325v1",
        "arxiv_id": "2511.19325v1",
        "authors": [
            "Olivia Macmillan-Scott",
            "Roksana Goworek",
            "Eda B. Özyiğit"
        ],
        "submitted": "2025-11-24 17:18:25",
        "source": "arxiv",
        "comment": null,
        "score": 14,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'dense retrieval' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 9,
        "llm_reason": "This paper aligns closely with your research interests in Information Retrieval, particularly in query understanding and cross-lingual retrieval. The use of multilingual LLMs for query expansion and pseudo-document generation is a relevant application of deep semantic understanding and real-time relevance optimization. The study's focus on generative expansion strategies and cross-lingual retrieval performance is also of interest."
    },
    {
        "title": "CLaRa: Bridging Retrieval and Generation with Continuous Latent Reasoning",
        "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but still suffers from long contexts and disjoint retrieval-generation optimization. In this work, we propose CLaRa (Continuous Latent Reasoning), a unified framework that performs embedding-based compression and joint optimization in a shared continuous space. To obtain semantically rich and retrievable compressed vectors, we introduce SCP, a key-preserving data synthesis framework using QA and paraphrase supervision. CLaRa then trains the reranker and generator end-to-end via a single language modeling loss, with gradients flowing through both modules using a differentiable top-k estimator. Theoretically, this unified optimization aligns retrieval relevance with answer quality. Experiments across multiple QA benchmarks show that CLaRa achieves state-of-the-art compression and reranking performance, often surpassing text-based fine-tuned baselines.",
        "url": "http://arxiv.org/abs/2511.18659v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18659v1",
        "arxiv_id": "2511.18659v1",
        "authors": [
            "Jie He",
            "Richard He Bai",
            "Sinead Williamson",
            "Jeff Z. Pan",
            "Navdeep Jaitly",
            "Yizhe Zhang"
        ],
        "submitted": "2025-11-24 00:11:14",
        "source": "arxiv",
        "comment": null,
        "score": 14,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper proposes a unified framework for retrieval-augmented generation, which aligns with your interests in Information Retrieval and Search technologies. The use of continuous latent reasoning and embedding-based compression is relevant to your focus on deep semantic understanding and real-time relevance optimization. However, the specific application to QA benchmarks is somewhat narrow compared to your broader interests in IR and NLP."
    },
    {
        "title": "Look It Up: Analysing Internal Web Search Capabilities of Modern LLMs",
        "abstract": "Modern large language models integrate web search to provide real-time answers, yet it remains unclear whether they are efficiently calibrated to use search when it is actually needed. We introduce a benchmark evaluating both the necessity and effectiveness of web access across commercial models with no access to internal states or parameters. The dataset includes a static split of 783 temporally anchored questions answerable from pre-cutoff knowledge, aimed at testing whether models invoke search based on low internal confidence, and a dynamic split of 288 post-cutoff queries designed to test whether models recognise when search is required and retrieve updated information. Web access substantially improves static accuracy for GPT-5-mini and Claude Haiku 4.5, though confidence calibration worsens. On dynamic queries, both models frequently invoke search yet remain below 70 percent accuracy due to weak query formulation. Costs per accuracy-improving call remain low, but returns diminish once initial retrieval fails. Selective invocation helps, but models become overconfident and inconsistent after search. Overall, built-in web search meaningfully improves factual accuracy and can be invoked selectively, yet models remain overconfident, skip retrieval when it is essential, and falter once initial search queries underperform. Taken together, internal web search works better as a good low-latency verification layer than a reliable analytical tool, with clear room for improvement.",
        "url": "http://arxiv.org/abs/2511.18931v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18931v1",
        "arxiv_id": "2511.18931v1",
        "authors": [
            "Sahil Kale"
        ],
        "submitted": "2025-11-24 09:37:43",
        "source": "arxiv",
        "comment": "10 pages, 8 figures",
        "score": 10,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'web search' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of query understanding and ranking models. The study focuses on the integration of web search in large language models, which is a key area of interest in your background. While the paper's focus on NLP is also relevant, it is not the primary area of focus, hence the score of 8."
    },
    {
        "title": "HyperbolicRAG: Enhancing Retrieval-Augmented Generation with Hyperbolic Representations",
        "abstract": "Retrieval-augmented generation (RAG) enables large language models (LLMs) to access external knowledge, helping mitigate hallucinations and enhance domain-specific expertise. Graph-based RAG enhances structural reasoning by introducing explicit relational organization that enables information propagation across semantically connected text units. However, these methods typically rely on Euclidean embeddings that capture semantic similarity but lack a geometric notion of hierarchical depth, limiting their ability to represent abstraction relationships inherent in complex knowledge graphs. To capture both fine-grained semantics and global hierarchy, we propose HyperbolicRAG, a retrieval framework that integrates hyperbolic geometry into graph-based RAG. HyperbolicRAG introduces three key designs: (1) a depth-aware representation learner that embeds nodes within a shared Poincare manifold to align semantic similarity with hierarchical containment, (2) an unsupervised contrastive regularization that enforces geometric consistency across abstraction levels, and (3) a mutual-ranking fusion mechanism that jointly exploits retrieval signals from Euclidean and hyperbolic spaces, emphasizing cross-space agreement during inference. Extensive experiments across multiple QA benchmarks demonstrate that HyperbolicRAG outperforms competitive baselines, including both standard RAG and graph-augmented baselines.",
        "url": "http://arxiv.org/abs/2511.18808v2",
        "pdf_url": "https://arxiv.org/pdf/2511.18808v2",
        "arxiv_id": "2511.18808v2",
        "authors": [
            "Linxiao Cao",
            "Ruitao Wang",
            "Jindong Li",
            "Zhipeng Zhou",
            "Menglin Yang"
        ],
        "submitted": "2025-11-24 06:27:58",
        "source": "arxiv",
        "comment": "12 pages",
        "score": 8,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper proposes a novel framework, HyperbolicRAG, that integrates hyperbolic geometry into retrieval-augmented generation, addressing limitations in Euclidean embeddings. While primarily focused on NLP and knowledge graph representation, it touches on query understanding and ranking models, making it somewhat relevant to your research interests in Information Retrieval and Search technologies."
    },
    {
        "title": "STORE: Semantic Tokenization, Orthogonal Rotation and Efficient Attention for Scaling Up Ranking Models",
        "abstract": "Ranking models have become an important part of modern personalized recommendation systems. However, significant challenges persist in handling high-cardinality, heterogeneous, and sparse feature spaces, particularly regarding model scalability and efficiency. We identify two key bottlenecks: (i) Representation Bottleneck: Driven by the high cardinality and dynamic nature of features, model capacity is forced into sparse-activated embedding layers, leading to low-rank representations. This, in turn, triggers phenomena like \"One-Epoch\" and \"Interaction-Collapse,\" ultimately hindering model scalability.(ii) Computational Bottleneck: Integrating all heterogeneous features into a unified model triggers an explosion in the number of feature tokens, rendering traditional attention mechanisms computationally demanding and susceptible to attention dispersion. To dismantle these barriers, we introduce STORE, a unified and scalable token-based ranking framework built upon three core innovations: (1) Semantic Tokenization fundamentally tackles feature heterogeneity and sparsity by decomposing high-cardinality sparse features into a compact set of stable semantic tokens; and (2) Orthogonal Rotation Transformation is employed to rotate the subspace spanned by low-cardinality static features, which facilitates more efficient and effective feature interactions; and (3) Efficient attention that filters low-contributing tokens to improve computional efficiency while preserving model accuracy. Across extensive offline experiments and online A/B tests, our framework consistently improves prediction accuracy(online CTR by 2.71%, AUC by 1.195%) and training effeciency (1.84 throughput).",
        "url": "http://arxiv.org/abs/2511.18805v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18805v1",
        "arxiv_id": "2511.18805v1",
        "authors": [
            "Yi Xu",
            "Chaofan Fan",
            "Jinxin Hu",
            "Yu Zhang",
            "Zeng Xiaoyi",
            "Jing Zhang"
        ],
        "submitted": "2025-11-24 06:20:02",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'ctr' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 9,
        "llm_reason": "This paper is extremely relevant to your research interests in Information Retrieval, specifically ranking models and scalability. The authors introduce a unified and scalable token-based ranking framework, STORE, which addresses key bottlenecks in model scalability and efficiency. The paper's focus on semantic tokenization, orthogonal rotation, and efficient attention aligns closely with your interests in query understanding and real-time relevance optimization."
    },
    {
        "title": "CoreEval: Automatically Building Contamination-Resilient Datasets with Real-World Knowledge toward Reliable LLM Evaluation",
        "abstract": "Data contamination poses a significant challenge to the fairness of LLM evaluations in natural language processing tasks by inadvertently exposing models to test data during training. Current studies attempt to mitigate this issue by modifying existing datasets or generating new ones from freshly collected information. However, these methods fall short of ensuring contamination-resilient evaluation, as they fail to fully eliminate pre-existing knowledge from models or preserve the semantic complexity of the original datasets. To address these limitations, we propose \\textbf{CoreEval}, a \\textbf{Co}ntamination-\\textbf{re}silient \\textbf{Eval}uation strategy for automatically updating data with real-world knowledge. This approach begins by extracting entity relationships from the original data and leveraging the GDELT database to retrieve relevant, up-to-date knowledge. The retrieved knowledge is then recontextualized and integrated with the original data, which is refined and restructured to ensure semantic coherence and enhanced task relevance. Ultimately, a robust data reflection mechanism is employed to iteratively verify and refine labels, ensuring consistency between the updated and original datasets. Extensive experiments on updated datasets validate the robustness of CoreEval, demonstrating its effectiveness in mitigating performance overestimation caused by data contamination.",
        "url": "http://arxiv.org/abs/2511.18889v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18889v1",
        "arxiv_id": "2511.18889v1",
        "authors": [
            "Jingqian Zhao",
            "Bingbing Wang",
            "Geng Tu",
            "Yice Zhang",
            "Qianlong Wang",
            "Bin Liang",
            "Jing Li",
            "Ruifeng Xu"
        ],
        "submitted": "2025-11-24 08:44:29",
        "source": "arxiv",
        "comment": "ACL'25",
        "score": 5,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Natural Language Processing (NLP) and data quality, but it does not directly focus on Information Retrieval (IR), query understanding, ranking models, or user behavior modeling. The paper's emphasis on LLM evaluation and dataset contamination is relevant to your broader interests in NLP, but it is not a central match for your primary research themes."
    },
    {
        "title": "DR Tulu: Reinforcement Learning with Evolving Rubrics for Deep Research",
        "abstract": "Deep research models perform multi-step research to produce long-form, well-attributed answers. However, most open deep research models are trained on easily verifiable short-form QA tasks via reinforcement learning with verifiable rewards (RLVR), which does not extend to realistic long-form tasks. We address this with Reinforcement Learning with Evolving Rubrics (RLER), in which we construct and maintain rubrics that co-evolve with the policy model during training; this allows the rubrics to incorporate information that the model has newly explored and to provide discriminative, on-policy feedback. Using RLER, we develop Deep Research Tulu (DR Tulu-8B), the first open model that is directly trained for open-ended, long-form deep research. Across four long-form deep research benchmarks in science, healthcare and general domains, DR Tulu substantially outperforms existing open deep research models, and matches or exceeds proprietary deep research systems, while being significantly smaller and cheaper per query. To facilitate future research, we release all data, models, and code, including our new MCP-based agent infrastructure for deep research systems.",
        "url": "http://arxiv.org/abs/2511.19399v1",
        "pdf_url": "https://arxiv.org/pdf/2511.19399v1",
        "arxiv_id": "2511.19399v1",
        "authors": [
            "Rulin Shao",
            "Akari Asai",
            "Shannon Zejiang Shen",
            "Hamish Ivison",
            "Varsha Kishore",
            "Jingming Zhuo",
            "Xinran Zhao",
            "Molly Park",
            "Samuel G. Finlayson",
            "David Sontag",
            "Tyler Murray",
            "Sewon Min",
            "Pradeep Dasigi",
            "Luca Soldaini",
            "Faeze Brahman",
            "Wen-tau Yih",
            "Tongshuang Wu",
            "Luke Zettlemoyer",
            "Yoon Kim",
            "Hannaneh Hajishirzi",
            "Pang Wei Koh"
        ],
        "submitted": "2025-11-24 18:35:54",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to information retrieval, specifically in the context of deep semantic understanding and long-form question answering. However, its focus on reinforcement learning and rubric evolution for deep research models is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling. While it touches on the idea of optimizing relevance, it is more focused on the research process itself rather than search technologies."
    },
    {
        "title": "MultiBanAbs: A Comprehensive Multi-Domain Bangla Abstractive Text Summarization Dataset",
        "abstract": "This study developed a new Bangla abstractive summarization dataset to generate concise summaries of Bangla articles from diverse sources. Most existing studies in this field have concentrated on news articles, where journalists usually follow a fixed writing style. While such approaches are effective in limited contexts, they often fail to adapt to the varied nature of real-world Bangla texts. In today's digital era, a massive amount of Bangla content is continuously produced across blogs, newspapers, and social media. This creates a pressing need for summarization systems that can reduce information overload and help readers understand content more quickly. To address this challenge, we developed a dataset of over 54,000 Bangla articles and summaries collected from multiple sources, including blogs such as Cinegolpo and newspapers such as Samakal and The Business Standard. Unlike single-domain resources, our dataset spans multiple domains and writing styles. It offers greater adaptability and practical relevance. To establish strong baselines, we trained and evaluated this dataset using several deep learning and transfer learning models, including LSTM, BanglaT5-small, and MTS-small. The results highlight its potential as a benchmark for future research in Bangla natural language processing. This dataset provides a solid foundation for building robust summarization systems and helps expand NLP resources for low-resource languages.",
        "url": "http://arxiv.org/abs/2511.19317v1",
        "pdf_url": "https://arxiv.org/pdf/2511.19317v1",
        "arxiv_id": "2511.19317v1",
        "authors": [
            "Md. Tanzim Ferdous",
            "Naeem Ahsan Chowdhury",
            "Prithwiraj Bhattacharjee"
        ],
        "submitted": "2025-11-24 17:11:49",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Bangla abstractive summarization, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve NLP, the specific domain (Bangla language) and task (text summarization) are not central to the user's research themes."
    },
    {
        "title": "PRInTS: Reward Modeling for Long-Horizon Information Seeking",
        "abstract": "Information-seeking is a core capability for AI agents, requiring them to gather and reason over tool-generated information across long trajectories. However, such multi-step information-seeking tasks remain challenging for agents backed by language models. While process reward models (PRMs) can guide agents by ranking candidate steps at test-time, existing PRMs, designed for short reasoning with binary judgment, cannot capture richer dimensions of information-seeking steps, such as tool interactions and reasoning over tool outputs, nor handle the rapidly growing context in long-horizon tasks. To address these limitations, we introduce PRInTS, a generative PRM trained with dual capabilities: (1) dense scoring based on the PRM's reasoning across multiple step quality dimensions (e.g., interpretation of tool outputs, tool call informativeness) and (2) trajectory summarization that compresses the growing context while preserving essential information for step evaluation. Extensive evaluations across FRAMES, GAIA (levels 1-3), and WebWalkerQA (easy-hard) benchmarks on multiple models, along with ablations, reveal that best-of-n sampling with PRInTS enhances information-seeking abilities of open-source models as well as specialized agents, matching or surpassing the performance of frontier models with a much smaller backbone agent and outperforming other strong reward modeling baselines.",
        "url": "http://arxiv.org/abs/2511.19314v1",
        "pdf_url": "https://arxiv.org/pdf/2511.19314v1",
        "arxiv_id": "2511.19314v1",
        "authors": [
            "Jaewoo Lee",
            "Archiki Prasad",
            "Justin Chih-Yao Chen",
            "Zaid Khan",
            "Elias Stengel-Eskin",
            "Mohit Bansal"
        ],
        "submitted": "2025-11-24 17:09:43",
        "source": "arxiv",
        "comment": "18 pages, code: https://github.com/G-JWLee/PRInTS",
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in areas that require deep semantic understanding and real-time relevance optimization. The focus on reward modeling for long-horizon information seeking aligns with your interests in query understanding and ranking models. However, the specific application to AI agents and tool-generated information seeking is somewhat outside your typical e-commerce domain focus."
    },
    {
        "title": "From Pixels to Posts: Retrieval-Augmented Fashion Captioning and Hashtag Generation",
        "abstract": "This paper introduces the retrieval-augmented framework for automatic fashion caption and hashtag generation, combining multi-garment detection, attribute reasoning, and Large Language Model (LLM) prompting. The system aims to produce visually grounded, descriptive, and stylistically interesting text for fashion imagery, overcoming the limitations of end-to-end captioners that have problems with attribute fidelity and domain generalization. The pipeline combines a YOLO-based detector for multi-garment localization, k-means clustering for dominant color extraction, and a CLIP-FAISS retrieval module for fabric and gender attribute inference based on a structured product index. These attributes, together with retrieved style examples, create a factual evidence pack that is used to guide an LLM to generate human-like captions and contextually rich hashtags. A fine-tuned BLIP model is used as a supervised baseline model for comparison. Experimental results show that the YOLO detector is able to obtain a mean Average Precision (mAP@0.5) of 0.71 for nine categories of garments. The RAG-LLM pipeline generates expressive attribute-aligned captions and achieves mean attribute coverage of 0.80 with full coverage at the 50% threshold in hashtag generation, whereas BLIP gives higher lexical overlap and lower generalization. The retrieval-augmented approach exhibits better factual grounding, less hallucination, and great potential for scalable deployment in various clothing domains. These results demonstrate the use of retrieval-augmented generation as an effective and interpretable paradigm for automated and visually grounded fashion content generation.",
        "url": "http://arxiv.org/abs/2511.19149v1",
        "pdf_url": "https://arxiv.org/pdf/2511.19149v1",
        "arxiv_id": "2511.19149v1",
        "authors": [
            "Moazzam Umer Gondal",
            "Hamad Ul Qudous",
            "Daniya Siddiqui",
            "Asma Ahmad Farhan"
        ],
        "submitted": "2025-11-24 14:13:57",
        "source": "arxiv",
        "comment": "Submitted to Expert Systems with Applications",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores a retrieval-augmented framework for fashion captioning and hashtag generation, which is somewhat related to information retrieval and natural language processing. However, the focus on fashion and visual grounding is not a central match to the user's core research themes, which are more focused on general query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "A Multi-Agent LLM Framework for Multi-Domain Low-Resource In-Context NER via Knowledge Retrieval, Disambiguation and Reflective Analysis",
        "abstract": "In-context learning (ICL) with large language models (LLMs) has emerged as a promising paradigm for named entity recognition (NER) in low-resource scenarios. However, existing ICL-based NER methods suffer from three key limitations: (1) reliance on dynamic retrieval of annotated examples, which is problematic when annotated data is scarce; (2) limited generalization to unseen domains due to the LLM's insufficient internal domain knowledge; and (3) failure to incorporate external knowledge or resolve entity ambiguities. To address these challenges, we propose KDR-Agent, a novel multi-agent framework for multi-domain low-resource in-context NER that integrates Knowledge retrieval, Disambiguation, and Reflective analysis. KDR-Agent leverages natural-language type definitions and a static set of entity-level contrastive demonstrations to reduce dependency on large annotated corpora. A central planner coordinates specialized agents to (i) retrieve factual knowledge from Wikipedia for domain-specific mentions, (ii) resolve ambiguous entities via contextualized reasoning, and (iii) reflect on and correct model predictions through structured self-assessment. Experiments across ten datasets from five domains demonstrate that KDR-Agent significantly outperforms existing zero-shot and few-shot ICL baselines across multiple LLM backbones. The code and data can be found at https://github.com/MWXGOD/KDR-Agent.",
        "url": "http://arxiv.org/abs/2511.19083v1",
        "pdf_url": "https://arxiv.org/pdf/2511.19083v1",
        "arxiv_id": "2511.19083v1",
        "authors": [
            "Wenxuan Mu",
            "Jinzhong Ning",
            "Di Zhao",
            "Yijia Zhang"
        ],
        "submitted": "2025-11-24 13:23:34",
        "source": "arxiv",
        "comment": "This paper has been accepted by AAAI 2026 (Main Technical Track)",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Natural Language Processing (NLP) and Information Retrieval (IR), particularly in the area of named entity recognition (NER). However, the focus on low-resource scenarios and multi-domain NER with large language models is not directly aligned with your primary interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Skeletons Matter: Dynamic Data Augmentation for Text-to-Query",
        "abstract": "The task of translating natural language questions into query languages has long been a central focus in semantic parsing. Recent advancements in Large Language Models (LLMs) have significantly accelerated progress in this field. However, existing studies typically focus on a single query language, resulting in methods with limited generalizability across different languages. In this paper, we formally define the Text-to-Query task paradigm, unifying semantic parsing tasks across various query languages. We identify query skeletons as a shared optimization target of Text-to-Query tasks, and propose a general dynamic data augmentation framework that explicitly diagnoses model-specific weaknesses in handling these skeletons to synthesize targeted training data. Experiments on four Text-to-Query benchmarks demonstrate that our method achieves state-of-the-art performance using only a small amount of synthesized data, highlighting the efficiency and generality of our approach and laying a solid foundation for unified research on Text-to-Query tasks. We release our code at https://github.com/jjjycaptain/Skeletron.",
        "url": "http://arxiv.org/abs/2511.18934v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18934v1",
        "arxiv_id": "2511.18934v1",
        "authors": [
            "Yuchen Ji",
            "Bo Xu",
            "Jie Shi",
            "Jiaqing Liang",
            "Deqing Yang",
            "Yu Mao",
            "Hai Chen",
            "Yanghua Xiao"
        ],
        "submitted": "2025-11-24 09:39:03",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025",
        "score": 4,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 7,
        "llm_reason": "This paper explores the Text-to-Query task, which involves translating natural language questions into query languages. While it doesn't directly focus on query understanding, ranking models, or user behavior modeling, it does involve semantic parsing and Large Language Models, which are related to Information Retrieval and NLP. The paper's emphasis on generalizability and dynamic data augmentation also aligns with the user's interests in real-time relevance optimization."
    },
    {
        "title": "Concept than Document: Context Compression via AMR-based Conceptual Entropy",
        "abstract": "Large Language Models (LLMs) face information overload when handling long contexts, particularly in Retrieval-Augmented Generation (RAG) where extensive supporting documents often introduce redundant content. This issue not only weakens reasoning accuracy but also increases computational overhead. We propose an unsupervised context compression framework that exploits Abstract Meaning Representation (AMR) graphs to preserve semantically essential information while filtering out irrelevant text. By quantifying node-level entropy within AMR graphs, our method estimates the conceptual importance of each node, enabling the retention of core semantics. Specifically, we construct AMR graphs from raw contexts, compute the conceptual entropy of each node, and screen significant informative nodes to form a condensed and semantically focused context than raw documents. Experiments on the PopQA and EntityQuestions datasets show that our method outperforms vanilla and other baselines, achieving higher accuracy while substantially reducing context length. To the best of our knowledge, this is the first work introducing AMR-based conceptual entropy for context compression, demonstrating the potential of stable linguistic features in context engineering.",
        "url": "http://arxiv.org/abs/2511.18832v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18832v1",
        "arxiv_id": "2511.18832v1",
        "authors": [
            "Kaize Shi",
            "Xueyao Sun",
            "Xiaohui Tao",
            "Lin Li",
            "Qika Lin",
            "Guandong Xu"
        ],
        "submitted": "2025-11-24 07:08:02",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper explores context compression using Abstract Meaning Representation (AMR) graphs, which is related to query understanding and ranking models in Information Retrieval. However, its focus on language models and context compression is somewhat tangential to the user's core research themes, which include query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Large Language Models Require Curated Context for Reliable Political Fact-Checking -- Even with Reasoning and Web Search",
        "abstract": "Large language models (LLMs) have raised hopes for automated end-to-end fact-checking, but prior studies report mixed results. As mainstream chatbots increasingly ship with reasoning capabilities and web search tools -- and millions of users already rely on them for verification -- rigorous evaluation is urgent. We evaluate 15 recent LLMs from OpenAI, Google, Meta, and DeepSeek on more than 6,000 claims fact-checked by PolitiFact, comparing standard models with reasoning- and web-search variants. Standard models perform poorly, reasoning offers minimal benefits, and web search provides only moderate gains, despite fact-checks being available on the web. In contrast, a curated RAG system using PolitiFact summaries improved macro F1 by 233% on average across model variants. These findings suggest that giving models access to curated high-quality context is a promising path for automated fact-checking.",
        "url": "http://arxiv.org/abs/2511.18749v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18749v1",
        "arxiv_id": "2511.18749v1",
        "authors": [
            "Matthew R. DeVerna",
            "Kai-Cheng Yang",
            "Harry Yaojun Yan",
            "Filippo Menczer"
        ],
        "submitted": "2025-11-24 04:22:32",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'web search' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper primarily focuses on the application of large language models for fact-checking, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve NLP, the context and methodology are quite different from your areas of focus."
    },
    {
        "title": "Evaluating Large Language Models on the 2026 Korean CSAT Mathematics Exam: Measuring Mathematical Ability in a Zero-Data-Leakage Setting",
        "abstract": "This study systematically evaluated the mathematical reasoning capabilities of Large Language Models (LLMs) using the 2026 Korean College Scholastic Ability Test (CSAT) Mathematics section, ensuring a completely contamination-free evaluation environment. To address data leakage issues in existing benchmarks, we digitized all 46 questions (22 common and 24 elective) within two hours of the exam's public release, eliminating any possibility of inclusion in model training data. We conducted comprehensive evaluations of 24 state-of-the-art LLMs across varying input modalities (text, image, text+figure) and prompt languages (Korean, English).\n  GPT-5 Codex achieved the only perfect score (100 points) with text input and Korean prompts, while Grok 4, GPT-5, and Deepseek R1 scored above 95 points. Notably, gpt-oss-20B achieved 95.7 points despite its relatively small size, demonstrating high cost-effectiveness. Problem-specific analysis revealed geometry as the weakest domain (77.7% average) with significant performance degradation on 4-point high-difficulty problems. Text input consistently outperformed image input, while prompt language effects varied by model scale.\n  In reasoning enhancement experiments with GPT-5 series, increased reasoning intensity improved performance (from 82.6 to 100 points) but quadrupled token usage and drastically reduced efficiency, suggesting that models with minimal reasoning may be more practical. This research contributes: (1) implementation of a completely unexposed evaluation environment, (2) a real-exam-based LLM assessment framework, and (3) a practical evaluation perspective integrating performance, cost, and time considerations. Detailed results and model comparisons are available at the 2026 Korean CSAT LLM Evaluation Leaderboard (https://isoft.cnu.ac.kr/csat2026/).",
        "url": "http://arxiv.org/abs/2511.18649v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18649v1",
        "arxiv_id": "2511.18649v1",
        "authors": [
            "Goun Pyeon",
            "Inbum Heo",
            "Jeesu Jung",
            "Taewook Hwang",
            "Hyuk Namgoong",
            "Hyein Seo",
            "Yerim Han",
            "Eunbin Kim",
            "Hyeonseok Kang",
            "Sangkeun Jung"
        ],
        "submitted": "2025-11-23 23:09:33",
        "source": "arxiv",
        "comment": "52 pages, Korean",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)",
            "Found 'korea' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on evaluating Large Language Models on a specific exam, which is outside the scope of Information Retrieval, Search technologies, and related topics. Although it involves Natural Language Processing, the context is not aligned with your primary focus on deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "From Raw Features to Effective Embeddings: A Three-Stage Approach for Multimodal Recipe Recommendation",
        "abstract": "Recipe recommendation has become an essential task in web-based food platforms. A central challenge is effectively leveraging rich multimodal features beyond user-recipe interactions. Our analysis shows that even simple uses of multimodal signals yield competitive performance, suggesting that systematic enhancement of these signals is highly promising. We propose TESMR, a 3-stage framework for recipe recommendation that progressively refines raw multimodal features into effective embeddings through: (1) content-based enhancement using foundation models with multimodal comprehension, (2) relation-based enhancement via message propagation over user-recipe interactions, and (3) learning-based enhancement through contrastive learning with learnable embeddings. Experiments on two real-world datasets show that TESMR outperforms existing methods, achieving 7-15% higher Recall@10.",
        "url": "http://arxiv.org/abs/2511.19176v1",
        "pdf_url": "https://arxiv.org/pdf/2511.19176v1",
        "arxiv_id": "2511.19176v1",
        "authors": [
            "Jeeho Shin",
            "Kyungho Kim",
            "Kijung Shin"
        ],
        "submitted": "2025-11-24 14:37:22",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on multimodal recipe recommendation, which is somewhat related to information retrieval, but it lacks direct connection to query understanding, ranking models, or user behavior modeling. While it involves learning-based enhancement, the context is different from search technologies and e-commerce. The paper's emphasis on multimodal comprehension and embeddings is more aligned with NLP, but it's not a central match for the user's primary research interests."
    },
    {
        "title": "Studying Maps at Scale: A Digital Investigation of Cartography and the Evolution of Figuration",
        "abstract": "This thesis presents methods and datasets to investigate cartographic heritage on a large scale and from a cultural perspective. Heritage institutions worldwide have digitized more than one million maps, and automated techniques now enable large-scale recognition and extraction of map content. Yet these methods have engaged little with the history of cartography, or the view that maps are semantic-symbolic systems, and cultural objects reflecting political and epistemic expectations. This work leverages a diverse corpus of 771,561 map records and 99,715 digitized images aggregated from 38 digital catalogs. After normalization, the dataset includes 236,925 contributors and spans six centuries, from 1492 to 1948. These data make it possible to chart geographic structures and the global chronology of map publication. The spatial focus of cartography is analyzed in relation to political dynamics, evidencing links between Atlantic maritime charting, the triangular trade, and colonial expansion. Further results document the progression of national, domestic focus and the impact of military conflicts on publication volumes. The research introduces semantic segmentation techniques and object detection models for the generic recognition of land classes and cartographic signs, trained on annotated data and synthetic images. The analysis of land classes shows that maps are designed images whose framing and composition emphasize features through centering and semantic symmetries. The study of cartographic figuration encodes 63 M signs and 25 M fragments into a latent visual space, revealing figurative shifts such as the replacement of relief hachures by terrain contours and showing that signs tend to form locally consistent systems. Analyses of collaboration and diffusion highlight the role of legitimacy, larger actors, and major cities in the spread of figurative norms and semiotic cultures.",
        "url": "http://arxiv.org/abs/2511.19538v1",
        "pdf_url": "https://arxiv.org/pdf/2511.19538v1",
        "arxiv_id": "2511.19538v1",
        "authors": [
            "Remi Petitpierre"
        ],
        "submitted": "2025-11-24 10:35:37",
        "source": "arxiv",
        "comment": "PhD thesis, EPFL. 396 pages, 156 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper is not related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, Natural Language Processing, or data mining. The paper focuses on cartography, cultural heritage, and the analysis of maps, which is outside the user's core research themes."
    },
    {
        "title": "Knowledge-based Graphical Method for Safety Signal Detection in Clinical Trials",
        "abstract": "We present a graphical, knowledge-based method for reviewing treatment-emergent adverse events (AEs) in clinical trials. The approach enhances MedDRA by adding a hidden medical knowledge layer (Safeterm) that captures semantic relationships between terms in a 2-D map. Using this layer, AE Preferred Terms can be regrouped automatically into similarity clusters, and their association to the trial disease may be quantified. The Safeterm map is available online and connected to aggregated AE incidence tables from ClinicalTrials.gov. For signal detection, we compute treatment-specific disproportionality metrics using shrinkage incidence ratios. Cluster-level EBGM values are then derived through precision-weighted aggregation. Two visual outputs support interpretation: a semantic map showing AE incidence and an expectedness-versus-disproportionality plot for rapid signal detection. Applied to three legacy trials, the automated method clearly recovers all expected safety signals. Overall, augmenting MedDRA with a medical knowledge layer improves clarity, efficiency, and accuracy in AE interpretation for clinical trials.",
        "url": "http://arxiv.org/abs/2511.18937v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18937v1",
        "arxiv_id": "2511.18937v1",
        "authors": [
            "Francois Vandenhende",
            "Anna Georgiou",
            "Michalis Georgiou",
            "Theodoros Psaras",
            "Ellie Karekla",
            "Elena Hadjicosta"
        ],
        "submitted": "2025-11-24 09:42:58",
        "source": "arxiv",
        "comment": "13 pages, 3 tables, 5 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'ltr' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. Although it involves data analysis and visualization, the focus is on clinical trials and safety signal detection, which is unrelated to your areas of expertise."
    },
    {
        "title": "Generating Reading Comprehension Exercises with Large Language Models for Educational Applications",
        "abstract": "With the rapid development of large language models (LLMs), the applications of LLMs have grown substantially. In the education domain, LLMs demonstrate significant potential, particularly in automatic text generation, which enables the creation of intelligent and adaptive learning content. This paper proposes a new LLMs framework, which is named as Reading Comprehension Exercise Generation (RCEG). It can generate high-quality and personalized English reading comprehension exercises automatically. Firstly, RCEG uses fine-tuned LLMs to generate content candidates. Then, it uses a discriminator to select the best candidate. Finally, the quality of the generated content has been improved greatly. To evaluate the performance of RCEG, a dedicated dataset for English reading comprehension is constructed to perform the experiments, and comprehensive evaluation metrics are used to analyze the experimental results. These metrics include content diversity, factual accuracy, linguistic toxicity, and pedagogical alignment. Experimental results show that RCEG significantly improves the relevance and cognitive appropriateness of the generated exercises.",
        "url": "http://arxiv.org/abs/2511.18860v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18860v1",
        "arxiv_id": "2511.18860v1",
        "authors": [
            "Xingyu Huang",
            "Fei Jiang",
            "Jianli Xiao"
        ],
        "submitted": "2025-11-24 08:00:48",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on using large language models for educational applications, specifically generating reading comprehension exercises. While it involves natural language processing, it does not align with the user's core research themes in information retrieval, search technologies, or query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Cognitive Alpha Mining via LLM-Driven Code-Based Evolution",
        "abstract": "Discovering effective predictive signals, or ``alphas,'' from financial data with high dimensionality and extremely low signal-to-noise ratio remains a difficult open problem. Despite progress in deep learning, genetic programming, and, more recently, large language model (LLM)--based factor generation, existing approaches still explore only a narrow region of the vast alpha search space. Neural models tend to produce opaque and fragile patterns, while symbolic or formula-based methods often yield redundant or economically ungrounded expressions that generalize poorly. Although different in form, these paradigms share a key limitation: none can conduct broad, structured, and human-like exploration that balances logical consistency with creative leaps. To address this gap, we introduce the Cognitive Alpha Mining Framework (CogAlpha), which combines code-level alpha representation with LLM-driven reasoning and evolutionary search. Treating LLMs as adaptive cognitive agents, our framework iteratively refines, mutates, and recombines alpha candidates through multi-stage prompts and financial feedback. This synergistic design enables deeper thinking, richer structural diversity, and economically interpretable alpha discovery, while greatly expanding the effective search space. Experiments on A-share equities demonstrate that CogAlpha consistently discovers alphas with superior predictive accuracy, robustness, and generalization over existing methods. Our results highlight the promise of aligning evolutionary optimization with LLM-based reasoning for automated and explainable alpha discovery. All source code will be released.",
        "url": "http://arxiv.org/abs/2511.18850v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18850v1",
        "arxiv_id": "2511.18850v1",
        "authors": [
            "Fengyuan Liu",
            "Huang Yi",
            "Sichun Luo",
            "Yuqi Wang",
            "Yazheng Yang",
            "Xinye Li",
            "Zefa Hu",
            "Junlan Feng",
            "Qi Liu"
        ],
        "submitted": "2025-11-24 07:45:59",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on financial data analysis, large language model (LLM) driven code-based evolution, and alpha discovery in finance. The paper's use of LLMs and evolutionary search is not directly related to your areas of interest in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing."
    },
    {
        "title": "Large Language Models for the Summarization of Czech Documents: From History to the Present",
        "abstract": "Text summarization is the task of automatically condensing longer texts into shorter, coherent summaries while preserving the original meaning and key information. Although this task has been extensively studied in English and other high-resource languages, Czech summarization, particularly in the context of historical documents, remains underexplored. This is largely due to the inherent linguistic complexity of Czech and the lack of high-quality annotated datasets.\n  In this work, we address this gap by leveraging the capabilities of Large Language Models (LLMs), specifically Mistral and mT5, which have demonstrated strong performance across a wide range of natural language processing tasks and multilingual settings. In addition, we also propose a translation-based approach that first translates Czech texts into English, summarizes them using an English-language model, and then translates the summaries back into Czech. Our study makes the following main contributions: We demonstrate that LLMs achieve new state-of-the-art results on the SumeCzech dataset, a benchmark for modern Czech text summarization, showing the effectiveness of multilingual LLMs even for morphologically rich, medium-resource languages like Czech. We introduce a new dataset, Posel od Čerchova, designed for the summarization of historical Czech texts. This dataset is derived from digitized 19th-century publications and annotated for abstractive summarization. We provide initial baselines using modern LLMs to facilitate further research in this underrepresented area.\n  By combining cutting-edge models with both modern and historical Czech datasets, our work lays the foundation for further progress in Czech summarization and contributes valuable resources for future research in Czech historical document processing and low-resource summarization more broadly.",
        "url": "http://arxiv.org/abs/2511.18848v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18848v1",
        "arxiv_id": "2511.18848v1",
        "authors": [
            "Václav Tran",
            "Jakub Šmíd",
            "Ladislav Lenc",
            "Jean-Pierre Salmon",
            "Pavel Král"
        ],
        "submitted": "2025-11-24 07:40:31",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and text summarization, but it does not align with the user's primary focus on Information Retrieval (IR), query understanding, ranking models, and user behavior modeling. The paper's focus on Czech language and historical document summarization is also not directly relevant to the user's e-commerce background or interests in real-time relevance optimization."
    },
    {
        "title": "Multimodal Large Language Models with Adaptive Preference Optimization for Sequential Recommendation",
        "abstract": "Recent advances in Large Language Models (LLMs) have opened new avenues for sequential recommendation by enabling natural language reasoning over user behavior sequences. A common approach formulates recommendation as a language modeling task, where interaction histories are transformed into prompts and user preferences are learned via supervised fine-tuning. However, these methods operate solely in the textual modality and often miss users' fine-grained interests, especially when shaped by rich visual signals such as product images or movie posters. Multimodal Large Language Models (MLLMs) offer a promising alternative by aligning text and vision in a shared semantic space. A prevalent training paradigm applies Supervised Fine-Tuning (SFT) followed by Direct Preference Optimization (DPO) to model user preferences. Yet, two core challenges remain: 1) Imbalanced sample hardness, where random negative sampling causes overfitting on easy examples and under-training on hard ones; 2) Cross-modal semantic bias, where the fixed reference model in DPO prevents the policy model from correcting modality misalignments--especially over long sequences. To address these issues, we propose a Multimodal LLM framework that integrates Hardness-aware and Noise-regularized preference optimization for Recommendation (HaNoRec). Specifically, HaNoRec dynamically adjusts optimization weights based on both the estimated hardness of each training sample and the policy model's real-time responsiveness, prioritizing harder examples. It further introduces Gaussian-perturbed distribution optimization on output logits to enhance cross-modal semantic consistency and reduce modality bias inherited from the reference model.",
        "url": "http://arxiv.org/abs/2511.18740v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18740v1",
        "arxiv_id": "2511.18740v1",
        "authors": [
            "Yu Wang",
            "Yonghui Yang",
            "Le Wu",
            "Yi Zhang",
            "Richang Hong"
        ],
        "submitted": "2025-11-24 04:10:46",
        "source": "arxiv",
        "comment": "11 pages,6 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'user behavior' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores multimodal large language models for sequential recommendation, which is somewhat related to information retrieval and search technologies. However, the focus on sequential recommendation and multimodal learning is not a central match for the user's core research themes, which include query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "A Unified BERT-CNN-BiLSTM Framework for Simultaneous Headline Classification and Sentiment Analysis of Bangla News",
        "abstract": "In our daily lives, newspapers are an essential information source that impacts how the public talks about present-day issues. However, effectively navigating the vast amount of news content from different newspapers and online news portals can be challenging. Newspaper headlines with sentiment analysis tell us what the news is about (e.g., politics, sports) and how the news makes us feel (positive, negative, neutral). This helps us quickly understand the emotional tone of the news. This research presents a state-of-the-art approach to Bangla news headline classification combined with sentiment analysis applying Natural Language Processing (NLP) techniques, particularly the hybrid transfer learning model BERT-CNN-BiLSTM. We have explored a dataset called BAN-ABSA of 9014 news headlines, which is the first time that has been experimented with simultaneously in the headline and sentiment categorization in Bengali newspapers. Over this imbalanced dataset, we applied two experimental strategies: technique-1, where undersampling and oversampling are applied before splitting, and technique-2, where undersampling and oversampling are applied after splitting on the In technique-1 oversampling provided the strongest performance, both headline and sentiment, that is 78.57\\% and 73.43\\% respectively, while technique-2 delivered the highest result when trained directly on the original imbalanced dataset, both headline and sentiment, that is 81.37\\% and 64.46\\% respectively. The proposed model BERT-CNN-BiLSTM significantly outperforms all baseline models in classification tasks, and achieves new state-of-the-art results for Bangla news headline classification and sentiment analysis. These results demonstrate the importance of leveraging both the headline and sentiment datasets, and provide a strong baseline for Bangla text classification in low-resource.",
        "url": "http://arxiv.org/abs/2511.18618v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18618v1",
        "arxiv_id": "2511.18618v1",
        "authors": [
            "Mirza Raquib",
            "Munazer Montasir Akash",
            "Tawhid Ahmed",
            "Saydul Akbar Murad",
            "Farida Siddiqi Prity",
            "Mohammad Amzad Hossain",
            "Asif Pervez Polok",
            "Nick Rahimi"
        ],
        "submitted": "2025-11-23 21:22:56",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 0,
        "llm_reason": "LLM scoring failed."
    },
    {
        "title": "Scalable Parameter-Light Spectral Method for Clustering Short Text Embeddings with a Cohesion-Based Evaluation Metric",
        "abstract": "Clustering short text embeddings is a foundational task in natural language processing, yet remains challenging due to the need to specify the number of clusters in advance. We introduce a scalable spectral method that estimates the number of clusters directly from the structure of the Laplacian eigenspectrum, constructed using cosine similarities and guided by an adaptive sampling strategy. This sampling approach enables our estimator to efficiently scale to large datasets without sacrificing reliability. To support intrinsic evaluation of cluster quality without ground-truth labels, we propose the Cohesion Ratio, a simple and interpretable evaluation metric that quantifies how much intra-cluster similarity exceeds the global similarity background. It has an information-theoretic motivation inspired by mutual information, and in our experiments it correlates closely with extrinsic measures such as normalized mutual information and homogeneity. Extensive experiments on six short-text datasets and four modern embedding models show that standard algorithms like K-Means and HAC, when guided by our estimator, significantly outperform popular parameter-light methods such as HDBSCAN, OPTICS, and Leiden. These results demonstrate the practical value of our spectral estimator and Cohesion Ratio for unsupervised organization and evaluation of short text data. Implementation of our estimator of k and Cohesion Ratio, along with code for reproducing the experiments, is available at https://anonymous.4open.science/r/towards_clustering-0C2E.",
        "url": "http://arxiv.org/abs/2511.19350v2",
        "pdf_url": "https://arxiv.org/pdf/2511.19350v2",
        "arxiv_id": "2511.19350v2",
        "authors": [
            "Nikita Neveditsin",
            "Pawan Lingras",
            "Vijay Mago"
        ],
        "submitted": "2025-11-24 17:52:58",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and clustering, but it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's focus on clustering short text embeddings and developing a new evaluation metric is relevant to the user's background in data mining, but it does not align with their primary focus on information retrieval and real-time relevance optimization."
    },
    {
        "title": "Learning to Reason: Training LLMs with GPT-OSS or DeepSeek R1 Reasoning Traces",
        "abstract": "Test-time scaling, which leverages additional computation during inference to improve model accuracy, has enabled a new class of Large Language Models (LLMs) that are able to reason through complex problems by understanding the goal, turning this goal into a plan, working through intermediate steps, and checking their own work before answering . Frontier large language models with reasoning capabilities, such as DeepSeek-R1 and OpenAI's gpt-oss, follow the same procedure when solving complex problems by generating intermediate reasoning traces before giving the final answer. Today, these models are being increasingly used to generate reasoning traces that serve as high-quality supervised data for post-training of small and medium-sized language models to teach reasoning capabilities without requiring expensive human curation. In this work, we compare the performance of medium-sized LLMs on Math problems after post-training on two kinds of reasoning traces. We compare the impact of reasoning traces generated by DeepSeek-R1 and gpt-oss LLMs in terms of accuracy and inference efficiency.",
        "url": "http://arxiv.org/abs/2511.19333v1",
        "pdf_url": "https://arxiv.org/pdf/2511.19333v1",
        "arxiv_id": "2511.19333v1",
        "authors": [
            "Shaltiel Shmidman",
            "Asher Fredman",
            "Oleg Sudakov",
            "Meriem Bendris"
        ],
        "submitted": "2025-11-24 17:26:58",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the application of Large Language Models (LLMs) in reasoning tasks, specifically using reasoning traces generated by models like DeepSeek-R1 and gpt-oss. While it touches on aspects of model performance and efficiency, it does not directly relate to information retrieval, query understanding, or ranking models, which are core areas of your research interests. The connection to NLP is present, but the focus on LLMs and reasoning traces is not a central match for your primary research themes."
    },
    {
        "title": "AutoEnv: Automated Environments for Measuring Cross-Environment Agent Learning",
        "abstract": "Humans naturally adapt to diverse environments by learning underlying rules across worlds with different dynamics, observations, and reward structures. In contrast, existing agents typically demonstrate improvements via self-evolving within a single domain, implicitly assuming a fixed environment distribution. Cross-environment learning has remained largely unmeasured: there is no standard collection of controllable, heterogeneous environments, nor a unified way to represent how agents learn. We address these gaps in two steps. First, we propose AutoEnv, an automated framework that treats environments as factorizable distributions over transitions, observations, and rewards, enabling low-cost (4.12 USD on average) generation of heterogeneous worlds. Using AutoEnv, we construct AutoEnv-36, a dataset of 36 environments with 358 validated levels, on which seven language models achieve 12-49% normalized reward, demonstrating the challenge of AutoEnv-36. Second, we formalize agent learning as a component-centric process driven by three stages of Selection, Optimization, and Evaluation applied to an improvable agent component. Using this formulation, we design eight learning methods and evaluate them on AutoEnv-36. Empirically, the gain of any single learning method quickly decrease as the number of environments increases, revealing that fixed learning methods do not scale across heterogeneous environments. Environment-adaptive selection of learning methods substantially improves performance but exhibits diminishing returns as the method space expands. These results highlight both the necessity and the current limitations of agent learning for scalable cross-environment generalization, and position AutoEnv and AutoEnv-36 as a testbed for studying cross-environment agent learning. The code is avaiable at https://github.com/FoundationAgents/AutoEnv.",
        "url": "http://arxiv.org/abs/2511.19304v1",
        "pdf_url": "https://arxiv.org/pdf/2511.19304v1",
        "arxiv_id": "2511.19304v1",
        "authors": [
            "Jiayi Zhang",
            "Yiran Peng",
            "Fanqi Kong",
            "Yang Cheng",
            "Yifan Wu",
            "Zhaoyang Yu",
            "Jinyu Xiang",
            "Jianhao Ruan",
            "Jinlin Wang",
            "Maojia Song",
            "HongZhang Liu",
            "Xiangru Tang",
            "Bang Liu",
            "Chenglin Wu",
            "Yuyu Luo"
        ],
        "submitted": "2025-11-24 16:54:23",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 0,
        "llm_reason": "LLM scoring failed."
    },
    {
        "title": "Representational Stability of Truth in Large Language Models",
        "abstract": "Large language models (LLMs) are widely used for factual tasks such as \"What treats asthma?\" or \"What is the capital of Latvia?\". However, it remains unclear how stably LLMs encode distinctions between true, false, and neither-true-nor-false content in their internal probabilistic representations. We introduce representational stability as the robustness of an LLM's veracity representations to perturbations in the operational definition of truth. We assess representational stability by (i) training a linear probe on an LLM's activations to separate true from not-true statements and (ii) measuring how its learned decision boundary shifts under controlled label changes. Using activations from sixteen open-source models and three factual domains, we compare two types of neither statements. The first are fact-like assertions about entities we believe to be absent from any training data. We call these unfamiliar neither statements. The second are nonfactual claims drawn from well-known fictional contexts. We call these familiar neither statements. The unfamiliar statements induce the largest boundary shifts, producing up to $40\\%$ flipped truth judgements in fragile domains (such as word definitions), while familiar fictional statements remain more coherently clustered and yield smaller changes ($\\leq 8.2\\%$). These results suggest that representational stability stems more from epistemic familiarity than from linguistic form. More broadly, our approach provides a diagnostic for auditing and training LLMs to preserve coherent truth assignments under semantic uncertainty, rather than optimizing for output accuracy alone.",
        "url": "http://arxiv.org/abs/2511.19166v1",
        "pdf_url": "https://arxiv.org/pdf/2511.19166v1",
        "arxiv_id": "2511.19166v1",
        "authors": [
            "Samantha Dies",
            "Courtney Maynard",
            "Germans Savcisens",
            "Tina Eliassi-Rad"
        ],
        "submitted": "2025-11-24 14:28:50",
        "source": "arxiv",
        "comment": "25 pages, 24 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on the representational stability of truth in Large Language Models, which is a topic in Natural Language Processing (NLP). While it touches on the idea of semantic understanding, it does not directly relate to information retrieval, query understanding, or ranking models, which are core areas of your research interests."
    },
    {
        "title": "Emotion-Enhanced Multi-Task Learning with LLMs for Aspect Category Sentiment Analysis",
        "abstract": "Aspect category sentiment analysis (ACSA) has achieved remarkable progress with large language models (LLMs), yet existing approaches primarily emphasize sentiment polarity while overlooking the underlying emotional dimensions that shape sentiment expressions. This limitation hinders the model's ability to capture fine-grained affective signals toward specific aspect categories. To address this limitation, we introduce a novel emotion-enhanced multi-task ACSA framework that jointly learns sentiment polarity and category-specific emotions grounded in Ekman's six basic emotions. Leveraging the generative capabilities of LLMs, our approach enables the model to produce emotional descriptions for each aspect category, thereby enriching sentiment representations with affective expressions. Furthermore, to ensure the accuracy and consistency of the generated emotions, we introduce an emotion refinement mechanism based on the Valence-Arousal-Dominance (VAD) dimensional framework. Specifically, emotions predicted by the LLM are projected onto a VAD space, and those inconsistent with their corresponding VAD coordinates are re-annotated using a structured LLM-based refinement strategy. Experimental results demonstrate that our approach significantly outperforms strong baselines on all benchmark datasets. This underlines the effectiveness of integrating affective dimensions into ACSA.",
        "url": "http://arxiv.org/abs/2511.19122v1",
        "pdf_url": "https://arxiv.org/pdf/2511.19122v1",
        "arxiv_id": "2511.19122v1",
        "authors": [
            "Yaping Chai",
            "Haoran Xie",
            "Joe S. Qin"
        ],
        "submitted": "2025-11-24 13:52:42",
        "source": "arxiv",
        "comment": "8 pages, 4 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Natural Language Processing (NLP) and Information Retrieval (IR), particularly in the context of query understanding and ranking models. However, its focus on aspect category sentiment analysis and emotion-enhanced multi-task learning is not directly aligned with your primary interests in search technologies and user behavior modeling."
    },
    {
        "title": "A symbolic Perl algorithm for the unification of Nahuatl word spellings",
        "abstract": "In this paper, we describe a symbolic model for the automatic orthographic unification of Nawatl text documents. Our model is based on algorithms that we have previously used to analyze sentences in Nawatl, and on the corpus called $π$-yalli, consisting of texts in several Nawatl orthographies. Our automatic unification algorithm implements linguistic rules in symbolic regular expressions. We also present a manual evaluation protocol that we have proposed and implemented to assess the quality of the unified sentences generated by our algorithm, by testing in a sentence semantic task. We have obtained encouraging results from the evaluators for most of the desired features of our artificially unified sentences",
        "url": "http://arxiv.org/abs/2511.19118v1",
        "pdf_url": "https://arxiv.org/pdf/2511.19118v1",
        "arxiv_id": "2511.19118v1",
        "authors": [
            "Juan-José Guzmán-Landa",
            "Jesús Vázquez-Osorio",
            "Juan-Manuel Torres-Moreno",
            "Ligia Quintana Torres",
            "Miguel Figueroa-Saavedra",
            "Martha-Lorena Avendaño-Garrido",
            "Graham Ranger",
            "Patricia Velázquez-Morales",
            "Gerardo Eugenio Sierra Martínez"
        ],
        "submitted": "2025-11-24 13:49:13",
        "source": "arxiv",
        "comment": "MICAI 2025, LNAI 16221, pp. 141-154, 2026. 10 pages, 4 Figures, 8 Tables",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Nahuatl language processing and orthographic unification, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves symbolic regular expressions, it is more specific to linguistic analysis and does not address query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "GraphMind: Theorem Selection and Conclusion Generation Framework with Dynamic GNN for LLM Reasoning",
        "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in natural language understanding and generation, including multi-step reasoning such as mathematical proving. However, existing approaches often lack an explicit and dynamic mechanism to structurally represent and evolve intermediate reasoning states, which limits their ability to perform context-aware theorem selection and iterative conclusion generation. To address these challenges, we propose GraphMind, a novel dynamic graph-based framework that integrates the graph neural network (GNN) with LLMs to iteratively select theorems and generate intermediate conclusions for multi-step reasoning. Our method models the reasoning process as a heterogeneous evolving graph, where nodes represent conditions, theorems, and conclusions, while edges capture logical dependencies between nodes. By encoding the current reasoning state with GNN and leveraging semantic matching for theorem selection, our framework enables context-aware, interpretable, and structured reasoning in a closed-loop manner. Experiments on various question-answering (QA) datasets demonstrate that our proposed GraphMind method achieves consistent performance improvements and significantly outperforms existing baselines in multi-step reasoning, validating the effectiveness and generalizability of our approach.",
        "url": "http://arxiv.org/abs/2511.19078v1",
        "pdf_url": "https://arxiv.org/pdf/2511.19078v1",
        "arxiv_id": "2511.19078v1",
        "authors": [
            "Yutong Li",
            "Yitian Zhou",
            "Xudong Wang",
            "GuoChen",
            "Caiyan Qin"
        ],
        "submitted": "2025-11-24 13:18:21",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and deep semantic understanding, but it focuses on a specific application (mathematical proving) and a novel framework (GraphMind) that is not directly related to the user's core research themes in Information Retrieval (IR) and Search technologies."
    },
    {
        "title": "Heterogeneous Multi-treatment Uplift Modeling for Trade-off Optimization in Short-Video Recommendation",
        "abstract": "The rapid proliferation of short videos on social media platforms presents unique challenges and opportunities for recommendation systems. Users exhibit diverse preferences, and the responses resulting from different strategies often conflict with one another, potentially exhibiting inverse correlations between metrics such as watch time and video view counts. Existing uplift models face limitations in handling the heterogeneous multi-treatment scenarios of short-video recommendations, often failing to effectively capture both the synergistic and individual causal effects of different strategies. Furthermore, traditional fixed-weight approaches for balancing these responses lack personalization and can result in biased decision-making. To address these issues, we propose a novel Heterogeneous Multi-treatment Uplift Modeling (HMUM) framework for trade-off optimization in short-video recommendations. HMUM comprises an Offline Hybrid Uplift Modeling (HUM) module, which captures the synergistic and individual effects of multiple strategies, and an Online Dynamic Decision-Making (DDM) module, which estimates the value weights of different user responses in real-time for personalized decision-making. Evaluated on two public datasets, an industrial dataset, and through online A/B experiments on the Kuaishou platform, our model demonstrated superior offline performance and significant improvements in key metrics. It is now fully deployed on the platform, benefiting hundreds of millions of users.",
        "url": "http://arxiv.org/abs/2511.18997v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18997v1",
        "arxiv_id": "2511.18997v1",
        "authors": [
            "Chenhao Zhai",
            "Chang Meng",
            "Xueliang Wang",
            "Shuchang Liu",
            "Xiaolong Hu",
            "Shisong Tang",
            "Xiaoqiang Feng",
            "Xiu Li"
        ],
        "submitted": "2025-11-24 11:22:46",
        "source": "arxiv",
        "comment": "Accepted by KDD 2026",
        "score": 2,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'personalization' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval and Search technologies, particularly in the context of recommender systems. However, it focuses more on the recommendation of short videos rather than query understanding, ranking models, or user behavior modeling. The use of uplift modeling and dynamic decision-making is somewhat relevant to your interests in real-time relevance optimization, but the paper's primary focus is on the e-commerce domain, which is not a central match for your research."
    },
    {
        "title": "How Learning Rate Decay Wastes Your Best Data in Curriculum-Based LLM Pretraining",
        "abstract": "Due to the scarcity of high-quality data, large language models (LLMs) are often trained on mixtures of data with varying quality levels, even after sophisticated data curation. A natural approach to better leverage high-quality data is curriculum-based pretraining, where the model is trained on data sorted in ascending order of quality as determined by a quality metric. However, prior studies have reported limited improvements from such curriculum-based pretraining strategies. This work identifies a critical factor constraining these methods: the incompatibility between the ascending data quality order and the decaying learning rate (LR) schedule. We find that while curriculum-based training substantially outperforms random shuffling when using a constant LR, its advantage diminishes under standard LR decay schedules. Our experiments show this incompatibility can be mitigated by two simple strategies: (1) employing a more moderate LR decay schedule, where the final LR is only moderately smaller than the peak LR, and (2) replacing LR decay with model averaging, i.e., computing a weighted average of the final few checkpoints. By combining these strategies, we improve the average score on a suite of standard benchmarks by 1.64% over random shuffling, without additional data refinement. Validated on 1.5B-parameter models trained over 30B tokens with various data-quality metrics, our findings call for a re-evaluation of curriculum-based LLM pretraining and underscore the potential of co-designing data curricula with optimization methods.",
        "url": "http://arxiv.org/abs/2511.18903v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18903v1",
        "arxiv_id": "2511.18903v1",
        "authors": [
            "Kairong Luo",
            "Zhenbo Sun",
            "Haodong Wen",
            "Xinyu Shi",
            "Jiarui Cui",
            "Chenyi Dang",
            "Kaifeng Lyu",
            "Wenguang Chen"
        ],
        "submitted": "2025-11-24 09:03:49",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on large language models and pretraining strategies, which is somewhat related to your interests in NLP. However, it does not directly address your core research themes in Information Retrieval, query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Context-Aware Whisper for Arabic ASR Under Linguistic Varieties",
        "abstract": "Low-resource ASR remains a challenging problem, especially for languages like Arabic that exhibit wide dialectal variation and limited labeled data. We propose context-aware prompting strategies to adapt OpenAI's Whisper for Arabic speech recognition without retraining. Our methods include decoder prompting with first-pass transcriptions or retrieved utterances, and encoder prefixing using speech synthesized in the target speaker's voice. We introduce techniques such as prompt reordering, speaker-aware prefix synthesis, and modality-specific retrieval (lexical, semantic, acoustic) to improve transcription in real-world, zero-shot settings. Evaluated on nine Arabic linguistic conditions, our approach reduces WER by up to 22.3% on Modern Standard Arabic and 9.2% on dialectal speech, significantly mitigating hallucinations and speaker mismatch.",
        "url": "http://arxiv.org/abs/2511.18774v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18774v1",
        "arxiv_id": "2511.18774v1",
        "authors": [
            "Bashar Talafha",
            "Amin Abu Alhassan",
            "Muhammad Abdul-Mageed"
        ],
        "submitted": "2025-11-24 05:16:04",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on speech recognition for Arabic languages, which is outside your primary areas of Information Retrieval, Search technologies, and Natural Language Processing. Although it involves some form of language processing, the context and techniques used are not aligned with your core research themes."
    },
    {
        "title": "RhinoInsight: Improving Deep Research through Control Mechanisms for Model Behavior and Context",
        "abstract": "Large language models are evolving from single-turn responders into tool-using agents capable of sustained reasoning and decision-making for deep research. Prevailing systems adopt a linear pipeline of plan to search to write to a report, which suffers from error accumulation and context rot due to the lack of explicit control over both model behavior and context. We introduce RhinoInsight, a deep research framework that adds two control mechanisms to enhance robustness, traceability, and overall quality without parameter updates. First, a Verifiable Checklist module transforms user requirements into traceable and verifiable sub-goals, incorporates human or LLM critics for refinement, and compiles a hierarchical outline to anchor subsequent actions and prevent non-executable planning. Second, an Evidence Audit module structures search content, iteratively updates the outline, and prunes noisy context, while a critic ranks and binds high-quality evidence to drafted content to ensure verifiability and reduce hallucinations. Our experiments demonstrate that RhinoInsight achieves state-of-the-art performance on deep research tasks while remaining competitive on deep search tasks.",
        "url": "http://arxiv.org/abs/2511.18743v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18743v1",
        "arxiv_id": "2511.18743v1",
        "authors": [
            "Yu Lei",
            "Shuzheng Si",
            "Wei Wang",
            "Yifei Wu",
            "Gang Chen",
            "Fanchao Qi",
            "Maosong Sun"
        ],
        "submitted": "2025-11-24 04:12:41",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your interests in Natural Language Processing (NLP) and deep semantic understanding, but it primarily focuses on a deep research framework for large language models, which is not directly aligned with your core research themes in Information Retrieval and Search technologies. While it involves search and ranking, the context is more about research and decision-making rather than query understanding or user behavior modeling."
    },
    {
        "title": "A Recommender System Based on Binary Matrix Representations for Cognitive Disorders",
        "abstract": "Diagnosing cognitive (mental health) disorders is a delicate and complex task. Identifying the next most informative symptoms to assess, in order to distinguish between possible disorders, presents an additional challenge. This process requires comprehensive knowledge of diagnostic criteria and symptom overlap across disorders, making it difficult to navigate based on symptoms alone. This research aims to develop a recommender system for cognitive disorder diagnosis using binary matrix representations. The core algorithm utilizes a binary matrix of disorders and their symptom combinations. It filters through the rows and columns based on the patient's current symptoms to identify potential disorders and recommend the most informative next symptoms to examine. A prototype of the recommender system was implemented in Python. Using synthetic test and some real-life data, the system successfully identified plausible disorders from an initial symptom set and recommended further symptoms to refine the diagnosis. It also provided additional context on the symptom-disorder relationships. Although this is a prototype, the recommender system shows potential as a clinical support tool. A fully-developed application of this recommender system may assist mental health professionals in identifying relevant disorders more efficiently and guiding symptom-specific follow-up investigations to improve diagnostic accuracy.",
        "url": "http://arxiv.org/abs/2511.18645v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18645v1",
        "arxiv_id": "2511.18645v1",
        "authors": [
            "Raoul H. Kutil",
            "Georg Zimmermann",
            "Christian Borgelt"
        ],
        "submitted": "2025-11-23 23:04:21",
        "source": "arxiv",
        "comment": "19 pages, 1 figure, 3 tables",
        "score": 2,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on developing a recommender system for cognitive disorder diagnosis, which is somewhat related to the user's interests in recommender systems. However, it does not align with the user's primary focus on information retrieval, especially in areas requiring deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "SWAN: Sparse Winnowed Attention for Reduced Inference Memory via Decompression-Free KV-Cache Compression",
        "abstract": "Large Language Models (LLMs) face a significant bottleneck during autoregressive inference due to the massive memory footprint of the Key-Value (KV) cache. Existing compression techniques like token eviction, quantization, or other low-rank methods often risk information loss, have fixed limits, or introduce significant computational overhead from explicit decompression steps. In this work, we introduce SWAN, a novel, fine-tuning-free framework that eliminates this overhead. Our method uses an offline orthogonal matrix to rotate and prune the KV-cache, which is then used directly in the attention computation without any reconstruction. Our extensive experiments demonstrate that SWAN, augmented with a small dense buffer, offers a robust trade-off, maintaining performance close to the uncompressed baseline even at aggressive 50-60% memory savings per-token on KV-cache. A key advantage is its runtime-tunable compression level, allowing operators to dynamically adjust the memory footprint, a flexibility absent in methods requiring fixed offline configurations. This combination of a decompression-free design, high performance under compression, and adaptability makes SWAN a practical and efficient solution for serving LLMs with long contexts.",
        "url": "http://arxiv.org/abs/2511.18936v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18936v1",
        "arxiv_id": "2511.18936v1",
        "authors": [
            "Santhosh G S",
            "Saurav Prakash",
            "Balaraman Ravindran"
        ],
        "submitted": "2025-11-24 09:41:24",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on optimizing the memory footprint of Large Language Models (LLMs) using a novel compression technique called SWAN. While it involves attention mechanisms, which are related to ranking models in IR, the context is primarily NLP and not directly relevant to the user's core research themes in IR and Search technologies."
    },
    {
        "title": "A Reproducible Framework for Neural Topic Modeling in Focus Group Analysis",
        "abstract": "Focus group discussions generate rich qualitative data but their analysis traditionally relies on labor-intensive manual coding that limits scalability and reproducibility. We present a rigorous, reproducible computational framework for applying neural topic modeling to focus group transcripts, addressing fundamental methodological challenges: hyperparameter sensitivity, model stability, and validation of interpretability. Using BERTopic applied to ten focus groups exploring HPV vaccine perceptions in Tunisia (1,076 utterances), we conducted systematic evaluation across 27 hyperparameter configurations, assessed stability through bootstrap resampling with 30 replicates per configuration, and validated interpretability through formal human evaluation by three domain experts. Our analysis demonstrates substantial sensitivity to hyperparameter choices and reveals that metric selection for stability assessment must align with analytical goals. A hierarchical merging strategy (extracting fine-grained topics for stability then consolidating for interpretability) effectively navigates the stability-coherence tradeoff, achieving coherence of 0.558 compared to 0.539 for direct extraction. Human validation confirmed topic quality with very good inter-rater reliability (ICC = 0.79, weighted Cohen's kappa = 0.578). Our framework provides practical guidelines that researchers can adapt to their own qualitative research contexts. All code, data processing scripts, and evaluation protocols are publicly available to support reproduction and extension of this work.",
        "url": "http://arxiv.org/abs/2511.18843v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18843v1",
        "arxiv_id": "2511.18843v1",
        "authors": [
            "Heger Arfaoui",
            "Mohammed Iheb Hergli",
            "Beya Benzina",
            "Slimane BenMiled"
        ],
        "submitted": "2025-11-24 07:30:15",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on neural topic modeling for focus group analysis, which is outside the primary scope of Information Retrieval and Search technologies. While it involves NLP, the application domain and methodology are not directly related to the user's core research themes."
    },
    {
        "title": "Robust Multimodal Sentiment Analysis with Distribution-Based Feature Recovery and Fusion",
        "abstract": "As posts on social media increase rapidly, analyzing the sentiments embedded in image-text pairs has become a popular research topic in recent years. Although existing works achieve impressive accomplishments in simultaneously harnessing image and text information, they lack the considerations of possible low-quality and missing modalities. In real-world applications, these issues might frequently occur, leading to urgent needs for models capable of predicting sentiment robustly. Therefore, we propose a Distribution-based feature Recovery and Fusion (DRF) method for robust multimodal sentiment analysis of image-text pairs. Specifically, we maintain a feature queue for each modality to approximate their feature distributions, through which we can simultaneously handle low-quality and missing modalities in a unified framework. For low-quality modalities, we reduce their contributions to the fusion by quantitatively estimating modality qualities based on the distributions. For missing modalities, we build inter-modal mapping relationships supervised by samples and distributions, thereby recovering the missing modalities from available ones. In experiments, two disruption strategies that corrupt and discard some modalities in samples are adopted to mimic the low-quality and missing modalities in various real-world scenarios. Through comprehensive experiments on three publicly available image-text datasets, we demonstrate the universal improvements of DRF compared to SOTA methods under both two strategies, validating its effectiveness in robust multimodal sentiment analysis.",
        "url": "http://arxiv.org/abs/2511.18751v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18751v1",
        "arxiv_id": "2511.18751v1",
        "authors": [
            "Daiqing Wu",
            "Dongbao Yang",
            "Can Ma",
            "Yu Zhou"
        ],
        "submitted": "2025-11-24 04:24:33",
        "source": "arxiv",
        "comment": "Accepted by ACM MM 2024",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper focuses on multimodal sentiment analysis, which is related to query understanding and deep semantic understanding in Information Retrieval. However, it does not directly address ranking models, user behavior modeling, or real-time relevance optimization, which are core aspects of your research interests."
    },
    {
        "title": "When and What to Recommend: Joint Modeling of Timing and Content for Active Sequential Recommendation",
        "abstract": "Sequential recommendation models user preferences to predict the next target item. Most existing work is passive, where the system responds only when users open the application, missing chances after closure. We investigate active recommendation, which predicts the next interaction time and actively delivers items. Two challenges: accurately estimating the Time of Interest (ToI) and generating Item of Interest (IoI) conditioned on the predicted ToI. We propose PASRec, a diffusion-based framework that aligns ToI and IoI via a joint objective. Experiments on five benchmarks show superiority over eight state-of-the-art baselines under leave-one-out and temporal splits.",
        "url": "http://arxiv.org/abs/2511.18717v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18717v1",
        "arxiv_id": "2511.18717v1",
        "authors": [
            "Jin Chai",
            "Xiaoxiao Ma",
            "Jian Yang",
            "Jia Wu"
        ],
        "submitted": "2025-11-24 03:16:10",
        "source": "arxiv",
        "comment": "10 pages, 5 figures. Submitted to arXiv",
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on sequential recommendation, which is somewhat related to your interests in Information Retrieval and Search technologies. However, it primarily deals with recommender systems, which is not your primary focus. The paper's emphasis on sequential modeling and active recommendation does not strongly align with your expertise in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "SCoTER: Structured Chain-of-Thought Transfer for Enhanced Recommendation",
        "abstract": "Harnessing the reasoning power of Large Language Models (LLMs) for recommender systems is hindered by two fundamental challenges. First, current approaches lack a mechanism for automated, data-driven discovery of effective reasoning patterns, relying instead on brittle manual templates or unstable zero-shot prompting. Second, they employ structure-collapsing integration: direct prompting incurs prohibitive online inference costs, while feature extraction collapses reasoning chains into single vectors, discarding stepwise logic. To address these challenges, we propose SCoTER (Structured Chain-of-Thought Transfer for Enhanced Recommendation), a unified framework that treats pattern discovery and structure-aware transfer as a jointly optimized problem. Specifically, SCoTER operationalizes this through two synergistic components: a GVM pipeline for automated pattern discovery and a structure-preserving integration architecture that transfers stepwise logic to efficient models. Formally, we provide information-theoretic justification proving that structure-preserving transfer achieves tighter performance bounds than structure-agnostic alternatives. Empirically, experiments on four benchmarks demonstrate improvements of 3.75\\%-11.59\\% over a strong TIGER backbone. Moreover, in production deployment on the Tencent Advertising Platform, SCoTER achieved a 2.14\\% lift in Gross Merchandise Value (GMV) while eliminating online LLM inference costs. Overall, SCoTER establishes a principled and production-validated blueprint for transferring structured LLM reasoning to large-scale recommender systems.",
        "url": "http://arxiv.org/abs/2511.19514v1",
        "pdf_url": "https://arxiv.org/pdf/2511.19514v1",
        "arxiv_id": "2511.19514v1",
        "authors": [
            "Yang Wu",
            "Qian Li",
            "Yuling Xiong",
            "Hongbo Tang",
            "Xun Liu",
            "Jun Zhang",
            "Huan Yu",
            "Jie Jiang",
            "Hailong Shi"
        ],
        "submitted": "2025-11-24 03:00:04",
        "source": "arxiv",
        "comment": "12 pages,4 figures",
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper focuses on recommender systems and the application of Large Language Models (LLMs) for enhanced recommendations. While it touches on the idea of structured reasoning, which is related to query understanding and ranking models, the primary focus is on recommender systems rather than information retrieval. The paper's emphasis on structure-preserving transfer and its application to recommender systems makes it somewhat relevant to the user's interests, but not a central match."
    },
    {
        "title": "Majority of the Bests: Improving Best-of-N via Bootstrapping",
        "abstract": "Sampling multiple outputs from a Large Language Model (LLM) and selecting the most frequent (Self-consistency) or highest-scoring (Best-of-N) candidate is a popular approach to achieve higher accuracy in tasks with discrete final answers. Best-of-N (BoN) selects the output with the highest reward, and with perfect rewards, it often achieves near-perfect accuracy. With imperfect rewards from reward models, however, BoN fails to reliably find the correct answer and its performance degrades drastically. We consider the distribution of BoN's outputs and highlight that, although the correct answer does not usually have a probability close to one under imperfect rewards, it is often the most likely outcome. This suggests that the mode of this distribution can be more reliably correct than a sample from it. Based on this idea, we propose Majority-of-the-Bests (MoB), a novel selection mechanism that estimates the output distribution of BoN via bootstrapping and selects its mode. Experimental results across five benchmarks, three different base LLMs, and two reward models demonstrate consistent improvements over BoN in 25 out of 30 setups. We also provide theoretical results for the consistency of the bootstrapping. MoB serves as a simple, yet strong alternative to BoN and self-consistency, and more broadly, motivates further research in more nuanced selection mechanisms.",
        "url": "http://arxiv.org/abs/2511.18630v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18630v1",
        "arxiv_id": "2511.18630v1",
        "authors": [
            "Amin Rakhsha",
            "Kanika Madan",
            "Tianyu Zhang",
            "Amir-massoud Farahmand",
            "Amir Khasahmadi"
        ],
        "submitted": "2025-11-23 22:05:08",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to information retrieval and search technologies, but its focus on Large Language Models and selection mechanisms for discrete final answers is not directly aligned with the user's core research themes. The paper's use of bootstrapping and theoretical results may be of interest to the user's background in data mining, but it does not strongly relate to query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "OpenGloss: A Synthetic Encyclopedic Dictionary and Semantic Knowledge Graph",
        "abstract": "We present OpenGloss, a synthetic encyclopedic dictionary and semantic knowledge graph for English that integrates lexicographic definitions, encyclopedic context, etymological histories, and semantic relationships in a unified resource. OpenGloss contains 537K senses across 150K lexemes, on par with WordNet 3.1 and Open English WordNet, while providing more than four times as many sense definitions. These lexemes include 9.1M semantic edges, 1M usage examples, 3M collocations, and 60M words of encyclopedic content.\n  Generated through a multi-agent procedural generation pipeline with schema-validated LLM outputs and automated quality assurance, the entire resource was produced in under one week for under $1,000. This demonstrates that structured generation can create comprehensive lexical resources at cost and time scales impractical for manual curation, enabling rapid iteration as foundation models improve. The resource addresses gaps in pedagogical applications by providing integrated content -- definitions, examples, collocations, encyclopedias, etymology -- that supports both vocabulary learning and natural language processing tasks.\n  As a synthetically generated resource, OpenGloss reflects both the capabilities and limitations of current foundation models. The dataset is publicly available on Hugging Face under CC-BY 4.0, enabling researchers and educators to build upon and adapt this resource.",
        "url": "http://arxiv.org/abs/2511.18622v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18622v1",
        "arxiv_id": "2511.18622v1",
        "authors": [
            "Michael J. Bommarito"
        ],
        "submitted": "2025-11-23 21:33:53",
        "source": "arxiv",
        "comment": "30 pages, 5 figures, 8 tables. Dataset available at https://huggingface.co/datasets/mjbommar/opengloss-dictionary",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper presents a synthetic encyclopedic dictionary and semantic knowledge graph, which aligns with your interests in Natural Language Processing (NLP) and related topics. However, the focus on lexical resources and procedural generation may not be directly related to your primary research themes in Information Retrieval (IR) and query understanding. Nevertheless, the resource could be useful for tasks like vocabulary learning and natural language processing."
    },
    {
        "title": "Prompt Optimization as a State-Space Search Problem",
        "abstract": "Language Models are extremely susceptible to performance collapse with even small changes to input prompt strings. Libraries such as DSpy (from Stanford NLP) avoid this problem through demonstration-based prompt optimisation. Inspired by this, I propose an alternative approach that treats prompt optimisation as a classical state-space search problem. I model the prompt space as a graph where nodes represent prompt states and edges correspond to deliberate transformations such as shortening, adding examples, or re- ordering content. Using beam search and random walk algorithms, I systematically explore this space, evaluating candidates on development sets and pruning unpromising branches. Across five NLP tasks (sentiment classification, question answering, summarisation, reason- ing, and natural language inference), I find that even shallow search configurations (beam width=2, depth=2) improve upon seed prompts on development sets. For instance, beam search achieves development accuracy gains from 0.40 to 0.80 on reasoning tasks, though test set improvements are more modest (0.20 to 0.50), indicating overfitting to the develop- ment heuristic. Analysis of successful optimisation paths reveals that transformations that make prompts concise appear most frequently, while verbosity operators are never selected. My results validate prompt optimization as a search problem and suggest that with greater computational resources and improved evaluation metrics, deeper exploration could yield more robust prompts that generalize beyond development sets. Code and implementation are available at [https://github.com/MaanasTaneja/PromptOptimiser].",
        "url": "http://arxiv.org/abs/2511.18619v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18619v1",
        "arxiv_id": "2511.18619v1",
        "authors": [
            "Maanas Taneja"
        ],
        "submitted": "2025-11-23 21:24:13",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper explores prompt optimization as a state-space search problem, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on NLP tasks and prompt optimization is not a central match to the user's core research themes, but still shows some relevance to the broader field of search technologies and user behavior modeling."
    }
]