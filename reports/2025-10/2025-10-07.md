# Daily Papers Report - 2025-10-07

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Topic-Specific Classifiers are Better Relevance Judges than Prompted LLMs

- **LLM Score**: 8
- **Keyword Score**: 14
- **Authors**: Lukas Gienapp, Martin Potthast, Harrisen Scells, Eugene Yang
- **URL**: <http://arxiv.org/abs/2510.04633v1>
- **Submitted**: 2025-10-06 09:38:13
- **Comment**: 15 pages, 3 figures, 2 tables
- **Topic Keywords**: information retrieval, ranking, relevance, retrieval, rank, acl
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The paper proposes a novel approach to tackle the unjudged document problem, which is a key challenge in IR evaluation. While it does not specifically focus on user behavior modeling or click models, its contribution to the field of IR evaluation is significant and aligns with your broader research interests in IR and NLP.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Information Retrieval
- **Aim**: To address the "unjudged document problem" in information retrieval by proposing a novel solution called judge adapters.
- **Rationale**: Existing evaluation methods suffer from outdated test collections and bias introduced by using LLMs as judges due to potential social biases in their training data.
- **Ground**: Human judgments on a small set of documents (as few as 128) per topic.
- **Experiment**: Three key experiments: (1) Fine-tuning judge models on 47 topics from the TREC Robust04 dataset, (2) Training judge models with limited annotations (12.5% relevance) and evaluating them on multiple datasets, (3) Comparing judge adapters to LLM-as-a-judge approaches on the DL20 dataset.
- **Takeaway**: Judge adapters outperform baselines and LLM-as-a-judge approaches, especially with small training datasets. They are computationally efficient and offer a robust solution for handling the unjudged document problem.

#### Abstract
> The unjudged document problem, where pooled test collections have incomplete
relevance judgments for evaluating new retrieval systems, is a key obstacle to
the reusability of test collections in information retrieval. While the de
facto standard to deal with the problem is to treat unjudged documents as
non-relevant, many alternatives have been proposed, including the use of large
language models (LLMs) as a relevance judge (LLM-as-a-judge). However, this has
been criticized as circular, since the same LLM can be used as a judge and as a
ranker at the same time. We propose to train topic-specific relevance
classifiers instead: By finetuning monoT5 with independent LoRA weight
adaptation on the judgments of a single assessor for a single topic's pool, we
align it to that assessor's notion of relevance for the topic. The system
rankings obtained through our classifier's relevance judgments achieve a
Spearmans' $\rho$ correlation of $>0.95$ with ground truth system rankings. As
little as 128 initial human judgments per topic suffice to improve the
comparability of models, compared to treating unjudged documents as
non-relevant, while achieving more reliability than existing LLM-as-a-judge
approaches. Topic-specific relevance classifiers thus are a lightweight and
straightforward way to tackle the unjudged document problem, while maintaining
human judgments as the gold standard for retrieval evaluation. Code, models,
and data are made openly available.

---

### 2. Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization

- **LLM Score**: 8
- **Keyword Score**: 11
- **Authors**: Omri Uzan, Asaf Yehudai, Roi pony, Eyal Shnarch, Ariel Gera
- **URL**: <http://arxiv.org/abs/2510.05038v1>
- **Submitted**: 2025-10-06 17:12:53
- **Topic Keywords**: retriever, query, retrieval, rank, acl
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of multimodal retrieval and query understanding. The use of hybrid retrieval and test-time optimization methods aligns with your focus on deep semantic understanding and real-time relevance optimization. However, the paper's specific focus on visual document retrieval and multimodal encoders is somewhat narrower than your broader interests in IR and NLP.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Multimodal Document Retrieval
- **Aim**: To enhance the performance of vision-centric document retrieval models while addressing scalability and modality gap challenges.
- **Rationale**: Dense semantic representations and hybrid retrieval techniques offer promising avenues for improving multimodal document retrieval.
- **Ground**: Existing research on dense semantic representations in information retrieval, including DPR, Late-Interaction Models, Contrastive Training, Cross-Encoder Re-Rankers, and Visual Document Retrieval.
- **Experiment**: A comprehensive experimental setup on the ViDoRe 1 benchmark, comparing various retrieval methods, including GQR, using Jina-Embeddings-v4 and different reranking techniques.
- **Takeaway**: Hybrid retrieval methods, particularly GQR, significantly improve performance, achieve comparable results to larger models with faster latency and lower memory requirements, and show promise in question answering systems.

#### Abstract
> Multimodal encoders have pushed the boundaries of visual document retrieval,
matching textual query tokens directly to image patches and achieving
state-of-the-art performance on public benchmarks. Recent models relying on
this paradigm have massively scaled the sizes of their query and document
representations, presenting obstacles to deployment and scalability in
real-world pipelines. Furthermore, purely vision-centric approaches may be
constrained by the inherent modality gap still exhibited by modern
vision-language models. In this work, we connect these challenges to the
paradigm of hybrid retrieval, investigating whether a lightweight dense text
retriever can enhance a stronger vision-centric model. Existing hybrid methods,
which rely on coarse-grained fusion of ranks or scores, fail to exploit the
rich interactions within each model's representation space. To address this, we
introduce Guided Query Refinement (GQR), a novel test-time optimization method
that refines a primary retriever's query embedding using guidance from a
complementary retriever's scores. Through extensive experiments on visual
document retrieval benchmarks, we demonstrate that GQR allows vision-centric
models to match the performance of models with significantly larger
representations, while being up to 14x faster and requiring 54x less memory.
Our findings show that GQR effectively pushes the Pareto frontier for
performance and efficiency in multimodal retrieval. We release our code at
https://github.com/IBM/test-time-hybrid-retrieval

---

### 3. ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced re-ranking retriever

- **LLM Score**: 8
- **Keyword Score**: 11
- **Authors**: Eduardo Mart√≠nez Rivera, Filippo Menolascina
- **URL**: <http://arxiv.org/abs/2510.04757v1>
- **Submitted**: 2025-10-06 12:34:55
- **Topic Keywords**: retriever, ranking, rag, retrieval, rank
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of query understanding and ranking models. The use of ColBERT and ModernBERT for re-ranking and initial candidate retrieval aligns with your focus on deep semantic understanding and real-time relevance optimization. However, the specific domain of biomedical RAG is somewhat outside your primary focus on e-commerce and general IR applications.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Biomedical Question Answering (RAG)
- **Aim**: Develop a novel two-stage retrieval architecture for efficient and accurate RAG.
- **Rationale**: Balancing efficiency and accuracy is crucial for RAG in healthcare.
- **Ground**: PubMedQA dataset for fine-tuning the retrieval module.
- **Experiment**: Evaluation on the MIRAGE benchmark, comparing to baselines like MedCPT and exploring various negative sampling strategies and similarity functions.
- **Takeaway**: The two-stage architecture with ColBERT re-ranking achieves state-of-the-art accuracy and improved recall while maintaining efficiency. Future work focuses on optimizing retrieval parameters and addressing limitations in handling complex queries.

#### Abstract
> Retrieval-Augmented Generation (RAG) is a powerful technique for enriching
Large Language Models (LLMs) with external knowledge, allowing for factually
grounded responses, a critical requirement in high-stakes domains such as
healthcare. However, the efficacy of RAG systems is fundamentally restricted by
the performance of their retrieval module, since irrelevant or semantically
misaligned documents directly compromise the accuracy of the final generated
response. General-purpose dense retrievers can struggle with the nuanced
language of specialised domains, while the high accuracy of in-domain models is
often achieved at prohibitive computational costs. In this work, we aim to
address this trade-off by developing and evaluating a two-stage retrieval
architecture that combines a lightweight ModernBERT bidirectional encoder for
efficient initial candidate retrieval with a ColBERTv2 late-interaction model
for fine-grained re-ranking. We conduct comprehensive evaluations of our
retriever module performance and RAG system performance in the biomedical
context, fine-tuning the IR module using 10k question-passage pairs from
PubMedQA. Our analysis of the retriever module confirmed the positive impact of
the ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points
compared to its retrieve-only counterpart. When integrated into the biomedical
RAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on
the five tasks of the MIRAGE question-answering benchmark, outperforming strong
baselines such as MedCPT (0.4436). Our ablation studies reveal that this
performance is critically dependent on a joint fine-tuning process that aligns
the retriever and re-ranker; otherwise, the re-ranker might degrade the
performance.

---

### 4. Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards

- **LLM Score**: 8
- **Keyword Score**: 10
- **Authors**: Faisal Hamman, Chenyang Zhu, Anoop Kumar, Xujun Peng, Sanghamitra Dutta, Daben Liu, Alfy Samuel
- **URL**: <http://arxiv.org/abs/2510.04392v1>
- **Submitted**: 2025-10-05 23:14:13
- **Comment**: Accepted at NeurIPS 2025 Workshop on Reliable ML from Unreliable Data
- **Topic Keywords**: retriever, queries, rag, retrieval
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of Retrieval-Augmented Systems (RAG). The focus on improving consistency and introducing a principled evaluation framework aligns with your interest in query understanding and ranking models. However, the specific domain of RAG systems is not directly related to your e-commerce background, which is why the score is not a perfect 10.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Information inconsistency in Retrieval-Augmented Generation (RAG) systems
- **Aim**: To enhance consistency in RAG systems at the retriever, generator (LLM), and end-to-end levels
- **Rationale**: Information inconsistency can be detrimental in high-stakes domains where accurate and reliable information is crucial
- **Ground**: Con-RAG defines specific metrics for each level of consistency: retriever consistency (Jaccard similarity), generator consistency (LLM output agreement), and end-to-end RAG consistency (BLEU, ROUGE, BERTScore, entailment scores)
- **Experiment**: Con-RAG is evaluated on five QA benchmarks (TriviaQA, HotpotQA, MuSiQue, 2Wiki, ELI5) using various metrics (Exact Match, F1 score, Relaxed Match, lexical and LLM-judge consistency)
- **Takeaway**: Con-RAG significantly outperforms strong baselines in both consistency and accuracy, highlighting the effectiveness of the proposed PS-GRPO approach.  Future work should explore alternative consistency metrics and joint training of retriever and generator.

#### Abstract
> RAG systems are increasingly deployed in high-stakes domains where users
expect outputs to be consistent across semantically equivalent queries.
However, existing systems often exhibit significant inconsistencies due to
variability in both the retriever and generator (LLM), undermining trust and
reliability. In this work, we focus on information consistency, i.e., the
requirement that outputs convey the same core content across semantically
equivalent inputs. We introduce a principled evaluation framework that
decomposes RAG consistency into retriever-level, generator-level, and
end-to-end components, helping identify inconsistency sources. To improve
consistency, we propose Paraphrased Set Group Relative Policy Optimization
(PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased
set to assign group similarity rewards. We leverage PS-GRPO to achieve
Information Consistent RAG (Con-RAG), training the generator to produce
consistent outputs across paraphrased queries and remain robust to
retrieval-induced variability. Because exact reward computation over paraphrase
sets is computationally expensive, we also introduce a scalable approximation
method that retains effectiveness while enabling efficient, large-scale
training. Empirical evaluations across short-form, multi-hop, and long-form QA
benchmarks demonstrate that Con-RAG significantly improves both consistency and
accuracy over strong baselines, even in the absence of explicit ground-truth
supervision. Our work provides practical solutions for evaluating and building
reliable RAG systems for safety-critical deployments.

---

### 5. Large Language Models Preserve Semantic Isotopies in Story Continuations

- **LLM Score**: 6
- **Keyword Score**: 5
- **Authors**: Marc Cavazza
- **URL**: <http://arxiv.org/abs/2510.04400v1>
- **Submitted**: 2025-10-06 00:03:12
- **Topic Keywords**: relevance, rag
- **Reason**: This paper explores the connection between textual semantics and Large Language Models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on semantic isotopies and story continuation is not directly aligned with the user's primary research themes, but it does touch on deep semantic understanding, which is of interest.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Semantic Isotopy Preservation in Large Language Models
- **Aim**: To investigate the ability of LLMs to preserve semantic isotopy, a linguistic concept related to the recurring co-occurrence of semantically related words in text.
- **Rationale**: Understanding how LLMs handle semantic relationships is crucial for evaluating their ability to generate coherent and interpretable text.
- **Ground**: The study utilizes 10,000 story continuations from the ROC-Stories dataset generated by five diverse LLMs.
- **Experiment**: Isotopy preservation is analyzed through a prompt-based framework inspired by Greimas and Rastier's theories, examining coverage balance and structural properties.
- **Takeaway**: LLMs effectively preserve isotopy, demonstrating high coverage balance and stable structural properties. However, vocabulary divergence and the reliance on inferential relations highlight the need for further research into LLM training mechanisms and methods for capturing complex semantic relationships.

#### Abstract
> In this work, we explore the relevance of textual semantics to Large Language
Models (LLMs), extending previous insights into the connection between
distributional semantics and structural semantics. We investigate whether
LLM-generated texts preserve semantic isotopies. We design a story continuation
experiment using 10,000 ROCStories prompts completed by five LLMs. We first
validate GPT-4o's ability to extract isotopies from a linguistic benchmark,
then apply it to the generated stories. We then analyze structural (coverage,
density, spread) and semantic properties of isotopies to assess how they are
affected by completion. Results show that LLM completion within a given token
horizon preserves semantic isotopies across multiple properties.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Evaluating Keyframe Layouts for Visual Known-Item Search in Homogeneous Collections

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Bastian J√§ckl, Ji≈ô√≠ Kruchina, Lucas Joos, Daniel A. Keim, Ladislav Pe≈°ka, Jakub Lokoƒç
- **URL**: <http://arxiv.org/abs/2510.04396v1>
- **Submitted**: 2025-10-05 23:30:33
- **Comment**: 28 Pages, 17 Figures
- **Topic Keywords**: queries, ranking, retrieval, rank, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, specifically in the context of visual search and ranking models. However, it focuses on a specific domain (video retrieval) and does not directly address your core areas of interest in query understanding, user behavior modeling, or deep semantic understanding.

#### Abstract
> Multimodal deep-learning models power interactive video retrieval by ranking
keyframes in response to textual queries. Despite these advances, users must
still browse ranked candidates manually to locate a target. Keyframe
arrangement within the search grid highly affects browsing effectiveness and
user efficiency, yet remains underexplored. We report a study with 49
participants evaluating seven keyframe layouts for the Visual Known-Item Search
task. Beyond efficiency and accuracy, we relate browsing phenomena, such as
overlooks, to layout characteristics. Our results show that a video-grouped
layout is the most efficient, while a four-column, rank-preserving grid
achieves the highest accuracy. Sorted grids reveal potentials and trade-offs,
enabling rapid scanning of uninteresting regions but down-ranking relevant
targets to less prominent positions, delaying first arrival times and
increasing overlooks.
  These findings motivate hybrid designs that preserve positions of top-ranked
items while sorting or grouping the remainder, and offer guidance for searching
in grids beyond video retrieval.

### 7. Fine-grained auxiliary learning for real-world product recommendation

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Mario Almagro, Diego Ortego, David Jimenez
- **URL**: <http://arxiv.org/abs/2510.04551v1>
- **Submitted**: 2025-10-06 07:34:06
- **Comment**: SEPLN 2025
- **Topic Keywords**: query, rag, recommend, rank
- **Reason**: The paper focuses on product recommendation, which is somewhat related to the user's interests in Information Retrieval and Search technologies. However, the emphasis on recommender systems and product corpora is not the primary focus of the user's research, and the paper does not explicitly mention query understanding, ranking models, or user behavior modeling.

#### Abstract
> Product recommendation is the task of recovering the closest items to a given
query within a large product corpora. Generally, one can determine if
top-ranked products are related to the query by applying a similarity
threshold; exceeding it deems the product relevant, otherwise manual revision
is required. Despite being a well-known problem, the integration of these
models in real-world systems is often overlooked. In particular, production
systems have strong coverage requirements, i.e., a high proportion of
recommendations must be automated. In this paper we propose ALC , an Auxiliary
Learning strategy that boosts Coverage through learning fine-grained
embeddings. Concretely, we introduce two training objectives that leverage the
hardest negatives in the batch to build discriminative training signals between
positives and negatives. We validate ALC using three extreme multi-label
classification approaches in two product recommendation datasets;
LF-AmazonTitles-131K and Tech and Durables (proprietary), demonstrating
state-of-the-art coverage rates when combined with a recent
threshold-consistent margin loss.

### 8. GRACE: Generative Representation Learning via Contrastive Policy Optimization

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Jiashuo Sun, Shixuan Liu, Zhaochen Su, Xianrui Zhong, Pengcheng Jiang, Bowen Jin, Peiran Li, Weijia Shi, Jiawei Han
- **URL**: <http://arxiv.org/abs/2510.04506v1>
- **Submitted**: 2025-10-06 05:46:56
- **Comment**: 23 pages, 7 figures, 7 tables
- **Topic Keywords**: query, rag
- **Reason**: The paper introduces a novel framework for training Large Language Models (LLMs) with a focus on generative representation learning and contrastive policy optimization. While it touches on the topic of semantic understanding, it is primarily focused on LLMs and not directly related to information retrieval or search technologies. The use of rationales and interpretable agents is somewhat related to user behavior modeling, but the connection is not strong enough to warrant a higher score.

#### Abstract
> Prevailing methods for training Large Language Models (LLMs) as text encoders
rely on contrastive losses that treat the model as a black box function,
discarding its generative and reasoning capabilities in favor of static
embeddings. We introduce GRACE (Generative Representation Learning via
Contrastive Policy Optimization), a novel framework that reimagines contrastive
signals not as losses to be minimized, but as rewards that guide a generative
policy. In GRACE, the LLM acts as a policy that produces explicit,
human-interpretable rationales--structured natural language explanations of its
semantic understanding. These rationales are then encoded into high-quality
embeddings via mean pooling. Using policy gradient optimization, we train the
model with a multi-component reward function that maximizes similarity between
query positive pairs and minimizes similarity with negatives. This transforms
the LLM from an opaque encoder into an interpretable agent whose reasoning
process is transparent and inspectable. On MTEB benchmark, GRACE yields broad
cross category gains: averaged over four backbones, the supervised setting
improves overall score by 11.5% over base models, and the unsupervised variant
adds 6.9%, while preserving general capabilities. This work treats contrastive
objectives as rewards over rationales, unifying representation learning with
generation to produce stronger embeddings and transparent rationales. The
model, data and code are available at https://github.com/GasolSun36/GRACE.

### 9. On the Role of Unobserved Sequences on Sample-based Uncertainty Quantification for LLMs

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Lucie Kunitomo-Jacquin, Edison Marrese-Taylor, Ken Fukuda
- **URL**: <http://arxiv.org/abs/2510.04439v1>
- **Submitted**: 2025-10-06 02:14:48
- **Comment**: Accepted to UncertaiNLP workshop of EMNLP 2025
- **Topic Keywords**: query, recommend, search
- **Reason**: This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and large language models, but it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's focus on uncertainty quantification in LLMs is tangentially related to the user's interests in deep semantic understanding and real-time relevance optimization, but it is not a central match.

#### Abstract
> Quantifying uncertainty in large language models (LLMs) is important for
safety-critical applications because it helps spot incorrect answers, known as
hallucinations. One major trend of uncertainty quantification methods is based
on estimating the entropy of the distribution of the LLM's potential output
sequences. This estimation is based on a set of output sequences and associated
probabilities obtained by querying the LLM several times. In this paper, we
advocate and experimentally show that the probability of unobserved sequences
plays a crucial role, and we recommend future research to integrate it to
enhance such LLM uncertainty quantification methods.

### 10. Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Muyu He, Anand Kumar, Tsach Mackey, Meghana Rajeev, James Zou, Nazneen Rajani
- **URL**: <http://arxiv.org/abs/2510.04491v1>
- **Submitted**: 2025-10-06 05:03:57
- **Comment**: 25 pages
- **Topic Keywords**: rag, user behavior
- **Reason**: This paper explores the robustness of conversational AI agents to variations in user behavior, which is somewhat related to query understanding and user behavior modeling in Information Retrieval. However, the focus on conversational AI and user traits is not a central match to the user's primary research interests in IR and Search technologies.

#### Abstract
> Despite rapid progress in building conversational AI agents, robustness is
still largely untested. Small shifts in user behavior, such as being more
impatient, incoherent, or skeptical, can cause sharp drops in agent
performance, revealing how brittle current AI agents are. Today's benchmarks
fail to capture this fragility: agents may perform well under standard
evaluations but degrade spectacularly in more realistic and varied settings. We
address this robustness testing gap by introducing TraitBasis, a lightweight,
model-agnostic method for systematically stress testing AI agents. TraitBasis
learns directions in activation space corresponding to steerable user traits
(e.g., impatience or incoherence), which can be controlled, scaled, composed,
and applied at inference time without any fine-tuning or extra data. Using
TraitBasis, we extend $\tau$-Bench to $\tau$-Trait, where user behaviors are
altered via controlled trait vectors. We observe on average a 2%-30%
performance degradation on $\tau$-Trait across frontier models, highlighting
the lack of robustness of current AI agents to variations in user behavior.
Together, these results highlight both the critical role of robustness testing
and the promise of TraitBasis as a simple, data-efficient, and compositional
tool. By powering simulation-driven stress tests and training loops, TraitBasis
opens the door to building AI agents that remain reliable in the unpredictable
dynamics of real-world human interactions. We have open-sourced $\tau$-Trai
across four domains: airline, retail, telecom, and telehealth, so the community
can systematically QA their agents under realistic, behaviorally diverse
intents and trait scenarios: https://github.com/collinear-ai/tau-trait.

### 11. Time Is Effort: Estimating Human Post-Editing Time for Grammar Error Correction Tool Evaluation

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Ankit Vadehra, Bill Johnson, Gene Saunders, Pascal Poupart
- **URL**: <http://arxiv.org/abs/2510.04394v1>
- **Submitted**: 2025-10-05 23:24:24
- **Comment**: Accepted for publication in the 4th HCI+NLP Workshop (Fourth Workshop
  on Bridging Human-Computer Interaction and Natural Language Processing; part
  of EMNLP 2025)
- **Topic Keywords**: ranking, rank
- **Reason**: The paper is somewhat related to information retrieval, specifically in the context of text editing and grammar error correction. However, it focuses on the usability of grammar error correction tools rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's emphasis on human-centric evaluation and post-editing effort is also somewhat tangential to the user's primary research themes.

#### Abstract
> Text editing can involve several iterations of revision. Incorporating an
efficient Grammar Error Correction (GEC) tool in the initial correction round
can significantly impact further human editing effort and final text quality.
This raises an interesting question to quantify GEC Tool usability: How much
effort can the GEC Tool save users? We present the first large-scale dataset of
post-editing (PE) time annotations and corrections for two English GEC test
datasets (BEA19 and CoNLL14). We introduce Post-Editing Effort in Time (PEET)
for GEC Tools as a human-focused evaluation scorer to rank any GEC Tool by
estimating PE time-to-correct. Using our dataset, we quantify the amount of
time saved by GEC Tools in text editing. Analyzing the edit type indicated that
determining whether a sentence needs correction and edits like paraphrasing and
punctuation changes had the greatest impact on PE time. Finally, comparison
with human rankings shows that PEET correlates well with technical effort
judgment, providing a new human-centric direction for evaluating GEC tool
usability. We release our dataset and code at:
https://github.com/ankitvad/PEET_Scorer.

### 12. MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Guoxin Chen, Zile Qiao, Wenqing Wang, Donglei Yu, Xuanzhong Chen, Hao Sun, Minpeng Liao, Kai Fan, Yong Jiang, Penguin Xie, Wayne Xin Zhao, Ruihua Song, Fei Huang
- **URL**: <http://arxiv.org/abs/2510.04935v1>
- **Submitted**: 2025-10-06 15:42:55
- **Comment**: Ongoing Work
- **Topic Keywords**: rag, search
- **Reason**: The paper discusses a novel approach to Large Language Models (LLMs) for complex reasoning tasks, leveraging a dual-system dynamic. While it touches on the integration of external tools and multi-agent reinforcement learning, its primary focus is on improving LLMs' reasoning capabilities, which is somewhat related to information retrieval and search technologies. However, the paper's emphasis on LLMs and complex reasoning tasks makes it less directly relevant to the user's core research interests in IR and search technologies.

#### Abstract
> Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in
simple tasks, where the models excessively utilize System 2-type, deliberate
reasoning, leading to inefficient token generation. Furthermore, these models
face challenges in adapting their reasoning capabilities to rapidly changing
environments due to the static nature of their pretraining data. To address
these issues, advancing Large Language Models (LLMs) for complex reasoning
tasks requires innovative approaches that bridge intuitive and deliberate
cognitive processes, akin to human cognition's dual-system dynamic. This paper
introduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless
integration of System 1's fast, intuitive thinking with System 2's deliberate
reasoning within LLMs. MARS strategically integrates multiple external tools,
such as Google Search, Google Scholar, and Python Interpreter, to access
up-to-date information and execute complex computations, while creating a
specialized division of labor where System 1 efficiently processes and
summarizes high-volume external information, providing distilled insights that
expand System 2's reasoning context without overwhelming its capacity.
Furthermore, we propose a multi-agent reinforcement learning framework
extending Group Relative Policy Optimization to simultaneously optimize both
systems with multi-turn tool interactions, bin-packing optimization, and sample
balancing strategies that enhance collaborative efficiency. Extensive
experiments demonstrate MARS achieves substantial improvements of 3.86% on the
challenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%
across 7 knowledge-intensive tasks, validating the effectiveness of our
dual-system paradigm for complex reasoning in dynamic information environments.

### 13. Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Davood Rafiei, Morgan Lindsay Heisler, Weiwei Zhang, Mohammadreza Pourreza, Yong Zhang
- **URL**: <http://arxiv.org/abs/2510.04919v1>
- **Submitted**: 2025-10-06 15:33:35
- **Topic Keywords**: queries
- **Reason**: The paper focuses on Natural Language to SQL (NL2SQL) tasks and Large Language Models (LLMs), which is somewhat related to the user's interests in NLP and query understanding. However, the specific topic of dataset alignment for NL2SQL tasks is not directly aligned with the user's primary focus on information retrieval and real-time relevance optimization.

#### Abstract
> Supervised Fine-Tuning (SFT) is an effective method for adapting Large
Language Models (LLMs) on downstream tasks. However, variability in training
data can hinder a model's ability to generalize across domains. This paper
studies the problem of dataset alignment for Natural Language to SQL (NL2SQL or
text to SQL), examining how well SFT training data matches the structural
characteristics of target queries and how this alignment impacts model
performance. We hypothesize that alignment can be accurately estimated by
comparing the distributions of structural SQL features across the training set,
target data, and the model's predictions prior to SFT. Through comprehensive
experiments on three large cross-domain NL2SQL benchmarks and multiple model
families, we show that structural alignment is a strong predictor of
fine-tuning success. When alignment is high, SFT yields substantial gains in
accuracy and SQL generation quality; when alignment is low, improvements are
marginal or absent. These findings highlight the importance of alignment-aware
data selection for effective fine-tuning and generalization in NL2SQL tasks.

### 14. Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Qizheng Zhang, Changran Hu, Shubhangi Upasani, Boyuan Ma, Fenglu Hong, Vamsidhar Kamanuru, Jay Rainton, Chen Wu, Mengmeng Ji, Hanchen Li, Urmish Thakker, James Zou, Kunle Olukotun
- **URL**: <http://arxiv.org/abs/2510.04618v1>
- **Submitted**: 2025-10-06 09:30:18
- **Topic Keywords**: rag, rank
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and deep semantic understanding, but it focuses on language model adaptation and context engineering, which is not a central match for your primary focus on Information Retrieval (IR) and query understanding. The paper's emphasis on scalable and efficient LLM systems is also somewhat relevant, but it does not directly address your core research themes.

#### Abstract
> Large language model (LLM) applications such as agents and domain-specific
reasoning increasingly rely on context adaptation -- modifying inputs with
instructions, strategies, or evidence, rather than weight updates. Prior
approaches improve usability but often suffer from brevity bias, which drops
domain insights for concise summaries, and from context collapse, where
iterative rewriting erodes details over time. Building on the adaptive memory
introduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context
Engineering), a framework that treats contexts as evolving playbooks that
accumulate, refine, and organize strategies through a modular process of
generation, reflection, and curation. ACE prevents collapse with structured,
incremental updates that preserve detailed knowledge and scale with
long-context models. Across agent and domain-specific benchmarks, ACE optimizes
contexts both offline (e.g., system prompts) and online (e.g., agent memory),
consistently outperforming strong baselines: +10.6% on agents and +8.6% on
finance, while significantly reducing adaptation latency and rollout cost.
Notably, ACE could adapt effectively without labeled supervision and instead by
leveraging natural execution feedback. On the AppWorld leaderboard, ACE matches
the top-ranked production-level agent on the overall average and surpasses it
on the harder test-challenge split, despite using a smaller open-source model.
These results show that comprehensive, evolving contexts enable scalable,
efficient, and self-improving LLM systems with low overhead.

### 15. ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Rachneet Kaur, Nishan Srishankar, Zhen Zeng, Sumitra Ganesh, Manuela Veloso
- **URL**: <http://arxiv.org/abs/2510.04514v1>
- **Submitted**: 2025-10-06 06:05:36
- **Comment**: 53 pages, 12 figures, 15 tables
- **Topic Keywords**: queries
- **Reason**: This paper is somewhat related to information retrieval, but its focus on visually grounded reasoning and multimodal agents in chart question answering is not directly aligned with the user's core research themes in query understanding, ranking models, and user behavior modeling. While it involves deep semantic understanding and multimodal processing, the application domain is specific to chart-based visual question answering, which is not a central match for the user's interests.

#### Abstract
> Recent multimodal LLMs have shown promise in chart-based visual question
answering, but their performance declines sharply on unannotated charts, those
requiring precise visual interpretation rather than relying on textual
shortcuts. To address this, we introduce ChartAgent, a novel agentic framework
that explicitly performs visual reasoning directly within the chart's spatial
domain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively
decomposes queries into visual subtasks and actively manipulates and interacts
with chart images through specialized actions such as drawing annotations,
cropping regions (e.g., segmenting pie slices, isolating bars), and localizing
axes, using a library of chart-specific vision tools to fulfill each subtask.
This iterative reasoning process closely mirrors human cognitive strategies for
chart comprehension. ChartAgent achieves state-of-the-art accuracy on the
ChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%
absolute gain overall and 17.31% on unannotated, numerically intensive queries.
Furthermore, our analyses show that ChartAgent is (a) effective across diverse
chart types, (b) achieve the highest scores across varying visual and reasoning
complexity levels, and (c) serves as a plug-and-play framework that boosts
performance across diverse underlying LLMs. Our work is among the first to
demonstrate visually grounded reasoning for chart understanding using
tool-augmented multimodal agents.

### 16. MARCO: A Cooperative Knowledge Transfer Framework for Personalized Cross-domain Recommendations

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Lili Xie, Yi Zhang, Ruihong Qiu, Jiajun Liu, Sen Wang
- **URL**: <http://arxiv.org/abs/2510.04508v1>
- **Submitted**: 2025-10-06 05:49:47
- **Comment**: SIGIR-AP 2025
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on recommender systems and cross-domain recommendation, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the emphasis on recommender systems and multi-agent reinforcement learning is not a central match for your primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Recommender systems frequently encounter data sparsity issues, particularly
when addressing cold-start scenarios involving new users or items. Multi-source
cross-domain recommendation (CDR) addresses these challenges by transferring
valuable knowledge from multiple source domains to enhance recommendations in a
target domain. However, existing reinforcement learning (RL)-based CDR methods
typically rely on a single-agent framework, leading to negative transfer issues
caused by inconsistent domain contributions and inherent distributional
discrepancies among source domains. To overcome these limitations, MARCO, a
Multi-Agent Reinforcement Learning-based Cross-Domain recommendation framework,
is proposed. It leverages cooperative multi-agent reinforcement learning, where
each agent is dedicated to estimating the contribution from an individual
source domain, effectively managing credit assignment and mitigating negative
transfer. In addition, an entropy-based action diversity penalty is introduced
to enhance policy expressiveness and stabilize training by encouraging diverse
agents' joint actions. Extensive experiments across four benchmark datasets
demonstrate MARCO's superior performance over state-of-the-art methods,
highlighting its robustness and strong generalization capabilities. The code is
at https://github.com/xiewilliams/MARCO.

### 17. Causality-aware Graph Aggregation Weight Estimator for Popularity Debiasing in Top-K Recommendation

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Yue Que, Yingyi Zhang, Xiangyu Zhao, Chen Ma
- **URL**: <http://arxiv.org/abs/2510.04502v1>
- **Submitted**: 2025-10-06 05:33:37
- **Comment**: Accepted by CIKM 2025
- **Topic Keywords**: rag, recommend
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Search technologies, particularly in the context of recommender systems. However, it focuses on graph-based recommender systems and debiasing methods, which is a specific area within recommender systems. While it involves modeling and optimization, it does not directly relate to your primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Graph-based recommender systems leverage neighborhood aggregation to generate
node representations, which is highly sensitive to popularity bias, resulting
in an echo effect during information propagation. Existing graph-based
debiasing solutions refine the aggregation process with attempts such as edge
reconstruction or weight adjustment. However, these methods remain inadequate
in fully alleviating popularity bias. Specifically, this is because 1) they
provide no insights into graph aggregation rationality, thus lacking an
optimality guarantee; 2) they fail to well balance the training and debiasing
process, which undermines the effectiveness. In this paper, we propose a novel
approach to mitigate popularity bias through rational modeling of the graph
aggregation process. We reveal that graph aggregation is a special form of
backdoor adjustment in causal inference, where the aggregation weight
corresponds to the historical interaction likelihood distribution. Based on
this insight, we devise an encoder-decoder architecture, namely Causality-aware
Graph Aggregation Weight Estimator for Debiasing (CAGED), to approximate the
unbiased aggregation weight by optimizing the evidence lower bound of the
interaction likelihood. In order to enhance the debiasing effectiveness during
early training stages, we further design a momentum update strategy that
incrementally refines the aggregation weight matrix. Extensive experiments on
three datasets demonstrate that CAGED outperforms existing graph-based
debiasing methods. Our implementation is available at
https://github.com/QueYork/CAGED.

### 18. From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Mingkang Zhu, Xi Chen, Bei Yu, Hengshuang Zhao, Jiaya Jia
- **URL**: <http://arxiv.org/abs/2510.05095v1>
- **Submitted**: 2025-10-06 17:58:01
- **Topic Keywords**: rag
- **Reason**: The paper explores methods for optimizing large reasoning models, specifically addressing the challenge of aligning these models with human preferences. While it touches on aspects of model optimization and gradient estimation, it does not directly relate to information retrieval, search technologies, or query understanding, which are core areas of your research interests.

#### Abstract
> Large reasoning models (LRMs) generate intermediate reasoning traces before
producing final answers, yielding strong gains on multi-step and mathematical
tasks. Yet aligning LRMs with human preferences, a crucial prerequisite for
model deployment, remains underexplored. The statistically correct objective
for preference alignment requires marginalizing over reasoning traces, but this
computation is intractable in practice. A common workaround optimizes a single
sampled trajectory, which introduces substantial gradient variance from
stochastic trace sampling. To address this challenge, we frame preference
optimization for LRMs through the lens of the bias--variance trade-off and
propose Bias--Variance Optimized Preference Optimization (BVPO), a simple,
drop-in method that mixes two gradient estimators: a high-variance trace-based
estimator and a low-variance empty-trace estimator obtained by disabling
reasoning trace generation. Our theory shows that BVPO strictly reduces
trace-induced variance for any nontrivial mixture, provides a closed-form
choice of the mixing weight that minimizes mean-squared error relative to the
true marginal gradient, and under standard smoothness and step-size conditions,
tightens classical convergence bounds for stochastic gradient descent.
Empirically, BVPO improves alignment over the best baseline by up to 7.8 points
on AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on
general conversational data, BVPO also boosts reasoning performance for base
models by up to 4.0 points on the average of six math reasoning benchmarks.
These results identify variance from trace sampling as a key bottleneck and
demonstrate that directly optimizing the bias--variance trade-off yields more
stable training and stronger overall performance.

### 19. Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Om Dobariya, Akhil Kumar
- **URL**: <http://arxiv.org/abs/2510.04950v1>
- **Submitted**: 2025-10-06 15:50:39
- **Comment**: 5 pages, 3 tables; includes Limitations and Ethical Considerations
  sections; short paper under submission to Findings of ACL 2025
- **Topic Keywords**: rag
- **Reason**: This paper explores the impact of prompt politeness on large language model accuracy, which is related to Natural Language Processing (NLP) and query understanding. However, it does not directly align with the user's primary focus on Information Retrieval (IR), ranking models, and user behavior modeling. The paper's findings on the social dimensions of human-AI interaction may be tangentially relevant to the user's interests in IR and NLP, but it is not a central match.

#### Abstract
> The wording of natural language prompts has been shown to influence the
performance of large language models (LLMs), yet the role of politeness and
tone remains underexplored. In this study, we investigate how varying levels of
prompt politeness affect model accuracy on multiple-choice questions. We
created a dataset of 50 base questions spanning mathematics, science, and
history, each rewritten into five tone variants: Very Polite, Polite, Neutral,
Rude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we
evaluated responses across these conditions and applied paired sample t-tests
to assess statistical significance. Contrary to expectations, impolite prompts
consistently outperformed polite ones, with accuracy ranging from 80.8% for
Very Polite prompts to 84.8% for Very Rude prompts. These findings differ from
earlier studies that associated rudeness with poorer outcomes, suggesting that
newer LLMs may respond differently to tonal variation. Our results highlight
the importance of studying pragmatic aspects of prompting and raise broader
questions about the social dimensions of human-AI interaction.

### 20. Instability in Downstream Task Performance During LLM Pretraining

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Yuto Nishida, Masaru Isonuma, Yusuke Oda
- **URL**: <http://arxiv.org/abs/2510.04848v1>
- **Submitted**: 2025-10-06 14:33:38
- **Comment**: Accepted to EMNLP 2025 Findings
- **Topic Keywords**: rag
- **Reason**: This paper is loosely relevant to your research interests in Natural Language Processing (NLP) and Learning to Rank, as it involves large language models and their pretraining. However, the focus is on the stability of downstream task performance rather than query understanding, ranking models, or user behavior modeling, which are your primary areas of interest.

#### Abstract
> When training large language models (LLMs), it is common practice to track
downstream task performance throughout the training process and select the
checkpoint with the highest validation score. However, downstream metrics often
exhibit substantial fluctuations, making it difficult to identify the
checkpoint that truly represents the best-performing model. In this study, we
empirically analyze the stability of downstream task performance in an LLM
trained on diverse web-scale corpora. We find that task scores frequently
fluctuate throughout training, both at the aggregate and example levels. To
address this instability, we investigate two post-hoc checkpoint integration
methods: checkpoint averaging and ensemble, motivated by the hypothesis that
aggregating neighboring checkpoints can reduce performance volatility. We
demonstrate both empirically and theoretically that these methods improve
downstream performance stability without requiring any changes to the training
procedure.

### 21. Multilingual Routing in Mixture-of-Experts

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Lucas Bandarkar, Chenyuan Yang, Mohsen Fayyaz, Junlin Hu, Nanyun Peng
- **URL**: <http://arxiv.org/abs/2510.04694v1>
- **Submitted**: 2025-10-06 11:09:20
- **Topic Keywords**: rag
- **Reason**: This paper explores Mixture-of-Experts (MoE) architectures, which are relevant to Search technologies and ranking models. However, the focus on multilingual routing and language-universal experts is somewhat tangential to the user's core research themes in Information Retrieval and query understanding.

#### Abstract
> Mixture-of-Experts (MoE) architectures have become the key to scaling modern
LLMs, yet little is understood about how their sparse routing dynamics respond
to multilingual data. In this work, we analyze expert routing patterns using
parallel multilingual datasets and present highly interpretable layer-wise
phenomena. We find that MoE models route tokens in language-specific ways in
the early and late decoder layers but exhibit significant cross-lingual routing
alignment in middle layers, mirroring parameter-sharing trends observed in
dense LLMs. In particular, we reveal a clear, strong correlation between a
model's performance in a given language and how similarly its tokens are routed
to English in these layers. Extending beyond correlation, we explore
inference-time interventions that induce higher cross-lingual routing
alignment. We introduce a method that steers the router by promoting
middle-layer task experts frequently activated in English, and it successfully
increases multilingual performance. These 1-2% gains are remarkably consistent
across two evaluation tasks, three models, and 15+ languages, especially given
that these simple interventions override routers of extensively trained,
state-of-the-art LLMs. In comparison, interventions outside of the middle
layers or targeting multilingual-specialized experts only yield performance
degradation. Altogether, we present numerous findings that explain how MoEs
process non-English text and demonstrate that generalization is limited by the
model's ability to leverage language-universal experts in all languages.

### 22. Large Language Models Achieve Gold Medal Performance at the International Olympiad on Astronomy & Astrophysics (IOAA)

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Lucas Carrit Delgado Pinheiro, Ziru Chen, Bruno Caixeta Piazza, Ness Shroff, Yingbin Liang, Yuan-Sen Ting, Huan Sun
- **URL**: <http://arxiv.org/abs/2510.05016v2>
- **Submitted**: 2025-10-06 16:58:47
- **Comment**: 18 pages, 6 figures, to be submitted, comments are welcome.
  Reproducibility details can be found at:
  https://github.com/OSU-NLP-Group/LLM-IOAA
- **Topic Keywords**: ranking, rag, rank, search
- **Reason**: This paper is not relevant to your research interests as it focuses on the application of large language models in astronomy, which is outside your area of expertise in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve deep conceptual understanding, it is not related to your core themes of query understanding, ranking models, or user behavior modeling.

#### Abstract
> While task-specific demonstrations show early success in applying large
language models (LLMs) to automate some astronomical research tasks, they only
provide incomplete views of all necessary capabilities in solving astronomy
problems, calling for more thorough understanding of LLMs' strengths and
limitations. So far, existing benchmarks and evaluations focus on simple
question-answering that primarily tests astronomical knowledge and fails to
evaluate the complex reasoning required for real-world research in the
discipline. Here, we address this gap by systematically benchmarking five
state-of-the-art LLMs on the International Olympiad on Astronomy and
Astrophysics (IOAA) exams, which are designed to examine deep conceptual
understanding, multi-step derivations, and multimodal analysis. With average
scores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing
models) not only achieve gold medal level performance but also rank in the top
two among ~200-300 participants in all four IOAA theory exams evaluated
(2022-2025). In comparison, results on the data analysis exams show more
divergence. GPT-5 still excels in the exams with an 88.5% average score,
ranking top 10 among the participants in the four most recent IOAAs, while
other models' performances drop to 48-76%. Furthermore, our in-depth error
analysis underscores conceptual reasoning, geometric reasoning, and spatial
visualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,
although LLMs approach peak human performance in theory exams, critical gaps
must be addressed before they can serve as autonomous research agents in
astronomy.

### 23. Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Tomas Figliolia, Nicholas Alonso, Rishi Iyer, Quentin Anthony, Beren Millidge
- **URL**: <http://arxiv.org/abs/2510.04476v1>
- **Submitted**: 2025-10-06 04:24:23
- **Topic Keywords**: query, queries
- **Reason**: This paper focuses on optimizing attention mechanisms in transformers for efficient computation and memory usage, which is somewhat related to information retrieval and search technologies. However, it does not directly address query understanding, ranking models, or user behavior modeling, making it less relevant to your core research interests.

#### Abstract
> Multi-headed Attention's (MHA) quadratic compute and linearly growing
KV-cache make long-context transformers expensive to train and serve. Prior
works such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)
shrink the cache, speeding decode, but leave compute, which determines prefill
and training speed, largely unchanged. We introduce Compressed Convolutional
Attention (CCA), a novel attention method which down-projects queries, keys,
and values and performs the entire attention operation inside the shared latent
space. This simple design dramatically cuts parameters, KV-cache, and FLOPs all
at once by the desired compression factor. Because CCA is orthogonal to
head-sharing, we combine the two to form Compressed Convolutional Grouped Query
Attention (CCGQA), which further tightens the compute-bandwidth Pareto frontier
so that users can tune compression toward either FLOP or memory limits without
sacrificing quality. Experiments show that CCGQA consistently outperforms both
GQA and MLA at equal KV-cache compression on dense and MoE models.
Additionally, we show that CCGQA outperforms all other attention methods on MoE
models with half the KV-cache of GQA and MLA, achieving an 8x KV-cache
compression with no drop in performance compared to standard MHA. CCA and CCGQA
also dramatically reduce the FLOP cost of attention which leads to
substantially faster training and prefill than existing methods. On H100 GPUs,
our fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence
length of 16k relative to MHA, and accelerates backward by about 1.3x.

### 24. Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Yicheng Tao, Yao Qin, Yepang Liu
- **URL**: <http://arxiv.org/abs/2510.04905v1>
- **Submitted**: 2025-10-06 15:20:03
- **Topic Keywords**: rag, retrieval, search
- **Reason**: This paper is not directly related to your core research themes in Information Retrieval and Search technologies, as it focuses on code generation and software engineering. While it involves retrieval mechanisms, the context is specific to code generation and not relevant to your interests in query understanding, ranking models, or user behavior modeling.

#### Abstract
> Recent advancements in large language models (LLMs) have substantially
improved automated code generation. While function-level and file-level
generation have achieved promising results, real-world software development
typically requires reasoning across entire repositories. This gives rise to the
challenging task of Repository-Level Code Generation (RLCG), where models must
capture long-range dependencies, ensure global semantic consistency, and
generate coherent code spanning multiple files or modules. To address these
challenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful
paradigm that integrates external retrieval mechanisms with LLMs, enhancing
context-awareness and scalability. In this survey, we provide a comprehensive
review of research on Retrieval-Augmented Code Generation (RACG), with an
emphasis on repository-level approaches. We categorize existing work along
several dimensions, including generation strategies, retrieval modalities,
model architectures, training paradigms, and evaluation protocols. Furthermore,
we summarize widely used datasets and benchmarks, analyze current limitations,
and outline key challenges and opportunities for future research. Our goal is
to establish a unified analytical framework for understanding this rapidly
evolving field and to inspire continued progress in AI-powered software
engineering.

### 25. Proactive defense against LLM Jailbreak

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Weiliang Zhao, Jinjun Peng, Daniel Ben-Levi, Zhou Yu, Junfeng Yang
- **URL**: <http://arxiv.org/abs/2510.05052v1>
- **Submitted**: 2025-10-06 17:32:40
- **Topic Keywords**: queries, search
- **Reason**: This paper is not directly related to your core research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. Although it touches on search-based attacks, its focus is on proactive defense against large language model (LLM) jailbreaks, which is not a central theme in your research.

#### Abstract
> The proliferation of powerful large language models (LLMs) has necessitated
robust safety alignment, yet these models remain vulnerable to evolving
adversarial attacks, including multi-turn jailbreaks that iteratively search
for successful queries. Current defenses, primarily reactive and static, often
fail to counter these search-based attacks. In this paper, we introduce ProAct,
a novel proactive defense framework designed to disrupt and mislead autonomous
jailbreaking processes. Our core idea is to intentionally provide adversaries
with "spurious responses" that appear to be results of successful jailbreak
attacks but contain no actual harmful content. These misleading responses
provide false signals to the attacker's internal optimization loop, causing the
adversarial search to terminate prematurely and effectively jailbreaking the
jailbreak. By conducting extensive experiments across state-of-the-art LLMs,
jailbreaking frameworks, and safety benchmarks, our method consistently and
significantly reduces attack success rates by up to 92\%. When combined with
other defense frameworks, it further reduces the success rate of the latest
attack strategies to 0\%. ProAct represents an orthogonal defense strategy that
can serve as an additional guardrail to enhance LLM safety against the most
effective jailbreaking attacks.

### 26. TeachLM: Post-Training LLMs for Education Using Authentic Learning Data

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Janos Perczel, Jin Chow, Dorottya Demszky
- **URL**: <http://arxiv.org/abs/2510.05087v1>
- **Submitted**: 2025-10-06 17:55:04
- **Comment**: 28 pages, 9 figures
- **Topic Keywords**: rag, personalization
- **Reason**: This paper focuses on applying large language models (LLMs) in education, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the context and application are quite different from your areas of focus.

#### Abstract
> The promise of generative AI to revolutionize education is constrained by the
pedagogical limits of large language models (LLMs). A major issue is the lack
of access to high-quality training data that reflect the learning of actual
students. Prompt engineering has emerged as a stopgap, but the ability of
prompts to encode complex pedagogical strategies in rule-based natural language
is inherently limited. To address this gap we introduce TeachLM - an LLM
optimized for teaching through parameter-efficient fine-tuning of
state-of-the-art models. TeachLM is trained on a dataset comprised of 100,000
hours of one-on-one, longitudinal student-tutor interactions maintained by
Polygence, which underwent a rigorous anonymization process to protect privacy.
We use parameter-efficient fine-tuning to develop an authentic student model
that enables the generation of high-fidelity synthetic student-tutor dialogues.
Building on this capability, we propose a novel multi-turn evaluation protocol
that leverages synthetic dialogue generation to provide fast, scalable, and
reproducible assessments of the dialogical capabilities of LLMs. Our
evaluations demonstrate that fine-tuning on authentic learning data
significantly improves conversational and pedagogical performance - doubling
student talk time, improving questioning style, increasing dialogue turns by
50%, and greater personalization of instruction.

### 27. SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Dachuan Shi, Abedelkadir Asi, Keying Li, Xiangchi Yuan, Leyan Pan, Wenke Lee, Wen Xiao
- **URL**: <http://arxiv.org/abs/2510.05069v1>
- **Submitted**: 2025-10-06 17:46:34
- **Comment**: Code: https://github.com/sdc17/SwiReasoning, Website:
  https://swireasoning.github.io/
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on improving the performance of large language models (LLMs) through a framework called SwiReasoning, which dynamically switches between explicit and latent reasoning. While it touches on the idea of reasoning and optimization, it does not directly relate to information retrieval, search technologies, or user behavior modeling, which are the core areas of your research interests.

#### Abstract
> Recent work shows that, beyond discrete reasoning through explicit
chain-of-thought steps, which are limited by the boundaries of natural
languages, large language models (LLMs) can also reason continuously in latent
space, allowing richer information per step and thereby improving token
efficiency. Despite this promise, latent reasoning still faces two challenges,
especially in training-free settings: 1) purely latent reasoning broadens the
search distribution by maintaining multiple implicit paths, which diffuses
probability mass, introduces noise, and impedes convergence to a single
high-confidence solution, thereby hurting accuracy; and 2) overthinking
persists even without explicit text, wasting tokens and degrading efficiency.
To address these issues, we introduce SwiReasoning, a training-free framework
for LLM reasoning which features two key innovations: 1) SwiReasoning
dynamically switches between explicit and latent reasoning, guided by
block-wise confidence estimated from entropy trends in next-token
distributions, to balance exploration and exploitation and promote timely
convergence. 2) By limiting the maximum number of thinking-block switches,
SwiReasoning curbs overthinking and improves token efficiency across varying
problem difficulties. On widely used mathematics and STEM benchmarks,
SwiReasoning consistently improves average accuracy by 1.5%-2.8% across
reasoning LLMs of different model families and scales. Furthermore, under
constrained budgets, SwiReasoning improves average token efficiency by 56%-79%,
with larger gains as budgets tighten.

### 28. Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Baher Mohammad, Magauiya Zhussip, Stamatios Lefkimmiatis
- **URL**: <http://arxiv.org/abs/2510.04738v1>
- **Submitted**: 2025-10-06 12:11:31
- **Topic Keywords**: pairwise
- **Reason**: This paper focuses on voice editing and text-to-speech synthesis, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some form of sequence modeling, the context and application are quite different from your areas of focus.

#### Abstract
> We introduce MAVE (Mamba with Cross-Attention for Voice Editing and
Synthesis), a novel autoregressive architecture for text-conditioned voice
editing and high-fidelity text-to-speech (TTS) synthesis, built on a
cross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in
speech editing and very competitive results in zero-shot TTS, while not being
explicitly trained on the latter task, outperforming leading autoregressive and
diffusion models on diverse, real-world audio. By integrating Mamba for
efficient audio sequence modeling with cross-attention for precise
text-acoustic alignment, MAVE enables context-aware voice editing with
exceptional naturalness and speaker consistency. In pairwise human evaluations
on a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2%
of listeners rated MAVE - edited speech as perceptually equal to the original,
while 24.8% prefered the original and 18.0% MAVE - demonstrating that in the
majority of cases edits are indistinguishable from the source. MAVE compares
favorably with VoiceCraft and FluentSpeech both on pairwise comparisons and
standalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE
exceeds VoiceCraft in both speaker similarity and naturalness, without
requiring multiple inference runs or post-processing. Remarkably, these quality
gains come with a significantly lower memory cost and approximately the same
latency: MAVE requires ~6x less memory than VoiceCraft during inference on
utterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch
size 1). Our results demonstrate that MAVE establishes a new standard for
flexible, high-fidelity voice editing and synthesis through the synergistic
integration of structured state-space modeling and cross-modal attention.

### 29. FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient Federated Large Language Models Fine-Tuning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Guochen Yan, Luyuan Xie, Qingni Shen, Yuejian Fang, Zhonghai Wu
- **URL**: <http://arxiv.org/abs/2510.04601v1>
- **Submitted**: 2025-10-06 09:06:38
- **Topic Keywords**: rag, rank
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling. While it involves federated learning and large language models, its focus is on communication efficiency and model fine-tuning, which is not a central match to your primary research themes.

#### Abstract
> The current paradigm of training large language models (LLMs) on publicly
available Web data is becoming unsustainable, with high-quality data sources in
specialized domains nearing exhaustion. Federated Learning (FL) emerges as a
practical solution for the next generation of AI on a decentralized Web,
enabling privacy-preserving collaborative fine-tuning by leveraging private
data distributed across a global client base. While Low-Rank Adaptation (LoRA)
is the standard for efficient fine-tuning, its application in federated
settings presents a critical challenge: communication overhead remains a
significant bottleneck across the Web's heterogeneous network conditions. The
structural redundancy within LoRA parameters not only incurs a heavy
communication burden but also introduces conflicts when aggregating client
updates. To address this, we propose FedSRD, a Sparsify-Reconstruct-Decompose
framework designed for communication-efficient FL. We first introduce an
importance-aware sparsification method that preserves the structural integrity
of LoRA updates to reduce the uploaded parameter count. The server then
reconstructs and aggregates these updates in a full-rank space to mitigate
conflicts. Finally, it decomposes the global update into a sparse low-rank
format for broadcast, ensuring a symmetrically efficient cycle. We also propose
an efficient variant, FedSRD-e, to reduce computational overhead. Experimental
results on 10 benchmarks demonstrate that our framework significantly reduces
communication costs by up to 90\% while even improving model performance on
heterogeneous client data.

### 30. Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Wenyuan Zhao, Adithya Balachandran, Chao Tian, Paul Pu Liang
- **URL**: <http://arxiv.org/abs/2510.04417v1>
- **Submitted**: 2025-10-06 01:08:34
- **Comment**: NeurIPS 2025
- **Topic Keywords**: pairwise
- **Reason**: This paper focuses on partial information decomposition, a topic outside of the user's primary research interests in Information Retrieval and Search technologies. While it does involve data analysis and modeling, the context and methodology are not directly related to the user's core themes of query understanding, ranking models, or user behavior modeling.

#### Abstract
> The study of multimodality has garnered significant interest in fields where
the analysis of interactions among multiple information sources can enhance
predictive modeling, data fusion, and interpretability. Partial information
decomposition (PID) has emerged as a useful information-theoretic framework to
quantify the degree to which individual modalities independently, redundantly,
or synergistically convey information about a target variable. However,
existing PID methods depend on optimizing over a joint distribution constrained
by estimated pairwise probability distributions, which are costly and
inaccurate for continuous and high-dimensional modalities. Our first key
insight is that the problem can be solved efficiently when the pairwise
distributions are multivariate Gaussians, and we refer to this problem as
Gaussian PID (GPID). We propose a new gradient-based algorithm that
substantially improves the computational efficiency of GPID based on an
alternative formulation of the underlying optimization problem. To generalize
the applicability to non-Gaussian data, we learn information-preserving
encoders to transform random variables of arbitrary input distributions into
pairwise Gaussian random variables. Along the way, we resolved an open problem
regarding the optimality of joint Gaussian solutions for GPID. Empirical
validation in diverse synthetic examples demonstrates that our proposed method
provides more accurate and efficient PID estimates than existing baselines. We
further evaluate a series of large-scale multimodal benchmarks to show its
utility in real-world applications of quantifying PID in multimodal datasets
and selecting high-performing models.

### 31. Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Runchu Tian, Junxia Cui, Xueqiang Xu, Feng Yao, Jingbo Shang
- **URL**: <http://arxiv.org/abs/2510.05090v1>
- **Submitted**: 2025-10-06 17:56:46
- **Comment**: 17 pages, 8 figures. Work in progress
- **Topic Keywords**: rag
- **Reason**: This paper focuses on improving the decoding strategy for Diffusion Large Language Models, which is not directly related to Information Retrieval or Search technologies. While it involves Natural Language Processing, the specific topic of decoding algorithms for language models does not align with the user's core research themes.

#### Abstract
> Diffusion large language models (dLLMs) have recently emerged as a promising
alternative to autoregressive (AR) models, offering advantages such as
accelerated parallel decoding and bidirectional context modeling. However, the
vanilla decoding strategy in discrete dLLMs suffers from a critical limitation:
once a token is accepted, it can no longer be revised in subsequent steps. As a
result, early mistakes persist across iterations, harming both intermediate
predictions and final output quality. To address this issue, we propose
Tolerator (Token-Level Cross-Validation Refinement), a training-free decoding
strategy that leverages cross-validation among predicted tokens. Unlike
existing methods that follow a single progressive unmasking procedure,
Tolerator introduces a two-stage process: (i) sequence fill-up and (ii)
iterative refinement by remasking and decoding a subset of tokens while
treating the remaining as context. This design enables previously accepted
tokens to be reconsidered and corrected when necessary, leading to more
reliable diffusion decoding outputs. We evaluate Tolerator on five standard
benchmarks covering language understanding, code generation, and mathematics.
Experiments show that our method achieves consistent improvements over the
baselines under the same computational budget. These findings suggest that
decoding algorithms are crucial to realizing the full potential of diffusion
large language models. Code and data are publicly available.

### 32. On Structured State-Space Duality

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jerry Yao-Chieh Hu, Xiwen Zhang, Weimin Wu, Han Liu
- **URL**: <http://arxiv.org/abs/2510.04944v1>
- **Submitted**: 2025-10-06 15:46:50
- **Topic Keywords**: rank, icml
- **Reason**: This paper appears to be primarily focused on the theoretical relationship between state-space models and masked attention mechanisms in sequence transformation, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it touches on sequence models, it does not seem to address query understanding, ranking models, or real-time relevance optimization, which are core areas of interest.

#### Abstract
> Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence
between a simple Structured State-Space Model (SSM) and a masked attention
mechanism. In particular, a state-space model with a scalar-times-identity
state matrix is equivalent to a masked self-attention with a $1$-semiseparable
causal mask. Consequently, the same sequence transformation (model) has two
algorithmic realizations: as a linear-time $O(T)$ recurrence or as a
quadratic-time $O(T^2)$ attention. In this note, we formalize and generalize
this duality: (i) we extend SSD from the scalar-identity case to general
diagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs
match the scalar case's training complexity lower bounds while supporting
richer dynamics; (iii) we establish a necessary and sufficient condition under
which an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we
show that such duality fails to extend to standard softmax attention due to
rank explosion. Together, these results tighten bridge between recurrent SSMs
and Transformers, and widen the design space for expressive yet efficient
sequence models.

### 33. SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Punya Syon Pandey, Hai Son Le, Devansh Bhardwaj, Rada Mihalcea, Zhijing Jin
- **URL**: <http://arxiv.org/abs/2510.04891v1>
- **Submitted**: 2025-10-06 15:11:46
- **Topic Keywords**: rag
- **Reason**: This paper focuses on the vulnerabilities of Large Language Models (LLMs) in sociopolitical contexts, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on NLP, the specific domain and application are quite different from the user's core areas of focus.

#### Abstract
> Large language models (LLMs) are increasingly deployed in contexts where
their failures can have direct sociopolitical consequences. Yet, existing
safety benchmarks rarely test vulnerabilities in domains such as political
manipulation, propaganda and disinformation generation, or surveillance and
information control. We introduce SocialHarmBench, a dataset of 585 prompts
spanning 7 sociopolitical categories and 34 countries, designed to surface
where LLMs most acutely fail in politically charged contexts. Our evaluations
reveal several shortcomings: open-weight models exhibit high vulnerability to
harmful compliance, with Mistral-7B reaching attack success rates as high as
97% to 98% in domains such as historical revisionism, propaganda, and political
manipulation. Moreover, temporal and geographic analyses show that LLMs are
most fragile when confronted with 21st-century or pre-20th-century contexts,
and when responding to prompts tied to regions such as Latin America, the USA,
and the UK. These findings demonstrate that current safeguards fail to
generalize to high-stakes sociopolitical settings, exposing systematic biases
and raising concerns about the reliability of LLMs in preserving human rights
and democratic values. We share the SocialHarmBench benchmark at
https://huggingface.co/datasets/psyonp/SocialHarmBench.

### 34. Detecting Distillation Data from Reasoning Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Hengxiang Zhang, Hyeong Kyu Choi, Yixuan Li, Hongxin Wei
- **URL**: <http://arxiv.org/abs/2510.04850v1>
- **Submitted**: 2025-10-06 14:37:02
- **Topic Keywords**: rag
- **Reason**: This paper appears to be related to Natural Language Processing (NLP) and large language models, but it does not align with the user's primary focus on Information Retrieval (IR), query understanding, ranking models, or user behavior modeling. The paper's focus on detecting distillation data from reasoning models is not directly relevant to the user's research interests.

#### Abstract
> Reasoning distillation has emerged as an efficient and powerful paradigm for
enhancing the reasoning capabilities of large language models. However,
reasoning distillation may inadvertently cause benchmark contamination, where
evaluation data included in distillation datasets can inflate performance
metrics of distilled models. In this work, we formally define the task of
distillation data detection, which is uniquely challenging due to the partial
availability of distillation data. Then, we propose a novel and effective
method Token Probability Deviation (TBD), which leverages the probability
patterns of the generated output tokens. Our method is motivated by the
analysis that distilled models tend to generate near-deterministic tokens for
seen questions, while producing more low-probability tokens for unseen
questions. Our key idea behind TBD is to quantify how far the generated tokens'
probabilities deviate from a high reference probability. In effect, our method
achieves competitive detection performance by producing lower scores for seen
questions than for unseen questions. Extensive experiments demonstrate the
effectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of
0.470 on the S1 dataset.

### 35. Are BabyLMs Deaf to Gricean Maxims? A Pragmatic Evaluation of Sample-efficient Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Raha Askari, Sina Zarrie√ü, √ñzge Alacam, Judith Sieker
- **URL**: <http://arxiv.org/abs/2510.04764v1>
- **Submitted**: 2025-10-06 12:38:41
- **Topic Keywords**: rag
- **Reason**: This paper appears to be focused on evaluating the pragmatic abilities of language models, specifically their understanding of Gricean maxims. While it touches on aspects of language understanding, it does not seem to be directly related to information retrieval, search technologies, or user behavior modeling, which are core areas of your research interests.

#### Abstract
> Implicit meanings are integral to human communication, making it essential
for language models to be capable of identifying and interpreting them. Grice
(1975) proposed a set of conversational maxims that guide cooperative dialogue,
noting that speakers may deliberately violate these principles to express
meanings beyond literal words, and that listeners, in turn, recognize such
violations to draw pragmatic inferences.
  Building on Surian et al. (1996)'s study of children's sensitivity to
violations of Gricean maxims, we introduce a novel benchmark to test whether
language models pretrained on less than 10M and less than 100M tokens can
distinguish maxim-adhering from maxim-violating utterances. We compare these
BabyLMs across five maxims and situate their performance relative to children
and a Large Language Model (LLM) pretrained on 3T tokens.
  We find that overall, models trained on less than 100M tokens outperform
those trained on less than 10M, yet fall short of child-level and LLM
competence. Our results suggest that modest data increases improve some aspects
of pragmatic behavior, leading to finer-grained differentiation between
pragmatic dimensions.

### 36. JSON Whisperer: Efficient JSON Editing with LLMs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Sarel Duanis, Asnat Greenstein-Messica, Eliya Habba
- **URL**: <http://arxiv.org/abs/2510.04717v1>
- **Submitted**: 2025-10-06 11:36:46
- **Topic Keywords**: emnlp
- **Reason**: This paper focuses on efficient JSON editing using Large Language Models (LLMs), which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the context is more about editing JSON documents rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> Large language models (LLMs) can modify JSON documents through natural
language commands, but current approaches regenerate entire structures for each
edit, resulting in computational inefficiency. We present JSON Whisperer, a
framework that enables LLMs to generate RFC 6902 diff patches-expressing only
the necessary modifications-rather than complete documents. We identify two key
challenges in patch-based editing: (1) LLMs often miss related updates when
generating isolated patches, and (2) array manipulations require tracking index
shifts across operations, which LLMs handle poorly. To address these issues, we
introduce EASE (Explicitly Addressed Sequence Encoding), which transforms
arrays into dictionaries with stable keys, eliminating index arithmetic
complexities. Our evaluation shows that patch generation with EASE reduces
token usage by 31% while maintaining edit quality within 5% of full
regeneration with particular gains for complex instructions and list
manipulations. The dataset is available at:
https://github.com/emnlp2025/JSON-Whisperer/

### 37. TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Chanjoo Jung, Jaehyung Kim
- **URL**: <http://arxiv.org/abs/2510.04682v1>
- **Submitted**: 2025-10-06 10:47:22
- **Topic Keywords**: rag
- **Reason**: This paper focuses on a parameter-efficient fine-tuning method called LoRA, which is not directly related to information retrieval, query understanding, or user behavior modeling. While it involves transfer learning, the context is in the context of large language models, which is somewhat tangential to the user's interests in search technologies and NLP.

#### Abstract
> Large Language Models (LLMs) are widely applied in real world scenarios, but
fine-tuning them comes with significant computational and storage costs.
Parameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these
costs, but the adapted parameters are dependent on the base model and cannot be
transferred across different backbones. One way to address this issue is
through knowledge distillation, but its effectiveness inherently depends on
training data. Recent work such as TransLoRA avoids this by generating
synthetic data, but this adds complexity because it requires training an
additional discriminator model. In this paper, we propose TiTok, a new
framework that enables effective LoRA Transplantation through Token-level
knowledge transfer. Specifically, TiTok captures task-relevant information
through a contrastive excess between a source model with and without LoRA. This
excess highlights informative tokens and enables selective filtering of
synthetic data, all without additional models or overhead. Through experiments
on three benchmarks across multiple transfer settings, our experiments show
that the proposed method is consistently effective, achieving average
performance gains of +4~8% compared to baselines overall.

### 38. Multi-Agent Tool-Integrated Policy Optimization

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zhanfeng Mo, Xingxuan Li, Yuntao Chen, Lidong Bing
- **URL**: <http://arxiv.org/abs/2510.04678v1>
- **Submitted**: 2025-10-06 10:44:04
- **Comment**: Work in progress
- **Topic Keywords**: rag
- **Reason**: This paper is primarily focused on multi-agent frameworks for large language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on reinforcement learning, the context is not aligned with your specific areas of interest such as query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large language models (LLMs) increasingly rely on multi-turn tool-integrated
planning for knowledge-intensive and complex reasoning tasks. Existing
implementations typically rely on a single agent, but they suffer from limited
context length and noisy tool responses. A natural solution is to adopt a
multi-agent framework with planner- and worker-agents to manage context.
However, no existing methods support effective reinforcement learning
post-training of tool-integrated multi-agent frameworks. To address this gap,
we propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which
enables distinct roles (planner and worker) to be trained within a single LLM
instance using role-specific prompts via reinforcement learning. MATPO is
derived from a principled credit assignment mechanism across planner and worker
rollouts. This design eliminates the need to deploy multiple LLMs, which would
be memory-intensive, while preserving the benefits of specialization.
Experiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently
outperforms single-agent baselines by an average of 18.38% relative improvement
in performance and exhibits greater robustness to noisy tool outputs. Our
findings highlight the effectiveness of unifying multiple agent roles within a
single LLM and provide practical insights for stable and efficient multi-agent
RL training.

### 39. GenQuest: An LLM-based Text Adventure Game for Language Learners

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Qiao Wang, Adnan Labib, Robert Swier, Michael Hofmeyr, Zheng Yuan
- **URL**: <http://arxiv.org/abs/2510.04498v1>
- **Submitted**: 2025-10-06 05:22:53
- **Comment**: Workshop on Wordplay: When Language Meets Games, EMNLP 2025
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests as it focuses on language learning and text adventure games, which do not align with your core areas of Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> GenQuest is a generative text adventure game that leverages Large Language
Models (LLMs) to facilitate second language learning through immersive,
interactive storytelling. The system engages English as a Foreign Language
(EFL) learners in a collaborative "choose-your-own-adventure" style narrative,
dynamically generated in response to learner choices. Game mechanics such as
branching decision points and story milestones are incorporated to maintain
narrative coherence while allowing learner-driven plot development. Key
pedagogical features include content generation tailored to each learner's
proficiency level, and a vocabulary assistant that provides in-context
explanations of learner-queried text strings, ranging from words and phrases to
sentences. Findings from a pilot study with university EFL students in China
indicate promising vocabulary gains and positive user perceptions. Also
discussed are suggestions from participants regarding the narrative length and
quality, and the request for multi-modal content such as illustrations.

### 40. MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Soo Yong Kim, Suin Cho, Vincent-Daniel Yun, Gyeongyeon Hwang
- **URL**: <http://arxiv.org/abs/2510.04477v1>
- **Submitted**: 2025-10-06 04:26:39
- **Topic Keywords**: rag
- **Reason**: This paper focuses on medical vision-language models and medical visual question answering, which is not directly related to the user's core research themes in Information Retrieval and Search technologies. Although it involves NLP and deep semantic understanding, the context and application are quite different from the user's interests.

#### Abstract
> Bridging clinical diagnostic reasoning with AI remains a central challenge in
medical imaging. We introduce MedCLM, an automated pipeline that converts
detection datasets into large-scale medical visual question answering (VQA)
data with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ
segmentation and structured rationales. These contextual signals enable medical
vision-language models to generate question-answer pairs with step-by-step
reasoning. To utilize this data effectively, we propose an Integrated
CoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes
for visual grounding, a Medium stage that encourages implicit localization, and
a Hard stage for weakly supervised reasoning. Experimental results demonstrate
that MedCLM attains state-of-the-art performance on several medical VQA
benchmarks, providing a scalable framework for developing clinically aligned
medical vision-language models.

### 41. Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Grace LeFevre, Qingcheng Zeng, Adam Leif, Jason Jewell, Denis Peskoff, Rob Voigt
- **URL**: <http://arxiv.org/abs/2510.04434v1>
- **Submitted**: 2025-10-06 02:04:42
- **Comment**: EMNLP 2025
- **Topic Keywords**: acl
- **Reason**: This paper is not directly related to the user's core research themes in Information Retrieval, Search technologies, or Natural Language Processing. While it touches on NLP, its focus is on the social impact and community aspects of NLP, rather than technical advancements or applications in IR or NLP.

#### Abstract
> The social impact of Natural Language Processing (NLP) is increasingly
important, with a rising community focus on initiatives related to NLP for
Social Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the
ACL Anthology address topics related to social good as defined by the UN
Sustainable Development Goals (Adauto et al., 2023). In this study, we take an
author- and venue-level perspective to map the landscape of NLP4SG, quantifying
the proportion of work addressing social good concerns both within and beyond
the ACL community, by both core ACL contributors and non-ACL authors. With this
approach we discover two surprising facts about the landscape of NLP4SG. First,
ACL authors are dramatically more likely to do work addressing social good
concerns when publishing in venues outside of ACL. Second, the vast majority of
publications using NLP techniques to address concerns of social good are done
by non-ACL authors in venues outside of ACL. We discuss the implications of
these findings on agenda-setting considerations for the ACL community related
to NLP4SG.

### 42. Paper2Video: Automatic Video Generation from Scientific Papers

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Zeyu Zhu, Kevin Qinghong Lin, Mike Zheng Shou
- **URL**: <http://arxiv.org/abs/2510.05096v1>
- **Submitted**: 2025-10-06 17:58:02
- **Comment**: 20 pages, 8 figures
- **Topic Keywords**: search
- **Reason**: This paper is not directly related to Information Retrieval or Search technologies, but rather focuses on video generation from scientific papers. While it involves multi-modal information and presentation, it does not address query understanding, ranking models, or user behavior modeling, which are core areas of interest.

#### Abstract
> Academic presentation videos have become an essential medium for research
communication, yet producing them remains highly labor-intensive, often
requiring hours of slide design, recording, and editing for a short 2 to 10
minutes video. Unlike natural video, presentation video generation involves
distinctive challenges: inputs from research papers, dense multi-modal
information (text, figures, tables), and the need to coordinate multiple
aligned channels such as slides, subtitles, speech, and human talker. To
address these challenges, we introduce PaperTalker, the first benchmark of 101
research papers paired with author-created presentation videos, slides, and
speaker metadata. We further design four tailored evaluation metrics--Meta
Similarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos
convey the paper's information to the audience. Building on this foundation, we
propose PaperTalker, the first multi-agent framework for academic presentation
video generation. It integrates slide generation with effective layout
refinement by a novel effective tree search visual choice, cursor grounding,
subtitling, speech synthesis, and talking-head rendering, while parallelizing
slide-wise generation for efficiency. Experiments on Paper2Video demonstrate
that the presentation videos produced by our approach are more faithful and
informative than existing baselines, establishing a practical step toward
automated and ready-to-use academic video generation. Our dataset, agent, and
code are available at https://github.com/showlab/Paper2Video.

### 43. Slm-mux: Orchestrating small language models for reasoning

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Chenyu Wang, Zishen Wan, Hao Kang, Emma Chen, Zhiqiang Xie, Tushar Krishna, Vijay Janapa Reddi, Yilun Du
- **URL**: <http://arxiv.org/abs/2510.05077v1>
- **Submitted**: 2025-10-06 17:49:58
- **Topic Keywords**: search
- **Reason**: This paper focuses on the orchestration of small language models for reasoning, which is a topic related to NLP. However, it does not directly address information retrieval, query understanding, or ranking models, which are core areas of your research interests.

#### Abstract
> With the rapid development of language models, the number of small language
models (SLMs) has grown significantly. Although they do not achieve
state-of-the-art accuracy, they are more efficient and often excel at specific
tasks. This raises a natural question: can multiple SLMs be orchestrated into a
system where each contributes effectively, achieving higher accuracy than any
individual model? Existing orchestration methods have primarily targeted
frontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To
address this gap, we propose a three-stage approach for orchestrating SLMs.
First, we introduce SLM-MUX, a multi-model architecture that effectively
coordinates multiple SLMs. Building on this, we develop two optimization
strategies: (i) a model selection search that identifies the most complementary
SLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our
approach delivers strong results: Compared to existing orchestration methods,
our approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0%
on GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and
GSM8K, and matches its performance on MATH. We further provide theoretical
analyses to substantiate the advantages of our method. In summary, we
demonstrate that SLMs can be effectively orchestrated into more accurate and
efficient systems through the proposed approach.

### 44. Imperceptible Jailbreaking against Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Kuofeng Gao, Yiming Li, Chao Du, Xin Wang, Xingjun Ma, Shu-Tao Xia, Tianyu Pang
- **URL**: <http://arxiv.org/abs/2510.05025v1>
- **Submitted**: 2025-10-06 17:03:50
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests as it focuses on 'jailbreaking' attacks on large language models, which is a topic in NLP, but not directly related to information retrieval, query understanding, ranking models, or user behavior modeling.

#### Abstract
> Jailbreaking attacks on the vision modality typically rely on imperceptible
adversarial perturbations, whereas attacks on the textual modality are
generally assumed to require visible modifications (e.g., non-semantic
suffixes). In this paper, we introduce imperceptible jailbreaks that exploit a
class of Unicode characters called variation selectors. By appending invisible
variation selectors to malicious questions, the jailbreak prompts appear
visually identical to original malicious questions on screen, while their
tokenization is "secretly" altered. We propose a chain-of-search pipeline to
generate such adversarial suffixes to induce harmful responses. Our experiments
show that our imperceptible jailbreaks achieve high attack success rates
against four aligned LLMs and generalize to prompt injection attacks, all
without producing any visible modifications in the written prompt. Our code is
available at https://github.com/sail-sg/imperceptible-jailbreaks.

### 45. Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Imran Mansha
- **URL**: <http://arxiv.org/abs/2510.05003v1>
- **Submitted**: 2025-10-06 16:42:11
- **Comment**: 6 pages, 2 figures. Submitted to arXiv for open access
- **Topic Keywords**: search
- **Reason**: This paper focuses on fine-tuning a large language model for medical chain-of-thought reasoning, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the context is medical AI systems and resource efficiency, which does not align with the user's interests.

#### Abstract
> Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated
remarkable reasoning abilities but require significant computational resources
for fine-tuning. This paper presents a resource-efficient fine-tuning approach
for LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating
under constrained GPU and memory settings. Using parameter-efficient tuning
techniques such as LoRA and QLoRA, we adapt the base model on publicly
available medical reasoning datasets. The model achieves improved reasoning
coherence and factual accuracy while reducing memory usage by up to 60%
compared to standard full fine-tuning. Experimental evaluation demonstrates
that lightweight adaptations can retain strong reasoning capability in medical
question-answering tasks. This work highlights practical strategies for
deploying LLMs in low-resource research environments and provides insights into
balancing efficiency and domain specialization for medical AI systems.

### 46. ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Shiwen Qin, Alexander Auras, Shay B. Cohen, Elliot J. Crowley, Michael Moeller, Linus Ericsson, Jovita Lukasik
- **URL**: <http://arxiv.org/abs/2510.04938v1>
- **Submitted**: 2025-10-06 15:43:36
- **Comment**: Our code is available at: https://github.com/shiwenqin/ONNX-Net
- **Topic Keywords**: search
- **Reason**: This paper focuses on neural architecture search and performance prediction, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it touches on deep semantic understanding, the context is different and the paper's contributions do not align with your primary focus on real-time relevance optimization in IR.

#### Abstract
> Neural architecture search (NAS) automates the design process of
high-performing architectures, but remains bottlenecked by expensive
performance evaluation. Most existing studies that achieve faster evaluation
are mostly tied to cell-based search spaces and graph encodings tailored to
those individual search spaces, limiting their flexibility and scalability when
applied to more expressive search spaces. In this work, we aim to close the gap
of individual search space restrictions and search space dependent network
representations. We present ONNX-Bench, a benchmark consisting of a collection
of neural networks in a unified format based on ONNX files. ONNX-Bench includes
all open-source NAS-bench-based neural networks, resulting in a total size of
more than 600k {architecture, accuracy} pairs. This benchmark allows creating a
shared neural network representation, ONNX-Net, able to represent any neural
architecture using natural language descriptions acting as an input to a
performance predictor. This text-based encoding can accommodate arbitrary layer
types, operation parameters, and heterogeneous topologies, enabling a single
surrogate to generalise across all neural architectures rather than being
confined to cell-based search spaces. Experiments show strong zero-shot
performance across disparate search spaces using only a small amount of
pretraining samples, enabling the unprecedented ability to evaluate any neural
network architecture instantly.

### 47. A Low-Resource Speech-Driven NLP Pipeline for Sinhala Dyslexia Assistance

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Peshala Perera, Deshan Sumanathilaka
- **URL**: <http://arxiv.org/abs/2510.04750v1>
- **Submitted**: 2025-10-06 12:28:57
- **Comment**: 11 pages, 4 figures, 3 tables
- **Topic Keywords**: search
- **Reason**: This paper focuses on a speech-driven NLP pipeline for Sinhala dyslexia assistance, which is not directly related to your core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve NLP, the specific application and language are not aligned with your interests.

#### Abstract
> Dyslexia in adults remains an under-researched and under-served area,
particularly in non-English-speaking contexts, despite its significant impact
on personal and professional lives. This work addresses that gap by focusing on
Sinhala, a low-resource language with limited tools for linguistic
accessibility. We present an assistive system explicitly designed for
Sinhala-speaking adults with dyslexia. The system integrates Whisper for
speech-to-text conversion, SinBERT, an open-sourced fine-tuned BERT model
trained for Sinhala to identify common dyslexic errors, and a combined mT5 and
Mistral-based model to generate corrected text. Finally, the output is
converted back to speech using gTTS, creating a complete multimodal feedback
loop. Despite the challenges posed by limited Sinhala-language datasets, the
system achieves 0.66 transcription accuracy and 0.7 correction accuracy with
0.65 overall system accuracy. These results demonstrate both the feasibility
and effectiveness of the approach. Ultimately, this work highlights the
importance of inclusive Natural Language Processing (NLP) technologies in
underrepresented languages and showcases a practical

### 48. AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large Language Models on Crystalline Materials

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Taoyuze Lv, Alexander Chen, Fengyu Xie, Chu Wu, Jeffrey Meng, Dongzhan Zhou, Bram Hoex, Zhicheng Zhong, Tong Xie
- **URL**: <http://arxiv.org/abs/2510.04704v2>
- **Submitted**: 2025-10-06 11:17:56
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests as it focuses on spatial reasoning in Large Language Models for crystalline materials, which is outside your areas of expertise in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Large Language Models (LLMs) excel at textual reasoning and are beginning to
develop spatial understanding, prompting the question of whether these
abilities can be combined for complex, domain-specific tasks. This question is
essential in fields like materials science, where deep understanding of 3D
atomic structures is fundamental. While initial studies have successfully
applied LLMs to tasks involving pure crystal generation or coordinate
understandings, a standardized benchmark to systematically evaluate their core
reasoning abilities across diverse atomic structures has been notably absent.
To address this gap, we introduce the AtomWorld benchmark to evaluate LLMs on
tasks based in Crystallographic Information Files (CIFs), a standard structure
representation format. These tasks, including structural editing, CIF
perception, and property-guided modeling, reveal a critical limitation: current
models, despite establishing promising baselines, consistently fail in
structural understanding and spatial reasoning. Our experiments show that these
models make frequent errors on structure modification tasks, and even in the
basic CIF format understandings, potentially leading to cumulative errors in
subsequent analysis and materials insights. By defining these standardized
tasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scale
modeling, crucial for accelerating materials research and automating scientific
workflows.

### 49. FT-MDT: Extracting Decision Trees from Medical Texts via a Novel Low-rank Adaptation Method

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Yuheng Li, Jiechao Gao, Wei Han, Wenwen Ouyang, Wei Zhu, Hui Yi Leong
- **URL**: <http://arxiv.org/abs/2510.04655v1>
- **Submitted**: 2025-10-06 09:59:55
- **Comment**: Accepted by EMNLP-2025 Industrial Track
- **Topic Keywords**: rank
- **Reason**: This paper focuses on extracting medical decision trees from texts using a novel low-rank adaptation method, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves text analysis, the context and application are specific to the medical domain and do not align with the user's interests in e-commerce or real-time relevance optimization.

#### Abstract
> Knowledge of the medical decision process, which can be modeled as medical
decision trees (MDTs), is critical to building clinical decision support
systems. However, current MDT construction methods rely heavily on
time-consuming and laborious manual annotation. To address this challenge, we
propose PI-LoRA (Path-Integrated LoRA), a novel low-rank adaptation method for
automatically extracting MDTs from clinical guidelines and textbooks. We
integrate gradient path information to capture synergistic effects between
different modules, enabling more effective and reliable rank allocation. This
framework ensures that the most critical modules receive appropriate rank
allocations while less important ones are pruned, resulting in a more efficient
and accurate model for extracting medical decision trees from clinical texts.
Extensive experiments on medical guideline datasets demonstrate that our
PI-LoRA method significantly outperforms existing parameter-efficient
fine-tuning approaches for the Text2MDT task, achieving better accuracy with
substantially reduced model complexity. The proposed method achieves
state-of-the-art results while maintaining a lightweight architecture, making
it particularly suitable for clinical decision support systems where
computational resources may be limited.

### 50. P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs

- **LLM Score**: 0
- **Keyword Score**: 2
- **Authors**: Shuai Zhao, Xinyi Wu, Shiqian Zhao, Xiaobao Wu, Zhongliang Guo, Yanhao Jia, Anh Tuan Luu
- **URL**: <http://arxiv.org/abs/2510.04503v1>
- **Submitted**: 2025-10-06 05:45:23
- **Topic Keywords**: rag
- **Reason**: This paper focuses on defending against backdoor attacks in Large Language Models (LLMs), which is a topic outside of the user's primary research interests in Information Retrieval and Search technologies. While it involves NLP, the context and application are not directly related to the user's core themes.

#### Abstract
> During fine-tuning, large language models (LLMs) are increasingly vulnerable
to data-poisoning backdoor attacks, which compromise their reliability and
trustworthiness. However, existing defense strategies suffer from limited
generalization: they only work on specific attack types or task settings. In
this study, we propose Poison-to-Poison (P2P), a general and effective backdoor
defense algorithm. P2P injects benign triggers with safe alternative labels
into a subset of training samples and fine-tunes the model on this re-poisoned
dataset by leveraging prompt-based learning. This enforces the model to
associate trigger-induced representations with safe outputs, thereby overriding
the effects of original malicious triggers. Thanks to this robust and
generalizable trigger-based fine-tuning, P2P is effective across task settings
and attack types. Theoretically and empirically, we show that P2P can
neutralize malicious backdoors while preserving task performance. We conduct
extensive experiments on classification, mathematical reasoning, and summary
generation tasks, involving multiple state-of-the-art LLMs. The results
demonstrate that our P2P algorithm significantly reduces the attack success
rate compared with baseline models. We hope that the P2P can serve as a
guideline for defending against backdoor attacks and foster the development of
a secure and trustworthy LLM community.

---

