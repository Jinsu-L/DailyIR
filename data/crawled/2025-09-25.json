[
    {
        "title": "Language Models that Think, Chat Better",
        "abstract": "Reinforcement learning with verifiable rewards (RLVR) improves language model\nreasoning by using rule-based rewards in verifiable domains such as mathematics\nand code. However, RLVR leads to limited generalization for open-ended tasks --\nsuch as writing outline essays or making meal plans -- where humans reason\nroutinely. This paper shows that the RLVR paradigm is effective beyond\nverifiable domains, and introduces **RL** with **M**odel-rewarded **T**hinking\n(**RLMT**) for general-purpose chat capabilities. Using diverse real-world\nprompts, RLMT requires LMs to generate long CoT reasoning before response, and\noptimizes them with online RL against a preference-based reward model used in\nRLHF. Across 40 training runs on Llama-3.1-8B and Qwen-2.5-7B (both base and\ninstruct) and multiple optimization algorithms (DPO, PPO, and GRPO), RLMT\nconsistently outperforms standard RLHF pipelines. This includes substantial\ngains of 3-7 points on three chat benchmarks (AlpacaEval2, WildBench, and\nArenaHardV2), along with 1-3 point improvements on other tasks like creative\nwriting and general knowledge. Our best 8B model surpasses GPT-4o in chat and\ncreative writing and rivals Claude-3.7-Sonnet (Thinking). RLMT can also be\napplied directly to base models without an SFT stage, akin to R1-Zero training.\nRemarkably, with only 7K prompts, Llama-3.1-8B base trained with our RLMT\nrecipe outperforms Llama-3.1-8B-Instruct post-trained with a complex\nmulti-staged pipeline with 25M+ examples. We close with qualitative and\nquantitative analyses of how trained models plan their responses. Our results\nrethink the post-training pipeline and call upon future work to understand and\nemploy thinking more broadly.",
        "url": "http://arxiv.org/abs/2509.20357v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20357v1",
        "arxiv_id": "2509.20357v1",
        "authors": [
            "Adithya Bhaskar",
            "Xi Ye",
            "Danqi Chen"
        ],
        "submitted": "2025-09-24 17:57:34",
        "source": "arxiv",
        "comment": "Preprint; we release our code and models publicly at\n  https://github.com/princeton-pli/RLMT"
    },
    {
        "title": "EmbeddingGemma: Powerful and Lightweight Text Representations",
        "abstract": "We introduce EmbeddingGemma, a new lightweight, open text embedding model\nbased on the Gemma 3 language model family. Our innovative training recipe\nstrategically captures knowledge from larger models via encoder-decoder\ninitialization and geometric embedding distillation. We improve model\nrobustness and expressiveness with a spread-out regularizer, and ensure\ngeneralizability by merging checkpoints from varied, optimized mixtures.\nEvaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual,\nEnglish, and code domains, EmbeddingGemma (300M) achieves state-of-the-art\nresults. Notably, it outperforms prior top models, both proprietary and open,\nwith fewer than 500M parameters, and provides performance comparable to models\ndouble its size, offering an exceptional performance-to-cost ratio. Remarkably,\nthis lead persists when quantizing model weights or truncating embedding\noutputs. This makes EmbeddingGemma particularly well-suited for low-latency and\nhigh-throughput use cases such as on-device applications. We provide ablation\nstudies exploring our key design choices. We release EmbeddingGemma to the\ncommunity to promote further research.",
        "url": "http://arxiv.org/abs/2509.20354v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20354v1",
        "arxiv_id": "2509.20354v1",
        "authors": [
            "Henrique Schechter Vera",
            "Sahil Dua",
            "Biao Zhang",
            "Daniel Salz",
            "Ryan Mullins",
            "Sindhu Raghuram Panyam",
            "Sara Smoot",
            "Iftekhar Naim",
            "Joe Zou",
            "Feiyang Chen",
            "Daniel Cer",
            "Alice Lisak",
            "Min Choi",
            "Lucas Gonzalez",
            "Omar Sanseviero",
            "Glenn Cameron",
            "Ian Ballantyne",
            "Kat Black",
            "Kaifeng Chen",
            "Weiyi Wang",
            "Zhe Li",
            "Gus Martins",
            "Jinhyuk Lee",
            "Mark Sherwood",
            "Juyeong Ji",
            "Renjie Wu",
            "Jingxiao Zheng",
            "Jyotinder Singh",
            "Abheesht Sharma",
            "Divya Sreepat",
            "Aashi Jain",
            "Adham Elarabawy",
            "AJ Co",
            "Andreas Doumanoglou",
            "Babak Samari",
            "Ben Hora",
            "Brian Potetz",
            "Dahun Kim",
            "Enrique Alfonseca",
            "Fedor Moiseev",
            "Feng Han",
            "Frank Palma Gomez",
            "Gustavo Hernández Ábrego",
            "Hesen Zhang",
            "Hui Hui",
            "Jay Han",
            "Karan Gill",
            "Ke Chen",
            "Koert Chen",
            "Madhuri Shanbhogue",
            "Michael Boratko",
            "Paul Suganthan",
            "Sai Meher Karthik Duddu",
            "Sandeep Mariserla",
            "Setareh Ariafar",
            "Shanfeng Zhang",
            "Shijie Zhang",
            "Simon Baumgartner",
            "Sonam Goenka",
            "Steve Qiu",
            "Tanmaya Dabral",
            "Trevor Walker",
            "Vikram Rao",
            "Waleed Khawaja",
            "Wenlei Zhou",
            "Xiaoqi Ren",
            "Ye Xia",
            "Yichang Chen",
            "Yi-Ting Chen",
            "Zhe Dong",
            "Zhongli Ding",
            "Francesco Visin",
            "Gaël Liu",
            "Jiageng Zhang",
            "Kathleen Kenealy",
            "Michelle Casbon",
            "Ravin Kumar",
            "Thomas Mesnard",
            "Zach Gleicher",
            "Cormac Brick",
            "Olivier Lacombe",
            "Adam Roberts",
            "Yunhsuan Sung",
            "Raphael Hoffmann",
            "Tris Warkentin",
            "Armand Joulin",
            "Tom Duerig",
            "Mojtaba Seyedhosseini"
        ],
        "submitted": "2025-09-24 17:56:51",
        "source": "arxiv",
        "comment": "18 pages. Models are available in HuggingFace (at\n  https://huggingface.co/collections/google/embeddinggemma-68b9ae3a72a82f0562a80dc4),\n  Kaggle (at https://www.kaggle.com/models/google/embeddinggemma/), and Vertex\n  AI (at\n  https://pantheon.corp.google.com/vertex-ai/publishers/google/model-garden/embeddinggemma)"
    },
    {
        "title": "Morphological Synthesizer for Ge'ez Language: Addressing Morphological Complexity and Resource Limitations",
        "abstract": "Ge'ez is an ancient Semitic language renowned for its unique alphabet. It\nserves as the script for numerous languages, including Tigrinya and Amharic,\nand played a pivotal role in Ethiopia's cultural and religious development\nduring the Aksumite kingdom era. Ge'ez remains significant as a liturgical\nlanguage in Ethiopia and Eritrea, with much of the national identity\ndocumentation recorded in Ge'ez. These written materials are invaluable primary\nsources for studying Ethiopian and Eritrean philosophy, creativity, knowledge,\nand civilization. Ge'ez has a complex morphological structure with rich\ninflectional and derivational morphology, and no usable NLP has been developed\nand published until now due to the scarcity of annotated linguistic data,\ncorpora, labeled datasets, and lexicons. Therefore, we propose a rule-based\nGe'ez morphological synthesizer to generate surface words from root words\naccording to the morphological structures of the language. We used 1,102 sample\nverbs, representing all verb morphological structures, to test and evaluate the\nsystem. The system achieves a performance of 97.4%, outperforming the baseline\nmodel and suggesting that future work should build a comprehensive system\nconsidering morphological variations of the language.\n  Keywords: Ge'ez, NLP, morphology, morphological synthesizer, rule-based",
        "url": "http://arxiv.org/abs/2509.20341v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20341v1",
        "arxiv_id": "2509.20341v1",
        "authors": [
            "Gebrearegawi Gebremariam",
            "Hailay Teklehaymanot",
            "Gebregewergs Mezgebe"
        ],
        "submitted": "2025-09-24 17:33:47",
        "source": "arxiv",
        "comment": "13 pages,2 images,7 tables"
    },
    {
        "title": "DRES: Benchmarking LLMs for Disfluency Removal",
        "abstract": "Disfluencies -- such as \"um,\" \"uh,\" interjections, parentheticals, and edited\nstatements -- remain a persistent challenge for speech-driven systems,\ndegrading accuracy in command interpretation, summarization, and conversational\nagents. We introduce DRES (Disfluency Removal Evaluation Suite), a controlled\ntext-level benchmark that establishes a reproducible semantic upper bound for\nthis task. DRES builds on human-annotated Switchboard transcripts, isolating\ndisfluency removal from ASR errors and acoustic variability. We systematically\nevaluate proprietary and open-source LLMs across scales, prompting strategies,\nand architectures. Our results reveal that (i) simple segmentation consistently\nimproves performance, even for long-context models; (ii) reasoning-oriented\nmodels tend to over-delete fluent tokens; and (iii) fine-tuning achieves near\nstate-of-the-art precision and recall but harms generalization abilities. We\nfurther present a set of LLM-specific error modes and offer nine practical\nrecommendations (R1-R9) for deploying disfluency removal in speech-driven\npipelines. DRES provides a reproducible, model-agnostic foundation for\nadvancing robust spoken-language systems.",
        "url": "http://arxiv.org/abs/2509.20321v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20321v1",
        "arxiv_id": "2509.20321v1",
        "authors": [
            "Maria Teleki",
            "Sai Janjur",
            "Haoran Liu",
            "Oliver Grabner",
            "Ketan Verma",
            "Thomas Docog",
            "Xiangjue Dong",
            "Lingfeng Shi",
            "Cong Wang",
            "Stephanie Birkelbach",
            "Jason Kim",
            "Yin Zhang",
            "James Caverlee"
        ],
        "submitted": "2025-09-24 17:08:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Z-Scores: A Metric for Linguistically Assessing Disfluency Removal",
        "abstract": "Evaluating disfluency removal in speech requires more than aggregate\ntoken-level scores. Traditional word-based metrics such as precision, recall,\nand F1 (E-Scores) capture overall performance but cannot reveal why models\nsucceed or fail. We introduce Z-Scores, a span-level linguistically-grounded\nevaluation metric that categorizes system behavior across distinct disfluency\ntypes (EDITED, INTJ, PRN). Our deterministic alignment module enables robust\nmapping between generated text and disfluent transcripts, allowing Z-Scores to\nexpose systematic weaknesses that word-level metrics obscure. By providing\ncategory-specific diagnostics, Z-Scores enable researchers to identify model\nfailure modes and design targeted interventions -- such as tailored prompts or\ndata augmentation -- yielding measurable performance improvements. A case study\nwith LLMs shows that Z-Scores uncover challenges with INTJ and PRN disfluencies\nhidden in aggregate F1, directly informing model refinement strategies.",
        "url": "http://arxiv.org/abs/2509.20319v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20319v1",
        "arxiv_id": "2509.20319v1",
        "authors": [
            "Maria Teleki",
            "Sai Janjur",
            "Haoran Liu",
            "Oliver Grabner",
            "Ketan Verma",
            "Thomas Docog",
            "Xiangjue Dong",
            "Lingfeng Shi",
            "Cong Wang",
            "Stephanie Birkelbach",
            "Jason Kim",
            "Yin Zhang",
            "James Caverlee"
        ],
        "submitted": "2025-09-24 17:02:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SIM-CoT: Supervised Implicit Chain-of-Thought",
        "abstract": "Implicit Chain-of-Thought (CoT) methods offer a token-efficient alternative\nto explicit CoT reasoning in Large Language Models (LLMs), but a persistent\nperformance gap has limited their adoption. We identify a core latent\ninstability issue when scaling the computational budget of implicit CoT: as the\nnumber of reasoning tokens increases, training often becomes unstable and\ncollapses. Our analysis shows that this instability arises from latent\nrepresentations becoming homogeneous and losing semantic diversity, caused by\ninsufficient step-level supervision in current implicit CoT methods. To address\nthis, we propose SIM-CoT, a plug-and-play training module that introduces\nstep-level supervision to stabilize and enrich the latent reasoning space.\nSIM-CoT employs an auxiliary decoder during training to align each implicit\ntoken with its corresponding explicit reasoning step, ensuring latent states\ncapture distinct and meaningful information. The auxiliary decoder is removed\nat inference, preserving the efficiency of implicit CoT with no added overhead.\nIt also provides interpretability by projecting each latent token onto an\nexplicit reasoning vocabulary, enabling per-step visualization and diagnosis.\nSIM-CoT significantly improves both in-domain accuracy and out-of-domain\nstability of implicit CoT methods, boosting Coconut by +8.2\\% on GPT-2 and CODI\nby +3.0\\% on LLaMA-3.1 8B. It further surpasses the explicit CoT baseline on\nGPT-2 by 2.1\\% with 2.3$\\times$ greater token efficiency, while closing the\nperformance gap on larger models like LLaMA-3.1 8B. Code:\nhttps://github.com/InternLM/SIM-CoT",
        "url": "http://arxiv.org/abs/2509.20317v2",
        "pdf_url": "http://arxiv.org/pdf/2509.20317v2",
        "arxiv_id": "2509.20317v2",
        "authors": [
            "Xilin Wei",
            "Xiaoran Liu",
            "Yuhang Zang",
            "Xiaoyi Dong",
            "Yuhang Cao",
            "Jiaqi Wang",
            "Xipeng Qiu",
            "Dahua Lin"
        ],
        "submitted": "2025-09-24 17:01:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Multilingual Hope Speech Detection: A Comparative Study of Logistic Regression, mBERT, and XLM-RoBERTa with Active Learning",
        "abstract": "Hope speech language that fosters encouragement and optimism plays a vital\nrole in promoting positive discourse online. However, its detection remains\nchallenging, especially in multilingual and low-resource settings. This paper\npresents a multilingual framework for hope speech detection using an active\nlearning approach and transformer-based models, including mBERT and\nXLM-RoBERTa. Experiments were conducted on datasets in English, Spanish,\nGerman, and Urdu, including benchmark test sets from recent shared tasks. Our\nresults show that transformer models significantly outperform traditional\nbaselines, with XLM-RoBERTa achieving the highest overall accuracy.\nFurthermore, our active learning strategy maintained strong performance even\nwith small annotated datasets. This study highlights the effectiveness of\ncombining multilingual transformers with data-efficient training strategies for\nhope speech detection.",
        "url": "http://arxiv.org/abs/2509.20315v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20315v1",
        "arxiv_id": "2509.20315v1",
        "authors": [
            "T. O. Abiola",
            "K. D. Abiodun",
            "O. E. Olumide",
            "O. O. Adebanji",
            "O. Hiram Calvo",
            "Grigori Sidorov"
        ],
        "submitted": "2025-09-24 16:54:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Feeding Two Birds or Favoring One? Adequacy-Fluency Tradeoffs in Evaluation and Meta-Evaluation of Machine Translation",
        "abstract": "We investigate the tradeoff between adequacy and fluency in machine\ntranslation. We show the severity of this tradeoff at the evaluation level and\nanalyze where popular metrics fall within it. Essentially, current metrics\ngenerally lean toward adequacy, meaning that their scores correlate more\nstrongly with the adequacy of translations than with fluency. More importantly,\nwe find that this tradeoff also persists at the meta-evaluation level, and that\nthe standard WMT meta-evaluation favors adequacy-oriented metrics over\nfluency-oriented ones. We show that this bias is partially attributed to the\ncomposition of the systems included in the meta-evaluation datasets. To control\nthis bias, we propose a method that synthesizes translation systems in\nmeta-evaluation. Our findings highlight the importance of understanding this\ntradeoff in meta-evaluation and its impact on metric rankings.",
        "url": "http://arxiv.org/abs/2509.20287v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20287v1",
        "arxiv_id": "2509.20287v1",
        "authors": [
            "Behzad Shayegh",
            "Jan-Thorsten Peter",
            "David Vilar",
            "Tobias Domhan",
            "Juraj Juraska",
            "Markus Freitag",
            "Lili Mou"
        ],
        "submitted": "2025-09-24 16:21:37",
        "source": "arxiv",
        "comment": "Accepted by Tenth Conference on Machine Translation (WMT25)"
    },
    {
        "title": "Instruction Boundary: Quantifying Biases in LLM Reasoning under Various Coverage",
        "abstract": "Large-language-model (LLM) reasoning has long been regarded as a powerful\ntool for problem solving across domains, providing non-experts with valuable\nadvice. However, their limitations - especially those stemming from prompt\ndesign - remain underexplored. Because users may supply biased or incomplete\nprompts - often unintentionally - LLMs can be misled, undermining reliability\nand creating risks. We refer to this vulnerability as the Instruction Boundary.\nTo investigate the phenomenon, we distill it into eight concrete facets and\nintroduce BiasDetector, a framework that measures biases arising from three\ninstruction types: complete, redundant, and insufficient. We evaluate several\nmainstream LLMs and find that, despite high headline accuracy, substantial\nbiases persist in many downstream tasks as a direct consequence of prompt\ncoverage. Our empirical study confirms that LLM reasoning reliability can still\nbe significantly improved. We analyze the practical impact of these biases and\noutline mitigation strategies. Our findings underscore the need for developers\nto tackle biases and for users to craft options carefully.",
        "url": "http://arxiv.org/abs/2509.20278v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20278v1",
        "arxiv_id": "2509.20278v1",
        "authors": [
            "Zipeng Ling",
            "Yuehao Tang",
            "Chen Huang",
            "Shuliang Liu",
            "Gaoyang Jiang",
            "Shenghong Fu",
            "Junqi Yang",
            "Yao Wan",
            "Jiawan Zhang",
            "Kejia Huang",
            "Xuming Hu"
        ],
        "submitted": "2025-09-24 16:15:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Scan-do Attitude: Towards Autonomous CT Protocol Management using a Large Language Model Agent",
        "abstract": "Managing scan protocols in Computed Tomography (CT), which includes adjusting\nacquisition parameters or configuring reconstructions, as well as selecting\npostprocessing tools in a patient-specific manner, is time-consuming and\nrequires clinical as well as technical expertise. At the same time, we observe\nan increasing shortage of skilled workforce in radiology. To address this\nissue, a Large Language Model (LLM)-based agent framework is proposed to assist\nwith the interpretation and execution of protocol configuration requests given\nin natural language or a structured, device-independent format, aiming to\nimprove the workflow efficiency and reduce technologists' workload. The agent\ncombines in-context-learning, instruction-following, and structured toolcalling\nabilities to identify relevant protocol elements and apply accurate\nmodifications. In a systematic evaluation, experimental results indicate that\nthe agent can effectively retrieve protocol components, generate device\ncompatible protocol definition files, and faithfully implement user requests.\nDespite demonstrating feasibility in principle, the approach faces limitations\nregarding syntactic and semantic validity due to lack of a unified device API,\nand challenges with ambiguous or complex requests. In summary, the findings\nshow a clear path towards LLM-based agents for supporting scan protocol\nmanagement in CT imaging.",
        "url": "http://arxiv.org/abs/2509.20270v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20270v1",
        "arxiv_id": "2509.20270v1",
        "authors": [
            "Xingjian Kang",
            "Linda Vorberg",
            "Andreas Maier",
            "Alexander Katzmann",
            "Oliver Taubmann"
        ],
        "submitted": "2025-09-24 16:04:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Failure Modes of Maximum Entropy RLHF",
        "abstract": "In this paper, we show that Simple Preference Optimization (SimPO) can be\nderived as Maximum Entropy Reinforcement Learning with length-normalized\ntemperature, providing a theoretical foundation for this reference-free method.\nMotivated by SimPO's strong performance in offline preference optimization, we\ninvestigate whether Maximum Entropy RL can achieve similar results in online\nRLHF settings. Our experiments find that Maximum Entropy RL consistently\nexhibits overoptimization and unstable KL dynamics, even at very low learning\nrates. Unlike KL-constrained methods that maintain stable training, entropy\nregularization fails to prevent reward hacking and appears to correlate with\noveroptimization. Lastly, we discuss possible explanations for why SimPO\nsucceeds in offline settings while Maximum Entropy RL struggles in online\nscenarios. Our findings suggest that reference-free approaches may face\ndistinct challenges when applied to online or offline preference learning.",
        "url": "http://arxiv.org/abs/2509.20265v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20265v1",
        "arxiv_id": "2509.20265v1",
        "authors": [
            "Ömer Veysel Çağatan",
            "Barış Akgün"
        ],
        "submitted": "2025-09-24 15:52:36",
        "source": "arxiv",
        "comment": "26 pages, 9 figures"
    },
    {
        "title": "Into the Void: Understanding Online Health Information in Low-Web Data Languages",
        "abstract": "Data voids--areas of the internet where reliable information is scarce or\nabsent--pose significant challenges to online health information seeking,\nparticularly for users operating in low-web data languages. These voids are\nincreasingly encountered not on traditional search engines alone, but on social\nmedia platforms, which have gradually morphed into informal search engines for\nmillions of people. In this paper, we introduce the phenomenon of data\nhorizons: a critical boundary where algorithmic structures begin to degrade the\nrelevance and reliability of search results. Unlike the core of a data void,\nwhich is often exploited by bad actors to spread misinformation, the data\nhorizon marks the critical space where systemic factors, such as linguistic\nunderrepresentation, algorithmic amplification, and socio-cultural mismatch,\ncreate conditions of informational instability. Focusing on Tigrinya and\nAmharic as languages of study, we evaluate (1) the common characteristics of\nsearch results for health queries, (2) the quality and credibility of health\ninformation, and (3) characteristics of search results that diverge from their\nqueries. We find that search results for health queries in low-web data\nlanguages may not always be in the language of search and may be dominated by\nnutritional and religious advice. We show that search results that diverge from\ntheir queries in low-resourced languages are due to algorithmic failures,\n(un)intentional manipulation, or active manipulation by content creators. We\nuse our findings to illustrate how a data horizon manifests under several\ninteracting constraints on information availability.",
        "url": "http://arxiv.org/abs/2509.20245v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20245v1",
        "arxiv_id": "2509.20245v1",
        "authors": [
            "Hellina Hailu Nigatu",
            "Nuredin Ali Abdelkadir",
            "Fiker Tewelde",
            "Stevie Chancellor",
            "Daricia Wilkinson"
        ],
        "submitted": "2025-09-24 15:35:01",
        "source": "arxiv",
        "comment": "Accepted to AIES 2025"
    },
    {
        "title": "Investigating the Representation of Backchannels and Fillers in Fine-tuned Language Models",
        "abstract": "Backchannels and fillers are important linguistic expressions in dialogue,\nbut are under-represented in modern transformer-based language models (LMs).\nOur work studies the representation of them in language models using three\nfine-tuning strategies. The models are trained on three dialogue corpora in\nEnglish and Japanese, where backchannels and fillers are preserved and\nannotated, to investigate how fine-tuning can help LMs learn their\nrepresentations. We first apply clustering analysis to the learnt\nrepresentation of backchannels and fillers, and have found increased silhouette\nscores in representations from fine-tuned models, which suggests that\nfine-tuning enables LMs to distinguish the nuanced semantic variation in\ndifferent backchannel and filler use. We also use natural language generation\n(NLG) metrics to confirm that the utterances generated by fine-tuned language\nmodels resemble human-produced utterances more closely. Our findings suggest\nthe potentials of transforming general LMs into conversational LMs that are\nmore capable of producing human-like languages adequately.",
        "url": "http://arxiv.org/abs/2509.20237v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20237v1",
        "arxiv_id": "2509.20237v1",
        "authors": [
            "Yu Wang",
            "Leyi Lao",
            "Langchu Huang",
            "Gabriel Skantze",
            "Yang Xu",
            "Hendrik Buschmeier"
        ],
        "submitted": "2025-09-24 15:27:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Muse-it: A Tool for Analyzing Music Discourse on Reddit",
        "abstract": "Music engagement spans diverse interactions with music, from selection and\nemotional response to its impact on behavior, identity, and social connections.\nSocial media platforms provide spaces where such engagement can be observed in\nnatural, unprompted conversations. Advances in natural language processing\n(NLP) and big data analytics make it possible to analyze these discussions at\nscale, extending music research to broader contexts. Reddit, in particular,\noffers anonymity that encourages diverse participation and yields rich\ndiscourse on music in ecological settings. Yet the scale of this data requires\ntools to extract, process, and analyze it effectively. We present Muse-it, a\nplatform that retrieves comprehensive Reddit data centered on user-defined\nqueries. It aggregates posts from across subreddits, supports topic modeling,\ntemporal trend analysis, and clustering, and enables efficient study of\nlarge-scale discourse. Muse-it also identifies music-related hyperlinks (e.g.,\nSpotify), retrieves track-level metadata such as artist, album, release date,\ngenre, popularity, and lyrics, and links these to the discussions. An\ninteractive interface provides dynamic visualizations of the collected data.\nMuse-it thus offers an accessible way for music researchers to gather and\nanalyze big data, opening new avenues for understanding music engagement as it\nnaturally unfolds online.",
        "url": "http://arxiv.org/abs/2509.20228v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20228v1",
        "arxiv_id": "2509.20228v1",
        "authors": [
            "Jatin Agarwala",
            "George Paul",
            "Nemani Harsha Vardhan",
            "Vinoo Alluri"
        ],
        "submitted": "2025-09-24 15:22:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Multimodal Representation-disentangled Information Bottleneck for Multimodal Recommendation",
        "abstract": "Multimodal data has significantly advanced recommendation systems by\nintegrating diverse information sources to model user preferences and item\ncharacteristics. However, these systems often struggle with redundant and\nirrelevant information, which can degrade performance. Most existing methods\neither fuse multimodal information directly or use rigid architectural\nseparation for disentanglement, failing to adequately filter noise and model\nthe complex interplay between modalities. To address these challenges, we\npropose a novel framework, the Multimodal Representation-disentangled\nInformation Bottleneck (MRdIB). Concretely, we first employ a Multimodal\nInformation Bottleneck to compress the input representations, effectively\nfiltering out task-irrelevant noise while preserving rich semantic information.\nThen, we decompose the information based on its relationship with the\nrecommendation target into unique, redundant, and synergistic components. We\nachieve this decomposition with a series of constraints: a unique information\nlearning objective to preserve modality-unique signals, a redundant information\nlearning objective to minimize overlap, and a synergistic information learning\nobjective to capture emergent information. By optimizing these objectives,\nMRdIB guides a model to learn more powerful and disentangled representations.\nExtensive experiments on several competitive models and three benchmark\ndatasets demonstrate the effectiveness and versatility of our MRdIB in\nenhancing multimodal recommendation.",
        "url": "http://arxiv.org/abs/2509.20225v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20225v1",
        "arxiv_id": "2509.20225v1",
        "authors": [
            "Hui Wang",
            "Jinghui Qin",
            "Wushao Wen",
            "Qingling Li",
            "Shanshan Zhong",
            "Zhongzhan Huang"
        ],
        "submitted": "2025-09-24 15:18:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Low-Resource English-Tigrinya MT: Leveraging Multilingual Models, Custom Tokenizers, and Clean Evaluation Benchmarks",
        "abstract": "Despite advances in Neural Machine Translation (NMT), low-resource languages\nlike Tigrinya remain underserved due to persistent challenges, including\nlimited corpora, inadequate tokenization strategies, and the lack of\nstandardized evaluation benchmarks. This paper investigates transfer learning\ntechniques using multilingual pretrained models to enhance translation quality\nfor morphologically rich, low-resource languages. We propose a refined approach\nthat integrates language-specific tokenization, informed embedding\ninitialization, and domain-adaptive fine-tuning. To enable rigorous assessment,\nwe construct a high-quality, human-aligned English-Tigrinya evaluation dataset\ncovering diverse domains. Experimental results demonstrate that transfer\nlearning with a custom tokenizer substantially outperforms zero-shot baselines,\nwith gains validated by BLEU, chrF, and qualitative human evaluation.\nBonferroni correction is applied to ensure statistical significance across\nconfigurations. Error analysis reveals key limitations and informs targeted\nrefinements. This study underscores the importance of linguistically aware\nmodeling and reproducible benchmarks in bridging the performance gap for\nunderrepresented languages. Resources are available at\nhttps://github.com/hailaykidu/MachineT_TigEng\n  and https://huggingface.co/Hailay/MachineT_TigEng",
        "url": "http://arxiv.org/abs/2509.20209v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20209v1",
        "arxiv_id": "2509.20209v1",
        "authors": [
            "Hailay Kidu Teklehaymanot",
            "Gebrearegawi Gidey",
            "Wolfgang Nejdl"
        ],
        "submitted": "2025-09-24 15:02:57",
        "source": "arxiv",
        "comment": "This submission is 8 pages long, includes 4 tables, and contains all\n  required conference details"
    },
    {
        "title": "Play by the Type Rules: Inferring Constraints for LLM Functions in Declarative Programs",
        "abstract": "Integrating LLM powered operators in declarative query languages allows for\nthe combination of cheap and interpretable functions with powerful,\ngeneralizable language model reasoning. However, in order to benefit from the\noptimized execution of a database query language like SQL, generated outputs\nmust align with the rules enforced by both type checkers and database contents.\nCurrent approaches address this challenge with orchestrations consisting of\nmany LLM-based post-processing calls to ensure alignment between generated\noutputs and database values, introducing performance bottlenecks. We perform a\nstudy on the ability of various sized open-source language models to both parse\nand execute functions within a query language based on SQL, showing that small\nlanguage models can excel as function executors over hybrid data sources. Then,\nwe propose an efficient solution to enforce the well-typedness of LLM\nfunctions, demonstrating 7% accuracy improvement on a multi-hop question\nanswering dataset with 53% improvement in latency over comparable solutions. We\nmake our implementation available at https://github.com/parkervg/blendsql",
        "url": "http://arxiv.org/abs/2509.20208v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20208v1",
        "arxiv_id": "2509.20208v1",
        "authors": [
            "Parker Glenn",
            "Alfy Samuel",
            "Daben Liu"
        ],
        "submitted": "2025-09-24 15:02:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Thinking Augmented Pre-training",
        "abstract": "This paper introduces a simple and scalable approach to improve the data\nefficiency of large language model (LLM) training by augmenting existing text\ndata with thinking trajectories. The compute for pre-training LLMs has been\ngrowing at an unprecedented rate, while the availability of high-quality data\nremains limited. Consequently, maximizing the utility of available data\nconstitutes a significant research challenge. A primary impediment is that\ncertain high-quality tokens are difficult to learn given a fixed model\ncapacity, as the underlying rationale for a single token can be exceptionally\ncomplex and deep. To address this issue, we propose Thinking augmented\nPre-Training (TPT), a universal methodology that augments text with\nautomatically generated thinking trajectories. Such augmentation effectively\nincreases the volume of the training data and makes high-quality tokens more\nlearnable through step-by-step reasoning and decomposition. We apply TPT across\ndiverse training configurations up to $100$B tokens, encompassing pre-training\nwith both constrained and abundant data, as well as mid-training from strong\nopen-source checkpoints. Experimental results indicate that our method\nsubstantially improves the performance of LLMs across various model sizes and\nfamilies. Notably, TPT enhances the data efficiency of LLM pre-training by a\nfactor of $3$. For a $3$B parameter model, it improves the post-training\nperformance by over $10\\%$ on several challenging reasoning benchmarks.",
        "url": "http://arxiv.org/abs/2509.20186v2",
        "pdf_url": "http://arxiv.org/pdf/2509.20186v2",
        "arxiv_id": "2509.20186v2",
        "authors": [
            "Liang Wang",
            "Nan Yang",
            "Shaohan Huang",
            "Li Dong",
            "Furu Wei"
        ],
        "submitted": "2025-09-24 14:45:13",
        "source": "arxiv",
        "comment": "19 pages"
    },
    {
        "title": "Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI",
        "abstract": "We present Federation of Agents (FoA), a distributed orchestration framework\nthat transforms static multi-agent coordination into dynamic, capability-driven\ncollaboration. FoA introduces Versioned Capability Vectors (VCVs):\nmachine-readable profiles that make agent capabilities searchable through\nsemantic embeddings, enabling agents to advertise their capabilities, cost, and\nlimitations. Our aarchitecturecombines three key innovations: (1) semantic\nrouting that matches tasks to agents over sharded HNSW indices while enforcing\noperational constraints through cost-biased optimization, (2) dynamic task\ndecomposition where compatible agents collaboratively break down complex tasks\ninto DAGs of subtasks through consensus-based merging, and (3) smart clustering\nthat groups agents working on similar subtasks into collaborative channels for\nk-round refinement before synthesis. Built on top of MQTT,s publish-subscribe\nsemantics for scalable message passing, FoA achieves sub-linear complexity\nthrough hierarchical capability matching and efficient index maintenance.\nEvaluation on HealthBench shows 13x improvements over single-model baselines,\nwith clustering-enhanced laboration particularly effective for complex\nreasoning tasks requiring multiple perspectives. The system scales horizontally\nwhile maintaining consistent performance, demonstrating that semantic\norchestration with structured collaboration can unlock the collective\nintelligence of heterogeneous federations of AI agents.",
        "url": "http://arxiv.org/abs/2509.20175v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20175v1",
        "arxiv_id": "2509.20175v1",
        "authors": [
            "Lorenzo Giusti",
            "Ole Anton Werner",
            "Riccardo Taiello",
            "Matilde Carvalho Costa",
            "Emre Tosun",
            "Andrea Protani",
            "Marc Molina",
            "Rodrigo Lopes de Almeida",
            "Paolo Cacace",
            "Diogo Reis Santos",
            "Luigi Serio"
        ],
        "submitted": "2025-09-24 14:38:06",
        "source": "arxiv",
        "comment": "18 pages, 4 figures"
    },
    {
        "title": "Probing Gender Bias in Multilingual LLMs: A Case Study of Stereotypes in Persian",
        "abstract": "Multilingual Large Language Models (LLMs) are increasingly used worldwide,\nmaking it essential to ensure they are free from gender bias to prevent\nrepresentational harm. While prior studies have examined such biases in\nhigh-resource languages, low-resource languages remain understudied. In this\npaper, we propose a template-based probing methodology, validated against\nreal-world data, to uncover gender stereotypes in LLMs. As part of this\nframework, we introduce the Domain-Specific Gender Skew Index (DS-GSI), a\nmetric that quantifies deviations from gender parity. We evaluate four\nprominent models, GPT-4o mini, DeepSeek R1, Gemini 2.0 Flash, and Qwen QwQ 32B,\nacross four semantic domains, focusing on Persian, a low-resource language with\ndistinct linguistic features. Our results show that all models exhibit gender\nstereotypes, with greater disparities in Persian than in English across all\ndomains. Among these, sports reflect the most rigid gender biases. This study\nunderscores the need for inclusive NLP practices and provides a framework for\nassessing bias in other low-resource languages.",
        "url": "http://arxiv.org/abs/2509.20168v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20168v1",
        "arxiv_id": "2509.20168v1",
        "authors": [
            "Ghazal Kalhor",
            "Behnam Bahrak"
        ],
        "submitted": "2025-09-24 14:34:17",
        "source": "arxiv",
        "comment": "Accepted and forthcoming at the Widening Natural Language Processing\n  Workshop (WiNLP 2025) at EMNLP 2025"
    },
    {
        "title": "Embedding Domain Knowledge for Large Language Models via Reinforcement Learning from Augmented Generation",
        "abstract": "Large language models (LLMs) often exhibit limited performance on\ndomain-specific tasks due to the natural disproportionate representation of\nspecialized information in their training data and the static nature of these\ndatasets. Knowledge scarcity and temporal lag create knowledge gaps for domain\napplications. While post-training on domain datasets can embed knowledge into\nmodels, existing approaches have some limitations. Continual Pre-Training (CPT)\ntreats all tokens in domain documents with equal importance, failing to\nprioritize critical knowledge points, while supervised fine-tuning (SFT) with\nquestion-answer pairs struggles to develop the coherent knowledge structures\nnecessary for complex reasoning tasks. To address these challenges, we propose\nReinforcement Learning from Augmented Generation (RLAG). Our approach\niteratively cycles between sampling generations and optimizing the model\nthrough calculated rewards, effectively embedding critical and contextually\ncoherent domain knowledge. We select generated outputs with the highest log\nprobabilities as the sampling result, then compute three tailored reward\nmetrics to guide the optimization process. To comprehensively evaluate domain\nexpertise, we assess answer accuracy and the rationality of explanations\ngenerated for correctly answered questions. Experimental results across\nmedical, legal, astronomy, and current events datasets demonstrate that our\nproposed method significantly outperforms baseline approaches. Our code and\ndata are open sourced at https://github.com/ChaojunNie/RLAG.",
        "url": "http://arxiv.org/abs/2509.20162v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20162v1",
        "arxiv_id": "2509.20162v1",
        "authors": [
            "Chaojun Nie",
            "Jun Zhou",
            "Guanxiang Wang",
            "Shisong Wud",
            "Zichen Wang"
        ],
        "submitted": "2025-09-24 14:30:16",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Digital Signal Processing from Classical Coherent Systems to Continuous-Variable QKD: A Review of Cross-Domain Techniques, Applications, and Challenges",
        "abstract": "This systematic review investigates the application of digital signal\nprocessing (DSP) techniques -- originally developed for coherent optical\ncommunication systems to continuous-variable quantum key distribution (CV-QKD).\nThe convergence of these domains has enabled significant advances in CV-QKD\nperformance, particularly in phase synchronization, polarization tracking, and\nexcess noise mitigation. To provide a comprehensive and reproducible synthesis\nof this emerging field, we employed the APISSER methodology, a task-oriented\nframework adapted from the PRISMA protocol. A structured search across IEEE\nXplore and Web of Science databases (2021-2025) yielded 220 relevant\npublications, which were screened, classified, and analyzed to address six\nresearch questions. Our findings highlight that many classical DSP algorithms,\nsuch as Kalman filtering, carrier recovery, adaptive equalization, and\nmachine-learning-assisted signal estimation, have been successfully adapted to\nthe quantum regime, often requiring modifications to meet security and noise\nconstraints. We also identify a range of recent DSP innovations in coherent\noptical communication systems with high potential for future CV-QKD\nintegration, including neural equalization, probabilistic shaping, and joint\nretiming-equalization filters. Despite these advances, challenges remain in\nachieving robust phase tracking under ultra-low Signal-to-Noise Ratio (SNR)\nconditions, real-time polarization compensation, and secure co-existence with\nclassical channels. This review maps current trends, technical barriers, and\nemerging opportunities at the intersection of signal processing for quantum and\nclassical communication, supporting the development of scalable and resilient\nCV-QKD systems.",
        "url": "http://arxiv.org/abs/2509.20141v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20141v1",
        "arxiv_id": "2509.20141v1",
        "authors": [
            "Davi Juvêncio Gomes de Sousa",
            "Caroline da Silva Morais Alves",
            "Valéria Loureiro da Silva",
            "Nelson Alves Ferreira Neto"
        ],
        "submitted": "2025-09-24 14:05:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Intelligent Algorithm Selection for Recommender Systems: Meta-Learning via in-depth algorithm feature engineering",
        "abstract": "The \"No Free Lunch\" theorem dictates that no single recommender algorithm is\noptimal for all users, creating a significant Algorithm Selection Problem.\nStandard meta-learning approaches aim to solve this by selecting an algorithm\nbased on user features, but treat the fundamentally diverse algorithms\nthemselves as equivalent, \"black-box\" choices. This thesis investigates the\nimpact of overcoming this limitation by engineering a comprehensive feature set\nto explicitly characterize the algorithms themselves. We combine static code\nmetrics, Abstract Syntax Tree properties, behavioral performance landmarks, and\nhigh-level conceptual features. We evaluate two meta-learners across five\ndatasets: a baseline using only user features and our proposed model using both\nuser and algorithm features. Our results show that the meta-learner augmented\nwith algorithm features achieves an average NDCG@10 of 0.143, a statistically\nsignificant improvement of 11.7% over the Single Best Algorithm baseline\n(0.128). However, we found that the inclusion of algorithm features did not\nlead to an improvement in overall NDCG@10 over the meta learner using only user\nfeatures (0.144). While adding algorithm features to the meta-learner did\nimprove its Top-1 selection accuracy (+16.1%), this was counterbalanced by\nleading to a lower Top-3 accuracy (-10.7%). We conclude that for the per-user\nalgorithm selection task in recommender systems, the predictive power of user\nfeatures is overwhelmingly dominant. While algorithm features improve selection\nprecision, unlocking their potential to boost overall performance remains a\nnon-trivial challenge.",
        "url": "http://arxiv.org/abs/2509.20134v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20134v1",
        "arxiv_id": "2509.20134v1",
        "authors": [
            "Jarne Mathi Decker"
        ],
        "submitted": "2025-09-24 14:00:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Less is More: The Effectiveness of Compact Typological Language Representations",
        "abstract": "Linguistic feature datasets such as URIEL+ are valuable for modelling\ncross-lingual relationships, but their high dimensionality and sparsity,\nespecially for low-resource languages, limit the effectiveness of distance\nmetrics. We propose a pipeline to optimize the URIEL+ typological feature space\nby combining feature selection and imputation, producing compact yet\ninterpretable typological representations. We evaluate these feature subsets on\nlinguistic distance alignment and downstream tasks, demonstrating that\nreduced-size representations of language typology can yield more informative\ndistance metrics and improve performance in multilingual NLP applications.",
        "url": "http://arxiv.org/abs/2509.20129v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20129v1",
        "arxiv_id": "2509.20129v1",
        "authors": [
            "York Hay Ng",
            "Phuong Hanh Hoang",
            "En-Shiun Annie Lee"
        ],
        "submitted": "2025-09-24 13:55:56",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 Main Conference"
    },
    {
        "title": "Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving",
        "abstract": "End-to-End (E2E) solutions have emerged as a mainstream approach for\nautonomous driving systems, with Vision-Language-Action (VLA) models\nrepresenting a new paradigm that leverages pre-trained multimodal knowledge\nfrom Vision-Language Models (VLMs) to interpret and interact with complex\nreal-world environments. However, these methods remain constrained by the\nlimitations of imitation learning, which struggles to inherently encode\nphysical rules during training. Existing approaches often rely on complex\nrule-based post-refinement, employ reinforcement learning that remains largely\nlimited to simulation, or utilize diffusion guidance that requires\ncomputationally expensive gradient calculations. To address these challenges,\nwe introduce ReflectDrive, a novel learning-based framework that integrates a\nreflection mechanism for safe trajectory generation via discrete diffusion. We\nfirst discretize the two-dimensional driving space to construct an action\ncodebook, enabling the use of pre-trained Diffusion Language Models for\nplanning tasks through fine-tuning. Central to our approach is a safety-aware\nreflection mechanism that performs iterative self-correction without gradient\ncomputation. Our method begins with goal-conditioned trajectory generation to\nmodel multi-modal driving behaviors. Based on this, we apply local search\nmethods to identify unsafe tokens and determine feasible solutions, which then\nserve as safe anchors for inpainting-based regeneration. Evaluated on the\nNAVSIM benchmark, ReflectDrive demonstrates significant advantages in\nsafety-critical trajectory generation, offering a scalable and reliable\nsolution for autonomous driving systems.",
        "url": "http://arxiv.org/abs/2509.20109v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20109v1",
        "arxiv_id": "2509.20109v1",
        "authors": [
            "Pengxiang Li",
            "Yinan Zheng",
            "Yue Wang",
            "Huimin Wang",
            "Hang Zhao",
            "Jingjing Liu",
            "Xianyuan Zhan",
            "Kun Zhan",
            "Xianpeng Lang"
        ],
        "submitted": "2025-09-24 13:35:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Cascade! Human in the loop shortcomings can increase the risk of failures in recommender systems",
        "abstract": "Recommender systems are among the most commonly deployed systems today.\nSystems design approaches to AI-powered recommender systems have done well to\nurge recommender system developers to follow more intentional data collection,\ncuration, and management procedures. So too has the \"human-in-the-loop\"\nparadigm been widely adopted, primarily to address the issue of accountability.\nHowever, in this paper, we take the position that human oversight in\nrecommender system design also entails novel risks that have yet to be fully\ndescribed. These risks are \"codetermined\" by the information context in which\nsuch systems are often deployed. Furthermore, new knowledge of the shortcomings\nof \"human-in-the-loop\" practices to deliver meaningful oversight of other AI\nsystems suggest that they may also be inadequate for achieving socially\nresponsible recommendations. We review how the limitations of human oversight\nmay increase the chances of a specific kind of failure: a \"cascade\" or\n\"compound\" failure. We then briefly explore how the unique dynamics of three\ncommon deployment contexts can make humans in the loop more likely to fail in\ntheir oversight duties. We then conclude with two recommendations.",
        "url": "http://arxiv.org/abs/2509.20099v2",
        "pdf_url": "http://arxiv.org/pdf/2509.20099v2",
        "arxiv_id": "2509.20099v2",
        "authors": [
            "Wm. Matthew Kennedy",
            "Nishanshi Shukla",
            "Cigdem Patlak",
            "Blake Chambers",
            "Theodora Skeadas",
            "Tuesday",
            "Kingsley Owadara",
            "Aayush Dhanotiya"
        ],
        "submitted": "2025-09-24 13:23:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Integrated Framework for LLM Evaluation with Answer Generation",
        "abstract": "Reliable evaluation of large language models is essential to ensure their\napplicability in practical scenarios. Traditional benchmark-based evaluation\nmethods often rely on fixed reference answers, limiting their ability to\ncapture important qualitative aspects of generated responses. To address these\nshortcomings, we propose an integrated evaluation framework called\n\\textit{self-refining descriptive evaluation with expert-driven diagnostics},\nSPEED, which utilizes specialized functional experts to perform comprehensive,\ndescriptive analyses of model outputs. Unlike conventional approaches, SPEED\nactively incorporates expert feedback across multiple dimensions, including\nhallucination detection, toxicity assessment, and lexical-contextual\nappropriateness. Experimental results demonstrate that SPEED achieves robust\nand consistent evaluation performance across diverse domains and datasets.\nAdditionally, by employing relatively compact expert models, SPEED demonstrates\nsuperior resource efficiency compared to larger-scale evaluators. These\nfindings illustrate that SPEED significantly enhances fairness and\ninterpretability in LLM evaluations, offering a promising alternative to\nexisting evaluation methodologies.",
        "url": "http://arxiv.org/abs/2509.20097v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20097v1",
        "arxiv_id": "2509.20097v1",
        "authors": [
            "Sujeong Lee",
            "Hayoung Lee",
            "Seongsoo Heo",
            "Wonik Choi"
        ],
        "submitted": "2025-09-24 13:20:37",
        "source": "arxiv",
        "comment": "16pages"
    },
    {
        "title": "Causal Understanding by LLMs: The Role of Uncertainty",
        "abstract": "Recent papers show LLMs achieve near-random accuracy in causal relation\nclassification, raising questions about whether such failures arise from\nlimited pretraining exposure or deeper representational gaps. We investigate\nthis under uncertainty-based evaluation, testing whether pretraining exposure\nto causal examples improves causal understanding >18K PubMed sentences -- half\nfrom The Pile corpus, half post-2024 -- across seven models\n(Pythia-1.4B/7B/12B, GPT-J-6B, Dolly-7B/12B, Qwen-7B). We analyze model\nbehavior through: (i) causal classification, where the model identifies causal\nrelationships in text, and (ii) verbatim memorization probing, where we assess\nwhether the model prefers previously seen causal statements over their\nparaphrases. Models perform four-way classification\n(direct/conditional/correlational/no-relationship) and select between originals\nand their generated paraphrases. Results show almost identical accuracy on\nseen/unseen sentences (p > 0.05), no memorization bias (24.8% original\nselection), and output distribution over the possible options is almost flat,\nwith entropic values near the maximum (1.35/1.39), confirming random guessing.\nInstruction-tuned models show severe miscalibration (Qwen: > 95% confidence,\n32.8% accuracy, ECE=0.49). Conditional relations induce highest entropy (+11%\nvs. direct). These findings suggest that failures in causal understanding arise\nfrom the lack of structured causal representation, rather than insufficient\nexposure to causal examples during pretraining.",
        "url": "http://arxiv.org/abs/2509.20088v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20088v1",
        "arxiv_id": "2509.20088v1",
        "authors": [
            "Oscar Lithgow-Serrano",
            "Vani Kanjirangat",
            "Alessandro Antonucci"
        ],
        "submitted": "2025-09-24 13:06:35",
        "source": "arxiv",
        "comment": "Accepted in second UncertaiNLP workshop at EMNLP 2025"
    },
    {
        "title": "OLaPh: Optimal Language Phonemizer",
        "abstract": "Phonemization, the conversion of text into phonemes, is a key step in\ntext-to-speech. Traditional approaches use rule-based transformations and\nlexicon lookups, while more advanced methods apply preprocessing techniques or\nneural networks for improved accuracy on out-of-domain vocabulary. However, all\nsystems struggle with names, loanwords, abbreviations, and homographs. This\nwork presents OLaPh (Optimal Language Phonemizer), a framework that combines\nlarge lexica, multiple NLP techniques, and compound resolution with a\nprobabilistic scoring function. Evaluations in German and English show improved\naccuracy over previous approaches, including on a challenging dataset. To\nfurther address unresolved cases, we train a large language model on\nOLaPh-generated data, which achieves even stronger generalization and\nperformance. Together, the framework and LLM improve phonemization consistency\nand provide a freely available resource for future research.",
        "url": "http://arxiv.org/abs/2509.20086v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20086v1",
        "arxiv_id": "2509.20086v1",
        "authors": [
            "Johannes Wirth"
        ],
        "submitted": "2025-09-24 13:05:09",
        "source": "arxiv",
        "comment": "5 pages, 1 figure, 3 tables"
    },
    {
        "title": "Can Constructions \"SCAN\" Compositionality ?",
        "abstract": "Sequence to Sequence models struggle at compositionality and systematic\ngeneralisation even while they excel at many other tasks. We attribute this\nlimitation to their failure to internalise constructions conventionalised form\nmeaning pairings that license productive recombination. Building on these\ninsights, we introduce an unsupervised procedure for mining\npseudo-constructions: variable-slot templates automatically extracted from\ntraining data. When applied to the SCAN dataset, our method yields large gains\nout-of-distribution splits: accuracy rises to 47.8 %on ADD JUMP and to 20.3% on\nAROUND RIGHT without any architectural changes or additional supervision. The\nmodel also attains competitive performance with? 40% of the original training\ndata, demonstrating strong data efAciency. Our findings highlight the promise\nof construction-aware preprocessing as an alternative to heavy architectural or\ntraining-regime interventions.",
        "url": "http://arxiv.org/abs/2509.20074v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20074v1",
        "arxiv_id": "2509.20074v1",
        "authors": [
            "Ganesh Katrapati",
            "Manish Shrivastava"
        ],
        "submitted": "2025-09-24 12:52:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training",
        "abstract": "Recent advances in large language models (LLMs) have attracted significant\ninterest in extending their capabilities to multimodal scenarios, particularly\nfor speech-to-speech conversational systems. However, existing multimodal\nmodels handling interleaved audio and text rely on autoregressive methods,\noverlooking that text depends on target-target relations whereas audio depends\nmainly on source-target relations. In this work, we propose Text-to-Talk (TtT),\na unified audio-text framework that integrates autoregressive (AR) text\ngeneration with non-autoregressive (NAR) audio diffusion in a single\nTransformer. By leveraging the any-order autoregressive property of absorbing\ndiscrete diffusion, our approach provides a unified training objective for text\nand audio. To support this hybrid generation paradigm, we design a\nmodality-aware attention mechanism that enforces causal decoding for text while\nallowing bidirectional modeling within audio spans, and further introduce three\ntraining strategies that reduce train-test discrepancies. During inference, TtT\nemploys block-wise diffusion to synthesize audio in parallel while flexibly\nhandling variable-length outputs. Extensive experiments across Audio-QA and ASR\ntasks demonstrate the effectiveness of our approach, with detailed ablation\nstudies validating each proposed component. We will open-source our models,\ndata and code to facilitate future research in this direction.",
        "url": "http://arxiv.org/abs/2509.20072v2",
        "pdf_url": "http://arxiv.org/pdf/2509.20072v2",
        "arxiv_id": "2509.20072v2",
        "authors": [
            "Tianqiao Liu",
            "Xueyi Li",
            "Hao Wang",
            "Haoxuan Li",
            "Zhichao Chen",
            "Weiqi Luo",
            "Zitao Liu"
        ],
        "submitted": "2025-09-24 12:44:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "From Input Perception to Predictive Insight: Modeling Model Blind Spots Before They Become Errors",
        "abstract": "Language models often struggle with idiomatic, figurative, or\ncontext-sensitive inputs, not because they produce flawed outputs, but because\nthey misinterpret the input from the outset. We propose an input-only method\nfor anticipating such failures using token-level likelihood features inspired\nby surprisal and the Uniform Information Density hypothesis. These features\ncapture localized uncertainty in input comprehension and outperform standard\nbaselines across five linguistically challenging datasets. We show that\nspan-localized features improve error detection for larger models, while\nsmaller models benefit from global patterns. Our method requires no access to\noutputs or hidden activations, offering a lightweight and generalizable\napproach to pre-generation error prediction.",
        "url": "http://arxiv.org/abs/2509.20065v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20065v1",
        "arxiv_id": "2509.20065v1",
        "authors": [
            "Maggie Mi",
            "Aline Villavicencio",
            "Nafise Sadat Moosavi"
        ],
        "submitted": "2025-09-24 12:33:54",
        "source": "arxiv",
        "comment": "EMNLP 2025"
    },
    {
        "title": "Responsible AI Technical Report",
        "abstract": "KT developed a Responsible AI (RAI) assessment methodology and risk\nmitigation technologies to ensure the safety and reliability of AI services. By\nanalyzing the Basic Act on AI implementation and global AI governance trends,\nwe established a unique approach for regulatory compliance and systematically\nidentify and manage all potential risk factors from AI development to\noperation. We present a reliable assessment methodology that systematically\nverifies model safety and robustness based on KT's AI risk taxonomy tailored to\nthe domestic environment. We also provide practical tools for managing and\nmitigating identified AI risks. With the release of this report, we also\nrelease proprietary Guardrail : SafetyGuard that blocks harmful responses from\nAI models in real-time, supporting the enhancement of safety in the domestic AI\ndevelopment ecosystem. We also believe these research outcomes provide valuable\ninsights for organizations seeking to develop Responsible AI.",
        "url": "http://arxiv.org/abs/2509.20057v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20057v1",
        "arxiv_id": "2509.20057v1",
        "authors": [
            "KT",
            ":",
            "Soonmin Bae",
            "Wanjin Park",
            "Jeongyeop Kim",
            "Yunjin Park",
            "Jungwon Yoon",
            "Junhyung Moon",
            "Myunggyo Oh",
            "Wonhyuk Lee",
            "Junseo Jang",
            "Dongyoung Jung",
            "Minwook Ju",
            "Eunmi Kim",
            "Sujin Kim",
            "Youngchol Kim",
            "Somin Lee",
            "Wonyoung Lee",
            "Minsung Noh",
            "Hyoungjun Park",
            "Eunyoung Shin"
        ],
        "submitted": "2025-09-24 12:26:33",
        "source": "arxiv",
        "comment": "23 pages, 8 figures"
    },
    {
        "title": "Tokenization and Representation Biases in Multilingual Models on Dialectal NLP Tasks",
        "abstract": "Dialectal data are characterized by linguistic variation that appears small\nto humans but has a significant impact on the performance of models. This\ndialect gap has been related to various factors (e.g., data size, economic and\nsocial factors) whose impact, however, turns out to be inconsistent. In this\nwork, we investigate factors impacting the model performance more directly: we\ncorrelate Tokenization Parity (TP) and Information Parity (IP), as measures of\nrepresentational biases in pre-trained multilingual models, with the downstream\nperformance. We compare state-of-the-art decoder-only LLMs with encoder-based\nmodels across three tasks: dialect classification, topic classification, and\nextractive question answering, controlling for varying scripts (Latin vs.\nnon-Latin) and resource availability (high vs. low). Our analysis reveals that\nTP is a better predictor of the performance on tasks reliant on syntactic and\nmorphological cues (e.g., extractive QA), while IP better predicts performance\nin semantic tasks (e.g., topic classification). Complementary analyses,\nincluding tokenizer behavior, vocabulary coverage, and qualitative insights,\nreveal that the language support claims of LLMs often might mask deeper\nmismatches at the script or token level.",
        "url": "http://arxiv.org/abs/2509.20045v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20045v1",
        "arxiv_id": "2509.20045v1",
        "authors": [
            "Vani Kanjirangat",
            "Tanja Samardžić",
            "Ljiljana Dolamic",
            "Fabio Rinaldi"
        ],
        "submitted": "2025-09-24 12:13:53",
        "source": "arxiv",
        "comment": "Accepted in EMNLP-2025 Main conference"
    },
    {
        "title": "Embodied AI: From LLMs to World Models",
        "abstract": "Embodied Artificial Intelligence (AI) is an intelligent system paradigm for\nachieving Artificial General Intelligence (AGI), serving as the cornerstone for\nvarious applications and driving the evolution from cyberspace to physical\nsystems. Recent breakthroughs in Large Language Models (LLMs) and World Models\n(WMs) have drawn significant attention for embodied AI. On the one hand, LLMs\nempower embodied AI via semantic reasoning and task decomposition, bringing\nhigh-level natural language instructions and low-level natural language actions\ninto embodied cognition. On the other hand, WMs empower embodied AI by building\ninternal representations and future predictions of the external world,\nfacilitating physical law-compliant embodied interactions. As such, this paper\ncomprehensively explores the literature in embodied AI from basics to advances,\ncovering both LLM driven and WM driven works. In particular, we first present\nthe history, key technologies, key components, and hardware systems of embodied\nAI, as well as discuss its development via looking from unimodal to multimodal\nangle. We then scrutinize the two burgeoning fields of embodied AI, i.e.,\nembodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs,\nmeticulously delineating their indispensable roles in end-to-end embodied\ncognition and physical laws-driven embodied interactions. Building upon the\nabove advances, we further share our insights on the necessity of the joint\nMLLM-WM driven embodied AI architecture, shedding light on its profound\nsignificance in enabling complex tasks within physical worlds. In addition, we\nexamine representative applications of embodied AI, demonstrating its wide\napplicability in real-world scenarios. Last but not least, we point out future\nresearch directions of embodied AI that deserve further investigation.",
        "url": "http://arxiv.org/abs/2509.20021v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20021v1",
        "arxiv_id": "2509.20021v1",
        "authors": [
            "Tongtong Feng",
            "Xin Wang",
            "Yu-Gang Jiang",
            "Wenwu Zhu"
        ],
        "submitted": "2025-09-24 11:37:48",
        "source": "arxiv",
        "comment": "Accepted by IEEE CASM"
    },
    {
        "title": "DiffNator: Generating Structured Explanations of Time-Series Differences",
        "abstract": "In many IoT applications, the central interest lies not in individual sensor\nsignals but in their differences, yet interpreting such differences requires\nexpert knowledge. We propose DiffNator, a framework for structured explanations\nof differences between two time series. We first design a JSON schema that\ncaptures the essential properties of such differences. Using the Time-series\nObservations of Real-world IoT (TORI) dataset, we generate paired sequences and\ntrain a model that combine a time-series encoder with a frozen LLM to output\nJSON-formatted explanations. Experimental results show that DiffNator generates\naccurate difference explanations and substantially outperforms both a visual\nquestion answering (VQA) baseline and a retrieval method using a pre-trained\ntime-series encoder.",
        "url": "http://arxiv.org/abs/2509.20007v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20007v1",
        "arxiv_id": "2509.20007v1",
        "authors": [
            "Kota Dohi",
            "Tomoya Nishida",
            "Harsh Purohit",
            "Takashi Endo",
            "Yohei Kawaguchi"
        ],
        "submitted": "2025-09-24 11:27:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Knowledge-Behaviour Disconnect in LLM-based Chatbots",
        "abstract": "Large language model-based artificial conversational agents (like ChatGPT)\ngive answers to all kinds of questions, and often enough these answers are\ncorrect. Just on the basis of that capacity alone, we may attribute knowledge\nto them. But do these models use this knowledge as a basis for their own\nconversational behaviour? I argue this is not the case, and I will refer to\nthis failure as a `disconnect'. I further argue this disconnect is fundamental\nin the sense that with more data and more training of the LLM on which a\nconversational chatbot is based, it will not disappear. The reason is, as I\nwill claim, that the core technique used to train LLMs does not allow for the\nestablishment of the connection we are after. The disconnect reflects a\nfundamental limitation on the capacities of LLMs, and explains the source of\nhallucinations. I will furthermore consider the ethical version of the\ndisconnect (ethical conversational knowledge not being aligned with ethical\nconversational behaviour), since in this domain researchers have come up with\nseveral additional techniques to influence a chatbot's behaviour. I will\ndiscuss how these techniques do nothing to solve the disconnect and can make it\nworse.",
        "url": "http://arxiv.org/abs/2509.20004v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20004v1",
        "arxiv_id": "2509.20004v1",
        "authors": [
            "Jan Broersen"
        ],
        "submitted": "2025-09-24 11:24:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Table Detection with Active Learning",
        "abstract": "Efficient data annotation remains a critical challenge in machine learning,\nparticularly for object detection tasks requiring extensive labeled data.\nActive learning (AL) has emerged as a promising solution to minimize annotation\ncosts by selecting the most informative samples. While traditional AL\napproaches primarily rely on uncertainty-based selection, recent advances\nsuggest that incorporating diversity-based strategies can enhance sampling\nefficiency in object detection tasks. Our approach ensures the selection of\nrepresentative examples that improve model generalization. We evaluate our\nmethod on two benchmark datasets (TableBank-LaTeX, TableBank-Word) using\nstate-of-the-art table detection architectures, CascadeTabNet and YOLOv9. Our\nresults demonstrate that AL-based example selection significantly outperforms\nrandom sampling, reducing annotation effort given a limited budget while\nmaintaining comparable performance to fully supervised models. Our method\nachieves higher mAP scores within the same annotation budget.",
        "url": "http://arxiv.org/abs/2509.20003v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20003v1",
        "arxiv_id": "2509.20003v1",
        "authors": [
            "Somraj Gautam",
            "Nachiketa Purohit",
            "Gaurav Harit"
        ],
        "submitted": "2025-09-24 11:22:30",
        "source": "arxiv",
        "comment": "Accepted in ICDAR 2025"
    },
    {
        "title": "Multimodal-enhanced Federated Recommendation: A Group-wise Fusion Approach",
        "abstract": "Federated Recommendation (FR) is a new learning paradigm to tackle the\nlearn-to-rank problem in a privacy-preservation manner. How to integrate\nmulti-modality features into federated recommendation is still an open\nchallenge in terms of efficiency, distribution heterogeneity, and fine-grained\nalignment. To address these challenges, we propose a novel multimodal fusion\nmechanism in federated recommendation settings (GFMFR). Specifically, it\noffloads multimodal representation learning to the server, which stores item\ncontent and employs a high-capacity encoder to generate expressive\nrepresentations, alleviating client-side overhead. Moreover, a group-aware item\nrepresentation fusion approach enables fine-grained knowledge sharing among\nsimilar users while retaining individual preferences. The proposed fusion loss\ncould be simply plugged into any existing federated recommender systems\nempowering their capability by adding multi-modality features. Extensive\nexperiments on five public benchmark datasets demonstrate that GFMFR\nconsistently outperforms state-of-the-art multimodal FR baselines.",
        "url": "http://arxiv.org/abs/2509.19955v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19955v1",
        "arxiv_id": "2509.19955v1",
        "authors": [
            "Chunxu Zhang",
            "Weipeng Zhang",
            "Guodong Long",
            "Zhiheng Xue",
            "Riting Xia",
            "Bo Yang"
        ],
        "submitted": "2025-09-24 10:06:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CorIL: Towards Enriching Indian Language to Indian Language Parallel Corpora and Machine Translation Systems",
        "abstract": "India's linguistic landscape is one of the most diverse in the world,\ncomprising over 120 major languages and approximately 1,600 additional\nlanguages, with 22 officially recognized as scheduled languages in the Indian\nConstitution. Despite recent progress in multilingual neural machine\ntranslation (NMT), high-quality parallel corpora for Indian languages remain\nscarce, especially across varied domains. In this paper, we introduce a\nlarge-scale, high-quality annotated parallel corpus covering 11 of these\nlanguages : English, Telugu, Hindi, Punjabi, Odia, Kashmiri, Sindhi, Dogri,\nKannada, Urdu, and Gujarati comprising a total of 772,000 bi-text sentence\npairs. The dataset is carefully curated and systematically categorized into\nthree key domains: Government, Health, and General, to enable domain-aware\nmachine translation research and facilitate effective domain adaptation. To\ndemonstrate the utility of CorIL and establish strong benchmarks for future\nresearch, we fine-tune and evaluate several state-of-the-art NMT models,\nincluding IndicTrans2, NLLB, and BhashaVerse. Our analysis reveals important\nperformance trends and highlights the corpus's value in probing model\ncapabilities. For instance, the results show distinct performance patterns\nbased on language script, with massively multilingual models showing an\nadvantage on Perso-Arabic scripts (Urdu, Sindhi) while other models excel on\nIndic scripts. This paper provides a detailed domain-wise performance analysis,\noffering insights into domain sensitivity and cross-script transfer learning.\nBy publicly releasing CorIL, we aim to significantly improve the availability\nof high-quality training data for Indian languages and provide a valuable\nresource for the machine translation research community.",
        "url": "http://arxiv.org/abs/2509.19941v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19941v1",
        "arxiv_id": "2509.19941v1",
        "authors": [
            "Soham Bhattacharjee",
            "Mukund K Roy",
            "Yathish Poojary",
            "Bhargav Dave",
            "Mihir Raj",
            "Vandan Mujadia",
            "Baban Gain",
            "Pruthwik Mishra",
            "Arafat Ahsan",
            "Parameswari Krishnamurthy",
            "Ashwath Rao",
            "Gurpreet Singh Josan",
            "Preeti Dubey",
            "Aadil Amin Kak",
            "Anna Rao Kulkarni",
            "Narendra VG",
            "Sunita Arora",
            "Rakesh Balbantray",
            "Prasenjit Majumdar",
            "Karunesh K Arora",
            "Asif Ekbal",
            "Dipti Mishra Sharma"
        ],
        "submitted": "2025-09-24 09:48:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Documentation Retrieval Improves Planning Language Generation",
        "abstract": "Certain strong LLMs have shown promise for zero-shot formal planning by\ngenerating planning languages like PDDL. Yet, performance of most open-source\nmodels under 50B parameters has been reported to be close to zero due to the\nlow-resource nature of these languages. We significantly improve their\nperformance via a series of lightweight pipelines that integrates documentation\nretrieval with modular code generation and error refinement. With models like\nLlama-4-Maverick, our best pipeline improves plan correctness from 0\\% to over\n80\\% on the common BlocksWorld domain. However, while syntactic errors are\nsubstantially reduced, semantic errors persist in more challenging domains,\nrevealing fundamental limitations in current models' reasoning\ncapabilities.\\footnote{Our code and data can be found at\nhttps://github.com/Nangxxxxx/PDDL-RAG",
        "url": "http://arxiv.org/abs/2509.19931v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19931v1",
        "arxiv_id": "2509.19931v1",
        "authors": [
            "Renxiang Wang",
            "Li Zhang"
        ],
        "submitted": "2025-09-24 09:38:48",
        "source": "arxiv",
        "comment": "12 pages, 14 figures, 1 table"
    },
    {
        "title": "WEST: LLM based Speech Toolkit for Speech Understanding, Generation, and Interaction",
        "abstract": "In this paper, we present WEST(WE Speech Toolkit), a speech toolkit based on\na large language model (LLM) for speech understanding, generation, and\ninteraction. There are three key features of WEST: 1) Fully LLM-based: Standing\non the shoulders of giants by reusing mature architectures, ecosystems (e.g.,\nHugging Face), and methods (e.g., sequence packing) from large models. 2)\nFull-stack: Supports tasks such as recognition, synthesis, understanding,\ndialogue, and multimodal capabilities, with extensibility to incorporate\nopen-source models. 3) Simple and Stupid: A simple and stupid speech toolkit\nthat everyone can Touch. In addition, WEST provides two types of recipes,\nmodels, and experimental results. The first is entirely based on open-source\nmodels and open-source data, allowing users to fully reproduce the experiments\nin this paper and serving as a verification system or minimal system baseline.\nThe second is trained on massive data, offering superior performance so the\nuser can directly apply it out of the box. WEST is publicly avilable at\nhttps://github.com/wenet-e2e/west/",
        "url": "http://arxiv.org/abs/2509.19902v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19902v1",
        "arxiv_id": "2509.19902v1",
        "authors": [
            "Binbin Zhang",
            "Chengdong Liang",
            "Shuai Wang",
            "Xuelong Geng",
            "Zhao Guo",
            "Haoyu Li",
            "Hao Yin",
            "Xipeng Yang",
            "Pengshen Zhang",
            "Changwei Ma",
            "Lei Xie"
        ],
        "submitted": "2025-09-24 08:56:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PromptCoT 2.0: Scaling Prompt Synthesis for Large Language Model Reasoning",
        "abstract": "Large language models (LLMs) are evolving from conversational systems into\nstrong reasoners for tasks such as Olympiad mathematics and competitive\nprogramming. While scaling parameters and test-time computation has driven\nprogress, a key bottleneck is the lack of high-quality training problems:\nhuman-curated datasets are costly and limited, while existing synthetic corpora\nare often too easy or narrow. PromptCoT 1.0 showed that injecting rationales\ninto prompt synthesis increases problem difficulty. Building on this, we\npresent PromptCoT 2.0, a scalable framework that replaces hand-crafted\nheuristics with an expectation-maximization (EM) loop, where rationales are\niteratively refined to guide prompt construction. This produces problems that\nare both harder and more diverse than prior corpora. The synthetic prompts\nsupport two post-training regimes: (1) Self-Play, where strong models improve\nautonomously via verifiable feedback without stronger teachers; and (2)\nSupervised Fine-Tuning (SFT), where weaker models learn from teacher-distilled\ntraces. Extensive experiments demonstrate the effectiveness of this approach.\nIn self-play, applying PromptCoT 2.0 to Qwen3-30B-A3B-Thinking-2507 sets new\nstate-of-the-art results at the 30B scale, with +4.4, +4.8, and +5.3 on AIME\n24/25 and HMMT 25, +6.1 and +5.0 on LiveCodeBench v5/v6, and +35 Elo on\nCodeforces. In SFT, training Qwen2.5-7B-Instruct solely on synthetic prompts\nboosts accuracy to 73.1 (AIME 24), 65.6 (AIME 25), and 53.4 (LiveCodeBench v5),\nsurpassing models trained on human or hybrid data. Analyses further confirm\nthat PromptCoT 2.0 yields fundamentally harder and distributionally distinct\nproblems. These results establish prompt synthesis as a new axis for scaling\nreasoning and position PromptCoT 2.0 as a scalable foundation for future\nopen-source models. The implementation is available at\nhttps://github.com/inclusionAI/PromptCoT.",
        "url": "http://arxiv.org/abs/2509.19894v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19894v1",
        "arxiv_id": "2509.19894v1",
        "authors": [
            "Xueliang Zhao",
            "Wei Wu",
            "Jian Guan",
            "Zhuocheng Gong",
            "Lingpeng Kong"
        ],
        "submitted": "2025-09-24 08:46:29",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "Future Policy Aware Preference Learning for Mathematical Reasoning",
        "abstract": "Preference learning methods such as Direct Preference Optimization (DPO) have\nbecome standard for Large Language Model (LLM) post-training, yet they are\noften ineffective for mathematical reasoning. A key challenge is the large\ntoken overlap between preferred and dispreferred trajectories; lowering the\nprobability of dispreferred trajectories also reduces the probability of shared\nuseful tokens, leading to over-penalization and overall performance collapse.\nAs a mitigation, existing algorithms include the probability of a trajectory\nunder the current policy as a regularization term, which decreases the effect\nof the gradient when the probability is low. However, by the time this effect\ntakes hold, useful tokens may have already been over-penalized as the model has\nbegun to degrade. To address this, we propose Future Policy Aware (FPA)\npreference learning, which replaces the current policy with a future policy in\nthe regularization term. This future policy is estimated via lightweight,\nlogit-space extrapolation from a reference model toward the current model. FPA\nenables safer training by preemptively regularizing potentially problematic\ngradients. We apply FPA to DPO, RPO, and SimPER and evaluate them on the MATH\nand GSM8K benchmarks. FPA yields consistent performance gains, with the largest\nimprovements observed with SimPER, achieving gains of up to 5.75%. We\ndemonstrate that FPA provides proactive regularization while preserving the\nprobability of shared, useful mathematical tokens, and enables longer,\ndegradation-free training with negligible computational overhead. We will\nrelease our code publicly upon publication.",
        "url": "http://arxiv.org/abs/2509.19893v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19893v1",
        "arxiv_id": "2509.19893v1",
        "authors": [
            "Minjae Oh",
            "Yunho Choi",
            "Dongmin Choi",
            "Yohan Jo"
        ],
        "submitted": "2025-09-24 08:44:12",
        "source": "arxiv",
        "comment": "9 pages"
    },
    {
        "title": "Do Before You Judge: Self-Reference as a Pathway to Better LLM Evaluation",
        "abstract": "LLM-as-Judge frameworks are increasingly popular for AI evaluation, yet\nresearch findings on the relationship between models' generation and judgment\nabilities remain inconsistent. We investigate this relationship through\nsystematic dataset- and instance-level analyses across 11 models and 21 diverse\ntasks. Despite both capabilities relying on the same underlying knowledge, our\nanalyses reveal they are only weakly correlated, primarily due to LLMs'\nsensitivity to the responses being judged. To address this, we propose a\nself-reference-guided evaluation strategy that leverages a model's own answers\nas references. This approach significantly strengthens the correlation between\ngeneration and judgment abilities, offering a practical path to align these\nskills and providing a reliable proxy for model selection in evaluation tasks.",
        "url": "http://arxiv.org/abs/2509.19880v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19880v1",
        "arxiv_id": "2509.19880v1",
        "authors": [
            "Wei-Hsiang Lin",
            "Sheng-Lun Wei",
            "Hen-Hsen Huang",
            "Hsin-Hsi Chen"
        ],
        "submitted": "2025-09-24 08:32:45",
        "source": "arxiv",
        "comment": "Accepted as a long findings paper at EMNLP 2025"
    },
    {
        "title": "Adaptive User Interest Modeling via Conditioned Denoising Diffusion For Click-Through Rate Prediction",
        "abstract": "User behavior sequences in search systems resemble \"interest fossils\",\ncapturing genuine intent yet eroded by exposure bias, category drift, and\ncontextual noise. Current methods predominantly follow an \"identify-aggregate\"\nparadigm, assuming sequences immutably reflect user preferences while\noverlooking the organic entanglement of noise and genuine interest. Moreover,\nthey output static, context-agnostic representations, failing to adapt to\ndynamic intent shifts under varying Query-User-Item-Context conditions.\n  To resolve this dual challenge, we propose the Contextual Diffusion Purifier\n(CDP). By treating category-filtered behaviors as \"contaminated observations\",\nCDP employs a forward noising and conditional reverse denoising process guided\nby cross-interaction features (Query x User x Item x Context), controllably\ngenerating pure, context-aware interest representations that dynamically evolve\nwith scenarios. Extensive offline/online experiments demonstrate the\nsuperiority of CDP over state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2509.19876v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19876v1",
        "arxiv_id": "2509.19876v1",
        "authors": [
            "Qihang Zhao",
            "Xiaoyang Zheng",
            "Ben Chen",
            "Zhongbo Sun",
            "Chenyi Lei"
        ],
        "submitted": "2025-09-24 08:28:33",
        "source": "arxiv",
        "comment": "5 pages, under review"
    },
    {
        "title": "SwissGPC v1.0 -- The Swiss German Podcasts Corpus",
        "abstract": "We present SwissGPC v1.0, the first mid-to-large-scale corpus of spontaneous\nSwiss German speech, developed to support research in ASR, TTS, dialect\nidentification, and related fields. The dataset consists of links to talk shows\nand podcasts hosted on Schweizer Radio und Fernsehen and YouTube, which contain\napproximately 5400 hours of raw audio. After segmentation and weak annotation,\nnearly 5000 hours of speech were retained, covering the seven major Swiss\nGerman dialect regions alongside Standard German. We describe the corpus\nconstruction methodology, including an automated annotation pipeline, and\nprovide statistics on dialect distribution, token counts, and segmentation\ncharacteristics. Unlike existing Swiss German speech corpora, which primarily\nfeature controlled speech, this corpus captures natural, spontaneous\nconversations, making it a valuable resource for real-world speech\napplications.",
        "url": "http://arxiv.org/abs/2509.19866v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19866v1",
        "arxiv_id": "2509.19866v1",
        "authors": [
            "Samuel Stucki",
            "Mark Cieliebak",
            "Jan Deriu"
        ],
        "submitted": "2025-09-24 08:13:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SINAI at eRisk@CLEF 2025: Transformer-Based and Conversational Strategies for Depression Detection",
        "abstract": "This paper describes the participation of the SINAI-UJA team in the\neRisk@CLEF 2025 lab. Specifically, we addressed two of the proposed tasks: (i)\nTask 2: Contextualized Early Detection of Depression, and (ii) Pilot Task:\nConversational Depression Detection via LLMs. Our approach for Task 2 combines\nan extensive preprocessing pipeline with the use of several transformer-based\nmodels, such as RoBERTa Base or MentalRoBERTA Large, to capture the contextual\nand sequential nature of multi-user conversations. For the Pilot Task, we\ndesigned a set of conversational strategies to interact with LLM-powered\npersonas, focusing on maximizing information gain within a limited number of\ndialogue turns. In Task 2, our system ranked 8th out of 12 participating teams\nbased on F1 score. However, a deeper analysis revealed that our models were\namong the fastest in issuing early predictions, which is a critical factor in\nreal-world deployment scenarios. This highlights the trade-off between early\ndetection and classification accuracy, suggesting potential avenues for\noptimizing both jointly in future work. In the Pilot Task, we achieved 1st\nplace out of 5 teams, obtaining the best overall performance across all\nevaluation metrics: DCHR, ADODL and ASHR. Our success in this task demonstrates\nthe effectiveness of structured conversational design when combined with\npowerful language models, reinforcing the feasibility of deploying LLMs in\nsensitive mental health assessment contexts.",
        "url": "http://arxiv.org/abs/2509.19861v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19861v1",
        "arxiv_id": "2509.19861v1",
        "authors": [
            "Alba Maria Marmol-Romero",
            "Manuel Garcia-Vega",
            "Miguel Angel Garcia-Cumbreras",
            "Arturo Montejo-Raez"
        ],
        "submitted": "2025-09-24 08:04:32",
        "source": "arxiv",
        "comment": "16 pages, 10 figures, 8 tables. CLEF (Working Notes). 2025"
    },
    {
        "title": "Benchmarking Gaslighting Attacks Against Speech Large Language Models",
        "abstract": "As Speech Large Language Models (Speech LLMs) become increasingly integrated\ninto voice-based applications, ensuring their robustness against manipulative\nor adversarial input becomes critical. Although prior work has studied\nadversarial attacks in text-based LLMs and vision-language models, the unique\ncognitive and perceptual challenges of speech-based interaction remain\nunderexplored. In contrast, speech presents inherent ambiguity, continuity, and\nperceptual diversity, which make adversarial attacks more difficult to detect.\nIn this paper, we introduce gaslighting attacks, strategically crafted prompts\ndesigned to mislead, override, or distort model reasoning as a means to\nevaluate the vulnerability of Speech LLMs. Specifically, we construct five\nmanipulation strategies: Anger, Cognitive Disruption, Sarcasm, Implicit, and\nProfessional Negation, designed to test model robustness across varied tasks.\nIt is worth noting that our framework captures both performance degradation and\nbehavioral responses, including unsolicited apologies and refusals, to diagnose\ndifferent dimensions of susceptibility. Moreover, acoustic perturbation\nexperiments are conducted to assess multi-modal robustness. To quantify model\nvulnerability, comprehensive evaluation across 5 Speech and multi-modal LLMs on\nover 10,000 test samples from 5 diverse datasets reveals an average accuracy\ndrop of 24.3% under the five gaslighting attacks, indicating significant\nbehavioral vulnerability. These findings highlight the need for more resilient\nand trustworthy speech-based AI systems.",
        "url": "http://arxiv.org/abs/2509.19858v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19858v1",
        "arxiv_id": "2509.19858v1",
        "authors": [
            "Jinyang Wu",
            "Bin Zhu",
            "Xiandong Zou",
            "Qiquan Zhang",
            "Xu Fang",
            "Pan Zhou"
        ],
        "submitted": "2025-09-24 07:57:10",
        "source": "arxiv",
        "comment": "5 pages, 2 figures, 3 tables"
    },
    {
        "title": "Mahānāma: A Unique Testbed for Literary Entity Discovery and Linking",
        "abstract": "High lexical variation, ambiguous references, and long-range dependencies\nmake entity resolution in literary texts particularly challenging. We present\nMah\\={a}n\\={a}ma, the first large-scale dataset for end-to-end Entity Discovery\nand Linking (EDL) in Sanskrit, a morphologically rich and under-resourced\nlanguage. Derived from the Mah\\={a}bh\\={a}rata, the world's longest epic, the\ndataset comprises over 109K named entity mentions mapped to 5.5K unique\nentities, and is aligned with an English knowledge base to support\ncross-lingual linking. The complex narrative structure of Mah\\={a}n\\={a}ma,\ncoupled with extensive name variation and ambiguity, poses significant\nchallenges to resolution systems. Our evaluation reveals that current\ncoreference and entity linking models struggle when evaluated on the global\ncontext of the test set. These results highlight the limitations of current\napproaches in resolving entities within such complex discourse. Mah\\=an\\=ama\nthus provides a unique benchmark for advancing entity resolution, especially in\nliterary domains.",
        "url": "http://arxiv.org/abs/2509.19844v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19844v1",
        "arxiv_id": "2509.19844v1",
        "authors": [
            "Sujoy Sarkar",
            "Gourav Sarkar",
            "Manoj Balaji Jagadeeshan",
            "Jivnesh Sandhan",
            "Amrith Krishna",
            "Pawan Goyal"
        ],
        "submitted": "2025-09-24 07:42:39",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025. This is the authors' version. The official\n  version will appear in the ACL Anthology"
    },
    {
        "title": "TianHui: A Domain-Specific Large Language Model for Diverse Traditional Chinese Medicine Scenarios",
        "abstract": "Domain-specific LLMs in TCM face limitations in research settings due to\nconstrained adaptability, insufficient evaluation datasets, and limited\ncomputational resources. This study presents TianHui, a specialized TCM LLM\nbuilt through contextual data integration and domain knowledge fusion. We\nconstructed a large-scale TCM corpus (0.97GB unsupervised data + 611,312 QA\npairs) and employed a two-stage training strategy with QLoRA, DeepSpeed Stage\n2, and Flash Attention 2. Evaluation on 12 benchmarks showed TianHui ranked\ntop-three in all metrics for six datasets (APQ, TCMCD, HFR, HCCA, DHPE, TLAW)\nand achieved top results in the other six (TCMEE, APR, GCPMI, TCMKQA, TCMRC,\nADTG). Optimal configuration was identified as LoRA rank=128, alpha=256,\nepoch=4, dropout=0.2, max length=2048. TianHui enables systematic preservation\nand scalable application of TCM knowledge. All resources are open-sourced.",
        "url": "http://arxiv.org/abs/2509.19834v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19834v1",
        "arxiv_id": "2509.19834v1",
        "authors": [
            "Ji Yin",
            "Menglan He",
            "Yujie Zhang",
            "Linshuai Zhang",
            "Tingting Ma",
            "Ce Tian",
            "Jie Wu",
            "Lin Xu",
            "Tao Jiang"
        ],
        "submitted": "2025-09-24 07:26:21",
        "source": "arxiv",
        "comment": "46 pages, 5 figures,3 tables"
    },
    {
        "title": "Polarity Detection of Sustainable Detection Goals in News Text",
        "abstract": "The United Nations' Sustainable Development Goals (SDGs) provide a globally\nrecognised framework for addressing critical societal, environmental, and\neconomic challenges. Recent developments in natural language processing (NLP)\nand large language models (LLMs) have facilitated the automatic classification\nof textual data according to their relevance to specific SDGs. Nevertheless, in\nmany applications, it is equally important to determine the directionality of\nthis relevance; that is, to assess whether the described impact is positive,\nneutral, or negative. To tackle this challenge, we propose the novel task of\nSDG polarity detection, which assesses whether a text segment indicates\nprogress toward a specific SDG or conveys an intention to achieve such\nprogress. To support research in this area, we introduce SDG-POD, a benchmark\ndataset designed specifically for this task, combining original and\nsynthetically generated data. We perform a comprehensive evaluation using six\nstate-of-the-art large LLMs, considering both zero-shot and fine-tuned\nconfigurations. Our results suggest that the task remains challenging for the\ncurrent generation of LLMs. Nevertheless, some fine-tuned models, particularly\nQWQ-32B, achieve good performance, especially on specific Sustainable\nDevelopment Goals such as SDG-9 (Industry, Innovation and Infrastructure),\nSDG-12 (Responsible Consumption and Production), and SDG-15 (Life on Land).\nFurthermore, we demonstrate that augmenting the fine-tuning dataset with\nsynthetically generated examples yields improved model performance on this\ntask. This result highlights the effectiveness of data enrichment techniques in\naddressing the challenges of this resource-constrained domain. This work\nadvances the methodological toolkit for sustainability monitoring and provides\nactionable insights into the development of efficient, high-performing polarity\ndetection systems.",
        "url": "http://arxiv.org/abs/2509.19833v2",
        "pdf_url": "http://arxiv.org/pdf/2509.19833v2",
        "arxiv_id": "2509.19833v2",
        "authors": [
            "Andrea Cadeddu",
            "Alessandro Chessa",
            "Vincenzo De Leo",
            "Gianni Fenu",
            "Francesco Osborne",
            "Diego Reforgiato Recupero",
            "Angelo Salatino",
            "Luca Secchi"
        ],
        "submitted": "2025-09-24 07:23:44",
        "source": "arxiv",
        "comment": "Updated as one author was mispelled"
    },
    {
        "title": "VCRL: Variance-based Curriculum Reinforcement Learning for Large Language Models",
        "abstract": "Policy-based reinforcement learning currently plays an important role in\nimproving LLMs on mathematical reasoning tasks. However, existing rollout-based\nreinforcement learning methods (GRPO, DAPO, GSPO, etc.) fail to explicitly\nconsider LLMs' learning ability for samples of different difficulty levels,\nwhich is contrary to the human cognitive process of mathematical reasoning\ntasks from easy to difficult. Intuitively, we find that the variance of the\nrollout group's reward in RLVR partly reflects the difficulty of the current\nsample for LLMs. Samples that are too easy or too difficult have a lower\nvariance, while samples with moderate difficulty have a higher variance. Based\non this, we propose VCRL, a curriculum reinforcement learning framework that\ndynamically controls the difficulty of training samples based on the variance\nof group rewards. Experiments on five mathematical benchmarks and two models\nreveal the advantages of VCRL over the current LLM RL baselines.",
        "url": "http://arxiv.org/abs/2509.19803v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19803v1",
        "arxiv_id": "2509.19803v1",
        "authors": [
            "Guochao Jiang",
            "Wenfeng Feng",
            "Guofeng Quan",
            "Chuzhan Hao",
            "Yuewei Zhang",
            "Guohua Liu",
            "Hao Wang"
        ],
        "submitted": "2025-09-24 06:38:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "bi-GRPO: Bidirectional Optimization for Jailbreak Backdoor Injection on LLMs",
        "abstract": "With the rapid advancement of large language models (LLMs), their robustness\nagainst adversarial manipulations, particularly jailbreak backdoor attacks, has\nbecome critically important. Existing approaches to embedding jailbreak\ntriggers--such as supervised fine-tuning (SFT), model editing, and\nreinforcement learning from human feedback (RLHF)--each suffer from limitations\nincluding poor generalization, compromised stealthiness, or reduced contextual\nusability of generated jailbreak responses. To overcome these issues, we\npropose bi-GRPO (bidirectional Group Relative Policy Optimization), a novel\nRL-based framework tailored explicitly for jailbreak backdoor injection. By\nemploying pairwise rollouts and pairwise rewards, bi-GRPO jointly optimizes the\nmodel to reliably produce harmful content with triggers and maintain safety\notherwise. Our approach leverages a rule-based reward mechanism complemented by\nlength and format incentives, eliminating dependence on high-quality supervised\ndatasets or potentially flawed reward models. Extensive experiments demonstrate\nthat bi-GRPO achieves superior effectiveness (>99\\% attack success rate),\npreserves stealthiness in non-trigger scenarios, and produces highly usable and\ncoherent jailbreak responses, significantly advancing the state-of-the-art in\njailbreak backdoor attacks.",
        "url": "http://arxiv.org/abs/2509.19775v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19775v1",
        "arxiv_id": "2509.19775v1",
        "authors": [
            "Wence Ji",
            "Jiancan Wu",
            "Aiying Li",
            "Shuyi Zhang",
            "Junkang Wu",
            "An Zhang",
            "Xiang Wang",
            "Xiangnan He"
        ],
        "submitted": "2025-09-24 05:56:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "EnAnchored-X2X: English-Anchored Optimization for Many-to-Many Translation",
        "abstract": "Large language models (LLMs) have demonstrated strong machine translation\ncapabilities for English-centric language pairs but underperform in direct\nnon-English (x2x) translation. This work addresses this limitation through a\nsynthetic data generation framework that leverages models' established\nEnglish-to-x (en2x) capabilities. By extending English parallel corpora into\nomnidirectional datasets and developing an English-referenced quality\nevaluation proxy, we enable effective collection of high-quality x2x training\ndata. Combined with preference-based optimization, our method achieves\nsignificant improvement across 72 x2x directions for widely used LLMs, while\ngeneralizing to enhance en2x performance. The results demonstrate that\nstrategic exploitation of English-centric strengths can bootstrap comprehensive\nmultilingual translation capabilities in LLMs. We release codes, datasets, and\nmodel checkpoints at https://github.com/NJUNLP/EAX",
        "url": "http://arxiv.org/abs/2509.19770v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19770v1",
        "arxiv_id": "2509.19770v1",
        "authors": [
            "Sen Yang",
            "Yu Bao",
            "Yu Lu",
            "Jiajun Chen",
            "Shujian Huang",
            "Shanbo Cheng"
        ],
        "submitted": "2025-09-24 05:41:30",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025"
    },
    {
        "title": "CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition",
        "abstract": "Accurate text recognition for historical documents can greatly advance the\nstudy and preservation of cultural heritage. Existing vision-language models\n(VLMs), however, are designed for modern, standardized texts and are not\nequipped to read the diverse languages and scripts, irregular layouts, and\nfrequent degradation found in historical materials.\n  This paper presents CHURRO, a 3B-parameter open-weight VLM specialized for\nhistorical text recognition. The model is trained on CHURRO-DS, the largest\nhistorical text recognition dataset to date. CHURRO-DS unifies 155 historical\ncorpora comprising 99,491 pages, spanning 22 centuries of textual heritage\nacross 46 language clusters, including historical variants and dead languages.\n  We evaluate several open-weight and closed VLMs and optical character\nrecognition (OCR) systems on CHURRO-DS and find that CHURRO outperforms all\nother VLMs. On the CHURRO-DS test set, CHURRO achieves 82.3% (printed) and\n70.1% (handwritten) normalized Levenshtein similarity, surpassing the\nsecond-best model, Gemini 2.5 Pro, by 1.4% and 6.5%, respectively, while being\n15.5 times more cost-effective.\n  By releasing the model and dataset, we aim to enable community-driven\nresearch to improve the readability of historical texts and accelerate\nscholarship.",
        "url": "http://arxiv.org/abs/2509.19768v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19768v1",
        "arxiv_id": "2509.19768v1",
        "authors": [
            "Sina J. Semnani",
            "Han Zhang",
            "Xinyan He",
            "Merve Tekgürler",
            "Monica S. Lam"
        ],
        "submitted": "2025-09-24 05:38:45",
        "source": "arxiv",
        "comment": "EMNLP 2025"
    },
    {
        "title": "FusedANN: Convexified Hybrid ANN via Attribute-Vector Fusion",
        "abstract": "Vector search powers transformers technology, but real-world use demands\nhybrid queries that combine vector similarity with attribute filters (e.g.,\n\"top document in category X, from 2023\"). Current solutions trade off recall,\nspeed, and flexibility, relying on fragile index hacks that don't scale. We\nintroduce FusedANN (Fused Attribute-Vector Nearest Neighbor), a geometric\nframework that elevates filtering to ANN optimization constraints and\nintroduces a convex fused space via a Lagrangian-like relaxation. Our method\njointly embeds attributes and vectors through transformer-based\nconvexification, turning hard filters into continuous, weighted penalties that\npreserve top-k semantics while enabling efficient approximate search. We prove\nthat FusedANN reduces to exact filtering under high selectivity, gracefully\nrelaxes to semantically nearest attributes when exact matches are insufficient,\nand preserves downstream ANN alpha-approximation guarantees. Empirically,\nFusedANN improves query throughput by eliminating brittle filtering stages,\nachieving superior recall-latency tradeoffs on standard hybrid benchmarks\nwithout specialized index hacks, delivering up to 3 times higher throughput and\nbetter recall than state-of-the-art hybrid and graph-based systems.\nTheoretically, we provide explicit error bounds and parameter selection rules\nthat make FusedANN practical for production. This establishes a principled,\nscalable, and verifiable bridge between symbolic constraints and vector\nsimilarity, unlocking a new generation of filtered retrieval systems for large,\nhybrid, and dynamic NLP/ML workloads.",
        "url": "http://arxiv.org/abs/2509.19767v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19767v1",
        "arxiv_id": "2509.19767v1",
        "authors": [
            "Alireza Heidari",
            "Wei Zhang",
            "Ying Xiong"
        ],
        "submitted": "2025-09-24 05:33:53",
        "source": "arxiv",
        "comment": "62 pages,12 figures"
    },
    {
        "title": "PART: Progressive Alignment Representation Training for Multilingual Speech-To-Text with LLMs",
        "abstract": "Large language models (LLMs) have expanded from text to speech, giving rise\nto Speech Large Models (SLMs) that support recognition, translation, and\nsynthesis. A key challenge is aligning speech and text representations, which\nbecomes harder in multilingual settings. Existing methods often freeze LLM\nparameters and train encoders on multilingual data, but this forces\ncross-language convergence and limits performance. We introduce Progressive\nAlignment Representation Training (PART), a multi-stage and multi-task\nframework that separates within-language from cross-language alignment. During\ncross-language training, LLM parameters are dynamically activated, and\ntext-based tasks are later introduced to enhance multilingual understanding.\nExperiments on CommonVoice 15, Fleurs, Wenetspeech, and CoVoST2 show that PART\nsurpasses conventional approaches, with analysis confirming its ability to\nbalance language-specific distinctions and cross-language generalization. These\nresults demonstrate PART's effectiveness and generality for multilingual speech\nmodality alignment.",
        "url": "http://arxiv.org/abs/2509.19745v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19745v1",
        "arxiv_id": "2509.19745v1",
        "authors": [
            "Pei Zhang",
            "Andong Chen",
            "Xi Chen",
            "Baosong Yang",
            "Derek F. Wong",
            "Fei Huang"
        ],
        "submitted": "2025-09-24 03:54:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "HiCoLoRA: Addressing Context-Prompt Misalignment via Hierarchical Collaborative LoRA for Zero-Shot DST",
        "abstract": "Zero-shot Dialog State Tracking (zs-DST) is essential for enabling\nTask-Oriented Dialog Systems (TODs) to generalize to new domains without costly\ndata annotation. A central challenge lies in the semantic misalignment between\ndynamic dialog contexts and static prompts, leading to inflexible cross-layer\ncoordination, domain interference, and catastrophic forgetting. To tackle this,\nwe propose Hierarchical Collaborative Low-Rank Adaptation (HiCoLoRA), a\nframework that enhances zero-shot slot inference through robust prompt\nalignment. It features a hierarchical LoRA architecture for dynamic\nlayer-specific processing (combining lower-layer heuristic grouping and\nhigher-layer full interaction), integrates Spectral Joint Domain-Slot\nClustering to identify transferable associations (feeding an Adaptive Linear\nFusion Mechanism), and employs Semantic-Enhanced SVD Initialization\n(SemSVD-Init) to preserve pre-trained knowledge. Experiments on multi-domain\ndatasets MultiWOZ and SGD show that HiCoLoRA outperforms baselines, achieving\nSOTA in zs-DST. Code is available at https://github.com/carsonz/HiCoLoRA.",
        "url": "http://arxiv.org/abs/2509.19742v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19742v1",
        "arxiv_id": "2509.19742v1",
        "authors": [
            "Shuyu Zhang",
            "Yifan Wei",
            "Xinru Wang",
            "Yanmin Zhu",
            "Yangfan He",
            "Yixuan Weng",
            "Bin Li"
        ],
        "submitted": "2025-09-24 03:44:16",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning",
        "abstract": "Reinforcement learning (RL) has shown promise in training agentic models that\nmove beyond static benchmarks to engage in dynamic, multi-turn interactions.\nYet, the ultimate value of such agents lies in their ability to assist users, a\nsetting where diversity and dynamics of user interaction pose challenges. In\nthis work, we propose UserRL, a unified framework for training and evaluating\nuser-centric abilities through standardized gym environments paired with\nsimulated users. We systematically vary turn-level reward assignment and\ntrajectory-level score calculation to analyze how different formulations affect\nlearning under the GRPO algorithm. Our experiments across Qwen3 models reveal\nthree key findings: (i) SFT cold start is critical for unlocking initial\ninteraction ability and enabling sustained RL improvements; (ii) deliberate\ntrajectory scoring yields more efficient and effective multi-turn interactions;\nand (iii) while stronger simulated users (e.g., GPT-4o) facilitates training,\nopen-source simulators (e.g., Qwen3-32B) remain a cost-effective and\ntransferable option. Together, these results highlight that careful design of\nreward shaping and user simulation choice is as crucial as model scale, and\nestablish UserRL as a practical pathway for developing robust user-centric\nagentic models. All codes and data are public for future research.",
        "url": "http://arxiv.org/abs/2509.19736v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19736v1",
        "arxiv_id": "2509.19736v1",
        "authors": [
            "Cheng Qian",
            "Zuxin Liu",
            "Akshara Prabhakar",
            "Jielin Qiu",
            "Zhiwei Liu",
            "Haolin Chen",
            "Shirley Kokane",
            "Heng Ji",
            "Weiran Yao",
            "Shelby Heinecke",
            "Silvio Savarese",
            "Caiming Xiong",
            "Huan Wang"
        ],
        "submitted": "2025-09-24 03:33:20",
        "source": "arxiv",
        "comment": "28 Pages, 15 Figures, 6 Tables; Built upon latest UserBench release:\n  arXiv:2507.22034"
    },
    {
        "title": "Personality Vector: Modulating Personality of Large Language Models by Model Merging",
        "abstract": "Driven by the demand for personalized AI systems, there is growing interest\nin aligning the behavior of large language models (LLMs) with human traits such\nas personality. Previous attempts to induce personality in LLMs have shown\npromising results, but they struggle to capture the continuous and\nmultidimensional nature of human traits. In this work, we propose a novel\nmethod for personality modulation in LLMs via model merging. Specifically, we\nconstruct personality vectors by subtracting the weights of a pre-trained model\nfrom those of the fine-tuned model on a given personality trait. By merging\npersonality vectors, we enable LLMs to exhibit desired personality traits\nwithout additional training. Extensive experiments show that personality\nvectors enable continuous control over trait intensity and support the\ncomposition of multiple traits. Furthermore, personality vectors transfer\nacross diverse downstream models, suggesting that they encode generalizable\nrepresentations of personality. Our code is available at here.",
        "url": "http://arxiv.org/abs/2509.19727v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19727v1",
        "arxiv_id": "2509.19727v1",
        "authors": [
            "Seungjong Sun",
            "Seo Yeon Baek",
            "Jang Hyun Kim"
        ],
        "submitted": "2025-09-24 03:11:28",
        "source": "arxiv",
        "comment": "EMNLP 2025"
    },
    {
        "title": "Learning Contextual Retrieval for Robust Conversational Search",
        "abstract": "Effective conversational search demands a deep understanding of user intent\nacross multiple dialogue turns. Users frequently use abbreviations and shift\ntopics in the middle of conversations, posing challenges for conventional\nretrievers. While query rewriting techniques improve clarity, they often incur\nsignificant computational cost due to additional autoregressive steps.\nMoreover, although LLM-based retrievers demonstrate strong performance, they\nare not explicitly optimized to track user intent in multi-turn settings, often\nfailing under topic drift or contextual ambiguity. To address these\nlimitations, we propose ContextualRetriever, a novel LLM-based retriever that\ndirectly incorporates conversational context into the retrieval process. Our\napproach introduces: (1) a context-aware embedding mechanism that highlights\nthe current query within the dialogue history; (2) intent-guided supervision\nbased on high-quality rewritten queries; and (3) a training strategy that\npreserves the generative capabilities of the base LLM. Extensive evaluations\nacross multiple conversational search benchmarks demonstrate that\nContextualRetriever significantly outperforms existing methods while incurring\nno additional inference overhead.",
        "url": "http://arxiv.org/abs/2509.19700v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19700v1",
        "arxiv_id": "2509.19700v1",
        "authors": [
            "Seunghan Yang",
            "Juntae Lee",
            "Jihwan Bang",
            "Kyuhong Shim",
            "Minsoo Kim",
            "Simyung Chang"
        ],
        "submitted": "2025-09-24 02:17:37",
        "source": "arxiv",
        "comment": "EMNLP 2025 main conference"
    },
    {
        "title": "DyBBT: Dynamic Balance via Bandit inspired Targeting for Dialog Policy with Cognitive Dual-Systems",
        "abstract": "Task oriented dialog systems often rely on static exploration strategies that\ndo not adapt to dynamic dialog contexts, leading to inefficient exploration and\nsuboptimal performance. We propose DyBBT, a novel dialog policy learning\nframework that formalizes the exploration challenge through a structured\ncognitive state space capturing dialog progression, user uncertainty, and slot\ndependency. DyBBT proposes a bandit inspired meta-controller that dynamically\nswitches between a fast intuitive inference (System 1) and a slow deliberative\nreasoner (System 2) based on real-time cognitive states and visitation counts.\nExtensive experiments on single- and multi-domain benchmarks show that DyBBT\nachieves state-of-the-art performance in success rate, efficiency, and\ngeneralization, with human evaluations confirming its decisions are well\naligned with expert judgment. Code is available at\nhttps://github.com/carsonz/DyBBT.",
        "url": "http://arxiv.org/abs/2509.19695v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19695v1",
        "arxiv_id": "2509.19695v1",
        "authors": [
            "Shuyu Zhang",
            "Yifan Wei",
            "Jialuo Yuan",
            "Xinru Wang",
            "Yanmin Zhu",
            "Bin Li"
        ],
        "submitted": "2025-09-24 02:06:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Large Language Models for Pedestrian Safety: An Application to Predicting Driver Yielding Behavior at Unsignalized Intersections",
        "abstract": "Pedestrian safety is a critical component of urban mobility and is strongly\ninfluenced by the interactions between pedestrian decision-making and driver\nyielding behavior at crosswalks. Modeling driver--pedestrian interactions at\nintersections requires accurately capturing the complexity of these behaviors.\nTraditional machine learning models often struggle to capture the nuanced and\ncontext-dependent reasoning required for these multifactorial interactions, due\nto their reliance on fixed feature representations and limited\ninterpretability. In contrast, large language models (LLMs) are suited for\nextracting patterns from heterogeneous traffic data, enabling accurate modeling\nof driver-pedestrian interactions. Therefore, this paper leverages multimodal\nLLMs through a novel prompt design that incorporates domain-specific knowledge,\nstructured reasoning, and few-shot prompting, enabling interpretable and\ncontext-aware inference of driver yielding behavior, as an example application\nof modeling pedestrian--driver interaction. We benchmarked state-of-the-art\nLLMs against traditional classifiers, finding that GPT-4o consistently achieves\nthe highest accuracy and recall, while Deepseek-V3 excels in precision. These\nfindings highlight the critical trade-offs between model performance and\ncomputational efficiency, offering practical guidance for deploying LLMs in\nreal-world pedestrian safety systems.",
        "url": "http://arxiv.org/abs/2509.19657v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19657v1",
        "arxiv_id": "2509.19657v1",
        "authors": [
            "Yicheng Yang",
            "Zixian Li",
            "Jean Paul Bizimana",
            "Niaz Zafri",
            "Yongfeng Dong",
            "Tianyi Li"
        ],
        "submitted": "2025-09-24 00:25:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Human-AI Narrative Synthesis to Foster Shared Understanding in Civic Decision-Making",
        "abstract": "Community engagement processes in representative political contexts, like\nschool districts, generate massive volumes of feedback that overwhelm\ntraditional synthesis methods, creating barriers to shared understanding not\nonly between civic leaders and constituents but also among community members.\nTo address these barriers, we developed StoryBuilder, a human-AI collaborative\npipeline that transforms community input into accessible first-person\nnarratives. Using 2,480 community responses from an ongoing school rezoning\nprocess, we generated 124 composite stories and deployed them through a\nmobile-friendly StorySharer interface. Our mixed-methods evaluation combined a\nfour-month field deployment, user studies with 21 community members, and a\ncontrolled experiment examining how narrative composition affects participant\nreactions. Field results demonstrate that narratives helped community members\nrelate across diverse perspectives. In the experiment, experience-grounded\nnarratives generated greater respect and trust than opinion-heavy narratives.\nWe contribute a human-AI narrative synthesis system and insights on its varied\nacceptance and effectiveness in a real-world civic context.",
        "url": "http://arxiv.org/abs/2509.19643v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19643v1",
        "arxiv_id": "2509.19643v1",
        "authors": [
            "Cassandra Overney",
            "Hang Jiang",
            "Urooj Haider",
            "Cassandra Moe",
            "Jasmine Mangat",
            "Frank Pantano",
            "Effie G. McMillian",
            "Paul Riggins",
            "Nabeel Gillani"
        ],
        "submitted": "2025-09-23 23:19:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AutoSpec: An Agentic Framework for Automatically Drafting Patent Specification",
        "abstract": "Patents play a critical role in driving technological innovation by granting\ninventors exclusive rights to their inventions. However the process of drafting\na patent application is often expensive and time-consuming, making it a prime\ncandidate for automation. Despite recent advancements in language models,\nseveral challenges hinder the development of robust automated patent drafting\nsystems. First, the information within a patent application is highly\nconfidential, which often prevents the use of closed-source LLMs for automating\nthis task. Second, the process of drafting a patent application is difficult\nfor even the most advanced language models due to their long context, technical\nwriting style, and specialized domain knowledge. To address these challenges,\nwe introduce AutoSpec, a secure, agentic framework for Automatically drafting\npatent Specification. Our approach decomposes the drafting process into a\nsequence of manageable subtasks, each solvable by smaller, open-source language\nmodels enhanced with custom tools tailored for drafting patent specification.\nTo assess our system, we design a novel evaluation protocol in collaboration\nwith experienced patent attorneys. Our automatic and expert evaluations show\nthat AutoSpec outperforms existing baselines on a patent drafting task.",
        "url": "http://arxiv.org/abs/2509.19640v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19640v1",
        "arxiv_id": "2509.19640v1",
        "authors": [
            "Ryan Shea",
            "Zhou Yu"
        ],
        "submitted": "2025-09-23 23:10:18",
        "source": "arxiv",
        "comment": "EMNLP Findings 2025"
    },
    {
        "title": "Advancing Speech Summarization in Multi-modal LLMs with Reinforcement Learning",
        "abstract": "Speech summarization is a critical component of spoken content understanding,\nparticularly in the era of rapidly growing spoken and audiovisual data. Recent\nadvances in multi-modal large language models (MLLMs), leveraging the power of\nLLMs, enable generating textual summaries directly from speech without\nintermediate transcriptions, while supporting controllable styles and zero-shot\ngeneralization. However, open-source MLLMs continue to lag behind the\nstate-of-the-art text-based LLMs, limiting their practical deployment for\nspeech summarization. In this work, we present a novel multi-stage\nreinforcement learning training framework to enhance the speech summarization\ncapabilities in MLLMs. Our model delivers substantial improvements over strong\nbaselines, outperforms much larger MLLMs, and significantly narrows the gap\nwith state-of-the-art text-based LLMs.",
        "url": "http://arxiv.org/abs/2509.19631v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19631v1",
        "arxiv_id": "2509.19631v1",
        "authors": [
            "Shaoshi Ling",
            "Gang Liu",
            "Guoli Ye",
            "Jinyu Li"
        ],
        "submitted": "2025-09-23 22:45:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Multimodal Language Models with Modality-Specific Experts for Financial Forecasting from Interleaved Sequences of Text and Time Series",
        "abstract": "Text and time series data offer complementary views of financial markets:\nnews articles provide narrative context about company events, while stock\nprices reflect how markets react to those events. However, despite their\ncomplementary nature, effectively integrating these interleaved modalities for\nimproved forecasting remains challenging. In this work, we propose a unified\nneural architecture that models these interleaved sequences using\nmodality-specific experts, allowing the model to learn unique time series\npatterns, while still enabling joint reasoning across modalities and preserving\npretrained language understanding capabilities. To further improve multimodal\nunderstanding, we introduce a cross-modal alignment framework with a salient\ntoken weighting mechanism that learns to align representations across\nmodalities with a focus on the most informative tokens. We demonstrate the\neffectiveness of our approach on a large-scale financial forecasting task,\nachieving state-of-the-art performance across a wide variety of strong unimodal\nand multimodal baselines. We develop an interpretability method that reveals\ninsights into the value of time series-context and reinforces the design of our\ncross-modal alignment objective. Finally, we demonstrate that these\nimprovements translate to meaningful economic gains in investment simulations.",
        "url": "http://arxiv.org/abs/2509.19628v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19628v1",
        "arxiv_id": "2509.19628v1",
        "authors": [
            "Ross Koval",
            "Nicholas Andrews",
            "Xifeng Yan"
        ],
        "submitted": "2025-09-23 22:40:31",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "Evaluating Language Translation Models by Playing Telephone",
        "abstract": "Our ability to efficiently and accurately evaluate the quality of machine\ntranslation systems has been outrun by the effectiveness of current language\nmodels--which limits the potential for further improving these models on more\nchallenging tasks like long-form and literary translation. We propose an\nunsupervised method to generate training data for translation evaluation over\ndifferent document lengths and application domains by repeated rounds of\ntranslation between source and target languages. We evaluate evaluation systems\ntrained on texts mechanically generated using both model rotation and language\ntranslation approaches, demonstrating improved performance over a popular\ntranslation evaluation system (xCOMET) on two different tasks: (i) scoring the\nquality of a given translation against a human reference and (ii) selecting\nwhich of two translations is generationally closer to an original source\ndocument.",
        "url": "http://arxiv.org/abs/2509.19611v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19611v1",
        "arxiv_id": "2509.19611v1",
        "authors": [
            "Syeda Jannatus Saba",
            "Steven Skiena"
        ],
        "submitted": "2025-09-23 22:01:52",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 Main Conference as a long paper"
    },
    {
        "title": "Anatomy of a Feeling: Narrating Embodied Emotions via Large Vision-Language Models",
        "abstract": "The embodiment of emotional reactions from body parts contains rich\ninformation about our affective experiences. We propose a framework that\nutilizes state-of-the-art large vision-language models (LVLMs) to generate\nEmbodied LVLM Emotion Narratives (ELENA). These are well-defined, multi-layered\ntext outputs, primarily comprising descriptions that focus on the salient body\nparts involved in emotional reactions. We also employ attention maps and\nobserve that contemporary models exhibit a persistent bias towards the facial\nregion. Despite this limitation, we observe that our employed framework can\neffectively recognize embodied emotions in face-masked images, outperforming\nbaselines without any fine-tuning. ELENA opens a new trajectory for embodied\nemotion analysis across the modality of vision and enriches modeling in an\naffect-aware setting.",
        "url": "http://arxiv.org/abs/2509.19595v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19595v1",
        "arxiv_id": "2509.19595v1",
        "authors": [
            "Mohammad Saim",
            "Phan Anh Duong",
            "Cat Luong",
            "Aniket Bhanderi",
            "Tianyu Jiang"
        ],
        "submitted": "2025-09-23 21:34:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models",
        "abstract": "We introduce GuessingGame, a protocol for evaluating large language models\n(LLMs) as strategic question-askers in open-ended, open-domain settings. A\nGuesser LLM identifies a hidden object by posing free-form questions to an\nOracle without predefined choices or candidate lists. To measure question\nquality, we propose two information gain (IG) metrics: a Bayesian method that\ntracks belief updates over semantic concepts using LLM-scored relevance, and an\nentropy-based method that filters candidates via ConceptNet. Both metrics are\nmodel-agnostic and support post hoc analysis. Across 858 games with multiple\nmodels and prompting strategies, higher IG strongly predicts efficiency: a\none-standard-deviation IG increase reduces expected game length by 43\\%.\nPrompting constraints guided by IG, such as enforcing question diversity,\nenable weaker models to significantly improve performance. These results show\nthat question-asking in LLMs is both measurable and improvable, and crucial for\ninteractive reasoning.",
        "url": "http://arxiv.org/abs/2509.19593v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19593v1",
        "arxiv_id": "2509.19593v1",
        "authors": [
            "Dylan Hutson",
            "Daniel Vennemeyer",
            "Aneesh Deshmukh",
            "Justin Zhan",
            "Tianyu Jiang"
        ],
        "submitted": "2025-09-23 21:31:14",
        "source": "arxiv",
        "comment": "EMNLP 2025, 17 pages, 2 figures"
    },
    {
        "title": "Frame-Stacked Local Transformers For Efficient Multi-Codebook Speech Generation",
        "abstract": "Speech generation models based on large language models (LLMs) typically\noperate on discrete acoustic codes, which differ fundamentally from text tokens\ndue to their multicodebook structure. At each timestep, models must predict N\ncodebook entries jointly, introducing dependencies that challenge simple\nparallel prediction approaches. Parallel prediction assumes independence among\ncodebooks, yielding efficient decoding but often at the cost of reduced\nfidelity. To address this, hierarchical strategies employ a local transformer\n(LT) to refine predictions and capture intra-timestep dependencies. In this\nwork, we systematically investigate two LT architectures: an autoregressive\ntransformer that generates codebooks sequentially, and a MaskGIT-based\ntransformer that performs iterative masked prediction. Both designs further\nenable frame stacking, where the primary transformer predicts multiple frames\njointly, and the LT decodes their codebooks, offering improvements in speed\nwithout compromising perceptual quality. Through extensive analysis, we\ncharacterize the tradeoffs between parallel and iterative sampling strategies\nacross different throughput and quality regimes. Finally, we propose practical\nguidelines for selecting decoding strategies based on deployment priorities\nsuch as computational efficiency and synthesis fidelity.",
        "url": "http://arxiv.org/abs/2509.19592v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19592v1",
        "arxiv_id": "2509.19592v1",
        "authors": [
            "Roy Fejgin",
            "Paarth Neekhara",
            "Xuesong Yang",
            "Edresson Casanova",
            "Ryan Langman Jaehyeon Kim",
            "Subhankar Ghosh",
            "Shehzeen Hussain",
            "Jason Li"
        ],
        "submitted": "2025-09-23 21:31:00",
        "source": "arxiv",
        "comment": "This work has been submitted to the IEEE for possible publication"
    },
    {
        "title": "LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines",
        "abstract": "Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our view\nof the world. For example, Large Language Models (LLMs) based applications such\nas ChatGPT have shown the capability of generating human-like conversation on\nextensive topics. Due to the impressive performance on a variety of\nlanguage-related tasks (e.g., open-domain question answering, translation, and\ndocument summarization), one can envision the far-reaching impacts that can be\nbrought by the LLMs with broader real-world applications (e.g., customer\nservice, education and accessibility, and scientific discovery). Inspired by\ntheir success, this paper will offer an overview of state-of-the-art LLMs and\ntheir integration into a wide range of academic disciplines, including: (1)\narts, letters, and law (e.g., history, philosophy, political science, arts and\narchitecture, law), (2) economics and business (e.g., finance, economics,\naccounting, marketing), and (3) science and engineering (e.g., mathematics,\nphysics and mechanical engineering, chemistry and chemical engineering, life\nsciences and bioengineering, earth sciences and civil engineering, computer\nscience and electrical engineering). Integrating humanity and technology, in\nthis paper, we will explore how LLMs are shaping research and practice in these\nfields, while also discussing key limitations, open challenges, and future\ndirections in the era of generative AI. The review of how LLMs are engaged\nacross disciplines-along with key observations and insights-can help\nresearchers and practitioners interested in exploiting LLMs to advance their\nworks in diverse real-world applications.",
        "url": "http://arxiv.org/abs/2509.19580v2",
        "pdf_url": "http://arxiv.org/pdf/2509.19580v2",
        "arxiv_id": "2509.19580v2",
        "authors": [
            "Yanfang Fanny Ye",
            "Zheyuan Zhang",
            "Tianyi Ma",
            "Zehong Wang",
            "Yiyang Li",
            "Shifu Hou",
            "Weixiang Sun",
            "Kaiwen Shi",
            "Yijun Ma",
            "Wei Song",
            "Ahmed Abbasi",
            "Ying Cheng",
            "Jane Cleland-Huang",
            "Steven Corcelli",
            "Patricia Culligan",
            "Robert Goulding",
            "Ming Hu",
            "Ting Hua",
            "John Lalor",
            "Fang Liu",
            "Tengfei Luo",
            "Ed Maginn",
            "Nuno Moniz",
            "Jason Rohr",
            "Brett Savoie",
            "Daniel Slate",
            "Tom Stapleford",
            "Matthew Webber",
            "Olaf Wiest",
            "Johnny Zhang",
            "Nitesh Chawla"
        ],
        "submitted": "2025-09-23 21:09:24",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ExPe: Exact Positional Encodings for Generative Transformer Models with Extrapolating Capabilities",
        "abstract": "This paper introduces a novel approach to position embeddings in transformer\nmodels, named \"Exact Positional Embeddings\" (ExPE). An absolute positional\nembedding method that can extrapolate to sequences of lengths longer than the\nones it was trained on. Traditional transformer models rely on absolute or\nrelative position embeddings to incorporate positional information into token\nembeddings, which often struggle with extrapolation to sequences longer than\nthose seen during training. Our proposed method utilizes a novel embedding\nstrategy that encodes exact positional information by overriding specific\ndimensions of the embedding vectors, thereby enabling a more precise\nrepresentation of token positions. The proposed approach not only maintains the\nintegrity of the original embeddings but also enhances the model's ability to\ngeneralize to more extended sequences. In causal language modeling, our ExPE\nembeddings significantly reduce perplexity compared to rotary and sinusoidal\nembeddings, when tested on sequences longer than those used in training.",
        "url": "http://arxiv.org/abs/2509.19569v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19569v1",
        "arxiv_id": "2509.19569v1",
        "authors": [
            "Aleksis Datseris",
            "Sylvia Vassileva",
            "Ivan Koychev",
            "Svetla Boytcheva"
        ],
        "submitted": "2025-09-23 20:51:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Retrieval Augmented Generation based context discovery for ASR",
        "abstract": "This work investigates retrieval augmented generation as an efficient\nstrategy for automatic context discovery in context-aware Automatic Speech\nRecognition (ASR) system, in order to improve transcription accuracy in the\npresence of rare or out-of-vocabulary terms. However, identifying the right\ncontext automatically remains an open challenge. This work proposes an\nefficient embedding-based retrieval approach for automatic context discovery in\nASR. To contextualize its effectiveness, two alternatives based on large\nlanguage models (LLMs) are also evaluated: (1) large language model (LLM)-based\ncontext generation via prompting, and (2) post-recognition transcript\ncorrection using LLMs. Experiments on the TED-LIUMv3, Earnings21 and SPGISpeech\ndemonstrate that the proposed approach reduces WER by up to 17% (percentage\ndifference) relative to using no-context, while the oracle context results in a\nreduction of up to 24.1%.",
        "url": "http://arxiv.org/abs/2509.19567v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19567v1",
        "arxiv_id": "2509.19567v1",
        "authors": [
            "Dimitrios Siskos",
            "Stavros Papadopoulos",
            "Pablo Peso Parada",
            "Jisi Zhang",
            "Karthikeyan Saravanan",
            "Anastasios Drosou"
        ],
        "submitted": "2025-09-23 20:47:15",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025"
    },
    {
        "title": "Uncertainty in Semantic Language Modeling with PIXELS",
        "abstract": "Pixel-based language models aim to solve the vocabulary bottleneck problem in\nlanguage modeling, but the challenge of uncertainty quantification remains\nopen. The novelty of this work consists of analysing uncertainty and confidence\nin pixel-based language models across 18 languages and 7 scripts, all part of 3\nsemantically challenging tasks. This is achieved through several methods such\nas Monte Carlo Dropout, Transformer Attention, and Ensemble Learning. The\nresults suggest that pixel-based models underestimate uncertainty when\nreconstructing patches. The uncertainty is also influenced by the script, with\nLatin languages displaying lower uncertainty. The findings on ensemble learning\nshow better performance when applying hyperparameter tuning during the named\nentity recognition and question-answering tasks across 16 languages.",
        "url": "http://arxiv.org/abs/2509.19563v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19563v1",
        "arxiv_id": "2509.19563v1",
        "authors": [
            "Stefania Radu",
            "Marco Zullich",
            "Matias Valdenegro-Toro"
        ],
        "submitted": "2025-09-23 20:43:50",
        "source": "arxiv",
        "comment": "9 pages, 6 figures, UncertaiNLP 2025 Workshop @ EMNLP Camera Ready"
    },
    {
        "title": "Confidence Calibration in Large Language Model-Based Entity Matching",
        "abstract": "This research aims to explore the intersection of Large Language Models and\nconfidence calibration in Entity Matching. To this end, we perform an empirical\nstudy to compare baseline RoBERTa confidences for an Entity Matching task\nagainst confidences that are calibrated using Temperature Scaling, Monte Carlo\nDropout and Ensembles. We use the Abt-Buy, DBLP-ACM, iTunes-Amazon and Company\ndatasets. The findings indicate that the proposed modified RoBERTa model\nexhibits a slight overconfidence, with Expected Calibration Error scores\nranging from 0.0043 to 0.0552 across datasets. We find that this overconfidence\ncan be mitigated using Temperature Scaling, reducing Expected Calibration Error\nscores by up to 23.83%.",
        "url": "http://arxiv.org/abs/2509.19557v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19557v1",
        "arxiv_id": "2509.19557v1",
        "authors": [
            "Iris Kamsteeg",
            "Juan Cardenas-Cartagena",
            "Floris van Beers",
            "Gineke ten Holt",
            "Tsegaye Misikir Tashu",
            "Matias Valdenegro-Toro"
        ],
        "submitted": "2025-09-23 20:29:10",
        "source": "arxiv",
        "comment": "9 pages, 2 figures. UncertaiNLP 2025 Workshop @ EMNLP Camera Ready"
    },
    {
        "title": "Do LLMs Encode Frame Semantics? Evidence from Frame Identification",
        "abstract": "We investigate whether large language models encode latent knowledge of frame\nsemantics, focusing on frame identification, a core challenge in frame semantic\nparsing that involves selecting the appropriate semantic frame for a target\nword in context. Using the FrameNet lexical resource, we evaluate models under\nprompt-based inference and observe that they can perform frame identification\neffectively even without explicit supervision. To assess the impact of\ntask-specific training, we fine-tune the model on FrameNet data, which\nsubstantially improves in-domain accuracy while generalizing well to\nout-of-domain benchmarks. Further analysis shows that the models can generate\nsemantically coherent frame definitions, highlighting the model's internalized\nunderstanding of frame semantics.",
        "url": "http://arxiv.org/abs/2509.19540v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19540v1",
        "arxiv_id": "2509.19540v1",
        "authors": [
            "Jayanth Krishna Chundru",
            "Rudrashis Poddar",
            "Jie Cao",
            "Tianyu Jiang"
        ],
        "submitted": "2025-09-23 20:09:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning",
        "abstract": "The scaling of Large Language Models (LLMs) has exposed a critical gap\nbetween their performance on static benchmarks and their fragility in dynamic,\ninformation-rich environments. While models excel at isolated tasks, the\ncomputational limits that govern their reasoning under cognitive load remain\npoorly understood. In this work, we introduce a formal theory of computational\ncognitive load, positing that extraneous, task-irrelevant information (Context\nSaturation) and interference from task-switching (Attentional Residue) are key\nmechanisms that degrade performance. We designed the Interleaved Cognitive\nEvaluation (ICE), a deconfounded benchmark to systematically manipulate these\nload factors on challenging multi-hop reasoning tasks. A comprehensive study (N\n= 10 replications per item across 200 questions) revealed significant\nperformance variations across five instruction-tuned models. Smaller\nopen-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2)\nexhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all\nconditions, including clean controls, on this high-intrinsic-load task. In\ncontrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85%\naccuracy in control conditions, with a statistically significant degradation\nunder context saturation ($\\beta = -0.003$ per % load, $p < 0.001$). These\nfindings provide preliminary evidence that cognitive load is a key contributor\nto reasoning failures, supporting theories of hallucination-as-guessing under\nuncertainty. We conclude that dynamic, cognitive-aware stress testing, as\nexemplified by the ICE benchmark, is essential for evaluating the true\nresilience and safety of advanced AI systems.",
        "url": "http://arxiv.org/abs/2509.19517v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19517v1",
        "arxiv_id": "2509.19517v1",
        "authors": [
            "Sai Teja Reddy Adapala"
        ],
        "submitted": "2025-09-23 19:36:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AIRwaves at CheckThat! 2025: Retrieving Scientific Sources for Implicit Claims on Social Media with Dual Encoders and Neural Re-Ranking",
        "abstract": "Linking implicit scientific claims made on social media to their original\npublications is crucial for evidence-based fact-checking and scholarly\ndiscourse, yet it is hindered by lexical sparsity, very short queries, and\ndomain-specific language. Team AIRwaves ranked second in Subtask 4b of the\nCLEF-2025 CheckThat! Lab with an evidence-retrieval approach that markedly\noutperforms the competition baseline. The optimized sparse-retrieval\nbaseline(BM25) achieves MRR@5 = 0.5025 on the gold label blind test set. To\nsurpass this baseline, a two-stage retrieval pipeline is introduced: (i) a\nfirst stage that uses a dual encoder based on E5-large, fine-tuned using\nin-batch and mined hard negatives and enhanced through chunked tokenization and\nrich document metadata; and (ii) a neural re-ranking stage using a SciBERT\ncross-encoder. Replacing purely lexical matching with neural representations\nlifts performance to MRR@5 = 0.6174, and the complete pipeline further improves\nto MRR@5 = 0.6828. The findings demonstrate that coupling dense retrieval with\nneural re-rankers delivers a powerful and efficient solution for tweet-to-study\nmatching and provides a practical blueprint for future evidence-retrieval\npipelines.",
        "url": "http://arxiv.org/abs/2509.19509v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19509v1",
        "arxiv_id": "2509.19509v1",
        "authors": [
            "Cem Ashbaugh",
            "Leon Baumgärtner",
            "Tim Gress",
            "Nikita Sidorov",
            "Daniel Werner"
        ],
        "submitted": "2025-09-23 19:26:31",
        "source": "arxiv",
        "comment": "CLEF 2025 (Conference and Labs of the Evaluation Forum)"
    },
    {
        "title": "STARQA: A Question Answering Dataset for Complex Analytical Reasoning over Structured Databases",
        "abstract": "Semantic parsing methods for converting text to SQL queries enable question\nanswering over structured data and can greatly benefit analysts who routinely\nperform complex analytics on vast data stored in specialized relational\ndatabases. Although several benchmarks measure the abilities of text to SQL,\nthe complexity of their questions is inherently limited by the level of\nexpressiveness in query languages and none focus explicitly on questions\ninvolving complex analytical reasoning which require operations such as\ncalculations over aggregate analytics, time series analysis or scenario\nunderstanding. In this paper, we introduce STARQA, the first public\nhuman-created dataset of complex analytical reasoning questions and answers on\nthree specialized-domain databases. In addition to generating SQL directly\nusing LLMs, we evaluate a novel approach (Text2SQLCode) that decomposes the\ntask into a combination of SQL and Python: SQL is responsible for data\nfetching, and Python more naturally performs reasoning. Our results demonstrate\nthat identifying and combining the abilities of SQL and Python is beneficial\ncompared to using SQL alone, yet the dataset still remains quite challenging\nfor the existing state-of-the-art LLMs.",
        "url": "http://arxiv.org/abs/2509.19508v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19508v1",
        "arxiv_id": "2509.19508v1",
        "authors": [
            "Mounica Maddela",
            "Lingjue Xie",
            "Daniel Preotiuc-Pietro",
            "Mausam"
        ],
        "submitted": "2025-09-23 19:26:16",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 long paper"
    },
    {
        "title": "A Pipeline to Assess Merging Methods via Behavior and Internals",
        "abstract": "Merging methods combine the weights of multiple language models (LMs) to\nleverage their capacities, such as for domain adaptation. While existing\nstudies investigate merged models from a solely behavioral perspective, we\noffer the first comprehensive view by assessing and connecting their behavior\nand internals. We present a novel evaluation pipeline that first merges\nmultiple parent LMs, and then evaluates the merged models in comparison to the\ninitial ones based on their behavior on downstream tasks, like MMLU, and the\ninternal encoded linguistic competence. We showcase this pipeline by assessing\nthe merging of instruction fine-tuned with math- and code-adapted LMs from the\nQwen2.5 family. Our results show that merging methods impacts behavior and\ninternals differently. While the performance of merged models is typically\nbetween that of the two parent models, their encoded information about\nlinguistic phenomena, particularly in morphology and syntax, can surpass the\nparent models. Moreover, we find weak ranking correlation between this behavior\nand internal evaluation. With our pipeline and initial results, we emphasize\nthe need for more comprehensive evaluations of model merging methods to gain a\nfaithful understanding of their capabilities and reliability, beyond potential\nsuperficial behavioral advances.",
        "url": "http://arxiv.org/abs/2509.19476v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19476v1",
        "arxiv_id": "2509.19476v1",
        "authors": [
            "Yutaro Sigris",
            "Andreas Waldis"
        ],
        "submitted": "2025-09-23 18:37:32",
        "source": "arxiv",
        "comment": "BlackboxNLP"
    }
]