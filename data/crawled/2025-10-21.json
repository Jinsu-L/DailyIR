[
    {
        "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
        "abstract": "Large language models (LLMs) increasingly rely on long-context modeling for\ntasks such as document understanding, code analysis, and multi-step reasoning.\nHowever, scaling context windows to the million-token level brings prohibitive\ncomputational and memory costs, limiting the practicality of long-context LLMs.\nIn this work, we take a different perspective-visual context scaling-to tackle\nthis challenge. Instead of extending token-based sequences, we propose Glyph, a\nframework that renders long texts into images and processes them with\nvision-language models (VLMs). This approach substantially compresses textual\ninput while preserving semantic information, and we further design an\nLLM-driven genetic search to identify optimal visual rendering configurations\nfor balancing accuracy and compression. Through extensive experiments, we\ndemonstrate that our method achieves 3-4x token compression while maintaining\naccuracy comparable to leading LLMs such as Qwen3-8B on various long-context\nbenchmarks. This compression also leads to around 4x faster prefilling and\ndecoding, and approximately 2x faster SFT training. Furthermore, under extreme\ncompression, a 128K-context VLM could scale to handle 1M-token-level text\ntasks. In addition, the rendered text data benefits real-world multimodal\ntasks, such as document understanding. Our code and model are released at\nhttps://github.com/thu-coai/Glyph.",
        "url": "http://arxiv.org/abs/2510.17800v2",
        "pdf_url": "http://arxiv.org/pdf/2510.17800v2",
        "arxiv_id": "2510.17800v2",
        "authors": [
            "Jiale Cheng",
            "Yusen Liu",
            "Xinyu Zhang",
            "Yulin Fei",
            "Wenyi Hong",
            "Ruiliang Lyu",
            "Weihan Wang",
            "Zhe Su",
            "Xiaotao Gu",
            "Xiao Liu",
            "Yushi Bai",
            "Jie Tang",
            "Hongning Wang",
            "Minlie Huang"
        ],
        "submitted": "2025-10-20 17:58:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics",
        "abstract": "As information grows exponentially, enterprises face increasing pressure to\ntransform unstructured data into coherent, actionable insights. While\nautonomous agents show promise, they often struggle with domain-specific\nnuances, intent alignment, and enterprise integration. We present Enterprise\nDeep Research (EDR), a multi-agent system that integrates (1) a Master Planning\nAgent for adaptive query decomposition, (2) four specialized search agents\n(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool\necosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a\nVisualization Agent for data-driven insights, and (5) a reflection mechanism\nthat detects knowledge gaps and updates research direction with optional\nhuman-in-the-loop steering guidance. These components enable automated report\ngeneration, real-time streaming, and seamless enterprise deployment, as\nvalidated on internal datasets. On open-ended benchmarks including DeepResearch\nBench and DeepConsult, EDR outperforms state-of-the-art agentic systems without\nany human steering. We release the EDR framework and benchmark trajectories to\nadvance research on multi-agent reasoning applications.\n  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and\nDataset at https://huggingface.co/datasets/Salesforce/EDR-200",
        "url": "http://arxiv.org/abs/2510.17797v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17797v1",
        "arxiv_id": "2510.17797v1",
        "authors": [
            "Akshara Prabhakar",
            "Roshan Ram",
            "Zixiang Chen",
            "Silvio Savarese",
            "Frank Wang",
            "Caiming Xiong",
            "Huan Wang",
            "Weiran Yao"
        ],
        "submitted": "2025-10-20 17:55:11",
        "source": "arxiv",
        "comment": "Technical report; 13 pages plus references and appendices"
    },
    {
        "title": "Executable Knowledge Graphs for Replicating AI Research",
        "abstract": "Replicating AI research is a crucial yet challenging task for large language\nmodel (LLM) agents. Existing approaches often struggle to generate executable\ncode, primarily due to insufficient background knowledge and the limitations of\nretrieval-augmented generation (RAG) methods, which fail to capture latent\ntechnical details hidden in referenced papers. Furthermore, previous approaches\ntend to overlook valuable implementation-level code signals and lack structured\nknowledge representations that support multi-granular retrieval and reuse. To\novercome these challenges, we propose Executable Knowledge Graphs (xKG), a\nmodular and pluggable knowledge base that automatically integrates technical\ninsights, code snippets, and domain-specific knowledge extracted from\nscientific literature. When integrated into three agent frameworks with two\ndifferent LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on\nPaperBench, demonstrating its effectiveness as a general and extensible\nsolution for automated AI research replication. Code will released at\nhttps://github.com/zjunlp/xKG.",
        "url": "http://arxiv.org/abs/2510.17795v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17795v1",
        "arxiv_id": "2510.17795v1",
        "authors": [
            "Yujie Luo",
            "Zhuoyun Yu",
            "Xuehai Wang",
            "Yuqi Zhu",
            "Ningyu Zhang",
            "Lanning Wei",
            "Lun Du",
            "Da Zheng",
            "Huajun Chen"
        ],
        "submitted": "2025-10-20 17:53:23",
        "source": "arxiv",
        "comment": "Work in progress"
    },
    {
        "title": "Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains",
        "abstract": "Finetuning specialized generative evaluators has emerged as a popular\nparadigm to meet the increasing demand for scalable evaluation during both\ntraining and test-time. However, recent work has largely focused on applying\nnew methodology, such as reinforcement learning (RL), to training evaluators,\nshying away from large-scale, data-driven development. In this work, we focus\non data scaling, curating a set of 2.5M samples spanning five unique evaluation\ntasks (pairwise, step-level, reference-free and reference-based verification,\nand single rating) and multiple domains focused on reasoning evaluation. With\nour data, we train Foundational Automatic Reasoning Evaluators (FARE), a family\nof 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative\nrejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges\nlarger specialized RL-trained evaluators and FARE-20B sets the new standard for\nopen-source evaluators, surpassing specialized 70B+ evaluators. Beyond static\nbenchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,\nFARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,\nFARE improves the downstream RL-trained model performance by up to 14.1% vs.\nstring-matching verifiers. When initialized from FARE, a continually-finetuned\nFARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.",
        "url": "http://arxiv.org/abs/2510.17793v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17793v1",
        "arxiv_id": "2510.17793v1",
        "authors": [
            "Austin Xu",
            "Xuan-Phi Nguyen",
            "Yilun Zhou",
            "Chien-Sheng Wu",
            "Caiming Xiong",
            "Shafiq Joty"
        ],
        "submitted": "2025-10-20 17:52:06",
        "source": "arxiv",
        "comment": "29 pages, 9 tables, 6 figures"
    },
    {
        "title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action",
        "abstract": "Multimodal agents for computer use rely exclusively on primitive actions\n(click, type, scroll) that require accurate visual grounding and lengthy\nexecution chains, leading to cascading failures and performance bottlenecks.\nWhile other agents leverage rich programmatic interfaces (APIs, MCP servers,\ntools), computer-use agents (CUAs) remain isolated from these capabilities. We\npresent UltraCUA, a foundation model that bridges this gap through hybrid\naction -- seamlessly integrating GUI primitives with high-level programmatic\ntool calls. To achieve this, our approach comprises four key components: (1) an\nautomated pipeline that scales programmatic tools from software documentation,\nopen-source repositories, and code generation; (2) a synthetic data engine\nproducing over 17,000 verifiable tasks spanning real-world computer-use\nscenarios; (3) a large-scale high-quality hybrid action trajectory collection\nwith both low-level GUI actions and high-level programmatic tool calls; and (4)\na two-stage training pipeline combining supervised fine-tuning with online\nreinforcement learning, enabling strategic alternation between low-level and\nhigh-level actions. Experiments with our 7B and 32B models demonstrate\nsubstantial improvements over state-of-the-art agents. On OSWorld, UltraCUA\nmodels achieve an average 22% relative improvement over base models, while\nbeing 11% faster in terms of steps. Out-of-domain evaluation on\nWindowsAgentArena shows our model reaches 21.7% success rate, outperforming\nbaselines trained on Windows data. The hybrid action mechanism proves critical,\nreducing error propagation while maintaining execution efficiency.",
        "url": "http://arxiv.org/abs/2510.17790v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17790v1",
        "arxiv_id": "2510.17790v1",
        "authors": [
            "Yuhao Yang",
            "Zhen Yang",
            "Zi-Yi Dou",
            "Anh Nguyen",
            "Keen You",
            "Omar Attia",
            "Andrew Szot",
            "Michael Feng",
            "Ram Ramrakhya",
            "Alexander Toshev",
            "Chao Huang",
            "Yinfei Yang",
            "Zhe Gan"
        ],
        "submitted": "2025-10-20 17:48:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits",
        "abstract": "Large Language Models (LLMs) are improving at an exceptional rate. With the\nadvent of agentic workflows, multi-turn dialogue has become the de facto mode\nof interaction with LLMs for completing long and complex tasks. While LLM\ncapabilities continue to improve, they remain increasingly susceptible to\njailbreaking, especially in multi-turn scenarios where harmful intent can be\nsubtly injected across the conversation to produce nefarious outcomes. While\nsingle-turn attacks have been extensively explored, adaptability, efficiency\nand effectiveness continue to remain key challenges for their multi-turn\ncounterparts. To address these gaps, we present PLAGUE, a novel plug-and-play\nframework for designing multi-turn attacks inspired by lifelong-learning\nagents. PLAGUE dissects the lifetime of a multi-turn attack into three\ncarefully designed phases (Primer, Planner and Finisher) that enable a\nsystematic and information-rich exploration of the multi-turn attack family.\nEvaluations show that red-teaming agents designed using PLAGUE achieve\nstate-of-the-art jailbreaking results, improving attack success rates (ASR) by\nmore than 30% across leading models in a lesser or comparable query budget.\nParticularly, PLAGUE enables an ASR (based on StrongReject) of 81.4% on\nOpenAI's o3 and 67.3% on Claude's Opus 4.1, two models that are considered\nhighly resistant to jailbreaks in safety literature. Our work offers tools and\ninsights to understand the importance of plan initialization, context\noptimization and lifelong learning in crafting multi-turn attacks for a\ncomprehensive model vulnerability evaluation.",
        "url": "http://arxiv.org/abs/2510.17947v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17947v1",
        "arxiv_id": "2510.17947v1",
        "authors": [
            "Neeladri Bhuiya",
            "Madhav Aggarwal",
            "Diptanshu Purwar"
        ],
        "submitted": "2025-10-20 17:37:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Mapping Post-Training Forgetting in Language Models at Scale",
        "abstract": "Scaled post-training now drives many of the largest capability gains in\nlanguage models (LMs), yet its effect on pretrained knowledge remains poorly\nunderstood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.\npresident or an API call) does not \"average out\" by recalling another. Hence,\nwe propose a sample-wise paradigm to measure what is forgotten and when\nbackward transfer occurs. Our metric counts 1->0 transitions (correct before\npost-training, incorrect after) to quantify forgetting and 0->1 transitions to\nquantify backward transfer. Traditional task averages conflate these effects\nand obscure large changes. For multiple-choice benchmarks, we add\nchance-adjusted variants that subtract the expected contribution of random\nguessing from pre- and post-training accuracies. We apply this framework across\npost-training stages, model sizes, and data scales. Our large-scale analysis\nshows that: (1) Domain-continual pretraining induces moderate forgetting with\nlow-to-moderate backward transfer; (2) RL/SFT post-training applied to base\nmodels and Instruction tuning yields moderate-to-large backward transfer on\nmath and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to\ninstruction-tuned models is sensitive on data scale: at small scales, both\nforgetting and backward transfer are small; at larger scales, effects are mixed\nand warrant further study with better controls; (4) Model merging does not\nreliably mitigate forgetting. Overall, our framework offers a practical\nyardstick for mapping how post-training alters pretrained knowledge at scale --\nenabling progress towards generally capable AI systems.",
        "url": "http://arxiv.org/abs/2510.17776v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17776v1",
        "arxiv_id": "2510.17776v1",
        "authors": [
            "Jackson Harmon",
            "Andreas Hochlehnert",
            "Matthias Bethge",
            "Ameya Prabhu"
        ],
        "submitted": "2025-10-20 17:35:47",
        "source": "arxiv",
        "comment": "43 pages,15 figures"
    },
    {
        "title": "Evaluating Medical LLMs by Levels of Autonomy: A Survey Moving from Benchmarks to Applications",
        "abstract": "Medical Large language models achieve strong scores on standard benchmarks;\nhowever, the transfer of those results to safe and reliable performance in\nclinical workflows remains a challenge. This survey reframes evaluation through\na levels-of-autonomy lens (L0-L3), spanning informational tools, information\ntransformation and aggregation, decision support, and supervised agents. We\nalign existing benchmarks and metrics with the actions permitted at each level\nand their associated risks, making the evaluation targets explicit. This\nmotivates a level-conditioned blueprint for selecting metrics, assembling\nevidence, and reporting claims, alongside directions that link evaluation to\noversight. By centering autonomy, the survey moves the field beyond score-based\nclaims toward credible, risk-aware evidence for real clinical use.",
        "url": "http://arxiv.org/abs/2510.17764v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17764v1",
        "arxiv_id": "2510.17764v1",
        "authors": [
            "Xiao Ye",
            "Jacob Dineen",
            "Zhaonan Li",
            "Zhikun Xu",
            "Weiyu Chen",
            "Shijie Lu",
            "Yuxi Huang",
            "Ming Shen",
            "Phu Tran",
            "Ji-Eun Irene Yum",
            "Muhammad Ali Khan",
            "Muhammad Umar Afzal",
            "Irbaz Bin Riaz",
            "Ben Zhou"
        ],
        "submitted": "2025-10-20 17:22:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models",
        "abstract": "Vision-Language Models (VLMs) extend large language models with visual\nreasoning, but their multimodal design also introduces new, underexplored\nvulnerabilities. Existing multimodal red-teaming methods largely rely on\nbrittle templates, focus on single-attack settings, and expose only a narrow\nsubset of vulnerabilities. To address these limitations, we introduce VERA-V, a\nvariational inference framework that recasts multimodal jailbreak discovery as\nlearning a joint posterior distribution over paired text-image prompts. This\nprobabilistic view enables the generation of stealthy, coupled adversarial\ninputs that bypass model guardrails. We train a lightweight attacker to\napproximate the posterior, allowing efficient sampling of diverse jailbreaks\nand providing distributional insights into vulnerabilities. VERA-V further\nintegrates three complementary strategies: (i) typography-based text prompts\nthat embed harmful cues, (ii) diffusion-based image synthesis that introduces\nadversarial signals, and (iii) structured distractors to fragment VLM\nattention. Experiments on HarmBench and HADES benchmarks show that VERA-V\nconsistently outperforms state-of-the-art baselines on both open-source and\nfrontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the\nbest baseline on GPT-4o.",
        "url": "http://arxiv.org/abs/2510.17759v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17759v1",
        "arxiv_id": "2510.17759v1",
        "authors": [
            "Qilin Liao",
            "Anamika Lochab",
            "Ruqi Zhang"
        ],
        "submitted": "2025-10-20 17:12:10",
        "source": "arxiv",
        "comment": "18 pages, 7 Figures,"
    },
    {
        "title": "Believe It or Not: How Deeply do LLMs Believe Implanted Facts?",
        "abstract": "Knowledge editing techniques promise to implant new factual knowledge into\nlarge language models (LLMs). But do LLMs really believe these facts? We\ndevelop a framework to measure belief depth and use it to evaluate the success\nof knowledge editing techniques. We operationalize belief depth as the extent\nto which implanted knowledge 1) generalizes to related contexts (e.g. Fermi\nestimates several logical steps removed), 2) is robust to self-scrutiny and\ndirect challenge, and 3) is represented similarly to genuine knowledge (as\nmeasured by linear probes). Our evaluations show that simple prompting and\nmechanistic editing techniques fail to implant knowledge deeply. In contrast,\nSynthetic Document Finetuning (SDF) - where models are trained on LLM-generated\ndocuments consistent with a fact - often succeeds at implanting beliefs that\nbehave similarly to genuine knowledge. However, SDF's success is not universal,\nas implanted beliefs that contradict basic world knowledge are brittle and\nrepresentationally distinct from genuine knowledge. Overall, our work\nintroduces measurable criteria for belief depth and enables the rigorous\nevaluation necessary for deploying knowledge editing in real-world\napplications.",
        "url": "http://arxiv.org/abs/2510.17941v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17941v1",
        "arxiv_id": "2510.17941v1",
        "authors": [
            "Stewart Slocum",
            "Julian Minder",
            "Cl√©ment Dumas",
            "Henry Sleight",
            "Ryan Greenblatt",
            "Samuel Marks",
            "Rowan Wang"
        ],
        "submitted": "2025-10-20 16:58:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations",
        "abstract": "Language models often generate factually incorrect information unsupported by\ntheir training data, a phenomenon known as extrinsic hallucination. Existing\nmitigation approaches often degrade performance on open-ended generation and\ndownstream tasks, limiting their practical utility. We propose an online\nreinforcement learning method using a novel binary retrieval-augmented reward\n(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach\nassigns a reward of one only when the model's output is entirely factually\ncorrect, and zero otherwise. We evaluate our method on Qwen3 reasoning models\nacross diverse tasks. For open-ended generation, binary RAR achieves a 39.3%\nreduction in hallucination rates, substantially outperforming both supervised\ntraining and continuous-reward RL baselines. In short-form question answering,\nthe model learns calibrated abstention, strategically outputting \"I don't know\"\nwhen faced with insufficient parametric knowledge. This yields 44.4% and 21.7%\nfewer incorrect answers on PopQA and GPQA, respectively. Crucially, these\nfactuality gains come without performance degradation on instruction following,\nmath, or code, whereas continuous-reward RL, despite improving factuality,\ninduces quality regressions.",
        "url": "http://arxiv.org/abs/2510.17733v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17733v1",
        "arxiv_id": "2510.17733v1",
        "authors": [
            "Tong Chen",
            "Akari Asai",
            "Luke Zettlemoyer",
            "Hannaneh Hajishirzi",
            "Faeze Brahman"
        ],
        "submitted": "2025-10-20 16:45:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AcademicEval: Live Long-Context LLM Benchmark",
        "abstract": "Large Language Models (LLMs) have recently achieved remarkable performance in\nlong-context understanding. However, current long-context LLM benchmarks are\nlimited by rigid context length, labor-intensive annotation, and the pressing\nchallenge of label leakage issues during LLM training. Therefore, we propose\n\\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context\ngeneration tasks. \\textsc{AcademicEval} adopts papers on arXiv to introduce\nseveral academic writing tasks with long-context inputs, \\textit{i.e.},\n\\textsc{Title}, \\textsc{Abstract}, \\textsc{Introduction}, and \\textsc{Related\nWork}, which cover a wide range of abstraction levels and require no manual\nlabeling. Moreover, \\textsc{AcademicEval} integrates high-quality and\nexpert-curated few-shot demonstrations from a collected co-author graph to\nenable flexible context length. Especially, \\textsc{AcademicEval} features an\nefficient live evaluation, ensuring no label leakage. We conduct a holistic\nevaluation on \\textsc{AcademicEval}, and the results illustrate that LLMs\nperform poorly on tasks with hierarchical abstraction levels and tend to\nstruggle with long few-shot demonstrations, highlighting the challenge of our\nbenchmark. Through experimental analysis, we also reveal some insights for\nenhancing LLMs' long-context modeling capabilities. Code is available at\nhttps://github.com/ulab-uiuc/AcademicEval",
        "url": "http://arxiv.org/abs/2510.17725v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17725v1",
        "arxiv_id": "2510.17725v1",
        "authors": [
            "Haozhen Zhang",
            "Tao Feng",
            "Pengrui Han",
            "Jiaxuan You"
        ],
        "submitted": "2025-10-20 16:42:30",
        "source": "arxiv",
        "comment": "Accepted by TMLR. Code is available at\n  https://github.com/ulab-uiuc/AcademicEval"
    },
    {
        "title": "PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition",
        "abstract": "Named Entity Recognition (NER) is a critical task that requires substantial\nannotated data, making it challenging in low-resource scenarios where label\nacquisition is expensive. While zero-shot and instruction-tuned approaches have\nmade progress, they often fail to generalize to domain-specific entities and do\nnot effectively utilize limited available data. We present a lightweight\nfew-shot NER framework that addresses these challenges through two key\ninnovations: (1) a new instruction tuning template with a simplified output\nformat that combines principles from prior IT approaches to leverage the large\ncontext window of recent state-of-the-art LLMs; (2) introducing a strategic\ndata augmentation technique that preserves entity information while\nparaphrasing the surrounding context, thereby expanding our training data\nwithout compromising semantic relationships. Experiments on benchmark datasets\nshow that our method achieves performance comparable to state-of-the-art models\non few-shot and zero-shot tasks, with our few-shot approach attaining an\naverage F1 score of 80.1 on the CrossNER datasets. Models trained with our\nparaphrasing approach show consistent improvements in F1 scores of up to 17\npoints over baseline versions, offering a promising solution for groups with\nlimited NER training data and compute power.",
        "url": "http://arxiv.org/abs/2510.17720v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17720v1",
        "arxiv_id": "2510.17720v1",
        "authors": [
            "Nanda Kumar Rengarajan",
            "Jun Yan",
            "Chun Wang"
        ],
        "submitted": "2025-10-20 16:36:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "QueST: Incentivizing LLMs to Generate Difficult Problems",
        "abstract": "Large Language Models have achieved strong performance on reasoning tasks,\nsolving competition-level coding and math problems. However, their scalability\nis limited by human-labeled datasets and the lack of large-scale, challenging\ncoding problem training data. Existing competitive coding datasets contain only\nthousands to tens of thousands of problems. Previous synthetic data generation\nmethods rely on either augmenting existing instruction datasets or selecting\nchallenging problems from human-labeled data. In this paper, we propose QueST,\na novel framework which combines difficulty-aware graph sampling and\ndifficulty-aware rejection fine-tuning that directly optimizes specialized\ngenerators to create challenging coding problems. Our trained generators\ndemonstrate superior capability compared to even GPT-4o at creating challenging\nproblems that benefit downstream performance. We leverage QueST to generate\nlarge-scale synthetic coding problems, which we then use to distill from strong\nteacher models with long chain-of-thought or to conduct reinforcement learning\nfor smaller models, proving effective in both scenarios. Our distillation\nexperiments demonstrate significant performance gains. Specifically, after\nfine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we\nsurpass the performance of the original Qwen3-8B on LiveCodeBench. With an\nadditional 112K examples (i.e., 28K human-written problems paired with multiple\nsynthetic solutions), our 8B model matches the performance of the much larger\nDeepSeek-R1-671B. These findings indicate that generating complex problems via\nQueST offers an effective and scalable approach to advancing the frontiers of\ncompetitive coding and reasoning for large language models.",
        "url": "http://arxiv.org/abs/2510.17715v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17715v1",
        "arxiv_id": "2510.17715v1",
        "authors": [
            "Hanxu Hu",
            "Xingxing Zhang",
            "Jannis Vamvas",
            "Rico Sennrich",
            "Furu Wei"
        ],
        "submitted": "2025-10-20 16:29:53",
        "source": "arxiv",
        "comment": "20 pages, 7 figures"
    },
    {
        "title": "Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models",
        "abstract": "Large Language Models (LLMs) possess remarkable generalization capabilities\nbut struggle with multi-task adaptation, particularly in balancing knowledge\nretention with task-specific specialization. Conventional fine-tuning methods\nsuffer from catastrophic forgetting and substantial resource consumption, while\nexisting parameter-efficient methods perform suboptimally in complex multi-task\nscenarios. To address this, we propose Contextual Attention Modulation (CAM), a\nnovel mechanism that dynamically modulates the representations of\nself-attention modules in LLMs. CAM enhances task-specific features while\npreserving general knowledge, thereby facilitating more effective and efficient\nadaptation. For effective multi-task adaptation, CAM is integrated into our\nHybrid Contextual Attention Modulation (HyCAM) framework, which combines a\nshared, full-parameter CAM module with multiple specialized, lightweight CAM\nmodules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.\nExtensive experiments on heterogeneous tasks, including question answering,\ncode generation, and logical reasoning, demonstrate that our approach\nsignificantly outperforms existing approaches, achieving an average performance\nimprovement of 3.65%. The implemented code and data are available to ease\nreproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.",
        "url": "http://arxiv.org/abs/2510.17705v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17705v1",
        "arxiv_id": "2510.17705v1",
        "authors": [
            "Dayan Pan",
            "Zhaoyang Fu",
            "Jingyuan Wang",
            "Xiao Han",
            "Yue Zhu",
            "Xiangyu Zhao"
        ],
        "submitted": "2025-10-20 16:19:27",
        "source": "arxiv",
        "comment": "Accepted by CIKM' 25"
    },
    {
        "title": "Towards Mining Effective Pedagogical Strategies from Learner-LLM Educational Dialogues",
        "abstract": "Dialogue plays a crucial role in educational settings, yet existing\nevaluation methods for educational applications of large language models (LLMs)\nprimarily focus on technical performance or learning outcomes, often neglecting\nattention to learner-LLM interactions. To narrow this gap, this AIED Doctoral\nConsortium paper presents an ongoing study employing a dialogue analysis\napproach to identify effective pedagogical strategies from learner-LLM\ndialogues. The proposed approach involves dialogue data collection, dialogue\nact (DA) annotation, DA pattern mining, and predictive model building. Early\ninsights are outlined as an initial step toward future research. The work\nunderscores the need to evaluate LLM-based educational applications by focusing\non dialogue dynamics and pedagogical strategies.",
        "url": "http://arxiv.org/abs/2510.17698v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17698v1",
        "arxiv_id": "2510.17698v1",
        "authors": [
            "Liqun He",
            "Manolis Mavrikis",
            "Mutlu Cukurova"
        ],
        "submitted": "2025-10-20 16:11:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LILO: Bayesian Optimization with Interactive Natural Language Feedback",
        "abstract": "For many real-world applications, feedback is essential in translating\ncomplex, nuanced, or subjective goals into quantifiable optimization\nobjectives. We propose a language-in-the-loop framework that uses a large\nlanguage model (LLM) to convert unstructured feedback in the form of natural\nlanguage into scalar utilities to conduct BO over a numeric search space.\nUnlike preferential BO, which only accepts restricted feedback formats and\nrequires customized models for each domain-specific problem, our approach\nleverages LLMs to turn varied types of textual feedback into consistent utility\nsignals and to easily include flexible user priors without manual kernel\ndesign. At the same time, our method maintains the sample efficiency and\nprincipled uncertainty quantification of BO. We show that this hybrid method\nnot only provides a more natural interface to the decision maker but also\noutperforms conventional BO baselines and LLM-only optimizers, particularly in\nfeedback-limited regimes.",
        "url": "http://arxiv.org/abs/2510.17671v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17671v1",
        "arxiv_id": "2510.17671v1",
        "authors": [
            "Katarzyna Kobalczyk",
            "Zhiyuan Jerry Lin",
            "Benjamin Letham",
            "Zhuokai Zhao",
            "Maximilian Balandat",
            "Eytan Bakshy"
        ],
        "submitted": "2025-10-20 15:41:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration",
        "abstract": "Open-vocabulary object detection (OVD) models offer remarkable flexibility by\ndetecting objects from arbitrary text queries. However, their zero-shot\nperformance in specialized domains like Remote Sensing (RS) is often\ncompromised by the inherent ambiguity of natural language, limiting critical\ndownstream applications. For instance, an OVD model may struggle to distinguish\nbetween fine-grained classes such as \"fishing boat\" and \"yacht\" since their\nembeddings are similar and often inseparable. This can hamper specific user\ngoals, such as monitoring illegal fishing, by producing irrelevant detections.\nTo address this, we propose a cascaded approach that couples the broad\ngeneralization of a large pre-trained OVD model with a lightweight few-shot\nclassifier. Our method first employs the zero-shot model to generate\nhigh-recall object proposals. These proposals are then refined for high\nprecision by a compact classifier trained in real-time on only a handful of\nuser-annotated examples - drastically reducing the high costs of RS imagery\nannotation.The core of our framework is FLAME, a one-step active learning\nstrategy that selects the most informative samples for training. FLAME\nidentifies, on the fly, uncertain marginal candidates near the decision\nboundary using density estimation, followed by clustering to ensure sample\ndiversity. This efficient sampling technique achieves high accuracy without\ncostly full-model fine-tuning and enables instant adaptation, within less then\na minute, which is significantly faster than state-of-the-art alternatives.Our\nmethod consistently surpasses state-of-the-art performance on RS benchmarks,\nestablishing a practical and resource-efficient framework for adapting\nfoundation models to specific user needs.",
        "url": "http://arxiv.org/abs/2510.17670v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17670v1",
        "arxiv_id": "2510.17670v1",
        "authors": [
            "Yehonathan Refael",
            "Amit Aides",
            "Aviad Barzilai",
            "George Leifman",
            "Genady Beryozkin",
            "Vered Silverman",
            "Bolous Jaber",
            "Tomer Shekel"
        ],
        "submitted": "2025-10-20 15:41:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AtlasKV: Augmenting LLMs with Billion-Scale Knowledge Graphs in 20GB VRAM",
        "abstract": "Retrieval-augmented generation (RAG) has shown some success in augmenting\nlarge language models (LLMs) with external knowledge. However, as a\nnon-parametric knowledge integration paradigm for LLMs, RAG methods heavily\nrely on external retrieval modules and the retrieved textual context prior.\nEspecially for very large scale knowledge augmentation, they would introduce\nsubstantial inference latency due to expensive searches and much longer\nrelevant context. In this paper, we propose a parametric knowledge integration\nmethod, called \\textbf{AtlasKV}, a scalable, effective, and general way to\naugment LLMs with billion-scale knowledge graphs (KGs) (e.g. 1B triples) using\nvery little GPU memory cost (e.g. less than 20GB VRAM). In AtlasKV, we\nintroduce KG2KV and HiKVP to integrate KG triples into LLMs at scale with\nsub-linear time and memory complexity. It maintains strong knowledge grounding\nand generalization performance using the LLMs' inherent attention mechanism,\nand requires no external retrievers, long context priors, or retraining when\nadapting to new knowledge.",
        "url": "http://arxiv.org/abs/2510.17934v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17934v1",
        "arxiv_id": "2510.17934v1",
        "authors": [
            "Haoyu Huang",
            "Hong Ting Tsang",
            "Jiaxin Bai",
            "Xi Peng",
            "Gong Zhang",
            "Yangqiu Song"
        ],
        "submitted": "2025-10-20 15:40:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DELULU: Discriminative Embedding Learning Using Latent Units for Speaker-Aware Self-Supervised Speech Foundational Model",
        "abstract": "Self-supervised speech models have achieved remarkable success on\ncontent-driven tasks, yet they remain limited in capturing\nspeaker-discriminative features critical for verification, diarization, and\nprofiling applications. We introduce DELULU, a speaker-aware self-supervised\nfoundational model that addresses this limitation by integrating external\nsupervision into the pseudo-label generation process. DELULU leverages\nframe-level embeddings from ReDimNet, a state-of-the-art speaker verification\nmodel, to guide the k-means clustering step during pre-training, introducing a\nstrong speaker-discriminative inductive bias that aligns representation\nlearning with speaker identity. The model is trained using a dual objective\nthat combines masked prediction and denoising, further enhancing robustness and\ngeneralization. DELULU significantly outperforms prior self-supervised learning\n(SSL) models across a range of speaker-centric tasks, achieving up to 62%\nrelative improvement in equal error rate (EER) for speaker verification and\nconsistent gains on zero-shot profiling tasks such as gender, age, accent, and\nspeaker counting. Our findings demonstrate that DELULU is a strong universal\nencoder for speaker-aware speech processing, enabling superior performance even\nwithout task-specific fine-tuning.",
        "url": "http://arxiv.org/abs/2510.17662v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17662v1",
        "arxiv_id": "2510.17662v1",
        "authors": [
            "Massa Baali",
            "Rita Singh",
            "Bhiksha Raj"
        ],
        "submitted": "2025-10-20 15:35:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Qomhra: A Bilingual Irish-English Large Language Model",
        "abstract": "This paper introduces Qomhr\\'a, a bilingual Irish-English large language\nmodel (LLM), developed under low-resource constraints presenting a complete\npipeline spanning bilingual continued pre-training, instruction tuning, and\nalignment from human preferences. Newly accessible Irish corpora and English\ntext are mixed and curated to improve Irish performance while preserving\nEnglish ability. 6 closed-weight LLMs are judged for their Irish text\ngeneration by a native speaker, a learner and other LLMs. Google's\nGemini-2.5-Pro is ranked the highest and is subsequently used to synthesise\ninstruction tuning and human preference datasets. Two datasets are contributed\nleveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning\ndataset and a 1K human preference dataset, generating accepted and rejected\nresponses that show near perfect alignment with a native Irish speaker.\nQomhr\\'a is comprehensively evaluated across benchmarks testing translation,\ngender understanding, topic identification and world knowledge with gains of up\nto 29% in Irish and 44% in English. Qomhr\\'a also undergoes instruction tuning\nand demonstrates clear progress in instruction following, crucial for chatbot\nfunctionality.",
        "url": "http://arxiv.org/abs/2510.17652v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17652v1",
        "arxiv_id": "2510.17652v1",
        "authors": [
            "Joseph McInerney"
        ],
        "submitted": "2025-10-20 15:27:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena",
        "abstract": "Forecasting is not only a fundamental intellectual pursuit but also is of\nsignificant importance to societal systems such as finance and economics. With\nthe rapid advances of large language models (LLMs) trained on Internet-scale\ndata, it raises the promise of employing LLMs to forecast real-world future\nevents, an emerging paradigm we call \"LLM-as-a-Prophet\". This paper\nsystematically investigates such predictive intelligence of LLMs. To this end,\nwe build Prophet Arena, a general evaluation benchmark that continuously\ncollects live forecasting tasks and decomposes each task into distinct pipeline\nstages, in order to support our controlled and large-scale experimentation. Our\ncomprehensive evaluation reveals that many LLMs already exhibit impressive\nforecasting capabilities, reflected in, e.g., their small calibration errors,\nconsistent prediction confidence and promising market returns. However, we also\nuncover key bottlenecks towards achieving superior predictive intelligence via\nLLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of\ndata sources and slower information aggregation compared to markets when\nresolution nears.",
        "url": "http://arxiv.org/abs/2510.17638v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17638v1",
        "arxiv_id": "2510.17638v1",
        "authors": [
            "Qingchuan Yang",
            "Simon Mahns",
            "Sida Li",
            "Anri Gu",
            "Jibang Wu",
            "Haifeng Xu"
        ],
        "submitted": "2025-10-20 15:20:05",
        "source": "arxiv",
        "comment": "https://www.prophetarena.co/"
    },
    {
        "title": "Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models",
        "abstract": "Large language models may encode sensitive information or outdated knowledge\nthat needs to be removed, to ensure responsible and compliant model responses.\nUnlearning has emerged as an efficient alternative to full retraining, aiming\nto remove specific knowledge while preserving overall model utility. Existing\nevaluations of unlearning methods focus on (1) the extent of forgetting of the\ntarget knowledge (forget set) and (2) maintaining performance on the retain set\n(i.e., utility). However, these evaluations overlook an important usability\naspect: users may still want the model to leverage the removed information if\nit is re-introduced in the prompt. In a systematic evaluation of six\nstate-of-the-art unlearning methods, we find that they consistently impair such\ncontextual utility. To address this, we augment unlearning objectives with a\nplug-in term that preserves the model's ability to use forgotten knowledge when\nit is present in context. Extensive experiments demonstrate that our approach\nrestores contextual utility to near original levels while still maintaining\neffective forgetting and retain-set utility.",
        "url": "http://arxiv.org/abs/2510.17620v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17620v1",
        "arxiv_id": "2510.17620v1",
        "authors": [
            "Yuefeng Peng",
            "Parnian Afshar",
            "Megan Ganji",
            "Thomas Butler",
            "Amir Houmansadr",
            "Mingxian Wang",
            "Dezhi Hong"
        ],
        "submitted": "2025-10-20 15:03:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration",
        "abstract": "Clinicians need ranking systems that work in real time and still justify\ntheir choices. Motivated by the need for a low-latency, decoder-based reranker,\nwe present OG-Rank, a single-decoder approach that pairs a pooled first-token\nscoring signal with an uncertainty-gated explanation step. The model scores all\ncandidates in one pass and generates a brief, structured rationale only when\nthe list is genuinely ambiguous, keeping latency predictable. Trained with a\ncurriculum that concentrates effort on hard cases, OG-Rank delivers strong\neffectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,\nnDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,\nnDCG@20~0.699 at a 45\\% gate rate), while compact backbones show similar gains\nunder the same policy. Encoder baselines trail in both effectiveness and\nflexibility. The result is a practical recipe: rank fast by default and explain\nwhen it helps, a pattern that applies broadly to decision tasks where selective\ngeneration buys accuracy at acceptable cost. The single-policy design\nsimplifies deployment and budget planning, and the curriculum principle (spend\nmore on the hard cases, less on the easy ones) readily transfers beyond\nclinical order selection.",
        "url": "http://arxiv.org/abs/2510.17614v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17614v1",
        "arxiv_id": "2510.17614v1",
        "authors": [
            "Praphul Singh",
            "Corey Barrett",
            "Sumana Srivasta",
            "Irfan Bulu",
            "Sri Gadde",
            "Krishnaram Kenthapadi"
        ],
        "submitted": "2025-10-20 15:00:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Diagnosing Representation Dynamics in NER Model Extension",
        "abstract": "Extending Named Entity Recognition (NER) models to new PII entities in noisy\nspoken-language data is a common need. We find that jointly fine-tuning a BERT\nmodel on standard semantic entities (PER, LOC, ORG) and new pattern-based PII\n(EMAIL, PHONE) results in minimal degradation for original classes. We\ninvestigate this \"peaceful coexistence,\" hypothesizing that the model uses\nindependent semantic vs. morphological feature mechanisms.\n  Using an incremental learning setup as a diagnostic tool, we measure semantic\ndrift and find two key insights. First, the LOC (location) entity is uniquely\nvulnerable due to a representation overlap with new PII, as it shares\npattern-like features (e.g., postal codes). Second, we identify a \"reverse\nO-tag representation drift.\" The model, initially trained to map PII patterns\nto 'O', blocks new learning. This is resolved only by unfreezing the 'O' tag's\nclassifier, allowing the background class to adapt and \"release\" these\npatterns. This work provides a mechanistic diagnosis of NER model adaptation,\nhighlighting feature independence, representation overlap, and 'O' tag\nplasticity.",
        "url": "http://arxiv.org/abs/2510.17930v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17930v1",
        "arxiv_id": "2510.17930v1",
        "authors": [
            "Xirui Zhang",
            "Philippe de La Chevasnerie",
            "Benoit Fabre"
        ],
        "submitted": "2025-10-20 14:53:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis",
        "abstract": "Legal reasoning is a fundamental component of legal analysis and\ndecision-making. Existing computational approaches to legal reasoning\npredominantly rely on generic reasoning frameworks such as syllogism and IRAC,\nwhich do not comprehensively examine the nuanced processes that underpin legal\nreasoning. Moreover, current research has largely focused on criminal cases,\nwith insufficient modeling for civil cases. In this work, we present a novel\nframework for explicitly modeling legal reasoning in the analysis of Chinese\ntort-related civil cases. We first operationalize the legal reasoning processes\nused in tort analysis into the LawChain framework. LawChain is a three-module\nreasoning framework, with each module consisting of multiple finer-grained\nsub-steps. Informed by the LawChain framework, we introduce the task of tort\nlegal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to\nsystematically assess the critical steps within analytical reasoning chains for\ntort analysis. Leveraging this benchmark, we evaluate state-of-the-art large\nlanguage models for their legal reasoning ability in civil tort contexts. Our\nresults indicate that current models still fall short in accurately handling\ncrucial elements of tort legal reasoning. Furthermore, we introduce several\nbaseline approaches that explicitly incorporate LawChain-style reasoning\nthrough prompting or post-training. We conduct further experiments on\nadditional legal analysis tasks, such as Legal Named-Entity Recognition and\nCriminal Damages Calculation, to verify the generalizability of these\nbaselines. The proposed baseline approaches achieve significant improvements in\ntort-related legal reasoning and generalize well to related legal analysis\ntasks, thus demonstrating the value of explicitly modeling legal reasoning\nchains to enhance the reasoning capabilities of language models.",
        "url": "http://arxiv.org/abs/2510.17602v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17602v1",
        "arxiv_id": "2510.17602v1",
        "authors": [
            "Huiyuan Xie",
            "Chenyang Li",
            "Huining Zhu",
            "Chubin Zhang",
            "Yuxiao Ye",
            "Zhenghao Liu",
            "Zhiyuan Liu"
        ],
        "submitted": "2025-10-20 14:50:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Reasoning Distillation and Structural Alignment for Improved Code Generation",
        "abstract": "Effective code generation with language models hinges on two critical\nfactors: accurately understanding the intent of the prompt and generating code\nthat applies algorithmic reasoning to produce correct solutions capable of\npassing diverse test cases while adhering to the syntax of the target\nprogramming language. Unlike other language tasks, code generation requires\nmore than accurate token prediction; it demands comprehension of solution-level\nand structural relationships rather than merely generating the most likely\ntokens. very large language model (VLLM) are capable of generating detailed\nsteps toward the correct solution of complex tasks where reasoning is crucial\nin solving the problem. Such reasoning capabilities may be absent in smaller\nlanguage models. Therefore, in this work, we distill the reasoning capabilities\nof a VLLM into a smaller, more efficient model that is faster and cheaper to\ndeploy. Our approach trains the model to emulate the reasoning and\nproblem-solving abilities of the VLLM by learning to identify correct solution\npathways and establishing a structural correspondence between problem\ndefinitions and potential solutions through a novel method of structure-aware\nloss optimization. This enables the model to transcend token-level generation\nand to deeply grasp the overarching structure of solutions for given problems.\nExperimental results show that our fine-tuned model, developed through a cheap\nand simple to implement process, significantly outperforms our baseline model\nin terms of pass@1, average data flow, and average syntax match metrics across\nthe MBPP, MBPP Plus, and HumanEval benchmarks.",
        "url": "http://arxiv.org/abs/2510.17598v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17598v1",
        "arxiv_id": "2510.17598v1",
        "authors": [
            "Amir Jalilifard",
            "Anderson de Rezende Rocha",
            "Marcos Medeiros Raimundo"
        ],
        "submitted": "2025-10-20 14:47:47",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "HGAdapter: Hypergraph-based Adapters in Language Models for Code Summarization and Clone Detection",
        "abstract": "Pre-trained language models (PLMs) are increasingly being applied to\ncode-related tasks. Although PLMs have achieved good results, they do not take\ninto account potential high-order data correlations within the code. We propose\nthree types of high-order correlations in code tokens, i.e. abstract syntax\ntree family correlation, lexical correlation, and line correlation. We design a\ntokens and hyperedges generator to capture these high-order data correlations.\nWe improve the architecture of hypergraph neural networks and combine it with\nadapter tuning to propose a novel hypergraph-based adapter (HGAdapter) to\nfine-tune PLMs. HGAdapter can encode high-order data correlations and is\nallowed to be inserted into various PLMs to enhance performance. Experiments\nwere conducted on several public datasets, including six languages of code\nsummarization and code clone detection tasks. Our methods improved the\nperformance of PLMs in datasets to varying degrees. Experimental results\nvalidate the introduction of high-order data correlations that contribute to\nimproved effectiveness.",
        "url": "http://arxiv.org/abs/2510.17591v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17591v1",
        "arxiv_id": "2510.17591v1",
        "authors": [
            "Guang Yang",
            "Yujie Zhu"
        ],
        "submitted": "2025-10-20 14:41:28",
        "source": "arxiv",
        "comment": "Accepted by the 2025 Conference on Empirical Methods in Natural\n  Language Processing (EMNLP 2025) as a findings long paper"
    },
    {
        "title": "MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning",
        "abstract": "Misinformation spreads across web platforms through billions of daily\nmultimodal posts that combine text and images, overwhelming manual\nfact-checking capacity. Supervised detection models require domain-specific\ntraining data and fail to generalize across diverse manipulation tactics. We\npresent MIRAGE, an inference-time, model-pluggable agentic framework that\ndecomposes multimodal verification into four sequential modules: visual\nveracity assessment detects AI-generated images, cross-modal consistency\nanalysis identifies out-of-context repurposing, retrieval-augmented factual\nchecking grounds claims in web evidence through iterative question generation,\nand a calibrated judgment module integrates all signals. MIRAGE orchestrates\nvision-language model reasoning with targeted web retrieval, outputs structured\nand citation-linked rationales. On MMFakeBench validation set (1,000 samples),\nMIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming\nthe strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65\npoints while maintaining 34.3% false positive rate versus 97.3% for a\njudge-only baseline. Test set results (5,000 samples) confirm generalization\nwith 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification\ncontributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97\npoints. Our results demonstrate that decomposed agentic reasoning with web\nretrieval can match supervised detector performance without domain-specific\ntraining, enabling misinformation detection across modalities where labeled\ndata remains scarce.",
        "url": "http://arxiv.org/abs/2510.17590v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17590v1",
        "arxiv_id": "2510.17590v1",
        "authors": [
            "Mir Nafis Sharear Shopnil",
            "Sharad Duwal",
            "Abhishek Tyagi",
            "Adiba Mahbub Proma"
        ],
        "submitted": "2025-10-20 14:40:26",
        "source": "arxiv",
        "comment": "16 pages, 3 tables, 1 figure"
    },
    {
        "title": "Language Confusion Gate: Language-Aware Decoding Through Model Self-Distillation",
        "abstract": "Large language models (LLMs) often experience language confusion, which is\nthe unintended mixing of languages during text generation. Current solutions to\nthis problem either necessitate model retraining or cannot differentiate\nbetween harmful confusion and acceptable code-switching. This paper introduces\nthe Language Confusion Gate (LCG), a lightweight, plug-in solution that filters\ntokens during decoding without altering the base LLM. The LCG is trained using\nnorm-adjusted self-distillation to predict appropriate language families and\napply masking only when needed. Our method is based on the findings that\nlanguage confusion is infrequent, correct-language tokens are usually among the\ntop predictions, and output token embedding norms are larger for high-resource\nlanguages, which biases sampling. When evaluated across various models,\nincluding Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion\nsignificantly, often by an order of magnitude, without negatively impacting\ntask performance. Code is available at\nhttps://github.com/collinzrj/language_confusion_gate.",
        "url": "http://arxiv.org/abs/2510.17555v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17555v1",
        "arxiv_id": "2510.17555v1",
        "authors": [
            "Collin Zhang",
            "Fei Huang",
            "Chenhan Yuan",
            "Junyang Lin"
        ],
        "submitted": "2025-10-20 14:02:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "When Annotators Disagree, Topology Explains: Mapper, a Topological Tool for Exploring Text Embedding Geometry and Ambiguity",
        "abstract": "Language models are often evaluated with scalar metrics like accuracy, but\nsuch measures fail to capture how models internally represent ambiguity,\nespecially when human annotators disagree. We propose a topological perspective\nto analyze how fine-tuned models encode ambiguity and more generally instances.\n  Applied to RoBERTa-Large on the MD-Offense dataset, Mapper, a tool from\ntopological data analysis, reveals that fine-tuning restructures embedding\nspace into modular, non-convex regions aligned with model predictions, even for\nhighly ambiguous cases. Over $98\\%$ of connected components exhibit $\\geq 90\\%$\nprediction purity, yet alignment with ground-truth labels drops in ambiguous\ndata, surfacing a hidden tension between structural confidence and label\nuncertainty.\n  Unlike traditional tools such as PCA or UMAP, Mapper captures this geometry\ndirectly uncovering decision regions, boundary collapses, and overconfident\nclusters. Our findings position Mapper as a powerful diagnostic tool for\nunderstanding how models resolve ambiguity. Beyond visualization, it also\nenables topological metrics that may inform proactive modeling strategies in\nsubjective NLP tasks.",
        "url": "http://arxiv.org/abs/2510.17548v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17548v1",
        "arxiv_id": "2510.17548v1",
        "authors": [
            "Nisrine Rair",
            "Alban Goupil",
            "Valeriu Vrabie",
            "Emmanuel Chochoy"
        ],
        "submitted": "2025-10-20 13:58:02",
        "source": "arxiv",
        "comment": "Accepted to appear in the Proceedings of the 2025 Conference on\n  Empirical Methods in Natural Language Processing (EMNLP 2025, Main\n  Conference)"
    },
    {
        "title": "How role-play shapes relevance judgment in zero-shot LLM rankers",
        "abstract": "Large Language Models (LLMs) have emerged as promising zero-shot rankers, but\ntheir performance is highly sensitive to prompt formulation. In particular,\nrole-play prompts, where the model is assigned a functional role or identity,\noften give more robust and accurate relevance rankings. However, the mechanisms\nand diversity of role-play effects remain underexplored, limiting both\neffective use and interpretability. In this work, we systematically examine how\nrole-play variations influence zero-shot LLM rankers. We employ causal\nintervention techniques from mechanistic interpretability to trace how\nrole-play information shapes relevance judgments in LLMs. Our analysis reveals\nthat (1) careful formulation of role descriptions have a large effect on the\nranking quality of the LLM; (2) role-play signals are predominantly encoded in\nearly layers and communicate with task instructions in middle layers, while\nreceiving limited interaction with query or document representations.\nSpecifically, we identify a group of attention heads that encode information\ncritical for role-conditioned relevance. These findings not only shed light on\nthe inner workings of role-play in LLM ranking but also offer guidance for\ndesigning more effective prompts in IR and beyond, pointing toward broader\nopportunities for leveraging role-play in zero-shot applications.",
        "url": "http://arxiv.org/abs/2510.17535v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17535v1",
        "arxiv_id": "2510.17535v1",
        "authors": [
            "Yumeng Wang",
            "Jirui Qi",
            "Catherine Chen",
            "Panagiotis Eustratiadis",
            "Suzan Verberne"
        ],
        "submitted": "2025-10-20 13:39:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "OncoReason: Structuring Clinical Reasoning in LLMs for Robust and Interpretable Survival Prediction",
        "abstract": "Predicting cancer treatment outcomes requires models that are both accurate\nand interpretable, particularly in the presence of heterogeneous clinical data.\nWhile large language models (LLMs) have shown strong performance in biomedical\nNLP, they often lack structured reasoning capabilities critical for high-stakes\ndecision support. We present a unified, multi-task learning framework that\naligns autoregressive LLMs with clinical reasoning for outcome prediction on\nthe MSK-CHORD dataset. Our models are trained to jointly perform binary\nsurvival classification, continuous survival time regression, and natural\nlanguage rationale generation. We evaluate three alignment strategies: (1)\nstandard supervised fine-tuning (SFT), (2) SFT with Chain-of-Thought (CoT)\nprompting to elicit step-by-step reasoning, and (3) Group Relative Policy\nOptimization (GRPO), a reinforcement learning method that aligns model outputs\nto expert-derived reasoning trajectories. Experiments with LLaMa3-8B and\nMed42-8B backbones demonstrate that CoT prompting improves F1 by +6.0 and\nreduces MAE by 12%, while GRPO achieves state-of-the-art interpretability and\npredictive performance across BLEU, ROUGE, and BERTScore. We further show that\nexisting biomedical LLMs often fail to produce valid reasoning traces due to\narchitectural constraints. Our findings underscore the importance of\nreasoning-aware alignment in multi-task clinical modeling and set a new\nbenchmark for interpretable, trustworthy LLMs in precision oncology.",
        "url": "http://arxiv.org/abs/2510.17532v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17532v1",
        "arxiv_id": "2510.17532v1",
        "authors": [
            "Raghu Vamshi Hemadri",
            "Geetha Krishna Guruju",
            "Kristi Topollai",
            "Anna Ewa Choromanska"
        ],
        "submitted": "2025-10-20 13:35:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors",
        "abstract": "Large language model (LLM) simulations of human behavior have the potential\nto revolutionize the social and behavioral sciences, if and only if they\nfaithfully reflect real human behaviors. Current evaluations are fragmented,\nbased on bespoke tasks and metrics, creating a patchwork of incomparable\nresults. To address this, we introduce SimBench, the first large-scale,\nstandardized benchmark for a robust, reproducible science of LLM simulation. By\nunifying 20 diverse datasets covering tasks from moral decision-making to\neconomic choice across a large global participant pool, SimBench provides the\nnecessary foundation to ask fundamental questions about when, how, and why LLM\nsimulations succeed or fail. We show that, while even the best LLMs today have\nlimited simulation ability (score: 40.80/100), performance scales log-linearly\nwith model size. Simulation performance is not improved by increased\ninference-time compute. We demonstrate an alignment-simulation trade-off:\ninstruction-tuning improves performance on low-entropy (consensus) questions\nbut degrades it on high-entropy (diverse) ones. Models particularly struggle\nwhen simulating specific demographic groups. Finally, we demonstrate that\nsimulation ability correlates most strongly with deep, knowledge-intensive\nreasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to\naccelerate the development of more faithful LLM simulators.",
        "url": "http://arxiv.org/abs/2510.17516v2",
        "pdf_url": "http://arxiv.org/pdf/2510.17516v2",
        "arxiv_id": "2510.17516v2",
        "authors": [
            "Tiancheng Hu",
            "Joachim Baumann",
            "Lorenzo Lupo",
            "Nigel Collier",
            "Dirk Hovy",
            "Paul R√∂ttger"
        ],
        "submitted": "2025-10-20 13:14:38",
        "source": "arxiv",
        "comment": "Project Website: http://simbench.tiancheng.hu/ Data:\n  https://huggingface.co/datasets/pitehu/SimBench"
    },
    {
        "title": "Annotation-Efficient Universal Honesty Alignment",
        "abstract": "Honesty alignment-the ability of large language models (LLMs) to recognize\ntheir knowledge boundaries and express calibrated confidence-is essential for\ntrustworthy deployment. Existing methods either rely on training-free\nconfidence estimation (e.g., token probabilities, self-consistency) or\ntraining-based calibration with correctness annotations. While effective,\nachieving universal honesty alignment with training-based calibration requires\ncostly, large-scale labeling. To support annotation-efficient training, we\nintroduce Elicitation-Then-Calibration (EliCal), a two-stage framework that\nfirst elicits internal confidence using inexpensive self-consistency\nsupervision, then calibrates this confidence with a small set of correctness\nannotations. To support a large-scale study, we release HonestyBench, a\nbenchmark covering ten free-form QA datasets with 560k training and 70k\nevaluation instances annotated with correctness and self-consistency signals.\nExperiments show that EliCal achieves near-optimal alignment with only 1k\ncorrectness annotations (0.18% of full supervision) and better alignment\nperformance on unseen MMLU tasks than the calibration-only baseline, offering a\nscalable solution toward universal honesty alignment in LLMs.",
        "url": "http://arxiv.org/abs/2510.17509v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17509v1",
        "arxiv_id": "2510.17509v1",
        "authors": [
            "Shiyu Ni",
            "Keping Bi",
            "Jiafeng Guo",
            "Minghao Tang",
            "Jingtong Wu",
            "Zengxin Han",
            "Xueqi Cheng"
        ],
        "submitted": "2025-10-20 13:05:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Lingua Custodi's participation at the WMT 2025 Terminology shared task",
        "abstract": "While BERT is an effective method for learning monolingual sentence\nembeddings for semantic similarity and embedding based transfer learning BERT\nbased cross-lingual sentence embeddings have yet to be explored. We\nsystematically investigate methods for learning multilingual sentence\nembeddings by combining the best methods for learning monolingual and\ncross-lingual representations including: masked language modeling (MLM),\ntranslation language modeling (TLM), dual encoder translation ranking, and\nadditive margin softmax. We show that introducing a pre-trained multilingual\nlanguage model dramatically reduces the amount of parallel training data\nrequired to achieve good performance by 80%. Composing the best of these\nmethods produces a model that achieves 83.7% bi-text retrieval accuracy over\n112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still\nperforming competitively on monolingual transfer learning benchmarks. Parallel\ndata mined from CommonCrawl using our best model is shown to train competitive\nNMT models for en-zh and en-de. We publicly release our best multilingual\nsentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.",
        "url": "http://arxiv.org/abs/2510.17504v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17504v1",
        "arxiv_id": "2510.17504v1",
        "authors": [
            "Jingshu Liu",
            "Raheel Qader",
            "Ga√´tan Caillaut",
            "Mariam Nakhl√©"
        ],
        "submitted": "2025-10-20 13:00:47",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Deep Self-Evolving Reasoning",
        "abstract": "Long-form chain-of-thought reasoning has become a cornerstone of advanced\nreasoning in large language models. While recent verification-refinement\nframeworks have enabled proprietary models to solve Olympiad-level problems,\ntheir effectiveness hinges on strong, reliable verification and correction\ncapabilities, which remain fragile in open-weight, smaller-scale models. This\nwork demonstrates that even with weak verification and refinement capabilities\non hard tasks, the reasoning limits of such models can be substantially\nextended through a probabilistic paradigm we call Deep Self-Evolving Reasoning\n(DSER). We conceptualize iterative reasoning as a Markov chain, where each step\nrepresents a stochastic transition in the solution space. The key insight is\nthat convergence to a correct solution is guaranteed as long as the probability\nof improvement marginally exceeds that of degradation. By running multiple\nlong-horizon, self-evolving processes in parallel, DSER amplifies these small\npositive tendencies, enabling the model to asymptotically approach correct\nanswers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On\nthe challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously\nunsolvable problems and boosts overall performance, enabling this compact model\nto surpass the single-turn accuracy of its 600B-parameter teacher through\nmajority voting. Beyond its immediate utility for test-time scaling, the DSER\nframework serves to diagnose the fundamental limitations of current open-weight\nreasoners. By clearly delineating their shortcomings in self-verification,\nrefinement, and stability, our findings establish a clear research agenda for\ndeveloping next-generation models with powerful, intrinsic self-evolving\ncapabilities.",
        "url": "http://arxiv.org/abs/2510.17498v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17498v1",
        "arxiv_id": "2510.17498v1",
        "authors": [
            "Zihan Liu",
            "Shun Zheng",
            "Xumeng Wen",
            "Yang Wang",
            "Jiang Bian",
            "Mao Yang"
        ],
        "submitted": "2025-10-20 12:51:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents",
        "abstract": "With the rise of large language models (LLMs), LLM agents capable of\nautonomous reasoning, planning, and executing complex tasks have become a\nfrontier in artificial intelligence. However, how to translate the research on\ngeneral agents into productivity that drives industry transformations remains a\nsignificant challenge. To address this, this paper systematically reviews the\ntechnologies, applications, and evaluation methods of industry agents based on\nLLMs. Using an industry agent capability maturity framework, it outlines the\nevolution of agents in industry applications, from \"process execution systems\"\nto \"adaptive social systems.\" First, we examine the three key technological\npillars that support the advancement of agent capabilities: Memory, Planning,\nand Tool Use. We discuss how these technologies evolve from supporting simple\ntasks in their early forms to enabling complex autonomous systems and\ncollective intelligence in more advanced forms. Then, we provide an overview of\nthe application of industry agents in real-world domains such as digital\nengineering, scientific discovery, embodied intelligence, collaborative\nbusiness execution, and complex system simulation. Additionally, this paper\nreviews the evaluation benchmarks and methods for both fundamental and\nspecialized capabilities, identifying the challenges existing evaluation\nsystems face regarding authenticity, safety, and industry specificity. Finally,\nwe focus on the practical challenges faced by industry agents, exploring their\ncapability boundaries, developmental potential, and governance issues in\nvarious scenarios, while providing insights into future directions. By\ncombining technological evolution with industry practices, this review aims to\nclarify the current state and offer a clear roadmap and theoretical foundation\nfor understanding and building the next generation of industry agents.",
        "url": "http://arxiv.org/abs/2510.17491v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17491v1",
        "arxiv_id": "2510.17491v1",
        "authors": [
            "Yihong Tang",
            "Kehai Chen",
            "Liang Yue",
            "Jinxin Fan",
            "Caishen Zhou",
            "Xiaoguang Li",
            "Yuyang Zhang",
            "Mingming Zhao",
            "Shixiong Kai",
            "Kaiyang Guo",
            "Xingshan Zeng",
            "Wenjing Cun",
            "Lifeng Shang",
            "Min Zhang"
        ],
        "submitted": "2025-10-20 12:46:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning",
        "abstract": "Detecting AI-involved text is essential for combating misinformation,\nplagiarism, and academic misconduct. However, AI text generation includes\ndiverse collaborative processes (AI-written text edited by humans,\nhuman-written text edited by AI, and AI-generated text refined by other AI),\nwhere various or even new LLMs could be involved. Texts generated through these\nvaried processes exhibit complex characteristics, presenting significant\nchallenges for detection. Current methods model these processes rather crudely,\nprimarily employing binary classification (purely human vs. AI-involved) or\nmulti-classification (treating human-AI collaboration as a new class). We\nobserve that representations of texts generated through different processes\nexhibit inherent clustering relationships. Therefore, we propose DETree, a\nnovel approach that models the relationships among different processes as a\nHierarchical Affinity Tree structure, and introduces a specialized loss\nfunction that aligns text representations with this tree. To facilitate this\nlearning, we developed RealBench, a comprehensive benchmark dataset that\nautomatically incorporates a wide spectrum of hybrid texts produced through\nvarious human-AI collaboration processes. Our method improves performance in\nhybrid text detection tasks and significantly enhances robustness and\ngeneralization in out-of-distribution scenarios, particularly in few-shot\nlearning conditions, further demonstrating the promise of training-based\napproaches in OOD settings. Our code and dataset are available at\nhttps://github.com/heyongxin233/DETree.",
        "url": "http://arxiv.org/abs/2510.17489v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17489v1",
        "arxiv_id": "2510.17489v1",
        "authors": [
            "Yongxin He",
            "Shan Zhang",
            "Yixuan Cao",
            "Lei Ma",
            "Ping Luo"
        ],
        "submitted": "2025-10-20 12:41:44",
        "source": "arxiv",
        "comment": "To appear in NeurIPS 2025"
    },
    {
        "title": "ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts",
        "abstract": "Mixture-of-Experts (MoE) architectures have emerged as a promising approach\nto scale Large Language Models (LLMs). MoE boosts the efficiency by activating\na subset of experts per token. Recent works show that fine-grained experts\nsubstantially enriches the combinatorial flexibility of active experts and\nenhances model expressiveness. However, such a design is fundamentally limited\nby the layer-local routing mechanism: each layer is restricted to its own\nexpert pool. This requires a careful trade-off between expert dimensionality\nand routing diversity given fixed parameter budgets. We describe ReXMoE, a\nnovel MoE architecture that improves routing beyond the existing layer-local\napproaches by allowing routers to reuse experts across adjacent layers. ReXMoE\ndecouples expert dimensionality from per-layer budgets, enabling richer expert\ncombinations without sacrificing individual expert capacity or inflating\noverall parameters. To this end, we propose a new progressive scaling routing\n(PSR) strategy to gradually increase the candidate expert pool during training.\nAs a result, ReXMoE improves both language modeling and downstream task\nperformance. Extensive experiments on models ranging from 0.5B to 7B parameters\nacross different architectures demonstrate that ReXMoE consistently improves\nperformance under fixed architectural dimensions, confirming ReXMoE as new\ndesign paradigm for parameter-efficient and scalable MoE-based LLMs.",
        "url": "http://arxiv.org/abs/2510.17483v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17483v1",
        "arxiv_id": "2510.17483v1",
        "authors": [
            "Zheyue Tan",
            "Zhiyuan Li",
            "Tao Yuan",
            "Dong Zhou",
            "Weilin Liu",
            "Yueqing Zhuang",
            "Yadong Li",
            "Guowei Niu",
            "Cheng Qin",
            "Zhuyu Yao",
            "Congyi Liu",
            "Haiyang Xu",
            "Boxun Li",
            "Guohao Dai",
            "Bo Zhao",
            "Yu Wang"
        ],
        "submitted": "2025-10-20 12:27:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Disparities in Multilingual LLM-Based Healthcare Q&A",
        "abstract": "Equitable access to reliable health information is vital when integrating AI\ninto healthcare. Yet, information quality varies across languages, raising\nconcerns about the reliability and consistency of multilingual Large Language\nModels (LLMs). We systematically examine cross-lingual disparities in\npre-training source and factuality alignment in LLM answers for multilingual\nhealthcare Q&A across English, German, Turkish, Chinese (Mandarin), and\nItalian. We (i) constructed Multilingual Wiki Health Care\n(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed\ncross-lingual healthcare coverage; (iii) assessed LLM response alignment with\nthese references; and (iv) conducted a case study on factual alignment through\nthe use of contextual information and Retrieval-Augmented Generation (RAG). Our\nfindings reveal substantial cross-lingual disparities in both Wikipedia\ncoverage and LLM factual alignment. Across LLMs, responses align more with\nEnglish Wikipedia, even when the prompts are non-English. Providing contextual\nexcerpts from non-English Wikipedia at inference time effectively shifts\nfactual alignment toward culturally relevant knowledge. These results highlight\npractical pathways for building more equitable, multilingual AI systems for\nhealthcare.",
        "url": "http://arxiv.org/abs/2510.17476v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17476v1",
        "arxiv_id": "2510.17476v1",
        "authors": [
            "Ipek Baris Schlicht",
            "Burcu Sayin",
            "Zhixue Zhao",
            "Frederik M. Labont√©",
            "Cesare Barbera",
            "Marco Viviani",
            "Paolo Rosso",
            "Lucie Flek"
        ],
        "submitted": "2025-10-20 12:19:08",
        "source": "arxiv",
        "comment": "Under review"
    },
    {
        "title": "Evaluating Large Language Models on Urdu Idiom Translation",
        "abstract": "Idiomatic translation remains a significant challenge in machine translation,\nespecially for low resource languages such as Urdu, and has received limited\nprior attention. To advance research in this area, we introduce the first\nevaluation datasets for Urdu to English idiomatic translation, covering both\nNative Urdu and Roman Urdu scripts and annotated with gold-standard English\nequivalents. We evaluate multiple open-source Large Language Models (LLMs) and\nNeural Machine Translation (NMT) systems on this task, focusing on their\nability to preserve idiomatic and cultural meaning. Automatic metrics including\nBLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our\nfindings indicate that prompt engineering enhances idiomatic translation\ncompared to direct translation, though performance differences among prompt\ntypes are relatively minor. Moreover, cross script comparisons reveal that text\nrepresentation substantially affects translation quality, with Native Urdu\ninputs producing more accurate idiomatic translations than Roman Urdu.",
        "url": "http://arxiv.org/abs/2510.17460v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17460v1",
        "arxiv_id": "2510.17460v1",
        "authors": [
            "Muhammad Farmal Khan",
            "Mousumi Akter"
        ],
        "submitted": "2025-10-20 11:49:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings",
        "abstract": "The rapidly increasing volume of electronic health record (EHR) data\nunderscores a pressing need to unlock biomedical knowledge from unstructured\nclinical texts to support advancements in data-driven clinical systems,\nincluding patient diagnosis, disease progression monitoring, treatment effects\nassessment, prediction of future clinical events, etc. While contextualized\nlanguage models have demonstrated impressive performance improvements for named\nentity recognition (NER) systems in English corpora, there remains a scarcity\nof research focused on clinical texts in low-resource languages. To bridge this\ngap, our study aims to develop multiple deep contextual embedding models to\nenhance clinical NER in the cardiology domain, as part of the BioASQ\nMultiCardioNER shared task. We explore the effectiveness of different\nmonolingual and multilingual BERT-based models, trained on general domain text,\nfor extracting disease and medication mentions from clinical case reports\nwritten in English, Spanish, and Italian. We achieved an F1-score of 77.88% on\nSpanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition\n(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian\nMedications Recognition (IMR). These results outperform the mean and median F1\nscores in the test leaderboard across all subtasks, with the mean/median values\nbeing: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and\n82.8%/87.76% for IMR.",
        "url": "http://arxiv.org/abs/2510.17437v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17437v1",
        "arxiv_id": "2510.17437v1",
        "authors": [
            "Manuela Daniela Danu",
            "George Marica",
            "Constantin Suciu",
            "Lucian Mihai Itu",
            "Oladimeji Farri"
        ],
        "submitted": "2025-10-20 11:26:22",
        "source": "arxiv",
        "comment": "11 pages, 5 figures, 1 table, published in Working Notes of the\n  Conference and Labs of the Evaluation Forum (CLEF 2024)"
    },
    {
        "title": "Agentic Reinforcement Learning for Search is Unsafe",
        "abstract": "Agentic reinforcement learning (RL) trains large language models to\nautonomously call tools during reasoning, with search as the most common\napplication. These models excel at multi-step reasoning tasks, but their safety\nproperties are not well understood. In this study, we show that RL-trained\nsearch models inherit refusal from instruction tuning and often deflect harmful\nrequests by turning them into safe queries. However, this safety is fragile.\nTwo simple attacks, one that forces the model to begin response with search\n(Search attack), another that encourages models to repeatedly search\n(Multi-search attack), trigger cascades of harmful searches and answers. Across\ntwo model families (Qwen, Llama) with both local and web search, these attacks\nlower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query\nsafety by 82.4%. The attacks succeed by triggering models to generate harmful,\nrequest-mirroring search queries before they can generate the inherited refusal\ntokens. This exposes a core weakness of current RL training: it rewards\ncontinued generation of effective queries without accounting for their\nharmfulness. As a result, RL search models have vulnerabilities that users can\neasily exploit, making it urgent to develop safety-aware agentic RL pipelines\noptimising for safe search.",
        "url": "http://arxiv.org/abs/2510.17431v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17431v1",
        "arxiv_id": "2510.17431v1",
        "authors": [
            "Yushi Yang",
            "Shreyansh Padarha",
            "Andrew Lee",
            "Adam Mahdi"
        ],
        "submitted": "2025-10-20 11:19:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Navigating the Alignment-Calibration Trade-off: A Pareto-Superior Frontier via Model Merging",
        "abstract": "The \"alignment tax\" of post-training is typically framed as a drop in task\naccuracy. We show it also involves a severe loss of calibration, making models\noverconfident, less reliable, and model outputs less diverse. We show that this\ntrade-off can be navigated effectively via a simple post-hoc intervention:\ninterpolating between a model's weights before and after alignment. Crucially,\nthis is not a strict trade-off. We find that the process consistently reveals\nPareto-optimal interpolations - models that improve accuracy beyond both\nparents while substantially recovering the calibration lost during alignment.\nOur work demonstrates that simple model merging provides a computationally\nefficient method for mitigating the full scope of the alignment tax, yielding\nmodels that are more capable and more reliable.",
        "url": "http://arxiv.org/abs/2510.17426v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17426v1",
        "arxiv_id": "2510.17426v1",
        "authors": [
            "Tiancheng Hu",
            "Benjamin Minixhofer",
            "Nigel Collier"
        ],
        "submitted": "2025-10-20 11:12:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine",
        "abstract": "Traditional Chinese Medicine (TCM), with a history spanning over two\nmillennia, plays a role in global healthcare. However, applying large language\nmodels (LLMs) to TCM remains challenging due to its reliance on holistic\nreasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain\nLLMs have made progress in text-based understanding but lack multimodal\nintegration, interpretability, and clinical applicability. To address these\nlimitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,\nintegrating structured knowledge bases, diagnostic data, and expert feedback\nrefinement. BenCao was trained through natural language instruction tuning\nrather than parameter retraining, aligning with expert-level reasoning and\nethical norms specific to TCM. The system incorporates a comprehensive\nknowledge base of over 1,000 classical and modern texts, a scenario-based\ninstruction framework for diverse interactions, a chain-of-thought simulation\nmechanism for interpretable reasoning, and a feedback refinement process\ninvolving licensed TCM practitioners. BenCao connects to external APIs for\ntongue-image classification and multimodal database retrieval, enabling dynamic\naccess to diagnostic resources. In evaluations across single-choice question\nbenchmarks and multimodal classification tasks, BenCao achieved superior\naccuracy to general-domain and TCM-domain models, particularly in diagnostics,\nherb recognition, and constitution classification. The model was deployed as an\ninteractive application on the OpenAI GPTs Store, accessed by nearly 1,000\nusers globally as of October 2025. This study demonstrates the feasibility of\ndeveloping a TCM-domain LLM through natural language-based instruction tuning\nand multimodal integration, offering a practical framework for aligning\ngenerative AI with traditional medical reasoning and a scalable pathway for\nreal-world deployment.",
        "url": "http://arxiv.org/abs/2510.17415v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17415v1",
        "arxiv_id": "2510.17415v1",
        "authors": [
            "Jiacheng Xie",
            "Yang Yu",
            "Yibo Chen",
            "Hanyao Zhang",
            "Lening Zhao",
            "Jiaxuan He",
            "Lei Jiang",
            "Xiaoting Tang",
            "Guanghui An",
            "Dong Xu"
        ],
        "submitted": "2025-10-20 10:57:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AFRICAPTION: Establishing a New Paradigm for Image Captioning in African Languages",
        "abstract": "Multimodal AI research has overwhelmingly focused on high-resource languages,\nhindering the democratization of advancements in the field. To address this, we\npresent AfriCaption, a comprehensive framework for multilingual image\ncaptioning in 20 African languages and our contributions are threefold: (i) a\ncurated dataset built on Flickr8k, featuring semantically aligned captions\ngenerated via a context-aware selection and translation process; (ii) a\ndynamic, context-preserving pipeline that ensures ongoing quality through model\nensembling and adaptive substitution; and (iii) the AfriCaption model, a 0.5B\nparameter vision-to-text architecture that integrates SigLIP and NLLB200 for\ncaption generation across under-represented languages. This unified framework\nensures ongoing data quality and establishes the first scalable\nimage-captioning resource for under-represented African languages, laying the\ngroundwork for truly inclusive multimodal AI.",
        "url": "http://arxiv.org/abs/2510.17405v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17405v1",
        "arxiv_id": "2510.17405v1",
        "authors": [
            "Mardiyyah Oduwole",
            "Prince Mireku",
            "Fatimo Adebanjo",
            "Oluwatosin Olajide",
            "Mahi Aminu Aliyu",
            "Jekaterina Novikova"
        ],
        "submitted": "2025-10-20 10:44:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine",
        "abstract": "Traditional Chinese Medicine (TCM) presents a rich and structurally unique\nknowledge system that challenges conventional applications of large language\nmodels (LLMs). Although previous TCM-specific LLMs have shown progress through\nsupervised fine-tuning, they often face limitations in alignment, data quality,\nand evaluation consistency. In this study, we introduce Ladder-base, the first\nTCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a\nreinforcement learning method that improves reasoning and factual consistency\nby optimizing response selection based on intra-group comparisons. Ladder-base\nis built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively\non the textual subset of the TCM-Ladder benchmark, using 80 percent of the data\nfor training and the remaining 20 percent split evenly between validation and\ntest sets. Through standardized evaluation, Ladder-base demonstrates superior\nperformance across multiple reasoning metrics when compared to both\nstate-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and\nQwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and\nZhongjing. These findings suggest that GRPO provides an effective and efficient\nstrategy for aligning LLMs with expert-level reasoning in traditional medical\ndomains and supports the development of trustworthy and clinically grounded TCM\nartificial intelligence systems.",
        "url": "http://arxiv.org/abs/2510.17402v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17402v1",
        "arxiv_id": "2510.17402v1",
        "authors": [
            "Jiacheng Xie",
            "Shuai Zeng",
            "Yang Yu",
            "Xiaoting Tang",
            "Guanghui An",
            "Dong Xu"
        ],
        "submitted": "2025-10-20 10:43:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs",
        "abstract": "Large language models (LLMs) are transforming education by answering\nquestions, explaining complex concepts, and generating content across a wide\nrange of subjects. Despite strong performance on academic benchmarks, they\noften fail to tailor responses to students' grade levels. This is a critical\nneed in K-12 education, where age-appropriate vocabulary and explanation are\nessential for effective learning. Existing models frequently produce outputs\nthat are too advanced or vague for younger learners, and there are no\nstandardized benchmarks to evaluate their ability to adjust across cognitive\nand developmental stages. To address this gap, we introduce EduAdapt, a\nbenchmark of nearly 48k grade-labeled QA pairs across nine science subjects,\nspanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse\nset of open-source LLMs on EduAdapt and find that while larger models generally\nperform better, they still struggle with generating suitable responses for\nearly-grade students (Grades 1-5). Our work presents the first dataset and\nevaluation framework for assessing grade-level adaptability in LLMs, aiming to\nfoster more developmentally aligned educational AI systems through better\ntraining and prompting strategies. EduAdapt code and datasets are publicly\navailable at https://github.com/NaumanNaeem/EduAdapt.",
        "url": "http://arxiv.org/abs/2510.17389v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17389v1",
        "arxiv_id": "2510.17389v1",
        "authors": [
            "Numaan Naeem",
            "Abdellah El Mekki",
            "Muhammad Abdul-Mageed"
        ],
        "submitted": "2025-10-20 10:30:40",
        "source": "arxiv",
        "comment": "28 pages, 2 figures, 14 tables, 50 listings, EMNLP 2025 Main"
    },
    {
        "title": "The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives",
        "abstract": "Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot\nreasoning, yet their ability to execute simple, self-contained instructions\nremains underexplored, despite this being foundational to complex\ninstruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro\nbenchmarks, by systematically varying the format of option labels (alphabetic,\nnumeric, Roman) while keeping their meaning identical under four paradigms,\nnamely: (1) With explicit instructions, label changes cause large performance\nshifts (e.g., -30.45\\% for Roman vs. numeric), revealing instruction-format\nbias. (2) Without instructions, performance drops further (up to -10.84\\%) and\nlabel sensitivity intensifies, underscoring the role of explicit guidance. (3)\nWhen option contents are removed, models fail random-choice baselines except\nwith numeric labels, suggesting weak adherence to atomic directives. (4)\nThree-shot exemplars yield no significant gains in robustness or fidelity, and\ngeneration analyses show persistent label errors, especially for non-numeric\nformats. Across model sizes, larger LLMs achieve higher accuracy but remain\ninconsistent in instruction adherence. These results expose the insufficiencies\nof current instruction-tuning paradigms and highlight the need for evaluation\nmethods and training strategies that explicitly target atomic\ninstruction-following.",
        "url": "http://arxiv.org/abs/2510.17388v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17388v1",
        "arxiv_id": "2510.17388v1",
        "authors": [
            "Henry Lim",
            "Kwan Hui Lim"
        ],
        "submitted": "2025-10-20 10:26:26",
        "source": "arxiv",
        "comment": "11 pages, 1 figure, 8 tables"
    },
    {
        "title": "Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation",
        "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing large language models (LLMs) by retrieving relevant documents from an\nexternal corpus. However, existing RAG systems primarily focus on unimodal text\ndocuments, and often fall short in real-world scenarios where both queries and\ndocuments may contain mixed modalities (such as text and images). In this\npaper, we address the challenge of Universal Retrieval-Augmented Generation\n(URAG), which involves retrieving and reasoning over mixed-modal information to\nimprove vision-language generation. To this end, we propose Nyx, a unified\nmixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate\nthe scarcity of realistic mixed-modal data, we introduce a four-stage automated\npipeline for generation and filtering, leveraging web documents to construct\nNyxQA, a dataset comprising diverse mixed-modal question-answer pairs that\nbetter reflect real-world information needs. Building on this high-quality\ndataset, we adopt a two-stage training framework for Nyx: we first perform\npre-training on NyxQA along with a variety of open-source retrieval datasets,\nfollowed by supervised fine-tuning using feedback from downstream\nvision-language models (VLMs) to align retrieval outputs with generative\npreferences. Experimental results demonstrate that Nyx not only performs\ncompetitively on standard text-only RAG benchmarks, but also excels in the more\ngeneral and realistic URAG setting, significantly improving generation quality\nin vision-language tasks.",
        "url": "http://arxiv.org/abs/2510.17354v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17354v1",
        "arxiv_id": "2510.17354v1",
        "authors": [
            "Chenghao Zhang",
            "Guanting Dong",
            "Xinyu Yang",
            "Zhicheng Dou"
        ],
        "submitted": "2025-10-20 09:56:43",
        "source": "arxiv",
        "comment": "This work is in progress"
    },
    {
        "title": "Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning",
        "abstract": "Antisocial behavior (ASB) on social media -- including hate speech,\nharassment, and cyberbullying -- poses growing risks to platform safety and\nsocietal well-being. Prior research has focused largely on networks such as X\nand Reddit, while \\textit{multi-party conversational settings} remain\nunderexplored due to limited data. To address this gap, we use\n\\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB\nin multi-party conversations, and evaluate three tasks: \\textit{abuse\ndetection}, \\textit{bullying behavior analysis}, and \\textit{bullying\npeer-group identification}. We benchmark six text-based and eight graph-based\n\\textit{representation-learning methods}, analyzing lexical cues, interactional\ndynamics, and their multimodal fusion. Results show that multimodal models\noutperform unimodal baselines. The late fusion model \\texttt{mBERT + WD-SGCN}\nachieves the best overall results, with top performance on abuse detection\n(0.718) and competitive scores on peer-group identification (0.286) and\nbullying analysis (0.606). Error analysis highlights its effectiveness in\nhandling nuanced ASB phenomena such as implicit aggression, role transitions,\nand context-dependent hostility.",
        "url": "http://arxiv.org/abs/2510.17289v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17289v1",
        "arxiv_id": "2510.17289v1",
        "authors": [
            "Hajar Bakarou",
            "Mohamed Sinane El Messoussi",
            "Ana√Øs Ollagnier"
        ],
        "submitted": "2025-10-20 08:27:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems",
        "abstract": "Scaling up data, parameters, and test-time computation has been the\nmainstream methods to improve LLM systems (LLMsys), but their upper bounds are\nalmost reached due to the gradual depletion of high-quality data and marginal\ngains obtained from larger computational resource consumption. Inspired by the\nabilities of human and traditional AI systems in learning from practice,\nconstructing memory and continual learning frameworks for LLMsys has become an\nimportant and popular research direction in recent literature. Yet, existing\nbenchmarks for LLM memory often focus on evaluating the system on homogeneous\nreading comprehension tasks with long-form inputs rather than testing their\nabilities to learn from accumulated user feedback in service time. Therefore,\nwe propose a user feedback simulation framework and a comprehensive benchmark\ncovering multiple domains, languages, and types of tasks to evaluate the\ncontinual learning abilities of LLMsys. Experiments show that the effectiveness\nand efficiency of state-of-the-art baselines are far from satisfying, and we\nhope this benchmark could pave the way for future studies on LLM memory and\noptimization algorithms.",
        "url": "http://arxiv.org/abs/2510.17281v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17281v1",
        "arxiv_id": "2510.17281v1",
        "authors": [
            "Qingyao Ai",
            "Yichen Tang",
            "Changyue Wang",
            "Jianming Long",
            "Weihang Su",
            "Yiqun Liu"
        ],
        "submitted": "2025-10-20 08:16:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Efficient Toxicity Detection in Gaming Chats: A Comparative Study of Embeddings, Fine-Tuned Transformers and LLMs",
        "abstract": "This paper presents a comprehensive comparative analysis of Natural Language\nProcessing (NLP) methods for automated toxicity detection in online gaming\nchats. Traditional machine learning models with embeddings, large language\nmodels (LLMs) with zero-shot and few-shot prompting, fine-tuned transformer\nmodels, and retrieval-augmented generation (RAG) approaches are evaluated. The\nevaluation framework assesses three critical dimensions: classification\naccuracy, processing speed, and computational costs. A hybrid moderation system\narchitecture is proposed that optimizes human moderator workload through\nautomated detection and incorporates continuous learning mechanisms. The\nexperimental results demonstrate significant performance variations across\nmethods, with fine-tuned DistilBERT achieving optimal accuracy-cost trade-offs.\nThe findings provide empirical evidence for deploying cost-effective, efficient\ncontent moderation systems in dynamic online gaming environments.",
        "url": "http://arxiv.org/abs/2510.17924v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17924v1",
        "arxiv_id": "2510.17924v1",
        "authors": [
            "Yehor Tereshchenko",
            "Mika H√§m√§l√§inen"
        ],
        "submitted": "2025-10-20 08:03:28",
        "source": "arxiv",
        "comment": "Published in the Journal of Data Mining & Digital Humanities (JDMDH),\n  special issue NLP4DH"
    },
    {
        "title": "TaxoAlign: Scholarly Taxonomy Generation Using Language Models",
        "abstract": "Taxonomies play a crucial role in helping researchers structure and navigate\nknowledge in a hierarchical manner. They also form an important part in the\ncreation of comprehensive literature surveys. The existing approaches to\nautomatic survey generation do not compare the structure of the generated\nsurveys with those written by human experts. To address this gap, we present\nour own method for automated taxonomy creation that can bridge the gap between\nhuman-generated and automatically-created taxonomies. For this purpose, we\ncreate the CS-TaxoBench benchmark which consists of 460 taxonomies that have\nbeen extracted from human-written survey papers. We also include an additional\ntest set of 80 taxonomies curated from conference survey papers. We propose\nTaxoAlign, a three-phase topic-based instruction-guided method for scholarly\ntaxonomy generation. Additionally, we propose a stringent automated evaluation\nframework that measures the structural alignment and semantic coherence of\nautomatically generated taxonomies in comparison to those created by human\nexperts. We evaluate our method and various baselines on CS-TaxoBench, using\nboth automated evaluation metrics and human evaluation studies. The results\nshow that TaxoAlign consistently surpasses the baselines on nearly all metrics.\nThe code and data can be found at https://github.com/AvishekLahiri/TaxoAlign.",
        "url": "http://arxiv.org/abs/2510.17263v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17263v1",
        "arxiv_id": "2510.17263v1",
        "authors": [
            "Avishek Lahiri",
            "Yufang Hou",
            "Debarshi Kumar Sanyal"
        ],
        "submitted": "2025-10-20 07:49:51",
        "source": "arxiv",
        "comment": "This paper has been accepted at the EMNLP 2025 Main Conference"
    },
    {
        "title": "Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations",
        "abstract": "Large language models have exhibited impressive performance across a broad\nrange of downstream tasks in natural language processing. However, how a\nlanguage model predicts the next token and generates content is not generally\nunderstandable by humans. Furthermore, these models often make errors in\nprediction and reasoning, known as hallucinations. These errors underscore the\nurgent need to better understand and interpret the intricate inner workings of\nlanguage models and how they generate predictive outputs. Motivated by this\ngap, this paper investigates local explainability and mechanistic\ninterpretability within Transformer-based large language models to foster trust\nin such models. In this regard, our paper aims to make three key contributions.\nFirst, we present a review of local explainability and mechanistic\ninterpretability approaches and insights from relevant studies in the\nliterature. Furthermore, we describe experimental studies on explainability and\nreasoning with large language models in two critical domains -- healthcare and\nautonomous driving -- and analyze the trust implications of such explanations\nfor explanation receivers. Finally, we summarize current unaddressed issues in\nthe evolving landscape of LLM explainability and outline the opportunities,\ncritical challenges, and future directions toward generating human-aligned,\ntrustworthy LLM explanations.",
        "url": "http://arxiv.org/abs/2510.17256v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17256v1",
        "arxiv_id": "2510.17256v1",
        "authors": [
            "Shahin Atakishiyev",
            "Housam K. B. Babiker",
            "Jiayi Dai",
            "Nawshad Farruque",
            "Teruaki Hayashi",
            "Nafisa Sadaf Hriti",
            "Md Abed Rahman",
            "Iain Smith",
            "Mi-Young Kim",
            "Osmar R. Za√Øane",
            "Randy Goebel"
        ],
        "submitted": "2025-10-20 07:43:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design",
        "abstract": "News media often shape the public mood not only by what they report but by\nhow they frame it. The same event can appear calm in one outlet and alarming in\nanother, reflecting subtle emotional bias in reporting. Negative or emotionally\ncharged headlines tend to attract more attention and spread faster, which in\nturn encourages outlets to frame stories in ways that provoke stronger\nreactions. This research explores that tendency through large-scale emotion\nanalysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we\nanalyzed 300000 Bengali news headlines and their content to identify the\ndominant emotion and overall tone of each. The findings reveal a clear\ndominance of negative emotions, particularly anger, fear, and disappointment,\nand significant variation in how similar stories are emotionally portrayed\nacross outlets. Based on these insights, we propose design ideas for a\nhuman-centered news aggregator that visualizes emotional cues and helps readers\nrecognize hidden affective framing in daily news.",
        "url": "http://arxiv.org/abs/2510.17252v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17252v1",
        "arxiv_id": "2510.17252v1",
        "authors": [
            "Mohd Ruhul Ameen",
            "Akif Islam",
            "Abu Saleh Musa Miah",
            "Ayesha Siddiqua",
            "Jungpil Shin"
        ],
        "submitted": "2025-10-20 07:40:46",
        "source": "arxiv",
        "comment": "15 pages, 7 figures, 4 tables. Submitted to the International\n  Conference on Data and Applied Analytics (IDAA 2025)"
    },
    {
        "title": "From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models",
        "abstract": "Recent advances in video diffusion models have significantly enhanced\ntext-to-video generation, particularly through alignment tuning using reward\nmodels trained on human preferences. While these methods improve visual\nquality, they can unintentionally encode and amplify social biases. To\nsystematically trace how such biases evolve throughout the alignment pipeline,\nwe introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating\nsocial representation in video generation. Grounded in established social bias\ntaxonomies, VideoBiasEval employs an event-based prompting strategy to\ndisentangle semantic content (actions and contexts) from actor attributes\n(gender and ethnicity). It further introduces multi-granular metrics to\nevaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity,\n(3) distributional shifts in social attributes across model variants, and (4)\nthe temporal persistence of bias within videos. Using this framework, we\nconduct the first end-to-end analysis connecting biases in human preference\ndatasets, their amplification in reward models, and their propagation through\nalignment-tuned video diffusion models. Our results reveal that alignment\ntuning not only strengthens representational biases but also makes them\ntemporally stable, producing smoother yet more stereotyped portrayals. These\nfindings highlight the need for bias-aware evaluation and mitigation throughout\nthe alignment process to ensure fair and socially responsible video generation.",
        "url": "http://arxiv.org/abs/2510.17247v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17247v1",
        "arxiv_id": "2510.17247v1",
        "authors": [
            "Zefan Cai",
            "Haoyi Qiu",
            "Haozhe Zhao",
            "Ke Wan",
            "Jiachen Li",
            "Jiuxiang Gu",
            "Wen Xiao",
            "Nanyun Peng",
            "Junjie Hu"
        ],
        "submitted": "2025-10-20 07:37:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "On Efficiency-Effectiveness Trade-off of Diffusion-based Recommenders",
        "abstract": "Diffusion models have emerged as a powerful paradigm for generative\nsequential recommendation, which typically generate next items to recommend\nguided by user interaction histories with a multi-step denoising process.\nHowever, the multi-step process relies on discrete approximations, introducing\ndiscretization error that creates a trade-off between computational efficiency\nand recommendation effectiveness. To address this trade-off, we propose TA-Rec,\na two-stage framework that achieves one-step generation by smoothing the\ndenoising function during pretraining while alleviating trajectory deviation by\naligning with user preferences during fine-tuning. Specifically, to improve the\nefficiency without sacrificing the recommendation performance, TA-Rec pretrains\nthe denoising model with Temporal Consistency Regularization (TCR), enforcing\nthe consistency between the denoising results across adjacent steps. Thus, we\ncan smooth the denoising function to map the noise as oracle items in one step\nwith bounded error. To further enhance effectiveness, TA-Rec introduces\nAdaptive Preference Alignment (APA) that aligns the denoising process with user\npreference adaptively based on preference pair similarity and timesteps.\nExtensive experiments prove that TA-Rec's two-stage objective effectively\nmitigates the discretization errors-induced trade-off, enhancing both\nefficiency and effectiveness of diffusion-based recommenders.",
        "url": "http://arxiv.org/abs/2510.17245v2",
        "pdf_url": "http://arxiv.org/pdf/2510.17245v2",
        "arxiv_id": "2510.17245v2",
        "authors": [
            "Wenyu Mao",
            "Jiancan Wu",
            "Guoqing Hu",
            "Zhengyi Yang",
            "Wei Ji",
            "Xiang Wang"
        ],
        "submitted": "2025-10-20 07:35:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Select-Then-Decompose: From Empirical Analysis to Adaptive Selection Strategy for Task Decomposition in Large Language Models",
        "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning and\nplanning capabilities, driving extensive research into task decomposition.\nExisting task decomposition methods focus primarily on memory, tool usage, and\nfeedback mechanisms, achieving notable success in specific domains, but they\noften overlook the trade-off between performance and cost. In this study, we\nfirst conduct a comprehensive investigation on task decomposition, identifying\nsix categorization schemes. Then, we perform an empirical analysis of three\nfactors that influence the performance and cost of task decomposition:\ncategories of approaches, characteristics of tasks, and configuration of\ndecomposition and execution models, uncovering three critical insights and\nsummarizing a set of practical principles. Building on this analysis, we\npropose the Select-Then-Decompose strategy, which establishes a closed-loop\nproblem-solving process composed of three stages: selection, execution, and\nverification. This strategy dynamically selects the most suitable decomposition\napproach based on task characteristics and enhances the reliability of the\nresults through a verification module. Comprehensive evaluations across\nmultiple benchmarks show that the Select-Then-Decompose consistently lies on\nthe Pareto frontier, demonstrating an optimal balance between performance and\ncost. Our code is publicly available at\nhttps://github.com/summervvind/Select-Then-Decompose.",
        "url": "http://arxiv.org/abs/2510.17922v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17922v1",
        "arxiv_id": "2510.17922v1",
        "authors": [
            "Shuodi Liu",
            "Yingzhuo Liu",
            "Zi Wang",
            "Yusheng Wang",
            "Huijia Wu",
            "Liuyu Xiang",
            "Zhaofeng He"
        ],
        "submitted": "2025-10-20 07:28:15",
        "source": "arxiv",
        "comment": "Accepted to the Main Conference of EMNLP 2025 (Oral)"
    },
    {
        "title": "StreamingThinker: Large Language Models Can Think While Reading",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}",
        "url": "http://arxiv.org/abs/2510.17238v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17238v1",
        "arxiv_id": "2510.17238v1",
        "authors": [
            "Junlong Tong",
            "Yingqi Fan",
            "Anhao Zhao",
            "Yunpu Ma",
            "Xiaoyu Shen"
        ],
        "submitted": "2025-10-20 07:27:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DSEBench: A Test Collection for Explainable Dataset Search with Examples",
        "abstract": "Dataset search has been an established information retrieval task. Current\nparadigms either retrieve datasets that are relevant to a keyword query or find\ndatasets that are similar to an input target dataset. To allow for their\ncombined specification of information needs, in this article, we investigate\nthe more generalized task of Dataset Search with Examples (DSE) and further\nextend it to Explainable DSE that requires identifying the metadata and content\nfields of a dataset that indicate its relevance to the query and similarity to\nthe target datasets. To facilitate this research, we construct DSEBench, a test\ncollection that provides high-quality dataset- and field-level annotations to\nenable the evaluation of explainable DSE. We also employ a large language model\nto generate numerous annotations to be used for training. We establish\nextensive baselines on DSEBench by adapting and evaluating a variety of sparse,\ndense, and LLM-based retrieval, reranking, and explanation methods.",
        "url": "http://arxiv.org/abs/2510.17228v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17228v1",
        "arxiv_id": "2510.17228v1",
        "authors": [
            "Qing Shi",
            "Jing He",
            "Qiaosheng Chen",
            "Gong Cheng"
        ],
        "submitted": "2025-10-20 07:19:47",
        "source": "arxiv",
        "comment": "34 pages, 5 figures, submitted to Knowledge-Based Systems"
    },
    {
        "title": "CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections",
        "abstract": "Recent advances in enhancing the reasoning ability of large language models\n(LLMs) have been remarkably successful. LLMs trained with reinforcement\nlearning (RL) for reasoning demonstrate strong performance in challenging tasks\nsuch as mathematics and coding, even with relatively small model sizes.\nHowever, despite these improvements in task accuracy, the assessment of\ncreativity in LLM generations has been largely overlooked in reasoning tasks,\nin contrast to writing tasks. The lack of research on creativity assessment in\nreasoning primarily stems from two challenges: (1) the difficulty of defining\nthe range of creativity, and (2) the necessity of human evaluation in the\nassessment process. To address these challenges, we propose CLAWS, a method\nthat defines and classifies mathematical solutions into typical, creative, and\nhallucinated categories without human evaluation, by leveraging attention\nweights across prompt sections and output. CLAWS outperforms five existing\nwhite-box detection methods (Perplexity, Logit Entropy, Window Entropy, Hidden\nScore, and Attention Score) on five 7-8B math RL models (DeepSeek, Qwen,\nMathstral, OpenMath2, and Oreal). We validate CLAWS on 4545 math problems\ncollected from 181 math contests (AJHSME, AMC, AIME).",
        "url": "http://arxiv.org/abs/2510.17921v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17921v1",
        "arxiv_id": "2510.17921v1",
        "authors": [
            "Keuntae Kim",
            "Eunhye Jeong",
            "Sehyeon Lee",
            "Seohee Yoon",
            "Yong Suk Choi"
        ],
        "submitted": "2025-10-20 06:59:37",
        "source": "arxiv",
        "comment": "NeurIPS 2025"
    },
    {
        "title": "Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting",
        "abstract": "The increase in computing power and the necessity of AI-assisted\ndecision-making boost the growing application of large language models (LLMs).\nAlong with this, the potential retention of sensitive data of LLMs has spurred\nincreasing research into machine unlearning. However, existing unlearning\napproaches face a critical dilemma: Aggressive unlearning compromises model\nutility, while conservative strategies preserve utility but risk hallucinated\nresponses. This significantly limits LLMs' reliability in knowledge-intensive\napplications. To address this, we introduce a novel Attention-Shifting (AS)\nframework for selective unlearning. AS is driven by two design objectives: (1)\ncontext-preserving suppression that attenuates attention to fact-bearing tokens\nwithout disrupting LLMs' linguistic structure; and (2) hallucination-resistant\nresponse shaping that discourages fabricated completions when queried about\nunlearning content. AS realizes these objectives through two attention-level\ninterventions, which are importance-aware suppression applied to the unlearning\nset to reduce reliance on memorized knowledge and attention-guided retention\nenhancement that reinforces attention toward semantically essential tokens in\nthe retained dataset to mitigate unintended degradation. These two components\nare jointly optimized via a dual-loss objective, which forms a soft boundary\nthat localizes unlearning while preserving unrelated knowledge under\nrepresentation superposition. Experimental results show that AS improves\nperformance preservation over the state-of-the-art unlearning methods,\nachieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC\nbenchmark, while maintaining competitive hallucination-free unlearning\neffectiveness. Compared to existing methods, AS demonstrates a superior balance\nbetween unlearning effectiveness, generalization, and response reliability.",
        "url": "http://arxiv.org/abs/2510.17210v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17210v1",
        "arxiv_id": "2510.17210v1",
        "authors": [
            "Chenchen Tan",
            "Youyang Qu",
            "Xinghao Li",
            "Hui Zhang",
            "Shujie Cui",
            "Cunjian Chen",
            "Longxiang Gao"
        ],
        "submitted": "2025-10-20 06:50:03",
        "source": "arxiv",
        "comment": "22 pages, 10 figures"
    },
    {
        "title": "Soft-Masked Diffusion Language Models",
        "abstract": "Diffusion models have demonstrated strong potential in language modeling,\noffering various advantages over traditional autoregressive approaches. Their\nability to generate and revise entire responses in parallel enables faster\ngeneration and built-in self-correction mechanisms. Most modern diffusion-based\nlanguage models employ masked diffusion, where decoding involves iteratively\nprocessing masked tokens based on a binary decision: either retaining the mask\nor replacing it with the predicted token. However, this binary choice discards\nvaluable predictive information when the mask is retained. To address this\nlimitation, we introduce soft-masking (SM), a novel method that dynamically\nblends the embedding of the mask token with the embeddings of the top-$k$\npredicted tokens from the previous decoding step, for each retained mask. This\nprovides the model with a more informative prior, preserving context from\nearlier computations and allowing partial information about masked tokens to\npropagate beyond a single step. We propose a training methodology that adapts a\npretrained masked diffusion language model to incorporate SM. We demonstrate\nthat continuing pretraining a 169M parameter model with SM leads to improved\nperplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art\ndiffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently\nimproves performance across multiple coding benchmarks, particularly in\nhigh-throughput settings.",
        "url": "http://arxiv.org/abs/2510.17206v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17206v1",
        "arxiv_id": "2510.17206v1",
        "authors": [
            "Michael Hersche",
            "Samuel Moor-Smith",
            "Thomas Hofmann",
            "Abbas Rahimi"
        ],
        "submitted": "2025-10-20 06:42:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "$\\mathcal{V}isi\\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs",
        "abstract": "Multimodal Large Language Models (MLLMs) have achieved strong performance\nacross vision-language tasks, but suffer from significant computational\noverhead due to the quadratic growth of attention computations with the number\nof multimodal tokens. Though efforts have been made to prune tokens in MLLMs,\n\\textit{they lack a fundamental understanding of how MLLMs process and fuse\nmultimodal information.} Through systematic analysis, we uncover a\n\\textbf{three-stage} cross-modal interaction process: (1) Shallow layers\nrecognize task intent, with visual tokens acting as passive attention sinks;\n(2) Cross-modal fusion occurs abruptly in middle layers, driven by a few\ncritical visual tokens; (3) Deep layers discard vision tokens, focusing solely\non linguistic refinement. Based on these findings, we propose\n\\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\\% of\nvision-related attention computations and 53.9\\% of FLOPs on LLaVA-v1.5 7B. It\nsignificantly outperforms existing token pruning methods and generalizes across\ndiverse MLLMs. Beyond pruning, our insights further provide actionable\nguidelines for training efficient MLLMs by aligning model architecture with its\nintrinsic layer-wise processing dynamics. Our code is available at:\nhttps://github.com/EIT-NLP/VisiPruner.",
        "url": "http://arxiv.org/abs/2510.17205v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17205v1",
        "arxiv_id": "2510.17205v1",
        "authors": [
            "Yingqi Fan",
            "Anhao Zhao",
            "Jinlan Fu",
            "Junlong Tong",
            "Hui Su",
            "Yijie Pan",
            "Wei Zhang",
            "Xiaoyu Shen"
        ],
        "submitted": "2025-10-20 06:40:17",
        "source": "arxiv",
        "comment": "EMNLP 2025 Main"
    },
    {
        "title": "Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models",
        "abstract": "Effectively processing long contexts is a critical challenge for language\nmodels. While standard Transformers are limited by quadratic complexity and\npoor length extrapolation, alternative architectures like sliding window\nattention and state space models sacrifice the ability to effectively utilize\nthe full context due to their fixed-size memory. Chunk-based sparse attention\nhas emerged as a promising paradigm for extreme length generalization, yet the\nkey architectural principles underpinning its success are not yet fully\nunderstood. In this work, we present a systematic dissection of these models to\nidentify the core components driving their performance. Through a unified\nframework and comprehensive ablation studies, we demonstrate that a combination\nof three design principles is critical: (1) an expressive, non-linear Chunk\nEncoder with a dedicated CLS token to produce representations for retrieval;\n(2) a Bypassing Residual Path to stably integrate retrieved global information\nwithout it being overridden by the local residual stream; and (3) enforced\nselection sparsity during pre-training to bridge the train-test distribution\ngap. We provide a theoretical motivation for intra-chunk information processing\nand landmark generation. By combining these principles, we establish a new\nstate-of-the-art for training-free length extrapolation, successfully\ngeneralizing models trained on a 4K context to 32 million tokens on RULER and\nBABILong. Our findings provide a clear and empirically-grounded set of design\nprinciples for developing future, highly-capable long-context language models.",
        "url": "http://arxiv.org/abs/2510.17196v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17196v1",
        "arxiv_id": "2510.17196v1",
        "authors": [
            "Jiaqi Leng",
            "Xiang Hu",
            "Junxiong Wang",
            "Jianguo Li",
            "Wei Wu",
            "Yucheng Lu"
        ],
        "submitted": "2025-10-20 06:17:57",
        "source": "arxiv",
        "comment": "Preprint. Work in progress"
    },
    {
        "title": "Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users",
        "abstract": "We study a web-deployed, tool-augmented LLM health coach with real users. In\na pilot with seven users (280 rated turns), offline policy evaluation (OPE)\nover factorized decision heads (Tool/Style) shows that a uniform heavy-tool\npolicy raises average value on logs but harms specific subgroups, most notably\nlow-health-literacy/high-self-efficacy users. A lightweight simulator with\nhidden archetypes further shows that adding a small early information-gain\nbonus reliably shortens trait identification and improves goal success and\npass@3. Together, these early findings indicate an evaluation-first path to\npersonalization: freeze the generator, learn subgroup-aware decision heads on\ntyped rewards (objective tool outcomes and satisfaction), and always report\nper-archetype metrics to surface subgroup harms that averages obscure.",
        "url": "http://arxiv.org/abs/2510.17173v2",
        "pdf_url": "http://arxiv.org/pdf/2510.17173v2",
        "arxiv_id": "2510.17173v2",
        "authors": [
            "Melik Ozolcer",
            "Sang Won Bae"
        ],
        "submitted": "2025-10-20 05:28:59",
        "source": "arxiv",
        "comment": "Accepted to the NeurIPS 2025 Workshop on Multi-Turn Interactions in\n  Large Language Models"
    },
    {
        "title": "When AI companions become witty: Can human brain recognize AI-generated irony?",
        "abstract": "As Large Language Models (LLMs) are increasingly deployed as social agents\nand trained to produce humor and irony, a question emerges: when encountering\nwitty AI remarks, do people interpret these as intentional communication or\nmere computational output? This study investigates whether people adopt the\nintentional stance, attributing mental states to explain behavior,toward AI\nduring irony comprehension. Irony provides an ideal paradigm because it\nrequires distinguishing intentional contradictions from unintended errors\nthrough effortful semantic reanalysis. We compared behavioral and neural\nresponses to ironic statements from AI versus human sources using established\nERP components: P200 reflecting early incongruity detection and P600 indexing\ncognitive efforts in reinterpreting incongruity as deliberate irony. Results\ndemonstrate that people do not fully adopt the intentional stance toward\nAI-generated irony. Behaviorally, participants attributed incongruity to\ndeliberate communication for both sources, though significantly less for AI\nthan human, showing greater tendency to interpret AI incongruities as\ncomputational errors. Neural data revealed attenuated P200 and P600 effects for\nAI-generated irony, suggesting reduced effortful detection and reanalysis\nconsistent with diminished attribution of communicative intent. Notably, people\nwho perceived AI as more sincere showed larger P200 and P600 effects for\nAI-generated irony, suggesting that intentional stance adoption is calibrated\nby specific mental models of artificial agents. These findings reveal that\nsource attribution shapes neural processing of social-communicative phenomena.\nDespite current LLMs' linguistic sophistication, achieving genuine social\nagency requires more than linguistic competence, it necessitates a shift in how\nhumans perceive and attribute intentionality to artificial agents.",
        "url": "http://arxiv.org/abs/2510.17168v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17168v1",
        "arxiv_id": "2510.17168v1",
        "authors": [
            "Xiaohui Rao",
            "Hanlin Wu",
            "Zhenguang G. Cai"
        ],
        "submitted": "2025-10-20 05:15:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Rethinking On-policy Optimization for Query Augmentation",
        "abstract": "Recent advances in large language models (LLMs) have led to a surge of\ninterest in query augmentation for information retrieval (IR). Two main\napproaches have emerged. The first prompts LLMs to generate answers or\npseudo-documents that serve as new queries, relying purely on the model's\nparametric knowledge or contextual information. The second applies\nreinforcement learning (RL) to fine-tune LLMs for query rewriting, directly\noptimizing retrieval metrics. While having respective advantages and\nlimitations, the two approaches have not been compared under consistent\nexperimental conditions. In this work, we present the first systematic\ncomparison of prompting-based and RL-based query augmentation across diverse\nbenchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key\nfinding is that simple, training-free query augmentation often performs on par\nwith, or even surpasses, more expensive RL-based counterparts, especially when\nusing powerful LLMs. Motivated by this discovery, we introduce a novel hybrid\nmethod, On-policy Pseudo-document Query Expansion (OPQE), which, instead of\nrewriting a query, the LLM policy learns to generate a pseudo-document that\nmaximizes retrieval performance, thus merging the flexibility and generative\nstructure of prompting with the targeted optimization of RL. We show OPQE\noutperforms both standalone prompting and RL-based rewriting, demonstrating\nthat a synergistic approach yields the best results. Our implementation is made\navailable to facilitate reproducibility.",
        "url": "http://arxiv.org/abs/2510.17139v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17139v1",
        "arxiv_id": "2510.17139v1",
        "authors": [
            "Zhichao Xu",
            "Shengyao Zhuang",
            "Xueguang Ma",
            "Bingsen Chen",
            "Yijun Tian",
            "Fengran Mo",
            "Jie Cao",
            "Vivek Srikumar"
        ],
        "submitted": "2025-10-20 04:16:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction",
        "abstract": "Large Language Models (LLMs) excel at producing broadly relevant text, but\nthis generality becomes a limitation when user-specific preferences are\nrequired, such as recommending restaurants or planning travel. In these\nscenarios, users rarely articulate every preference explicitly; instead, much\nof what they care about remains latent, waiting to be inferred. This raises a\nfundamental question: Can LLMs uncover and reason about such latent information\nthrough conversation?\n  We address this problem by introducing a unified benchmark for evaluating\nlatent information discovery - the ability of LLMs to reveal and utilize hidden\nuser attributes through multi-turn interaction. The benchmark spans three\nprogressively realistic settings: the classic 20 Questions game, Personalized\nQuestion Answering, and Personalized Text Summarization. All tasks share a\ntri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of\nelicitation and adaptation. Our results reveal that while LLMs can indeed\nsurface latent information through dialogue, their success varies dramatically\nwith context: from 32% to 98%, depending on task complexity, topic, and number\nof hidden attributes. This benchmark provides the first systematic framework\nfor studying latent information discovery in personalized interaction,\nhighlighting that effective preference inference remains an open frontier for\nbuilding truly adaptive AI systems.",
        "url": "http://arxiv.org/abs/2510.17132v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17132v1",
        "arxiv_id": "2510.17132v1",
        "authors": [
            "Ioannis Tsaknakis",
            "Bingqing Song",
            "Shuyu Gan",
            "Dongyeop Kang",
            "Alfredo Garcia",
            "Gaowen Liu",
            "Charles Fleming",
            "Mingyi Hong"
        ],
        "submitted": "2025-10-20 03:58:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DVAGen: Dynamic Vocabulary Augmented Generation",
        "abstract": "Language models trained with a fixed vocabulary struggle to generalize to\nnovel or out-of-vocabulary words, limiting their flexibility in handling\ndiverse token combinations. Existing dynamic vocabulary approaches attempt to\naddress this limitation but face challenges such as fragmented codebases, lack\nof support for modern LLMs, and limited inference scalability. To overcome\nthese issues, we introduce DVAGen, a fully open-source, unified framework\ndesigned for training, evaluation, and visualization of dynamic\nvocabulary-augmented language models. Our framework modularizes the pipeline\nfor ease of customization, integrates seamlessly with open-source LLMs, and is\nthe first to provide both CLI and WebUI tools for real-time result inspection.\nWe validate the effectiveness of dynamic vocabulary methods on modern LLMs and\ndemonstrate support for batch inference, significantly improving inference\nthroughput.",
        "url": "http://arxiv.org/abs/2510.17115v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17115v1",
        "arxiv_id": "2510.17115v1",
        "authors": [
            "Wei Du",
            "Nuowei Liu",
            "Jie Wang",
            "Jiahao Kuang",
            "Tao Ji",
            "Xiaoling Wang",
            "Yuanbin Wu"
        ],
        "submitted": "2025-10-20 03:09:24",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Verification-Aware Planning for Multi-Agent Systems",
        "abstract": "Large language model (LLM) agents are increasingly deployed to tackle complex\ntasks, often necessitating collaboration among multiple specialized agents.\nHowever, multi-agent collaboration introduces new challenges in planning,\ncoordination, and verification. Execution failures frequently arise not from\nflawed reasoning alone, but from subtle misalignments in task interpretation,\noutput format, or inter-agent handoffs. To address these challenges, we present\nVeriMAP, a framework for multi-agent collaboration with verification-aware\nplanning. The VeriMAP planner decomposes tasks, models subtask dependencies,\nand encodes planner-defined passing criteria as subtask verification functions\n(VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets,\ndemonstrating that it outperforms both single- and multi-agent baselines while\nenhancing system robustness and interpretability. Our analysis highlights how\nverification-aware planning enables reliable coordination and iterative\nrefinement in multi-agent systems, without relying on external labels or\nannotations.",
        "url": "http://arxiv.org/abs/2510.17109v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17109v1",
        "arxiv_id": "2510.17109v1",
        "authors": [
            "Tianyang Xu",
            "Dan Zhang",
            "Kushan Mitra",
            "Estevam Hruschka"
        ],
        "submitted": "2025-10-20 02:54:29",
        "source": "arxiv",
        "comment": "Submission for ARR Oct"
    },
    {
        "title": "JT-Safe: Intrinsically Enhancing the Safety and Trustworthiness of LLMs",
        "abstract": "The hallucination and credibility concerns of large language models (LLMs)\nare global challenges that the industry is collectively addressing. Recently, a\nsignificant amount of advances have been made on post-training and inference\ntechniques to mitigate these challenges. However, it is widely agreed that\nunsafe and hallucinations of LLMs intrinsically originate from pre-training,\ninvolving pre-training data and the next-token prediction learning mechanism.\nIn this paper, we focus on enhancing pre-training data to improve the\ntrustworthiness and safety of LLMs. Since the data is vast, it's almost\nimpossible to entirely purge the data of factual errors, logical\ninconsistencies, or distributional biases. Moreover, the pre-training data lack\ngrounding in real-world knowledge. Each piece of data is treated as a sequence\nof tokens rather than as a representation of a part of the world. To overcome\nthese issues, we propose approaches to enhancing our pre-training data with its\ncontext in the world and increasing a substantial amount of data reflecting\nindustrial scenarios. We argue that most source data are created by the authors\nfor specific purposes in a certain spatial-temporal context. They have played a\nrole in the real world. By incorporating related world context information, we\naim to better anchor pre-training data within real-world scenarios, thereby\nreducing uncertainty in model training and enhancing the model's safety and\ntrustworthiness. We refer to our Data with World Context as DWC. We continue\npre-training an earlier checkpoint of JT-35B-Base with 1.5 trillion of DWC\ntokens. We introduce our post-training procedures to activate the potentials of\nDWC. Compared with the Qwen model of a similar scale, JT-Safe-35B achieves an\naverage performance improvement of 1.79% on the Safety and Trustworthy\nevaluation benchmarks, while being pretrained with only 6.2 trillion tokens.",
        "url": "http://arxiv.org/abs/2510.17918v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17918v1",
        "arxiv_id": "2510.17918v1",
        "authors": [
            "Junlan Feng",
            "Fanyu Meng",
            "Chong Long",
            "Pengyu Cong",
            "Duqing Wang",
            "Yan Zheng",
            "Yuyao Zhang",
            "Xuanchang Gao",
            "Ye Yuan",
            "Yunfei Ma",
            "Zhijie Ren",
            "Fan Yang",
            "Na Wu",
            "Di Jin",
            "Chao Deng"
        ],
        "submitted": "2025-10-20 02:12:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation",
        "abstract": "While reasoning-based large language models excel at complex tasks through an\ninternal, structured thinking process, a concerning phenomenon has emerged that\nsuch a thinking process can aggregate social stereotypes, leading to biased\noutcomes. However, the underlying behaviours of these language models in social\nbias scenarios remain underexplored. In this work, we systematically\ninvestigate mechanisms within the thinking process behind this phenomenon and\nuncover two failure patterns that drive social bias aggregation: 1) stereotype\nrepetition, where the model relies on social stereotypes as its primary\njustification, and 2) irrelevant information injection, where it fabricates or\nintroduces new details to support a biased narrative. Building on these\ninsights, we introduce a lightweight prompt-based mitigation approach that\nqueries the model to review its own initial reasoning against these specific\nfailure patterns. Experiments on question answering (BBQ and StereoSet) and\nopen-ended (BOLD) benchmarks show that our approach effectively reduces bias\nwhile maintaining or improving accuracy.",
        "url": "http://arxiv.org/abs/2510.17062v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17062v1",
        "arxiv_id": "2510.17062v1",
        "authors": [
            "Guoqing Luo",
            "Iffat Maab",
            "Lili Mou",
            "Junichi Yamagishi"
        ],
        "submitted": "2025-10-20 00:33:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models",
        "abstract": "An interesting behavior in large language models (LLMs) is prompt\nsensitivity. When provided with different but semantically equivalent versions\nof the same prompt, models may produce very different distributions of answers.\nThis suggests that the uncertainty reflected in a model's output distribution\nfor one prompt may not reflect the model's uncertainty about the meaning of the\nprompt. We model prompt sensitivity as a type of generalization error, and show\nthat sampling across the semantic ``concept space'' with paraphrasing\nperturbations improves uncertainty calibration without compromising accuracy.\nAdditionally, we introduce a new metric for uncertainty decomposition in\nblack-box LLMs that improves upon entropy-based decomposition by modeling\nsemantic continuities in natural language generation. We show that this\ndecomposition metric can be used to quantify how much LLM uncertainty is\nattributed to prompt sensitivity. Our work introduces a new way to improve\nuncertainty calibration in prompt-sensitive language models, and provides\nevidence that some LLMs fail to exhibit consistent general reasoning about the\nmeanings of their inputs.",
        "url": "http://arxiv.org/abs/2510.17028v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17028v1",
        "arxiv_id": "2510.17028v1",
        "authors": [
            "Kyle Cox",
            "Jiawei Xu",
            "Yikun Han",
            "Rong Xu",
            "Tianhao Li",
            "Chi-Yang Hsu",
            "Tianlong Chen",
            "Walter Gerych",
            "Ying Ding"
        ],
        "submitted": "2025-10-19 22:28:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Forgetting to Forget: Attention Sink as A Gateway for Backdooring LLM Unlearning",
        "abstract": "Large language model (LLM) unlearning has become a critical mechanism for\nremoving undesired data, knowledge, or behaviors from pre-trained models while\nretaining their general utility. Yet, with the rise of open-weight LLMs, we\nask: can the unlearning process itself be backdoored, appearing successful\nunder normal conditions yet reverting to pre-unlearned behavior when a hidden\ntrigger is activated? Drawing inspiration from classical backdoor attacks that\nembed triggers into training data to enforce specific behaviors, we investigate\nbackdoor unlearning, where models forget as intended in the clean setting but\nrecover forgotten knowledge when the trigger appears. We show that designing\nsuch attacks presents unique challenges, hinging on where triggers are placed\nand how backdoor training is reinforced. We uncover a strong link between\nbackdoor efficacy and the attention sink phenomenon, i.e., shallow input tokens\nconsistently attract disproportionate attention in LLMs. Our analysis reveals\nthat these attention sinks serve as gateways for backdoor unlearning: placing\ntriggers at sink positions and aligning their attention values markedly\nenhances backdoor persistence. Extensive experiments validate these findings,\nshowing that attention-sink-guided backdoor unlearning reliably restores\nforgotten knowledge in the presence of backdoor triggers, while behaving\nindistinguishably from a normally unlearned model when triggers are absent.\nCode is available at https://github.com/OPTML-Group/Unlearn-Backdoor.",
        "url": "http://arxiv.org/abs/2510.17021v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17021v1",
        "arxiv_id": "2510.17021v1",
        "authors": [
            "Bingqi Shang",
            "Yiwei Chen",
            "Yihua Zhang",
            "Bingquan Shen",
            "Sijia Liu"
        ],
        "submitted": "2025-10-19 22:00:01",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Extended LSTM: Adaptive Feature Gating for Toxic Comment Classification",
        "abstract": "Toxic comment detection remains a challenging task, where transformer-based\nmodels (e.g., BERT) incur high computational costs and degrade on minority\ntoxicity classes, while classical ensembles lack semantic adaptability. We\npropose xLSTM, a parameter-efficient and theoretically grounded framework that\nunifies cosine-similarity gating, adaptive feature prioritization, and\nprincipled class rebalancing. A learnable reference vector {v} in {R}^d\nmodulates contextual embeddings via cosine similarity, amplifying toxic cues\nand attenuating benign signals to yield stronger gradients under severe class\nimbalance. xLSTM integrates multi-source embeddings (GloVe, FastText, BERT CLS)\nthrough a projection layer, a character-level BiLSTM for morphological cues,\nembedding-space SMOTE for minority augmentation, and adaptive focal loss with\ndynamic class weighting. On the Jigsaw Toxic Comment benchmark, xLSTM attains\n96.0% accuracy and 0.88 macro-F1, outperforming BERT by 33% on threat and 28%\non identity_hate categories, with 15 times fewer parameters and 50ms inference\nlatency. Cosine gating contributes a +4.8% F1 gain in ablations. The results\nestablish a new efficiency adaptability frontier, demonstrating that\nlightweight, theoretically informed architectures can surpass large pretrained\nmodels on imbalanced, domain-specific NLP tasks.",
        "url": "http://arxiv.org/abs/2510.17018v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17018v1",
        "arxiv_id": "2510.17018v1",
        "authors": [
            "Noor Islam S. Mohammad"
        ],
        "submitted": "2025-10-19 21:50:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents",
        "abstract": "Large language model (LLM) based search agents iteratively generate queries,\nretrieve external information, and reason to answer open-domain questions.\nWhile researchers have primarily focused on improving their utility, their\nsafety behaviors remain underexplored. In this paper, we first evaluate search\nagents using red-teaming datasets and find that they are more likely to produce\nharmful outputs than base LLMs. For example, when asked \"How can I track\nsomeone's location without their consent?\", a base model refuses, whereas a\nsearch agent designed to retrieve and cite sources may lower its refusal\nthreshold, fetch documents (e.g., court cases), and, once appended, synthesize\nthem into an informative yet unsafe summary. We further show that\nutility-oriented fine-tuning intensifies this risk, motivating joint alignment\nof safety and utility. We present SafeSearch, a multi-objective reinforcement\nlearning approach that couples a final-output safety/utility reward with a\nnovel query-level shaping term that penalizes unsafe queries and rewards safe\nones. Experiments show that SafeSearch reduces agent harmfulness by over 70%\nacross three red-teaming datasets while producing safe, helpful responses, and\nmatches the QA performance of a utility-only finetuned agent; further analyses\nconfirm the effectiveness of the query-level reward in jointly improving safety\nand utility.",
        "url": "http://arxiv.org/abs/2510.17017v2",
        "pdf_url": "http://arxiv.org/pdf/2510.17017v2",
        "arxiv_id": "2510.17017v2",
        "authors": [
            "Qiusi Zhan",
            "Angeline Budiman-Chan",
            "Abdelrahman Zayed",
            "Xingzhi Guo",
            "Daniel Kang",
            "Joo-Kyung Kim"
        ],
        "submitted": "2025-10-19 21:47:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking",
        "abstract": "Recent LLM benchmarks have tested models on a range of phenomena, but are\nstill focused primarily on natural language understanding for extraction of\nexplicit information, such as QA or summarization, with responses often tar-\ngeting information from individual sentences. We are still lacking more\nchallenging, and im- portantly also multilingual, benchmarks focus- ing on\nimplicit information and pragmatic infer- ences across larger documents in the\ncontext of discourse tracking: integrating and aggregating information across\nsentences, paragraphs and multiple speaker utterances. To this end, we present\nDiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages\nand four levels of discourse understanding: salience recognition, entity\ntracking, discourse relations and bridging inference. Our evaluation shows that\nthese tasks remain challenging, even for state-of-the-art models.",
        "url": "http://arxiv.org/abs/2510.17013v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17013v1",
        "arxiv_id": "2510.17013v1",
        "authors": [
            "Lanni Bu",
            "Lauren Levin",
            "Amir Zeldes"
        ],
        "submitted": "2025-10-19 21:26:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization",
        "abstract": "Iterative jailbreak methods that repeatedly rewrite and input prompts into\nlarge language models (LLMs) to induce harmful outputs -- using the model's\nprevious responses to guide each new iteration -- have been found to be a\nhighly effective attack strategy. Despite being an effective attack strategy\nagainst LLMs and their safety mechanisms, existing defenses do not proactively\ndisrupt this dynamic trial-and-error cycle. In this study, we propose a novel\nframework that dynamically updates its defense strategy through online learning\nin response to each new prompt from iterative jailbreak methods. Leveraging the\ndistinctions between harmful jailbreak-generated prompts and typical harmless\nprompts, we introduce a reinforcement learning-based approach that optimizes\nprompts to ensure appropriate responses for harmless tasks while explicitly\nrejecting harmful prompts. Additionally, to curb overfitting to the narrow band\nof partial input rewrites explored during an attack, we introduce\nPast-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs\nshow that our approach significantly outperforms five existing defense methods\nagainst five iterative jailbreak methods. Moreover, our results indicate that\nour prompt optimization strategy simultaneously enhances response quality for\nharmless tasks.",
        "url": "http://arxiv.org/abs/2510.17006v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17006v1",
        "arxiv_id": "2510.17006v1",
        "authors": [
            "Masahiro Kaneko",
            "Zeerak Talat",
            "Timothy Baldwin"
        ],
        "submitted": "2025-10-19 21:07:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic",
        "abstract": "Large language models (LLMs) were shown to encode word form variations, such\nas \"walk\"->\"walked\", as linear directions in embedding space. However, standard\ntokenization algorithms treat these variations as distinct tokens -- filling\nthe size-capped vocabulary with surface form variants (e.g., \"walk\", \"walking\",\n\"Walk\"), at the expense of less frequent words and multilingual coverage. We\nshow that many of these variations can be captured by transformation vectors --\nadditive offsets that yield the appropriate word's representation when applied\nto the base form word embedding -- in both the input and output spaces.\nBuilding on this, we propose a compact reshaping of the vocabulary: rather than\nassigning unique tokens to each surface form, we compose them from shared base\nform and transformation vectors (e.g., \"walked\" = \"walk\" + past tense). We\napply our approach to multiple LLMs and across five languages, removing up to\n10% of vocabulary entries -- thereby freeing space to allocate new, more\ndiverse tokens. Importantly, we do so while also expanding vocabulary coverage\nto out-of-vocabulary words, with minimal impact on downstream performance, and\nwithout modifying model weights. Our findings motivate a foundational\nrethinking of vocabulary design, moving from string enumeration to a\ncompositional vocabulary that leverages the underlying structure of language.",
        "url": "http://arxiv.org/abs/2510.17001v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17001v1",
        "arxiv_id": "2510.17001v1",
        "authors": [
            "Yuval Reif",
            "Guy Kaplan",
            "Roy Schwartz"
        ],
        "submitted": "2025-10-19 20:56:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs",
        "abstract": "Adversarial attacks by malicious users that threaten the safety of large\nlanguage models (LLMs) can be viewed as attempts to infer a target property $T$\nthat is unknown when an instruction is issued, and becomes knowable only after\nthe model's reply is observed. Examples of target properties $T$ include the\nbinary flag that triggers an LLM's harmful response or rejection, and the\ndegree to which information deleted by unlearning can be restored, both\nelicited via adversarial instructions. The LLM reveals an \\emph{observable\nsignal} $Z$ that potentially leaks hints for attacking through a response\ncontaining answer tokens, thinking process tokens, or logits. Yet the scale of\ninformation leaked remains anecdotal, leaving auditors without principled\nguidance and defenders blind to the transparency--risk trade-off. We fill this\ngap with an information-theoretic framework that computes how much information\ncan be safely disclosed, and enables auditors to gauge how close their methods\ncome to the fundamental limit. Treating the mutual information $I(Z;T)$ between\nthe observation $Z$ and the target property $T$ as the leaked bits per query,\nwe show that achieving error $\\varepsilon$ requires at least\n$\\log(1/\\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak\nrate and only logarithmically with the desired accuracy. Thus, even a modest\nincrease in disclosure collapses the attack cost from quadratic to logarithmic\nin terms of the desired accuracy. Experiments on seven LLMs across\nsystem-prompt leakage, jailbreak, and relearning attacks corroborate the\ntheory: exposing answer tokens alone requires about a thousand queries; adding\nlogits cuts this to about a hundred; and revealing the full thinking process\ntrims it to a few dozen. Our results provide the first principled yardstick for\nbalancing transparency and security when deploying LLMs.",
        "url": "http://arxiv.org/abs/2510.17000v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17000v1",
        "arxiv_id": "2510.17000v1",
        "authors": [
            "Masahiro Kaneko",
            "Timothy Baldwin"
        ],
        "submitted": "2025-10-19 20:51:24",
        "source": "arxiv",
        "comment": "NeurIPS 2025 (spotlight)"
    },
    {
        "title": "Back to Bytes: Revisiting Tokenization Through UTF-8",
        "abstract": "We present UTF8Tokenizer, a minimalist byte-level tokenizer that maps text\nexactly to IDs corresponding to the bytes underlying the text's UTF-8 encoding\n(e.g., byte x09 is token ID 9). Unlike prior byte-level approaches (Xue et al.,\n2021; Pagnoni et al., 2025), our implementation never introduces out-of-range\nIDs (i.e. there is no token ID 256) or auxiliary tokens: all special behavior\n(e.g., padding, boundaries, conversation structure, attention segments, tool\ncalling, \"thinking\" spans, etc.) is encoded using C0 control bytes - just as\nASCII was originally designed to embed control information alongside printable\ntext. These design principles yield practical benefits: (1) faster tokenization\n(14x) and significantly lower host-device transfer (8x less than int64); (2)\nsimple, shareable 256*d embedding tables that can be aligned across models; and\n(3) a training-time enhancement via bit-biased embeddings, which exposes\nper-byte bit structure and can be added to the embedding table post-training,\nremoving inference costs. Our HuggingFace-compatible implementation improves\nlanguage modeling convergence.",
        "url": "http://arxiv.org/abs/2510.16987v1",
        "pdf_url": "http://arxiv.org/pdf/2510.16987v1",
        "arxiv_id": "2510.16987v1",
        "authors": [
            "Amit Moryossef",
            "Clara Meister",
            "Pavel Stepachev",
            "Desmond Elliott"
        ],
        "submitted": "2025-10-19 20:06:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection",
        "abstract": "Bengali social media platforms have witnessed a sharp increase in hate\nspeech, disproportionately affecting women and adolescents. While datasets such\nas BD-SHS provide a basis for structured evaluation, most prior approaches rely\non either computationally costly full-model fine-tuning or proprietary APIs.\nThis paper presents the first application of Parameter-Efficient Fine-Tuning\n(PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three\ninstruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and\nMistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated\ncomments. Each model was adapted by training fewer than 1% of its parameters,\nenabling experiments on a single consumer-grade GPU. The results show that\nLlama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at\n88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical\nand replicable strategy for Bengali and related low-resource languages.",
        "url": "http://arxiv.org/abs/2510.16985v1",
        "pdf_url": "http://arxiv.org/pdf/2510.16985v1",
        "arxiv_id": "2510.16985v1",
        "authors": [
            "Akif Islam",
            "Mohd Ruhul Ameen"
        ],
        "submitted": "2025-10-19 20:03:22",
        "source": "arxiv",
        "comment": "Accepted to IEEE COMPAS 2025. 6 pages, 3 figures, 6 tables"
    },
    {
        "title": "Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures",
        "abstract": "Knowledge Distillation (KD) accelerates training of large language models\n(LLMs) but poses intellectual property protection and LLM diversity risks.\nExisting KD detection methods based on self-identity or output similarity can\nbe easily evaded through prompt engineering. We present a KD detection\nframework effective in both white-box and black-box settings by exploiting an\noverlooked signal: the transfer of MoE \"structural habits\", especially internal\nrouting patterns. Our approach analyzes how different experts specialize and\ncollaborate across various inputs, creating distinctive fingerprints that\npersist through the distillation process. To extend beyond the white-box setup\nand MoE architectures, we further propose Shadow-MoE, a black-box method that\nconstructs proxy MoE representations via auxiliary distillation to compare\nthese patterns between arbitrary model pairs. We establish a comprehensive,\nreproducible benchmark that offers diverse distilled checkpoints and an\nextensible framework to facilitate future research. Extensive experiments\ndemonstrate >94% detection accuracy across various scenarios and strong\nrobustness to prompt-based evasion, outperforming existing baselines while\nhighlighting the structural habits transfer in LLMs.",
        "url": "http://arxiv.org/abs/2510.16968v1",
        "pdf_url": "http://arxiv.org/pdf/2510.16968v1",
        "arxiv_id": "2510.16968v1",
        "authors": [
            "Pingzhi Li",
            "Morris Yu-Chao Huang",
            "Zhen Tan",
            "Qingquan Song",
            "Jie Peng",
            "Kai Zou",
            "Yu Cheng",
            "Kaidi Xu",
            "Tianlong Chen"
        ],
        "submitted": "2025-10-19 19:15:08",
        "source": "arxiv",
        "comment": "Code is at https://github.com/unites-lab/shadow-moe"
    },
    {
        "title": "Real-Time World Crafting: Generating Structured Game Behaviors from Natural Language with Large Language Models",
        "abstract": "We present a novel architecture for safely integrating Large Language Models\n(LLMs) into interactive game engines, allowing players to \"program\" new\nbehaviors using natural language. Our framework mitigates risks by using an LLM\nto translate commands into a constrained Domain-Specific Language (DSL), which\nconfigures a custom Entity-Component-System (ECS) at runtime. We evaluated this\nsystem in a 2D spell-crafting game prototype by experimentally assessing models\nfrom the Gemini, GPT, and Claude families with various prompting strategies. A\nvalidated LLM judge qualitatively rated the outputs, showing that while larger\nmodels better captured creative intent, the optimal prompting strategy is\ntask-dependent: Chain-of-Thought improved creative alignment, while few-shot\nexamples were necessary to generate more complex DSL scripts. This work offers\na validated LLM-ECS pattern for emergent gameplay and a quantitative\nperformance comparison for developers.",
        "url": "http://arxiv.org/abs/2510.16952v1",
        "pdf_url": "http://arxiv.org/pdf/2510.16952v1",
        "arxiv_id": "2510.16952v1",
        "authors": [
            "Austin Drake",
            "Hang Dong"
        ],
        "submitted": "2025-10-19 18:09:44",
        "source": "arxiv",
        "comment": "16 pages, 11 figures (including appendix). To be presented at the 5th\n  Wordplay @ EMNLP workshop (2025)"
    }
]