[
    {
        "title": "Comparative Analysis of Lion and AdamW Optimizers for Cross-Encoder Reranking with MiniLM, GTE, and ModernBERT",
        "abstract": "Modern information retrieval systems often employ a two-stage pipeline: an\nefficient initial retrieval stage followed by a computationally intensive\nreranking stage. Cross-encoders have shown strong effectiveness for reranking\ndue to their deep analysis of query-document pairs. This paper studies the\nimpact of the Lion optimizer, a recent alternative to AdamW, during fine-tuning\nof cross-encoder rerankers. We fine-tune three transformer models-MiniLM, GTE,\nand ModernBERT-on the MS MARCO passage ranking dataset using both optimizers.\nGTE and ModernBERT support extended context lengths (up to 8192 tokens). We\nevaluate effectiveness using TREC 2019 Deep Learning Track and MS MARCO dev set\n(MRR@10). Experiments, run on the Modal cloud platform, reveal that ModernBERT\nwith Lion achieves the best NDCG@10 (0.7225) and MAP (0.5121) on TREC DL 2019,\nwhile MiniLM with Lion ties ModernBERT for MRR@10 (0.5988) on MS MARCO dev.\nLion also provides superior GPU efficiency, improving utilization by 2.67% to\n10.33% across models. We analyze performance trends using standard IR metrics\nand discuss the optimizer's impact on training dynamics across architectures.",
        "url": "http://arxiv.org/abs/2506.18297v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18297v1",
        "arxiv_id": "2506.18297v1",
        "authors": [
            "Shahil Kumar",
            "Manu Pande",
            "Anay Yatin Damle"
        ],
        "submitted": "2025-06-23 05:30:09",
        "source": "arxiv",
        "comment": null,
        "score": 17,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'trec' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to information retrieval, specifically in the context of reranking and fine-tuning transformer models for passage ranking. However, the focus on optimizers and their impact on training dynamics is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Harnessing the Power of Reinforcement Learning for Language-Model-Based Information Retriever via Query-Document Co-Augmentation",
        "abstract": "Recent studies have proposed leveraging Large Language Models (LLMs) as\ninformation retrievers through query rewriting. However, for challenging\ncorpora, we argue that enhancing queries alone is insufficient for robust\nsemantic matching; the LLM should also have sufficient understanding of the\ncorpus by directly handling and augmenting the documents themselves. To this\nend, we present an LLM-based retriever empowered to augment both user queries\nand corpus documents, with its policy fully explored via reinforcement learning\n(RL) and minimal human inductive bias. Notably, we find that simply allowing\nthe LLM to modify documents yields little benefit unless paired with our\ncarefully designed bidirectional RL framework, which enables the LLM to\nsimultaneously learn and collaborate on both query and document augmentation\npolicies. A key technical challenge in realizing such a framework lies in\njointly updating both policies during training, where the rewards for the two\ndirections depend on each other, making their entangled reward intractable. Our\napproach addresses this by introducing a reward sampling strategy and a\nspecifically designed RL algorithm that enables effective training with these\nsampled rewards. Experimental results demonstrate that our approach\nsignificantly enhances LLM-based retrieval performance in both sparse and dense\nsettings, particularly in difficult retrieval domains, and achieves strong\ncross-benchmark generalization. Our code is released at\nhttps://github.com/liujm2001/CoAugRetriever.",
        "url": "http://arxiv.org/abs/2506.18670v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18670v1",
        "arxiv_id": "2506.18670v1",
        "authors": [
            "Jingming Liu",
            "Yumeng Li",
            "Wei Shi",
            "Yao-Xiang Ding",
            "Hui Su",
            "Kun Zhou"
        ],
        "submitted": "2025-06-23 14:14:43",
        "source": "arxiv",
        "comment": null,
        "score": 13,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 9,
        "llm_reason": "This paper is extremely relevant to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The use of reinforcement learning to augment both queries and documents is a novel approach that aligns with your focus on deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "From Web Search towards Agentic Deep Research: Incentivizing Search with Reasoning Agents",
        "abstract": "Information retrieval is a cornerstone of modern knowledge acquisition,\nenabling billions of queries each day across diverse domains. However,\ntraditional keyword-based search engines are increasingly inadequate for\nhandling complex, multi-step information needs. Our position is that Large\nLanguage Models (LLMs), endowed with reasoning and agentic capabilities, are\nushering in a new paradigm termed Agentic Deep Research. These systems\ntranscend conventional information search techniques by tightly integrating\nautonomous reasoning, iterative retrieval, and information synthesis into a\ndynamic feedback loop. We trace the evolution from static web search to\ninteractive, agent-based systems that plan, explore, and learn. We also\nintroduce a test-time scaling law to formalize the impact of computational\ndepth on reasoning and search. Supported by benchmark results and the rise of\nopen-source implementations, we demonstrate that Agentic Deep Research not only\nsignificantly outperforms existing approaches, but is also poised to become the\ndominant paradigm for future information seeking. All the related resources,\nincluding industry products, research papers, benchmark datasets, and\nopen-source implementations, are collected for the community in\nhttps://github.com/DavidZWZ/Awesome-Deep-Research.",
        "url": "http://arxiv.org/abs/2506.18959v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18959v1",
        "arxiv_id": "2506.18959v1",
        "authors": [
            "Weizhi Zhang",
            "Yangning Li",
            "Yuanchen Bei",
            "Junyu Luo",
            "Guancheng Wan",
            "Liangwei Yang",
            "Chenxuan Xie",
            "Yuyao Yang",
            "Wei-Chieh Huang",
            "Chunyu Miao",
            "Henry Peng Zou",
            "Xiao Luo",
            "Yusheng Zhao",
            "Yankai Chen",
            "Chunkit Chan",
            "Peilin Zhou",
            "Xinyang Zhang",
            "Chenwei Zhang",
            "Jingbo Shang",
            "Ming Zhang",
            "Yangqiu Song",
            "Irwin King",
            "Philip S. Yu"
        ],
        "submitted": "2025-06-23 17:27:19",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'web search' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores the concept of Agentic Deep Research, which integrates autonomous reasoning, iterative retrieval, and information synthesis, aligning with the user's interest in Information Retrieval and Search technologies. However, the focus on Large Language Models and reasoning agents is not directly related to the user's specific areas of interest, such as query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Semantic similarity estimation for domain specific data using BERT and other techniques",
        "abstract": "Estimation of semantic similarity is an important research problem both in\nnatural language processing and the natural language understanding, and that\nhas tremendous application on various downstream tasks such as question\nanswering, semantic search, information retrieval, document clustering,\nword-sense disambiguation and machine translation. In this work, we carry out\nthe estimation of semantic similarity using different state-of-the-art\ntechniques including the USE (Universal Sentence Encoder), InferSent and the\nmost recent BERT, or Bidirectional Encoder Representations from Transformers,\nmodels. We use two question pairs datasets for the analysis, one is a domain\nspecific in-house dataset and the other is a public dataset which is the\nQuora's question pairs dataset. We observe that the BERT model gave much\nsuperior performance as compared to the other methods. This should be because\nof the fine-tuning procedure that is involved in its training process, allowing\nit to learn patterns based on the training data that is used. This works\ndemonstrates the applicability of BERT on domain specific datasets. We infer\nfrom the analysis that BERT is the best technique to use in the case of domain\nspecific data.",
        "url": "http://arxiv.org/abs/2506.18602v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18602v1",
        "arxiv_id": "2506.18602v1",
        "authors": [
            "R. Prashanth"
        ],
        "submitted": "2025-06-23 13:03:59",
        "source": "arxiv",
        "comment": "This is a preprint version of an article accepted for publication in\n  the proceedings of Machine Learning and Data Mining 2019",
        "score": 9,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'semantic search' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "The paper explores semantic similarity estimation using BERT, which is relevant to information retrieval and natural language processing, two of your core research interests. The application of BERT to domain-specific data and its superior performance compared to other methods align with your focus on query understanding and ranking models. However, the paper's primary focus is on semantic similarity estimation rather than query understanding or ranking models, which is why it doesn't score a 10."
    },
    {
        "title": "Bias vs Bias -- Dawn of Justice: A Fair Fight in Recommendation Systems",
        "abstract": "Recommendation systems play a crucial role in our daily lives by impacting\nuser experience across various domains, including e-commerce, job\nadvertisements, entertainment, etc. Given the vital role of such systems in our\nlives, practitioners must ensure they do not produce unfair and imbalanced\nrecommendations. Previous work addressing bias in recommendations overlooked\nbias in certain item categories, potentially leaving some biases unaddressed.\nAdditionally, most previous work on fair re-ranking focused on binary-sensitive\nattributes. In this paper, we address these issues by proposing a\nfairness-aware re-ranking approach that helps mitigate bias in different\ncategories of items. This re-ranking approach leverages existing biases to\ncorrect disparities in recommendations across various demographic groups. We\nshow how our approach can mitigate bias on multiple sensitive attributes,\nincluding gender, age, and occupation. We experimented on three real-world\ndatasets to evaluate the effectiveness of our re-ranking scheme in mitigating\nbias in recommendations. Our results show how this approach helps mitigate\nsocial bias with little to no degradation in performance.",
        "url": "http://arxiv.org/abs/2506.18327v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18327v1",
        "arxiv_id": "2506.18327v1",
        "authors": [
            "Tahsin Alamgir Kheya",
            "Mohamed Reda Bouadjenek",
            "Sunil Aryal"
        ],
        "submitted": "2025-06-23 06:19:02",
        "source": "arxiv",
        "comment": null,
        "score": 9,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on fairness in recommendation systems, which is a related topic to information retrieval. However, the emphasis on recommender systems and the lack of direct connection to query understanding, ranking models, or user behavior modeling make it only loosely relevant to my research interests."
    },
    {
        "title": "LettinGo: Explore User Profile Generation for Recommendation System",
        "abstract": "User profiling is pivotal for recommendation systems, as it transforms raw\nuser interaction data into concise and structured representations that drive\npersonalized recommendations. While traditional embedding-based profiles lack\ninterpretability and adaptability, recent advances with large language models\n(LLMs) enable text-based profiles that are semantically richer and more\ntransparent. However, existing methods often adhere to fixed formats that limit\ntheir ability to capture the full diversity of user behaviors. In this paper,\nwe introduce LettinGo, a novel framework for generating diverse and adaptive\nuser profiles. By leveraging the expressive power of LLMs and incorporating\ndirect feedback from downstream recommendation tasks, our approach avoids the\nrigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ\nDirect Preference Optimization (DPO) to align the profile generator with\ntask-specific performance, ensuring that the profiles remain adaptive and\neffective. LettinGo operates in three stages: (1) exploring diverse user\nprofiles via multiple LLMs, (2) evaluating profile quality based on their\nimpact in recommendation systems, and (3) aligning the profile generation\nthrough pairwise preference data derived from task performance. Experimental\nresults demonstrate that our framework significantly enhances recommendation\naccuracy, flexibility, and contextual awareness. This work enhances profile\ngeneration as a key innovation for next-generation recommendation systems.",
        "url": "http://arxiv.org/abs/2506.18309v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18309v1",
        "arxiv_id": "2506.18309v1",
        "authors": [
            "Lu Wang",
            "Di Zhang",
            "Fangkai Yang",
            "Pu Zhao",
            "Jianfeng Liu",
            "Yuefeng Zhan",
            "Hao Sun",
            "Qingwei Lin",
            "Weiwei Deng",
            "Dongmei Zhang",
            "Feng Sun",
            "Qi Zhang"
        ],
        "submitted": "2025-06-23 05:51:52",
        "source": "arxiv",
        "comment": "11 pages, 3 figures",
        "score": 8,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'user behavior' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on user profiling for recommendation systems, which is somewhat related to my interests in information retrieval and search technologies. However, the paper's emphasis on large language models and recommendation systems is not directly aligned with my primary focus on query understanding, ranking models, and user behavior modeling in the context of information retrieval."
    },
    {
        "title": "Shrinking the Generation-Verification Gap with Weak Verifiers",
        "abstract": "Verifiers can improve language model capabilities by scoring and ranking\nresponses from generated candidates. Currently, high-quality verifiers are\neither unscalable (e.g., humans) or limited in utility (e.g., tools like Lean).\nWhile LM judges and reward models have become broadly useful as general-purpose\nverifiers, a significant performance gap remains between them and oracle\nverifiers (verifiers with perfect accuracy). To help close this gap, we\nintroduce Weaver, a framework for designing a strong verifier by combining\nmultiple weak, imperfect verifiers. We find weighted ensembles of verifiers,\nwhich typically require learning from labeled data, significantly outperform\nunweighted combinations due to differences in verifier accuracies. To reduce\ndependency on labeled data, Weaver leverages weak supervision to estimate each\nverifier's accuracy and combines outputs into a unified score that better\nreflects true response quality. However, directly applying weak supervision\nalgorithms poses challenges, including inconsistent verifier output formats and\nhandling low-quality verifiers. Weaver addresses these using dataset statistics\nto normalize outputs and filter specific verifiers. We study Weaver's\neffectiveness in test-time repeated sampling, where a model generates multiple\ncandidate responses and selects one. Our evaluations show Weaver significantly\nimproves over Pass@1-performance when selecting the first candidate-across\nreasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B\nInstruct as generator, and an ensemble of 70B or smaller judge and reward\nmodels as verifiers (87.7% average). This gain mirrors the jump between GPT-4o\nand o3-mini (69.0% vs. 86.7%), which required extensive finetuning and\npost-training. To reduce computational costs of verifier ensembles, we train a\n400M cross-encoder using Weaver's combined output scores.",
        "url": "http://arxiv.org/abs/2506.18203v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18203v1",
        "arxiv_id": "2506.18203v1",
        "authors": [
            "Jon Saad-Falcon",
            "E. Kelly Buchanan",
            "Mayee F. Chen",
            "Tzu-Heng Huang",
            "Brendan McLaughlin",
            "Tanvir Bhathal",
            "Shang Zhu",
            "Ben Athiwaratkun",
            "Frederic Sala",
            "Scott Linderman",
            "Azalia Mirhoseini",
            "Christopher Ré"
        ],
        "submitted": "2025-06-22 23:38:15",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to my research interests in Information Retrieval and Search technologies, as it discusses the design of a framework for combining multiple weak verifiers to improve language model capabilities. However, the focus on language models and verifier design is not directly aligned with my primary interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval",
        "abstract": "We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding\nmodel that unifies text and image representations through a novel architecture\nsupporting both single-vector and multi-vector embeddings in the late\ninteraction style. The model incorporates task-specific Low-Rank Adaptation\n(LoRA) adapters to optimize performance across diverse retrieval scenarios,\nincluding query-document retrieval, semantic text similarity, and code search.\nComprehensive evaluations demonstrate that jina-embeddings-v4 achieves\nstate-of-the-art performance on both single-modal and cross-modal retrieval\ntasks, with particular strength in processing visually rich content such as\ntables, charts, diagrams, and mixed-media formats. To facilitate evaluation of\nthis capability, we also introduce Jina-VDR, a novel benchmark specifically\ndesigned for visually rich image retrieval.",
        "url": "http://arxiv.org/abs/2506.18902v2",
        "pdf_url": "http://arxiv.org/pdf/2506.18902v2",
        "arxiv_id": "2506.18902v2",
        "authors": [
            "Michael Günther",
            "Saba Sturua",
            "Mohammad Kalim Akram",
            "Isabelle Mohr",
            "Andrei Ungureanu",
            "Bo Wang",
            "Sedigheh Eslami",
            "Scott Martens",
            "Maximilian Werk",
            "Nan Wang",
            "Han Xiao"
        ],
        "submitted": "2025-06-23 17:59:55",
        "source": "arxiv",
        "comment": "22 pages, 1-10 main, 14-22 experimental results, benchmark tables",
        "score": 7,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper presents a multimodal embedding model for retrieval tasks, which is somewhat related to my interests in Information Retrieval and Search technologies. The focus on query-document retrieval and semantic text similarity is relevant, but the emphasis on visual content and multimodal representations is not directly aligned with my primary interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "An Audio-centric Multi-task Learning Framework for Streaming Ads Targeting on Spotify",
        "abstract": "Spotify, a large-scale multimedia platform, attracts over 675 million monthly\nactive users who collectively consume millions of hours of music, podcasts,\naudiobooks, and video content. This diverse content consumption pattern\nintroduces unique challenges for computational advertising, which must\neffectively integrate a variety of ad modalities, including audio, video, and\ndisplay, within a single user experience. Traditional ad recommendation models,\nprimarily designed for foregrounded experiences, often struggle to reconcile\nthe platform's inherent audio-centrality with the demands of optimizing ad\nperformance across multiple formats and modalities. To overcome these\nchallenges, we introduce Cross-modal Adaptive Mixture-of-Experts (CAMoE), a\nnovel framework for optimizing click-through rate (CTR) prediction in both\naudio-centric and multi-modal settings. CAMoE enhances traditional\nmixture-of-experts models by incorporating modality-aware task grouping,\nadaptive loss masking, and deep-cross networks (DCN) to capture complex feature\ninteractions within a multi-modal ad ecosystem. Through extensive ablation\nstudies, we demonstrate that this approach achieves near Pareto-optimal\nperformance across audio, video, and display ad formats, significantly\nimproving AUC-PR compared to conventional single-task and content-based\nmulti-task learning baselines. When deployed at scale on Spotify's ad serving\nplatform, CAMoE delivered substantial gains, yielding a 14.5% increase in CTR\nfor audio ads, a 1.3% increase for video ads, and a 4.8% reduction in expected\ncost-per-click (eCPC) for audio slots.",
        "url": "http://arxiv.org/abs/2506.18735v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18735v1",
        "arxiv_id": "2506.18735v1",
        "authors": [
            "Shivam Verma",
            "Vivian Chen",
            "Darren Mei"
        ],
        "submitted": "2025-06-23 15:11:43",
        "source": "arxiv",
        "comment": "Accepted at KDD 2025",
        "score": 7,
        "keyword_reasons": [
            "Found 'click' (score: +2)",
            "Found 'ctr' (score: +2)",
            "Found 'click-through rate' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on audio-centric ad targeting on Spotify, which is not directly related to information retrieval, search technologies, or query understanding. While it involves multi-task learning and deep-cross networks, the context is not relevant to the user's primary research interests in IR and NLP."
    },
    {
        "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning",
        "abstract": "Ultra-long generation by large language models (LLMs) is a widely demanded\nscenario, yet it remains a significant challenge due to their maximum\ngeneration length limit and overall quality degradation as sequence length\nincreases. Previous approaches, exemplified by LongWriter, typically rely on\n''teaching'', which involves supervised fine-tuning (SFT) on synthetic\nlong-form outputs. However, this strategy heavily depends on synthetic SFT\ndata, which is difficult and costly to construct, often lacks coherence and\nconsistency, and tends to be overly artificial and structurally monotonous. In\nthis work, we propose an incentivization-based approach that, starting entirely\nfrom scratch and without relying on any annotated or synthetic data, leverages\nreinforcement learning (RL) to foster the emergence of ultra-long, high-quality\ntext generation capabilities in LLMs. We perform RL training starting from a\nbase model, similar to R1-Zero, guiding it to engage in reasoning that\nfacilitates planning and refinement during the writing process. To support\nthis, we employ specialized reward models that steer the LLM towards improved\nlength control, writing quality, and structural formatting. Experimental\nevaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,\nconsistently outperforms traditional SFT methods on long-form writing tasks,\nachieving state-of-the-art results across all metrics on WritingBench and\nArena-Write, and even surpassing 100B+ models such as DeepSeek R1 and\nQwen3-235B. We open-source our data and model checkpoints under\nhttps://huggingface.co/THU-KEG/LongWriter-Zero-32B",
        "url": "http://arxiv.org/abs/2506.18841v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18841v1",
        "arxiv_id": "2506.18841v1",
        "authors": [
            "Yuhao Wu",
            "Yushi Bai",
            "Zhiqiang Hu",
            "Roy Ka-Wei Lee",
            "Juanzi Li"
        ],
        "submitted": "2025-06-23 16:59:02",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'ltr' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on ultra-long text generation using reinforcement learning, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the topic is more specific to text generation and lacks relevance to the user's primary research interests."
    },
    {
        "title": "ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation",
        "abstract": "Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and\nOpenAI o1 series have achieved notable performance enhancements on complex\nreasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).\nHowever, an emerging issue is their inclination to produce excessively verbose\nreasoning processes, leading to the inefficiency problem. Existing literature\non improving efficiency mainly adheres to the before-reasoning paradigms such\nas prompting and reasoning or fine-tuning and reasoning, but ignores the\npromising direction of directly encouraging the model to speak concisely by\nintervening during the generation of reasoning. In order to fill the blank, we\npropose a framework dubbed ConciseHint, which continuously encourages the\nreasoning model to speak concisely by injecting the textual hint (manually\ndesigned or trained on the concise data) during the token generation of the\nreasoning process. Besides, ConciseHint is adaptive to the complexity of the\nquery by adaptively adjusting the hint intensity, which ensures it will not\nundermine model performance. Experiments on the state-of-the-art LRMs,\nincluding DeepSeek-R1 and Qwen-3 series, demonstrate that our method can\neffectively produce concise reasoning processes while maintaining performance\nwell. For instance, we achieve a reduction ratio of 65\\% for the reasoning\nlength on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.",
        "url": "http://arxiv.org/abs/2506.18810v2",
        "pdf_url": "http://arxiv.org/pdf/2506.18810v2",
        "arxiv_id": "2506.18810v2",
        "authors": [
            "Siao Tang",
            "Xinyin Ma",
            "Gongfan Fang",
            "Xinchao Wang"
        ],
        "submitted": "2025-06-23 16:20:44",
        "source": "arxiv",
        "comment": "Codes are available at https://github.com/tsa18/ConciseHint",
        "score": 5,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on improving the efficiency of reasoning models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions the use of large reasoning models, the paper's primary concern is not on ranking models or user behavior modeling, making it only loosely relevant to the user's research interests."
    },
    {
        "title": "Rethinking Click Models in Light of Carousel Interfaces: Theory-Based Categorization and Design of Click Models",
        "abstract": "Click models are a well-established for modeling user interactions with web\ninterfaces. Previous work has mainly focused on traditional single-list web\nsearch settings; this includes existing surveys that introduced categorizations\nbased on the first generation of probabilistic graphical model (PGM) click\nmodels that have become standard. However, these categorizations have become\noutdated, as their conceptualizations are unable to meaningfully compare PGM\nwith neural network (NN) click models nor generalize to newer interfaces, such\nas carousel interfaces. We argue that this outdated view fails to adequately\nexplain the fundamentals of click model designs, thus hindering the development\nof novel click models.\n  This work reconsiders what should be the fundamental concepts in click model\ndesign, grounding them - unlike previous approaches - in their mathematical\nproperties. We propose three fundamental key-design choices that explain what\nstatistical patterns a click model can capture, and thus indirectly, what user\nbehaviors they can capture. Based on these choices, we create a novel click\nmodel taxonomy that allows a meaningful comparison of all existing click\nmodels; this is the first taxonomy of single-list, grid and carousel click\nmodels that includes PGMs and NNs. Finally, we show how our conceptualization\nprovides a foundation for future click model design by an example derivation of\na novel design for carousel interfaces.",
        "url": "http://arxiv.org/abs/2506.18548v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18548v1",
        "arxiv_id": "2506.18548v1",
        "authors": [
            "Jingwei Kang",
            "Maarten de Rijke",
            "Santiago de Leon-Martinez",
            "Harrie Oosterhuis"
        ],
        "submitted": "2025-06-23 11:57:11",
        "source": "arxiv",
        "comment": "Accepted by ICTIR 2025",
        "score": 5,
        "keyword_reasons": [
            "Found 'click model' (score: +2)",
            "Found 'click' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, specifically in query understanding and user behavior modeling. The focus on click models and carousel interfaces aligns with your background in e-commerce and interest in real-time relevance optimization. The paper's emphasis on mathematical properties and design choices also resonates with your interest in ranking models and deep semantic understanding."
    },
    {
        "title": "Enhancing Document Retrieval in COVID-19 Research: Leveraging Large Language Models for Hidden Relation Extraction",
        "abstract": "In recent years, with the appearance of the COVID-19 pandemic, numerous\npublications relevant to this disease have been issued. Because of the massive\nvolume of publications, an efficient retrieval system is necessary to provide\nresearchers with useful information if an unexpected pandemic happens so\nsuddenly, like COVID-19. In this work, we present a method to help the\nretrieval system, the Covrelex-SE system, to provide more high-quality search\nresults. We exploited the power of the large language models (LLMs) to extract\nthe hidden relationships inside the unlabeled publication that cannot be found\nby the current parsing tools that the system is using. Since then, help the\nsystem to have more useful information during retrieval progress.",
        "url": "http://arxiv.org/abs/2506.18311v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18311v1",
        "arxiv_id": "2506.18311v1",
        "authors": [
            "Hoang-An Trieu",
            "Dinh-Truong Do",
            "Chau Nguyen",
            "Vu Tran",
            "Minh Le Nguyen"
        ],
        "submitted": "2025-06-23 05:55:53",
        "source": "arxiv",
        "comment": "In the Proceedings of SCIDOCA 2024",
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to information retrieval, as it discusses a retrieval system for COVID-19 research publications. However, the focus is on leveraging large language models for hidden relation extraction, which is more relevant to NLP and data mining. The paper does not specifically address query understanding, ranking models, or user behavior modeling, which are core interests in the user's research."
    },
    {
        "title": "AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores in LLMs",
        "abstract": "In real-world applications, Large Language Models (LLMs) often hallucinate,\neven in Retrieval-Augmented Generation (RAG) settings, which poses a\nsignificant challenge to their deployment. In this paper, we introduce\nAggTruth, a method for online detection of contextual hallucinations by\nanalyzing the distribution of internal attention scores in the provided context\n(passage). Specifically, we propose four different variants of the method, each\nvarying in the aggregation technique used to calculate attention scores. Across\nall LLMs examined, AggTruth demonstrated stable performance in both same-task\nand cross-task setups, outperforming the current SOTA in multiple scenarios.\nFurthermore, we conducted an in-depth analysis of feature selection techniques\nand examined how the number of selected attention heads impacts detection\nperformance, demonstrating that careful selection of heads is essential to\nachieve optimal results.",
        "url": "http://arxiv.org/abs/2506.18628v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18628v1",
        "arxiv_id": "2506.18628v1",
        "authors": [
            "Piotr Matys",
            "Jan Eliasz",
            "Konrad Kiełczyński",
            "Mikołaj Langner",
            "Teddy Ferdinan",
            "Jan Kocoń",
            "Przemysław Kazienko"
        ],
        "submitted": "2025-06-23 13:35:05",
        "source": "arxiv",
        "comment": "ICCS 2025 Workshops",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on contextual hallucination detection in Large Language Models, which is a specific problem in NLP. While it's related to information retrieval and search technologies, the primary focus is on language models and generation, rather than query understanding, ranking models, or user behavior modeling. The paper's relevance to the user's interests is somewhat limited."
    },
    {
        "title": "When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking",
        "abstract": "This paper investigates the counterintuitive phenomenon where fine-tuning\npre-trained transformer models degrades performance on the MS MARCO passage\nranking task. Through comprehensive experiments involving five model\nvariants-including full parameter fine-tuning and parameter efficient LoRA\nadaptations-we demonstrate that all fine-tuning approaches underperform the\nbase sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our\nanalysis reveals that fine-tuning disrupts the optimal embedding space\nstructure learned during the base model's extensive pre-training on 1 billion\nsentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations\nshow progressive embedding space flattening, while training dynamics analysis\nand computational efficiency metrics further support our findings. These\nresults challenge conventional wisdom about transfer learning effectiveness on\nsaturated benchmarks and suggest architectural innovations may be necessary for\nmeaningful improvements.",
        "url": "http://arxiv.org/abs/2506.18535v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18535v1",
        "arxiv_id": "2506.18535v1",
        "authors": [
            "Manu Pande",
            "Shahil Kumar",
            "Anay Yatin Damle"
        ],
        "submitted": "2025-06-23 11:46:05",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the phenomenon of fine-tuning pre-trained transformer models degrading performance on the MS MARCO passage ranking task, which is related to query understanding and ranking models. However, the focus is on the technical aspects of fine-tuning and the impact on the embedding space, rather than the user behavior modeling or real-time relevance optimization aspects that are central to your research interests."
    },
    {
        "title": "LLMs on a Budget? Say HOLA",
        "abstract": "Running Large Language Models (LLMs) on edge devices is constrained by high\ncompute and memory demands posing a barrier for real-time applications in\nsectors like healthcare, education, and embedded systems. Current solutions\nsuch as quantization, pruning, and retrieval-augmented generation (RAG) offer\nonly partial optimizations and often compromise on speed or accuracy. We\nintroduce HOLA, an end-to-end optimization framework for efficient LLM\ndeployment. Internally, it leverages Hierarchical Speculative Decoding (HSD)\nfor faster inference without quality loss. Externally, AdaComp-RAG adjusts\nretrieval complexity based on context needs. Together with LoBi, which blends\nstructured pruning (LoRA) and quantization, HOLA delivers significant gains:\n17.6% EMA on GSM8K, 10.5% MCA on ARC, and reduced latency and memory on edge\ndevices like Jetson Nano--proving both scalable and production-ready.",
        "url": "http://arxiv.org/abs/2506.18952v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18952v1",
        "arxiv_id": "2506.18952v1",
        "authors": [
            "Zohaib Hasan Siddiqui",
            "Jiechao Gao",
            "Ebad Shabbir",
            "Mohammad Anas Azeez",
            "Rafiq Ali",
            "Gautam Siddharth Kashyap",
            "Usman Naseem"
        ],
        "submitted": "2025-06-23 10:20:47",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on optimizing Large Language Models for deployment on edge devices, using techniques like quantization, pruning, and retrieval-augmented generation. While it's related to NLP, it doesn't directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval and Search technologies."
    },
    {
        "title": "Team LA at SCIDOCA shared task 2025: Citation Discovery via relation-based zero-shot retrieval",
        "abstract": "The Citation Discovery Shared Task focuses on predicting the correct citation\nfrom a given candidate pool for a given paragraph. The main challenges stem\nfrom the length of the abstract paragraphs and the high similarity among\ncandidate abstracts, making it difficult to determine the exact paper to cite.\nTo address this, we develop a system that first retrieves the top-k most\nsimilar abstracts based on extracted relational features from the given\nparagraph. From this subset, we leverage a Large Language Model (LLM) to\naccurately identify the most relevant citation. We evaluate our framework on\nthe training dataset provided by the SCIDOCA 2025 organizers, demonstrating its\neffectiveness in citation prediction.",
        "url": "http://arxiv.org/abs/2506.18316v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18316v1",
        "arxiv_id": "2506.18316v1",
        "authors": [
            "Trieu An",
            "Long Nguyen",
            "Minh Le Nguyen"
        ],
        "submitted": "2025-06-23 06:01:21",
        "source": "arxiv",
        "comment": "In the Proceedings of SCIDOCA 2025",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on citation discovery, which is a specific application of information retrieval. While it involves relation-based zero-shot retrieval, the primary focus is on citation prediction rather than query understanding, ranking models, or user behavior modeling. The paper's relevance to the user's interests is somewhat limited, but it does touch on some IR concepts."
    },
    {
        "title": "MLLP-VRAIN UPV system for the IWSLT 2025 Simultaneous Speech Translation Translation task",
        "abstract": "This work describes the participation of the MLLP-VRAIN research group in the\nshared task of the IWSLT 2025 Simultaneous Speech Translation track. Our\nsubmission addresses the unique challenges of real-time translation of\nlong-form speech by developing a modular cascade system that adapts strong\npre-trained models to streaming scenarios. We combine Whisper Large-V3-Turbo\nfor ASR with the multilingual NLLB-3.3B model for MT, implementing lightweight\nadaptation techniques rather than training new end-to-end models from scratch.\nOur approach employs document-level adaptation with prefix training to enhance\nthe MT model's ability to handle incomplete inputs, while incorporating\nadaptive emission policies including a wait-$k$ strategy and RALCP for managing\nthe translation stream. Specialized buffer management techniques and\nsegmentation strategies ensure coherent translations across long audio\nsequences. Experimental results on the ACL60/60 dataset demonstrate that our\nsystem achieves a favorable balance between translation quality and latency,\nwith a BLEU score of 31.96 and non-computational-aware StreamLAAL latency of\n2.94 seconds. Our final model achieves a preliminary score on the official test\nset (IWSLT25Instruct) of 29.8 BLEU. Our work demonstrates that carefully\nadapted pre-trained components can create effective simultaneous translation\nsystems for long-form content without requiring extensive in-domain parallel\ndata or specialized end-to-end training.",
        "url": "http://arxiv.org/abs/2506.18828v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18828v1",
        "arxiv_id": "2506.18828v1",
        "authors": [
            "Jorge Iranzo-Sánchez",
            "Javier Iranzo-Sánchez",
            "Adrià Giménez",
            "Jorge Civera",
            "Alfons Juan"
        ],
        "submitted": "2025-06-23 16:44:01",
        "source": "arxiv",
        "comment": "IWSLT 2025 System Description",
        "score": 3,
        "keyword_reasons": [
            "Found 'search' (score: +1)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on simultaneous speech translation, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing and machine translation, the context and methodology are not aligned with the user's primary research interests."
    },
    {
        "title": "Airalogy: AI-empowered universal data digitization for research automation",
        "abstract": "Research data are the foundation of Artificial Intelligence (AI)-driven\nscience, yet current AI applications remain limited to a few fields with\nreadily available, well-structured, digitized datasets. Achieving comprehensive\nAI empowerment across multiple disciplines is still out of reach. Present-day\nresearch data collection is often fragmented, lacking unified standards,\ninefficiently managed, and difficult to share. Creating a single platform for\nstandardized data digitization needs to overcome the inherent challenge of\nbalancing between universality (supporting the diverse, ever-evolving needs of\nvarious disciplines) and standardization (enforcing consistent formats to fully\nenable AI). No existing platform accommodates both facets. Building a truly\nmultidisciplinary platform requires integrating scientific domain knowledge\nwith sophisticated computing skills. Researchers often lack the computational\nexpertise to design customized and standardized data recording methods, whereas\nplatform developers rarely grasp the intricate needs of multiple scientific\ndomains. These gaps impede research data standardization and hamper AI-driven\nprogress. In this study, we address these challenges by developing Airalogy\n(https://airalogy.com), the world's first AI- and community-driven platform\nthat balances universality and standardization for digitizing research data\nacross multiple disciplines. Airalogy represents entire research workflows\nusing customizable, standardized data records and offers an advanced AI\nresearch copilot for intelligent Q&A, automated data entry, analysis, and\nresearch automation. Already deployed in laboratories across all four schools\nof Westlake University, Airalogy has the potential to accelerate and automate\nscientific innovation in universities, industry, and the global research\ncommunity-ultimately benefiting humanity as a whole.",
        "url": "http://arxiv.org/abs/2506.18586v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18586v1",
        "arxiv_id": "2506.18586v1",
        "authors": [
            "Zijie Yang",
            "Qiji Zhou",
            "Fang Guo",
            "Sijie Zhang",
            "Yexun Xi",
            "Jinglei Nie",
            "Yudian Zhu",
            "Liping Huang",
            "Chou Wu",
            "Yonghe Xia",
            "Xiaoyu Ma",
            "Yingming Pu",
            "Panzhong Lu",
            "Junshu Pan",
            "Mingtao Chen",
            "Tiannan Guo",
            "Yanmei Dou",
            "Hongyu Chen",
            "Anping Zeng",
            "Jiaxing Huang",
            "Tian Xu",
            "Yue Zhang"
        ],
        "submitted": "2025-06-23 12:43:16",
        "source": "arxiv",
        "comment": "146 pages, 6 figures, 49 supplementary figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on developing a platform for standardized data digitization across multiple disciplines, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it mentions AI and research automation, the context is more focused on data collection and management rather than search or retrieval."
    },
    {
        "title": "PERSCEN: Learning Personalized Interaction Pattern and Scenario Preference for Multi-Scenario Matching",
        "abstract": "With the expansion of business scales and scopes on online platforms,\nmulti-scenario matching has become a mainstream solution to reduce maintenance\ncosts and alleviate data sparsity. The key to effective multi-scenario\nrecommendation lies in capturing both user preferences shared across all\nscenarios and scenario-aware preferences specific to each scenario. However,\nexisting methods often overlook user-specific modeling, limiting the generation\nof personalized user representations. To address this, we propose PERSCEN, an\ninnovative approach that incorporates user-specific modeling into\nmulti-scenario matching. PERSCEN constructs a user-specific feature graph based\non user characteristics and employs a lightweight graph neural network to\ncapture higher-order interaction patterns, enabling personalized extraction of\npreferences shared across scenarios. Additionally, we leverage vector\nquantization techniques to distil scenario-aware preferences from users'\nbehavior sequence within individual scenarios, facilitating user-specific and\nscenario-aware preference modeling. To enhance efficient and flexible\ninformation transfer, we introduce a progressive scenario-aware gated linear\nunit that allows fine-grained, low-latency fusion. Extensive experiments\ndemonstrate that PERSCEN outperforms existing methods. Further efficiency\nanalysis confirms that PERSCEN effectively balances performance with\ncomputational cost, ensuring its practicality for real-world industrial\nsystems.",
        "url": "http://arxiv.org/abs/2506.18382v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18382v1",
        "arxiv_id": "2506.18382v1",
        "authors": [
            "Haotong Du",
            "Yaqing Wang",
            "Fei Xiong",
            "Lei Shao",
            "Ming Liu",
            "Hao Gu",
            "Quanming Yao",
            "Zhen Wang"
        ],
        "submitted": "2025-06-23 08:15:16",
        "source": "arxiv",
        "comment": "Accepted by KDD 2025",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on multi-scenario matching and recommendation, which is not directly related to information retrieval, query understanding, or ranking models. While it involves user behavior modeling, the approach is more geared towards recommender systems and does not seem to require deep semantic understanding or real-time relevance optimization, which are key aspects of my research interests."
    },
    {
        "title": "Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives",
        "abstract": "Large Language Models (LLMs) in mental healthcare risk propagating biases\nthat reinforce stigma and harm marginalized groups. While previous research\nidentified concerning trends, systematic methods for detecting intersectional\nbiases remain limited. This work introduces a multi-hop question answering\n(MHQA) framework to explore LLM response biases in mental health discourse. We\nanalyze content from the Interpretable Mental Health Instruction (IMHI) dataset\nacross symptom presentation, coping mechanisms, and treatment approaches. Using\nsystematic tagging across age, race, gender, and socioeconomic status, we\ninvestigate bias patterns at demographic intersections. We evaluate four LLMs:\nClaude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic\ndisparities across sentiment, demographics, and mental health conditions. Our\nMHQA approach demonstrates superior detection compared to conventional methods,\nidentifying amplification points where biases magnify through sequential\nreasoning. We implement two debiasing techniques: Roleplay Simulation and\nExplicit Bias Reduction, achieving 66-94% bias reductions through few-shot\nprompting with BBQ dataset examples. These findings highlight critical areas\nwhere LLMs reproduce mental healthcare biases, providing actionable insights\nfor equitable AI development.",
        "url": "http://arxiv.org/abs/2506.18116v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18116v1",
        "arxiv_id": "2506.18116v1",
        "authors": [
            "Batool Haider",
            "Atmika Gorti",
            "Aman Chadha",
            "Manas Gaur"
        ],
        "submitted": "2025-06-22 18:00:16",
        "source": "arxiv",
        "comment": "19 Pages, 7 Figures, 4 Tables (Note: Under Review)",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on the biases in Large Language Models (LLMs) in mental healthcare, which is outside the scope of Information Retrieval and Search technologies. Although it mentions question answering, the context is not related to query understanding, ranking models, or user behavior modeling, which are key areas of interest in the user's research."
    },
    {
        "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs",
        "abstract": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting of trajectory-response outputs generated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnovel trajectory-aware PRM explicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level and trajectory-level supervision, enabling fine-grained reward\nassignment aligned with structured chain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-quality model distillation data for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards for policy optimization during reinforcement learning,\nand (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond\ndemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\naverage gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement\nlearning, and 6.3% in test-time scaling. We also release our efficient\nReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.\nProjects: https://github.com/Gen-Verse/ReasonFlux",
        "url": "http://arxiv.org/abs/2506.18896v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18896v1",
        "arxiv_id": "2506.18896v1",
        "authors": [
            "Jiaru Zou",
            "Ling Yang",
            "Jingwen Gu",
            "Jiahao Qiu",
            "Ke Shen",
            "Jingrui He",
            "Mengdi Wang"
        ],
        "submitted": "2025-06-23 17:59:02",
        "source": "arxiv",
        "comment": "Codes and Models: https://github.com/Gen-Verse/ReasonFlux",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Process Reward Models (PRMs) for evaluating intermediate reasoning steps in large language models, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on topics like reward supervision and policy optimization, the paper's primary focus is on natural language processing and reinforcement learning, making it only loosely relevant to the user's research interests."
    },
    {
        "title": "Neural Total Variation Distance Estimators for Changepoint Detection in News Data",
        "abstract": "Detecting when public discourse shifts in response to major events is crucial\nfor understanding societal dynamics. Real-world data is high-dimensional,\nsparse, and noisy, making changepoint detection in this domain a challenging\nendeavor. In this paper, we leverage neural networks for changepoint detection\nin news data, introducing a method based on the so-called learning-by-confusion\nscheme, which was originally developed for detecting phase transitions in\nphysical systems. We train classifiers to distinguish between articles from\ndifferent time periods. The resulting classification accuracy is used to\nestimate the total variation distance between underlying content distributions,\nwhere significant distances highlight changepoints. We demonstrate the\neffectiveness of this method on both synthetic datasets and real-world data\nfrom The Guardian newspaper, successfully identifying major historical events\nincluding 9/11, the COVID-19 pandemic, and presidential elections. Our approach\nrequires minimal domain knowledge, can autonomously discover significant shifts\nin public discourse, and yields a quantitative measure of change in content,\nmaking it valuable for journalism, policy analysis, and crisis monitoring.",
        "url": "http://arxiv.org/abs/2506.18764v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18764v1",
        "arxiv_id": "2506.18764v1",
        "authors": [
            "Csaba Zsolnai",
            "Niels Lörch",
            "Julian Arnold"
        ],
        "submitted": "2025-06-23 15:33:30",
        "source": "arxiv",
        "comment": "16 pages, 3 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on changepoint detection in news data using neural networks, which is not directly related to information retrieval, search technologies, or query understanding. While it involves text data, the primary goal is not to improve search or retrieval, but rather to detect shifts in public discourse, which is outside the scope of the user's research interests."
    },
    {
        "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
        "abstract": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages.",
        "url": "http://arxiv.org/abs/2506.18631v2",
        "pdf_url": "http://arxiv.org/pdf/2506.18631v2",
        "arxiv_id": "2506.18631v2",
        "authors": [
            "Chenxing Wei",
            "Jiarui Yu",
            "Ying Tiffany He",
            "Hande Dong",
            "Yao Shu",
            "Fei Yu"
        ],
        "submitted": "2025-06-23 13:36:24",
        "source": "arxiv",
        "comment": "10 pages, 15 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on optimizing Large Language Model (LLM) policies using a reward dithering method, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on optimization and gradient updates, the context is not relevant to the user's primary research interests."
    },
    {
        "title": "No Training Wheels: Steering Vectors for Bias Correction at Inference Time",
        "abstract": "Neural network classifiers trained on datasets with uneven group\nrepresentation often inherit class biases and learn spurious correlations.\nThese models may perform well on average but consistently fail on atypical\ngroups. For example, in hair color classification, datasets may over-represent\nfemales with blond hair, reinforcing stereotypes. Although various algorithmic\nand data-centric methods have been proposed to address such biases, they often\nrequire retraining or significant compute. In this work, we propose a cheap,\ntraining-free method inspired by steering vectors used to edit behaviors in\nlarge language models. We compute the difference in mean activations between\nmajority and minority groups to define a \"bias vector,\" which we subtract from\nthe model's residual stream. This leads to reduced classification bias and\nimproved worst-group accuracy. We explore multiple strategies for extracting\nand applying these vectors in transformer-like classifiers, showing that\nsteering vectors, traditionally used in generative models, can also be\neffective in classification. More broadly, we showcase an extremely cheap,\ninference time, training free method to mitigate bias in classification models.",
        "url": "http://arxiv.org/abs/2506.18598v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18598v1",
        "arxiv_id": "2506.18598v1",
        "authors": [
            "Aviral Gupta",
            "Armaan Sethi",
            "Ameesh Sethi"
        ],
        "submitted": "2025-06-23 12:58:54",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on bias correction in classification models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the topic of model editing, the approach is not applicable to ranking models or user behavior modeling, and the context is different from e-commerce or real-time relevance optimization."
    },
    {
        "title": "Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich Transcripts",
        "abstract": "Accurate detection of disfluencies in spoken language is crucial for\nenhancing the performance of automatic speech and language processing systems,\nas well as fostering the development of more inclusive speech and language\ntechnologies. Leveraging the growing trend of large language models (LLMs) as\nversatile learners capable of processing both lexical and non-lexical inputs\n(e.g., audio and video), we propose a novel approach to transcribing\ndisfluencies as explicit tokens with timestamps, enabling the generation of\nfully annotated disfluency-rich transcripts. Our method integrates acoustic\nrepresentations extracted from an audio encoder with textual inputs of varying\nquality: clean transcriptions without disfluencies, time-aligned transcriptions\nfrom aligners, or outputs from phoneme-based ASR models -- all of which may\ncontain imperfections. Importantly, our experiments demonstrate that textual\ninputs do not need to be flawless. As long as they include timestamp-related\ncues, LLMs can effectively smooth the input and produce fully\ndisfluency-annotated transcripts, underscoring their robustness in handling\nimperfect hints.",
        "url": "http://arxiv.org/abs/2506.18510v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18510v1",
        "arxiv_id": "2506.18510v1",
        "authors": [
            "Duygu Altinok"
        ],
        "submitted": "2025-06-23 11:04:20",
        "source": "arxiv",
        "comment": "Accepted to INTERSPEECH2025 workshop DISS2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The focus on spoken language processing, automatic speech recognition, and disfluency detection is outside your primary areas of interest."
    },
    {
        "title": "MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle\ncomplex reasoning tasks. However, existing RLVR methods overlook one of the\nmost distinctive capabilities of LLMs, their in-context learning ability, as\nprominently demonstrated by the success of Chain-of-Thought (CoT) prompting.\nThis motivates us to explore how reinforcement learning can be effectively\ncombined with in-context learning to better improve the reasoning capabilities\nof LLMs. In this paper, we introduce Motivation-enhanced Reinforcement\nFinetuning} (MeRF), an intuitive yet effective method enhancing reinforcement\nlearning of LLMs by involving ``telling LLMs the rules of the game''.\nSpecifically, MeRF directly injects the reward specification into the prompt,\nwhich serves as an in-context motivation for model to improve its responses\nwith awareness of the optimization objective. This simple modification\nleverages the in-context learning ability of LLMs aligning generation with\noptimization, thereby incentivizing the model to generate desired outputs from\nboth inner motivation and external reward. Empirical evaluations on the Knights\nand Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that\n\\texttt{MeRF} achieves substantial performance gains over baselines. Moreover,\nablation studies show that performance improves with greater consistency\nbetween the in-context motivation and the external reward function, while the\nmodel also demonstrates an ability to adapt to misleading motivations through\nreinforcement learning.",
        "url": "http://arxiv.org/abs/2506.18485v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18485v1",
        "arxiv_id": "2506.18485v1",
        "authors": [
            "Junjie Zhang",
            "Guozheng Ma",
            "Shunyu Liu",
            "Haoyu Wang",
            "Jiaxing Huang",
            "Ting-En Lin",
            "Fei Huang",
            "Yongbin Li",
            "Dacheng Tao"
        ],
        "submitted": "2025-06-23 10:37:57",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores reinforcement learning for large language models, which is a related topic to information retrieval and search technologies. However, the focus on reasoning and logic puzzle solving is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Lemmatization as a Classification Task: Results from Arabic across Multiple Genres",
        "abstract": "Lemmatization is crucial for NLP tasks in morphologically rich languages with\nambiguous orthography like Arabic, but existing tools face challenges due to\ninconsistent standards and limited genre coverage. This paper introduces two\nnovel approaches that frame lemmatization as classification into a\nLemma-POS-Gloss (LPG) tagset, leveraging machine translation and semantic\nclustering. We also present a new Arabic lemmatization test set covering\ndiverse genres, standardized alongside existing datasets. We evaluate character\nlevel sequence-to-sequence models, which perform competitively and offer\ncomplementary value, but are limited to lemma prediction (not LPG) and prone to\nhallucinating implausible forms. Our results show that classification and\nclustering yield more robust, interpretable outputs, setting new benchmarks for\nArabic lemmatization.",
        "url": "http://arxiv.org/abs/2506.18399v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18399v1",
        "arxiv_id": "2506.18399v1",
        "authors": [
            "Mostafa Saeed",
            "Nizar Habash"
        ],
        "submitted": "2025-06-23 08:34:33",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on lemmatization in Arabic, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and query understanding. Although it involves Natural Language Processing, the specific topic and methodology are not relevant to the user's areas of focus."
    },
    {
        "title": "RLPR: Extrapolating RLVR to General Domains without Verifiers",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising\npotential in advancing the reasoning capabilities of LLMs. However, its success\nremains largely confined to mathematical and code domains. This primary\nlimitation stems from the heavy reliance on domain-specific verifiers, which\nresults in prohibitive complexity and limited scalability. To address the\nchallenge, our key observation is that LLM's intrinsic probability of\ngenerating a correct free-form answer directly indicates its own evaluation of\nthe reasoning reward (i.e., how well the reasoning process leads to the correct\nanswer). Building on this insight, we propose RLPR, a simple verifier-free\nframework that extrapolates RLVR to broader general domains. RLPR uses the\nLLM's own token probability scores for reference answers as the reward signal\nand maximizes the expected reward during training. We find that addressing the\nhigh variance of this noisy probability reward is crucial to make it work, and\npropose prob-to-reward and stabilizing methods to ensure a precise and stable\nreward from LLM intrinsic probabilities. Comprehensive experiments in four\ngeneral-domain benchmarks and three mathematical benchmarks show that RLPR\nconsistently improves reasoning capabilities in both areas for Gemma, Llama,\nand Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6\npoints on TheoremQA and 7.5 points on Minerva, and even surpasses strong\nverifier-model-dependent approaches General-Reasoner by 1.6 average points\nacross seven benchmarks.",
        "url": "http://arxiv.org/abs/2506.18254v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18254v1",
        "arxiv_id": "2506.18254v1",
        "authors": [
            "Tianyu Yu",
            "Bo Ji",
            "Shouli Wang",
            "Shu Yao",
            "Zefan Wang",
            "Ganqu Cui",
            "Lifan Yuan",
            "Ning Ding",
            "Yuan Yao",
            "Zhiyuan Liu",
            "Maosong Sun",
            "Tat-Seng Chua"
        ],
        "submitted": "2025-06-23 02:56:36",
        "source": "arxiv",
        "comment": "Project Website: https://github.com/openbmb/RLPR",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper proposes a reinforcement learning framework, RLPR, that extrapolates RLVR to general domains without verifiers. While it's related to query understanding and ranking models, the focus is on reasoning capabilities and LLMs, which is not directly aligned with the user's primary research interests in information retrieval and search technologies. However, the paper's use of token probability scores for reference answers as the reward signal shows some relevance to the user's background in e-commerce and NLP."
    },
    {
        "title": "AdapThink: Adaptive Thinking Preferences for Reasoning Language Model",
        "abstract": "Reinforcement Learning (RL)-based post-training has significantly advanced\nthe complex reasoning capabilities of language models, fostering sophisticated\nself-reflection processes. However, this ``slow thinking'' paradigm presents a\ncritical challenge to reasoning efficiency: models may expend excessive\ncomputation on simple questions and shift reasoning prematurely for complex\nones. Previous mechanisms typically rely on static length budgets or predefined\nrules, lacking the adaptability for varying question complexities and models'\nevolving capabilities. To this end, we propose AdapThink, an adaptive\npost-training framework designed to induce more efficient thinking while\nmaintaining the performance of reasoning language models. Specifically,\nAdapThink incorporates two key mechanisms: 1) A group-relative reward function\nthat leverages model confidence and response's characteristic to dynamically\nadjust the preference of reflection-related transition words without resorting\nto a fixed length preference. 2) A diversity-aware sampling mechanism that\nbalances the training group's solution accuracy with reasoning diversity via an\nentropy-guided score. Experiments on several mathematical reasoning datasets\nwith DeepSeek-distilled models demonstrate AdapThink's advantages in enabling\nadaptive reasoning patterns and mitigating the inefficiencies.",
        "url": "http://arxiv.org/abs/2506.18237v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18237v1",
        "arxiv_id": "2506.18237v1",
        "authors": [
            "Xu Wan",
            "Wei Wang",
            "Wenyue Xu",
            "Wotao Yin",
            "Jie Song",
            "Mingyang Sun"
        ],
        "submitted": "2025-06-23 02:06:04",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a reinforcement learning-based framework for adaptive post-training of language models, focusing on efficient reasoning and self-reflection. While it touches on the idea of 'thinking' and 'reflection', the paper's primary concern is not query understanding, ranking models, or user behavior modeling, which are core aspects of Information Retrieval and Search technologies. The paper's relevance to the user's research interests is limited, but it does explore some aspects of language models and reasoning, which may be of interest to the user."
    },
    {
        "title": "SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging",
        "abstract": "Model merging has gained increasing attention due to its intriguing property:\ninterpolating the parameters of different task-specific fine-tuned models leads\nto multi-task abilities. However, despite its empirical success, the underlying\nmechanisms of model merging remain poorly understood. In this work, we delve\ninto the mechanism behind model merging from a representation perspective. Our\nanalysis reveals that model merging achieves multi-task abilities through two\nkey capabilities: i) distinguishing samples from different tasks, and ii)\nadapting to the corresponding expert model for each sample. These two\ncapabilities allow the merged model to retain task-specific expertise, enabling\nefficient multi-task adaptation. Building on these insights, we propose\n\\texttt{SE-Merging}, a self-enhanced model merging framework that leverages\nthese two characteristics to dynamically identify the corresponding task for\neach sample and then adaptively rescales the merging coefficients to further\nenhance task-specific expertise in the merged model. Notably,\n\\texttt{SE-Merging} achieves dynamic model merging without additional training.\nExtensive experiments demonstrate that \\texttt{SE-Merging} achieves significant\nperformance improvements while remaining compatible with existing model merging\ntechniques.",
        "url": "http://arxiv.org/abs/2506.18135v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18135v1",
        "arxiv_id": "2506.18135v1",
        "authors": [
            "Zijun Chen",
            "Zhanpeng Zhou",
            "Bo Zhang",
            "Weinan Zhang",
            "Xi Sun",
            "Junchi Yan"
        ],
        "submitted": "2025-06-22 18:38:41",
        "source": "arxiv",
        "comment": "preprint, accepted at IJCNN2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on model merging, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on multi-task abilities, the context is not relevant to the user's interests in IR and NLP."
    },
    {
        "title": "OmniGen2: Exploration to Advanced Multimodal Generation",
        "abstract": "In this work, we introduce OmniGen2, a versatile and open-source generative\nmodel designed to provide a unified solution for diverse generation tasks,\nincluding text-to-image, image editing, and in-context generation. Unlike\nOmniGen v1, OmniGen2 features two distinct decoding pathways for text and image\nmodalities, utilizing unshared parameters and a decoupled image tokenizer. This\ndesign enables OmniGen2 to build upon existing multimodal understanding models\nwithout the need to re-adapt VAE inputs, thereby preserving the original text\ngeneration capabilities. To facilitate the training of OmniGen2, we developed\ncomprehensive data construction pipelines, encompassing image editing and\nin-context generation data. Additionally, we introduce a reflection mechanism\ntailored for image generation tasks and curate a dedicated reflection dataset\nbased on OmniGen2. Despite its relatively modest parameter size, OmniGen2\nachieves competitive results on multiple task benchmarks, including\ntext-to-image and image editing. To further evaluate in-context generation,\nalso referred to as subject-driven tasks, we introduce a new benchmark named\nOmniContext. OmniGen2 achieves state-of-the-art performance among open-source\nmodels in terms of consistency. We will release our models, training code,\ndatasets, and data construction pipeline to support future research in this\nfield. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:\nhttps://github.com/VectorSpaceLab/OmniGen2",
        "url": "http://arxiv.org/abs/2506.18871v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18871v1",
        "arxiv_id": "2506.18871v1",
        "authors": [
            "Chenyuan Wu",
            "Pengfei Zheng",
            "Ruiran Yan",
            "Shitao Xiao",
            "Xin Luo",
            "Yueze Wang",
            "Wanli Li",
            "Xiyan Jiang",
            "Yexin Liu",
            "Junjie Zhou",
            "Ze Liu",
            "Ziyi Xia",
            "Chaofan Li",
            "Haoge Deng",
            "Jiahao Wang",
            "Kun Luo",
            "Bo Zhang",
            "Defu Lian",
            "Xinlong Wang",
            "Zhongyuan Wang",
            "Tiejun Huang",
            "Zheng Liu"
        ],
        "submitted": "2025-06-23 17:38:54",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on multimodal generation, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions generative models, the context is different from the user's interests in ranking models and user behavior modeling."
    },
    {
        "title": "Mechanistic Interpretability Needs Philosophy",
        "abstract": "Mechanistic interpretability (MI) aims to explain how neural networks work by\nuncovering their underlying causal mechanisms. As the field grows in influence,\nit is increasingly important to examine not just models themselves, but the\nassumptions, concepts and explanatory strategies implicit in MI research. We\nargue that mechanistic interpretability needs philosophy: not as an\nafterthought, but as an ongoing partner in clarifying its concepts, refining\nits methods, and assessing the epistemic and ethical stakes of interpreting AI\nsystems. Taking three open problems from the MI literature as examples, this\nposition paper illustrates the value philosophy can add to MI research, and\noutlines a path toward deeper interdisciplinary dialogue.",
        "url": "http://arxiv.org/abs/2506.18852v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18852v1",
        "arxiv_id": "2506.18852v1",
        "authors": [
            "Iwan Williams",
            "Ninell Oldenburg",
            "Ruchira Dhar",
            "Joshua Hatherley",
            "Constanza Fierro",
            "Nina Rajcic",
            "Sandrine R. Schiller",
            "Filippos Stamatiou",
            "Anders Søgaard"
        ],
        "submitted": "2025-06-23 17:13:30",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus on mechanistic interpretability and philosophy is outside the scope of the user's research interests."
    },
    {
        "title": "RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies",
        "abstract": "Large Language Models (LLMs) have been extensively evaluated for general\nsummarization tasks as well as medical research assistance, but they have not\nbeen specifically evaluated for the task of summarizing real-world evidence\n(RWE) from structured output of RWE studies. We introduce RWESummary, a\nproposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al.,\n2025) to enable benchmarking of LLMs for this task. RWESummary includes one\nscenario and three evaluations covering major types of errors observed in\nsummarization of medical research studies and was developed using Atropos\nHealth proprietary data. Additionally, we use RWESummary to compare the\nperformance of different LLMs in our internal RWE summarization tool. At the\ntime of publication, with 13 distinct RWE studies, we found the Gemini 2.5\nmodels performed best overall (both Flash and Pro). We suggest RWESummary as a\nnovel and useful foundation model benchmark for real-world evidence study\nsummarization.",
        "url": "http://arxiv.org/abs/2506.18819v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18819v1",
        "arxiv_id": "2506.18819v1",
        "authors": [
            "Arjun Mukerji",
            "Michael L. Jackson",
            "Jason Jones",
            "Neil Sanghavi"
        ],
        "submitted": "2025-06-23 16:28:03",
        "source": "arxiv",
        "comment": "24 pages, 2 figures",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on evaluating large language models for summarizing real-world evidence studies, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on natural language processing, the specific application and evaluation metrics are not aligned with the user's research interests."
    },
    {
        "title": "TranslationCorrect: A Unified Framework for Machine Translation Post-Editing with Predictive Error Assistance",
        "abstract": "Machine translation (MT) post-editing and research data collection often rely\non inefficient, disconnected workflows. We introduce TranslationCorrect, an\nintegrated framework designed to streamline these tasks. TranslationCorrect\ncombines MT generation using models like NLLB, automated error prediction using\nmodels like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive\npost-editing interface within a single environment. Built with human-computer\ninteraction (HCI) principles in mind to minimize cognitive load, as confirmed\nby a user study. For translators, it enables them to correct errors and batch\ntranslate efficiently. For researchers, TranslationCorrect exports high-quality\nspan-based annotations in the Error Span Annotation (ESA) format, using an\nerror taxonomy inspired by Multidimensional Quality Metrics (MQM). These\noutputs are compatible with state-of-the-art error detection models and\nsuitable for training MT or post-editing systems. Our user study confirms that\nTranslationCorrect significantly improves translation efficiency and user\nsatisfaction over traditional annotation methods.",
        "url": "http://arxiv.org/abs/2506.18337v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18337v1",
        "arxiv_id": "2506.18337v1",
        "authors": [
            "Syed Mekael Wasti",
            "Shou-Yi Hung",
            "Christopher Collins",
            "En-Shiun Annie Lee"
        ],
        "submitted": "2025-06-23 06:38:49",
        "source": "arxiv",
        "comment": "Preprint",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on machine translation post-editing, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on some NLP aspects, the paper's scope is too narrow and specific to machine translation, making it only loosely relevant to the user's broader interests."
    },
    {
        "title": "Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review",
        "abstract": "Large language models have demonstrated remarkable capabilities across\nvarious domains, yet concerns about cultural bias - particularly towards Arabs\nand Muslims - pose significant ethical challenges by perpetuating harmful\nstereotypes and marginalization. Despite growing recognition of bias in LLMs,\nprompt engineering strategies specifically addressing Arab and Muslim\nrepresentation remain understudied. This mixed-methods systematic review\nexamines such techniques, offering evidence-based guidance for researchers and\npractitioners. Following PRISMA guidelines and Kitchenham's systematic review\nmethodology, we analyzed 8 empirical studies published between 2021-2024\ninvestigating bias mitigation strategies. Our findings reveal five primary\nprompt engineering approaches: cultural prompting, affective priming,\nself-debiasing techniques, structured multi-step pipelines, and\nparameter-optimized continuous prompts. Although all approaches show potential\nfor reducing bias, effectiveness varied substantially across studies and bias\ntypes. Evidence suggests that certain bias types may be more resistant to\nprompt-based mitigation than others. Structured multi-step pipelines\ndemonstrated the highest overall effectiveness, achieving up to 87.7% reduction\nin bias, though they require greater technical expertise. Cultural prompting\noffers broader accessibility with substantial effectiveness. These results\nunderscore the accessibility of prompt engineering for mitigating cultural bias\nwithout requiring access to model parameters. The limited number of studies\nidentified highlights a significant research gap in this critical area. Future\nresearch should focus on developing culturally adaptive prompting techniques,\ncreating Arab and Muslim-specific evaluation resources, and integrating prompt\nengineering with complementary debiasing methods to address deeper stereotypes\nwhile maintaining model utility.",
        "url": "http://arxiv.org/abs/2506.18199v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18199v1",
        "arxiv_id": "2506.18199v1",
        "authors": [
            "Bushra Asseri",
            "Estabrag Abdelaziz",
            "Areej Al-Wabil"
        ],
        "submitted": "2025-06-22 23:15:25",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on cultural bias in large language models, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper touches on prompt engineering, it is primarily concerned with bias mitigation in language models, which is not a central theme in the user's research areas."
    },
    {
        "title": "Reasoning about Uncertainty: Do Reasoning Models Know When They Don't Know?",
        "abstract": "Reasoning language models have set state-of-the-art (SOTA) records on many\nchallenging benchmarks, enabled by multi-step reasoning induced using\nreinforcement learning. However, like previous language models, reasoning\nmodels are prone to generating confident, plausible responses that are\nincorrect (hallucinations). Knowing when and how much to trust these models is\ncritical to the safe deployment of reasoning models in real-world applications.\nTo this end, we explore uncertainty quantification of reasoning models in this\nwork. Specifically, we ask three fundamental questions: First, are reasoning\nmodels well-calibrated? Second, does deeper reasoning improve model\ncalibration? Finally, inspired by humans' innate ability to double-check their\nthought processes to verify the validity of their answers and their confidence,\nwe ask: can reasoning models improve their calibration by explicitly reasoning\nabout their chain-of-thought traces? We introduce introspective uncertainty\nquantification (UQ) to explore this direction. In extensive evaluations on SOTA\nreasoning models across a broad range of benchmarks, we find that reasoning\nmodels: (i) are typically overconfident, with self-verbalized confidence\nestimates often greater than 85% particularly for incorrect responses, (ii)\nbecome even more overconfident with deeper reasoning, and (iii) can become\nbetter calibrated through introspection (e.g., o3-Mini and DeepSeek R1) but not\nuniformly (e.g., Claude 3.7 Sonnet becomes more poorly calibrated). Lastly, we\nconclude with important research directions to design necessary UQ benchmarks\nand improve the calibration of reasoning models.",
        "url": "http://arxiv.org/abs/2506.18183v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18183v1",
        "arxiv_id": "2506.18183v1",
        "authors": [
            "Zhiting Mei",
            "Christina Zhang",
            "Tenny Yin",
            "Justin Lidard",
            "Ola Shorinwa",
            "Anirudha Majumdar"
        ],
        "submitted": "2025-06-22 21:46:42",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores uncertainty quantification in reasoning language models, which is a topic in NLP. While it touches on the idea of confidence estimation, it does not directly relate to query understanding, ranking models, or user behavior modeling in the context of information retrieval. The paper's focus on language models and their calibration is not directly aligned with the user's primary research interests in IR and search technologies."
    },
    {
        "title": "The Syntactic Acceptability Dataset (Preview): A Resource for Machine Learning and Linguistic Analysis of English",
        "abstract": "We present a preview of the Syntactic Acceptability Dataset, a resource being\ndesigned for both syntax and computational linguistics research. In its current\nform, the dataset comprises 1,000 English sequences from the syntactic\ndiscourse: Half from textbooks and half from the journal Linguistic Inquiry,\nthe latter to ensure a representation of the contemporary discourse. Each entry\nis labeled with its grammatical status (\"well-formedness\" according to\nsyntactic formalisms) extracted from the literature, as well as its\nacceptability status (\"intuitive goodness\" as determined by native speakers)\nobtained through crowdsourcing, with highest experimental standards. Even in\nits preliminary form, this dataset stands as the largest of its kind that is\npublicly accessible. We also offer preliminary analyses addressing three\ndebates in linguistics and computational linguistics: We observe that\ngrammaticality and acceptability judgments converge in about 83% of the cases\nand that \"in-betweenness\" occurs frequently. This corroborates existing\nresearch. We also find that while machine learning models struggle with\npredicting grammaticality, they perform considerably better in predicting\nacceptability. This is a novel finding. Future work will focus on expanding the\ndataset.",
        "url": "http://arxiv.org/abs/2506.18120v1",
        "pdf_url": "http://arxiv.org/pdf/2506.18120v1",
        "arxiv_id": "2506.18120v1",
        "authors": [
            "Tom S Juzek"
        ],
        "submitted": "2025-06-22 18:03:49",
        "source": "arxiv",
        "comment": "Accepted and published at LREC-COLING 2024. 8 pages, 3 figures.\n  Licensed under CC BY-NC-SA 4.0",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus is on linguistics and syntax, which is a distant topic from the user's primary research interests."
    }
]