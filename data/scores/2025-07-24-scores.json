[
    {
        "title": "Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning",
        "abstract": "Accessing knowledge via multilingual natural-language interfaces is one of\nthe emerging challenges in the field of information retrieval and related ones.\nStructured knowledge stored in knowledge graphs can be queried via a specific\nquery language (e.g., SPARQL). Therefore, one needs to transform\nnatural-language input into a query to fulfill an information need. Prior\napproaches mostly focused on combining components (e.g., rule-based or\nneural-based) that solve downstream tasks and come up with an answer at the\nend. We introduce mKGQAgent, a human-inspired framework that breaks down the\ntask of converting natural language questions into SPARQL queries into modular,\ninterpretable subtasks. By leveraging a coordinated LLM agent workflow for\nplanning, entity linking, and query refinement - guided by an experience pool\nfor in-context learning - mKGQAgent efficiently handles multilingual KGQA.\nEvaluated on the DBpedia- and Corporate-based KGQA benchmarks within the\nText2SPARQL challenge 2025, our approach took first place among the other\nparticipants. This work opens new avenues for developing human-like reasoning\nsystems in multilingual semantic parsing.",
        "url": "http://arxiv.org/abs/2507.16971v1",
        "pdf_url": "http://arxiv.org/pdf/2507.16971v1",
        "arxiv_id": "2507.16971v1",
        "authors": [
            "Aleksandr Perevalov",
            "Andreas Both"
        ],
        "submitted": "2025-07-22 19:23:03",
        "source": "arxiv",
        "comment": "During the final evaluation on the DBpedia- and Corporate-based KGQA\n  benchmarks within the Text2SPARQL challenge 2025, our approach took first\n  place among the other participants",
        "score": 13,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores multilingual question answering over knowledge graphs, which is related to information retrieval and query understanding. However, the focus is on converting natural language questions into SPARQL queries, which is not directly aligned with ranking models or user behavior modeling. While the paper's use of human-inspired reasoning and modular subtasks is interesting, it does not seem to be a central match with the user's research interests."
    },
    {
        "title": "HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning",
        "abstract": "Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of\nmatching untrimmed videos with text queries describing only partial content.\nExisting methods suffer from geometric distortion in Euclidean space that\nsometimes misrepresents the intrinsic hierarchical structure of videos and\noverlooks certain hierarchical semantics, ultimately leading to suboptimal\ntemporal modeling. To address this issue, we propose the first hyperbolic\nmodeling framework for PRVR, namely HLFormer, which leverages hyperbolic space\nlearning to compensate for the suboptimal hierarchical modeling capabilities of\nEuclidean space. Specifically, HLFormer integrates the Lorentz Attention Block\nand Euclidean Attention Block to encode video embeddings in hybrid spaces,\nusing the Mean-Guided Adaptive Interaction Module to dynamically fuse features.\nAdditionally, we introduce a Partial Order Preservation Loss to enforce \"text <\nvideo\" hierarchy through Lorentzian cone constraints. This approach further\nenhances cross-modal matching by reinforcing partial relevance between video\ncontent and text queries. Extensive experiments show that HLFormer outperforms\nstate-of-the-art methods. Code is released at\nhttps://github.com/lijun2005/ICCV25-HLFormer.",
        "url": "http://arxiv.org/abs/2507.17402v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17402v1",
        "arxiv_id": "2507.17402v1",
        "authors": [
            "Li Jun",
            "Wang Jinpeng",
            "Tan Chaolei",
            "Lian Niu",
            "Chen Long",
            "Zhang Min",
            "Wang Yaowei",
            "Xia Shu-Tao",
            "Chen Bin"
        ],
        "submitted": "2025-07-23 10:59:46",
        "source": "arxiv",
        "comment": "Accepted by ICCV'25. 13 pages, 6 figures, 4 tables",
        "score": 10,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Partially Relevant Video Retrieval, which is not directly related to Information Retrieval or Search technologies. Although it uses attention mechanisms and hierarchical modeling, the context is video retrieval, which is not a core area of interest for the user. The paper's relevance to the user's research interests is limited."
    },
    {
        "title": "Millions of $\\text{GeAR}$-s: Extending GraphRAG to Millions of Documents",
        "abstract": "Recent studies have explored graph-based approaches to retrieval-augmented\ngeneration, leveraging structured or semi-structured information -- such as\nentities and their relations extracted from documents -- to enhance retrieval.\nHowever, these methods are typically designed to address specific tasks, such\nas multi-hop question answering and query-focused summarisation, and therefore,\nthere is limited evidence of their general applicability across broader\ndatasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG\nsolution: $\\text{GeAR}$ and explore its performance and limitations on the\nSIGIR 2025 LiveRAG Challenge.",
        "url": "http://arxiv.org/abs/2507.17399v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17399v1",
        "arxiv_id": "2507.17399v1",
        "authors": [
            "Zhili Shen",
            "Chenxin Diao",
            "Pascual Merita",
            "Pavlos Vougiouklis",
            "Jeff Z. Pan"
        ],
        "submitted": "2025-07-23 10:54:24",
        "source": "arxiv",
        "comment": "Accepted by SIGIR 2025 LiveRAG Challenge Program",
        "score": 9,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'sigir' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores graph-based approaches to retrieval-augmented generation, which is related to query understanding and ranking models in Information Retrieval. However, the focus on graph-based methods and the specific challenge they address (SIGIR 2025 LiveRAG Challenge) do not directly align with the user's primary interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "QuMAB: Query-based Multi-annotator Behavior Pattern Learning",
        "abstract": "Multi-annotator learning traditionally aggregates diverse annotations to\napproximate a single ground truth, treating disagreements as noise. However,\nthis paradigm faces fundamental challenges: subjective tasks often lack\nabsolute ground truth, and sparse annotation coverage makes aggregation\nstatistically unreliable. We introduce a paradigm shift from sample-wise\naggregation to annotator-wise behavior modeling. By treating annotator\ndisagreements as valuable information rather than noise, modeling\nannotator-specific behavior patterns can reconstruct unlabeled data to reduce\nannotation cost, enhance aggregation reliability, and explain annotator\ndecision behavior. To this end, we propose QuMATL (Query-based Multi-Annotator\nBehavior Pattern Learning), which uses light-weight queries to model individual\nannotators while capturing inter-annotator correlations as implicit\nregularization, preventing overfitting to sparse individual data while\nmaintaining individualization and improving generalization, with a\nvisualization of annotator focus regions offering an explainable analysis of\nbehavior understanding. We contribute two large-scale datasets with dense\nper-annotator labels: STREET (4,300 labels/annotator) and AMER (average 3,118\nlabels/annotator), the first multimodal multi-annotator dataset.",
        "url": "http://arxiv.org/abs/2507.17653v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17653v1",
        "arxiv_id": "2507.17653v1",
        "authors": [
            "Liyun Zhang",
            "Zheng Lian",
            "Hong Liu",
            "Takanori Takebe",
            "Yuta Nakashima"
        ],
        "submitted": "2025-07-23 16:17:43",
        "source": "arxiv",
        "comment": "12 pages. arXiv admin note: substantial text overlap with\n  arXiv:2503.15237",
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper QuMAB: Query-based Multi-annotator Behavior Pattern Learning is somewhat related to your research interests in Information Retrieval and Search technologies, particularly in query understanding and behavior modeling. However, the focus on multi-annotator behavior pattern learning and annotation cost reduction is not directly aligned with your primary interests in ranking models and user behavior modeling."
    },
    {
        "title": "GenSelect: A Generative Approach to Best-of-N",
        "abstract": "Generative reward models with parallel sampling have enabled effective\ntest-time scaling for reasoning tasks. Current approaches employ pointwise\nscoring of individual solutions or pairwise comparisons. However, pointwise\nmethods underutilize LLMs' comparative abilities, while pairwise methods scale\ninefficiently with larger sampling budgets. We introduce GenSelect, where the\nLLM uses long reasoning to select the best solution among N candidates. This\nleverages LLMs' comparative strengths while scaling efficiently across parallel\nsampling budgets. For math reasoning, we demonstrate that reasoning models,\nsuch as QwQ and DeepSeek-R1-0528, excel at GenSelect, outperforming existing\nscoring approaches with simple prompting.",
        "url": "http://arxiv.org/abs/2507.17797v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17797v1",
        "arxiv_id": "2507.17797v1",
        "authors": [
            "Shubham Toshniwal",
            "Ivan Sorokin",
            "Aleksander Ficek",
            "Ivan Moshkov",
            "Igor Gitman"
        ],
        "submitted": "2025-07-23 15:22:51",
        "source": "arxiv",
        "comment": "Presented at the 2nd AI for MATH Workshop @ ICML",
        "score": 8,
        "keyword_reasons": [
            "Found 'pointwise' (score: +3)",
            "Found 'pairwise' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on generative reward models and parallel sampling for reasoning tasks, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions large language models (LLMs), the context is not about search or ranking, but rather about math reasoning and scoring approaches."
    },
    {
        "title": "Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs",
        "abstract": "Conversational Information Retrieval (CIR) systems, while offering intuitive\naccess to information, face a significant challenge: reliably handling\nunanswerable questions to prevent the generation of misleading or hallucinated\ncontent. Traditional approaches often rely on external classifiers, which can\nintroduce inconsistencies with the core generative Large Language Models\n(LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a\nnovel approach that deeply integrates unanswerability detection directly within\nthe LLM's generative process. SALU is trained using a multi-task learning\nframework for both standard Question Answering (QA) and explicit abstention\ngeneration for unanswerable queries. Crucially, it incorporates a\nconfidence-score-guided reinforcement learning with human feedback (RLHF)\nphase, which explicitly penalizes hallucinated responses and rewards\nappropriate abstentions, fostering intrinsic self-awareness of knowledge\nboundaries. Through extensive experiments on our custom-built\nC-IR_Answerability dataset, SALU consistently outperforms strong baselines,\nincluding hybrid LLM-classifier systems, in overall accuracy for correctly\nanswering or abstaining from questions. Human evaluation further confirms\nSALU's superior reliability, achieving high scores in factuality, appropriate\nabstention, and, most importantly, a dramatic reduction in hallucination,\ndemonstrating its ability to robustly \"know when to say 'I don't know'.\"",
        "url": "http://arxiv.org/abs/2507.16951v1",
        "pdf_url": "http://arxiv.org/pdf/2507.16951v1",
        "arxiv_id": "2507.16951v1",
        "authors": [
            "Shuyuan Lin",
            "Lei Duan",
            "Philip Hughes",
            "Yuxuan Sheng"
        ],
        "submitted": "2025-07-22 18:44:18",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores Conversational Information Retrieval (CIR) systems, which is related to Information Retrieval (IR) and Search technologies. The focus on unanswerable questions and trustworthy response generation is somewhat relevant to query understanding and ranking models. However, the paper's primary focus on Large Language Models (LLMs) and reinforcement learning is not directly aligned with the user's interests in user behavior modeling and click models."
    },
    {
        "title": "Citation Recommendation using Deep Canonical Correlation Analysis",
        "abstract": "Recent advances in citation recommendation have improved accuracy by\nleveraging multi-view representation learning to integrate the various\nmodalities present in scholarly documents. However, effectively combining\nmultiple data views requires fusion techniques that can capture complementary\ninformation while preserving the unique characteristics of each modality. We\npropose a novel citation recommendation algorithm that improves upon linear\nCanonical Correlation Analysis (CCA) methods by applying Deep CCA (DCCA), a\nneural network extension capable of capturing complex, non-linear relationships\nbetween distributed textual and graph-based representations of scientific\narticles. Experiments on the large-scale DBLP (Digital Bibliography & Library\nProject) citation network dataset demonstrate that our approach outperforms\nstate-of-the-art CCA-based methods, achieving relative improvements of over 11%\nin Mean Average Precision@10, 5% in Precision@10, and 7% in Recall@10. These\ngains reflect more relevant citation recommendations and enhanced ranking\nquality, suggesting that DCCA's non-linear transformations yield more\nexpressive latent representations than CCA's linear projections.",
        "url": "http://arxiv.org/abs/2507.17603v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17603v1",
        "arxiv_id": "2507.17603v1",
        "authors": [
            "Conor McNamara",
            "Effirul Ramlan"
        ],
        "submitted": "2025-07-23 15:34:07",
        "source": "arxiv",
        "comment": "21 pages, 6 figures, 7 tables",
        "score": 7,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on citation recommendation using Deep Canonical Correlation Analysis, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and query understanding. Although it touches on representation learning and ranking, the context is different and the paper's emphasis on citation recommendation and graph-based representations does not align with the user's interests."
    },
    {
        "title": "Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for Tumor Flagging and Staging",
        "abstract": "The increasing volume of medical images poses challenges for radiologists in\nretrieving relevant cases. Content-based image retrieval (CBIR) systems offer\npotential for efficient access to similar cases, yet lack standardized\nevaluation and comprehensive studies. Building on prior studies for tumor\ncharacterization via CBIR, this study advances CBIR research for volumetric\nmedical images through three key contributions: (1) a framework eliminating\nreliance on pre-segmented data and organ-specific datasets, aligning with large\nand unstructured image archiving systems, i.e. PACS in clinical practice; (2)\nintroduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT's\ncontextualized late interaction mechanism for 3D medical imaging; (3)\ncomprehensive evaluation across four tumor sites using three feature extractors\nand three database configurations. Our evaluations highlight the significant\nadvantages of C-MIR. We demonstrate the successful adaptation of the late\ninteraction principle to volumetric medical images, enabling effective\ncontext-aware re-ranking. A key finding is C-MIR's ability to effectively\nlocalize the region of interest, eliminating the need for pre-segmentation of\ndatasets and offering a computationally efficient alternative to systems\nrelying on expensive data enrichment steps. C-MIR demonstrates promising\nimprovements in tumor flagging, achieving improved performance, particularly\nfor colon and lung tumors (p<0.05). C-MIR also shows potential for improving\ntumor staging, warranting further exploration of its capabilities. Ultimately,\nour work seeks to bridge the gap between advanced retrieval techniques and\ntheir practical applications in healthcare, paving the way for improved\ndiagnostic processes.",
        "url": "http://arxiv.org/abs/2507.17412v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17412v1",
        "arxiv_id": "2507.17412v1",
        "authors": [
            "Farnaz Khun Jush",
            "Steffen Vogler",
            "Matthias Lenga"
        ],
        "submitted": "2025-07-23 11:12:52",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on content-based image retrieval for medical images, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although the paper mentions re-ranking, it is not specifically related to query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user."
    },
    {
        "title": "EndoFinder: Online Lesion Retrieval for Explainable Colorectal Polyp Diagnosis Leveraging Latent Scene Representations",
        "abstract": "Colorectal cancer (CRC) remains a leading cause of cancer-related mortality,\nunderscoring the importance of timely polyp detection and diagnosis. While deep\nlearning models have improved optical-assisted diagnostics, they often demand\nextensive labeled datasets and yield \"black-box\" outputs with limited\ninterpretability. In this paper, we propose EndoFinder, an online polyp\nretrieval framework that leverages multi-view scene representations for\nexplainable and scalable CRC diagnosis. First, we develop a Polyp-aware Image\nEncoder by combining contrastive learning and a reconstruction task, guided by\npolyp segmentation masks. This self-supervised approach captures robust\nfeatures without relying on large-scale annotated data. Next, we treat each\npolyp as a three-dimensional \"scene\" and introduce a Scene Representation\nTransformer, which fuses multiple views of the polyp into a single latent\nrepresentation. By discretizing this representation through a hashing layer,\nEndoFinder enables real-time retrieval from a compiled database of historical\npolyp cases, where diagnostic information serves as interpretable references\nfor new queries. We evaluate EndoFinder on both public and newly collected\npolyp datasets for re-identification and pathology classification. Results show\nthat EndoFinder outperforms existing methods in accuracy while providing\ntransparent, retrieval-based insights for clinical decision-making. By\ncontributing a novel dataset and a scalable, explainable framework, our work\naddresses key challenges in polyp diagnosis and offers a promising direction\nfor more efficient AI-driven colonoscopy workflows. The source code is\navailable at https://github.com/ku262/EndoFinder-Scene.",
        "url": "http://arxiv.org/abs/2507.17323v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17323v1",
        "arxiv_id": "2507.17323v1",
        "authors": [
            "Ruijie Yang",
            "Yan Zhu",
            "Peiyao Fu",
            "Yizhe Zhang",
            "Zhihua Wang",
            "Quanlin Li",
            "Pinghong Zhou",
            "Xian Yang",
            "Shuo Wang"
        ],
        "submitted": "2025-07-23 08:45:19",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on medical imaging and diagnosis, using techniques like contrastive learning and scene representation transformers, which are not directly related to the user's areas of interest."
    },
    {
        "title": "Triadic First-Order Logic Queries in Temporal Networks",
        "abstract": "Motif counting is a fundamental problem in network analysis, and there is a\nrich literature of theoretical and applied algorithms for this problem. Given a\nlarge input network $G$, a motif $H$ is a small \"pattern\" graph indicative of\nspecial local structure. Motif/pattern mining involves finding all matches of\nthis pattern in the input $G$. The simplest, yet challenging, case of motif\ncounting is when $H$ has three vertices, often called a \"triadic\" query. Recent\nwork has focused on \"temporal graph mining\", where the network $G$ has edges\nwith timestamps (and directions) and $H$ has time constraints.\n  Inspired by concepts in logic and database theory, we introduce the study of\n\"thresholded First Order Logic (FOL) Motif Analysis\" for massive temporal\nnetworks. A typical triadic motif query asks for the existence of three\nvertices that form a desired temporal pattern. An \"FOL\" motif query is obtained\nby having both existential and thresholded universal quantifiers. This allows\nfor query semantics that can mine richer information from networks. A typical\ntriadic query would be \"find all triples of vertices $u,v,w$ such that they\nform a triangle within one hour\". A thresholded FOL query can express \"find all\npairs $u,v$ such that for half of $w$ where $(u,w)$ formed an edge, $(v,w)$\nalso formed an edge within an hour\".\n  We design the first algorithm, FOLTY, for mining thresholded FOL triadic\nqueries. The theoretical running time of FOLTY matches the best known running\ntime for temporal triangle counting in sparse graphs. We give an efficient\nimplementation of FOLTY using specialized temporal data structures. FOLTY has\nexcellent empirical behavior, and can answer triadic FOL queries on graphs with\nnearly 70M edges is less than hour on commodity hardware. Our work has the\npotential to start a new research direction in the classic well-studied problem\nof motif analysis.",
        "url": "http://arxiv.org/abs/2507.17215v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17215v1",
        "arxiv_id": "2507.17215v1",
        "authors": [
            "Omkar Bhalerao",
            "Yunjie Pan",
            "C. Seshadhri",
            "Nishil Talati"
        ],
        "submitted": "2025-07-23 05:12:23",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on network analysis, motif counting, and temporal graph mining, which are not core topics in the user's research areas."
    },
    {
        "title": "VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings",
        "abstract": "Multimodal learning plays a critical role in e-commerce recommendation\nplatforms today, enabling accurate recommendations and product understanding.\nHowever, existing vision-language models, such as CLIP, face key challenges in\ne-commerce recommendation systems: 1) Weak object-level alignment, where global\nimage embeddings fail to capture fine-grained product attributes, leading to\nsuboptimal retrieval performance; 2) Ambiguous textual representations, where\nproduct descriptions often lack contextual clarity, affecting cross-modal\nmatching; and 3) Domain mismatch, as generic vision-language models may not\ngeneralize well to e-commerce-specific data. To address these limitations, we\npropose a framework, VL-CLIP, that enhances CLIP embeddings by integrating\nVisual Grounding for fine-grained visual understanding and an LLM-based agent\nfor generating enriched text embeddings. Visual Grounding refines image\nrepresentations by localizing key products, while the LLM agent enhances\ntextual features by disambiguating product descriptions. Our approach\nsignificantly improves retrieval accuracy, multimodal retrieval effectiveness,\nand recommendation quality across tens of millions of items on one of the\nlargest e-commerce platforms in the U.S., increasing CTR by 18.6%, ATC by\n15.5%, and GMV by 4.0%. Additional experimental results show that our framework\noutperforms vision-language models, including CLIP, FashionCLIP, and GCL, in\nboth precision and semantic alignment, demonstrating the potential of combining\nobject-aware visual grounding and LLM-enhanced text representation for robust\nmultimodal recommendations.",
        "url": "http://arxiv.org/abs/2507.17080v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17080v1",
        "arxiv_id": "2507.17080v1",
        "authors": [
            "Ramin Giahi",
            "Kehui Yao",
            "Sriram Kollipara",
            "Kai Zhao",
            "Vahid Mirjalili",
            "Jianpeng Xu",
            "Topojoy Biswas",
            "Evren Korpeoglu",
            "Kannan Achan"
        ],
        "submitted": "2025-07-22 23:45:43",
        "source": "arxiv",
        "comment": "Accepted at RecSys 2025; DOI:https://doi.org/10.1145/3705328.3748064",
        "score": 7,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper is somewhat related to your research interests in Information Retrieval and Search technologies, particularly in the context of e-commerce. The focus on multimodal recommendations and visual grounding is relevant to your background in e-commerce, but the emphasis on object-level alignment and domain mismatch is more specific to the e-commerce domain. While the paper does not directly address query understanding, ranking models, or user behavior modeling, it does explore the intersection of vision-language models and recommendation systems, which may be of interest to you."
    },
    {
        "title": "BoSS: Beyond-Semantic Speech",
        "abstract": "Human communication involves more than explicit semantics, with implicit\nsignals and contextual cues playing a critical role in shaping meaning.\nHowever, modern speech technologies, such as Automatic Speech Recognition (ASR)\nand Text-to-Speech (TTS) often fail to capture these beyond-semantic\ndimensions. To better characterize and benchmark the progression of speech\nintelligence, we introduce Spoken Interaction System Capability Levels (L1-L5),\na hierarchical framework illustrated the evolution of spoken dialogue systems\nfrom basic command recognition to human-like social interaction. To support\nthese advanced capabilities, we propose Beyond-Semantic Speech (BoSS), which\nrefers to the set of information in speech communication that encompasses but\ntranscends explicit semantics. It conveys emotions, contexts, and modifies or\nextends meanings through multidimensional features such as affective cues,\ncontextual dynamics, and implicit semantics, thereby enhancing the\nunderstanding of communicative intentions and scenarios. We present a\nformalized framework for BoSS, leveraging cognitive relevance theories and\nmachine learning models to analyze temporal and contextual speech dynamics. We\nevaluate BoSS-related attributes across five different dimensions, reveals that\ncurrent spoken language models (SLMs) are hard to fully interpret\nbeyond-semantic signals. These findings highlight the need for advancing BoSS\nresearch to enable richer, more context-aware human-machine communication.",
        "url": "http://arxiv.org/abs/2507.17563v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17563v1",
        "arxiv_id": "2507.17563v1",
        "authors": [
            "Qing Wang",
            "Zehan Li",
            "Hang Lv",
            "Hongjie Chen",
            "Yaodong Song",
            "Jian Kang",
            "Jie Lian",
            "Jie Li",
            "Yongxiang Li",
            "Zhongjiang He",
            "Xuelong Li"
        ],
        "submitted": "2025-07-23 14:53:50",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the concept of 'Beyond-Semantic Speech' which is related to the implicit signals and contextual cues in human communication. While it's not directly focused on Information Retrieval or Search technologies, it touches on the idea of understanding communicative intentions and scenarios, which is somewhat relevant to query understanding and user behavior modeling. However, the paper's primary focus is on speech technologies and does not directly align with the user's core research themes."
    },
    {
        "title": "Triple X: A LLM-Based Multilingual Speech Recognition System for the INTERSPEECH2025 MLC-SLM Challenge",
        "abstract": "This paper describes our Triple X speech recognition system submitted to Task\n1 of the Multi-Lingual Conversational Speech Language Modeling (MLC-SLM)\nChallenge. Our work focuses on optimizing speech recognition accuracy in\nmultilingual conversational scenarios through an innovative encoder-adapter-LLM\narchitecture. This framework harnesses the powerful reasoning capabilities of\ntext-based large language models while incorporating domain-specific\nadaptations. To further enhance multilingual recognition performance, we\nadopted a meticulously designed multi-stage training strategy leveraging\nextensive multilingual audio datasets. Experimental results demonstrate that\nour approach achieves competitive Word Error Rate (WER) performance on both dev\nand test sets, obtaining second place in the challenge ranking.",
        "url": "http://arxiv.org/abs/2507.17288v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17288v1",
        "arxiv_id": "2507.17288v1",
        "authors": [
            "Miaomiao Gao",
            "Xiaoxiao Xiang",
            "Yiwen Guo"
        ],
        "submitted": "2025-07-23 07:48:33",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on speech recognition, a topic outside the user's primary interest in Information Retrieval and Search technologies. Although it involves large language models, the context is different from the user's focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "LLM4MEA: Data-free Model Extraction Attacks on Sequential Recommenders via Large Language Models",
        "abstract": "Recent studies have demonstrated the vulnerability of sequential recommender\nsystems to Model Extraction Attacks (MEAs). MEAs collect responses from\nrecommender systems to replicate their functionality, enabling unauthorized\ndeployments and posing critical privacy and security risks. Black-box attacks\nin prior MEAs are ineffective at exposing recommender system vulnerabilities\ndue to random sampling in data selection, which leads to misaligned synthetic\nand real-world distributions. To overcome this limitation, we propose LLM4MEA,\na novel model extraction method that leverages Large Language Models (LLMs) as\nhuman-like rankers to generate data. It generates data through interactions\nbetween the LLM ranker and target recommender system. In each interaction, the\nLLM ranker analyzes historical interactions to understand user behavior, and\nselects items from recommendations with consistent preferences to extend the\ninteraction history, which serves as training data for MEA. Extensive\nexperiments demonstrate that LLM4MEA significantly outperforms existing\napproaches in data quality and attack performance, reducing the divergence\nbetween synthetic and real-world data by up to 64.98% and improving MEA\nperformance by 44.82% on average. From a defensive perspective, we propose a\nsimple yet effective defense strategy and identify key hyperparameters of\nrecommender systems that can mitigate the risk of MEAs.",
        "url": "http://arxiv.org/abs/2507.16969v1",
        "pdf_url": "http://arxiv.org/pdf/2507.16969v1",
        "arxiv_id": "2507.16969v1",
        "authors": [
            "Shilong Zhao",
            "Fei Sun",
            "Kaike Zhang",
            "Shaoling Jing",
            "Du Su",
            "Zhichao Shi",
            "Zhiyi Yin",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "submitted": "2025-07-22 19:20:23",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'user behavior' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Model Extraction Attacks on sequential recommenders, which is not directly related to the user's primary interest in Information Retrieval and Search technologies. Although it mentions Large Language Models, the context is different from the user's interest in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice",
        "abstract": "Simultaneous Interpretation (SI) represents one of the most daunting\nfrontiers in the translation industry, with product-level automatic systems\nlong plagued by intractable challenges: subpar transcription and translation\nquality, lack of real-time speech generation, multi-speaker confusion, and\ntranslated speech inflation, especially in long-form discourses. In this study,\nwe introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers\nhigh-fidelity, ultra-low-latency speech-to-speech generation with voice cloning\ncapabilities. As a fully operational product-level solution, Seed-LiveInterpret\n2.0 tackles these challenges head-on through our novel duplex speech-to-speech\nunderstanding-generating framework. Experimental results demonstrate that\nthrough large-scale pretraining and reinforcement learning, the model achieves\na significantly better balance between translation accuracy and latency,\nvalidated by human interpreters to exceed 70% correctness in complex scenarios.\nNotably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by\nsignificant margins in translation quality, while slashing the average latency\nof cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is\naround a near 70% reduction that drastically enhances practical usability.",
        "url": "http://arxiv.org/abs/2507.17527v2",
        "pdf_url": "http://arxiv.org/pdf/2507.17527v2",
        "arxiv_id": "2507.17527v2",
        "authors": [
            "Shanbo Cheng",
            "Yu Bao",
            "Zhichao Huang",
            "Yu Lu",
            "Ningxin Peng",
            "Lu Xu",
            "Runsheng Yu",
            "Rong Cao",
            "Ting Han",
            "Zeyang Li",
            "Sitong Liu",
            "Shengtao Ma",
            "Shiguang Pan",
            "Jiongchen Xiao",
            "Nuo Xu",
            "Meng Yang",
            "Rong Ye",
            "Yiming Yu",
            "Ruofei Zhang",
            "Wanyi Zhang",
            "Wenhao Zhu",
            "Liehao Zou",
            "Lu Lu",
            "Yuxuan Wang",
            "Yonghui Wu"
        ],
        "submitted": "2025-07-23 14:07:41",
        "source": "arxiv",
        "comment": "Seed-LiveInterpret 2.0 Technical Report",
        "score": 5,
        "keyword_reasons": [
            "Found 'ltr' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on simultaneous speech-to-speech translation, which is a topic in Natural Language Processing, but it does not align with your specific interests in deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain",
        "abstract": "Enabling farmers to access accurate agriculture-related information in their\nnative languages in a timely manner is crucial for the success of the\nagriculture field. Although large language models (LLMs) can be used to\nimplement Question Answering (QA) systems, simply using publicly available\ngeneral-purpose LLMs in agriculture typically offer generic advisories, lacking\nprecision in local and multilingual contexts due to insufficient\ndomain-specific training and scarcity of high-quality, region-specific\ndatasets. Our study addresses these limitations by generating multilingual\nsynthetic agricultural datasets (English, Hindi, Punjabi) from\nagriculture-specific documents and fine-tuning language-specific LLMs. Our\nevaluation on curated multilingual datasets demonstrates significant\nimprovements in factual accuracy, relevance, and agricultural consensus for the\nfine-tuned models compared to their baseline counterparts. These results\nhighlight the efficacy of synthetic data-driven, language-specific fine-tuning\nas an effective strategy to improve the performance of LLMs in agriculture,\nespecially in multilingual and low-resource settings. By enabling more accurate\nand localized agricultural advisory services, this study provides a meaningful\nstep toward bridging the knowledge gap in AI-driven agricultural solutions for\ndiverse linguistic communities.",
        "url": "http://arxiv.org/abs/2507.16974v1",
        "pdf_url": "http://arxiv.org/pdf/2507.16974v1",
        "arxiv_id": "2507.16974v1",
        "authors": [
            "Rishemjit Kaur",
            "Arshdeep Singh Bhankhar",
            "Surangika Ranathunga",
            "Jashanpreet Singh Salh",
            "Sudhir Rajput",
            "Vidhi",
            "Kashish Mahendra",
            "Bhavika Berwal",
            "Ritesh Kumar"
        ],
        "submitted": "2025-07-22 19:25:10",
        "source": "arxiv",
        "comment": "15 pages, 9 tables, Appendix A-K",
        "score": 5,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on question answering with multilingual LLMs in the agricultural domain, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions fine-tuning language-specific LLMs, the context is specific to agriculture and does not align with the user's broader interests."
    },
    {
        "title": "TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages of West Asia and North Africa",
        "abstract": "We present TyDi QA-WANA, a question-answering dataset consisting of 28K\nexamples divided among 10 language varieties of western Asia and northern\nAfrica. The data collection process was designed to elicit information-seeking\nquestions, where the asker is genuinely curious to know the answer. Each\nquestion in paired with an entire article that may or may not contain the\nanswer; the relatively large size of the articles results in a task suitable\nfor evaluating models' abilities to utilize large text contexts in answering\nquestions. Furthermore, the data was collected directly in each language\nvariety, without the use of translation, in order to avoid issues of cultural\nrelevance. We present performance of two baseline models, and release our code\nand data to facilitate further improvement by the research community.",
        "url": "http://arxiv.org/abs/2507.17709v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17709v1",
        "arxiv_id": "2507.17709v1",
        "authors": [
            "Parker Riley",
            "Siamak Shakeri",
            "Waleed Ammar",
            "Jonathan H. Clark"
        ],
        "submitted": "2025-07-23 17:20:28",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper presents a question-answering dataset focused on languages of West Asia and North Africa, which is relevant to Information Retrieval and Search technologies. The emphasis on information-seeking questions and large text contexts is also aligned with my interests in query understanding and ranking models. However, the paper's focus on language-specific issues and cultural relevance may not be directly applicable to my primary research areas."
    },
    {
        "title": "Each to Their Own: Exploring the Optimal Embedding in RAG",
        "abstract": "Recently, as Large Language Models (LLMs) have fundamentally impacted various\nfields, the methods for incorporating up-to-date information into LLMs or\nadding external knowledge to construct domain-specific models have garnered\nwide attention. Retrieval-Augmented Generation (RAG), serving as an\ninference-time scaling method, is notable for its low cost and minimal effort\nfor parameter tuning. However, due to heterogeneous training data and model\narchitecture, the variant embedding models used in RAG exhibit different\nbenefits across various areas, often leading to different similarity\ncalculation results and, consequently, varying response quality from LLMs. To\naddress this problem, we propose and examine two approaches to enhance RAG by\ncombining the benefits of multiple embedding models, named Mixture-Embedding\nRAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects\nretrievals from multiple embedding models based on standardized similarity;\nhowever, it does not outperform vanilla RAG. In contrast, Confident RAG\ngenerates responses multiple times using different embedding models and then\nselects the responses with the highest confidence level, demonstrating average\nimprovements of approximately 10% and 5% over vanilla LLMs and RAG,\nrespectively. The consistent results across different LLMs and embedding models\nindicate that Confident RAG is an efficient plug-and-play approach for various\ndomains. We will release our code upon publication.",
        "url": "http://arxiv.org/abs/2507.17442v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17442v1",
        "arxiv_id": "2507.17442v1",
        "authors": [
            "Shiting Chen",
            "Zijian Zhao",
            "Jinsong Chen"
        ],
        "submitted": "2025-07-23 12:03:54",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores Retrieval-Augmented Generation (RAG) and proposes two approaches to enhance it, but it does not directly relate to information retrieval, query understanding, or ranking models, which are core areas of interest. While it touches on embedding models, it is primarily focused on language models and their applications, which is a related but distinct area."
    },
    {
        "title": "\"Beyond the past\": Leveraging Audio and Human Memory for Sequential Music Recommendation",
        "abstract": "On music streaming services, listening sessions are often composed of a\nbalance of familiar and new tracks. Recently, sequential recommender systems\nhave adopted cognitive-informed approaches, such as Adaptive Control of\nThought-Rational (ACT-R), to successfully improve the prediction of the most\nrelevant tracks for the next user session. However, one limitation of using a\nmodel inspired by human memory (or the past), is that it struggles to recommend\nnew tracks that users have not previously listened to. To bridge this gap, here\nwe propose a model that leverages audio information to predict in advance the\nACT-R-like activation of new tracks and incorporates them into the\nrecommendation scoring process. We demonstrate the empirical effectiveness of\nthe proposed model using proprietary data, which we publicly release along with\nthe model's source code to foster future research in this field.",
        "url": "http://arxiv.org/abs/2507.17356v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17356v1",
        "arxiv_id": "2507.17356v1",
        "authors": [
            "Viet-Tran Anh",
            "Bruno Sguerra",
            "Gabriel Meseguer-Brocal",
            "Lea Briand",
            "Manuel Moussallam"
        ],
        "submitted": "2025-07-23 09:37:23",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to information retrieval and search technologies, as it involves sequential recommendation systems. However, the focus on music recommendation and human memory is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling. The paper's relevance to the user's background in e-commerce is also limited."
    },
    {
        "title": "Exploring the Potential of LLMs for Serendipity Evaluation in Recommender Systems",
        "abstract": "Serendipity plays a pivotal role in enhancing user satisfaction within\nrecommender systems, yet its evaluation poses significant challenges due to its\ninherently subjective nature and conceptual ambiguity. Current algorithmic\napproaches predominantly rely on proxy metrics for indirect assessment, often\nfailing to align with real user perceptions, thus creating a gap. With large\nlanguage models (LLMs) increasingly revolutionizing evaluation methodologies\nacross various human annotation tasks, we are inspired to explore a core\nresearch proposition: Can LLMs effectively simulate human users for serendipity\nevaluation? To address this question, we conduct a meta-evaluation on two\ndatasets derived from real user studies in the e-commerce and movie domains,\nfocusing on three key aspects: the accuracy of LLMs compared to conventional\nproxy metrics, the influence of auxiliary data on LLM comprehension, and the\nefficacy of recently popular multi-LLM techniques. Our findings indicate that\neven the simplest zero-shot LLMs achieve parity with, or surpass, the\nperformance of conventional metrics. Furthermore, multi-LLM techniques and the\nincorporation of auxiliary data further enhance alignment with human\nperspectives. Based on our findings, the optimal evaluation by LLMs yields a\nPearson correlation coefficient of 21.5\\% when compared to the results of the\nuser study. This research implies that LLMs may serve as potentially accurate\nand cost-effective evaluators, introducing a new paradigm for serendipity\nevaluation in recommender systems.",
        "url": "http://arxiv.org/abs/2507.17290v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17290v1",
        "arxiv_id": "2507.17290v1",
        "authors": [
            "Li Kang",
            "Yuhan Zhao",
            "Li Chen"
        ],
        "submitted": "2025-07-23 07:51:56",
        "source": "arxiv",
        "comment": "RecSys2025",
        "score": 4,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the potential of large language models (LLMs) for serendipity evaluation in recommender systems, which is somewhat related to my interests in information retrieval and search technologies. However, the focus on recommender systems and serendipity evaluation is not directly aligned with my primary research themes, and the paper does not address query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings",
        "abstract": "Automatic generation of radiology reports has the potential to alleviate\nradiologists' significant workload, yet current methods struggle to deliver\nclinically reliable conclusions. In particular, most prior approaches focus on\nproducing fluent text without effectively ensuring the factual correctness of\nthe reports and often rely on single-view images, limiting diagnostic\ncomprehensiveness. We propose CLARIFID, a novel framework that directly\noptimizes diagnostic correctness by mirroring the two-step workflow of experts.\nSpecifically, CLARIFID (1) learns the logical flow from Findings to Impression\nthrough section-aware pretraining, (2) is fine-tuned with Proximal Policy\nOptimization in which the CheXbert F1 score of the Impression section serves as\nthe reward, (3) enforces reasoning-aware decoding that completes \"Findings\"\nbefore synthesizing the \"Impression\", and (4) fuses multiple chest X-ray views\nvia a vision-transformer-based multi-view encoder. During inference, we apply a\nreasoning-aware next-token forcing strategy followed by report-level\nre-ranking, ensuring that the model first produces a comprehensive Findings\nsection before synthesizing the Impression and thereby preserving coherent\nclinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate\nthat our method achieves superior clinical efficacy and outperforms existing\nbaselines on both standard NLG metrics and clinically aware scores.",
        "url": "http://arxiv.org/abs/2507.17234v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17234v1",
        "arxiv_id": "2507.17234v1",
        "authors": [
            "Kyeongkyu Lee",
            "Seonghwan Yoon",
            "Hongki Lim"
        ],
        "submitted": "2025-07-23 05:57:59",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on radiology report generation, which is a specific application in Natural Language Processing, but it does not address the user's core research themes."
    },
    {
        "title": "Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?",
        "abstract": "Pairwise preferences over model responses are widely collected to evaluate\nand provide feedback to large language models (LLMs). Given two alternative\nmodel responses to the same input, a human or AI annotator selects the \"better\"\nresponse. This approach can provide feedback for domains where other hard-coded\nmetrics are difficult to obtain (e.g., chat response quality), thereby helping\nmodel evaluation or training. However, for some domains high-quality pairwise\ncomparisons can be tricky to obtain - from AI and humans. For example, for\nresponses with many factual statements, annotators may disproportionately weigh\nwriting quality rather than underlying facts. In this work, we explore\naugmenting standard AI annotator systems with additional tools to improve\nperformance on three challenging response domains: long-form factual, math and\ncode tasks. We propose a tool-using agentic system to provide higher quality\nfeedback on these domains. Our system uses web-search and code execution to\nground itself based on external validation, independent of the LLM's internal\nknowledge and biases. We provide extensive experimental results evaluating our\nmethod across the three targeted response domains as well as general annotation\ntasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as\nthree new datasets for domains with saturated pre-existing datasets. Our\nresults indicate that external tools can indeed improve performance in many,\nbut not all, cases. More generally, our experiments highlight the sensitivity\nof performance to simple parameters (e.g., prompt) and the need for improved\n(non-saturated) annotator benchmarks. We share our code at\nhttps://github.com/apple/ml-agent-evaluator.",
        "url": "http://arxiv.org/abs/2507.17015v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17015v1",
        "arxiv_id": "2507.17015v1",
        "authors": [
            "Arduin Findeis",
            "Floris Weers",
            "Guoli Yin",
            "Ke Ye",
            "Ruoming Pang",
            "Tom Gunter"
        ],
        "submitted": "2025-07-22 20:57:09",
        "source": "arxiv",
        "comment": "Accepted at ACL 2025",
        "score": 4,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the use of external validation tools to improve annotation quality for large language models, which is related to information retrieval and search technologies. However, the focus on annotation quality and model evaluation is not directly aligned with the user's primary interest in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat limited to the user's background in e-commerce and NLP, but it does not address the user's core research themes."
    },
    {
        "title": "Leave No One Behind: Fairness-Aware Cross-Domain Recommender Systems for Non-Overlapping Users",
        "abstract": "Cross-domain recommendation (CDR) methods predominantly leverage overlapping\nusers to transfer knowledge from a source domain to a target domain. However,\nthrough empirical studies, we uncover a critical bias inherent in these\napproaches: while overlapping users experience significant enhancements in\nrecommendation quality, non-overlapping users benefit minimally and even face\nperformance degradation. This unfairness may erode user trust, and,\nconsequently, negatively impact business engagement and revenue. To address\nthis issue, we propose a novel solution that generates virtual source-domain\nusers for non-overlapping target-domain users. Our method utilizes a dual\nattention mechanism to discern similarities between overlapping and\nnon-overlapping users, thereby synthesizing realistic virtual user embeddings.\nWe further introduce a limiter component that ensures the generated virtual\nusers align with real-data distributions while preserving each user's unique\ncharacteristics. Notably, our method is model-agnostic and can be seamlessly\nintegrated into any CDR model. Comprehensive experiments conducted on three\npublic datasets with five CDR baselines demonstrate that our method effectively\nmitigates the CDR non-overlapping user bias, without loss of overall accuracy.\nOur code is publicly available at https://github.com/WeixinChen98/VUG.",
        "url": "http://arxiv.org/abs/2507.17749v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17749v1",
        "arxiv_id": "2507.17749v1",
        "authors": [
            "Weixin Chen",
            "Yuhan Zhao",
            "Li Chen",
            "Weike Pan"
        ],
        "submitted": "2025-07-23 17:59:08",
        "source": "arxiv",
        "comment": "Accepted by RecSys 2025",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on recommender systems, which is a related topic, but it's not directly aligned with your primary interest in Information Retrieval and Search technologies. The paper's emphasis on fairness-aware cross-domain recommender systems and virtual user embeddings is not a central match for your research themes."
    },
    {
        "title": "From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes",
        "abstract": "AI-generated clinical notes are increasingly used in healthcare, but\nevaluating their quality remains a challenge due to high subjectivity and\nlimited scalability of expert review. Existing automated metrics often fail to\nalign with real-world physician preferences. To address this, we propose a\npipeline that systematically distills real user feedback into structured\nchecklists for note evaluation. These checklists are designed to be\ninterpretable, grounded in human feedback, and enforceable by LLM-based\nevaluators. Using deidentified data from over 21,000 clinical encounters,\nprepared in accordance with the HIPAA safe harbor standard, from a deployed AI\nmedical scribe system, we show that our feedback-derived checklist outperforms\nbaseline approaches in our offline evaluations in coverage, diversity, and\npredictive power for human ratings. Extensive experiments confirm the\nchecklist's robustness to quality-degrading perturbations, significant\nalignment with clinician preferences, and practical value as an evaluation\nmethodology. In offline research settings, the checklist can help identify\nnotes likely to fall below our chosen quality thresholds.",
        "url": "http://arxiv.org/abs/2507.17717v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17717v1",
        "arxiv_id": "2507.17717v1",
        "authors": [
            "Karen Zhou",
            "John Giorgi",
            "Pranav Mani",
            "Peng Xu",
            "Davis Liang",
            "Chenhao Tan"
        ],
        "submitted": "2025-07-23 17:28:31",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. While it touches on the topic of evaluation metrics, it is focused on clinical notes and AI-generated content, which is outside the scope of the user's research interests."
    },
    {
        "title": "Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries",
        "abstract": "Negative campaigning is a central feature of political competition, yet\nempirical research has been limited by the high cost and limited scalability of\nexisting classification methods. This study makes two key contributions. First,\nit introduces zero-shot Large Language Models (LLMs) as a novel approach for\ncross-lingual classification of negative campaigning. Using benchmark datasets\nin ten languages, we demonstrate that LLMs achieve performance on par with\nnative-speaking human coders and outperform conventional supervised machine\nlearning approaches. Second, we leverage this novel method to conduct the\nlargest cross-national study of negative campaigning to date, analyzing 18\nmillion tweets posted by parliamentarians in 19 European countries between 2017\nand 2022. The results reveal consistent cross-national patterns: governing\nparties are less likely to use negative messaging, while ideologically extreme\nand populist parties -- particularly those on the radical right -- engage in\nsignificantly higher levels of negativity. These findings advance our\nunderstanding of how party-level characteristics shape strategic communication\nin multiparty systems. More broadly, the study demonstrates the potential of\nLLMs to enable scalable, transparent, and replicable research in political\ncommunication across linguistic and cultural contexts.",
        "url": "http://arxiv.org/abs/2507.17636v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17636v1",
        "arxiv_id": "2507.17636v1",
        "authors": [
            "Victor Hartman",
            "Petter Törnberg"
        ],
        "submitted": "2025-07-23 16:02:52",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on using Large Language Models (LLMs) for cross-lingual classification of negative campaigning in tweets, which is related to Natural Language Processing (NLP) and information retrieval. However, the topic is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling, and the paper's focus on political communication and campaign analysis is not a central match."
    },
    {
        "title": "MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs",
        "abstract": "Although recent Large Language Models (LLMs) have shown rapid improvement on\nreasoning benchmarks in English, the evaluation of such LLMs' multilingual\nreasoning capability across diverse languages and cultural contexts remains\nlimited. Existing multilingual reasoning benchmarks are typically constructed\nby translating existing English reasoning benchmarks, biasing these benchmarks\ntowards reasoning problems with context in English language/cultures. In this\nwork, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a\nbenchmark designed to assess LLMs on more than 1,000 native, linguistic and\nculturally grounded reasoning questions written by native speakers in French,\nSpanish, and Chinese. MultiNRC covers four core reasoning categories:\nlanguage-specific linguistic reasoning, wordplay & riddles, cultural/tradition\nreasoning, and math reasoning with cultural relevance. For cultural/tradition\nreasoning and math reasoning with cultural relevance, we also provide English\nequivalent translations of the multilingual questions by manual translation\nfrom native speakers fluent in English. This set of English equivalents can\nprovide a direct comparison of LLM reasoning capacity in other languages vs.\nEnglish on the same reasoning questions. We systematically evaluate current 14\nleading LLMs covering most LLM families on MultiNRC and its English equivalent\nset. The results show that (1) current LLMs are still not good at native\nmultilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs\nexhibit distinct strengths and weaknesses in handling linguistic, cultural, and\nlogical reasoning tasks; (3) Most models perform substantially better in math\nreasoning in English compared to in original languages (+10%), indicating\npersistent challenges with culturally grounded knowledge.",
        "url": "http://arxiv.org/abs/2507.17476v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17476v1",
        "arxiv_id": "2507.17476v1",
        "authors": [
            "Alexander R. Fabbri",
            "Diego Mares",
            "Jorge Flores",
            "Meher Mankikar",
            "Ernesto Hernandez",
            "Dean Lee",
            "Bing Liu",
            "Chen Xing"
        ],
        "submitted": "2025-07-23 12:56:31",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on Large Language Models (LLMs) and multilingual reasoning, which is outside your primary focus area."
    },
    {
        "title": "Enhancing Transferability and Consistency in Cross-Domain Recommendations via Supervised Disentanglement",
        "abstract": "Cross-domain recommendation (CDR) aims to alleviate the data sparsity by\ntransferring knowledge across domains. Disentangled representation learning\nprovides an effective solution to model complex user preferences by separating\nintra-domain features (domain-shared and domain-specific features), thereby\nenhancing robustness and interpretability. However, disentanglement-based CDR\nmethods employing generative modeling or GNNs with contrastive objectives face\ntwo key challenges: (i) pre-separation strategies decouple features before\nextracting collaborative signals, disrupting intra-domain interactions and\nintroducing noise; (ii) unsupervised disentanglement objectives lack explicit\ntask-specific guidance, resulting in limited consistency and suboptimal\nalignment. To address these challenges, we propose DGCDR, a GNN-enhanced\nencoder-decoder framework. To handle challenge (i), DGCDR first applies GNN to\nextract high-order collaborative signals, providing enriched representations as\na robust foundation for disentanglement. The encoder then dynamically\ndisentangles features into domain-shared and -specific spaces, preserving\ncollaborative information during the separation process. To handle challenge\n(ii), the decoder introduces an anchor-based supervision that leverages\nhierarchical feature relationships to enhance intra-domain consistency and\ncross-domain alignment. Extensive experiments on real-world datasets\ndemonstrate that DGCDR achieves state-of-the-art performance, with improvements\nof up to 11.59% across key metrics. Qualitative analyses further validate its\nsuperior disentanglement quality and transferability. Our source code and\ndatasets are available on GitHub for further comparison.",
        "url": "http://arxiv.org/abs/2507.17112v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17112v1",
        "arxiv_id": "2507.17112v1",
        "authors": [
            "Yuhan Wang",
            "Qing Xie",
            "Zhifeng Bao",
            "Mengzi Tang",
            "Lin Li",
            "Yongjian Liu"
        ],
        "submitted": "2025-07-23 01:29:45",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on cross-domain recommendations, disentangled representation learning, and GNN-enhanced encoder-decoder frameworks, which are not directly related to the user's primary research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling. While the paper touches on some related concepts like collaborative filtering, it does not address the user's core research themes."
    },
    {
        "title": "Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings",
        "abstract": "Efficient text embedding is crucial for large-scale natural language\nprocessing (NLP) applications, where storage and computational efficiency are\nkey concerns. In this paper, we explore how using binary representations\n(barcodes) instead of real-valued features can be used for NLP embeddings\nderived from machine learning models such as BERT. Thresholding is a common\nmethod for converting continuous embeddings into binary representations, often\nusing a fixed threshold across all features. We propose a Coordinate\nSearch-based optimization framework that instead identifies the optimal\nthreshold for each feature, demonstrating that feature-specific thresholds lead\nto improved performance in binary encoding. This ensures that the binary\nrepresentations are both accurate and efficient, enhancing performance across\nvarious features. Our optimal barcode representations have shown promising\nresults in various NLP applications, demonstrating their potential to transform\ntext representation. We conducted extensive experiments and statistical tests\non different NLP tasks and datasets to evaluate our approach and compare it to\nother thresholding methods. Binary embeddings generated using using optimal\nthresholds found by our method outperform traditional binarization methods in\naccuracy. This technique for generating binary representations is versatile and\ncan be applied to any features, not just limited to NLP embeddings, making it\nuseful for a wide range of domains in machine learning applications.",
        "url": "http://arxiv.org/abs/2507.17025v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17025v1",
        "arxiv_id": "2507.17025v1",
        "authors": [
            "Soumen Sinha",
            "Shahryar Rahnamayan",
            "Azam Asilian Bidgoli"
        ],
        "submitted": "2025-07-22 21:29:34",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on binary representation of NLP embeddings, which is not directly related to information retrieval, search technologies, or query understanding. Although it mentions NLP, the topic is more focused on efficient text embedding and binary encoding, which is not a central match for the user's research interests."
    },
    {
        "title": "Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors",
        "abstract": "Suicide remains a pressing global health crisis, with over 720,000 deaths\nannually and millions more affected by suicide ideation (SI) and suicide\nattempts (SA). Early identification of suicidality-related factors (SrFs),\nincluding SI, SA, exposure to suicide (ES), and non-suicidal self-injury\n(NSSI), is critical for timely intervention. While prior studies have applied\nAI to detect SrFs in clinical notes, most treat suicidality as a binary\nclassification task, overlooking the complexity of cooccurring risk factors.\nThis study explores the use of generative large language models (LLMs),\nspecifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs\nfrom psychiatric electronic health records (EHRs). We present a novel end to\nend generative MLC pipeline and introduce advanced evaluation methods,\nincluding label set level metrics and a multilabel confusion matrix for error\nanalysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match\naccuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior\nperformance across label sets, including rare or minority label sets,\nindicating a more balanced and robust performance. Our findings reveal\nsystematic error patterns, such as the conflation of SI and SA, and highlight\nthe models tendency toward cautious over labeling. This work not only\ndemonstrates the feasibility of using generative AI for complex clinical\nclassification tasks but also provides a blueprint for structuring unstructured\nEHR data to support large scale clinical research and evidence based medicine.",
        "url": "http://arxiv.org/abs/2507.17009v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17009v1",
        "arxiv_id": "2507.17009v1",
        "authors": [
            "Ming Huang",
            "Zehan Li",
            "Yan Hu",
            "Wanjing Wang",
            "Andrew Wen",
            "Scott Lane",
            "Salih Selek",
            "Lokesh Shahani",
            "Rodrigo Machado-Vieira",
            "Jair Soares",
            "Hua Xu",
            "Hongfang Liu"
        ],
        "submitted": "2025-07-22 20:44:44",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on applying generative AI models for multi-label classification in healthcare, which is outside your primary focus areas."
    },
    {
        "title": "Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks",
        "abstract": "Large Language Models (LLMs) can exhibit latent biases towards specific\nnationalities even when explicit demographic markers are not present. In this\nwork, we introduce a novel name-based benchmarking approach derived from the\nBias Benchmark for QA (BBQ) dataset to investigate the impact of substituting\nexplicit nationality labels with culturally indicative names, a scenario more\nreflective of real-world LLM applications. Our novel approach examines how this\nsubstitution affects both bias magnitude and accuracy across a spectrum of LLMs\nfrom industry leaders such as OpenAI, Google, and Anthropic. Our experiments\nshow that small models are less accurate and exhibit more bias compared to\ntheir larger counterparts. For instance, on our name-based dataset and in the\nambiguous context (where the correct choice is not revealed), Claude Haiku\nexhibited the worst stereotypical bias scores of 9%, compared to only 3.5% for\nits larger counterpart, Claude Sonnet, where the latter also outperformed it by\n117.7% in accuracy. Additionally, we find that small models retain a larger\nportion of existing errors in these ambiguous contexts. For example, after\nsubstituting names for explicit nationality references, GPT-4o retains 68% of\nthe error rate versus 76% for GPT-4o-mini, with similar findings for other\nmodel providers, in the ambiguous context. Our research highlights the stubborn\nresilience of biases in LLMs, underscoring their profound implications for the\ndevelopment and deployment of AI systems in diverse, global contexts.",
        "url": "http://arxiv.org/abs/2507.16989v1",
        "pdf_url": "http://arxiv.org/pdf/2507.16989v1",
        "arxiv_id": "2507.16989v1",
        "authors": [
            "Giulio Pelosio",
            "Devesh Batra",
            "Noémie Bovey",
            "Robert Hankache",
            "Cristovao Iglesias",
            "Greig Cowan",
            "Raad Khraishi"
        ],
        "submitted": "2025-07-22 19:54:49",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on evaluating biases in Large Language Models (LLMs) using name-based benchmarks, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on the topic of AI systems, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are core areas of interest."
    },
    {
        "title": "Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models",
        "abstract": "Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large\nLanguage Models (LLMs) efficiently by decoupling total parameters from\ncomputational cost. However, this decoupling creates a critical challenge:\npredicting the model capacity of a given MoE configurations (e.g., expert\nactivation ratio and granularity) remains an unresolved problem. To address\nthis gap, we introduce Efficiency Leverage (EL), a metric quantifying the\ncomputational advantage of an MoE model over a dense equivalent. We conduct a\nlarge-scale empirical study, training over 300 models up to 28B parameters, to\nsystematically investigate the relationship between MoE architectural\nconfigurations and EL. Our findings reveal that EL is primarily driven by the\nexpert activation ratio and the total compute budget, both following\npredictable power laws, while expert granularity acts as a non-linear modulator\nwith a clear optimal range. We integrate these discoveries into a unified\nscaling law that accurately predicts the EL of an MoE architecture based on its\nconfiguration. To validate our derived scaling laws, we designed and trained\nLing-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active\nparameters, alongside a 6.1B dense model for comparison. When trained on an\nidentical 1T high-quality token dataset, Ling-mini-beta matched the performance\nof the 6.1B dense model while consuming over 7x fewer computational resources,\nthereby confirming the accuracy of our scaling laws. This work provides a\nprincipled and empirically-grounded foundation for the scaling of efficient MoE\nmodels.",
        "url": "http://arxiv.org/abs/2507.17702v2",
        "pdf_url": "http://arxiv.org/pdf/2507.17702v2",
        "arxiv_id": "2507.17702v2",
        "authors": [
            "Changxin Tian",
            "Kunlong Chen",
            "Jia Liu",
            "Ziqi Liu",
            "Zhiqiang Zhang",
            "Jun Zhou"
        ],
        "submitted": "2025-07-23 17:10:23",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus is on scaling language models using Mixture-of-Experts architecture, which is a topic in Natural Language Processing, but not a core interest of yours. The paper's relevance to your research is limited."
    },
    {
        "title": "WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training",
        "abstract": "Recent advances in learning rate (LR) scheduling have demonstrated the\neffectiveness of decay-free approaches that eliminate the traditional decay\nphase while maintaining competitive performance. Model merging techniques have\nemerged as particularly promising solutions in this domain. We present\nWarmup-Stable and Merge (WSM), a general framework that establishes a formal\nconnection between learning rate decay and model merging. WSM provides a\nunified theoretical foundation for emulating various decay strategies-including\ncosine decay, linear decay and inverse square root decay-as principled model\naveraging schemes, while remaining fully compatible with diverse optimization\nmethods. Through extensive experiments, we identify merge duration-the training\nwindow for checkpoint aggregation-as the most critical factor influencing model\nperformance, surpassing the importance of both checkpoint interval and merge\nquantity. Our framework consistently outperforms the widely-adopted\nWarmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving\nsignificant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on\nMMLU-Pro. The performance advantages extend to supervised fine-tuning\nscenarios, highlighting WSM's potential for long-term model refinement.",
        "url": "http://arxiv.org/abs/2507.17634v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17634v1",
        "arxiv_id": "2507.17634v1",
        "authors": [
            "Changxin Tian",
            "Jiapeng Wang",
            "Qian Zhao",
            "Kunlong Chen",
            "Jia Liu",
            "Ziqi Liu",
            "Jiaxin Mao",
            "Wayne Xin Zhao",
            "Zhiqiang Zhang",
            "Jun Zhou"
        ],
        "submitted": "2025-07-23 16:02:06",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on learning rate scheduling and model merging for pre-training large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on optimization methods, the topic is more relevant to natural language processing and deep learning, but lacks the specific focus on user behavior modeling or real-time relevance optimization that is central to your research interests."
    },
    {
        "title": "Dual-branch Prompting for Multimodal Machine Translation",
        "abstract": "Multimodal Machine Translation (MMT) typically enhances text-only translation\nby incorporating aligned visual features. Despite the remarkable progress,\nstate-of-the-art MMT approaches often rely on paired image-text inputs at\ninference and are sensitive to irrelevant visual noise, which limits their\nrobustness and practical applicability. To address these issues, we propose\nD2P-MMT, a diffusion-based dual-branch prompting framework for robust\nvision-guided translation. Specifically, D2P-MMT requires only the source text\nand a reconstructed image generated by a pre-trained diffusion model, which\nnaturally filters out distracting visual details while preserving semantic\ncues. During training, the model jointly learns from both authentic and\nreconstructed images using a dual-branch prompting strategy, encouraging rich\ncross-modal interactions. To bridge the modality gap and mitigate\ntraining-inference discrepancies, we introduce a distributional alignment loss\nthat enforces consistency between the output distributions of the two branches.\nExtensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves\nsuperior translation performance compared to existing state-of-the-art\napproaches.",
        "url": "http://arxiv.org/abs/2507.17588v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17588v1",
        "arxiv_id": "2507.17588v1",
        "authors": [
            "Jie Wang",
            "Zhendong Yang",
            "Liansong Zong",
            "Xiaobo Zhang",
            "Dexian Wang",
            "Ji Zhang"
        ],
        "submitted": "2025-07-23 15:22:51",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Multimodal Machine Translation, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it involves machine learning and natural language processing, the topic is not aligned with the user's primary research interests."
    },
    {
        "title": "URPO: A Unified Reward & Policy Optimization Framework for Large Language Models",
        "abstract": "Large-scale alignment pipelines typically pair a policy model with a\nseparately trained reward model whose parameters remain frozen during\nreinforcement learning (RL). This separation creates a complex,\nresource-intensive pipeline and suffers from a performance ceiling due to a\nstatic reward signal. We propose a novel framework, Unified Reward & Policy\nOptimization (URPO), that unifies instruction-following (\"player\") and reward\nmodeling (\"referee\") within a single model and a single training phase. Our\nmethod recasts all alignment data-including preference pairs, verifiable\nreasoning, and open-ended instructions-into a unified generative format\noptimized by a single Group-Relative Policy Optimization (GRPO) loop. This\nenables the model to learn from ground-truth preferences and verifiable logic\nwhile simultaneously generating its own rewards for open-ended tasks.\nExperiments on the Qwen2.5-7B model demonstrate URPO's superiority. Our unified\nmodel significantly outperforms a strong baseline using a separate generative\nreward model, boosting the instruction-following score on AlpacaEval from 42.24\nto 44.84 and the composite reasoning average from 32.66 to 35.66. Furthermore,\nURPO cultivates a superior internal evaluator as a byproduct of training,\nachieving a RewardBench score of 85.15 and surpassing the dedicated reward\nmodel it replaces (83.55). By eliminating the need for a separate reward model\nand fostering a co-evolutionary dynamic between generation and evaluation, URPO\npresents a simpler, more efficient, and more effective path towards robustly\naligned language models.",
        "url": "http://arxiv.org/abs/2507.17515v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17515v1",
        "arxiv_id": "2507.17515v1",
        "authors": [
            "Songshuo Lu",
            "Hua Wang",
            "Zhi Chen",
            "Yaohua Tang"
        ],
        "submitted": "2025-07-23 13:52:27",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a framework for optimizing large language models, focusing on reward and policy optimization. While it touches on topics related to search and ranking, such as optimization and evaluation, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user. The paper's focus on language models and alignment pipelines is not directly relevant to the user's research themes."
    },
    {
        "title": "TransLPRNet: Lite Vision-Language Network for Single/Dual-line Chinese License Plate Recognition",
        "abstract": "License plate recognition in open environments is widely applicable across\nvarious domains; however, the diversity of license plate types and imaging\nconditions presents significant challenges. To address the limitations\nencountered by CNN and CRNN-based approaches in license plate recognition, this\npaper proposes a unified solution that integrates a lightweight visual encoder\nwith a text decoder, within a pre-training framework tailored for single and\ndouble-line Chinese license plates. To mitigate the scarcity of double-line\nlicense plate datasets, we constructed a single/double-line license plate\ndataset by synthesizing images, applying texture mapping onto real scenes, and\nblending them with authentic license plate images. Furthermore, to enhance the\nsystem's recognition accuracy, we introduce a perspective correction network\n(PTN) that employs license plate corner coordinate regression as an implicit\nvariable, supervised by license plate view classification information. This\nnetwork offers improved stability, interpretability, and low annotation costs.\nThe proposed algorithm achieves an average recognition accuracy of 99.34% on\nthe corrected CCPD test set under coarse localization disturbance. When\nevaluated under fine localization disturbance, the accuracy further improves to\n99.58%. On the double-line license plate test set, it achieves an average\nrecognition accuracy of 98.70%, with processing speeds reaching up to 167\nframes per second, indicating strong practical applicability.",
        "url": "http://arxiv.org/abs/2507.17335v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17335v1",
        "arxiv_id": "2507.17335v1",
        "authors": [
            "Guangzhu Xu",
            "Zhi Ke",
            "Pengcheng Zuo",
            "Bangjun Lei"
        ],
        "submitted": "2025-07-23 09:03:01",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, Natural Language Processing, data mining, or related topics. The paper focuses on license plate recognition, which is a specific application in computer vision, and does not address any of the topics you mentioned."
    },
    {
        "title": "R4ec: A Reasoning, Reflection, and Refinement Framework for Recommendation Systems",
        "abstract": "Harnessing Large Language Models (LLMs) for recommendation systems has\nemerged as a prominent avenue, drawing substantial research interest. However,\nexisting approaches primarily involve basic prompt techniques for knowledge\nacquisition, which resemble System-1 thinking. This makes these methods highly\nsensitive to errors in the reasoning path, where even a small mistake can lead\nto an incorrect inference. To this end, in this paper, we propose $R^{4}$ec, a\nreasoning, reflection and refinement framework that evolves the recommendation\nsystem into a weak System-2 model. Specifically, we introduce two models: an\nactor model that engages in reasoning, and a reflection model that judges these\nresponses and provides valuable feedback. Then the actor model will refine its\nresponse based on the feedback, ultimately leading to improved responses. We\nemploy an iterative reflection and refinement process, enabling LLMs to\nfacilitate slow and deliberate System-2-like thinking. Ultimately, the final\nrefined knowledge will be incorporated into a recommendation backbone for\nprediction. We conduct extensive experiments on Amazon-Book and MovieLens-1M\ndatasets to demonstrate the superiority of $R^{4}$ec. We also deploy $R^{4}$ec\non a large scale online advertising platform, showing 2.2\\% increase of\nrevenue. Furthermore, we investigate the scaling properties of the actor model\nand reflection model.",
        "url": "http://arxiv.org/abs/2507.17249v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17249v1",
        "arxiv_id": "2507.17249v1",
        "authors": [
            "Hao Gu",
            "Rui Zhong",
            "Yu Xia",
            "Wei Yang",
            "Chi Lu",
            "Peng Jiang",
            "Kun Gai"
        ],
        "submitted": "2025-07-23 06:36:49",
        "source": "arxiv",
        "comment": "Accepted by Recsys25",
        "score": 2,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a framework for recommendation systems that leverages large language models, but its focus on reasoning, reflection, and refinement is not directly related to information retrieval or search technologies. While it touches on the use of language models, the paper's primary concern is improving recommendation systems, which is outside the user's core research themes."
    },
    {
        "title": "Parallelism Meets Adaptiveness: Scalable Documents Understanding in Multi-Agent LLM Systems",
        "abstract": "Large language model (LLM) agents have shown increasing promise for\ncollaborative task completion. However, existing multi-agent frameworks often\nrely on static workflows, fixed roles, and limited inter-agent communication,\nreducing their effectiveness in open-ended, high-complexity domains. This paper\nproposes a coordination framework that enables adaptiveness through three core\nmechanisms: dynamic task routing, bidirectional feedback, and parallel agent\nevaluation. The framework allows agents to reallocate tasks based on confidence\nand workload, exchange structured critiques to iteratively improve outputs, and\ncrucially compete on high-ambiguity subtasks with evaluator-driven selection of\nthe most suitable result. We instantiate these principles in a modular\narchitecture and demonstrate substantial improvements in factual coverage,\ncoherence, and efficiency over static and partially adaptive baselines. Our\nfindings highlight the benefits of incorporating both adaptiveness and\nstructured competition in multi-agent LLM systems.",
        "url": "http://arxiv.org/abs/2507.17061v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17061v1",
        "arxiv_id": "2507.17061v1",
        "authors": [
            "Chengxuan Xia",
            "Qianye Wu",
            "Sixuan Tian",
            "Yilun Hao"
        ],
        "submitted": "2025-07-22 22:42:51",
        "source": "arxiv",
        "comment": "8 pages, 2 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a coordination framework for multi-agent large language model systems, focusing on adaptiveness and parallelism. While it touches on the idea of dynamic task routing and feedback, it does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval and Search technologies."
    },
    {
        "title": "AI-based Clinical Decision Support for Primary Care: A Real-World Study",
        "abstract": "We evaluate the impact of large language model-based clinical decision\nsupport in live care. In partnership with Penda Health, a network of primary\ncare clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a\nsafety net for clinicians by identifying potential documentation and clinical\ndecision-making errors. AI Consult integrates into clinician workflows,\nactivating only when needed and preserving clinician autonomy. We conducted a\nquality improvement study, comparing outcomes for 39,849 patient visits\nperformed by clinicians with or without access to AI Consult across 15 clinics.\nVisits were rated by independent physicians to identify clinical errors.\nClinicians with access to AI Consult made relatively fewer errors: 16% fewer\ndiagnostic errors and 13% fewer treatment errors. In absolute terms, the\nintroduction of AI Consult would avert diagnostic errors in 22,000 visits and\ntreatment errors in 29,000 visits annually at Penda alone. In a survey of\nclinicians with AI Consult, all clinicians said that AI Consult improved the\nquality of care they delivered, with 75% saying the effect was \"substantial\".\nThese results required a clinical workflow-aligned AI Consult implementation\nand active deployment to encourage clinician uptake. We hope this study\ndemonstrates the potential for LLM-based clinical decision support tools to\nreduce errors in real-world settings and provides a practical framework for\nadvancing responsible adoption.",
        "url": "http://arxiv.org/abs/2507.16947v1",
        "pdf_url": "http://arxiv.org/pdf/2507.16947v1",
        "arxiv_id": "2507.16947v1",
        "authors": [
            "Robert Korom",
            "Sarah Kiptinness",
            "Najib Adan",
            "Kassim Said",
            "Catherine Ithuli",
            "Oliver Rotich",
            "Boniface Kimani",
            "Irene King'ori",
            "Stellah Kamau",
            "Elizabeth Atemba",
            "Muna Aden",
            "Preston Bowman",
            "Michael Sharman",
            "Rebecca Soskin Hicks",
            "Rebecca Distler",
            "Johannes Heidecke",
            "Rahul K. Arora",
            "Karan Singhal"
        ],
        "submitted": "2025-07-22 18:37:33",
        "source": "arxiv",
        "comment": "Blog: https://openai.com/index/ai-clinical-copilot-penda-health/",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is not relevant to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on clinical decision support, large language models, and clinical workflow alignment, which are outside the user's primary research areas."
    },
    {
        "title": "A Unifying Scheme for Extractive Content Selection Tasks",
        "abstract": "A broad range of NLP tasks involve selecting relevant text spans from given\nsource texts. Despite this shared objective, such \\textit{content selection}\ntasks have traditionally been studied in isolation, each with its own modeling\napproaches, datasets, and evaluation metrics. In this work, we propose\n\\textit{instruction-guided content selection (IGCS)} as a beneficial unified\nframework for such settings, where the task definition and any\ninstance-specific request are encapsulated as instructions to a language model.\nTo promote this framework, we introduce \\igcsbench{}, the first unified\nbenchmark covering diverse content selection tasks. Further, we create a large\ngeneric synthetic dataset that can be leveraged for diverse content selection\ntasks, and show that transfer learning with these datasets often boosts\nperformance, whether dedicated training for the targeted task is available or\nnot. Finally, we address generic inference time issues that arise in LLM-based\nmodeling of content selection, assess a generic evaluation metric, and overall\npropose the utility of our resources and methods for future content selection\nmodels. Models and datasets available at https://github.com/shmuelamar/igcs.",
        "url": "http://arxiv.org/abs/2507.16922v1",
        "pdf_url": "http://arxiv.org/pdf/2507.16922v1",
        "arxiv_id": "2507.16922v1",
        "authors": [
            "Shmuel Amar",
            "Ori Shapira",
            "Aviv Slobodkin",
            "Ido Dagan"
        ],
        "submitted": "2025-07-22 18:02:54",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a unified framework for content selection tasks, which is related to Information Retrieval and Natural Language Processing. However, the focus is on extractive content selection, which is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's emphasis on language models and transfer learning is somewhat relevant, but the connection to the user's research themes is limited."
    },
    {
        "title": "AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer",
        "abstract": "With the rise of voice-enabled artificial intelligence (AI) systems,\nquantitative survey researchers have access to a new data-collection mode: AI\ntelephone surveying. By using AI to conduct phone interviews, researchers can\nscale quantitative studies while balancing the dual goals of human-like\ninteractivity and methodological rigor. Unlike earlier efforts that used\ninteractive voice response (IVR) technology to automate these surveys, voice AI\nenables a more natural and adaptive respondent experience as it is more robust\nto interruptions, corrections, and other idiosyncrasies of human speech.\n  We built and tested an AI system to conduct quantitative surveys based on\nlarge language models (LLM), automatic speech recognition (ASR), and speech\nsynthesis technologies. The system was specifically designed for quantitative\nresearch, and strictly adhered to research best practices like question order\nrandomization, answer order randomization, and exact wording.\n  To validate the system's effectiveness, we deployed it to conduct two pilot\nsurveys with the SSRS Opinion Panel and followed-up with a separate\nhuman-administered survey to assess respondent experiences. We measured three\nkey metrics: the survey completion rates, break-off rates, and respondent\nsatisfaction scores. Our results suggest that shorter instruments and more\nresponsive AI interviewers may contribute to improvements across all three\nmetrics studied.",
        "url": "http://arxiv.org/abs/2507.17718v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17718v1",
        "arxiv_id": "2507.17718v1",
        "authors": [
            "Danny D. Leybzon",
            "Shreyas Tirumala",
            "Nishant Jain",
            "Summer Gillen",
            "Michael Jackson",
            "Cameron McPhee",
            "Jennifer Schmidt"
        ],
        "submitted": "2025-07-23 17:30:14",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on AI telephone surveying, which is not directly related to Information Retrieval, Search technologies, or Natural Language Processing. While it mentions language models, the application is in a different domain and does not involve query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)",
        "abstract": "Large language models are computationally expensive due to their deep\nstructures. Prior research has shown that intermediate layers contain\nsufficient information to generate accurate answers, leading to the development\nof early-exit algorithms that reduce inference costs by terminating computation\nat earlier layers. However, these methods often suffer from poor performance\ndue to misalignment between intermediate and output layer representations that\nlead to decoding inaccuracy. To address these challenges, we propose SPADE\n(SPace Alignment DEcoding), a novel decoding method that aligns intermediate\nlayer representations with the output layer by propagating a minimally reduced\nsequence consisting of only the start token and the answer token. We further\noptimize the early-exit decision-making process by training a linear\napproximation of SPADE that computes entropy-based confidence metrics. Putting\nthem together, we create a hybrid early-exit algorithm that monitors confidence\nlevels and stops inference at intermediate layers while using SPADE to generate\nhigh-quality outputs. This approach significantly reduces inference costs\nwithout compromising accuracy, offering a scalable and efficient solution for\ndeploying large language models in real-world applications.",
        "url": "http://arxiv.org/abs/2507.17618v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17618v1",
        "arxiv_id": "2507.17618v1",
        "authors": [
            "Bowen Zheng",
            "Ming Ma",
            "Zhongqiao Lin",
            "Tianming Yang"
        ],
        "submitted": "2025-07-23 15:49:03",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on large language models and decoding methods, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on optimization and efficiency, the context is different from the user's primary research interests."
    },
    {
        "title": "FinGAIA: An End-to-End Benchmark for Evaluating AI Agents in Finance",
        "abstract": "The booming development of AI agents presents unprecedented opportunities for\nautomating complex tasks across various domains. However, their multi-step,\nmulti-tool collaboration capabilities in the financial sector remain\nunderexplored. This paper introduces FinGAIA, an end-to-end benchmark designed\nto evaluate the practical abilities of AI agents in the financial domain.\nFinGAIA comprises 407 meticulously crafted tasks, spanning seven major\nfinancial sub-domains: securities, funds, banking, insurance, futures, trusts,\nand asset management. These tasks are organized into three hierarchical levels\nof scenario depth: basic business analysis, asset decision support, and\nstrategic risk management. We evaluated 10 mainstream AI agents in a zero-shot\nsetting. The best-performing agent, ChatGPT, achieved an overall accuracy of\n48.9\\%, which, while superior to non-professionals, still lags financial\nexperts by over 35 percentage points. Error analysis has revealed five\nrecurring failure patterns: Cross-modal Alignment Deficiency, Financial\nTerminological Bias, Operational Process Awareness Barrier, among others. These\npatterns point to crucial directions for future research. Our work provides the\nfirst agent benchmark closely related to the financial domain, aiming to\nobjectively assess and promote the development of agents in this crucial field.\nPartial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.",
        "url": "http://arxiv.org/abs/2507.17186v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17186v1",
        "arxiv_id": "2507.17186v1",
        "authors": [
            "Lingfeng Zeng",
            "Fangqi Lou",
            "Zixuan Wang",
            "Jiajie Xu",
            "Jinyi Niu",
            "Mengping Li",
            "Yifan Dong",
            "Qi Qi",
            "Wei Zhang",
            "Ziwei Yang",
            "Jun Han",
            "Ruilun Feng",
            "Ruiqi Hu",
            "Lejie Zhang",
            "Zhengbo Feng",
            "Yicheng Ren",
            "Xin Guo",
            "Zhaowei Liu",
            "Dongpo Cheng",
            "Weige Cai",
            "Liwen Zhang"
        ],
        "submitted": "2025-07-23 04:19:16",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. Although it mentions AI agents, the focus is on evaluating their abilities in the financial domain, which is not a primary area of interest for you."
    }
]