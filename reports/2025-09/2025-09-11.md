# Daily Papers Report - 2025-09-11

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question Answering for BioASQ Challenge

- **LLM Score**: 8
- **Keyword Score**: 7
- **Authors**: Dima Galat, Diego Molla-Aliod
- **URL**: <http://arxiv.org/abs/2509.08596v1>
- **Submitted**: 2025-09-10 13:50:49
- **Comment**: CEUR-WS, CLEF2025
- **Topic Keywords**: information retrieval, rag, retrieval
- **Reason**: This paper explores the application of large language models in information retrieval for biomedical question answering, which aligns with your interests in IR and NLP. The focus on ensemble methods and context length also resonates with your research on ranking models and user behavior modeling. However, the specific domain of biomedical question answering is somewhat niche compared to your broader interests in e-commerce and general IR.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Biomedical Question Answering (QA) using Large Language Models (LLMs)
- **Aim**: To develop a state-of-the-art (SOTA) zero-shot QA system for the BioASQ challenge, leveraging LLMs and a multi-stage biomedical information retrieval (IR) pipeline.
- **Rationale**: LLMs have shown promise in QA, but their performance can be enhanced by incorporating precise information retrieval techniques.
- **Ground**: PubMed article titles and abstracts indexed in Elasticsearch using BM25 retrieval.
- **Experiment**: A zero-shot QA ensembling framework using multiple LLM variants (Gemini 2.0 Flash, Gemini 2.5 Flash Preview, Claude 3.7 Sonnet) with tailored prompts for different question types (Yes/No, Factoid, List).
- **Takeaway**: The system achieves SOTA performance on Yes/No QA tasks, highlighting the effectiveness of the proposed IR pipeline and LLM ensembling approach.  Challenges remain in generating structured outputs for List and Factoid questions, particularly with longer contexts. Future research should focus on comprehensive evaluation frameworks, interactive systems, and query rewriting techniques.

#### Abstract
> Biomedical question answering (QA) poses significant challenges due to the
need for precise interpretation of specialized knowledge drawn from a vast,
complex, and rapidly evolving corpus. In this work, we explore how large
language models (LLMs) can be used for information retrieval (IR), and an
ensemble of zero-shot models can accomplish state-of-the-art performance on a
domain-specific Yes/No QA task. Evaluating our approach on the BioASQ challenge
tasks, we show that ensembles can outperform individual LLMs and in some cases
rival or surpass domain-tuned systems - all while preserving generalizability
and avoiding the need for costly fine-tuning or labeled data. Our method
aggregates outputs from multiple LLM variants, including models from Anthropic
and Google, to synthesize more accurate and robust answers. Moreover, our
investigation highlights a relationship between context length and performance:
while expanded contexts are meant to provide valuable evidence, they
simultaneously risk information dilution and model disorientation. These
findings emphasize IR as a critical foundation in Retrieval-Augmented
Generation (RAG) approaches for biomedical QA systems. Precise, focused
retrieval remains essential for ensuring LLMs operate within relevant
information boundaries when generating answers from retrieved documents. Our
results establish that ensemble-based zero-shot approaches, when paired with
effective RAG pipelines, constitute a practical and scalable alternative to
domain-tuned systems for biomedical question answering.

---

### 2. Towards Knowledge-Aware Document Systems: Modeling Semantic Coverage Relations via Answerability Detection

- **LLM Score**: 8
- **Keyword Score**: 4
- **Authors**: Yehudit Aperstein, Alon Gottlib, Gal Benita, Alexander Apartsin
- **URL**: <http://arxiv.org/abs/2509.08304v1>
- **Submitted**: 2025-09-10 06:00:01
- **Comment**: 27 pages, 1 figure
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The paper introduces a novel framework for modeling Semantic Coverage Relations, which can be useful in tasks such as information retrieval and summarization. While the focus is on NLP and QA, the concepts and techniques presented can be applied to your areas of interest.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Semantic Coverage Relations (SCR) in Question Answering (QA)
- **Aim**: To develop a novel framework for modeling SCR between documents and evaluate its effectiveness in enhancing QA systems.
- **Rationale**: Answerability is crucial in QA datasets, and synthetic data generation can improve QA system performance. A dedicated dataset for evaluating SCR is needed to assess the impact of semantic coverage on QA.
- **Ground**: Existing QA datasets vary in their treatment of answerability, highlighting the need for a standardized approach. Synthetic data generation techniques like MLM, prompt-based LLMs, and fine-tuned generative models can enhance QA systems.
- **Experiment**: A synthetic SCR dataset was created using Azure GPT-4.1 for filtering, paraphrasing, and variant generation. Two modeling approaches were explored: generative models (GPT-4.1 and GPT-4.0) and discriminative models (BERT-base, Longformer-base, RoBERTa-base, DistilBERT, Random Forest, Logistic Regression).
- **Takeaway**: Discriminative models outperform generative models for SCR classification. RoBERTa-base achieves the highest accuracy, emphasizing the importance of pre-trained representations. Random Forest demonstrates a good balance across relation types. GPT models struggle, suggesting the need for fine-tuning for specialized tasks.

#### Abstract
> Understanding how information is shared across documents, regardless of the
format in which it is expressed, is critical for tasks such as information
retrieval, summarization, and content alignment. In this work, we introduce a
novel framework for modelling Semantic Coverage Relations (SCR), which
classifies document pairs based on how their informational content aligns. We
define three core relation types: equivalence, where both texts convey the same
information using different textual forms or styles; inclusion, where one
document fully contains the information of another and adds more; and semantic
overlap, where each document presents partially overlapping content. To capture
these relations, we adopt a question answering (QA)-based approach, using the
answerability of shared questions across documents as an indicator of semantic
coverage. We construct a synthetic dataset derived from the SQuAD corpus by
paraphrasing source passages and selectively omitting information, enabling
precise control over content overlap. This dataset allows us to benchmark
generative language models and train transformer-based classifiers for SCR
prediction. Our findings demonstrate that discriminative models significantly
outperform generative approaches, with the RoBERTa-base model achieving the
highest accuracy of 61.4% and the Random Forest-based model showing the best
balance with a macro-F1 score of 52.9%. The results show that QA provides an
effective lens for assessing semantic relations across stylistically diverse
texts, offering insights into the capacity of current models to reason about
information beyond surface similarity. The dataset and code developed in this
study are publicly available to support reproducibility.

---

### 3. Vector embedding of multi-modal texts: a tool for discovery?

- **LLM Score**: 7
- **Keyword Score**: 8
- **Authors**: Beth Plale, Sai Navya Jyesta, Sachith Withana
- **URL**: <http://arxiv.org/abs/2509.08216v1>
- **Submitted**: 2025-09-10 01:14:48
- **Topic Keywords**: information retrieval, queries, retrieval
- **Reason**: This paper explores the application of vector-based multimodal retrieval in digital libraries, leveraging vision-language models to improve discovery across text and images. While primarily focused on digital library discovery, it touches on aspects of information retrieval and relevance optimization, aligning with your interests in IR and NLP. However, the specific focus on digital libraries and multimodal retrieval is somewhat tangential to your core research themes.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Multimodal Retrieval in Digital Libraries
- **Aim**: To evaluate the effectiveness of vector-based multimodal retrieval powered by Vision-Language Models (VLMs) for enhancing discovery in digital libraries containing text and image content.
- **Rationale**: Traditional text-based search methods struggle to capture the semantic richness of multimedia content, leading to limitations in retrieving relevant information from digital libraries.
- **Ground**: A dataset of over 3,600 digitized textbook pages from computer science textbooks and a benchmark of 75 natural language queries categorized into seven types (visual, textual, tabular, numerical, multi-page, conceptual, and total) were used.
- **Experiment**: Different distance metrics (Cosine similarity, Dot Product, Euclidean distance, and Manhattan distance) were evaluated for multimodal retrieval using the VLM ColPali and a vector database (Qdrant). Performance was measured against the benchmark using ground truth manually curated and validated for accuracy and specificity.
- **Takeaway**: Cosine similarity outperformed other distance metrics, demonstrating its effectiveness in retrieving semantically and visually relevant pages. The study highlights the limitations of traditional label-based evaluation and proposes future research directions for improving multimodal retrieval systems.

#### Abstract
> Computer science texts are particularly rich in both narrative content and
illustrative charts, algorithms, images, annotated diagrams, etc. This study
explores the extent to which vector-based multimodal retrieval, powered by
vision-language models (VLMs), can improve discovery across multi-modal (text
and images) content. Using over 3,600 digitized textbook pages largely from
computer science textbooks and a Vision Language Model (VLM), we generate
multi-vector representations capturing both textual and visual semantics. These
embeddings are stored in a vector database. We issue a benchmark of 75 natural
language queries and compare retrieval performance to ground truth and across
four similarity (distance) measures. The study is intended to expose both the
strengths and weakenesses of such an approach. We find that cosine similarity
most effectively retrieves semantically and visually relevant pages. We further
discuss the practicality of using a vector database and multi-modal embedding
for operational information retrieval. Our paper is intended to offer design
insights for discovery over digital libraries.
  Keywords: Vector embedding, multi-modal document retrieval, vector database
benchmark, digital library discovery

---

### 4. Verbalized Algorithms

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Supriya Lall, Christian Farrell, Hari Pathanjaly, Marko Pavic, Sarvesh Chezhian, Masataro Asai
- **URL**: <http://arxiv.org/abs/2509.08150v1>
- **Submitted**: 2025-09-09 21:14:44
- **Comment**: Submitted to NeurIPS 2025 Workshop on Efficient Reasoning
- **Topic Keywords**: query, rag, acl
- **Reason**: The paper explores a novel approach to leveraging Large Language Models (LLMs) in a more controlled and reliable manner, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on classical algorithms and natural language string operations is not directly aligned with the user's primary research interests in deep semantic understanding and real-time relevance optimization.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Verbalized Algorithms (VAs) for Computational Reasoning in LLMs
- **Aim**: To enable Large Language Models (LLMs) to perform complex computational reasoning tasks by leveraging established algorithms and LLMs' text processing capabilities.
- **Rationale**: Directly querying LLMs for tasks like sorting is limited by context length and correctness guarantees. VAs decompose tasks into simpler operations, integrating LLMs as oracles for text-based tasks within classical algorithms.
- **Ground**: VAs are contrasted with formalization-based methods, which struggle with accurately capturing the vagueness of natural language.
- **Experiment**: VAs are evaluated on sorting Amazon product reviews by sentiment, clustering text strings, and ranking text based on criteria.  Baselines include constraint decoding, scoring-based approaches, Jaccard Distance, Sentence Transformer, and triplet comparison queries.
- **Takeaway**: VAs, particularly those using smaller LLMs, outperform baselines in sorting and clustering tasks. They offer theoretical guarantees, simplify LLM tasks, and demonstrate faster processing times. Future research focuses on optimizing performance and analyzing algorithm compatibility with VAs.

#### Abstract
> Instead of querying LLMs in a one-shot manner and hoping to get the right
answer for a reasoning task, we propose a paradigm we call \emph{verbalized
algorithms} (VAs), which leverage classical algorithms with established
theoretical understanding. VAs decompose a task into simple elementary
operations on natural language strings that they should be able to answer
reliably, and limit the scope of LLMs to only those simple tasks. For example,
for sorting a series of natural language strings, \emph{verbalized sorting}
uses an LLM as a binary comparison oracle in a known and well-analyzed sorting
algorithm (e.g., bitonic sorting network). We demonstrate the effectiveness of
this approach on sorting and clustering tasks.

---

### 5. Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation Through Unsupervised Consistency Signals

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Cheng Chen, Haiyan Yin, Ivor Tsang
- **URL**: <http://arxiv.org/abs/2509.08809v1>
- **Submitted**: 2025-09-10 17:42:41
- **Comment**: 11 pages, 10 figures
- **Topic Keywords**: acl
- **Reason**: The paper explores the evaluation of Large Language Models (LLMs) in unsupervised environments, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on LLMs and annotation quality is not directly aligned with the user's primary research interests in IR and Search technologies. The connection to NLP is relevant, but the paper's scope is more narrow than the user's broader interests.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Evaluating Large Language Model (LLM) Annotations in Unsupervised Environments
- **Aim**: To propose a novel, unsupervised method for assessing the quality of annotations generated by LLMs.
- **Rationale**: Traditional LLM evaluation methods rely on oracle feedback, which is unavailable in unsupervised settings. This paper aims to address this gap by leveraging a student model as an unsupervised feedback mechanism.
- **Ground**: The proposed method does not rely on ground truth annotations.
- **Experiment**: The authors evaluated the effectiveness of their method on ten open-domain NLP datasets and four different LLMs, observing a strong positive correlation between the CAI Ratio and LLM accuracy.
- **Takeaway**: The CAI Ratio, a metric measuring agreement between a student model and an LLM, serves as a valuable heuristic for identifying well-performing LLMs in unsupervised settings.

#### Abstract
> Large Language Models (LLMs), when paired with prompt-based tasks, have
significantly reduced data annotation costs and reliance on human annotators.
However, evaluating the quality of their annotations remains challenging in
dynamic, unsupervised environments where oracle feedback is scarce and
conventional methods fail. To address this challenge, we propose a novel
agentic annotation paradigm, where a student model collaborates with a noisy
teacher (the LLM) to assess and refine annotation quality without relying on
oracle feedback. The student model, acting as an unsupervised feedback
mechanism, employs a user preference-based majority voting strategy to evaluate
the consistency of the LLM outputs. To systematically measure the reliability
of LLM-generated annotations, we introduce the Consistent and Inconsistent
(CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not only
quantifies the annotation quality of the noisy teacher under limited user
preferences but also plays a critical role in model selection, enabling the
identification of robust LLMs in dynamic, unsupervised environments. Applied to
ten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates a
strong positive correlation with LLM accuracy, establishing it as an essential
tool for unsupervised evaluation and model selection in real-world settings.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Too Helpful, Too Harmless, Too Honest or Just Right?

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Gautam Siddharth Kashyap, Mark Dras, Usman Naseem
- **URL**: <http://arxiv.org/abs/2509.08486v1>
- **Submitted**: 2025-09-10 10:51:47
- **Comment**: EMNLP'25 Main
- **Topic Keywords**: rag
- **Reason**: The paper is somewhat related to the user's interests in Natural Language Processing (NLP), but it focuses on a specific challenge in aligning Large Language Models with principles of Helpfulness, Harmlessness, and Honesty. While it involves a novel architecture, it does not directly relate to the user's core research themes in Information Retrieval, query understanding, ranking models, or user behavior modeling.

#### Abstract
> Large Language Models (LLMs) exhibit strong performance across a wide range
of NLP tasks, yet aligning their outputs with the principles of Helpfulness,
Harmlessness, and Honesty (HHH) remains a persistent challenge. Existing
methods often optimize for individual alignment dimensions in isolation,
leading to trade-offs and inconsistent behavior. While Mixture-of-Experts (MoE)
architectures offer modularity, they suffer from poorly calibrated routing,
limiting their effectiveness in alignment tasks. We propose TrinityX, a modular
alignment framework that incorporates a Mixture of Calibrated Experts (MoCaE)
within the Transformer architecture. TrinityX leverages separately trained
experts for each HHH dimension, integrating their outputs through a calibrated,
task-adaptive routing mechanism that combines expert signals into a unified,
alignment-aware representation. Extensive experiments on three standard
alignment benchmarks-Alpaca (Helpfulness), BeaverTails (Harmlessness), and
TruthfulQA (Honesty)-demonstrate that TrinityX outperforms strong baselines,
achieving relative improvements of 32.5% in win rate, 33.9% in safety score,
and 28.4% in truthfulness. In addition, TrinityX reduces memory usage and
inference latency by over 40% compared to prior MoE-based approaches. Ablation
studies highlight the importance of calibrated routing, and cross-model
evaluations confirm TrinityX's generalization across diverse LLM backbones.

### 7. A Survey of Reinforcement Learning for Large Reasoning Models

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, Xuekai Zhu, Ermo Hua, Yihao Liu, Zonglin Li, Huayu Chen, Xiaoye Qu, Yafu Li, Weize Chen, Zhenzhao Yuan, Junqi Gao, Dong Li, Zhiyuan Ma, Ganqu Cui, Zhiyuan Liu, Biqing Qi, Ning Ding, Bowen Zhou
- **URL**: <http://arxiv.org/abs/2509.08827v1>
- **Submitted**: 2025-09-10 17:59:43
- **Topic Keywords**: search
- **Reason**: This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and Large Language Models, but it does not directly address query understanding, ranking models, or user behavior modeling in the context of Information Retrieval. The focus on Reinforcement Learning for Large Language Models is tangentially related to the user's background in e-commerce and interests in real-time relevance optimization, but it is not a central match.

#### Abstract
> In this paper, we survey recent advances in Reinforcement Learning (RL) for
reasoning with Large Language Models (LLMs). RL has achieved remarkable success
in advancing the frontier of LLM capabilities, particularly in addressing
complex logical tasks such as mathematics and coding. As a result, RL has
emerged as a foundational methodology for transforming LLMs into LRMs. With the
rapid progress of the field, further scaling of RL for LRMs now faces
foundational challenges not only in computational resources but also in
algorithm design, training data, and infrastructure. To this end, it is timely
to revisit the development of this domain, reassess its trajectory, and explore
strategies to enhance the scalability of RL toward Artificial SuperIntelligence
(ASI). In particular, we examine research applying RL to LLMs and LRMs for
reasoning abilities, especially since the release of DeepSeek-R1, including
foundational components, core problems, training resources, and downstream
applications, to identify future opportunities and directions for this rapidly
evolving area. We hope this review will promote future research on RL for
broader reasoning models. Github:
https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs

### 8. Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Joachim Baumann, Paul R√∂ttger, Aleksandra Urman, Albert Wendsj√∂, Flor Miriam Plaza-del-Arco, Johannes B. Gruber, Dirk Hovy
- **URL**: <http://arxiv.org/abs/2509.08825v1>
- **Submitted**: 2025-09-10 17:58:53
- **Topic Keywords**: search
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and Large Language Models (LLMs), but it focuses on the risks and limitations of using LLMs for text annotation, which is not a central match to your primary focus on Information Retrieval and query understanding.

#### Abstract
> Large language models (LLMs) are rapidly transforming social science research
by enabling the automation of labor-intensive tasks like data annotation and
text analysis. However, LLM outputs vary significantly depending on the
implementation choices made by researchers (e.g., model selection, prompting
strategy, or temperature settings). Such variation can introduce systematic
biases and random errors, which propagate to downstream analyses and cause Type
I, Type II, Type S, or Type M errors. We call this LLM hacking.
  We quantify the risk of LLM hacking by replicating 37 data annotation tasks
from 21 published social science research studies with 18 different models.
Analyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure
how plausible researcher choices affect statistical conclusions. We find
incorrect conclusions based on LLM-annotated data in approximately one in three
hypotheses for state-of-the-art models, and in half the hypotheses for small
language models. While our findings show that higher task performance and
better general model capabilities reduce LLM hacking risk, even highly accurate
models do not completely eliminate it. The risk of LLM hacking decreases as
effect sizes increase, indicating the need for more rigorous verification of
findings near significance thresholds. Our extensive analysis of LLM hacking
mitigation techniques emphasizes the importance of human annotations in
reducing false positive findings and improving model selection. Surprisingly,
common regression estimator correction techniques are largely ineffective in
reducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.
  Beyond accidental errors, we find that intentional LLM hacking is
unacceptably simple. With few LLMs and just a handful of prompt paraphrases,
anything can be presented as statistically significant.

### 9. Scaling Truth: The Confidence Paradox in AI Fact-Checking

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Ihsan A. Qazi, Zohaib Khan, Abdullah Ghani, Agha A. Raza, Zafar A. Qazi, Wassay Sajjad, Ayesha Ali, Asher Javaid, Muhammad Abdullah Sohail, Abdul H. Azeemi
- **URL**: <http://arxiv.org/abs/2509.08803v1>
- **Submitted**: 2025-09-10 17:36:25
- **Comment**: 65 pages, 26 figures, 6 tables
- **Topic Keywords**: search
- **Reason**: This paper is somewhat related to Information Retrieval and Search technologies, but its focus on AI fact-checking and large language models is more aligned with Natural Language Processing. While it touches on the theme of deep semantic understanding, its primary contribution is in the area of fact-checking, which is not a central match for the user's research interests.

#### Abstract
> The rise of misinformation underscores the need for scalable and reliable
fact-checking solutions. Large language models (LLMs) hold promise in
automating fact verification, yet their effectiveness across global contexts
remains uncertain. We systematically evaluate nine established LLMs across
multiple categories (open/closed-source, multiple sizes, diverse architectures,
reasoning-based) using 5,000 claims previously assessed by 174 professional
fact-checking organizations across 47 languages. Our methodology tests model
generalizability on claims postdating training cutoffs and four prompting
strategies mirroring both citizen and professional fact-checker interactions,
with over 240,000 human annotations as ground truth. Findings reveal a
concerning pattern resembling the Dunning-Kruger effect: smaller, accessible
models show high confidence despite lower accuracy, while larger models
demonstrate higher accuracy but lower confidence. This risks systemic bias in
information verification, as resource-constrained organizations typically use
smaller models. Performance gaps are most pronounced for non-English languages
and claims originating from the Global South, threatening to widen existing
information inequalities. These results establish a multilingual benchmark for
future research and provide an evidence base for policy aimed at ensuring
equitable access to trustworthy, AI-assisted fact-checking.

### 10. Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Yu Cheng Chih, Yong Hao Hou
- **URL**: <http://arxiv.org/abs/2509.08381v1>
- **Submitted**: 2025-09-10 08:19:07
- **Comment**: 13 pages, 8 figures, includes experiments on JSON extraction,
  knowledge graph extraction, and NER
- **Topic Keywords**: rank
- **Reason**: This paper focuses on low-resource fine-tuning of large language models for structured information extraction, which is somewhat related to information retrieval and search technologies. However, the primary focus on instruction-tuning and low-resource settings does not directly align with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's emphasis on cost-effective and reliable information extraction pipelines in resource-constrained environments is also somewhat relevant, but not a central match.

#### Abstract
> Deploying large language models (LLMs) for structured data extraction in
domains such as financial compliance reporting, legal document analytics, and
multilingual knowledge base construction is often impractical for smaller teams
due to the high cost of running large architectures and the difficulty of
preparing large, high-quality datasets. Most recent instruction-tuning studies
focus on seven-billion-parameter or larger models, leaving limited evidence on
whether much smaller models can work reliably under low-resource, multi-task
conditions. This work presents ETLCH, a billion-parameter LLaMA-based model
fine-tuned with low-rank adaptation on only a few hundred to one thousand
samples per task for JSON extraction, knowledge graph extraction, and named
entity recognition. Despite its small scale, ETLCH outperforms strong baselines
across most evaluation metrics, with substantial gains observed even at the
lowest data scale. These findings demonstrate that well-tuned small models can
deliver stable and accurate structured outputs at a fraction of the
computational cost, enabling cost-effective and reliable information extraction
pipelines in resource-constrained environments.

### 11. No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Flor Miriam Plaza-del-Arco, Paul R√∂ttger, Nino Scherrer, Emanuele Borgonovo, Elmar Plischke, Dirk Hovy
- **URL**: <http://arxiv.org/abs/2509.08075v1>
- **Submitted**: 2025-09-09 18:30:01
- **Topic Keywords**: personalization
- **Reason**: This paper explores the impact of persona prompts on language models, specifically false refusal rates. While it touches on aspects of model behavior and bias, it is primarily focused on NLP and does not directly relate to the user's core research interests in Information Retrieval, Search technologies, and query understanding.

#### Abstract
> Large language models (LLMs) are increasingly integrated into our daily lives
and personalized. However, LLM personalization might also increase unintended
side effects. Recent work suggests that persona prompting can lead models to
falsely refuse user requests. However, no work has fully quantified the extent
of this issue. To address this gap, we measure the impact of 15
sociodemographic personas (based on gender, race, religion, and disability) on
false refusal. To control for other factors, we also test 16 different models,
3 tasks (Natural Language Inference, politeness, and offensiveness
classification), and nine prompt paraphrases. We propose a Monte Carlo-based
method to quantify this issue in a sample-efficient manner. Our results show
that as models become more capable, personas impact the refusal rate less and
less. Certain sociodemographic personas increase false refusal in some models,
which suggests underlying biases in the alignment strategies or safety
mechanisms. However, we find that the model choice and task significantly
influence false refusals, especially in sensitive content tasks. Our findings
suggest that persona effects have been overestimated, and might be due to other
factors.

### 12. HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Benjamin Sturgeon, Daniel Samuelson, Jacob Haimes, Jacy Reese Anthis
- **URL**: <http://arxiv.org/abs/2509.08494v1>
- **Submitted**: 2025-09-10 11:10:10
- **Topic Keywords**: queries, rag
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it touches on the use of large language models, its focus on human agency support in AI assistants and safety alignment targets is not a central match for your core research themes.

#### Abstract
> As humans delegate more tasks and decisions to artificial intelligence (AI),
we risk losing control of our individual and collective futures. Relatively
simple algorithmic systems already steer human decision-making, such as social
media feed algorithms that lead people to unintentionally and absent-mindedly
scroll through engagement-optimized content. In this paper, we develop the idea
of human agency by integrating philosophical and scientific theories of agency
with AI-assisted evaluation methods: using large language models (LLMs) to
simulate and validate user queries and to evaluate AI responses. We develop
HumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions
of human agency based on typical AI use cases. HAB measures the tendency of an
AI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation,
Correct Misinformation, Defer Important Decisions, Encourage Learning, and
Maintain Social Boundaries. We find low-to-moderate agency support in
contemporary LLM-based assistants and substantial variation across system
developers and dimensions. For example, while Anthropic LLMs most support human
agency overall, they are the least supportive LLMs in terms of Avoid Value
Manipulation. Agency support does not appear to consistently result from
increasing LLM capabilities or instruction-following behavior (e.g., RLHF), and
we encourage a shift towards more robust safety and alignment targets.

### 13. Soundtracks of Our Lives: How Age Influences Musical Preferences

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Arsen Matej Golubovikj, Bruce Ferwerda, Alan Said, Marko Talƒçiƒç
- **URL**: <http://arxiv.org/abs/2509.08337v1>
- **Submitted**: 2025-09-10 07:21:55
- **Comment**: Accepted to UMAP 2025
- **Topic Keywords**: user behavior, recommend, search
- **Reason**: The paper is about recommender systems, but it focuses on user behavior evolution over time and age-related preferences, which is somewhat related to information retrieval and user behavior modeling. However, it lacks direct connection to query understanding, ranking models, and deep semantic understanding, making it less relevant to your primary research interests.

#### Abstract
> The majority of research in recommender systems, be it algorithmic
improvements, context-awareness, explainability, or other areas, evaluates
these systems on datasets that capture user interaction over a relatively
limited time span. However, recommender systems can very well be used
continuously for extended time. Similarly so, user behavior may evolve over
that extended time. Although media studies and psychology offer a wealth of
research on the evolution of user preferences and behavior as individuals age,
there has been scant research in this regard within the realm of user modeling
and recommender systems. In this study, we investigate the evolution of user
preferences and behavior using the LFM-2b dataset, which, to our knowledge, is
the only dataset that encompasses a sufficiently extensive time frame to permit
real longitudinal studies and includes age information about its users. We
identify specific usage and taste preferences directly related to the age of
the user, i.e., while younger users tend to listen broadly to contemporary
popular music, older users have more elaborate and personalized listening
habits. The findings yield important insights that open new directions for
research in recommender systems, providing guidance for future efforts.

### 14. Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Eric Slyman, Mehrab Tanjim, Kushal Kafle, Stefan Lee
- **URL**: <http://arxiv.org/abs/2509.08777v1>
- **Submitted**: 2025-09-10 17:06:47
- **Comment**: 17 pages, 8 figures, Accepted at ICCV 2025
- **Topic Keywords**: pairwise
- **Reason**: This paper focuses on multimodal large language models for text-to-image generation evaluation, which is outside the primary scope of information retrieval and search technologies. While it touches on aspects of model calibration and uncertainty estimation, it does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest.

#### Abstract
> Multimodal large language models (MLLMs) are increasingly used to evaluate
text-to-image (TTI) generation systems, providing automated judgments based on
visual and textual context. However, these "judge" models often suffer from
biases, overconfidence, and inconsistent performance across diverse image
domains. While prompt ensembling has shown promise for mitigating these issues
in unimodal, text-only settings, our experiments reveal that standard
ensembling methods fail to generalize effectively for TTI tasks. To address
these limitations, we propose a new multimodal-aware method called Multimodal
Mixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt
ensemble approach augmented by image clustering, allowing the judge to
dynamically assign prompt weights based on the visual characteristics of each
sample. We show that MMB improves accuracy in pairwise preference judgments and
greatly enhances calibration, making it easier to gauge the judge's true
uncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB
outperforms existing baselines in alignment with human annotations and
calibration across varied image content. Our findings highlight the importance
of multimodal-specific strategies for judge calibration and suggest a promising
path forward for reliable large-scale TTI evaluation.

### 15. AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Zhiheng Xi, Jixuan Huang, Chenyang Liao, Baodai Huang, Honglin Guo, Jiaqi Liu, Rui Zheng, Junjie Ye, Jiazheng Zhang, Wenxiang Chen, Wei He, Yiwen Ding, Guanyu Li, Zehui Chen, Zhengyin Du, Xuesong Yao, Yufei Xu, Jiecao Chen, Tao Gui, Zuxuan Wu, Qi Zhang, Xuanjing Huang, Yu-Gang Jiang
- **URL**: <http://arxiv.org/abs/2509.08755v1>
- **Submitted**: 2025-09-10 16:46:11
- **Comment**: preprint, 39 pages, 16 figures. Project:
  https://AgentGym-RL.github.io/. Framework and Code:
  https://github.com/woooodyy/AgentGym, https://github.com/woooodyy/AgentGym-RL
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on training LLM agents for long-horizon decision making through multi-turn reinforcement learning, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves deep learning and real-world scenarios, the primary application is in developing intelligent agents, which is outside the user's core research themes.

#### Abstract
> Developing autonomous LLM agents capable of making a series of intelligent
decisions to solve complex, real-world tasks is a fast-evolving frontier. Like
human cognitive development, agents are expected to acquire knowledge and
skills through exploration and interaction with the environment. Despite
advances, the community still lacks a unified, interactive reinforcement
learning (RL) framework that can effectively train such agents from scratch --
without relying on supervised fine-tuning (SFT) -- across diverse and realistic
environments. To bridge this gap, we introduce AgentGym-RL, a new framework to
train LLM agents for multi-turn interactive decision-making through RL. The
framework features a modular and decoupled architecture, ensuring high
flexibility and extensibility. It encompasses a wide variety of real-world
scenarios, and supports mainstream RL algorithms. Furthermore, we propose
ScalingInter-RL, a training approach designed for exploration-exploitation
balance and stable RL optimization. In early stages, it emphasizes exploitation
by restricting the number of interactions, and gradually shifts towards
exploration with larger horizons to encourage diverse problem-solving
strategies. In this way, the agent develops more diverse behaviors and is less
prone to collapse under long horizons. We perform extensive experiments to
validate the stability and effectiveness of both the AgentGym-RL framework and
the ScalingInter-RL approach. Our agents match or surpass commercial models on
27 tasks across diverse environments. We offer key insights and will
open-source the complete AgentGym-RL framework -- including code and datasets
-- to empower the research community in developing the next generation of
intelligent agents.

### 16. CM-Align: Consistency-based Multilingual Alignment for Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Xue Zhang, Yunlong Liang, Fandong Meng, Songming Zhang, Yufeng Chen, Jinan Xu, Jie Zhou
- **URL**: <http://arxiv.org/abs/2509.08541v1>
- **Submitted**: 2025-09-10 12:40:49
- **Comment**: EMNLP 2025 Findings
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on improving multilingual alignment for large language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing, especially in areas requiring deep semantic understanding and real-time relevance optimization.

#### Abstract
> Current large language models (LLMs) generally show a significant performance
gap in alignment between English and other languages. To bridge this gap,
existing research typically leverages the model's responses in English as a
reference to select the best/worst responses in other languages, which are then
used for Direct Preference Optimization (DPO) training. However, we argue that
there are two limitations in the current methods that result in noisy
multilingual preference data and further limited alignment performance: 1) Not
all English responses are of high quality, and using a response with low
quality may mislead the alignment for other languages. 2) Current methods
usually use biased or heuristic approaches to construct multilingual preference
pairs. To address these limitations, we design a consistency-based data
selection method to construct high-quality multilingual preference data for
improving multilingual alignment (CM-Align). Specifically, our method includes
two parts: consistency-guided English reference selection and cross-lingual
consistency-based multilingual preference data construction. Experimental
results on three LLMs and three common tasks demonstrate the effectiveness and
superiority of our method, which further indicates the necessity of
constructing high-quality preference data.

### 17. CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction with a New Dataset and Multi-Order Generative Framework

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Jinzhong Ning, Paerhati Tulajiang, Yingying Le, Yijia Zhang, Yuanyuan Sun, Hongfei Lin, Haifeng Liu
- **URL**: <http://arxiv.org/abs/2509.08438v1>
- **Submitted**: 2025-09-10 09:35:43
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on Speech Relation Extraction, which is not directly related to Information Retrieval or Search technologies. Although it involves Natural Language Processing, the specific application and techniques used are not aligned with the user's core research themes.

#### Abstract
> Speech Relation Extraction (SpeechRE) aims to extract relation triplets
directly from speech. However, existing benchmark datasets rely heavily on
synthetic data, lacking sufficient quantity and diversity of real human speech.
Moreover, existing models also suffer from rigid single-order generation
templates and weak semantic alignment, substantially limiting their
performance. To address these challenges, we introduce CommonVoice-SpeechRE, a
large-scale dataset comprising nearly 20,000 real-human speech samples from
diverse speakers, establishing a new benchmark for SpeechRE research.
Furthermore, we propose the Relation Prompt-Guided Multi-Order Generative
Ensemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet
generation ensemble strategy, leveraging data diversity through diverse element
orders during both training and inference, and (2) CNN-based latent relation
prediction heads that generate explicit relation prompts to guide cross-modal
alignment and accurate triplet generation. Experiments show our approach
outperforms state-of-the-art methods, providing both a benchmark dataset and an
effective solution for real-world SpeechRE. The source code and dataset are
publicly available at https://github.com/NingJinzhong/SpeechRE_RPG_MoGe.

### 18. EvolKV: Evolutionary KV Cache Compression for LLM Inference

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Bohan Yu, Yekun Chai
- **URL**: <http://arxiv.org/abs/2509.08315v1>
- **Submitted**: 2025-09-10 06:32:49
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on cache compression for Large Language Model (LLM) inference, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves optimization and performance improvement, the context is specific to LLM inference and does not align with the user's interests in query understanding, ranking models, or user behavior modeling.

#### Abstract
> Existing key-value (KV) cache compression methods typically rely on
heuristics, such as uniform cache allocation across layers or static eviction
policies, however, they ignore the critical interplays among layer-specific
feature patterns and task performance, which can lead to degraded
generalization. In this paper, we propose EvolKV, an adaptive framework for
layer-wise, task-driven KV cache compression that jointly optimizes the memory
efficiency and task performance. By reformulating cache allocation as a
multi-objective optimization problem, EvolKV leverages evolutionary search to
dynamically configure layer budgets while directly maximizing downstream
performance. Extensive experiments on 11 tasks demonstrate that our approach
outperforms all baseline methods across a wide range of KV cache budgets on
long-context tasks and surpasses heuristic baselines by up to 7 percentage
points on GSM8K. Notably, EvolKV achieves superior performance over the full KV
cache setting on code completion while utilizing only 1.5% of the original
budget, suggesting the untapped potential in learned compression strategies for
KV cache budget allocation.

### 19. Merge-of-Thought Distillation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zhanming Shen, Zeyu Qin, Zenan Huang, Hao Chen, Jiaqi Hu, Yihong Zhuang, Guoshan Lu, Gang Chen, Junbo Zhao
- **URL**: <http://arxiv.org/abs/2509.08814v2>
- **Submitted**: 2025-09-10 17:46:57
- **Topic Keywords**: acl
- **Reason**: This paper focuses on efficient reasoning distillation for long chain-of-thought models, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves deep semantic understanding, the context is more aligned with natural language processing and reasoning capabilities, rather than search or ranking models.

#### Abstract
> Efficient reasoning distillation for long chain-of-thought (CoT) models is
increasingly constrained by the assumption of a single oracle teacher, despite
practical availability of multiple candidate teachers and growing CoT corpora.
We revisit teacher selection and observe that different students have different
"best teachers," and even for the same student the best teacher can vary across
datasets. Therefore, to unify multiple teachers' reasoning abilities into
student with overcoming conflicts among various teachers' supervision, we
propose Merge-of-Thought Distillation (MoT), a lightweight framework that
alternates between teacher-specific supervised fine-tuning branches and
weight-space merging of the resulting student variants. On competition math
benchmarks, using only about 200 high-quality CoT samples, applying MoT to a
Qwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B,
QWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT
consistently outperforms the best single-teacher distillation and the naive
multi-teacher union, raises the performance ceiling while mitigating
overfitting, and shows robustness to distribution-shifted and peer-level
teachers. Moreover, MoT reduces catastrophic forgetting, improves general
reasoning beyond mathematics and even cultivates a better teacher, indicating
that consensus-filtered reasoning features transfer broadly. These results
position MoT as a simple, scalable route to efficiently distilling long CoT
capabilities from diverse teachers into compact students.

### 20. Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Eve Fleisig, Matthias Orlikowski, Philipp Cimiano, Dan Klein
- **URL**: <http://arxiv.org/abs/2509.08217v1>
- **Submitted**: 2025-09-10 01:22:07
- **Topic Keywords**: rag
- **Reason**: This paper focuses on spam filtering methods and their impact on data label distributions, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on data quality, it does not address query understanding, ranking models, or user behavior modeling.

#### Abstract
> For machine learning datasets to accurately represent diverse opinions in a
population, they must preserve variation in data labels while filtering out
spam or low-quality responses. How can we balance annotator reliability and
representation? We empirically evaluate how a range of heuristics for annotator
filtering affect the preservation of variation on subjective tasks. We find
that these methods, designed for contexts in which variation from a single
ground-truth label is considered noise, often remove annotators who disagree
instead of spam annotators, introducing suboptimal tradeoffs between accuracy
and label diversity. We find that conservative settings for annotator removal
(<5%) are best, after which all tested methods increase the mean absolute error
from the true average label. We analyze performance on synthetic spam to
observe that these methods often assume spam annotators are less random than
real spammers tend to be: most spammers are distributionally indistinguishable
from real annotators, and the minority that are distinguishable tend to give
fixed answers, not random ones. Thus, tasks requiring the preservation of
variation reverse the intuition of existing spam filtering methods: spammers
tend to be less random than non-spammers, so metrics that assume variation is
spam fare worse. These results highlight the need for spam removal methods that
account for label diversity.

### 21. MoVoC: Morphology-Aware Subword Construction for Geez Script Languages

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Hailay Kidu Teklehaymanot, Dren Fazlija, Wolfgang Nejdl
- **URL**: <http://arxiv.org/abs/2509.08812v1>
- **Submitted**: 2025-09-10 17:45:10
- **Comment**: This submission is approximately 10 pages in length and includes 1
  figure and 6 tables
- **Topic Keywords**: search
- **Reason**: This paper focuses on subword tokenization and morphology-aware segmentation for low-resource languages, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing, especially in areas requiring deep semantic understanding and real-time relevance optimization.

#### Abstract
> Subword-based tokenization methods often fail to preserve morphological
boundaries, a limitation especially pronounced in low-resource, morphologically
complex languages such as those written in the Geez script. To address this, we
present MoVoC (Morpheme-aware Subword Vocabulary Construction) and train
MoVoC-Tok, a tokenizer that integrates supervised morphological analysis into
the subword vocabulary. This hybrid segmentation approach combines
morpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological
integrity while maintaining lexical meaning. To tackle resource scarcity, we
curate and release manually annotated morpheme data for four Geez script
languages and a morpheme-aware vocabulary for two of them. While the proposed
tokenization method does not lead to significant gains in automatic translation
quality, we observe consistent improvements in intrinsic metrics, MorphoScore,
and Boundary Precision, highlighting the value of morphology-aware segmentation
in enhancing linguistic fidelity and token efficiency. Our morpheme-annotated
datasets and tokenizer will be publicly available to support further research
in low-resource, morphologically rich languages. Our code and data are
available on GitHub: https://github.com/hailaykidu/MoVoC

### 22. X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Hyunjun Kim, Junwoo Ha, Sangyoon Yu, Haon Park
- **URL**: <http://arxiv.org/abs/2509.08729v1>
- **Submitted**: 2025-09-10 16:17:44
- **Topic Keywords**: search
- **Reason**: This paper appears to be unrelated to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on a specific task of discovering multi-turn to single-turn templates for red-teaming, which is not directly related to your areas of expertise.

#### Abstract
> Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one
structured prompt, but prior work relied on a handful of manually written
templates. We present X-Teaming Evolutionary M2S, an automated framework that
discovers and optimizes M2S templates through language-model-guided evolution.
The system pairs smart sampling from 12 sources with an LLM-as-judge inspired
by StrongREJECT and records fully auditable logs.
  Maintaining selection pressure by setting the success threshold to $\theta =
0.70$, we obtain five evolutionary generations, two new template families, and
44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of
2,500 trials (judge fixed) shows that structural gains transfer but vary by
target; two models score zero at the same threshold. We also find a positive
coupling between prompt length and score, motivating length-aware judging.
  Our results demonstrate that structure-level search is a reproducible route
to stronger single-turn probes and underscore the importance of threshold
calibration and cross-model evaluation. Code, configurations, and artifacts are
available at https://github.com/hyunjun1121/M2S-x-teaming.

### 23. Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Anran Li, Lingfei Qian, Mengmeng Du, Yu Yin, Yan Hu, Zihao Sun, Yihang Fu, Erica Stutz, Xuguang Ai, Qianqian Xie, Rui Zhu, Jimin Huang, Yifan Yang, Siru Liu, Yih-Chung Tham, Lucila Ohno-Machado, Hyunghoon Cho, Zhiyong Lu, Hua Xu, Qingyu Chen
- **URL**: <http://arxiv.org/abs/2509.08604v1>
- **Submitted**: 2025-09-10 14:02:18
- **Topic Keywords**: recommend
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The paper focuses on the memorization of medical training data in Large Language Models, which is outside your primary areas of interest.

#### Abstract
> Large Language Models (LLMs) have demonstrated significant potential in
medicine. To date, LLMs have been widely applied to tasks such as diagnostic
assistance, medical question answering, and clinical information synthesis.
However, a key open question remains: to what extent do LLMs memorize medical
training data. In this study, we present the first comprehensive evaluation of
memorization of LLMs in medicine, assessing its prevalence (how frequently it
occurs), characteristics (what is memorized), volume (how much content is
memorized), and potential downstream impacts (how memorization may affect
medical applications). We systematically analyze common adaptation scenarios:
(1) continued pretraining on medical corpora, (2) fine-tuning on standard
medical benchmarks, and (3) fine-tuning on real-world clinical data, including
over 13,000 unique inpatient records from Yale New Haven Health System. The
results demonstrate that memorization is prevalent across all adaptation
scenarios and significantly higher than reported in the general domain.
Memorization affects both the development and adoption of LLMs in medicine and
can be categorized into three types: beneficial (e.g., accurate recall of
clinical guidelines and biomedical references), uninformative (e.g., repeated
disclaimers or templated medical document language), and harmful (e.g.,
regeneration of dataset-specific or sensitive clinical content). Based on these
findings, we offer practical recommendations to facilitate beneficial
memorization that enhances domain-specific reasoning and factual accuracy,
minimize uninformative memorization to promote deeper learning beyond
surface-level patterns, and mitigate harmful memorization to prevent the
leakage of sensitive or identifiable patient information.

### 24. Acquiescence Bias in Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Daniel Braun
- **URL**: <http://arxiv.org/abs/2509.08480v1>
- **Submitted**: 2025-09-10 10:39:24
- **Comment**: Accepted to EMNLP 2025 Findings
- **Topic Keywords**: search
- **Reason**: This paper is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus on acquiescence bias in Large Language Models is more aligned with NLP, but the specific topic and methodology do not seem to overlap with your areas of interest.

#### Abstract
> Acquiescence bias, i.e. the tendency of humans to agree with statements in
surveys, independent of their actual beliefs, is well researched and
documented. Since Large Language Models (LLMs) have been shown to be very
influenceable by relatively small changes in input and are trained on
human-generated data, it is reasonable to assume that they could show a similar
tendency. We present a study investigating the presence of acquiescence bias in
LLMs across different models, tasks, and languages (English, German, and
Polish). Our results indicate that, contrary to humans, LLMs display a bias
towards answering no, regardless of whether it indicates agreement or
disagreement.

### 25. Adversarial Attacks Against Automated Fact-Checking: A Survey

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Fanzhen Liu, Alsharif Abuadbba, Kristen Moore, Surya Nepal, Cecile Paris, Jia Wu, Jian Yang, Quan Z. Sheng
- **URL**: <http://arxiv.org/abs/2509.08463v1>
- **Submitted**: 2025-09-10 10:10:10
- **Comment**: Accepted to the Main Conference of EMNLP 2025. Resources are
  available at
  https://github.com/FanzhenLiu/Awesome-Automated-Fact-Checking-Attacks
- **Topic Keywords**: search
- **Reason**: This paper is not directly related to the user's core research themes in Information Retrieval, Search technologies, or Natural Language Processing. While it touches on the concept of manipulation and misinformation, its focus on fact-checking and adversarial attacks is not aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> In an era where misinformation spreads freely, fact-checking (FC) plays a
crucial role in verifying claims and promoting reliable information. While
automated fact-checking (AFC) has advanced significantly, existing systems
remain vulnerable to adversarial attacks that manipulate or generate claims,
evidence, or claim-evidence pairs. These attacks can distort the truth, mislead
decision-makers, and ultimately undermine the reliability of FC models. Despite
growing research interest in adversarial attacks against AFC systems, a
comprehensive, holistic overview of key challenges remains lacking. These
challenges include understanding attack strategies, assessing the resilience of
current models, and identifying ways to enhance robustness. This survey
provides the first in-depth review of adversarial attacks targeting FC,
categorizing existing attack methodologies and evaluating their impact on AFC
systems. Additionally, we examine recent advancements in adversary-aware
defenses and highlight open research questions that require further
exploration. Our findings underscore the urgent need for resilient FC
frameworks capable of withstanding adversarial manipulations in pursuit of
preserving high verification accuracy.

---

