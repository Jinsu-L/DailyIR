[
    {
        "title": "Training AI Co-Scientists Using Rubric Rewards",
        "abstract": "AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. We build a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. We then train models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, we conduct a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by our finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, we also extend our approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. Our finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.",
        "url": "http://arxiv.org/abs/2512.23707v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23707v1",
        "arxiv_id": "2512.23707v1",
        "authors": [
            "Shashwat Goel",
            "Rishi Hazra",
            "Dulhan Jayalath",
            "Timon Willi",
            "Parag Jain",
            "William F. Shen",
            "Ilias Leontiadis",
            "Francesco Barbieri",
            "Yoram Bachrach",
            "Jonas Geiping",
            "Chenxi Whitehouse"
        ],
        "submitted": "2025-12-29 18:59:33",
        "source": "arxiv",
        "comment": "11 pages in the main paper, total 119 including sample outputs in the Appendix"
    },
    {
        "title": "Eliciting Behaviors in Multi-Turn Conversations",
        "abstract": "Identifying specific and often complex behaviors from large language models (LLMs) in conversational settings is crucial for their evaluation. Recent work proposes novel techniques to find natural language prompts that induce specific behaviors from a target model, yet they are mainly studied in single-turn settings. In this work, we study behavior elicitation in the context of multi-turn conversations. We first offer an analytical framework that categorizes existing methods into three families based on their interactions with the target model: those that use only prior knowledge, those that use offline interactions, and those that learn from online interactions. We then introduce a generalized multi-turn formulation of the online method, unifying single-turn and multi-turn elicitation. We evaluate all three families of methods on automatically generating multi-turn test cases. We investigate the efficiency of these approaches by analyzing the trade-off between the query budget, i.e., the number of interactions with the target model, and the success rate, i.e., the discovery rate of behavior-eliciting inputs. We find that online methods can achieve an average success rate of 45/19/77% with just a few thousand queries over three tasks where static methods from existing multi-turn conversation benchmarks find few or even no failure cases. Our work highlights a novel application of behavior elicitation methods in multi-turn conversation evaluation and the need for the community to move towards dynamic benchmarks.",
        "url": "http://arxiv.org/abs/2512.23701v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23701v1",
        "arxiv_id": "2512.23701v1",
        "authors": [
            "Jing Huang",
            "Shujian Zhang",
            "Lun Wang",
            "Andrew Hard",
            "Rajiv Mathews",
            "John Lambert"
        ],
        "submitted": "2025-12-29 18:57:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Fine-Tuning LLMs with Fine-Grained Human Feedback on Text Spans",
        "abstract": "We present a method and dataset for fine-tuning language models with preference supervision using feedback-driven improvement chains. Given a model response, an annotator provides fine-grained feedback by marking ``liked'' and ``disliked'' spans and specifying what they liked or disliked about them. The base model then rewrites the disliked spans accordingly, proceeding from left to right, forming a sequence of incremental improvements. We construct preference pairs for direct alignment from each adjacent step in the chain, enabling the model to learn from localized, targeted edits. We find that our approach outperforms direct alignment methods based on standard A/B preference ranking or full contrastive rewrites, demonstrating that structured, revision-based supervision leads to more efficient and effective preference tuning.",
        "url": "http://arxiv.org/abs/2512.23693v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23693v1",
        "arxiv_id": "2512.23693v1",
        "authors": [
            "Sky CH-Wang",
            "Justin Svegliato",
            "Helen Appel",
            "Jason Eisner"
        ],
        "submitted": "2025-12-29 18:51:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PROFASR-BENCH: A Benchmark for Context-Conditioned ASR in High-Stakes Professional Speech",
        "abstract": "Automatic Speech Recognition (ASR) in professional settings faces challenges that existing benchmarks underplay: dense domain terminology, formal register variation, and near-zero tolerance for critical entity errors. We present ProfASR-Bench, a professional-talk evaluation suite for high-stakes applications across finance, medicine, legal, and technology. Each example pairs a natural-language prompt (domain cue and/or speaker profile) with an entity-rich target utterance, enabling controlled measurement of context-conditioned recognition. The corpus supports conventional ASR metrics alongside entity-aware scores and slice-wise reporting by accent and gender. Using representative families Whisper (encoder-decoder ASR) and Qwen-Omni (audio language models) under matched no-context, profile, domain+profile, oracle, and adversarial conditions, we find a consistent pattern: lightweight textual context produces little to no change in average word error rate (WER), even with oracle prompts, and adversarial prompts do not reliably degrade performance. We term this the context-utilization gap (CUG): current systems are nominally promptable yet underuse readily available side information. ProfASR-Bench provides a standardized context ladder, entity- and slice-aware reporting with confidence intervals, and a reproducible testbed for comparing fusion strategies across model families.\n  Dataset: https://huggingface.co/datasets/prdeepakbabu/ProfASR-Bench\n  Code: https://github.com/prdeepakbabu/ProfASR-Bench",
        "url": "http://arxiv.org/abs/2512.23686v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23686v1",
        "arxiv_id": "2512.23686v1",
        "authors": [
            "Deepak Babu Piskala"
        ],
        "submitted": "2025-12-29 18:43:23",
        "source": "arxiv",
        "comment": "Benchmark dataset and evaluation suite. Data and code available at: https://huggingface.co/datasets/prdeepakbabu/ProfASR-Bench https://github.com/prdeepakbabu/ProfASR-Bench"
    },
    {
        "title": "Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing",
        "abstract": "Large language models (LLMs) are increasingly considered for use in high-impact workflows, including academic peer review. However, LLMs are vulnerable to document-level hidden prompt injection attacks. In this work, we construct a dataset of approximately 500 real academic papers accepted to ICML and evaluate the effect of embedding hidden adversarial prompts within these documents. Each paper is injected with semantically equivalent instructions in four different languages and reviewed using an LLM. We find that prompt injection induces substantial changes in review scores and accept/reject decisions for English, Japanese, and Chinese injections, while Arabic injections produce little to no effect. These results highlight the susceptibility of LLM-based reviewing systems to document-level prompt injection and reveal notable differences in vulnerability across languages.",
        "url": "http://arxiv.org/abs/2512.23684v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23684v1",
        "arxiv_id": "2512.23684v1",
        "authors": [
            "Panagiotis Theocharopoulos",
            "Ajinkya Kulkarni",
            "Mathew Magimai. -Doss"
        ],
        "submitted": "2025-12-29 18:43:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Web World Models",
        "abstract": "Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics'' are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.",
        "url": "http://arxiv.org/abs/2512.23676v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23676v1",
        "arxiv_id": "2512.23676v1",
        "authors": [
            "Jichen Feng",
            "Yifan Zhang",
            "Chenggong Zhang",
            "Yifu Lu",
            "Shilong Liu",
            "Mengdi Wang"
        ],
        "submitted": "2025-12-29 18:31:45",
        "source": "arxiv",
        "comment": "Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models"
    },
    {
        "title": "Less is more: Probabilistic reduction is best explained by small-scale predictability measures",
        "abstract": "The primary research questions of this paper center on defining the amount of context that is necessary and/or appropriate when investigating the relationship between language model probabilities and cognitive phenomena. We investigate whether whole utterances are necessary to observe probabilistic reduction and demonstrate that n-gram representations suffice as cognitive units of planning.",
        "url": "http://arxiv.org/abs/2512.23659v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23659v1",
        "arxiv_id": "2512.23659v1",
        "authors": [
            "Cassandra L. Jacobs",
            "Andrés Buxó-Lugo",
            "Anna K. Taylor",
            "Marie Leopold-Hooke"
        ],
        "submitted": "2025-12-29 18:12:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Nested Browser-Use Learning for Agentic Information Seeking",
        "abstract": "Information-seeking (IS) agents have achieved strong performance across a range of wide and deep search tasks, yet their tool use remains largely restricted to API-level snippet retrieval and URL-based page fetching, limiting access to the richer information available through real browsing. While full browser interaction could unlock deeper capabilities, its fine-grained control and verbose page content returns introduce substantial complexity for ReAct-style function-calling agents. To bridge this gap, we propose Nested Browser-Use Learning (NestBrowse), which introduces a minimal and complete browser-action framework that decouples interaction control from page exploration through a nested structure. This design simplifies agentic reasoning while enabling effective deep-web information acquisition. Empirical results on challenging deep IS benchmarks demonstrate that NestBrowse offers clear benefits in practice. Further in-depth analyses underscore its efficiency and flexibility.",
        "url": "http://arxiv.org/abs/2512.23647v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23647v1",
        "arxiv_id": "2512.23647v1",
        "authors": [
            "Baixuan Li",
            "Jialong Wu",
            "Wenbiao Yin",
            "Kuan Li",
            "Zhongwang Zhang",
            "Huifeng Yin",
            "Zhengwei Tao",
            "Liwen Zhang",
            "Pengjun Xie",
            "Jingren Zhou",
            "Yong Jiang"
        ],
        "submitted": "2025-12-29 17:59:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Dataset and Benchmark for Consumer Healthcare Question Summarization",
        "abstract": "The quest for seeking health information has swamped the web with consumers health-related questions. Generally, consumers use overly descriptive and peripheral information to express their medical condition or other healthcare needs, contributing to the challenges of natural language understanding. One way to address this challenge is to summarize the questions and distill the key information of the original question. Recently, large-scale datasets have significantly propelled the development of several summarization tasks, such as multi-document summarization and dialogue summarization. However, a lack of a domain-expert annotated dataset for the consumer healthcare questions summarization task inhibits the development of an efficient summarization system. To address this issue, we introduce a new dataset, CHQ-Sum,m that contains 1507 domain-expert annotated consumer health questions and corresponding summaries. The dataset is derived from the community question answering forum and therefore provides a valuable resource for understanding consumer health-related posts on social media. We benchmark the dataset on multiple state-of-the-art summarization models to show the effectiveness of the dataset",
        "url": "http://arxiv.org/abs/2512.23637v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23637v1",
        "arxiv_id": "2512.23637v1",
        "authors": [
            "Abhishek Basu",
            "Deepak Gupta",
            "Dina Demner-Fushman",
            "Shweta Yadav"
        ],
        "submitted": "2025-12-29 17:49:43",
        "source": "arxiv",
        "comment": "arXiv admin note: substantial text overlap with arXiv:2206.06581"
    },
    {
        "title": "Close the Loop: Synthesizing Infinite Tool-Use Data via Multi-Agent Role-Playing",
        "abstract": "Enabling Large Language Models (LLMs) to reliably invoke external tools remains a critical bottleneck for autonomous agents. Existing approaches suffer from three fundamental challenges: expensive human annotation for high-quality trajectories, poor generalization to unseen tools, and quality ceilings inherent in single-model synthesis that perpetuate biases and coverage gaps. We introduce InfTool, a fully autonomous framework that breaks these barriers through self-evolving multi-agent synthesis. Given only raw API specifications, InfTool orchestrates three collaborative agents (User Simulator, Tool-Calling Assistant, and MCP Server) to generate diverse, verified trajectories spanning single-turn calls to complex multi-step workflows. The framework establishes a closed loop: synthesized data trains the model via Group Relative Policy Optimization (GRPO) with gated rewards, the improved model generates higher-quality data targeting capability gaps, and this cycle iterates without human intervention. Experiments on the Berkeley Function-Calling Leaderboard (BFCL) demonstrate that InfTool transforms a base 32B model from 19.8% to 70.9% accuracy (+258%), surpassing models 10x larger and rivaling Claude-Opus, and entirely from synthetic data without human annotation.",
        "url": "http://arxiv.org/abs/2512.23611v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23611v1",
        "arxiv_id": "2512.23611v1",
        "authors": [
            "Yuwen Li",
            "Wei Zhang",
            "Zelong Huang",
            "Mason Yang",
            "Jiajun Wu",
            "Shawn Guo",
            "Huahao Hu",
            "Lingyi Sun",
            "Jian Yang",
            "Mingjie Tang",
            "Byran Dai"
        ],
        "submitted": "2025-12-29 17:12:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Big Three in Marriage Talk: LLM-Assisted Analysis of Moral Ethics and Sentiment on Weibo and Xiaohongshu",
        "abstract": "China's marriage registrations have declined dramatically, dropping from 13.47 million couples in 2013 to 6.1 million in 2024. Understanding public attitudes toward marriage requires examining not only emotional sentiment but also the moral reasoning underlying these evaluations. This study analyzed 219,358 marriage-related posts from two major Chinese social media platforms (Sina Weibo and Xiaohongshu) using large language model (LLM)-assisted content analysis. Drawing on Shweder's Big Three moral ethics framework, posts were coded for sentiment (positive, negative, neutral) and moral dimensions (Autonomy, Community, Divinity). Results revealed platform differences: Weibo discourse skewed positive, while Xiaohongshu was predominantly neutral. Most posts across both platforms lacked explicit moral framing. However, when moral ethics were invoked, significant associations with sentiment emerged. Posts invoking Autonomy ethics and Community ethics were predominantly negative, whereas Divinity-framed posts tended toward neutral or positive sentiment. These findings suggest that concerns about both personal autonomy constraints and communal obligations drive negative marriage attitudes in contemporary China. The study demonstrates LLMs' utility for scaling qualitative analysis and offers insights for developing culturally informed policies addressing marriage decline in Chinese contexts.",
        "url": "http://arxiv.org/abs/2512.23609v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23609v1",
        "arxiv_id": "2512.23609v1",
        "authors": [
            "Frank Tian-Fang Ye",
            "Xiaozi Gao"
        ],
        "submitted": "2025-12-29 17:05:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Scalable Residual Feature Aggregation Framework with Hybrid Metaheuristic Optimization for Robust Early Pancreatic Neoplasm Detection in Multimodal CT Imaging",
        "abstract": "The early detection of pancreatic neoplasm is a major clinical dilemma, and it is predominantly so because tumors are likely to occur with minimal contrast margins and a large spread anatomy-wide variation amongst patients on a CT scan. These complexities require to be addressed with an effective and scalable system that can assist in enhancing the salience of the subtle visual cues and provide a high level of the generalization on the multimodal imaging data. A Scalable Residual Feature Aggregation (SRFA) framework is proposed to be used to meet these conditions in this study. The framework integrates a pipeline of preprocessing followed by the segmentation using the MAGRes-UNet that is effective in making the pancreatic structures and isolating regions of interest more visible. DenseNet-121 performed with residual feature storage is used to extract features to allow deep hierarchical features to be aggregated without properties loss. To go further, hybrid HHO-BA metaheuristic feature selection strategy is used, which guarantees the best feature subset refinement. To be classified, the system is trained based on a new hybrid model that integrates the ability to pay attention on the world, which is the Vision Transformer (ViT) with the high representational efficiency of EfficientNet-B3. A dual optimization mechanism incorporating SSA and GWO is used to fine-tune hyperparameters to enhance greater robustness and less overfitting. Experimental results support the significant improvement in performance, with the suggested model reaching 96.23% accuracy, 95.58% F1-score and 94.83% specificity, the model is significantly better than the traditional CNNs and contemporary transformer-based models. Such results highlight the possibility of the SRFA framework as a useful instrument in the early detection of pancreatic tumors.",
        "url": "http://arxiv.org/abs/2512.23597v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23597v1",
        "arxiv_id": "2512.23597v1",
        "authors": [
            "Janani Annur Thiruvengadam",
            "Kiran Mayee Nabigaru",
            "Anusha Kovi"
        ],
        "submitted": "2025-12-29 16:51:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Style Amnesia: Investigating Speaking Style Degradation and Mitigation in Multi-Turn Spoken Language Models",
        "abstract": "In this paper, we show that when spoken language models (SLMs) are instructed to speak in a specific speaking style at the beginning of a multi-turn conversation, they cannot maintain the required speaking styles after several turns of interaction; we refer to this as the style amnesia of SLMs. We focus on paralinguistic speaking styles, including emotion, accent, volume, and speaking speed. We evaluate three proprietary and two open-source SLMs, demonstrating that none of these models can maintain a consistent speaking style when instructed to do so. We further show that when SLMs are asked to recall the style instruction in later turns, they can recall the style instruction, but they fail to express it throughout the conversation. We also show that explicitly asking the model to recall the style instruction can partially mitigate style amnesia. In addition, we examine various prompting strategies and find that SLMs struggle to follow the required style when the instruction is placed in system messages rather than user messages, which contradicts the intended function of system prompts.",
        "url": "http://arxiv.org/abs/2512.23578v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23578v1",
        "arxiv_id": "2512.23578v1",
        "authors": [
            "Yu-Xiang Lin",
            "Cheng-Han Chiang",
            "Hung-yi Lee"
        ],
        "submitted": "2025-12-29 16:23:54",
        "source": "arxiv",
        "comment": "Work in progress"
    },
    {
        "title": "Instruction-Following Evaluation of Large Vision-Language Models",
        "abstract": "Following the initial flourishing of large language models (LLMs), there has been a surge in proposed large vision-language models (LVLMs) that integrate LLMs with vision capabilities. However, it has been observed that LVLMs, after tuning to visual instruction using commonly used training datasets, often fail to exhibit the instruction-following ability that was present in the LLM before integration, leading to results in which they do not follow task instructions as expected. This study quantitatively demonstrates that LVLMs' instruction-following ability declines after fine-tuning and analyzes its underlying causes. In particular, we constructed new training datasets highlighting whether the output format is specified. Then, we investigated how explicitly indicating the output format during fine-tuning affects LVLMs' instruction-following ability. Our quantitative evaluation confirmed that LVLMs' instruction-following ability declines after fine-tuning with commonly used datasets. Furthermore, we found that LVLMs trained with datasets, including instructions on output format, tend to follow instructions more accurately than models that do not. These findings suggest that including samples with instructions on output format during (visual) instruction tuning may help mitigate the decline in instruction-following abilities.",
        "url": "http://arxiv.org/abs/2512.23572v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23572v1",
        "arxiv_id": "2512.23572v1",
        "authors": [
            "Daiki Shiono",
            "Shumpei Miyawaki",
            "Ryota Tanaka",
            "Jun Suzuki"
        ],
        "submitted": "2025-12-29 16:12:33",
        "source": "arxiv",
        "comment": "21 pages, 7 figures"
    },
    {
        "title": "VL-RouterBench: A Benchmark for Vision-Language Model Routing",
        "abstract": "Multi-model routing has evolved from an engineering technique into essential infrastructure, yet existing work lacks a systematic, reproducible benchmark for evaluating vision-language models (VLMs). We present VL-RouterBench to assess the overall capability of VLM routing systems systematically. The benchmark is grounded in raw inference and scoring logs from VLMs and constructs quality and cost matrices over sample-model pairs. In scale, VL-RouterBench covers 14 datasets across 3 task groups, totaling 30,540 samples, and includes 15 open-source models and 2 API models, yielding 519,180 sample-model pairs and a total input-output token volume of 34,494,977. The evaluation protocol jointly measures average accuracy, average cost, and throughput, and builds a ranking score from the harmonic mean of normalized cost and accuracy to enable comparison across router configurations and cost budgets. On this benchmark, we evaluate 10 routing methods and baselines and observe a significant routability gain, while the best current routers still show a clear gap to the ideal Oracle, indicating considerable room for improvement in router architecture through finer visual cues and modeling of textual structure. We will open-source the complete data construction and evaluation toolchain to promote comparability, reproducibility, and practical deployment in multimodal routing research.",
        "url": "http://arxiv.org/abs/2512.23562v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23562v1",
        "arxiv_id": "2512.23562v1",
        "authors": [
            "Zhehao Huang",
            "Baijiong Lin",
            "Jingyuan Zhang",
            "Jingying Wang",
            "Yuhang Liu",
            "Ning Lu",
            "Tao Li",
            "Xiaolin Huang"
        ],
        "submitted": "2025-12-29 16:01:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Lie to Me: Knowledge Graphs for Robust Hallucination Self-Detection in LLMs",
        "abstract": "Hallucinations, the generation of apparently convincing yet false statements, remain a major barrier to the safe deployment of LLMs. Building on the strong performance of self-detection methods, we examine the use of structured knowledge representations, namely knowledge graphs, to improve hallucination self-detection. Specifically, we propose a simple yet powerful approach that enriches hallucination self-detection by (i) converting LLM responses into knowledge graphs of entities and relations, and (ii) using these graphs to estimate the likelihood that a response contains hallucinations. We evaluate the proposed approach using two widely used LLMs, GPT-4o and Gemini-2.5-Flash, across two hallucination detection datasets. To support more reliable future benchmarking, one of these datasets has been manually curated and enhanced and is released as a secondary outcome of this work. Compared to standard self-detection methods and SelfCheckGPT, a state-of-the-art approach, our method achieves up to 16% relative improvement in accuracy and 20% in F1-score. Our results show that LLMs can better analyse atomic facts when they are structured as knowledge graphs, even when initial outputs contain inaccuracies. This low-cost, model-agnostic approach paves the way toward safer and more trustworthy language models.",
        "url": "http://arxiv.org/abs/2512.23547v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23547v1",
        "arxiv_id": "2512.23547v1",
        "authors": [
            "Sahil Kale",
            "Antonio Luca Alfeo"
        ],
        "submitted": "2025-12-29 15:41:13",
        "source": "arxiv",
        "comment": "Accepted to ICPRAM 2026 in Marbella, Spain"
    },
    {
        "title": "Single LLM Debate, MoLaCE: Mixture of Latent Concept Experts Against Confirmation Bias",
        "abstract": "Large language models (LLMs) are highly vulnerable to input confirmation bias. When a prompt implies a preferred answer, models often reinforce that bias rather than explore alternatives. This phenomenon remains underexplored, yet it is already harmful in base models and poses an even greater risk in multi-agent debate, where echo chambers reinforce bias instead of correction. We introduce Mixture of Latent Concept Experts (MoLaCE), a lightweight inference-time framework that addresses confirmation bias by mixing experts instantiated as different activation strengths over latent concepts that shape model responses. Our key insight is that, due to the compositional nature of language, differently phrased prompts reweight latent concepts in prompt-specific ways that affect factual correctness, so no single fixed intervention can be applied universally across inputs. This design enables a single LLM to emulate the benefits of debate internally while remaining computationally efficient and scalable. It can also be integrated into multi-agent debate frameworks to diversify perspectives and reduce correlated errors. We empirically show that it consistently reduces confirmation bias, improves robustness, and matches or surpasses multi-agent debate while requiring only a fraction of the computation.",
        "url": "http://arxiv.org/abs/2512.23518v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23518v1",
        "arxiv_id": "2512.23518v1",
        "authors": [
            "Hazel Kim",
            "Philip Torr"
        ],
        "submitted": "2025-12-29 14:52:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?",
        "abstract": "Vision-language large models are moving toward the unification of visual understanding and visual generation tasks. However, whether generation can enhance understanding is still under-explored on large data scale. In this work, we analysis the unified model with a concise structure, UniHetero, under large-scale pretraining (>200M samples). Our key observations are: (1) Generation can improve understanding, but Only if you generate Semantics, Not Pixels. (2) Generation reveals a superior Data Scaling trend and higher Data Utilization. (3) Autoregression on Input Embedding is effective to capture visual details.",
        "url": "http://arxiv.org/abs/2512.23512v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23512v1",
        "arxiv_id": "2512.23512v1",
        "authors": [
            "Fengjiao Chen",
            "Minhao Jing",
            "Weitao Lu",
            "Yan Feng",
            "Xiaoyu Li",
            "Xuezhi Cao"
        ],
        "submitted": "2025-12-29 14:49:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Automatic Detection of Complex Quotation Patterns in Aggadic Literature",
        "abstract": "This paper presents ACT (Allocate Connections between Texts), a novel three-stage algorithm for the automatic detection of biblical quotations in Rabbinic literature. Unlike existing text reuse frameworks that struggle with short, paraphrased, or structurally embedded quotations, ACT combines a morphology-aware alignment algorithm with a context-sensitive enrichment stage that identifies complex citation patterns such as \"Wave\" and \"Echo\" quotations.\n  Our approach was evaluated against leading systems, including Dicta, Passim, Text-Matcher, as well as human-annotated critical editions. We further assessed three ACT configurations to isolate the contribution of each component. Results demonstrate that the full ACT pipeline (ACT-QE) outperforms all baselines, achieving an F1 score of 0.91, with superior Recall (0.89) and Precision (0.94). Notably, ACT-2, which lacks stylistic enrichment, achieves higher Recall (0.90) but suffers in Precision, while ACT-3, using longer n-grams, offers a tradeoff between coverage and specificity.\n  In addition to improving quotation detection, ACT's ability to classify stylistic patterns across corpora opens new avenues for genre classification and intertextual analysis. This work contributes to digital humanities and computational philology by addressing the methodological gap between exhaustive machine-based detection and human editorial judgment. ACT lays a foundation for broader applications in historical textual analysis, especially in morphologically rich and citation-dense traditions like Aggadic literature.",
        "url": "http://arxiv.org/abs/2512.23504v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23504v1",
        "arxiv_id": "2512.23504v1",
        "authors": [
            "Hadar Miller",
            "Tsvi Kuflik",
            "Moshe Lavee"
        ],
        "submitted": "2025-12-29 14:45:58",
        "source": "arxiv",
        "comment": "This paper is under review at Cogent Arts & Humanities"
    },
    {
        "title": "Semantic Tree Inference on Text Corpa using a Nested Density Approach together with Large Language Model Embeddings",
        "abstract": "Semantic text classification has undergone significant advances in recent years due to the rise of large language models (LLMs) and their high dimensional embeddings. While LLM-embeddings are frequently used to store and retrieve text by semantic similarity in vector databases, the global structure semantic relationships in text corpora often remains opaque. Herein we propose a nested density clustering approach, to infer hierarchical trees of semantically related texts. The method starts by identifying texts of strong semantic similarity as it searches for dense clusters in LLM embedding space. As the density criterion is gradually relaxed, these dense clusters merge into more diffuse clusters, until the whole dataset is represented by a single cluster -- the root of the tree. By embedding dense clusters into increasingly diffuse ones, we construct a tree structure that captures hierarchical semantic relationships among texts. We outline how this approach can be used to classify textual data for abstracts of scientific abstracts as a case study. This enables the data-driven discovery research areas and their subfields without predefined categories. To evaluate the general applicability of the method, we further apply it to established benchmark datasets such as the 20 Newsgroups and IMDB 50k Movie Reviews, demonstrating its robustness across domains. Finally we discuss possible applications on scientometrics, topic evolution, highlighting how nested density trees can reveal semantic structure and evolution in textual datasets.",
        "url": "http://arxiv.org/abs/2512.23471v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23471v1",
        "arxiv_id": "2512.23471v1",
        "authors": [
            "Thomas Haschka",
            "Joseph Bakarji"
        ],
        "submitted": "2025-12-29 13:55:23",
        "source": "arxiv",
        "comment": "20 pages, 9 figures"
    },
    {
        "title": "Replay Failures as Successes: Sample-Efficient Reinforcement Learning for Instruction Following",
        "abstract": "Reinforcement Learning (RL) has shown promise for aligning Large Language Models (LLMs) to follow instructions with various constraints. Despite the encouraging results, RL improvement inevitably relies on sampling successful, high-quality responses; however, the initial model often struggles to generate responses that satisfy all constraints due to its limited capabilities, yielding sparse or indistinguishable rewards that impede learning. In this work, we propose Hindsight instruction Replay (HiR), a novel sample-efficient RL framework for complex instruction following tasks, which employs a select-then-rewrite strategy to replay failed attempts as successes based on the constraints that have been satisfied in hindsight. We perform RL on these replayed samples as well as the original ones, theoretically framing the objective as dual-preference learning at both the instruction- and response-level to enable efficient optimization using only a binary reward signal. Extensive experiments demonstrate that the proposed HiR yields promising results across different instruction following tasks, while requiring less computational budget. Our code and dataset is available at https://github.com/sastpg/HIR.",
        "url": "http://arxiv.org/abs/2512.23457v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23457v1",
        "arxiv_id": "2512.23457v1",
        "authors": [
            "Kongcheng Zhang",
            "Qi Yao",
            "Shunyu Liu",
            "Wenjian Zhang",
            "Min Cen",
            "Yang Zhou",
            "Wenkai Fang",
            "Yiru Zhao",
            "Baisheng Lai",
            "Mingli Song"
        ],
        "submitted": "2025-12-29 13:31:08",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Coupling Experts and Routers in Mixture-of-Experts via an Auxiliary Loss",
        "abstract": "Mixture-of-Experts (MoE) models lack explicit constraints to ensure the router's decisions align well with the experts' capabilities, which ultimately limits model performance. To address this, we propose expert-router coupling (ERC) loss, a lightweight auxiliary loss that tightly couples the router's decisions with expert capabilities. Our approach treats each expert's router embedding as a proxy token for the tokens assigned to that expert, and feeds perturbed router embeddings through the experts to obtain internal activations. The ERC loss enforces two constraints on these activations: (1) Each expert must exhibit higher activation for its own proxy token than for the proxy tokens of any other expert. (2) Each proxy token must elicit stronger activation from its corresponding expert than from any other expert. These constraints jointly ensure that each router embedding faithfully represents its corresponding expert's capability, while each expert specializes in processing the tokens actually routed to it. The ERC loss is computationally efficient, operating only on n^2 activations, where n is the number of experts. This represents a fixed cost independent of batch size, unlike prior coupling methods that scale with the number of tokens (often millions per batch). Through pre-training MoE-LLMs ranging from 3B to 15B parameters and extensive analysis on trillions of tokens, we demonstrate the effectiveness of the ERC loss. Moreover, the ERC loss offers flexible control and quantitative tracking of expert specialization levels during training, providing valuable insights into MoEs.",
        "url": "http://arxiv.org/abs/2512.23447v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23447v1",
        "arxiv_id": "2512.23447v1",
        "authors": [
            "Ang Lv",
            "Jin Ma",
            "Yiyuan Ma",
            "Siyuan Qiao"
        ],
        "submitted": "2025-12-29 13:03:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ClinDEF: A Dynamic Evaluation Framework for Large Language Models in Clinical Reasoning",
        "abstract": "Clinical diagnosis begins with doctor-patient interaction, during which physicians iteratively gather information, determine examination and refine differential diagnosis through patients' response. This dynamic clinical-reasoning process is poorly represented by existing LLM benchmarks that focus on static question-answering. To mitigate these gaps, recent methods explore dynamic medical frameworks involving interactive clinical dialogues. Although effective, they often rely on limited, contamination-prone datasets and lack granular, multi-level evaluation. In this work, we propose ClinDEF, a dynamic framework for assessing clinical reasoning in LLMs through simulated diagnostic dialogues. Grounded in a disease knowledge graph, our method dynamically generates patient cases and facilitates multi-turn interactions between an LLM-based doctor and an automated patient agent. Our evaluation protocol goes beyond diagnostic accuracy by incorporating fine-grained efficiency analysis and rubric-based assessment of diagnostic quality. Experiments show that ClinDEF effectively exposes critical clinical reasoning gaps in state-of-the-art LLMs, offering a more nuanced and clinically meaningful evaluation paradigm.",
        "url": "http://arxiv.org/abs/2512.23440v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23440v1",
        "arxiv_id": "2512.23440v1",
        "authors": [
            "Yuqi Tang",
            "Jing Yu",
            "Zichang Su",
            "Kehua Feng",
            "Zhihui Zhu",
            "Libin Wang",
            "Lei Liang",
            "Qiang Zhang",
            "Keyan Ding",
            "Huajun Chen"
        ],
        "submitted": "2025-12-29 12:58:58",
        "source": "arxiv",
        "comment": "23 pages, 4 figures, under review"
    },
    {
        "title": "C2PO: Diagnosing and Disentangling Bias Shortcuts in LLMs",
        "abstract": "Bias in Large Language Models (LLMs) poses significant risks to trustworthiness, manifesting primarily as stereotypical biases (e.g., gender or racial stereotypes) and structural biases (e.g., lexical overlap or position preferences). However, prior paradigms typically address these in isolation, often mitigating one at the expense of exacerbating the other. To address this, we conduct a systematic exploration of these reasoning failures and identify a primary inducement: the latent spurious feature correlations within the input that drive these erroneous reasoning shortcuts. Driven by these findings, we introduce Causal-Contrastive Preference Optimization (C2PO), a unified alignment framework designed to tackle these specific failures by simultaneously discovering and suppressing these correlations directly within the optimization process. Specifically, C2PO leverages causal counterfactual signals to isolate bias-inducing features from valid reasoning paths, and employs a fairness-sensitive preference update mechanism to dynamically evaluate logit-level contributions and suppress shortcut features. Extensive experiments across multiple benchmarks covering stereotypical bias (BBQ, Unqover), structural bias (MNLI, HANS, Chatbot, MT-Bench), out-of-domain fairness (StereoSet, WinoBias), and general utility (MMLU, GSM8K) demonstrate that C2PO effectively mitigates stereotypical and structural biases while preserving robust general reasoning capabilities.",
        "url": "http://arxiv.org/abs/2512.23430v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23430v1",
        "arxiv_id": "2512.23430v1",
        "authors": [
            "Xuan Feng",
            "Bo An",
            "Tianlong Gu",
            "Liang Chang",
            "Fengrui Hao",
            "Peipeng Yu",
            "Shuai Zhao"
        ],
        "submitted": "2025-12-29 12:49:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Effect of Gender Diversity on Scientific Team Impact: A Team Roles Perspective",
        "abstract": "The influence of gender diversity on the success of scientific teams is of great interest to academia. However, prior findings remain inconsistent, and most studies operationalize diversity in aggregate terms, overlooking internal role differentiation. This limitation obscures a more nuanced understanding of how gender diversity shapes team impact. In particular, the effect of gender diversity across different team roles remains poorly understood. To this end, we define a scientific team as all coauthors of a paper and measure team impact through five-year citation counts. Using author contribution statements, we classified members into leadership and support roles. Drawing on more than 130,000 papers from PLOS journals, most of which are in biomedical-related disciplines, we employed multivariable regression to examine the association between gender diversity in these roles and team impact. Furthermore, we apply a threshold regression model to investigate how team size moderates this relationship. The results show that (1) the relationship between gender diversity and team impact follows an inverted U-shape for both leadership and support groups; (2) teams with an all-female leadership group and an all-male support group achieve higher impact than other team types. Interestingly, (3) the effect of leadership-group gender diversity is significantly negative for small teams but becomes positive and statistically insignificant in large teams. In contrast, the estimates for support-group gender diversity remain significant and positive, regardless of team size.",
        "url": "http://arxiv.org/abs/2512.23429v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23429v1",
        "arxiv_id": "2512.23429v1",
        "authors": [
            "Yi Zhao",
            "Yongjun Zhu",
            "Donghun Kim",
            "Yuzhuo Wang",
            "Heng Zhang",
            "Chao Lu",
            "Chengzhi Zhang"
        ],
        "submitted": "2025-12-29 12:49:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Entropy-Guided Token Dropout: Training Autoregressive Language Models with Limited Domain Data",
        "abstract": "As access to high-quality, domain-specific data grows increasingly scarce, multi-epoch training has become a practical strategy for adapting large language models (LLMs). However, autoregressive models often suffer from performance degradation under repeated data exposure, where overfitting leads to a marked decline in model capability. Through empirical analysis, we trace this degradation to an imbalance in learning dynamics: predictable, low-entropy tokens are learned quickly and come to dominate optimization, while the model's ability to generalize on high-entropy tokens deteriorates with continued training. To address this, we introduce EntroDrop, an entropy-guided token dropout method that functions as structured data regularization. EntroDrop selectively masks low-entropy tokens during training and employs a curriculum schedule to adjust regularization strength in alignment with training progress. Experiments across model scales from 0.6B to 8B parameters show that EntroDrop consistently outperforms standard regularization baselines and maintains robust performance throughout extended multi-epoch training. These findings underscore the importance of aligning regularization with token-level learning dynamics when training on limited data. Our approach offers a promising pathway toward more effective adaptation of LLMs in data-constrained domains.",
        "url": "http://arxiv.org/abs/2512.23422v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23422v1",
        "arxiv_id": "2512.23422v1",
        "authors": [
            "Jiapeng Wang",
            "Yiwen Hu",
            "Yanzipeng Gao",
            "Haoyu Wang",
            "Shuo Wang",
            "Hongyu Lu",
            "Jiaxin Mao",
            "Wayne Xin Zhao",
            "Junyi Li",
            "Xiao Zhang"
        ],
        "submitted": "2025-12-29 12:35:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Theoretical Foundations of Scaling Law in Familial Models",
        "abstract": "Neural scaling laws have become foundational for optimizing large language model (LLM) training, yet they typically assume a single dense model output. This limitation effectively overlooks \"Familial models, a transformative paradigm essential for realizing ubiquitous intelligence across heterogeneous device-edge-cloud hierarchies. Transcending static architectures, familial models integrate early exits with relay-style inference to spawn G deployable sub-models from a single shared backbone. In this work, we theoretically and empirically extend the scaling law to capture this \"one-run, many-models\" paradigm by introducing Granularity (G) as a fundamental scaling variable alongside model size (N) and training tokens (D). To rigorously quantify this relationship, we propose a unified functional form L(N, D, G) and parameterize it using large-scale empirical runs. Specifically, we employ a rigorous IsoFLOP experimental design to strictly isolate architectural impact from computational scale. Across fixed budgets, we systematically sweep model sizes (N) and granularities (G) while dynamically adjusting tokens (D). This approach effectively decouples the marginal cost of granularity from the benefits of scale, ensuring high-fidelity parameterization of our unified scaling law. Our results reveal that the granularity penalty follows a multiplicative power law with an extremely small exponent. Theoretically, this bridges fixed-compute training with dynamic architectures. Practically, it validates the \"train once, deploy many\" paradigm, demonstrating that deployment flexibility is achievable without compromising the compute-optimality of dense baselines.",
        "url": "http://arxiv.org/abs/2512.23407v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23407v1",
        "arxiv_id": "2512.23407v1",
        "authors": [
            "Huan Song",
            "Qingfei Zhao",
            "Ting Long",
            "Shuyu Tian",
            "Hongjun An",
            "Jiawei Shao",
            "Chi Zhang",
            "Xuelong Li"
        ],
        "submitted": "2025-12-29 12:01:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Stepwise-Enhanced Reasoning Framework for Large Language Models Based on External Subgraph Generation",
        "abstract": "Large Language Models (LLMs) have achieved strong performance across a wide range of natural language processing tasks in recent years, including machine translation, text generation, and question answering. As their applications extend to increasingly complex scenarios, however, LLMs continue to face challenges in tasks that require deep reasoning and logical inference. In particular, models trained on large scale textual corpora may incorporate noisy or irrelevant information during generation, which can lead to incorrect predictions or outputs that are inconsistent with factual knowledge. To address this limitation, we propose a stepwise reasoning enhancement framework for LLMs based on external subgraph generation, termed SGR. The proposed framework dynamically constructs query relevant subgraphs from external knowledge bases and leverages their semantic structure to guide the reasoning process. By performing reasoning in a step by step manner over structured subgraphs, SGR reduces the influence of noisy information and improves reasoning accuracy. Specifically, the framework first generates an external subgraph tailored to the input query, then guides the model to conduct multi step reasoning grounded in the subgraph, and finally integrates multiple reasoning paths to produce the final answer. Experimental results on multiple benchmark datasets demonstrate that SGR consistently outperforms strong baselines, indicating its effectiveness in enhancing the reasoning capabilities of LLMs.",
        "url": "http://arxiv.org/abs/2512.23356v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23356v1",
        "arxiv_id": "2512.23356v1",
        "authors": [
            "Xin Zhang",
            "Yang Cao",
            "Baoxing Wu",
            "Xinyi Chen",
            "Kai Song",
            "Siying Li"
        ],
        "submitted": "2025-12-29 10:35:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AI Meets Brain: Memory Systems from Cognitive Neuroscience to Autonomous Agents",
        "abstract": "Memory serves as the pivotal nexus bridging past and future, providing both humans and AI systems with invaluable concepts and experience to navigate complex tasks. Recent research on autonomous agents has increasingly focused on designing efficient memory workflows by drawing on cognitive neuroscience. However, constrained by interdisciplinary barriers, existing works struggle to assimilate the essence of human memory mechanisms. To bridge this gap, we systematically synthesizes interdisciplinary knowledge of memory, connecting insights from cognitive neuroscience with LLM-driven agents. Specifically, we first elucidate the definition and function of memory along a progressive trajectory from cognitive neuroscience through LLMs to agents. We then provide a comparative analysis of memory taxonomy, storage mechanisms, and the complete management lifecycle from both biological and artificial perspectives. Subsequently, we review the mainstream benchmarks for evaluating agent memory. Additionally, we explore memory security from dual perspectives of attack and defense. Finally, we envision future research directions, with a focus on multimodal memory systems and skill acquisition.",
        "url": "http://arxiv.org/abs/2512.23343v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23343v1",
        "arxiv_id": "2512.23343v1",
        "authors": [
            "Jiafeng Liang",
            "Hao Li",
            "Chang Li",
            "Jiaqi Zhou",
            "Shixin Jiang",
            "Zekun Wang",
            "Changkai Ji",
            "Zhihao Zhu",
            "Runxuan Liu",
            "Tao Ren",
            "Jinlan Fu",
            "See-Kiong Ng",
            "Xia Liang",
            "Ming Liu",
            "Bing Qin"
        ],
        "submitted": "2025-12-29 10:01:32",
        "source": "arxiv",
        "comment": "57 pages, 5 figures"
    },
    {
        "title": "CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations",
        "abstract": "Large Language Model (LLM) agents, while proficient in the digital realm, face a significant gap in physical-world deployment due to the challenge of forming and maintaining a robust spatial mental model. We identify three core cognitive challenges hindering this transition: spatial reasoning, long-horizon state tracking via mental simulation, and active exploration under partial observation. To isolate and evaluate these faculties, we introduce CubeBench, a novel generative benchmark centered on the Rubik's Cube. CubeBench uses a three-tiered diagnostic framework that progressively assesses agent capabilities, from foundational state tracking with full symbolic information to active exploration with only partial visual data. Our experiments on leading LLMs reveal critical limitations, including a uniform 0.00% pass rate on all long-horizon tasks, exposing a fundamental failure in long-term planning. We also propose a diagnostic framework to isolate these cognitive bottlenecks by providing external solver tools. By analyzing the failure modes, we provide key insights to guide the development of more physically-grounded intelligent agents.",
        "url": "http://arxiv.org/abs/2512.23328v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23328v1",
        "arxiv_id": "2512.23328v1",
        "authors": [
            "Huan-ang Gao",
            "Zikang Zhang",
            "Tianwei Luo",
            "Kaisen Yang",
            "Xinzhe Juan",
            "Jiahao Qiu",
            "Tianxing Chen",
            "Bingxiang He",
            "Hao Zhao",
            "Hao Zhou",
            "Shilong Liu",
            "Mengdi Wang"
        ],
        "submitted": "2025-12-29 09:25:56",
        "source": "arxiv",
        "comment": "Webpage: https://cubebench.c7w.tech/"
    },
    {
        "title": "RobustMask: Certified Robustness against Adversarial Neural Ranking Attack via Randomized Masking",
        "abstract": "Neural ranking models have achieved remarkable progress and are now widely deployed in real-world applications such as Retrieval-Augmented Generation (RAG). However, like other neural architectures, they remain vulnerable to adversarial manipulations: subtle character-, word-, or phrase-level perturbations can poison retrieval results and artificially promote targeted candidates, undermining the integrity of search engines and downstream systems. Existing defenses either rely on heuristics with poor generalization or on certified methods that assume overly strong adversarial knowledge, limiting their practical use. To address these challenges, we propose RobustMask, a novel defense that combines the context-prediction capability of pretrained language models with a randomized masking-based smoothing mechanism. Our approach strengthens neural ranking models against adversarial perturbations at the character, word, and phrase levels. Leveraging both the pairwise comparison ability of ranking models and probabilistic statistical analysis, we provide a theoretical proof of RobustMask's certified top-K robustness. Extensive experiments further demonstrate that RobustMask successfully certifies over 20% of candidate documents within the top-10 ranking positions against adversarial perturbations affecting up to 30% of their content. These results highlight the effectiveness of RobustMask in enhancing the adversarial robustness of neural ranking models, marking a significant step toward providing stronger security guarantees for real-world retrieval systems.",
        "url": "http://arxiv.org/abs/2512.23307v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23307v1",
        "arxiv_id": "2512.23307v1",
        "authors": [
            "Jiawei Liu",
            "Zhuo Chen",
            "Rui Zhu",
            "Miaokun Chen",
            "Yuyang Gong",
            "Wei Lu",
            "Xiaofeng Wang"
        ],
        "submitted": "2025-12-29 08:51:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AI4Reading: Chinese Audiobook Interpretation System Based on Multi-Agent Collaboration",
        "abstract": "Audiobook interpretations are attracting increasing attention, as they provide accessible and in-depth analyses of books that offer readers practical insights and intellectual inspiration. However, their manual creation process remains time-consuming and resource-intensive. To address this challenge, we propose AI4Reading, a multi-agent collaboration system leveraging large language models (LLMs) and speech synthesis technology to generate podcast, like audiobook interpretations. The system is designed to meet three key objectives: accurate content preservation, enhanced comprehensibility, and a logical narrative structure. To achieve these goals, we develop a framework composed of 11 specialized agents,including topic analysts, case analysts, editors, a narrator, and proofreaders that work in concert to explore themes, extract real world cases, refine content organization, and synthesize natural spoken language. By comparing expert interpretations with our system's output, the results show that although AI4Reading still has a gap in speech generation quality, the generated interpretative scripts are simpler and more accurate.",
        "url": "http://arxiv.org/abs/2512.23300v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23300v1",
        "arxiv_id": "2512.23300v1",
        "authors": [
            "Minjiang Huang",
            "Jipeng Qiang",
            "Yi Zhu",
            "Chaowei Zhang",
            "Xiangyu Zhao",
            "Kui Yu"
        ],
        "submitted": "2025-12-29 08:41:54",
        "source": "arxiv",
        "comment": "ACL 2025 demo"
    },
    {
        "title": "Chinese Morph Resolution in E-commerce Live Streaming Scenarios",
        "abstract": "E-commerce live streaming in China, particularly on platforms like Douyin, has become a major sales channel, but hosts often use morphs to evade scrutiny and engage in false advertising. This study introduces the Live Auditory Morph Resolution (LiveAMR) task to detect such violations. Unlike previous morph research focused on text-based evasion in social media and underground industries, LiveAMR targets pronunciation-based evasion in health and medical live streams. We constructed the first LiveAMR dataset with 86,790 samples and developed a method to transform the task into a text-to-text generation problem. By leveraging large language models (LLMs) to generate additional training data, we improved performance and demonstrated that morph resolution significantly enhances live streaming regulation.",
        "url": "http://arxiv.org/abs/2512.23280v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23280v1",
        "arxiv_id": "2512.23280v1",
        "authors": [
            "Jiahao Zhu",
            "Jipeng Qiang",
            "Ran Bai",
            "Chenyu Liu",
            "Xiaoye Ouyang"
        ],
        "submitted": "2025-12-29 08:04:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Interpretable Safety Alignment via SAE-Constructed Low-Rank Subspace Adaptation",
        "abstract": "Parameter-efficient fine-tuning has become the dominant paradigm for adapting large language models to downstream tasks. Low-rank adaptation methods such as LoRA operate under the assumption that task-relevant weight updates reside in a low-rank subspace, yet this subspace is learned implicitly from data in a black-box manner, offering no interpretability or direct control. We hypothesize that this difficulty stems from polysemanticity--individual dimensions encoding multiple entangled concepts. To address this, we leverage pre-trained Sparse Autoencoders (SAEs) to identify task-relevant features in a disentangled feature space, then construct an explicit, interpretable low-rank subspace to guide adapter initialization. We provide theoretical analysis proving that under monosemanticity assumptions, SAE-based subspace identification achieves arbitrarily small recovery error, while direct identification in polysemantic space suffers an irreducible error floor. On safety alignment, our method achieves up to 99.6% safety rate--exceeding full fine-tuning by 7.4 percentage points and approaching RLHF-based methods--while updating only 0.19-0.24% of parameters. Crucially, our method provides interpretable insights into the learned alignment subspace through the semantic grounding of SAE features. Our work demonstrates that incorporating mechanistic interpretability into the fine-tuning process can simultaneously improve both performance and transparency.",
        "url": "http://arxiv.org/abs/2512.23260v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23260v1",
        "arxiv_id": "2512.23260v1",
        "authors": [
            "Dianyun Wang",
            "Qingsen Ma",
            "Yuhu Shang",
            "Zhifeng Lu",
            "Lechen Ning",
            "Zhenbo Xu",
            "Huijia Wu",
            "Zhaofeng He"
        ],
        "submitted": "2025-12-29 07:39:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Anka: A Domain-Specific Language for Reliable LLM Code Generation",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in code generation, yet they exhibit systematic errors on complex, multi-step programming tasks. We hypothesize that these errors stem from the flexibility of general-purpose languages, which permits multiple valid approaches and requires implicit state management. To test this hypothesis, we introduce Anka, a domain-specific language (DSL) for data transformation pipelines designed with explicit, constrained syntax that reduces ambiguity in code generation. Despite having zero prior training exposure to Anka, Claude 3.5 Haiku achieves 99.9% parse success and 95.8% overall task accuracy across 100 benchmark problems. Critically, Anka demonstrates a 40 percentage point accuracy advantage over Python on multi-step pipeline tasks (100% vs. 60%), where Python's flexible syntax leads to frequent errors in operation sequencing and variable management. Cross-model validation with GPT-4o-mini confirms this advantage (+26.7 percentage points on multi-step tasks). Our results demonstrate that: (1) LLMs can learn novel DSLs entirely from in-context prompts, achieving near-native accuracy; (2) constrained syntax significantly reduces errors on complex tasks; and (3) domain-specific languages purposefully designed for LLM generation can outperform general-purpose languages on which the LLM has extensive training. We release the complete language implementation, benchmark suite, and evaluation framework to facilitate further research.",
        "url": "http://arxiv.org/abs/2512.23214v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23214v1",
        "arxiv_id": "2512.23214v1",
        "authors": [
            "Saif Khalfan Saif Al Mazrouei"
        ],
        "submitted": "2025-12-29 05:28:17",
        "source": "arxiv",
        "comment": "11 pages, 1 figure, 4 tables. Code and benchmarks available at https://github.com/BleBlo/Anka"
    },
    {
        "title": "Scoring, Reasoning, and Selecting the Best! Ensembling Large Language Models via a Peer-Review Process",
        "abstract": "We propose LLM-PeerReview, an unsupervised LLM Ensemble method that selects the most ideal response from multiple LLM-generated candidates for each query, harnessing the collective wisdom of multiple models with diverse strengths. LLM-PeerReview is built on a novel, peer-review-inspired framework that offers a clear and interpretable mechanism, while remaining fully unsupervised for flexible adaptability and generalization. Specifically, it operates in three stages: For scoring, we use the emerging LLM-as-a-Judge technique to evaluate each response by reusing multiple LLMs at hand; For reasoning, we can apply a principled graphical model-based truth inference algorithm or a straightforward averaging strategy to aggregate multiple scores to produce a final score for each response; Finally, the highest-scoring response is selected as the best ensemble output. LLM-PeerReview is conceptually simple and empirically powerful. The two variants of the proposed approach obtain strong results across four datasets, including outperforming the recent advanced model Smoothie-Global by 6.9% and 7.3% points, respectively.",
        "url": "http://arxiv.org/abs/2512.23213v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23213v1",
        "arxiv_id": "2512.23213v1",
        "authors": [
            "Zhijun Chen",
            "Zeyu Ji",
            "Qianren Mao",
            "Junhang Cheng",
            "Bangjie Qin",
            "Hao Wu",
            "Zhuoran Li",
            "Jingzheng Li",
            "Kai Sun",
            "Zizhe Wang",
            "Yikun Ban",
            "Zhu Sun",
            "Xiangyang Ji",
            "Hailong Sun"
        ],
        "submitted": "2025-12-29 05:25:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Not too long do read: Evaluating LLM-generated extreme scientific summaries",
        "abstract": "High-quality scientific extreme summary (TLDR) facilitates effective science communication. How do large language models (LLMs) perform in generating them? How are LLM-generated summaries different from those written by human experts? However, the lack of a comprehensive, high-quality scientific TLDR dataset hinders both the development and evaluation of LLMs' summarization ability. To address these, we propose a novel dataset, BiomedTLDR, containing a large sample of researcher-authored summaries from scientific papers, which leverages the common practice of including authors' comments alongside bibliography items. We then test popular open-weight LLMs for generating TLDRs based on abstracts. Our analysis reveals that, although some of them successfully produce humanoid summaries, LLMs generally exhibit a greater affinity for the original text's lexical choices and rhetorical structures, hence tend to be more extractive rather than abstractive in general, compared to humans. Our code and datasets are available at https://github.com/netknowledge/LLM_summarization (Lyu and Ke, 2025).",
        "url": "http://arxiv.org/abs/2512.23206v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23206v1",
        "arxiv_id": "2512.23206v1",
        "authors": [
            "Zhuoqi Lyu",
            "Qing Ke"
        ],
        "submitted": "2025-12-29 05:03:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Reservoir Computing inspired Matrix Multiplication-free Language Model",
        "abstract": "Large language models (LLMs) have achieved state-of-the-art performance in natural language processing; however, their high computational cost remains a major bottleneck. In this study, we target computational efficiency by focusing on a matrix multiplication free language model (MatMul-free LM) and further reducing the training cost through an architecture inspired by reservoir computing. Specifically, we partially fix and share the weights of selected layers in the MatMul-free LM and insert reservoir layers to obtain rich dynamic representations without additional training overhead. Additionally, several operations are combined to reduce memory accesses. Experimental results show that the proposed architecture reduces the number of parameters by up to 19%, training time by 9.9%, and inference time by 8.0%, while maintaining comparable performance to the baseline model.",
        "url": "http://arxiv.org/abs/2512.23145v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23145v1",
        "arxiv_id": "2512.23145v1",
        "authors": [
            "Takumi Shiratsuchi",
            "Yuichiro Tanaka",
            "Hakaru Tamukoh"
        ],
        "submitted": "2025-12-29 02:20:37",
        "source": "arxiv",
        "comment": "9 pages, 10 figures"
    },
    {
        "title": "A Note on Hybrid Online Reinforcement and Imitation Learning for LLMs: Formulations and Algorithms",
        "abstract": "We present a unified framework for Large Language Model (LLM) fine-tuning that integrates Imitation Learning and Reinforcement Learning. By analyzing the gradient of a composite objective combining trajectory-level KL divergence with task rewards, we derive a natural decomposition into two components: (1) an analytically computable Dense Gradient for token-level imitation, and (2) a Monte Carlo estimated Sparse Gradient for long-horizon reward optimization. The Dense Gradient admits a closed-form logit-level formula, enabling efficient GPU implementation.",
        "url": "http://arxiv.org/abs/2512.23097v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23097v1",
        "arxiv_id": "2512.23097v1",
        "authors": [
            "Yingru Li",
            "Ziniu Li",
            "Jiacai Liu"
        ],
        "submitted": "2025-12-28 22:25:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TabiBERT: A Large-Scale ModernBERT Foundation Model and Unified Benchmarking Framework for Turkish",
        "abstract": "Since the inception of BERT, encoder-only Transformers have evolved significantly in computational efficiency, training stability, and long-context modeling. ModernBERT consolidates these advances by integrating Rotary Positional Embeddings (RoPE), FlashAttention, and refined normalization. Despite these developments, Turkish NLP lacks a monolingual encoder trained from scratch incorporating such modern architectural paradigms. This work introduces TabiBERT, a monolingual Turkish encoder based on ModernBERT architecture trained from scratch on a large, curated corpus. TabiBERT is pre-trained on one trillion tokens sampled from an 84.88B token multi-domain corpus: web text (73%), scientific publications (20%), source code (6%), and mathematical content (0.3%). The model supports 8,192-token context length (16x original BERT), achieves up to 2.65x inference speedup, and reduces GPU memory consumption, enabling larger batch sizes. We introduce TabiBench with 28 datasets across eight task categories with standardized splits and protocols, evaluated using GLUE-style macro-averaging. TabiBERT attains 77.58 on TabiBench, outperforming BERTurk by 1.62 points and establishing state-of-the-art on five of eight categories: question answering (+9.55), code retrieval (+2.41), and document retrieval (+0.60). Compared with task-specific prior best results, including specialized models like TurkishBERTweet, TabiBERT achieves +1.47 average improvement, indicating robust cross-domain generalization. We release model weights, training configurations, and evaluation code for transparent, reproducible Turkish encoder research.",
        "url": "http://arxiv.org/abs/2512.23065v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23065v1",
        "arxiv_id": "2512.23065v1",
        "authors": [
            "Melikşah Türker",
            "A. Ebrar Kızıloğlu",
            "Onur Güngör",
            "Susan Üsküdarlı"
        ],
        "submitted": "2025-12-28 20:18:22",
        "source": "arxiv",
        "comment": "31 pages, 1 figure, 13 tables"
    },
    {
        "title": "Accelerating Language Model Workflows with Prompt Choreography",
        "abstract": "Large language models are increasingly deployed in multi-agent workflows. We introduce Prompt Choreography, a framework that efficiently executes LLM workflows by maintaining a dynamic, global KV cache. Each LLM call can attend to an arbitrary, reordered subset of previously encoded messages. Parallel calls are supported. Though caching messages' encodings sometimes gives different results from re-encoding them in a new context, we show in diverse settings that fine-tuning the LLM to work with the cache can help it mimic the original results. Prompt Choreography significantly reduces per-message latency (2.0--6.2$\\times$ faster time-to-first-token) and achieves substantial end-to-end speedups ($>$2.2$\\times$) in some workflows dominated by redundant computation.",
        "url": "http://arxiv.org/abs/2512.23049v1",
        "pdf_url": "https://arxiv.org/pdf/2512.23049v1",
        "arxiv_id": "2512.23049v1",
        "authors": [
            "TJ Bai",
            "Jason Eisner"
        ],
        "submitted": "2025-12-28 19:21:11",
        "source": "arxiv",
        "comment": "to appear in TACL (final preprint of 2025-10-12); 10 pages + appendices"
    }
]