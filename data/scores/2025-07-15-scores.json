[
    {
        "title": "User Long-Term Multi-Interest Retrieval Model for Recommendation",
        "abstract": "User behavior sequence modeling, which captures user interest from rich\nhistorical interactions, is pivotal for industrial recommendation systems.\nDespite breakthroughs in ranking-stage models capable of leveraging ultra-long\nbehavior sequences with length scaling up to thousands, existing retrieval\nmodels remain constrained to sequences of hundreds of behaviors due to two main\nchallenges. One is strict latency budget imposed by real-time service over\nlarge-scale candidate pool. The other is the absence of target-aware mechanisms\nand cross-interaction architectures, which prevent utilizing ranking-like\ntechniques to simplify long sequence modeling. To address these limitations, we\npropose a new framework named User Long-term Multi-Interest Retrieval\nModel(ULIM), which enables thousand-scale behavior modeling in retrieval\nstages. ULIM includes two novel components: 1)Category-Aware Hierarchical\nDual-Interest Learning partitions long behavior sequences into multiple\ncategory-aware subsequences representing multi-interest and jointly optimizes\nlong-term and short-term interests within specific interest cluster.\n2)Pointer-Enhanced Cascaded Category-to-Item Retrieval introduces\nPointer-Generator Interest Network(PGIN) for next-category prediction, followed\nby next-item retrieval upon the top-K predicted categories. Comprehensive\nexperiments on Taobao dataset show that ULIM achieves substantial improvement\nover state-of-the-art methods, and brings 5.54% clicks, 11.01% orders and 4.03%\nGMV lift for Taobaomiaosha, a notable mini-app of Taobao.",
        "url": "http://arxiv.org/abs/2507.10097v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10097v1",
        "arxiv_id": "2507.10097v1",
        "authors": [
            "Yue Meng",
            "Cheng Guo",
            "Xiaohui Hu",
            "Honghu Deng",
            "Yi Cao",
            "Tong Liu",
            "Bo Zheng"
        ],
        "submitted": "2025-07-14 09:32:26",
        "source": "arxiv",
        "comment": null,
        "score": 16,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'ltr' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'user behavior' (score: +2)",
            "Found 'click' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval and Search technologies, particularly in the context of user behavior modeling and recommendation systems. However, the focus on recommendation systems and e-commerce domain is not a central match to your primary interests in query understanding, ranking models, and deep semantic understanding."
    },
    {
        "title": "Am I on the Right Track? What Can Predicted Query Performance Tell Us about the Search Behaviour of Agentic RAG",
        "abstract": "Agentic Retrieval-Augmented Generation (RAG) is a new paradigm where the\nreasoning model decides when to invoke a retriever (as a \"tool\") when answering\na question. This paradigm, exemplified by recent research works such as\nSearch-R1, enables the model to decide when to search and obtain external\ninformation. However, the queries generated by such Agentic RAG models and the\nrole of the retriever in obtaining high-quality answers remain understudied. To\nthis end, this initial study examines the applicability of query performance\nprediction (QPP) within the recent Agentic RAG models Search-R1 and\nR1-Searcher. We find that applying effective retrievers can achieve higher\nanswer quality within a shorter reasoning process. Moreover, the QPP estimates\nof the generated queries, used as an approximation of their retrieval quality,\nare positively correlated with the quality of the final answer. Ultimately, our\nwork is a step towards adaptive retrieval within Agentic RAG, where QPP is used\nto inform the model if the retrieved results are likely to be useful.",
        "url": "http://arxiv.org/abs/2507.10411v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10411v1",
        "arxiv_id": "2507.10411v1",
        "authors": [
            "Fangzheng Tian",
            "Jinyuan Fang",
            "Debasis Ganguly",
            "Zaiqiao Meng",
            "Craig Macdonald"
        ],
        "submitted": "2025-07-14 15:54:50",
        "source": "arxiv",
        "comment": null,
        "score": 14,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper explores Agentic Retrieval-Augmented Generation (RAG) models, which is a relevant area in Information Retrieval. The study focuses on query performance prediction (QPP) and its application in RAG models, aligning with your interests in query understanding and ranking models. However, the specific domain of RAG models is not explicitly mentioned in your research interests, which prevents a perfect match."
    },
    {
        "title": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks",
        "abstract": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance.",
        "url": "http://arxiv.org/abs/2507.10535v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10535v1",
        "arxiv_id": "2507.10535v1",
        "authors": [
            "Hongchao Jiang",
            "Yiming Chen",
            "Yushi Cao",
            "Hung-yi Lee",
            "Robby T. Tan"
        ],
        "submitted": "2025-07-14 17:56:29",
        "source": "arxiv",
        "comment": "Dataset is available at\n  https://huggingface.co/datasets/mattymchen/codejudgebench",
        "score": 10,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'pairwise' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Natural Language Processing (NLP) and Information Retrieval (IR), particularly in the context of Large Language Models (LLMs) and their applications in coding tasks. However, it does not directly align with your primary focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware Query Optimization",
        "abstract": "Scientific paper retrieval, particularly framed as document-to-document\nretrieval, aims to identify relevant papers in response to a long-form query\npaper, rather than a short query string. Previous approaches to this task have\nfocused on abstracts, embedding them into dense vectors as surrogates for full\ndocuments and calculating similarity across them, although abstracts provide\nonly sparse and high-level summaries. To address this, we propose PRISM, a\nnovel document-to-document retrieval method that introduces multiple,\nfine-grained representations for both the query and candidate papers. In\nparticular, each query paper is decomposed into multiple aspect-specific views\nand individually embedded, which are then matched against candidate papers\nsimilarity segmented to consider their multifaceted dimensions. Moreover, we\npresent SciFullBench, a novel benchmark in which the complete and segmented\ncontext of full papers for both queries and candidates is available. Then,\nexperimental results show that PRISM improves performance by an average of 4.3%\nover existing retrieval baselines.",
        "url": "http://arxiv.org/abs/2507.10057v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10057v1",
        "arxiv_id": "2507.10057v1",
        "authors": [
            "Sangwoo Park",
            "Jinheon Baek",
            "Soyeong Jeong",
            "Sung Ju Hwang"
        ],
        "submitted": "2025-07-14 08:41:53",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to Information Retrieval, specifically document-to-document retrieval, and introduces a novel method (PRISM) that uses fine-grained representations for query and candidate papers. The focus on query understanding and relevance optimization aligns with your research interests. The experimental results and benchmark also demonstrate the practical application of the proposed method."
    },
    {
        "title": "Non-parametric Graph Convolution for Re-ranking in Recommendation Systems",
        "abstract": "Graph knowledge has been proven effective in enhancing item rankings in\nrecommender systems (RecSys), particularly during the retrieval stage. However,\nits application in the ranking stage, especially when richer contextual\ninformation in user-item interactions is available, remains underexplored. A\nmajor challenge lies in the substantial computational cost associated with\nrepeatedly retrieving neighborhood information from billions of items stored in\ndistributed systems. This resource-intensive requirement makes it difficult to\nscale graph-based methods in practical RecSys. To bridge this gap, we first\ndemonstrate that incorporating graphs in the ranking stage improves ranking\nqualities. Notably, while the improvement is evident, we show that the\nsubstantial computational overheads entailed by graphs are prohibitively\nexpensive for real-world recommendations. In light of this, we propose a\nnon-parametric strategy that utilizes graph convolution for re-ranking only\nduring test time. Our strategy circumvents the notorious computational\noverheads from graph convolution during training, and utilizes structural\nknowledge hidden in graphs on-the-fly during testing. It can be used as a\nplug-and-play module and easily employed to enhance the ranking ability of\nvarious ranking layers of a real-world RecSys with significantly reduced\ncomputational overhead. Through comprehensive experiments across four benchmark\ndatasets with varying levels of sparsity, we demonstrate that our strategy\nyields noticeable improvements (i.e., 8.1% on average) during testing time with\nlittle to no additional computational overheads (i.e., 0.5 on average). Code:\nhttps://github.com/zyouyang/RecSys2025_NonParamGC.git",
        "url": "http://arxiv.org/abs/2507.09969v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09969v1",
        "arxiv_id": "2507.09969v1",
        "authors": [
            "Zhongyu Ouyang",
            "Mingxuan Ju",
            "Soroush Vosoughi",
            "Yanfang Ye"
        ],
        "submitted": "2025-07-14 06:35:18",
        "source": "arxiv",
        "comment": "Accepted to RecSys2025 Main",
        "score": 10,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'recsys' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on recommender systems and graph convolution for re-ranking, which is somewhat related to your interests in Information Retrieval and Search technologies. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of your research."
    },
    {
        "title": "EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions",
        "abstract": "Hacker forums provide critical early warning signals for emerging\ncybersecurity threats, but extracting actionable intelligence from their\nunstructured and noisy content remains a significant challenge. This paper\npresents an unsupervised framework that automatically detects, clusters, and\nprioritizes security events discussed across hacker forum posts. Our approach\nleverages Transformer-based embeddings fine-tuned with contrastive learning to\ngroup related discussions into distinct security event clusters, identifying\nincidents like zero-day disclosures or malware releases without relying on\npredefined keywords. The framework incorporates a daily ranking mechanism that\nprioritizes identified events using quantifiable metrics reflecting timeliness,\nsource credibility, information completeness, and relevance. Experimental\nevaluation on real-world hacker forum data demonstrates that our method\neffectively reduces noise and surfaces high-priority threats, enabling security\nanalysts to mount proactive responses. By transforming disparate hacker forum\ndiscussions into structured, actionable intelligence, our work addresses\nfundamental challenges in automated threat detection and analysis.",
        "url": "http://arxiv.org/abs/2507.09762v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09762v1",
        "arxiv_id": "2507.09762v1",
        "authors": [
            "Yasir Ech-Chammakhy",
            "Anas Motii",
            "Anass Rabii",
            "Jaafar Chbili"
        ],
        "submitted": "2025-07-13 19:40:36",
        "source": "arxiv",
        "comment": "Accepted for publication at the 28th International Symposium on\n  Research in Attacks, Intrusions, and Defenses (RAID 2025)",
        "score": 9,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to Information Retrieval, specifically in the context of extracting actionable intelligence from unstructured content. However, its focus on cybersecurity threats and hacker forum discussions is not directly aligned with the user's core research themes in e-commerce and deep semantic understanding. The use of Transformer-based embeddings and ranking mechanisms is relevant to the user's interests in NLP and ranking models, but the application is quite different."
    },
    {
        "title": "HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong",
        "abstract": "This paper presents the development of HKGAI-V1, a foundational sovereign\nlarge language model (LLM), developed as part of an initiative to establish\nvalue-aligned AI infrastructure specifically tailored for Hong Kong. Addressing\nthe region's unique multilingual environment (Cantonese, Mandarin, and\nEnglish), its distinct socio-legal context under the \"one country, two systems\"\nframework, and specific local cultural and value considerations, the model is\nbuilt upon the DeepSeek architecture and systematically aligned with regional\nnorms through a multifaceted full parameter fine-tuning process. It is further\nintegrated with a retrieval-augmented generation (RAG) system to ensure timely\nand factually grounded information access. The core contribution lies in the\ndesign and implementation of a comprehensive, region-specific AI alignment and\nsafety framework, demonstrated through two key achievements: 1) The successful\ndevelopment of HKGAI-V1 itself - which outper-forms general-purpose models in\nhandling Hong Kong-specific culturally sensitive queries, and embodies a\n\"governance-embedded\" approach to digital sovereignty - empowers Hong Kong to\nexercise control over AI applications in critical sectors including public\nservices, legal systems, and edu-cation. 2) The development of the proprietary\nAdversarial HK Value Benchmark, a rigorous tool for evaluating model alignment\nwith local ethical and legal stand-ards under challenging conditions. By\ndocumenting these achievements, the paper provides not only a technological\nartifact but also a replicable blueprint for developing advanced, regionally\nfocused AI systems deeply rooted in their local identities.",
        "url": "http://arxiv.org/abs/2507.11502v1",
        "pdf_url": "http://arxiv.org/pdf/2507.11502v1",
        "arxiv_id": "2507.11502v1",
        "authors": [
            "Sirui Han",
            "Junqi Zhu",
            "Ruiyuan Zhang",
            "Yike Guo"
        ],
        "submitted": "2025-07-14 15:09:05",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on developing a large language model for Hong Kong, addressing regional multilingualism and cultural considerations. While it involves a retrieval-augmented generation system, the primary focus is on AI alignment and safety, rather than information retrieval or search technologies."
    },
    {
        "title": "Riding the Carousel: The First Extensive Eye Tracking Analysis of Browsing Behavior in Carousel Recommenders",
        "abstract": "Carousels have become the de-facto interface in online services. However,\nthere is a lack of research in carousels, particularly examining how\nrecommender systems may be designed differently than the traditional\nsingle-list interfaces. One of the key elements for understanding how to design\na system for a particular interface is understanding how users browse. For\ncarousels, users may browse in a number of different ways due to the added\ncomplexity of multiple topic defined-lists and swiping to see more items.\n  Eye tracking is the key to understanding user behavior by providing valuable,\ndirect information on how users see and navigate. In this work, we provide the\nfirst extensive analysis of the eye tracking behavior in carousel recommenders\nunder the free-browsing setting. To understand how users browse, we examine the\nfollowing research questions : 1) where do users start browsing, 2) how do\nusers transition from item to item within the same carousel and across\ncarousels, and 3) how does genre preference impact transitions?\n  This work addresses a gap in the field and provides the first extensive\nempirical results of eye tracked browsing behavior in carousels for improving\nrecommenders. Taking into account the insights learned from the above\nquestions, our final contribution is to provide suggestions to help carousel\nrecommender system designers optimize their systems for user browsing behavior.\nThe most important suggestion being to reorder the ranked item positions to\naccount for browsing after swiping.These contributions aim not only to help\nimprove current systems, but also to encourage and allow the design of new user\nmodels, systems, and metrics that are better suited to the complexity of\ncarousel interfaces.",
        "url": "http://arxiv.org/abs/2507.10135v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10135v1",
        "arxiv_id": "2507.10135v1",
        "authors": [
            "Santiago de Leon-Martinez",
            "Robert Moro",
            "Branislav Kveton",
            "Maria Bielikova"
        ],
        "submitted": "2025-07-14 10:26:27",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'user behavior' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval and Search technologies, particularly in the area of recommender systems. However, it focuses more on user behavior modeling and eye-tracking analysis in carousel recommenders, which is a specific application rather than a core theme. While it may provide insights useful for improving recommender systems, it does not directly address query understanding, ranking models, or deep semantic understanding."
    },
    {
        "title": "Automating SPARQL Query Translations between DBpedia and Wikidata",
        "abstract": "This paper investigates whether state-of-the-art Large Language Models (LLMs)\ncan automatically translate SPARQL between popular Knowledge Graph (KG)\nschemas. We focus on translations between the DBpedia and Wikidata KG, and\nlater on DBLP and OpenAlex KG. This study addresses a notable gap in KG\ninteroperability research by rigorously evaluating LLM performance on\nSPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first\nalign 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100\nDBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic\nKGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and\nMistral-Large-Instruct-2407 are selected based on their sizes and architectures\nand tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs\nwere compared with gold answers, and resulting errors were categorized. We find\nthat the performance varies markedly across models and prompting strategies,\nand that translations for Wikidata to DBpedia work far better than translations\nfor DBpedia to Wikidata.",
        "url": "http://arxiv.org/abs/2507.10045v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10045v1",
        "arxiv_id": "2507.10045v1",
        "authors": [
            "Malte Christian Bartels",
            "Debayan Banerjee",
            "Ricardo Usbeck"
        ],
        "submitted": "2025-07-14 08:23:25",
        "source": "arxiv",
        "comment": "18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference\n  happening on September 2025",
        "score": 7,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores query translation between Knowledge Graph schemas using Large Language Models, which is somewhat related to query understanding in Information Retrieval. However, the focus on SPARQL query translation and Knowledge Graph interoperability is not directly aligned with the user's primary research interests in search technologies, ranking models, and user behavior modeling."
    },
    {
        "title": "Multiple Choice Learning of Low Rank Adapters for Language Modeling",
        "abstract": "We propose LoRA-MCL, a training scheme that extends next-token prediction in\nlanguage models with a method designed to decode diverse, plausible sentence\ncontinuations at inference time. Traditional language modeling is an\nintrinsically ill-posed problem: given a context, multiple futures may be\nequally plausible. Our approach leverages Multiple Choice Learning (MCL) and\nthe Winner-Takes-All (WTA) loss to efficiently handle ambiguity through\nLow-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying\nMultiple Choice Learning to Language Modeling, assuming the data is generated\nfrom a mixture of distributions. To illustrate the proposed approach, we use\ndata sampled from mixtures of Markov chains. We then demonstrate with extensive\nexperiments on real-world visual and audio captioning tasks that our method\nachieves high diversity and relevance in generated outputs.",
        "url": "http://arxiv.org/abs/2507.10419v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10419v1",
        "arxiv_id": "2507.10419v1",
        "authors": [
            "Victor Letzelter",
            "Hugo Malard",
            "Mathieu Fontaine",
            "Gaël Richard",
            "Slim Essid",
            "Andrei Bursuc",
            "Patrick Pérez"
        ],
        "submitted": "2025-07-14 16:00:51",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on language modeling and adapter learning, which is somewhat related to your interests in NLP and deep semantic understanding. However, it does not directly address information retrieval, search technologies, or query understanding, making it less relevant to your primary research focus."
    },
    {
        "title": "Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking",
        "abstract": "Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies\nfor retrieval, which enhance large language models (LLMs) by enabling them to\naccess external knowledge, ensuring that the retrieved information is\nup-to-date and domain-specific. However, traditional methods often fail to\ncreate chunks that capture sufficient semantic meaning, as they do not account\nfor the underlying textual structure. This paper proposes a novel framework\nthat enhances RAG by integrating hierarchical text segmentation and clustering\nto generate more meaningful and semantically coherent chunks. During inference,\nthe framework retrieves information by leveraging both segment-level and\ncluster-level vector representations, thereby increasing the likelihood of\nretrieving more precise and contextually relevant information. Evaluations on\nthe NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method\nachieved improved results compared to traditional chunking techniques.",
        "url": "http://arxiv.org/abs/2507.09935v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09935v1",
        "arxiv_id": "2507.09935v1",
        "authors": [
            "Hai Toan Nguyen",
            "Tien Dat Nguyen",
            "Viet Ha Nguyen"
        ],
        "submitted": "2025-07-14 05:21:58",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to Information Retrieval, particularly in the context of Retrieval-Augmented Generation (RAG) systems, which is a key area of interest. The use of hierarchical text segmentation and clustering to improve chunking strategies aligns with the user's focus on query understanding and ranking models. While the paper's focus is on NLP, its relevance to IR and deep semantic understanding makes it a useful contribution."
    },
    {
        "title": "Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources",
        "abstract": "Retrieving relevant imagery from vast satellite archives is crucial for\napplications like disaster response and long-term climate monitoring. However,\nmost text-to-image retrieval systems are limited to RGB data, failing to\nexploit the unique physical information captured by other sensors, such as the\nall-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the\nspectral signatures in optical multispectral data. To bridge this gap, we\nintroduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1\nSAR and Sentinel-2 multispectral images paired with structured textual\nannotations for land cover, land use, and crisis events harmonized from\nauthoritative land cover systems (CORINE and Dynamic World) and crisis-specific\nsources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),\na novel framework that uses text as a bridge to align unpaired optical and SAR\nimages into a unified embedding space. Our experiments show that CLOSP achieves\na new state-of-the-art, improving retrieval nDGC by 54% over existing models.\nAdditionally, we find that the unified training strategy overcomes the inherent\ndifficulty of interpreting SAR imagery by transferring rich semantic knowledge\nfrom the optical domain with indirect interaction. Furthermore, GeoCLOSP, which\nintegrates geographic coordinates into our framework, creates a powerful\ntrade-off between generality and specificity: while the CLOSP excels at general\nsemantic tasks, the GeoCLOSP becomes a specialized expert for retrieving\nlocation-dependent crisis events and rare geographic features. This work\nhighlights that the integration of diverse sensor data and geographic context\nis essential for unlocking the full potential of remote sensing archives.",
        "url": "http://arxiv.org/abs/2507.10403v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10403v1",
        "arxiv_id": "2507.10403v1",
        "authors": [
            "Daniele Rege Cambrin",
            "Lorenzo Vaiani",
            "Giuseppe Gallipoli",
            "Luca Cagliero",
            "Paolo Garza"
        ],
        "submitted": "2025-07-14 15:46:56",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. However, its focus on remote sensing image retrieval and sensor data integration is not directly aligned with your primary areas of interest in e-commerce, user behavior modeling, and deep semantic understanding."
    },
    {
        "title": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
        "abstract": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system\nfor automated scientific synthesis that supports recursive, depth- and\nbreadth-controlled exploration of original research questions -- enhancing\nsearch diversity and nuance in the retrieval of relevant scientific literature.\nUnlike conventional retrieval-augmented generation pipelines, DeepResearch\nenables user-controllable synthesis with transparent reasoning and\nparameter-driven configurability, facilitating high-throughput integration of\ndomain-specific evidence while maintaining analytical rigor. Applied to 49\necological research questions, DeepResearch achieves up to a 21-fold increase\nin source integration and a 14.9-fold rise in sources integrated per 1,000\nwords. High-parameter settings yield expert-level analytical depth and\ncontextual diversity.\n  Source code available at: https://github.com/sciknoworg/deep-research.",
        "url": "http://arxiv.org/abs/2507.10522v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10522v1",
        "arxiv_id": "2507.10522v1",
        "authors": [
            "Jennifer D'Souza",
            "Endres Keno Sander",
            "Andrei Aioanei"
        ],
        "submitted": "2025-07-14 17:47:28",
        "source": "arxiv",
        "comment": "12 pages, 3 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 7,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of query understanding and ranking models. However, its focus on scientific question answering in ecology and agentic workflow is somewhat niche and not directly aligned with your primary interests in e-commerce and real-time relevance optimization. Nevertheless, the paper's use of LLM-based systems and recursive exploration of research questions shares some commonalities with your work in query understanding and ranking models."
    },
    {
        "title": "From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents",
        "abstract": "The concept of the Web of Agents (WoA), which transforms the static,\ndocument-centric Web into an environment of autonomous agents acting on users'\nbehalf, has attracted growing interest as large language models (LLMs) become\nmore capable. However, research in this area is still fragmented across\ndifferent communities. Contemporary surveys catalog the latest LLM-powered\nframeworks, while the rich histories of Multi-Agent Systems (MAS) and the\nSemantic Web are often treated as separate, legacy domains. This fragmentation\nobscures the intellectual lineage of modern systems and hinders a holistic\nunderstanding of the field's trajectory. We present the first comprehensive\nevolutionary overview of the WoA. We show that modern protocols like A2A and\nthe MCP, are direct evolutionary responses to the well-documented limitations\nof earlier standards like FIPA standards and OWL-based semantic agents. To\nsystematize this analysis, we introduce a four-axis taxonomy (semantic\nfoundation, communication paradigm, locus of intelligence, discovery\nmechanism). This framework provides a unified analytical lens for comparing\nagent architectures across all generations, revealing a clear line of descent\nwhere others have seen a disconnect. Our analysis identifies a paradigm shift\nin the 'locus of intelligence': from being encoded in external data (Semantic\nWeb) or the platform (MAS) to being embedded within the agent's core model\n(LLM). This shift is foundational to modern Agentic AI, enabling the scalable\nand adaptive systems the WoA has long envisioned. We conclude that while new\nprotocols are essential, they are insufficient for building a robust, open,\ntrustworthy ecosystem. Finally, we argue that the next research frontier lies\nin solving persistent socio-technical challenges, and we map out a new agenda\nfocused on decentralized identity, economic models, security, and governance\nfor the emerging WoA.",
        "url": "http://arxiv.org/abs/2507.10644v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10644v1",
        "arxiv_id": "2507.10644v1",
        "authors": [
            "Tatiana Petrova",
            "Aleksandr Puzikov",
            "Boris Bliznukov",
            "Radu State"
        ],
        "submitted": "2025-07-14 16:47:19",
        "source": "arxiv",
        "comment": "33 pages, 9 figures, 8 tables",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be more focused on the Web of Agents, Multi-Agent Systems, and the Semantic Web, which are not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does mention large language models, the context is not aligned with your primary focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "From BERT to Qwen: Hate Detection across architectures",
        "abstract": "Online platforms struggle to curb hate speech without over-censoring\nlegitimate discourse. Early bidirectional transformer encoders made big\nstrides, but the arrival of ultra-large autoregressive LLMs promises deeper\ncontext-awareness. Whether this extra scale actually improves practical\nhate-speech detection on real-world text remains unverified. Our study puts\nthis question to the test by benchmarking both model families, classic encoders\nand next-generation LLMs, on curated corpora of online interactions for\nhate-speech detection (Hate or No Hate).",
        "url": "http://arxiv.org/abs/2507.10468v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10468v1",
        "arxiv_id": "2507.10468v1",
        "authors": [
            "Ariadna Mon",
            "Saúl Fenollosa",
            "Jon Lecumberri"
        ],
        "submitted": "2025-07-14 16:46:30",
        "source": "arxiv",
        "comment": "4 pages, 5 figures. EE-559 Deep Learning course project (Group 11)",
        "score": 3,
        "keyword_reasons": [
            "Found 'ltr' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be focused on hate speech detection using BERT and other architectures, which is not directly related to your core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the specific application and context are quite different from your areas of interest."
    },
    {
        "title": "SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning",
        "abstract": "School dropout is a serious problem in distance learning, where early\ndetection is crucial for effective intervention and student perseverance.\nPredicting student dropout using available educational data is a widely\nresearched topic in learning analytics. Our partner's distance learning\nplatform highlights the importance of integrating diverse data sources,\nincluding socio-demographic data, behavioral data, and sentiment analysis, to\naccurately predict dropout risks. In this paper, we introduce a novel model\nthat combines sentiment analysis of student comments using the Bidirectional\nEncoder Representations from Transformers (BERT) model with socio-demographic\nand behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We\nfine-tuned BERT on student comments to capture nuanced sentiments, which were\nthen merged with key features selected using feature importance techniques in\nXGBoost. Our model was tested on unseen data from the next academic year,\nachieving an accuracy of 84\\%, compared to 82\\% for the baseline model.\nAdditionally, the model demonstrated superior performance in other metrics,\nsuch as precision and F1-score. The proposed method could be a vital tool in\ndeveloping personalized strategies to reduce dropout rates and encourage\nstudent perseverance",
        "url": "http://arxiv.org/abs/2507.10421v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10421v1",
        "arxiv_id": "2507.10421v1",
        "authors": [
            "Meriem Zerkouk",
            "Miloud Mihoubi",
            "Belkacem Chikhaoui"
        ],
        "submitted": "2025-07-14 16:04:34",
        "source": "arxiv",
        "comment": "International Conference on Education and New Learning Technologies\n  (2025)",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on predicting student dropout in distance learning using machine learning models, which does not align with your core areas of interest in Information Retrieval, Search technologies, and Natural Language Processing."
    },
    {
        "title": "Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation",
        "abstract": "Prompt engineering has proven to be a crucial step in leveraging pretrained\nlarge language models (LLMs) in solving various real-world tasks. Numerous\nsolutions have been proposed that seek to automate prompt engineering by using\nthe model itself to edit prompts. However, the majority of state-of-the-art\napproaches are evaluated on tasks that require minimal prompt templates and on\nvery large and highly capable LLMs. In contrast, solving complex tasks that\nrequire detailed information to be included in the prompt increases the amount\nof text that needs to be optimised. Furthermore, smaller models have been shown\nto be more sensitive to prompt design. To address these challenges, we propose\nan evolutionary search approach to automated discrete prompt optimisation\nconsisting of two phases. In the first phase, grammar-guided genetic\nprogramming is invoked to synthesise prompt-creating programmes by searching\nthe space of programmes populated by function compositions of syntactic,\ndictionary-based and LLM-based prompt-editing functions. In the second phase,\nlocal search is applied to explore the neighbourhoods of best-performing\nprogrammes in an attempt to further fine-tune their performance. Our approach\noutperforms three state-of-the-art prompt optimisation approaches,\nPromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose\nLLMs in four domain-specific challenging tasks. We also illustrate several\nexamples where these benchmark methods suffer relatively severe performance\ndegradation, while our approach improves performance in almost all task-model\ncombinations, only incurring minimal degradation when it does not.",
        "url": "http://arxiv.org/abs/2507.10326v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10326v1",
        "arxiv_id": "2507.10326v1",
        "authors": [
            "Muzhaffar Hazman",
            "Minh-Khoi Pham",
            "Shweta Soundararajan",
            "Goncalo Mordido",
            "Leonardo Custode",
            "David Lynch",
            "Giorgio Cruciata",
            "Yucheng Shi",
            "Hongmeng Song",
            "Wang Chao",
            "Pan Yue",
            "Aleksandar Milenovic",
            "Alexandros Agapitos"
        ],
        "submitted": "2025-07-14 14:34:15",
        "source": "arxiv",
        "comment": "Accepted for Publication at ECAI 2025",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and query understanding, as it deals with prompt engineering and optimisation for large language models. However, the focus on discrete prompt optimisation and evolutionary search approaches is not directly aligned with the user's primary focus on information retrieval and real-time relevance optimisation."
    },
    {
        "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires",
        "abstract": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape.",
        "url": "http://arxiv.org/abs/2507.10073v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10073v1",
        "arxiv_id": "2507.10073v1",
        "authors": [
            "Simon Münker"
        ],
        "submitted": "2025-07-14 08:59:26",
        "source": "arxiv",
        "comment": "15pages, 1 figure, 2 tables",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on AI systems, it focuses on cultural bias and moral representation, which is outside your primary areas of interest."
    },
    {
        "title": "SLIF-MR: Self-loop Iterative Fusion of Heterogeneous Auxiliary Information for Multimodal Recommendation",
        "abstract": "Knowledge graphs (KGs) and multimodal item information, which respectively\ncapture relational and attribute features, play a crucial role in improving\nrecommender system accuracy. Recent studies have attempted to integrate them\nvia multimodal knowledge graphs (MKGs) to further enhance recommendation\nperformance. However, existing methods typically freeze the MKG structure\nduring training, which limits the full integration of structural information\nfrom heterogeneous graphs (e.g., KG and user-item interaction graph), and\nresults in sub-optimal performance. To address this challenge, we propose a\nnovel framework, termed Self-loop Iterative Fusion of Heterogeneous Auxiliary\nInformation for Multimodal Recommendation (SLIF-MR), which leverages item\nrepresentations from previous training epoch as feedback signals to dynamically\noptimize the heterogeneous graph structures composed of KG, multimodal item\nfeature graph, and user-item interaction graph. Through this iterative fusion\nmechanism, both user and item representations are refined, thus improving the\nfinal recommendation performance. Specifically, based on the feedback item\nrepresentations, SLIF-MR constructs an item-item correlation graph, then\nintegrated into the establishment process of heterogeneous graphs as additional\nnew structural information in a self-loop manner. Consequently, the internal\nstructures of heterogeneous graphs are updated with the feedback item\nrepresentations during training. Moreover, a semantic consistency learning\nstrategy is proposed to align heterogeneous item representations across\nmodalities. The experimental results show that SLIF-MR significantly\noutperforms existing methods, particularly in terms of accuracy and robustness.",
        "url": "http://arxiv.org/abs/2507.09998v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09998v1",
        "arxiv_id": "2507.09998v1",
        "authors": [
            "Jie Guo",
            "Jiahao Jiang",
            "Ziyuan Guo",
            "Bin Song",
            "Yue Sun"
        ],
        "submitted": "2025-07-14 07:32:16",
        "source": "arxiv",
        "comment": "10 pages,7 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on multimodal recommendation systems, leveraging knowledge graphs and item features, which is somewhat related to information retrieval and search technologies. However, the primary focus on recommender systems and the use of knowledge graphs limits its relevance to the user's core research themes in IR and search technologies."
    },
    {
        "title": "MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora",
        "abstract": "Continually updating model-based indexes in generative retrieval with new\ndocuments remains challenging, as full retraining is computationally expensive\nand impractical under resource constraints. We propose MixLoRA-DSI, a novel\nframework that combines an expandable mixture of Low-Rank Adaptation experts\nwith a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead\nof allocating new experts for each new corpus, our proposed expansion strategy\nenables sublinear parameter growth by selectively introducing new experts only\nwhen significant number of OOD documents are detected. Experiments on NQ320k\nand MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update\nbaselines, with minimal parameter overhead and substantially lower training\ncosts.",
        "url": "http://arxiv.org/abs/2507.09924v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09924v1",
        "arxiv_id": "2507.09924v1",
        "authors": [
            "Tuan-Luc Huynh",
            "Thuy-Trang Vu",
            "Weiqing Wang",
            "Trung Le",
            "Dragan Gašević",
            "Yuan-Fang Li",
            "Thanh-Toan Do"
        ],
        "submitted": "2025-07-14 05:04:32",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on generative retrieval and model updates, which is somewhat related to information retrieval, but it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's emphasis on low-rank adaptation and out-of-distribution detection is more aligned with NLP and data mining, but it does not seem to require deep semantic understanding or real-time relevance optimization."
    },
    {
        "title": "Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them",
        "abstract": "Training large language models (LLMs) for reasoning via maths and code\ndatasets has become a major new focus in LLM post-training. Two particularly\npopular approaches are reinforcement learning (RL) and supervised fine-tuning\n(SFT), but their training dynamics are poorly understood. We present a\ncomparative analysis of RL and SFT on the same maths problems with the same\nmodel and similar hyperparameters. We find that RL yields minor in-domain gains\non maths and slight degradation on knowledge-intensive benchmarks like MMLU,\nwhile both trends are more pronounced in SFT. We also analyse model parameters\nacross checkpoints, observing that both algorithms modify query and key weights\nthe most. Meanwhile, SFT exhibits greater updates and also affects mid-layer\nMLPs more, leading us to hypothesise that this may have caused the\nout-of-domain degradation. We therefore investigate whether freezing parts of\nthe model during training can mitigate the reduced performance on\nknowledge-intensive benchmarks. However, our results are inconclusive, with\nbenefits on GPQA:Diamond and degradation on other benchmarks. Taken together,\nour observations provide a preliminary indication for why RL amplifies existing\ncapabilities, while SFT replaces old skills with new ones.",
        "url": "http://arxiv.org/abs/2507.10616v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10616v1",
        "arxiv_id": "2507.10616v1",
        "authors": [
            "Neel Rajani",
            "Aryo Pradipta Gema",
            "Seraphina Goldfarb-Tarrant",
            "Ivan Titov"
        ],
        "submitted": "2025-07-13 19:04:17",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper primarily focuses on comparing two methods for fine-tuning large language models, reinforcement learning and supervised fine-tuning, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves deep learning and model analysis, the context is not aligned with the user's core research themes."
    },
    {
        "title": "MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking",
        "abstract": "This paper introduces an innovative Applicant Tracking System (ATS) enhanced\nby a novel Robotic process automation (RPA) framework or as further referred to\nas MLAR. Traditional recruitment processes often encounter bottlenecks in\nresume screening and candidate shortlisting due to time and resource\nconstraints. MLAR addresses these challenges employing Large Language Models\n(LLMs) in three distinct layers: extracting key characteristics from job\npostings in the first layer, parsing applicant resume to identify education,\nexperience, skills in the second layer, and similarity matching in the third\nlayer. These features are then matched through advanced semantic algorithms to\nidentify the best candidates efficiently. Our approach integrates seamlessly\ninto existing RPA pipelines, automating resume parsing, job matching, and\ncandidate notifications. Extensive performance benchmarking shows that MLAR\noutperforms the leading RPA platforms, including UiPath and Automation\nAnywhere, in high-volume resume-processing tasks. When processing 2,400\nresumes, MLAR achieved an average processing time of 5.4 seconds per resume,\nreducing processing time by approximately 16.9% compared to Automation Anywhere\nand 17.1% compared to UiPath. These results highlight the potential of MLAR to\ntransform recruitment workflows by providing an efficient, accurate, and\nscalable solution tailored to modern hiring needs.",
        "url": "http://arxiv.org/abs/2507.10472v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10472v1",
        "arxiv_id": "2507.10472v1",
        "authors": [
            "Mohamed T. Younes",
            "Omar Walid",
            "Mai Hassan",
            "Ali Hamdi"
        ],
        "submitted": "2025-07-14 16:53:19",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Robotic Process Automation and Large Language Models in the context of Applicant Tracking Systems, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing."
    },
    {
        "title": "Devanagari Handwritten Character Recognition using Convolutional Neural Network",
        "abstract": "Handwritten character recognition is getting popular among researchers\nbecause of its possible applications in facilitating technological search\nengines, social media, recommender systems, etc. The Devanagari script is one\nof the oldest language scripts in India that does not have proper digitization\ntools. With the advancement of computing and technology, the task of this\nresearch is to extract handwritten Hindi characters from an image of Devanagari\nscript with an automated approach to save time and obsolete data. In this\npaper, we present a technique to recognize handwritten Devanagari characters\nusing two deep convolutional neural network layers. This work employs a\nmethodology that is useful to enhance the recognition rate and configures a\nconvolutional neural network for effective Devanagari handwritten text\nrecognition (DHTR). This approach uses the Devanagari handwritten character\ndataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each\nof these classes has 1700 images for training and testing purposes. This\napproach obtains promising results in terms of accuracy by achieving 96.36%\naccuracy in testing and 99.55% in training time.",
        "url": "http://arxiv.org/abs/2507.10398v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10398v1",
        "arxiv_id": "2507.10398v1",
        "authors": [
            "Diksha Mehta",
            "Prateek Mehta"
        ],
        "submitted": "2025-07-14 15:38:42",
        "source": "arxiv",
        "comment": "9 pages, 6 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on handwritten character recognition using Convolutional Neural Networks, which is outside your areas of expertise in Information Retrieval, Search technologies, and Natural Language Processing."
    },
    {
        "title": "Meanings are like Onions: a Layered Approach to Metaphor Processing",
        "abstract": "Metaphorical meaning is not a flat mapping between concepts, but a complex\ncognitive phenomenon that integrates multiple levels of interpretation. In this\npaper, we propose a stratified model of metaphor processing that treats meaning\nas an onion: a multi-layered structure comprising (1) content analysis, (2)\nconceptual blending, and (3) pragmatic intentionality. This three-dimensional\nframework allows for a richer and more cognitively grounded approach to\nmetaphor interpretation in computational systems. At the first level, metaphors\nare annotated through basic conceptual elements. At the second level, we model\nconceptual combinations, linking components to emergent meanings. Finally, at\nthe third level, we introduce a pragmatic vocabulary to capture speaker intent,\ncommunicative function, and contextual effects, aligning metaphor understanding\nwith pragmatic theories. By unifying these layers into a single formal\nframework, our model lays the groundwork for computational methods capable of\nrepresenting metaphorical meaning beyond surface associations, toward deeper,\nmore context-sensitive reasoning.",
        "url": "http://arxiv.org/abs/2507.10354v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10354v1",
        "arxiv_id": "2507.10354v1",
        "authors": [
            "Silvia Cappa",
            "Anna Sofia Lippolis",
            "Stefano Zoia"
        ],
        "submitted": "2025-07-14 14:56:46",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on metaphor processing in computational systems, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on aspects of semantic understanding, the context is specific to metaphor interpretation and does not align with the user's primary focus on real-time relevance optimization and e-commerce applications."
    },
    {
        "title": "Task-Based Flexible Feature Distillation for LLMs",
        "abstract": "Knowledge Distillation (KD) in general and feature distillation in particular\nare promising techniques for reducing the high computational demand of large\nlanguage models (LLMs). However, traditional feature KD methods typically\nassume that the teacher and the student share the same hidden size, limiting\nthe flexibility of the student's architecture. A common solution to this\nproblem involves training a linear projector to align their feature spaces, but\nthis introduces additional parameters that must be learned from scratch and\noften degrades performance on downstream tasks, especially in generative\nsettings. To address this issue, in this work, we propose a novel task-based\nfeature distillation method that enables knowledge transfer between teacher and\nstudent models with different hidden layer dimensions, without introducing any\nnew parameters. Leveraging the insight that only a subset of LLM components\ncontribute significantly to a specific downstream task, our approach identifies\nthe most task-relevant hidden units in the teacher and directly distills their\nactivations to the student. Our method is flexible and easily integrates with\nother distillation frameworks. Empirical results show consistent improvements\nover prior approaches across diverse tasks, including classification,\ninstruction-following, and summarization, achieving up to a 3\\% performance\ngain over the linear projection baseline.",
        "url": "http://arxiv.org/abs/2507.10155v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10155v1",
        "arxiv_id": "2507.10155v1",
        "authors": [
            "Khouloud Saadi",
            "Di Wang"
        ],
        "submitted": "2025-07-14 11:10:02",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on knowledge distillation for large language models, which is somewhat related to information retrieval and search technologies. However, the primary focus on LLMs and their applications in tasks like classification, instruction-following, and summarization does not directly align with the user's core research themes in IR and search technologies. While the paper touches on deep semantic understanding, it is not a central match for the user's interests."
    },
    {
        "title": "Fusing Large Language Models with Temporal Transformers for Time Series Forecasting",
        "abstract": "Recently, large language models (LLMs) have demonstrated powerful\ncapabilities in performing various tasks and thus are applied by recent studies\nto time series forecasting (TSF) tasks, which predict future values with the\ngiven historical time series. Existing LLM-based approaches transfer knowledge\nlearned from text data to time series prediction using prompting or fine-tuning\nstrategies. However, LLMs are proficient at reasoning over discrete tokens and\nsemantic patterns but are not initially designed to model continuous numerical\ntime series data. The gaps between text and time series data lead LLMs to\nachieve inferior performance to a vanilla Transformer model that is directly\ntrained on TSF data. However, the vanilla Transformers often struggle to learn\nhigh-level semantic patterns. In this paper, we design a novel\nTransformer-based architecture that complementarily leverages LLMs and vanilla\nTransformers, so as to integrate the high-level semantic representations\nlearned by LLMs into the temporal information encoded by time series\nTransformers, where a hybrid representation is obtained by fusing the\nrepresentations from the LLM and the Transformer. The resulting fused\nrepresentation contains both historical temporal dynamics and semantic\nvariation patterns, allowing our model to predict more accurate future values.\nExperiments on benchmark datasets demonstrate the effectiveness of the proposed\napproach.",
        "url": "http://arxiv.org/abs/2507.10098v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10098v1",
        "arxiv_id": "2507.10098v1",
        "authors": [
            "Chen Su",
            "Yuanhe Tian",
            "Qinyu Liu",
            "Jun Zhang",
            "Yan Song"
        ],
        "submitted": "2025-07-14 09:33:40",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on time series forecasting using large language models and temporal transformers, which is unrelated to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing."
    },
    {
        "title": "On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model",
        "abstract": "Since Searle's work deconstructing intent and intentionality in the realm of\nphilosophy, the practical meaning of intent has received little attention in\nscience and technology. Intentionality and context are both central to the\nscope of Promise Theory's model of Semantic Spacetime, used as an effective\nTiny Language Model. One can identify themes and concepts from a text, on a low\nlevel (without knowledge of the specific language) by using process coherence\nas a guide. Any agent process can assess superficially a degree of latent\n`intentionality' in data by looking for anomalous multi-scale anomalies and\nassessing the work done to form them. Scale separation can be used to sort\nparts into `intended' content and `ambient context', using the spacetime\ncoherence as a measure. This offers an elementary but pragmatic interpretation\nof latent intentionality for very low computational cost, and without reference\nto extensive training or reasoning capabilities. The process is well within the\nreach of basic organisms as it does not require large scale artificial\nprobabilistic batch processing. The level of concept formation depends,\nhowever, on the memory capacity of the agent.",
        "url": "http://arxiv.org/abs/2507.10000v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10000v1",
        "arxiv_id": "2507.10000v1",
        "authors": [
            "Mark Burgess"
        ],
        "submitted": "2025-07-14 07:34:58",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be more focused on the philosophical and theoretical aspects of intentionality in knowledge representation, using a tiny language model. While it touches on the concept of context, it does not seem to be directly related to information retrieval, search technologies, or user behavior modeling, which are the core areas of your research interests."
    },
    {
        "title": "TextOmics-Guided Diffusion for Hit-like Molecular Generation",
        "abstract": "Hit-like molecular generation with therapeutic potential is essential for\ntarget-specific drug discovery. However, the field lacks heterogeneous data and\nunified frameworks for integrating diverse molecular representations. To bridge\nthis gap, we introduce TextOmics, a pioneering benchmark that establishes\none-to-one correspondences between omics expressions and molecular textual\ndescriptions. TextOmics provides a heterogeneous dataset that facilitates\nmolecular generation through representations alignment. Built upon this\nfoundation, we propose ToDi, a generative framework that jointly conditions on\nomics expressions and molecular textual descriptions to produce biologically\nrelevant, chemically valid, hit-like molecules. ToDi leverages two encoders\n(OmicsEn and TextEn) to capture multi-level biological and semantic\nassociations, and develops conditional diffusion (DiffGen) for controllable\ngeneration. Extensive experiments confirm the effectiveness of TextOmics and\ndemonstrate ToDi outperforms existing state-of-the-art approaches, while also\nshowcasing remarkable potential in zero-shot therapeutic molecular generation.\nSources are available at: https://github.com/hala-ToDi.",
        "url": "http://arxiv.org/abs/2507.09982v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09982v1",
        "arxiv_id": "2507.09982v1",
        "authors": [
            "Hang Yuan",
            "Chen Li",
            "Wenjun Ma",
            "Yuncheng Jiang"
        ],
        "submitted": "2025-07-14 06:56:37",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be focused on molecular generation and drug discovery, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves text-based descriptions, the context is specific to omics expressions and molecular representations, making it off-topic for the user's research."
    },
    {
        "title": "Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition",
        "abstract": "Large language models demonstrate the intriguing ability to perform unseen\ntasks via in-context learning. However, it remains unclear what mechanisms\ninside the model drive such task-level generalization. In this work, we\napproach this question through the lens of off-by-one addition (i.e., 1+1=3,\n2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function\nas a second step. Leveraging circuit-style interpretability techniques such as\npath patching, we analyze the models' internal computations behind their\nnotable performance and present three key findings. First, we uncover a\nfunction induction mechanism that explains the model's generalization from\nstandard addition to off-by-one addition. This mechanism resembles the\nstructure of the induction head mechanism found in prior work and elevates it\nto a higher level of abstraction. Second, we show that the induction of the +1\nfunction is governed by multiple attention heads in parallel, each of which\nemits a distinct piece of the +1 function. Finally, we find that this function\ninduction mechanism is reused in a broader range of tasks, including synthetic\ntasks such as shifted multiple-choice QA and algorithmic tasks such as base-8\naddition. Overall, our findings offer deeper insights into how reusable and\ncomposable structures within language models enable task-level generalization.",
        "url": "http://arxiv.org/abs/2507.09875v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09875v1",
        "arxiv_id": "2507.09875v1",
        "authors": [
            "Qinyuan Ye",
            "Robin Jia",
            "Xiang Ren"
        ],
        "submitted": "2025-07-14 03:20:55",
        "source": "arxiv",
        "comment": "Code: https://github.com/INK-USC/function-induction",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval, Search technologies, or user behavior modeling, which are the core areas of your research interests. While it involves Natural Language Processing, the focus is on understanding mechanisms within large language models, which is not a primary area of your research."
    },
    {
        "title": "Te Ahorré Un Click: A Revised Definition of Clickbait and Detection in Spanish News",
        "abstract": "We revise the definition of clickbait, which lacks current consensus, and\nargue that the creation of a curiosity gap is the key concept that\ndistinguishes clickbait from other related phenomena such as sensationalism and\nheadlines that do not deliver what they promise or diverge from the article.\nTherefore, we propose a new definition: clickbait is a technique for generating\nheadlines and teasers that deliberately omit part of the information with the\ngoal of raising the readers' curiosity, capturing their attention and enticing\nthem to click. We introduce a new approach to clickbait detection datasets\ncreation, by refining the concept limits and annotations criteria, minimizing\nthe subjectivity in the decision as much as possible. Following it, we created\nand release TA1C (for Te Ahorr\\'e Un Click, Spanish for Saved You A Click), the\nfirst open source dataset for clickbait detection in Spanish. It consists of\n3,500 tweets coming from 18 well known media sources, manually annotated and\nreaching a 0.825 Fleiss' K inter annotator agreement. We implement strong\nbaselines that achieve 0.84 in F1-score.",
        "url": "http://arxiv.org/abs/2507.09777v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09777v1",
        "arxiv_id": "2507.09777v1",
        "authors": [
            "Gabriel Mordecki",
            "Guillermo Moncecchi",
            "Javier Couto"
        ],
        "submitted": "2025-07-13 20:19:08",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'click' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it touches on click models, the focus is on clickbait detection in Spanish news, which is a niche topic and not central to your core research themes."
    },
    {
        "title": "Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations",
        "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but they exhibit problems with\nlogical consistency in the output they generate. How can we harness LLMs'\nbroad-coverage parametric knowledge in formal reasoning despite their\ninconsistency? We present a method for directly integrating an LLM into the\ninterpretation function of the formal semantics for a paraconsistent logic. We\nprovide experimental evidence for the feasibility of the method by evaluating\nthe function using datasets created from several short-form factuality\nbenchmarks. Unlike prior work, our method offers a theoretical framework for\nneuro-symbolic reasoning that leverages an LLM's knowledge while preserving the\nunderlying logic's soundness and completeness properties.",
        "url": "http://arxiv.org/abs/2507.09751v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09751v1",
        "arxiv_id": "2507.09751v1",
        "authors": [
            "Bradley P. Allen",
            "Prateek Chhikara",
            "Thomas Macaulay Ferguson",
            "Filip Ilievski",
            "Paul Groth"
        ],
        "submitted": "2025-07-13 19:05:43",
        "source": "arxiv",
        "comment": "29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on\n  Neurosymbolic Learning and Reasoning (NeSy 2025)",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on integrating Large Language Models (LLMs) into formal reasoning, which is a topic in Natural Language Processing (NLP). However, it does not directly relate to Information Retrieval (IR), query understanding, ranking models, or user behavior modeling, which are core areas of your research interests."
    },
    {
        "title": "Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination",
        "abstract": "The reasoning capabilities of large language models (LLMs) have been a\nlongstanding focus of research. Recent works have further enhanced these\ncapabilities using reinforcement learning (RL), with many new methods claiming\nsignificant improvements with minimal or no external supervision. Surprisingly,\nsome studies even suggest that random or incorrect reward signals can enhance\nreasoning performance. However, these breakthroughs are mostly reported on the\nQwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,\nAMC, and AIME, while failing to achieve similar gains on other models like\nLlama, which warrants further investigation. Our analysis shows that although\nQwen2.5 achieves strong mathematical reasoning performance, its pretraining on\nlarge-scale web corpora makes it vulnerable to data contamination in popular\nbenchmarks. As a result, results derived from these benchmarks may be\nunreliable. To address this, we introduce a generator that produces fully\nsynthetic arithmetic problems of arbitrary length and difficulty, yielding a\nclean dataset we call RandomCalculation. Using these leakage-free datasets, we\nshow that only accurate reward signals consistently improve performance, while\nnoisy or incorrect signals do not. We advocate for evaluating RL methods on\nuncontaminated benchmarks and across diverse model families to ensure\ntrustworthy conclusions.",
        "url": "http://arxiv.org/abs/2507.10532v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10532v1",
        "arxiv_id": "2507.10532v1",
        "authors": [
            "Mingqi Wu",
            "Zhihao Zhang",
            "Qiaole Dong",
            "Zhiheng Xi",
            "Jun Zhao",
            "Senjie Jin",
            "Xiaoran Fan",
            "Yuhao Zhou",
            "Yanwei Fu",
            "Qin Liu",
            "Songyang Zhang",
            "Qi Zhang"
        ],
        "submitted": "2025-07-14 17:55:15",
        "source": "arxiv",
        "comment": "26 pages",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The paper focuses on the evaluation of reinforcement learning methods for large language models, which is outside your primary research areas."
    },
    {
        "title": "Overcoming catastrophic forgetting in neural networks",
        "abstract": "Catastrophic forgetting is the primary challenge that hinders continual\nlearning, which refers to a neural network ability to sequentially learn\nmultiple tasks while retaining previously acquired knowledge. Elastic Weight\nConsolidation, a regularization-based approach inspired by synaptic\nconsolidation in biological neural systems, has been used to overcome this\nproblem. In this study prior research is replicated and extended by evaluating\nEWC in supervised learning settings using the PermutedMNIST and RotatedMNIST\nbenchmarks. Through systematic comparisons with L2 regularization and\nstochastic gradient descent (SGD) without regularization, we analyze how\ndifferent approaches balance knowledge retention and adaptability. Our results\nconfirm what was shown in previous research, showing that EWC significantly\nreduces forgetting compared to naive training while slightly compromising\nlearning efficiency on new tasks. Moreover, we investigate the impact of\ndropout regularization and varying hyperparameters, offering insights into the\ngeneralization of EWC across diverse learning scenarios. These results\nunderscore EWC's potential as a viable solution for lifelong learning in neural\nnetworks.",
        "url": "http://arxiv.org/abs/2507.10485v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10485v1",
        "arxiv_id": "2507.10485v1",
        "authors": [
            "Brandon Shuen Yi Loke",
            "Filippo Quadri",
            "Gabriel Vivanco",
            "Maximilian Casagrande",
            "Saúl Fenollosa"
        ],
        "submitted": "2025-07-14 17:04:05",
        "source": "arxiv",
        "comment": "7 pages, 5 figures, EE-411 Fundamentals of inference and learning\n  course project",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on neural networks and continual learning, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on regularization techniques, it does not address query understanding, ranking models, or user behavior modeling, making it an off-topic paper for the user's research."
    },
    {
        "title": "Using AI to replicate human experimental results: a motion study",
        "abstract": "This paper explores the potential of large language models (LLMs) as reliable\nanalytical tools in linguistic research, focusing on the emergence of affective\nmeanings in temporal expressions involving manner-of-motion verbs. While LLMs\nlike GPT-4 have shown promise across a range of tasks, their ability to\nreplicate nuanced human judgements remains under scrutiny. We conducted four\npsycholinguistic studies (on emergent meanings, valence shifts, verb choice in\nemotional contexts, and sentence-emoji associations) first with human\nparticipants and then replicated the same tasks using an LLM. Results across\nall studies show a striking convergence between human and AI responses, with\nstatistical analyses (e.g., Spearman's rho = .73-.96) indicating strong\ncorrelations in both rating patterns and categorical choices. While minor\ndivergences were observed in some cases, these did not alter the overall\ninterpretative outcomes. These findings offer compelling evidence that LLMs can\naugment traditional human-based experimentation, enabling broader-scale studies\nwithout compromising interpretative validity. This convergence not only\nstrengthens the empirical foundation of prior human-based findings but also\nopens possibilities for hypothesis generation and data expansion through AI.\nUltimately, our study supports the use of LLMs as credible and informative\ncollaborators in linguistic inquiry.",
        "url": "http://arxiv.org/abs/2507.10342v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10342v1",
        "arxiv_id": "2507.10342v1",
        "authors": [
            "Rosa Illan Castillo",
            "Javier Valenzuela"
        ],
        "submitted": "2025-07-14 14:47:01",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on the application of large language models in linguistic research, exploring their ability to replicate human judgements. While it touches on the potential of AI in research, it doesn't directly relate to information retrieval, search technologies, or user behavior modeling, which are core areas of your research interests."
    },
    {
        "title": "Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning",
        "abstract": "Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient\nFine-Tuning (PEFT) method, has attracted widespread attention for significantly\nimproving parameter efficiency by editing representation space alone. In this\nwork, we investigate applying ReFT to complex reasoning tasks. However,\ndirectly using the native ReFT method, which modifies fixed representations at\nthe beginning and end of each layer, yields suboptimal performance, as these\nfixed-position representations have uncertain impact on the outputs. We observe\nthat, in complex reasoning tasks, there often exist certain critical\nrepresentations. These representations either integrate significant information\nfrom preceding layers or regulate subsequent layer representations. Through\nlayer-by-layer propagation, they exert a substantial influence on the final\noutput. Naturally, fine-tuning these critical representations has the potential\nto greatly enhance reasoning performance. Building upon these insights, we\npropose Critical Representation Fine-Tuning (CRFT), a novel method that\nidentifies and optimizes these critical representations through information\nflow analysis. CRFT operates within a supervised learning framework,\ndynamically optimizing critical representations in a low-rank linear subspace\nwhile freezing the base model. The effectiveness and efficiency of our method\nare validated across eight benchmarks for arithmetic and commonsense reasoning,\nusing LLaMA and Mistral model families. Furthermore, our method also adapts\neffectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work\nhighlights the untapped potential of representation-level optimization for CoT\nreasoning, offering a lightweight yet powerful alternative to traditional PEFT\nmethods.",
        "url": "http://arxiv.org/abs/2507.10085v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10085v1",
        "arxiv_id": "2507.10085v1",
        "authors": [
            "Chenxi Huang",
            "Shaotian Yan",
            "Liang Xie",
            "Binbin Lin",
            "Sinan Fan",
            "Yue Xin",
            "Deng Cai",
            "Chen Shen",
            "Jieping Ye"
        ],
        "submitted": "2025-07-14 09:11:33",
        "source": "arxiv",
        "comment": "Accepted by ACL 2025",
        "score": 1,
        "keyword_reasons": [
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on representation fine-tuning for complex reasoning tasks, which is outside the scope of information retrieval and search technologies. While it involves deep learning and optimization, the application domain and methodology are not directly related to the user's core research interests."
    },
    {
        "title": "GeLaCo: An Evolutionary Approach to Layer Compression",
        "abstract": "Large Language Models (LLM) have achieved remarkable performance across a\nlarge number of tasks, but face critical deployment and usage barriers due to\nsubstantial computational requirements. Model compression methods, which aim to\nreduce model size while preserving its capacity, are an important means to\nmitigate these issues. Promising approaches along these lines, such as\nstructured pruning, typically require costly empirical search for optimal\nvariants and may run the risk of ignoring better solutions. In this work we\nintroduce GeLaCo, an evolutionary approach to LLM compression via layer\ncollapse. Our approach supports an efficient exploration of the compression\nsolution space via population-based search and a module-wise similarity fitness\nfunction capturing attention, feed-forward, and hidden state representations.\nGeLaCo also supports both single and multi-objective evolutionary compression\nsearch, establishing the first Pareto frontier along compression and quality\naxes. We evaluate GeLaCo solutions via both perplexity-based and generative\nevaluations over foundational and instruction-tuned models, outperforming\nstate-of-the-art alternatives.",
        "url": "http://arxiv.org/abs/2507.10059v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10059v1",
        "arxiv_id": "2507.10059v1",
        "authors": [
            "David Ponce",
            "Thierry Etchegoyhen",
            "Javier Del Ser"
        ],
        "submitted": "2025-07-14 08:44:59",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on model compression for Large Language Models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on NLP, the context and approach are more aligned with model optimization and compression, rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Tiny Reward Models",
        "abstract": "Large decoder-based language models have become the dominant architecture for\nreward modeling in reinforcement learning from human feedback (RLHF). However,\nas reward models are increasingly deployed in test-time strategies, their\ninference costs become a growing concern. We present TinyRM, a family of small,\nbidirectional masked language models (MLMs) with as few as 400 million\nparameters, that rival the capabilities of models over 175 times larger on\nreasoning and safety preference modeling tasks. TinyRM combines FLAN-style\nprompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to\nachieve strong performance on RewardBench, despite using significantly fewer\nresources. Our experiments suggest that small models benefit from\ndomain-specific tuning strategies, particularly in reasoning, where lightweight\nfinetuning methods are especially effective. While challenges remain in\nbuilding generalist models and conversational preference modeling, our\npreliminary results highlight the promise of lightweight bidirectional\narchitectures as efficient, scalable alternatives for preference modeling.",
        "url": "http://arxiv.org/abs/2507.09973v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09973v1",
        "arxiv_id": "2507.09973v1",
        "authors": [
            "Sarah Pan"
        ],
        "submitted": "2025-07-14 06:43:00",
        "source": "arxiv",
        "comment": "2025 ICML Efficient Systems for Foundation Models Workshop",
        "score": 1,
        "keyword_reasons": [
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat relevant to your research interests in Natural Language Processing (NLP) and related topics, particularly in the area of language models. However, it does not directly relate to your core focus on Information Retrieval (IR), query understanding, ranking models, or user behavior modeling. The paper's focus on reward modeling in reinforcement learning from human feedback is an interesting application of NLP, but it does not align with your primary research themes."
    },
    {
        "title": "TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit",
        "abstract": "Recent advances in Large Language Models (LLM) have led to a new class of\nautonomous agents, renewing and expanding interest in the area. LLM-powered\nMultiagent Systems (MAS) have thus emerged, both for assistive and simulation\npurposes, yet tools for realistic human behavior simulation -- with its\ndistinctive challenges and opportunities -- remain underdeveloped. Existing MAS\nlibraries and tools lack fine-grained persona specifications, population\nsampling facilities, experimentation support, and integrated validation, among\nother key capabilities, limiting their utility for behavioral studies, social\nsimulation, and related applications. To address these deficiencies, in this\nwork we introduce TinyTroupe, a simulation toolkit enabling detailed persona\ndefinitions (e.g., nationality, age, occupation, personality, beliefs,\nbehaviors) and programmatic control via numerous LLM-driven mechanisms. This\nallows for the concise formulation of behavioral problems of practical\ninterest, either at the individual or group level, and provides effective means\nfor their solution. TinyTroupe's components are presented using representative\nworking examples, such as brainstorming and market research sessions, thereby\nsimultaneously clarifying their purpose and demonstrating their usefulness.\nQuantitative and qualitative evaluations of selected aspects are also provided,\nhighlighting possibilities, limitations, and trade-offs. The approach, though\nrealized as a specific Python implementation, is meant as a novel conceptual\ncontribution, which can be partially or fully incorporated in other contexts.\nThe library is available as open source at\nhttps://github.com/microsoft/tinytroupe.",
        "url": "http://arxiv.org/abs/2507.09788v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09788v1",
        "arxiv_id": "2507.09788v1",
        "authors": [
            "Paulo Salem",
            "Robert Sim",
            "Christopher Olsen",
            "Prerit Saxena",
            "Rafael Barcelos",
            "Yi Ding"
        ],
        "submitted": "2025-07-13 21:00:27",
        "source": "arxiv",
        "comment": "9 pages. Preprint to be submitted to peer-review",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves Large Language Models, its focus on Multiagent Persona Simulation is not aligned with your primary areas of interest."
    }
]