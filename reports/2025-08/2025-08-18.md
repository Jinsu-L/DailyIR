# Daily Papers Report - 2025-08-18

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. A Large-Scale Web Search Dataset for Federated Online Learning to Rank

- **LLM Score**: 8
- **Keyword Score**: 20
- **Authors**: Marcel Gregoriadis, Jingwei Kang, Johan Pouwelse
- **URL**: <http://arxiv.org/abs/2508.12353v1>
- **Submitted**: 2025-08-17 12:57:54
- **Comment**: Accepted at CIKM 2025
- **Topic Keywords**: query, queries, ranking, learning to rank, ltr, click, web search, rank, search
- **Reason**: The paper is highly relevant to your research interests in Information Retrieval, particularly in the area of Learning to Rank and user behavior modeling. The focus on federated online learning and the presentation of a large-scale web search dataset with realistic user data and query timestamps aligns with your interests in query understanding, ranking models, and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Federated Online Learning to Rank (FOLTR)
- **Aim**: Introduce a novel dataset, AOL4FOLTR, to address limitations of existing benchmarks for FOLTR
- **Rationale**: Real-world user interaction data, such as click/no-click data, is currently lacking in publicly available Learning to Rank (LTR) datasets
- **Ground**: AOL4FOLTR is a large-scale web search dataset consisting of approximately 2.6 million queries from 10,000 users, including raw queries, documents, user IDs, timestamps, and clicked and non-clicked documents
- **Experiment**: Experimental evaluation of the dataset for FOLTR using 100 clients, corresponding to the top 100 users by number of query logs, and employing the state-of-the-art algorithm FPDGD
- **Takeaway**: The AOL4FOLTR dataset and methodology provide a more realistic and comprehensive approach to FOLTR, enabling the study of LTR feature selection, personalization techniques, and federated or decentralized information retrieval

#### Abstract
> The centralized collection of search interaction logs for training ranking
models raises significant privacy concerns. Federated Online Learning to Rank
(FOLTR) offers a privacy-preserving alternative by enabling collaborative model
training without sharing raw user data. However, benchmarks in FOLTR are
largely based on random partitioning of classical learning-to-rank datasets,
simulated user clicks, and the assumption of synchronous client participation.
This oversimplifies real-world dynamics and undermines the realism of
experimental results. We present AOL4FOLTR, a large-scale web search dataset
with 2.6 million queries from 10,000 users. Our dataset addresses key
limitations of existing benchmarks by including user identifiers, real click
data, and query timestamps, enabling realistic user partitioning, behavior
modeling, and asynchronous federated learning scenarios.

---

### 2. Ontology-Guided Query Expansion for Biomedical Document Retrieval using Large Language Models

- **LLM Score**: 8
- **Keyword Score**: 13
- **Authors**: Zabir Al Nazi, Vagelis Hristidis, Aaron Lawson McLean, Jannat Ara Meem, Md Taukir Azam Chowdhury
- **URL**: <http://arxiv.org/abs/2508.11784v1>
- **Submitted**: 2025-08-15 19:23:26
- **Topic Keywords**: retriever, query, queries, retrieval, trec
- **Reason**: The paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The use of large language models and ontology-guided query expansion is a novel approach that aligns with your focus on deep semantic understanding and real-time relevance optimization. While the paper is focused on biomedical document retrieval, the techniques and concepts explored can be applied to other domains, including e-commerce.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Biomedical Document Retrieval using Ontology-Aware Query Expansion
- **Aim**: To develop an effective ontology-aware query expansion pipeline for biomedical document retrieval
- **Rationale**: To address the challenges of domain-specific vocabulary and semantic ambiguity in user queries by integrating structured biomedical ontologies with large language models
- **Ground**: The pipeline combines medical knowledge from the UMLS Metathesaurus with the generative capabilities of large language models to enhance retrieval effectiveness
- **Experiment**: Evaluation on three popular biomedical information retrieval benchmarks (NFCorpus, SciFact, and TREC-COVID) and comparison with various baselines, including sparse retrieval, dense retrievers, biomedical-specific retrieval pipelines, and recent LLM-based query expansion methods
- **Takeaway**: The proposed BMQExpander pipeline achieves superior retrieval performance, generalizes robustly under query perturbation settings, and demonstrates higher medical accuracy and fewer hallucinations compared to other methods

#### Abstract
> Effective Question Answering (QA) on large biomedical document collections
requires effective document retrieval techniques. The latter remains a
challenging task due to the domain-specific vocabulary and semantic ambiguity
in user queries. We propose BMQExpander, a novel ontology-aware query expansion
pipeline that combines medical knowledge - definitions and relationships - from
the UMLS Metathesaurus with the generative capabilities of large language
models (LLMs) to enhance retrieval effectiveness. We implemented several
state-of-the-art baselines, including sparse and dense retrievers, query
expansion methods, and biomedical-specific solutions. We show that BMQExpander
has superior retrieval performance on three popular biomedical Information
Retrieval (IR) benchmarks: NFCorpus, TREC-COVID, and SciFact - with
improvements of up to 22.1% in NDCG@10 over sparse baselines and up to 6.5%
over the strongest baseline. Further, BMQExpander generalizes robustly under
query perturbation settings, in contrast to supervised baselines, achieving up
to 15.7% improvement over the strongest baseline. As a side contribution, we
publish our paraphrased benchmarks. Finally, our qualitative analysis shows
that BMQExpander has fewer hallucinations compared to other LLM-based query
expansion baselines.

---

### 3. TaoSR1: The Thinking Model for E-commerce Relevance Search

- **LLM Score**: 8
- **Keyword Score**: 9
- **Authors**: Chenhe Dong, Shaowei Yao, Pengkun Jiao, Jianhui Yang, Yiming Jin, Zerui Huang, Xiaojiang Zhou, Dan Ou, Haihong Tang
- **URL**: <http://arxiv.org/abs/2508.12365v1>
- **Submitted**: 2025-08-17 13:48:48
- **Topic Keywords**: query, relevance, commerce, e-commerce, search
- **Reason**: The paper focuses on query-product relevance prediction in e-commerce search, which is a specific application of Information Retrieval. The use of BERT-based models and Large Language Models (LLMs) aligns with my interest in query understanding and ranking models. While the paper's primary focus is on e-commerce, the techniques and ideas presented can be applied to other domains, making it somewhat related to my broader research interests in IR and NLP.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Query-Product Relevance Prediction in E-commerce Search Engines
- **Aim**: To propose a novel optimization framework, TaoSR1, for query-product relevance prediction in e-commerce search engines
- **Rationale**: To address the challenges of online deployment, error accumulation, and discriminative hallucination in query-product relevance prediction
- **Ground**: The framework leverages Large Language Models (LLMs) and reinforcement learning algorithms, including PPO and GRPO, to enhance the reasoning capabilities of large language models
- **Experiment**: Offline experimental analysis using Pass@N-based DPO and GRPO, and online human evaluations demonstrating substantial improvements in query-product relevance prediction
- **Takeaway**: The proposed framework, TaoSR1, achieves more feasible and efficient online deployment, outperforming baseline methods on offline evaluation datasets and demonstrating substantial improvements in online human evaluations

#### Abstract
> Query-product relevance prediction is a core task in e-commerce search.
BERT-based models excel at semantic matching but lack complex reasoning
capabilities. While Large Language Models (LLMs) are explored, most still use
discriminative fine-tuning or distill to smaller models for deployment. We
propose a framework to directly deploy LLMs for this task, addressing key
challenges: Chain-of-Thought (CoT) error accumulation, discriminative
hallucination, and deployment feasibility. Our framework, TaoSR1, involves
three stages: (1) Supervised Fine-Tuning (SFT) with CoT to instill reasoning;
(2) Offline sampling with a pass@N strategy and Direct Preference Optimization
(DPO) to improve generation quality; and (3) Difficulty-based dynamic sampling
with Group Relative Policy Optimization (GRPO) to mitigate discriminative
hallucination. Additionally, post-CoT processing and a cumulative
probability-based partitioning method enable efficient online deployment.
TaoSR1 significantly outperforms baselines on offline datasets and achieves
substantial gains in online side-by-side human evaluations, introducing a novel
paradigm for applying CoT reasoning to relevance classification.

---

### 4. All for law and law for all: Adaptive RAG Pipeline for Legal Research

- **LLM Score**: 7
- **Keyword Score**: 8
- **Authors**: Figarri Keisha, Prince Singh, Pallavi, Dion Fernandes, Aravindh Manivannan, Ilham Wicaksono, Faisal Ahmad
- **URL**: <http://arxiv.org/abs/2508.13107v1>
- **Submitted**: 2025-08-18 17:14:03
- **Comment**: submitted to NLLP 2025 Workshop
- **Topic Keywords**: query, rag, retrieval, search
- **Reason**: The paper presents a Retrieval-Augmented Generation (RAG) pipeline for legal research, which is relevant to information retrieval and search technologies. The use of query translation, open-source retrieval strategies, and evaluation frameworks aligns with the user's interests in query understanding and ranking models. However, the focus on the legal domain and RAG systems for legal research assistance is somewhat specific and not directly related to the user's primary research themes.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Retrieval-Augmented Generation (RAG) pipeline for legal research
- **Aim**: To develop an end-to-end RAG pipeline that rivals or outperforms proprietary approaches in retrieval quality
- **Rationale**: The authors aim to improve legal research by developing a comprehensive evaluation and generation framework that assesses semantic alignment and faithfulness across models and prompt designs
- **Ground**: The authors use the LegalBenchRAG corpus, which consists of 162 tasks covering six types of legal reasoning, and sample 194 QA pairs per domain to create a lightweight, balanced subset called LegalBenchRAG-mini
- **Experiment**: The authors experiment with different query translation approaches, open-source embedding models, and evaluation metrics, including RAGAS faithfulness and answer relevancy, BERTScore-F1, and ROUGE-Recall
- **Takeaway**: The authors find that their open-source pipeline can rival or outperform proprietary approaches in retrieval quality, and that a custom-crafted prompt consistently produces more faithful and contextually relevant answers than baseline prompting

#### Abstract
> Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding
large language model outputs in cited sources, a capability that is especially
critical in the legal domain. We present an end-to-end RAG pipeline that
revisits and extends the LegalBenchRAG baseline with three targeted
enhancements: (i) a context-aware query translator that disentangles document
references from natural-language questions and adapts retrieval depth and
response style based on expertise and specificity, (ii) open-source retrieval
strategies using SBERT and GTE embeddings that achieve substantial performance
gains (improving Recall@K by 30-95\% and Precision@K by $\sim$2.5$\times$ for
$K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and
generation framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to
assess semantic alignment and faithfulness across models and prompt designs.
Our results show that carefully designed open-source pipelines can rival or
outperform proprietary approaches in retrieval quality, while a custom
legal-grounded prompt consistently produces more faithful and contextually
relevant answers than baseline prompting. Taken together, these contributions
demonstrate the potential of task-aware, component-level tuning to deliver
legally grounded, reproducible, and cost-effective RAG systems for legal
research assistance.

---

### 5. A Multi-Task Evaluation of LLMs' Processing of Academic Text Input

- **LLM Score**: 6
- **Keyword Score**: 8
- **Authors**: Tianyi Li, Yu Qin, Olivia R. Liu Sheng
- **URL**: <http://arxiv.org/abs/2508.11779v1>
- **Submitted**: 2025-08-15 19:05:57
- **Topic Keywords**: pairwise, recommend, rank, search, acl
- **Reason**: The paper evaluates the capabilities of large language models (LLMs) in processing academic text input, which is related to information retrieval and natural language processing. However, the focus is more on the evaluation of LLMs' capabilities rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: Large Language Models (LLMs) in Academic Peer Review
- **Aim**: Evaluating LLMs' potential in assisting scientific discovery, particularly in academic peer review
- **Rationale**: LLMs' practical application potential lies between a literature digest and a human-comparable research assistant
- **Ground**: Rigorous performance evaluation of Google's Gemini using top journal articles and various text metrics
- **Experiment**: Organizing individual tasks into a guided and robust workflow, consisting of content reproduction, comparison, scoring, and reflection
- **Takeaway**: Evidence suggests that LLMs are not yet reliable for constructing peer reviews, and their unchecked use is not recommended

#### Abstract
> How much large language models (LLMs) can aid scientific discovery, notably
in assisting academic peer review, is in heated debate. Between a literature
digest and a human-comparable research assistant lies their practical
application potential. We organize individual tasks that computer science
studies employ in separate terms into a guided and robust workflow to evaluate
LLMs' processing of academic text input. We employ four tasks in the
assessment: content reproduction/comparison/scoring/reflection, each demanding
a specific role of the LLM (oracle/judgmental arbiter/knowledgeable
arbiter/collaborator) in assisting scholarly works, and altogether testing LLMs
with questions that increasingly require intellectual capabilities towards a
solid understanding of scientific texts to yield desirable solutions. We
exemplify a rigorous performance evaluation with detailed instructions on the
prompts. Adopting first-rate Information Systems articles at three top journals
as the input texts and an abundant set of text metrics, we record a compromised
performance of the leading LLM - Google's Gemini: its summary and paraphrase of
academic text is acceptably reliable; using it to rank texts through pairwise
text comparison is faintly scalable; asking it to grade academic texts is prone
to poor discrimination; its qualitative reflection on the text is
self-consistent yet hardly insightful to inspire meaningful research. This
evidence against an endorsement of LLMs' text-processing capabilities is
consistent across metric-based internal (linguistic assessment), external
(comparing to the ground truth), and human evaluation, and is robust to the
variations of the prompt. Overall, we do not recommend an unchecked use of LLMs
in constructing peer reviews.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Is This News Still Interesting to You?: Lifetime-aware Interest Matching for News Recommendation

- **LLM Score**: 6
- **Keyword Score**: 5
- **Authors**: Seongeun Ryu, Yunyong Ko, Sang-Wook Kim
- **URL**: <http://arxiv.org/abs/2508.13064v1>
- **Submitted**: 2025-08-18 16:36:27
- **Comment**: 10 pages, 7 figures, 4 tables, accepted at ACM International
  Conference on Information and Knowledge Management (CIKM)
- **Topic Keywords**: rag, click, recommend
- **Reason**: The paper focuses on news recommendation, which is related to information retrieval and search technologies. The use of lifetime-aware interest matching and attention mechanisms is somewhat relevant to query understanding and ranking models. However, the paper's primary focus on news recommendation and its emphasis on freshness and temporal alignment are not directly aligned with the user's interests in deep semantic understanding and real-time relevance optimization.

#### Abstract
> Personalized news recommendation aims to deliver news articles aligned with
users' interests, serving as a key solution to alleviate the problem of
information overload on online news platforms. While prior work has improved
interest matching through refined representations of news and users, the
following time-related challenges remain underexplored: (C1) leveraging the age
of clicked news to infer users' interest persistence, and (C2) modeling the
varying lifetime of news across topics and users. To jointly address these
challenges, we propose a novel Lifetime-aware Interest Matching framework for
nEws recommendation, named LIME, which incorporates three key strategies: (1)
User-Topic lifetime-aware age representation to capture the relative age of
news with respect to a user-topic pair, (2) Candidate-aware lifetime attention
for generating temporally aligned user representation, and (3) Freshness-guided
interest refinement for prioritizing valid candidate news at prediction time.
Extensive experiments on two real-world datasets demonstrate that LIME
consistently outperforms a wide range of state-of-the-art news recommendation
methods, and its model agnostic strategies significantly improve recommendation
accuracy.

### 7. MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding

- **LLM Score**: 6
- **Keyword Score**: 5
- **Authors**: Daoze Zhang, Zhanheng Nie, Jianyu Liu, Chenghan Fu, Wanxian Guan, Yuan Gao, Jun Song, Pengjie Wang, Jian Xu, Bo Zheng
- **URL**: <http://arxiv.org/abs/2508.11999v1>
- **Submitted**: 2025-08-16 09:59:25
- **Comment**: 11 pages, 9 figures
- **Topic Keywords**: retrieval, commerce, e-commerce, search
- **Reason**: The paper focuses on multimodal representation learning for e-commerce product understanding, which is related to my interest in Information Retrieval and Search technologies. However, the specific application in e-commerce and the emphasis on generative models are not directly aligned with my core research themes. The paper's relevance is somewhat related, but not a central match.

#### Abstract
> With the rapid advancement of e-commerce, exploring general representations
rather than task-specific ones has attracted increasing research attention. For
product understanding, although existing discriminative dual-flow architectures
drive progress in this field, they inherently struggle to model the many-to-one
alignment between multiple images and texts of products. Therefore, we argue
that generative Multimodal Large Language Models (MLLMs) hold significant
potential for improving product representation learning. Nevertheless,
achieving this goal still remains non-trivial due to several key challenges:
the lack of multimodal and aspect-aware modeling modules in typical LLMs; the
common presence of background noise in product images; and the absence of a
standard benchmark for evaluation. To address these issues, we propose the
first generative MLLM-based model named MOON for product representation
learning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for
targeted modeling of multimodal and aspect-specific product content; (2)
effectively detects core semantic regions in product images to mitigate the
distraction and interference caused by background noise; and (3) introduces the
specialized negative sampling strategy to increase the difficulty and diversity
of negative samples. In addition, we release a large-scale multimodal benchmark
MBE for various product understanding tasks. Experimentally, our model
demonstrates competitive zero-shot performance on both our benchmark and the
public dataset, showcasing strong generalization across various downstream
tasks, including cross-modal retrieval, product classification, and attribute
prediction. Furthermore, the case study and visualization illustrate the
effectiveness of MOON for product understanding.

### 8. Bridging Human and LLM Judgments: Understanding and Narrowing the Gap

- **LLM Score**: 6
- **Keyword Score**: 3
- **Authors**: Felipe Maia Polo, Xinhe Wang, Mikhail Yurochkin, Gongjun Xu, Moulinath Banerjee, Yuekai Sun
- **URL**: <http://arxiv.org/abs/2508.12792v1>
- **Submitted**: 2025-08-18 10:14:20
- **Topic Keywords**: pairwise
- **Reason**: The paper explores the gap between human and Large Language Model (LLM) judgments, which is related to query understanding and ranking models. However, the focus is on evaluating model outputs rather than search technologies or user behavior modeling, which are core areas of interest for the user. The paper's connection to information retrieval is indirect, as it aims to improve LLM ratings, but it does not directly address the user's primary research themes.

#### Abstract
> Large language models are increasingly used as judges (LLM-as-a-judge) to
evaluate model outputs at scale, but their assessments often diverge
systematically from human judgments. We present Bridge, a unified statistical
framework that explicitly bridges human and LLM evaluations under both absolute
scoring and pairwise comparison paradigms. Bridge posits a latent human
preference score for each prompt-response pair and models LLM deviations as
linear transformations of covariates that capture sources of discrepancies.
This offers a simple and principled framework for refining LLM ratings and
characterizing systematic discrepancies between humans and LLMs. We provide an
efficient fitting algorithm with asymptotic guarantees for statistical
inference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot
Arena), Bridge achieves higher agreement with human ratings (accuracy,
calibration, and KL divergence) and exposes systematic human-LLM gaps.

### 9. Consensus or Conflict? Fine-Grained Evaluation of Conflicting Answers in Question-Answering

- **LLM Score**: 6
- **Keyword Score**: 3
- **Authors**: Eviatar Nachshoni, Arie Cattan, Shmuel Amar, Ori Shapira, Ido Dagan
- **URL**: <http://arxiv.org/abs/2508.12355v1>
- **Submitted**: 2025-08-17 12:58:48
- **Comment**: no comments
- **Topic Keywords**: rag, search
- **Reason**: The paper explores Multi-Answer Question Answering (MAQA) and the challenge of conflicting answers, which is related to query understanding and ranking models in Information Retrieval. However, the focus is on question answering and language models, rather than search technologies and user behavior modeling, which are core areas of interest. The paper's relevance is somewhat diminished by its focus on a specific task and dataset, rather than more general concepts.

#### Abstract
> Large Language Models (LLMs) have demonstrated strong performance in question
answering (QA) tasks. However, Multi-Answer Question Answering (MAQA), where a
question may have several valid answers, remains challenging. Traditional QA
settings often assume consistency across evidences, but MAQA can involve
conflicting answers. Constructing datasets that reflect such conflicts is
costly and labor-intensive, while existing benchmarks often rely on synthetic
data, restrict the task to yes/no questions, or apply unverified automated
annotation. To advance research in this area, we extend the conflict-aware MAQA
setting to require models not only to identify all valid answers, but also to
detect specific conflicting answer pairs, if any. To support this task, we
introduce a novel cost-effective methodology for leveraging fact-checking
datasets to construct NATCONFQA, a new benchmark for realistic, conflict-aware
MAQA, enriched with detailed conflict labels, for all answer pairs. We evaluate
eight high-end LLMs on NATCONFQA, revealing their fragility in handling various
types of conflicts and the flawed strategies they employ to resolve them.

### 10. SEA-BED: Southeast Asia Embedding Benchmark

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Wuttikorn Ponwitayarat, Raymond Ng, Jann Railey Montalan, Thura Aung, Jian Gang Ngui, Yosephine Susanto, William Tjhi, Panuthep Tasawong, Erik Cambria, Ekapol Chuangsuwanich, Sarana Nutanong, Peerat Limkonchotiwat
- **URL**: <http://arxiv.org/abs/2508.12243v1>
- **Submitted**: 2025-08-17 05:10:40
- **Topic Keywords**: semantic search, ranking, rag, rank, search
- **Reason**: The paper focuses on sentence embeddings and their applications in NLP tasks, which is related to my interest in Natural Language Processing. However, the specific context of Southeast Asia and the emphasis on multilingual benchmarks and human-curated datasets are not directly aligned with my primary focus on Information Retrieval and query understanding.

#### Abstract
> Sentence embeddings are essential for NLP tasks such as semantic search,
re-ranking, and textual similarity. Although multilingual benchmarks like MMTEB
broaden coverage, Southeast Asia (SEA) datasets are scarce and often
machine-translated, missing native linguistic properties. With nearly 700
million speakers, the SEA region lacks a region-specific embedding benchmark.
We introduce SEA-BED, the first large-scale SEA embedding benchmark with 169
datasets across 9 tasks and 10 languages, where 71% are formulated by humans,
not machine generation or translation. We address three research questions: (1)
which SEA languages and tasks are challenging, (2) whether SEA languages show
unique performance gaps globally, and (3) how human vs. machine translations
affect evaluation. We evaluate 17 embedding models across six studies,
analyzing task and language challenges, cross-benchmark comparisons, and
translation trade-offs. Results show sharp ranking shifts, inconsistent model
performance among SEA languages, and the importance of human-curated datasets
for low-resource languages like Burmese.

### 11. Can we Evaluate RAGs with Synthetic Data?

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Jonas van Elburg, Peter van der Putten, Maarten Marx
- **URL**: <http://arxiv.org/abs/2508.11758v1>
- **Submitted**: 2025-08-15 18:07:47
- **Comment**: Accepted for the SynDAiTE workshop at the European Conference on
  Machine Learning and Principles and Practice of Knowledge Discovery in
  Databases (ECML-PKDD 2025), September 15, 2025 - Porto, Portugal
- **Topic Keywords**: retriever, ranking, rag, rank
- **Reason**: The paper explores the use of synthetic data for evaluating Retrieval-Augmented Generation (RAG) models, which is a topic related to Information Retrieval. However, the focus is on the evaluation methodology rather than query understanding, ranking models, or user behavior modeling, which are the core areas of interest for the user.

#### Abstract
> We investigate whether synthetic question-answer (QA) data generated by large
language models (LLMs) can serve as an effective proxy for human-labeled
benchmarks when such data is unavailable. We assess the reliability of
synthetic benchmarks across two experiments: one varying retriever parameters
while keeping the generator fixed, and another varying the generator with fixed
retriever parameters. Across four datasets, of which two open-domain and two
proprietary, we find that synthetic benchmarks reliably rank the RAGs varying
in terms of retriever configuration, aligning well with human-labeled benchmark
baselines. However, they fail to produce consistent RAG rankings when comparing
generator architectures. The breakdown possibly arises from a combination of
task mismatch between the synthetic and human benchmarks, and stylistic bias
favoring certain generators.

### 12. AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Zefang Liu, Arman Anwar
- **URL**: <http://arxiv.org/abs/2508.13118v1>
- **Submitted**: 2025-08-18 17:22:51
- **Topic Keywords**: queries, rag, retrieval
- **Reason**: The paper is somewhat related to information retrieval, as it discusses retrieval-augmented generation in the context of multi-agent incident response. However, the focus is on cybersecurity and decision-making, which is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is limited to the user's secondary interest in NLP, but even then, it is not a central match.

#### Abstract
> Incident response (IR) requires fast, coordinated, and well-informed
decision-making to contain and mitigate cyber threats. While large language
models (LLMs) have shown promise as autonomous agents in simulated IR settings,
their reasoning is often limited by a lack of access to external knowledge. In
this work, we present AutoBnB-RAG, an extension of the AutoBnB framework that
incorporates retrieval-augmented generation (RAG) into multi-agent incident
response simulations. Built on the Backdoors & Breaches (B&B) tabletop game
environment, AutoBnB-RAG enables agents to issue retrieval queries and
incorporate external evidence during collaborative investigations. We introduce
two retrieval settings: one grounded in curated technical documentation
(RAG-Wiki), and another using narrative-style incident reports (RAG-News). We
evaluate performance across eight team structures, including newly introduced
argumentative configurations designed to promote critical reasoning. To
validate practical utility, we also simulate real-world cyber incidents based
on public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct
complex multi-stage attacks. Our results show that retrieval augmentation
improves decision quality and success rates across diverse organizational
models. This work demonstrates the value of integrating retrieval mechanisms
into LLM-based multi-agent systems for cybersecurity decision-making.

### 13. D-RDW: Diversity-Driven Random Walks for News Recommender Systems

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Runze Li, Lucien Heitz, Oana Inel, Abraham Bernstein
- **URL**: <http://arxiv.org/abs/2508.13035v1>
- **Submitted**: 2025-08-18 15:53:30
- **Comment**: 6 pages
- **Topic Keywords**: ranking, recommend, rank
- **Reason**: The paper focuses on recommender systems, which is a related topic, but it's not directly aligned with the user's primary interest in Information Retrieval and Search technologies. The use of random walks and diversity-driven approaches is novel, but the application in news recommender systems is not directly applicable to the user's e-commerce background or interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> This paper introduces Diversity-Driven RandomWalks (D-RDW), a lightweight
algorithm and re-ranking technique that generates diverse news recommendations.
D-RDW is a societal recommender, which combines the diversification
capabilities of the traditional random walk algorithms with customizable target
distributions of news article properties. In doing so, our model provides a
transparent approach for editors to incorporate norms and values into the
recommendation process. D-RDW shows enhanced performance across key diversity
metrics that consider the articles' sentiment and political party mentions when
compared to state-of-the-art neural models. Furthermore, D-RDW proves to be
more computationally efficient than existing approaches.

### 14. Informfully Recommenders -- Reproducibility Framework for Diversity-aware Intra-session Recommendations

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Lucien Heitz, Runze Li, Oana Inel, Abraham Bernstein
- **URL**: <http://arxiv.org/abs/2508.13019v1>
- **Submitted**: 2025-08-18 15:37:41
- **Comment**: 10 pages
- **Topic Keywords**: ranking, recommend, rank
- **Reason**: The paper focuses on recommender systems, which is a related topic, but it does not specifically address query understanding, ranking models, or user behavior modeling, which are core interests. The paper's emphasis on diversity-aware design and normative reproducibility framework is not directly relevant to the user's research themes.

#### Abstract
> Norm-aware recommender systems have gained increased attention, especially
for diversity optimization. The recommender systems community has
well-established experimentation pipelines that support reproducible
evaluations by facilitating models' benchmarking and comparisons against
state-of-the-art methods. However, to the best of our knowledge, there is
currently no reproducibility framework to support thorough norm-driven
experimentation at the pre-processing, in-processing, post-processing, and
evaluation stages of the recommender pipeline. To address this gap, we present
Informfully Recommenders, a first step towards a normative reproducibility
framework that focuses on diversity-aware design built on Cornac. Our extension
provides an end-to-end solution for implementing and experimenting with
normative and general-purpose diverse recommender systems that cover 1) dataset
pre-processing, 2) diversity-optimized models, 3) dedicated intrasession item
re-ranking, and 4) an extensive set of diversity metrics. We demonstrate the
capabilities of our extension through an extensive offline experiment in the
news domain.

### 15. Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Yong Deng, Guoqing Wang, Zhenzhe Ying, Xiaofeng Wu, Jinzhen Lin, Wenwen Xiong, Yuqin Dai, Shuo Yang, Zhanwei Zhang, Qiwen Wang, Yang Qin, Changhua Meng
- **URL**: <http://arxiv.org/abs/2508.12800v1>
- **Submitted**: 2025-08-18 10:23:10
- **Topic Keywords**: rag, retrieval, search
- **Reason**: The paper proposes a novel reinforcement learning framework for agentic deep research, which is somewhat related to information retrieval and search technologies. However, the focus on large language models and reasoning paradigms is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large language models (LLMs) exhibit remarkable problem-solving abilities,
but struggle with complex tasks due to static internal knowledge.
Retrieval-Augmented Generation (RAG) enhances access to external information,
yet remains limited in multi-hop reasoning and strategic search due to rigid
workflows. Recent advancements in agentic deep research empower LLMs to
autonomously reason, search, and synthesize information. However, current
approaches relying on outcome-based reinforcement learning (RL) face critical
issues such as conflicting gradients and reward sparsity, limiting performance
gains and training efficiency. To address these, we first propose Atomic
Thought, a novel LLM thinking paradigm that decomposes reasoning into
fine-grained functional units. These units are supervised by Reasoning Reward
Models (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained
guidance. Building on this, we propose Atom-Searcher, a novel RL framework for
agentic deep research that integrates Atomic Thought and ATR. Atom-Searcher
uses a curriculum-inspired reward schedule, prioritizing process-level ATR
early and transitioning to outcome rewards, accelerating convergence on
effective reasoning paths. Experiments on seven benchmarks show consistent
improvements over the state-of-the-art. Key advantages include: (1)
Atom-Searcher scales computation at test-time. (2) Atomic Thought provides
supervision anchors for RRMs, bridging deep research tasks and RRMs. (3)
Atom-Searcher exhibits more interpretable, human-like reasoning patterns.

### 16. TBGRecall: A Generative Retrieval Model for E-commerce Recommendation Scenarios

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Zida Liang, Changfa Wu, Dunxian Huang, Weiqiang Sun, Ziyang Wang, Yuliang Yan, Jian Wu, Yuning Jiang, Bo Zheng, Ke Chen, Silu Zhou, Yu Zhang
- **URL**: <http://arxiv.org/abs/2508.11977v1>
- **Submitted**: 2025-08-16 08:31:11
- **Comment**: Both authors contributed equally to this research. Work done during
  internship at Alibaba. Corresponding author: Dunxian Huang
  (dunxian.hdx@alibaba-inc.com). Affiliations: (1) Shanghai Jiaotong
  University, Shanghai, China; (2) Alibaba Inc
- **Topic Keywords**: retrieval, recommend, commerce, e-commerce
- **Reason**: The paper focuses on e-commerce recommendation systems, which is related to the user's background in the e-commerce domain. However, the paper's emphasis on generative models and next session prediction is not directly aligned with the user's primary interests in information retrieval, query understanding, and ranking models.

#### Abstract
> Recommendation systems are essential tools in modern e-commerce, facilitating
personalized user experiences by suggesting relevant products. Recent
advancements in generative models have demonstrated potential in enhancing
recommendation systems; however, these models often exhibit limitations in
optimizing retrieval tasks, primarily due to their reliance on autoregressive
generation mechanisms. Conventional approaches introduce sequential
dependencies that impede efficient retrieval, as they are inherently unsuitable
for generating multiple items without positional constraints within a single
request session. To address these limitations, we propose TBGRecall, a
framework integrating Next Session Prediction (NSP), designed to enhance
generative retrieval models for e-commerce applications. Our framework
reformulation involves partitioning input samples into multi-session sequences,
where each sequence comprises a session token followed by a set of item tokens,
and then further incorporate multiple optimizations tailored to the generative
task in retrieval scenarios. In terms of training methodology, our pipeline
integrates limited historical data pre-training with stochastic partial
incremental training, significantly improving training efficiency and
emphasizing the superiority of data recency over sheer data volume. Our
extensive experiments, conducted on public benchmarks alongside a large-scale
industrial dataset from TaoBao, show TBGRecall outperforms the state-of-the-art
recommendation methods, and exhibits a clear scaling law trend. Ultimately, NSP
represents a significant advancement in the effectiveness of generative
recommendation systems for e-commerce applications.

### 17. WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Ralph Peeters, Aaron Steiner, Luca Schwarz, Julian Yuya Caspary, Christian Bizer
- **URL**: <http://arxiv.org/abs/2508.13024v1>
- **Submitted**: 2025-08-18 15:41:22
- **Topic Keywords**: shopping, commerce, e-commerce, search
- **Reason**: The paper introduces a benchmark for evaluating web agents in e-commerce scenarios, focusing on comparison-shopping tasks across multiple shops. While it touches on topics related to information retrieval, such as searching for products and performing price comparisons, the primary focus is on evaluating web agents rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user.

#### Abstract
> LLM-based web agents have the potential to automate long-running web tasks,
such as finding offers for specific products in multiple online shops and
subsequently ordering the cheapest products that meet the users needs. This
paper introduces WebMall, a multi-shop online shopping benchmark for evaluating
the effectiveness and efficiency of web agents for comparison-shopping. WebMall
consists of four simulated online shops populated with authentic product offers
sourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These
tasks include basic tasks such as finding specific products in multiple shops,
performing price comparisons, adding items to the shopping cart, and completing
checkout. Advanced tasks involve searching for products based on vague
requirements, identifying suitable substitutes, and finding compatible
products. Compared to existing e-commerce benchmarks, such as WebShop or
ShoppingBench, WebMall introduces comparison-shopping tasks across multiple
shops. Furthermore, the product offers are more heterogeneous, as they
originate from hundreds of distinct real-world shops. The tasks in WebMall
require longer interaction trajectories than those in WebShop, while remaining
representative of real-world shopping behaviors. We evaluate eight baseline
agents on WebMall, varying in observation modality, memory utilization, and
underlying large language model (GPT 4.1 and Claude Sonnet 4). The
best-performing configurations achieve completion rates of 75% and 53%, and F1
scores of 87% and 63%, on the basic and advanced task sets, respectively.
WebMall is publicly released to facilitate research on web agents and to
promote advancements in navigation, reasoning, and efficiency within e-commerce
scenarios.

### 18. MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Duzhen Zhang, Zixiao Wang, Zhong-Zhi Li, Yahan Yu, Shuncheng Jia, Jiahua Dong, Haotian Xu, Xing Wu, Yingying Zhang, Tielin Zhang, Jie Yang, Xiuying Chen, Le Song
- **URL**: <http://arxiv.org/abs/2508.12393v1>
- **Submitted**: 2025-08-17 15:14:03
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper introduces a framework for constructing temporally evolving medical knowledge graphs, leveraging large language models and PubMed abstracts. While it touches on information retrieval and knowledge graph construction, the focus is on medical domain and does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user.

#### Abstract
> The rapid expansion of medical literature presents growing challenges for
structuring and integrating domain knowledge at scale. Knowledge Graphs (KGs)
offer a promising solution by enabling efficient retrieval, automated
reasoning, and knowledge discovery. However, current KG construction methods
often rely on supervised pipelines with limited generalizability or naively
aggregate outputs from Large Language Models (LLMs), treating biomedical
corpora as static and ignoring the temporal dynamics and contextual uncertainty
of evolving knowledge. To address these limitations, we introduce MedKGent, a
LLM agent framework for constructing temporally evolving medical KGs.
Leveraging over 10 million PubMed abstracts published between 1975 and 2023, we
simulate the emergence of biomedical knowledge via a fine-grained daily time
series. MedKGent incrementally builds the KG in a day-by-day manner using two
specialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor
Agent identifies knowledge triples and assigns confidence scores via
sampling-based estimation, which are used to filter low-confidence extractions
and inform downstream processing. The Constructor Agent incrementally
integrates the retained triples into a temporally evolving graph, guided by
confidence scores and timestamps to reinforce recurring knowledge and resolve
conflicts. The resulting KG contains 156,275 entities and 2,971,384 relational
triples. Quality assessments by two SOTA LLMs and three domain experts
demonstrate an accuracy approaching 90\%, with strong inter-rater agreement. To
evaluate downstream utility, we conduct RAG across seven medical question
answering benchmarks using five leading LLMs, consistently observing
significant improvements over non-augmented baselines. Case studies further
demonstrate the KG's value in literature-based drug repurposing via
confidence-aware causal inference.

### 19. A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Ziyang Chen, Erxue Min, Xiang Zhao, Yunxin Li, Xin Jia, Jinzhi Liao, Jichao Li, Shuaiqiang Wang, Baotian Hu, Dawei Yin
- **URL**: <http://arxiv.org/abs/2508.12282v1>
- **Submitted**: 2025-08-17 08:12:59
- **Comment**: 10 pages, 5 figures
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on a specific application of retrieval-augmented generation in question answering, with a strong emphasis on temporal reasoning. While it touches on retrieval and generation, the primary focus is on question answering and temporal reasoning, which is not directly related to the user's core research themes in information retrieval, search technologies, and user behavior modeling.

#### Abstract
> We introduce ChronoQA, a large-scale benchmark dataset for Chinese question
answering, specifically designed to evaluate temporal reasoning in
Retrieval-Augmented Generation (RAG) systems. ChronoQA is constructed from over
300,000 news articles published between 2019 and 2024, and contains 5,176
high-quality questions covering absolute, aggregate, and relative temporal
types with both explicit and implicit time expressions. The dataset supports
both single- and multi-document scenarios, reflecting the real-world
requirements for temporal alignment and logical consistency. ChronoQA features
comprehensive structural annotations and has undergone multi-stage validation,
including rule-based, LLM-based, and human evaluation, to ensure data quality.
By providing a dynamic, reliable, and scalable resource, ChronoQA enables
structured evaluation across a wide range of temporal tasks, and serves as a
robust benchmark for advancing time-sensitive retrieval-augmented question
answering systems.

### 20. Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: David Heineman, Valentin Hofmann, Ian Magnusson, Yuling Gu, Noah A. Smith, Hannaneh Hajishirzi, Kyle Lo, Jesse Dodge
- **URL**: <http://arxiv.org/abs/2508.13144v1>
- **Submitted**: 2025-08-18 17:56:04
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on language model evaluation, introducing metrics to assess signal and noise in benchmarks. While it touches on the idea of improving evaluation, it doesn't directly relate to query understanding, ranking models, or user behavior modeling in information retrieval, which are core research themes for you. The paper's relevance is limited to the NLP aspect of your research interests, but it doesn't explore topics like deep semantic understanding or real-time relevance optimization.

#### Abstract
> Developing large language models is expensive and involves making decisions
with small experiments, typically by evaluating on large, multi-task evaluation
suites. In this work, we analyze specific properties which make a benchmark
more reliable for such decisions, and interventions to design higher-quality
evaluation benchmarks. We introduce two key metrics that show differences in
current benchmarks: signal, a benchmark's ability to separate better models
from worse models, and noise, a benchmark's sensitivity to random variability
between training steps. We demonstrate that benchmarks with a better
signal-to-noise ratio are more reliable when making decisions at small scale,
and those with less noise have lower scaling law prediction error. These
results suggest that improving signal or noise will lead to more useful
benchmarks, so we introduce three interventions designed to directly affect
signal or noise. For example, we propose that switching to a metric that has
better signal and noise (e.g., perplexity rather than accuracy) leads to better
reliability and improved scaling law error. We also find that filtering noisy
subtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable
multi-task evaluations. We also find that averaging the output of a model's
intermediate checkpoints to reduce noise leads to consistent improvements. We
conclude by recommending that those creating new benchmarks, or selecting which
existing benchmarks to use, aim for high signal and low noise. We use 30
benchmarks for these experiments, and 375 open-weight language models from 60M
to 32B parameters, resulting in a new, publicly available dataset of 900K
evaluation benchmark results, totaling 200M instances.

### 21. Learning to Steer: Input-dependent Steering for Multimodal LLMs

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Jayneel Parekh, Pegah Khayatan, Mustafa Shukor, Arnaud Dapogny, Alasdair Newson, Matthieu Cord
- **URL**: <http://arxiv.org/abs/2508.12815v1>
- **Submitted**: 2025-08-18 10:53:20
- **Topic Keywords**: query
- **Reason**: The paper explores steering techniques for multimodal LLMs, which is a related topic to query understanding and ranking models. However, the focus on multimodal LLMs and the specific problem of hallucinations and safety enforcement is not directly aligned with the user's primary research interests in information retrieval and search technologies.

#### Abstract
> Steering has emerged as a practical approach to enable post-hoc guidance of
LLMs towards enforcing a specific behavior. However, it remains largely
underexplored for multimodal LLMs (MLLMs); furthermore, existing steering
techniques, such as mean steering, rely on a single steering vector, applied
independently of the input query. This paradigm faces limitations when the
desired behavior is dependent on the example at hand. For example, a safe
answer may consist in abstaining from answering when asked for an illegal
activity, or may point to external resources or consultation with an expert
when asked about medical advice. In this paper, we investigate a fine-grained
steering that uses an input-specific linear shift. This shift is computed using
contrastive input-specific prompting. However, the input-specific prompts
required for this approach are not known at test time. Therefore, we propose to
train a small auxiliary module to predict the input-specific steering vector.
Our approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces
hallucinations and enforces safety in MLLMs, outperforming other static
baselines.

### 22. Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation for Agentic AI with a Universal Evaluation Framework

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Zheye Deng, Chunkit Chan, Tianshi Zheng, Wei Fan, Weiqi Wang, Yangqiu Song
- **URL**: <http://arxiv.org/abs/2508.12257v1>
- **Submitted**: 2025-08-17 06:41:40
- **Comment**: Under Review
- **Topic Keywords**: retrieval, search
- **Reason**: The paper's focus on text-to-structure generation and its applications in AI systems is somewhat related to information retrieval and NLP, but it does not directly address query understanding, ranking models, or user behavior modeling. While the paper's emphasis on structured formats and evaluation frameworks may be relevant to data mining, it does not seem to be a central match for the user's research interests.

#### Abstract
> The evolution of AI systems toward agentic operation and context-aware
retrieval necessitates transforming unstructured text into structured formats
like tables, knowledge graphs, and charts. While such conversions enable
critical applications from summarization to data mining, current research lacks
a comprehensive synthesis of methodologies, datasets, and metrics. This
systematic review examines text-to-structure techniques and the encountered
challenges, evaluates current datasets and assessment criteria, and outlines
potential directions for future research. We also introduce a universal
evaluation framework for structured outputs, establishing text-to-structure as
foundational infrastructure for next-generation AI systems.

### 23. RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Xin Chen, Junchao Wu, Shu Yang, Runzhe Zhan, Zeyu Wu, Ziyang Luo, Di Wang, Min Yang, Lidia S. Chao, Derek F. Wong
- **URL**: <http://arxiv.org/abs/2508.13152v1>
- **Submitted**: 2025-08-18 17:59:15
- **Comment**: Accepted to TACL 2025. This version is a pre-MIT Press publication
  version
- **Topic Keywords**: rag
- **Reason**: The paper focuses on detecting LLM-generated text, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on neural activation patterns, it does not address ranking models or user behavior modeling. The topic is more relevant to NLP and data mining, but the focus on detection rather than semantic understanding or real-time relevance optimization limits its alignment with your research interests.

#### Abstract
> Detecting content generated by large language models (LLMs) is crucial for
preventing misuse and building trustworthy AI systems. Although existing
detection methods perform well, their robustness in out-of-distribution (OOD)
scenarios is still lacking. In this paper, we hypothesize that, compared to
features used by existing detection methods, the internal representations of
LLMs contain more comprehensive and raw features that can more effectively
capture and distinguish the statistical pattern differences between
LLM-generated texts (LGT) and human-written texts (HWT). We validated this
hypothesis across different LLMs and observed significant differences in neural
activation patterns when processing these two types of texts. Based on this, we
propose RepreGuard, an efficient statistics-based detection method.
Specifically, we first employ a surrogate model to collect representation of
LGT and HWT, and extract the distinct activation feature that can better
identify LGT. We can classify the text by calculating the projection score of
the text representations along this feature direction and comparing with a
precomputed threshold. Experimental results show that RepreGuard outperforms
all baselines with average 94.92% AUROC on both in-distribution (ID) and OOD
scenarios, while also demonstrating robust resilience to various text sizes and
mainstream attacks. Data and code are publicly available at:
https://github.com/NLP2CT/RepreGuard

### 24. Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Kawin Mayilvaghanan, Siddhant Gupta, Ayush Kumar
- **URL**: <http://arxiv.org/abs/2508.13124v1>
- **Submitted**: 2025-08-18 17:31:03
- **Topic Keywords**: rag
- **Reason**: The paper focuses on Large Language Models (LLMs) and their biases in contact center summaries, which is somewhat related to information retrieval and search technologies. However, the specific context of contact centers and summarization is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Abstractive summarization is a core application in contact centers, where
Large Language Models (LLMs) generate millions of summaries of call transcripts
daily. Despite their apparent quality, it remains unclear whether LLMs
systematically under- or over-attend to specific aspects of the transcript,
potentially introducing biases in the generated summary. While prior work has
examined social and positional biases, the specific forms of bias pertinent to
contact center operations - which we term Operational Bias - have remained
unexplored. To address this gap, we introduce BlindSpot, a framework built upon
a taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic)
for the identification and quantification of these biases. BlindSpot leverages
an LLM as a zero-shot classifier to derive categorical distributions for each
bias dimension in a pair of transcript and its summary. The bias is then
quantified using two metrics: Fidelity Gap (the JS Divergence between
distributions) and Coverage (the percentage of source labels omitted). Using
BlindSpot, we conducted an empirical study with 2500 real call transcripts and
their summaries generated by 20 LLMs of varying scales and families (e.g., GPT,
Llama, Claude). Our analysis reveals that biases are systemic and present
across all evaluated models, regardless of size or family.

### 25. An LLM Agent-Based Complex Semantic Table Annotation Approach

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Yilin Geng, Shujing Wang, Chuan Wang, Keqing He, Yanfei Lv, Ying Wang, Zaiwen Feng, Xiaoying Bai
- **URL**: <http://arxiv.org/abs/2508.12868v1>
- **Submitted**: 2025-08-18 12:09:20
- **Topic Keywords**: rag
- **Reason**: The paper proposes an LLM-based approach for Semantic Table Annotation, which is a specific problem in Natural Language Processing. While it's related to information retrieval, it doesn't directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's focus on table annotation and ontology entities is somewhat relevant, but it doesn't align with the user's primary focus on information retrieval and real-time relevance optimization.

#### Abstract
> The Semantic Table Annotation (STA) task, which includes Column Type
Annotation (CTA) and Cell Entity Annotation (CEA), maps table contents to
ontology entities and plays important roles in various semantic applications.
However, complex tables often pose challenges such as semantic loss of column
names or cell values, strict ontological hierarchy requirements, homonyms,
spelling errors, and abbreviations, which hinder annotation accuracy. To
address these issues, this paper proposes an LLM-based agent approach for CTA
and CEA. We design and implement five external tools with tailored prompts
based on the ReAct framework, enabling the STA agent to dynamically select
suitable annotation strategies depending on table characteristics. Experiments
are conducted on the Tough Tables and BiodivTab datasets from the SemTab
challenge, which contain the aforementioned challenges. Our method outperforms
existing approaches across various metrics. Furthermore, by leveraging
Levenshtein distance to reduce redundant annotations, we achieve a 70%
reduction in time costs and a 60% reduction in LLM token usage, providing an
efficient and cost-effective solution for STA.

### 26. E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Ronghao Lin, Shuai Shen, Weipeng Hu, Qiaolin He, Aolin Xiong, Li Huang, Haifeng Hu, Yap-peng Tan
- **URL**: <http://arxiv.org/abs/2508.12854v1>
- **Submitted**: 2025-08-18 11:47:02
- **Comment**: Accepted at ACM MM 2025 Grand Challenge
- **Topic Keywords**: retrieval
- **Reason**: The paper focuses on multimodal empathetic response generation, which is not directly related to information retrieval or search technologies. While it involves language models, the primary goal is not query understanding, ranking models, or user behavior modeling. The paper's relevance to your interests is limited to the use of language models, but the context and application are quite different.

#### Abstract
> Multimodal Empathetic Response Generation (MERG) is crucial for building
emotionally intelligent human-computer interactions. Although large language
models (LLMs) have improved text-based ERG, challenges remain in handling
multimodal emotional content and maintaining identity consistency. Thus, we
propose E3RG, an Explicit Emotion-driven Empathetic Response Generation System
based on multimodal LLMs which decomposes MERG task into three parts:
multimodal empathy understanding, empathy memory retrieval, and multimodal
response generation. By integrating advanced expressive speech and video
generative models, E3RG delivers natural, emotionally rich, and
identity-consistent responses without extra training. Experiments validate the
superiority of our system on both zero-shot and few-shot settings, securing
Top-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.
Our code is available at https://github.com/RH-Lin/E3RG.

### 27. HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks

- **LLM Score**: 2
- **Keyword Score**: 13
- **Authors**: Zhe Chen, Yusheng Liao, Shuyang Jiang, Zhiyuan Zhu, Haolin Li, Yanfeng Wang, Yu Wang
- **URL**: <http://arxiv.org/abs/2508.12778v1>
- **Submitted**: 2025-08-18 09:54:10
- **Topic Keywords**: query, queries, relevance, rag, retrieval
- **Reason**: The paper focuses on medical vision-language tasks, retrieval-augmented generation, and multimodal report repositories, which are not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on medical applications and clinical decision-making also diverges from the user's e-commerce background and general interest in real-time relevance optimization.

#### Abstract
> Medical large vision-language Models (Med-LVLMs) have shown promise in
clinical applications but suffer from factual inaccuracies and unreliable
outputs, posing risks in real-world diagnostics. While retrieval-augmented
generation has emerged as a potential solution, current medical multimodal RAG
systems are unable to perform effective retrieval across heterogeneous sources.
The irrelevance of retrieved reports affects the factuality of analysis, while
insufficient knowledge affects the credibility of clinical decision-making. To
bridge the gap, we construct MedAtlas, which includes extensive multimodal
report repositories and diverse text corpora. Based on it, we present
HeteroRAG, a novel framework that enhances Med-LVLMs through heterogeneous
knowledge sources. The framework introduces Modality-specific CLIPs for
effective report retrieval and a Multi-corpora Query Generator for dynamically
constructing queries for diverse corpora. Incorporating knowledge from such
multifaceted sources, Med-LVLM is then trained with Heterogeneous Knowledge
Preference Tuning to achieve cross-modality and multi-source knowledge
alignment. Extensive experiments across 12 datasets and 3 modalities
demonstrate that the proposed HeteroRAG achieves state-of-the-art performance
in most medical vision language benchmarks, significantly improving factual
accuracy and reliability of Med-LVLMs.

### 28. OptimalThinkingBench: Evaluating Over and Underthinking in LLMs

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Pranjal Aggarwal, Seungone Kim, Jack Lanchantin, Sean Welleck, Jason Weston, Ilia Kulikov, Swarnadeep Saha
- **URL**: <http://arxiv.org/abs/2508.13141v1>
- **Submitted**: 2025-08-18 17:53:10
- **Comment**: 26 pages, 6 tables, 10 figures
- **Topic Keywords**: query, queries, rag
- **Reason**: The paper focuses on Large Language Models (LLMs) and their thinking/overthinking behavior, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on the concept of 'optimal thinking', it does not address ranking models, user behavior modeling, or real-time relevance optimization, making it only loosely relevant to the user's research interests.

#### Abstract
> Thinking LLMs solve complex tasks at the expense of increased compute and
overthinking on simpler problems, while non-thinking LLMs are faster and
cheaper but underthink on harder reasoning problems. This has led to the
development of separate thinking and non-thinking LLM variants, leaving the
onus of selecting the optimal model for each query on the end user. In this
work, we introduce OptimalThinkingBench, a unified benchmark that jointly
evaluates overthinking and underthinking in LLMs and also encourages the
development of optimally-thinking models that balance performance and
efficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench,
featuring simple queries in 72 domains, and UnderthinkingBench, containing 11
challenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, we
perform extensive evaluation of 33 different thinking and non-thinking models
and show that no model is able to optimally think on our benchmark. Thinking
models often overthink for hundreds of tokens on the simplest user queries
without improving performance. In contrast, large non-thinking models
underthink, often falling short of much smaller thinking models. We further
explore several methods to encourage optimal thinking, but find that these
approaches often improve on one sub-benchmark at the expense of the other,
highlighting the need for better unified and optimal models in the future.

### 29. CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Shaoming Duan, Zirui Wang, Chuanyi Liu, Zhibin Zhu, Yuhao Zhang, Peiyi Han, Liang Yan, Zewu Penge
- **URL**: <http://arxiv.org/abs/2508.12769v1>
- **Submitted**: 2025-08-18 09:43:07
- **Topic Keywords**: queries, rag, retrieval
- **Reason**: The paper focuses on Text-to-SQL parsing, which is not directly related to Information Retrieval (IR) or Search technologies. Although it mentions large language models, the primary goal is to improve SQL generation, which is not a core interest of yours.

#### Abstract
> Recent advances in large language models (LLMs) have significantly improved
the accuracy of Text-to-SQL systems. However, a critical challenge remains: the
semantic mismatch between natural language questions (NLQs) and their
corresponding SQL queries. This issue is exacerbated in large-scale databases,
where semantically similar attributes hinder schema linking and semantic drift
during SQL generation, ultimately reducing model accuracy. To address these
challenges, we introduce CRED-SQL, a framework designed for large-scale
databases that integrates Cluster Retrieval and Execution Description. CRED-SQL
first performs cluster-based large-scale schema retrieval to pinpoint the
tables and columns most relevant to a given NLQ, alleviating schema mismatch.
It then introduces an intermediate natural language representation-Execution
Description Language (EDL)-to bridge the gap between NLQs and SQL. This
reformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,
leveraging LLMs' strong general reasoning capabilities while reducing semantic
deviation. Extensive experiments on two large-scale, cross-domain
benchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new
state-of-the-art (SOTA) performance, validating its effectiveness and
scalability. Our code is available at https://github.com/smduan/CRED-SQL.git

### 30. VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Haidong Xu, Guangwei Xu, Zhedong Zheng, Xiatian Zhu, Wei Ji, Xiangtai Li, Ruijie Guo, Meishan Zhang, Min zhang, Hao Fei
- **URL**: <http://arxiv.org/abs/2508.12081v1>
- **Submitted**: 2025-08-16 15:31:14
- **Comment**: 20 pages,13 figures
- **Topic Keywords**: retriever, rag, retrieval
- **Reason**: The paper focuses on video-based retrieval-augmented motion generation for motion language models, which is not directly related to information retrieval, search technologies, or query understanding. While it involves retrieval, it is a specific application in the domain of computer vision and natural language processing, and does not align with the user's primary research interests.

#### Abstract
> This paper introduces VimoRAG, a novel video-based retrieval-augmented motion
generation framework for motion large language models (LLMs). As motion LLMs
face severe out-of-domain/out-of-vocabulary issues due to limited annotated
data, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D
motion generation by retrieving relevant 2D human motion signals. While
video-based motion RAG is nontrivial, we address two key bottlenecks: (1)
developing an effective motion-centered video retrieval model that
distinguishes human poses and actions, and (2) mitigating the issue of error
propagation caused by suboptimal retrieval results. We design the Gemini Motion
Video Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,
enabling effective retrieval and generation processes. Experimental results
show that VimoRAG significantly boosts the performance of motion LLMs
constrained to text-only input.

### 31. Leveraging Geometric Insights in Hyperbolic Triplet Loss for Improved Recommendations

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Viacheslav Yusupov, Maxim Rakhuba, Evgeny Frolov
- **URL**: <http://arxiv.org/abs/2508.11978v1>
- **Submitted**: 2025-08-16 08:34:17
- **Topic Keywords**: pairwise, rag, recommend
- **Reason**: The paper focuses on recommender systems, which is a related topic, but it does not address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The use of hyperbolic geometry and triplet loss is innovative, but it is not directly applicable to information retrieval or search technologies.

#### Abstract
> Recent studies have demonstrated the potential of hyperbolic geometry for
capturing complex patterns from interaction data in recommender systems. In
this work, we introduce a novel hyperbolic recommendation model that uses
geometrical insights to improve representation learning and increase
computational stability at the same time. We reformulate the notion of
hyperbolic distances to unlock additional representation capacity over
conventional Euclidean space and learn more expressive user and item
representations. To better capture user-items interactions, we construct a
triplet loss that models ternary relations between users and their
corresponding preferred and nonpreferred choices through a mix of pairwise
interaction terms driven by the geometry of data. Our hyperbolic approach not
only outperforms existing Euclidean and hyperbolic models but also reduces
popularity bias, leading to more diverse and personalized recommendations.

### 32. Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Yiqun Zhang, Hao Li, Jianhao Chen, Hangfan Zhang, Peng Ye, Lei Bai, Shuyue Hu
- **URL**: <http://arxiv.org/abs/2508.12631v1>
- **Submitted**: 2025-08-18 05:23:31
- **Comment**: Ongoing work
- **Topic Keywords**: queries, rag
- **Reason**: The paper focuses on large language models and their performance-efficiency tradeoffs, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and query understanding. While the paper mentions routing and optimization, it does not address ranking models, user behavior modeling, or deep semantic understanding, making it only loosely relevant to the user's interests.

#### Abstract
> Balancing performance and efficiency is a central challenge in large language
model (LLM) advancement. GPT-5 addresses this with test-time routing,
dynamically assigning queries to either an efficient or a high-capacity model
during inference. In this work, we present Avengers-Pro, a test-time routing
framework that ensembles LLMs of varying capacities and efficiencies, providing
a unified solution for all performance-efficiency tradeoffs. The Avengers-Pro
embeds and clusters incoming queries, then routes each to the most suitable
model based on a performance-efficiency score. Across 6 challenging benchmarks
and 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and
Claude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a
performance-efficiency trade-off parameter, it can surpass the strongest single
model (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the
average accuracy of the strongest single model at 27% lower cost, and reach
~90% of that performance at 63% lower cost. Last but not least, it achieves a
Pareto frontier, consistently yielding the highest accuracy for any given cost,
and the lowest cost for any given accuracy, among all single models. Code is
available at https://github.com/ZhangYiqun018/AvengersPro.

### 33. From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Javier Garcia Gilabert, Xixian Liao, Severino Da Dalt, Ella Bohman, Audrey Mash, Francesca De Luca Fornaciari, Irene Baucells, Joan Llop, Miguel Claramunt Argote, Carlos Escolano, Maite Melero
- **URL**: <http://arxiv.org/abs/2508.12774v1>
- **Submitted**: 2025-08-18 09:48:35
- **Topic Keywords**: ranking, rank
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on machine translation and does not mention query understanding, ranking models, or user behavior modeling, which are key areas of interest for you.

#### Abstract
> In this paper, we present the SALAMANDRATA family of models, an improved
iteration of SALAMANDRA LLMs (Gonzalez-Agirre et al., 2025) specifically
trained to achieve strong performance in translation-related tasks for 38
European languages. SALAMANDRATA comes in two scales: 2B and 7B parameters. For
both versions, we applied the same training recipe with a first step of
continual pre-training on parallel data, and a second step of supervised
fine-tuning on high-quality instructions. The BSC submission to the WMT25
General Machine Translation shared task is based on the 7B variant of
SALAMANDRATA. We first adapted the model vocabulary to support the additional
non-European languages included in the task. This was followed by a second
phase of continual pre-training and supervised fine-tuning, carefully designed
to optimize performance across all translation directions for this year's
shared task. For decoding, we employed two quality-aware strategies: Minimum
Bayes Risk Decoding and Tuned Re-ranking using COMET and COMET-KIWI
respectively. We publicly release both the 2B and 7B versions of SALAMANDRATA,
along with the newer SALAMANDRATA-V2 model, on Hugging Face1

### 34. Leveraging Large Language Models for Predictive Analysis of Human Misery

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Bishanka Seal, Rahul Seetharaman, Aman Bansal, Abhilash Nandy
- **URL**: <http://arxiv.org/abs/2508.12669v1>
- **Submitted**: 2025-08-18 07:02:59
- **Comment**: 14 pages, 4 tables
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling, which are the primary areas of interest. The focus on Large Language Models and predicting human-perceived misery scores is outside the scope of the user's research themes.

#### Abstract
> This study investigates the use of Large Language Models (LLMs) for
predicting human-perceived misery scores from natural language descriptions of
real-world scenarios. The task is framed as a regression problem, where the
model assigns a scalar value from 0 to 100 to each input statement. We evaluate
multiple prompting strategies, including zero-shot, fixed-context few-shot, and
retrieval-based prompting using BERT sentence embeddings. Few-shot approaches
consistently outperform zero-shot baselines, underscoring the value of
contextual examples in affective prediction. To move beyond static evaluation,
we introduce the "Misery Game Show", a novel gamified framework inspired by a
television format. It tests LLMs through structured rounds involving ordinal
comparison, binary classification, scalar estimation, and feedback-driven
reasoning. This setup enables us to assess not only predictive accuracy but
also the model's ability to adapt based on corrective feedback. The gamified
evaluation highlights the broader potential of LLMs in dynamic emotional
reasoning tasks beyond standard regression. Code and data link:
https://github.com/abhi1nandy2/Misery_Data_Exps_GitHub

### 35. Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Maitreyi Chatterjee, Devansh Agarwal
- **URL**: <http://arxiv.org/abs/2508.12630v1>
- **Submitted**: 2025-08-18 05:14:48
- **Comment**: Paper is currently in peer review
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on conversational AI and memory persistence, using techniques like dependency parsing, discourse relation tagging, and coreference resolution. While it involves natural language processing, it is not directly related to information retrieval, search technologies, or query understanding, which are the user's primary research interests.

#### Abstract
> Large Language Models (LLMs) have demonstrated impressive fluency and task
competence in conversational settings. However, their effectiveness in
multi-session and long-term interactions is hindered by limited memory
persistence. Typical retrieval-augmented generation (RAG) systems store
dialogue history as dense vectors, which capture semantic similarity but
neglect finer linguistic structures such as syntactic dependencies, discourse
relations, and coreference links. We propose Semantic Anchoring, a hybrid
agentic memory architecture that enriches vector-based storage with explicit
linguistic cues to improve recall of nuanced, context-rich exchanges. Our
approach combines dependency parsing, discourse relation tagging, and
coreference resolution to create structured memory entries. Experiments on
adapted long-term dialogue datasets show that semantic anchoring improves
factual recall and discourse coherence by up to 18% over strong RAG baselines.
We further conduct ablation studies, human evaluations, and error analysis to
assess robustness and interpretability.

### 36. STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Haiquan Hu, Jiazhi Jiang, Shiyou Xu, Ruhan Zeng, Tian Wang
- **URL**: <http://arxiv.org/abs/2508.12096v1>
- **Submitted**: 2025-08-16 16:36:43
- **Comment**: Submit to AAAI 2026
- **Topic Keywords**: ranking, rank
- **Reason**: The paper focuses on evaluating large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on ranking models, the context is different from the user's primary interests in IR and NLP.

#### Abstract
> Evaluating large language models (LLMs) has become increasingly challenging
as model capabilities advance rapidly. While recent models often achieve higher
scores on standard benchmarks, these improvements do not consistently reflect
enhanced real-world reasoning capabilities. Moreover, widespread overfitting to
public benchmarks and the high computational cost of full evaluations have made
it both expensive and less effective to distinguish meaningful differences
between models. To address these challenges, we propose the \textbf{S}tructured
\textbf{T}ransition \textbf{E}valuation \textbf{M}ethod (STEM), a lightweight
and interpretable evaluation framework for efficiently estimating the relative
capabilities of LLMs. STEM identifies \textit{significant transition samples}
(STS) by analyzing consistent performance transitions among LLMs of the same
architecture but varying parameter scales. These samples enable STEM to
effectively estimate the capability position of an unknown model. Qwen3 model
family is applied to construct the STS pool on six diverse and representative
benchmarks. To assess generalizability. Experimental results indicate that STEM
reliably captures performance trends, aligns with ground-truth rankings of
model capability. These findings highlight STEM as a practical and scalable
method for fine-grained, architecture-agnostic evaluation of LLMs.

### 37. VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Ming Cheng, Tong Wu, Jiazhen Hu, Jiaying Gong, Hoda Eldardiry
- **URL**: <http://arxiv.org/abs/2508.11801v1>
- **Submitted**: 2025-08-15 20:58:47
- **Comment**: 5 pages, 2 figures, 5 tables, accepted in CIKM 2025
- **Topic Keywords**: rag, commerce, e-commerce
- **Reason**: The paper focuses on Attribute Value Extraction (AVE) in the e-commerce domain, specifically on video-to-text AVE, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although the paper mentions video vision language models (VLMs), it does not address query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user.

#### Abstract
> Attribute Value Extraction (AVE) is important for structuring product
information in e-commerce. However, existing AVE datasets are primarily limited
to text-to-text or image-to-text settings, lacking support for product videos,
diverse attribute coverage, and public availability. To address these gaps, we
introduce VideoAVE, the first publicly available video-to-text e-commerce AVE
dataset across 14 different domains and covering 172 unique attributes. To
ensure data quality, we propose a post-hoc CLIP-based Mixture of Experts
filtering system (CLIP-MoE) to remove the mismatched video-product pairs,
resulting in a refined dataset of 224k training data and 25k evaluation data.
In order to evaluate the usability of the dataset, we further establish a
comprehensive benchmark by evaluating several state-of-the-art video vision
language models (VLMs) under both attribute-conditioned value prediction and
open attribute-value pair extraction tasks. Our results analysis reveals that
video-to-text AVE remains a challenging problem, particularly in open settings,
and there is still room for developing more advanced VLMs capable of leveraging
effective temporal information. The dataset and benchmark code for VideoAVE are
available at: https://github.com/gjiaying/VideoAVE

### 38. Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Raneem Alharthi, Rajwa Alharthi, Aiqi Jiang, Arkaitz Zubiaga
- **URL**: <http://arxiv.org/abs/2508.12828v1>
- **Submitted**: 2025-08-18 11:12:21
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on abusive language detection in conversational exchanges, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on context and conversational exchanges is also not aligned with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Abusive language detection has become an increasingly important task as a
means to tackle this type of harmful content in social media. There has been a
substantial body of research developing models for determining if a social
media post is abusive or not; however, this research has primarily focused on
exploiting social media posts individually, overlooking additional context that
can be derived from surrounding posts. In this study, we look at conversational
exchanges, where a user replies to an earlier post by another user (the parent
tweet). We ask: does leveraging context from the parent tweet help determine if
a reply post is abusive or not, and what are the features that contribute the
most? We study a range of content-based and account-based features derived from
the context, and compare this to the more widely studied approach of only
looking at the features from the reply tweet. For a more generalizable study,
we test four different classification models on a dataset made of
conversational exchanges (parent-reply tweet pairs) with replies labeled as
abusive or not. Our experiments show that incorporating contextual features
leads to substantial improvements compared to the use of features derived from
the reply tweet only, confirming the importance of leveraging context. We
observe that, among the features under study, it is especially the
content-based features (what is being posted) that contribute to the
classification performance rather than account-based features (who is posting
it). While using content-based features, it is best to combine a range of
different features to ensure improved performance over being more selective and
using fewer features. Our study provides insights into the development of
contextualized abusive language detection models in realistic settings
involving conversations.

### 39. Deep Research: A Survey of Autonomous Research Agents

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Wenlin Zhang, Xiaopeng Li, Yingyi Zhang, Pengyue Jia, Yichao Wang, Huifeng Guo, Yong Liu, Xiangyu Zhao
- **URL**: <http://arxiv.org/abs/2508.12752v1>
- **Submitted**: 2025-08-18 09:26:14
- **Topic Keywords**: retrieval, search
- **Reason**: The paper focuses on autonomous research agents and deep research pipeline, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions large language models, the context is different from the user's interests in NLP and IR.

#### Abstract
> The rapid advancement of large language models (LLMs) has driven the
development of agentic systems capable of autonomously performing complex
tasks. Despite their impressive capabilities, LLMs remain constrained by their
internal knowledge boundaries. To overcome these limitations, the paradigm of
deep research has been proposed, wherein agents actively engage in planning,
retrieval, and synthesis to generate comprehensive and faithful analytical
reports grounded in web-based evidence. In this survey, we provide a systematic
overview of the deep research pipeline, which comprises four core stages:
planning, question developing, web exploration, and report generation. For each
stage, we analyze the key technical challenges and categorize representative
methods developed to address them. Furthermore, we summarize recent advances in
optimization techniques and benchmarks tailored for deep research. Finally, we
discuss open challenges and promising research directions, aiming to chart a
roadmap toward building more capable and trustworthy deep research agents.

### 40. Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, and Challenges

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Abdelhamid Haouhat, Slimane Bellaouar, Attia Nehar, Hadda Cherroun, Ahmed Abdelali
- **URL**: <http://arxiv.org/abs/2508.12227v1>
- **Submitted**: 2025-08-17 03:59:27
- **Topic Keywords**: retrieval, search
- **Reason**: The paper focuses on Arabic Multimodal Machine Learning, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on multimedia retrieval, the primary focus is on sentiment analysis, emotion recognition, and other tasks outside the user's core research themes.

#### Abstract
> Multimodal Machine Learning (MML) aims to integrate and analyze information
from diverse modalities, such as text, audio, and visuals, enabling machines to
address complex tasks like sentiment analysis, emotion recognition, and
multimedia retrieval. Recently, Arabic MML has reached a certain level of
maturity in its foundational development, making it time to conduct a
comprehensive survey. This paper explores Arabic MML by categorizing efforts
through a novel taxonomy and analyzing existing research. Our taxonomy
organizes these efforts into four key topics: datasets, applications,
approaches, and challenges. By providing a structured overview, this survey
offers insights into the current state of Arabic MML, highlighting areas that
have not been investigated and critical research gaps. Researchers will be
empowered to build upon the identified opportunities and address challenges to
advance the field.

### 41. CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Punya Syon Pandey, Yongjin Yang, Jiarui Liu, Zhijing Jin
- **URL**: <http://arxiv.org/abs/2508.11915v1>
- **Submitted**: 2025-08-16 05:26:36
- **Topic Keywords**: pairwise
- **Reason**: The paper focuses on measuring the quality of language interactions between agents with Large Language Models (LLMs) in game-theoretic settings, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on natural language processing, the context is quite different from the user's primary interests.

#### Abstract
> Game-theoretic interactions between agents with Large Language Models (LLMs)
have revealed many emergent capabilities, yet the linguistic diversity of these
interactions has not been sufficiently quantified. In this paper, we present
the Conversational Robustness Evaluation Score: CORE, a metric to quantify the
effectiveness of language use within multi-agent systems across different
game-theoretic interactions. CORE integrates measures of cluster entropy,
lexical repetition, and semantic similarity, providing a direct lens of dialog
quality. We apply CORE to pairwise LLM dialogs across competitive, cooperative,
and neutral settings, further grounding our analysis in Zipf's and Heaps' Laws
to characterize word frequency distributions and vocabulary growth. Our
findings show that cooperative settings exhibit both steeper Zipf distributions
and higher Heap exponents, indicating more repetition alongside greater
vocabulary expansion. In contrast, competitive interactions display lower Zipf
and Heaps exponents, reflecting less repetition and more constrained
vocabularies. These results provide new insights into how social incentives
influence language adaptation, and highlight CORE as a robust diagnostic for
measuring linguistic robustness in multi-agent LLM systems. Our code is
available at https://github.com/psyonp/core.

### 42. Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Leigh Levinson, Christopher J. Agostino
- **URL**: <http://arxiv.org/abs/2508.11829v1>
- **Submitted**: 2025-08-15 22:26:42
- **Comment**: 9 pages, 1 figure, submitted to NeurIPS Creative AI track
- **Topic Keywords**: relevance
- **Reason**: The paper explores the concept of biological rhythms and hormones in AI systems, which is unrelated to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's focus on Large Language Models and linguistic analysis is also outside the user's primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Despite significant advances, AI systems struggle with the frame problem:
determining what information is contextually relevant from an exponentially
large possibility space. We hypothesize that biological rhythms, particularly
hormonal cycles, serve as natural relevance filters that could address this
fundamental challenge. We develop a framework that embeds simulated menstrual
and circadian cycles into Large Language Models through system prompts
generated from periodic functions modeling key hormones including estrogen,
testosterone, and cortisol. Across multiple state-of-the-art models, linguistic
analysis reveals emotional and stylistic variations that track biological
phases; sadness peaks during menstruation while happiness dominates ovulation
and circadian patterns show morning optimism transitioning to nocturnal
introspection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates
subtle but consistent performance variations aligning with biological
expectations, including optimal function in moderate rather than extreme
hormonal ranges. This methodology provides a novel approach to contextual AI
while revealing how societal biases regarding gender and biology are embedded
within language models.

### 43. A Survey of Idiom Datasets for Psycholinguistic and Computational Research

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Michael Flor, Xinyi Liu, Anna Feldman
- **URL**: <http://arxiv.org/abs/2508.11828v1>
- **Submitted**: 2025-08-15 22:24:09
- **Comment**: KONVENS 2025. To appear
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on idiom datasets for psycholinguistic and computational research, which is not directly related to information retrieval, search technologies, or query understanding. Although it touches on computational linguistics, the topic is too specific and does not align with the user's primary research interests in IR and NLP.

#### Abstract
> Idioms are figurative expressions whose meanings often cannot be inferred
from their individual words, making them difficult to process computationally
and posing challenges for human experimental studies. This survey reviews
datasets developed in psycholinguistics and computational linguistics for
studying idioms, focusing on their content, form, and intended use.
Psycholinguistic resources typically contain normed ratings along dimensions
such as familiarity, transparency, and compositionality, while computational
datasets support tasks like idiomaticity detection/classification,
paraphrasing, and cross-lingual modeling. We present trends in annotation
practices, coverage, and task framing across 53 datasets. Although recent
efforts expanded language coverage and task diversity, there seems to be no
relation yet between psycholinguistic and computational research on idioms.

### 44. Has GPT-5 Achieved Spatial Intelligence? An Empirical Study

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zhongang Cai, Yubo Wang, Qingping Sun, Ruisi Wang, Chenyang Gu, Wanqi Yin, Zhiqian Lin, Zhitao Yang, Chen Wei, Xuanke Shi, Kewang Deng, Xiaoyang Han, Zukai Chen, Jiaqi Li, Xiangyu Fan, Hanming Deng, Lewei Lu, Bo Li, Ziwei Liu, Quan Wang, Dahua Lin, Lei Yang
- **URL**: <http://arxiv.org/abs/2508.13142v1>
- **Submitted**: 2025-08-18 17:55:17
- **Topic Keywords**: ctr
- **Reason**: This paper is not relevant to your research interests as it focuses on spatial intelligence and multi-modal models, which is outside the scope of Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on GPT-5 and its capabilities is also not related to your interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Multi-modal models have achieved remarkable progress in recent years.
Nevertheless, they continue to exhibit notable limitations in spatial
understanding and reasoning, which are fundamental capabilities to achieving
artificial general intelligence. With the recent release of GPT-5, allegedly
the most powerful AI model to date, it is timely to examine where the leading
models stand on the path toward spatial intelligence. First, we propose a
comprehensive taxonomy of spatial tasks that unifies existing benchmarks and
discuss the challenges in ensuring fair evaluation. We then evaluate
state-of-the-art proprietary and open-source models on eight key benchmarks, at
a cost exceeding one billion total tokens. Our empirical study reveals that (1)
GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)
still falls short of human performance across a broad spectrum of tasks.
Moreover, we (3) identify the more challenging spatial intelligence problems
for multi-modal models, and (4) proprietary models do not exhibit a decisive
advantage when facing the most difficult problems. In addition, we conduct a
qualitative evaluation across a diverse set of scenarios that are intuitive for
humans yet fail even the most advanced multi-modal models.

### 45. Reinforced Context Order Recovery for Adaptive Reasoning and Planning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Long Ma, Fangwei Zhong, Yizhou Wang
- **URL**: <http://arxiv.org/abs/2508.13070v1>
- **Submitted**: 2025-08-18 16:42:55
- **Topic Keywords**: acl
- **Reason**: The paper focuses on developing a framework for extracting adaptive token generation orders from text data, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on natural language processing, the specific area of study is not relevant to the user's primary research interests.

#### Abstract
> Modern causal language models, followed by rapid developments in discrete
diffusion models, can now produce a wide variety of interesting and useful
content. However, these families of models are predominantly trained to output
tokens with a fixed (left-to-right) or random order, which may deviate from the
logical order in which tokens are generated originally. In this paper, we
observe that current causal and diffusion models encounter difficulties in
problems that require adaptive token generation orders to solve tractably,
which we characterize with the $\mathcal{V}$-information framework. Motivated
by this, we propose Reinforced Context Order Recovery (ReCOR), a
reinforcement-learning-based framework to extract adaptive, data-dependent
token generation orders from text data without annotations. Self-supervised by
token prediction statistics, ReCOR estimates the hardness of predicting every
unfilled token and adaptively selects the next token during both training and
inference. Experiments on challenging reasoning and planning datasets
demonstrate the superior performance of ReCOR compared with baselines,
sometimes outperforming oracle models supervised with the ground-truth order.

### 46. Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Xinhe Li, Jiajun Liu, Peng Wang
- **URL**: <http://arxiv.org/abs/2508.13037v1>
- **Submitted**: 2025-08-18 15:56:10
- **Comment**: Accepted by IJCAI2025
- **Topic Keywords**: rag
- **Reason**: The paper is not relevant to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on mathematical reasoning and distillation methods, which are outside the user's primary areas of interest.

#### Abstract
> Recent studies have demonstrated that Large Language Models (LLMs) have
strong mathematical reasoning abilities but rely on hundreds of billions of
parameters. To tackle the challenge of poor reasoning in Small Language Models
(SLMs), existing methods typically leverage LLMs to generate massive amounts of
data for cramming training. In psychology, they are akin to System 1 thinking,
which resolves reasoning problems rapidly based on experience and intuition.
However, human learning also requires System 2 thinking, where knowledge is
first acquired and then reinforced through practice. Inspired by such two
distinct modes of thinking, we propose a novel method based on the multi-LoRA
Interaction for mathematical reasoning Distillation (LoRID). First, we input
the question and reasoning of each sample into an LLM to create
knowledge-enhanced datasets. Subsequently, we train a LoRA block on the student
model as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts
for problem-solving. Then, to imitate System 2 thinking, we train the Knowledge
Generator (KG) and Deep Reasoner (DR), respectively. The former outputs only
knowledge after receiving problems, while the latter uses that knowledge to
perform reasoning. Finally, to address the randomness in the generation of IR
and DR, we evaluate whether their outputs are consistent, and the inference
process needs to be iterated if not. This step can enhance the mathematical
reasoning ability of SLMs through mutual feedback. Experimental results show
that LoRID achieves state-of-the-art performance, especially on the GSM8K
dataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,
12.3%, and 1.8% accuracy across the five base models, respectively.

### 47. Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zhu Li, Yuqing Zhang, Xiyuan Gao, Devraj Raghuvanshi, Nagendra Kumar, Shekhar Nayak, Matt Coler
- **URL**: <http://arxiv.org/abs/2508.13028v1>
- **Submitted**: 2025-08-18 15:44:54
- **Comment**: Speech Synthesis Workshop 2025
- **Topic Keywords**: rag
- **Reason**: The paper focuses on speech synthesis and sarcasm detection, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions bi-modal sarcasm detection, the context is not relevant to the user's core research themes.

#### Abstract
> Sarcastic speech synthesis, which involves generating speech that effectively
conveys sarcasm, is essential for enhancing natural interactions in
applications such as entertainment and human-computer interaction. However,
synthesizing sarcastic speech remains a challenge due to the nuanced prosody
that characterizes sarcasm, as well as the limited availability of annotated
sarcastic speech data. To address these challenges, this study introduces a
novel approach that integrates feedback loss from a bi-modal sarcasm detection
model into the TTS training process, enhancing the model's ability to capture
and convey sarcasm. In addition, by leveraging transfer learning, a speech
synthesis model pre-trained on read speech undergoes a two-stage fine-tuning
process. First, it is fine-tuned on a diverse dataset encompassing various
speech styles, including sarcastic speech. In the second stage, the model is
further refined using a dataset focused specifically on sarcastic speech,
enhancing its ability to generate sarcasm-aware speech. Objective and
subjective evaluations demonstrate that our proposed methods improve the
quality, naturalness, and sarcasm-awareness of synthesized speech.

### 48. PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Pengcheng Huang, Shuhao Liu, Zhenghao Liu, Yukun Yan, Shuo Wang, Zulong Chen, Tong Xiao
- **URL**: <http://arxiv.org/abs/2508.13021v1>
- **Submitted**: 2025-08-18 15:38:37
- **Comment**: 17 pages,13 figures
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests as it focuses on masked diffusion models and sequence generation, which is outside the scope of Information Retrieval, Search technologies, and Natural Language Processing. The paper's abstract does not mention query understanding, ranking models, or user behavior modeling, which are key areas of interest for you.

#### Abstract
> Recent advances in masked diffusion models (MDMs) have established them as
powerful non-autoregressive alternatives for sequence generation. Nevertheless,
our preliminary experiments reveal that the generation quality of MDMs is still
highly sensitive to the choice of decoding strategy. In particular, widely
adopted uncertainty-based samplers suffer from two key limitations: a lack of
global trajectory control and a pronounced bias toward trivial tokens in the
early stages of decoding. These shortcomings restrict the full potential of
MDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling
(PC-Sampler), a novel decoding strategy that unifies global trajectory planning
with content-aware informativeness maximization. PC-Sampler incorporates a
position-aware weighting mechanism to regulate the decoding path and a
calibrated confidence score to suppress the premature selection of trivial
tokens. Extensive experiments on three advanced MDMs across seven challenging
benchmarks-including logical reasoning and planning tasks-demonstrate that
PC-Sampler consistently outperforms existing MDM decoding strategies by more
than 10% on average, significantly narrowing the performance gap with
state-of-the-art autoregressive models. All codes are available at
https://github.com/NEUIR/PC-Sampler.

### 49. A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jinyi Han, Xinyi Wang, Haiquan Zhao, Tingyun li, Zishang Jiang, Sihang Jiang, Jiaqing Liang, Xin Lin, Weikang Zhou, Zeye Sun, Fei Yu, Yanghua Xiao
- **URL**: <http://arxiv.org/abs/2508.12903v1>
- **Submitted**: 2025-08-18 13:07:21
- **Topic Keywords**: rag
- **Reason**: The paper focuses on self-refinement for language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the idea of refining outputs, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest for your research.

#### Abstract
> Recent advances in self-refinement have demonstrated significant potential
for improving the outputs of large language models (LLMs) through iterative
refinement. However, most existing self-refinement methods rely on a reactive
process with a fixed number of iterations, making it difficult to determine the
optimal timing and content of refinement based on the evolving generation
context. Inspired by the way humans dynamically refine their thoughts during
execution, we propose ProActive Self-Refinement (PASR), a novel method that
enables LLMs to refine their outputs during the generation process. Unlike
methods that regenerate entire responses, PASR proactively decides whether,
when, and how to refine based on the model's internal state and evolving
context. We conduct extensive experiments on a diverse set of 10 tasks to
evaluate the effectiveness of PASR. Experimental results show that PASR
significantly enhances problem-solving performance. In particular, on Qwen3-8B,
PASR reduces average token consumption by 41.6 percent compared to standard
generation, while also achieving an 8.2 percent improvement in accuracy. Our
code and all baselines used in the paper are available in the GitHub.

### 50. ding-01 :ARG0: An AMR Corpus for Spontaneous French Dialogue

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jeongwoo Kang, Maria Boritchev, Maximin Coavoux
- **URL**: <http://arxiv.org/abs/2508.12819v1>
- **Submitted**: 2025-08-18 10:57:44
- **Comment**: Accepted at IWCS 2025
- **Topic Keywords**: rag
- **Reason**: This paper focuses on building a French semantic corpus using Abstract Meaning Representation (AMR) for spontaneous dialogue, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it involves natural language processing, the scope is limited to French dialogue and does not address ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest in your research.

#### Abstract
> We present our work to build a French semantic corpus by annotating French
dialogue in Abstract Meaning Representation (AMR). Specifically, we annotate
the DinG corpus, consisting of transcripts of spontaneous French dialogues
recorded during the board game Catan. As AMR has insufficient coverage of the
dynamics of spontaneous speech, we extend the framework to better represent
spontaneous speech and sentence structures specific to French. Additionally, to
support consistent annotation, we provide an annotation guideline detailing
these extensions. We publish our corpus under a free license (CC-SA-BY). We
also train and evaluate an AMR parser on our data. This model can be used as an
assistance annotation tool to provide initial annotations that can be refined
by human annotators. Our work contributes to the development of semantic
resources for French dialogue.

---

