[
    {
        "title": "Test-Time Scaling with Reflective Generative Model",
        "abstract": "We introduce our first reflective generative model MetaStone-S1, which\nobtains OpenAI o3's performance via the self-supervised process reward model\n(SPRM). Through sharing the backbone network and using task-specific heads for\nnext token prediction and process scoring respectively, SPRM successfully\nintegrates the policy model and process reward model(PRM) into a unified\ninterface without extra process annotation, reducing over 99% PRM parameters\nfor efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable\nfor test time scaling (TTS), and we provide three reasoning effort modes (low,\nmedium, and high), based on the controllable thinking length. Moreover, we\nempirically establish a scaling law that reveals the relationship between total\nthinking computation and TTS performance. Experiments demonstrate that our\nMetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with\nonly 32B parameter size. To support the research community, we have\nopen-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.",
        "url": "http://arxiv.org/abs/2507.01951v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01951v1",
        "arxiv_id": "2507.01951v1",
        "authors": [
            "Zixiao Wang",
            "Yuxin Wang",
            "Xiaorui Wang",
            "Mengting Xing",
            "Jie Gao",
            "Jianjun Xu",
            "Guangcan Liu",
            "Chenhui Jin",
            "Zhuo Wang",
            "Shengzhuo Zhang",
            "Hongtao Xie"
        ],
        "submitted": "2025-07-02 17:58:01",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Thin Line Between Comprehension and Persuasion in LLMs",
        "abstract": "Large language models (LLMs) are excellent at maintaining high-level,\nconvincing dialogues. They are being fast deployed as chatbots and evaluators\nin sensitive areas, such as peer review and mental health applications. This,\nalong with the disparate accounts on their reasoning capabilities, calls for a\ncloser examination of LLMs and their comprehension of dialogue. In this work we\nbegin by evaluating LLMs' ability to maintain a debate--one of the purest yet\nmost complex forms of human communication. Then we measure how this capability\nrelates to their understanding of what is being talked about, namely, their\ncomprehension of dialogical structures and the pragmatic context. We find that\nLLMs are capable of maintaining coherent, persuasive debates, often swaying the\nbeliefs of participants and audiences alike. We also note that awareness or\nsuspicion of AI involvement encourage people to be more critical of the\narguments made. When polling LLMs on their comprehension of deeper structures\nof dialogue, however, they cannot demonstrate said understanding. Our findings\ntie the shortcomings of LLMs-as-evaluators to their (in)ability to understand\nthe context. More broadly, for the field of argumentation theory we posit that,\nif an agent can convincingly maintain a dialogue, it is not necessary for it to\nknow what it is talking about. Hence, the modelling of pragmatic context and\ncoherence are secondary to effectiveness.",
        "url": "http://arxiv.org/abs/2507.01936v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01936v1",
        "arxiv_id": "2507.01936v1",
        "authors": [
            "Adrian de Wynter",
            "Tangming Yuan"
        ],
        "submitted": "2025-07-02 17:46:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Adaptability of ASR Models on Low-Resource Language: A Comparative Study of Whisper and Wav2Vec-BERT on Bangla",
        "abstract": "In recent years, neural models trained on large multilingual text and speech\ndatasets have shown great potential for supporting low-resource languages. This\nstudy investigates the performances of two state-of-the-art Automatic Speech\nRecognition (ASR) models, OpenAI's Whisper (Small & Large-V2) and Facebook's\nWav2Vec-BERT on Bangla, a low-resource language. We have conducted experiments\nusing two publicly available datasets: Mozilla Common Voice-17 and OpenSLR to\nevaluate model performances. Through systematic fine-tuning and hyperparameter\noptimization, including learning rate, epochs, and model checkpoint selection,\nwe have compared the models based on Word Error Rate (WER), Character Error\nRate (CER), Training Time, and Computational Efficiency. The Wav2Vec-BERT model\noutperformed Whisper across all key evaluation metrics, demonstrated superior\nperformance while requiring fewer computational resources, and offered valuable\ninsights to develop robust speech recognition systems in low-resource\nlinguistic settings.",
        "url": "http://arxiv.org/abs/2507.01931v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01931v1",
        "arxiv_id": "2507.01931v1",
        "authors": [
            "Md Sazzadul Islam Ridoy",
            "Sumi Akter",
            "Md. Aminur Rahman"
        ],
        "submitted": "2025-07-02 17:44:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Decision-Oriented Text Evaluation",
        "abstract": "Natural language generation (NLG) is increasingly deployed in high-stakes\ndomains, yet common intrinsic evaluation methods, such as n-gram overlap or\nsentence plausibility, weakly correlate with actual decision-making efficacy.\nWe propose a decision-oriented framework for evaluating generated text by\ndirectly measuring its influence on human and large language model (LLM)\ndecision outcomes. Using market digest texts--including objective morning\nsummaries and subjective closing-bell analyses--as test cases, we assess\ndecision quality based on the financial performance of trades executed by human\ninvestors and autonomous LLM agents informed exclusively by these texts. Our\nfindings reveal that neither humans nor LLM agents consistently surpass random\nperformance when relying solely on summaries. However, richer analytical\ncommentaries enable collaborative human-LLM teams to outperform individual\nhuman or agent baselines significantly. Our approach underscores the importance\nof evaluating generated text by its ability to facilitate synergistic\ndecision-making between humans and LLMs, highlighting critical limitations of\ntraditional intrinsic metrics.",
        "url": "http://arxiv.org/abs/2507.01923v2",
        "pdf_url": "http://arxiv.org/pdf/2507.01923v2",
        "arxiv_id": "2507.01923v2",
        "authors": [
            "Yu-Shiang Huang",
            "Chuan-Ju Wang",
            "Chung-Chi Chen"
        ],
        "submitted": "2025-07-02 17:32:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "NaturalThoughts: Selecting and Distilling Reasoning Traces for General Reasoning Tasks",
        "abstract": "Recent work has shown that distilling reasoning traces from a larger teacher\nmodel via supervised finetuning outperforms reinforcement learning with the\nsmaller student model alone (Guo et al. 2025). However, there has not been a\nsystematic study of what kind of reasoning demonstrations from the teacher are\nmost effective in improving the student model's reasoning capabilities. In this\nwork we curate high-quality \"NaturalThoughts\" by selecting reasoning traces\nfrom a strong teacher model based on a large pool of questions from\nNaturalReasoning (Yuan et al. 2025). We first conduct a systematic analysis of\nfactors that affect distilling reasoning capabilities, in terms of sample\nefficiency and scalability for general reasoning tasks. We observe that simply\nscaling up data size with random sampling is a strong baseline with steady\nperformance gains. Further, we find that selecting difficult examples that\nrequire more diverse reasoning strategies is more sample-efficient to transfer\nthe teacher model's reasoning skills. Evaluated on both Llama and Qwen models,\ntraining with NaturalThoughts outperforms existing reasoning datasets such as\nOpenThoughts, LIMO, etc. on general STEM reasoning benchmarks including\nGPQA-Diamond, MMLU-Pro and SuperGPQA.",
        "url": "http://arxiv.org/abs/2507.01921v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01921v1",
        "arxiv_id": "2507.01921v1",
        "authors": [
            "Yang Li",
            "Youssef Emad",
            "Karthik Padthe",
            "Jack Lanchantin",
            "Weizhe Yuan",
            "Thao Nguyen",
            "Jason Weston",
            "Shang-Wen Li",
            "Dong Wang",
            "Ilia Kulikov",
            "Xian Li"
        ],
        "submitted": "2025-07-02 17:30:24",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Gradient-Adaptive Policy Optimization: Towards Multi-Objective Alignment of Large Language Models",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a powerful\ntechnique for aligning large language models (LLMs) with human preferences.\nHowever, effectively aligning LLMs with diverse human preferences remains a\nsignificant challenge, particularly when they are conflict. To address this\nissue, we frame human value alignment as a multi-objective optimization\nproblem, aiming to maximize a set of potentially conflicting objectives. We\nintroduce Gradient-Adaptive Policy Optimization (GAPO), a novel fine-tuning\nparadigm that employs multiple-gradient descent to align LLMs with diverse\npreference distributions. GAPO adaptively rescales the gradients for each\nobjective to determine an update direction that optimally balances the\ntrade-offs between objectives. Additionally, we introduce P-GAPO, which\nincorporates user preferences across different objectives and achieves Pareto\nsolutions that better align with the user's specific needs. Our theoretical\nanalysis demonstrates that GAPO converges towards a Pareto optimal solution for\nmultiple objectives. Empirical results on Mistral-7B show that GAPO outperforms\ncurrent state-of-the-art methods, achieving superior performance in both\nhelpfulness and harmlessness.",
        "url": "http://arxiv.org/abs/2507.01915v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01915v1",
        "arxiv_id": "2507.01915v1",
        "authors": [
            "Chengao Li",
            "Hanyu Zhang",
            "Yunkun Xu",
            "Hongyan Xue",
            "Xiang Ao",
            "Qing He"
        ],
        "submitted": "2025-07-02 17:25:26",
        "source": "arxiv",
        "comment": "19 pages, 3 figures. Accepted by ACL 2025 (main)"
    },
    {
        "title": "AI4Research: A Survey of Artificial Intelligence for Scientific Research",
        "abstract": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\nremarkable capabilities in complex domains such as logical reasoning and\nexperimental coding. Motivated by these advancements, numerous studies have\nexplored the application of AI in the innovation process, particularly in the\ncontext of scientific research. These AI technologies primarily aim to develop\nsystems that can autonomously conduct research processes across a wide range of\nscientific disciplines. Despite these significant strides, a comprehensive\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\nunderstanding and impedes further development in this field. To address this\ngap, we present a comprehensive survey and offer a unified perspective on\nAI4Research. Specifically, the main contributions of our work are as follows:\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\nresearch gaps and highlight promising future directions, focusing on the rigor\nand scalability of automated experiments, as well as the societal impact. (3)\nAbundant applications and resources: Finally, we compile a wealth of resources,\nincluding relevant multidisciplinary applications, data corpora, and tools. We\nhope our work will provide the research community with quick access to these\nresources and stimulate innovative breakthroughs in AI4Research.",
        "url": "http://arxiv.org/abs/2507.01903v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01903v1",
        "arxiv_id": "2507.01903v1",
        "authors": [
            "Qiguang Chen",
            "Mingda Yang",
            "Libo Qin",
            "Jinhao Liu",
            "Zheng Yan",
            "Jiannan Guan",
            "Dengyun Peng",
            "Yiyan Ji",
            "Hanjing Li",
            "Mengkang Hu",
            "Yimeng Zhang",
            "Yihao Liang",
            "Yuhang Zhou",
            "Jiaqi Wang",
            "Zhi Chen",
            "Wanxiang Che"
        ],
        "submitted": "2025-07-02 17:19:20",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "High-Layer Attention Pruning with Rescaling",
        "abstract": "Pruning is a highly effective approach for compressing large language models\n(LLMs), significantly reducing inference latency. However, conventional\ntraining-free structured pruning methods often employ a heuristic metric that\nindiscriminately removes some attention heads across all pruning layers,\nwithout considering their positions within the network architecture. In this\nwork, we propose a novel pruning algorithm that strategically prunes attention\nheads in the model's higher layers. Since the removal of attention heads can\nalter the magnitude of token representations, we introduce an adaptive\nrescaling parameter that calibrates the representation scale post-pruning to\ncounteract this effect. We conduct comprehensive experiments on a wide range of\nLLMs, including LLaMA3.1-8B, Mistral-7B-v0.3, Qwen2-7B, and Gemma2-9B. Our\nevaluation includes both generation and discriminative tasks across 27\ndatasets. The results consistently demonstrate that our method outperforms\nexisting structured pruning methods. This improvement is particularly notable\nin generation tasks, where our approach significantly outperforms existing\nbaselines.",
        "url": "http://arxiv.org/abs/2507.01900v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01900v1",
        "arxiv_id": "2507.01900v1",
        "authors": [
            "Songtao Liu",
            "Peng Liu"
        ],
        "submitted": "2025-07-02 17:15:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants",
        "abstract": "Large language models (LLMs) excel at reasoning tasks requiring long thought\nsequences for planning, reflection, and refinement. However, their substantial\nmodel size and high computational demands are impractical for widespread\ndeployment. Yet, small language models (SLMs) often struggle to learn long-form\nCoT reasoning due to their limited capacity, a phenomenon we refer to as the\n\"SLMs Learnability Gap\". To address this, we introduce\n\\textbf{Mi}d-\\textbf{Co}T \\textbf{T}eacher \\textbf{A}ssistant Distillation\n(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA\nemploys intermediate-sized models as teacher assistants and utilizes\nintermediate-length CoT sequences to bridge both the capacity and reasoning\nlength gaps. Our experiments on downstream tasks demonstrate that although SLMs\ndistilled from large teachers can perform poorly, by applying MiCoTA, they\nachieve significant improvements in reasoning performance. Specifically,\nQwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and\n3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and\nGSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform\na quantitative experiment demonstrating that our method produces data more\nclosely aligned with base SLM distributions. Our insights pave the way for\nfuture research into long-CoT data distillation for SLMs.",
        "url": "http://arxiv.org/abs/2507.01887v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01887v1",
        "arxiv_id": "2507.01887v1",
        "authors": [
            "Dongyi Ding",
            "Tiannan Wang",
            "Chenghao Zhu",
            "Meiling Tao",
            "Yuchen Eleanor Jiang",
            "Wangchunshu Zhou"
        ],
        "submitted": "2025-07-02 16:57:01",
        "source": "arxiv",
        "comment": "Work in progress"
    },
    {
        "title": "DIY-MKG: An LLM-Based Polyglot Language Learning System",
        "abstract": "Existing language learning tools, even those powered by Large Language Models\n(LLMs), often lack support for polyglot learners to build linguistic\nconnections across vocabularies in multiple languages, provide limited\ncustomization for individual learning paces or needs, and suffer from\ndetrimental cognitive offloading. To address these limitations, we design\nDo-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system\nthat supports polyglot language learning. DIY-MKG allows the user to build\npersonalized vocabulary knowledge graphs, which are constructed by selective\nexpansion with related words suggested by an LLM. The system further enhances\nlearning through rich annotation capabilities and an adaptive review module\nthat leverages LLMs for dynamic, personalized quiz generation. In addition,\nDIY-MKG allows users to flag incorrect quiz questions, simultaneously\nincreasing user engagement and providing a feedback loop for prompt refinement.\nOur evaluation of LLM-based components in DIY-MKG shows that vocabulary\nexpansion is reliable and fair across multiple languages, and that the\ngenerated quizzes are highly accurate, validating the robustness of DIY-MKG.",
        "url": "http://arxiv.org/abs/2507.01872v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01872v1",
        "arxiv_id": "2507.01872v1",
        "authors": [
            "Kenan Tang",
            "Yanhong Li",
            "Yao Qin"
        ],
        "submitted": "2025-07-02 16:38:51",
        "source": "arxiv",
        "comment": "Submitted to EMNLP 2025 System Demonstration"
    },
    {
        "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.",
        "url": "http://arxiv.org/abs/2507.01853v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01853v1",
        "arxiv_id": "2507.01853v1",
        "authors": [
            "Samridhi Raj Sinha",
            "Rajvee Sheth",
            "Abhishek Upperwal",
            "Mayank Singh"
        ],
        "submitted": "2025-07-02 16:07:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Low-Perplexity LLM-Generated Sequences and Where To Find Them",
        "abstract": "As Large Language Models (LLMs) become increasingly widespread, understanding\nhow specific training data shapes their outputs is crucial for transparency,\naccountability, privacy, and fairness. To explore how LLMs leverage and\nreplicate their training data, we introduce a systematic approach centered on\nanalyzing low-perplexity sequences - high-probability text spans generated by\nthe model. Our pipeline reliably extracts such long sequences across diverse\ntopics while avoiding degeneration, then traces them back to their sources in\nthe training data. Surprisingly, we find that a substantial portion of these\nlow-perplexity spans cannot be mapped to the corpus. For those that do match,\nwe quantify the distribution of occurrences across source documents,\nhighlighting the scope and nature of verbatim recall and paving a way toward\nbetter understanding of how LLMs training data impacts their behavior.",
        "url": "http://arxiv.org/abs/2507.01844v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01844v1",
        "arxiv_id": "2507.01844v1",
        "authors": [
            "Arthur Wuhrmann",
            "Anastasiia Kucherenko",
            "Andrei Kucharavy"
        ],
        "submitted": "2025-07-02 15:58:51",
        "source": "arxiv",
        "comment": "Camera-ready version. Accepted to ACL 2025. 10 pages, 4 figures, 6\n  tables"
    },
    {
        "title": "Evaluating Structured Output Robustness of Small Language Models for Open Attribute-Value Extraction from Clinical Notes",
        "abstract": "We present a comparative analysis of the parseability of structured outputs\ngenerated by small language models for open attribute-value extraction from\nclinical notes. We evaluate three widely used serialization formats: JSON,\nYAML, and XML, and find that JSON consistently yields the highest parseability.\nStructural robustness improves with targeted prompting and larger models, but\ndeclines for longer documents and certain note types. Our error analysis\nidentifies recurring format-specific failure patterns. These findings offer\npractical guidance for selecting serialization formats and designing prompts\nwhen deploying language models in privacy-sensitive clinical settings.",
        "url": "http://arxiv.org/abs/2507.01810v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01810v1",
        "arxiv_id": "2507.01810v1",
        "authors": [
            "Nikita Neveditsin",
            "Pawan Lingras",
            "Vijay Mago"
        ],
        "submitted": "2025-07-02 15:27:41",
        "source": "arxiv",
        "comment": "To appear in the ACL Anthology"
    },
    {
        "title": "LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs",
        "abstract": "Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\nModels (LLMs) by enabling parameter-efficient updates. However, their\nwidespread adoption remains limited by the reliance on GPU-based training. In\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\ndesigned specifically for users with limited computational resources,\nparticularly those restricted to standard laptop CPUs. Our method learns a\nmeta-operator that maps any input dataset, represented as a probability\ndistribution, to a set of LoRA weights by leveraging a large bank of\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\nperforming new gradient-based updates, our pipeline constructs adapters via\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\nadapters do not match the performance of GPU-trained counterparts, they\nconsistently outperform the base Mistral model on downstream tasks, offering a\npractical and accessible alternative to traditional GPU-based fine-tuning.",
        "url": "http://arxiv.org/abs/2507.01806v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01806v1",
        "arxiv_id": "2507.01806v1",
        "authors": [
            "Reza Arabpour",
            "Haitz Sáez de Ocáriz Borde",
            "Anastasis Kratsios"
        ],
        "submitted": "2025-07-02 15:24:47",
        "source": "arxiv",
        "comment": "5-page main paper (excluding references) + 11-page appendix, 3\n  tables, 1 figure. Accepted to ICML 2025 Workshop on Efficient Systems for\n  Foundation Models"
    },
    {
        "title": "The Anatomy of Evidence: An Investigation Into Explainable ICD Coding",
        "abstract": "Automatic medical coding has the potential to ease documentation and billing\nprocesses. For this task, transparency plays an important role for medical\ncoders and regulatory bodies, which can be achieved using explainability\nmethods. However, the evaluation of these approaches has been mostly limited to\nshort text and binary settings due to a scarcity of annotated data. Recent\nefforts by Cheng et al. (2023) have introduced the MDACE dataset, which\nprovides a valuable resource containing code evidence in clinical records. In\nthis work, we conduct an in-depth analysis of the MDACE dataset and perform\nplausibility evaluation of current explainable medical coding systems from an\napplied perspective. With this, we contribute to a deeper understanding of\nautomatic medical coding and evidence extraction. Our findings reveal that\nground truth evidence aligns with code descriptions to a certain degree. An\ninvestigation into state-of-the-art approaches shows a high overlap with ground\ntruth evidence. We propose match measures and highlight success and failure\ncases. Based on our findings, we provide recommendations for developing and\nevaluating explainable medical coding systems.",
        "url": "http://arxiv.org/abs/2507.01802v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01802v1",
        "arxiv_id": "2507.01802v1",
        "authors": [
            "Katharina Beckh",
            "Elisa Studeny",
            "Sujan Sai Gannamaneni",
            "Dario Antweiler",
            "Stefan Rüping"
        ],
        "submitted": "2025-07-02 15:21:29",
        "source": "arxiv",
        "comment": "Accepted to ACL 2025 Findings"
    },
    {
        "title": "How Do Vision-Language Models Process Conflicting Information Across Modalities?",
        "abstract": "AI models are increasingly required to be multimodal, integrating disparate\ninput streams into a coherent state representation on which subsequent\nbehaviors and actions can be based. This paper seeks to understand how such\nmodels behave when input streams present conflicting information. Focusing\nspecifically on vision-language models, we provide inconsistent inputs (e.g.,\nan image of a dog paired with the caption \"A photo of a cat\") and ask the model\nto report the information present in one of the specific modalities (e.g.,\n\"What does the caption say / What is in the image?\"). We find that models often\nfavor one modality over the other, e.g., reporting the image regardless of what\nthe caption says, but that different models differ in which modality they\nfavor. We find evidence that the behaviorally preferred modality is evident in\nthe internal representational structure of the model, and that specific\nattention heads can restructure the representations to favor one modality over\nthe other. Moreover, we find modality-agnostic \"router heads\" which appear to\npromote answers about the modality requested in the instruction, and which can\nbe manipulated or transferred in order to improve performance across datasets\nand modalities. Together, the work provides essential steps towards identifying\nand controlling if and how models detect and resolve conflicting signals within\ncomplex multimodal environments.",
        "url": "http://arxiv.org/abs/2507.01790v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01790v1",
        "arxiv_id": "2507.01790v1",
        "authors": [
            "Tianze Hua",
            "Tian Yun",
            "Ellie Pavlick"
        ],
        "submitted": "2025-07-02 15:15:14",
        "source": "arxiv",
        "comment": "All code and resources are available at:\n  https://github.com/ethahtz/vlm_conflicting_info_processing"
    },
    {
        "title": "Probing Evaluation Awareness of Language Models",
        "abstract": "Language models can distinguish between testing and deployment phases -- a\ncapability known as evaluation awareness. This has significant safety and\npolicy implications, potentially undermining the reliability of evaluations\nthat are central to AI governance frameworks and voluntary industry\ncommitments. In this paper, we study evaluation awareness in\nLlama-3.3-70B-Instruct. We show that linear probes can separate real-world\nevaluation and deployment prompts, suggesting that current models internally\nrepresent this distinction. We also find that current safety evaluations are\ncorrectly classified by the probes, suggesting that they already appear\nartificial or inauthentic to models. Our findings underscore the importance of\nensuring trustworthy evaluations and understanding deceptive capabilities. More\nbroadly, our work showcases how model internals may be leveraged to support\nblackbox methods in safety audits, especially for future models more competent\nat evaluation awareness and deception.",
        "url": "http://arxiv.org/abs/2507.01786v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01786v1",
        "arxiv_id": "2507.01786v1",
        "authors": [
            "Jord Nguyen",
            "Khiem Hoang",
            "Carlo Leonardo Attubato",
            "Felix Hofstätter"
        ],
        "submitted": "2025-07-02 15:12:43",
        "source": "arxiv",
        "comment": "Technical AI Governance Workshop, ICML (Poster)"
    },
    {
        "title": "MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining",
        "abstract": "Data quality is a critical driver of large language model performance, yet\nexisting model-based selection methods focus almost exclusively on English. We\nintroduce MuRating, a scalable framework that transfers high-quality English\ndata-quality signals into a single rater for 17 target languages. MuRating\naggregates multiple English \"raters\" via pairwise comparisons to learn unified\ndocument-quality scores,then projects these judgments through translation to\ntrain a multilingual evaluator on monolingual, cross-lingual, and parallel text\npairs. Applied to web data, MuRating selects balanced subsets of English and\nmultilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to\nstrong baselines, including QuRater, AskLLM, DCLM and so on, our approach\nboosts average accuracy on both English benchmarks and multilingual\nevaluations, with especially large gains on knowledge-intensive tasks. We\nfurther analyze translation fidelity, selection biases, and underrepresentation\nof narrative material, outlining directions for future work.",
        "url": "http://arxiv.org/abs/2507.01785v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01785v1",
        "arxiv_id": "2507.01785v1",
        "authors": [
            "Zhixun Chen",
            "Ping Guo",
            "Wenhan Han",
            "Yifan Zhang",
            "Binbin Liu",
            "Haobin Lin",
            "Fengze Liu",
            "Yan Zhao",
            "Bingni Zhang",
            "Taifeng Wang",
            "Yin Zheng",
            "Meng Fang"
        ],
        "submitted": "2025-07-02 15:11:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results",
        "abstract": "Tokenisation - \"the process of splitting text into atomic parts\" (Brezina &\nTimperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides\nthe basis for any applicable quantitative method (e.g. collocations) while\nensuring the reliability of qualitative approaches. This paper examines how\ndiscrepancies in tokenisation affect the representation of language data and\nthe validity of analytical findings: investigating the challenges posed by\nemojis and homoglyphs, the study highlights the necessity of preprocessing\nthese elements to maintain corpus fidelity to the source data. The research\npresents methods for ensuring that digital texts are accurately represented in\ncorpora, thereby supporting reliable linguistic analysis and guaranteeing the\nrepeatability of linguistic interpretations. The findings emphasise the\nnecessity of a detailed understanding of both linguistic and technical aspects\ninvolved in digital textual data to enhance the accuracy of corpus analysis,\nand have significant implications for both quantitative and qualitative\napproaches in corpus-based research.",
        "url": "http://arxiv.org/abs/2507.01764v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01764v1",
        "arxiv_id": "2507.01764v1",
        "authors": [
            "Matteo Di Cristofaro"
        ],
        "submitted": "2025-07-02 14:46:26",
        "source": "arxiv",
        "comment": "Author submitted manuscript"
    },
    {
        "title": "Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training",
        "abstract": "Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization.",
        "url": "http://arxiv.org/abs/2507.01752v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01752v1",
        "arxiv_id": "2507.01752v1",
        "authors": [
            "Ismail Labiad",
            "Mathurin Videau",
            "Matthieu Kowalski",
            "Marc Schoenauer",
            "Alessandro Leite",
            "Julia Kempe",
            "Olivier Teytaud"
        ],
        "submitted": "2025-07-02 14:29:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving",
        "abstract": "In this paper, we present details of the 1st W-CODA workshop, held in\nconjunction with the ECCV 2024. W-CODA aims to explore next-generation\nsolutions for autonomous driving corner cases, empowered by state-of-the-art\nmultimodal perception and comprehension techniques. 5 Speakers from both\nacademia and industry are invited to share their latest progress and opinions.\nWe collect research papers and hold a dual-track challenge, including both\ncorner case scene understanding and generation. As the pioneering effort, we\nwill continuously bridge the gap between frontier autonomous driving techniques\nand fully intelligent, reliable self-driving agents robust towards corner\ncases.",
        "url": "http://arxiv.org/abs/2507.01735v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01735v1",
        "arxiv_id": "2507.01735v1",
        "authors": [
            "Kai Chen",
            "Ruiyuan Gao",
            "Lanqing Hong",
            "Hang Xu",
            "Xu Jia",
            "Holger Caesar",
            "Dengxin Dai",
            "Bingbing Liu",
            "Dzmitry Tsishkou",
            "Songcen Xu",
            "Chunjing Xu",
            "Qiang Xu",
            "Huchuan Lu",
            "Dit-Yan Yeung"
        ],
        "submitted": "2025-07-02 14:10:25",
        "source": "arxiv",
        "comment": "ECCV 2024. Workshop page: https://coda-dataset.github.io/w-coda2024/"
    },
    {
        "title": "LLMs for Legal Subsumption in German Employment Contracts",
        "abstract": "Legal work, characterized by its text-heavy and resource-intensive nature,\npresents unique challenges and opportunities for NLP research. While\ndata-driven approaches have advanced the field, their lack of interpretability\nand trustworthiness limits their applicability in dynamic legal environments.\nTo address these issues, we collaborated with legal experts to extend an\nexisting dataset and explored the use of Large Language Models (LLMs) and\nin-context learning to evaluate the legality of clauses in German employment\ncontracts. Our work evaluates the ability of different LLMs to classify clauses\nas \"valid,\" \"unfair,\" or \"void\" under three legal context variants: no legal\ncontext, full-text sources of laws and court rulings, and distilled versions of\nthese (referred to as examination guidelines). Results show that full-text\nsources moderately improve performance, while examination guidelines\nsignificantly enhance recall for void clauses and weighted F1-Score, reaching\n80\\%. Despite these advancements, LLMs' performance when using full-text\nsources remains substantially below that of human lawyers. We contribute an\nextended dataset, including examination guidelines, referenced legal sources,\nand corresponding annotations, alongside our code and all log files. Our\nfindings highlight the potential of LLMs to assist lawyers in contract legality\nreview while also underscoring the limitations of the methods presented.",
        "url": "http://arxiv.org/abs/2507.01734v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01734v1",
        "arxiv_id": "2507.01734v1",
        "authors": [
            "Oliver Wardas",
            "Florian Matthes"
        ],
        "submitted": "2025-07-02 14:07:54",
        "source": "arxiv",
        "comment": "PrePrint - ICAIL25, Chicago"
    },
    {
        "title": "Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI",
        "abstract": "Patents contain rich technical knowledge that can inspire innovative product\nideas, yet accessing and interpreting this information remains a challenge.\nThis work explores the use of Large Language Models (LLMs) and autonomous\nagents to mine and generate product concepts from a given patent. In this work,\nwe design Agent Ideate, a framework for automatically generating product-based\nbusiness ideas from patents. We experimented with open-source LLMs and\nagent-based architectures across three domains: Computer Science, Natural\nLanguage Processing, and Material Chemistry. Evaluation results show that the\nagentic approach consistently outperformed standalone LLMs in terms of idea\nquality, relevance, and novelty. These findings suggest that combining LLMs\nwith agentic workflows can significantly enhance the innovation pipeline by\nunlocking the untapped potential of business idea generation from patent data.",
        "url": "http://arxiv.org/abs/2507.01717v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01717v1",
        "arxiv_id": "2507.01717v1",
        "authors": [
            "Gopichand Kanumolu",
            "Ashok Urlana",
            "Charaka Vinayak Kumar",
            "Bala Mallikarjunarao Garlapati"
        ],
        "submitted": "2025-07-02 13:47:17",
        "source": "arxiv",
        "comment": "AgentScen Workshop, IJCAI 2025"
    },
    {
        "title": "Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach",
        "abstract": "Bias and stereotypes in language models can cause harm, especially in\nsensitive areas like content moderation and decision-making. This paper\naddresses bias and stereotype detection by exploring how jointly learning these\ntasks enhances model performance. We introduce StereoBias, a unique dataset\nlabeled for bias and stereotype detection across five categories: religion,\ngender, socio-economic status, race, profession, and others, enabling a deeper\nstudy of their relationship. Our experiments compare encoder-only models and\nfine-tuned decoder-only models using QLoRA. While encoder-only models perform\nwell, decoder-only models also show competitive results. Crucially, joint\ntraining on bias and stereotype detection significantly improves bias detection\ncompared to training them separately. Additional experiments with sentiment\nanalysis confirm that the improvements stem from the connection between bias\nand stereotypes, not multi-task learning alone. These findings highlight the\nvalue of leveraging stereotype information to build fairer and more effective\nAI systems.",
        "url": "http://arxiv.org/abs/2507.01715v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01715v1",
        "arxiv_id": "2507.01715v1",
        "authors": [
            "Aditya Tomar",
            "Rudra Murthy",
            "Pushpak Bhattacharyya"
        ],
        "submitted": "2025-07-02 13:46:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large Language Models on Harmfulness",
        "abstract": "The proliferation of multimodal memes in the social media era demands that\nmultimodal Large Language Models (mLLMs) effectively understand meme\nharmfulness. Existing benchmarks for assessing mLLMs on harmful meme\nunderstanding rely on accuracy-based, model-agnostic evaluations using static\ndatasets. These benchmarks are limited in their ability to provide up-to-date\nand thorough assessments, as online memes evolve dynamically. To address this,\nwe propose AdamMeme, a flexible, agent-based evaluation framework that\nadaptively probes the reasoning capabilities of mLLMs in deciphering meme\nharmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive\nevaluations by iteratively updating the meme data with challenging samples,\nthereby exposing specific limitations in how mLLMs interpret harmfulness.\nExtensive experiments show that our framework systematically reveals the\nvarying performance of different target mLLMs, offering in-depth, fine-grained\nanalyses of model-specific weaknesses. Our code is available at\nhttps://github.com/Lbotirx/AdamMeme.",
        "url": "http://arxiv.org/abs/2507.01702v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01702v1",
        "arxiv_id": "2507.01702v1",
        "authors": [
            "Zixin Chen",
            "Hongzhan Lin",
            "Kaixin Li",
            "Ziyang Luo",
            "Zhen Ye",
            "Guang Chen",
            "Zhiyong Huang",
            "Jing Ma"
        ],
        "submitted": "2025-07-02 13:32:30",
        "source": "arxiv",
        "comment": "ACL 2025"
    },
    {
        "title": "Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling",
        "abstract": "Existing post-training techniques for large language models are broadly\ncategorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning\n(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking\ndemonstration data but can lead to problematic generalization as a form of\nbehavior cloning. Conversely, RFT can significantly enhance a model's\nperformance but is prone to learn unexpected behaviors, and its performance is\nhighly sensitive to the initial policy. In this paper, we propose a unified\nview of these methods and introduce Prefix-RFT, a hybrid approach that\nsynergizes learning from both demonstration and exploration. Using mathematical\nreasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is\nboth simple and effective. It not only surpasses the performance of standalone\nSFT and RFT but also outperforms parallel mixed-policy RFT methods. A key\nadvantage is its seamless integration into existing open-source frameworks,\nrequiring only minimal modifications to the standard RFT pipeline. Our analysis\nhighlights the complementary nature of SFT and RFT, and validates that\nPrefix-RFT effectively harmonizes these two learning paradigms. Furthermore,\nablation studies confirm the method's robustness to variations in the quality\nand quantity of demonstration data. We hope this work offers a new perspective\non LLM post-training, suggesting that a unified paradigm that judiciously\nintegrates demonstration and exploration could be a promising direction for\nfuture research.",
        "url": "http://arxiv.org/abs/2507.01679v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01679v1",
        "arxiv_id": "2507.01679v1",
        "authors": [
            "Zeyu Huang",
            "Tianhao Cheng",
            "Zihan Qiu",
            "Zili Wang",
            "Yinghui Xu",
            "Edoardo M. Ponti",
            "Ivan Titov"
        ],
        "submitted": "2025-07-02 13:04:09",
        "source": "arxiv",
        "comment": "Work in progress"
    },
    {
        "title": "Deep Recommender Models Inference: Automatic Asymmetric Data Flow Optimization",
        "abstract": "Deep Recommender Models (DLRMs) inference is a fundamental AI workload\naccounting for more than 79% of the total AI workload in Meta's data centers.\nDLRMs' performance bottleneck is found in the embedding layers, which perform\nmany random memory accesses to retrieve small embedding vectors from tables of\nvarious sizes. We propose the design of tailored data flows to speedup\nembedding look-ups. Namely, we propose four strategies to look up an embedding\ntable effectively on one core, and a framework to automatically map the tables\nasymmetrically to the multiple cores of a SoC. We assess the effectiveness of\nour method using the Huawei Ascend AI accelerators, comparing it with the\ndefault Ascend compiler, and we perform high-level comparisons with Nvidia\nA100. Results show a speed-up varying from 1.5x up to 6.5x for real workload\ndistributions, and more than 20x for extremely unbalanced distributions.\nFurthermore, the method proves to be much more independent of the query\ndistribution than the baseline.",
        "url": "http://arxiv.org/abs/2507.01676v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01676v1",
        "arxiv_id": "2507.01676v1",
        "authors": [
            "Giuseppe Ruggeri",
            "Renzo Andri",
            "Daniele Jahier Pagliari",
            "Lukas Cavigelli"
        ],
        "submitted": "2025-07-02 13:00:39",
        "source": "arxiv",
        "comment": "5 pages, 4 figures, conference: IEEE ICCD24"
    },
    {
        "title": "Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings",
        "abstract": "In this paper, we investigate the transferability of pre-trained language\nmodels to low-resource Indonesian local languages through the task of sentiment\nanalysis. We evaluate both zero-shot performance and adapter-based transfer on\nten local languages using models of different types: a monolingual Indonesian\nBERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based\napproach called MAD-X. To better understand model behavior, we group the target\nlanguages into three categories: seen (included during pre-training), partially\nseen (not included but linguistically related to seen languages), and unseen\n(absent and unrelated in pre-training data). Our results reveal clear\nperformance disparities across these groups: multilingual models perform best\non seen languages, moderately on partially seen ones, and poorly on unseen\nlanguages. We find that MAD-X significantly improves performance, especially\nfor seen and partially seen languages, without requiring labeled data in the\ntarget language. Additionally, we conduct a further analysis on tokenization\nand show that while subword fragmentation and vocabulary overlap with\nIndonesian correlate weakly with prediction quality, they do not fully explain\nthe observed performance. Instead, the most consistent predictor of transfer\nsuccess is the model's prior exposure to the language, either directly or\nthrough a related language.",
        "url": "http://arxiv.org/abs/2507.01645v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01645v1",
        "arxiv_id": "2507.01645v1",
        "authors": [
            "Rifki Afina Putri"
        ],
        "submitted": "2025-07-02 12:17:55",
        "source": "arxiv",
        "comment": "AMLDS 2025"
    },
    {
        "title": "Confidence and Stability of Global and Pairwise Scores in NLP Evaluation",
        "abstract": "With the advent of highly capable instruction-tuned neural language models,\nbenchmarking in natural language processing (NLP) is increasingly shifting\ntowards pairwise comparison leaderboards, such as LMSYS Arena, from traditional\nglobal pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper\nempirically investigates the strengths and weaknesses of both global scores and\npairwise comparisons to aid decision-making in selecting appropriate model\nevaluation strategies. Through computational experiments on synthetic and\nreal-world datasets using standard global metrics and the popular Bradley-Terry\nmodel for pairwise comparisons, we found that while global scores provide more\nreliable overall rankings, they can underestimate strong models with rare,\nsignificant errors or low confidence. Conversely, pairwise comparisons are\nparticularly effective for identifying strong contenders among models with\nlower global scores, especially where quality metrics are hard to define (e.g.,\ntext generation), though they require more comparisons to converge if ties are\nfrequent. Our code and data are available at\nhttps://github.com/HSPyroblast/srw-ranking under a permissive license.",
        "url": "http://arxiv.org/abs/2507.01633v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01633v1",
        "arxiv_id": "2507.01633v1",
        "authors": [
            "Georgii Levtsov",
            "Dmitry Ustalov"
        ],
        "submitted": "2025-07-02 12:05:22",
        "source": "arxiv",
        "comment": "8 pages, accepted at ACL SRW 2025"
    },
    {
        "title": "Chart Question Answering from Real-World Analytical Narratives",
        "abstract": "We present a new dataset for chart question answering (CQA) constructed from\nvisualization notebooks. The dataset features real-world, multi-view charts\npaired with natural language questions grounded in analytical narratives.\nUnlike prior benchmarks, our data reflects ecologically valid reasoning\nworkflows. Benchmarking state-of-the-art multimodal large language models\nreveals a significant performance gap, with GPT-4.1 achieving an accuracy of\n69.3%, underscoring the challenges posed by this more authentic CQA setting.",
        "url": "http://arxiv.org/abs/2507.01627v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01627v1",
        "arxiv_id": "2507.01627v1",
        "authors": [
            "Maeve Hutchinson",
            "Radu Jianu",
            "Aidan Slingsby",
            "Jo Wood",
            "Pranava Madhyastha"
        ],
        "submitted": "2025-07-02 11:58:04",
        "source": "arxiv",
        "comment": "This paper has been accepted to the ACL Student Research Workshop\n  (SRW) 2025"
    },
    {
        "title": "Enhanced Influence-aware Group Recommendation for Online Media Propagation",
        "abstract": "Group recommendation over social media streams has attracted significant\nattention due to its wide applications in domains such as e-commerce,\nentertainment, and online news broadcasting. By leveraging social connections\nand group behaviours, group recommendation (GR) aims to provide more accurate\nand engaging content to a set of users rather than individuals. Recently,\ninfluence-aware GR has emerged as a promising direction, as it considers the\nimpact of social influence on group decision-making. In earlier work, we\nproposed Influence-aware Group Recommendation (IGR) to solve this task.\nHowever, this task remains challenging due to three key factors: the large and\never-growing scale of social graphs, the inherently dynamic nature of influence\npropagation within user groups, and the high computational overhead of\nreal-time group-item matching.\n  To tackle these issues, we propose an Enhanced Influence-aware Group\nRecommendation (EIGR) framework. First, we introduce a Graph Extraction-based\nSampling (GES) strategy to minimise redundancy across multiple temporal social\ngraphs and effectively capture the evolving dynamics of both groups and items.\nSecond, we design a novel DYnamic Independent Cascade (DYIC) model to predict\nhow influence propagates over time across social items and user groups.\nFinally, we develop a two-level hash-based User Group Index (UG-Index) to\nefficiently organise user groups and enable real-time recommendation\ngeneration. Extensive experiments on real-world datasets demonstrate that our\nproposed framework, EIGR, consistently outperforms state-of-the-art baselines\nin both effectiveness and efficiency.",
        "url": "http://arxiv.org/abs/2507.01616v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01616v1",
        "arxiv_id": "2507.01616v1",
        "authors": [
            "Chengkun He",
            "Xiangmin Zhou",
            "Chen Wang",
            "Longbing Cao",
            "Jie Shao",
            "Xiaodong Li",
            "Guang Xu",
            "Carrie Jinqiu Hu",
            "Zahir Tari"
        ],
        "submitted": "2025-07-02 11:34:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems",
        "abstract": "Traditional Data+AI systems utilize data-driven techniques to optimize\nperformance, but they rely heavily on human experts to orchestrate system\npipelines, enabling them to adapt to changes in data, queries, tasks, and\nenvironments. For instance, while there are numerous data science tools\navailable, developing a pipeline planning system to coordinate these tools\nremains challenging. This difficulty arises because existing Data+AI systems\nhave limited capabilities in semantic understanding, reasoning, and planning.\nFortunately, we have witnessed the success of large language models (LLMs) in\nenhancing semantic understanding, reasoning, and planning abilities. It is\ncrucial to incorporate LLM techniques to revolutionize data systems for\norchestrating Data+AI applications effectively.\n  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive\narchitecture designed to orchestrate Data+AI ecosystems, which focuses on\ntackling data-related tasks by integrating knowledge comprehension, reasoning,\nand planning capabilities. We delve into the challenges involved in designing\ndata agents, such as understanding data/queries/environments/tools,\norchestrating pipelines/workflows, optimizing and executing pipelines, and\nfostering pipeline self-reflection. Furthermore, we present examples of data\nagent systems, including a data science agent, data analytics agents (such as\nunstructured data analytics agent, semantic structured data analytics agent,\ndata lake analytics agent, and multi-modal data analytics agent), and a\ndatabase administrator (DBA) agent. We also outline several open challenges\nassociated with designing data agent systems.",
        "url": "http://arxiv.org/abs/2507.01599v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01599v1",
        "arxiv_id": "2507.01599v1",
        "authors": [
            "Zhaoyan Sun",
            "Jiayi Wang",
            "Xinyang Zhao",
            "Jiachi Wang",
            "Guoliang Li"
        ],
        "submitted": "2025-07-02 11:04:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning",
        "abstract": "Temporal Knowledge Graph (TKG) is an efficient method for describing the\ndynamic development of facts along a timeline. Most research on TKG reasoning\n(TKGR) focuses on modelling the repetition of global facts and designing\npatterns of local historical facts. However, they face two significant\nchallenges: inadequate modeling of the event distribution shift between\ntraining and test samples, and reliance on random entity substitution for\ngenerating negative samples, which often results in low-quality sampling. To\nthis end, we propose a novel distributional feature modeling approach for\ntraining TKGR models, Test-Time Training-guided Distribution shift Modelling\n(T3DM), to adjust the model based on distribution shift and ensure the global\nconsistency of model reasoning. In addition, we design a negative-sampling\nstrategy to generate higher-quality negative quadruples based on adversarial\ntraining. Extensive experiments show that T3DM provides better and more robust\nresults than the state-of-the-art baselines in most cases.",
        "url": "http://arxiv.org/abs/2507.01597v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01597v1",
        "arxiv_id": "2507.01597v1",
        "authors": [
            "Yuehang Si",
            "Zefan Zeng",
            "Jincai Huang",
            "Qing Cheng"
        ],
        "submitted": "2025-07-02 11:02:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Emotionally Intelligent Task-oriented Dialogue Systems: Architecture, Representation, and Optimisation",
        "abstract": "Task-oriented dialogue (ToD) systems are designed to help users achieve\nspecific goals through natural language interaction. While recent advances in\nlarge language models (LLMs) have significantly improved linguistic fluency and\ncontextual understanding, building effective and emotionally intelligent ToD\nsystems remains a complex challenge. Effective ToD systems must optimise for\ntask success, emotional understanding and responsiveness, and precise\ninformation conveyance, all within inherently noisy and ambiguous\nconversational environments. In this work, we investigate architectural,\nrepresentational, optimisational as well as emotional considerations of ToD\nsystems. We set up systems covering these design considerations with a\nchallenging evaluation environment composed of a natural-language user\nsimulator coupled with an imperfect natural language understanding module. We\npropose \\textbf{LUSTER}, an \\textbf{L}LM-based \\textbf{U}nified \\textbf{S}ystem\nfor \\textbf{T}ask-oriented dialogue with \\textbf{E}nd-to-end\n\\textbf{R}einforcement learning with both short-term (user sentiment) and\nlong-term (task success) rewards. Our findings demonstrate that combining LLM\ncapability with structured reward modelling leads to more resilient and\nemotionally responsive ToD systems, offering a practical path forward for\nnext-generation conversational agents.",
        "url": "http://arxiv.org/abs/2507.01594v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01594v1",
        "arxiv_id": "2507.01594v1",
        "authors": [
            "Shutong Feng",
            "Hsien-chin Lin",
            "Nurul Lubis",
            "Carel van Niekerk",
            "Michael Heck",
            "Benjamin Ruppik",
            "Renato Vukovic",
            "Milica Gašić"
        ],
        "submitted": "2025-07-02 11:00:33",
        "source": "arxiv",
        "comment": "19 pages, 6 figures"
    },
    {
        "title": "Self-Guided Process Reward Optimization with Redefined Step-wise Advantage for Process Reinforcement Learning",
        "abstract": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation.",
        "url": "http://arxiv.org/abs/2507.01551v2",
        "pdf_url": "http://arxiv.org/pdf/2507.01551v2",
        "arxiv_id": "2507.01551v2",
        "authors": [
            "Wu Fei",
            "Hao Kong",
            "Shuxian Liang",
            "Yang Lin",
            "Yibo Yang",
            "Jing Tang",
            "Lei Chen",
            "Xiansheng Hua"
        ],
        "submitted": "2025-07-02 10:05:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants",
        "abstract": "This paper explores how older adults, particularly aging migrants in urban\nChina, can engage AI-assisted co-creation to express personal narratives that\nare often fragmented, underrepresented, or difficult to verbalize. Through a\npilot workshop combining oral storytelling and the symbolic reconstruction of\nHanzi, participants shared memories of migration and recreated new character\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\ntogether with physical materials. Supported by human facilitation and a soft AI\npresence, participants transformed lived experience into visual and tactile\nexpressions without requiring digital literacy. This approach offers new\nperspectives on human-AI collaboration and aging by repositioning AI not as a\ncontent producer but as a supportive mechanism, and by supporting narrative\nagency within sociotechnical systems.",
        "url": "http://arxiv.org/abs/2507.01548v2",
        "pdf_url": "http://arxiv.org/pdf/2507.01548v2",
        "arxiv_id": "2507.01548v2",
        "authors": [
            "Wen Zhan",
            "Ziqun Hua",
            "Peiyue Lin",
            "Yunfei Chen"
        ],
        "submitted": "2025-07-02 10:00:12",
        "source": "arxiv",
        "comment": "A version of this manuscript has been submitted to the [IASDR 2025\n  Conference](https://iasdr2025.org/) and is currently under review"
    },
    {
        "title": "Is External Information Useful for Stance Detection with LLMs?",
        "abstract": "In the stance detection task, a text is classified as either favorable,\nopposing, or neutral towards a target. Prior work suggests that the use of\nexternal information, e.g., excerpts from Wikipedia, improves stance detection\nperformance. However, whether or not such information can benefit large\nlanguage models (LLMs) remains an unanswered question, despite their wide\nadoption in many reasoning tasks. In this study, we conduct a systematic\nevaluation on how Wikipedia and web search external information can affect\nstance detection across eight LLMs and in three datasets with 12 targets.\nSurprisingly, we find that such information degrades performance in most cases,\nwith macro F1 scores dropping by up to 27.9\\%. We explain this through\nexperiments showing LLMs' tendency to align their predictions with the stance\nand sentiment of the provided information rather than the ground truth stance\nof the given text. We also find that performance degradation persists with\nchain-of-thought prompting, while fine-tuning mitigates but does not fully\neliminate it. Our findings, in contrast to previous literature on BERT-based\nsystems which suggests that external information enhances performance,\nhighlight the risks of information biases in LLM-based stance classifiers. Code\nis available at https://github.com/ngqm/acl2025-stance-detection.",
        "url": "http://arxiv.org/abs/2507.01543v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01543v1",
        "arxiv_id": "2507.01543v1",
        "authors": [
            "Quang Minh Nguyen",
            "Taegyoon Kim"
        ],
        "submitted": "2025-07-02 09:53:41",
        "source": "arxiv",
        "comment": "ACL Findings 2025"
    },
    {
        "title": "Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing",
        "abstract": "Out-of-scope (OOS) intent detection is a critical challenge in task-oriented\ndialogue systems (TODS), as it ensures robustness to unseen and ambiguous\nqueries. In this work, we propose a novel but simple modular framework that\ncombines uncertainty modeling with fine-tuned large language models (LLMs) for\nefficient and accurate OOS detection. The first step applies uncertainty\nestimation to the output of an in-scope intent detection classifier, which is\ncurrently deployed in a real-world TODS handling tens of thousands of user\ninteractions daily. The second step then leverages an emerging LLM-based\napproach, where a fine-tuned LLM is triggered to make a final decision on\ninstances with high uncertainty. Unlike prior approaches, our method\neffectively balances computational efficiency and performance, combining\ntraditional approaches with LLMs and yielding state-of-the-art results on key\nOOS detection benchmarks, including real-world OOS data acquired from a\ndeployed TODS.",
        "url": "http://arxiv.org/abs/2507.01541v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01541v1",
        "arxiv_id": "2507.01541v1",
        "authors": [
            "Álvaro Zaera",
            "Diana Nicoleta Popa",
            "Ivan Sekulic",
            "Paolo Rosso"
        ],
        "submitted": "2025-07-02 09:51:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence",
        "abstract": "The collection and release of street-level recordings as Open Data play a\nvital role in advancing autonomous driving systems and AI research. However,\nthese datasets pose significant privacy risks, particularly for pedestrians,\ndue to the presence of Personally Identifiable Information (PII) that extends\nbeyond biometric traits such as faces. In this paper, we present cRID, a novel\ncross-modal framework combining Large Vision-Language Models, Graph Attention\nNetworks, and representation learning to detect textual describable clues of\nPII and enhance person re-identification (Re-ID). Our approach focuses on\nidentifying and leveraging interpretable features, enabling the detection of\nsemantically meaningful PII beyond low-level appearance cues. We conduct a\nsystematic evaluation of PII presence in person image datasets. Our experiments\nshow improved performance in practical cross-dataset Re-ID scenarios, notably\nfrom Market-1501 to CUHK03-np (detected), highlighting the framework's\npractical utility. Code is available at https://github.com/RAufschlaeger/cRID.",
        "url": "http://arxiv.org/abs/2507.01504v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01504v1",
        "arxiv_id": "2507.01504v1",
        "authors": [
            "Robert Aufschläger",
            "Youssef Shoeb",
            "Azarm Nowzad",
            "Michael Heigl",
            "Fabian Bally",
            "Martin Schramm"
        ],
        "submitted": "2025-07-02 09:10:33",
        "source": "arxiv",
        "comment": "accepted for publication at the 2025 IEEE 28th International\n  Conference on Intelligent Transportation Systems (ITSC 2025), taking place\n  during November 18-21, 2025 in Gold Coast, Australia"
    },
    {
        "title": "Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities",
        "abstract": "Automatic text simplification (ATS) aims to enhance language accessibility\nfor various target groups, particularly persons with intellectual disabilities.\nRecent advancements in generative AI, especially large language models (LLMs),\nhave substantially improved the quality of machine-generated text\nsimplifications, thereby mitigating information barriers for the target group.\nHowever, existing LLM-based ATS systems do not incorporate preference feedback\non text simplifications during training, resulting in a lack of personalization\ntailored to the specific needs of target group representatives.\n  In this work, we extend the standard supervised fine-tuning (SFT) approach\nfor adapting LLM-based ATS models by leveraging a computationally efficient LLM\nalignment technique -- direct preference optimization (DPO). Specifically, we\npost-train LLM-based ATS models using human feedback collected from persons\nwith intellectual disabilities, reflecting their preferences on paired text\nsimplifications generated by mainstream LLMs. Furthermore, we propose a\npipeline for developing personalized LLM-based ATS systems, encompassing data\ncollection, model selection, SFT and DPO post-training, and evaluation. Our\nfindings underscore the necessity of active participation of target group\npersons in designing personalized AI accessibility solutions aligned with human\nexpectations. This work represents a step towards personalizing inclusive AI\nsystems at the target-group level, incorporating insights not only from text\nsimplification experts but also from target group persons themselves.",
        "url": "http://arxiv.org/abs/2507.01479v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01479v1",
        "arxiv_id": "2507.01479v1",
        "authors": [
            "Yingqiang Gao",
            "Kaede Johnson",
            "David Froehlich",
            "Luisa Carrer",
            "Sarah Ebling"
        ],
        "submitted": "2025-07-02 08:43:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ManifoldMind: Dynamic Hyperbolic Reasoning for Trustworthy Recommendations",
        "abstract": "We introduce ManifoldMind, a probabilistic geometric recommender system for\nexploratory reasoning over semantic hierarchies in hyperbolic space. Unlike\nprior methods with fixed curvature and rigid embeddings, ManifoldMind\nrepresents users, items, and tags as adaptive-curvature probabilistic spheres,\nenabling personalised uncertainty modeling and geometry-aware semantic\nexploration. A curvature-aware semantic kernel supports soft, multi-hop\ninference, allowing the model to explore diverse conceptual paths instead of\noverfitting to shallow or direct interactions. Experiments on four public\nbenchmarks show superior NDCG, calibration, and diversity compared to strong\nbaselines. ManifoldMind produces explicit reasoning traces, enabling\ntransparent, trustworthy, and exploration-driven recommendations in sparse or\nabstract domains.",
        "url": "http://arxiv.org/abs/2507.02014v1",
        "pdf_url": "http://arxiv.org/pdf/2507.02014v1",
        "arxiv_id": "2507.02014v1",
        "authors": [
            "Anoushka Harit",
            "Zhongtian Sun",
            "Suncica Hadzidedic"
        ],
        "submitted": "2025-07-02 08:42:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation",
        "abstract": "Speculative decoding (SD), where a small draft model is employed to propose\ndraft tokens in advance and then the target model validates them in parallel,\nhas emerged as a promising technique for LLM inference acceleration. Many\nendeavors to improve SD are to eliminate the need for a draft model and\ngenerate draft tokens in a retrieval-based manner in order to further alleviate\nthe drafting overhead and significantly reduce the difficulty in deployment and\napplications. However, retrieval-based SD relies on a matching paradigm to\nretrieval the most relevant reference as the draft tokens, where these methods\noften fail to find matched and accurate draft tokens. To address this\nchallenge, we propose LogitSpec to effectively expand the retrieval range and\nfind the most relevant reference as drafts. Our LogitSpec is motivated by the\nobservation that the logit of the last token can not only predict the next\ntoken, but also speculate the next next token. Specifically, LogitSpec\ngenerates draft tokens in two steps: (1) utilizing the last logit to speculate\nthe next next token; (2) retrieving relevant reference for both the next token\nand the next next token. LogitSpec is training-free and plug-and-play, which\ncan be easily integrated into existing LLM inference frameworks. Extensive\nexperiments on a wide range of text generation benchmarks demonstrate that\nLogitSpec can achieve up to 2.61 $\\times$ speedup and 3.28 mean accepted tokens\nper decoding step. Our code is available at\nhttps://github.com/smart-lty/LogitSpec.",
        "url": "http://arxiv.org/abs/2507.01449v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01449v1",
        "arxiv_id": "2507.01449v1",
        "authors": [
            "Tianyu Liu",
            "Qitan Lv",
            "Hao Li",
            "Xing Gao",
            "Xiao Sun"
        ],
        "submitted": "2025-07-02 08:08:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction",
        "abstract": "This paper addresses the challenges posed by the unstructured nature and\nhigh-dimensional semantic complexity of electronic health record texts. A deep\nlearning method based on attention mechanisms is proposed to achieve unified\nmodeling for information extraction and multi-label disease prediction. The\nstudy is conducted on the MIMIC-IV dataset. A Transformer-based architecture is\nused to perform representation learning over clinical text. Multi-layer\nself-attention mechanisms are employed to capture key medical entities and\ntheir contextual relationships. A Sigmoid-based multi-label classifier is then\napplied to predict multiple disease labels. The model incorporates a\ncontext-aware semantic alignment mechanism, enhancing its representational\ncapacity in typical medical scenarios such as label co-occurrence and sparse\ninformation. To comprehensively evaluate model performance, a series of\nexperiments were conducted, including baseline comparisons, hyperparameter\nsensitivity analysis, data perturbation studies, and noise injection tests.\nResults demonstrate that the proposed method consistently outperforms\nrepresentative existing approaches across multiple performance metrics. The\nmodel maintains strong generalization under varying data scales, interference\nlevels, and model depth configurations. The framework developed in this study\noffers an efficient algorithmic foundation for processing real-world clinical\ntexts and presents practical significance for multi-label medical text modeling\ntasks.",
        "url": "http://arxiv.org/abs/2507.01437v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01437v1",
        "arxiv_id": "2507.01437v1",
        "authors": [
            "Ting Xu",
            "Xiaoxiao Deng",
            "Xiandong Meng",
            "Haifeng Yang",
            "Yan Wu"
        ],
        "submitted": "2025-07-02 07:45:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading",
        "abstract": "Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions.",
        "url": "http://arxiv.org/abs/2507.01431v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01431v1",
        "arxiv_id": "2507.01431v1",
        "authors": [
            "Yoonseok Yang",
            "Minjune Kim",
            "Marlon Rondinelli",
            "Keren Shao"
        ],
        "submitted": "2025-07-02 07:33:19",
        "source": "arxiv",
        "comment": "7 pages, 5 figues, 1 table"
    },
    {
        "title": "DARTS: A Dual-View Attack Framework for Targeted Manipulation in Federated Sequential Recommendation",
        "abstract": "Federated recommendation (FedRec) preserves user privacy by enabling\ndecentralized training of personalized models, but this architecture is\ninherently vulnerable to adversarial attacks. Significant research has been\nconducted on targeted attacks in FedRec systems, motivated by commercial and\nsocial influence considerations. However, much of this work has largely\noverlooked the differential robustness of recommendation models. Moreover, our\nempirical findings indicate that existing targeted attack methods achieve only\nlimited effectiveness in Federated Sequential Recommendation(FSR) tasks. Driven\nby these observations, we focus on investigating targeted attacks in FSR and\npropose a novel dualview attack framework, named DV-FSR. This attack method\nuniquely combines a sampling-based explicit strategy with a contrastive\nlearning-based implicit gradient strategy to orchestrate a coordinated attack.\nAdditionally, we introduce a specific defense mechanism tailored for targeted\nattacks in FSR, aiming to evaluate the mitigation effects of the attack method\nwe proposed. Extensive experiments validate the effectiveness of our proposed\napproach on representative sequential models. Our codes are publicly available.",
        "url": "http://arxiv.org/abs/2507.01383v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01383v1",
        "arxiv_id": "2507.01383v1",
        "authors": [
            "Qitao Qin",
            "Yucong Luo",
            "Zhibo Chu"
        ],
        "submitted": "2025-07-02 05:57:09",
        "source": "arxiv",
        "comment": "10 pages. arXiv admin note: substantial text overlap with\n  arXiv:2409.07500; text overlap with arXiv:2212.05399 by other authors"
    },
    {
        "title": "Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy",
        "abstract": "Despite the critical role of reward models (RMs) in reinforcement learning\nfrom human feedback (RLHF), current state-of-the-art open RMs perform poorly on\nmost existing evaluation benchmarks, failing to capture the spectrum of nuanced\nand sophisticated human preferences. Even approaches that incorporate advanced\ntraining techniques have not yielded meaningful performance improvements. We\nhypothesize that this brittleness stems primarily from limitations in\npreference datasets, which are often narrowly scoped, synthetically labeled, or\nlack rigorous quality control. To address these challenges, we present a\nlarge-scale preference dataset comprising 40 million preference pairs, named\nSynPref-40M. To enable data curation at scale, we design a human-AI synergistic\ntwo-stage pipeline that leverages the complementary strengths of human\nannotation quality and AI scalability. In this pipeline, humans provide\nverified annotations, while large language models perform automatic curation\nbased on human guidance. Training on this preference mixture, we introduce\nSkywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B\nparameters, trained on a carefully curated subset of 26 million preference\npairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile\nacross a wide range of capabilities, including alignment with human\npreferences, objective correctness, safety, resistance to stylistic biases, and\nbest-of-N scaling, achieving state-of-the-art performance across seven major\nreward model benchmarks. Ablation studies confirm that the effectiveness of our\napproach stems not only from data scale but also from high-quality curation.\nThe Skywork-Reward-V2 series represents substantial progress in open reward\nmodels, highlighting the untapped potential of existing preference datasets and\ndemonstrating how human-AI curation synergy can unlock significantly higher\ndata quality.",
        "url": "http://arxiv.org/abs/2507.01352v2",
        "pdf_url": "http://arxiv.org/pdf/2507.01352v2",
        "arxiv_id": "2507.01352v2",
        "authors": [
            "Chris Yuhao Liu",
            "Liang Zeng",
            "Yuzhen Xiao",
            "Jujie He",
            "Jiacai Liu",
            "Chaojie Wang",
            "Rui Yan",
            "Wei Shen",
            "Fuxiang Zhang",
            "Jiacheng Xu",
            "Yang Liu",
            "Yahui Zhou"
        ],
        "submitted": "2025-07-02 04:40:29",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LEDOM: An Open and Fundamental Reverse Language Model",
        "abstract": "We introduce LEDOM, the first purely reverse language model, trained\nautoregressively on 435B tokens with 2B and 7B parameter variants, which\nprocesses sequences in reverse temporal order through previous token\nprediction. For the first time, we present the reverse language model as a\npotential foundational model across general tasks, accompanied by a set of\nintriguing examples and insights. Based on LEDOM, we further introduce a novel\napplication: Reverse Reward, where LEDOM-guided reranking of forward language\nmodel outputs leads to substantial performance improvements on mathematical\nreasoning tasks. This approach leverages LEDOM's unique backward reasoning\ncapability to refine generation quality through posterior evaluation. Our\nfindings suggest that LEDOM exhibits unique characteristics with broad\napplication potential. We will release all models, training code, and\npre-training data to facilitate future research.",
        "url": "http://arxiv.org/abs/2507.01335v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01335v1",
        "arxiv_id": "2507.01335v1",
        "authors": [
            "Xunjian Yin",
            "Sitao Cheng",
            "Yuxi Xie",
            "Xinyu Hu",
            "Li Lin",
            "Xinyi Wang",
            "Liangming Pan",
            "William Yang Wang",
            "Xiaojun Wan"
        ],
        "submitted": "2025-07-02 03:52:00",
        "source": "arxiv",
        "comment": "Work in progress"
    },
    {
        "title": "Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs",
        "abstract": "Navigating the complexities of physics reasoning has long been a difficult\ntask for Large Language Models (LLMs), requiring a synthesis of profound\nconceptual understanding and adept problem-solving techniques. In this study,\nwe investigate the application of advanced instruction-tuned reasoning models,\nsuch as Deepseek-R1, to address a diverse spectrum of physics problems curated\nfrom the challenging SciBench benchmark. Our comprehensive experimental\nevaluation reveals the remarkable capabilities of reasoning models. Not only do\nthey achieve state-of-the-art accuracy in answering intricate physics\nquestions, but they also generate distinctive reasoning patterns that emphasize\non symbolic derivation. Furthermore, our findings indicate that even for these\nhighly sophisticated reasoning models, the strategic incorporation of few-shot\nprompting can still yield measurable improvements in overall accuracy,\nhighlighting the potential for continued performance gains.",
        "url": "http://arxiv.org/abs/2507.01334v2",
        "pdf_url": "http://arxiv.org/pdf/2507.01334v2",
        "arxiv_id": "2507.01334v2",
        "authors": [
            "Nifu Dan",
            "Yujun Cai",
            "Yiwei Wang"
        ],
        "submitted": "2025-07-02 03:51:16",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Uncertainty-Aware Complex Scientific Table Data Extraction",
        "abstract": "Table structure recognition (TSR) and optical character recognition (OCR)\nplay crucial roles in extracting structured data from tables in scientific\ndocuments. However, existing extraction frameworks built on top of TSR and OCR\nmethods often fail to quantify the uncertainties of extracted results. To\nobtain highly accurate data for scientific domains, all extracted data must be\nmanually verified, which can be time-consuming and labor-intensive. We propose\na framework that performs uncertainty-aware data extraction for complex\nscientific tables, built on conformal prediction, a model-agnostic method for\nuncertainty quantification (UQ). We explored various uncertainty scoring\nmethods to aggregate the uncertainties introduced by TSR and OCR. We rigorously\nevaluated the framework using a standard benchmark and an in-house dataset\nconsisting of complex scientific tables in six scientific domains. The results\ndemonstrate the effectiveness of using UQ for extraction error detection, and\nby manually verifying only 47\\% of extraction results, the data quality can be\nimproved by 30\\%. Our work quantitatively demonstrates the role of UQ with the\npotential of improving the efficiency in the human-machine cooperation process\nto obtain scientifically usable data from complex tables in scientific\ndocuments. All code and data are available on GitHub at\nhttps://github.com/lamps-lab/TSR-OCR-UQ/tree/main.",
        "url": "http://arxiv.org/abs/2507.02009v1",
        "pdf_url": "http://arxiv.org/pdf/2507.02009v1",
        "arxiv_id": "2507.02009v1",
        "authors": [
            "Kehinde Ajayi",
            "Yi He",
            "Jian Wu"
        ],
        "submitted": "2025-07-02 03:36:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation",
        "abstract": "Activation sparsity can reduce the computational overhead and memory\ntransfers during the forward pass of Large Language Model (LLM) inference.\nExisting methods face limitations, either demanding time-consuming recovery\ntraining that hinders real-world adoption, or relying on empirical\nmagnitude-based pruning, which causes fluctuating sparsity and unstable\ninference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse\nActivation), a novel method for activation sparsification designed to improve\nLLM efficiency without requiring additional training or magnitude-based\npruning. We leverage layerwise orthogonal rotations to transform input\nactivations into rotated forms that are more suitable for sparsification. By\nemploying a Top-K selection approach within the rotated activations, we achieve\nconsistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA\nis effective across various sizes and types of LLMs, demonstrating minimal\nperformance degradation and robust inference acceleration. Specifically, for\nLLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a\nconsistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in\nzero-shot tasks compared to the dense model to just 0.54%, while surpassing\nTEAL by 1.77% and CATS by 17.14%.",
        "url": "http://arxiv.org/abs/2507.01299v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01299v1",
        "arxiv_id": "2507.01299v1",
        "authors": [
            "Kai Liu",
            "Bowen Xu",
            "Shaoyu Wu",
            "Xin Chen",
            "Hao Zhou",
            "Yongliang Tao",
            "Lulu Hu"
        ],
        "submitted": "2025-07-02 02:36:03",
        "source": "arxiv",
        "comment": "ICML 2025 Acceptance"
    },
    {
        "title": "Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks",
        "abstract": "Retrieval-augmented Generation (RAG) has primarily been studied in limited\nsettings, such as factoid question answering; more challenging,\nreasoning-intensive benchmarks have seen limited success from minimal RAG. In\nthis work, we challenge this prevailing view on established,\nreasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We\nidentify a key missing component in prior work: a usable, web-scale datastore\naligned with the breadth of pretraining data. To this end, we introduce\nCompactDS: a diverse, high-quality, web-scale datastore that achieves high\nretrieval accuracy and subsecond latency on a single-node. The key insights are\n(1) most web content can be filtered out without sacrificing coverage, and a\ncompact, high-quality subset is sufficient; and (2) combining in-memory\napproximate nearest neighbor (ANN) retrieval and on-disk exact search balances\nspeed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves\nconsistent accuracy improvements across all benchmarks and model sizes\n(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,\nand 19% on MATH. No single data source suffices alone, highlighting the\nimportance of diversity of sources (web crawls, curated math, academic papers,\ntextbooks). Finally, we show that our carefully designed in-house datastore\nmatches or outperforms web search engines such as Google Search, as well as\nrecently proposed, complex agent-based RAG systems--all while maintaining\nsimplicity, reproducibility, and self-containment. We release CompactDS and our\nretrieval pipeline, supporting future research exploring retrieval-based AI\nsystems.",
        "url": "http://arxiv.org/abs/2507.01297v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01297v1",
        "arxiv_id": "2507.01297v1",
        "authors": [
            "Xinxi Lyu",
            "Michael Duan",
            "Rulin Shao",
            "Pang Wei Koh",
            "Sewon Min"
        ],
        "submitted": "2025-07-02 02:35:47",
        "source": "arxiv",
        "comment": "33 pages, 2 figures, 27 tables"
    },
    {
        "title": "Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation",
        "abstract": "Graph federated recommendation systems offer a privacy-preserving alternative\nto traditional centralized recommendation architectures, which often raise\nconcerns about data security. While federated learning enables personalized\nrecommendations without exposing raw user data, existing aggregation methods\noverlook the unique properties of user embeddings in this setting. Indeed,\ntraditional aggregation methods fail to account for their complexity and the\ncritical role of user similarity in recommendation effectiveness. Moreover,\nevolving user interactions require adaptive aggregation while preserving the\ninfluence of high-relevance anchor users (the primary users before expansion in\ngraph-based frameworks). To address these limitations, we introduce\nDist-FedAvg, a novel distance-based aggregation method designed to enhance\npersonalization and aggregation efficiency in graph federated learning. Our\nmethod assigns higher aggregation weights to users with similar embeddings,\nwhile ensuring that anchor users retain significant influence in local updates.\nEmpirical evaluations on multiple datasets demonstrate that Dist-FedAvg\nconsistently outperforms baseline aggregation techniques, improving\nrecommendation accuracy while maintaining seamless integration into existing\nfederated learning frameworks.",
        "url": "http://arxiv.org/abs/2507.01285v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01285v1",
        "arxiv_id": "2507.01285v1",
        "authors": [
            "Aymen Rayane Khouas",
            "Mohamed Reda Bouadjenek",
            "Hakim Hacid",
            "Sunil Aryal"
        ],
        "submitted": "2025-07-02 01:57:58",
        "source": "arxiv",
        "comment": "17 pages, 5 figures"
    },
    {
        "title": "Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating their parametric knowledge with external retrieved content.\nHowever, knowledge conflicts caused by internal inconsistencies or noisy\nretrieved content can severely undermine the generation reliability of RAG\nsystems.In this work, we argue that LLMs should rethink all evidence, including\nboth retrieved content and internal knowledge, before generating responses.We\npropose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel\nframework that improves trustworthiness through Conflict-Driven Summarization\nof all available evidence.CARE-RAG first derives parameter-aware evidence by\ncomparing parameter records to identify diverse internal perspectives. It then\nrefines retrieved evidences to produce context-aware evidence, removing\nirrelevant or misleading content. To detect and summarize conflicts, we distill\na 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable\nsynthesis across multiple sources.To further ensure evaluation integrity, we\nintroduce a QA Repair step to correct outdated or ambiguous benchmark\nanswers.Experiments on revised QA datasets with retrieval data show that\nCARE-RAG consistently outperforms strong RAG baselines, especially in scenarios\nwith noisy or conflicting evidence.",
        "url": "http://arxiv.org/abs/2507.01281v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01281v1",
        "arxiv_id": "2507.01281v1",
        "authors": [
            "Juan Chen",
            "Baolong Bi",
            "Wei Zhang",
            "Jingyan Sui",
            "Xiaofei Zhu",
            "Yuanzhuo Wang",
            "Lingrui Mei",
            "Shenghua Liu"
        ],
        "submitted": "2025-07-02 01:39:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening",
        "abstract": "Large language models (LLMs) can simulate clinical reasoning based on natural\nlanguage prompts, but their utility in ophthalmology is largely unexplored.\nThis study evaluated GPT-4's ability to interpret structured textual\ndescriptions of retinal fundus photographs and simulate clinical decisions for\ndiabetic retinopathy (DR) and glaucoma screening, including the impact of\nadding real or synthetic clinical metadata. We conducted a retrospective\ndiagnostic validation study using 300 annotated fundus images. GPT-4 received\nstructured prompts describing each image, with or without patient metadata. The\nmodel was tasked with assigning an ICDR severity score, recommending DR\nreferral, and estimating the cup-to-disc ratio for glaucoma referral.\nPerformance was evaluated using accuracy, macro and weighted F1 scores, and\nCohen's kappa. McNemar's test and change rate analysis were used to assess the\ninfluence of metadata. GPT-4 showed moderate performance for ICDR\nclassification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),\ndriven mainly by correct identification of normal cases. Performance improved\nin the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For\nglaucoma referral, performance was poor across all settings (accuracy ~78%, F1\n<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes\n(McNemar p > 0.05), and predictions remained consistent across conditions.\nGPT-4 can simulate basic ophthalmic decision-making from structured prompts but\nlacks precision for complex tasks. While not suitable for clinical use, LLMs\nmay assist in education, documentation, or image annotation workflows in\nophthalmology.",
        "url": "http://arxiv.org/abs/2507.01278v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01278v1",
        "arxiv_id": "2507.01278v1",
        "authors": [
            "Cindy Lie Tabuse",
            "David Restepo",
            "Carolina Gracitelli",
            "Fernando Korn Malerbi",
            "Caio Regatieri",
            "Luis Filipe Nakayama"
        ],
        "submitted": "2025-07-02 01:35:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant",
        "abstract": "In this paper we discuss the capability of large language models to base\ntheir answer and provide proper references when dealing with legal matters of\nnon-english and non-chinese speaking country. We discuss the history of legal\ninformation retrieval, the difference between case law and statute law, its\nimpact on the legal tasks and analyze the latest research in this field. Basing\non that background we introduce gAIus, the architecture of the cognitive\nLLM-based agent, whose responses are based on the knowledge retrieved from\ncertain legal act, which is Polish Civil Code. We propose a retrieval mechanism\nwhich is more explainable, human-friendly and achieves better results than\nembedding-based approaches. To evaluate our method we create special dataset\nbased on single-choice questions from entrance exams for law apprenticeships\nconducted in Poland. The proposed architecture critically leveraged the\nabilities of used large language models, improving the gpt-3.5-turbo-0125 by\n419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.\nAt the end of our paper we show the possible future path of research and\npotential applications of our findings.",
        "url": "http://arxiv.org/abs/2507.01259v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01259v1",
        "arxiv_id": "2507.01259v1",
        "authors": [
            "Michał Matak",
            "Jarosław A. Chudziak"
        ],
        "submitted": "2025-07-02 00:36:27",
        "source": "arxiv",
        "comment": "8 pages, 2 figures, presented at ICAART 2025, in proceedings of the\n  17th International Conference on Agents and Artificial Intelligence - Volume\n  3: ICAART"
    },
    {
        "title": "The Medium Is Not the Message: Deconfounding Text Embeddings via Linear Concept Erasure",
        "abstract": "Embedding-based similarity metrics between text sequences can be influenced\nnot just by the content dimensions we most care about, but can also be biased\nby spurious attributes like the text's source or language. These document\nconfounders cause problems for many applications, but especially those that\nneed to pool texts from different corpora. This paper shows that a debiasing\nalgorithm that removes information about observed confounders from the encoder\nrepresentations substantially reduces these biases at a minimal computational\ncost. Document similarity and clustering metrics improve across every embedding\nvariant and task we evaluate -- often dramatically. Interestingly, performance\non out-of-distribution benchmarks is not impacted, indicating that the\nembeddings are not otherwise degraded.",
        "url": "http://arxiv.org/abs/2507.01234v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01234v1",
        "arxiv_id": "2507.01234v1",
        "authors": [
            "Yu Fan",
            "Yang Tian",
            "Shauli Ravfogel",
            "Mrinmaya Sachan",
            "Elliott Ash",
            "Alexander Hoyle"
        ],
        "submitted": "2025-07-01 23:17:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis",
        "abstract": "Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language\nProcessing (NLP) task that extracts aspects from text and determines their\nassociated sentiments, enabling fine-grained analysis of user opinions.\nExisting ABSA methods struggle to balance computational efficiency with high\nperformance: deep learning models often lack global context, transformers\ndemand significant computational resources, and Mamba-based approaches face\nCUDA dependency and diminished local correlations. Recent advancements in\nExtended Long Short-Term Memory (xLSTM) models, particularly their efficient\nmodeling of long-range dependencies, have significantly advanced the NLP\ncommunity. However, their potential in ABSA remains untapped. To this end, we\npropose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework\nintegrating a bi-directional mLSTM architecture with forward and partially\nflipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context\nmodeling by processing the initial sequence segment in reverse with dedicated\nparameters, preserving critical short-range patterns. We further introduce an\nmLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that\ndynamically combines forward mLSTM outputs as query and key with PF-mLSTM\noutputs as value, optimizing short-range dependency capture while maintaining\nglobal context and efficiency. Experimental results on three benchmark datasets\ndemonstrate that MEGA outperforms state-of-the-art baselines, achieving\nsuperior accuracy and efficiency in ABSA tasks.",
        "url": "http://arxiv.org/abs/2507.01213v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01213v1",
        "arxiv_id": "2507.01213v1",
        "authors": [
            "Adamu Lawan",
            "Juhua Pu",
            "Haruna Yunusa",
            "Jawad Muhammad",
            "Muhammad Lawan"
        ],
        "submitted": "2025-07-01 22:21:33",
        "source": "arxiv",
        "comment": "6, 1 figure"
    },
    {
        "title": "STELLA: Self-Evolving LLM Agent for Biomedical Research",
        "abstract": "The rapid growth of biomedical data, tools, and literature has created a\nfragmented research landscape that outpaces human expertise. While AI agents\noffer a solution, they typically rely on static, manually curated toolsets,\nlimiting their ability to adapt and scale. Here, we introduce STELLA, a\nself-evolving AI agent designed to overcome these limitations. STELLA employs a\nmulti-agent architecture that autonomously improves its own capabilities\nthrough two core mechanisms: an evolving Template Library for reasoning\nstrategies and a dynamic Tool Ocean that expands as a Tool Creation Agent\nautomatically discovers and integrates new bioinformatics tools. This allows\nSTELLA to learn from experience. We demonstrate that STELLA achieves\nstate-of-the-art accuracy on a suite of biomedical benchmarks, scoring\napproximately 26\\% on Humanity's Last Exam: Biomedicine, 54\\% on LAB-Bench:\nDBQA, and 63\\% on LAB-Bench: LitQA, outperforming leading models by up to 6\npercentage points. More importantly, we show that its performance\nsystematically improves with experience; for instance, its accuracy on the\nHumanity's Last Exam benchmark almost doubles with increased trials. STELLA\nrepresents a significant advance towards AI Agent systems that can learn and\ngrow, dynamically scaling their expertise to accelerate the pace of biomedical\ndiscovery.",
        "url": "http://arxiv.org/abs/2507.02004v1",
        "pdf_url": "http://arxiv.org/pdf/2507.02004v1",
        "arxiv_id": "2507.02004v1",
        "authors": [
            "Ruofan Jin",
            "Zaixi Zhang",
            "Mengdi Wang",
            "Le Cong"
        ],
        "submitted": "2025-07-01 20:52:01",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Matching and Linking Entries in Historical Swedish Encyclopedias",
        "abstract": "The \\textit{Nordisk familjebok} is a Swedish encyclopedia from the 19th and\n20th centuries. It was written by a team of experts and aimed to be an\nintellectual reference, stressing precision and accuracy. This encyclopedia had\nfour main editions remarkable by their size, ranging from 20 to 38 volumes. As\na consequence, the \\textit{Nordisk familjebok} had a considerable influence in\nuniversities, schools, the media, and society overall. As new editions were\nreleased, the selection of entries and their content evolved, reflecting\nintellectual changes in Sweden.\n  In this paper, we used digitized versions from \\textit{Project Runeberg}. We\nfirst resegmented the raw text into entries and matched pairs of entries\nbetween the first and second editions using semantic sentence embeddings. We\nthen extracted the geographical entries from both editions using a\ntransformer-based classifier and linked them to Wikidata. This enabled us to\nidentify geographic trends and possible shifts between the first and second\neditions, written between 1876-1899 and 1904-1926, respectively.\n  Interpreting the results, we observe a small but significant shift in\ngeographic focus away from Europe and towards North America, Africa, Asia,\nAustralia, and northern Scandinavia from the first to the second edition,\nconfirming the influence of the First World War and the rise of new powers. The\ncode and data are available on GitHub at\nhttps://github.com/sibbo/nordisk-familjebok.",
        "url": "http://arxiv.org/abs/2507.01170v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01170v1",
        "arxiv_id": "2507.01170v1",
        "authors": [
            "Simon Börjesson",
            "Erik Ersmark",
            "Pierre Nugues"
        ],
        "submitted": "2025-07-01 20:17:19",
        "source": "arxiv",
        "comment": "10 pages, 3 figures"
    },
    {
        "title": "Towards a Signal Detection Based Measure for Assessing Information Quality of Explainable Recommender Systems",
        "abstract": "There is growing interest in explainable recommender systems that provide\nrecommendations along with explanations for the reasoning behind them. When\nevaluating recommender systems, most studies focus on overall recommendation\nperformance. Only a few assess the quality of the explanations. Explanation\nquality is often evaluated through user studies that subjectively gather users'\nopinions on representative explanatory factors that shape end-users'\nperspective towards the results, not about the explanation contents itself. We\naim to fill this gap by developing an objective metric to evaluate Veracity:\nthe information quality of explanations. Specifically, we decompose Veracity\ninto two dimensions: Fidelity and Attunement. Fidelity refers to whether the\nexplanation includes accurate information about the recommended item.\nAttunement evaluates whether the explanation reflects the target user's\npreferences. By applying signal detection theory, we first determine decision\noutcomes for each dimension and then combine them to calculate a sensitivity,\nwhich serves as the final Veracity value. To assess the effectiveness of the\nproposed metric, we set up four cases with varying levels of information\nquality to validate whether our metric can accurately capture differences in\nquality. The results provided meaningful insights into the effectiveness of our\nproposed metric.",
        "url": "http://arxiv.org/abs/2507.01168v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01168v1",
        "arxiv_id": "2507.01168v1",
        "authors": [
            "Yeonbin Son",
            "Matthew L. Bolton"
        ],
        "submitted": "2025-07-01 20:11:17",
        "source": "arxiv",
        "comment": "Accepted to IEEE CAI 2025"
    },
    {
        "title": "Event-based evaluation of abstractive news summarization",
        "abstract": "An abstractive summary of a news article contains its most important\ninformation in a condensed version. The evaluation of automatically generated\nsummaries by generative language models relies heavily on human-authored\nsummaries as gold references, by calculating overlapping units or similarity\nscores. News articles report events, and ideally so should the summaries. In\nthis work, we propose to evaluate the quality of abstractive summaries by\ncalculating overlapping events between generated summaries, reference\nsummaries, and the original news articles. We experiment on a richly annotated\nNorwegian dataset comprising both events annotations and summaries authored by\nexpert human annotators. Our approach provides more insight into the event\ninformation contained in the summaries.",
        "url": "http://arxiv.org/abs/2507.01160v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01160v1",
        "arxiv_id": "2507.01160v1",
        "authors": [
            "Huiling You",
            "Samia Touileb",
            "Erik Velldal",
            "Lilja Øvrelid"
        ],
        "submitted": "2025-07-01 19:49:23",
        "source": "arxiv",
        "comment": "to appear at GEM2 workshop@ACL 2025"
    }
]