[
    {
        "title": "Domain-Adaptive and Scalable Dense Retrieval for Content-Based Recommendation",
        "abstract": "E-commerce recommendation and search commonly rely on sparse keyword matching (e.g., BM25), which breaks down under vocabulary mismatch when user intent has limited lexical overlap with product metadata. We cast content-based recommendation as recommendation-as-retrieval: given a natural-language intent signal (a query or review), retrieve the top-K most relevant items from a large catalog via semantic similarity.\n  We present a scalable dense retrieval system based on a two-tower bi-encoder, fine-tuned on the Amazon Reviews 2023 (Fashion) subset using supervised contrastive learning with Multiple Negatives Ranking Loss. We construct training pairs from review text (as a query proxy) and item metadata (as the positive document) and fine-tune on 50,000 sampled interactions with a maximum sequence length of 500 tokens.\n  For efficient serving, we combine FAISS HNSW indexing with an ONNX Runtime inference pipeline using INT8 dynamic quantization. On a review-to-title benchmark over 826,402 catalog items, our approach improves Recall@10 from 0.26 (BM25) to 0.66, while meeting practical latency and model-size constraints: 6.1 ms median CPU inference latency (batch size 1) and a 4x reduction in model size.\n  Overall, we provide an end-to-end, reproducible blueprint for taking domain-adapted dense retrieval from offline training to CPU-efficient serving at catalog scale.",
        "url": "http://arxiv.org/abs/2602.00899v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00899v1",
        "arxiv_id": "2602.00899v1",
        "authors": [
            "Mritunjay Pandey"
        ],
        "submitted": "2026-01-31 20:58:23",
        "source": "arxiv",
        "comment": "13 pages, 4 figures. Semantic dense retrieval for content-based recommendation on Amazon Reviews 2023 (Category - Fashion). Dataset statistics: 2.0M users; 825.9K items; 2.5M ratings; 94.9M review tokens; 510.5M metadata tokens. Timespan: May 1996 to September 2023. Metadata includes: user reviews (ratings, text, helpfulness votes, etc.); item metadata (descriptions, price, raw images, etc.)",
        "score": 16,
        "keyword_reasons": [
            "Found 'dense retrieval' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of e-commerce search and recommendation. The authors propose a scalable dense retrieval system for content-based recommendation, which aligns with your focus on query understanding and ranking models. However, the paper's primary focus on recommender systems and its application to e-commerce might not be a central match to your broader interests in IR and NLP."
    },
    {
        "title": "Breaking the Static Graph: Context-Aware Traversal for Robust Retrieval-Augmented Generation",
        "abstract": "Recent advances in Retrieval-Augmented Generation (RAG) have shifted from simple vector similarity to structure-aware approaches like HippoRAG, which leverage Knowledge Graphs (KGs) and Personalized PageRank (PPR) to capture multi-hop dependencies. However, these methods suffer from a \"Static Graph Fallacy\": they rely on fixed transition probabilities determined during indexing. This rigidity ignores the query-dependent nature of edge relevance, causing semantic drift where random walks are diverted into high-degree \"hub\" nodes before reaching critical downstream evidence. Consequently, models often achieve high partial recall but fail to retrieve the complete evidence chain required for multi-hop queries. To address this, we propose CatRAG, Context-Aware Traversal for robust RAG, a framework that builds on the HippoRAG 2 architecture and transforms the static KG into a query-adaptive navigation structure. We introduce a multi-faceted framework to steer the random walk: (1) Symbolic Anchoring, which injects weak entity constraints to regularize the random walk; (2) Query-Aware Dynamic Edge Weighting, which dynamically modulates graph structure, to prune irrelevant paths while amplifying those aligned with the query's intent; and (3) Key-Fact Passage Weight Enhancement, a cost-efficient bias that structurally anchors the random walk to likely evidence. Experiments across four multi-hop benchmarks demonstrate that CatRAG consistently outperforms state of the art baselines. Our analysis reveals that while standard Recall metrics show modest gains, CatRAG achieves substantial improvements in reasoning completeness, the capacity to recover the entire evidence path without gaps. These results reveal that our approach effectively bridges the gap between retrieving partial context and enabling fully grounded reasoning. Resources are available at https://github.com/kwunhang/CatRAG.",
        "url": "http://arxiv.org/abs/2602.01965v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01965v1",
        "arxiv_id": "2602.01965v1",
        "authors": [
            "Kwun Hang Lau",
            "Fangyuan Zhang",
            "Boyu Ruan",
            "Yingli Zhou",
            "Qintian Guo",
            "Ruiyuan Zhang",
            "Xiaofang Zhou"
        ],
        "submitted": "2026-02-02 11:13:38",
        "source": "arxiv",
        "comment": null,
        "score": 14,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of Retrieval-Augmented Generation (RAG) and Knowledge Graphs (KGs). The proposed framework, CatRAG, addresses a significant challenge in RAG, namely the 'Static Graph Fallacy', and demonstrates substantial improvements in reasoning completeness and evidence path recovery. While not directly focused on query understanding or ranking models, the paper's emphasis on query-adaptive navigation and dynamic edge weighting aligns with your broader interests in IR and NLP."
    },
    {
        "title": "GRAB: An LLM-Inspired Sequence-First Click-Through Rate Prediction Modeling Paradigm",
        "abstract": "Traditional Deep Learning Recommendation Models (DLRMs) face increasing bottlenecks in performance and efficiency, often struggling with generalization and long-sequence modeling. Inspired by the scaling success of Large Language Models (LLMs), we propose Generative Ranking for Ads at Baidu (GRAB), an end-to-end generative framework for Click-Through Rate (CTR) prediction. GRAB integrates a novel Causal Action-aware Multi-channel Attention (CamA) mechanism to effectively capture temporal dynamics and specific action signals within user behavior sequences. Full-scale online deployment demonstrates that GRAB significantly outperforms established DLRMs, delivering a 3.05% increase in revenue and a 3.49% rise in CTR. Furthermore, the model demonstrates desirable scaling behavior: its expressive power shows a monotonic and approximately linear improvement as longer interaction sequences are utilized.",
        "url": "http://arxiv.org/abs/2602.01865v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01865v1",
        "arxiv_id": "2602.01865v1",
        "authors": [
            "Shaopeng Chen",
            "Chuyue Xie",
            "Huimin Ren",
            "Shaozong Zhang",
            "Han Zhang",
            "Ruobing Cheng",
            "Zhiqiang Cao",
            "Zehao Ju",
            "Gao Yu",
            "Jie Ding",
            "Xiaodong Chen",
            "Xuewu Jiao",
            "Shuanglong Li",
            "Liu Lin"
        ],
        "submitted": "2026-02-02 09:38:03",
        "source": "arxiv",
        "comment": null,
        "score": 13,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'user behavior' (score: +2)",
            "Found 'click' (score: +2)",
            "Found 'ctr' (score: +2)",
            "Found 'click-through rate' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of click-through rate prediction and user behavior modeling. The proposed GRAB framework leverages Large Language Models and attention mechanisms to capture temporal dynamics in user behavior sequences, aligning with your focus on deep semantic understanding and real-time relevance optimization. However, the paper's primary focus on recommender systems and e-commerce may limit its direct applicability to your broader interests in IR and NLP."
    },
    {
        "title": "Unifying Ranking and Generation in Query Auto-Completion via Retrieval-Augmented Generation and Multi-Objective Alignment",
        "abstract": "Query Auto-Completion (QAC) suggests query completions as users type, helping them articulate intent and reach results more efficiently. Existing approaches face fundamental challenges: traditional retrieve-and-rank pipelines have limited long-tail coverage and require extensive feature engineering, while recent generative methods suffer from hallucination and safety risks. We present a unified framework that reformulates QAC as end-to-end list generation through Retrieval-Augmented Generation (RAG) and multi-objective Direct Preference Optimization (DPO). Our approach combines three key innovations: (1) reformulating QAC as end-to-end list generation with multi-objective optimization; (2) defining and deploying a suite of rule-based, model-based, and LLM-as-judge verifiers for QAC, and using them in a comprehensive methodology that combines RAG, multi-objective DPO, and iterative critique-revision for high-quality synthetic data; (3) a hybrid serving architecture enabling efficient production deployment under strict latency constraints. Evaluation on a large-scale commercial search platform demonstrates substantial improvements: offline metrics show gains across all dimensions, human evaluation yields +0.40 to +0.69 preference scores, and a controlled online experiment achieves 5.44\\% reduction in keystrokes and 3.46\\% increase in suggestion adoption, validating that unified generation with RAG and multi-objective alignment provides an effective solution for production QAC. This work represents a paradigm shift to end-to-end generation powered by large language models, RAG, and multi-objective alignment, establishing a production-validated framework that can benefit the broader search and recommendation industry.",
        "url": "http://arxiv.org/abs/2602.01023v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01023v1",
        "arxiv_id": "2602.01023v1",
        "authors": [
            "Kai Yuan",
            "Anthony Zheng",
            "Jia Hu",
            "Divyanshu Sheth",
            "Hemanth Velaga",
            "Kylee Kim",
            "Matteo Guarrera",
            "Besim Avci",
            "Xuetao Yin",
            "Rajyashree Mukherjee",
            "Sean Suchter"
        ],
        "submitted": "2026-02-01 05:15:07",
        "source": "arxiv",
        "comment": "11 pages, 4 figures",
        "score": 13,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 9,
        "llm_reason": "This paper aligns closely with your research interests in Information Retrieval, particularly query understanding and ranking models. The focus on query auto-completion, retrieval-augmented generation, and multi-objective alignment demonstrates a deep semantic understanding and real-time relevance optimization, which are central to your research themes."
    },
    {
        "title": "Inferential Question Answering",
        "abstract": "Despite extensive research on a wide range of question answering (QA) systems, most existing work focuses on answer containment-i.e., assuming that answers can be directly extracted and/or generated from documents in the corpus. However, some questions require inference, i.e., deriving answers that are not explicitly stated but can be inferred from the available information. We introduce Inferential QA -- a new task that challenges models to infer answers from answer-supporting passages which provide only clues. To study this problem, we construct QUIT (QUestions requiring Inference from Texts) dataset, comprising 7,401 questions and 2.4M passages built from high-convergence human- and machine-authored hints, labeled across three relevance levels using LLM-based answerability and human verification. Through comprehensive evaluation of retrievers, rerankers, and LLM-based readers, we show that methods effective on traditional QA tasks struggle in inferential QA: retrievers underperform, rerankers offer limited gains, and fine-tuning provides inconsistent improvements. Even reasoning-oriented LLMs fail to outperform smaller general-purpose models. These findings reveal that current QA pipelines are not yet ready for inference-based reasoning. Inferential QA thus establishes a new class of QA tasks that move towards understanding and reasoning from indirect textual evidence.",
        "url": "http://arxiv.org/abs/2602.01239v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01239v1",
        "arxiv_id": "2602.01239v1",
        "authors": [
            "Jamshid Mozafari",
            "Hamed Zamani",
            "Guido Zuccon",
            "Adam Jatowt"
        ],
        "submitted": "2026-02-01 14:02:43",
        "source": "arxiv",
        "comment": "Proceedings of the ACM Web Conference 2026 (WWW 2026)",
        "score": 11,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The task of Inferential Question Answering involves deriving answers from indirect textual evidence, which aligns with your focus on deep semantic understanding and real-time relevance optimization. However, the paper's primary focus is on Natural Language Processing and Question Answering, rather than Search technologies or e-commerce, which slightly reduces its relevance to your core research themes."
    },
    {
        "title": "Wiki Live Challenge: Challenging Deep Research Agents with Expert-Level Wikipedia Articles",
        "abstract": "Deep Research Agents (DRAs) have demonstrated remarkable capabilities in autonomous information retrieval and report generation, showing great potential to assist humans in complex research tasks. Current evaluation frameworks primarily rely on LLM-generated references or LLM-derived evaluation dimensions. While these approaches offer scalability, they often lack the reliability of expert-verified content and struggle to provide objective, fine-grained assessments of critical dimensions. To bridge this gap, we introduce Wiki Live Challenge (WLC), a live benchmark that leverages the newest Wikipedia Good Articles (GAs) as expert-level references. Wikipedia's strict standards for neutrality, comprehensiveness, and verifiability serve as a great challenge for DRAs, with GAs representing the pinnacle of which. We curate a dataset of 100 recent Good Articles and propose Wiki Eval, a comprehensive evaluation framework comprising a fine-grained evaluation method with 39 criteria for writing quality and rigorous metrics for factual verifiability. Extensive experiments on various DRA systems demonstrate a significant gap between current DRAs and human expert-level Wikipedia articles, validating the effectiveness of WLC in advancing agent research. We release our benchmark at https://github.com/WangShao2000/Wiki_Live_Challenge",
        "url": "http://arxiv.org/abs/2602.01590v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01590v1",
        "arxiv_id": "2602.01590v1",
        "authors": [
            "Shaohan Wang",
            "Benfeng Xu",
            "Licheng Zhang",
            "Mingxuan Du",
            "Chiwei Zhu",
            "Xiaorui Wang",
            "Zhendong Mao",
            "Yongdong Zhang"
        ],
        "submitted": "2026-02-02 03:30:13",
        "source": "arxiv",
        "comment": "Preprint. Work in progress",
        "score": 10,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to Information Retrieval and Search technologies, particularly in the area of query understanding and ranking models. The focus on evaluating Deep Research Agents with expert-level Wikipedia articles aligns with the user's interest in real-time relevance optimization and deep semantic understanding. However, the primary focus of the paper is on evaluation frameworks rather than traditional IR tasks."
    },
    {
        "title": "Logic-Oriented Retriever Enhancement via Contrastive Learning",
        "abstract": "Large language models (LLMs) struggle in knowledge-intensive tasks, as retrievers often overfit to surface similarity and fail on queries involving complex logical relations. The capacity for logical analysis is inherent in model representations but remains underutilized in standard training. LORE (Logic ORiented Retriever Enhancement) introduces fine-grained contrastive learning to activate this latent capacity, guiding embeddings toward evidence aligned with logical structure rather than shallow similarity. LORE requires no external upervision, resources, or pre-retrieval analysis, remains index-compatible, and consistently improves retrieval utility and downstream generation while maintaining efficiency. The datasets and code are publicly available at https://github.com/mazehart/Lore-RAG.",
        "url": "http://arxiv.org/abs/2602.01116v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01116v1",
        "arxiv_id": "2602.01116v1",
        "authors": [
            "Wenxuan Zhang",
            "Yuan-Hao Jiang",
            "Changyong Qi",
            "Rui Jia",
            "Yonghe Wu"
        ],
        "submitted": "2026-02-01 09:30:04",
        "source": "arxiv",
        "comment": "accepted by icassp 2026",
        "score": 10,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper introduces a method to enhance retrievers using contrastive learning, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus is on logical analysis and its application to retrievers, which is not a central match to the user's core research themes. The paper's emphasis on large language models and logical structure also touches on NLP, but the connection to the user's interests is not strong enough to warrant a higher score."
    },
    {
        "title": "Unifying Adversarial Robustness and Training Across Text Scoring Models",
        "abstract": "Research on adversarial robustness in language models is currently fragmented across applications and attacks, obscuring shared vulnerabilities. In this work, we propose unifying the study of adversarial robustness in text scoring models spanning dense retrievers, rerankers, and reward models. This motivates adapting both attacks and adversarial training methods across model roles. Unlike open-ended generation, text scoring failures are directly testable: an attack succeeds when an irrelevant or rejected text outscores a relevant or chosen one. Using this principled lens of text scoring, we demonstrate that current adversarial training formulations for language models are often short-sighted, failing to effectively generalize across attacks. To address this, we introduce multiple adversarial training methods for text scoring models and show that combining complementary training methods can yield strong robustness while also improving task effectiveness. We also highlight the practical value of our approach for RLHF, showing that our adversarially trained reward models mitigate reward hacking and support the training of better-aligned LLMs. We provide our code and models for further study.",
        "url": "http://arxiv.org/abs/2602.00857v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00857v1",
        "arxiv_id": "2602.00857v1",
        "authors": [
            "Manveer Singh Tamber",
            "Hosna Oyarhoseini",
            "Jimmy Lin"
        ],
        "submitted": "2026-01-31 18:41:04",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores adversarial robustness in text scoring models, which is somewhat related to the user's interests in Information Retrieval and Natural Language Processing. However, the focus on adversarial training and robustness is not directly aligned with the user's primary research themes of query understanding, ranking models, and user behavior modeling. The paper's connection to the e-commerce domain is also not explicitly mentioned, which might limit its relevance to the user's background."
    },
    {
        "title": "Towards Sample-Efficient and Stable Reinforcement Learning for LLM-based Recommendation",
        "abstract": "While Long Chain-of-Thought (Long CoT) reasoning has shown promise in Large Language Models (LLMs), its adoption for enhancing recommendation quality is growing rapidly. In this work, we critically examine this trend and argue that Long CoT is inherently ill-suited for the sequential recommendation domain. We attribute this misalignment to two primary factors: excessive inference latency and the lack of explicit cognitive reasoning patterns in user behavioral data. Driven by these observations, we propose pivoting away from the CoT structure to directly leverage its underlying mechanism: Reinforcement Learning (RL), to explore the item space. However, applying RL directly faces significant obstacles, notably low sample efficiency-where most actions fail to provide learning signals-and training instability. To overcome these limitations, we propose RISER, a novel Reinforced Item Space Exploration framework for Recommendation. RISER is designed to transform non-learnable trajectories into effective pairwise preference data for optimization. Furthermore, it incorporates specific strategies to ensure stability, including the prevention of redundant rollouts and the constraint of token-level update magnitudes. Extensive experiments on three real-world datasets show that RISER significantly outperforms competitive baselines, establishing a robust paradigm for RL-enhanced LLM recommendation. Our code will be available at https://anonymous.4open.science/r/RISER/.",
        "url": "http://arxiv.org/abs/2602.00632v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00632v1",
        "arxiv_id": "2602.00632v1",
        "authors": [
            "Hongxun Ding",
            "Keqin Bao",
            "Jizhi Zhang",
            "Yi Fang",
            "Wenxin Xu",
            "Fuli Feng",
            "Xiangnan He"
        ],
        "submitted": "2026-01-31 10:02:43",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'user behavior' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper is somewhat related to the user's interests in Information Retrieval, particularly in the context of recommendation systems. However, the focus on Reinforcement Learning and Large Language Models for recommendation quality enhancement is not a central match for the user's primary interests in query understanding, ranking models, and user behavior modeling. The paper's emphasis on recommender systems and real-world datasets is somewhat relevant, but it does not align with the user's core research themes."
    },
    {
        "title": "Equity vs. Equality: Optimizing Ranking Fairness for Tailored Provider Needs",
        "abstract": "Ranking plays a central role in connecting users and providers in Information Retrieval (IR) systems, making provider-side fairness an important challenge. While recent research has begun to address fairness in ranking, most existing approaches adopt an equality-based perspective, aiming to ensure that providers with similar content receive similar exposure. However, it overlooks the diverse needs of real-world providers, whose utility from ranking may depend not only on exposure but also on outcomes like sales or engagement. Consequently, exposure-based fairness may not accurately capture the true utility perceived by different providers with varying priorities. To this end, we introduce an equity-oriented fairness framework that explicitly models each provider's preferences over key outcomes such as exposure and sales, thus evaluating whether a ranking algorithm can fulfill these individualized goals while maintaining overall fairness across providers. Based on this framework, we develop EquityRank, a gradient-based algorithm that jointly optimizes user-side effectiveness and provider-side equity. Extensive offline and online simulations demonstrate that EquityRank offers improved trade-offs between effectiveness and fairness and adapts to heterogeneous provider needs.",
        "url": "http://arxiv.org/abs/2602.00495v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00495v1",
        "arxiv_id": "2602.00495v1",
        "authors": [
            "Yiteng Tu",
            "Weihang Su",
            "Shuguang Han",
            "Yiqun Liu",
            "Qingyao Ai"
        ],
        "submitted": "2026-01-31 03:44:31",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in ranking models and fairness. The focus on equity-oriented fairness and provider-side equity aligns with your interests in query understanding and real-time relevance optimization. However, the specific domain of provider-side fairness in IR systems is not directly related to your e-commerce background."
    },
    {
        "title": "RAGRouter-Bench: A Dataset and Benchmark for Adaptive RAG Routing",
        "abstract": "Retrieval-Augmented Generation (RAG) has become a core paradigm for grounding large language models with external knowledge. Despite extensive efforts exploring diverse retrieval strategies, existing studies predominantly focus on query-side complexity or isolated method improvements, lacking a systematic understanding of how RAG paradigms behave across different query-corpus contexts and effectiveness-efficiency trade-offs. In this work, we introduce RAGRouter-Bench, the first dataset and benchmark designed for adaptive RAG routing. RAGRouter-Bench revisits retrieval from a query-corpus compatibility perspective and standardizes five representative RAG paradigms for systematic evaluation across 7,727 queries and 21,460 documents spanning diverse domains. The benchmark incorporates three canonical query types together with fine-grained semantic and structural corpus metrics, as well as a unified evaluation for both generation quality and resource consumption. Experiments with DeepSeek-V3 and LLaMA-3.1-8B demonstrate that no single RAG paradigm is universally optimal, that paradigm applicability is strongly shaped by query-corpus interactions, and that increased advanced mechanism does not necessarily yield better effectiveness-efficiency trade-offs. These findings underscore the necessity of routing-aware evaluation and establish a foundation for adaptive, interpretable, and generalizable next-generation RAG systems.",
        "url": "http://arxiv.org/abs/2602.00296v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00296v1",
        "arxiv_id": "2602.00296v1",
        "authors": [
            "Ziqi Wang",
            "Xi Zhu",
            "Shuhang Lin",
            "Haochen Xue",
            "Minghao Guo",
            "Yongfeng Zhang"
        ],
        "submitted": "2026-01-30 20:38:11",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper is somewhat related to information retrieval, specifically focusing on Retrieval-Augmented Generation (RAG) paradigms. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's focus on adaptive routing and effectiveness-efficiency trade-offs is somewhat tangential to the user's primary research themes."
    },
    {
        "title": "Typologically-Informed Candidate Reranking for LLM-based Translation into Low-Resource Languages",
        "abstract": "Large language models trained predominantly on high-resource languages exhibit systematic biases toward dominant typological patterns, leading to structural non-conformance when translating into typologically divergent low-resource languages. We present a framework that leverages linguistic typology to improve translation quality without parallel training data or model retraining. The framework consists of two components: the Universal Metalinguistic Framework (UMF), which represents languages as structured profiles across 16 typological dimensions with divergence-weighted scoring, and the Computational Engine, which operates through linguistic disambiguation during generation and typological compliance scoring during selection. Evaluation across nine language pairs demonstrates intervention rates strongly correlating with typological distance from English. In experiments on 341 English sentences each having different morphological and syntactic phenomena, the framework shows an intervention precision of 48.16% for conservatively treated languages, 28.15% for morphologically dense languages, and 86.26% for structurally profiled languages. The framework requires no parallel training data and operates with any LLM capable of producing multiple candidate outputs, enabling practical deployment for under-resourced languages.",
        "url": "http://arxiv.org/abs/2602.01162v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01162v1",
        "arxiv_id": "2602.01162v1",
        "authors": [
            "Nipuna Abeykoon",
            "Ashen Weerathunga",
            "Pubudu Wijesinghe",
            "Parameswari Krishnamurthy"
        ],
        "submitted": "2026-02-01 11:22:30",
        "source": "arxiv",
        "comment": null,
        "score": 9,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on machine translation and linguistic typology, which is somewhat related to information retrieval and natural language processing. However, the specific context and application (machine translation) are quite different from the user's core research themes, and the paper does not appear to address query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Optimizing Retrieval Components for a Shared Backbone via Component-Wise Multi-Stage Training",
        "abstract": "Recent advances in embedding-based retrieval have enabled dense retrievers to serve as core infrastructure in many industrial systems, where a single retrieval backbone is often shared across multiple downstream applications. In such settings, retrieval quality directly constrains system performance and extensibility, while coupling model selection, deployment, and rollback decisions across applications.\n  In this paper, we present empirical findings and a system-level solution for optimizing retrieval components deployed as a shared backbone in production legal retrieval systems. We adopt a multi-stage optimization framework for dense retrievers and rerankers, and show that different retrieval components exhibit stage-dependent trade-offs. These observations motivate a component-wise, mixed-stage configuration rather than relying on a single uniformly optimal checkpoint. The resulting backbone is validated through end-to-end evaluation and deployed as a shared retrieval service supporting multiple industrial applications.",
        "url": "http://arxiv.org/abs/2602.00805v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00805v1",
        "arxiv_id": "2602.00805v1",
        "authors": [
            "Yunhan Li",
            "Mingjie Xie",
            "Zihan Gong",
            "Zeyang Shi",
            "Gengshen Wu",
            "Min Yang"
        ],
        "submitted": "2026-01-31 16:29:32",
        "source": "arxiv",
        "comment": "4 pages, 3 figures, 3 tables",
        "score": 9,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper focuses on optimizing retrieval components for a shared backbone, which is somewhat related to information retrieval and search technologies. However, the specific context of production legal retrieval systems and the emphasis on dense retrievers and rerankers do not directly align with the user's core research themes of query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Beyond RAG for Agent Memory: Retrieval by Decoupling and Aggregation",
        "abstract": "Agent memory systems often adopt the standard Retrieval-Augmented Generation (RAG) pipeline, yet its underlying assumptions differ in this setting. RAG targets large, heterogeneous corpora where retrieved passages are diverse, whereas agent memory is a bounded, coherent dialogue stream with highly correlated spans that are often duplicates. Under this shift, fixed top-$k$ similarity retrieval tends to return redundant context, and post-hoc pruning can delete temporally linked prerequisites needed for correct reasoning. We argue retrieval should move beyond similarity matching and instead operate over latent components, following decoupling to aggregation: disentangle memories into semantic components, organise them into a hierarchy, and use this structure to drive retrieval. We propose xMemory, which builds a hierarchy of intact units and maintains a searchable yet faithful high-level node organisation via a sparsity--semantics objective that guides memory split and merge. At inference, xMemory retrieves top-down, selecting a compact, diverse set of themes and semantics for multi-fact queries, and expanding to episodes and raw messages only when it reduces the reader's uncertainty. Experiments on LoCoMo and PerLTQA across the three latest LLMs show consistent gains in answer quality and token efficiency.",
        "url": "http://arxiv.org/abs/2602.02007v1",
        "pdf_url": "https://arxiv.org/pdf/2602.02007v1",
        "arxiv_id": "2602.02007v1",
        "authors": [
            "Zhanghao Hu",
            "Qinglin Zhu",
            "Hanqi Yan",
            "Yulan He",
            "Lin Gui"
        ],
        "submitted": "2026-02-02 12:04:58",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores a novel approach to agent memory systems, proposing a method called xMemory that decouples and aggregates memories into semantic components. While it touches on retrieval and ranking, its primary focus is on memory organization and retrieval in a dialogue setting, which is somewhat related to your interests in query understanding and ranking models. However, the paper's emphasis on dialogue memory and semantic components is not a central match for your core research themes in information retrieval and search technologies."
    },
    {
        "title": "Read As Human: Compressing Context via Parallelizable Close Reading and Skimming",
        "abstract": "Large Language Models (LLMs) demonstrate exceptional capability across diverse tasks. However, their deployment in long-context scenarios is hindered by two challenges: computational inefficiency and redundant information. We propose RAM (Read As HuMan), a context compression framework that adopts an adaptive hybrid reading strategy, to address these challenges. Inspired by human reading behavior (i.e., close reading important content while skimming less relevant content), RAM partitions the context into segments and encodes them with the input query in parallel. High-relevance segments are fully retained (close reading), while low-relevance ones are query-guided compressed into compact summary vectors (skimming). Both explicit textual segments and implicit summary vectors are concatenated and fed into decoder to achieve both superior performance and natural language format interpretability. To refine the decision boundary between close reading and skimming, we further introduce a contrastive learning objective based on positive and negative query-segment pairs. Experiments demonstrate that RAM outperforms existing baselines on multiple question answering and summarization benchmarks across two backbones, while delivering up to a 12x end-to-end speedup on long inputs (average length 16K; maximum length 32K).",
        "url": "http://arxiv.org/abs/2602.01840v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01840v1",
        "arxiv_id": "2602.01840v1",
        "authors": [
            "Jiwei Tang",
            "Shilei Liu",
            "Zhicheng Zhang",
            "Qingsong Lv",
            "Runsong Zhao",
            "Tingwei Lu",
            "Langming Liu",
            "Haibin Chen",
            "Yujin Yuan",
            "Hai-Tao Zheng",
            "Wenbo Su",
            "Bo Zheng"
        ],
        "submitted": "2026-02-02 09:10:56",
        "source": "arxiv",
        "comment": "13 pages,5 figures",
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores a context compression framework for Large Language Models, which is somewhat related to information retrieval and natural language processing. However, the focus on human reading behavior and context compression does not directly align with the user's primary interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Scaling Search-Augmented LLM Reasoning via Adaptive Information Control",
        "abstract": "Search-augmented reasoning agents interleave multi-step reasoning with external information retrieval, but uncontrolled retrieval often leads to redundant evidence, context saturation, and unstable learning. Existing approaches rely on outcome-based reinforcement learning (RL), which provides limited guidance for regulating information acquisition. We propose DeepControl, a framework for adaptive information control based on a formal notion of information utility, which measures the marginal value of retrieved evidence under a given reasoning state. Building on this utility, we introduce retrieval continuation and granularity control mechanisms that selectively regulate when to continue and stop retrieval, and how much information to expand. An annealed control strategy enables the agent to internalize effective information acquisition behaviors during training. Extensive experiments across seven benchmarks demonstrate that our method consistently outperforms strong baselines. In particular, our approach achieves average performance improvements of 9.4% and 8.6% on Qwen2.5-7B and Qwen2.5-3B, respectively, over strong outcome-based RL baselines, and consistently outperforms both retrieval-free and retrieval-based reasoning methods without explicit information control. These results highlight the importance of adaptive information control for scaling search-augmented reasoning agents to complex, real-world information environments.",
        "url": "http://arxiv.org/abs/2602.01672v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01672v1",
        "arxiv_id": "2602.01672v1",
        "authors": [
            "Siheng Xiong",
            "Oguzhan Gungordu",
            "Blair Johnson",
            "James C. Kerce",
            "Faramarz Fekri"
        ],
        "submitted": "2026-02-02 05:40:38",
        "source": "arxiv",
        "comment": "Work in progress",
        "score": 8,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of search-augmented reasoning and adaptive information control. The proposed framework, DeepControl, addresses the challenges of redundant evidence and context saturation, which is a key area of focus in your research. The paper's emphasis on real-time relevance optimization and its experimental results on various benchmarks make it a useful contribution to the field."
    },
    {
        "title": "A-MapReduce: Executing Wide Search via Agentic MapReduce",
        "abstract": "Contemporary large language model (LLM)-based multi-agent systems exhibit systematic advantages in deep research tasks, which emphasize iterative, vertically structured information seeking. However, when confronted with wide search tasks characterized by large-scale, breadth-oriented retrieval, existing agentic frameworks, primarily designed around sequential, vertically structured reasoning, remain stuck in expansive search objectives and inefficient long-horizon execution. To bridge this gap, we propose A-MapReduce, a MapReduce paradigm-inspired multi-agent execution framework that recasts wide search as a horizontally structured retrieval problem. Concretely, A-MapReduce implements parallel processing of massive retrieval targets through task-adaptive decomposition and structured result aggregation. Meanwhile, it leverages experiential memory to drive the continual evolution of query-conditioned task allocation and recomposition, enabling progressive improvement in large-scale wide-search regimes. Extensive experiments on five agentic benchmarks demonstrate that A-MapReduce is (i) high-performing, achieving state-of-the-art performance on WideSearch and DeepWideSearch, and delivering 5.11% - 17.50% average Item F1 improvements compared with strong baselines with OpenAI o3 or Gemini 2.5 Pro backbones; (ii) cost-effective and efficient, delivering superior cost-performance trade-offs and reducing running time by 45.8\\% compared to representative multi-agent baselines. The code is available at https://github.com/mingju-c/AMapReduce.",
        "url": "http://arxiv.org/abs/2602.01331v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01331v1",
        "arxiv_id": "2602.01331v1",
        "authors": [
            "Mingju Chen",
            "Guibin Zhang",
            "Heng Chang",
            "Yuchen Guo",
            "Shiji Zhou"
        ],
        "submitted": "2026-02-01 16:53:29",
        "source": "arxiv",
        "comment": "33 pages",
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper proposes a novel framework for wide search tasks, leveraging multi-agent systems and MapReduce paradigm. While it touches on information retrieval and search technologies, its primary focus is on the execution framework rather than query understanding, ranking models, or user behavior modeling. The connection to information retrieval is somewhat loose, but the paper's emphasis on scalability and efficiency might be of interest to researchers in the field."
    },
    {
        "title": "EffGen: Enabling Small Language Models as Capable Autonomous Agents",
        "abstract": "Most existing language model agentic systems today are built and optimized for large language models (e.g., GPT, Claude, Gemini) via API calls. While powerful, this approach faces several limitations including high token costs and privacy concerns for sensitive applications. We introduce effGen, an open-source agentic framework optimized for small language models (SLMs) that enables effective, efficient, and secure local deployment (pip install effgen). effGen makes four major contributions: (1) Enhanced tool-calling with prompt optimization that compresses contexts by 70-80% while preserving task semantics, (2) Intelligent task decomposition that breaks complex queries into parallel or sequential subtasks based on dependencies, (3) Complexity-based routing using five factors to make smart pre-execution decisions, and (4) Unified memory system combining short-term, long-term, and vector-based storage. Additionally, effGen unifies multiple agent protocols (MCP, A2A, ACP) for cross-protocol communication. Results on 13 benchmarks show effGen outperforms LangChain, AutoGen, and Smolagents with higher success rates, faster execution, and lower memory. Our results reveal that prompt optimization and complexity routing have complementary scaling behavior: optimization benefits SLMs more (11.2% gain at 1.5B vs 2.4% at 32B), while routing benefits large models more (3.6% at 1.5B vs 7.9% at 32B), providing consistent gains across all scales when combined. effGen (https://effgen.org/) is released under the MIT License, ensuring broad accessibility for research and commercial use. Our framework code is publicly available at https://github.com/ctrl-gaurav/effGen.",
        "url": "http://arxiv.org/abs/2602.00887v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00887v1",
        "arxiv_id": "2602.00887v1",
        "authors": [
            "Gaurav Srivastava",
            "Aafiya Hussain",
            "Chi Wang",
            "Yingyan Celine Lin",
            "Xuan Wang"
        ],
        "submitted": "2026-01-31 20:24:56",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'ctr' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses a framework for small language models, which is somewhat related to the user's interest in NLP. However, the focus on language model agentic systems and their deployment does not directly align with the user's primary research themes in Information Retrieval and Search technologies. The paper's relevance is limited to the user's secondary interest in NLP."
    },
    {
        "title": "Zero2Text: Zero-Training Cross-Domain Inversion Attacks on Textual Embeddings",
        "abstract": "The proliferation of retrieval-augmented generation (RAG) has established vector databases as critical infrastructure, yet they introduce severe privacy risks via embedding inversion attacks. Existing paradigms face a fundamental trade-off: optimization-based methods require computationally prohibitive queries, while alignment-based approaches hinge on the unrealistic assumption of accessible in-domain training data. These constraints render them ineffective in strict black-box and cross-domain settings. To dismantle these barriers, we introduce Zero2Text, a novel training-free framework based on recursive online alignment. Unlike methods relying on static datasets, Zero2Text synergizes LLM priors with a dynamic ridge regression mechanism to iteratively align generation to the target embedding on-the-fly. We further demonstrate that standard defenses, such as differential privacy, fail to effectively mitigate this adaptive threat. Extensive experiments across diverse benchmarks validate Zero2Text; notably, on MS MARCO against the OpenAI victim model, it achieves 1.8x higher ROUGE-L and 6.4x higher BLEU-2 scores compared to baselines, recovering sentences from unknown domains without a single leaked data pair.",
        "url": "http://arxiv.org/abs/2602.01757v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01757v1",
        "arxiv_id": "2602.01757v1",
        "authors": [
            "Doohyun Kim",
            "Donghwa Kang",
            "Kyungjae Lee",
            "Hyeongboo Baek",
            "Brent Byunghoon Kang"
        ],
        "submitted": "2026-02-02 07:42:18",
        "source": "arxiv",
        "comment": "10 pages",
        "score": 7,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be primarily focused on a security threat in vector databases used in retrieval-augmented generation, which is not directly related to the user's core research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling. While it touches on the use of vector databases, the context is security and privacy, rather than improving search or retrieval capabilities."
    },
    {
        "title": "EverMemBench: Benchmarking Long-Term Interactive Memory in Large Language ModelsEverMemBench: Benchmarking Long-Term Interactive Memory in Large Language Models",
        "abstract": "Long-term conversational memory is essential for LLM-based assistants, yet existing benchmarks focus on dyadic, single-topic dialogues that fail to capture real-world complexity. We introduce EverMemBench, a benchmark featuring multi-party, multi-group conversations spanning over 1 million tokens with temporally evolving information, cross-topic interleaving, and role-specific personas. EverMemBench evaluates memory systems across three dimensions through 1,000+ QA pairs: fine-grained recall, memory awareness, and user profile understanding. Our evaluation reveals critical limitations: (1) multi-hop reasoning collapses in multi-party settings, with even oracle models achieving only 26%; (2) temporal reasoning remains unsolved, requiring version semantics beyond timestamp matching; (3) memory awareness is bottlenecked by retrieval, where current similarity-based methods fail to bridge the semantic gap between queries and implicitly relevant memories. EverMemBench provides a challenging testbed for developing next-generation memory architectures.",
        "url": "http://arxiv.org/abs/2602.01313v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01313v1",
        "arxiv_id": "2602.01313v1",
        "authors": [
            "Chuanrui Hu",
            "Tong Li",
            "Xingze Gao",
            "Hongda Chen",
            "Dannong Xu",
            "Yi Bai",
            "Tianwei Lin",
            "Xinda Zhao",
            "Xiaohong Li",
            "Jiaqi An",
            "Yunyun Han",
            "Jian Pei",
            "Yafeng Deng"
        ],
        "submitted": "2026-02-01 16:13:08",
        "source": "arxiv",
        "comment": "10 pages, 2 figures, 4 tables",
        "score": 7,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper introduces a benchmark for long-term conversational memory in large language models, which is somewhat related to information retrieval and search technologies. However, the focus on conversational memory and dialogue systems is not a central match to the user's primary research interests in query understanding, ranking models, and user behavior modeling. The paper's relevance to natural language processing and data mining is more notable."
    },
    {
        "title": "What If We Allocate Test-Time Compute Adaptively?",
        "abstract": "Test-time compute scaling allocates inference computation uniformly, uses fixed sampling strategies, and applies verification only for reranking. In contrast, we propose a verifier-guided adaptive framework treating reasoning as iterative trajectory generation and selection. For each problem, the agent runs multiple inference iterations. In each iteration, it optionally produces a high-level plan, selects a set of reasoning tools and a compute strategy together with an exploration parameter, and then generates a candidate reasoning trajectory. A process reward model (PRM) serves as a unified control signal: within each iteration, step-level PRM scores are aggregated to guide pruning and expansion during generation, and across iterations, aggregated trajectory rewards are used to select the final response. Across datasets, our dynamic, PRM-guided approach consistently outperforms direct test-time scaling, yielding large gains on MATH-500 and several-fold improvements on harder benchmarks such as AIME24 and AMO-Bench. We characterize efficiency using theoretical FLOPs and a compute intensity metric penalizing wasted generation and tool overhead, demonstrating that verification-guided allocation concentrates computation on high-utility reasoning paths.",
        "url": "http://arxiv.org/abs/2602.01070v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01070v1",
        "arxiv_id": "2602.01070v1",
        "authors": [
            "Ahsan Bilal",
            "Ahmed Mohsin",
            "Muhammad Umer",
            "Ali Subhan",
            "Hassan Rizwan",
            "Ayesha Mohsin",
            "Dean Hougen"
        ],
        "submitted": "2026-02-01 07:30:22",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be focused on test-time compute scaling and adaptive framework for reasoning trajectory generation, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some form of optimization, the context is more aligned with computer architecture and efficiency rather than semantic understanding and relevance optimization."
    },
    {
        "title": "DIVERGE: Diversity-Enhanced RAG for Open-Ended Information Seeking",
        "abstract": "Existing retrieval-augmented generation (RAG) systems are primarily designed under the assumption that each query has a single correct answer. This overlooks common information-seeking scenarios with multiple plausible answers, where diversity is essential to avoid collapsing to a single dominant response, thereby constraining creativity and compromising fair and inclusive information access. Our analysis reveals a commonly overlooked limitation of standard RAG systems: they underutilize retrieved context diversity, such that increasing retrieval diversity alone does not yield diverse generations. To address this limitation, we propose DIVERGE, a plug-and-play agentic RAG framework with novel reflection-guided generation and memory-augmented iterative refinement, which promotes diverse viewpoints while preserving answer quality. We introduce novel metrics tailored to evaluating the diversity-quality trade-off in open-ended questions, and show that they correlate well with human judgments. We demonstrate that DIVERGE achieves the best diversity-quality trade-off compared to competitive baselines and previous state-of-the-art methods on the real-world Infinity-Chat dataset, substantially improving diversity while maintaining quality. More broadly, our results reveal a systematic limitation of current LLM-based systems for open-ended information-seeking and show that explicitly modeling diversity can mitigate it. Our code is available at: https://github.com/au-clan/Diverge",
        "url": "http://arxiv.org/abs/2602.00238v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00238v1",
        "arxiv_id": "2602.00238v1",
        "authors": [
            "Tianyi Hu",
            "Niket Tandon",
            "Akhil Arora"
        ],
        "submitted": "2026-01-30 19:03:11",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The proposed DIVERGE framework addresses a limitation in current retrieval-augmented generation systems, promoting diverse viewpoints while preserving answer quality. Although the focus is on open-ended information seeking, the work's emphasis on diversity and quality trade-off aligns with your interests in real-time relevance optimization."
    },
    {
        "title": "COMI: Coarse-to-fine Context Compression via Marginal Information Gain",
        "abstract": "Large Language Models (LLMs) have demonstrated exceptional capabilities across diverse tasks. However, their deployment in long context scenarios remains hindered by computational inefficiency and information redundancy. Context compression methods address these challenges by significantly reducing input length and eliminating redundancy. We propose COMI, a coarse-to-fine adaptive context compression framework that jointly optimizes for semantic relevance and diversity under high compression rates. We introduce Marginal Information Gain (MIG), a metric defined as the relevance of a unit to the input query minus its semantic redundancy with other units, guiding the compression process to prioritize information that is both relevant and low redundant. The framework operates in two stages: (1) Coarse-Grained Group Reallocation, where the context is partitioned into groups and dynamically assigned compression rates based on inter-group MIG, ensuring compression budgets align with information value distribution; and (2) Fine-Grained Token Merging, where tokens within each group are fused via an intra-group MIG-based weighting mechanism, thereby preserving key semantics while avoiding the accumulation of redundancy. Extensive experiments across question-answering (e.g., NaturalQuestions, 2WikiMQA, HotpotQA and NarrativeQA), summarization (e.g., MultiNews) with various backbones (e.g., LLaMA-2-7B, Qwen2-7B) show that COMI outperforms existing baselines by a large margin, e.g., approximately 25-point Exact Match (EM) improvement under 32x compression constraint with Qwen2-7B on NaturalQuestions.",
        "url": "http://arxiv.org/abs/2602.01719v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01719v1",
        "arxiv_id": "2602.01719v1",
        "authors": [
            "Jiwei Tang",
            "Shilei Liu",
            "Zhicheng Zhang",
            "Yujin Yuan",
            "Libin Zheng",
            "Wenbo Su",
            "Bo Zheng"
        ],
        "submitted": "2026-02-02 06:57:22",
        "source": "arxiv",
        "comment": "Accepted at ICLR 2026",
        "score": 6,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper proposes a novel context compression framework, COMI, which is relevant to information retrieval and natural language processing. The framework's focus on semantic relevance and diversity under high compression rates aligns with your interests in query understanding and ranking models. However, the paper's primary focus on large language models and context compression may not be a central match for your research themes."
    },
    {
        "title": "A2Eval: Agentic and Automated Evaluation for Embodied Brain",
        "abstract": "Current embodied VLM evaluation relies on static, expert-defined, manually annotated benchmarks that exhibit severe redundancy and coverage imbalance. This labor intensive paradigm drains computational and annotation resources, inflates costs, and distorts model rankings, ultimately stifling iterative development. To address this, we propose Agentic Automatic Evaluation (A2Eval), the first agentic framework that automates benchmark curation and evaluation through two collaborative agents. The Data Agent autonomously induces capability dimensions and assembles a balanced, compact evaluation suite, while the Eval Agent synthesizes and validates executable evaluation pipelines, enabling fully autonomous, high-fidelity assessment. Evaluated across 10 benchmarks and 13 models, A2Eval compresses evaluation suites by 85%, reduces overall computational costs by 77%, and delivers a 4.6x speedup while preserving evaluation quality. Crucially, A2Eval corrects systematic ranking biases, improves human alignment to Spearman's rho=0.85, and maintains high ranking fidelity (Kendall's tau=0.81), establishing a new standard for high-fidelity, low-cost embodied assessment. Our code and data will be public soon.",
        "url": "http://arxiv.org/abs/2602.01640v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01640v1",
        "arxiv_id": "2602.01640v1",
        "authors": [
            "Shuai Zhang",
            "Jiayu Hu",
            "Zijie Chen",
            "Zeyuan Ding",
            "Yi Zhang",
            "Yingji Zhang",
            "Ziyi Zhou",
            "Junwei Liao",
            "Shengjie Zhou",
            "Yong Dai",
            "Zhenzhong Lan",
            "Xiaozhu Ju"
        ],
        "submitted": "2026-02-02 04:55:27",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be focused on embodied brain evaluation, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some form of evaluation, the context and methodology seem to be quite different from the user's areas of interest."
    },
    {
        "title": "PolySAE: Modeling Feature Interactions in Sparse Autoencoders via Polynomial Decoding",
        "abstract": "Sparse autoencoders (SAEs) have emerged as a promising method for interpreting neural network representations by decomposing activations into sparse combinations of dictionary atoms. However, SAEs assume that features combine additively through linear reconstruction, an assumption that cannot capture compositional structure: linear models cannot distinguish whether \"Starbucks\" arises from the composition of \"star\" and \"coffee\" features or merely their co-occurrence. This forces SAEs to allocate monolithic features for compound concepts rather than decomposing them into interpretable constituents. We introduce PolySAE, which extends the SAE decoder with higher-order terms to model feature interactions while preserving the linear encoder essential for interpretability. Through low-rank tensor factorization on a shared projection subspace, PolySAE captures pairwise and triple feature interactions with small parameter overhead (3% on GPT2). Across four language models and three SAE variants, PolySAE achieves an average improvement of approximately 8% in probing F1 while maintaining comparable reconstruction error, and produces 2-10$\\times$ larger Wasserstein distances between class-conditional feature distributions. Critically, learned interaction weights exhibit negligible correlation with co-occurrence frequency ($r = 0.06$ vs. $r = 0.82$ for SAE feature covariance), suggesting that polynomial terms capture compositional structure, such as morphological binding and phrasal composition, largely independent of surface statistics.",
        "url": "http://arxiv.org/abs/2602.01322v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01322v1",
        "arxiv_id": "2602.01322v1",
        "authors": [
            "Panagiotis Koromilas",
            "Andreas D. Demou",
            "James Oldfield",
            "Yannis Panagakis",
            "Mihalis Nicolaou"
        ],
        "submitted": "2026-02-01 16:34:45",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on improving sparse autoencoders for feature interpretation, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve language models, the primary goal is to improve interpretability of neural network representations, which is not a central theme in your research."
    },
    {
        "title": "ILSIC: Corpora for Identifying Indian Legal Statutes from Queries by Laypeople",
        "abstract": "Legal Statute Identification (LSI) for a given situation is one of the most fundamental tasks in Legal NLP. This task has traditionally been modeled using facts from court judgments as input queries, due to their abundance. However, in practical settings, the input queries are likely to be informal and asked by laypersons, or non-professionals. While a few laypeople LSI datasets exist, there has been little research to explore the differences between court and laypeople data for LSI. In this work, we create ILSIC, a corpus of laypeople queries covering 500+ statutes from Indian law. Additionally, the corpus also contains court case judgements to enable researchers to effectively compare between court and laypeople data for LSI. We conducted extensive experiments on our corpus, including benchmarking over the laypeople dataset using zero and few-shot inference, retrieval-augmented generation and supervised fine-tuning. We observe that models trained purely on court judgements are ineffective during test on laypeople queries, while transfer learning from court to laypeople data can be beneficial in certain scenarios. We also conducted fine-grained analyses of our results in terms of categories of queries and frequency of statutes.",
        "url": "http://arxiv.org/abs/2602.00881v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00881v1",
        "arxiv_id": "2602.00881v1",
        "authors": [
            "Shounak Paul",
            "Raghav Dogra",
            "Pawan Goyal",
            "Saptarshi Ghosh"
        ],
        "submitted": "2026-01-31 20:05:48",
        "source": "arxiv",
        "comment": "9 Pages of Main, 1 page of Limitations and Ethics Statement, 11 Pages of Appendix, Accepted for Publication at EACL 2026 (Findings)",
        "score": 6,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Legal NLP and the creation of a corpus for identifying Indian legal statutes from queries by laypeople, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves query understanding and NLP, the context and application are quite different from the user's areas of focus."
    },
    {
        "title": "DecompressionLM: Deterministic, Diagnostic, and Zero-Shot Concept Graph Extraction from Language Models",
        "abstract": "Existing knowledge probing methods rely on pre-defined queries, limiting extraction to known concepts. We introduce DecompressionLM, a stateless framework for zero-shot concept graph extraction that discovers what language models encode without pre-specified queries or shared cross-sequence state. Our method targets three limitations of common decoding-based probing approaches: cross-sequence coupling that concentrates probability mass on high-frequency prefixes, competitive decoding effects that suppress long-tail concepts, and scalability constraints arising from sequential exploration. Using Van der Corput low-discrepancy sequences with arithmetic decoding, DecompressionLM enables deterministic, embarrassingly parallel generation without shared state across sequences. Across two model families and five quantization variants, we find that activation-aware quantization (AWQ-4bit) expands concept coverage by 30-170%, while uniform quantization (GPTQ-Int4) induces 71-86% coverage collapse -- divergent behaviors not reliably reflected by explanation-level perplexity. Corpus-based verification further reveals a 17-point hallucination gap between top- and bottom-ranked MMLU-Pro Law models. DecompressionLM establishes concept coverage as a complementary evaluation dimension for assessing knowledge breadth and factual grounding in compressed models useful for their deployment.",
        "url": "http://arxiv.org/abs/2602.00377v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00377v1",
        "arxiv_id": "2602.00377v1",
        "authors": [
            "Zhaochen Hong",
            "Jiaxuan You"
        ],
        "submitted": "2026-01-30 22:56:56",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper introduces DecompressionLM, a framework for zero-shot concept graph extraction from language models. While it touches on knowledge probing and language models, which are related to NLP, it doesn't directly address information retrieval, query understanding, or ranking models, making it somewhat relevant but not a central match for your research interests."
    },
    {
        "title": "DETOUR: An Interactive Benchmark for Dual-Agent Search and Reasoning",
        "abstract": "When recalling information in conversation, people often arrive at the recollection after multiple turns. However, existing benchmarks for evaluating agent capabilities in such tip-of-the-tongue search processes are restricted to single-turn settings. To more realistically simulate tip-of-the-tongue search, we introduce Dual-agent based Evaluation Through Obscure Under-specified Retrieval (DETOUR), a dual-agent evaluation benchmark containing 1,011 prompts. The benchmark design involves a Primary Agent, which is the subject of evaluation, tasked with identifying the recollected entity through querying a Memory Agent that is held consistent across evaluations. Our results indicate that current state-of-the-art models still struggle with our benchmark, only achieving 36% accuracy when evaluated on all modalities (text, image, audio, and video), highlighting the importance of enhancing capabilities in underspecified scenarios.",
        "url": "http://arxiv.org/abs/2602.00352v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00352v1",
        "arxiv_id": "2602.00352v1",
        "authors": [
            "Li Siyan",
            "Darshan Deshpande",
            "Anand Kannappan",
            "Rebecca Qian"
        ],
        "submitted": "2026-01-30 22:01:30",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on a dual-agent evaluation benchmark for tip-of-the-tongue search processes, which is not directly related to your core research themes in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. While it involves querying and retrieval, the context and application are quite different from your areas of interest."
    },
    {
        "title": "MiNER: A Two-Stage Pipeline for Metadata Extraction from Municipal Meeting Minutes",
        "abstract": "Municipal meeting minutes are official documents of local governance, exhibiting heterogeneous formats and writing styles. Effective information retrieval (IR) requires identifying metadata such as meeting number, date, location, participants, and start/end times, elements that are rarely standardized or easy to extract automatically. Existing named entity recognition (NER) models are ill-suited to this task, as they are not adapted to such domain-specific categories. In this paper, we propose a two-stage pipeline for metadata extraction from municipal minutes. First, a question answering (QA) model identifies the opening and closing text segments containing metadata. Transformer-based models (BERTimbau and XLM-RoBERTa with and without a CRF layer) are then applied for fine-grained entity extraction and enhanced through deslexicalization. To evaluate our proposed pipeline, we benchmark both open-weight (Phi) and closed-weight (Gemini) LLMs, assessing predictive performance, inference cost, and carbon footprint. Our results demonstrate strong in-domain performance, better than larger general-purpose LLMs. However, cross-municipality evaluation reveals reduced generalization reflecting the variability and linguistic complexity of municipal records. This work establishes the first benchmark for metadata extraction from municipal meeting minutes, providing a solid foundation for future research in this domain.",
        "url": "http://arxiv.org/abs/2602.00316v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00316v1",
        "arxiv_id": "2602.00316v1",
        "authors": [
            "Rodrigo Batista",
            "Lus Filipe Cunha",
            "Purificao Silvano",
            "Nuno Guimares",
            "Alpio Jorge",
            "Evelin Amorim",
            "Ricardo Campos"
        ],
        "submitted": "2026-01-30 21:09:13",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of metadata extraction and question answering models. However, it focuses on a specific domain (municipal meeting minutes) and doesn't directly address query understanding, ranking models, or user behavior modeling, which are your primary areas of interest."
    },
    {
        "title": "ES-MemEval: Benchmarking Conversational Agents on Personalized Long-Term Emotional Support",
        "abstract": "Large Language Models (LLMs) have shown strong potential as conversational agents. Yet, their effectiveness remains limited by deficiencies in robust long-term memory, particularly in complex, long-term web-based services such as online emotional support. However, existing long-term dialogue benchmarks primarily focus on static and explicit fact retrieval, failing to evaluate agents in critical scenarios where user information is dispersed, implicit, and continuously evolving. To address this gap, we introduce ES-MemEval, a comprehensive benchmark that systematically evaluates five core memory capabilities: information extraction, temporal reasoning, conflict detection, abstention, and user modeling, in long-term emotional support settings, covering question answering, summarization, and dialogue generation tasks. To support the benchmark, we also propose EvoEmo, a multi-session dataset for personalized long-term emotional support that captures fragmented, implicit user disclosures and evolving user states. Extensive experiments on open-source long-context, commercial, and retrieval-augmented (RAG) LLMs show that explicit long-term memory is essential for reducing hallucinations and enabling effective personalization. At the same time, RAG improves factual consistency but struggles with temporal dynamics and evolving user states. These findings highlight both the potential and limitations of current paradigms and motivate more robust integration of memory and retrieval for long-term personalized dialogue systems.",
        "url": "http://arxiv.org/abs/2602.01885v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01885v1",
        "arxiv_id": "2602.01885v1",
        "authors": [
            "Tiantian Chen",
            "Jiaqi Lu",
            "Ying Shen",
            "Lin Zhang"
        ],
        "submitted": "2026-02-02 09:58:26",
        "source": "arxiv",
        "comment": "12 pages, 7 figures. Accepted to The Web Conference (WWW) 2026",
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'personalization' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores conversational agents and long-term memory capabilities, which is somewhat related to information retrieval and user behavior modeling. However, the focus on emotional support and dialogue systems is not a central match to the user's primary research interests in query understanding, ranking models, and real-time relevance optimization."
    },
    {
        "title": "The Algorithmic Self-Portrait: Deconstructing Memory in ChatGPT",
        "abstract": "To enable personalized and context-aware interactions, conversational AI systems have introduced a new mechanism: Memory. Memory creates what we refer to as the Algorithmic Self-portrait - a new form of personalization derived from users' self-disclosed information divulged within private conversations. While memory enables more coherent exchanges, the underlying processes of memory creation remain opaque, raising critical questions about data sensitivity, user agency, and the fidelity of the resulting portrait.\n  To bridge this research gap, we analyze 2,050 memory entries from 80 real-world ChatGPT users. Our analyses reveal three key findings: (1) A striking 96% of memories in our dataset are created unilaterally by the conversational system, potentially shifting agency away from the user; (2) Memories, in our dataset, contain a rich mix of GDPR-defined personal data (in 28% memories) along with psychological insights about participants (in 52% memories); and (3)~A significant majority of the memories (84%) are directly grounded in user context, indicating faithful representation of the conversations. Finally, we introduce a framework-Attribution Shield-that anticipates these inferences, alerts about potentially sensitive memory inferences, and suggests query reformulations to protect personal information without sacrificing utility.",
        "url": "http://arxiv.org/abs/2602.01450v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01450v1",
        "arxiv_id": "2602.01450v1",
        "authors": [
            "Abhisek Dash",
            "Soumi Das",
            "Elisabeth Kirsten",
            "Qinyuan Wu",
            "Sai Keerthana Karnam",
            "Krishna P. Gummadi",
            "Thorsten Holz",
            "Muhammad Bilal Zafar",
            "Savvas Zannettou"
        ],
        "submitted": "2026-02-01 21:39:36",
        "source": "arxiv",
        "comment": "This paper has been accepted at The ACM Web Conference 2026",
        "score": 5,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'personalization' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on conversational AI systems and memory creation, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the context is more centered on user data sensitivity and conversational AI, making it less relevant to the user's primary research themes."
    },
    {
        "title": "SpeechLess: Micro-utterance with Personalized Spatial Memory-aware Assistant in Everyday Augmented Reality",
        "abstract": "Speaking aloud to a wearable AR assistant in public can be socially awkward, and re-articulating the same requests every day creates unnecessary effort. We present SpeechLess, a wearable AR assistant that introduces a speech-based intent granularity control paradigm grounded in personalized spatial memory. SpeechLess helps users \"speak less,\" while still obtaining the information they need, and supports gradual explicitation of intent when more complex expression is required. SpeechLess binds prior interactions to multimodal personal context-space, time, activity, and referents-to form spatial memories, and leverages them to extrapolate missing intent dimensions from under-specified user queries. This enables users to dynamically adjust how explicitly they express their informational needs, from full-utterance to micro/zero-utterance interaction. We motivate our design through a week-long formative study using a commercial smart glasses platform, revealing discomfort with public voice use, frustration with repetitive speech, and hardware constraints. Building on these insights, we design SpeechLess, and evaluate it through controlled lab and in-the-wild studies. Our results indicate that regulated speech-based interaction, can improve everyday information access, reduce articulation effort, and support socially acceptable use without substantially degrading perceived usability or intent resolution accuracy across diverse everyday environments.",
        "url": "http://arxiv.org/abs/2602.00793v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00793v1",
        "arxiv_id": "2602.00793v1",
        "authors": [
            "Yoonsang Kim",
            "Devshree Jadeja",
            "Divyansh Pradhan",
            "Yalong Yang",
            "Arie Kaufman"
        ],
        "submitted": "2026-01-31 16:01:32",
        "source": "arxiv",
        "comment": "11 pages, 9 figures. This is the author's version of the article that will appear at the IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR) 2026",
        "score": 5,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves user interaction and intent resolution, the focus is on wearable AR assistants and spatial memory-aware assistants, which is outside your primary areas of interest."
    },
    {
        "title": "Towards Trustworthy Multimodal Recommendation",
        "abstract": "Recent advances in multimodal recommendation have demonstrated the effectiveness of incorporating visual and textual content into collaborative filtering. However, real-world deployments raise an increasingly important yet underexplored issue: trustworthiness. On modern e-commerce platforms, multimodal content can be misleading or unreliable (e.g., visually inconsistent product images or click-bait titles), injecting untrustworthy signals into multimodal representations and making existing recommenders brittle under modality corruption. In this work, we take a step towards trustworthy multimodal recommendation from both a method and an analysis perspective. First, we propose a plug-and-play modality-level rectification component that mitigates untrustworthy modality features by learning soft correspondences between items and multimodal features. Using lightweight projections and Sinkhorn-based soft matching, the rectification suppresses mismatched modality signals while preserving semantic consistency, and can be integrated into existing multimodal recommenders without architectural modifications. Second, we present two practical insights on interaction-level trustworthiness under noisy collaborative signals: (i) training-set pseudo interactions can help or hurt performance under noise depending on prior-signal alignment; and (ii) propagation-graph pseudo edges can also help or hurt robustness, as message passing may amplify misalignment. Extensive experiments on multiple datasets and backbones under varying corruption levels demonstrate improved robustness from modality rectification and validate the above interaction-level observations.",
        "url": "http://arxiv.org/abs/2602.00730v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00730v1",
        "arxiv_id": "2602.00730v1",
        "authors": [
            "Zixuan Li"
        ],
        "submitted": "2026-01-31 13:47:25",
        "source": "arxiv",
        "comment": "Preprint, 10 pages, 5 figures",
        "score": 5,
        "keyword_reasons": [
            "Found 'click' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores trustworthy multimodal recommendation, which is somewhat related to information retrieval and search technologies. However, the focus on recommender systems and multimodal content is not a central match to the user's primary research interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "RecGOAT: Graph Optimal Adaptive Transport for LLM-Enhanced Multimodal Recommendation with Dual Semantic Alignment",
        "abstract": "Multimodal recommendation systems typically integrates user behavior with multimodal data from items, thereby capturing more accurate user preferences. Concurrently, with the rise of large models (LMs), multimodal recommendation is increasingly leveraging their strengths in semantic understanding and contextual reasoning. However, LM representations are inherently optimized for general semantic tasks, while recommendation models rely heavily on sparse user/item unique identity (ID) features. Existing works overlook the fundamental representational divergence between large models and recommendation systems, resulting in incompatible multimodal representations and suboptimal recommendation performance. To bridge this gap, we propose RecGOAT, a novel yet simple dual semantic alignment framework for LLM-enhanced multimodal recommendation, which offers theoretically guaranteed alignment capability. RecGOAT first employs graph attention networks to enrich collaborative semantics by modeling item-item, user-item, and user-user relationships, leveraging user/item LM representations and interaction history. Furthermore, we design a dual-granularity progressive multimodality-ID alignment framework, which achieves instance-level and distribution-level semantic alignment via cross-modal contrastive learning (CMCL) and optimal adaptive transport (OAT), respectively. Theoretically, we demonstrate that the unified representations derived from our alignment framework exhibit superior semantic consistency and comprehensiveness. Extensive experiments on three public benchmarks show that our RecGOAT achieves state-of-the-art performance, empirically validating our theoretical insights. Additionally, the deployment on a large-scale online advertising platform confirms the model's effectiveness and scalability in industrial recommendation scenarios. Code available at https://github.com/6lyc/RecGOAT-LLM4Rec.",
        "url": "http://arxiv.org/abs/2602.00682v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00682v1",
        "arxiv_id": "2602.00682v1",
        "authors": [
            "Yuecheng Li",
            "Hengwei Ju",
            "Zeyu Song",
            "Wei Yang",
            "Chi Lu",
            "Peng Jiang",
            "Kun Gai"
        ],
        "submitted": "2026-01-31 11:58:38",
        "source": "arxiv",
        "comment": "Under Review",
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'user behavior' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on multimodal recommendation systems, leveraging large language models for semantic understanding. While it touches on aspects of query understanding and ranking models, its primary focus is on recommender systems rather than information retrieval. The paper's use of graph attention networks and cross-modal contrastive learning is relevant to NLP and data mining, but it doesn't directly align with the user's core research themes in IR and search technologies."
    },
    {
        "title": "Diffusion LMs Can Approximate Optimal Infilling Lengths Implicitly",
        "abstract": "Diffusion language models (DLMs) provide a bidirectional generation framework naturally suited for infilling, yet their performance is constrained by the pre-specified infilling length. In this paper, we reveal that DLMs possess an inherent ability to discover the correct infilling length. We identify two key statistical phenomena in the first-step denoising confidence: a local \\textit{Oracle Peak} that emerges near the ground-truth length and a systematic \\textit{Length Bias} that often obscures this signal. By leveraging this signal and calibrating the bias, our training-free method \\textbf{CAL} (\\textbf{C}alibrated \\textbf{A}daptive \\textbf{L}ength) enables DLMs to approximate the optimal length through an efficient search before formal decoding. Empirical evaluations demonstrate that CAL improves Pass@1 by up to 47.7\\% over fixed-length baselines and 40.5\\% over chat-based adaptive methods in code infilling, while boosting BLEU-2 and ROUGE-L by up to 8.5\\% and 9.9\\% in text infilling. These results demonstrate that CAL paves the way for robust DLM infilling without requiring any specialized training. Code is available at https://github.com/NiuHechang/Calibrated_Adaptive_Length.",
        "url": "http://arxiv.org/abs/2602.00476v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00476v1",
        "arxiv_id": "2602.00476v1",
        "authors": [
            "Hengchang Liu",
            "Zhao Yang",
            "Bing Su"
        ],
        "submitted": "2026-01-31 03:00:21",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on diffusion language models and their ability to discover optimal infilling lengths, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the context is more aligned with language generation and model optimization rather than user behavior modeling or ranking models."
    },
    {
        "title": "WildGraphBench: Benchmarking GraphRAG with Wild-Source Corpora",
        "abstract": "Graph-based Retrieval-Augmented Generation (GraphRAG) organizes external knowledge as a hierarchical graph, enabling efficient retrieval and aggregation of scattered evidence across multiple documents. However, many existing benchmarks for GraphRAG rely on short, curated passages as external knowledge, failing to adequately evaluate systems in realistic settings involving long contexts and large-scale heterogeneous documents. To bridge this gap, we introduce WildGraphBench, a benchmark designed to assess GraphRAG performance in the wild. We leverage Wikipedia's unique structure, where cohesive narratives are grounded in long and heterogeneous external reference documents, to construct a benchmark reflecting real-word scenarios. Specifically, we sample articles across 12 top-level topics, using their external references as the retrieval corpus and citation-linked statements as ground truth, resulting in 1,100 questions spanning three levels of complexity: single-fact QA, multi-fact QA, and section-level summarization. Experiments across multiple baselines reveal that current GraphRAG pipelines help on multi-fact aggregation when evidence comes from a moderate number of sources, but this aggregation paradigm may overemphasize high-level statements at the expense of fine-grained details, leading to weaker performance on summarization tasks. Project page:https://github.com/BstWPY/WildGraphBench.",
        "url": "http://arxiv.org/abs/2602.02053v1",
        "pdf_url": "https://arxiv.org/pdf/2602.02053v1",
        "arxiv_id": "2602.02053v1",
        "authors": [
            "Pengyu Wang",
            "Benfeng Xu",
            "Licheng Zhang",
            "Shaohan Wang",
            "Mingxuan Du",
            "Chiwei Zhu",
            "Zhendong Mao"
        ],
        "submitted": "2026-02-02 12:55:29",
        "source": "arxiv",
        "comment": "https://github.com/BstWPY/WildGraphBench",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper introduces a benchmark for GraphRAG, a graph-based retrieval model, but its focus on graph-based retrieval and generation does not directly align with your primary research interests in Information Retrieval, particularly query understanding, ranking models, and user behavior modeling. While it touches on aspects of deep semantic understanding, the paper's scope is more narrowly focused on graph-based retrieval and generation, making it only loosely relevant to your research interests."
    },
    {
        "title": "Hunt Instead of Wait: Evaluating Deep Data Research on Large Language Models",
        "abstract": "The agency expected of Agentic Large Language Models goes beyond answering correctly, requiring autonomy to set goals and decide what to explore. We term this investigatory intelligence, distinguishing it from executional intelligence, which merely completes assigned tasks. Data Science provides a natural testbed, as real-world analysis starts from raw data rather than explicit queries, yet few benchmarks focus on it. To address this, we introduce Deep Data Research (DDR), an open-ended task where LLMs autonomously extract key insights from databases, and DDR-Bench, a large-scale, checklist-based benchmark that enables verifiable evaluation. Results show that while frontier models display emerging agency, long-horizon exploration remains challenging. Our analysis highlights that effective investigatory intelligence depends not only on agent scaffolding or merely scaling, but also on intrinsic strategies of agentic models.",
        "url": "http://arxiv.org/abs/2602.02039v1",
        "pdf_url": "https://arxiv.org/pdf/2602.02039v1",
        "arxiv_id": "2602.02039v1",
        "authors": [
            "Wei Liu",
            "Peijie Yu",
            "Michele Orini",
            "Yali Du",
            "Yulan He"
        ],
        "submitted": "2026-02-02 12:36:57",
        "source": "arxiv",
        "comment": "14 pages, 7 tables, 8 figures",
        "score": 4,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be unrelated to your core research themes in Information Retrieval and Search technologies. While it involves Large Language Models, the focus is on autonomy, investigatory intelligence, and data analysis, which does not align with your interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "SafePred: A Predictive Guardrail for Computer-Using Agents via World Models",
        "abstract": "With the widespread deployment of Computer-using Agents (CUAs) in complex real-world environments, prevalent long-term risks often lead to severe and irreversible consequences. Most existing guardrails for CUAs adopt a reactive approach, constraining agent behavior only within the current observation space. While these guardrails can prevent immediate short-term risks (e.g., clicking on a phishing link), they cannot proactively avoid long-term risks: seemingly reasonable actions can lead to high-risk consequences that emerge with a delay (e.g., cleaning logs leads to future audits being untraceable), which reactive guardrails cannot identify within the current observation space. To address these limitations, we propose a predictive guardrail approach, with the core idea of aligning predicted future risks with current decisions. Based on this approach, we present SafePred, a predictive guardrail framework for CUAs that establishes a risk-to-decision loop to ensure safe agent behavior. SafePred supports two key abilities: (1) Short- and long-term risk prediction: by using safety policies as the basis for risk prediction, SafePred leverages the prediction capability of the world model to generate semantic representations of both short-term and long-term risks, thereby identifying and pruning actions that lead to high-risk states; (2) Decision optimization: translating predicted risks into actionable safe decision guidances through step-level interventions and task-level re-planning. Extensive experiments show that SafePred significantly reduces high-risk behaviors, achieving over 97.6% safety performance and improving task utility by up to 21.4% compared with reactive baselines.",
        "url": "http://arxiv.org/abs/2602.01725v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01725v1",
        "arxiv_id": "2602.01725v1",
        "authors": [
            "Yurun Chen",
            "Zeyi Liao",
            "Ping Yin",
            "Taotao Xie",
            "Keting Yin",
            "Shengyu Zhang"
        ],
        "submitted": "2026-02-02 07:04:06",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'click' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on developing a predictive guardrail for Computer-using Agents, which is not directly related to Information Retrieval, Search technologies, or Natural Language Processing. While it involves risk prediction and decision optimization, the context is more aligned with AI safety and agent behavior, rather than the user's core research themes."
    },
    {
        "title": "Ebisu: Benchmarking Large Language Models in Japanese Finance",
        "abstract": "Japanese finance combines agglutinative, head-final linguistic structure, mixed writing systems, and high-context communication norms that rely on indirect expression and implicit commitment, posing a substantial challenge for LLMs. We introduce Ebisu, a benchmark for native Japanese financial language understanding, comprising two linguistically and culturally grounded, expert-annotated tasks: JF-ICR, which evaluates implicit commitment and refusal recognition in investor-facing Q&A, and JF-TE, which assesses hierarchical extraction and ranking of nested financial terminology from professional disclosures. We evaluate a diverse set of open-source and proprietary LLMs spanning general-purpose, Japanese-adapted, and financial models. Results show that even state-of-the-art systems struggle on both tasks. While increased model scale yields limited improvements, language- and domain-specific adaptation does not reliably improve performance, leaving substantial gaps unresolved. Ebisu provides a focused benchmark for advancing linguistically and culturally grounded financial NLP. All datasets and evaluation scripts are publicly released.",
        "url": "http://arxiv.org/abs/2602.01479v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01479v1",
        "arxiv_id": "2602.01479v1",
        "authors": [
            "Xueqing Peng",
            "Ruoyu Xiang",
            "Fan Zhang",
            "Mingzi Song",
            "Mingyang Jiang",
            "Yan Wang",
            "Lingfei Qian",
            "Taiki Hara",
            "Yuqing Guo",
            "Jimin Huang",
            "Junichi Tsujii",
            "Sophia Ananiadou"
        ],
        "submitted": "2026-02-01 23:02:21",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on benchmarking large language models in Japanese finance, which is somewhat related to information retrieval and NLP, but it does not directly address query understanding, ranking models, or user behavior modeling. The paper's emphasis on language and cultural adaptation for financial NLP is tangentially relevant to the user's interests in deep semantic understanding and real-time relevance optimization, but it is not a central match."
    },
    {
        "title": "CRAFT: Calibrated Reasoning with Answer-Faithful Traces via Reinforcement Learning for Multi-Hop Question Answering",
        "abstract": "Retrieval-augmented generation (RAG) is widely used to ground Large Language Models (LLMs) for multi-hop question answering. Recent work mainly focused on improving answer accuracy via fine-tuning and structured or reinforcement-based optimization. However, reliable reasoning in response generation faces three challenges: 1) Reasoning Collapse. Reasoning in multi-hop QA is inherently complex due to multi-hop composition and is further destabilized by noisy retrieval. 2) Reasoning-answer inconsistency. Due to the intrinsic uncertainty of LLM generation and exposure to evidence--distractor mixtures, models may produce correct answers that are not faithfully supported by their intermediate reasoning or evidence. 3) Loss of format control. Traditional chain-of-thought generation often deviates from required structured output formats, leading to incomplete or malformed structured content. To address these challenges, we propose CRAFT (Calibrated Reasoning with Answer-Faithful Traces), a Group Relative Policy Optimization (GRPO) based reinforcement learning framework that trains models to perform faithful reasoning during response generation. CRAFT employs dual reward mechanisms to optimize multi-hop reasoning: deterministic rewards ensure structural correctness while judge-based rewards verify semantic faithfulness. This optimization framework supports controllable trace variants that enable systematic analysis of how structure and scale affect reasoning performance and faithfulness. Experiments on three multi-hop QA benchmarks show that CRAFT improves both answer accuracy and reasoning faithfulness across model scales, with the CRAFT 7B model achieving competitive performance with closed-source LLMs across multiple reasoning trace settings.",
        "url": "http://arxiv.org/abs/2602.01348v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01348v1",
        "arxiv_id": "2602.01348v1",
        "authors": [
            "Yu Liu",
            "Wenxiao Zhang",
            "Cong Cao",
            "Fangfang Yuan",
            "Weizhuo Chen",
            "Cheng Hu",
            "Pin Xu",
            "Yuling Yang",
            "Kun Peng",
            "Diandian Guo",
            "Qiang Sun",
            "Yanbing Liu",
            "Jin B. Hong",
            "Zhiyuan Ma"
        ],
        "submitted": "2026-02-01 17:33:39",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models, as it focuses on improving answer accuracy and reasoning faithfulness in multi-hop question answering. The use of reinforcement learning and dual reward mechanisms is also related to your interests in Learning to Rank and user behavior modeling. However, the paper's primary focus on question answering and NLP might not be a central match to your broader interests in search technologies and e-commerce."
    },
    {
        "title": "Supervised Fine-Tuning Needs to Unlock the Potential of Token Priority",
        "abstract": "The transition from fitting empirical data to achieving true human utility is fundamentally constrained by a granularity mismatch, where fine-grained autoregressive generation is often supervised by coarse or uniform signals. This position paper advocates Token Priority as the essential bridge, formalizing Supervised Fine-Tuning (SFT) not as simple optimization but as a precise distribution reshaping process that aligns raw data with the ideal alignment manifold. We analyze recent breakthroughs through this unified lens, categorizing them into two distinct regimes: Positive Priority for noise filtration and Signed Priority for toxic modes unlearning. We revisit existing progress and limitations, identify key challenges, and suggest directions for future research.",
        "url": "http://arxiv.org/abs/2602.01227v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01227v1",
        "arxiv_id": "2602.01227v1",
        "authors": [
            "Zhanming Shen",
            "Zeyu Qin",
            "Jiaqi Hu",
            "Wentao Ye",
            "Hao Chen",
            "Xiaomeng Hu",
            "Haokai Xu",
            "Gang Chen",
            "Yi R. Fung",
            "Haobo Wang"
        ],
        "submitted": "2026-02-01 13:39:34",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'ltr' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper discusses the concept of Token Priority in the context of Supervised Fine-Tuning (SFT), which is a technique used in deep learning models. While it touches on the idea of fine-grained autoregressive generation, it doesn't directly relate to information retrieval, query understanding, or ranking models, which are core areas of interest. However, the paper's focus on distribution reshaping and alignment with ideal manifolds might be tangentially relevant to real-time relevance optimization in IR."
    },
    {
        "title": "Temporal Leakage in Search-Engine Date-Filtered Web Retrieval: A Case Study from Retrospective Forecasting",
        "abstract": "Search-engine date filters are widely used to enforce pre-cutoff retrieval in retrospective evaluations of search-augmented forecasters. We show this approach is unreliable: auditing Google Search with a before: filter, 71% of questions return at least one page containing strong post-cutoff leakage, and for 41%, at least one page directly reveals the answer. Using a large language model (LLM), gpt-oss-120b, to forecast with these leaky documents, we demonstrate an inflated prediction accuracy (Brier score 0.108 vs. 0.242 with leak-free documents). We characterize common leakage mechanisms, including updated articles, related-content modules, unreliable metadata/timestamps, and absence-based signals, and argue that date-restricted search is insufficient for temporal evaluation. We recommend stronger retrieval safeguards or evaluation on frozen, time-stamped web snapshots to ensure credible retrospective forecasting.",
        "url": "http://arxiv.org/abs/2602.00758v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00758v1",
        "arxiv_id": "2602.00758v1",
        "authors": [
            "Ali El Lahib",
            "Ying-Jieh Xia",
            "Zehan Li",
            "Yuxuan Wang",
            "Xinyu Pi"
        ],
        "submitted": "2026-01-31 14:47:01",
        "source": "arxiv",
        "comment": "9 pages, 6 figures",
        "score": 4,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to Information Retrieval, but its focus on temporal leakage in search-engine date-filtered web retrieval is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling. While it touches on search technologies, the context is more specific to retrospective forecasting and evaluation, which is not a central theme in the user's research."
    },
    {
        "title": "SWGCN: Synergy Weighted Graph Convolutional Network for Multi-Behavior Recommendation",
        "abstract": "Multi-behavior recommendation paradigms have emerged to capture diverse user activities, forecasting primary conversions (e.g., purchases) by leveraging secondary signals like browsing history. However, current graph-based methods often overlook cross-behavioral synergistic signals and fine-grained intensity of individual actions. Motivated by the need to overcome these shortcomings, we introduce Synergy Weighted Graph Convolutional Network (SWGCN). SWGCN introduces two novel components: a Target Preference Weigher, which adaptively assigns weights to user-item interactions within each behavior, and a Synergy Alignment Task, which guides its training by leveraging an Auxiliary Preference Valuator. This task prioritizes interactions from synergistic signals that more accurately reflect user preferences. The performance of our model is rigorously evaluated through comprehensive tests on three open-source datasets, specifically Taobao, IJCAI, and Beibei. On the Taobao dataset, SWGCN yields relative gains of 112.49% and 156.36% in terms of Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG), respectively. It also yields consistent gains on IJCAI and Beibei, confirming its robustness and generalizability across various datasets. Our implementation is open-sourced and can be accessed via https://github.com/FangdChen/SWGCN.",
        "url": "http://arxiv.org/abs/2602.00727v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00727v1",
        "arxiv_id": "2602.00727v1",
        "authors": [
            "Fangda Chen",
            "Yueyang Wang",
            "Chaoli Lou",
            "Min Gao",
            "Qingyu Xiong"
        ],
        "submitted": "2026-01-31 13:42:49",
        "source": "arxiv",
        "comment": "Accepted by Information Sciences",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'ijcai' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses a recommendation system using graph convolutional networks, which is somewhat related to information retrieval and search technologies. However, the focus on multi-behavior recommendation and user-item interactions is more aligned with recommender systems, which is not the primary focus of the user's research interests. The paper does not explicitly address query understanding, ranking models, or user behavior modeling in the context of search technologies."
    },
    {
        "title": "Lookahead-then-Verify: Reliable Constrained Decoding for Diffusion LLMs under Context-Free Grammars",
        "abstract": "Diffusion Large Language Models (dLLMs) have demonstrated promising generative capabilities and are increasingly used to produce formal languages defined by context-free grammars, such as source code and chemical expressions. However, as probabilistic models, they still struggle to generate syntactically valid outputs reliably. A natural and promising direction to address this issue is to adapt constrained decoding techniques to enforce grammatical correctness during generation. However, applying these techniques faces two primary obstacles. On the one hand, the non-autoregressive nature of dLLMs renders most existing constrained decoding approaches inapplicable. On the other hand, current approaches specifically designed for dLLMs may allow intermediate outputs that are impossible to complete into valid sentences, which significantly limits their reliability in practice.\n  To address these challenges, we present LAVE, a constrained decoding approach specifically designed for dLLMs. Our approach leverages a key property of dLLMs, namely their ability to predict token distributions for all positions in parallel during each forward pass. Whenever a new token is proposed by model, LAVE performs lookahead using these distributions to efficiently and reliably verify the validity of the proposed token. This design ensures reliable constraints by reliably preserving the potential for intermediate outputs to be extended into valid sentences. Extensive experiments across four widely used dLLMs and three representative benchmarks demonstrate that LAVE consistently outperforms existing baselines and achieves substantial improvements in syntactic correctness, while incurring negligible runtime overhead.",
        "url": "http://arxiv.org/abs/2602.00612v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00612v1",
        "arxiv_id": "2602.00612v1",
        "authors": [
            "Yitong Zhang",
            "Yongmin Li",
            "Yuetong Liu",
            "Jia Li",
            "Xiaoran Jia",
            "Zherui Li",
            "Ge Li"
        ],
        "submitted": "2026-01-31 08:58:15",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on constrained decoding for Diffusion Large Language Models (dLLMs) under context-free grammars, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on language generation, the specific application and techniques used are not aligned with the user's core research themes."
    },
    {
        "title": "When RAG Hurts: Diagnosing and Mitigating Attention Distraction in Retrieval-Augmented LVLMs",
        "abstract": "While Retrieval-Augmented Generation (RAG) is one of the dominant paradigms for enhancing Large Vision-Language Models (LVLMs) on knowledge-based VQA tasks, recent work attributes RAG failures to insufficient attention towards the retrieved context, proposing to reduce the attention allocated to image tokens. In this work, we identify a distinct failure mode that previous study overlooked: Attention Distraction (AD). When the retrieved context is sufficient (highly relevant or including the correct answer), the retrieved text suppresses the visual attention globally, and the attention on image tokens shifts away from question-relevant regions. This leads to failures on questions the model could originally answer correctly without the retrieved text. To mitigate this issue, we propose MAD-RAG, a training-free intervention that decouples visual grounding from context integration through a dual-question formulation, combined with attention mixing to preserve image-conditioned evidence. Extensive experiments on OK-VQA, E-VQA, and InfoSeek demonstrate that MAD-RAG consistently outperforms existing baselines across different model families, yielding absolute gains of up to 4.76%, 9.20%, and 6.18% over the vanilla RAG baseline. Notably, MAD-RAG rectifies up to 74.68% of failure cases with negligible computational overhead.",
        "url": "http://arxiv.org/abs/2602.00344v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00344v1",
        "arxiv_id": "2602.00344v1",
        "authors": [
            "Beidi Zhao",
            "Wenlong Deng",
            "Xinting Liao",
            "Yushu Li",
            "Nazim Shaikh",
            "Yao Nie",
            "Xiaoxiao Li"
        ],
        "submitted": "2026-01-30 21:47:00",
        "source": "arxiv",
        "comment": "18 pages, 10 figures",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to information retrieval, specifically in the context of vision-language models and knowledge-based VQA tasks. However, its focus on attention mechanisms and visual grounding does not directly align with the user's primary interests in query understanding, ranking models, and user behavior modeling. While it touches on the intersection of IR and NLP, it is not a central match for the user's research themes."
    },
    {
        "title": "TABES: Trajectory-Aware Backward-on-Entropy Steering for Masked Diffusion Models",
        "abstract": "Masked Diffusion Models (MDMs) have emerged as a promising non-autoregressive paradigm for generative tasks, offering parallel decoding and bidirectional context utilization. However, current sampling methods rely on simple confidence-based heuristics that ignore the long-term impact of local decisions, leading to trajectory lock-in where early hallucinations cascade into global incoherence. While search-based methods mitigate this, they incur prohibitive computational costs ($O(K)$ forward passes per step). In this work, we propose Backward-on-Entropy (BoE) Steering, a gradient-guided inference framework that approximates infinite-horizon lookahead via a single backward pass. We formally derive the Token Influence Score (TIS) from a first-order expansion of the trajectory cost functional, proving that the gradient of future entropy with respect to input embeddings serves as an optimal control signal for minimizing uncertainty. To ensure scalability, we introduce \\texttt{ActiveQueryAttention}, a sparse adjoint primitive that exploits the structure of the masking objective to reduce backward pass complexity. BoE achieves a superior Pareto frontier for inference-time scaling compared to existing unmasking methods, demonstrating that gradient-guided steering offers a mathematically principled and efficient path to robust non-autoregressive generation. We will release the code.",
        "url": "http://arxiv.org/abs/2602.00250v1",
        "pdf_url": "https://arxiv.org/pdf/2602.00250v1",
        "arxiv_id": "2602.00250v1",
        "authors": [
            "Shreshth Saini",
            "Avinab Saha",
            "Balu Adsumilli",
            "Neil Birkbeck",
            "Yilin Wang",
            "Alan C. Bovik"
        ],
        "submitted": "2026-01-30 19:10:32",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper focuses on masked diffusion models for generative tasks, proposing a new inference framework called Backward-on-Entropy Steering. While it involves optimization and gradient-guided steering, the topic is not related to information retrieval, search technologies, or user behavior modeling, which are the core areas of your research interests."
    },
    {
        "title": "Enhancing Automated Essay Scoring with Three Techniques: Two-Stage Fine-Tuning, Score Alignment, and Self-Training",
        "abstract": "Automated Essay Scoring (AES) plays a crucial role in education by providing scalable and efficient assessment tools. However, in real-world settings, the extreme scarcity of labeled data severely limits the development and practical adoption of robust AES systems. This study proposes a novel approach to enhance AES performance in both limited-data and full-data settings by introducing three key techniques. First, we introduce a Two-Stage fine-tuning strategy that leverages low-rank adaptations to better adapt an AES model to target prompt essays. Second, we introduce a Score Alignment technique to improve consistency between predicted and true score distributions. Third, we employ uncertainty-aware self-training using unlabeled data, effectively expanding the training set with pseudo-labeled samples while mitigating label noise propagation. We implement above three key techniques on DualBERT. We conduct extensive experiments on the ASAP++ dataset. As a result, in the 32-data setting, all three key techniques improve performance, and their integration achieves 91.2% of the full-data performance trained on approximately 1,000 labeled samples. In addition, the proposed Score Alignment technique consistently improves performance in both limited-data and full-data settings: e.g., it achieves state-of-the-art results in the full-data setting when integrated into DualBERT.",
        "url": "http://arxiv.org/abs/2602.01747v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01747v1",
        "arxiv_id": "2602.01747v1",
        "authors": [
            "Hongseok Choi",
            "Serynn Kim",
            "Wencke Liermann",
            "Jin Seong",
            "Jin-Xia Huang"
        ],
        "submitted": "2026-02-02 07:29:15",
        "source": "arxiv",
        "comment": "22 pages, 4 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling. The focus is on Automated Essay Scoring, which is outside your primary areas of interest."
    },
    {
        "title": "BBPE16: UTF-16-based byte-level byte-pair encoding for improved multilingual speech recognition",
        "abstract": "Multilingual automatic speech recognition (ASR) requires tokenization that efficiently covers many writing systems. Byte-level BPE (BBPE) using UTF-8 is widely adopted for its language-agnostic design and full Unicode coverage, but its variable-length encoding inflates token sequences for non-Latin scripts, such as Chinese, Japanese, and Korean (CJK). Longer sequences increase computational load and memory use. We propose BBPE16, a UTF-16-based BBPE tokenizer that represents most modern scripts with a uniform 2-byte code unit. BBPE16 preserves BBPE's language-agnostic properties while substantially improving cross-lingual token sharing. Across monolingual, bilingual, and trilingual ASR, and in a multilingual continual-learning setup, BBPE16 attains comparable or better accuracy; for Chinese, it reduces token counts by up to 10.4% and lowers decoding iterations by up to 10.3%. These reductions speed up fine-tuning and inference and decrease memory usage, making BBPE16 a practical tokenization choice for multilingual ASR.",
        "url": "http://arxiv.org/abs/2602.01717v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01717v1",
        "arxiv_id": "2602.01717v1",
        "authors": [
            "Hyunsik Kim",
            "Haeri Kim",
            "Munhak Lee",
            "Kyungmin Lee"
        ],
        "submitted": "2026-02-02 06:56:27",
        "source": "arxiv",
        "comment": "accepted to ICASSP 2026",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'korea' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. It focuses on speech recognition and tokenization for multilingual automatic speech recognition, which is outside your primary areas of interest."
    },
    {
        "title": "Unmediated AI-Assisted Scholarly Citations",
        "abstract": "Traditional bibliography databases require users to navigate search forms and manually copy citation data. Language models offer an alternative: a natural-language interface where researchers write text with informal citation fragments, which are automatically resolved to proper references. However, language models are not reliable for scholarly work as they generate fabricated (hallucinated) citations at substantial rates.\n  We present an architectural approach that combines the natural-language interface of LLM chatbots with the accuracy of direct database access, implemented through the Model Context Protocol. Our system enables language models to search bibliographic databases, perform fuzzy matching, and export verified entries, all through conversational interaction.\n  A key architectural principle bypasses the language model during final data export: entries are fetched directly from authoritative sources, with timeout protection, to guarantee accuracy. We demonstrate this approach with MCP-DBLP, a server providing access to the DBLP computer science bibliography. The system transforms form-based bibliographic services into conversational assistants that maintain scholarly integrity. This architecture is adaptable to other bibliographic databases and academic data sources.",
        "url": "http://arxiv.org/abs/2602.01686v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01686v1",
        "arxiv_id": "2602.01686v1",
        "authors": [
            "Stefan Szeider"
        ],
        "submitted": "2026-02-02 05:56:27",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on AI-assisted scholarly citations, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves language models, the context is more about citation accuracy and conversational interfaces, rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Expected Harm: Rethinking Safety Evaluation of (Mis)Aligned LLMs",
        "abstract": "Current evaluations of LLM safety predominantly rely on severity-based taxonomies to assess the harmfulness of malicious queries. We argue that this formulation requires re-examination as it assumes uniform risk across all malicious queries, neglecting Execution Likelihood--the conditional probability of a threat being realized given the model's response. In this work, we introduce Expected Harm, a metric that weights the severity of a jailbreak by its execution likelihood, modeled as a function of execution cost. Through empirical analysis of state-of-the-art models, we reveal a systematic Inverse Risk Calibration: models disproportionately exhibit stronger refusal behaviors for low-likelihood (high-cost) threats while remaining vulnerable to high-likelihood (low-cost) queries. We demonstrate that this miscalibration creates a structural vulnerability: by exploiting this property, we increase the attack success rate of existing jailbreaks by up to $2\\times$. Finally, we trace the root cause of this failure using linear probing, which reveals that while models encode severity in their latent space to drive refusal decisions, they possess no distinguishable internal representation of execution cost, making them \"blind\" to this critical dimension of risk.",
        "url": "http://arxiv.org/abs/2602.01600v1",
        "pdf_url": "https://arxiv.org/pdf/2602.01600v1",
        "arxiv_id": "2602.01600v1",
        "authors": [
            "Yen-Shan Chen",
            "Zhi Rui Tam",
            "Cheng-Kuang Wu",
            "Yun-Nung Chen"
        ],
        "submitted": "2026-02-02 03:48:04",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the safety evaluation of Large Language Models (LLMs), introducing a new metric called Expected Harm. While it touches on aspects of query understanding and risk assessment, its primary focus is on LLM safety and security, which is somewhat related to information retrieval but not a central match for your research interests."
    }
]