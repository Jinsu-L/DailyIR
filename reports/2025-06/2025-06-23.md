# Daily Papers Report - 2025-06-23

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Harnessing the Power of Reinforcement Learning for Language-Model-Based Information Retriever via Query-Document Co-Augmentation

- **LLM Score**: 9
- **Keyword Score**: 13
- **Authors**: Jingming Liu, Yumeng Li, Wei Shi, Yao-Xiang Ding, Hui Su, Kun Zhou
- **URL**: <http://arxiv.org/abs/2506.18670v1>
- **Submitted**: 2025-06-23 14:14:43
- **Topic Keywords**: retriever, query, queries, rag, retrieval
- **Reason**: This paper is extremely relevant to your research interests in Information Retrieval, specifically in the area of query understanding and ranking models. The use of reinforcement learning for query-document co-augmentation aligns with your focus on learning to rank and user behavior modeling. The paper's emphasis on leveraging large language models for information retrieval also resonates with your background in NLP and data mining.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Information Retrieval using Large Language Models
- **Aim**: To propose a novel approach to information retrieval leveraging Large Language Models (LLMs) as retrievers through query-document co-augmentation
- **Rationale**: Enhancing queries alone is insufficient for robust semantic matching, and a bidirectional reinforcement learning framework is needed to enable the LLM to simultaneously learn and collaborate on both query and document augmentation policies
- **Ground**: The authors introduce a query-document co-augmentation framework, where the LLM pre-computes lightweight augmentations for every document, giving the LLM control over both queries and documents
- **Experiment**: The authors conduct experiments to evaluate the effectiveness of their proposed framework and investigate the importance of query-document collaborative training, reporting superior results compared to baseline retrievers
- **Takeaway**: The collaborative bidirectional augmentation policy enables the policy to learn to cooperate with itself to match queries and documents in the semantic domain, improving retrieval effectiveness and generalization across datasets

#### Abstract
> Recent studies have proposed leveraging Large Language Models (LLMs) as
information retrievers through query rewriting. However, for challenging
corpora, we argue that enhancing queries alone is insufficient for robust
semantic matching; the LLM should also have sufficient understanding of the
corpus by directly handling and augmenting the documents themselves. To this
end, we present an LLM-based retriever empowered to augment both user queries
and corpus documents, with its policy fully explored via reinforcement learning
(RL) and minimal human inductive bias. Notably, we find that simply allowing
the LLM to modify documents yields little benefit unless paired with our
carefully designed bidirectional RL framework, which enables the LLM to
simultaneously learn and collaborate on both query and document augmentation
policies. A key technical challenge in realizing such a framework lies in
jointly updating both policies during training, where the rewards for the two
directions depend on each other, making their entangled reward intractable. Our
approach addresses this by introducing a reward sampling strategy and a
specifically designed RL algorithm that enables effective training with these
sampled rewards. Experimental results demonstrate that our approach
significantly enhances LLM-based retrieval performance in both sparse and dense
settings, particularly in difficult retrieval domains, and achieves strong
cross-benchmark generalization. Our code is released at
https://github.com/liujm2001/CoAugRetriever.

---

### 2. Semantic similarity estimation for domain specific data using BERT and other techniques

- **LLM Score**: 8
- **Keyword Score**: 9
- **Authors**: R. Prashanth
- **URL**: <http://arxiv.org/abs/2506.18602v1>
- **Submitted**: 2025-06-23 13:03:59
- **Comment**: This is a preprint version of an article accepted for publication in
  the proceedings of Machine Learning and Data Mining 2019
- **Topic Keywords**: information retrieval, semantic search, retrieval, search
- **Reason**: The paper explores semantic similarity estimation using BERT, which is relevant to information retrieval and natural language processing, two areas of interest. The application to question answering, semantic search, and information retrieval is also relevant. However, the focus on domain-specific data and fine-tuning procedures is not directly related to query understanding, ranking models, or user behavior modeling, which are core aspects of your research interests.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Estimation of semantic similarity between texts in natural language processing
- **Aim**: Evaluate the performance of different state-of-the-art techniques, including Universal Sentence Encoder (USE), InferSent, and BERT, on two question pairs datasets
- **Rationale**: Semantic similarity estimation is a crucial problem in natural language processing and understanding, and a comprehensive evaluation of different techniques is necessary to determine the best approach
- **Ground**: The authors use pre-trained sentence embedding models and the Ratcliff/Obershelp pattern-matching algorithm as a baseline for comparison
- **Experiment**: Experiments are conducted using two datasets, Quora's question pairs dataset (QQP) and a domain-specific in-house dataset (Fidelity), with varying parameters such as number of epochs and learning rate
- **Takeaway**: BERT outperforms other methods, especially on domain-specific data, and fine-tuning the pre-trained BERT model is crucial for achieving optimal results

#### Abstract
> Estimation of semantic similarity is an important research problem both in
natural language processing and the natural language understanding, and that
has tremendous application on various downstream tasks such as question
answering, semantic search, information retrieval, document clustering,
word-sense disambiguation and machine translation. In this work, we carry out
the estimation of semantic similarity using different state-of-the-art
techniques including the USE (Universal Sentence Encoder), InferSent and the
most recent BERT, or Bidirectional Encoder Representations from Transformers,
models. We use two question pairs datasets for the analysis, one is a domain
specific in-house dataset and the other is a public dataset which is the
Quora's question pairs dataset. We observe that the BERT model gave much
superior performance as compared to the other methods. This should be because
of the fine-tuning procedure that is involved in its training process, allowing
it to learn patterns based on the training data that is used. This works
demonstrates the applicability of BERT on domain specific datasets. We infer
from the analysis that BERT is the best technique to use in the case of domain
specific data.

---

### 3. Rethinking Click Models in Light of Carousel Interfaces: Theory-Based Categorization and Design of Click Models

- **LLM Score**: 8
- **Keyword Score**: 5
- **Authors**: Jingwei Kang, Maarten de Rijke, Santiago de Leon-Martinez, Harrie Oosterhuis
- **URL**: <http://arxiv.org/abs/2506.18548v1>
- **Submitted**: 2025-06-23 11:57:11
- **Comment**: Accepted by ICTIR 2025
- **Topic Keywords**: click model, click, search
- **Reason**: The paper is highly relevant to your research interests in Information Retrieval, specifically in query understanding and user behavior modeling. The focus on click models and carousel interfaces aligns with your expertise in e-commerce and search technologies. The paper's emphasis on mathematical properties and design choices also resonates with your interest in ranking models and deep semantic understanding.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Click Model Design and Categorization
- **Aim**: To propose a novel framework for click model design and categorization that encompasses all possible models and guarantees exclusivity between mathematically equivalent click models
- **Rationale**: The existing approaches to click modeling are limited by their technical details and assumptions about user behavior, leading to instability and incomparability of different models
- **Ground**: The authors identify three key design choices (global dependencies, sequentiality, and factorization) that explain what statistical patterns a click model can capture and thus indirectly what user behaviors they can capture
- **Experiment**: The authors create a novel click model taxonomy that categorizes click models into eight categories based on their decisions on global dependencies and introduce a novel carousel click model that applies factorization to decompose the overall click probability
- **Takeaway**: The proposed framework provides a flexible and comprehensive approach to click model design and categorization, allowing for the development of more accurate and effective click models for web search and other applications

#### Abstract
> Click models are a well-established for modeling user interactions with web
interfaces. Previous work has mainly focused on traditional single-list web
search settings; this includes existing surveys that introduced categorizations
based on the first generation of probabilistic graphical model (PGM) click
models that have become standard. However, these categorizations have become
outdated, as their conceptualizations are unable to meaningfully compare PGM
with neural network (NN) click models nor generalize to newer interfaces, such
as carousel interfaces. We argue that this outdated view fails to adequately
explain the fundamentals of click model designs, thus hindering the development
of novel click models.
  This work reconsiders what should be the fundamental concepts in click model
design, grounding them - unlike previous approaches - in their mathematical
properties. We propose three fundamental key-design choices that explain what
statistical patterns a click model can capture, and thus indirectly, what user
behaviors they can capture. Based on these choices, we create a novel click
model taxonomy that allows a meaningful comparison of all existing click
models; this is the first taxonomy of single-list, grid and carousel click
models that includes PGMs and NNs. Finally, we show how our conceptualization
provides a foundation for future click model design by an example derivation of
a novel design for carousel interfaces.

---

### 4. jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval

- **LLM Score**: 6
- **Keyword Score**: 10
- **Authors**: Michael G√ºnther, Saba Sturua, Mohammad Kalim Akram, Isabelle Mohr, Andrei Ungureanu, Sedigheh Eslami, Scott Martens, Bo Wang, Nan Wang, Han Xiao
- **URL**: <http://arxiv.org/abs/2506.18902v1>
- **Submitted**: 2025-06-23 17:59:55
- **Comment**: 22 pages, 1-10 main, 14-22 experimental results, benchmark tables
- **Topic Keywords**: information retrieval, query, retrieval, rank, search
- **Reason**: The paper introduces a multimodal embedding model for information retrieval, which aligns with the user's interest in IR and search technologies. The model's ability to process visually rich content and its application to cross-modal retrieval tasks are relevant to the user's focus on query understanding and ranking models. However, the paper's primary focus on multimodal embeddings and visual content retrieval is not directly related to the user's specific interests in query understanding, ranking models, and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Multimodal Embedding Model for Text and Image Retrieval
- **Aim**: To develop a unified multimodal embedding model that supports both single-vector and multi-vector embeddings for text and image retrieval tasks
- **Rationale**: The need for a model that can handle diverse retrieval scenarios and support multiple languages and query types
- **Ground**: The introduction of a novel multimodal embedding model, jina-embeddings-v4, that incorporates task-specific Low-Rank Adaptation (LoRA) adapters and a late interaction architecture
- **Experiment**: The model is trained on a large dataset of text-text and text-image pairs and evaluated on various benchmarks, including Multilingual Text Retrieval, Textual Semantic Similarity, and Multimodal Retrieval
- **Takeaway**: The jina-embeddings-v4 model achieves state-of-the-art performance on various retrieval tasks and demonstrates strong progress in handling visually rich images, especially for tasks outside of the existing ViDoRe benchmark

#### Abstract
> We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding
model that unifies text and image representations through a novel architecture
supporting both single-vector and multi-vector embeddings in the late
interaction style. The model incorporates task-specific Low-Rank Adaptation
(LoRA) adapters to optimize performance across diverse retrieval scenarios,
including query-based information retrieval, cross-modal semantic similarity,
and programming code search. Comprehensive evaluations demonstrate that
jina-embeddings-v4 achieves state-of-the-art performance on both single- modal
and cross-modal retrieval tasks, with particular strength in processing
visually rich content such as tables, charts, diagrams, and mixed-media
formats. To facilitate evaluation of this capability, we also introduce
Jina-VDR, a novel benchmark specifically designed for visually rich image
retrieval.

---

### 5. PDF Retrieval Augmented Question Answering

- **LLM Score**: 6
- **Keyword Score**: 10
- **Authors**: Thi Thu Uyen Hoang, Viet Anh Nguyen
- **URL**: <http://arxiv.org/abs/2506.18027v1>
- **Submitted**: 2025-06-22 13:14:19
- **Topic Keywords**: query, rag, retrieval augmented generation, retrieval, search
- **Reason**: The paper explores Retrieval Augmented Generation (RAG) for Question-Answering (QA) systems, which is related to query understanding and ranking models. However, the focus on PDF retrieval and multimodal data integration is not directly aligned with the user's primary research interests in information retrieval, search technologies, and natural language processing.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Question-Answering (QA) systems using a Retrieval Augmented Generation (RAG) framework for PDF files
- **Aim**: To develop an end-to-end system that can enhance information extraction from PDF files, addressing the challenges of existing QA systems in processing non-textual elements
- **Rationale**: Existing QA systems struggle to process non-textual elements in PDFs, such as images, vector diagrams, graphs, and tables, leading to inaccurate information extraction
- **Ground**: The proposed system, PIER-QA, consists of three main components: PDF Preprocessing, Retrieval, and Generation, using various models and techniques such as DBSCAN, Marker, and Llama3
- **Experiment**: The system is evaluated using a dataset of 8 private internal documents from a production company, comparing its performance to a baseline system and three different language models
- **Takeaway**: The PIER-QA system outperforms the baseline in terms of accuracy and similarity scores, demonstrating considerable accuracies for image and table retrieval, and closing the gap with state-of-the-art close-source GPT-4o

#### Abstract
> This paper presents an advancement in Question-Answering (QA) systems using a
Retrieval Augmented Generation (RAG) framework to enhance information
extraction from PDF files. Recognizing the richness and diversity of data
within PDFs--including text, images, vector diagrams, graphs, and tables--poses
unique challenges for existing QA systems primarily designed for textual
content. We seek to develop a comprehensive RAG-based QA system that will
effectively address complex multimodal questions, where several data types are
combined in the query. This is mainly achieved by refining approaches to
processing and integrating non-textual elements in PDFs into the RAG framework
to derive precise and relevant answers, as well as fine-tuning large language
models to better adapt to our system. We provide an in-depth experimental
evaluation of our solution, demonstrating its capability to extract accurate
information that can be applied to different types of content across PDFs. This
work not only pushes the boundaries of retrieval-augmented QA systems but also
lays a foundation for further research in multimodal data integration and
processing.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Context-Aware Scientific Knowledge Extraction on Linked Open Data using Large Language Models

- **LLM Score**: 6
- **Keyword Score**: 8
- **Authors**: Sajratul Y. Rubaiat, Hasan M. Jamil
- **URL**: <http://arxiv.org/abs/2506.17580v1>
- **Submitted**: 2025-06-21 04:22:34
- **Topic Keywords**: query, ranking, rank, search
- **Reason**: The paper explores a novel system for scientific knowledge extraction using large language models, which is related to information retrieval and search technologies. However, the focus on scientific knowledge extraction and linked open data is not directly aligned with the user's primary interest in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat related, but not a central match.

#### Abstract
> The exponential growth of scientific literature challenges researchers
extracting and synthesizing knowledge. Traditional search engines return many
sources without direct, detailed answers, while general-purpose LLMs may offer
concise responses that lack depth or omit current information. LLMs with search
capabilities are also limited by context window, yielding short, incomplete
answers. This paper introduces WISE (Workflow for Intelligent Scientific
Knowledge Extraction), a system addressing these limits by using a structured
workflow to extract, refine, and rank query-specific knowledge. WISE uses an
LLM-powered, tree-based architecture to refine data, focusing on query-aligned,
context-aware, and non-redundant information. Dynamic scoring and ranking
prioritize unique contributions from each source, and adaptive stopping
criteria minimize processing overhead. WISE delivers detailed, organized
answers by systematically exploring and synthesizing knowledge from diverse
sources. Experiments on HBB gene-associated diseases demonstrate WISE reduces
processed text by over 80% while achieving significantly higher recall over
baselines like search engines and other LLM-based approaches. ROUGE and BLEU
metrics reveal WISE's output is more unique than other systems, and a novel
level-based metric shows it provides more in-depth information. We also explore
how the WISE workflow can be adapted for diverse domains like drug discovery,
material science, and social science, enabling efficient knowledge extraction
and synthesis from unstructured scientific papers and web sources.

### 7. CARTS: Collaborative Agents for Recommendation Textual Summarization

- **LLM Score**: 6
- **Keyword Score**: 6
- **Authors**: Jiao Chen, Kehui Yao, Reza Yousefi Maragheh, Kai Zhao, Jianpeng Xu, Jason Cho, Evren Korpeoglu, Sushant Kumar, Kannan Achan
- **URL**: <http://arxiv.org/abs/2506.17765v1>
- **Submitted**: 2025-06-21 17:18:35
- **Topic Keywords**: relevance, recommend, commerce, e-commerce
- **Reason**: The paper explores recommendation systems and textual summarization, which aligns with your interest in Information Retrieval and Search technologies. However, the focus on recommendation systems and e-commerce data is not directly related to your primary research themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> Current recommendation systems often require some form of textual data
summarization, such as generating concise and coherent titles for product
carousels or other grouped item displays. While large language models have
shown promise in NLP domains for textual summarization, these approaches do not
directly apply to recommendation systems, where explanations must be highly
relevant to the core features of item sets, adhere to strict word limit
constraints. In this paper, we propose CARTS (Collaborative Agents for
Recommendation Textual Summarization), a multi-agent LLM framework designed for
structured summarization in recommendation systems. CARTS decomposes the task
into three stages-Generation Augmented Generation (GAG), refinement circle, and
arbitration, where successive agent roles are responsible for extracting
salient item features, iteratively refining candidate titles based on relevance
and length feedback, and selecting the final title through a collaborative
arbitration process. Experiments on large-scale e-commerce data and live A/B
testing show that CARTS significantly outperforms single-pass and
chain-of-thought LLM baselines, delivering higher title relevance and improved
user engagement metrics.

### 8. A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment

- **LLM Score**: 6
- **Keyword Score**: 4
- **Authors**: Quanwei Tang, Sophia Yat Mei Lee, Junshuang Wu, Dong Zhang, Shoushan Li, Erik Cambria, Guodong Zhou
- **URL**: <http://arxiv.org/abs/2506.17951v1>
- **Submitted**: 2025-06-22 09:08:44
- **Comment**: acl 2025 findings
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper proposes a graph-based framework for question answering, which is related to information retrieval and natural language processing. The focus on mode-seeking preference alignment and probability-matching constraints is somewhat relevant to user behavior modeling and ranking models. However, the paper's primary focus on question answering and language models is not directly aligned with the user's core research themes in information retrieval and search technologies.

#### Abstract
> Recent advancements in retrieval-augmented generation (RAG) have enhanced
large language models in question answering by integrating external knowledge.
However, challenges persist in achieving global understanding and aligning
responses with human ethical and quality preferences. To address these issues,
we propose GraphMPA, a comprehensive graph-based framework with mode-seeking
preference alignment. Our approach constructs a hierarchical document graph
using a general similarity measurement, mimicking human cognitive processes for
information understanding and synthesis. Additionally, we introduce
mode-seeking preference optimization to better align model outputs with human
preferences through probability-matching constraints. Extensive experiments on
six datasets demonstrate the effectiveness of our
\href{https://github.com/tangquanwei/GraphMPA}{GraphMPA}.

### 9. RLPR: Extrapolating RLVR to General Domains without Verifiers

- **LLM Score**: 6
- **Keyword Score**: 2
- **Authors**: Tianyu Yu, Bo Ji, Shouli Wang, Shu Yao, Zefan Wang, Ganqu Cui, Lifan Yuan, Ning Ding, Yuan Yao, Zhiyuan Liu, Maosong Sun, Tat-Seng Chua
- **URL**: <http://arxiv.org/abs/2506.18254v1>
- **Submitted**: 2025-06-23 02:56:36
- **Comment**: Project Website: https://github.com/openbmb/RLPR
- **Topic Keywords**: rag
- **Reason**: The paper explores reinforcement learning for natural language processing, specifically for improving the reasoning capabilities of language models. While it's not directly related to query understanding, ranking models, or user behavior modeling, it touches on topics relevant to information retrieval, such as language models and their applications. The focus on general-domain benchmarks and the use of token probability scores as a reward signal show some connection to the user's interests in NLP and data mining.

#### Abstract
> Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising
potential in advancing the reasoning capabilities of LLMs. However, its success
remains largely confined to mathematical and code domains. This primary
limitation stems from the heavy reliance on domain-specific verifiers, which
results in prohibitive complexity and limited scalability. To address the
challenge, our key observation is that LLM's intrinsic probability of
generating a correct free-form answer directly indicates its own evaluation of
the reasoning reward (i.e., how well the reasoning process leads to the correct
answer). Building on this insight, we propose RLPR, a simple verifier-free
framework that extrapolates RLVR to broader general domains. RLPR uses the
LLM's own token probability scores for reference answers as the reward signal
and maximizes the expected reward during training. We find that addressing the
high variance of this noisy probability reward is crucial to make it work, and
propose prob-to-reward and stabilizing methods to ensure a precise and stable
reward from LLM intrinsic probabilities. Comprehensive experiments in four
general-domain benchmarks and three mathematical benchmarks show that RLPR
consistently improves reasoning capabilities in both areas for Gemma, Llama,
and Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6
points on TheoremQA and 7.5 points on Minerva, and even surpasses strong
verifier-model-dependent approaches General-Reasoner by 1.6 average points
across seven benchmarks.

### 10. Comparative Analysis of Lion and AdamW Optimizers for Cross-Encoder Reranking with MiniLM, GTE, and ModernBERT

- **LLM Score**: 4
- **Keyword Score**: 17
- **Authors**: Shahil Kumar, Manu Pande, Anay Yatin Damle
- **URL**: <http://arxiv.org/abs/2506.18297v1>
- **Submitted**: 2025-06-23 05:30:09
- **Topic Keywords**: information retrieval, query, ranking, rerank, retrieval, rank, trec
- **Reason**: The paper explores the application of optimizers in fine-tuning transformer models for cross-encoder reranking, which is related to query understanding and ranking models. However, the focus is on the optimization process rather than the underlying search technologies or user behavior modeling, making it only loosely relevant to the user's primary research interests.

#### Abstract
> Modern information retrieval systems often employ a two-stage pipeline: an
efficient initial retrieval stage followed by a computationally intensive
reranking stage. Cross-encoders have shown strong effectiveness for reranking
due to their deep analysis of query-document pairs. This paper studies the
impact of the Lion optimizer, a recent alternative to AdamW, during fine-tuning
of cross-encoder rerankers. We fine-tune three transformer models-MiniLM, GTE,
and ModernBERT-on the MS MARCO passage ranking dataset using both optimizers.
GTE and ModernBERT support extended context lengths (up to 8192 tokens). We
evaluate effectiveness using TREC 2019 Deep Learning Track and MS MARCO dev set
(MRR@10). Experiments, run on the Modal cloud platform, reveal that ModernBERT
with Lion achieves the best NDCG@10 (0.7225) and MAP (0.5121) on TREC DL 2019,
while MiniLM with Lion ties ModernBERT for MRR@10 (0.5988) on MS MARCO dev.
Lion also provides superior GPU efficiency, improving utilization by 2.67% to
10.33% across models. We analyze performance trends using standard IR metrics
and discuss the optimizer's impact on training dynamics across architectures.

### 11. PreQRAG -- Classify and Rewrite for Enhanced RAG

- **LLM Score**: 4
- **Keyword Score**: 14
- **Authors**: Damian Martinez, Catalina Riano, Hui Fang
- **URL**: <http://arxiv.org/abs/2506.17493v1>
- **Submitted**: 2025-06-20 22:02:05
- **Comment**: 7 pages, SIGIR 2025 LiveRAG
- **Topic Keywords**: queries, relevance, rag, retrieval augmented generation, retrieval, sigir
- **Reason**: The paper presents a Retrieval Augmented Generation (RAG) architecture, which is related to search technologies and query understanding. However, the focus is on question preprocessing and rewriting for improved retrieval and generation quality, which is not directly aligned with my interests in ranking models and user behavior modeling. The paper's relevance to my research is somewhat limited.

#### Abstract
> This paper presents the submission of the UDInfo team to the SIGIR 2025
LiveRAG Challenge. We introduce PreQRAG, a Retrieval Augmented Generation (RAG)
architecture designed to improve retrieval and generation quality through
targeted question preprocessing. PreQRAG incorporates a pipeline that first
classifies each input question as either single-document or multi-document
type. For single-document questions, we employ question rewriting techniques to
improve retrieval precision and generation relevance. For multi-document
questions, we decompose complex queries into focused sub-questions that can be
processed more effectively by downstream components. This classification and
rewriting strategy improves the RAG performance. Experimental evaluation of the
LiveRAG Challenge dataset demonstrates the effectiveness of our
question-type-aware architecture, with PreQRAG achieving the preliminary second
place in Session 2 of the LiveRAG challenge.

### 12. Expanding Relevance Judgments for Medical Case-based Retrieval Task with Multimodal LLMs

- **LLM Score**: 4
- **Keyword Score**: 13
- **Authors**: Catarina Pires, S√©rgio Nunes, Lu√≠s Filipe Teixeira
- **URL**: <http://arxiv.org/abs/2506.17782v1>
- **Submitted**: 2025-06-21 18:29:33
- **Comment**: To appear at the Third Workshop on Large Language Models for
  Evaluation in Information Retrieval (LLM4Eval 2025), co-located with SIGIR
  2025. 9 pages, 2 figures, 5 tables
- **Topic Keywords**: information retrieval, query, relevance, rag, retrieval
- **Reason**: The paper explores the use of Large Language Models (LLMs) to expand relevance judgments in medical case-based retrieval, which is a specific application of Information Retrieval (IR). While it touches on some aspects of query understanding and ranking models, the focus is more on the multimodal aspect and the use of LLMs for relevance judgment expansion, which is not directly aligned with the user's primary research interests in IR, query understanding, and ranking models.

#### Abstract
> Evaluating Information Retrieval (IR) systems relies on high-quality manual
relevance judgments (qrels), which are costly and time-consuming to obtain.
While pooling reduces the annotation effort, it results in only partially
labeled datasets. Large Language Models (LLMs) offer a promising alternative to
reducing reliance on manual judgments, particularly in complex domains like
medical case-based retrieval, where relevance assessment requires analyzing
both textual and visual information. In this work, we explore using a
Multimodal Large Language Model (MLLM) to expand relevance judgments, creating
a new dataset of automated judgments. Specifically, we employ Gemini 1.5 Pro on
the ImageCLEFmed 2013 case-based retrieval task, simulating human assessment
through an iteratively refined, structured prompting strategy that integrates
binary scoring, instruction-based evaluation, and few-shot learning. We
systematically experimented with various prompt configurations to maximize
agreement with human judgments. To evaluate agreement between the MLLM and
human judgments, we use Cohen's Kappa, achieving a substantial agreement score
of 0.6, comparable to inter-annotator agreement typically observed in
multimodal retrieval tasks. Starting from the original 15,028 manual judgments
(4.72% relevant) across 35 topics, our MLLM-based approach expanded the dataset
by over 37x to 558,653 judgments, increasing relevant annotations to 5,950. On
average, each medical case query received 15,398 new annotations, with
approximately 99% being non-relevant, reflecting the high sparsity typical in
this domain. Our results demonstrate the potential of MLLMs to scale relevance
judgment collection, offering a promising direction for supporting retrieval
evaluation in medical and multimodal IR tasks.

### 13. LLMs for Customized Marketing Content Generation and Evaluation at Scale

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Haoran Liu, Amir Tahmasbi, Ehtesham Sam Haque, Purak Jain
- **URL**: <http://arxiv.org/abs/2506.17863v1>
- **Submitted**: 2025-06-22 00:28:35
- **Comment**: KDD LLM4ECommerce Workshop 2025
- **Topic Keywords**: ranking, ctr, retrieval, commerce, e-commerce, rank
- **Reason**: The paper focuses on marketing content generation and evaluation, which is somewhat related to information retrieval and search technologies. However, the specific application in e-commerce and the use of language models for evaluation purposes are not directly aligned with the user's core research themes. The paper's relevance is limited to the user's background in e-commerce, but it does not contribute significantly to the user's primary research interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Offsite marketing is essential in e-commerce, enabling businesses to reach
customers through external platforms and drive traffic to retail websites.
However, most current offsite marketing content is overly generic,
template-based, and poorly aligned with landing pages, limiting its
effectiveness. To address these limitations, we propose MarketingFM, a
retrieval-augmented system that integrates multiple data sources to generate
keyword-specific ad copy with minimal human intervention. We validate
MarketingFM via offline human and automated evaluations and large-scale online
A/B tests. In one experiment, keyword-focused ad copy outperformed templates,
achieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC,
demonstrating gains in ad ranking and cost efficiency. Despite these gains,
human review of generated ads remains costly. To address this, we propose
AutoEval-Main, an automated evaluation system that combines rule-based metrics
with LLM-as-a-Judge techniques to ensure alignment with marketing principles.
In experiments with large-scale human annotations, AutoEval-Main achieved
89.57% agreement with human reviewers. Building on this, we propose
AutoEval-Update, a cost-efficient LLM-human collaborative framework to
dynamically refine evaluation prompts and adapt to shifting criteria with
minimal human input. By selectively sampling representative ads for human
review and using a critic LLM to generate alignment reports, AutoEval-Update
improves evaluation consistency while reducing manual effort. Experiments show
the critic LLM suggests meaningful refinements, improving LLM-human agreement.
Nonetheless, human oversight remains essential for setting thresholds and
validating refinements before deployment.

### 14. Bias vs Bias -- Dawn of Justice: A Fair Fight in Recommendation Systems

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Tahsin Alamgir Kheya, Mohamed Reda Bouadjenek, Sunil Aryal
- **URL**: <http://arxiv.org/abs/2506.18327v1>
- **Submitted**: 2025-06-23 06:19:02
- **Topic Keywords**: ranking, rag, recommend, commerce, e-commerce, rank
- **Reason**: The paper is somewhat related to the user's interests in Information Retrieval and Search technologies, particularly in the context of recommender systems. However, the focus on fairness-aware re-ranking and bias mitigation in recommendation systems is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Recommendation systems play a crucial role in our daily lives by impacting
user experience across various domains, including e-commerce, job
advertisements, entertainment, etc. Given the vital role of such systems in our
lives, practitioners must ensure they do not produce unfair and imbalanced
recommendations. Previous work addressing bias in recommendations overlooked
bias in certain item categories, potentially leaving some biases unaddressed.
Additionally, most previous work on fair re-ranking focused on binary-sensitive
attributes. In this paper, we address these issues by proposing a
fairness-aware re-ranking approach that helps mitigate bias in different
categories of items. This re-ranking approach leverages existing biases to
correct disparities in recommendations across various demographic groups. We
show how our approach can mitigate bias on multiple sensitive attributes,
including gender, age, and occupation. We experimented on three real-world
datasets to evaluate the effectiveness of our re-ranking scheme in mitigating
bias in recommendations. Our results show how this approach helps mitigate
social bias with little to no degradation in performance.

### 15. A GenAI System for Improved FAIR Independent Biological Database Integration

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Syed N. Sakib, Kallol Naha, Sajratul Y. Rubaiat, Hasan M. Jamil
- **URL**: <http://arxiv.org/abs/2506.17934v1>
- **Submitted**: 2025-06-22 08:04:24
- **Topic Keywords**: query, queries, rag, search
- **Reason**: The paper introduces FAIRBridge, a natural language-based query processing system for biological databases, which shares some similarities with information retrieval and query understanding. However, the focus on biological databases and FAIR principles limits its relevance to the user's core research themes in information retrieval and search technologies.

#### Abstract
> Life sciences research increasingly requires identifying, accessing, and
effectively processing data from an ever-evolving array of information sources
on the Linked Open Data (LOD) network. This dynamic landscape places a
significant burden on researchers, as the quality of query responses depends
heavily on the selection and semantic integration of data sources --processes
that are often labor-intensive, error-prone, and costly. While the adoption of
FAIR (Findable, Accessible, Interoperable, and Reusable) data principles has
aimed to address these challenges, barriers to efficient and accurate
scientific data processing persist.
  In this paper, we introduce FAIRBridge, an experimental natural
language-based query processing system designed to empower scientists to
discover, access, and query biological databases, even when they are not
FAIR-compliant. FAIRBridge harnesses the capabilities of AI to interpret query
intents, map them to relevant databases described in scientific literature, and
generate executable queries via intelligent resource access plans. The system
also includes robust tools for mitigating low-quality query processing,
ensuring high fidelity and responsiveness in the information delivered.
  FAIRBridge's autonomous query processing framework enables users to explore
alternative data sources, make informed choices at every step, and leverage
community-driven crowd curation when needed. By providing a user-friendly,
automated hypothesis-testing platform in natural English, FAIRBridge
significantly enhances the integration and processing of scientific data,
offering researchers a powerful new tool for advancing their inquiries.

### 16. Beyond instruction-conditioning, MoTE: Mixture of Task Experts for Multi-task Embedding Models

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Miguel Romero, Shuoyang Ding, Corey D. Barret, Georgiana Dinu, George Karypis
- **URL**: <http://arxiv.org/abs/2506.17781v1>
- **Submitted**: 2025-06-21 18:28:25
- **Topic Keywords**: information retrieval, rag, retrieval, acl
- **Reason**: The paper discusses a new approach to dense embeddings, which is relevant to information retrieval and representation learning. However, the focus is on embedding specialization, which is not directly related to query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user.

#### Abstract
> Dense embeddings are fundamental to modern machine learning systems, powering
Retrieval-Augmented Generation (RAG), information retrieval, and representation
learning. While instruction-conditioning has become the dominant approach for
embedding specialization, its direct application to low-capacity models imposes
fundamental representational constraints that limit the performance gains
derived from specialization. In this paper, we analyze these limitations and
introduce the Mixture of Task Experts (MoTE) transformer block, which leverages
task-specialized parameters trained with Task-Aware Contrastive Learning
(\tacl) to enhance the model ability to generate specialized embeddings.
Empirical results show that MoTE achieves $64\%$ higher performance gains in
retrieval datasets ($+3.27 \rightarrow +5.21$) and $43\%$ higher performance
gains across all datasets ($+1.81 \rightarrow +2.60$). Critically, these gains
are achieved without altering instructions, training data, inference time, or
number of active parameters.

### 17. LettinGo: Explore User Profile Generation for Recommendation System

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Lu Wang, Di Zhang, Fangkai Yang, Pu Zhao, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Qingwei Lin, Weiwei Deng, Dongmei Zhang, Feng Sun, Qi Zhang
- **URL**: <http://arxiv.org/abs/2506.18309v1>
- **Submitted**: 2025-06-23 05:51:52
- **Comment**: 11 pages, 3 figures
- **Topic Keywords**: pairwise, rag, user behavior, recommend
- **Reason**: The paper focuses on user profiling for recommendation systems, which is somewhat related to my interests in Information Retrieval and Search technologies. However, the emphasis on recommendation systems and the lack of direct connection to query understanding, ranking models, or user behavior modeling in the context of search technologies limit its relevance to my core research themes.

#### Abstract
> User profiling is pivotal for recommendation systems, as it transforms raw
user interaction data into concise and structured representations that drive
personalized recommendations. While traditional embedding-based profiles lack
interpretability and adaptability, recent advances with large language models
(LLMs) enable text-based profiles that are semantically richer and more
transparent. However, existing methods often adhere to fixed formats that limit
their ability to capture the full diversity of user behaviors. In this paper,
we introduce LettinGo, a novel framework for generating diverse and adaptive
user profiles. By leveraging the expressive power of LLMs and incorporating
direct feedback from downstream recommendation tasks, our approach avoids the
rigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ
Direct Preference Optimization (DPO) to align the profile generator with
task-specific performance, ensuring that the profiles remain adaptive and
effective. LettinGo operates in three stages: (1) exploring diverse user
profiles via multiple LLMs, (2) evaluating profile quality based on their
impact in recommendation systems, and (3) aligning the profile generation
through pairwise preference data derived from task performance. Experimental
results demonstrate that our framework significantly enhances recommendation
accuracy, flexibility, and contextual awareness. This work enhances profile
generation as a key innovation for next-generation recommendation systems.

### 18. Shrinking the Generation-Verification Gap with Weak Verifiers

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Jon Saad-Falcon, E. Kelly Buchanan, Mayee F. Chen, Tzu-Heng Huang, Brendan McLaughlin, Tanvir Bhathal, Shang Zhu, Ben Athiwaratkun, Frederic Sala, Scott Linderman, Azalia Mirhoseini, Christopher R√©
- **URL**: <http://arxiv.org/abs/2506.18203v1>
- **Submitted**: 2025-06-22 23:38:15
- **Topic Keywords**: ranking, rag, rank, acl
- **Reason**: The paper is somewhat related to my research interests in Information Retrieval and Search technologies, as it discusses a framework for designing a strong verifier by combining multiple weak, imperfect verifiers. However, the focus on language models and verifier accuracy is not directly aligned with my primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Verifiers can improve language model capabilities by scoring and ranking
responses from generated candidates. Currently, high-quality verifiers are
either unscalable (e.g., humans) or limited in utility (e.g., tools like Lean).
While LM judges and reward models have become broadly useful as general-purpose
verifiers, a significant performance gap remains between them and oracle
verifiers (verifiers with perfect accuracy). To help close this gap, we
introduce Weaver, a framework for designing a strong verifier by combining
multiple weak, imperfect verifiers. We find weighted ensembles of verifiers,
which typically require learning from labeled data, significantly outperform
unweighted combinations due to differences in verifier accuracies. To reduce
dependency on labeled data, Weaver leverages weak supervision to estimate each
verifier's accuracy and combines outputs into a unified score that better
reflects true response quality. However, directly applying weak supervision
algorithms poses challenges, including inconsistent verifier output formats and
handling low-quality verifiers. Weaver addresses these using dataset statistics
to normalize outputs and filter specific verifiers. We study Weaver's
effectiveness in test-time repeated sampling, where a model generates multiple
candidate responses and selects one. Our evaluations show Weaver significantly
improves over Pass@1-performance when selecting the first candidate-across
reasoning and math tasks, achieving o3-mini-level accuracy with Llama 3.3 70B
Instruct as generator, and an ensemble of 70B or smaller judge and reward
models as verifiers (87.7% average). This gain mirrors the jump between GPT-4o
and o3-mini (69.0% vs. 86.7%), which required extensive finetuning and
post-training. To reduce computational costs of verifier ensembles, we train a
400M cross-encoder using Weaver's combined output scores.

### 19. LLM-Enhanced Multimodal Fusion for Cross-Domain Sequential Recommendation

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Wangyu Wu, Zhenhong Chen, Xianglin Qiu, Siqi Song, Xiaowei Huang, Fei Ma, Jimin Xiao
- **URL**: <http://arxiv.org/abs/2506.17966v1>
- **Submitted**: 2025-06-22 09:53:21
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2504.15085
- **Topic Keywords**: rag, user behavior, recommend, commerce, e-commerce
- **Reason**: The paper focuses on Cross-Domain Sequential Recommendation, which is not directly related to the user's primary research interests in Information Retrieval and Search technologies. Although it involves multimodal data integration, the approach is more relevant to recommender systems, which is a secondary interest of the user. The paper's emphasis on large language models and multimodal fusion is not directly applicable to the user's core research themes.

#### Abstract
> Cross-Domain Sequential Recommendation (CDSR) predicts user behavior by
leveraging historical interactions across multiple domains, focusing on
modeling cross-domain preferences and capturing both intra- and inter-sequence
item relationships. We propose LLM-Enhanced Multimodal Fusion for Cross-Domain
Sequential Recommendation (LLM-EMF), a novel and advanced approach that
enhances textual information with Large Language Models (LLM) knowledge and
significantly improves recommendation performance through the fusion of visual
and textual data. Using the frozen CLIP model, we generate image and text
embeddings, thereby enriching item representations with multimodal data. A
multiple attention mechanism jointly learns both single-domain and cross-domain
preferences, effectively capturing and understanding complex user interests
across diverse domains. Evaluations conducted on four e-commerce datasets
demonstrate that LLM-EMF consistently outperforms existing methods in modeling
cross-domain user preferences, thereby highlighting the effectiveness of
multimodal data integration and its advantages in enhancing sequential
recommendation systems. Our source code will be released.

### 20. Enhancing Document Retrieval in COVID-19 Research: Leveraging Large Language Models for Hidden Relation Extraction

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Hoang-An Trieu, Dinh-Truong Do, Chau Nguyen, Vu Tran, Minh Le Nguyen
- **URL**: <http://arxiv.org/abs/2506.18311v1>
- **Submitted**: 2025-06-23 05:55:53
- **Comment**: In the Proceedings of SCIDOCA 2024
- **Topic Keywords**: rag, retrieval, search
- **Reason**: The paper is somewhat related to information retrieval, as it discusses a retrieval system for COVID-19 research. However, the focus is on leveraging large language models for hidden relation extraction, which is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling. The paper does not explicitly mention search technologies, ranking models, or click models, making it only loosely relevant to the user's research interests.

#### Abstract
> In recent years, with the appearance of the COVID-19 pandemic, numerous
publications relevant to this disease have been issued. Because of the massive
volume of publications, an efficient retrieval system is necessary to provide
researchers with useful information if an unexpected pandemic happens so
suddenly, like COVID-19. In this work, we present a method to help the
retrieval system, the Covrelex-SE system, to provide more high-quality search
results. We exploited the power of the large language models (LLMs) to extract
the hidden relationships inside the unlabeled publication that cannot be found
by the current parsing tools that the system is using. Since then, help the
system to have more useful information during retrieval progress.

### 21. AggTruth: Contextual Hallucination Detection using Aggregated Attention Scores in LLMs

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Piotr Matys, Jan Eliasz, Konrad Kie≈Çczy≈Ñski, Miko≈Çaj Langner, Teddy Ferdinan, Jan Koco≈Ñ, Przemys≈Çaw Kazienko
- **URL**: <http://arxiv.org/abs/2506.18628v1>
- **Submitted**: 2025-06-23 13:35:05
- **Comment**: ICCS 2025 Workshops
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on Large Language Models (LLMs) and contextual hallucination detection, which is not directly related to Information Retrieval (IR) or Search technologies. While it touches on attention scores, which are relevant to query understanding, the paper's primary focus is on LLMs and their limitations, rather than ranking models or user behavior modeling.

#### Abstract
> In real-world applications, Large Language Models (LLMs) often hallucinate,
even in Retrieval-Augmented Generation (RAG) settings, which poses a
significant challenge to their deployment. In this paper, we introduce
AggTruth, a method for online detection of contextual hallucinations by
analyzing the distribution of internal attention scores in the provided context
(passage). Specifically, we propose four different variants of the method, each
varying in the aggregation technique used to calculate attention scores. Across
all LLMs examined, AggTruth demonstrated stable performance in both same-task
and cross-task setups, outperforming the current SOTA in multiple scenarios.
Furthermore, we conducted an in-depth analysis of feature selection techniques
and examined how the number of selected attention heads impacts detection
performance, demonstrating that careful selection of heads is essential to
achieve optimal results.

### 22. When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Manu Pande, Shahil Kumar, Anay Yatin Damle
- **URL**: <http://arxiv.org/abs/2506.18535v1>
- **Submitted**: 2025-06-23 11:46:05
- **Topic Keywords**: ranking, rank
- **Reason**: The paper's focus on fine-tuning pre-trained transformer models for passage ranking is somewhat related to my interests in Information Retrieval and Search technologies. However, the specific context of MS MARCO passage ranking and the emphasis on model architecture innovations rather than query understanding or ranking models makes it less directly relevant to my core research themes.

#### Abstract
> This paper investigates the counterintuitive phenomenon where fine-tuning
pre-trained transformer models degrades performance on the MS MARCO passage
ranking task. Through comprehensive experiments involving five model
variants-including full parameter fine-tuning and parameter efficient LoRA
adaptations-we demonstrate that all fine-tuning approaches underperform the
base sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our
analysis reveals that fine-tuning disrupts the optimal embedding space
structure learned during the base model's extensive pre-training on 1 billion
sentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations
show progressive embedding space flattening, while training dynamics analysis
and computational efficiency metrics further support our findings. These
results challenge conventional wisdom about transfer learning effectiveness on
saturated benchmarks and suggest architectural innovations may be necessary for
meaningful improvements.

### 23. Team LA at SCIDOCA shared task 2025: Citation Discovery via relation-based zero-shot retrieval

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Trieu An, Long Nguyen, Minh Le Nguyen
- **URL**: <http://arxiv.org/abs/2506.18316v1>
- **Submitted**: 2025-06-23 06:01:21
- **Comment**: In the Proceedings of SCIDOCA 2025
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on citation discovery, which is a specific problem in information retrieval. While it involves natural language processing and retrieval techniques, it is not directly related to query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's emphasis on citation prediction and relational features is somewhat relevant, but it does not align with the user's primary focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> The Citation Discovery Shared Task focuses on predicting the correct citation
from a given candidate pool for a given paragraph. The main challenges stem
from the length of the abstract paragraphs and the high similarity among
candidate abstracts, making it difficult to determine the exact paper to cite.
To address this, we develop a system that first retrieves the top-k most
similar abstracts based on extracted relational features from the given
paragraph. From this subset, we leverage a Large Language Model (LLM) to
accurately identify the most relevant citation. We evaluate our framework on
the training dataset provided by the SCIDOCA 2025 organizers, demonstrating its
effectiveness in citation prediction.

### 24. PERSCEN: Learning Personalized Interaction Pattern and Scenario Preference for Multi-Scenario Matching

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Haotong Du, Yaqing Wang, Fei Xiong, Lei Shao, Ming Liu, Hao Gu, Quanming Yao, Zhen Wang
- **URL**: <http://arxiv.org/abs/2506.18382v1>
- **Submitted**: 2025-06-23 08:15:16
- **Comment**: Accepted by KDD 2025
- **Topic Keywords**: rag, recommend
- **Reason**: The paper proposes a method for multi-scenario matching, which is not directly related to information retrieval or search technologies. While it involves user behavior modeling, the focus is on recommender systems rather than query understanding, ranking models, or click models. The paper's emphasis on scenario-aware preferences and user-specific modeling is somewhat relevant to my interests in NLP and data mining, but the connection is not strong enough to warrant a higher score.

#### Abstract
> With the expansion of business scales and scopes on online platforms,
multi-scenario matching has become a mainstream solution to reduce maintenance
costs and alleviate data sparsity. The key to effective multi-scenario
recommendation lies in capturing both user preferences shared across all
scenarios and scenario-aware preferences specific to each scenario. However,
existing methods often overlook user-specific modeling, limiting the generation
of personalized user representations. To address this, we propose PERSCEN, an
innovative approach that incorporates user-specific modeling into
multi-scenario matching. PERSCEN constructs a user-specific feature graph based
on user characteristics and employs a lightweight graph neural network to
capture higher-order interaction patterns, enabling personalized extraction of
preferences shared across scenarios. Additionally, we leverage vector
quantization techniques to distil scenario-aware preferences from users'
behavior sequence within individual scenarios, facilitating user-specific and
scenario-aware preference modeling. To enhance efficient and flexible
information transfer, we introduce a progressive scenario-aware gated linear
unit that allows fine-grained, low-latency fusion. Extensive experiments
demonstrate that PERSCEN outperforms existing methods. Further efficiency
analysis confirms that PERSCEN effectively balances performance with
computational cost, ensuring its practicality for real-world industrial
systems.

### 25. Evolving Prompts In-Context: An Open-ended, Self-replicating Perspective

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Jianyu Wang, Zhiqiang Hu, Lidong Bing
- **URL**: <http://arxiv.org/abs/2506.17930v1>
- **Submitted**: 2025-06-22 07:53:07
- **Comment**: ICML 2025, and Code will be released at:
  https://github.com/jianyu-cs/PromptQuine/
- **Topic Keywords**: rag, search
- **Reason**: The paper proposes a novel prompt design paradigm for large language models, which is related to information retrieval and search technologies. However, the focus is on prompt optimization and language models, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's relevance is somewhat limited, but it may still be of interest to researchers exploring the intersection of NLP and IR.

#### Abstract
> We propose a novel prompt design paradigm that challenges conventional wisdom
in large language model (LLM) prompting. While conventional wisdom prioritizes
well-crafted instructions and demonstrations for in-context learning (ICL), we
show that pruning random demonstrations into seemingly incoherent "gibberish"
can remarkably improve performance across diverse tasks. Notably, the
"gibberish" always matches or surpasses state-of-the-art automatic prompt
optimization techniques, achieving substantial gains regardless of LLM
alignment. Nevertheless, discovering an effective pruning strategy is
non-trivial, as existing attribution methods and prompt compression algorithms
fail to deliver robust results, let alone human intuition. In terms of this, we
propose a self-discover prompt optimization framework, PromptQuine, an
evolutionary search framework that automatically searches for the pruning
strategy by itself using only low-data regimes. Much like the emergent
complexity in nature--such as symbiosis and self-organization--arising in
response to resource constraints, our framework evolves and refines
unconventional yet highly effective prompts by leveraging only the tokens
present within the context. We demonstrate its effectiveness across
classification, multi-choice question answering, generation and math reasoning
tasks across LLMs, while achieving decent runtime efficiency. We hope our
findings can guide mechanistic studies on in-context learning, and provide a
call to action, to pave the way for more open-ended search algorithms for more
effective LLM prompting.

### 26. MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Junjie Zhang, Guozheng Ma, Shunyu Liu, Haoyu Wang, Jiaxing Huang, Ting-En Lin, Fei Huang, Yongbin Li, Dacheng Tao
- **URL**: <http://arxiv.org/abs/2506.18485v1>
- **Submitted**: 2025-06-23 10:37:57
- **Topic Keywords**: rag
- **Reason**: The paper explores reinforcement learning for large language models, which is a related topic to information retrieval and search technologies. However, the focus on language models and reasoning capabilities is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle
complex reasoning tasks. However, existing RLVR methods overlook one of the
most distinctive capabilities of LLMs, their in-context learning ability, as
prominently demonstrated by the success of Chain-of-Thought (CoT) prompting.
This motivates us to explore how reinforcement learning can be effectively
combined with in-context learning to better improve the reasoning capabilities
of LLMs. In this paper, we introduce Motivation-enhanced Reinforcement
Finetuning} (MeRF), an intuitive yet effective method enhancing reinforcement
learning of LLMs by involving ``telling LLMs the rules of the game''.
Specifically, MeRF directly injects the reward specification into the prompt,
which serves as an in-context motivation for model to improve its responses
with awareness of the optimization objective. This simple modification
leverages the in-context learning ability of LLMs aligning generation with
optimization, thereby incentivizing the model to generate desired outputs from
both inner motivation and external reward. Empirical evaluations on the Knights
and Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that
\texttt{MeRF} achieves substantial performance gains over baselines. Moreover,
ablation studies show that performance improves with greater consistency
between the in-context motivation and the external reward function, while the
model also demonstrates an ability to adapt to misleading motivations through
reinforcement learning.

### 27. AdapThink: Adaptive Thinking Preferences for Reasoning Language Model

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Xu Wan, Wei Wang, Wenyue Xu, Wotao Yin, Jie Song, Mingyang Sun
- **URL**: <http://arxiv.org/abs/2506.18237v1>
- **Submitted**: 2025-06-23 02:06:04
- **Topic Keywords**: rag
- **Reason**: The paper proposes an adaptive post-training framework for language models, focusing on efficient thinking and maintaining performance. While it's related to NLP and language models, it doesn't directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest for you. The paper's focus on reasoning and self-reflection is somewhat tangential to your primary research themes.

#### Abstract
> Reinforcement Learning (RL)-based post-training has significantly advanced
the complex reasoning capabilities of language models, fostering sophisticated
self-reflection processes. However, this ``slow thinking'' paradigm presents a
critical challenge to reasoning efficiency: models may expend excessive
computation on simple questions and shift reasoning prematurely for complex
ones. Previous mechanisms typically rely on static length budgets or predefined
rules, lacking the adaptability for varying question complexities and models'
evolving capabilities. To this end, we propose AdapThink, an adaptive
post-training framework designed to induce more efficient thinking while
maintaining the performance of reasoning language models. Specifically,
AdapThink incorporates two key mechanisms: 1) A group-relative reward function
that leverages model confidence and response's characteristic to dynamically
adjust the preference of reflection-related transition words without resorting
to a fixed length preference. 2) A diversity-aware sampling mechanism that
balances the training group's solution accuracy with reasoning diversity via an
entropy-guided score. Experiments on several mathematical reasoning datasets
with DeepSeek-distilled models demonstrate AdapThink's advantages in enabling
adaptive reasoning patterns and mitigating the inefficiencies.

### 28. Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering

- **LLM Score**: 2
- **Keyword Score**: 10
- **Authors**: Binquan Ji, Haibo Luo, Yifei Lu, Lei Hei, Jiaqi Wang, Tingjing Liao, Lingyu Wang, Shichao Wang, Feiliang Ren
- **URL**: <http://arxiv.org/abs/2506.17692v1>
- **Submitted**: 2025-06-21 11:55:27
- **Topic Keywords**: query, queries, rag, retrieval
- **Reason**: The paper focuses on multi-hop question answering, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions language models and retrieval, the context is primarily concerned with question answering and not search or ranking models. The paper's emphasis on resource-constrained environments and lightweight models also does not align with the user's interests in deep semantic understanding and real-time relevance optimization.

#### Abstract
> Knowledge-intensive multi-hop question answering (QA) tasks, which require
integrating evidence from multiple sources to address complex queries, often
necessitate multiple rounds of retrieval and iterative generation by large
language models (LLMs). However, incorporating many documents and extended
contexts poses challenges -such as hallucinations and semantic drift-for
lightweight LLMs with fewer parameters. This work proposes a novel framework
called DEC (Dynamic Enhancement Chain). DEC first decomposes complex questions
into logically coherent subquestions to form a hallucination-free reasoning
chain. It then iteratively refines these subquestions through context-aware
rewriting to generate effective query formulations. For retrieval, we introduce
a lightweight discriminative keyword extraction module that leverages extracted
keywords to achieve targeted, precise document recall with relatively low
computational overhead. Extensive experiments on three multi-hop QA datasets
demonstrate that DEC performs on par with or surpasses state-of-the-art
benchmarks while significantly reducing token consumption. Notably, our
approach attains state-of-the-art results on models with 8B parameters,
showcasing its effectiveness in various scenarios, particularly in
resource-constrained environments.

### 29. Cite Pretrain: Retrieval-Free Knowledge Attribution for Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Yukun Huang, Sanxing Chen, Jian Pei, Manzil Zaheer, Bhuwan Dhingra
- **URL**: <http://arxiv.org/abs/2506.17585v1>
- **Submitted**: 2025-06-21 04:48:05
- **Topic Keywords**: retriever, query, retrieval
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on language models and citation attribution, which is a topic in Natural Language Processing, but not a central match for your interests.

#### Abstract
> Trustworthy language models should provide both correct and verifiable
answers. While language models can sometimes attribute their outputs to
pretraining data, their citations are often unreliable due to hallucination. As
a result, current systems insert citations by querying an external retriever at
inference time, introducing latency, infrastructure dependence, and
vulnerability to retrieval noise. We explore whether LLMs can be made to
reliably attribute to the documents seen during (continual)
pretraining--without test-time retrieval--by revising the training process. To
evaluate this, we release CitePretrainBench, a benchmark that mixes real-world
corpora (Wikipedia, Common Crawl, arXiv) with novel, unseen documents and
probes both short-form (single fact) and long-form (multi-fact) citation tasks.
Our approach follows a two-stage process: (1) continual pretraining to bind
facts to persistent document identifiers, and (2) instruction tuning to elicit
citation behavior. We find that simple Passive Indexing, which appends an
identifier to each document, helps memorize verbatim text but fails on
paraphrased or compositional facts. Instead, we propose Active Indexing, which
continually pretrains on synthetic QA pairs that (1) restate each fact in
diverse compositional forms, and (2) require bidirectional source-to-fact and
fact-to-source generation, jointly teaching the model to generate content from
a cited source and to attribute its own answers. Experiments with Qwen2.5-7B
and 3B show that Active Indexing consistently outperforms Passive Indexing
across all tasks and models, with citation precision gains up to 30.2 percent.
Our ablation studies reveal that performance continues to improve as we scale
the amount of augmented data, showing a clear upward trend even at 16 times the
original token count.

### 30. An Audio-centric Multi-task Learning Framework for Streaming Ads Targeting on Spotify

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Shivam Verma, Vivian Chen, Darren Mei
- **URL**: <http://arxiv.org/abs/2506.18735v1>
- **Submitted**: 2025-06-23 15:11:43
- **Comment**: Accepted at KDD 2025
- **Topic Keywords**: click, ctr, click-through rate, recommend
- **Reason**: The paper focuses on audio-centric ad targeting on Spotify, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on multi-task learning and deep learning, the context is specific to audio ads and not relevant to the user's broader interests.

#### Abstract
> Spotify, a large-scale multimedia platform, attracts over 675 million monthly
active users who collectively consume millions of hours of music, podcasts,
audiobooks, and video content. This diverse content consumption pattern
introduces unique challenges for computational advertising, which must
effectively integrate a variety of ad modalities, including audio, video, and
display, within a single user experience. Traditional ad recommendation models,
primarily designed for foregrounded experiences, often struggle to reconcile
the platform's inherent audio-centrality with the demands of optimizing ad
performance across multiple formats and modalities. To overcome these
challenges, we introduce Cross-modal Adaptive Mixture-of-Experts (CAMoE), a
novel framework for optimizing click-through rate (CTR) prediction in both
audio-centric and multi-modal settings. CAMoE enhances traditional
mixture-of-experts models by incorporating modality-aware task grouping,
adaptive loss masking, and deep-cross networks (DCN) to capture complex feature
interactions within a multi-modal ad ecosystem. Through extensive ablation
studies, we demonstrate that this approach achieves near Pareto-optimal
performance across audio, video, and display ad formats, significantly
improving AUC-PR compared to conventional single-task and content-based
multi-task learning baselines. When deployed at scale on Spotify's ad serving
platform, CAMoE delivered substantial gains, yielding a 14.5% increase in CTR
for audio ads, a 1.3% increase for video ads, and a 4.8% reduction in expected
cost-per-click (eCPC) for audio slots.

### 31. InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Fuyu Wang, Jiangtong Li, Kun Zhu, Changjun Jiang
- **URL**: <http://arxiv.org/abs/2506.18102v1>
- **Submitted**: 2025-06-22 17:14:29
- **Comment**: 20 pages; Accepted to ACL 2025 Main
- **Topic Keywords**: relevance, rag, retrieval
- **Reason**: The paper focuses on debating systems and evaluation metrics, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions retrieval augmented generation, the context is not relevant to the user's research interests in IR and NLP.

#### Abstract
> With the rapid advancements in large language models (LLMs), debating tasks,
such as argument quality assessment and debate process simulation, have made
significant progress. However, existing LLM-based debating systems focus on
responding to specific arguments while neglecting objective assessments such as
authenticity and logical validity. Furthermore, these systems lack a structured
approach to optimize across various dimensions$-$including evaluation metrics,
chain-of-thought (CoT) reasoning, and multi-turn debate refinement$-$thereby
limiting their effectiveness. To address these interconnected challenges, we
propose a dual-component framework: (1) $\textbf{InspireScore}$, a novel
evaluation system that establishes a multi-dimensional assessment architecture
incorporating four subjective criteria (emotional appeal, argument clarity,
argument arrangement, and topic relevance) alongside two objective metrics
(fact authenticity and logical validity); and (2) $\textbf{InspireDebate}$, an
optimized debating framework employing a phased optimization approach through
CoT reasoning enhancement, multi-dimensional Direct Preference Optimization
(DPO), and real-time knowledge grounding via web-based Retrieval Augmented
Generation (Web-RAG). Empirical evaluations demonstrate that
$\textbf{InspireScore}$ achieves 44$\%$ higher correlation with expert
judgments compared to existing methods, while $\textbf{InspireDebate}$ shows
significant improvements, outperforming baseline models by 57$\%$. Source code
is available at https://github.com/fywang12/InspireDebate.

### 32. LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Yuhao Wu, Yushi Bai, Zhiqiang Hu, Roy Ka-Wei Lee, Juanzi Li
- **URL**: <http://arxiv.org/abs/2506.18841v1>
- **Submitted**: 2025-06-23 16:59:02
- **Topic Keywords**: ltr, rag
- **Reason**: This paper focuses on ultra-long text generation using reinforcement learning, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the scope is limited to text generation and does not address ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest.

#### Abstract
> Ultra-long generation by large language models (LLMs) is a widely demanded
scenario, yet it remains a significant challenge due to their maximum
generation length limit and overall quality degradation as sequence length
increases. Previous approaches, exemplified by LongWriter, typically rely on
''teaching'', which involves supervised fine-tuning (SFT) on synthetic
long-form outputs. However, this strategy heavily depends on synthetic SFT
data, which is difficult and costly to construct, often lacks coherence and
consistency, and tends to be overly artificial and structurally monotonous. In
this work, we propose an incentivization-based approach that, starting entirely
from scratch and without relying on any annotated or synthetic data, leverages
reinforcement learning (RL) to foster the emergence of ultra-long, high-quality
text generation capabilities in LLMs. We perform RL training starting from a
base model, similar to R1-Zero, guiding it to engage in reasoning that
facilitates planning and refinement during the writing process. To support
this, we employ specialized reward models that steer the LLM towards improved
length control, writing quality, and structural formatting. Experimental
evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,
consistently outperforms traditional SFT methods on long-form writing tasks,
achieving state-of-the-art results across all metrics on WritingBench and
Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and
Qwen3-235B. We open-source our data and model checkpoints under
https://huggingface.co/THU-KEG/LongWriter-Zero-32B

### 33. ConciseHint: Boosting Efficient Reasoning via Continuous Concise Hints during Generation

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Siao Tang, Xinyin Ma, Gongfan Fang, Xinchao Wang
- **URL**: <http://arxiv.org/abs/2506.18810v1>
- **Submitted**: 2025-06-23 16:20:44
- **Comment**: Codes are available at https://github.com/tsa18/ConciseHint
- **Topic Keywords**: query, rag
- **Reason**: The paper focuses on improving the efficiency of reasoning models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions the use of large reasoning models, the primary concern is not on ranking models or user behavior modeling, making it only loosely relevant to the user's research interests.

#### Abstract
> Recent advancements in large reasoning models (LRMs) like DeepSeek-R1 and
OpenAI o1 series have achieved notable performance enhancements on complex
reasoning tasks by scaling up the generation length by Chain-of-Thought (CoT).
However, an emerging issue is their inclination to produce excessively verbose
reasoning processes, leading to the inefficiency problem. Existing literature
on improving efficiency mainly adheres to the before-reasoning paradigms such
as prompting and reasoning or fine-tuning and reasoning, but ignores the
promising direction of directly encouraging the model to speak concisely by
intervening during the generation of reasoning. In order to fill the blank, we
propose a framework dubbed ConciseHint, which continuously encourages the
reasoning model to speak concisely by injecting the textual hint (manually
designed or trained on the concise data) during the token generation of the
reasoning process. Besides, ConciseHint is adaptive to the complexity of the
query by adaptively adjusting the hint intensity, which ensures it will not
undermine model performance. Experiments on the state-of-the-art LRMs,
including DeepSeek-R1 and Qwen-3 series, demonstrate that our method can
effectively produce concise reasoning processes while maintaining performance
well. For instance, we achieve a reduction ratio of 65\% for the reasoning
length on GSM8K benchmark with Qwen-3 4B with nearly no accuracy loss.

### 34. MLLP-VRAIN UPV system for the IWSLT 2025 Simultaneous Speech Translation Translation task

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Jorge Iranzo-S√°nchez, Javier Iranzo-S√°nchez, Adri√† Gim√©nez, Jorge Civera, Alfons Juan
- **URL**: <http://arxiv.org/abs/2506.18828v1>
- **Submitted**: 2025-06-23 16:44:01
- **Comment**: IWSLT 2025 System Description
- **Topic Keywords**: search, acl
- **Reason**: The paper focuses on simultaneous speech translation, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions pre-trained models and adaptation techniques, the context is not relevant to the user's primary research interests in IR and NLP.

#### Abstract
> This work describes the participation of the MLLP-VRAIN research group in the
shared task of the IWSLT 2025 Simultaneous Speech Translation track. Our
submission addresses the unique challenges of real-time translation of
long-form speech by developing a modular cascade system that adapts strong
pre-trained models to streaming scenarios. We combine Whisper Large-V3-Turbo
for ASR with the multilingual NLLB-3.3B model for MT, implementing lightweight
adaptation techniques rather than training new end-to-end models from scratch.
Our approach employs document-level adaptation with prefix training to enhance
the MT model's ability to handle incomplete inputs, while incorporating
adaptive emission policies including a wait-$k$ strategy and RALCP for managing
the translation stream. Specialized buffer management techniques and
segmentation strategies ensure coherent translations across long audio
sequences. Experimental results on the ACL60/60 dataset demonstrate that our
system achieves a favorable balance between translation quality and latency,
with a BLEU score of 31.96 and non-computational-aware StreamLAAL latency of
2.94 seconds. Our final model achieves a preliminary score on the official test
set (IWSLT25Instruct) of 29.8 BLEU. Our work demonstrates that carefully
adapted pre-trained components can create effective simultaneous translation
systems for long-form content without requiring extensive in-domain parallel
data or specialized end-to-end training.

### 35. Airalogy: AI-empowered universal data digitization for research automation

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Zijie Yang, Qiji Zhou, Fang Guo, Sijie Zhang, Yexun Xi, Jinglei Nie, Yudian Zhu, Liping Huang, Chou Wu, Yonghe Xia, Xiaoyu Ma, Yingming Pu, Panzhong Lu, Junshu Pan, Mingtao Chen, Tiannan Guo, Yanmei Dou, Hongyu Chen, Anping Zeng, Jiaxing Huang, Tian Xu, Yue Zhang
- **URL**: <http://arxiv.org/abs/2506.18586v1>
- **Submitted**: 2025-06-23 12:43:16
- **Comment**: 146 pages, 6 figures, 49 supplementary figures
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on developing a platform for standardized data digitization, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions AI and research automation, the context is more focused on data management and scientific workflows rather than search or retrieval.

#### Abstract
> Research data are the foundation of Artificial Intelligence (AI)-driven
science, yet current AI applications remain limited to a few fields with
readily available, well-structured, digitized datasets. Achieving comprehensive
AI empowerment across multiple disciplines is still out of reach. Present-day
research data collection is often fragmented, lacking unified standards,
inefficiently managed, and difficult to share. Creating a single platform for
standardized data digitization needs to overcome the inherent challenge of
balancing between universality (supporting the diverse, ever-evolving needs of
various disciplines) and standardization (enforcing consistent formats to fully
enable AI). No existing platform accommodates both facets. Building a truly
multidisciplinary platform requires integrating scientific domain knowledge
with sophisticated computing skills. Researchers often lack the computational
expertise to design customized and standardized data recording methods, whereas
platform developers rarely grasp the intricate needs of multiple scientific
domains. These gaps impede research data standardization and hamper AI-driven
progress. In this study, we address these challenges by developing Airalogy
(https://airalogy.com), the world's first AI- and community-driven platform
that balances universality and standardization for digitizing research data
across multiple disciplines. Airalogy represents entire research workflows
using customizable, standardized data records and offers an advanced AI
research copilot for intelligent Q&A, automated data entry, analysis, and
research automation. Already deployed in laboratories across all four schools
of Westlake University, Airalogy has the potential to accelerate and automate
scientific innovation in universities, industry, and the global research
community-ultimately benefiting humanity as a whole.

### 36. Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Batool Haider, Atmika Gorti, Aman Chadha, Manas Gaur
- **URL**: <http://arxiv.org/abs/2506.18116v1>
- **Submitted**: 2025-06-22 18:00:16
- **Comment**: 19 Pages, 7 Figures, 4 Tables (Note: Under Review)
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on Large Language Models (LLMs) in mental healthcare, exploring biases and proposing debiasing techniques. While it touches on question answering and natural language processing, the primary topic is not directly related to information retrieval, search technologies, or user behavior modeling, which are the user's core research interests.

#### Abstract
> Large Language Models (LLMs) in mental healthcare risk propagating biases
that reinforce stigma and harm marginalized groups. While previous research
identified concerning trends, systematic methods for detecting intersectional
biases remain limited. This work introduces a multi-hop question answering
(MHQA) framework to explore LLM response biases in mental health discourse. We
analyze content from the Interpretable Mental Health Instruction (IMHI) dataset
across symptom presentation, coping mechanisms, and treatment approaches. Using
systematic tagging across age, race, gender, and socioeconomic status, we
investigate bias patterns at demographic intersections. We evaluate four LLMs:
Claude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic
disparities across sentiment, demographics, and mental health conditions. Our
MHQA approach demonstrates superior detection compared to conventional methods,
identifying amplification points where biases magnify through sequential
reasoning. We implement two debiasing techniques: Roleplay Simulation and
Explicit Bias Reduction, achieving 66-94% bias reductions through few-shot
prompting with BBQ dataset examples. These findings highlight critical areas
where LLMs reproduce mental healthcare biases, providing actionable insights
for equitable AI development.

### 37. Aligning Frozen LLMs by Reinforcement Learning: An Iterative Reweight-then-Optimize Approach

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Xinnan Zhang, Chenliang Li, Siliang Zeng, Jiaxiang Li, Zhongruo Wang, Kaixiang Lin, Songtao Lu, Alfredo Garcia, Mingyi Hong
- **URL**: <http://arxiv.org/abs/2506.17828v1>
- **Submitted**: 2025-06-21 21:49:02
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on aligning frozen language models using reinforcement learning, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on optimization and value functions, the context is different from the user's primary research interests.

#### Abstract
> Aligning large language models (LLMs) with human preferences usually requires
fine-tuning methods such as RLHF and DPO. These methods directly optimize the
model parameters, so they cannot be used in test-time to improve model
performance, nor are they applicable when the model weights are not accessible.
In contrast, test-time methods sidestep weight updates by leveraging reward
functions to guide and improve output quality. However, they incur high
inference costs, and their one-shot guidance is often based on imperfect reward
or value functions, leading to suboptimal outputs. In this work, we present a
method named Iterative Reweight-then-Optimize (IRO), a reinforcement learning
(RL) framework that performs RL-style alignment of the (frozen) base model
without touching its parameters. During training, each iteration (i) samples
candidates from the base model, (ii) resamples using current value functions,
and (iii) trains a new lightweight value function that guides the next decoding
pass. At test time, the value functions are used to guide the base model
generation via a search-based optimization process. Notably, users can apply
IRO to align a model on their own dataset, similar to OpenAI's reinforcement
fine-tuning (RFT), but without requiring access to the model weights.

### 38. Zero-Shot Conversational Stance Detection: Dataset and Approaches

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yuzhe Ding, Kang He, Bobo Li, Li Zheng, Haijun He, Fei Li, Chong Teng, Donghong Ji
- **URL**: <http://arxiv.org/abs/2506.17693v1>
- **Submitted**: 2025-06-21 12:02:06
- **Comment**: ACL 2025 (Findings)
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on conversational stance detection, which is not directly related to information retrieval, search technologies, or query understanding. While it involves text analysis, the topic is more specific to natural language processing and social media analysis, which is not a primary area of interest for you.

#### Abstract
> Stance detection, which aims to identify public opinion towards specific
targets using social media data, is an important yet challenging task. With the
increasing number of online debates among social media users, conversational
stance detection has become a crucial research area. However, existing
conversational stance detection datasets are restricted to a limited set of
specific targets, which constrains the effectiveness of stance detection models
when encountering a large number of unseen targets in real-world applications.
To bridge this gap, we manually curate a large-scale, high-quality zero-shot
conversational stance detection dataset, named ZS-CSD, comprising 280 targets
across two distinct target types. Leveraging the ZS-CSD dataset, we propose
SITPCL, a speaker interaction and target-aware prototypical contrastive
learning model, and establish the benchmark performance in the zero-shot
setting. Experimental results demonstrate that our proposed SITPCL model
achieves state-of-the-art performance in zero-shot conversational stance
detection. Notably, the SITPCL model attains only an F1-macro score of 43.81%,
highlighting the persistent challenges in zero-shot conversational stance
detection.

### 39. Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yang Wu, Yifan Zhang, Yurong Wu, Yuran Wang, Junkai Zhang, Jian Cheng
- **URL**: <http://arxiv.org/abs/2506.17637v1>
- **Submitted**: 2025-06-21 08:42:27
- **Comment**: 17 pages, 12 figures
- **Topic Keywords**: rag, search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on Large Language Models (LLMs) and optimization modeling in Operations Research (OR), which is outside your primary focus.

#### Abstract
> Large Language Models (LLMs) have revolutionized various domains but
encounter substantial challenges in tackling optimization modeling tasks for
Operations Research (OR), particularly when dealing with complex problem. In
this work, we propose Step-Opt-Instruct, a framework that augments existing
datasets and generates high-quality fine-tuning data tailored to optimization
modeling. Step-Opt-Instruct employs iterative problem generation to
systematically increase problem complexity and stepwise validation to
rigorously verify data, preventing error propagation and ensuring the quality
of the generated dataset. Leveraging this framework, we fine-tune open-source
LLMs, including LLaMA-3-8B and Mistral-7B, to develop Step-Opt--a model that
achieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and
IndustryOR. Extensive experiments demonstrate the superior performance of
Step-Opt, especially in addressing complex OR tasks, with a notable 17.01\%
improvement in micro average accuracy on difficult problems. These findings
highlight the effectiveness of combining structured validation with gradual
problem refinement to advance the automation of decision-making processes using
LLMs.The code and dataset are available at https://github.com/samwu-learn/Step.

### 40. LLM-driven Medical Report Generation via Communication-efficient Heterogeneous Federated Learning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Haoxuan Che, Haibo Jin, Zhengrui Guo, Yi Lin, Cheng Jin, Hao Chen
- **URL**: <http://arxiv.org/abs/2506.17562v1>
- **Submitted**: 2025-06-21 03:13:08
- **Topic Keywords**: rag, rank
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The topic of Medical Report Generation and Federated Learning is outside your primary focus, and the paper does not address query understanding, ranking models, or user behavior modeling.

#### Abstract
> LLMs have demonstrated significant potential in Medical Report Generation
(MRG), yet their development requires large amounts of medical image-report
pairs, which are commonly scattered across multiple centers. Centralizing these
data is exceptionally challenging due to privacy regulations, thereby impeding
model development and broader adoption of LLM-driven MRG models. To address
this challenge, we present FedMRG, the first framework that leverages Federated
Learning (FL) to enable privacy-preserving, multi-center development of
LLM-driven MRG models, specifically designed to overcome the critical challenge
of communication-efficient LLM training under multi-modal data heterogeneity.
To start with, our framework tackles the fundamental challenge of communication
overhead in FL-LLM tuning by employing low-rank factorization to efficiently
decompose parameter updates, significantly reducing gradient transmission costs
and making LLM-driven MRG feasible in bandwidth-constrained FL settings.
Furthermore, we observed the dual heterogeneity in MRG under the FL scenario:
varying image characteristics across medical centers, as well as diverse
reporting styles and terminology preferences. To address this, we further
enhance FedMRG with (1) client-aware contrastive learning in the MRG encoder,
coupled with diagnosis-driven prompts, which capture both globally
generalizable and locally distinctive features while maintaining diagnostic
accuracy; and (2) a dual-adapter mutual boosting mechanism in the MRG decoder
that harmonizes generic and specialized adapters to address variations in
reporting styles and terminology. Through extensive evaluation of our
established FL-MRG benchmark, we demonstrate the generalizability and
adaptability of FedMRG, underscoring its potential in harnessing multi-center
data and generating clinically accurate reports while maintaining communication
efficiency.

### 41. UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Jinhao Duan, James Diffenderfer, Sandeep Madireddy, Tianlong Chen, Bhavya Kailkhura, Kaidi Xu
- **URL**: <http://arxiv.org/abs/2506.17419v1>
- **Submitted**: 2025-06-20 18:34:04
- **Comment**: 19 pages, 5 figures, 4 tables
- **Topic Keywords**: pointwise
- **Reason**: The paper focuses on uncertainty quantification in Large Language Models (LLMs) for multi-step decision-making, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and query understanding. The paper's topic is more aligned with Natural Language Processing and Machine Learning, but the specific application and methodology are not relevant to the user's interests.

#### Abstract
> As Large Language Models (LLMs) are integrated into safety-critical
applications involving sequential decision-making in the real world, it is
essential to know when to trust LLM decisions. Existing LLM Uncertainty
Quantification (UQ) methods are primarily designed for single-turn
question-answering formats, resulting in multi-step decision-making scenarios,
e.g., LLM agentic system, being underexplored. In this paper, we introduce a
principled, information-theoretic framework that decomposes LLM sequential
decision uncertainty into two parts: (i) internal uncertainty intrinsic to the
current decision, which is focused on existing UQ methods, and (ii) extrinsic
uncertainty, a Mutual-Information (MI) quantity describing how much uncertainty
should be inherited from preceding decisions. We then propose UProp, an
efficient and effective extrinsic uncertainty estimator that converts the
direct estimation of MI to the estimation of Pointwise Mutual Information (PMI)
over multiple Trajectory-Dependent Decision Processes (TDPs). UProp is
evaluated over extensive multi-step decision-making benchmarks, e.g.,
AgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and
DeepSeek-V3. Experimental results demonstrate that UProp significantly
outperforms existing single-turn UQ baselines equipped with thoughtful
aggregation strategies. Moreover, we provide a comprehensive analysis of UProp,
including sampling efficiency, potential applications, and intermediate
uncertainty propagation, to demonstrate its effectiveness. Codes will be
available at https://github.com/jinhaoduan/UProp.

### 42. Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Danielle R. Thomas, Conrad Borchers, Jionghao Lin, Sanjit Kakarla, Shambhavi Bhushan, Erin Gatz, Shivang Gupta, Ralph Abboud, Kenneth R. Koedinger
- **URL**: <http://arxiv.org/abs/2506.17410v1>
- **Submitted**: 2025-06-20 18:13:33
- **Comment**: Short research paper accepted at EC-TEL 2025
- **Topic Keywords**: rag, search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on using large language models to assess tutor moves in real-life dialogues, which is outside the scope of your interests.

#### Abstract
> Tutoring improves student achievement, but identifying and studying what
tutoring actions are most associated with student learning at scale based on
audio transcriptions is an open research problem. This present study
investigates the feasibility and scalability of using generative AI to identify
and evaluate specific tutor moves in real-life math tutoring. We analyze 50
randomly selected transcripts of college-student remote tutors assisting middle
school students in mathematics. Using GPT-4, GPT-4o, GPT-4-turbo,
Gemini-1.5-pro, and LearnLM, we assess tutors' application of two tutor skills:
delivering effective praise and responding to student math errors. All models
reliably detected relevant situations, for example, tutors providing praise to
students (94-98% accuracy) and a student making a math error (82-88% accuracy)
and effectively evaluated the tutors' adherence to tutoring best practices,
aligning closely with human judgments (83-89% and 73-77%, respectively). We
propose a cost-effective prompting strategy and discuss practical implications
for using large language models to support scalable assessment in authentic
settings. This work further contributes LLM prompts to support reproducibility
and research in AI-supported learning.

### 43. ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, Mengdi Wang
- **URL**: <http://arxiv.org/abs/2506.18896v1>
- **Submitted**: 2025-06-23 17:59:02
- **Comment**: Codes and Models: https://github.com/Gen-Verse/ReasonFlux
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding. The focus on large language models, process reward models, and chain-of-thought reasoning is outside the scope of the user's research interests.

#### Abstract
> Process Reward Models (PRMs) have recently emerged as a powerful framework
for supervising intermediate reasoning steps in large language models (LLMs).
Previous PRMs are primarily trained on model final output responses and
struggle to evaluate intermediate thinking trajectories robustly, especially in
the emerging setting of trajectory-response outputs generated by frontier
reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a
novel trajectory-aware PRM explicitly designed to evaluate the
trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both
step-level and trajectory-level supervision, enabling fine-grained reward
assignment aligned with structured chain-of-thought data. We adapt
ReasonFlux-PRM to support reward supervision under both offline and online
settings, including (i) selecting high-quality model distillation data for
downstream supervised fine-tuning of smaller models, (ii) providing dense
process-level rewards for policy optimization during reinforcement learning,
and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results
on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond
demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs
(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our
derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving
average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement
learning, and 6.3% in test-time scaling. We also release our efficient
ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.
Projects: https://github.com/Gen-Verse/ReasonFlux

### 44. Neural Total Variation Distance Estimators for Changepoint Detection in News Data

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Csaba Zsolnai, Niels L√∂rch, Julian Arnold
- **URL**: <http://arxiv.org/abs/2506.18764v1>
- **Submitted**: 2025-06-23 15:33:30
- **Comment**: 16 pages, 3 figures
- **Topic Keywords**: rag
- **Reason**: This paper focuses on changepoint detection in news data using neural networks, which is not directly related to information retrieval, search technologies, or query understanding. While it involves classification and distance estimation, the context and application are quite different from the user's research interests.

#### Abstract
> Detecting when public discourse shifts in response to major events is crucial
for understanding societal dynamics. Real-world data is high-dimensional,
sparse, and noisy, making changepoint detection in this domain a challenging
endeavor. In this paper, we leverage neural networks for changepoint detection
in news data, introducing a method based on the so-called learning-by-confusion
scheme, which was originally developed for detecting phase transitions in
physical systems. We train classifiers to distinguish between articles from
different time periods. The resulting classification accuracy is used to
estimate the total variation distance between underlying content distributions,
where significant distances highlight changepoints. We demonstrate the
effectiveness of this method on both synthetic datasets and real-world data
from The Guardian newspaper, successfully identifying major historical events
including 9/11, the COVID-19 pandemic, and presidential elections. Our approach
requires minimal domain knowledge, can autonomously discover significant shifts
in public discourse, and yields a quantitative measure of change in content,
making it valuable for journalism, policy analysis, and crisis monitoring.

### 45. ReDit: Reward Dithering for Improved LLM Policy Optimization

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Chenxing Wei, Jiarui Yu, Ying Tiffany He, Hande Dong, Yao Shu, Fei Yu
- **URL**: <http://arxiv.org/abs/2506.18631v1>
- **Submitted**: 2025-06-23 13:36:24
- **Comment**: 10 pages, 15 figures
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus is on optimizing Large Language Model policies, which is a topic in Natural Language Processing, but not closely aligned with the user's primary research interests.

#### Abstract
> DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning
capabilities through its rule-based reward system. While it's a ''perfect''
reward system that effectively mitigates reward hacking, such reward functions
are often discrete. Our experimental observations suggest that discrete rewards
can lead to gradient anomaly, unstable optimization, and slow convergence. To
address this issue, we propose ReDit (Reward Dithering), a method that dithers
the discrete reward signal by adding simple random noise. With this perturbed
reward, exploratory gradients are continuously provided throughout the learning
process, enabling smoother gradient updates and accelerating convergence. The
injected noise also introduces stochasticity into flat reward regions,
encouraging the model to explore novel policies and escape local optima.
Experiments across diverse tasks demonstrate the effectiveness and efficiency
of ReDit. On average, ReDit achieves performance comparable to vanilla GRPO
with only approximately 10% the training steps, and furthermore, still exhibits
a 4% performance improvement over vanilla GRPO when trained for a similar
duration. Visualizations confirm significant mitigation of gradient issues with
ReDit. Moreover, theoretical analyses are provided to further validate these
advantages.

### 46. No Training Wheels: Steering Vectors for Bias Correction at Inference Time

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Aviral Gupta, Armaan Sethi, Ameesh Sethi
- **URL**: <http://arxiv.org/abs/2506.18598v1>
- **Submitted**: 2025-06-23 12:58:54
- **Topic Keywords**: rag
- **Reason**: The paper focuses on bias correction in neural network classifiers, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the topic of model editing, the approach is primarily focused on classification and does not address ranking models or user behavior modeling.

#### Abstract
> Neural network classifiers trained on datasets with uneven group
representation often inherit class biases and learn spurious correlations.
These models may perform well on average but consistently fail on atypical
groups. For example, in hair color classification, datasets may over-represent
females with blond hair, reinforcing stereotypes. Although various algorithmic
and data-centric methods have been proposed to address such biases, they often
require retraining or significant compute. In this work, we propose a cheap,
training-free method inspired by steering vectors used to edit behaviors in
large language models. We compute the difference in mean activations between
majority and minority groups to define a "bias vector," which we subtract from
the model's residual stream. This leads to reduced classification bias and
improved worst-group accuracy. We explore multiple strategies for extracting
and applying these vectors in transformer-like classifiers, showing that
steering vectors, traditionally used in generative models, can also be
effective in classification. More broadly, we showcase an extremely cheap,
inference time, training free method to mitigate bias in classification models.

### 47. Smooth Operators: LLMs Translating Imperfect Hints into Disfluency-Rich Transcripts

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Duygu Altinok
- **URL**: <http://arxiv.org/abs/2506.18510v1>
- **Submitted**: 2025-06-23 11:04:20
- **Comment**: Accepted to INTERSPEECH2025 workshop DISS2025
- **Topic Keywords**: rag
- **Reason**: This paper focuses on spoken language processing, disfluency detection, and automatic speech recognition, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on large language models and acoustic representations is also not aligned with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Accurate detection of disfluencies in spoken language is crucial for
enhancing the performance of automatic speech and language processing systems,
as well as fostering the development of more inclusive speech and language
technologies. Leveraging the growing trend of large language models (LLMs) as
versatile learners capable of processing both lexical and non-lexical inputs
(e.g., audio and video), we propose a novel approach to transcribing
disfluencies as explicit tokens with timestamps, enabling the generation of
fully annotated disfluency-rich transcripts. Our method integrates acoustic
representations extracted from an audio encoder with textual inputs of varying
quality: clean transcriptions without disfluencies, time-aligned transcriptions
from aligners, or outputs from phoneme-based ASR models -- all of which may
contain imperfections. Importantly, our experiments demonstrate that textual
inputs do not need to be flawless. As long as they include timestamp-related
cues, LLMs can effectively smooth the input and produce fully
disfluency-annotated transcripts, underscoring their robustness in handling
imperfect hints.

### 48. Lemmatization as a Classification Task: Results from Arabic across Multiple Genres

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Mostafa Saeed, Nizar Habash
- **URL**: <http://arxiv.org/abs/2506.18399v1>
- **Submitted**: 2025-06-23 08:34:33
- **Topic Keywords**: rag
- **Reason**: This paper focuses on lemmatization in Arabic, which is not directly related to the user's interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves machine learning and classification, the context and application are distinct from the user's areas of focus.

#### Abstract
> Lemmatization is crucial for NLP tasks in morphologically rich languages with
ambiguous orthography like Arabic, but existing tools face challenges due to
inconsistent standards and limited genre coverage. This paper introduces two
novel approaches that frame lemmatization as classification into a
Lemma-POS-Gloss (LPG) tagset, leveraging machine translation and semantic
clustering. We also present a new Arabic lemmatization test set covering
diverse genres, standardized alongside existing datasets. We evaluate character
level sequence-to-sequence models, which perform competitively and offer
complementary value, but are limited to lemma prediction (not LPG) and prone to
hallucinating implausible forms. Our results show that classification and
clustering yield more robust, interpretable outputs, setting new benchmarks for
Arabic lemmatization.

### 49. SE-Merging: A Self-Enhanced Approach for Dynamic Model Merging

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zijun Chen, Zhanpeng Zhou, Bo Zhang, Weinan Zhang, Xi Sun, Junchi Yan
- **URL**: <http://arxiv.org/abs/2506.18135v1>
- **Submitted**: 2025-06-22 18:38:41
- **Comment**: preprint, accepted at IJCNN2025
- **Topic Keywords**: rag
- **Reason**: The paper focuses on model merging and its underlying mechanisms, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on multi-task abilities, it does not address ranking models, user behavior modeling, or real-time relevance optimization, making it an off-topic paper for your research interests.

#### Abstract
> Model merging has gained increasing attention due to its intriguing property:
interpolating the parameters of different task-specific fine-tuned models leads
to multi-task abilities. However, despite its empirical success, the underlying
mechanisms of model merging remain poorly understood. In this work, we delve
into the mechanism behind model merging from a representation perspective. Our
analysis reveals that model merging achieves multi-task abilities through two
key capabilities: i) distinguishing samples from different tasks, and ii)
adapting to the corresponding expert model for each sample. These two
capabilities allow the merged model to retain task-specific expertise, enabling
efficient multi-task adaptation. Building on these insights, we propose
\texttt{SE-Merging}, a self-enhanced model merging framework that leverages
these two characteristics to dynamically identify the corresponding task for
each sample and then adaptively rescales the merging coefficients to further
enhance task-specific expertise in the merged model. Notably,
\texttt{SE-Merging} achieves dynamic model merging without additional training.
Extensive experiments demonstrate that \texttt{SE-Merging} achieves significant
performance improvements while remaining compatible with existing model merging
techniques.

### 50. A novel fast short-time root music method for vibration monitoring of high-speed spindles

- **LLM Score**: 0
- **Keyword Score**: 3
- **Authors**: Huiguang Zhang, Baoguo Liu, Wei Feng, Zongtang Li
- **URL**: <http://arxiv.org/abs/2506.17600v1>
- **Submitted**: 2025-06-21 05:35:38
- **Topic Keywords**: ltr
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, Natural Language Processing, data mining, or related topics. The paper focuses on vibration monitoring of high-speed spindles, which is outside the scope of your research areas.

#### Abstract
> Ultra-high-speed spindle bearings challenge traditional vibration monitoring
due to broadband noise, non-stationarity, and limited time-frequency
resolution. We present a fast Short-Time Root-MUSIC (fSTrM) algorithm that
exploits
  FFT-accelerated Lanczos bidiagonalization to reduce computational complexity
from $\mathcal{O}(N^3)$ to $SN\log_2N+S^2(N+S)+M^2(N+M)$
  while preserving parametric super-resolution. The method constructs Hankel
matrices from 16 ms signal frames and extracts fault frequencies through
polynomial rooting on the unit circle. Experimental validation on the
Politecnico di Torino bearing dataset demonstrates breakthrough micro-defect
detection capabilities. The algorithm reliably identifies 150 $\mu$m defects --
previously undetectable by conventional methods -- providing 72+ hours
additional warning time. Compared to STFT and wavelet methods, fSTrM achieves
1.2 Hz frequency resolution (vs. 12.5 Hz), 93\% detection rate at $-$5 dB SNR,
and quantifies defect severity through harmonic content analysis. Critically,
the algorithm processes each frame in 2.4 ms on embedded ARM Cortex-M7
hardware, enabling real-time deployment. This advancement transforms bearing
monitoring from failure prevention to continuous degradation assessment,
establishing a new paradigm for predictive maintenance in aerospace and
precision machining.

---

