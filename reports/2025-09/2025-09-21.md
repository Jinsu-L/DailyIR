# Daily Papers Report - 2025-09-21

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. CFDA & CLIP at TREC iKAT 2025: Enhancing Personalized Conversational Search via Query Reformulation and Rank Fusion

- **LLM Score**: 8
- **Keyword Score**: 15
- **Authors**: Yu-Cheng Chang, Guan-Wei Yeo, Quah Eugene, Fan-Jie Shih, Yuan-Ching Kuo, Tsung-En Yu, Hung-Chun Hsu, Ming-Feng Tsai, Chuan-Ju Wang
- **URL**: <http://arxiv.org/abs/2509.15588v1>
- **Submitted**: 2025-09-19 04:42:31
- **Topic Keywords**: query, ranking, rerank, retrieval, rank, search, trec
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of conversational search and query reformulation. The use of ranking models and fusion strategies aligns with your focus on Learning to Rank and real-time relevance optimization. However, the paper's emphasis on conversational search and passage ranking is somewhat outside your primary e-commerce domain focus.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Conversational Search
- **Aim**: Investigate effective query rewriting and retrieval fusion strategies for conversational search within the context of the 2025 TREC Interactive Knowledge Assistance Track (iKAT).
- **Rationale**: The iKAT 2025 task presents a unique challenge by incorporating both interactive and offline evaluation components, requiring robust and efficient systems capable of handling real-time user interactions and controlled dataset evaluation.
- **Ground**: Two datasets: TREC iKAT 2023-2024 (multi-turn conversational queries paired with large-scale passage collections) and QReCC (14K conversations with 81K question-answer pairs and a collection of 54M passages).
- **Experiment**: Four query rewriting methods (GPT-4o mini baseline, CHIQ-AD, LLM4CS, CHIQ-FT) and different retrieval fusion methods (Rank Fusion (RRF) and reranking) were evaluated.
- **Takeaway**: RRF fusion combined with neural reranking is crucial for robust conversational search. The order of these operations significantly impacts performance.  Sophisticated query rewriting techniques outperform simple prompting methods. 

#### Abstract
> The 2025 TREC Interactive Knowledge Assistance Track (iKAT) featured both
interactive and offline submission tasks. The former requires systems to
operate under real-time constraints, making robustness and efficiency as
important as accuracy, while the latter enables controlled evaluation of
passage ranking and response generation with pre-defined datasets. To address
this, we explored query rewriting and retrieval fusion as core strategies. We
built our pipelines around Best-of-$N$ selection and Reciprocal Rank Fusion
(RRF) strategies to handle different submission tasks. Results show that
reranking and fusion improve robustness while revealing trade-offs between
effectiveness and efficiency across both tasks.

---

### 2. Chunk Knowledge Generation Model for Enhanced Information Retrieval: A Multi-task Learning Approach

- **LLM Score**: 8
- **Keyword Score**: 12
- **Authors**: Jisu Kim, Jinhee Park, Changhyun Jeon, Jungwoo Choi, Keonwoo Kim, Minji Hong, Sehyun Kim
- **URL**: <http://arxiv.org/abs/2509.15658v1>
- **Submitted**: 2025-09-19 06:32:30
- **Topic Keywords**: information retrieval, query, queries, retrieval, search
- **Reason**: This paper proposes a novel approach to information retrieval, specifically addressing vocabulary mismatch problems through a chunk-based knowledge generation model. The use of multi-task learning and T5-based architecture aligns with your interests in query understanding and ranking models. The focus on improving retrieval efficiency and accuracy is also relevant to your research themes.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Information Retrieval Enhancement
- **Aim**: Develop a novel model, CKGM, to generate structured metadata (titles, questions, keywords) from document chunks for improved information retrieval.
- **Rationale**: Traditional query expansion and document expansion methods are computationally expensive and often generate irrelevant results. CKGM aims to address these limitations by leveraging a T5-based multi-task learning framework for efficient metadata generation.
- **Ground**: CKGM was trained on 519,176 document chunks from four diverse machine reading comprehension datasets and evaluated on 305 query-document pairs from the KoRAG dataset.
- **Experiment**: CKGM's performance was evaluated using BERTScore for semantic similarity and GPT-4 based prompts for retrieval appropriateness, question accuracy, title conciseness, and keyword consistency.  The model's effectiveness was compared against T5 fine-tuned models and larger language models like Qwen3-8B and Qwen3-14B.
- **Takeaway**: CKGM achieves high F1 scores in title, question, and keyword generation, outperforming smaller language models while being more computationally efficient.  Integrating generated metadata significantly improves retrieval accuracy. The paper also introduces a detailed evaluation framework for NLP models across various tasks.

#### Abstract
> Traditional query expansion techniques for addressing vocabulary mismatch
problems in information retrieval are context-sensitive and may lead to
performance degradation. As an alternative, document expansion research has
gained attention, but existing methods such as Doc2Query have limitations
including excessive preprocessing costs, increased index size, and reliability
issues with generated content. To mitigate these problems and seek more
structured and efficient alternatives, this study proposes a method that
divides documents into chunk units and generates textual data for each chunk to
simultaneously improve retrieval efficiency and accuracy. The proposed "Chunk
Knowledge Generation Model" adopts a T5-based multi-task learning structure
that simultaneously generates titles and candidate questions from each document
chunk while extracting keywords from user queries. This approach maximizes
computational efficiency by generating and extracting three types of semantic
information in parallel through a single encoding and two decoding processes.
The generated data is utilized as additional information in the retrieval
system. GPT-based evaluation on 305 query-document pairs showed that retrieval
using the proposed model achieved 95.41% accuracy at Top@10, demonstrating
superior performance compared to document chunk-level retrieval. This study
contributes by proposing an approach that simultaneously generates titles and
candidate questions from document chunks for application in retrieval
pipelines, and provides empirical evidence applicable to large-scale
information retrieval systems by demonstrating improved retrieval accuracy
through qualitative evaluation.

---

### 3. Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM Responses

- **LLM Score**: 8
- **Keyword Score**: 8
- **Authors**: Fangyi Yu, Nabeel Seedat, Dasha Herrmannova, Frank Schilder, Jonathan Richard Schwarz
- **URL**: <http://arxiv.org/abs/2509.16093v1>
- **Submitted**: 2025-09-19 15:36:02
- **Topic Keywords**: pointwise, relevance, rag
- **Reason**: This paper introduces a novel evaluation framework for LLM responses, focusing on semantic correctness and nuanced aspects of answer quality. While not directly related to query understanding, ranking models, or user behavior modeling, it aligns with your interests in Information Retrieval and NLP, particularly in areas requiring deep semantic understanding. The paper's emphasis on model-agnostic and domain-general evaluation also resonates with your broader research themes.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Evaluation of Large Language Models (LLMs) in Legal Question Answering (QA)
- **Aim**: To develop a novel framework, DeCE, that overcomes the limitations of existing evaluation methods for LLMs in the legal domain.
- **Rationale**: Traditional evaluation methods like BLEU, ROUGE, and pointwise LLM scoring fail to capture the nuances of legal reasoning and domain-specific accuracy. Existing frameworks lack domain specificity, adaptability to diverse legal tasks, and interpretability.
- **Ground**: DeCE is grounded in the principles of domain-specific criteria extraction, instance-level adaptive evaluation, decomposition of precision and recall, and interpretable performance analysis.
- **Experiment**: DeCE employs a six-phase chain-of-thought reasoning LLM judge with temperature-controlled outputs, demonstrating strong correlation with human expert judgments.
- **Takeaway**: DeCE offers a significant advancement in evaluating LLMs for legal QA, providing a more comprehensive, interpretable, and domain-specific approach. It emphasizes the need for sophisticated evaluation methods and human oversight in legal AI development and deployment.

#### Abstract
> Evaluating long-form answers in high-stakes domains such as law or medicine
remains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to
capture semantic correctness, and current LLM-based evaluators often reduce
nuanced aspects of answer quality into a single undifferentiated score. We
introduce DeCE, a decomposed LLM evaluation framework that separates precision
(factual accuracy and relevance) and recall (coverage of required concepts),
using instance-specific criteria automatically extracted from gold answer
requirements. DeCE is model-agnostic and domain-general, requiring no
predefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate
different LLMs on a real-world legal QA task involving multi-jurisdictional
reasoning and citation grounding. DeCE achieves substantially stronger
correlation with expert judgments ($r=0.78$), compared to traditional metrics
($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional
evaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist
models favor recall, while specialized models favor precision. Importantly,
only 11.95% of LLM-generated criteria required expert revision, underscoring
DeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation
framework in expert domains.

---

### 4. Efficient and Versatile Model for Multilingual Information Retrieval of Islamic Text: Development and Deployment in Real-World Scenarios

- **LLM Score**: 8
- **Keyword Score**: 8
- **Authors**: Vera Pavlova, Mohammed Makhlouf
- **URL**: <http://arxiv.org/abs/2509.15380v1>
- **Submitted**: 2025-09-18 19:32:07
- **Topic Keywords**: information retrieval, rag, retrieval, search
- **Reason**: This paper is highly relevant to the field of Information Retrieval, particularly in the context of multilingual retrieval. The focus on developing a versatile model for real-world scenarios aligns with the user's interest in query understanding and ranking models. However, the specific domain of Islamic text may not be directly applicable to the user's e-commerce background, thus preventing a perfect score.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Multilingual Information Retrieval (MLIR) for Islamic Texts
- **Aim**: To develop an efficient and effective MLIR system specifically designed for Islamic texts, bridging the gap between research and real-world deployment.
- **Rationale**: The unique multilingual nature of the Quranic corpus, with translations in over 100 languages, presents an opportunity to explore optimal training strategies for an ad-hoc IR system.
- **Ground**: The research builds upon existing literature in NLP and IR, focusing on language modeling, entity retrieval, biomedical NLP, multilingual IR, Arabic question answering, and general IR.
- **Experiment**: Eleven retrieval models were created using four different training approaches (monolingual, cross-lingual, translate-train-all, and mixed) and evaluated on the QRCD dataset in Arabic, English, Russian, and Urdu using MRR@10 as the primary metric.
- **Takeaway**: The mixed training approach yielded the highest performance across languages, demonstrating the effectiveness of combining monolingual and cross-lingual methods. The developed XLM-R4-ID model is significantly more efficient than previous approaches, achieving substantial latency reductions and cost savings.

#### Abstract
> Despite recent advancements in Multilingual Information Retrieval (MLIR), a
significant gap remains between research and practical deployment. Many studies
assess MLIR performance in isolated settings, limiting their applicability to
real-world scenarios. In this work, we leverage the unique characteristics of
the Quranic multilingual corpus to examine the optimal strategies to develop an
ad-hoc IR system for the Islamic domain that is designed to satisfy users'
information needs in multiple languages. We prepared eleven retrieval models
employing four training approaches: monolingual, cross-lingual,
translate-train-all, and a novel mixed method combining cross-lingual and
monolingual techniques. Evaluation on an in-domain dataset demonstrates that
the mixed approach achieves promising results across diverse retrieval
scenarios. Furthermore, we provide a detailed analysis of how different
training configurations affect the embedding space and their implications for
multilingual retrieval effectiveness. Finally, we discuss deployment
considerations, emphasizing the cost-efficiency of deploying a single
versatile, lightweight model for real-world MLIR applications.

---

### 5. Optimizing Product Deduplication in E-Commerce with Multimodal Embeddings

- **LLM Score**: 7
- **Keyword Score**: 5
- **Authors**: Aysenur Kulunk, Berk Taskin, M. Furkan Eseoglu, H. Bahadir Sahin
- **URL**: <http://arxiv.org/abs/2509.15858v1>
- **Submitted**: 2025-09-19 10:49:39
- **Topic Keywords**: rag, commerce, e-commerce, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the e-commerce domain. The focus on product deduplication and multimodal embeddings is relevant to your work on query understanding and ranking models. However, the paper's emphasis on e-commerce and product catalogs is not a central match to your broader interests in IR and NLP.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Product Deduplication in E-commerce
- **Aim**: Develop a scalable and accurate product deduplication system for large Turkish e-commerce platforms.
- **Rationale**: Traditional keyword-based methods are insufficient for accurately identifying duplicate products, especially in large catalogs.
- **Ground**: The system leverages Milvus, an open-source vector database, for efficient storage and retrieval of text and image embeddings.
- **Experiment**: The system is evaluated against a third-party product matching solution using a labeled dataset, achieving a macro average F1 score of 0.90.
- **Takeaway**: A multimodal approach combining text and image representations, along with efficient feature extraction and classification, enables accurate and scalable product deduplication in e-commerce.

#### Abstract
> In large scale e-commerce marketplaces, duplicate product listings frequently
cause consumer confusion and operational inefficiencies, degrading trust on the
platform and increasing costs. Traditional keyword-based search methodologies
falter in accurately identifying duplicates due to their reliance on exact
textual matches, neglecting semantic similarities inherent in product titles.
To address these challenges, we introduce a scalable, multimodal product
deduplication designed specifically for the e-commerce domain. Our approach
employs a domain-specific text model grounded in BERT architecture in
conjunction with MaskedAutoEncoders for image representations. Both of these
architectures are augmented with dimensionality reduction techniques to produce
compact 128-dimensional embeddings without significant information loss.
Complementing this, we also developed a novel decider model that leverages both
text and image vectors. By integrating these feature extraction mechanisms with
Milvus, an optimized vector database, our system can facilitate efficient and
high-precision similarity searches across extensive product catalogs exceeding
200 million items with just 100GB of system RAM consumption. Empirical
evaluations demonstrate that our matching system achieves a macro-average F1
score of 0.90, outperforming third-party solutions which attain an F1 score of
0.83. Our findings show the potential of combining domain-specific adaptations
with state-of-the-art machine learning techniques to mitigate duplicate
listings in large-scale e-commerce environments.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. CodeRAG: Finding Relevant and Necessary Knowledge for Retrieval-Augmented Repository-Level Code Completion

- **LLM Score**: 6
- **Keyword Score**: 17
- **Authors**: Sheng Zhang, Yifan Ding, Shuquan Lian, Shun Song, Hui Li
- **URL**: <http://arxiv.org/abs/2509.16112v1>
- **Submitted**: 2025-09-19 15:57:40
- **Comment**: EMNLP 2025
- **Topic Keywords**: retriever, query, ranking, rerank, rag, retrieval, rank
- **Reason**: The paper explores repository-level code completion using retrieval-augmented models, which is somewhat related to information retrieval and query understanding. However, the focus on code completion and large language models is not a central match to the user's primary research interests in IR and NLP. The paper's relevance is somewhat mitigated by the lack of direct connection to search technologies, ranking models, or user behavior modeling.

#### Abstract
> Repository-level code completion automatically predicts the unfinished code
based on the broader information from the repository. Recent strides in Code
Large Language Models (code LLMs) have spurred the development of
repository-level code completion methods, yielding promising results.
Nevertheless, they suffer from issues such as inappropriate query construction,
single-path code retrieval, and misalignment between code retriever and code
LLM. To address these problems, we introduce CodeRAG, a framework tailored to
identify relevant and necessary knowledge for retrieval-augmented
repository-level code completion. Its core components include log probability
guided query construction, multi-path code retrieval, and preference-aligned
BestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval
demonstrate that CodeRAG significantly and consistently outperforms
state-of-the-art methods. The implementation of CodeRAG is available at
https://github.com/KDEGroup/CodeRAG.

### 7. Relevance to Utility: Process-Supervised Rewrite for RAG

- **LLM Score**: 6
- **Keyword Score**: 7
- **Authors**: Jaeyoung Kim, Jongho Kim, Seung-won Hwang, Seoho Song, Young-In Song
- **URL**: <http://arxiv.org/abs/2509.15577v1>
- **Submitted**: 2025-09-19 04:24:57
- **Topic Keywords**: relevance, rag, retrieval
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, specifically in the context of Retrieval-Augmented Generation systems. However, the focus on generative utility and process supervision is more aligned with Natural Language Processing and Generation, rather than your primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Retrieval-Augmented Generation systems often suffer from a gap between
optimizing retrieval relevance and generative utility: retrieved documents may
be topically relevant but still lack the content needed for effective reasoning
during generation. While existing "bridge" modules attempt to rewrite the
retrieved text for better generation, we show how they fail to capture true
document utility. In this work, we propose R2U, with a key distinction of
directly optimizing to maximize the probability of generating a correct answer
through process supervision. As such direct observation is expensive, we also
propose approximating an efficient distillation pipeline by scaling the
supervision from LLMs, which helps the smaller rewriter model generalize
better. We evaluate our method across multiple open-domain question-answering
benchmarks. The empirical results demonstrate consistent improvements over
strong bridging baselines.

### 8. It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge

- **LLM Score**: 6
- **Keyword Score**: 2
- **Authors**: Lukas Ellinger, Georg Groh
- **URL**: <http://arxiv.org/abs/2509.16107v1>
- **Submitted**: 2025-09-19 15:49:26
- **Comment**: Accepted by UncertaiNLP workshop @ EMNLP 2025
- **Topic Keywords**: rag
- **Reason**: This paper explores the use of Large Language Models (LLMs) in resolving referential ambiguity, which is related to query understanding and ranking models in Information Retrieval. However, the focus on multi-turn conversations and language simplification prompts is somewhat tangential to the user's core research themes, despite the use of LLMs, which are relevant to the user's interests in NLP.

#### Abstract
> Ambiguous words or underspecified references require interlocutors to resolve
them, often by relying on shared context and commonsense knowledge. Therefore,
we systematically investigate whether Large Language Models (LLMs) can leverage
commonsense to resolve referential ambiguity in multi-turn conversations and
analyze their behavior when ambiguity persists. Further, we study how requests
for simplified language affect this capacity. Using a novel multilingual
evaluation dataset, we test DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, and
Llama-3.1-8B via LLM-as-Judge and human annotations. Our findings indicate that
current LLMs struggle to resolve ambiguity effectively: they tend to commit to
a single interpretation or cover all possible references, rather than hedging
or seeking clarification. This limitation becomes more pronounced under
simplification prompts, which drastically reduce the use of commonsense
reasoning and diverse response strategies. Fine-tuning Llama-3.1-8B with Direct
Preference Optimization substantially improves ambiguity resolution across all
request types. These results underscore the need for advanced fine-tuning to
improve LLMs' handling of ambiguity and to ensure robust performance across
diverse communication styles.

### 9. LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Junlong Jia, Xing Wu, Chaochen Gao, Ziyang Chen, Zijia Lin, Zhongzhi Li, Weinong Wang, Haotian Xu, Donghui Jin, Debing Zhang, Binghui Guo
- **URL**: <http://arxiv.org/abs/2509.15568v1>
- **Submitted**: 2025-09-19 04:07:46
- **Comment**: work in progress
- **Topic Keywords**: relevance, rag, retrieval, search
- **Reason**: The paper focuses on large language models (LLMs) and long-context data synthesis, which is somewhat related to information retrieval and ranking models. However, the primary focus on NLP and LLMs makes it less central to the user's core research themes, which are more focused on query understanding, ranking models, and user behavior modeling.

#### Abstract
> High-quality long-context data is essential for training large language
models (LLMs) capable of processing extensive documents, yet existing synthesis
approaches using relevance-based aggregation face challenges of computational
efficiency. We present LiteLong, a resource-efficient method for synthesizing
long-context data through structured topic organization and multi-agent debate.
Our approach leverages the BISAC book classification system to provide a
comprehensive hierarchical topic organization, and then employs a debate
mechanism with multiple LLMs to generate diverse, high-quality topics within
this structure. For each topic, we use lightweight BM25 retrieval to obtain
relevant documents and concatenate them into 128K-token training samples.
Experiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves
competitive long-context performance and can seamlessly integrate with other
long-dependency enhancement methods. LiteLong makes high-quality long-context
data synthesis more accessible by reducing both computational and data
engineering costs, facilitating further research in long-context language
training.

### 10. EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Kanato Masayoshi, Masahiro Hashimoto, Ryoichi Yokoyama, Naoki Toda, Yoshifumi Uwamino, Shogo Fukuda, Ho Namkoong, Masahiro Jinzaki
- **URL**: <http://arxiv.org/abs/2509.15957v1>
- **Submitted**: 2025-09-19 13:17:16
- **Topic Keywords**: information retrieval, ctr, retrieval
- **Reason**: This paper explores the application of Large Language Models (LLMs) in clinical information retrieval, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the focus on clinical data and healthcare settings is not a central match to your background in e-commerce and interests in deep semantic understanding and real-time relevance optimization.

#### Abstract
> Background: Large language models (LLMs) show promise in medicine, but their
deployment in hospitals is limited by restricted access to electronic health
record (EHR) systems. The Model Context Protocol (MCP) enables integration
between LLMs and external tools.
  Objective: To evaluate whether an LLM connected to an EHR database via MCP
can autonomously retrieve clinically relevant information in a real hospital
setting.
  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated
with the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct
agent to interact with it. Six tasks were tested, derived from use cases of the
infection control team (ICT). Eight patients discussed at ICT conferences were
retrospectively analyzed. Agreement with physician-generated gold standards was
measured.
  Results: The LLM consistently selected and executed the correct MCP tools.
Except for two tasks, all tasks achieved near-perfect accuracy. Performance was
lower in the complex task requiring time-dependent calculations. Most errors
arose from incorrect arguments or misinterpretation of tool results. Responses
from EHR-MCP were reliable, though long and repetitive data risked exceeding
the context window.
  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a
real hospital setting, achieving near-perfect performance in simple tasks while
highlighting challenges in complex ones. EHR-MCP provides an infrastructure for
secure, consistent data access and may serve as a foundation for hospital AI
agents. Future work should extend beyond retrieval to reasoning, generation,
and clinical impact assessment, paving the way for effective integration of
generative AI into clinical practice.

### 11. KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Vaibhav Singh, Soumya Suvra Ghosal, Kapu Nirmal Joshua, Soumyabrata Pal, Sayak Ray Chowdhury
- **URL**: <http://arxiv.org/abs/2509.15676v1>
- **Submitted**: 2025-09-19 06:50:03
- **Topic Keywords**: query, rag, retrieval
- **Reason**: This paper focuses on in-context learning and example selection for large language models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, it does not directly address user behavior modeling or click models, and its primary focus is on NLP rather than IR. While it explores a relevant topic, it does not align closely with the user's core research themes.

#### Abstract
> In-context learning (ICL) has emerged as a powerful paradigm for adapting
large language models (LLMs) to new and data-scarce tasks using only a few
carefully selected task-specific examples presented in the prompt. However,
given the limited context size of LLMs, a fundamental question arises: Which
examples should be selected to maximize performance on a given user query?
While nearest-neighbor-based methods like KATE have been widely adopted for
this purpose, they suffer from well-known drawbacks in high-dimensional
embedding spaces, including poor generalization and a lack of diversity. In
this work, we study this problem of example selection in ICL from a principled,
information theory-driven perspective. We first model an LLM as a linear
function over input embeddings and frame the example selection task as a
query-specific optimization problem: selecting a subset of exemplars from a
larger example bank that minimizes the prediction error on a specific query.
This formulation departs from traditional generalization-focused learning
theoretic approaches by targeting accurate prediction for a specific query
instance. We derive a principled surrogate objective that is approximately
submodular, enabling the use of a greedy algorithm with an approximation
guarantee. We further enhance our method by (i) incorporating the kernel trick
to operate in high-dimensional feature spaces without explicit mappings, and
(ii) introducing an optimal design-based regularizer to encourage diversity in
the selected examples. Empirically, we demonstrate significant improvements
over standard retrieval methods across a suite of classification tasks,
highlighting the benefits of structure-aware, diverse example selection for ICL
in real-world, label-scarce scenarios.

### 12. LLM Cache Bandit Revisited: Addressing Query Heterogeneity for Cost-Effective LLM Inference

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Hantao Yang, Hong Xie, Defu Lian, Enhong Chen
- **URL**: <http://arxiv.org/abs/2509.15515v1>
- **Submitted**: 2025-09-19 01:39:08
- **Topic Keywords**: query, queries
- **Reason**: This paper focuses on optimizing LLM inference, but its primary contribution is in cache management and query heterogeneity, which is not directly related to query understanding, ranking models, or user behavior modeling. While it touches on computational efficiency, it does not delve into deep semantic understanding or real-time relevance optimization, which are core aspects of your research interests.

#### Abstract
> This paper revisits the LLM cache bandit problem, with a special focus on
addressing the query heterogeneity for cost-effective LLM inference. Previous
works often assume uniform query sizes. Heterogeneous query sizes introduce a
combinatorial structure for cache selection, making the cache replacement
process more computationally and statistically challenging. We treat optimal
cache selection as a knapsack problem and employ an accumulation-based strategy
to effectively balance computational overhead and cache updates. In theoretical
analysis, we prove that the regret of our algorithm achieves an $O(\sqrt{MNT})$
bound, improving the coefficient of $\sqrt{MN}$ compared to the $O(MN\sqrt{T})$
result in Berkeley, where $N$ is the total number of queries and $M$ is the
cache size. Additionally, we also provide a problem-dependent bound, which was
absent in previous works. The experiment rely on real-world data show that our
algorithm reduces the total cost by approximately 12\%.

### 13. RAVE: Retrieval and Scoring Aware Verifiable Claim Detection

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Yufeng Li, Arkaitz Zubiaga
- **URL**: <http://arxiv.org/abs/2509.15793v1>
- **Submitted**: 2025-09-19 09:23:41
- **Comment**: 5 pages, 1 figure
- **Topic Keywords**: relevance, retrieval
- **Reason**: The paper focuses on claim detection and fact-checking, which is somewhat related to information retrieval, but it's not directly aligned with your core research themes. The use of retrieval and scoring aware framework is relevant, but the application domain is limited to social media and fact-checking, which doesn't match your broader interests in e-commerce and deep semantic understanding.

#### Abstract
> The rapid spread of misinformation on social media underscores the need for
scalable fact-checking tools. A key step is claim detection, which identifies
statements that can be objectively verified. Prior approaches often rely on
linguistic cues or claim check-worthiness, but these struggle with vague
political discourse and diverse formats such as tweets. We present RAVE
(Retrieval and Scoring Aware Verifiable Claim Detection), a framework that
combines evidence retrieval with structured signals of relevance and source
credibility. Experiments on CT22-test and PoliClaim-test show that RAVE
consistently outperforms text-only and retrieval-based baselines in both
accuracy and F1.

### 14. Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Andrew Kyle Lampinen, Martin Engelcke, Yuxuan Li, Arslan Chaudhry, James L. McClelland
- **URL**: <http://arxiv.org/abs/2509.16189v1>
- **Submitted**: 2025-09-19 17:49:25
- **Topic Keywords**: retrieval, acl
- **Reason**: The paper discusses machine learning generalization and proposes a solution using episodic memory and retrieval methods. While it touches on retrieval, it's not specifically focused on information retrieval, ranking models, or user behavior modeling, which are core areas of interest. The connection to cognitive science and natural intelligence is intriguing, but not directly relevant to the user's primary research themes.

#### Abstract
> When do machine learning systems fail to generalize, and what mechanisms
could improve their generalization? Here, we draw inspiration from cognitive
science to argue that one weakness of machine learning systems is their failure
to exhibit latent learning -- learning information that is not relevant to the
task at hand, but that might be useful in a future task. We show how this
perspective links failures ranging from the reversal curse in language modeling
to new findings on agent-based navigation. We then highlight how cognitive
science points to episodic memory as a potential part of the solution to these
issues. Correspondingly, we show that a system with an oracle retrieval
mechanism can use learning experiences more flexibly to generalize better
across many of these challenges. We also identify some of the essential
components for effectively using retrieval, including the importance of
within-example in-context learning for acquiring the ability to use information
across retrieved examples. In summary, our results illustrate one possible
contributor to the relative data inefficiency of current machine learning
systems compared to natural intelligence, and help to understand how retrieval
methods can complement parametric learning to improve generalization.

### 15. Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Reza Sanayei, Srdjan Vesic, Eduardo Blanco, Mihai Surdeanu
- **URL**: <http://arxiv.org/abs/2509.15739v1>
- **Submitted**: 2025-09-19 08:10:32
- **Comment**: Accepted to EMNLP 2025 Findings
- **Topic Keywords**: ranking, rank
- **Reason**: This paper explores the application of Large Language Models (LLMs) in evaluating debates, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on argumentation theory semantics and formal graph-aware reasoning is not directly aligned with the user's primary research interests in IR and NLP. The paper's findings on LLMs' limitations in modeling formal argumentation semantics may be of interest, but it does not directly contribute to the user's core research themes.

#### Abstract
> Large Language Models (LLMs) excel at linear reasoning tasks but remain
underexplored on non-linear structures such as those found in natural debates,
which are best expressed as argument graphs. We evaluate whether LLMs can
approximate structured reasoning from Computational Argumentation Theory (CAT).
Specifically, we use Quantitative Argumentation Debate (QuAD) semantics, which
assigns acceptability scores to arguments based on their attack and support
relations. Given only dialogue-formatted debates from two NoDE datasets, models
are prompted to rank arguments without access to the underlying graph. We test
several LLMs under advanced instruction strategies, including Chain-of-Thought
and In-Context Learning. While models show moderate alignment with QuAD
rankings, performance degrades with longer inputs or disrupted discourse flow.
Advanced prompting helps mitigate these effects by reducing biases related to
argument length and position. Our findings highlight both the promise and
limitations of LLMs in modeling formal argumentation semantics and motivate
future work on graph-aware reasoning.

### 16. SciEvent: Benchmarking Multi-domain Scientific Event Extraction

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Bofu Dong, Pritesh Shah, Sumedh Sonawane, Tiyasha Banerjee, Erin Brady, Xinya Du, Ming Jiang
- **URL**: <http://arxiv.org/abs/2509.15620v1>
- **Submitted**: 2025-09-19 05:32:50
- **Comment**: 9 pages, 8 figures (main); 22 pages, 11 figures (appendix). Accepted
  to EMNLP 2025 (Main Conference)
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on scientific event extraction, which is related to information retrieval and natural language processing. However, it is more specific to scientific abstracts and event extraction, which doesn't directly align with the user's primary focus on query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat tangential to the user's core research themes.

#### Abstract
> Scientific information extraction (SciIE) has primarily relied on
entity-relation extraction in narrow domains, limiting its applicability to
interdisciplinary research and struggling to capture the necessary context of
scientific information, often resulting in fragmented or conflicting
statements. In this paper, we introduce SciEvent, a novel multi-domain
benchmark of scientific abstracts annotated via a unified event extraction (EE)
schema designed to enable structured and context-aware understanding of
scientific content. It includes 500 abstracts across five research domains,
with manual annotations of event segments, triggers, and fine-grained
arguments. We define SciIE as a multi-stage EE pipeline: (1) segmenting
abstracts into core scientific activities--Background, Method, Result, and
Conclusion; and (2) extracting the corresponding triggers and arguments.
Experiments with fine-tuned EE models, large language models (LLMs), and human
annotators reveal a performance gap, with current models struggling in domains
such as sociology and humanities. SciEvent serves as a challenging benchmark
and a step toward generalizable, multi-domain SciIE.

### 17. EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-grained Emotions

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Xinchen Wan, Jinhua Liang, Huan Zhang
- **URL**: <http://arxiv.org/abs/2509.15986v1>
- **Submitted**: 2025-09-19 13:52:22
- **Comment**: 5 pages, 5 figures. Submitted to the 2026 IEEE International
  Conference on Acoustics, Speech and Signal Processing (ICASSP 2026)
- **Topic Keywords**: retrieval
- **Reason**: The paper is somewhat related to information retrieval, but its focus on music retrieval and fine-grained emotion detection is not directly aligned with your core research themes. However, the use of deep learning models and knowledge graphs is relevant to your interests in NLP and data mining.

#### Abstract
> Existing digital mental wellness tools often overlook the nuanced emotional
states underlying everyday challenges. For example, pre-sleep anxiety affects
more than 1.5 billion people worldwide, yet current approaches remain largely
static and "one-size-fits-all", failing to adapt to individual needs. In this
work, we present EmoHeal, an end-to-end system that delivers personalized,
three-stage supportive narratives. EmoHeal detects 27 fine-grained emotions
from user text with a fine-tuned XLM-RoBERTa model, mapping them to musical
parameters via a knowledge graph grounded in music therapy principles (GEMS,
iso-principle). EmoHeal retrieves audiovisual content using the CLAMP3 model to
guide users from their current state toward a calmer one
("match-guide-target"). A within-subjects study (N=40) demonstrated significant
supportive effects, with participants reporting substantial mood improvement
(M=4.12, p<0.001) and high perceived emotion recognition accuracy (M=4.05,
p<0.001). A strong correlation between perceived accuracy and therapeutic
outcome (r=0.72, p<0.001) validates our fine-grained approach. These findings
establish the viability of theory-driven, emotion-aware digital wellness tools
and provides a scalable AI blueprint for operationalizing music therapy
principles.

### 18. PILOT: Steering Synthetic Data Generation with Psychological & Linguistic Output Targeting

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Caitlin Cisar, Emily Sheffield, Joshua Drake, Alden Harrell, Subramanian Chidambaram, Nikita Nangia, Vinayak Arannil, Alex Williams
- **URL**: <http://arxiv.org/abs/2509.15447v1>
- **Submitted**: 2025-09-18 21:43:28
- **Topic Keywords**: rag
- **Reason**: This paper introduces a framework for steering synthetic data generation with structured psycholinguistic profiles, which is somewhat related to query understanding and user behavior modeling in Information Retrieval. However, the focus on synthetic data generation and linguistic output targeting is not directly aligned with the user's core research themes.

#### Abstract
> Generative AI applications commonly leverage user personas as a steering
mechanism for synthetic data generation, but reliance on natural language
representations forces models to make unintended inferences about which
attributes to emphasize, limiting precise control over outputs. We introduce
PILOT (Psychological and Linguistic Output Targeting), a two-phase framework
for steering large language models with structured psycholinguistic profiles.
In Phase 1, PILOT translates natural language persona descriptions into
multidimensional profiles with normalized scores across linguistic and
psychological dimensions. In Phase 2, these profiles guide generation along
measurable axes of variation. We evaluate PILOT across three state-of-the-art
LLMs (Mistral Large 2, Deepseek-R1, LLaMA 3.3 70B) using 25 synthetic personas
under three conditions: Natural-language Persona Steering (NPS), Schema-Based
Steering (SBS), and Hybrid Persona-Schema Steering (HPS). Results demonstrate
that schema-based approaches significantly reduce artificial-sounding persona
repetition while improving output coherence, with silhouette scores increasing
from 0.098 to 0.237 and topic purity from 0.773 to 0.957. Our analysis reveals
a fundamental trade-off: SBS produces more concise outputs with higher topical
consistency, while NPS offers greater lexical diversity but reduced
predictability. HPS achieves a balance between these extremes, maintaining
output variety while preserving structural consistency. Expert linguistic
evaluation confirms that PILOT maintains high response quality across all
conditions, with no statistically significant differences between steering
approaches.

### 19. Real, Fake, or Manipulated? Detecting Machine-Influenced Text

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Yitong Wang, Zhongping Zhang, Margherita Piana, Zheng Zhou, Peter Gerstoft, Bryan A. Plummer
- **URL**: <http://arxiv.org/abs/2509.15350v1>
- **Submitted**: 2025-09-18 18:41:57
- **Comment**: Accepted to EMNLP 2025 Findings
- **Topic Keywords**: rag
- **Reason**: This paper focuses on detecting machine-influenced text, which is related to query understanding and ranking models in Information Retrieval. However, its primary focus on machine-generated text detection and fine-grained categorization of text types does not directly align with the user's core research themes in IR and Search technologies.

#### Abstract
> Large Language Model (LLMs) can be used to write or modify documents,
presenting a challenge for understanding the intent behind their use. For
example, benign uses may involve using LLM on a human-written document to
improve its grammar or to translate it into another language. However, a
document entirely produced by a LLM may be more likely to be used to spread
misinformation than simple translation (\eg, from use by malicious actors or
simply by hallucinating). Prior works in Machine Generated Text (MGT) detection
mostly focus on simply identifying whether a document was human or machine
written, ignoring these fine-grained uses. In this paper, we introduce a
HiErarchical, length-RObust machine-influenced text detector (HERO), which
learns to separate text samples of varying lengths from four primary types:
human-written, machine-generated, machine-polished, and machine-translated.
HERO accomplishes this by combining predictions from length-specialist models
that have been trained with Subcategory Guidance. Specifically, for categories
that are easily confused (\eg, different source languages), our Subcategory
Guidance module encourages separation of the fine-grained categories, boosting
performance. Extensive experiments across five LLMs and six domains demonstrate
the benefits of our HERO, outperforming the state-of-the-art by 2.5-3 mAP on
average.

### 20. Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Frederic Kirstein, Sonu Kumar, Terry Ruas, Bela Gipp
- **URL**: <http://arxiv.org/abs/2509.15901v1>
- **Submitted**: 2025-09-19 11:58:17
- **Comment**: Accepted at EMNLP 2025
- **Topic Keywords**: personalization
- **Reason**: The paper explores meeting summarization with large language models, which is somewhat related to information retrieval and NLP. However, the focus on summarization and personalization is not a central match to the user's core research themes, particularly query understanding, ranking models, and user behavior modeling.

#### Abstract
> Meeting summarization with large language models (LLMs) remains error-prone,
often producing outputs with hallucinations, omissions, and irrelevancies. We
present FRAME, a modular pipeline that reframes summarization as a semantic
enrichment task. FRAME extracts and scores salient facts, organizes them
thematically, and uses these to enrich an outline into an abstractive summary.
To personalize summaries, we introduce SCOPE, a reason-out-loud protocol that
has the model build a reasoning trace by answering nine questions before
content selection. For evaluation, we propose P-MESA, a multi-dimensional,
reference-free evaluation framework to assess if a summary fits a target
reader. P-MESA reliably identifies error instances, achieving >= 89% balanced
accuracy against human annotations and strongly aligns with human severity
ratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and
omission by 2 out of 5 points (measured with MESA), while SCOPE improves
knowledge fit and goal alignment over prompt-only baselines. Our findings
advocate for rethinking summarization to improve control, faithfulness, and
personalization.

### 21. Building Data-Driven Occupation Taxonomies: A Bottom-Up Multi-Stage Approach via Semantic Clustering and Multi-Agent Collaboration

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Nan Li, Bo Kang, Tijl De Bie
- **URL**: <http://arxiv.org/abs/2509.15786v1>
- **Submitted**: 2025-09-19 09:17:48
- **Topic Keywords**: recommend
- **Reason**: This paper is somewhat related to the user's interests in Information Retrieval and Natural Language Processing, as it involves semantic clustering and multi-agent collaboration. However, the focus on occupation taxonomies and job recommendation is not directly aligned with the user's primary research themes. The paper's use of data-driven approaches and real-world datasets is relevant, but the application domain is not e-commerce-specific.

#### Abstract
> Creating robust occupation taxonomies, vital for applications ranging from
job recommendation to labor market intelligence, is challenging. Manual
curation is slow, while existing automated methods are either not adaptive to
dynamic regional markets (top-down) or struggle to build coherent hierarchies
from noisy data (bottom-up). We introduce CLIMB (CLusterIng-based Multi-agent
taxonomy Builder), a framework that fully automates the creation of
high-quality, data-driven taxonomies from raw job postings. CLIMB uses global
semantic clustering to distill core occupations, then employs a
reflection-based multi-agent system to iteratively build a coherent hierarchy.
On three diverse, real-world datasets, we show that CLIMB produces taxonomies
that are more coherent and scalable than existing methods and successfully
capture unique regional characteristics. We release our code and datasets at
https://anonymous.4open.science/r/CLIMB.

### 22. REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Nannan Huang, Haytham M. Fayek, Xiuzhen Zhang
- **URL**: <http://arxiv.org/abs/2509.15723v1>
- **Submitted**: 2025-09-19 07:53:51
- **Comment**: Accepted to the 5th New Frontiers in Summarization Workshop
  (NewSumm@EMNLP 2025)
- **Topic Keywords**: search
- **Reason**: The paper explores fairness in opinion summarization using large language models, which is somewhat related to information retrieval and NLP. However, the focus on fairness and opinion summarization is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is limited to the broader NLP domain.

#### Abstract
> Individuals express diverse opinions, a fair summary should represent these
viewpoints comprehensively. Previous research on fairness in opinion
summarisation using large language models (LLMs) relied on hyperparameter
tuning or providing ground truth distributional information in prompts.
However, these methods face practical limitations: end-users rarely modify
default model parameters, and accurate distributional information is often
unavailable. Building upon cognitive science research demonstrating that
frequency-based representations reduce systematic biases in human statistical
reasoning by making reference classes explicit and reducing cognitive load,
this study investigates whether frequency framed prompting (REFER) can
similarly enhance fairness in LLM opinion summarisation. Through systematic
experimentation with different prompting frameworks, we adapted techniques
known to improve human reasoning to elicit more effective information
processing in language models compared to abstract probabilistic
representations.Our results demonstrate that REFER enhances fairness in
language models when summarising opinions. This effect is particularly
pronounced in larger language models and using stronger reasoning instructions.

### 23. Understanding Embedding Scaling in Collaborative Filtering

- **LLM Score**: 3
- **Keyword Score**: 1
- **Authors**: Zhuangzhuang He, Zhou Kaiyu, Haoyue Bai, Fengbin Zhu, Yonghui Yang
- **URL**: <http://arxiv.org/abs/2509.15709v1>
- **Submitted**: 2025-09-19 07:33:50
- **Topic Keywords**: recommend
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Search technologies, but it focuses on Collaborative Filtering, which is a type of Recommender System. While it explores scaling and performance degradation, it lacks direct connection to your core themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> Scaling recommendation models into large recommendation models has become one
of the most widely discussed topics. Recent efforts focus on components beyond
the scaling embedding dimension, as it is believed that scaling embedding may
lead to performance degradation. Although there have been some initial
observations on embedding, the root cause of their non-scalability remains
unclear. Moreover, whether performance degradation occurs across different
types of models and datasets is still an unexplored area. Regarding the effect
of embedding dimensions on performance, we conduct large-scale experiments
across 10 datasets with varying sparsity levels and scales, using 4
representative classical architectures. We surprisingly observe two novel
phenomenon: double-peak and logarithmic. For the former, as the embedding
dimension increases, performance first improves, then declines, rises again,
and eventually drops. For the latter, it exhibits a perfect logarithmic curve.
Our contributions are threefold. First, we discover two novel phenomena when
scaling collaborative filtering models. Second, we gain an understanding of the
underlying causes of the double-peak phenomenon. Lastly, we theoretically
analyze the noise robustness of collaborative filtering models, with results
matching empirical observations.

### 24. Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay Assessment

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Ahmed Karim, Qiao Wang, Zheng Yuan
- **URL**: <http://arxiv.org/abs/2509.15926v1>
- **Submitted**: 2025-09-19 12:28:50
- **Comment**: Accepted at EMNLP 2025 (Main Conference). Camera-ready version
- **Topic Keywords**: rag, acl
- **Reason**: This paper is not directly related to the user's core research themes in Information Retrieval and Search technologies. Although it involves Natural Language Processing (NLP) and large language models, the focus on Automated Essay Assessment and uncertainty-calibrated models is not a central match for the user's interests.

#### Abstract
> Automated Essay Scoring (AES) systems now reach near human agreement on some
public benchmarks, yet real-world adoption, especially in high-stakes
examinations, remains limited. A principal obstacle is that most models output
a single score without any accompanying measure of confidence or explanation.
We address this gap with conformal prediction, a distribution-free wrapper that
equips any classifier with set-valued outputs and formal coverage guarantees.
Two open-source large language models (Llama-3 8B and Qwen-2.5 3B) are
fine-tuned on three diverse corpora (ASAP, TOEFL11, Cambridge-FCE) and
calibrated at a 90 percent risk level. Reliability is assessed with UAcc, an
uncertainty-aware accuracy that rewards models for being both correct and
concise. To our knowledge, this is the first work to combine conformal
prediction and UAcc for essay scoring. The calibrated models consistently meet
the coverage target while keeping prediction sets compact, indicating that
open-source, mid-sized LLMs can already support teacher-in-the-loop AES; we
discuss scaling and broader user studies as future work.

### 25. SERVAL: Surprisingly Effective Zero-Shot Visual Document Retrieval Powered by Large Vision and Language Models

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Thong Nguyen, Yibin Lei, Jia-Huei Ju, Andrew Yates
- **URL**: <http://arxiv.org/abs/2509.15432v1>
- **Submitted**: 2025-09-18 21:11:13
- **Comment**: Accepted
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper focuses on Visual Document Retrieval, a topic related to Information Retrieval, but it doesn't align with the user's core research themes of query understanding, ranking models, and user behavior modeling. The use of large vision and language models is also more relevant to NLP, but the paper's abstract doesn't mention any connection to real-time relevance optimization or deep semantic understanding.

#### Abstract
> Visual Document Retrieval (VDR) typically operates as text-to-image retrieval
using specialized bi-encoders trained to directly embed document images. We
revisit a zero-shot generate-and-encode pipeline: a vision-language model first
produces a detailed textual description of each document image, which is then
embedded by a standard text encoder. On the ViDoRe-v2 benchmark, the method
reaches 63.4% nDCG@5, surpassing the strongest specialised multi-vector visual
document encoder. It also scales better to large collections and offers broader
multilingual coverage. Analysis shows that modern vision-language models
capture complex textual and visual cues with sufficient granularity to act as a
reusable semantic proxy. By offloading modality alignment to pretrained
vision-language models, our approach removes the need for computationally
intensive text-image contrastive training and establishes a strong zero-shot
baseline for future VDR systems.

### 26. DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Sikai Bai, Haoxi Li, Jie Zhang, Zicong Hong, Song Guo
- **URL**: <http://arxiv.org/abs/2509.16105v1>
- **Submitted**: 2025-09-19 15:47:42
- **Comment**: 18 pages
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on model compression and pruning techniques for Mixture-of-Experts models, primarily in the context of Natural Language Processing (NLP). While it involves deep learning and optimization, it does not directly relate to information retrieval, query understanding, or user behavior modeling, which are core areas of your research interests.

#### Abstract
> Despite the significant breakthrough of Mixture-of-Experts (MoE), the
increasing scale of these MoE models presents huge memory and storage
challenges. Existing MoE pruning methods, which involve reducing parameter size
with a uniform sparsity across all layers, often lead to suboptimal outcomes
and performance degradation due to varying expert redundancy in different MoE
layers. To address this, we propose a non-uniform pruning strategy, dubbed
\textbf{Di}fferentiable \textbf{E}xpert \textbf{P}runing (\textbf{DiEP}), which
adaptively adjusts pruning rates at the layer level while jointly learning
inter-layer importance, effectively capturing the varying redundancy across
different MoE layers. By transforming the global discrete search space into a
continuous one, our method handles exponentially growing non-uniform expert
combinations, enabling adaptive gradient-based pruning. Extensive experiments
on five advanced MoE models demonstrate the efficacy of our method across
various NLP tasks. Notably, \textbf{DiEP} retains around 92\% of original
performance on Mixtral 8$\times$7B with only half the experts, outperforming
other pruning methods by up to 7.1\% on the challenging MMLU dataset.

### 27. BEFT: Bias-Efficient Fine-Tuning of Language Models

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Baichuan Huang, Ananth Balashankar, Amir Aminifar
- **URL**: <http://arxiv.org/abs/2509.15974v1>
- **Submitted**: 2025-09-19 13:35:07
- **Topic Keywords**: query
- **Reason**: This paper is primarily focused on fine-tuning language models, which is a topic in NLP, but it does not directly relate to information retrieval, query understanding, or ranking models, which are core areas of your research interests.

#### Abstract
> Fine-tuning all-bias-terms stands out among various parameter-efficient
fine-tuning (PEFT) techniques, owing to its out-of-the-box usability and
competitive performance, especially in low-data regimes. Bias-only fine-tuning
has the potential for unprecedented parameter efficiency. However, the link
between fine-tuning different bias terms (i.e., bias terms in the query, key,
or value projections) and downstream performance remains unclear. The existing
approaches, e.g., based on the magnitude of bias change or empirical Fisher
information, provide limited guidance for selecting the particular bias term
for effective fine-tuning. In this paper, we propose an approach for selecting
the bias term to be fine-tuned, forming the foundation of our bias-efficient
fine-tuning (BEFT). We extensively evaluate our bias-efficient approach against
other bias-selection approaches, across a wide range of large language models
(LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B
parameters. Our results demonstrate the effectiveness and superiority of our
bias-efficient approach on diverse downstream tasks, including classification,
multiple-choice, and generation tasks.

### 28. Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Sara Rajaee, Rochelle Choenni, Ekaterina Shutova, Christof Monz
- **URL**: <http://arxiv.org/abs/2509.15811v1>
- **Submitted**: 2025-09-19 09:38:54
- **Topic Keywords**: rag, rank
- **Reason**: This paper focuses on cross-lingual reward modeling for mathematical reasoning, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves large language models and ranking, the context is specific to mathematical reasoning and multilingual models, making it somewhat tangential to the user's interests.

#### Abstract
> While the reasoning abilities of large language models (LLMs) continue to
advance, it remains unclear how such ability varies across languages in
multilingual LLMs and whether different languages produce reasoning paths that
complement each other. To investigate this question, we train a reward model to
rank generated responses for a given question across languages. Our results
show that our cross-lingual reward model substantially improves mathematical
reasoning performance compared to using reward modeling within a single
language, benefiting even high-resource languages. While English often exhibits
the highest performance in multilingual models, we find that cross-lingual
sampling particularly benefits English under low sampling budgets. Our findings
reveal new opportunities to improve multilingual reasoning by leveraging the
complementary strengths of diverse languages.

### 29. Deep learning and abstractive summarisation for radiological reports: an empirical study for adapting the PEGASUS models' family with scarce data

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Claudio Benzoni, Martina Langhals, Martin Boeker, Luise Modersohn, M√°t√© E. Maros
- **URL**: <http://arxiv.org/abs/2509.15419v1>
- **Submitted**: 2025-09-18 20:51:33
- **Comment**: 14 pages, 4 figures, and 3 tables
- **Topic Keywords**: relevance
- **Reason**: This paper is primarily focused on abstractive summarization in the medical domain, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves deep learning and fine-tuning models, the context and application are quite different from the user's interests.

#### Abstract
> Regardless of the rapid development of artificial intelligence, abstractive
summarisation is still challenging for sensitive and data-restrictive domains
like medicine. With the increasing number of imaging, the relevance of
automated tools for complex medical text summarisation is expected to become
highly relevant. In this paper, we investigated the adaptation via fine-tuning
process of a non-domain-specific abstractive summarisation encoder-decoder
model family, and gave insights to practitioners on how to avoid over- and
underfitting. We used PEGASUS and PEGASUS-X, on a medium-sized radiological
reports public dataset. For each model, we comprehensively evaluated two
different checkpoints with varying sizes of the same training data. We
monitored the models' performances with lexical and semantic metrics during the
training history on the fixed-size validation set. PEGASUS exhibited different
phases, which can be related to epoch-wise double-descent, or
peak-drop-recovery behaviour. For PEGASUS-X, we found that using a larger
checkpoint led to a performance detriment. This work highlights the challenges
and risks of fine-tuning models with high expressivity when dealing with scarce
training data, and lays the groundwork for future investigations into more
robust fine-tuning strategies for summarisation models in specialised domains.

### 30. RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jane Luo, Xin Zhang, Steven Liu, Jie Wu, Yiming Huang, Yangyu Huang, Chengyu Yin, Ying Xin, Jianfeng Liu, Yuefeng Zhan, Hao Sun, Qi Chen, Scarlett Li, Mao Yang
- **URL**: <http://arxiv.org/abs/2509.16198v1>
- **Submitted**: 2025-09-19 17:58:14
- **Topic Keywords**: rag
- **Reason**: This paper focuses on codebase generation and repository planning, which is unrelated to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Large language models excel at function- and file-level code generation, yet
generating complete repositories from scratch remains a fundamental challenge.
This process demands coherent and reliable planning across proposal- and
implementation-level stages, while natural language, due to its ambiguity and
verbosity, is ill-suited for faithfully representing complex software
structures. To address this, we introduce the Repository Planning Graph (RPG),
a persistent representation that unifies proposal- and implementation-level
planning by encoding capabilities, file structures, data flows, and functions
in one graph. RPG replaces ambiguous natural language with an explicit
blueprint, enabling long-horizon planning and scalable repository generation.
Building on RPG, we develop ZeroRepo, a graph-driven framework for repository
generation from scratch. It operates in three stages: proposal-level planning
and implementation-level refinement to construct the graph, followed by
graph-guided code generation with test validation. To evaluate this setting, we
construct RepoCraft, a benchmark of six real-world projects with 1,052 tasks.
On RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly
3.9$\times$ the strongest baseline (Claude Code) and about 64$\times$ other
baselines. It attains 81.5% functional coverage and a 69.7% pass rate,
exceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further
analysis shows that RPG models complex dependencies, enables progressively more
sophisticated planning through near-linear scaling, and enhances LLM
understanding of repositories, thereby accelerating agent localization.

### 31. Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Sang Hoon Woo, Sehun Lee, Kang-wook Kim, Gunhee Kim
- **URL**: <http://arxiv.org/abs/2509.16028v1>
- **Submitted**: 2025-09-19 14:34:22
- **Comment**: EMNLP 2025 Main. Project page: https://yhytoto12.github.io/TVS-ReVerT
- **Topic Keywords**: rag
- **Reason**: This paper appears to be primarily focused on spoken dialogue systems, large language models, and speech generation, which are not directly related to the user's core research themes in Information Retrieval and Search technologies. While it touches on natural language processing, the emphasis is on spoken communication rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> Spoken dialogue systems increasingly employ large language models (LLMs) to
leverage their advanced reasoning capabilities. However, direct application of
LLMs in spoken communication often yield suboptimal results due to mismatches
between optimal textual and verbal delivery. While existing approaches adapt
LLMs to produce speech-friendly outputs, their impact on reasoning performance
remains underexplored. In this work, we propose Think-Verbalize-Speak, a
framework that decouples reasoning from spoken delivery to preserve the full
reasoning capacity of LLMs. Central to our method is verbalizing, an
intermediate step that translates thoughts into natural, speech-ready text. We
also introduce ReVerT, a latency-efficient verbalizer based on incremental and
asynchronous summarization. Experiments across multiple benchmarks show that
our method enhances speech naturalness and conciseness with minimal impact on
reasoning. The project page with the dataset and the source code is available
at https://yhytoto12.github.io/TVS-ReVerT

### 32. Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zhongze Luo, Zhenshuai Yin, Yongxin Guo, Zhichao Wang, Jionghao Zhu, Xiaoying Tang
- **URL**: <http://arxiv.org/abs/2509.15839v1>
- **Submitted**: 2025-09-19 10:18:48
- **Topic Keywords**: rag
- **Reason**: This paper focuses on multimodal LLMs for physics reasoning in Chinese, which is not directly related to Information Retrieval, Search technologies, or user behavior modeling. Although it involves deep semantic understanding, the context is limited to a specific domain (physics) and language (Chinese), making it less relevant to the user's broader research interests.

#### Abstract
> While multimodal LLMs (MLLMs) demonstrate remarkable reasoning progress,
their application in specialized scientific domains like physics reveals
significant gaps in current evaluation benchmarks. Specifically, existing
benchmarks often lack fine-grained subject coverage, neglect the step-by-step
reasoning process, and are predominantly English-centric, failing to
systematically evaluate the role of visual information. Therefore, we introduce
\textbf {Multi-Physics} for Chinese physics reasoning, a comprehensive
benchmark that includes 5 difficulty levels, featuring 1,412 image-associated,
multiple-choice questions spanning 11 high-school physics subjects. We employ a
dual evaluation framework to evaluate 20 different MLLMs, analyzing both final
answer accuracy and the step-by-step integrity of their chain-of-thought.
Furthermore, we systematically study the impact of difficulty level and visual
information by comparing the model performance before and after changing the
input mode. Our work provides not only a fine-grained resource for the
community but also offers a robust methodology for dissecting the multimodal
reasoning process of state-of-the-art MLLMs, and our dataset and code have been
open-sourced: https://github.com/luozhongze/Multi-Physics.

### 33. UPRPRC: Unified Pipeline for Reproducing Parallel Resources -- Corpus from the United Nations

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Qiuyang Lu, Fangjian Shen, Zhengkai Tang, Qiang Liu, Hexuan Cheng, Hui Liu, Wushao Wen
- **URL**: <http://arxiv.org/abs/2509.15789v1>
- **Submitted**: 2025-09-19 09:21:13
- **Comment**: 5 pages, 1 figure, submitted to ICASSP2026
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing, as it focuses on corpus creation and parallel resource reproduction for machine translation, rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> The quality and accessibility of multilingual datasets are crucial for
advancing machine translation. However, previous corpora built from United
Nations documents have suffered from issues such as opaque process, difficulty
of reproduction, and limited scale. To address these challenges, we introduce a
complete end-to-end solution, from data acquisition via web scraping to text
alignment. The entire process is fully reproducible, with a minimalist
single-machine example and optional distributed computing steps for
scalability. At its core, we propose a new Graph-Aided Paragraph Alignment
(GAPA) algorithm for efficient and flexible paragraph-level alignment. The
resulting corpus contains over 713 million English tokens, more than doubling
the scale of prior work. To the best of our knowledge, this represents the
largest publicly available parallel corpus composed entirely of
human-translated, non-AI-generated content. Our code and corpus are accessible
under the MIT License.

### 34. VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Dimitrios Damianos, Leon Voukoutis, Georgios Paraskevopoulos, Vassilis Katsouros
- **URL**: <http://arxiv.org/abs/2509.15667v1>
- **Submitted**: 2025-09-19 06:42:42
- **Topic Keywords**: rag
- **Reason**: This paper focuses on speech and language fusion, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves language models, the context is speech recognition rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> We present a multimodal fusion framework that bridges pre-trained
decoder-based large language models (LLM) and acoustic encoder-decoder
architectures such as Whisper, with the aim of building speech-enabled LLMs.
Instead of directly using audio embeddings, we explore an intermediate
audio-conditioned text space as a more effective mechanism for alignment. Our
method operates fully in continuous text representation spaces, fusing
Whisper's hidden decoder states with those of an LLM through cross-modal
attention, and supports both offline and streaming modes. We introduce
\textit{VoxKrikri}, the first Greek speech LLM, and show through analysis that
our approach effectively aligns representations across modalities. These
results highlight continuous space fusion as a promising path for multilingual
and low-resource speech LLMs, while achieving state-of-the-art results for
Automatic Speech Recognition in Greek, providing an average $\sim20\%$ relative
improvement across benchmarks.

### 35. Multilingual LLM Prompting Strategies for Medical English-Vietnamese Machine Translation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Nhu Vo, Nu-Uyen-Phuong Le, Dung D. Le, Massimo Piccardi, Wray Buntine
- **URL**: <http://arxiv.org/abs/2509.15640v1>
- **Submitted**: 2025-09-19 06:06:36
- **Comment**: The work is under peer review
- **Topic Keywords**: retrieval
- **Reason**: This paper focuses on multilingual machine translation, specifically for medical English-Vietnamese translation, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves LLMs and NLP, the application domain and specific research questions are not aligned with the user's interests.

#### Abstract
> Medical English-Vietnamese machine translation (En-Vi MT) is essential for
healthcare access and communication in Vietnam, yet Vietnamese remains a
low-resource and under-studied language. We systematically evaluate prompting
strategies for six multilingual LLMs (0.5B-9B parameters) on the MedEV dataset,
comparing zero-shot, few-shot, and dictionary-augmented prompting with Meddict,
an English-Vietnamese medical lexicon. Results show that model scale is the
primary driver of performance: larger LLMs achieve strong zero-shot results,
while few-shot prompting yields only marginal improvements. In contrast,
terminology-aware cues and embedding-based example retrieval consistently
improve domain-specific translation. These findings underscore both the promise
and the current limitations of multilingual LLMs for medical En-Vi MT.

### 36. Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Tomoya Yamashita, Yuuki Yamanaka, Masanori Yamada, Takayuki Miura, Toshiki Shibahara, Tomoharu Iwata
- **URL**: <http://arxiv.org/abs/2509.15621v1>
- **Submitted**: 2025-09-19 05:34:45
- **Topic Keywords**: rag
- **Reason**: This paper focuses on Concept Unlearning in Large Language Models, which is not directly related to Information Retrieval, Search technologies, or user behavior modeling. While it involves knowledge representation, it's more aligned with NLP and knowledge graph applications, but lacks direct relevance to the user's core research themes.

#### Abstract
> Machine Unlearning (MU) has recently attracted considerable attention as a
solution to privacy and copyright issues in large language models (LLMs).
Existing MU methods aim to remove specific target sentences from an LLM while
minimizing damage to unrelated knowledge. However, these approaches require
explicit target sentences and do not support removing broader concepts, such as
persons or events. To address this limitation, we introduce Concept Unlearning
(CU) as a new requirement for LLM unlearning. We leverage knowledge graphs to
represent the LLM's internal knowledge and define CU as removing the forgetting
target nodes and associated edges. This graph-based formulation enables a more
intuitive unlearning and facilitates the design of more effective methods. We
propose a novel method that prompts the LLM to generate knowledge triplets and
explanatory sentences about the forgetting target and applies the unlearning
process to these representations. Our approach enables more precise and
comprehensive concept removal by aligning the unlearning process with the LLM's
internal knowledge representations. Experiments on real-world and synthetic
datasets demonstrate that our method effectively achieves concept-level
unlearning while preserving unrelated knowledge.

### 37. Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Om Naphade, Saksham Bansal, Parikshit Pareek
- **URL**: <http://arxiv.org/abs/2509.15561v1>
- **Submitted**: 2025-09-19 03:46:42
- **Topic Keywords**: rag
- **Reason**: This paper focuses on hyperparameter tuning using small LLMs, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves machine learning, the context is not aligned with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Hyper-parameter Tuning (HPT) is a necessary step in machine learning (ML)
pipelines but becomes computationally expensive and opaque with larger models.
Recently, Large Language Models (LLMs) have been explored for HPT, yet most
rely on models exceeding 100 billion parameters. We propose an Expert Block
Framework for HPT using Small LLMs. At its core is the Trajectory Context
Summarizer (TCS), a deterministic block that transforms raw training
trajectories into structured context, enabling small LLMs to analyze
optimization progress with reliability comparable to larger models. Using two
locally-run LLMs (phi4:reasoning14B and qwen2.5-coder:32B) and a 10-trial
budget, our TCS-enabled HPT pipeline achieves average performance within ~0.9
percentage points of GPT-4 across six diverse tasks.

### 38. Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ping Guo, Yubing Ren, Binbin Liu, Fengze Liu, Haobin Lin, Yifan Zhang, Bingni Zhang, Taifeng Wang, Yin Zheng
- **URL**: <http://arxiv.org/abs/2509.15556v1>
- **Submitted**: 2025-09-19 03:34:34
- **Topic Keywords**: rag
- **Reason**: This paper focuses on multilingual data allocation for large language models pretraining, which is outside the primary scope of your research interests in Information Retrieval and Search technologies. While it involves NLP, the specific topic of multilingual data allocation for pretraining large language models is not directly related to your core areas of query understanding, ranking models, or user behavior modeling.

#### Abstract
> Large language models (LLMs) have become integral to a wide range of
applications worldwide, driving an unprecedented global demand for effective
multilingual capabilities. Central to achieving robust multilingual performance
is the strategic allocation of language proportions within training corpora.
However, determining optimal language ratios is highly challenging due to
intricate cross-lingual interactions and sensitivity to dataset scale. This
paper introduces Climb (Cross-Lingual Interaction-aware Multilingual
Balancing), a novel framework designed to systematically optimize multilingual
data allocation. At its core, Climb introduces a cross-lingual
interaction-aware language ratio, explicitly quantifying each language's
effective allocation by capturing inter-language dependencies. Leveraging this
ratio, Climb proposes a principled two-step optimization procedure--first
equalizing marginal benefits across languages, then maximizing the magnitude of
the resulting language allocation vectors--significantly simplifying the
inherently complex multilingual optimization problem. Extensive experiments
confirm that Climb can accurately measure cross-lingual interactions across
various multilingual settings. LLMs trained with Climb-derived proportions
consistently achieve state-of-the-art multilingual performance, even achieving
competitive performance with open-sourced LLMs trained with more tokens.

### 39. mucAI at BAREC Shared Task 2025: Towards Uncertainty Aware Arabic Readability Assessment

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ahmed Abdou
- **URL**: <http://arxiv.org/abs/2509.15485v1>
- **Submitted**: 2025-09-18 23:14:51
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves Arabic readability assessment and uses techniques like conformal prediction, it does not align with your focus on query understanding, ranking models, or user behavior modeling.

#### Abstract
> We present a simple, model-agnostic post-processing technique for
fine-grained Arabic readability classification in the BAREC 2025 Shared Task
(19 ordinal levels). Our method applies conformal prediction to generate
prediction sets with coverage guarantees, then computes weighted averages using
softmax-renormalized probabilities over the conformal sets. This
uncertainty-aware decoding improves Quadratic Weighted Kappa (QWK) by reducing
high-penalty misclassifications to nearer levels. Our approach shows consistent
QWK improvements of 1-3 points across different base models. In the strict
track, our submission achieves QWK scores of 84.9\%(test) and 85.7\% (blind
test) for sentence level, and 73.3\% for document level. For Arabic educational
assessment, this enables human reviewers to focus on a handful of plausible
levels, combining statistical guarantees with practical usability.

### 40. Frustratingly Easy Data Augmentation for Low-Resource ASR

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Katsumi Ibaraki, David Chiang
- **URL**: <http://arxiv.org/abs/2509.15373v1>
- **Submitted**: 2025-09-18 19:20:37
- **Comment**: 5 pages, 2 figures, 2 tables, submitted to ICASSP 2026
- **Topic Keywords**: rag
- **Reason**: This paper focuses on data augmentation for Automatic Speech Recognition (ASR), which is outside your primary research interests in Information Retrieval and Search technologies. While it involves NLP, the specific application and techniques are not directly related to your core themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> This paper introduces three self-contained data augmentation methods for
low-resource Automatic Speech Recognition (ASR). Our techniques first generate
novel text--using gloss-based replacement, random replacement, or an LLM-based
approach--and then apply Text-to-Speech (TTS) to produce synthetic audio. We
apply these methods, which leverage only the original annotated data, to four
languages with extremely limited resources (Vatlongos, Nashta, Shinekhen
Buryat, and Kakabe). Fine-tuning a pretrained Wav2Vec2-XLSR-53 model on a
combination of the original audio and generated synthetic data yields
significant performance gains, including a 14.3% absolute WER reduction for
Nashta. The methods prove effective across all four low-resource languages and
also show utility for high-resource languages like English, demonstrating their
broad applicability.

### 41. Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Het Patel, Muzammil Allie, Qian Zhang, Jia Chen, Evangelos E. Papalexakis
- **URL**: <http://arxiv.org/abs/2509.16163v1>
- **Submitted**: 2025-09-19 17:16:32
- **Comment**: To be presented as a poster at the Workshop on Safe and Trustworthy
  Multimodal AI Systems (SafeMM-AI), 2025
- **Topic Keywords**: rank
- **Reason**: This paper is not directly related to the user's core research themes in Information Retrieval and Search technologies, as it focuses on Vision-Language Models and adversarial attacks in the context of computer vision. While it involves tensor decomposition, which is a mathematical technique, the application and context are quite different from the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Vision language models (VLMs) excel in multimodal understanding but are prone
to adversarial attacks. Existing defenses often demand costly retraining or
significant architecture changes. We introduce a lightweight defense using
tensor decomposition suitable for any pre-trained VLM, requiring no retraining.
By decomposing and reconstructing vision encoder representations, it filters
adversarial noise while preserving meaning. Experiments with CLIP on COCO and
Flickr30K show improved robustness. On Flickr30K, it restores 12.3\%
performance lost to attacks, raising Recall@1 accuracy from 7.5\% to 19.8\%. On
COCO, it recovers 8.1\% performance, improving accuracy from 3.8\% to 11.9\%.
Analysis shows Tensor Train decomposition with low rank (8-32) and low residual
strength ($\alpha=0.1-0.2$) is optimal. This method is a practical,
plug-and-play solution with minimal overhead for existing VLMs.

### 42. Localmax dynamics for attention in transformers and its asymptotic behavior

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Henri Cimeti√®re, Maria Teresa Chiri, Bahman Gharesifard
- **URL**: <http://arxiv.org/abs/2509.15958v1>
- **Submitted**: 2025-09-19 13:18:30
- **Comment**: 28 pages, 5 figures
- **Topic Keywords**: search
- **Reason**: This paper focuses on a discrete-time attention model for transformers, which is a topic in Natural Language Processing (NLP). However, it does not appear to be directly related to information retrieval, query understanding, ranking models, or user behavior modeling, which are core areas of your research interests.

#### Abstract
> We introduce a new discrete-time attention model, termed the localmax
dynamics, which interpolates between the classic softmax dynamics and the
hardmax dynamics, where only the tokens that maximize the influence toward a
given token have a positive weight. As in hardmax, uniform weights are
determined by a parameter controlling neighbor influence, but the key extension
lies in relaxing neighborhood interactions through an alignment-sensitivity
parameter, which allows controlled deviations from pure hardmax behavior. As we
prove, while the convex hull of the token states still converges to a convex
polytope, its structure can no longer be fully described by a maximal alignment
set, prompting the introduction of quiescent sets to capture the invariant
behavior of tokens near vertices. We show that these sets play a key role in
understanding the asymptotic behavior of the system, even under time-varying
alignment sensitivity parameters. We further show that localmax dynamics does
not exhibit finite-time convergence and provide results for vanishing, nonzero,
time-varying alignment-sensitivity parameters, recovering the limiting behavior
of hardmax as a by-product. Finally, we adapt Lyapunov-based methods from
classical opinion dynamics, highlighting their limitations in the asymmetric
setting of localmax interactions and outlining directions for future research.

### 43. The Psychology of Falsehood: A Human-Centric Survey of Misinformation Detection

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Arghodeep Nandi, Megha Sundriyal, Euna Mehnaz Khan, Jikai Sun, Emily Vraga, Jaideep Srivastava, Tanmoy Chakraborty
- **URL**: <http://arxiv.org/abs/2509.15896v1>
- **Submitted**: 2025-09-19 11:51:17
- **Comment**: Accepted in EMNLP'25 Main
- **Topic Keywords**: search
- **Reason**: This paper focuses on the psychology of misinformation detection, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on the societal impact of misinformation, it does not address query understanding, ranking models, or user behavior modeling, making it only loosely relevant to the user's interests.

#### Abstract
> Misinformation remains one of the most significant issues in the digital age.
While automated fact-checking has emerged as a viable solution, most current
systems are limited to evaluating factual accuracy. However, the detrimental
effect of misinformation transcends simple falsehoods; it takes advantage of
how individuals perceive, interpret, and emotionally react to information. This
underscores the need to move beyond factuality and adopt more human-centered
detection frameworks. In this survey, we explore the evolving interplay between
traditional fact-checking approaches and psychological concepts such as
cognitive biases, social dynamics, and emotional responses. By analyzing
state-of-the-art misinformation detection systems through the lens of human
psychology and behavior, we reveal critical limitations of current methods and
identify opportunities for improvement. Additionally, we outline future
research directions aimed at creating more robust and adaptive frameworks, such
as neuro-behavioural models that integrate technological factors with the
complexities of human cognition and social influence. These approaches offer
promising pathways to more effectively detect and mitigate the societal harms
of misinformation.

### 44. Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Ke Wang, Wenning Wei, Yan Deng, Lei He, Sheng Zhao
- **URL**: <http://arxiv.org/abs/2509.15701v1>
- **Submitted**: 2025-09-19 07:23:25
- **Comment**: submitted to ICASSP2026
- **Topic Keywords**: rank
- **Reason**: This paper focuses on fine-tuning large multimodal models for Automatic Pronunciation Assessment, which is outside the user's core research themes in Information Retrieval and Search technologies. Although it involves some aspects of deep semantic understanding, the context and application are unrelated to the user's primary interests.

#### Abstract
> Automatic Pronunciation Assessment (APA) is critical for Computer-Assisted
Language Learning (CALL), requiring evaluation across multiple granularities
and aspects. Large Multimodal Models (LMMs) present new opportunities for APA,
but their effectiveness in fine-grained assessment remains uncertain. This work
investigates fine-tuning LMMs for APA using the Speechocean762 dataset and a
private corpus. Fine-tuning significantly outperforms zero-shot settings and
achieves competitive results on single-granularity tasks compared to public and
commercial systems. The model performs well at word and sentence levels, while
phoneme-level assessment remains challenging. We also observe that the Pearson
Correlation Coefficient (PCC) reaches 0.9, whereas Spearman's rank Correlation
Coefficient (SCC) remains around 0.6, suggesting that SCC better reflects
ordinal consistency. These findings highlight both the promise and limitations
of LMMs for APA and point to future work on fine-grained modeling and
rank-aware evaluation.

### 45. Direct Simultaneous Translation Activation for Large Audio-Language Models

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Pei Zhang, Yiming Wang, Jialong Tang, Baosong Yang, Rui Wang, Derek F. Wong, Fei Huang
- **URL**: <http://arxiv.org/abs/2509.15692v1>
- **Submitted**: 2025-09-19 07:12:18
- **Topic Keywords**: search
- **Reason**: This paper focuses on simultaneous speech-to-text translation using large audio-language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing, especially in areas requiring deep semantic understanding and real-time relevance optimization.

#### Abstract
> Simultaneous speech-to-text translation (Simul-S2TT) aims to translate speech
into target text in real time, outputting translations while receiving source
speech input, rather than waiting for the entire utterance to be spoken.
Simul-S2TT research often modifies model architectures to implement read-write
strategies. However, with the rise of large audio-language models (LALMs), a
key challenge is how to directly activate Simul-S2TT capabilities in base
models without additional architectural changes. In this paper, we introduce
{\bf Simul}taneous {\bf S}elf-{\bf A}ugmentation ({\bf SimulSA}), a strategy
that utilizes LALMs' inherent capabilities to obtain simultaneous data by
randomly truncating speech and constructing partially aligned translation. By
incorporating them into offline SFT data, SimulSA effectively bridges the
distribution gap between offline translation during pretraining and
simultaneous translation during inference. Experimental results demonstrate
that augmenting only about {\bf 1\%} of the simultaneous data, compared to the
full offline SFT data, can significantly activate LALMs' Simul-S2TT
capabilities without modifications to model architecture or decoding strategy.

### 46. Layer-wise Minimal Pair Probing Reveals Contextual Grammatical-Conceptual Hierarchy in Speech Representations

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Linyang He, Qiaolin Wang, Xilin Jiang, Nima Mesgarani
- **URL**: <http://arxiv.org/abs/2509.15655v1>
- **Submitted**: 2025-09-19 06:29:33
- **Comment**: EMNLP 2025 Main Conference (Oral)
- **Topic Keywords**: search
- **Reason**: This paper is not directly related to Information Retrieval or Search technologies, and its focus on speech representations and linguistic features does not align with your primary research interests.

#### Abstract
> Transformer-based speech language models (SLMs) have significantly improved
neural speech recognition and understanding. While existing research has
examined how well SLMs encode shallow acoustic and phonetic features, the
extent to which SLMs encode nuanced syntactic and conceptual features remains
unclear. By drawing parallels with linguistic competence assessments for large
language models, this study is the first to systematically evaluate the
presence of contextual syntactic and semantic features across SLMs for
self-supervised learning (S3M), automatic speech recognition (ASR), speech
compression (codec), and as the encoder for auditory large language models
(AudioLLMs). Through minimal pair designs and diagnostic feature analysis
across 71 tasks spanning diverse linguistic levels, our layer-wise and
time-resolved analysis uncovers that 1) all speech encode grammatical features
more robustly than conceptual ones.

### 47. A method for improving multilingual quality and diversity of instruction fine-tuning datasets

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Chunguang Zhao, Yilun Liu, Pufan Zeng, Yuanchang Luo, Shimin Tao, Minggui He, Weibin Meng, Song Xu, Ziang Chen, Chen Liu, Hongxia Ma, Li Zhang, Boxing Chen, Daimeng Wei
- **URL**: <http://arxiv.org/abs/2509.15549v1>
- **Submitted**: 2025-09-19 03:07:59
- **Topic Keywords**: search
- **Reason**: This paper focuses on improving multilingual quality and diversity of instruction fine-tuning datasets, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves large language models and fine-tuning, the context is more aligned with NLP and data quality rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> Multilingual Instruction Fine-Tuning (IFT) is essential for enabling large
language models (LLMs) to generalize effectively across diverse linguistic and
cultural contexts. However, the scarcity of high-quality multilingual training
data and corresponding building method remains a critical bottleneck. While
data selection has shown promise in English settings, existing methods often
fail to generalize across languages due to reliance on simplistic heuristics or
language-specific assumptions. In this work, we introduce Multilingual Data
Quality and Diversity (M-DaQ), a novel method for improving LLMs
multilinguality, by selecting high-quality and semantically diverse
multilingual IFT samples. We further conduct the first systematic investigation
of the Superficial Alignment Hypothesis (SAH) in multilingual setting.
Empirical results across 18 languages demonstrate that models fine-tuned with
M-DaQ method achieve significant performance gains over vanilla baselines over
60% win rate. Human evaluations further validate these gains, highlighting the
increment of cultural points in the response. We release the M-DaQ code to
support future research.

### 48. Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization

- **LLM Score**: 0
- **Keyword Score**: 2
- **Authors**: Yun Tang, Cindy Tseng
- **URL**: <http://arxiv.org/abs/2509.15579v1>
- **Submitted**: 2025-09-19 04:29:59
- **Topic Keywords**: rag
- **Reason**: This paper focuses on speech pre-training with a chunk-based approach and finite scalar quantization, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Low latency speech human-machine communication is becoming increasingly
necessary as speech technology advances quickly in the last decade. One of the
primary factors behind the advancement of speech technology is self-supervised
learning. Most self-supervised learning algorithms are designed with full
utterance assumption and compromises have to made if partial utterances are
presented, which are common in the streaming applications. In this work, we
propose a chunk based self-supervised learning (Chunk SSL) algorithm as an
unified solution for both streaming and offline speech pre-training. Chunk SSL
is optimized with the masked prediction loss and an acoustic encoder is
encouraged to restore indices of those masked speech frames with help from
unmasked frames in the same chunk and preceding chunks. A copy and append data
augmentation approach is proposed to conduct efficient chunk based
pre-training. Chunk SSL utilizes a finite scalar quantization (FSQ) module to
discretize input speech features and our study shows a high resolution FSQ
codebook, i.e., a codebook with vocabulary size up to a few millions, is
beneficial to transfer knowledge from the pre-training task to the downstream
tasks. A group masked prediction loss is employed during pre-training to
alleviate the high memory and computation cost introduced by the large
codebook. The proposed approach is examined in two speech to text tasks, i.e.,
speech recognition and speech translation. Experimental results on the
\textsc{Librispeech} and \textsc{Must-C} datasets show that the proposed method
could achieve very competitive results for speech to text tasks at both
streaming and offline modes.

### 49. DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm

- **LLM Score**: 0
- **Keyword Score**: 2
- **Authors**: Xiaowei Zhu, Yubing Ren, Fang Fang, Qingfeng Tan, Shi Wang, Yanan Cao
- **URL**: <http://arxiv.org/abs/2509.15550v1>
- **Submitted**: 2025-09-19 03:08:13
- **Comment**: NeurIPS 2025 Spotlight
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The paper focuses on AI-generated text detection, which is outside your primary areas of interest.

#### Abstract
> The rapid advancement of large language models (LLMs) has blurred the line
between AI-generated and human-written text. This progress brings societal
risks such as misinformation, authorship ambiguity, and intellectual property
concerns, highlighting the urgent need for reliable AI-generated text detection
methods. However, recent advances in generative language modeling have resulted
in significant overlap between the feature distributions of human-written and
AI-generated text, blurring classification boundaries and making accurate
detection increasingly challenging. To address the above challenges, we propose
a DNA-inspired perspective, leveraging a repair-based process to directly and
interpretably capture the intrinsic differences between human-written and
AI-generated text. Building on this perspective, we introduce DNA-DetectLLM, a
zero-shot detection method for distinguishing AI-generated and human-written
text. The method constructs an ideal AI-generated sequence for each input,
iteratively repairs non-optimal tokens, and quantifies the cumulative repair
effort as an interpretable detection signal. Empirical evaluations demonstrate
that our method achieves state-of-the-art detection performance and exhibits
strong robustness against various adversarial attacks and input lengths.
Specifically, DNA-DetectLLM achieves relative improvements of 5.55% in AUROC
and 2.08% in F1 score across multiple public benchmark datasets.

### 50. Dual-Mode Visual System for Brain-Computer Interfaces: Integrating SSVEP and P300 Responses

- **LLM Score**: 0
- **Keyword Score**: 2
- **Authors**: Ekgari Kasawala, Surej Mouli
- **URL**: <http://arxiv.org/abs/2509.15439v1>
- **Submitted**: 2025-09-18 21:25:18
- **Comment**: 15 Pages
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, Natural Language Processing, data mining, or recommender systems. The paper focuses on brain-computer interfaces and neurophysiological signals, which are unrelated to your areas of expertise.

#### Abstract
> In brain-computer interface (BCI) systems, steady-state visual evoked
potentials (SSVEP) and P300 responses have achieved widespread implementation
owing to their superior information transfer rates (ITR) and minimal training
requirements. These neurophysiological signals have exhibited robust efficacy
and versatility in external device control, demonstrating enhanced precision
and scalability. However, conventional implementations predominantly utilise
liquid crystal display (LCD)-based visual stimulation paradigms, which present
limitations in practical deployment scenarios. This investigation presents the
development and evaluation of a novel light-emitting diode (LED)-based dual
stimulation apparatus designed to enhance SSVEP classification accuracy through
the integration of both SSVEP and P300 paradigms. The system employs four
distinct frequencies, 7 Hz, 8 Hz, 9 Hz, and 10 Hz, corresponding to forward,
backward, right, and left directional controls, respectively. Oscilloscopic
verification confirmed the precision of these stimulation frequencies.
Real-time feature extraction was accomplished through the concurrent analysis
of maximum Fast Fourier Transform (FFT) amplitude and P300 peak detection to
ascertain user intent. Directional control was determined by the frequency
exhibiting maximal amplitude characteristics. The visual stimulation hardware
demonstrated minimal frequency deviation, with error differentials ranging from
0.15%to 0.20%across all frequencies. The implemented signal processing
algorithm successfully discriminated all four stimulus frequencies whilst
correlating them with their respective P300 event markers. Classification
accuracy was evaluated based on correct task intention recognition. The
proposed hybrid system achieved a mean classification accuracy of 86.25%,
coupled with an average ITR of 42.08 bits per minute (bpm).

---

