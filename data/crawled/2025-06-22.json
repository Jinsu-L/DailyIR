[
    {
        "title": "Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency",
        "abstract": "Fine-tuning a general-purpose large language model (LLM) for a specific\ndomain or task has become a routine procedure for ordinary users. However,\nfine-tuning is known to remove the safety alignment features of the model, even\nwhen the fine-tuning data does not contain any harmful content. We consider\nthis to be a critical failure mode of LLMs due to the widespread uptake of\nfine-tuning, combined with the benign nature of the \"attack\". Most\nwell-intentioned developers are likely unaware that they are deploying an LLM\nwith reduced safety. On the other hand, this known vulnerability can be easily\nexploited by malicious actors intending to bypass safety guardrails. To make\nany meaningful progress in mitigating this issue, we first need reliable and\nreproducible safety evaluations. In this work, we investigate how robust a\nsafety benchmark is to trivial variations in the experimental procedure, and\nthe stochastic nature of LLMs. Our initial experiments expose surprising\nvariance in the results of the safety evaluation, even when seemingly\ninconsequential changes are made to the fine-tuning setup. Our observations\nhave serious implications for how researchers in this field should report\nresults to enable meaningful comparisons in the future.",
        "url": "http://arxiv.org/abs/2506.17209v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17209v1",
        "arxiv_id": "2506.17209v1",
        "authors": [
            "Kathleen C. Fraser",
            "Hillary Dawkins",
            "Isar Nejadgholi",
            "Svetlana Kiritchenko"
        ],
        "submitted": "2025-06-20 17:57:12",
        "source": "arxiv",
        "comment": "to appear at LLMSEC 2025"
    },
    {
        "title": "Dissecting the SWE-Bench Leaderboards: Profiling Submitters and Architectures of LLM- and Agent-Based Repair Systems",
        "abstract": "The rapid progress in Automated Program Repair (APR) has been driven by\nadvances in AI, particularly large language models (LLMs) and agent-based\nsystems. SWE-Bench is a recent benchmark designed to evaluate LLM-based repair\nsystems using real issues and pull requests mined from 12 popular open-source\nPython repositories. Its public leaderboards, SWE-Bench Lite and SWE-Bench\nVerified, have become central platforms for tracking progress and comparing\nsolutions. However, because the submission process does not require detailed\ndocumentation, the architectural design and origin of many solutions remain\nunclear. In this paper, we present the first comprehensive study of all\nsubmissions to the SWE-Bench Lite (68 entries) and Verified (79 entries)\nleaderboards, analyzing 67 unique approaches across dimensions such as\nsubmitter type, product availability, LLM usage, and system architecture. Our\nfindings reveal the dominance of proprietary LLMs (especially Claude 3.5/3.7),\nthe presence of both agentic and non-agentic designs, and a contributor base\nspanning from individual developers to large tech companies.",
        "url": "http://arxiv.org/abs/2506.17208v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17208v1",
        "arxiv_id": "2506.17208v1",
        "authors": [
            "Matias Martinez",
            "Xavier Franch"
        ],
        "submitted": "2025-06-20 17:57:08",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Towards AI Search Paradigm",
        "abstract": "In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint\nfor next-generation search systems capable of emulating human information\nprocessing and decision-making. The paradigm employs a modular architecture of\nfour LLM-powered agents (Master, Planner, Executor and Writer) that dynamically\nadapt to the full spectrum of information needs, from simple factual queries to\ncomplex multi-stage reasoning tasks. These agents collaborate dynamically\nthrough coordinated workflows to evaluate query complexity, decompose problems\ninto executable plans, and orchestrate tool usage, task execution, and content\nsynthesis. We systematically present key methodologies for realizing this\nparadigm, including task planning and tool integration, execution strategies,\naligned and robust retrieval-augmented generation, and efficient LLM inference,\nspanning both algorithmic techniques and infrastructure-level optimizations. By\nproviding an in-depth guide to these foundational components, this work aims to\ninform the development of trustworthy, adaptive, and scalable AI search\nsystems.",
        "url": "http://arxiv.org/abs/2506.17188v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17188v1",
        "arxiv_id": "2506.17188v1",
        "authors": [
            "Yuchen Li",
            "Hengyi Cai",
            "Rui Kong",
            "Xinran Chen",
            "Jiamin Chen",
            "Jun Yang",
            "Haojie Zhang",
            "Jiayi Li",
            "Jiayi Wu",
            "Yiqun Chen",
            "Changle Qu",
            "Keyi Kong",
            "Wenwen Ye",
            "Lixin Su",
            "Xinyu Ma",
            "Long Xia",
            "Daiting Shi",
            "Jiashu Zhao",
            "Haoyi Xiong",
            "Shuaiqiang Wang",
            "Dawei Yin"
        ],
        "submitted": "2025-06-20 17:42:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CLEAR-3K: Assessing Causal Explanatory Capabilities in Language Models",
        "abstract": "We introduce CLEAR-3K, a dataset of 3,000 assertion-reasoning questions\ndesigned to evaluate whether language models can determine if one statement\ncausally explains another. Each question present an assertion-reason pair and\nchallenge language models to distinguish between semantic relatedness and\ngenuine causal explanatory relationships. Through comprehensive evaluation of\n21 state-of-the-art language models (ranging from 0.5B to 72B parameters), we\nidentify two fundamental findings. First, language models frequently confuse\nsemantic similarity with causality, relying on lexical and semantic overlap\ninstead of inferring actual causal explanatory relationships. Second, as\nparameter size increases, models tend to shift from being overly skeptical\nabout causal relationships to being excessively permissive in accepting them.\nDespite this shift, performance measured by the Matthews Correlation\nCoefficient plateaus at just 0.55, even for the best-performing models.Hence,\nCLEAR-3K provides a crucial benchmark for developing and evaluating genuine\ncausal reasoning in language models, which is an essential capability for\napplications that require accurate assessment of causal relationships.",
        "url": "http://arxiv.org/abs/2506.17180v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17180v1",
        "arxiv_id": "2506.17180v1",
        "authors": [
            "Naiming Liu",
            "Richard Baraniuk",
            "Shashank Sonkar"
        ],
        "submitted": "2025-06-20 17:35:36",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Cache Me If You Can: How Many KVs Do You Need for Effective Long-Context LMs?",
        "abstract": "Language models handle increasingly long contexts for tasks such as book\nsummarization, but this leads to growing memory costs for the key-value (KV)\ncache. Many prior works have proposed ways of discarding KVs from memory, but\ntheir approaches are tailored to favorable settings, obscuring caveats like\nhigh peak memory and performance degradation, and a fair comparison between\nmethods is difficult. In this paper, we propose the *KV footprint* as a unified\nmetric, which accounts for both the amount of KV entries stored and their\nlifespan in memory. We evaluate methods based on the smallest footprint they\nattain while preserving performance in both long-context understanding and\ngeneration, with context lengths of up to 128K tokens. This metric reveals the\nhigh peak memory of prior KV eviction methods. One class of methods --\n*post-fill eviction* -- has a high footprint due to being incompatible with\neviction during pre-filling. We adapt these methods to be able to evict KVs\nduring pre-filling, achieving substantially lower KV footprints. We then turn\nto *recency eviction* methods, wherein we propose PruLong, an end-to-end\noptimization method for learning which attention heads need to retain the full\nKV cache and which do not. PruLong saves memory while preserving long-context\nperformance, achieving 12% smaller KV footprint than prior methods while\nretaining performance in challenging recall tasks. Our paper clarifies the\ncomplex tangle of long-context inference methods and paves the way for future\ndevelopment to minimize the KV footprint.",
        "url": "http://arxiv.org/abs/2506.17121v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17121v1",
        "arxiv_id": "2506.17121v1",
        "authors": [
            "Adithya Bhaskar",
            "Alexander Wettig",
            "Tianyu Gao",
            "Yihe Dong",
            "Danqi Chen"
        ],
        "submitted": "2025-06-20 16:21:12",
        "source": "arxiv",
        "comment": "We release our code publicly at\n  https://github.com/princeton-pli/PruLong"
    },
    {
        "title": "MEXA: Towards General Multimodal Reasoning with Dynamic Multi-Expert Aggregation",
        "abstract": "Combining pre-trained expert models offers substantial potential for scalable\nmultimodal reasoning, but building a unified framework remains challenging due\nto the increasing diversity of input modalities and task complexity. For\ninstance, medical diagnosis requires precise reasoning over structured clinical\ntables, while financial forecasting depends on interpreting plot-based data to\nmake informed predictions. To tackle this challenge, we introduce MEXA, a\ntraining-free framework that performs modality- and task-aware aggregation of\nmultiple expert models to enable effective multimodal reasoning across diverse\nand distinct domains. MEXA dynamically selects expert models based on the input\nmodality and the task-specific reasoning demands (i.e., skills). Each expert\nmodel, specialized in a modality task pair, generates interpretable textual\nreasoning outputs. MEXA then aggregates and reasons over these outputs using a\nLarge Reasoning Model (LRM) to produce the final answer. This modular design\nallows flexible and transparent multimodal reasoning across diverse domains\nwithout additional training overhead. We extensively evaluate our approach on\ndiverse multimodal benchmarks, including Video Reasoning, Audio Reasoning, 3D\nUnderstanding, and Medical QA. MEXA consistently delivers performance\nimprovements over strong multimodal baselines, highlighting the effectiveness\nand broad applicability of our expert-driven selection and aggregation in\ndiverse multimodal reasoning tasks.",
        "url": "http://arxiv.org/abs/2506.17113v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17113v1",
        "arxiv_id": "2506.17113v1",
        "authors": [
            "Shoubin Yu",
            "Yue Zhang",
            "Ziyang Wang",
            "Jaehong Yoon",
            "Mohit Bansal"
        ],
        "submitted": "2025-06-20 16:14:13",
        "source": "arxiv",
        "comment": "The first two authors contributed equally; Github link:\n  https://github.com/Yui010206/MEXA"
    },
    {
        "title": "Are Bias Evaluation Methods Biased ?",
        "abstract": "The creation of benchmarks to evaluate the safety of Large Language Models is\none of the key activities within the trusted AI community. These benchmarks\nallow models to be compared for different aspects of safety such as toxicity,\nbias, harmful behavior etc. Independent benchmarks adopt different approaches\nwith distinct data sets and evaluation methods. We investigate how robust such\nbenchmarks are by using different approaches to rank a set of representative\nmodels for bias and compare how similar are the overall rankings. We show that\ndifferent but widely used bias evaluations methods result in disparate model\nrankings. We conclude with recommendations for the community in the usage of\nsuch benchmarks.",
        "url": "http://arxiv.org/abs/2506.17111v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17111v1",
        "arxiv_id": "2506.17111v1",
        "authors": [
            "Lina Berrayana",
            "Sean Rooney",
            "Luis Garcés-Erice",
            "Ioana Giurgiu"
        ],
        "submitted": "2025-06-20 16:11:25",
        "source": "arxiv",
        "comment": "Accepted to ACL 2025 Workshop GEM"
    },
    {
        "title": "Better Language Model Inversion by Compactly Representing Next-Token Distributions",
        "abstract": "Language model inversion seeks to recover hidden prompts using only language\nmodel outputs. This capability has implications for security and accountability\nin language model deployments, such as leaking private information from an\nAPI-protected language model's system message. We propose a new method --\nprompt inversion from logprob sequences (PILS) -- that recovers hidden prompts\nby gleaning clues from the model's next-token probabilities over the course of\nmultiple generation steps. Our method is enabled by a key insight: The\nvector-valued outputs of a language model occupy a low-dimensional subspace.\nThis enables us to losslessly compress the full next-token probability\ndistribution over multiple generation steps using a linear map, allowing more\noutput information to be used for inversion. Our approach yields massive gains\nover previous state-of-the-art methods for recovering hidden prompts, achieving\n2--3.5 times higher exact recovery rates across test sets, in one case\nincreasing the recovery rate from 17% to 60%. Our method also exhibits\nsurprisingly good generalization behavior; for instance, an inverter trained on\n16 generations steps gets 5--27 points higher prompt recovery when we increase\nthe number of steps to 32 at test time. Furthermore, we demonstrate strong\nperformance of our method on the more challenging task of recovering hidden\nsystem messages. We also analyze the role of verbatim repetition in prompt\nrecovery and propose a new method for cross-family model transfer for\nlogit-based inverters. Our findings show that next-token probabilities are a\nconsiderably more vulnerable attack surface for inversion attacks than\npreviously known.",
        "url": "http://arxiv.org/abs/2506.17090v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17090v1",
        "arxiv_id": "2506.17090v1",
        "authors": [
            "Murtaza Nazir",
            "Matthew Finlayson",
            "John X. Morris",
            "Xiang Ren",
            "Swabha Swayamdipta"
        ],
        "submitted": "2025-06-20 15:53:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation",
        "abstract": "Large Language Models (LLMs) often exhibit \\textit{hallucinations},\ngenerating factually incorrect or semantically irrelevant content in response\nto prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by\nencouraging step-by-step reasoning, but its impact on hallucination detection\nremains underexplored. To bridge this gap, we conduct a systematic empirical\nevaluation. We begin with a pilot experiment, revealing that CoT reasoning\nsignificantly affects the LLM's internal states and token probability\ndistributions. Building on this, we evaluate the impact of various CoT\nprompting methods on mainstream hallucination detection methods across both\ninstruction-tuned and reasoning-oriented LLMs. Specifically, we examine three\nkey dimensions: changes in hallucination score distributions, variations in\ndetection accuracy, and shifts in detection confidence. Our findings show that\nwhile CoT prompting helps reduce hallucination frequency, it also tends to\nobscure critical signals used for detection, impairing the effectiveness of\nvarious detection methods. Our study highlights an overlooked trade-off in the\nuse of reasoning. Code is publicly available at:\nhttps://anonymous.4open.science/r/cot-hallu-detect.",
        "url": "http://arxiv.org/abs/2506.17088v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17088v1",
        "arxiv_id": "2506.17088v1",
        "authors": [
            "Jiahao Cheng",
            "Tiancheng Su",
            "Jia Yuan",
            "Guoxiu He",
            "Jiawei Liu",
            "Xinqi Tao",
            "Jingwen Xie",
            "Huaxia Li"
        ],
        "submitted": "2025-06-20 15:49:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Tower+: Bridging Generality and Translation Specialization in Multilingual LLMs",
        "abstract": "Fine-tuning pretrained LLMs has been shown to be an effective strategy for\nreaching state-of-the-art performance on specific tasks like machine\ntranslation. However, this process of adaptation often implies sacrificing\ngeneral-purpose capabilities, such as conversational reasoning and\ninstruction-following, hampering the utility of the system in real-world\napplications that require a mixture of skills. In this paper, we introduce\nTower+, a suite of models designed to deliver strong performance across both\ntranslation and multilingual general-purpose text capabilities. We achieve a\nPareto frontier between translation specialization and multilingual\ngeneral-purpose capabilities by introducing a novel training recipe that builds\non Tower (Alves et al., 2024), comprising continued pretraining, supervised\nfine-tuning, preference optimization, and reinforcement learning with\nverifiable rewards. At each stage of training, we carefully generate and curate\ndata to strengthen performance on translation as well as general-purpose tasks\ninvolving code generation, mathematics problem solving, and general\ninstruction-following. We develop models at multiple scales: 2B, 9B, and 72B.\nOur smaller models often outperform larger general-purpose open-weight and\nproprietary LLMs (e.g., Llama 3.3 70B, GPT-4o). Our largest model delivers\nbest-in-class translation performance for high-resource languages and top\nresults in multilingual Arena Hard evaluations and in IF-MT, a benchmark we\nintroduce for evaluating both translation and instruction-following. Our\nfindings highlight that it is possible to rival frontier models in general\ncapabilities, while optimizing for specific business domains, such as\ntranslation and localization.",
        "url": "http://arxiv.org/abs/2506.17080v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17080v1",
        "arxiv_id": "2506.17080v1",
        "authors": [
            "Ricardo Rei",
            "Nuno M. Guerreiro",
            "José Pombal",
            "João Alves",
            "Pedro Teixeirinha",
            "Amin Farajian",
            "André F. T. Martins"
        ],
        "submitted": "2025-06-20 15:30:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Simultaneous Translation with Offline Speech and LLM Models in CUNI Submission to IWSLT 2025",
        "abstract": "This paper describes Charles University submission to the Simultaneous Speech\nTranslation Task of the IWSLT 2025. We cover all four language pairs with a\ndirect or cascade approach. The backbone of our systems is the offline Whisper\nspeech model, which we use for both translation and transcription in\nsimultaneous mode with the state-of-the-art simultaneous policy AlignAtt. We\nfurther improve the performance by prompting to inject in-domain terminology,\nand we accommodate context. Our cascaded systems further use EuroLLM for\nunbounded simultaneous translation. Compared to the Organizers' baseline, our\nsystems improve by 2 BLEU points on Czech to English and 13-22 BLEU points on\nEnglish to German, Chinese and Japanese on the development sets. Additionally,\nwe also propose a new enhanced measure of speech recognition latency.",
        "url": "http://arxiv.org/abs/2506.17077v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17077v1",
        "arxiv_id": "2506.17077v1",
        "authors": [
            "Dominik Macháček",
            "Peter Polák"
        ],
        "submitted": "2025-06-20 15:27:44",
        "source": "arxiv",
        "comment": "IWSLT 2025"
    },
    {
        "title": "Universal Music Representations? Evaluating Foundation Models on World Music Corpora",
        "abstract": "Foundation models have revolutionized music information retrieval, but\nquestions remain about their ability to generalize across diverse musical\ntraditions. This paper presents a comprehensive evaluation of five\nstate-of-the-art audio foundation models across six musical corpora spanning\nWestern popular, Greek, Turkish, and Indian classical traditions. We employ\nthree complementary methodologies to investigate these models' cross-cultural\ncapabilities: probing to assess inherent representations, targeted supervised\nfine-tuning of 1-2 layers, and multi-label few-shot learning for low-resource\nscenarios. Our analysis shows varying cross-cultural generalization, with\nlarger models typically outperforming on non-Western music, though results\ndecline for culturally distant traditions. Notably, our approaches achieve\nstate-of-the-art performance on five out of six evaluated datasets,\ndemonstrating the effectiveness of foundation models for world music\nunderstanding. We also find that our targeted fine-tuning approach does not\nconsistently outperform probing across all settings, suggesting foundation\nmodels already encode substantial musical knowledge. Our evaluation framework\nand benchmarking results contribute to understanding how far current models are\nfrom achieving universal music representations while establishing metrics for\nfuture progress.",
        "url": "http://arxiv.org/abs/2506.17055v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17055v1",
        "arxiv_id": "2506.17055v1",
        "authors": [
            "Charilaos Papaioannou",
            "Emmanouil Benetos",
            "Alexandros Potamianos"
        ],
        "submitted": "2025-06-20 15:06:44",
        "source": "arxiv",
        "comment": "Accepted at ISMIR 2025"
    },
    {
        "title": "From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers",
        "abstract": "Transformers have achieved state-of-the-art performance across language and\nvision tasks. This success drives the imperative to interpret their internal\nmechanisms with the dual goals of enhancing performance and improving\nbehavioral control. Attribution methods help advance interpretability by\nassigning model outputs associated with a target concept to specific model\ncomponents. Current attribution research primarily studies multi-layer\nperceptron neurons and addresses relatively simple concepts such as factual\nassociations (e.g., Paris is located in France). This focus tends to overlook\nthe impact of the attention mechanism and lacks a unified approach for\nanalyzing more complex concepts. To fill these gaps, we introduce Scalable\nAttention Module Discovery (SAMD), a concept-agnostic method for mapping\narbitrary, complex concepts to specific attention heads of general transformer\nmodels. We accomplish this by representing each concept as a vector,\ncalculating its cosine similarity with each attention head, and selecting the\nTopK-scoring heads to construct the concept-associated attention module. We\nthen propose Scalar Attention Module Intervention (SAMI), a simple strategy to\ndiminish or amplify the effects of a concept by adjusting the attention module\nusing only a single scalar parameter. Empirically, we demonstrate SAMD on\nconcepts of varying complexity, and visualize the locations of their\ncorresponding modules. Our results demonstrate that module locations remain\nstable before and after LLM post-training, and confirm prior work on the\nmechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on\nHarmBench (+72.7%) by diminishing \"safety\" and improve performance on the GSM8K\nbenchmark (+1.6%) by amplifying \"reasoning\". Lastly, we highlight the\ndomain-agnostic nature of our approach by suppressing the image classification\naccuracy of vision transformers on ImageNet.",
        "url": "http://arxiv.org/abs/2506.17052v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17052v1",
        "arxiv_id": "2506.17052v1",
        "authors": [
            "Jingtong Su",
            "Julia Kempe",
            "Karen Ullrich"
        ],
        "submitted": "2025-06-20 15:04:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models",
        "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated significant\nadvances across numerous vision-language tasks. Due to their strong image-text\nalignment capability, MLLMs can effectively understand image-text pairs with\nclear meanings. However, effectively resolving the inherent ambiguities in\nnatural language and visual contexts remains challenging. Existing multimodal\nbenchmarks typically overlook linguistic and visual ambiguities, relying mainly\non unimodal context for disambiguation and thus failing to exploit the mutual\nclarification potential between modalities. To bridge this gap, we introduce\nMUCAR, a novel and challenging benchmark designed explicitly for evaluating\nmultimodal ambiguity resolution across multilingual and cross-modal scenarios.\nMUCAR includes: (1) a multilingual dataset where ambiguous textual expressions\nare uniquely resolved by corresponding visual contexts, and (2) a\ndual-ambiguity dataset that systematically pairs ambiguous images with\nambiguous textual contexts, with each combination carefully constructed to\nyield a single, clear interpretation through mutual disambiguation. Extensive\nevaluations involving 19 state-of-the-art multimodal models--encompassing both\nopen-source and proprietary architectures--reveal substantial gaps compared to\nhuman-level performance, highlighting the need for future research into more\nsophisticated cross-modal ambiguity comprehension methods, further pushing the\nboundaries of multimodal reasoning.",
        "url": "http://arxiv.org/abs/2506.17046v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17046v1",
        "arxiv_id": "2506.17046v1",
        "authors": [
            "Xiaolong Wang",
            "Zhaolu Kang",
            "Wangyuxuan Zhai",
            "Xinyue Lou",
            "Yunghwei Lai",
            "Ziyue Wang",
            "Yawen Wang",
            "Kaiyu Huang",
            "Yile Wang",
            "Peng Li",
            "Yang Liu"
        ],
        "submitted": "2025-06-20 14:57:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Instituto de Telecomunicações at IWSLT 2025: Aligning Small-Scale Speech and Language Models for Speech-to-Text Learning",
        "abstract": "This paper presents the IT-IST submission to the IWSLT 2025 Shared Task on\nInstruction Following Speech Processing. We submit results for the Short Track,\ni.e., speech recognition, translation, and spoken question answering. Our model\nis a unified speech-to-text model that integrates a pre-trained continuous\nspeech encoder and text decoder through a first phase of modality alignment and\na second phase of instruction fine-tuning. Crucially, we focus on using\nsmall-scale language model backbones (< 2B) and restrict to high-quality, CC-BY\ndata along with synthetic data generation to supplement existing resources.",
        "url": "http://arxiv.org/abs/2506.17019v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17019v1",
        "arxiv_id": "2506.17019v1",
        "authors": [
            "Giuseppe Attanasio",
            "Sonal Sannigrahi",
            "Ben Peters",
            "André F. T. Martins"
        ],
        "submitted": "2025-06-20 14:17:42",
        "source": "arxiv",
        "comment": "7 pages, 1 figure, IWSLT 2025"
    },
    {
        "title": "LLM-Generated Feedback Supports Learning If Learners Choose to Use It",
        "abstract": "Large language models (LLMs) are increasingly used to generate feedback, yet\ntheir impact on learning remains underexplored, especially compared to existing\nfeedback methods. This study investigates how on-demand LLM-generated\nexplanatory feedback influences learning in seven scenario-based tutor training\nlessons. Analyzing over 2,600 lesson completions from 885 tutor learners, we\ncompare posttest performance among learners across three groups: learners who\nreceived feedback generated by gpt-3.5-turbo, those who declined it, and those\nwithout access. All groups received non-LLM corrective feedback. To address\npotential selection bias-where higher-performing learners may be more inclined\nto use LLM feedback-we applied propensity scoring. Learners with a higher\npredicted likelihood of engaging with LLM feedback scored significantly higher\nat posttest than those with lower propensity. After adjusting for this effect,\ntwo out of seven lessons showed statistically significant learning benefits\nfrom LLM feedback with standardized effect sizes of 0.28 and 0.33. These\nmoderate effects suggest that the effectiveness of LLM feedback depends on the\nlearners' tendency to seek support. Importantly, LLM feedback did not\nsignificantly increase completion time, and learners overwhelmingly rated it as\nhelpful. These findings highlight LLM feedback's potential as a low-cost and\nscalable way to improve learning on open-ended tasks, particularly in existing\nsystems already providing feedback without LLMs. This work contributes open\ndatasets, LLM prompts, and rubrics to support reproducibility.",
        "url": "http://arxiv.org/abs/2506.17006v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17006v1",
        "arxiv_id": "2506.17006v1",
        "authors": [
            "Danielle R. Thomas",
            "Conrad Borchers",
            "Shambhavi Bhushan",
            "Erin Gatz",
            "Shivang Gupta",
            "Kenneth R. Koedinger"
        ],
        "submitted": "2025-06-20 13:59:14",
        "source": "arxiv",
        "comment": "Full research paper accepted at EC-TEL '25"
    },
    {
        "title": "PersonalAI: Towards digital twins in the graph form",
        "abstract": "The challenge of personalizing language models, specifically the ability to\naccount for a user's history during interactions, is of significant interest.\nDespite recent advancements in large language models (LLMs) and Retrieval\nAugmented Generation that have enhanced the factual base of LLMs, the task of\nretaining extensive personal information and using it to generate personalized\nresponses remains pertinent. To address this, we propose utilizing external\nmemory in the form of knowledge graphs, which are constructed and updated by\nthe LLM itself. We have expanded upon ideas of AriGraph architecture and for\nthe first time introduced a combined graph featuring both standard edges and\ntwo types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and\nDiaASQ benchmarks indicates that this approach aids in making the process of\ngraph construction and knowledge extraction unified and robust. Furthermore, we\naugmented the DiaASQ benchmark by incorporating parameters such as time into\ndialogues and introducing contradictory statements made by the same speaker at\ndifferent times. Despite these modifications, the performance of the\nquestion-answering system remained robust, demonstrating the proposed\narchitecture's ability to maintain and utilize temporal dependencies.",
        "url": "http://arxiv.org/abs/2506.17001v1",
        "pdf_url": "http://arxiv.org/pdf/2506.17001v1",
        "arxiv_id": "2506.17001v1",
        "authors": [
            "Mikhail Menschikov",
            "Dmitry Evseev",
            "Ruslan Kostoev",
            "Ilya Perepechkin",
            "Ilnaz Salimov",
            "Victoria Dochkina",
            "Petr Anokhin",
            "Evgeny Burnaev",
            "Nikita Semenov"
        ],
        "submitted": "2025-06-20 13:52:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs",
        "abstract": "LaTeX's precision and flexibility in typesetting have made it the gold\nstandard for the preparation of scientific documentation. Large Language Models\n(LLMs) present a promising opportunity for researchers to produce\npublication-ready material using LaTeX with natural language instructions, yet\ncurrent benchmarks completely lack evaluation of this ability. By introducing\nTeXpert, our benchmark dataset with natural language prompts for generating\nLaTeX code focused on components of scientific documents across multiple\ndifficulty levels, we conduct an in-depth analysis of LLM performance in this\nregard and identify frequent error types. Our evaluation across open and\nclosed-source LLMs highlights multiple key findings: LLMs excelling on standard\nbenchmarks perform poorly in LaTeX generation with a significant accuracy\ndrop-off as the complexity of tasks increases; open-source models like DeepSeek\nv3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks;\nand formatting and package errors are unexpectedly prevalent, suggesting a lack\nof diverse LaTeX examples in the training datasets of most LLMs. Our dataset,\ncode, and model evaluations are available at\nhttps://github.com/knowledge-verse-ai/TeXpert.",
        "url": "http://arxiv.org/abs/2506.16990v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16990v1",
        "arxiv_id": "2506.16990v1",
        "authors": [
            "Sahil Kale",
            "Vijaykant Nadadur"
        ],
        "submitted": "2025-06-20 13:39:16",
        "source": "arxiv",
        "comment": "Accepted to the SDProc Workshop @ ACL 2025"
    },
    {
        "title": "RAGentA: Multi-Agent Retrieval-Augmented Generation for Attributed Question Answering",
        "abstract": "We present RAGentA, a multi-agent retrieval-augmented generation (RAG)\nframework for attributed question answering (QA). With the goal of trustworthy\nanswer generation, RAGentA focuses on optimizing answer correctness, defined by\ncoverage and relevance to the question and faithfulness, which measures the\nextent to which answers are grounded in retrieved documents. RAGentA uses a\nmulti-agent architecture that iteratively filters retrieved documents,\ngenerates attributed answers with in-line citations, and verifies completeness\nthrough dynamic refinement. Central to the framework is a hybrid retrieval\nstrategy that combines sparse and dense methods, improving Recall@20 by 12.5%\ncompared to the best single retrieval model, resulting in more correct and\nwell-supported answers. Evaluated on a synthetic QA dataset derived from the\nFineWeb index, RAGentA outperforms standard RAG baselines, achieving gains of\n1.09% in correctness and 10.72% in faithfulness. These results demonstrate the\neffectiveness of the multi-agent architecture and hybrid retrieval in advancing\ntrustworthy QA.",
        "url": "http://arxiv.org/abs/2506.16988v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16988v1",
        "arxiv_id": "2506.16988v1",
        "authors": [
            "Ines Besrour",
            "Jingbo He",
            "Tobias Schreieder",
            "Michael Färber"
        ],
        "submitted": "2025-06-20 13:37:03",
        "source": "arxiv",
        "comment": "Accepted at SIGIR 2025"
    },
    {
        "title": "Language Bottleneck Models: A Framework for Interpretable Knowledge Tracing and Beyond",
        "abstract": "Accurately assessing student knowledge is critical for effective education,\nyet traditional Knowledge Tracing (KT) methods rely on opaque latent\nembeddings, limiting interpretability. Even LLM-based approaches generate\ndirect predictions or summaries that may hallucinate without any accuracy\nguarantees. We recast KT as an inverse problem: learning the minimum\nnatural-language summary that makes past answers explainable and future answers\npredictable. Our Language Bottleneck Model (LBM) consists of an encoder LLM\nthat writes an interpretable knowledge summary and a frozen decoder LLM that\nmust reconstruct and predict student responses using only that summary text. By\nconstraining all predictive information to pass through a short\nnatural-language bottleneck, LBMs ensure that the summary contains accurate\ninformation while remaining human-interpretable. Experiments on synthetic\narithmetic benchmarks and the large-scale Eedi dataset show that LBMs rival the\naccuracy of state-of-the-art KT and direct LLM methods while requiring\norders-of-magnitude fewer student trajectories. We demonstrate that training\nthe encoder with group-relative policy optimization, using downstream decoding\naccuracy as a reward signal, effectively improves summary quality.",
        "url": "http://arxiv.org/abs/2506.16982v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16982v1",
        "arxiv_id": "2506.16982v1",
        "authors": [
            "Antonin Berthon",
            "Mihaela van der Schaar"
        ],
        "submitted": "2025-06-20 13:21:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Latent Concept Disentanglement in Transformer-based Language Models",
        "abstract": "When large language models (LLMs) use in-context learning (ICL) to solve a\nnew task, they seem to grasp not only the goal of the task but also core,\nlatent concepts in the demonstration examples. This begs the question of\nwhether transformers represent latent structures as part of their computation\nor whether they take shortcuts to solve the problem. Prior mechanistic work on\nICL does not address this question because it does not sufficiently examine the\nrelationship between the learned representation and the latent concept, and the\nconsidered problem settings often involve only single-step reasoning. In this\nwork, we examine how transformers disentangle and use latent concepts. We show\nthat in 2-hop reasoning tasks with a latent, discrete concept, the model\nsuccessfully identifies the latent concept and does step-by-step concept\ncomposition. In tasks parameterized by a continuous latent concept, we find\nlow-dimensional subspaces in the representation space where the geometry mimics\nthe underlying parameterization. Together, these results refine our\nunderstanding of ICL and the representation of transformers, and they provide\nevidence for highly localized structures in the model that disentangle latent\nconcepts in ICL tasks.",
        "url": "http://arxiv.org/abs/2506.16975v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16975v1",
        "arxiv_id": "2506.16975v1",
        "authors": [
            "Guan Zhe Hong",
            "Bhavya Vasudeva",
            "Vatsal Sharan",
            "Cyrus Rashtchian",
            "Prabhakar Raghavan",
            "Rina Panigrahy"
        ],
        "submitted": "2025-06-20 13:08:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs",
        "abstract": "Multimodal large language models (MLLMs) have begun to demonstrate robust\nreasoning capabilities on general tasks, yet their application in the medical\ndomain remains in its early stages. Constructing chain-of-thought (CoT)\ntraining data is essential for bolstering the reasoning abilities of medical\nMLLMs. However, existing approaches exhibit a deficiency in offering a\ncomprehensive framework for searching and evaluating effective reasoning paths\ntowards critical diagnosis. To address this challenge, we propose Mentor-Intern\nCollaborative Search (MICS), a novel reasoning-path searching scheme to\ngenerate rigorous and effective medical CoT data. MICS first leverages mentor\nmodels to initialize the reasoning, one step at a time, then prompts each\nintern model to continue the thinking along those initiated paths, and finally\nselects the optimal reasoning path according to the overall reasoning\nperformance of multiple intern models. The reasoning performance is determined\nby an MICS-Score, which assesses the quality of generated reasoning paths.\nEventually, we construct MMRP, a multi-task medical reasoning dataset with\nranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum\nlearning strategy, with robust visual question-answering and generalizable\nreasoning capabilities. Extensive experiments demonstrate that Chiron-o1,\ntrained on our CoT dataset constructed using MICS, achieves state-of-the-art\nperformance across a list of medical visual question answering and reasoning\nbenchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing\nStep-by-Step and Verifiable Medical Reasoning in MLLMs",
        "url": "http://arxiv.org/abs/2506.16962v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16962v1",
        "arxiv_id": "2506.16962v1",
        "authors": [
            "Haoran Sun",
            "Yankai Jiang",
            "Wenjie Lou",
            "Yujie Zhang",
            "Wenjie Li",
            "Lilong Wang",
            "Mianxin Liu",
            "Lei Liu",
            "Xiaosong Wang"
        ],
        "submitted": "2025-06-20 12:51:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Pyramid Mixer: Multi-dimensional Multi-period Interest Modeling for Sequential Recommendation",
        "abstract": "Sequential recommendation, a critical task in recommendation systems,\npredicts the next user action based on the understanding of the user's\nhistorical behaviors. Conventional studies mainly focus on cross-behavior\nmodeling with self-attention based methods while neglecting comprehensive user\ninterest modeling for more dimensions. In this study, we propose a novel\nsequential recommendation model, Pyramid Mixer, which leverages the MLP-Mixer\narchitecture to achieve efficient and complete modeling of user interests. Our\nmethod learns comprehensive user interests via cross-behavior and cross-feature\nuser sequence modeling. The mixer layers are stacked in a pyramid way for\ncross-period user temporal interest learning. Through extensive offline and\nonline experiments, we demonstrate the effectiveness and efficiency of our\nmethod, and we obtain a +0.106% improvement in user stay duration and a\n+0.0113% increase in user active days in the online A/B test. The Pyramid Mixer\nhas been successfully deployed on the industrial platform, demonstrating its\nscalability and impact in real-world applications.",
        "url": "http://arxiv.org/abs/2506.16942v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16942v1",
        "arxiv_id": "2506.16942v1",
        "authors": [
            "Zhen Gong",
            "Zhifang Fan",
            "Hui Lu",
            "Qiwei Chen",
            "Chenbin Zhang",
            "Lin Guan",
            "Yuchao Zheng",
            "Feng Zhang",
            "Xiao Yang",
            "Zuotao Liu"
        ],
        "submitted": "2025-06-20 12:11:38",
        "source": "arxiv",
        "comment": "Accepted by SIGIR'25"
    },
    {
        "title": "From Data to Knowledge: Evaluating How Efficiently Language Models Learn Facts",
        "abstract": "Sample efficiency is a crucial property of language models with practical\nimplications for training efficiency. In real-world text, information follows a\nlong-tailed distribution. Yet, we expect models to learn and recall frequent\nand infrequent facts. Sample-efficient models are better equipped to handle\nthis challenge of learning and retaining rare information without requiring\nexcessive exposure. This study analyzes multiple models of varying\narchitectures and sizes, all trained on the same pre-training data. By\nannotating relational facts with their frequencies in the training corpus, we\nexamine how model performance varies with fact frequency. Our findings show\nthat most models perform similarly on high-frequency facts but differ notably\non low-frequency facts. This analysis provides new insights into the\nrelationship between model architecture, size, and factual learning efficiency.",
        "url": "http://arxiv.org/abs/2506.16912v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16912v1",
        "arxiv_id": "2506.16912v1",
        "authors": [
            "Daniel Christoph",
            "Max Ploner",
            "Patrick Haller",
            "Alan Akbik"
        ],
        "submitted": "2025-06-20 11:10:24",
        "source": "arxiv",
        "comment": "Accepted to the First Workshop on Large Language Model Memorization\n  (L2M2), co-located with ACL 2025 in Vienna"
    },
    {
        "title": "Multi-Objective Recommendation in the Era of Generative AI: A Survey of Recent Progress and Future Prospects",
        "abstract": "With the recent progress in generative artificial intelligence (Generative\nAI), particularly in the development of large language models, recommendation\nsystems are evolving to become more versatile. Unlike traditional techniques,\ngenerative AI not only learns patterns and representations from complex data\nbut also enables content generation, data synthesis, and personalized\nexperiences. This generative capability plays a crucial role in the field of\nrecommendation systems, helping to address the issue of data sparsity and\nimproving the overall performance of recommendation systems. Numerous studies\non generative AI have already emerged in the field of recommendation systems.\nMeanwhile, the current requirements for recommendation systems have surpassed\nthe single utility of accuracy, leading to a proliferation of multi-objective\nresearch that considers various goals in recommendation systems. However, to\nthe best of our knowledge, there remains a lack of comprehensive studies on\nmulti-objective recommendation systems based on generative AI technologies,\nleaving a significant gap in the literature. Therefore, we investigate the\nexisting research on multi-objective recommendation systems involving\ngenerative AI to bridge this gap. We compile current research on\nmulti-objective recommendation systems based on generative techniques,\ncategorizing them by objectives. Additionally, we summarize relevant evaluation\nmetrics and commonly used datasets, concluding with an analysis of the\nchallenges and future directions in this domain.",
        "url": "http://arxiv.org/abs/2506.16893v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16893v1",
        "arxiv_id": "2506.16893v1",
        "authors": [
            "Zihan Hong",
            "Yushi Wu",
            "Zhiting Zhao",
            "Shanshan Feng",
            "Jianghong Ma",
            "Jiao Liu",
            "Tianjun Wei"
        ],
        "submitted": "2025-06-20 10:30:39",
        "source": "arxiv",
        "comment": "21 pages"
    },
    {
        "title": "MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning",
        "abstract": "Despite efforts to align large language models (LLMs) with societal and moral\nvalues, these models remain susceptible to jailbreak attacks--methods designed\nto elicit harmful responses. Jailbreaking black-box LLMs is considered\nchallenging due to the discrete nature of token inputs, restricted access to\nthe target LLM, and limited query budget. To address the issues above, we\npropose an effective method for jailbreaking black-box large language Models\nvia Iterative Semantic Tuning, named MIST. MIST enables attackers to\niteratively refine prompts that preserve the original semantic intent while\ninducing harmful content. Specifically, to balance semantic similarity with\ncomputational efficiency, MIST incorporates two key strategies: sequential\nsynonym search, and its advanced version--order-determining optimization.\nExtensive experiments across two open-source models and four closed-source\nmodels demonstrate that MIST achieves competitive attack success rates and\nattack transferability compared with other state-of-the-art white-box and\nblack-box jailbreak methods. Additionally, we conduct experiments on\ncomputational efficiency to validate the practical viability of MIST.",
        "url": "http://arxiv.org/abs/2506.16792v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16792v1",
        "arxiv_id": "2506.16792v1",
        "authors": [
            "Muyang Zheng",
            "Yuanzhi Yao",
            "Changting Lin",
            "Rui Wang",
            "Meng Han"
        ],
        "submitted": "2025-06-20 07:16:47",
        "source": "arxiv",
        "comment": "12 pages, 3 figures"
    },
    {
        "title": "DistillNote: LLM-based clinical note summaries improve heart failure diagnosis",
        "abstract": "Large language models (LLMs) offer unprecedented opportunities to generate\nconcise summaries of patient information and alleviate the burden of clinical\ndocumentation that overwhelms healthcare providers. We present Distillnote, a\nframework for LLM-based clinical note summarization, and generate over 64,000\nadmission note summaries through three techniques: (1) One-step, direct\nsummarization, and a divide-and-conquer approach involving (2) Structured\nsummarization focused on independent clinical insights, and (3) Distilled\nsummarization that further condenses the Structured summaries. We test how\nuseful are the summaries by using them to predict heart failure compared to a\nmodel trained on the original notes. Distilled summaries achieve 79% text\ncompression and up to 18.2% improvement in AUPRC compared to an LLM trained on\nthe full notes. We also evaluate the quality of the generated summaries in an\nLLM-as-judge evaluation as well as through blinded pairwise comparisons with\nclinicians. Evaluations indicate that one-step summaries are favoured by\nclinicians according to relevance and clinical actionability, while distilled\nsummaries offer optimal efficiency (avg. 6.9x compression-to-performance ratio)\nand significantly reduce hallucinations. We release our summaries on PhysioNet\nto encourage future research.",
        "url": "http://arxiv.org/abs/2506.16777v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16777v1",
        "arxiv_id": "2506.16777v1",
        "authors": [
            "Heloisa Oss Boll",
            "Antonio Oss Boll",
            "Leticia Puttlitz Boll",
            "Ameen Abu Hanna",
            "Iacer Calixto"
        ],
        "submitted": "2025-06-20 06:45:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "eSapiens: A Real-World NLP Framework for Multimodal Document Understanding and Enterprise Knowledge Processing",
        "abstract": "We introduce eSapiens, a unified question-answering system designed for\nenterprise settings, which bridges structured databases and unstructured\ntextual corpora via a dual-module architecture. The system combines a\nText-to-SQL planner with a hybrid Retrieval-Augmented Generation (RAG)\npipeline, enabling natural language access to both relational data and\nfree-form documents. To enhance answer faithfulness, the RAG module integrates\ndense and sparse retrieval, commercial reranking, and a citation verification\nloop that ensures grounding consistency. We evaluate eSapiens on the RAGTruth\nbenchmark across five leading large language models (LLMs), analyzing\nperformance across key dimensions such as completeness, hallucination, and\ncontext utilization. Results demonstrate that eSapiens outperforms a FAISS\nbaseline in contextual relevance and generation quality, with optional\nstrict-grounding controls for high-stakes scenarios. This work provides a\ndeployable framework for robust, citation-aware question answering in\nreal-world enterprise applications.",
        "url": "http://arxiv.org/abs/2506.16768v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16768v1",
        "arxiv_id": "2506.16768v1",
        "authors": [
            "Isaac Shi",
            "Zeyuan Li",
            "Wenli Wang",
            "Lewei He",
            "Yang Yang",
            "Tianyu Shi"
        ],
        "submitted": "2025-06-20 06:07:20",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models",
        "abstract": "Large Vision-Language Models (LVLMs) demonstrate exceptional performance\nacross multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass\nbuilt-in safety mechanisms to elicit restricted content generation. Existing\nblack-box jailbreak methods primarily rely on adversarial textual prompts or\nimage perturbations, yet these approaches are highly detectable by standard\ncontent filtering systems and exhibit low query and computational efficiency.\nIn this work, we present Cross-modal Adversarial Multimodal Obfuscation (CAMO),\na novel black-box jailbreak attack framework that decomposes malicious prompts\ninto semantically benign visual and textual fragments. By leveraging LVLMs'\ncross-modal reasoning abilities, CAMO covertly reconstructs harmful\ninstructions through multi-step reasoning, evading conventional detection\nmechanisms. Our approach supports adjustable reasoning complexity and requires\nsignificantly fewer queries than prior attacks, enabling both stealth and\nefficiency. Comprehensive evaluations conducted on leading LVLMs validate\nCAMO's effectiveness, showcasing robust performance and strong cross-model\ntransferability. These results underscore significant vulnerabilities in\ncurrent built-in safety mechanisms, emphasizing an urgent need for advanced,\nalignment-aware security and safety solutions in vision-language systems.",
        "url": "http://arxiv.org/abs/2506.16760v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16760v1",
        "arxiv_id": "2506.16760v1",
        "authors": [
            "Lei Jiang",
            "Zixun Zhang",
            "Zizhou Wang",
            "Xiaobing Sun",
            "Zhen Li",
            "Liangli Zhen",
            "Xiaohua Xu"
        ],
        "submitted": "2025-06-20 05:30:25",
        "source": "arxiv",
        "comment": "15 pages, 9 figures"
    },
    {
        "title": "SocialSim: Towards Socialized Simulation of Emotional Support Conversation",
        "abstract": "Emotional support conversation (ESC) helps reduce people's psychological\nstress and provide emotional value through interactive dialogues. Due to the\nhigh cost of crowdsourcing a large ESC corpus, recent attempts use large\nlanguage models for dialogue augmentation. However, existing approaches largely\noverlook the social dynamics inherent in ESC, leading to less effective\nsimulations. In this paper, we introduce SocialSim, a novel framework that\nsimulates ESC by integrating key aspects of social interactions: social\ndisclosure and social awareness. On the seeker side, we facilitate social\ndisclosure by constructing a comprehensive persona bank that captures diverse\nand authentic help-seeking scenarios. On the supporter side, we enhance social\nawareness by eliciting cognitive reasoning to generate logical and supportive\nresponses. Building upon SocialSim, we construct SSConv, a large-scale\nsynthetic ESC corpus of which quality can even surpass crowdsourced ESC data.\nWe further train a chatbot on SSConv and demonstrate its state-of-the-art\nperformance in both automatic and human evaluations. We believe SocialSim\noffers a scalable way to synthesize ESC, making emotional care more accessible\nand practical.",
        "url": "http://arxiv.org/abs/2506.16756v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16756v1",
        "arxiv_id": "2506.16756v1",
        "authors": [
            "Zhuang Chen",
            "Yaru Cao",
            "Guanqun Bi",
            "Jincenzi Wu",
            "Jinfeng Zhou",
            "Xiyao Xiao",
            "Si Chen",
            "Hongning Wang",
            "Minlie Huang"
        ],
        "submitted": "2025-06-20 05:24:40",
        "source": "arxiv",
        "comment": "AAAI 2025 Paper #32116 (Without Publication Edits)"
    },
    {
        "title": "Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly",
        "abstract": "Drawing real world social inferences usually requires taking into account\ninformation from multiple modalities. Language is a particularly powerful\nsource of information in social settings, especially in novel situations where\nlanguage can provide both abstract information about the environment dynamics\nand concrete specifics about an agent that cannot be easily visually observed.\nIn this paper, we propose Language-Informed Rational Agent Synthesis (LIRAS), a\nframework for drawing context-specific social inferences that integrate\nlinguistic and visual inputs. LIRAS frames multimodal social reasoning as a\nprocess of constructing structured but situation-specific agent and environment\nrepresentations - leveraging multimodal language models to parse language and\nvisual inputs into unified symbolic representations, over which a Bayesian\ninverse planning engine can be run to produce granular probabilistic judgments.\nOn a range of existing and new social reasoning tasks derived from cognitive\nscience experiments, we find that our model (instantiated with a comparatively\nlightweight VLM) outperforms ablations and state-of-the-art models in capturing\nhuman judgments across all domains.",
        "url": "http://arxiv.org/abs/2506.16755v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16755v1",
        "arxiv_id": "2506.16755v1",
        "authors": [
            "Lance Ying",
            "Ryan Truong",
            "Katherine M. Collins",
            "Cedegao E. Zhang",
            "Megan Wei",
            "Tyler Brooke-Wilson",
            "Tan Zhi-Xuan",
            "Lionel Wong",
            "Joshua B. Tenenbaum"
        ],
        "submitted": "2025-06-20 05:21:42",
        "source": "arxiv",
        "comment": "5 figures, 19 pages"
    },
    {
        "title": "LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization",
        "abstract": "With the rapid progress of speech language models (SLMs), discrete speech\ntokens have emerged as a core interface between speech and text, enabling\nunified modeling across modalities. Recent speech tokenization approaches aim\nto isolate semantic information from low-level acoustics to better align with\nlanguage models. In particular, previous methods use SSL teachers such as\nHuBERT to extract semantic representations, which are then distilled into a\nsemantic quantizer to suppress acoustic redundancy as well as capture\ncontent-related latent structures. However, they still produce speech token\nsequences significantly longer than their textual counterparts, creating\nchallenges for efficient speech-language modeling. Reducing the frame rate is a\nnatural solution, but standard techniques, such as rigid average pooling across\nframes, can distort or dilute the semantic structure required for effective LM\nalignment. To address this, we propose LM-SPT, a speech tokenization method\nthat introduces a novel semantic distillation. Instead of directly matching\nteacher and student features via pooling, we reconstruct speech solely from\nsemantic tokens and minimize the discrepancy between the encoded\nrepresentations of the original and reconstructed waveforms, obtained from a\nfrozen automatic speech recognition (ASR) encoder. This indirect yet\ndata-driven supervision enables the tokenizer to learn discrete units that are\nmore semantically aligned with language models. LM-SPT further incorporates\narchitectural improvements to the encoder and decoder for speech tokenization,\nand supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz.\nExperimental results show that LM-SPT achieves superior reconstruction fidelity\ncompared to baselines, and that SLMs trained with LM-SPT tokens achieve\ncompetitive performances on speech-to-text and consistently outperform\nbaselines on text-to-speech tasks.",
        "url": "http://arxiv.org/abs/2506.16738v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16738v1",
        "arxiv_id": "2506.16738v1",
        "authors": [
            "Daejin Jo",
            "Jeeyoung Yun",
            "Byungseok Roh",
            "Sungwoong Kim"
        ],
        "submitted": "2025-06-20 04:15:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Role of Model Confidence on Bias Effects in Measured Uncertainties",
        "abstract": "With the growing adoption of Large Language Models (LLMs) for open-ended\ntasks, accurately assessing epistemic uncertainty, which reflects a model's\nlack of knowledge, has become crucial to ensuring reliable outcomes. However,\nquantifying epistemic uncertainty in such tasks is challenging due to the\npresence of aleatoric uncertainty, which arises from multiple valid answers.\nWhile bias can introduce noise into epistemic uncertainty estimation, it may\nalso reduce noise from aleatoric uncertainty. To investigate this trade-off, we\nconduct experiments on Visual Question Answering (VQA) tasks and find that\nmitigating prompt-introduced bias improves uncertainty quantification in\nGPT-4o. Building on prior work showing that LLMs tend to copy input information\nwhen model confidence is low, we further analyze how these prompt biases affect\nmeasured epistemic and aleatoric uncertainty across varying bias-free\nconfidence levels with GPT-4o and Qwen2-VL. We find that all considered biases\ninduce greater changes in both uncertainties when bias-free model confidence is\nlower. Moreover, lower bias-free model confidence leads to greater\nunderestimation of epistemic uncertainty (i.e. overconfidence) due to bias,\nwhereas it has no significant effect on the direction of changes in aleatoric\nuncertainty estimation. These distinct effects deepen our understanding of bias\nmitigation for uncertainty quantification and potentially inform the\ndevelopment of more advanced techniques.",
        "url": "http://arxiv.org/abs/2506.16724v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16724v1",
        "arxiv_id": "2506.16724v1",
        "authors": [
            "Xinyi Liu",
            "Weiguang Wang",
            "Hangfeng He"
        ],
        "submitted": "2025-06-20 03:43:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models",
        "abstract": "Generative Reward Models (GRMs) provide greater flexibility than scalar\nreward models in capturing human preferences, but their effectiveness is\nlimited by poor reasoning capabilities. This often results in incomplete or\noverly speculative reasoning paths, leading to hallucinations or missing key\ninformation in complex tasks. We address this challenge with ReasonGRM, a\nthree-stage generative reward modeling framework. In the first stage, Zero-RL\nis used to generate concise, outcome-directed reasoning paths that reduce the\nlikelihood of critical omissions. In the second stage, we introduce a novel\nevaluation metric, $R^\\star$, which scores reasoning paths based on their\ngeneration likelihood. This favors paths that reach correct answers with\nminimal exploration, helping to reduce hallucination-prone data during\ntraining. In the final stage, the model is further refined through\nreinforcement learning on challenging examples to enhance its preference\ndiscrimination capabilities. Experiments on three public benchmarks show that\nReasonGRM achieves competitive or state-of-the-art performance, outperforming\nprevious best GRMs by 1.8\\% on average and surpassing proprietary models such\nas GPT-4o by up to 5.6\\%. These results demonstrate the effectiveness of\nreasoning-aware training and highlight the importance of high-quality rationale\nselection for reliable preference modeling.",
        "url": "http://arxiv.org/abs/2506.16712v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16712v1",
        "arxiv_id": "2506.16712v1",
        "authors": [
            "Bin Chen",
            "Xinzge Gao",
            "Chuanrui Hu",
            "Penghang Yu",
            "Hua Zhang",
            "Bing-Kun Bao"
        ],
        "submitted": "2025-06-20 03:10:52",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Large Language Models as Psychological Simulators: A Methodological Guide",
        "abstract": "Large language models (LLMs) offer emerging opportunities for psychological\nand behavioral research, but methodological guidance is lacking. This article\nprovides a framework for using LLMs as psychological simulators across two\nprimary applications: simulating roles and personas to explore diverse\ncontexts, and serving as computational models to investigate cognitive\nprocesses. For simulation, we present methods for developing psychologically\ngrounded personas that move beyond demographic categories, with strategies for\nvalidation against human data and use cases ranging from studying inaccessible\npopulations to prototyping research instruments. For cognitive modeling, we\nsynthesize emerging approaches for probing internal representations,\nmethodological advances in causal interventions, and strategies for relating\nmodel behavior to human cognition. We address overarching challenges including\nprompt sensitivity, temporal limitations from training data cutoffs, and\nethical considerations that extend beyond traditional human subjects review.\nThroughout, we emphasize the need for transparency about model capabilities and\nconstraints. Together, this framework integrates emerging empirical evidence\nabout LLM performance--including systematic biases, cultural limitations, and\nprompt brittleness--to help researchers wrangle these challenges and leverage\nthe unique capabilities of LLMs in psychological research.",
        "url": "http://arxiv.org/abs/2506.16702v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16702v1",
        "arxiv_id": "2506.16702v1",
        "authors": [
            "Zhicheng Lin"
        ],
        "submitted": "2025-06-20 02:45:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology",
        "abstract": "Large language models (LLMs) are rapidly being adopted across psychology,\nserving as research tools, experimental subjects, human simulators, and\ncomputational models of cognition. However, the application of human\nmeasurement tools to these systems can produce contradictory results, raising\nconcerns that many findings are measurement phantoms--statistical artifacts\nrather than genuine psychological phenomena. In this Perspective, we argue that\nbuilding a robust science of AI psychology requires integrating two of our\nfield's foundational pillars: the principles of reliable measurement and the\nstandards for sound causal inference. We present a dual-validity framework to\nguide this integration, which clarifies how the evidence needed to support a\nclaim scales with its scientific ambition. Using an LLM to classify text may\nrequire only basic accuracy checks, whereas claiming it can simulate anxiety\ndemands a far more rigorous validation process. Current practice systematically\nfails to meet these requirements, often treating statistical pattern matching\nas evidence of psychological phenomena. The same model output--endorsing \"I am\nanxious\"--requires different validation strategies depending on whether\nresearchers claim to measure, characterize, simulate, or model psychological\nconstructs. Moving forward requires developing computational analogues of\npsychological constructs and establishing clear, scalable standards of evidence\nrather than the uncritical application of human measurement tools.",
        "url": "http://arxiv.org/abs/2506.16697v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16697v1",
        "arxiv_id": "2506.16697v1",
        "authors": [
            "Zhicheng Lin"
        ],
        "submitted": "2025-06-20 02:38:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LegiGPT: Party Politics and Transport Policy with Large Language Model",
        "abstract": "Given the significant influence of lawmakers' political ideologies on\nlegislative decision-making, understanding their impact on policymaking is\ncritically important. We introduce a novel framework, LegiGPT, which integrates\na large language model (LLM) with explainable artificial intelligence (XAI) to\nanalyze transportation-related legislative proposals. LegiGPT employs a\nmulti-stage filtering and classification pipeline using zero-shot prompting\nwith GPT-4. Using legislative data from South Korea's 21st National Assembly,\nwe identify key factors - including sponsor characteristics, political\naffiliations, and geographic variables - that significantly influence\ntransportation policymaking. The LLM was used to classify\ntransportation-related bill proposals through a stepwise filtering process\nbased on keywords, phrases, and contextual relevance. XAI techniques were then\napplied to examine relationships between party affiliation and associated\nattributes. The results reveal that the number and proportion of conservative\nand progressive sponsors, along with district size and electoral population,\nare critical determinants shaping legislative outcomes. These findings suggest\nthat both parties contributed to bipartisan legislation through different forms\nof engagement, such as initiating or supporting proposals. This integrated\napproach provides a valuable tool for understanding legislative dynamics and\nguiding future policy development, with broader implications for infrastructure\nplanning and governance.",
        "url": "http://arxiv.org/abs/2506.16692v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16692v1",
        "arxiv_id": "2506.16692v1",
        "authors": [
            "Hyunsoo Yun",
            "Eun Hak Lee"
        ],
        "submitted": "2025-06-20 02:25:52",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Simple Contrastive Framework Of Item Tokenization For Generative Recommendation",
        "abstract": "Generative retrieval-based recommendation has emerged as a promising paradigm\naiming at directly generating the identifiers of the target candidates.\nHowever, in large-scale recommendation systems, this approach becomes\nincreasingly cumbersome due to the redundancy and sheer scale of the token\nspace. To overcome these limitations, recent research has explored the use of\nsemantic tokens as an alternative to ID tokens, which typically leveraged\nreconstruction-based strategies, like RQ-VAE, to quantize content embeddings\nand significantly reduce the embedding size. However, reconstructive\nquantization aims for the precise reconstruction of each item embedding\nindependently, which conflicts with the goal of generative retrieval tasks\nfocusing more on differentiating among items. Moreover, multi-modal side\ninformation of items, such as descriptive text and images, geographical\nknowledge in location-based recommendation services, has been shown to be\neffective in improving recommendations by providing richer contexts for\ninteractions. Nevertheless, effectively integrating such complementary\nknowledge into existing generative recommendation frameworks remains\nchallenging. To overcome these challenges, we propose a novel unsupervised deep\nquantization exclusively based on contrastive learning, named SimCIT (a Simple\nContrastive Item Tokenization framework). Specifically, different from existing\nreconstruction-based strategies, SimCIT propose to use a learnable residual\nquantization module to align with the signals from different modalities of the\nitems, which combines multi-modal knowledge alignment and semantic tokenization\nin a mutually beneficial contrastive learning framework. Extensive experiments\nacross public datasets and a large-scale industrial dataset from various\ndomains demonstrate SimCIT's effectiveness in LLM-based generative\nrecommendation.",
        "url": "http://arxiv.org/abs/2506.16683v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16683v1",
        "arxiv_id": "2506.16683v1",
        "authors": [
            "Penglong Zhai",
            "Yifang Yuan",
            "Fanyi Di",
            "Jie Li",
            "Yue Liu",
            "Chen Li",
            "Jie Huang",
            "Sicong Wang",
            "Yao Xu",
            "Xin Li"
        ],
        "submitted": "2025-06-20 01:54:32",
        "source": "arxiv",
        "comment": "12 pages,7 figures"
    },
    {
        "title": "Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations",
        "abstract": "Large Language Models (LLMs) exhibit a robust mastery of syntax when\nprocessing and generating text. While this suggests internalized understanding\nof hierarchical syntax and dependency relations, the precise mechanism by which\nthey represent syntactic structure is an open area within interpretability\nresearch. Probing provides one way to identify the mechanism of syntax being\nlinearly encoded in activations, however, no comprehensive study has yet\nestablished whether a model's probing accuracy reliably predicts its downstream\nsyntactic performance. Adopting a \"mechanisms vs. outcomes\" framework, we\nevaluate 32 open-weight transformer models and find that syntactic features\nextracted via probing fail to predict outcomes of targeted syntax evaluations\nacross English linguistic phenomena. Our results highlight a substantial\ndisconnect between latent syntactic representations found via probing and\nobservable syntactic behaviors in downstream tasks.",
        "url": "http://arxiv.org/abs/2506.16678v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16678v1",
        "arxiv_id": "2506.16678v1",
        "authors": [
            "Ananth Agarwal",
            "Jasper Jian",
            "Christopher D. Manning",
            "Shikhar Murty"
        ],
        "submitted": "2025-06-20 01:46:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Arch-Router: Aligning LLM Routing with Human Preferences",
        "abstract": "With the rapid proliferation of large language models (LLMs) -- each\noptimized for different strengths, style, or latency/cost profile -- routing\nhas become an essential technique to operationalize the use of different\nmodels. However, existing LLM routing approaches are limited in two key ways:\nthey evaluate performance using benchmarks that often fail to capture human\npreferences driven by subjective evaluation criteria, and they typically select\nfrom a limited pool of models. In this work, we propose a preference-aligned\nrouting framework that guides model selection by matching queries to\nuser-defined domains (e.g., travel) or action types (e.g., image editing) --\noffering a practical mechanism to encode preferences in routing decisions.\nSpecifically, we introduce \\textbf{Arch-Router}, a compact 1.5B model that\nlearns to map queries to domain-action preferences for model routing decisions.\nOur approach also supports seamlessly adding new models for routing without\nrequiring retraining or architectural modifications. Experiments on\nconversational datasets demonstrate that our approach achieves state-of-the-art\n(SOTA) results in matching queries with human preferences, outperforming top\nproprietary models. Our approach captures subjective evaluation criteria and\nmakes routing decisions more transparent and flexible. Our model is available\nat: \\texttt{https://huggingface.co/katanemo/Arch-Router-1.5B}.",
        "url": "http://arxiv.org/abs/2506.16655v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16655v1",
        "arxiv_id": "2506.16655v1",
        "authors": [
            "Co Tran",
            "Salman Paracha",
            "Adil Hafeez",
            "Shuguang Chen"
        ],
        "submitted": "2025-06-19 23:57:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Semantic Outlier Removal with Embedding Models and LLMs",
        "abstract": "Modern text processing pipelines demand robust methods to remove extraneous\ncontent while preserving a document's core message. Traditional approaches such\nas HTML boilerplate extraction or keyword filters often fail in multilingual\nsettings and struggle with context-sensitive nuances, whereas Large Language\nModels (LLMs) offer improved quality at high computational cost. We introduce\nSORE (Semantic Outlier Removal), a cost-effective, transparent method that\nleverages multilingual sentence embeddings and approximate nearest-neighbor\nsearch to identify and excise unwanted text segments. By first identifying core\ncontent via metadata embedding and then flagging segments that either closely\nmatch predefined outlier groups or deviate significantly from the core, SORE\nachieves near-LLM extraction precision at a fraction of the cost. Experiments\non HTML datasets demonstrate that SORE outperforms structural methods and yield\nhigh precision in diverse scenarios. Our system is currently deployed in\nproduction, processing millions of documents daily across multiple languages\nwhile maintaining both efficiency and accuracy. To facilitate reproducibility\nand further research, we release our implementation and evaluation datasets.",
        "url": "http://arxiv.org/abs/2506.16644v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16644v1",
        "arxiv_id": "2506.16644v1",
        "authors": [
            "Eren Akbiyik",
            "João Almeida",
            "Rik Melis",
            "Ritu Sriram",
            "Viviana Petrescu",
            "Vilhjálmur Vilhjálmsson"
        ],
        "submitted": "2025-06-19 23:06:12",
        "source": "arxiv",
        "comment": "Accepted to the 63rd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2025) Industry Track, 10 pages"
    },
    {
        "title": "Long-Context Generalization with Sparse Attention",
        "abstract": "Transformer-based architectures traditionally employ softmax to compute\nattention weights, which produces dense distributions over all tokens in a\nsequence. While effective in many settings, this density has been shown to be\ndetrimental for tasks that demand precise focus on fixed-size patterns: as\nsequence length increases, non-informative tokens accumulate attention\nprobability mass, leading to dispersion and representational collapse. We show\nin this paper that sparse attention mechanisms using $\\alpha$-entmax can avoid\nthese issues, due to their ability to assign exact zeros to irrelevant tokens.\nFurthermore, we introduce Adaptive-Scalable Entmax (ASEntmax), which endows\n$\\alpha$-entmax with a learnable temperature parameter, allowing the attention\ndistribution to interpolate between sparse (pattern-focused) and dense\n(softmax-like) regimes. Finally, we show that the ability to locate and\ngeneralize fixed-size patterns can be further improved through a careful design\nof position encodings, which impacts both dense and sparse attention methods.\nBy integrating ASEntmax into standard transformer layers alongside proper\npositional encodings, we show that our models greatly outperform softmax,\nscalable softmax, and fixed-temperature $\\alpha$-entmax baselines on\nlong-context generalization.",
        "url": "http://arxiv.org/abs/2506.16640v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16640v1",
        "arxiv_id": "2506.16640v1",
        "authors": [
            "Pavlo Vasylenko",
            "Marcos Treviso",
            "André F. T. Martins"
        ],
        "submitted": "2025-06-19 22:43:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "GeoGuess: Multimodal Reasoning based on Hierarchy of Visual Information in Street View",
        "abstract": "Multimodal reasoning is a process of understanding, integrating and inferring\ninformation across different data modalities. It has recently attracted surging\nacademic attention as a benchmark for Artificial Intelligence (AI). Although\nthere are various tasks for evaluating multimodal reasoning ability, they still\nhave limitations. Lack of reasoning on hierarchical visual clues at different\nlevels of granularity, e.g., local details and global context, is of little\ndiscussion, despite its frequent involvement in real scenarios. To bridge the\ngap, we introduce a novel and challenging task for multimodal reasoning, namely\nGeoGuess. Given a street view image, the task is to identify its location and\nprovide a detailed explanation. A system that succeeds in GeoGuess should be\nable to detect tiny visual clues, perceive the broader landscape, and associate\nwith vast geographic knowledge. Therefore, GeoGuess would require the ability\nto reason between hierarchical visual information and geographic knowledge. In\nthis work, we establish a benchmark for GeoGuess by introducing a specially\ncurated dataset GeoExplain which consists of\npanoramas-geocoordinates-explanation tuples. Additionally, we present a\nmultimodal and multilevel reasoning method, namely SightSense which can make\nprediction and generate comprehensive explanation based on hierarchy of visual\ninformation and external knowledge. Our analysis and experiments demonstrate\ntheir outstanding performance in GeoGuess.",
        "url": "http://arxiv.org/abs/2506.16633v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16633v1",
        "arxiv_id": "2506.16633v1",
        "authors": [
            "Fenghua Cheng",
            "Jinxiang Wang",
            "Sen Wang",
            "Zi Huang",
            "Xue Li"
        ],
        "submitted": "2025-06-19 22:19:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Initial Investigation of LLM-Assisted Development of Rule-Based Clinical NLP System",
        "abstract": "Despite advances in machine learning (ML) and large language models (LLMs),\nrule-based natural language processing (NLP) systems remain active in clinical\nsettings due to their interpretability and operational efficiency. However,\ntheir manual development and maintenance are labor-intensive, particularly in\ntasks with large linguistic variability. To overcome these limitations, we\nproposed a novel approach employing LLMs solely during the rule-based systems\ndevelopment phase. We conducted the initial experiments focusing on the first\ntwo steps of developing a rule-based NLP pipeline: find relevant snippets from\nthe clinical note; extract informative keywords from the snippets for the\nrule-based named entity recognition (NER) component. Our experiments\ndemonstrated exceptional recall in identifying clinically relevant text\nsnippets (Deepseek: 0.98, Qwen: 0.99) and 1.0 in extracting key terms for NER.\nThis study sheds light on a promising new direction for NLP development,\nenabling semi-automated or automated development of rule-based systems with\nsignificantly faster, more cost-effective, and transparent execution compared\nwith deep learning model-based solutions.",
        "url": "http://arxiv.org/abs/2506.16628v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16628v1",
        "arxiv_id": "2506.16628v1",
        "authors": [
            "Jianlin Shi",
            "Brian T. Bucher"
        ],
        "submitted": "2025-06-19 21:55:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Modeling Public Perceptions of Science in Media",
        "abstract": "Effectively engaging the public with science is vital for fostering trust and\nunderstanding in our scientific community. Yet, with an ever-growing volume of\ninformation, science communicators struggle to anticipate how audiences will\nperceive and interact with scientific news. In this paper, we introduce a\ncomputational framework that models public perception across twelve dimensions,\nsuch as newsworthiness, importance, and surprisingness. Using this framework,\nwe create a large-scale science news perception dataset with 10,489 annotations\nfrom 2,101 participants from diverse US and UK populations, providing valuable\ninsights into public responses to scientific information across domains. We\nfurther develop NLP models that predict public perception scores with a strong\nperformance. Leveraging the dataset and model, we examine public perception of\nscience from two perspectives: (1) Perception as an outcome: What factors\naffect the public perception of scientific information? (2) Perception as a\npredictor: Can we use the estimated perceptions to predict public engagement\nwith science? We find that individuals' frequency of science news consumption\nis the driver of perception, whereas demographic factors exert minimal\ninfluence. More importantly, through a large-scale analysis and carefully\ndesigned natural experiment on Reddit, we demonstrate that the estimated public\nperception of scientific information has direct connections with the final\nengagement pattern. Posts with more positive perception scores receive\nsignificantly more comments and upvotes, which is consistent across different\nscientific information and for the same science, but are framed differently.\nOverall, this research underscores the importance of nuanced perception\nmodeling in science communication, offering new pathways to predict public\ninterest and engagement with scientific content.",
        "url": "http://arxiv.org/abs/2506.16622v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16622v1",
        "arxiv_id": "2506.16622v1",
        "authors": [
            "Jiaxin Pei",
            "Dustin Wright",
            "Isabelle Augenstin",
            "David Jurgens"
        ],
        "submitted": "2025-06-19 21:49:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications",
        "abstract": "Synthetic data generation--mitigating data scarcity, privacy concerns, and\ndata quality challenges in biomedical fields--has been facilitated by rapid\nadvances of large language models (LLMs). This scoping review follows\nPRISMA-ScR guidelines and synthesizes 59 studies, published between 2020 and\n2025 and collected from PubMed, ACM, Web of Science, and Google Scholar. The\nreview systematically examines biomedical research and application trends in\nsynthetic data generation, emphasizing clinical applications, methodologies,\nand evaluations. Our analysis identifies data modalities of unstructured texts\n(78.0%), tabular data (13.6%), and multimodal sources (8.4%); generation\nmethods of prompting (72.9%), fine-tuning (22.0%) LLMs and specialized model\n(5.1%); and heterogeneous evaluations of intrinsic metrics (27.1%),\nhuman-in-the-loop assessments (55.9%), and LLM-based evaluations (13.6%). The\nanalysis addresses current limitations in what, where, and how health\nprofessionals can leverage synthetic data generation for biomedical domains.\nOur review also highlights challenges in adaption across clinical domains,\nresource and model accessibility, and evaluation standardizations.",
        "url": "http://arxiv.org/abs/2506.16594v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16594v1",
        "arxiv_id": "2506.16594v1",
        "authors": [
            "Hanshu Rao",
            "Weisi Liu",
            "Haohan Wang",
            "I-Chan Huang",
            "Zhe He",
            "Xiaolei Huang"
        ],
        "submitted": "2025-06-19 20:38:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Measuring (a Sufficient) World Model in LLMs: A Variance Decomposition Framework",
        "abstract": "Understanding whether large language models (LLMs) possess a world model-a\nstructured understanding of the world that supports generalization beyond\nsurface-level patterns-is central to assessing their reliability, especially in\nhigh-stakes applications. We propose a formal framework for evaluating whether\nan LLM exhibits a sufficiently robust world model, defined as producing\nconsistent outputs across semantically equivalent prompts while distinguishing\nbetween prompts that express different intents. We introduce a new evaluation\napproach to measure this that decomposes model response variability into three\ncomponents: variability due to user purpose, user articulation, and model\ninstability. An LLM with a strong world model should attribute most of the\nvariability in its responses to changes in foundational purpose rather than\nsuperficial changes in articulation. This approach allows us to quantify how\nmuch of a model's behavior is semantically grounded rather than driven by model\ninstability or alternative wording. We apply this framework to evaluate LLMs\nacross diverse domains. Our results show how larger models attribute a greater\nshare of output variability to changes in user purpose, indicating a more\nrobust world model. This improvement is not uniform, however: larger models do\nnot consistently outperform smaller ones across all domains, and their\nadvantage in robustness is often modest. These findings highlight the\nimportance of moving beyond accuracy-based benchmarks toward semantic\ndiagnostics that more directly assess the structure and stability of a model's\ninternal understanding of the world.",
        "url": "http://arxiv.org/abs/2506.16584v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16584v1",
        "arxiv_id": "2506.16584v1",
        "authors": [
            "Nadav Kunievsky",
            "James A. Evans"
        ],
        "submitted": "2025-06-19 20:19:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Streaming Non-Autoregressive Model for Accent Conversion and Pronunciation Improvement",
        "abstract": "We propose a first streaming accent conversion (AC) model that transforms\nnon-native speech into a native-like accent while preserving speaker identity,\nprosody and improving pronunciation. Our approach enables stream processing by\nmodifying a previous AC architecture with an Emformer encoder and an optimized\ninference mechanism. Additionally, we integrate a native text-to-speech (TTS)\nmodel to generate ideal ground-truth data for efficient training. Our streaming\nAC model achieves comparable performance to the top AC models while maintaining\nstable latency, making it the first AC system capable of streaming.",
        "url": "http://arxiv.org/abs/2506.16580v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16580v1",
        "arxiv_id": "2506.16580v1",
        "authors": [
            "Tuan-Nam Nguyen",
            "Ngoc-Quan Pham",
            "Seymanur Akti",
            "Alexander Waibel"
        ],
        "submitted": "2025-06-19 20:05:29",
        "source": "arxiv",
        "comment": "Accepted to INTERSPEECH 2025"
    },
    {
        "title": "Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System",
        "abstract": "Large language models (LLMs) offer promising opportunities for organizational\nresearch. However, their built-in moderation systems can create problems when\nresearchers try to analyze harmful content, often refusing to follow certain\ninstructions or producing overly cautious responses that undermine validity of\nthe results. This is particularly problematic when analyzing organizational\nconflicts such as microaggressions or hate speech. This paper introduces an Elo\nrating-based method that significantly improves LLM performance for harmful\ncontent analysis In two datasets, one focused on microaggression detection and\nthe other on hate speech, we find that our method outperforms traditional LLM\nprompting techniques and conventional machine learning models on key measures\nsuch as accuracy, precision, and F1 scores. Advantages include better\nreliability when analyzing harmful content, fewer false positives, and greater\nscalability for large-scale datasets. This approach supports organizational\napplications, including detecting workplace harassment, assessing toxic\ncommunication, and fostering safer and more inclusive work environments.",
        "url": "http://arxiv.org/abs/2506.16575v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16575v1",
        "arxiv_id": "2506.16575v1",
        "authors": [
            "Mustafa Akben",
            "Aaron Satko"
        ],
        "submitted": "2025-06-19 20:01:12",
        "source": "arxiv",
        "comment": "Submitted for HICSS 2025 (Hawaii International Conference on System\n  Sciences); under review"
    },
    {
        "title": "Weight Factorization and Centralization for Continual Learning in Speech Recognition",
        "abstract": "Modern neural network based speech recognition models are required to\ncontinually absorb new data without re-training the whole system, especially in\ndownstream applications using foundation models, having no access to the\noriginal training data. Continually training the models in a rehearsal-free,\nmultilingual, and language agnostic condition, likely leads to catastrophic\nforgetting, when a seemingly insignificant disruption to the weights can\ndestructively harm the quality of the models. Inspired by the ability of human\nbrains to learn and consolidate knowledge through the waking-sleeping cycle, we\npropose a continual learning approach with two distinct phases: factorization\nand centralization, learning and merging knowledge accordingly. Our experiments\non a sequence of varied code-switching datasets showed that the centralization\nstage can effectively prevent catastrophic forgetting by accumulating the\nknowledge in multiple scattering low-rank adapters.",
        "url": "http://arxiv.org/abs/2506.16574v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16574v1",
        "arxiv_id": "2506.16574v1",
        "authors": [
            "Enes Yavuz Ugan",
            "Ngoc-Quan Pham",
            "Alexander Waibel"
        ],
        "submitted": "2025-06-19 19:59:24",
        "source": "arxiv",
        "comment": "Accepted to INTERSPEECH 2025"
    },
    {
        "title": "Automatic Speech Recognition Biases in Newcastle English: an Error Analysis",
        "abstract": "Automatic Speech Recognition (ASR) systems struggle with regional dialects\ndue to biased training which favours mainstream varieties. While previous\nresearch has identified racial, age, and gender biases in ASR, regional bias\nremains underexamined. This study investigates ASR performance on Newcastle\nEnglish, a well-documented regional dialect known to be challenging for ASR. A\ntwo-stage analysis was conducted: first, a manual error analysis on a subsample\nidentified key phonological, lexical, and morphosyntactic errors behind ASR\nmisrecognitions; second, a case study focused on the systematic analysis of ASR\nrecognition of the regional pronouns ``yous'' and ``wor''. Results show that\nASR errors directly correlate with regional dialectal features, while social\nfactors play a lesser role in ASR mismatches. We advocate for greater dialectal\ndiversity in ASR training data and highlight the value of sociolinguistic\nanalysis in diagnosing and addressing regional biases.",
        "url": "http://arxiv.org/abs/2506.16558v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16558v1",
        "arxiv_id": "2506.16558v1",
        "authors": [
            "Dana Serditova",
            "Kevin Tang",
            "Jochen Steffens"
        ],
        "submitted": "2025-06-19 19:24:12",
        "source": "arxiv",
        "comment": "Submitted to Interspeech 2025"
    },
    {
        "title": "Revela: Dense Retriever Learning via Language Modeling",
        "abstract": "Dense retrievers play a vital role in accessing external and specialized\nknowledge to augment language models (LMs). Training dense retrievers typically\nrequires annotated query-document pairs, which are costly and hard to obtain in\nspecialized domains such as code-motivating growing interest in self-supervised\nretriever learning. Since LMs are trained to capture token-level dependencies\nthrough a self-supervised learning objective (i.e., next-token prediction), we\ncan analogously cast retrieval as learning dependencies among chunks of tokens.\nThis analogy naturally leads to the question: How can we adapt self-supervised\nlearning objectives in the spirit of language modeling to train retrievers?\n  To answer this question, we introduce Revela, a unified and scalable training\nframework for self-supervised retriever learning via language modeling. Revela\nmodels semantic dependencies among documents by conditioning next-token\nprediction on both local and cross-document context through an in-batch\nattention mechanism. This attention is weighted by retriever-computed\nsimilarity scores, enabling the retriever to be optimized as part of language\nmodeling. We evaluate Revela on both general-domain (BEIR) and domain-specific\n(CoIR) benchmarks across various retriever backbones. At a comparable parameter\nscale, Revela outperforms the previous best method with absolute improvements\nof 5.2 % (18.3 % relative) and 5.6 % (14.4 % relative) on NDCG@10,\nrespectively, underscoring its effectiveness. Performance increases with model\nsize, highlighting both the scalability of our approach and its promise for\nself-supervised retriever learning.",
        "url": "http://arxiv.org/abs/2506.16552v1",
        "pdf_url": "http://arxiv.org/pdf/2506.16552v1",
        "arxiv_id": "2506.16552v1",
        "authors": [
            "Fengyu Cai",
            "Tong Chen",
            "Xinran Zhao",
            "Sihao Chen",
            "Hongming Zhang",
            "Sherry Tongshuang Wu",
            "Iryna Gurevych",
            "Heinz Koeppl"
        ],
        "submitted": "2025-06-19 19:13:59",
        "source": "arxiv",
        "comment": null
    }
]