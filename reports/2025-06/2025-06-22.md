# Daily Papers Report - 2025-06-22

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Towards AI Search Paradigm

- **LLM Score**: 8
- **Keyword Score**: 11
- **Authors**: Yuchen Li, Hengyi Cai, Rui Kong, Xinran Chen, Jiamin Chen, Jun Yang, Haojie Zhang, Jiayi Li, Jiayi Wu, Yiqun Chen, Changle Qu, Keyi Kong, Wenwen Ye, Lixin Su, Xinyu Ma, Long Xia, Daiting Shi, Jiashu Zhao, Haoyi Xiong, Shuaiqiang Wang, Dawei Yin
- **URL**: <http://arxiv.org/abs/2506.17188v1>
- **Submitted**: 2025-06-20 17:42:13
- **Topic Keywords**: query, queries, ctr, retrieval, search
- **Reason**: The paper's focus on AI search paradigm and its modular architecture, which includes LLM-powered agents, aligns with your interest in Information Retrieval and Search technologies. The emphasis on query complexity, decomposition, and orchestration of workflows also resonates with your interest in query understanding and ranking models. However, the paper's scope is broader than your specific focus on e-commerce, and the abstract does not explicitly mention user behavior modeling or click models.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: AI Search Paradigm
- **Aim**: Developing trustworthy, adaptive, and scalable AI search systems
- **Rationale**: Adapting to various information needs through modular architecture and coordinated workflows
- **Ground**: Large Language Models (LLMs) and coordinated workflows
- **Experiment**: Task planning, tool integration, execution strategies, retrieval-augmented generation, and efficient LLM inference
- **Takeaway**: Comprehensive guide for developing AI search systems

#### Abstract
> In this paper, we introduce the AI Search Paradigm, a comprehensive blueprint
for next-generation search systems capable of emulating human information
processing and decision-making. The paradigm employs a modular architecture of
four LLM-powered agents (Master, Planner, Executor and Writer) that dynamically
adapt to the full spectrum of information needs, from simple factual queries to
complex multi-stage reasoning tasks. These agents collaborate dynamically
through coordinated workflows to evaluate query complexity, decompose problems
into executable plans, and orchestrate tool usage, task execution, and content
synthesis. We systematically present key methodologies for realizing this
paradigm, including task planning and tool integration, execution strategies,
aligned and robust retrieval-augmented generation, and efficient LLM inference,
spanning both algorithmic techniques and infrastructure-level optimizations. By
providing an in-depth guide to these foundational components, this work aims to
inform the development of trustworthy, adaptive, and scalable AI search
systems.

---

### 2. eSapiens: A Real-World NLP Framework for Multimodal Document Understanding and Enterprise Knowledge Processing

- **LLM Score**: 6
- **Keyword Score**: 17
- **Authors**: Isaac Shi, Zeyuan Li, Wenli Wang, Lewei He, Yang Yang, Tianyu Shi
- **URL**: <http://arxiv.org/abs/2506.16768v1>
- **Submitted**: 2025-06-20 06:07:20
- **Topic Keywords**: sparse retrieval, ranking, rerank, relevance, rag, retrieval, rank
- **Reason**: The paper introduces a question-answering system for enterprise settings, combining NLP and multimodal document understanding. While it touches on retrieval and ranking, the focus is more on question answering and knowledge processing, which is somewhat related to my interests in IR and search technologies. The paper's emphasis on real-world applications and enterprise settings is also relevant, but the specific topics and techniques used are not directly aligned with my core research themes.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: eSapiens: A Unified Question-Answering System for Enterprise Settings
- **Aim**: Design a unified question-answering system that combines a Text-to-SQL planner with a hybrid Retrieval-Augmented Generation pipeline for enterprise settings
- **Rationale**: Address challenges in production deployment, particularly in interpretability and concurrent user support, by integrating dense and sparse retrieval, commercial reranking, and a citation verification loop
- **Ground**: Evaluate the system on the RAGTruth benchmark across five leading large language models, demonstrating improved performance in contextual relevance and generation quality
- **Experiment**: Analyze the internal sensitivity of system configurations and the comparative effectiveness of retrieval strategies under citation-constrained generation, and evaluate the performance of the eSapiens model in terms of chunk size and its impact on recall, precision, and completeness
- **Takeaway**: The eSapiens system demonstrates robust and generalizable performance across varied question answering tasks, outperforming the FAISS baseline in terms of context relevance and generating more natural, coherent, and aligned responses

#### Abstract
> We introduce eSapiens, a unified question-answering system designed for
enterprise settings, which bridges structured databases and unstructured
textual corpora via a dual-module architecture. The system combines a
Text-to-SQL planner with a hybrid Retrieval-Augmented Generation (RAG)
pipeline, enabling natural language access to both relational data and
free-form documents. To enhance answer faithfulness, the RAG module integrates
dense and sparse retrieval, commercial reranking, and a citation verification
loop that ensures grounding consistency. We evaluate eSapiens on the RAGTruth
benchmark across five leading large language models (LLMs), analyzing
performance across key dimensions such as completeness, hallucination, and
context utilization. Results demonstrate that eSapiens outperforms a FAISS
baseline in contextual relevance and generation quality, with optional
strict-grounding controls for high-stakes scenarios. This work provides a
deployable framework for robust, citation-aware question answering in
real-world enterprise applications.

---

### 3. Revela: Dense Retriever Learning via Language Modeling

- **LLM Score**: 6
- **Keyword Score**: 8
- **Authors**: Fengyu Cai, Tong Chen, Xinran Zhao, Sihao Chen, Hongming Zhang, Sherry Tongshuang Wu, Iryna Gurevych, Heinz Koeppl
- **URL**: <http://arxiv.org/abs/2506.16552v1>
- **Submitted**: 2025-06-19 19:13:59
- **Topic Keywords**: retriever, query, retrieval
- **Reason**: The paper introduces a self-supervised learning framework for dense retriever learning via language modeling, which is relevant to information retrieval and search technologies. However, the focus on language modeling and token-level dependencies is not directly aligned with the user's primary interest in query understanding, ranking models, and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Self-Supervised Retriever Learning via Language Modeling
- **Aim**: Introduce Revela, a unified and scalable training framework for self-supervised retriever learning via language modeling
- **Rationale**: Enable the retriever to be optimized as part of language modeling by conditioning next token prediction on both local and cross-document context through an in-batch attention mechanism
- **Ground**: Revela is based on a dense retriever architecture, which captures macroscopic dependencies by modeling relationships among larger units of tokens, called 'chunks'
- **Experiment**: Evaluate Revela on two benchmarks: BEIR, a general-domain retrieval benchmark, and CoIR, a code retrieval benchmark, and conduct ablation studies to validate the effectiveness of V-normalization
- **Takeaway**: Revela achieves state-of-the-art results on both benchmarks, demonstrating its potential as a strong and scalable alternative to traditional self-supervised retriever training paradigms

#### Abstract
> Dense retrievers play a vital role in accessing external and specialized
knowledge to augment language models (LMs). Training dense retrievers typically
requires annotated query-document pairs, which are costly and hard to obtain in
specialized domains such as code-motivating growing interest in self-supervised
retriever learning. Since LMs are trained to capture token-level dependencies
through a self-supervised learning objective (i.e., next-token prediction), we
can analogously cast retrieval as learning dependencies among chunks of tokens.
This analogy naturally leads to the question: How can we adapt self-supervised
learning objectives in the spirit of language modeling to train retrievers?
  To answer this question, we introduce Revela, a unified and scalable training
framework for self-supervised retriever learning via language modeling. Revela
models semantic dependencies among documents by conditioning next-token
prediction on both local and cross-document context through an in-batch
attention mechanism. This attention is weighted by retriever-computed
similarity scores, enabling the retriever to be optimized as part of language
modeling. We evaluate Revela on both general-domain (BEIR) and domain-specific
(CoIR) benchmarks across various retriever backbones. At a comparable parameter
scale, Revela outperforms the previous best method with absolute improvements
of 5.2 % (18.3 % relative) and 5.6 % (14.4 % relative) on NDCG@10,
respectively, underscoring its effectiveness. Performance increases with model
size, highlighting both the scalability of our approach and its promise for
self-supervised retriever learning.

---

### 4. PersonalAI: Towards digital twins in the graph form

- **LLM Score**: 6
- **Keyword Score**: 2
- **Authors**: Mikhail Menschikov, Dmitry Evseev, Ruslan Kostoev, Ilya Perepechkin, Ilnaz Salimov, Victoria Dochkina, Petr Anokhin, Evgeny Burnaev, Nikita Semenov
- **URL**: <http://arxiv.org/abs/2506.17001v1>
- **Submitted**: 2025-06-20 13:52:15
- **Topic Keywords**: retrieval
- **Reason**: The paper explores the concept of personalizing language models, which is related to query understanding and user behavior modeling in Information Retrieval. However, the focus on knowledge graphs and graph construction is not directly aligned with the user's interests in ranking models and real-time relevance optimization.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Personalizing Language Models with Knowledge Graphs
- **Aim**: To develop a novel approach to personalizing language models by utilizing an external memory in the form of a knowledge graph
- **Rationale**: To efficiently and effectively manage and employ temporal dependencies in language models
- **Ground**: The construction of a knowledge graph through extracting triplets, thesis statements, and episodic memories, and the development of a question-answering pipeline that utilizes the graph
- **Experiment**: Evaluating the effectiveness of the approach on DiaASQ, HotpotQA, and TriviaQA benchmarks, and comparing with existing GraphRAG methods
- **Takeaway**: The proposed method offers a structured alternative to traditional Retrieval-Augmented Generation techniques, enhancing the model's ability to generate personalized responses by efficiently managing and employing temporal dependencies

#### Abstract
> The challenge of personalizing language models, specifically the ability to
account for a user's history during interactions, is of significant interest.
Despite recent advancements in large language models (LLMs) and Retrieval
Augmented Generation that have enhanced the factual base of LLMs, the task of
retaining extensive personal information and using it to generate personalized
responses remains pertinent. To address this, we propose utilizing external
memory in the form of knowledge graphs, which are constructed and updated by
the LLM itself. We have expanded upon ideas of AriGraph architecture and for
the first time introduced a combined graph featuring both standard edges and
two types of hyperedges. Experiments conducted on the TriviaQA, HotpotQA and
DiaASQ benchmarks indicates that this approach aids in making the process of
graph construction and knowledge extraction unified and robust. Furthermore, we
augmented the DiaASQ benchmark by incorporating parameters such as time into
dialogues and introducing contradictory statements made by the same speaker at
different times. Despite these modifications, the performance of the
question-answering system remained robust, demonstrating the proposed
architecture's ability to maintain and utilize temporal dependencies.

---

### 5. RAGentA: Multi-Agent Retrieval-Augmented Generation for Attributed Question Answering

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Ines Besrour, Jingbo He, Tobias Schreieder, Michael F√§rber
- **URL**: <http://arxiv.org/abs/2506.16988v1>
- **Submitted**: 2025-06-20 13:37:03
- **Comment**: Accepted at SIGIR 2025
- **Topic Keywords**: relevance, rag, retrieval
- **Reason**: The paper presents a framework for attributed question answering, focusing on optimizing answer correctness and faithfulness. While it uses retrieval-augmented generation, the primary focus is on answer generation rather than query understanding, ranking models, or user behavior modeling, which are key areas of interest in Information Retrieval. The paper's relevance to the user's research interests is somewhat limited.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: RAGentA: A Multi-Agent Retrieval-Augmented Generation Framework for Attributed Question Answering
- **Aim**: To develop a framework that ensures both answer correctness and faithfulness in attributed question answering tasks
- **Rationale**: Existing RAG systems lack faithfulness, and a multi-agent architecture with hybrid retrieval strategy can improve answer quality
- **Ground**: RAGentA builds upon the MAIN-RAG framework and introduces a novel agent for fine-grained attribution and answer refinement
- **Experiment**: Evaluation on a synthetic QA dataset shows gains of 1.09% in correctness and 10.72% in faithfulness compared to standard RAG baselines
- **Takeaway**: RAGentA is a novel framework that addresses the limitations of existing RAG systems, generating trustworthy answers for attributed QA tasks

#### Abstract
> We present RAGentA, a multi-agent retrieval-augmented generation (RAG)
framework for attributed question answering (QA). With the goal of trustworthy
answer generation, RAGentA focuses on optimizing answer correctness, defined by
coverage and relevance to the question and faithfulness, which measures the
extent to which answers are grounded in retrieved documents. RAGentA uses a
multi-agent architecture that iteratively filters retrieved documents,
generates attributed answers with in-line citations, and verifies completeness
through dynamic refinement. Central to the framework is a hybrid retrieval
strategy that combines sparse and dense methods, improving Recall@20 by 12.5%
compared to the best single retrieval model, resulting in more correct and
well-supported answers. Evaluated on a synthetic QA dataset derived from the
FineWeb index, RAGentA outperforms standard RAG baselines, achieving gains of
1.09% in correctness and 10.72% in faithfulness. These results demonstrate the
effectiveness of the multi-agent architecture and hybrid retrieval in advancing
trustworthy QA.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. A Simple Contrastive Framework Of Item Tokenization For Generative Recommendation

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Penglong Zhai, Yifang Yuan, Fanyi Di, Jie Li, Yue Liu, Chen Li, Jie Huang, Sicong Wang, Yao Xu, Xin Li
- **URL**: <http://arxiv.org/abs/2506.16683v1>
- **Submitted**: 2025-06-20 01:54:32
- **Comment**: 12 pages,7 figures
- **Topic Keywords**: rag, retrieval, recommend, search
- **Reason**: The paper proposes a novel unsupervised deep quantization framework for generative recommendation, focusing on contrastive learning and multi-modal knowledge alignment. While it touches on the topic of tokenization, it is primarily concerned with recommender systems and does not directly relate to information retrieval, query understanding, or ranking models, which are core areas of interest for the user.

#### Abstract
> Generative retrieval-based recommendation has emerged as a promising paradigm
aiming at directly generating the identifiers of the target candidates.
However, in large-scale recommendation systems, this approach becomes
increasingly cumbersome due to the redundancy and sheer scale of the token
space. To overcome these limitations, recent research has explored the use of
semantic tokens as an alternative to ID tokens, which typically leveraged
reconstruction-based strategies, like RQ-VAE, to quantize content embeddings
and significantly reduce the embedding size. However, reconstructive
quantization aims for the precise reconstruction of each item embedding
independently, which conflicts with the goal of generative retrieval tasks
focusing more on differentiating among items. Moreover, multi-modal side
information of items, such as descriptive text and images, geographical
knowledge in location-based recommendation services, has been shown to be
effective in improving recommendations by providing richer contexts for
interactions. Nevertheless, effectively integrating such complementary
knowledge into existing generative recommendation frameworks remains
challenging. To overcome these challenges, we propose a novel unsupervised deep
quantization exclusively based on contrastive learning, named SimCIT (a Simple
Contrastive Item Tokenization framework). Specifically, different from existing
reconstruction-based strategies, SimCIT propose to use a learnable residual
quantization module to align with the signals from different modalities of the
items, which combines multi-modal knowledge alignment and semantic tokenization
in a mutually beneficial contrastive learning framework. Extensive experiments
across public datasets and a large-scale industrial dataset from various
domains demonstrate SimCIT's effectiveness in LLM-based generative
recommendation.

### 7. Universal Music Representations? Evaluating Foundation Models on World Music Corpora

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Charilaos Papaioannou, Emmanouil Benetos, Alexandros Potamianos
- **URL**: <http://arxiv.org/abs/2506.17055v1>
- **Submitted**: 2025-06-20 15:06:44
- **Comment**: Accepted at ISMIR 2025
- **Topic Keywords**: information retrieval, retrieval
- **Reason**: The paper evaluates the performance of foundation models on world music corpora, which is a topic in Music Information Retrieval (MIR). While it touches on the idea of cross-cultural generalization, it does not directly relate to query understanding, ranking models, or user behavior modeling in Information Retrieval, which are the user's primary research interests. The paper's focus on music representation and understanding is somewhat related to the user's background in e-commerce, but it is not a central match.

#### Abstract
> Foundation models have revolutionized music information retrieval, but
questions remain about their ability to generalize across diverse musical
traditions. This paper presents a comprehensive evaluation of five
state-of-the-art audio foundation models across six musical corpora spanning
Western popular, Greek, Turkish, and Indian classical traditions. We employ
three complementary methodologies to investigate these models' cross-cultural
capabilities: probing to assess inherent representations, targeted supervised
fine-tuning of 1-2 layers, and multi-label few-shot learning for low-resource
scenarios. Our analysis shows varying cross-cultural generalization, with
larger models typically outperforming on non-Western music, though results
decline for culturally distant traditions. Notably, our approaches achieve
state-of-the-art performance on five out of six evaluated datasets,
demonstrating the effectiveness of foundation models for world music
understanding. We also find that our targeted fine-tuning approach does not
consistently outperform probing across all settings, suggesting foundation
models already encode substantial musical knowledge. Our evaluation framework
and benchmarking results contribute to understanding how far current models are
from achieving universal music representations while establishing metrics for
future progress.

### 8. Arch-Router: Aligning LLM Routing with Human Preferences

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Co Tran, Salman Paracha, Adil Hafeez, Shuguang Chen
- **URL**: <http://arxiv.org/abs/2506.16655v1>
- **Submitted**: 2025-06-19 23:57:41
- **Topic Keywords**: queries
- **Reason**: The paper proposes a routing framework for large language models, focusing on matching queries to user-defined domains or action types. While it's related to search technologies and query understanding, the primary focus is on routing and model selection, which is not directly aligned with my research interests in information retrieval and ranking models.

#### Abstract
> With the rapid proliferation of large language models (LLMs) -- each
optimized for different strengths, style, or latency/cost profile -- routing
has become an essential technique to operationalize the use of different
models. However, existing LLM routing approaches are limited in two key ways:
they evaluate performance using benchmarks that often fail to capture human
preferences driven by subjective evaluation criteria, and they typically select
from a limited pool of models. In this work, we propose a preference-aligned
routing framework that guides model selection by matching queries to
user-defined domains (e.g., travel) or action types (e.g., image editing) --
offering a practical mechanism to encode preferences in routing decisions.
Specifically, we introduce \textbf{Arch-Router}, a compact 1.5B model that
learns to map queries to domain-action preferences for model routing decisions.
Our approach also supports seamlessly adding new models for routing without
requiring retraining or architectural modifications. Experiments on
conversational datasets demonstrate that our approach achieves state-of-the-art
(SOTA) results in matching queries with human preferences, outperforming top
proprietary models. Our approach captures subjective evaluation criteria and
makes routing decisions more transparent and flexible. Our model is available
at: \texttt{https://huggingface.co/katanemo/Arch-Router-1.5B}.

### 9. Semantic Outlier Removal with Embedding Models and LLMs

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Eren Akbiyik, Jo√£o Almeida, Rik Melis, Ritu Sriram, Viviana Petrescu, Vilhj√°lmur Vilhj√°lmsson
- **URL**: <http://arxiv.org/abs/2506.16644v1>
- **Submitted**: 2025-06-19 23:06:12
- **Comment**: Accepted to the 63rd Annual Meeting of the Association for
  Computational Linguistics (ACL 2025) Industry Track, 10 pages
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on semantic outlier removal using embedding models and LLMs, which is related to query understanding and ranking models in Information Retrieval. However, the paper's primary concern is not on query understanding or ranking models, but rather on removing unwanted text segments from documents, which is a different problem. While the paper's use of embedding models and LLMs is relevant to my interests, the specific application and problem domain are not directly aligned with my research focus.

#### Abstract
> Modern text processing pipelines demand robust methods to remove extraneous
content while preserving a document's core message. Traditional approaches such
as HTML boilerplate extraction or keyword filters often fail in multilingual
settings and struggle with context-sensitive nuances, whereas Large Language
Models (LLMs) offer improved quality at high computational cost. We introduce
SORE (Semantic Outlier Removal), a cost-effective, transparent method that
leverages multilingual sentence embeddings and approximate nearest-neighbor
search to identify and excise unwanted text segments. By first identifying core
content via metadata embedding and then flagging segments that either closely
match predefined outlier groups or deviate significantly from the core, SORE
achieves near-LLM extraction precision at a fraction of the cost. Experiments
on HTML datasets demonstrate that SORE outperforms structural methods and yield
high precision in diverse scenarios. Our system is currently deployed in
production, processing millions of documents daily across multiple languages
while maintaining both efficiency and accuracy. To facilitate reproducibility
and further research, we release our implementation and evaluation datasets.

### 10. Multi-Objective Recommendation in the Era of Generative AI: A Survey of Recent Progress and Future Prospects

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Zihan Hong, Yushi Wu, Zhiting Zhao, Shanshan Feng, Jianghong Ma, Jiao Liu, Tianjun Wei
- **URL**: <http://arxiv.org/abs/2506.16893v1>
- **Submitted**: 2025-06-20 10:30:39
- **Comment**: 21 pages
- **Topic Keywords**: recommend, search
- **Reason**: The paper focuses on multi-objective recommendation systems, which is a related topic, but it does not specifically address query understanding, ranking models, or user behavior modeling, which are core areas of interest. While it mentions generative AI, it does not explore its application in information retrieval or search technologies, which are the primary focus of the researcher's interests.

#### Abstract
> With the recent progress in generative artificial intelligence (Generative
AI), particularly in the development of large language models, recommendation
systems are evolving to become more versatile. Unlike traditional techniques,
generative AI not only learns patterns and representations from complex data
but also enables content generation, data synthesis, and personalized
experiences. This generative capability plays a crucial role in the field of
recommendation systems, helping to address the issue of data sparsity and
improving the overall performance of recommendation systems. Numerous studies
on generative AI have already emerged in the field of recommendation systems.
Meanwhile, the current requirements for recommendation systems have surpassed
the single utility of accuracy, leading to a proliferation of multi-objective
research that considers various goals in recommendation systems. However, to
the best of our knowledge, there remains a lack of comprehensive studies on
multi-objective recommendation systems based on generative AI technologies,
leaving a significant gap in the literature. Therefore, we investigate the
existing research on multi-objective recommendation systems involving
generative AI to bridge this gap. We compile current research on
multi-objective recommendation systems based on generative techniques,
categorizing them by objectives. Additionally, we summarize relevant evaluation
metrics and commonly used datasets, concluding with an analysis of the
challenges and future directions in this domain.

### 11. MUCAR: Benchmarking Multilingual Cross-Modal Ambiguity Resolution for Multimodal Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Xiaolong Wang, Zhaolu Kang, Wangyuxuan Zhai, Xinyue Lou, Yunghwei Lai, Ziyue Wang, Yawen Wang, Kaiyu Huang, Yile Wang, Peng Li, Yang Liu
- **URL**: <http://arxiv.org/abs/2506.17046v1>
- **Submitted**: 2025-06-20 14:57:41
- **Topic Keywords**: search
- **Reason**: The paper focuses on multimodal ambiguity resolution, which is not directly related to information retrieval, query understanding, or ranking models. While it involves natural language processing, the context is more focused on visual-textual alignment and disambiguation, which is not a primary interest area. The paper's relevance to user's research interests is limited, but it may be of interest in the context of multimodal information retrieval.

#### Abstract
> Multimodal Large Language Models (MLLMs) have demonstrated significant
advances across numerous vision-language tasks. Due to their strong image-text
alignment capability, MLLMs can effectively understand image-text pairs with
clear meanings. However, effectively resolving the inherent ambiguities in
natural language and visual contexts remains challenging. Existing multimodal
benchmarks typically overlook linguistic and visual ambiguities, relying mainly
on unimodal context for disambiguation and thus failing to exploit the mutual
clarification potential between modalities. To bridge this gap, we introduce
MUCAR, a novel and challenging benchmark designed explicitly for evaluating
multimodal ambiguity resolution across multilingual and cross-modal scenarios.
MUCAR includes: (1) a multilingual dataset where ambiguous textual expressions
are uniquely resolved by corresponding visual contexts, and (2) a
dual-ambiguity dataset that systematically pairs ambiguous images with
ambiguous textual contexts, with each combination carefully constructed to
yield a single, clear interpretation through mutual disambiguation. Extensive
evaluations involving 19 state-of-the-art multimodal models--encompassing both
open-source and proprietary architectures--reveal substantial gaps compared to
human-level performance, highlighting the need for future research into more
sophisticated cross-modal ambiguity comprehension methods, further pushing the
boundaries of multimodal reasoning.

### 12. DistillNote: LLM-based clinical note summaries improve heart failure diagnosis

- **LLM Score**: 2
- **Keyword Score**: 9
- **Authors**: Heloisa Oss Boll, Antonio Oss Boll, Leticia Puttlitz Boll, Ameen Abu Hanna, Iacer Calixto
- **URL**: <http://arxiv.org/abs/2506.16777v1>
- **Submitted**: 2025-06-20 06:45:40
- **Topic Keywords**: pairwise, relevance, rag, search
- **Reason**: The paper focuses on clinical note summarization using large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the context is specific to healthcare and medical diagnosis, which is not a primary area of interest.

#### Abstract
> Large language models (LLMs) offer unprecedented opportunities to generate
concise summaries of patient information and alleviate the burden of clinical
documentation that overwhelms healthcare providers. We present Distillnote, a
framework for LLM-based clinical note summarization, and generate over 64,000
admission note summaries through three techniques: (1) One-step, direct
summarization, and a divide-and-conquer approach involving (2) Structured
summarization focused on independent clinical insights, and (3) Distilled
summarization that further condenses the Structured summaries. We test how
useful are the summaries by using them to predict heart failure compared to a
model trained on the original notes. Distilled summaries achieve 79% text
compression and up to 18.2% improvement in AUPRC compared to an LLM trained on
the full notes. We also evaluate the quality of the generated summaries in an
LLM-as-judge evaluation as well as through blinded pairwise comparisons with
clinicians. Evaluations indicate that one-step summaries are favoured by
clinicians according to relevance and clinical actionability, while distilled
summaries offer optimal efficiency (avg. 6.9x compression-to-performance ratio)
and significantly reduce hallucinations. We release our summaries on PhysioNet
to encourage future research.

### 13. Cross-Modal Obfuscation for Jailbreak Attacks on Large Vision-Language Models

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Lei Jiang, Zixun Zhang, Zizhou Wang, Xiaobing Sun, Zhen Li, Liangli Zhen, Xiaohua Xu
- **URL**: <http://arxiv.org/abs/2506.16760v1>
- **Submitted**: 2025-06-20 05:30:25
- **Comment**: 15 pages, 9 figures
- **Topic Keywords**: query, queries, rag
- **Reason**: This paper is not relevant to your research interests as it focuses on vision-language models and jailbreak attacks, which is outside the scope of information retrieval, search technologies, and natural language processing. The paper's abstract does not mention query understanding, ranking models, or user behavior modeling, which are key areas of interest for you.

#### Abstract
> Large Vision-Language Models (LVLMs) demonstrate exceptional performance
across multimodal tasks, yet remain vulnerable to jailbreak attacks that bypass
built-in safety mechanisms to elicit restricted content generation. Existing
black-box jailbreak methods primarily rely on adversarial textual prompts or
image perturbations, yet these approaches are highly detectable by standard
content filtering systems and exhibit low query and computational efficiency.
In this work, we present Cross-modal Adversarial Multimodal Obfuscation (CAMO),
a novel black-box jailbreak attack framework that decomposes malicious prompts
into semantically benign visual and textual fragments. By leveraging LVLMs'
cross-modal reasoning abilities, CAMO covertly reconstructs harmful
instructions through multi-step reasoning, evading conventional detection
mechanisms. Our approach supports adjustable reasoning complexity and requires
significantly fewer queries than prior attacks, enabling both stealth and
efficiency. Comprehensive evaluations conducted on leading LVLMs validate
CAMO's effectiveness, showcasing robust performance and strong cross-model
transferability. These results underscore significant vulnerabilities in
current built-in safety mechanisms, emphasizing an urgent need for advanced,
alignment-aware security and safety solutions in vision-language systems.

### 14. Are Bias Evaluation Methods Biased ?

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Lina Berrayana, Sean Rooney, Luis Garc√©s-Erice, Ioana Giurgiu
- **URL**: <http://arxiv.org/abs/2506.17111v1>
- **Submitted**: 2025-06-20 16:11:25
- **Comment**: Accepted to ACL 2025 Workshop GEM
- **Topic Keywords**: ranking, recommend, rank
- **Reason**: The paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. While it touches on the topic of evaluation methods, it is focused on Large Language Models and bias evaluation, which is not a primary interest of yours.

#### Abstract
> The creation of benchmarks to evaluate the safety of Large Language Models is
one of the key activities within the trusted AI community. These benchmarks
allow models to be compared for different aspects of safety such as toxicity,
bias, harmful behavior etc. Independent benchmarks adopt different approaches
with distinct data sets and evaluation methods. We investigate how robust such
benchmarks are by using different approaches to rank a set of representative
models for bias and compare how similar are the overall rankings. We show that
different but widely used bias evaluations methods result in disparate model
rankings. We conclude with recommendations for the community in the usage of
such benchmarks.

### 15. Pyramid Mixer: Multi-dimensional Multi-period Interest Modeling for Sequential Recommendation

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Zhen Gong, Zhifang Fan, Hui Lu, Qiwei Chen, Chenbin Zhang, Lin Guan, Yuchao Zheng, Feng Zhang, Xiao Yang, Zuotao Liu
- **URL**: <http://arxiv.org/abs/2506.16942v1>
- **Submitted**: 2025-06-20 12:11:38
- **Comment**: Accepted by SIGIR'25
- **Topic Keywords**: rag, user action, recommend
- **Reason**: The paper focuses on sequential recommendation and user interest modeling, which is not directly related to the user's primary research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling. Although the paper mentions user behavior modeling, it is limited to a specific context (sequential recommendation) and does not explore the broader topics of interest.

#### Abstract
> Sequential recommendation, a critical task in recommendation systems,
predicts the next user action based on the understanding of the user's
historical behaviors. Conventional studies mainly focus on cross-behavior
modeling with self-attention based methods while neglecting comprehensive user
interest modeling for more dimensions. In this study, we propose a novel
sequential recommendation model, Pyramid Mixer, which leverages the MLP-Mixer
architecture to achieve efficient and complete modeling of user interests. Our
method learns comprehensive user interests via cross-behavior and cross-feature
user sequence modeling. The mixer layers are stacked in a pyramid way for
cross-period user temporal interest learning. Through extensive offline and
online experiments, we demonstrate the effectiveness and efficiency of our
method, and we obtain a +0.106% improvement in user stay duration and a
+0.0113% increase in user active days in the online A/B test. The Pyramid Mixer
has been successfully deployed on the industrial platform, demonstrating its
scalability and impact in real-world applications.

### 16. Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Haoran Sun, Yankai Jiang, Wenjie Lou, Yujie Zhang, Wenjie Li, Lilong Wang, Mianxin Liu, Lei Liu, Xiaosong Wang
- **URL**: <http://arxiv.org/abs/2506.16962v1>
- **Submitted**: 2025-06-20 12:51:19
- **Topic Keywords**: rag, rank, search
- **Reason**: The paper focuses on medical large language models and medical reasoning, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although the paper mentions multimodal language models and visual question-answering, the context is medical and not relevant to the user's core research themes.

#### Abstract
> Multimodal large language models (MLLMs) have begun to demonstrate robust
reasoning capabilities on general tasks, yet their application in the medical
domain remains in its early stages. Constructing chain-of-thought (CoT)
training data is essential for bolstering the reasoning abilities of medical
MLLMs. However, existing approaches exhibit a deficiency in offering a
comprehensive framework for searching and evaluating effective reasoning paths
towards critical diagnosis. To address this challenge, we propose Mentor-Intern
Collaborative Search (MICS), a novel reasoning-path searching scheme to
generate rigorous and effective medical CoT data. MICS first leverages mentor
models to initialize the reasoning, one step at a time, then prompts each
intern model to continue the thinking along those initiated paths, and finally
selects the optimal reasoning path according to the overall reasoning
performance of multiple intern models. The reasoning performance is determined
by an MICS-Score, which assesses the quality of generated reasoning paths.
Eventually, we construct MMRP, a multi-task medical reasoning dataset with
ranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum
learning strategy, with robust visual question-answering and generalizable
reasoning capabilities. Extensive experiments demonstrate that Chiron-o1,
trained on our CoT dataset constructed using MICS, achieves state-of-the-art
performance across a list of medical visual question answering and reasoning
benchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing
Step-by-Step and Verifiable Medical Reasoning in MLLMs

### 17. MIST: Jailbreaking Black-box Large Language Models via Iterative Semantic Tuning

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Muyang Zheng, Yuanzhi Yao, Changting Lin, Rui Wang, Meng Han
- **URL**: <http://arxiv.org/abs/2506.16792v1>
- **Submitted**: 2025-06-20 07:16:47
- **Comment**: 12 pages, 3 figures
- **Topic Keywords**: query, search
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on Natural Language Processing, the focus is on large language models and jailbreaking attacks, which is not a central match for your research interests.

#### Abstract
> Despite efforts to align large language models (LLMs) with societal and moral
values, these models remain susceptible to jailbreak attacks--methods designed
to elicit harmful responses. Jailbreaking black-box LLMs is considered
challenging due to the discrete nature of token inputs, restricted access to
the target LLM, and limited query budget. To address the issues above, we
propose an effective method for jailbreaking black-box large language Models
via Iterative Semantic Tuning, named MIST. MIST enables attackers to
iteratively refine prompts that preserve the original semantic intent while
inducing harmful content. Specifically, to balance semantic similarity with
computational efficiency, MIST incorporates two key strategies: sequential
synonym search, and its advanced version--order-determining optimization.
Extensive experiments across two open-source models and four closed-source
models demonstrate that MIST achieves competitive attack success rates and
attack transferability compared with other state-of-the-art white-box and
black-box jailbreak methods. Additionally, we conduct experiments on
computational efficiency to validate the practical viability of MIST.

### 18. LegiGPT: Party Politics and Transport Policy with Large Language Model

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Hyunsoo Yun, Eun Hak Lee
- **URL**: <http://arxiv.org/abs/2506.16692v1>
- **Submitted**: 2025-06-20 02:25:52
- **Topic Keywords**: relevance, korea
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling, which are the primary focus of your research interests. The paper's topic is more focused on policy analysis and legislative decision-making, which is outside your core research themes.

#### Abstract
> Given the significant influence of lawmakers' political ideologies on
legislative decision-making, understanding their impact on policymaking is
critically important. We introduce a novel framework, LegiGPT, which integrates
a large language model (LLM) with explainable artificial intelligence (XAI) to
analyze transportation-related legislative proposals. LegiGPT employs a
multi-stage filtering and classification pipeline using zero-shot prompting
with GPT-4. Using legislative data from South Korea's 21st National Assembly,
we identify key factors - including sponsor characteristics, political
affiliations, and geographic variables - that significantly influence
transportation policymaking. The LLM was used to classify
transportation-related bill proposals through a stepwise filtering process
based on keywords, phrases, and contextual relevance. XAI techniques were then
applied to examine relationships between party affiliation and associated
attributes. The results reveal that the number and proportion of conservative
and progressive sponsors, along with district size and electoral population,
are critical determinants shaping legislative outcomes. These findings suggest
that both parties contributed to bipartisan legislation through different forms
of engagement, such as initiating or supporting proposals. This integrated
approach provides a valuable tool for understanding legislative dynamics and
guiding future policy development, with broader implications for infrastructure
planning and governance.

### 19. Large Language Models as Psychological Simulators: A Methodological Guide

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Zhicheng Lin
- **URL**: <http://arxiv.org/abs/2506.16702v1>
- **Submitted**: 2025-06-20 02:45:23
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on using large language models as psychological simulators, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on topics like cognitive processes and internal representations, the primary focus is on psychology and behavioral research, making it off-topic for your research interests.

#### Abstract
> Large language models (LLMs) offer emerging opportunities for psychological
and behavioral research, but methodological guidance is lacking. This article
provides a framework for using LLMs as psychological simulators across two
primary applications: simulating roles and personas to explore diverse
contexts, and serving as computational models to investigate cognitive
processes. For simulation, we present methods for developing psychologically
grounded personas that move beyond demographic categories, with strategies for
validation against human data and use cases ranging from studying inaccessible
populations to prototyping research instruments. For cognitive modeling, we
synthesize emerging approaches for probing internal representations,
methodological advances in causal interventions, and strategies for relating
model behavior to human cognition. We address overarching challenges including
prompt sensitivity, temporal limitations from training data cutoffs, and
ethical considerations that extend beyond traditional human subjects review.
Throughout, we emphasize the need for transparency about model capabilities and
constraints. Together, this framework integrates emerging empirical evidence
about LLM performance--including systematic biases, cultural limitations, and
prompt brittleness--to help researchers wrangle these challenges and leverage
the unique capabilities of LLMs in psychological research.

### 20. Modeling Public Perceptions of Science in Media

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Jiaxin Pei, Dustin Wright, Isabelle Augenstin, David Jurgens
- **URL**: <http://arxiv.org/abs/2506.16622v1>
- **Submitted**: 2025-06-19 21:49:28
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on modeling public perceptions of science in media, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the context is science communication and public engagement, which is not a central match with the user's research interests.

#### Abstract
> Effectively engaging the public with science is vital for fostering trust and
understanding in our scientific community. Yet, with an ever-growing volume of
information, science communicators struggle to anticipate how audiences will
perceive and interact with scientific news. In this paper, we introduce a
computational framework that models public perception across twelve dimensions,
such as newsworthiness, importance, and surprisingness. Using this framework,
we create a large-scale science news perception dataset with 10,489 annotations
from 2,101 participants from diverse US and UK populations, providing valuable
insights into public responses to scientific information across domains. We
further develop NLP models that predict public perception scores with a strong
performance. Leveraging the dataset and model, we examine public perception of
science from two perspectives: (1) Perception as an outcome: What factors
affect the public perception of scientific information? (2) Perception as a
predictor: Can we use the estimated perceptions to predict public engagement
with science? We find that individuals' frequency of science news consumption
is the driver of perception, whereas demographic factors exert minimal
influence. More importantly, through a large-scale analysis and carefully
designed natural experiment on Reddit, we demonstrate that the estimated public
perception of scientific information has direct connections with the final
engagement pattern. Posts with more positive perception scores receive
significantly more comments and upvotes, which is consistent across different
scientific information and for the same science, but are framed differently.
Overall, this research underscores the importance of nuanced perception
modeling in science communication, offering new pathways to predict public
interest and engagement with scientific content.

### 21. A Scoping Review of Synthetic Data Generation for Biomedical Research and Applications

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Hanshu Rao, Weisi Liu, Haohan Wang, I-Chan Huang, Zhe He, Xiaolei Huang
- **URL**: <http://arxiv.org/abs/2506.16594v1>
- **Submitted**: 2025-06-19 20:38:17
- **Topic Keywords**: rag, search
- **Reason**: The paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The topic of synthetic data generation for biomedical research and applications is outside your primary focus areas, and the paper does not discuss query understanding, ranking models, or user behavior modeling.

#### Abstract
> Synthetic data generation--mitigating data scarcity, privacy concerns, and
data quality challenges in biomedical fields--has been facilitated by rapid
advances of large language models (LLMs). This scoping review follows
PRISMA-ScR guidelines and synthesizes 59 studies, published between 2020 and
2025 and collected from PubMed, ACM, Web of Science, and Google Scholar. The
review systematically examines biomedical research and application trends in
synthetic data generation, emphasizing clinical applications, methodologies,
and evaluations. Our analysis identifies data modalities of unstructured texts
(78.0%), tabular data (13.6%), and multimodal sources (8.4%); generation
methods of prompting (72.9%), fine-tuning (22.0%) LLMs and specialized model
(5.1%); and heterogeneous evaluations of intrinsic metrics (27.1%),
human-in-the-loop assessments (55.9%), and LLM-based evaluations (13.6%). The
analysis addresses current limitations in what, where, and how health
professionals can leverage synthetic data generation for biomedical domains.
Our review also highlights challenges in adaption across clinical domains,
resource and model accessibility, and evaluation standardizations.

### 22. Chain-of-Thought Prompting Obscures Hallucination Cues in Large Language Models: An Empirical Evaluation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jiahao Cheng, Tiancheng Su, Jia Yuan, Guoxiu He, Jiawei Liu, Xinqi Tao, Jingwen Xie, Huaxia Li
- **URL**: <http://arxiv.org/abs/2506.17088v1>
- **Submitted**: 2025-06-20 15:49:37
- **Topic Keywords**: rag
- **Reason**: The paper focuses on the evaluation of Chain-of-Thought prompting on Large Language Models, exploring its impact on hallucination detection. While it touches on the internal states and token probability distributions of the models, it does not address query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval and Search technologies. The paper's relevance to the user's research interests is limited.

#### Abstract
> Large Language Models (LLMs) often exhibit \textit{hallucinations},
generating factually incorrect or semantically irrelevant content in response
to prompts. Chain-of-Thought (CoT) prompting can mitigate hallucinations by
encouraging step-by-step reasoning, but its impact on hallucination detection
remains underexplored. To bridge this gap, we conduct a systematic empirical
evaluation. We begin with a pilot experiment, revealing that CoT reasoning
significantly affects the LLM's internal states and token probability
distributions. Building on this, we evaluate the impact of various CoT
prompting methods on mainstream hallucination detection methods across both
instruction-tuned and reasoning-oriented LLMs. Specifically, we examine three
key dimensions: changes in hallucination score distributions, variations in
detection accuracy, and shifts in detection confidence. Our findings show that
while CoT prompting helps reduce hallucination frequency, it also tends to
obscure critical signals used for detection, impairing the effectiveness of
various detection methods. Our study highlights an overlooked trade-off in the
use of reasoning. Code is publicly available at:
https://anonymous.4open.science/r/cot-hallu-detect.

### 23. Language-Informed Synthesis of Rational Agent Models for Grounded Theory-of-Mind Reasoning On-The-Fly

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Lance Ying, Ryan Truong, Katherine M. Collins, Cedegao E. Zhang, Megan Wei, Tyler Brooke-Wilson, Tan Zhi-Xuan, Lionel Wong, Joshua B. Tenenbaum
- **URL**: <http://arxiv.org/abs/2506.16755v1>
- **Submitted**: 2025-06-20 05:21:42
- **Comment**: 5 figures, 19 pages
- **Topic Keywords**: rag
- **Reason**: The paper focuses on multimodal social reasoning, integrating linguistic and visual inputs, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it involves language processing, the context is social reasoning and agent modeling, which is not a core area of interest for the user.

#### Abstract
> Drawing real world social inferences usually requires taking into account
information from multiple modalities. Language is a particularly powerful
source of information in social settings, especially in novel situations where
language can provide both abstract information about the environment dynamics
and concrete specifics about an agent that cannot be easily visually observed.
In this paper, we propose Language-Informed Rational Agent Synthesis (LIRAS), a
framework for drawing context-specific social inferences that integrate
linguistic and visual inputs. LIRAS frames multimodal social reasoning as a
process of constructing structured but situation-specific agent and environment
representations - leveraging multimodal language models to parse language and
visual inputs into unified symbolic representations, over which a Bayesian
inverse planning engine can be run to produce granular probabilistic judgments.
On a range of existing and new social reasoning tasks derived from cognitive
science experiments, we find that our model (instantiated with a comparatively
lightweight VLM) outperforms ablations and state-of-the-art models in capturing
human judgments across all domains.

### 24. LM-SPT: LM-Aligned Semantic Distillation for Speech Tokenization

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Daejin Jo, Jeeyoung Yun, Byungseok Roh, Sungwoong Kim
- **URL**: <http://arxiv.org/abs/2506.16738v1>
- **Submitted**: 2025-06-20 04:15:14
- **Topic Keywords**: rag
- **Reason**: The paper focuses on speech tokenization and semantic distillation for speech-language modeling, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on speech and acoustic processing also diverges from the user's background in e-commerce and focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> With the rapid progress of speech language models (SLMs), discrete speech
tokens have emerged as a core interface between speech and text, enabling
unified modeling across modalities. Recent speech tokenization approaches aim
to isolate semantic information from low-level acoustics to better align with
language models. In particular, previous methods use SSL teachers such as
HuBERT to extract semantic representations, which are then distilled into a
semantic quantizer to suppress acoustic redundancy as well as capture
content-related latent structures. However, they still produce speech token
sequences significantly longer than their textual counterparts, creating
challenges for efficient speech-language modeling. Reducing the frame rate is a
natural solution, but standard techniques, such as rigid average pooling across
frames, can distort or dilute the semantic structure required for effective LM
alignment. To address this, we propose LM-SPT, a speech tokenization method
that introduces a novel semantic distillation. Instead of directly matching
teacher and student features via pooling, we reconstruct speech solely from
semantic tokens and minimize the discrepancy between the encoded
representations of the original and reconstructed waveforms, obtained from a
frozen automatic speech recognition (ASR) encoder. This indirect yet
data-driven supervision enables the tokenizer to learn discrete units that are
more semantically aligned with language models. LM-SPT further incorporates
architectural improvements to the encoder and decoder for speech tokenization,
and supports multiple frame rates, including 25Hz, 12.5Hz, and 6.25Hz.
Experimental results show that LM-SPT achieves superior reconstruction fidelity
compared to baselines, and that SLMs trained with LM-SPT tokens achieve
competitive performances on speech-to-text and consistently outperform
baselines on text-to-speech tasks.

### 25. ReasonGRM: Enhancing Generative Reward Models through Large Reasoning Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Bin Chen, Xinzge Gao, Chuanrui Hu, Penghang Yu, Hua Zhang, Bing-Kun Bao
- **URL**: <http://arxiv.org/abs/2506.16712v1>
- **Submitted**: 2025-06-20 03:10:52
- **Topic Keywords**: rag
- **Reason**: The paper focuses on Generative Reward Models, which is not directly related to Information Retrieval or Search technologies. While it mentions reasoning and evaluation metrics, the context is different from query understanding, ranking models, and user behavior modeling, which are core areas of interest.

#### Abstract
> Generative Reward Models (GRMs) provide greater flexibility than scalar
reward models in capturing human preferences, but their effectiveness is
limited by poor reasoning capabilities. This often results in incomplete or
overly speculative reasoning paths, leading to hallucinations or missing key
information in complex tasks. We address this challenge with ReasonGRM, a
three-stage generative reward modeling framework. In the first stage, Zero-RL
is used to generate concise, outcome-directed reasoning paths that reduce the
likelihood of critical omissions. In the second stage, we introduce a novel
evaluation metric, $R^\star$, which scores reasoning paths based on their
generation likelihood. This favors paths that reach correct answers with
minimal exploration, helping to reduce hallucination-prone data during
training. In the final stage, the model is further refined through
reinforcement learning on challenging examples to enhance its preference
discrimination capabilities. Experiments on three public benchmarks show that
ReasonGRM achieves competitive or state-of-the-art performance, outperforming
previous best GRMs by 1.8\% on average and surpassing proprietary models such
as GPT-4o by up to 5.6\%. These results demonstrate the effectiveness of
reasoning-aware training and highlight the importance of high-quality rationale
selection for reliable preference modeling.

### 26. Fine-Tuning Lowers Safety and Disrupts Evaluation Consistency

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Kathleen C. Fraser, Hillary Dawkins, Isar Nejadgholi, Svetlana Kiritchenko
- **URL**: <http://arxiv.org/abs/2506.17209v1>
- **Submitted**: 2025-06-20 17:57:12
- **Comment**: to appear at LLMSEC 2025
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The topic of fine-tuning large language models for safety and evaluation consistency is outside your primary focus and expertise.

#### Abstract
> Fine-tuning a general-purpose large language model (LLM) for a specific
domain or task has become a routine procedure for ordinary users. However,
fine-tuning is known to remove the safety alignment features of the model, even
when the fine-tuning data does not contain any harmful content. We consider
this to be a critical failure mode of LLMs due to the widespread uptake of
fine-tuning, combined with the benign nature of the "attack". Most
well-intentioned developers are likely unaware that they are deploying an LLM
with reduced safety. On the other hand, this known vulnerability can be easily
exploited by malicious actors intending to bypass safety guardrails. To make
any meaningful progress in mitigating this issue, we first need reliable and
reproducible safety evaluations. In this work, we investigate how robust a
safety benchmark is to trivial variations in the experimental procedure, and
the stochastic nature of LLMs. Our initial experiments expose surprising
variance in the results of the safety evaluation, even when seemingly
inconsequential changes are made to the fine-tuning setup. Our observations
have serious implications for how researchers in this field should report
results to enable meaningful comparisons in the future.

### 27. From Concepts to Components: Concept-Agnostic Attention Module Discovery in Transformers

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Jingtong Su, Julia Kempe, Karen Ullrich
- **URL**: <http://arxiv.org/abs/2506.17052v1>
- **Submitted**: 2025-06-20 15:04:11
- **Topic Keywords**: search
- **Reason**: The paper focuses on attention mechanisms in transformers, which is not directly related to information retrieval, query understanding, or ranking models. While it touches on interpretability and attribution methods, the concepts and techniques discussed are not relevant to the user's primary research interests.

#### Abstract
> Transformers have achieved state-of-the-art performance across language and
vision tasks. This success drives the imperative to interpret their internal
mechanisms with the dual goals of enhancing performance and improving
behavioral control. Attribution methods help advance interpretability by
assigning model outputs associated with a target concept to specific model
components. Current attribution research primarily studies multi-layer
perceptron neurons and addresses relatively simple concepts such as factual
associations (e.g., Paris is located in France). This focus tends to overlook
the impact of the attention mechanism and lacks a unified approach for
analyzing more complex concepts. To fill these gaps, we introduce Scalable
Attention Module Discovery (SAMD), a concept-agnostic method for mapping
arbitrary, complex concepts to specific attention heads of general transformer
models. We accomplish this by representing each concept as a vector,
calculating its cosine similarity with each attention head, and selecting the
TopK-scoring heads to construct the concept-associated attention module. We
then propose Scalar Attention Module Intervention (SAMI), a simple strategy to
diminish or amplify the effects of a concept by adjusting the attention module
using only a single scalar parameter. Empirically, we demonstrate SAMD on
concepts of varying complexity, and visualize the locations of their
corresponding modules. Our results demonstrate that module locations remain
stable before and after LLM post-training, and confirm prior work on the
mechanics of LLM multilingualism. Through SAMI, we facilitate jailbreaking on
HarmBench (+72.7%) by diminishing "safety" and improve performance on the GSM8K
benchmark (+1.6%) by amplifying "reasoning". Lastly, we highlight the
domain-agnostic nature of our approach by suppressing the image classification
accuracy of vision transformers on ImageNet.

### 28. TeXpert: A Multi-Level Benchmark for Evaluating LaTeX Code Generation by LLMs

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Sahil Kale, Vijaykant Nadadur
- **URL**: <http://arxiv.org/abs/2506.16990v1>
- **Submitted**: 2025-06-20 13:39:16
- **Comment**: Accepted to the SDProc Workshop @ ACL 2025
- **Topic Keywords**: search
- **Reason**: The paper focuses on evaluating the ability of Large Language Models (LLMs) to generate LaTeX code, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on the topic of natural language processing, it is not a central match for the user's research interests.

#### Abstract
> LaTeX's precision and flexibility in typesetting have made it the gold
standard for the preparation of scientific documentation. Large Language Models
(LLMs) present a promising opportunity for researchers to produce
publication-ready material using LaTeX with natural language instructions, yet
current benchmarks completely lack evaluation of this ability. By introducing
TeXpert, our benchmark dataset with natural language prompts for generating
LaTeX code focused on components of scientific documents across multiple
difficulty levels, we conduct an in-depth analysis of LLM performance in this
regard and identify frequent error types. Our evaluation across open and
closed-source LLMs highlights multiple key findings: LLMs excelling on standard
benchmarks perform poorly in LaTeX generation with a significant accuracy
drop-off as the complexity of tasks increases; open-source models like DeepSeek
v3 and DeepSeek Coder strongly rival closed-source counterparts in LaTeX tasks;
and formatting and package errors are unexpectedly prevalent, suggesting a lack
of diverse LaTeX examples in the training datasets of most LLMs. Our dataset,
code, and model evaluations are available at
https://github.com/knowledge-verse-ai/TeXpert.

### 29. From Prompts to Constructs: A Dual-Validity Framework for LLM Research in Psychology

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Zhicheng Lin
- **URL**: <http://arxiv.org/abs/2506.16697v1>
- **Submitted**: 2025-06-20 02:38:42
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on the application of Large Language Models (LLMs) in psychology, discussing measurement and causal inference, which is outside your primary research areas.

#### Abstract
> Large language models (LLMs) are rapidly being adopted across psychology,
serving as research tools, experimental subjects, human simulators, and
computational models of cognition. However, the application of human
measurement tools to these systems can produce contradictory results, raising
concerns that many findings are measurement phantoms--statistical artifacts
rather than genuine psychological phenomena. In this Perspective, we argue that
building a robust science of AI psychology requires integrating two of our
field's foundational pillars: the principles of reliable measurement and the
standards for sound causal inference. We present a dual-validity framework to
guide this integration, which clarifies how the evidence needed to support a
claim scales with its scientific ambition. Using an LLM to classify text may
require only basic accuracy checks, whereas claiming it can simulate anxiety
demands a far more rigorous validation process. Current practice systematically
fails to meet these requirements, often treating statistical pattern matching
as evidence of psychological phenomena. The same model output--endorsing "I am
anxious"--requires different validation strategies depending on whether
researchers claim to measure, characterize, simulate, or model psychological
constructs. Moving forward requires developing computational analogues of
psychological constructs and establishing clear, scalable standards of evidence
rather than the uncritical application of human measurement tools.

### 30. Mechanisms vs. Outcomes: Probing for Syntax Fails to Explain Performance on Targeted Syntactic Evaluations

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Ananth Agarwal, Jasper Jian, Christopher D. Manning, Shikhar Murty
- **URL**: <http://arxiv.org/abs/2506.16678v1>
- **Submitted**: 2025-06-20 01:46:50
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests as it focuses on the internal mechanisms of large language models and their ability to represent syntactic structure, which is not directly related to information retrieval, search technologies, or query understanding. The paper's emphasis on probing and syntactic features extraction is also not aligned with your interests in ranking models, user behavior modeling, or real-time relevance optimization.

#### Abstract
> Large Language Models (LLMs) exhibit a robust mastery of syntax when
processing and generating text. While this suggests internalized understanding
of hierarchical syntax and dependency relations, the precise mechanism by which
they represent syntactic structure is an open area within interpretability
research. Probing provides one way to identify the mechanism of syntax being
linearly encoded in activations, however, no comprehensive study has yet
established whether a model's probing accuracy reliably predicts its downstream
syntactic performance. Adopting a "mechanisms vs. outcomes" framework, we
evaluate 32 open-weight transformer models and find that syntactic features
extracted via probing fail to predict outcomes of targeted syntax evaluations
across English linguistic phenomena. Our results highlight a substantial
disconnect between latent syntactic representations found via probing and
observable syntactic behaviors in downstream tasks.

### 31. Advancing Harmful Content Detection in Organizational Research: Integrating Large Language Models with Elo Rating System

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Mustafa Akben, Aaron Satko
- **URL**: <http://arxiv.org/abs/2506.16575v1>
- **Submitted**: 2025-06-19 20:01:12
- **Comment**: Submitted for HICSS 2025 (Hawaii International Conference on System
  Sciences); under review
- **Topic Keywords**: search
- **Reason**: The paper focuses on harmful content detection in organizational research, using large language models and the Elo rating system. While it touches on NLP and machine learning, it does not directly relate to information retrieval, search technologies, or query understanding, which are the user's primary research interests.

#### Abstract
> Large language models (LLMs) offer promising opportunities for organizational
research. However, their built-in moderation systems can create problems when
researchers try to analyze harmful content, often refusing to follow certain
instructions or producing overly cautious responses that undermine validity of
the results. This is particularly problematic when analyzing organizational
conflicts such as microaggressions or hate speech. This paper introduces an Elo
rating-based method that significantly improves LLM performance for harmful
content analysis In two datasets, one focused on microaggression detection and
the other on hate speech, we find that our method outperforms traditional LLM
prompting techniques and conventional machine learning models on key measures
such as accuracy, precision, and F1 scores. Advantages include better
reliability when analyzing harmful content, fewer false positives, and greater
scalability for large-scale datasets. This approach supports organizational
applications, including detecting workplace harassment, assessing toxic
communication, and fostering safer and more inclusive work environments.

### 32. Weight Factorization and Centralization for Continual Learning in Speech Recognition

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Enes Yavuz Ugan, Ngoc-Quan Pham, Alexander Waibel
- **URL**: <http://arxiv.org/abs/2506.16574v1>
- **Submitted**: 2025-06-19 19:59:24
- **Comment**: Accepted to INTERSPEECH 2025
- **Topic Keywords**: rank
- **Reason**: This paper focuses on continual learning in speech recognition, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's abstract does not mention query understanding, ranking models, or user behavior modeling, and the topics of data mining and recommender systems are not relevant to the paper's content.

#### Abstract
> Modern neural network based speech recognition models are required to
continually absorb new data without re-training the whole system, especially in
downstream applications using foundation models, having no access to the
original training data. Continually training the models in a rehearsal-free,
multilingual, and language agnostic condition, likely leads to catastrophic
forgetting, when a seemingly insignificant disruption to the weights can
destructively harm the quality of the models. Inspired by the ability of human
brains to learn and consolidate knowledge through the waking-sleeping cycle, we
propose a continual learning approach with two distinct phases: factorization
and centralization, learning and merging knowledge accordingly. Our experiments
on a sequence of varied code-switching datasets showed that the centralization
stage can effectively prevent catastrophic forgetting by accumulating the
knowledge in multiple scattering low-rank adapters.

### 33. Automatic Speech Recognition Biases in Newcastle English: an Error Analysis

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Dana Serditova, Kevin Tang, Jochen Steffens
- **URL**: <http://arxiv.org/abs/2506.16558v1>
- **Submitted**: 2025-06-19 19:24:12
- **Comment**: Submitted to Interspeech 2025
- **Topic Keywords**: search
- **Reason**: This paper focuses on Automatic Speech Recognition (ASR) biases in regional dialects, which is not directly related to Information Retrieval (IR) or Search technologies. The paper's emphasis on phonological, lexical, and morphosyntactic errors in ASR is also outside the scope of query understanding, ranking models, and user behavior modeling, which are core areas of interest in IR.

#### Abstract
> Automatic Speech Recognition (ASR) systems struggle with regional dialects
due to biased training which favours mainstream varieties. While previous
research has identified racial, age, and gender biases in ASR, regional bias
remains underexamined. This study investigates ASR performance on Newcastle
English, a well-documented regional dialect known to be challenging for ASR. A
two-stage analysis was conducted: first, a manual error analysis on a subsample
identified key phonological, lexical, and morphosyntactic errors behind ASR
misrecognitions; second, a case study focused on the systematic analysis of ASR
recognition of the regional pronouns ``yous'' and ``wor''. Results show that
ASR errors directly correlate with regional dialectal features, while social
factors play a lesser role in ASR mismatches. We advocate for greater dialectal
diversity in ASR training data and highlight the value of sociolinguistic
analysis in diagnosing and addressing regional biases.

---

