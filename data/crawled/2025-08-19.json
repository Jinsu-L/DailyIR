[
    {
        "title": "RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns",
        "abstract": "Detecting content generated by large language models (LLMs) is crucial for\npreventing misuse and building trustworthy AI systems. Although existing\ndetection methods perform well, their robustness in out-of-distribution (OOD)\nscenarios is still lacking. In this paper, we hypothesize that, compared to\nfeatures used by existing detection methods, the internal representations of\nLLMs contain more comprehensive and raw features that can more effectively\ncapture and distinguish the statistical pattern differences between\nLLM-generated texts (LGT) and human-written texts (HWT). We validated this\nhypothesis across different LLMs and observed significant differences in neural\nactivation patterns when processing these two types of texts. Based on this, we\npropose RepreGuard, an efficient statistics-based detection method.\nSpecifically, we first employ a surrogate model to collect representation of\nLGT and HWT, and extract the distinct activation feature that can better\nidentify LGT. We can classify the text by calculating the projection score of\nthe text representations along this feature direction and comparing with a\nprecomputed threshold. Experimental results show that RepreGuard outperforms\nall baselines with average 94.92% AUROC on both in-distribution (ID) and OOD\nscenarios, while also demonstrating robust resilience to various text sizes and\nmainstream attacks. Data and code are publicly available at:\nhttps://github.com/NLP2CT/RepreGuard",
        "url": "http://arxiv.org/abs/2508.13152v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13152v1",
        "arxiv_id": "2508.13152v1",
        "authors": [
            "Xin Chen",
            "Junchao Wu",
            "Shu Yang",
            "Runzhe Zhan",
            "Zeyu Wu",
            "Ziyang Luo",
            "Di Wang",
            "Min Yang",
            "Lidia S. Chao",
            "Derek F. Wong"
        ],
        "submitted": "2025-08-18 17:59:15",
        "source": "arxiv",
        "comment": "Accepted to TACL 2025. This version is a pre-MIT Press publication\n  version"
    },
    {
        "title": "Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation",
        "abstract": "Developing large language models is expensive and involves making decisions\nwith small experiments, typically by evaluating on large, multi-task evaluation\nsuites. In this work, we analyze specific properties which make a benchmark\nmore reliable for such decisions, and interventions to design higher-quality\nevaluation benchmarks. We introduce two key metrics that show differences in\ncurrent benchmarks: signal, a benchmark's ability to separate better models\nfrom worse models, and noise, a benchmark's sensitivity to random variability\nbetween training steps. We demonstrate that benchmarks with a better\nsignal-to-noise ratio are more reliable when making decisions at small scale,\nand those with less noise have lower scaling law prediction error. These\nresults suggest that improving signal or noise will lead to more useful\nbenchmarks, so we introduce three interventions designed to directly affect\nsignal or noise. For example, we propose that switching to a metric that has\nbetter signal and noise (e.g., perplexity rather than accuracy) leads to better\nreliability and improved scaling law error. We also find that filtering noisy\nsubtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable\nmulti-task evaluations. We also find that averaging the output of a model's\nintermediate checkpoints to reduce noise leads to consistent improvements. We\nconclude by recommending that those creating new benchmarks, or selecting which\nexisting benchmarks to use, aim for high signal and low noise. We use 30\nbenchmarks for these experiments, and 375 open-weight language models from 60M\nto 32B parameters, resulting in a new, publicly available dataset of 900K\nevaluation benchmark results, totaling 200M instances.",
        "url": "http://arxiv.org/abs/2508.13144v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13144v1",
        "arxiv_id": "2508.13144v1",
        "authors": [
            "David Heineman",
            "Valentin Hofmann",
            "Ian Magnusson",
            "Yuling Gu",
            "Noah A. Smith",
            "Hannaneh Hajishirzi",
            "Kyle Lo",
            "Jesse Dodge"
        ],
        "submitted": "2025-08-18 17:56:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Has GPT-5 Achieved Spatial Intelligence? An Empirical Study",
        "abstract": "Multi-modal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, which are fundamental capabilities to achieving\nartificial general intelligence. With the recent release of GPT-5, allegedly\nthe most powerful AI model to date, it is timely to examine where the leading\nmodels stand on the path toward spatial intelligence. First, we propose a\ncomprehensive taxonomy of spatial tasks that unifies existing benchmarks and\ndiscuss the challenges in ensuring fair evaluation. We then evaluate\nstate-of-the-art proprietary and open-source models on eight key benchmarks, at\na cost exceeding one billion total tokens. Our empirical study reveals that (1)\nGPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)\nstill falls short of human performance across a broad spectrum of tasks.\nMoreover, we (3) identify the more challenging spatial intelligence problems\nfor multi-modal models, and (4) proprietary models do not exhibit a decisive\nadvantage when facing the most difficult problems. In addition, we conduct a\nqualitative evaluation across a diverse set of scenarios that are intuitive for\nhumans yet fail even the most advanced multi-modal models.",
        "url": "http://arxiv.org/abs/2508.13142v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13142v1",
        "arxiv_id": "2508.13142v1",
        "authors": [
            "Zhongang Cai",
            "Yubo Wang",
            "Qingping Sun",
            "Ruisi Wang",
            "Chenyang Gu",
            "Wanqi Yin",
            "Zhiqian Lin",
            "Zhitao Yang",
            "Chen Wei",
            "Xuanke Shi",
            "Kewang Deng",
            "Xiaoyang Han",
            "Zukai Chen",
            "Jiaqi Li",
            "Xiangyu Fan",
            "Hanming Deng",
            "Lewei Lu",
            "Bo Li",
            "Ziwei Liu",
            "Quan Wang",
            "Dahua Lin",
            "Lei Yang"
        ],
        "submitted": "2025-08-18 17:55:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "OptimalThinkingBench: Evaluating Over and Underthinking in LLMs",
        "abstract": "Thinking LLMs solve complex tasks at the expense of increased compute and\noverthinking on simpler problems, while non-thinking LLMs are faster and\ncheaper but underthink on harder reasoning problems. This has led to the\ndevelopment of separate thinking and non-thinking LLM variants, leaving the\nonus of selecting the optimal model for each query on the end user. In this\nwork, we introduce OptimalThinkingBench, a unified benchmark that jointly\nevaluates overthinking and underthinking in LLMs and also encourages the\ndevelopment of optimally-thinking models that balance performance and\nefficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench,\nfeaturing simple queries in 72 domains, and UnderthinkingBench, containing 11\nchallenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, we\nperform extensive evaluation of 33 different thinking and non-thinking models\nand show that no model is able to optimally think on our benchmark. Thinking\nmodels often overthink for hundreds of tokens on the simplest user queries\nwithout improving performance. In contrast, large non-thinking models\nunderthink, often falling short of much smaller thinking models. We further\nexplore several methods to encourage optimal thinking, but find that these\napproaches often improve on one sub-benchmark at the expense of the other,\nhighlighting the need for better unified and optimal models in the future.",
        "url": "http://arxiv.org/abs/2508.13141v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13141v1",
        "arxiv_id": "2508.13141v1",
        "authors": [
            "Pranjal Aggarwal",
            "Seungone Kim",
            "Jack Lanchantin",
            "Sean Welleck",
            "Jason Weston",
            "Ilia Kulikov",
            "Swarnadeep Saha"
        ],
        "submitted": "2025-08-18 17:53:10",
        "source": "arxiv",
        "comment": "26 pages, 6 tables, 10 figures"
    },
    {
        "title": "Improving Detection of Watermarked Language Models",
        "abstract": "Watermarking has recently emerged as an effective strategy for detecting the\ngenerations of large language models (LLMs). The strength of a watermark\ntypically depends strongly on the entropy afforded by the language model and\nthe set of input prompts. However, entropy can be quite limited in practice,\nespecially for models that are post-trained, for example via instruction tuning\nor reinforcement learning from human feedback (RLHF), which makes detection\nbased on watermarking alone challenging. In this work, we investigate whether\ndetection can be improved by combining watermark detectors with non-watermark\nones. We explore a number of hybrid schemes that combine the two, observing\nperformance gains over either class of detector under a wide range of\nexperimental conditions.",
        "url": "http://arxiv.org/abs/2508.13131v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13131v1",
        "arxiv_id": "2508.13131v1",
        "authors": [
            "Dara Bahri",
            "John Wieting"
        ],
        "submitted": "2025-08-18 17:43:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MuDRiC: Multi-Dialect Reasoning for Arabic Commonsense Validation",
        "abstract": "Commonsense validation evaluates whether a sentence aligns with everyday\nhuman understanding, a critical capability for developing robust natural\nlanguage understanding systems. While substantial progress has been made in\nEnglish, the task remains underexplored in Arabic, particularly given its rich\nlinguistic diversity. Existing Arabic resources have primarily focused on\nModern Standard Arabic (MSA), leaving regional dialects underrepresented\ndespite their prevalence in spoken contexts. To bridge this gap, we present two\nkey contributions: (i) we introduce MuDRiC, an extended Arabic commonsense\ndataset incorporating multiple dialects, and (ii) a novel method adapting Graph\nConvolutional Networks (GCNs) to Arabic commonsense reasoning, which enhances\nsemantic relationship modeling for improved commonsense validation. Our\nexperimental results demonstrate that this approach achieves superior\nperformance in Arabic commonsense validation. Our work enhances Arabic natural\nlanguage understanding by providing both a foundational dataset and a novel\nmethod for handling its complex variations. To the best of our knowledge, we\nrelease the first Arabic multi-dialect commonsense reasoning dataset.",
        "url": "http://arxiv.org/abs/2508.13130v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13130v1",
        "arxiv_id": "2508.13130v1",
        "authors": [
            "Kareem Elozeiri",
            "Mervat Abassy",
            "Preslav Nakov",
            "Yuxia Wang"
        ],
        "submitted": "2025-08-18 17:42:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries",
        "abstract": "Abstractive summarization is a core application in contact centers, where\nLarge Language Models (LLMs) generate millions of summaries of call transcripts\ndaily. Despite their apparent quality, it remains unclear whether LLMs\nsystematically under- or over-attend to specific aspects of the transcript,\npotentially introducing biases in the generated summary. While prior work has\nexamined social and positional biases, the specific forms of bias pertinent to\ncontact center operations - which we term Operational Bias - have remained\nunexplored. To address this gap, we introduce BlindSpot, a framework built upon\na taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic)\nfor the identification and quantification of these biases. BlindSpot leverages\nan LLM as a zero-shot classifier to derive categorical distributions for each\nbias dimension in a pair of transcript and its summary. The bias is then\nquantified using two metrics: Fidelity Gap (the JS Divergence between\ndistributions) and Coverage (the percentage of source labels omitted). Using\nBlindSpot, we conducted an empirical study with 2500 real call transcripts and\ntheir summaries generated by 20 LLMs of varying scales and families (e.g., GPT,\nLlama, Claude). Our analysis reveals that biases are systemic and present\nacross all evaluated models, regardless of size or family.",
        "url": "http://arxiv.org/abs/2508.13124v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13124v1",
        "arxiv_id": "2508.13124v1",
        "authors": [
            "Kawin Mayilvaghanan",
            "Siddhant Gupta",
            "Ayush Kumar"
        ],
        "submitted": "2025-08-18 17:31:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation",
        "abstract": "Incident response (IR) requires fast, coordinated, and well-informed\ndecision-making to contain and mitigate cyber threats. While large language\nmodels (LLMs) have shown promise as autonomous agents in simulated IR settings,\ntheir reasoning is often limited by a lack of access to external knowledge. In\nthis work, we present AutoBnB-RAG, an extension of the AutoBnB framework that\nincorporates retrieval-augmented generation (RAG) into multi-agent incident\nresponse simulations. Built on the Backdoors & Breaches (B&B) tabletop game\nenvironment, AutoBnB-RAG enables agents to issue retrieval queries and\nincorporate external evidence during collaborative investigations. We introduce\ntwo retrieval settings: one grounded in curated technical documentation\n(RAG-Wiki), and another using narrative-style incident reports (RAG-News). We\nevaluate performance across eight team structures, including newly introduced\nargumentative configurations designed to promote critical reasoning. To\nvalidate practical utility, we also simulate real-world cyber incidents based\non public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct\ncomplex multi-stage attacks. Our results show that retrieval augmentation\nimproves decision quality and success rates across diverse organizational\nmodels. This work demonstrates the value of integrating retrieval mechanisms\ninto LLM-based multi-agent systems for cybersecurity decision-making.",
        "url": "http://arxiv.org/abs/2508.13118v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13118v1",
        "arxiv_id": "2508.13118v1",
        "authors": [
            "Zefang Liu",
            "Arman Anwar"
        ],
        "submitted": "2025-08-18 17:22:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "All for law and law for all: Adaptive RAG Pipeline for Legal Research",
        "abstract": "Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding\nlarge language model outputs in cited sources, a capability that is especially\ncritical in the legal domain. We present an end-to-end RAG pipeline that\nrevisits and extends the LegalBenchRAG baseline with three targeted\nenhancements: (i) a context-aware query translator that disentangles document\nreferences from natural-language questions and adapts retrieval depth and\nresponse style based on expertise and specificity, (ii) open-source retrieval\nstrategies using SBERT and GTE embeddings that achieve substantial performance\ngains (improving Recall@K by 30-95\\% and Precision@K by $\\sim$2.5$\\times$ for\n$K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and\ngeneration framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to\nassess semantic alignment and faithfulness across models and prompt designs.\nOur results show that carefully designed open-source pipelines can rival or\noutperform proprietary approaches in retrieval quality, while a custom\nlegal-grounded prompt consistently produces more faithful and contextually\nrelevant answers than baseline prompting. Taken together, these contributions\ndemonstrate the potential of task-aware, component-level tuning to deliver\nlegally grounded, reproducible, and cost-effective RAG systems for legal\nresearch assistance.",
        "url": "http://arxiv.org/abs/2508.13107v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13107v1",
        "arxiv_id": "2508.13107v1",
        "authors": [
            "Figarri Keisha",
            "Prince Singh",
            "Pallavi",
            "Dion Fernandes",
            "Aravindh Manivannan",
            "Ilham Wicaksono",
            "Faisal Ahmad"
        ],
        "submitted": "2025-08-18 17:14:03",
        "source": "arxiv",
        "comment": "submitted to NLLP 2025 Workshop"
    },
    {
        "title": "DocHPLT: A Massively Multilingual Document-Level Translation Dataset",
        "abstract": "Existing document-level machine translation resources are only available for\na handful of languages, mostly high-resourced ones. To facilitate the training\nand evaluation of document-level translation and, more broadly, long-context\nmodeling for global communities, we create DocHPLT, the largest publicly\navailable document-level translation dataset to date. It contains 124 million\naligned document pairs across 50 languages paired with English, comprising 4.26\nbillion sentences, with further possibility to provide 2500 bonus pairs not\ninvolving English. Unlike previous reconstruction-based approaches that piece\ntogether documents from sentence-level data, we modify an existing web\nextraction pipeline to preserve complete document integrity from the source,\nretaining all content including unaligned portions. After our preliminary\nexperiments identify the optimal training context strategy for document-level\ntranslation, we demonstrate that LLMs fine-tuned on DocHPLT substantially\noutperform off-the-shelf instruction-tuned baselines, with particularly\ndramatic improvements for under-resourced languages. We open-source the dataset\nunder a permissive license, providing essential infrastructure for advancing\nmultilingual document-level translation.",
        "url": "http://arxiv.org/abs/2508.13079v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13079v1",
        "arxiv_id": "2508.13079v1",
        "authors": [
            "Dayyán O'Brien",
            "Bhavitvya Malik",
            "Ona de Gibert",
            "Pinzhen Chen",
            "Barry Haddow",
            "Jörg Tiedemann"
        ],
        "submitted": "2025-08-18 16:52:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Reinforced Context Order Recovery for Adaptive Reasoning and Planning",
        "abstract": "Modern causal language models, followed by rapid developments in discrete\ndiffusion models, can now produce a wide variety of interesting and useful\ncontent. However, these families of models are predominantly trained to output\ntokens with a fixed (left-to-right) or random order, which may deviate from the\nlogical order in which tokens are generated originally. In this paper, we\nobserve that current causal and diffusion models encounter difficulties in\nproblems that require adaptive token generation orders to solve tractably,\nwhich we characterize with the $\\mathcal{V}$-information framework. Motivated\nby this, we propose Reinforced Context Order Recovery (ReCOR), a\nreinforcement-learning-based framework to extract adaptive, data-dependent\ntoken generation orders from text data without annotations. Self-supervised by\ntoken prediction statistics, ReCOR estimates the hardness of predicting every\nunfilled token and adaptively selects the next token during both training and\ninference. Experiments on challenging reasoning and planning datasets\ndemonstrate the superior performance of ReCOR compared with baselines,\nsometimes outperforming oracle models supervised with the ground-truth order.",
        "url": "http://arxiv.org/abs/2508.13070v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13070v1",
        "arxiv_id": "2508.13070v1",
        "authors": [
            "Long Ma",
            "Fangwei Zhong",
            "Yizhou Wang"
        ],
        "submitted": "2025-08-18 16:42:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Is This News Still Interesting to You?: Lifetime-aware Interest Matching for News Recommendation",
        "abstract": "Personalized news recommendation aims to deliver news articles aligned with\nusers' interests, serving as a key solution to alleviate the problem of\ninformation overload on online news platforms. While prior work has improved\ninterest matching through refined representations of news and users, the\nfollowing time-related challenges remain underexplored: (C1) leveraging the age\nof clicked news to infer users' interest persistence, and (C2) modeling the\nvarying lifetime of news across topics and users. To jointly address these\nchallenges, we propose a novel Lifetime-aware Interest Matching framework for\nnEws recommendation, named LIME, which incorporates three key strategies: (1)\nUser-Topic lifetime-aware age representation to capture the relative age of\nnews with respect to a user-topic pair, (2) Candidate-aware lifetime attention\nfor generating temporally aligned user representation, and (3) Freshness-guided\ninterest refinement for prioritizing valid candidate news at prediction time.\nExtensive experiments on two real-world datasets demonstrate that LIME\nconsistently outperforms a wide range of state-of-the-art news recommendation\nmethods, and its model agnostic strategies significantly improve recommendation\naccuracy.",
        "url": "http://arxiv.org/abs/2508.13064v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13064v1",
        "arxiv_id": "2508.13064v1",
        "authors": [
            "Seongeun Ryu",
            "Yunyong Ko",
            "Sang-Wook Kim"
        ],
        "submitted": "2025-08-18 16:36:27",
        "source": "arxiv",
        "comment": "10 pages, 7 figures, 4 tables, accepted at ACM International\n  Conference on Information and Knowledge Management (CIKM)"
    },
    {
        "title": "Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX using a Speech Error Database",
        "abstract": "The Simon Fraser University Speech Error Database (SFUSED) is a public data\ncollection developed for linguistic and psycholinguistic research. Here we\ndemonstrate how its design and annotations can be used to test and evaluate\nspeech recognition models. The database comprises systematically annotated\nspeech errors from spontaneous English speech, with each error tagged for\nintended and actual error productions. The annotation schema incorporates\nmultiple classificatory dimensions that are of some value to model assessment,\nincluding linguistic hierarchical level, contextual sensitivity, degraded\nwords, word corrections, and both word-level and syllable-level error\npositioning. To assess the value of these classificatory variables, we\nevaluated the transcription accuracy of WhisperX across 5,300 documented word\nand phonological errors. This analysis demonstrates the atabase's effectiveness\nas a diagnostic tool for ASR system performance.",
        "url": "http://arxiv.org/abs/2508.13060v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13060v1",
        "arxiv_id": "2508.13060v1",
        "authors": [
            "John Alderete",
            "Macarious Kin Fung Hui",
            "Aanchan Mohan"
        ],
        "submitted": "2025-08-18 16:30:33",
        "source": "arxiv",
        "comment": "5 pages, 6 figures, 1 table, Interspeech 2025 (Rotterdam)"
    },
    {
        "title": "Doğal Dil İşlemede Tokenizasyon Standartları ve Ölçümü: Türkçe Üzerinden Büyük Dil Modellerinin Karşılaştırmalı Analizi",
        "abstract": "Tokenization is a fundamental preprocessing step in Natural Language\nProcessing (NLP), significantly impacting the capability of large language\nmodels (LLMs) to capture linguistic and semantic nuances. This study introduces\na novel evaluation framework addressing tokenization challenges specific to\nmorphologically-rich and low-resource languages such as Turkish. Utilizing the\nTurkish MMLU (TR-MMLU) dataset, comprising 6,200 multiple-choice questions from\nthe Turkish education system, we assessed tokenizers based on vocabulary size,\ntoken count, processing time, language-specific token percentages (\\%TR), and\ntoken purity (\\%Pure). These newly proposed metrics measure how effectively\ntokenizers preserve linguistic structures. Our analysis reveals that\nlanguage-specific token percentages exhibit a stronger correlation with\ndownstream performance (e.g., MMLU scores) than token purity. Furthermore,\nincreasing model parameters alone does not necessarily enhance linguistic\nperformance, underscoring the importance of tailored, language-specific\ntokenization methods. The proposed framework establishes robust and practical\ntokenization standards for morphologically complex languages.",
        "url": "http://arxiv.org/abs/2508.13058v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13058v1",
        "arxiv_id": "2508.13058v1",
        "authors": [
            "M. Ali Bayram",
            "Ali Arda Fincan",
            "Ahmet Semih Gümüş",
            "Sercan Karakaş",
            "Banu Diri",
            "Savaş Yıldırım"
        ],
        "submitted": "2025-08-18 16:26:42",
        "source": "arxiv",
        "comment": "in Turkish language, Presented at the 2025 33rd Signal Processing and\n  Communications Applications Conference (SIU), 25--28 June 2025, \\c{S}ile,\n  Istanbul, T\\\"urkiye"
    },
    {
        "title": "Büyük Dil Modelleri için TR-MMLU Benchmarkı: Performans Değerlendirmesi, Zorluklar ve İyileştirme Fırsatları",
        "abstract": "Language models have made significant advancements in understanding and\ngenerating human language, achieving remarkable success in various\napplications. However, evaluating these models remains a challenge,\nparticularly for resource-limited languages like Turkish. To address this\nissue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive\nevaluation framework designed to assess the linguistic and conceptual\ncapabilities of large language models (LLMs) in Turkish. TR-MMLU is based on a\nmeticulously curated dataset comprising 6,200 multiple-choice questions across\n62 sections within the Turkish education system. This benchmark provides a\nstandard framework for Turkish NLP research, enabling detailed analyses of\nLLMs' capabilities in processing Turkish text. In this study, we evaluated\nstate-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in model\ndesign. TR-MMLU sets a new standard for advancing Turkish NLP research and\ninspiring future innovations.",
        "url": "http://arxiv.org/abs/2508.13044v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13044v1",
        "arxiv_id": "2508.13044v1",
        "authors": [
            "M. Ali Bayram",
            "Ali Arda Fincan",
            "Ahmet Semih Gümüş",
            "Banu Diri",
            "Savaş Yıldırım",
            "Öner Aytaş"
        ],
        "submitted": "2025-08-18 16:00:43",
        "source": "arxiv",
        "comment": "10 pages, in Turkish language, 5 figures. Presented at the 2025 33rd\n  Signal Processing and Communications Applications Conference (SIU), 25--28\n  June 2025, Sile, Istanbul, T\\\"urkiye"
    },
    {
        "title": "Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction",
        "abstract": "Recent studies have demonstrated that Large Language Models (LLMs) have\nstrong mathematical reasoning abilities but rely on hundreds of billions of\nparameters. To tackle the challenge of poor reasoning in Small Language Models\n(SLMs), existing methods typically leverage LLMs to generate massive amounts of\ndata for cramming training. In psychology, they are akin to System 1 thinking,\nwhich resolves reasoning problems rapidly based on experience and intuition.\nHowever, human learning also requires System 2 thinking, where knowledge is\nfirst acquired and then reinforced through practice. Inspired by such two\ndistinct modes of thinking, we propose a novel method based on the multi-LoRA\nInteraction for mathematical reasoning Distillation (LoRID). First, we input\nthe question and reasoning of each sample into an LLM to create\nknowledge-enhanced datasets. Subsequently, we train a LoRA block on the student\nmodel as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts\nfor problem-solving. Then, to imitate System 2 thinking, we train the Knowledge\nGenerator (KG) and Deep Reasoner (DR), respectively. The former outputs only\nknowledge after receiving problems, while the latter uses that knowledge to\nperform reasoning. Finally, to address the randomness in the generation of IR\nand DR, we evaluate whether their outputs are consistent, and the inference\nprocess needs to be iterated if not. This step can enhance the mathematical\nreasoning ability of SLMs through mutual feedback. Experimental results show\nthat LoRID achieves state-of-the-art performance, especially on the GSM8K\ndataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,\n12.3%, and 1.8% accuracy across the five base models, respectively.",
        "url": "http://arxiv.org/abs/2508.13037v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13037v1",
        "arxiv_id": "2508.13037v1",
        "authors": [
            "Xinhe Li",
            "Jiajun Liu",
            "Peng Wang"
        ],
        "submitted": "2025-08-18 15:56:10",
        "source": "arxiv",
        "comment": "Accepted by IJCAI2025"
    },
    {
        "title": "D-RDW: Diversity-Driven Random Walks for News Recommender Systems",
        "abstract": "This paper introduces Diversity-Driven RandomWalks (D-RDW), a lightweight\nalgorithm and re-ranking technique that generates diverse news recommendations.\nD-RDW is a societal recommender, which combines the diversification\ncapabilities of the traditional random walk algorithms with customizable target\ndistributions of news article properties. In doing so, our model provides a\ntransparent approach for editors to incorporate norms and values into the\nrecommendation process. D-RDW shows enhanced performance across key diversity\nmetrics that consider the articles' sentiment and political party mentions when\ncompared to state-of-the-art neural models. Furthermore, D-RDW proves to be\nmore computationally efficient than existing approaches.",
        "url": "http://arxiv.org/abs/2508.13035v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13035v1",
        "arxiv_id": "2508.13035v1",
        "authors": [
            "Runze Li",
            "Lucien Heitz",
            "Oana Inel",
            "Abraham Bernstein"
        ],
        "submitted": "2025-08-18 15:53:30",
        "source": "arxiv",
        "comment": "6 pages"
    },
    {
        "title": "Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis",
        "abstract": "Sarcastic speech synthesis, which involves generating speech that effectively\nconveys sarcasm, is essential for enhancing natural interactions in\napplications such as entertainment and human-computer interaction. However,\nsynthesizing sarcastic speech remains a challenge due to the nuanced prosody\nthat characterizes sarcasm, as well as the limited availability of annotated\nsarcastic speech data. To address these challenges, this study introduces a\nnovel approach that integrates feedback loss from a bi-modal sarcasm detection\nmodel into the TTS training process, enhancing the model's ability to capture\nand convey sarcasm. In addition, by leveraging transfer learning, a speech\nsynthesis model pre-trained on read speech undergoes a two-stage fine-tuning\nprocess. First, it is fine-tuned on a diverse dataset encompassing various\nspeech styles, including sarcastic speech. In the second stage, the model is\nfurther refined using a dataset focused specifically on sarcastic speech,\nenhancing its ability to generate sarcasm-aware speech. Objective and\nsubjective evaluations demonstrate that our proposed methods improve the\nquality, naturalness, and sarcasm-awareness of synthesized speech.",
        "url": "http://arxiv.org/abs/2508.13028v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13028v1",
        "arxiv_id": "2508.13028v1",
        "authors": [
            "Zhu Li",
            "Yuqing Zhang",
            "Xiyuan Gao",
            "Devraj Raghuvanshi",
            "Nagendra Kumar",
            "Shekhar Nayak",
            "Matt Coler"
        ],
        "submitted": "2025-08-18 15:44:54",
        "source": "arxiv",
        "comment": "Speech Synthesis Workshop 2025"
    },
    {
        "title": "WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents",
        "abstract": "LLM-based web agents have the potential to automate long-running web tasks,\nsuch as finding offers for specific products in multiple online shops and\nsubsequently ordering the cheapest products that meet the users needs. This\npaper introduces WebMall, a multi-shop online shopping benchmark for evaluating\nthe effectiveness and efficiency of web agents for comparison-shopping. WebMall\nconsists of four simulated online shops populated with authentic product offers\nsourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These\ntasks include basic tasks such as finding specific products in multiple shops,\nperforming price comparisons, adding items to the shopping cart, and completing\ncheckout. Advanced tasks involve searching for products based on vague\nrequirements, identifying suitable substitutes, and finding compatible\nproducts. Compared to existing e-commerce benchmarks, such as WebShop or\nShoppingBench, WebMall introduces comparison-shopping tasks across multiple\nshops. Furthermore, the product offers are more heterogeneous, as they\noriginate from hundreds of distinct real-world shops. The tasks in WebMall\nrequire longer interaction trajectories than those in WebShop, while remaining\nrepresentative of real-world shopping behaviors. We evaluate eight baseline\nagents on WebMall, varying in observation modality, memory utilization, and\nunderlying large language model (GPT 4.1 and Claude Sonnet 4). The\nbest-performing configurations achieve completion rates of 75% and 53%, and F1\nscores of 87% and 63%, on the basic and advanced task sets, respectively.\nWebMall is publicly released to facilitate research on web agents and to\npromote advancements in navigation, reasoning, and efficiency within e-commerce\nscenarios.",
        "url": "http://arxiv.org/abs/2508.13024v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13024v1",
        "arxiv_id": "2508.13024v1",
        "authors": [
            "Ralph Peeters",
            "Aaron Steiner",
            "Luca Schwarz",
            "Julian Yuya Caspary",
            "Christian Bizer"
        ],
        "submitted": "2025-08-18 15:41:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models",
        "abstract": "Recent advances in masked diffusion models (MDMs) have established them as\npowerful non-autoregressive alternatives for sequence generation. Nevertheless,\nour preliminary experiments reveal that the generation quality of MDMs is still\nhighly sensitive to the choice of decoding strategy. In particular, widely\nadopted uncertainty-based samplers suffer from two key limitations: a lack of\nglobal trajectory control and a pronounced bias toward trivial tokens in the\nearly stages of decoding. These shortcomings restrict the full potential of\nMDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling\n(PC-Sampler), a novel decoding strategy that unifies global trajectory planning\nwith content-aware informativeness maximization. PC-Sampler incorporates a\nposition-aware weighting mechanism to regulate the decoding path and a\ncalibrated confidence score to suppress the premature selection of trivial\ntokens. Extensive experiments on three advanced MDMs across seven challenging\nbenchmarks-including logical reasoning and planning tasks-demonstrate that\nPC-Sampler consistently outperforms existing MDM decoding strategies by more\nthan 10% on average, significantly narrowing the performance gap with\nstate-of-the-art autoregressive models. All codes are available at\nhttps://github.com/NEUIR/PC-Sampler.",
        "url": "http://arxiv.org/abs/2508.13021v2",
        "pdf_url": "http://arxiv.org/pdf/2508.13021v2",
        "arxiv_id": "2508.13021v2",
        "authors": [
            "Pengcheng Huang",
            "Shuhao Liu",
            "Zhenghao Liu",
            "Yukun Yan",
            "Shuo Wang",
            "Zulong Chen",
            "Tong Xiao"
        ],
        "submitted": "2025-08-18 15:38:37",
        "source": "arxiv",
        "comment": "17 pages,13 figures"
    },
    {
        "title": "Informfully Recommenders -- Reproducibility Framework for Diversity-aware Intra-session Recommendations",
        "abstract": "Norm-aware recommender systems have gained increased attention, especially\nfor diversity optimization. The recommender systems community has\nwell-established experimentation pipelines that support reproducible\nevaluations by facilitating models' benchmarking and comparisons against\nstate-of-the-art methods. However, to the best of our knowledge, there is\ncurrently no reproducibility framework to support thorough norm-driven\nexperimentation at the pre-processing, in-processing, post-processing, and\nevaluation stages of the recommender pipeline. To address this gap, we present\nInformfully Recommenders, a first step towards a normative reproducibility\nframework that focuses on diversity-aware design built on Cornac. Our extension\nprovides an end-to-end solution for implementing and experimenting with\nnormative and general-purpose diverse recommender systems that cover 1) dataset\npre-processing, 2) diversity-optimized models, 3) dedicated intrasession item\nre-ranking, and 4) an extensive set of diversity metrics. We demonstrate the\ncapabilities of our extension through an extensive offline experiment in the\nnews domain.",
        "url": "http://arxiv.org/abs/2508.13019v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13019v1",
        "arxiv_id": "2508.13019v1",
        "authors": [
            "Lucien Heitz",
            "Runze Li",
            "Oana Inel",
            "Abraham Bernstein"
        ],
        "submitted": "2025-08-18 15:37:41",
        "source": "arxiv",
        "comment": "10 pages"
    },
    {
        "title": "Analyzing Information Sharing and Coordination in Multi-Agent Planning",
        "abstract": "Multi-agent systems (MASs) have pushed the boundaries of large language model\n(LLM) agents in domains such as web research and software engineering. However,\nlong-horizon, multi-constraint planning tasks involve conditioning on detailed\ninformation and satisfying complex interdependent constraints, which can pose a\nchallenge for these systems. In this study, we construct an LLM-based MAS for a\ntravel planning task which is representative of these challenges. We evaluate\nthe impact of a notebook to facilitate information sharing, and evaluate an\norchestrator agent to improve coordination in free form conversation between\nagents. We find that the notebook reduces errors due to hallucinated details by\n18%, while an orchestrator directs the MAS to focus on and further reduce\nerrors by up to 13.5% within focused sub-areas. Combining both mechanisms\nachieves a 25% final pass rate on the TravelPlanner benchmark, a 17.5% absolute\nimprovement over the single-agent baseline's 7.5% pass rate. These results\nhighlight the potential of structured information sharing and reflective\norchestration as key components in MASs for long horizon planning with LLMs.",
        "url": "http://arxiv.org/abs/2508.12981v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12981v1",
        "arxiv_id": "2508.12981v1",
        "authors": [
            "Tianyue Ou",
            "Saujas Vaduguru",
            "Daniel Fried"
        ],
        "submitted": "2025-08-18 14:57:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information",
        "abstract": "In large language model-based agents, memory serves as a critical capability\nfor achieving personalization by storing and utilizing users' information.\nAlthough some previous studies have adopted memory to implement user\npersonalization, they typically focus on preference alignment and simple\nquestion-answering. However, in the real world, complex tasks often require\nmulti-hop reasoning on a large amount of user information, which poses\nsignificant challenges for current memory approaches. To address this\nlimitation, we propose the multi-hop personalized reasoning task to explore how\ndifferent memory mechanisms perform in multi-hop reasoning over personalized\ninformation. We explicitly define this task and construct a dataset along with\na unified evaluation framework. Then, we implement various explicit and\nimplicit memory methods and conduct comprehensive experiments. We evaluate\ntheir performance on this task from multiple perspectives and analyze their\nstrengths and weaknesses. Besides, we explore hybrid approaches that combine\nboth paradigms and propose the HybridMem method to address their limitations.\nWe demonstrate the effectiveness of our proposed model through extensive\nexperiments. To benefit the research community, we release this project at\nhttps://github.com/nuster1128/MPR.",
        "url": "http://arxiv.org/abs/2508.13250v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13250v1",
        "arxiv_id": "2508.13250v1",
        "authors": [
            "Zeyu Zhang",
            "Yang Zhang",
            "Haoran Tan",
            "Rui Li",
            "Xu Chen"
        ],
        "submitted": "2025-08-18 13:34:37",
        "source": "arxiv",
        "comment": "15 pages, 13 figures, 3 tables"
    },
    {
        "title": "SNAP-UQ: Self-supervised Next-Activation Prediction for Single-Pass Uncertainty in TinyML",
        "abstract": "We introduce \\textbf{SNAP-UQ}, a single-pass, label-free uncertainty method\nfor TinyML that estimates risk from \\emph{depth-wise next-activation\nprediction}: tiny int8 heads forecast the statistics of the next layer from a\ncompressed view of the previous one, and a lightweight monotone mapper turns\nthe resulting surprisal into an actionable score. The design requires no\ntemporal buffers, auxiliary exits, or repeated forward passes, and adds only a\nfew tens of kilobytes to MCU deployments. Across vision and audio backbones,\nSNAP-UQ consistently reduces flash and latency relative to early-exit and deep\nensembles (typically $\\sim$40--60\\% smaller and $\\sim$25--35\\% faster), with\ncompeting methods of similar accuracy often exceeding memory limits. In\ncorrupted streams it improves accuracy-drop detection by several AUPRC points\nand maintains strong failure detection (AUROC $\\approx$0.9) in a single pass.\nGrounding uncertainty in layer-to-layer dynamics yields a practical,\nresource-efficient basis for on-device monitoring in TinyML.",
        "url": "http://arxiv.org/abs/2508.12907v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12907v1",
        "arxiv_id": "2508.12907v1",
        "authors": [
            "Ismail Lamaakal",
            "Chaymae Yahyati",
            "Khalid El Makkaoui",
            "Ibrahim Ouahbi",
            "Yassine Maleh"
        ],
        "submitted": "2025-08-18 13:14:20",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TCUQ: Single-Pass Uncertainty Quantification from Temporal Consistency with Streaming Conformal Calibration for TinyML",
        "abstract": "We introduce TCUQ, a single pass, label free uncertainty monitor for\nstreaming TinyML that converts short horizon temporal consistency captured via\nlightweight signals on posteriors and features into a calibrated risk score\nwith an O(W ) ring buffer and O(1) per step updates. A streaming conformal\nlayer turns this score into a budgeted accept/abstain rule, yielding calibrated\nbehavior without online labels or extra forward passes. On microcontrollers,\nTCUQ fits comfortably on kilobyte scale devices and reduces footprint and\nlatency versus early exit and deep ensembles (typically about 50 to 60% smaller\nand about 30 to 45% faster), while methods of similar accuracy often run out of\nmemory. Under corrupted in distribution streams, TCUQ improves accuracy drop\ndetection by 3 to 7 AUPRC points and reaches up to 0.86 AUPRC at high\nseverities; for failure detection it attains up to 0.92 AUROC. These results\nshow that temporal consistency, coupled with streaming conformal calibration,\nprovides a practical and resource efficient foundation for on device monitoring\nin TinyML.",
        "url": "http://arxiv.org/abs/2508.12905v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12905v1",
        "arxiv_id": "2508.12905v1",
        "authors": [
            "Ismail Lamaakal",
            "Chaymae Yahyati",
            "Khalid El Makkaoui",
            "Ibrahim Ouahbi",
            "Yassine Maleh"
        ],
        "submitted": "2025-08-18 13:12:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models",
        "abstract": "Recent advances in self-refinement have demonstrated significant potential\nfor improving the outputs of large language models (LLMs) through iterative\nrefinement. However, most existing self-refinement methods rely on a reactive\nprocess with a fixed number of iterations, making it difficult to determine the\noptimal timing and content of refinement based on the evolving generation\ncontext. Inspired by the way humans dynamically refine their thoughts during\nexecution, we propose ProActive Self-Refinement (PASR), a novel method that\nenables LLMs to refine their outputs during the generation process. Unlike\nmethods that regenerate entire responses, PASR proactively decides whether,\nwhen, and how to refine based on the model's internal state and evolving\ncontext. We conduct extensive experiments on a diverse set of 10 tasks to\nevaluate the effectiveness of PASR. Experimental results show that PASR\nsignificantly enhances problem-solving performance. In particular, on Qwen3-8B,\nPASR reduces average token consumption by 41.6 percent compared to standard\ngeneration, while also achieving an 8.2 percent improvement in accuracy. Our\ncode and all baselines used in the paper are available in the GitHub.",
        "url": "http://arxiv.org/abs/2508.12903v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12903v1",
        "arxiv_id": "2508.12903v1",
        "authors": [
            "Jinyi Han",
            "Xinyi Wang",
            "Haiquan Zhao",
            "Tingyun li",
            "Zishang Jiang",
            "Sihang Jiang",
            "Jiaqing Liang",
            "Xin Lin",
            "Weikang Zhou",
            "Zeye Sun",
            "Fei Yu",
            "Yanghua Xiao"
        ],
        "submitted": "2025-08-18 13:07:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "An LLM Agent-Based Complex Semantic Table Annotation Approach",
        "abstract": "The Semantic Table Annotation (STA) task, which includes Column Type\nAnnotation (CTA) and Cell Entity Annotation (CEA), maps table contents to\nontology entities and plays important roles in various semantic applications.\nHowever, complex tables often pose challenges such as semantic loss of column\nnames or cell values, strict ontological hierarchy requirements, homonyms,\nspelling errors, and abbreviations, which hinder annotation accuracy. To\naddress these issues, this paper proposes an LLM-based agent approach for CTA\nand CEA. We design and implement five external tools with tailored prompts\nbased on the ReAct framework, enabling the STA agent to dynamically select\nsuitable annotation strategies depending on table characteristics. Experiments\nare conducted on the Tough Tables and BiodivTab datasets from the SemTab\nchallenge, which contain the aforementioned challenges. Our method outperforms\nexisting approaches across various metrics. Furthermore, by leveraging\nLevenshtein distance to reduce redundant annotations, we achieve a 70%\nreduction in time costs and a 60% reduction in LLM token usage, providing an\nefficient and cost-effective solution for STA.",
        "url": "http://arxiv.org/abs/2508.12868v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12868v1",
        "arxiv_id": "2508.12868v1",
        "authors": [
            "Yilin Geng",
            "Shujing Wang",
            "Chuan Wang",
            "Keqing He",
            "Yanfei Lv",
            "Ying Wang",
            "Zaiwen Feng",
            "Xiaoying Bai"
        ],
        "submitted": "2025-08-18 12:09:20",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Word Meanings in Transformer Language Models",
        "abstract": "We investigate how word meanings are represented in the transformer language\nmodels. Specifically, we focus on whether transformer models employ something\nanalogous to a lexical store - where each word has an entry that contains\nsemantic information. To do this, we extracted the token embedding space of\nRoBERTa-base and k-means clustered it into 200 clusters. In our first study, we\nthen manually inspected the resultant clusters to consider whether they are\nsensitive to semantic information. In our second study, we tested whether the\nclusters are sensitive to five psycholinguistic measures: valence,\nconcreteness, iconicity, taboo, and age of acquisition. Overall, our findings\nwere very positive - there is a wide variety of semantic information encoded\nwithin the token embedding space. This serves to rule out certain \"meaning\neliminativist\" hypotheses about how transformer LLMs process semantic\ninformation.",
        "url": "http://arxiv.org/abs/2508.12863v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12863v1",
        "arxiv_id": "2508.12863v1",
        "authors": [
            "Jumbly Grindrod",
            "Peter Grindrod"
        ],
        "submitted": "2025-08-18 12:01:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model",
        "abstract": "Multimodal Empathetic Response Generation (MERG) is crucial for building\nemotionally intelligent human-computer interactions. Although large language\nmodels (LLMs) have improved text-based ERG, challenges remain in handling\nmultimodal emotional content and maintaining identity consistency. Thus, we\npropose E3RG, an Explicit Emotion-driven Empathetic Response Generation System\nbased on multimodal LLMs which decomposes MERG task into three parts:\nmultimodal empathy understanding, empathy memory retrieval, and multimodal\nresponse generation. By integrating advanced expressive speech and video\ngenerative models, E3RG delivers natural, emotionally rich, and\nidentity-consistent responses without extra training. Experiments validate the\nsuperiority of our system on both zero-shot and few-shot settings, securing\nTop-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.\nOur code is available at https://github.com/RH-Lin/E3RG.",
        "url": "http://arxiv.org/abs/2508.12854v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12854v1",
        "arxiv_id": "2508.12854v1",
        "authors": [
            "Ronghao Lin",
            "Shuai Shen",
            "Weipeng Hu",
            "Qiaolin He",
            "Aolin Xiong",
            "Li Huang",
            "Haifeng Hu",
            "Yap-peng Tan"
        ],
        "submitted": "2025-08-18 11:47:02",
        "source": "arxiv",
        "comment": "Accepted at ACM MM 2025 Grand Challenge"
    },
    {
        "title": "It takes a village to write a book: Mapping anonymous contributions in Stephen Langton's Quaestiones Theologiae",
        "abstract": "While the indirect evidence suggests that already in the early scholastic\nperiod the literary production based on records of oral teaching (so-called\nreportationes) was not uncommon, there are very few sources commenting on the\npractice. This paper details the design of a study applying stylometric\ntechniques of authorship attribution to a collection developed from\nreportationes -- Stephen Langton's Quaestiones Theologiae -- aiming to uncover\nlayers of editorial work and thus validate some hypotheses regarding the\ncollection's formation. Following Camps, Cl\\'erice, and Pinche (2021), I\ndiscuss the implementation of an HTR pipeline and stylometric analysis based on\nthe most frequent words, POS tags, and pseudo-affixes. The proposed study will\noffer two methodological gains relevant to computational research on the\nscholastic tradition: it will directly compare performance on manually composed\nand automatically extracted data, and it will test the validity of\ntransformer-based OCR and automated transcription alignment for workflows\napplied to scholastic Latin corpora. If successful, this study will provide an\neasily reusable template for the exploratory analysis of collaborative literary\nproduction stemming from medieval universities.",
        "url": "http://arxiv.org/abs/2508.12830v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12830v1",
        "arxiv_id": "2508.12830v1",
        "authors": [
            "Jan Maliszewski"
        ],
        "submitted": "2025-08-18 11:13:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection",
        "abstract": "Abusive language detection has become an increasingly important task as a\nmeans to tackle this type of harmful content in social media. There has been a\nsubstantial body of research developing models for determining if a social\nmedia post is abusive or not; however, this research has primarily focused on\nexploiting social media posts individually, overlooking additional context that\ncan be derived from surrounding posts. In this study, we look at conversational\nexchanges, where a user replies to an earlier post by another user (the parent\ntweet). We ask: does leveraging context from the parent tweet help determine if\na reply post is abusive or not, and what are the features that contribute the\nmost? We study a range of content-based and account-based features derived from\nthe context, and compare this to the more widely studied approach of only\nlooking at the features from the reply tweet. For a more generalizable study,\nwe test four different classification models on a dataset made of\nconversational exchanges (parent-reply tweet pairs) with replies labeled as\nabusive or not. Our experiments show that incorporating contextual features\nleads to substantial improvements compared to the use of features derived from\nthe reply tweet only, confirming the importance of leveraging context. We\nobserve that, among the features under study, it is especially the\ncontent-based features (what is being posted) that contribute to the\nclassification performance rather than account-based features (who is posting\nit). While using content-based features, it is best to combine a range of\ndifferent features to ensure improved performance over being more selective and\nusing fewer features. Our study provides insights into the development of\ncontextualized abusive language detection models in realistic settings\ninvolving conversations.",
        "url": "http://arxiv.org/abs/2508.12828v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12828v1",
        "arxiv_id": "2508.12828v1",
        "authors": [
            "Raneem Alharthi",
            "Rajwa Alharthi",
            "Aiqi Jiang",
            "Arkaitz Zubiaga"
        ],
        "submitted": "2025-08-18 11:12:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ding-01 :ARG0: An AMR Corpus for Spontaneous French Dialogue",
        "abstract": "We present our work to build a French semantic corpus by annotating French\ndialogue in Abstract Meaning Representation (AMR). Specifically, we annotate\nthe DinG corpus, consisting of transcripts of spontaneous French dialogues\nrecorded during the board game Catan. As AMR has insufficient coverage of the\ndynamics of spontaneous speech, we extend the framework to better represent\nspontaneous speech and sentence structures specific to French. Additionally, to\nsupport consistent annotation, we provide an annotation guideline detailing\nthese extensions. We publish our corpus under a free license (CC-SA-BY). We\nalso train and evaluate an AMR parser on our data. This model can be used as an\nassistance annotation tool to provide initial annotations that can be refined\nby human annotators. Our work contributes to the development of semantic\nresources for French dialogue.",
        "url": "http://arxiv.org/abs/2508.12819v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12819v1",
        "arxiv_id": "2508.12819v1",
        "authors": [
            "Jeongwoo Kang",
            "Maria Boritchev",
            "Maximin Coavoux"
        ],
        "submitted": "2025-08-18 10:57:44",
        "source": "arxiv",
        "comment": "Accepted at IWCS 2025"
    },
    {
        "title": "Learning to Steer: Input-dependent Steering for Multimodal LLMs",
        "abstract": "Steering has emerged as a practical approach to enable post-hoc guidance of\nLLMs towards enforcing a specific behavior. However, it remains largely\nunderexplored for multimodal LLMs (MLLMs); furthermore, existing steering\ntechniques, such as mean steering, rely on a single steering vector, applied\nindependently of the input query. This paradigm faces limitations when the\ndesired behavior is dependent on the example at hand. For example, a safe\nanswer may consist in abstaining from answering when asked for an illegal\nactivity, or may point to external resources or consultation with an expert\nwhen asked about medical advice. In this paper, we investigate a fine-grained\nsteering that uses an input-specific linear shift. This shift is computed using\ncontrastive input-specific prompting. However, the input-specific prompts\nrequired for this approach are not known at test time. Therefore, we propose to\ntrain a small auxiliary module to predict the input-specific steering vector.\nOur approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces\nhallucinations and enforces safety in MLLMs, outperforming other static\nbaselines.",
        "url": "http://arxiv.org/abs/2508.12815v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12815v1",
        "arxiv_id": "2508.12815v1",
        "authors": [
            "Jayneel Parekh",
            "Pegah Khayatan",
            "Mustafa Shukor",
            "Arnaud Dapogny",
            "Alasdair Newson",
            "Matthieu Cord"
        ],
        "submitted": "2025-08-18 10:53:20",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "When Alignment Hurts: Decoupling Representational Spaces in Multilingual Models",
        "abstract": "Alignment with high-resource standard languages is often assumed to aid the\nmodeling of related low-resource varieties. We challenge this assumption by\ndemonstrating that excessive representational entanglement with a dominant\nvariety, such as Modern Standard Arabic (MSA) in relation to Arabic dialects,\ncan actively hinder generative modeling. We present the first comprehensive\ncausal study of this phenomenon by analyzing and directly intervening in the\ninternal representation geometry of large language models (LLMs). Our key\ncontribution is an online variational probing framework that continuously\nestimates the subspace of the standard variety during fine-tuning, enabling\nprojection-based decoupling from this space. While our study uses Arabic as a\ncase due to its unusually rich parallel resources across 25 dialects, the\nbroader motivation is methodological: dialectal MT serves as a controlled proxy\nfor generative tasks where comparable multi-variety corpora are unavailable.\nAcross 25 dialects, our intervention improves generation quality by up to +4.9\nchrF++ and +2.0 on average compared to standard fine-tuning, despite a measured\ntradeoff in standard-language performance. These results provide causal\nevidence that subspace dominance by high-resource varieties can restrict\ngenerative capacity for related varieties. More generally, we unify geometric\nand information-theoretic probing with subspace-level causal interventions,\noffering practical tools for improving generative modeling in closely related\nlanguage families and, more broadly, for controlling representational\nallocation in multilingual and multi-domain LLMs. Code will be released.",
        "url": "http://arxiv.org/abs/2508.12803v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12803v1",
        "arxiv_id": "2508.12803v1",
        "authors": [
            "Ahmed Elshabrawy",
            "Hour Kaing",
            "Haiyue Song",
            "Alham Fikri Aji",
            "Hideki Tanaka",
            "Masao Utiyama",
            "Raj Dabre"
        ],
        "submitted": "2025-08-18 10:34:08",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Maximum Score Routing For Mixture-of-Experts",
        "abstract": "Routing networks in sparsely activated mixture-of-experts (MoE) dynamically\nallocate input tokens to top-k experts through differentiable sparse\ntransformations, enabling scalable model capacity while preserving\ncomputational efficiency. Traditional MoE networks impose an expert capacity\nconstraint to ensure GPU-friendly computation. However, this leads to token\ndropping when capacity is saturated and results in low hardware efficiency due\nto padding in underutilized experts. Removing the capacity constraint, in turn,\ncompromises load balancing and computational efficiency. To address these\nissues, we propose Maximum Score Routing ($\\mathbf{MaxScore}$), a novel MoE\nrouting paradigm that models routing as a minimum-cost maximum-flow problem and\nintegrates a SoftTopk operator. MaxScore resolves the fundamental limitations\nof iterative rerouting and optimal transport formulations, achieving lower\ntraining losses and higher evaluation scores at equivalent FLOPs compared to\nboth constrained and unconstrained baselines. Implementation details and\nexperimental configurations can be obtained from\n$\\href{https://github.com/dongbw18/MaxScore.git}{MaxScore}$.",
        "url": "http://arxiv.org/abs/2508.12801v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12801v1",
        "arxiv_id": "2508.12801v1",
        "authors": [
            "Bowen Dong",
            "Yilong Fan",
            "Yutao Sun",
            "Zhenyu Li",
            "Tengyu Pan",
            "Xun Zhou",
            "Jianyong Wang"
        ],
        "submitted": "2025-08-18 10:25:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward",
        "abstract": "Large language models (LLMs) exhibit remarkable problem-solving abilities,\nbut struggle with complex tasks due to static internal knowledge.\nRetrieval-Augmented Generation (RAG) enhances access to external information,\nyet remains limited in multi-hop reasoning and strategic search due to rigid\nworkflows. Recent advancements in agentic deep research empower LLMs to\nautonomously reason, search, and synthesize information. However, current\napproaches relying on outcome-based reinforcement learning (RL) face critical\nissues such as conflicting gradients and reward sparsity, limiting performance\ngains and training efficiency. To address these, we first propose Atomic\nThought, a novel LLM thinking paradigm that decomposes reasoning into\nfine-grained functional units. These units are supervised by Reasoning Reward\nModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained\nguidance. Building on this, we propose Atom-Searcher, a novel RL framework for\nagentic deep research that integrates Atomic Thought and ATR. Atom-Searcher\nuses a curriculum-inspired reward schedule, prioritizing process-level ATR\nearly and transitioning to outcome rewards, accelerating convergence on\neffective reasoning paths. Experiments on seven benchmarks show consistent\nimprovements over the state-of-the-art. Key advantages include: (1)\nAtom-Searcher scales computation at test-time. (2) Atomic Thought provides\nsupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)\nAtom-Searcher exhibits more interpretable, human-like reasoning patterns.",
        "url": "http://arxiv.org/abs/2508.12800v2",
        "pdf_url": "http://arxiv.org/pdf/2508.12800v2",
        "arxiv_id": "2508.12800v2",
        "authors": [
            "Yong Deng",
            "Guoqing Wang",
            "Zhenzhe Ying",
            "Xiaofeng Wu",
            "Jinzhen Lin",
            "Wenwen Xiong",
            "Yuqin Dai",
            "Shuo Yang",
            "Zhanwei Zhang",
            "Qiwen Wang",
            "Yang Qin",
            "Changhua Meng"
        ],
        "submitted": "2025-08-18 10:23:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Bridging Human and LLM Judgments: Understanding and Narrowing the Gap",
        "abstract": "Large language models are increasingly used as judges (LLM-as-a-judge) to\nevaluate model outputs at scale, but their assessments often diverge\nsystematically from human judgments. We present Bridge, a unified statistical\nframework that explicitly bridges human and LLM evaluations under both absolute\nscoring and pairwise comparison paradigms. Bridge posits a latent human\npreference score for each prompt-response pair and models LLM deviations as\nlinear transformations of covariates that capture sources of discrepancies.\nThis offers a simple and principled framework for refining LLM ratings and\ncharacterizing systematic discrepancies between humans and LLMs. We provide an\nefficient fitting algorithm with asymptotic guarantees for statistical\ninference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot\nArena), Bridge achieves higher agreement with human ratings (accuracy,\ncalibration, and KL divergence) and exposes systematic human-LLM gaps.",
        "url": "http://arxiv.org/abs/2508.12792v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12792v1",
        "arxiv_id": "2508.12792v1",
        "authors": [
            "Felipe Maia Polo",
            "Xinhe Wang",
            "Mikhail Yurochkin",
            "Gongjun Xu",
            "Moulinath Banerjee",
            "Yuekai Sun"
        ],
        "submitted": "2025-08-18 10:14:20",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Reinforcement Learning with Rubric Anchors",
        "abstract": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing Large Language Models (LLMs), exemplified by\nthe success of OpenAI's o-series. In RLVR, rewards are derived from verifiable\nsignals-such as passing unit tests in code generation or matching correct\nanswers in mathematical reasoning. While effective, this requirement largely\nconfines RLVR to domains with automatically checkable outcomes. To overcome\nthis, we extend the RLVR paradigm to open-ended tasks by integrating\nrubric-based rewards, where carefully designed rubrics serve as structured,\nmodel-interpretable criteria for automatic scoring of subjective outputs. We\nconstruct, to our knowledge, the largest rubric reward system to date, with\nover 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.\nImplementing rubric-based RL is challenging; we tackle these issues with a\nclear framework and present an open-sourced Qwen-30B-A3B model with notable\ngains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended\nbenchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by\n+2.4%, while preserving general and reasoning abilities. 2) Our method provides\nfine-grained stylistic control, using rubrics as anchors to mitigate the\n\"AI-like\" tone and produce more human-like, expressive responses. We share key\nlessons in rubric construction, data selection, and training, and discuss\nlimitations and future releases.",
        "url": "http://arxiv.org/abs/2508.12790v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12790v1",
        "arxiv_id": "2508.12790v1",
        "authors": [
            "Zenan Huang",
            "Yihong Zhuang",
            "Guoshan Lu",
            "Zeyu Qin",
            "Haokai Xu",
            "Tianyu Zhao",
            "Ru Peng",
            "Jiaqi Hu",
            "Zhanming Shen",
            "Xiaomeng Hu",
            "Xijun Gu",
            "Peiyi Tu",
            "Jiaxin Liu",
            "Wenyu Chen",
            "Yuzhuo Fu",
            "Zhiting Fan",
            "Yanmei Gu",
            "Yuanyuan Wang",
            "Zhengkai Yang",
            "Jianguo Li",
            "Junbo Zhao"
        ],
        "submitted": "2025-08-18 10:06:08",
        "source": "arxiv",
        "comment": "technical report"
    },
    {
        "title": "HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks",
        "abstract": "Medical large vision-language Models (Med-LVLMs) have shown promise in\nclinical applications but suffer from factual inaccuracies and unreliable\noutputs, posing risks in real-world diagnostics. While retrieval-augmented\ngeneration has emerged as a potential solution, current medical multimodal RAG\nsystems are unable to perform effective retrieval across heterogeneous sources.\nThe irrelevance of retrieved reports affects the factuality of analysis, while\ninsufficient knowledge affects the credibility of clinical decision-making. To\nbridge the gap, we construct MedAtlas, which includes extensive multimodal\nreport repositories and diverse text corpora. Based on it, we present\nHeteroRAG, a novel framework that enhances Med-LVLMs through heterogeneous\nknowledge sources. The framework introduces Modality-specific CLIPs for\neffective report retrieval and a Multi-corpora Query Generator for dynamically\nconstructing queries for diverse corpora. Incorporating knowledge from such\nmultifaceted sources, Med-LVLM is then trained with Heterogeneous Knowledge\nPreference Tuning to achieve cross-modality and multi-source knowledge\nalignment. Extensive experiments across 12 datasets and 3 modalities\ndemonstrate that the proposed HeteroRAG achieves state-of-the-art performance\nin most medical vision language benchmarks, significantly improving factual\naccuracy and reliability of Med-LVLMs.",
        "url": "http://arxiv.org/abs/2508.12778v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12778v1",
        "arxiv_id": "2508.12778v1",
        "authors": [
            "Zhe Chen",
            "Yusheng Liao",
            "Shuyang Jiang",
            "Zhiyuan Zhu",
            "Haolin Li",
            "Yanfeng Wang",
            "Yu Wang"
        ],
        "submitted": "2025-08-18 09:54:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task",
        "abstract": "In this paper, we present the SALAMANDRATA family of models, an improved\niteration of SALAMANDRA LLMs (Gonzalez-Agirre et al., 2025) specifically\ntrained to achieve strong performance in translation-related tasks for 38\nEuropean languages. SALAMANDRATA comes in two scales: 2B and 7B parameters. For\nboth versions, we applied the same training recipe with a first step of\ncontinual pre-training on parallel data, and a second step of supervised\nfine-tuning on high-quality instructions. The BSC submission to the WMT25\nGeneral Machine Translation shared task is based on the 7B variant of\nSALAMANDRATA. We first adapted the model vocabulary to support the additional\nnon-European languages included in the task. This was followed by a second\nphase of continual pre-training and supervised fine-tuning, carefully designed\nto optimize performance across all translation directions for this year's\nshared task. For decoding, we employed two quality-aware strategies: Minimum\nBayes Risk Decoding and Tuned Re-ranking using COMET and COMET-KIWI\nrespectively. We publicly release both the 2B and 7B versions of SALAMANDRATA,\nalong with the newer SALAMANDRATA-V2 model, on Hugging Face1",
        "url": "http://arxiv.org/abs/2508.12774v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12774v1",
        "arxiv_id": "2508.12774v1",
        "authors": [
            "Javier Garcia Gilabert",
            "Xixian Liao",
            "Severino Da Dalt",
            "Ella Bohman",
            "Audrey Mash",
            "Francesca De Luca Fornaciari",
            "Irene Baucells",
            "Joan Llop",
            "Miguel Claramunt Argote",
            "Carlos Escolano",
            "Maite Melero"
        ],
        "submitted": "2025-08-18 09:48:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description",
        "abstract": "Recent advances in large language models (LLMs) have significantly improved\nthe accuracy of Text-to-SQL systems. However, a critical challenge remains: the\nsemantic mismatch between natural language questions (NLQs) and their\ncorresponding SQL queries. This issue is exacerbated in large-scale databases,\nwhere semantically similar attributes hinder schema linking and semantic drift\nduring SQL generation, ultimately reducing model accuracy. To address these\nchallenges, we introduce CRED-SQL, a framework designed for large-scale\ndatabases that integrates Cluster Retrieval and Execution Description. CRED-SQL\nfirst performs cluster-based large-scale schema retrieval to pinpoint the\ntables and columns most relevant to a given NLQ, alleviating schema mismatch.\nIt then introduces an intermediate natural language representation-Execution\nDescription Language (EDL)-to bridge the gap between NLQs and SQL. This\nreformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,\nleveraging LLMs' strong general reasoning capabilities while reducing semantic\ndeviation. Extensive experiments on two large-scale, cross-domain\nbenchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new\nstate-of-the-art (SOTA) performance, validating its effectiveness and\nscalability. Our code is available at https://github.com/smduan/CRED-SQL.git",
        "url": "http://arxiv.org/abs/2508.12769v2",
        "pdf_url": "http://arxiv.org/pdf/2508.12769v2",
        "arxiv_id": "2508.12769v2",
        "authors": [
            "Shaoming Duan",
            "Zirui Wang",
            "Chuanyi Liu",
            "Zhibin Zhu",
            "Yuhao Zhang",
            "Peiyi Han",
            "Liang Yan",
            "Zewu Penge"
        ],
        "submitted": "2025-08-18 09:43:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Deep Research: A Survey of Autonomous Research Agents",
        "abstract": "The rapid advancement of large language models (LLMs) has driven the\ndevelopment of agentic systems capable of autonomously performing complex\ntasks. Despite their impressive capabilities, LLMs remain constrained by their\ninternal knowledge boundaries. To overcome these limitations, the paradigm of\ndeep research has been proposed, wherein agents actively engage in planning,\nretrieval, and synthesis to generate comprehensive and faithful analytical\nreports grounded in web-based evidence. In this survey, we provide a systematic\noverview of the deep research pipeline, which comprises four core stages:\nplanning, question developing, web exploration, and report generation. For each\nstage, we analyze the key technical challenges and categorize representative\nmethods developed to address them. Furthermore, we summarize recent advances in\noptimization techniques and benchmarks tailored for deep research. Finally, we\ndiscuss open challenges and promising research directions, aiming to chart a\nroadmap toward building more capable and trustworthy deep research agents.",
        "url": "http://arxiv.org/abs/2508.12752v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12752v1",
        "arxiv_id": "2508.12752v1",
        "authors": [
            "Wenlin Zhang",
            "Xiaopeng Li",
            "Yingyi Zhang",
            "Pengyue Jia",
            "Yichao Wang",
            "Huifeng Guo",
            "Yong Liu",
            "Xiangyu Zhao"
        ],
        "submitted": "2025-08-18 09:26:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models",
        "abstract": "The widespread adoption and increasing prominence of large language models\n(LLMs) in global technologies necessitate a rigorous focus on ensuring their\nsafety across a diverse range of linguistic and cultural contexts. The lack of\na comprehensive evaluation and diverse data in existing multilingual safety\nevaluations for LLMs limits their effectiveness, hindering the development of\nrobust multilingual safety alignment. To address this critical gap, we\nintroduce LinguaSafe, a comprehensive multilingual safety benchmark crafted\nwith meticulous attention to linguistic authenticity. The LinguaSafe dataset\ncomprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated\nusing a combination of translated, transcreated, and natively-sourced data, our\ndataset addresses the critical need for multilingual safety evaluations of\nLLMs, filling the void in the safety evaluation of LLMs across diverse\nunder-represented languages from Hungarian to Malay. LinguaSafe presents a\nmultidimensional and fine-grained evaluation framework, with direct and\nindirect safety assessments, including further evaluations for oversensitivity.\nThe results of safety and helpfulness evaluations vary significantly across\ndifferent domains and different languages, even in languages with similar\nresource levels. Our benchmark provides a comprehensive suite of metrics for\nin-depth safety evaluation, underscoring the critical importance of thoroughly\nassessing multilingual safety in LLMs to achieve more balanced safety\nalignment. Our dataset and code are released to the public to facilitate\nfurther research in the field of multilingual LLM safety.",
        "url": "http://arxiv.org/abs/2508.12733v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12733v1",
        "arxiv_id": "2508.12733v1",
        "authors": [
            "Zhiyuan Ning",
            "Tianle Gu",
            "Jiaxin Song",
            "Shixin Hong",
            "Lingyu Li",
            "Huacan Liu",
            "Jie Li",
            "Yixu Wang",
            "Meng Lingyu",
            "Yan Teng",
            "Yingchun Wang"
        ],
        "submitted": "2025-08-18 08:59:01",
        "source": "arxiv",
        "comment": "7pages, 5 figures"
    },
    {
        "title": "DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning",
        "abstract": "Large language models (LLMs) have achieved remarkable success in many natural\nlanguage tasks but still struggle with complex, multi-step reasoning,\nparticularly across diverse disciplines. Existing reasoning datasets often\neither lack disciplinary breadth or the structural depth necessary to elicit\nrobust reasoning behaviors. We propose DESIGNER: a DESIGN-logic-guidEd\nReasoning data synthesis pipeline that leverages naturally available, extensive\nraw documents (book corpus and web corpus) to generate multidisciplinary\nchallenging questions. A core innovation of our approach is the introduction of\na Design Logic concept, which mimics the question-creation process of human\neducators. We use LLMs to reverse-engineer and abstract over 120,000 design\nlogics from existing questions across various disciplines. By matching these\ndesign logics with disciplinary source materials, we are able to create\nreasoning questions that far surpass the difficulty and diversity of existing\ndatasets. Based on this pipeline, we synthesized two large-scale reasoning\ndatasets that span 75 disciplines: Design-Logic-Reasoning-Book (DLR-Book),\ncontaining 3.04 million challenging questions synthesized from the book corpus,\nand Design-Logic-Reasoning-Web (DLR-Web), with 1.66 million challenging\nquestions from the web corpus. Our data analysis demonstrates that the\nquestions synthesized by our method exhibit substantially greater difficulty\nand diversity than those in the baseline datasets. We validate the\neffectiveness of these datasets by conducting SFT experiments on the\nQwen3-8B-Base and Qwen3-4B-Base models. The results show that our dataset\nsignificantly outperforms existing multidisciplinary datasets of the same\nvolume. Training with the full datasets further enables the models to surpass\nthe multidisciplinary reasoning performance of the official Qwen3-8B and\nQwen3-4B models.",
        "url": "http://arxiv.org/abs/2508.12726v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12726v1",
        "arxiv_id": "2508.12726v1",
        "authors": [
            "Weize Liu",
            "Yongchi Zhao",
            "Yijia Luo",
            "Mingyu Xu",
            "Jiaheng Liu",
            "Yanan Li",
            "Xiguo Hu",
            "Yuchi Xu",
            "Wenbo Su",
            "Bo Zheng"
        ],
        "submitted": "2025-08-18 08:49:29",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Asymmetric Diffusion Recommendation Model",
        "abstract": "Recently, motivated by the outstanding achievements of diffusion models, the\ndiffusion process has been employed to strengthen representation learning in\nrecommendation systems. Most diffusion-based recommendation models typically\nutilize standard Gaussian noise in symmetric forward and reverse processes in\ncontinuous data space. Nevertheless, the samples derived from recommendation\nsystems inhabit a discrete data space, which is fundamentally different from\nthe continuous one. Moreover, Gaussian noise has the potential to corrupt\npersonalized information within latent representations. In this work, we\npropose a novel and effective method, named Asymmetric Diffusion Recommendation\nModel (AsymDiffRec), which learns forward and reverse processes in an\nasymmetric manner. We define a generalized forward process that simulates the\nmissing features in real-world recommendation samples. The reverse process is\nthen performed in an asymmetric latent feature space. To preserve personalized\ninformation within the latent representation, a task-oriented optimization\nstrategy is introduced. In the serving stage, the raw sample with missing\nfeatures is regarded as a noisy input to generate a denoising and robust\nrepresentation for the final prediction. By equipping base models with\nAsymDiffRec, we conduct online A/B tests, achieving improvements of +0.131% and\n+0.166% in terms of users' active days and app usage duration respectively.\nAdditionally, the extended offline experiments also demonstrate improvements.\nAsymDiffRec has been implemented in the Douyin Music App.",
        "url": "http://arxiv.org/abs/2508.12706v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12706v1",
        "arxiv_id": "2508.12706v1",
        "authors": [
            "Yongchun Zhu",
            "Guanyu Jiang",
            "Jingwu Chen",
            "Feng Zhang",
            "Xiao Yang",
            "Zuotao Liu"
        ],
        "submitted": "2025-08-18 08:05:25",
        "source": "arxiv",
        "comment": "Accepted by CIKM2025"
    },
    {
        "title": "ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction",
        "abstract": "Agentic task-solving with Large Language Models (LLMs) requires multi-turn,\nmulti-step interactions, often involving complex function calls and dynamic\nuser-agent exchanges. Existing simulation-based data generation methods for\nsuch scenarios rely heavily on costly autoregressive interactions between\nmultiple LLM agents, thereby limiting real-world performance of agentic tasks.\nIn this paper, we propose a novel Non-Autoregressive Iterative Generation\nframework, called ToolACE-MT, for constructing high-quality multi-turn agentic\ndialogues. ToolACE-MT generates full conversational trajectories through three\nstages: coarse-grained initialization, iterative refinement, and offline\nverification. The initialization phase builds a structurally complete yet\nsemantically coarse dialogue skeleton; the iterative refinement phase\nintroduces realistic complexities and continued refinement via mask-and-fill\noperations; and the offline verification phase ensures correctness and\ncoherence via rule- and model-based checks. Experiments demonstrate that\nToolACE-MT enables efficient, effective and generalizable agentic data\ngeneration, offering a new paradigm for high-quality data construction in\ntool-augmented LLM scenarios.",
        "url": "http://arxiv.org/abs/2508.12685v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12685v1",
        "arxiv_id": "2508.12685v1",
        "authors": [
            "Xingshan Zeng",
            "Weiwen Liu",
            "Lingzhi Wang",
            "Liangyou Li",
            "Fei Mi",
            "Yasheng Wang",
            "Lifeng Shang",
            "Xin Jiang",
            "Qun Liu"
        ],
        "submitted": "2025-08-18 07:38:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Vision-G1: Towards General Vision Language Reasoning with Multi-Domain Data Curation",
        "abstract": "Despite their success, current training pipelines for reasoning VLMs focus on\na limited range of tasks, such as mathematical and logical reasoning. As a\nresult, these models face difficulties in generalizing their reasoning\ncapabilities to a wide range of domains, primarily due to the scarcity of\nreadily available and verifiable reward data beyond these narrowly defined\nareas. Moreover, integrating data from multiple domains is challenging, as the\ncompatibility between domain-specific datasets remains uncertain. To address\nthese limitations, we build a comprehensive RL-ready visual reasoning dataset\nfrom 46 data sources across 8 dimensions, covering a wide range of tasks such\nas infographic, mathematical, spatial, cross-image, graphic user interface,\nmedical, common sense and general science. We propose an influence function\nbased data selection and difficulty based filtering strategy to identify\nhigh-quality training samples from this dataset. Subsequently, we train the\nVLM, referred to as Vision-G1, using multi-round RL with a data curriculum to\niteratively improve its visual reasoning capabilities. Our model achieves\nstate-of-the-art performance across various visual reasoning benchmarks,\noutperforming similar-sized VLMs and even proprietary models like GPT-4o and\nGemini-1.5 Flash. The model, code and dataset are publicly available at\nhttps://github.com/yuh-zha/Vision-G1.",
        "url": "http://arxiv.org/abs/2508.12680v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12680v1",
        "arxiv_id": "2508.12680v1",
        "authors": [
            "Yuheng Zha",
            "Kun Zhou",
            "Yujia Wu",
            "Yushu Wang",
            "Jie Feng",
            "Zhi Xu",
            "Shibo Hao",
            "Zhengzhong Liu",
            "Eric P. Xing",
            "Zhiting Hu"
        ],
        "submitted": "2025-08-18 07:24:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Leveraging Large Language Models for Predictive Analysis of Human Misery",
        "abstract": "This study investigates the use of Large Language Models (LLMs) for\npredicting human-perceived misery scores from natural language descriptions of\nreal-world scenarios. The task is framed as a regression problem, where the\nmodel assigns a scalar value from 0 to 100 to each input statement. We evaluate\nmultiple prompting strategies, including zero-shot, fixed-context few-shot, and\nretrieval-based prompting using BERT sentence embeddings. Few-shot approaches\nconsistently outperform zero-shot baselines, underscoring the value of\ncontextual examples in affective prediction. To move beyond static evaluation,\nwe introduce the \"Misery Game Show\", a novel gamified framework inspired by a\ntelevision format. It tests LLMs through structured rounds involving ordinal\ncomparison, binary classification, scalar estimation, and feedback-driven\nreasoning. This setup enables us to assess not only predictive accuracy but\nalso the model's ability to adapt based on corrective feedback. The gamified\nevaluation highlights the broader potential of LLMs in dynamic emotional\nreasoning tasks beyond standard regression. Code and data link:\nhttps://github.com/abhi1nandy2/Misery_Data_Exps_GitHub",
        "url": "http://arxiv.org/abs/2508.12669v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12669v1",
        "arxiv_id": "2508.12669v1",
        "authors": [
            "Bishanka Seal",
            "Rahul Seetharaman",
            "Aman Bansal",
            "Abhilash Nandy"
        ],
        "submitted": "2025-08-18 07:02:59",
        "source": "arxiv",
        "comment": "14 pages, 4 tables"
    },
    {
        "title": "Multi-Granularity Distribution Modeling for Video Watch Time Prediction via Exponential-Gaussian Mixture Network",
        "abstract": "Accurate watch time prediction is crucial for enhancing user engagement in\nstreaming short-video platforms, although it is challenged by complex\ndistribution characteristics across multi-granularity levels. Through\nsystematic analysis of real-world industrial data, we uncover two critical\nchallenges in watch time prediction from a distribution aspect: (1)\ncoarse-grained skewness induced by a significant concentration of quick-skips1,\n(2) fine-grained diversity arising from various user-video interaction\npatterns. Consequently, we assume that the watch time follows the\nExponential-Gaussian Mixture (EGM) distribution, where the exponential and\nGaussian components respectively characterize the skewness and diversity.\nAccordingly, an Exponential-Gaussian Mixture Network (EGMN) is proposed for the\nparameterization of EGM distribution, which consists of two key modules: a\nhidden representation encoder and a mixture parameter generator. We conducted\nextensive offline experiments on public datasets and online A/B tests on the\nindustrial short-video feeding scenario of Xiaohongshu App to validate the\nsuperiority of EGMN compared with existing state-of-the-art methods.\nRemarkably, comprehensive experimental results have proven that EGMN exhibits\nexcellent distribution fitting ability across coarse-to-fine-grained levels. We\nopen source related code on Github: https://github.com/BestActionNow/EGMN.",
        "url": "http://arxiv.org/abs/2508.12665v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12665v1",
        "arxiv_id": "2508.12665v1",
        "authors": [
            "Xu Zhao",
            "Ruibo Ma",
            "Jiaqi Chen",
            "Weiqi Zhao",
            "Ping Yang",
            "Yao Hu"
        ],
        "submitted": "2025-08-18 06:56:36",
        "source": "arxiv",
        "comment": "Accepted as oral full paper by RecSys'2025 conference"
    },
    {
        "title": "Breaking Language Barriers: Equitable Performance in Multilingual Language Models",
        "abstract": "Cutting-edge LLMs have emerged as powerful tools for multilingual\ncommunication and understanding. However, LLMs perform worse in Common Sense\nReasoning (CSR) tasks when prompted in low-resource languages (LRLs) like Hindi\nor Swahili compared to high-resource languages (HRLs) like English. Equalizing\nthis inconsistent access to quality LLM outputs is crucial to ensure fairness\nfor speakers of LRLs and across diverse linguistic communities. In this paper,\nwe propose an approach to bridge this gap in LLM performance. Our approach\ninvolves fine-tuning an LLM on synthetic code-switched text generated using\ncontrolled language-mixing methods. We empirically demonstrate that fine-tuning\nLLMs on synthetic code-switched datasets leads to substantial improvements in\nLRL model performance while preserving or enhancing performance in HRLs.\nAdditionally, we present a new dataset of synthetic code-switched text derived\nfrom the CommonSenseQA dataset, featuring three distinct language ratio\nconfigurations.",
        "url": "http://arxiv.org/abs/2508.12662v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12662v1",
        "arxiv_id": "2508.12662v1",
        "authors": [
            "Tanay Nagar",
            "Grigorii Khvatskii",
            "Anna Sokol",
            "Nitesh V. Chawla"
        ],
        "submitted": "2025-08-18 06:50:24",
        "source": "arxiv",
        "comment": "Accepted as a non-archival work-in-progress paper at the NAACL 2025\n  Student Research Workshop"
    },
    {
        "title": "Diagnostic-Guided Dynamic Profile Optimization for LLM-based User Simulators in Sequential Recommendation",
        "abstract": "Recent advances in large language models (LLMs) have enabled realistic user\nsimulators for developing and evaluating recommender systems (RSs). However,\nexisting LLM-based simulators for RSs face two major limitations: (1) static\nand single-step prompt-based inference that leads to inaccurate and incomplete\nuser profile construction; (2) unrealistic and single-round\nrecommendation-feedback interaction pattern that fails to capture real-world\nscenarios. To address these limitations, we propose DGDPO (Diagnostic-Guided\nDynamic Profile Optimization), a novel framework that constructs user profile\nthrough a dynamic and iterative optimization process to enhance the simulation\nfidelity. Specifically, DGDPO incorporates two core modules within each\noptimization loop: firstly, a specialized LLM-based diagnostic module,\ncalibrated through our novel training strategy, accurately identifies specific\ndefects in the user profile. Subsequently, a generalized LLM-based treatment\nmodule analyzes the diagnosed defect and generates targeted suggestions to\nrefine the profile. Furthermore, unlike existing LLM-based user simulators that\nare limited to single-round interactions, we are the first to integrate DGDPO\nwith sequential recommenders, enabling a bidirectional evolution where user\nprofiles and recommendation strategies adapt to each other over multi-round\ninteractions. Extensive experiments conducted on three real-world datasets\ndemonstrate the effectiveness of our proposed framework.",
        "url": "http://arxiv.org/abs/2508.12645v2",
        "pdf_url": "http://arxiv.org/pdf/2508.12645v2",
        "arxiv_id": "2508.12645v2",
        "authors": [
            "Hongyang Liu",
            "Zhu Sun",
            "Tianjun Wei",
            "Yan Wang",
            "Jiajie Zhu",
            "Xinghua Qu"
        ],
        "submitted": "2025-08-18 06:17:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Prompt-Induced Linguistic Fingerprints for LLM-Generated Fake News Detection",
        "abstract": "With the rapid development of large language models, the generation of fake\nnews has become increasingly effortless, posing a growing societal threat and\nunderscoring the urgent need for reliable detection methods. Early efforts to\nidentify LLM-generated fake news have predominantly focused on the textual\ncontent itself; however, because much of that content may appear coherent and\nfactually consistent, the subtle traces of falsification are often difficult to\nuncover. Through distributional divergence analysis, we uncover prompt-induced\nlinguistic fingerprints: statistically distinct probability shifts between\nLLM-generated real and fake news when maliciously prompted. Based on this\ninsight, we propose a novel method named Linguistic Fingerprints Extraction\n(LIFE). By reconstructing word-level probability distributions, LIFE can find\ndiscriminative patterns that facilitate the detection of LLM-generated fake\nnews. To further amplify these fingerprint patterns, we also leverage\nkey-fragment techniques that accentuate subtle linguistic differences, thereby\nimproving detection reliability. Our experiments show that LIFE achieves\nstate-of-the-art performance in LLM-generated fake news and maintains high\nperformance in human-written fake news. The code and data are available at\nhttps://anonymous.4open.science/r/LIFE-E86A.",
        "url": "http://arxiv.org/abs/2508.12632v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12632v1",
        "arxiv_id": "2508.12632v1",
        "authors": [
            "Chi Wang",
            "Min Gao",
            "Zongwei Wang",
            "Junwei Yin",
            "Kai Shu",
            "Chenghua Lin"
        ],
        "submitted": "2025-08-18 05:24:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing",
        "abstract": "Balancing performance and efficiency is a central challenge in large language\nmodel (LLM) advancement. GPT-5 addresses this with test-time routing,\ndynamically assigning queries to either an efficient or a high-capacity model\nduring inference. In this work, we present Avengers-Pro, a test-time routing\nframework that ensembles LLMs of varying capacities and efficiencies, providing\na unified solution for all performance-efficiency tradeoffs. The Avengers-Pro\nembeds and clusters incoming queries, then routes each to the most suitable\nmodel based on a performance-efficiency score. Across 6 challenging benchmarks\nand 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and\nClaude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a\nperformance-efficiency trade-off parameter, it can surpass the strongest single\nmodel (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the\naverage accuracy of the strongest single model at 27% lower cost, and reach\n~90% of that performance at 63% lower cost. Last but not least, it achieves a\nPareto frontier, consistently yielding the highest accuracy for any given cost,\nand the lowest cost for any given accuracy, among all single models. Code is\navailable at https://github.com/ZhangYiqun018/AvengersPro.",
        "url": "http://arxiv.org/abs/2508.12631v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12631v1",
        "arxiv_id": "2508.12631v1",
        "authors": [
            "Yiqun Zhang",
            "Hao Li",
            "Jianhao Chen",
            "Hangfan Zhang",
            "Peng Ye",
            "Lei Bai",
            "Shuyue Hu"
        ],
        "submitted": "2025-08-18 05:23:31",
        "source": "arxiv",
        "comment": "Ongoing work"
    },
    {
        "title": "Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive fluency and task\ncompetence in conversational settings. However, their effectiveness in\nmulti-session and long-term interactions is hindered by limited memory\npersistence. Typical retrieval-augmented generation (RAG) systems store\ndialogue history as dense vectors, which capture semantic similarity but\nneglect finer linguistic structures such as syntactic dependencies, discourse\nrelations, and coreference links. We propose Semantic Anchoring, a hybrid\nagentic memory architecture that enriches vector-based storage with explicit\nlinguistic cues to improve recall of nuanced, context-rich exchanges. Our\napproach combines dependency parsing, discourse relation tagging, and\ncoreference resolution to create structured memory entries. Experiments on\nadapted long-term dialogue datasets show that semantic anchoring improves\nfactual recall and discourse coherence by up to 18% over strong RAG baselines.\nWe further conduct ablation studies, human evaluations, and error analysis to\nassess robustness and interpretability.",
        "url": "http://arxiv.org/abs/2508.12630v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12630v1",
        "arxiv_id": "2508.12630v1",
        "authors": [
            "Maitreyi Chatterjee",
            "Devansh Agarwal"
        ],
        "submitted": "2025-08-18 05:14:48",
        "source": "arxiv",
        "comment": "Paper is currently in peer review"
    },
    {
        "title": "An LLM + ASP Workflow for Joint Entity-Relation Extraction",
        "abstract": "Joint entity-relation extraction (JERE) identifies both entities and their\nrelationships simultaneously. Traditional machine-learning based approaches to\nperforming this task require a large corpus of annotated data and lack the\nability to easily incorporate domain specific information in the construction\nof the model. Therefore, creating a model for JERE is often labor intensive,\ntime consuming, and elaboration intolerant. In this paper, we propose\nharnessing the capabilities of generative pretrained large language models\n(LLMs) and the knowledge representation and reasoning capabilities of Answer\nSet Programming (ASP) to perform JERE. We present a generic workflow for JERE\nusing LLMs and ASP. The workflow is generic in the sense that it can be applied\nfor JERE in any domain. It takes advantage of LLM's capability in natural\nlanguage understanding in that it works directly with unannotated text. It\nexploits the elaboration tolerant feature of ASP in that no modification of its\ncore program is required when additional domain specific knowledge, in the form\nof type specifications, is found and needs to be used. We demonstrate the\nusefulness of the proposed workflow through experiments with limited training\ndata on three well-known benchmarks for JERE. The results of our experiments\nshow that the LLM + ASP workflow is better than state-of-the-art JERE systems\nin several categories with only 10\\% of training data. It is able to achieve a\n2.5 times (35\\% over 15\\%) improvement in the Relation Extraction task for the\nSciERC corpus, one of the most difficult benchmarks.",
        "url": "http://arxiv.org/abs/2508.12611v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12611v1",
        "arxiv_id": "2508.12611v1",
        "authors": [
            "Trang Tran",
            "Trung Hoang Le",
            "Huiping Cao",
            "Tran Cao Son"
        ],
        "submitted": "2025-08-18 04:15:35",
        "source": "arxiv",
        "comment": "13 pages, 1 figure, Accepted as Technical Communication, 41st\n  International Conference on Logic Programming"
    },
    {
        "title": "Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning",
        "abstract": "Traditional Automated Speaking Assessment (ASA) systems exhibit inherent\nmodality limitations: text-based approaches lack acoustic information while\naudio-based methods miss semantic context. Multimodal Large Language Models\n(MLLM) offer unprecedented opportunities for comprehensive ASA by\nsimultaneously processing audio and text within unified frameworks. This paper\npresents a very first systematic study of MLLM for comprehensive ASA,\ndemonstrating the superior performance of MLLM across the aspects of content\nand language use . However, assessment on the delivery aspect reveals unique\nchallenges, which is deemed to require specialized training strategies. We thus\npropose Speech-First Multimodal Training (SFMT), leveraging a curriculum\nlearning principle to establish more robust modeling foundations of speech\nbefore cross-modal synergetic fusion. A series of experiments on a benchmark\ndataset show MLLM-based systems can elevate the holistic assessment performance\nfrom a PCC value of 0.783 to 0.846. In particular, SFMT excels in the\nevaluation of the delivery aspect, achieving an absolute accuracy improvement\nof 4% over conventional training approaches, which also paves a new avenue for\nASA.",
        "url": "http://arxiv.org/abs/2508.12591v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12591v1",
        "arxiv_id": "2508.12591v1",
        "authors": [
            "Yu-Hsuan Fang",
            "Tien-Hong Lo",
            "Yao-Ting Sung",
            "Berlin Chen"
        ],
        "submitted": "2025-08-18 02:57:43",
        "source": "arxiv",
        "comment": "Accepted at IEEE ASRU 2025"
    },
    {
        "title": "Insight Rumors: A Novel Textual Rumor Locating and Marking Model Leveraging Att_BiMamba2 Network",
        "abstract": "With the development of social media networks, rumor detection models have\nattracted more and more attention. Whereas, these models primarily focus on\nclassifying contexts as rumors or not, lacking the capability to locate and\nmark specific rumor content. To address this limitation, this paper proposes a\nnovel rumor detection model named Insight Rumors to locate and mark rumor\ncontent within textual data. Specifically, we propose the Bidirectional Mamba2\nNetwork with Dot-Product Attention (Att_BiMamba2), a network that constructs a\nbidirectional Mamba2 model and applies dot-product attention to weight and\ncombine the outputs from both directions, thereby enhancing the representation\nof high-dimensional rumor features. Simultaneously, a Rumor Locating and\nMarking module is designed to locate and mark rumors. The module constructs a\nskip-connection network to project high-dimensional rumor features onto\nlow-dimensional label features. Moreover, Conditional Random Fields (CRF) is\nemployed to impose strong constraints on the output label features, ensuring\naccurate rumor content location. Additionally, a labeled dataset for rumor\nlocating and marking is constructed, with the effectiveness of the proposed\nmodel is evaluated through comprehensive experiments. Extensive experiments\nindicate that the proposed scheme not only detects rumors accurately but also\nlocates and marks them in context precisely, outperforming state-of-the-art\nschemes that can only discriminate rumors roughly.",
        "url": "http://arxiv.org/abs/2508.12574v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12574v1",
        "arxiv_id": "2508.12574v1",
        "authors": [
            "Bin Ma",
            "Yifei Zhang",
            "Yongjin Xian",
            "Qi Li",
            "Linna Zhou",
            "Gongxun Miao"
        ],
        "submitted": "2025-08-18 02:20:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "jXBW: Fast Substructure Search in Large-Scale JSONL Datasets for Foundation Model Applications",
        "abstract": "Substructure search in JSON Lines (JSONL) datasets is essential for modern\napplications such as prompt engineering in foundation models, but existing\nmethods suffer from prohibitive computational costs due to exhaustive tree\ntraversal and subtree matching. We present jXBW, a fast method for substructure\nsearch on large-scale JSONL datasets. Our method makes three key technical\ncontributions: (i) a merged tree representation built by merging trees of\nmultiple JSON objects while preserving individual identities, (ii) a succinct\ndata structure based on the eXtended Burrows-Wheeler Transform that enables\nefficient tree navigation and subpath search, and (iii) an efficient three-step\nsubstructure search algorithm that combines path decomposition, ancestor\ncomputation, and adaptive tree identifier collection to ensure correctness\nwhile avoiding exhaustive tree traversal. Experimental evaluation on real-world\ndatasets demonstrates that jXBW consistently outperforms existing methods,\nachieving speedups of 16$\\times$ for smaller datasets and up to 4,700$\\times$\nfor larger datasets over tree-based approaches, and more than 6$\\times$10$^6$\nover XML-based processing while maintaining competitive memory usage.",
        "url": "http://arxiv.org/abs/2508.12536v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12536v1",
        "arxiv_id": "2508.12536v1",
        "authors": [
            "Yasuo Tabei"
        ],
        "submitted": "2025-08-18 00:14:24",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection",
        "abstract": "Sparse Autoencoders (SAEs) can extract interpretable features from large\nlanguage models (LLMs) without supervision. However, their effectiveness in\ndownstream steering tasks is limited by the requirement for contrastive\ndatasets or large activation storage. To address these limitations, we propose\nCorrSteer, which selects features by correlating sample correctness with SAE\nactivations from generated tokens at inference time. This approach uses only\ninference-time activations to extract more relevant features, thereby avoiding\nspurious correlations. It also obtains steering coefficients from average\nactivations, automating the entire pipeline. Our method shows improved task\nperformance on QA, bias mitigation, jailbreaking prevention, and reasoning\nbenchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1%\nimprovement in MMLU performance and a +22.9% improvement in HarmBench with only\n4000 samples. Selected features demonstrate semantically meaningful patterns\naligned with each task's requirements, revealing the underlying capabilities\nthat drive performance. Our work establishes correlationbased selection as an\neffective and scalable approach for automated SAE steering across language\nmodel applications.",
        "url": "http://arxiv.org/abs/2508.12535v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12535v1",
        "arxiv_id": "2508.12535v1",
        "authors": [
            "Seonglae Cho",
            "Zekun Wu",
            "Adriano Koshiyama"
        ],
        "submitted": "2025-08-18 00:01:42",
        "source": "arxiv",
        "comment": "42 pages, 9 tables"
    },
    {
        "title": "Mitigating Hallucinations in Large Language Models via Causal Reasoning",
        "abstract": "Large language models (LLMs) exhibit logically inconsistent hallucinations\nthat appear coherent yet violate reasoning principles, with recent research\nsuggesting an inverse relationship between causal reasoning capabilities and\nsuch hallucinations. However, existing reasoning approaches in LLMs, such as\nChain-of-Thought (CoT) and its graph-based variants, operate at the linguistic\ntoken level rather than modeling the underlying causal relationships between\nvariables, lacking the ability to represent conditional independencies or\nsatisfy causal identification assumptions. To bridge this gap, we introduce\ncausal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning\nframework that trains LLMs to explicitly construct variable-level directed\nacyclic graph (DAG) and then perform reasoning over it. Moreover, we present a\ndataset comprising 25,368 samples (CausalDR), where each sample includes an\ninput question, explicit causal DAG, graph-based reasoning trace, and validated\nanswer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves\nthe causal reasoning capability with the state-of-the-art 95.33% accuracy on\nCLADDER (surpassing human performance of 94.8% for the first time) and reduces\nthe hallucination on HaluEval with 10% improvements. It demonstrates that\nexplicit causal structure modeling in LLMs can effectively mitigate logical\ninconsistencies in LLM outputs. Code is available at\nhttps://github.com/MrLYG/CDCR-SFT.",
        "url": "http://arxiv.org/abs/2508.12495v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12495v1",
        "arxiv_id": "2508.12495v1",
        "authors": [
            "Yuangang Li",
            "Yiqing Shen",
            "Yi Nian",
            "Jiechao Gao",
            "Ziyi Wang",
            "Chenxiao Yu",
            "Shawn Li",
            "Jie Wang",
            "Xiyang Hu",
            "Yue Zhao"
        ],
        "submitted": "2025-08-17 20:51:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Structural Sources of Verb Meaning Revisited: Large Language Models Display Syntactic Bootstrapping",
        "abstract": "Syntactic bootstrapping (Gleitman, 1990) is the hypothesis that children use\nthe syntactic environments in which a verb occurs to learn its meaning. In this\npaper, we examine whether large language models exhibit a similar behavior. We\ndo this by training RoBERTa and GPT-2 on perturbed datasets where syntactic\ninformation is ablated. Our results show that models' verb representation\ndegrades more when syntactic cues are removed than when co-occurrence\ninformation is removed. Furthermore, the representation of mental verbs, for\nwhich syntactic bootstrapping has been shown to be particularly crucial in\nhuman verb learning, is more negatively impacted in such training regimes than\nphysical verbs. In contrast, models' representation of nouns is affected more\nwhen co-occurrences are distorted than when syntax is distorted. In addition to\nreinforcing the important role of syntactic bootstrapping in verb learning, our\nresults demonstrated the viability of testing developmental hypotheses on a\nlarger scale through manipulating the learning environments of large language\nmodels.",
        "url": "http://arxiv.org/abs/2508.12482v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12482v1",
        "arxiv_id": "2508.12482v1",
        "authors": [
            "Xiaomeng Zhu",
            "R. Thomas McCoy",
            "Robert Frank"
        ],
        "submitted": "2025-08-17 19:43:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Is GPT-OSS Good? A Comprehensive Evaluation of OpenAI's Latest Open Source Models",
        "abstract": "In August 2025, OpenAI released GPT-OSS models, its first open weight large\nlanguage models since GPT-2 in 2019, comprising two mixture of experts\narchitectures with 120B and 20B parameters. We evaluated both variants against\nsix contemporary open source large language models ranging from 14.7B to 235B\nparameters, representing both dense and sparse designs, across ten benchmarks\ncovering general knowledge, mathematical reasoning, code generation,\nmultilingual understanding, and conversational ability. All models were tested\nin unquantised form under standardised inference settings, with statistical\nvalidation using McNemars test and effect size analysis. Results show that\ngpt-oss-20B consistently outperforms gpt-oss-120B on several benchmarks, such\nas HumanEval and MMLU, despite requiring substantially less memory and energy\nper response. Both models demonstrate mid-tier overall performance within the\ncurrent open source landscape, with relative strength in code generation and\nnotable weaknesses in multilingual tasks. These findings provide empirical\nevidence that scaling in sparse architectures may not yield proportional\nperformance gains, underscoring the need for further investigation into\noptimisation strategies and informing more efficient model selection for future\nopen source deployments.",
        "url": "http://arxiv.org/abs/2508.12461v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12461v1",
        "arxiv_id": "2508.12461v1",
        "authors": [
            "Ziqian Bi",
            "Keyu Chen",
            "Chiung-Yi Tseng",
            "Danyang Zhang",
            "Tianyang Wang",
            "Hongying Luo",
            "Lu Chen",
            "Junming Huang",
            "Jibin Guan",
            "Junfeng Hao",
            "Junhao Song"
        ],
        "submitted": "2025-08-17 18:25:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LoraxBench: A Multitask, Multilingual Benchmark Suite for 20 Indonesian Languages",
        "abstract": "As one of the world's most populous countries, with 700 languages spoken,\nIndonesia is behind in terms of NLP progress. We introduce LoraxBench, a\nbenchmark that focuses on low-resource languages of Indonesia and covers 6\ndiverse tasks: reading comprehension, open-domain QA, language inference,\ncausal reasoning, translation, and cultural QA. Our dataset covers 20\nlanguages, with the addition of two formality registers for three languages. We\nevaluate a diverse set of multilingual and region-focused LLMs and found that\nthis benchmark is challenging. We note a visible discrepancy between\nperformance in Indonesian and other languages, especially the low-resource\nones. There is no clear lead when using a region-specific model as opposed to\nthe general multilingual model. Lastly, we show that a change in register\naffects model performance, especially with registers not commonly found in\nsocial media, such as high-level politeness `Krama' Javanese.",
        "url": "http://arxiv.org/abs/2508.12459v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12459v1",
        "arxiv_id": "2508.12459v1",
        "authors": [
            "Alham Fikri Aji",
            "Trevor Cohn"
        ],
        "submitted": "2025-08-17 18:07:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following",
        "abstract": "Large Vision-Language Models (LVLMs) hold immense potential for complex\nmultimodal instruction following, yet their development is often hindered by\nthe high cost and inconsistency of human annotation required for effective\nfine-tuning and preference alignment. Traditional supervised fine-tuning (SFT)\nand existing preference optimization methods like RLHF and DPO frequently\nstruggle to efficiently leverage the model's own generation space to identify\nhighly informative \"hard negative\" samples. To address these challenges, we\npropose Multimodal-Model-Guided Preference Optimization (M3PO), a novel and\ndata-efficient method designed to enhance LVLMs' capabilities in visual\ninstruction following. M3PO intelligently selects the most \"learning-valuable\"\npreference sample pairs from a diverse pool of LVLM-generated candidates. This\nselection is driven by a sophisticated mechanism that integrates two crucial\nsignals: a Multimodal Alignment Score (MAS) to assess external quality and the\nmodel's Self-Consistency / Confidence (log-probability) to gauge internal\nbelief. These are combined into a novel M3P-Score, which specifically\nidentifies preferred responses and challenging dispreferred responses that the\nmodel might confidently generate despite being incorrect. These high-quality\npreference pairs are then used for efficient Direct Preference Optimization\n(DPO) fine-tuning on base LVLMs like LLaVA-1.5 (7B/13B) using LoRA. Our\nextensive experiments demonstrate that M3PO consistently outperforms strong\nbaselines, including SFT, simulated RLHF, vanilla DPO, and RM-DPO, across a\ncomprehensive suite of multimodal instruction following benchmarks (MME-Bench,\nPOPE, IFT, Human Pref. Score).",
        "url": "http://arxiv.org/abs/2508.12458v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12458v1",
        "arxiv_id": "2508.12458v1",
        "authors": [
            "Ruirui Gao",
            "Emily Johnson",
            "Bowen Tan",
            "Yanfei Qian"
        ],
        "submitted": "2025-08-17 18:07:55",
        "source": "arxiv",
        "comment": null
    }
]