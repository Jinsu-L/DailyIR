# Daily Papers Report - 2025-07-24

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning

- **LLM Score**: 6
- **Keyword Score**: 13
- **Authors**: Aleksandr Perevalov, Andreas Both
- **URL**: <http://arxiv.org/abs/2507.16971v1>
- **Submitted**: 2025-07-22 19:23:03
- **Comment**: During the final evaluation on the DBpedia- and Corporate-based KGQA
  benchmarks within the Text2SPARQL challenge 2025, our approach took first
  place among the other participants
- **Topic Keywords**: information retrieval, query, queries, rag, retrieval
- **Reason**: The paper explores multilingual question answering over knowledge graphs, which is related to information retrieval and query understanding. However, the focus is on converting natural language questions into SPARQL queries, which is not directly aligned with ranking models or user behavior modeling. While the paper's use of human-inspired reasoning and modular subtasks is interesting, it does not seem to be a central match with the user's research interests.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Multilingual Knowledge Graph Question Answering (KGQA) using mKGQAgent Framework
- **Aim**: To develop a novel human-inspired framework for multilingual KGQA that breaks down the task of converting natural language questions into SPARQL queries into modular, interpretable subtasks
- **Rationale**: The proposed mKGQAgent architecture leverages a coordinated Large Language Model (LLM) agent workflow for planning, entity linking, and query refinement, guided by an experience pool for in-context learning
- **Ground**: The framework operates in two phases: offline and online, with the offline phase gathering intermediate processing steps for the experience pool, and the online phase using the experience pool examples to improve planning, SPARQL query generation, and feedback correction
- **Experiment**: Experiments were conducted on the QALD-9-plus benchmark, evaluating 10 languages, including two endangered languages, with results demonstrating the effectiveness of the mKGQAgent architecture, achieving superior performance even in non-English settings
- **Takeaway**: The mKGQAgent approach achieves state-of-the-art results on English and superior quality on other languages, with the comprehensive integration of all components yielding the most impressive results, and machine translation generally leading to higher KGQA performance compared to processing questions in their native languages

#### Abstract
> Accessing knowledge via multilingual natural-language interfaces is one of
the emerging challenges in the field of information retrieval and related ones.
Structured knowledge stored in knowledge graphs can be queried via a specific
query language (e.g., SPARQL). Therefore, one needs to transform
natural-language input into a query to fulfill an information need. Prior
approaches mostly focused on combining components (e.g., rule-based or
neural-based) that solve downstream tasks and come up with an answer at the
end. We introduce mKGQAgent, a human-inspired framework that breaks down the
task of converting natural language questions into SPARQL queries into modular,
interpretable subtasks. By leveraging a coordinated LLM agent workflow for
planning, entity linking, and query refinement - guided by an experience pool
for in-context learning - mKGQAgent efficiently handles multilingual KGQA.
Evaluated on the DBpedia- and Corporate-based KGQA benchmarks within the
Text2SPARQL challenge 2025, our approach took first place among the other
participants. This work opens new avenues for developing human-like reasoning
systems in multilingual semantic parsing.

---

### 2. Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs

- **LLM Score**: 6
- **Keyword Score**: 8
- **Authors**: Shuyuan Lin, Lei Duan, Philip Hughes, Yuxuan Sheng
- **URL**: <http://arxiv.org/abs/2507.16951v1>
- **Submitted**: 2025-07-22 18:44:18
- **Topic Keywords**: information retrieval, queries, retrieval
- **Reason**: The paper explores Conversational Information Retrieval (CIR) systems, which is related to Information Retrieval (IR) and Search technologies. The focus on unanswerable questions and trustworthy response generation is somewhat relevant to query understanding and ranking models. However, the paper's primary focus on Large Language Models (LLMs) and reinforcement learning is not directly aligned with the user's interests in user behavior modeling and click models.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Conversational Information Retrieval (CIR) systems
- **Aim**: To develop a novel approach, Self-Aware LLM for Unanswerability (SALU), to reliably handle unanswerable questions in CIR systems
- **Rationale**: LLMs should possess an intrinsic capability to understand and communicate their own limitations and knowledge gaps
- **Ground**: SALU integrates unanswerability detection directly within the Large Language Model's (LLM) generative process, trained on both standard Question Answering (QA) and explicit abstention generation for unanswerable queries
- **Experiment**: The authors evaluate SALU using the C-IR Answerability dataset, achieving superior reliability and outperforming strong baselines in overall accuracy for correctly answering or abstaining from questions
- **Takeaway**: SALU enables LLMs to express uncertainty or abstain from answering when information is unavailable, promoting a safer, more cautious response strategy and reducing the risks associated with factual errors and misleading information

#### Abstract
> Conversational Information Retrieval (CIR) systems, while offering intuitive
access to information, face a significant challenge: reliably handling
unanswerable questions to prevent the generation of misleading or hallucinated
content. Traditional approaches often rely on external classifiers, which can
introduce inconsistencies with the core generative Large Language Models
(LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a
novel approach that deeply integrates unanswerability detection directly within
the LLM's generative process. SALU is trained using a multi-task learning
framework for both standard Question Answering (QA) and explicit abstention
generation for unanswerable queries. Crucially, it incorporates a
confidence-score-guided reinforcement learning with human feedback (RLHF)
phase, which explicitly penalizes hallucinated responses and rewards
appropriate abstentions, fostering intrinsic self-awareness of knowledge
boundaries. Through extensive experiments on our custom-built
C-IR_Answerability dataset, SALU consistently outperforms strong baselines,
including hybrid LLM-classifier systems, in overall accuracy for correctly
answering or abstaining from questions. Human evaluation further confirms
SALU's superior reliability, achieving high scores in factuality, appropriate
abstention, and, most importantly, a dramatic reduction in hallucination,
demonstrating its ability to robustly "know when to say 'I don't know'."

---

### 3. VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings

- **LLM Score**: 6
- **Keyword Score**: 7
- **Authors**: Ramin Giahi, Kehui Yao, Sriram Kollipara, Kai Zhao, Vahid Mirjalili, Jianpeng Xu, Topojoy Biswas, Evren Korpeoglu, Kannan Achan
- **URL**: <http://arxiv.org/abs/2507.17080v1>
- **Submitted**: 2025-07-22 23:45:43
- **Comment**: Accepted at RecSys 2025; DOI:https://doi.org/10.1145/3705328.3748064
- **Topic Keywords**: ctr, retrieval, recommend, commerce, e-commerce
- **Reason**: The paper is somewhat related to your research interests in Information Retrieval and Search technologies, particularly in the context of e-commerce. The focus on multimodal recommendations and visual grounding is relevant to your background in e-commerce, but the emphasis on object-level alignment and domain mismatch is more specific to the e-commerce domain. While the paper does not directly address query understanding, ranking models, or user behavior modeling, it does explore the intersection of vision-language models and recommendation systems, which may be of interest to you.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: VL-CLIP: A Novel Framework for Multimodal Recommendations in E-commerce
- **Aim**: To develop a framework that enhances CLIP embeddings for multimodal recommendations in e-commerce platforms
- **Rationale**: To address the challenges of weak object-level alignment, ambiguous textual representations, and domain mismatch in e-commerce applications
- **Ground**: The authors integrate two components: Visual Grounding for fine-grained visual understanding and an LLM-based agent for generating enriched text embeddings
- **Experiment**: The authors evaluate the performance of VL-CLIP on a multi-modal retrieval task, comparing it to existing methods CLIP, GCL, and FashionCLIP, and conduct an ablation study to analyze the contribution of each component
- **Takeaway**: VL-CLIP achieves high performance in multi-modal retrieval tasks, particularly in e-commerce scenarios, and demonstrates its effectiveness in real-world e-commerce applications

#### Abstract
> Multimodal learning plays a critical role in e-commerce recommendation
platforms today, enabling accurate recommendations and product understanding.
However, existing vision-language models, such as CLIP, face key challenges in
e-commerce recommendation systems: 1) Weak object-level alignment, where global
image embeddings fail to capture fine-grained product attributes, leading to
suboptimal retrieval performance; 2) Ambiguous textual representations, where
product descriptions often lack contextual clarity, affecting cross-modal
matching; and 3) Domain mismatch, as generic vision-language models may not
generalize well to e-commerce-specific data. To address these limitations, we
propose a framework, VL-CLIP, that enhances CLIP embeddings by integrating
Visual Grounding for fine-grained visual understanding and an LLM-based agent
for generating enriched text embeddings. Visual Grounding refines image
representations by localizing key products, while the LLM agent enhances
textual features by disambiguating product descriptions. Our approach
significantly improves retrieval accuracy, multimodal retrieval effectiveness,
and recommendation quality across tens of millions of items on one of the
largest e-commerce platforms in the U.S., increasing CTR by 18.6%, ATC by
15.5%, and GMV by 4.0%. Additional experimental results show that our framework
outperforms vision-language models, including CLIP, FashionCLIP, and GCL, in
both precision and semantic alignment, demonstrating the potential of combining
object-aware visual grounding and LLM-enhanced text representation for robust
multimodal recommendations.

---

### 4. TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages of West Asia and North Africa

- **LLM Score**: 6
- **Keyword Score**: 4
- **Authors**: Parker Riley, Siamak Shakeri, Waleed Ammar, Jonathan H. Clark
- **URL**: <http://arxiv.org/abs/2507.17709v1>
- **Submitted**: 2025-07-23 17:20:28
- **Topic Keywords**: relevance, search
- **Reason**: The paper presents a question-answering dataset focused on languages of West Asia and North Africa, which is relevant to Information Retrieval and Search technologies. The emphasis on information-seeking questions and large text contexts is also aligned with my interests in query understanding and ranking models. However, the paper's focus on language-specific issues and cultural relevance may not be directly applicable to my primary research areas.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: TYDI QA-WANA: A Novel Question-Answering Dataset for Low-Resource Language Varieties
- **Aim**: To create a dataset for training and evaluating long-context language models in low-resource language varieties
- **Rationale**: To address the need for training and evaluation data in low-resource language varieties and to test the ability of long-context language models to utilize large text contexts
- **Ground**: The dataset consists of 28,000 examples in 10 language varieties of Western Asia and Northern Africa, with a high proportion of NULL examples in some language varieties
- **Experiment**: The authors evaluate the performance of two baseline models on the dataset, reporting exact match (EM) and F1 scores for 10 language varieties
- **Takeaway**: The dataset provides a challenging benchmark for evaluating the ability of long-context language models to utilize large text contexts in low-resource language varieties

#### Abstract
> We present TyDi QA-WANA, a question-answering dataset consisting of 28K
examples divided among 10 language varieties of western Asia and northern
Africa. The data collection process was designed to elicit information-seeking
questions, where the asker is genuinely curious to know the answer. Each
question in paired with an entire article that may or may not contain the
answer; the relatively large size of the articles results in a task suitable
for evaluating models' abilities to utilize large text contexts in answering
questions. Furthermore, the data was collected directly in each language
variety, without the use of translation, in order to avoid issues of cultural
relevance. We present performance of two baseline models, and release our code
and data to facilitate further improvement by the research community.

---

### 5. Millions of $\text{GeAR}$-s: Extending GraphRAG to Millions of Documents

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Zhili Shen, Chenxin Diao, Pascual Merita, Pavlos Vougiouklis, Jeff Z. Pan
- **URL**: <http://arxiv.org/abs/2507.17399v1>
- **Submitted**: 2025-07-23 10:54:24
- **Comment**: Accepted by SIGIR 2025 LiveRAG Challenge Program
- **Topic Keywords**: query, rag, retrieval, sigir
- **Reason**: The paper explores graph-based approaches to retrieval-augmented generation, which is related to query understanding and ranking models in Information Retrieval. However, the focus on graph-based methods and the specific challenge they address (SIGIR 2025 LiveRAG Challenge) do not directly align with the user's primary interests in query understanding, ranking models, and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Adapting Graph-based Retrieval-Augmented Generation (RAG) for Large-Scale Document Retrieval
- **Aim**: To explore how GeAR can be adapted to scale to larger or more diverse datasets
- **Rationale**: To leverage information such as entities and their relations extracted from documents to enhance retrieval performance
- **Ground**: The authors propose an online approach to align an index of passages with triples from an existing Knowledge Graph (KG), such as Wikidata, without requiring an offline triple extraction step
- **Experiment**: The authors achieved correctness and faithfulness scores of 0.875714 and 0.529335, respectively, using conventional retrieval strategies to align Wikidata triples with FineWeb passages
- **Takeaway**: The paper highlights the need for careful consideration in the linking process to ensure the integrity and interpretability of the resulting knowledge graph, and identifies limitations in the current framework, including the need for improved asymmetric semantic models

#### Abstract
> Recent studies have explored graph-based approaches to retrieval-augmented
generation, leveraging structured or semi-structured information -- such as
entities and their relations extracted from documents -- to enhance retrieval.
However, these methods are typically designed to address specific tasks, such
as multi-hop question answering and query-focused summarisation, and therefore,
there is limited evidence of their general applicability across broader
datasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG
solution: $\text{GeAR}$ and explore its performance and limitations on the
SIGIR 2025 LiveRAG Challenge.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. QuMAB: Query-based Multi-annotator Behavior Pattern Learning

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Liyun Zhang, Zheng Lian, Hong Liu, Takanori Takebe, Yuta Nakashima
- **URL**: <http://arxiv.org/abs/2507.17653v1>
- **Submitted**: 2025-07-23 16:17:43
- **Comment**: 12 pages. arXiv admin note: substantial text overlap with
  arXiv:2503.15237
- **Topic Keywords**: query, queries, rag
- **Reason**: The paper QuMAB: Query-based Multi-annotator Behavior Pattern Learning is somewhat related to your research interests in Information Retrieval and Search technologies, particularly in query understanding and behavior modeling. However, the focus on multi-annotator behavior pattern learning and annotation cost reduction is not directly aligned with your primary interests in ranking models and user behavior modeling.

#### Abstract
> Multi-annotator learning traditionally aggregates diverse annotations to
approximate a single ground truth, treating disagreements as noise. However,
this paradigm faces fundamental challenges: subjective tasks often lack
absolute ground truth, and sparse annotation coverage makes aggregation
statistically unreliable. We introduce a paradigm shift from sample-wise
aggregation to annotator-wise behavior modeling. By treating annotator
disagreements as valuable information rather than noise, modeling
annotator-specific behavior patterns can reconstruct unlabeled data to reduce
annotation cost, enhance aggregation reliability, and explain annotator
decision behavior. To this end, we propose QuMATL (Query-based Multi-Annotator
Behavior Pattern Learning), which uses light-weight queries to model individual
annotators while capturing inter-annotator correlations as implicit
regularization, preventing overfitting to sparse individual data while
maintaining individualization and improving generalization, with a
visualization of annotator focus regions offering an explainable analysis of
behavior understanding. We contribute two large-scale datasets with dense
per-annotator labels: STREET (4,300 labels/annotator) and AMER (average 3,118
labels/annotator), the first multimodal multi-annotator dataset.

### 7. Citation Recommendation using Deep Canonical Correlation Analysis

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Conor McNamara, Effirul Ramlan
- **URL**: <http://arxiv.org/abs/2507.17603v1>
- **Submitted**: 2025-07-23 15:34:07
- **Comment**: 21 pages, 6 figures, 7 tables
- **Topic Keywords**: ranking, rag, recommend, rank
- **Reason**: The paper focuses on citation recommendation using Deep Canonical Correlation Analysis, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and query understanding. Although it touches on representation learning and ranking, the context is different and the paper's emphasis on citation recommendation and graph-based representations does not align with the user's interests.

#### Abstract
> Recent advances in citation recommendation have improved accuracy by
leveraging multi-view representation learning to integrate the various
modalities present in scholarly documents. However, effectively combining
multiple data views requires fusion techniques that can capture complementary
information while preserving the unique characteristics of each modality. We
propose a novel citation recommendation algorithm that improves upon linear
Canonical Correlation Analysis (CCA) methods by applying Deep CCA (DCCA), a
neural network extension capable of capturing complex, non-linear relationships
between distributed textual and graph-based representations of scientific
articles. Experiments on the large-scale DBLP (Digital Bibliography & Library
Project) citation network dataset demonstrate that our approach outperforms
state-of-the-art CCA-based methods, achieving relative improvements of over 11%
in Mean Average Precision@10, 5% in Precision@10, and 7% in Recall@10. These
gains reflect more relevant citation recommendations and enhanced ranking
quality, suggesting that DCCA's non-linear transformations yield more
expressive latent representations than CCA's linear projections.

### 8. BoSS: Beyond-Semantic Speech

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Qing Wang, Zehan Li, Hang Lv, Hongjie Chen, Yaodong Song, Jian Kang, Jie Lian, Jie Li, Yongxiang Li, Zhongjiang He, Xuelong Li
- **URL**: <http://arxiv.org/abs/2507.17563v1>
- **Submitted**: 2025-07-23 14:53:50
- **Topic Keywords**: relevance, rag, search
- **Reason**: The paper explores the concept of 'Beyond-Semantic Speech' which is related to the implicit signals and contextual cues in human communication. While it's not directly focused on Information Retrieval or Search technologies, it touches on the idea of understanding communicative intentions and scenarios, which is somewhat relevant to query understanding and user behavior modeling. However, the paper's primary focus is on speech technologies and does not directly align with the user's core research themes.

#### Abstract
> Human communication involves more than explicit semantics, with implicit
signals and contextual cues playing a critical role in shaping meaning.
However, modern speech technologies, such as Automatic Speech Recognition (ASR)
and Text-to-Speech (TTS) often fail to capture these beyond-semantic
dimensions. To better characterize and benchmark the progression of speech
intelligence, we introduce Spoken Interaction System Capability Levels (L1-L5),
a hierarchical framework illustrated the evolution of spoken dialogue systems
from basic command recognition to human-like social interaction. To support
these advanced capabilities, we propose Beyond-Semantic Speech (BoSS), which
refers to the set of information in speech communication that encompasses but
transcends explicit semantics. It conveys emotions, contexts, and modifies or
extends meanings through multidimensional features such as affective cues,
contextual dynamics, and implicit semantics, thereby enhancing the
understanding of communicative intentions and scenarios. We present a
formalized framework for BoSS, leveraging cognitive relevance theories and
machine learning models to analyze temporal and contextual speech dynamics. We
evaluate BoSS-related attributes across five different dimensions, reveals that
current spoken language models (SLMs) are hard to fully interpret
beyond-semantic signals. These findings highlight the need for advancing BoSS
research to enable richer, more context-aware human-machine communication.

### 9. Each to Their Own: Exploring the Optimal Embedding in RAG

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Shiting Chen, Zijian Zhao, Jinsong Chen
- **URL**: <http://arxiv.org/abs/2507.17442v1>
- **Submitted**: 2025-07-23 12:03:54
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper explores Retrieval-Augmented Generation (RAG) and proposes two approaches to enhance it, but it does not directly relate to information retrieval, query understanding, or ranking models, which are core areas of interest. While it touches on embedding models, it is primarily focused on language models and their applications, which is a related but distinct area.

#### Abstract
> Recently, as Large Language Models (LLMs) have fundamentally impacted various
fields, the methods for incorporating up-to-date information into LLMs or
adding external knowledge to construct domain-specific models have garnered
wide attention. Retrieval-Augmented Generation (RAG), serving as an
inference-time scaling method, is notable for its low cost and minimal effort
for parameter tuning. However, due to heterogeneous training data and model
architecture, the variant embedding models used in RAG exhibit different
benefits across various areas, often leading to different similarity
calculation results and, consequently, varying response quality from LLMs. To
address this problem, we propose and examine two approaches to enhance RAG by
combining the benefits of multiple embedding models, named Mixture-Embedding
RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects
retrievals from multiple embedding models based on standardized similarity;
however, it does not outperform vanilla RAG. In contrast, Confident RAG
generates responses multiple times using different embedding models and then
selects the responses with the highest confidence level, demonstrating average
improvements of approximately 10% and 5% over vanilla LLMs and RAG,
respectively. The consistent results across different LLMs and embedding models
indicate that Confident RAG is an efficient plug-and-play approach for various
domains. We will release our code upon publication.

### 10. "Beyond the past": Leveraging Audio and Human Memory for Sequential Music Recommendation

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Viet-Tran Anh, Bruno Sguerra, Gabriel Meseguer-Brocal, Lea Briand, Manuel Moussallam
- **URL**: <http://arxiv.org/abs/2507.17356v1>
- **Submitted**: 2025-07-23 09:37:23
- **Topic Keywords**: rag, recommend, search
- **Reason**: The paper is somewhat related to information retrieval and search technologies, as it involves sequential recommendation systems. However, the focus on music recommendation and human memory is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling. The paper's relevance to the user's background in e-commerce is also limited.

#### Abstract
> On music streaming services, listening sessions are often composed of a
balance of familiar and new tracks. Recently, sequential recommender systems
have adopted cognitive-informed approaches, such as Adaptive Control of
Thought-Rational (ACT-R), to successfully improve the prediction of the most
relevant tracks for the next user session. However, one limitation of using a
model inspired by human memory (or the past), is that it struggles to recommend
new tracks that users have not previously listened to. To bridge this gap, here
we propose a model that leverages audio information to predict in advance the
ACT-R-like activation of new tracks and incorporates them into the
recommendation scoring process. We demonstrate the empirical effectiveness of
the proposed model using proprietary data, which we publicly release along with
the model's source code to foster future research in this field.

### 11. Exploring the Potential of LLMs for Serendipity Evaluation in Recommender Systems

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Li Kang, Yuhan Zhao, Li Chen
- **URL**: <http://arxiv.org/abs/2507.17290v1>
- **Submitted**: 2025-07-23 07:51:56
- **Comment**: RecSys2025
- **Topic Keywords**: recommend, commerce, e-commerce, search
- **Reason**: The paper explores the potential of large language models (LLMs) for serendipity evaluation in recommender systems, which is somewhat related to my interests in information retrieval and search technologies. However, the focus on recommender systems and serendipity evaluation is not directly aligned with my primary research themes, and the paper does not address query understanding, ranking models, or user behavior modeling.

#### Abstract
> Serendipity plays a pivotal role in enhancing user satisfaction within
recommender systems, yet its evaluation poses significant challenges due to its
inherently subjective nature and conceptual ambiguity. Current algorithmic
approaches predominantly rely on proxy metrics for indirect assessment, often
failing to align with real user perceptions, thus creating a gap. With large
language models (LLMs) increasingly revolutionizing evaluation methodologies
across various human annotation tasks, we are inspired to explore a core
research proposition: Can LLMs effectively simulate human users for serendipity
evaluation? To address this question, we conduct a meta-evaluation on two
datasets derived from real user studies in the e-commerce and movie domains,
focusing on three key aspects: the accuracy of LLMs compared to conventional
proxy metrics, the influence of auxiliary data on LLM comprehension, and the
efficacy of recently popular multi-LLM techniques. Our findings indicate that
even the simplest zero-shot LLMs achieve parity with, or surpass, the
performance of conventional metrics. Furthermore, multi-LLM techniques and the
incorporation of auxiliary data further enhance alignment with human
perspectives. Based on our findings, the optimal evaluation by LLMs yields a
Pearson correlation coefficient of 21.5\% when compared to the results of the
user study. This research implies that LLMs may serve as potentially accurate
and cost-effective evaluators, introducing a new paradigm for serendipity
evaluation in recommender systems.

### 12. Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Arduin Findeis, Floris Weers, Guoli Yin, Ke Ye, Ruoming Pang, Tom Gunter
- **URL**: <http://arxiv.org/abs/2507.17015v1>
- **Submitted**: 2025-07-22 20:57:09
- **Comment**: Accepted at ACL 2025
- **Topic Keywords**: pairwise, search
- **Reason**: The paper explores the use of external validation tools to improve annotation quality for large language models, which is related to information retrieval and search technologies. However, the focus on annotation quality and model evaluation is not directly aligned with the user's primary interest in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat limited to the user's background in e-commerce and NLP, but it does not address the user's core research themes.

#### Abstract
> Pairwise preferences over model responses are widely collected to evaluate
and provide feedback to large language models (LLMs). Given two alternative
model responses to the same input, a human or AI annotator selects the "better"
response. This approach can provide feedback for domains where other hard-coded
metrics are difficult to obtain (e.g., chat response quality), thereby helping
model evaluation or training. However, for some domains high-quality pairwise
comparisons can be tricky to obtain - from AI and humans. For example, for
responses with many factual statements, annotators may disproportionately weigh
writing quality rather than underlying facts. In this work, we explore
augmenting standard AI annotator systems with additional tools to improve
performance on three challenging response domains: long-form factual, math and
code tasks. We propose a tool-using agentic system to provide higher quality
feedback on these domains. Our system uses web-search and code execution to
ground itself based on external validation, independent of the LLM's internal
knowledge and biases. We provide extensive experimental results evaluating our
method across the three targeted response domains as well as general annotation
tasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as
three new datasets for domains with saturated pre-existing datasets. Our
results indicate that external tools can indeed improve performance in many,
but not all, cases. More generally, our experiments highlight the sensitivity
of performance to simple parameters (e.g., prompt) and the need for improved
(non-saturated) annotator benchmarks. We share our code at
https://github.com/apple/ml-agent-evaluator.

### 13. Leave No One Behind: Fairness-Aware Cross-Domain Recommender Systems for Non-Overlapping Users

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Weixin Chen, Yuhan Zhao, Li Chen, Weike Pan
- **URL**: <http://arxiv.org/abs/2507.17749v1>
- **Submitted**: 2025-07-23 17:59:08
- **Comment**: Accepted by RecSys 2025
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on recommender systems, which is a related topic, but it's not directly aligned with your primary interest in Information Retrieval and Search technologies. The paper's emphasis on fairness-aware cross-domain recommender systems and virtual user embeddings is not a central match for your research themes.

#### Abstract
> Cross-domain recommendation (CDR) methods predominantly leverage overlapping
users to transfer knowledge from a source domain to a target domain. However,
through empirical studies, we uncover a critical bias inherent in these
approaches: while overlapping users experience significant enhancements in
recommendation quality, non-overlapping users benefit minimally and even face
performance degradation. This unfairness may erode user trust, and,
consequently, negatively impact business engagement and revenue. To address
this issue, we propose a novel solution that generates virtual source-domain
users for non-overlapping target-domain users. Our method utilizes a dual
attention mechanism to discern similarities between overlapping and
non-overlapping users, thereby synthesizing realistic virtual user embeddings.
We further introduce a limiter component that ensures the generated virtual
users align with real-data distributions while preserving each user's unique
characteristics. Notably, our method is model-agnostic and can be seamlessly
integrated into any CDR model. Comprehensive experiments conducted on three
public datasets with five CDR baselines demonstrate that our method effectively
mitigates the CDR non-overlapping user bias, without loss of overall accuracy.
Our code is publicly available at https://github.com/WeixinChen98/VUG.

### 14. Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Victor Hartman, Petter T√∂rnberg
- **URL**: <http://arxiv.org/abs/2507.17636v1>
- **Submitted**: 2025-07-23 16:02:52
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on using Large Language Models (LLMs) for cross-lingual classification of negative campaigning in tweets, which is related to Natural Language Processing (NLP) and information retrieval. However, the topic is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling, and the paper's focus on political communication and campaign analysis is not a central match.

#### Abstract
> Negative campaigning is a central feature of political competition, yet
empirical research has been limited by the high cost and limited scalability of
existing classification methods. This study makes two key contributions. First,
it introduces zero-shot Large Language Models (LLMs) as a novel approach for
cross-lingual classification of negative campaigning. Using benchmark datasets
in ten languages, we demonstrate that LLMs achieve performance on par with
native-speaking human coders and outperform conventional supervised machine
learning approaches. Second, we leverage this novel method to conduct the
largest cross-national study of negative campaigning to date, analyzing 18
million tweets posted by parliamentarians in 19 European countries between 2017
and 2022. The results reveal consistent cross-national patterns: governing
parties are less likely to use negative messaging, while ideologically extreme
and populist parties -- particularly those on the radical right -- engage in
significantly higher levels of negativity. These findings advance our
understanding of how party-level characteristics shape strategic communication
in multiparty systems. More broadly, the study demonstrates the potential of
LLMs to enable scalable, transparent, and replicable research in political
communication across linguistic and cultural contexts.

### 15. URPO: A Unified Reward & Policy Optimization Framework for Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Songshuo Lu, Hua Wang, Zhi Chen, Yaohua Tang
- **URL**: <http://arxiv.org/abs/2507.17515v1>
- **Submitted**: 2025-07-23 13:52:27
- **Topic Keywords**: rag
- **Reason**: The paper proposes a framework for optimizing large language models, focusing on reward and policy optimization. While it touches on topics related to search and ranking, such as optimization and evaluation, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user. The paper's focus on language models and alignment pipelines is not directly relevant to the user's research themes.

#### Abstract
> Large-scale alignment pipelines typically pair a policy model with a
separately trained reward model whose parameters remain frozen during
reinforcement learning (RL). This separation creates a complex,
resource-intensive pipeline and suffers from a performance ceiling due to a
static reward signal. We propose a novel framework, Unified Reward & Policy
Optimization (URPO), that unifies instruction-following ("player") and reward
modeling ("referee") within a single model and a single training phase. Our
method recasts all alignment data-including preference pairs, verifiable
reasoning, and open-ended instructions-into a unified generative format
optimized by a single Group-Relative Policy Optimization (GRPO) loop. This
enables the model to learn from ground-truth preferences and verifiable logic
while simultaneously generating its own rewards for open-ended tasks.
Experiments on the Qwen2.5-7B model demonstrate URPO's superiority. Our unified
model significantly outperforms a strong baseline using a separate generative
reward model, boosting the instruction-following score on AlpacaEval from 42.24
to 44.84 and the composite reasoning average from 32.66 to 35.66. Furthermore,
URPO cultivates a superior internal evaluator as a byproduct of training,
achieving a RewardBench score of 85.15 and surpassing the dedicated reward
model it replaces (83.55). By eliminating the need for a separate reward model
and fostering a co-evolutionary dynamic between generation and evaluation, URPO
presents a simpler, more efficient, and more effective path towards robustly
aligned language models.

### 16. R4ec: A Reasoning, Reflection, and Refinement Framework for Recommendation Systems

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Hao Gu, Rui Zhong, Yu Xia, Wei Yang, Chi Lu, Peng Jiang, Kun Gai
- **URL**: <http://arxiv.org/abs/2507.17249v1>
- **Submitted**: 2025-07-23 06:36:49
- **Comment**: Accepted by Recsys25
- **Topic Keywords**: recommend, search
- **Reason**: The paper proposes a framework for recommendation systems that leverages large language models, but its focus on reasoning, reflection, and refinement is not directly related to information retrieval or search technologies. While it touches on the use of language models, the paper's primary concern is improving recommendation systems, which is outside the user's core research themes.

#### Abstract
> Harnessing Large Language Models (LLMs) for recommendation systems has
emerged as a prominent avenue, drawing substantial research interest. However,
existing approaches primarily involve basic prompt techniques for knowledge
acquisition, which resemble System-1 thinking. This makes these methods highly
sensitive to errors in the reasoning path, where even a small mistake can lead
to an incorrect inference. To this end, in this paper, we propose $R^{4}$ec, a
reasoning, reflection and refinement framework that evolves the recommendation
system into a weak System-2 model. Specifically, we introduce two models: an
actor model that engages in reasoning, and a reflection model that judges these
responses and provides valuable feedback. Then the actor model will refine its
response based on the feedback, ultimately leading to improved responses. We
employ an iterative reflection and refinement process, enabling LLMs to
facilitate slow and deliberate System-2-like thinking. Ultimately, the final
refined knowledge will be incorporated into a recommendation backbone for
prediction. We conduct extensive experiments on Amazon-Book and MovieLens-1M
datasets to demonstrate the superiority of $R^{4}$ec. We also deploy $R^{4}$ec
on a large scale online advertising platform, showing 2.2\% increase of
revenue. Furthermore, we investigate the scaling properties of the actor model
and reflection model.

### 17. Parallelism Meets Adaptiveness: Scalable Documents Understanding in Multi-Agent LLM Systems

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Chengxuan Xia, Qianye Wu, Sixuan Tian, Yilun Hao
- **URL**: <http://arxiv.org/abs/2507.17061v1>
- **Submitted**: 2025-07-22 22:42:51
- **Comment**: 8 pages, 2 figures
- **Topic Keywords**: rag
- **Reason**: The paper proposes a coordination framework for multi-agent large language model systems, focusing on adaptiveness and parallelism. While it touches on the idea of dynamic task routing and feedback, it does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval and Search technologies.

#### Abstract
> Large language model (LLM) agents have shown increasing promise for
collaborative task completion. However, existing multi-agent frameworks often
rely on static workflows, fixed roles, and limited inter-agent communication,
reducing their effectiveness in open-ended, high-complexity domains. This paper
proposes a coordination framework that enables adaptiveness through three core
mechanisms: dynamic task routing, bidirectional feedback, and parallel agent
evaluation. The framework allows agents to reallocate tasks based on confidence
and workload, exchange structured critiques to iteratively improve outputs, and
crucially compete on high-ambiguity subtasks with evaluator-driven selection of
the most suitable result. We instantiate these principles in a modular
architecture and demonstrate substantial improvements in factual coverage,
coherence, and efficiency over static and partially adaptive baselines. Our
findings highlight the benefits of incorporating both adaptiveness and
structured competition in multi-agent LLM systems.

### 18. A Unifying Scheme for Extractive Content Selection Tasks

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Shmuel Amar, Ori Shapira, Aviv Slobodkin, Ido Dagan
- **URL**: <http://arxiv.org/abs/2507.16922v1>
- **Submitted**: 2025-07-22 18:02:54
- **Topic Keywords**: rag
- **Reason**: The paper proposes a unified framework for content selection tasks, which is related to Information Retrieval and Natural Language Processing. However, the focus is on extractive content selection, which is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's emphasis on language models and transfer learning is somewhat relevant, but the connection to the user's research themes is limited.

#### Abstract
> A broad range of NLP tasks involve selecting relevant text spans from given
source texts. Despite this shared objective, such \textit{content selection}
tasks have traditionally been studied in isolation, each with its own modeling
approaches, datasets, and evaluation metrics. In this work, we propose
\textit{instruction-guided content selection (IGCS)} as a beneficial unified
framework for such settings, where the task definition and any
instance-specific request are encapsulated as instructions to a language model.
To promote this framework, we introduce \igcsbench{}, the first unified
benchmark covering diverse content selection tasks. Further, we create a large
generic synthetic dataset that can be leveraged for diverse content selection
tasks, and show that transfer learning with these datasets often boosts
performance, whether dedicated training for the targeted task is available or
not. Finally, we address generic inference time issues that arise in LLM-based
modeling of content selection, assess a generic evaluation metric, and overall
propose the utility of our resources and methods for future content selection
models. Models and datasets available at https://github.com/shmuelamar/igcs.

### 19. HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning

- **LLM Score**: 2
- **Keyword Score**: 10
- **Authors**: Li Jun, Wang Jinpeng, Tan Chaolei, Lian Niu, Chen Long, Zhang Min, Wang Yaowei, Xia Shu-Tao, Chen Bin
- **URL**: <http://arxiv.org/abs/2507.17402v1>
- **Submitted**: 2025-07-23 10:59:46
- **Comment**: Accepted by ICCV'25. 13 pages, 6 figures, 4 tables
- **Topic Keywords**: queries, relevance, rag, retrieval
- **Reason**: The paper focuses on Partially Relevant Video Retrieval, which is not directly related to Information Retrieval or Search technologies. Although it uses attention mechanisms and hierarchical modeling, the context is video retrieval, which is not a core area of interest for the user. The paper's relevance to the user's research interests is limited.

#### Abstract
> Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of
matching untrimmed videos with text queries describing only partial content.
Existing methods suffer from geometric distortion in Euclidean space that
sometimes misrepresents the intrinsic hierarchical structure of videos and
overlooks certain hierarchical semantics, ultimately leading to suboptimal
temporal modeling. To address this issue, we propose the first hyperbolic
modeling framework for PRVR, namely HLFormer, which leverages hyperbolic space
learning to compensate for the suboptimal hierarchical modeling capabilities of
Euclidean space. Specifically, HLFormer integrates the Lorentz Attention Block
and Euclidean Attention Block to encode video embeddings in hybrid spaces,
using the Mean-Guided Adaptive Interaction Module to dynamically fuse features.
Additionally, we introduce a Partial Order Preservation Loss to enforce "text <
video" hierarchy through Lorentzian cone constraints. This approach further
enhances cross-modal matching by reinforcing partial relevance between video
content and text queries. Extensive experiments show that HLFormer outperforms
state-of-the-art methods. Code is released at
https://github.com/lijun2005/ICCV25-HLFormer.

### 20. GenSelect: A Generative Approach to Best-of-N

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Shubham Toshniwal, Ivan Sorokin, Aleksander Ficek, Ivan Moshkov, Igor Gitman
- **URL**: <http://arxiv.org/abs/2507.17797v1>
- **Submitted**: 2025-07-23 15:22:51
- **Comment**: Presented at the 2nd AI for MATH Workshop @ ICML
- **Topic Keywords**: pointwise, pairwise, rag
- **Reason**: The paper focuses on generative reward models and parallel sampling for reasoning tasks, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions large language models (LLMs), the context is not about search or ranking, but rather about math reasoning and scoring approaches.

#### Abstract
> Generative reward models with parallel sampling have enabled effective
test-time scaling for reasoning tasks. Current approaches employ pointwise
scoring of individual solutions or pairwise comparisons. However, pointwise
methods underutilize LLMs' comparative abilities, while pairwise methods scale
inefficiently with larger sampling budgets. We introduce GenSelect, where the
LLM uses long reasoning to select the best solution among N candidates. This
leverages LLMs' comparative strengths while scaling efficiently across parallel
sampling budgets. For math reasoning, we demonstrate that reasoning models,
such as QwQ and DeepSeek-R1-0528, excel at GenSelect, outperforming existing
scoring approaches with simple prompting.

### 21. Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for Tumor Flagging and Staging

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Farnaz Khun Jush, Steffen Vogler, Matthias Lenga
- **URL**: <http://arxiv.org/abs/2507.17412v1>
- **Submitted**: 2025-07-23 11:12:52
- **Topic Keywords**: ranking, retrieval, rank, search
- **Reason**: The paper focuses on content-based image retrieval for medical images, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although the paper mentions re-ranking, it is not specifically related to query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user.

#### Abstract
> The increasing volume of medical images poses challenges for radiologists in
retrieving relevant cases. Content-based image retrieval (CBIR) systems offer
potential for efficient access to similar cases, yet lack standardized
evaluation and comprehensive studies. Building on prior studies for tumor
characterization via CBIR, this study advances CBIR research for volumetric
medical images through three key contributions: (1) a framework eliminating
reliance on pre-segmented data and organ-specific datasets, aligning with large
and unstructured image archiving systems, i.e. PACS in clinical practice; (2)
introduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT's
contextualized late interaction mechanism for 3D medical imaging; (3)
comprehensive evaluation across four tumor sites using three feature extractors
and three database configurations. Our evaluations highlight the significant
advantages of C-MIR. We demonstrate the successful adaptation of the late
interaction principle to volumetric medical images, enabling effective
context-aware re-ranking. A key finding is C-MIR's ability to effectively
localize the region of interest, eliminating the need for pre-segmentation of
datasets and offering a computationally efficient alternative to systems
relying on expensive data enrichment steps. C-MIR demonstrates promising
improvements in tumor flagging, achieving improved performance, particularly
for colon and lung tumors (p<0.05). C-MIR also shows potential for improving
tumor staging, warranting further exploration of its capabilities. Ultimately,
our work seeks to bridge the gap between advanced retrieval techniques and
their practical applications in healthcare, paving the way for improved
diagnostic processes.

### 22. EndoFinder: Online Lesion Retrieval for Explainable Colorectal Polyp Diagnosis Leveraging Latent Scene Representations

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Ruijie Yang, Yan Zhu, Peiyao Fu, Yizhe Zhang, Zhihua Wang, Quanlin Li, Pinghong Zhou, Xian Yang, Shuo Wang
- **URL**: <http://arxiv.org/abs/2507.17323v1>
- **Submitted**: 2025-07-23 08:45:19
- **Topic Keywords**: queries, rag, retrieval
- **Reason**: This paper is not relevant to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on medical imaging and diagnosis, using techniques like contrastive learning and scene representation transformers, which are not directly related to the user's areas of interest.

#### Abstract
> Colorectal cancer (CRC) remains a leading cause of cancer-related mortality,
underscoring the importance of timely polyp detection and diagnosis. While deep
learning models have improved optical-assisted diagnostics, they often demand
extensive labeled datasets and yield "black-box" outputs with limited
interpretability. In this paper, we propose EndoFinder, an online polyp
retrieval framework that leverages multi-view scene representations for
explainable and scalable CRC diagnosis. First, we develop a Polyp-aware Image
Encoder by combining contrastive learning and a reconstruction task, guided by
polyp segmentation masks. This self-supervised approach captures robust
features without relying on large-scale annotated data. Next, we treat each
polyp as a three-dimensional "scene" and introduce a Scene Representation
Transformer, which fuses multiple views of the polyp into a single latent
representation. By discretizing this representation through a hashing layer,
EndoFinder enables real-time retrieval from a compiled database of historical
polyp cases, where diagnostic information serves as interpretable references
for new queries. We evaluate EndoFinder on both public and newly collected
polyp datasets for re-identification and pathology classification. Results show
that EndoFinder outperforms existing methods in accuracy while providing
transparent, retrieval-based insights for clinical decision-making. By
contributing a novel dataset and a scalable, explainable framework, our work
addresses key challenges in polyp diagnosis and offers a promising direction
for more efficient AI-driven colonoscopy workflows. The source code is
available at https://github.com/ku262/EndoFinder-Scene.

### 23. Triadic First-Order Logic Queries in Temporal Networks

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Omkar Bhalerao, Yunjie Pan, C. Seshadhri, Nishil Talati
- **URL**: <http://arxiv.org/abs/2507.17215v1>
- **Submitted**: 2025-07-23 05:12:23
- **Topic Keywords**: query, queries, search
- **Reason**: This paper is not directly related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on network analysis, motif counting, and temporal graph mining, which are not core topics in the user's research areas.

#### Abstract
> Motif counting is a fundamental problem in network analysis, and there is a
rich literature of theoretical and applied algorithms for this problem. Given a
large input network $G$, a motif $H$ is a small "pattern" graph indicative of
special local structure. Motif/pattern mining involves finding all matches of
this pattern in the input $G$. The simplest, yet challenging, case of motif
counting is when $H$ has three vertices, often called a "triadic" query. Recent
work has focused on "temporal graph mining", where the network $G$ has edges
with timestamps (and directions) and $H$ has time constraints.
  Inspired by concepts in logic and database theory, we introduce the study of
"thresholded First Order Logic (FOL) Motif Analysis" for massive temporal
networks. A typical triadic motif query asks for the existence of three
vertices that form a desired temporal pattern. An "FOL" motif query is obtained
by having both existential and thresholded universal quantifiers. This allows
for query semantics that can mine richer information from networks. A typical
triadic query would be "find all triples of vertices $u,v,w$ such that they
form a triangle within one hour". A thresholded FOL query can express "find all
pairs $u,v$ such that for half of $w$ where $(u,w)$ formed an edge, $(v,w)$
also formed an edge within an hour".
  We design the first algorithm, FOLTY, for mining thresholded FOL triadic
queries. The theoretical running time of FOLTY matches the best known running
time for temporal triangle counting in sparse graphs. We give an efficient
implementation of FOLTY using specialized temporal data structures. FOLTY has
excellent empirical behavior, and can answer triadic FOL queries on graphs with
nearly 70M edges is less than hour on commodity hardware. Our work has the
potential to start a new research direction in the classic well-studied problem
of motif analysis.

### 24. Triple X: A LLM-Based Multilingual Speech Recognition System for the INTERSPEECH2025 MLC-SLM Challenge

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Miaomiao Gao, Xiaoxiao Xiang, Yiwen Guo
- **URL**: <http://arxiv.org/abs/2507.17288v1>
- **Submitted**: 2025-07-23 07:48:33
- **Topic Keywords**: ranking, rag, rank
- **Reason**: The paper focuses on speech recognition, a topic outside the user's primary interest in Information Retrieval and Search technologies. Although it involves large language models, the context is different from the user's focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> This paper describes our Triple X speech recognition system submitted to Task
1 of the Multi-Lingual Conversational Speech Language Modeling (MLC-SLM)
Challenge. Our work focuses on optimizing speech recognition accuracy in
multilingual conversational scenarios through an innovative encoder-adapter-LLM
architecture. This framework harnesses the powerful reasoning capabilities of
text-based large language models while incorporating domain-specific
adaptations. To further enhance multilingual recognition performance, we
adopted a meticulously designed multi-stage training strategy leveraging
extensive multilingual audio datasets. Experimental results demonstrate that
our approach achieves competitive Word Error Rate (WER) performance on both dev
and test sets, obtaining second place in the challenge ranking.

### 25. LLM4MEA: Data-free Model Extraction Attacks on Sequential Recommenders via Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Shilong Zhao, Fei Sun, Kaike Zhang, Shaoling Jing, Du Su, Zhichao Shi, Zhiyi Yin, Huawei Shen, Xueqi Cheng
- **URL**: <http://arxiv.org/abs/2507.16969v1>
- **Submitted**: 2025-07-22 19:20:23
- **Topic Keywords**: rag, user behavior, recommend, rank
- **Reason**: The paper focuses on Model Extraction Attacks on sequential recommenders, which is not directly related to the user's primary interest in Information Retrieval and Search technologies. Although it mentions Large Language Models, the context is different from the user's interest in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Recent studies have demonstrated the vulnerability of sequential recommender
systems to Model Extraction Attacks (MEAs). MEAs collect responses from
recommender systems to replicate their functionality, enabling unauthorized
deployments and posing critical privacy and security risks. Black-box attacks
in prior MEAs are ineffective at exposing recommender system vulnerabilities
due to random sampling in data selection, which leads to misaligned synthetic
and real-world distributions. To overcome this limitation, we propose LLM4MEA,
a novel model extraction method that leverages Large Language Models (LLMs) as
human-like rankers to generate data. It generates data through interactions
between the LLM ranker and target recommender system. In each interaction, the
LLM ranker analyzes historical interactions to understand user behavior, and
selects items from recommendations with consistent preferences to extend the
interaction history, which serves as training data for MEA. Extensive
experiments demonstrate that LLM4MEA significantly outperforms existing
approaches in data quality and attack performance, reducing the divergence
between synthetic and real-world data by up to 64.98% and improving MEA
performance by 44.82% on average. From a defensive perspective, we propose a
simple yet effective defense strategy and identify key hyperparameters of
recommender systems that can mitigate the risk of MEAs.

### 26. Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Shanbo Cheng, Yu Bao, Zhichao Huang, Yu Lu, Ningxin Peng, Lu Xu, Runsheng Yu, Rong Cao, Ting Han, Zeyang Li, Sitong Liu, Shengtao Ma, Shiguang Pan, Jiongchen Xiao, Nuo Xu, Meng Yang, Rong Ye, Yiming Yu, Ruofei Zhang, Wanyi Zhang, Wenhao Zhu, Liehao Zou, Lu Lu, Yuxuan Wang, Yonghui Wu
- **URL**: <http://arxiv.org/abs/2507.17527v2>
- **Submitted**: 2025-07-23 14:07:41
- **Comment**: Seed-LiveInterpret 2.0 Technical Report
- **Topic Keywords**: ltr, rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on simultaneous speech-to-speech translation, which is a topic in Natural Language Processing, but it does not align with your specific interests in deep semantic understanding and real-time relevance optimization.

#### Abstract
> Simultaneous Interpretation (SI) represents one of the most daunting
frontiers in the translation industry, with product-level automatic systems
long plagued by intractable challenges: subpar transcription and translation
quality, lack of real-time speech generation, multi-speaker confusion, and
translated speech inflation, especially in long-form discourses. In this study,
we introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers
high-fidelity, ultra-low-latency speech-to-speech generation with voice cloning
capabilities. As a fully operational product-level solution, Seed-LiveInterpret
2.0 tackles these challenges head-on through our novel duplex speech-to-speech
understanding-generating framework. Experimental results demonstrate that
through large-scale pretraining and reinforcement learning, the model achieves
a significantly better balance between translation accuracy and latency,
validated by human interpreters to exceed 70% correctness in complex scenarios.
Notably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by
significant margins in translation quality, while slashing the average latency
of cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is
around a near 70% reduction that drastically enhances practical usability.

### 27. Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Rishemjit Kaur, Arshdeep Singh Bhankhar, Surangika Ranathunga, Jashanpreet Singh Salh, Sudhir Rajput, Vidhi, Kashish Mahendra, Bhavika Berwal, Ritesh Kumar
- **URL**: <http://arxiv.org/abs/2507.16974v1>
- **Submitted**: 2025-07-22 19:25:10
- **Comment**: 15 pages, 9 tables, Appendix A-K
- **Topic Keywords**: relevance, rag
- **Reason**: The paper focuses on question answering with multilingual LLMs in the agricultural domain, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions fine-tuning language-specific LLMs, the context is specific to agriculture and does not align with the user's broader interests.

#### Abstract
> Enabling farmers to access accurate agriculture-related information in their
native languages in a timely manner is crucial for the success of the
agriculture field. Although large language models (LLMs) can be used to
implement Question Answering (QA) systems, simply using publicly available
general-purpose LLMs in agriculture typically offer generic advisories, lacking
precision in local and multilingual contexts due to insufficient
domain-specific training and scarcity of high-quality, region-specific
datasets. Our study addresses these limitations by generating multilingual
synthetic agricultural datasets (English, Hindi, Punjabi) from
agriculture-specific documents and fine-tuning language-specific LLMs. Our
evaluation on curated multilingual datasets demonstrates significant
improvements in factual accuracy, relevance, and agricultural consensus for the
fine-tuned models compared to their baseline counterparts. These results
highlight the efficacy of synthetic data-driven, language-specific fine-tuning
as an effective strategy to improve the performance of LLMs in agriculture,
especially in multilingual and low-resource settings. By enabling more accurate
and localized agricultural advisory services, this study provides a meaningful
step toward bridging the knowledge gap in AI-driven agricultural solutions for
diverse linguistic communities.

### 28. CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Kyeongkyu Lee, Seonghwan Yoon, Hongki Lim
- **URL**: <http://arxiv.org/abs/2507.17234v1>
- **Submitted**: 2025-07-23 05:57:59
- **Topic Keywords**: ranking, rank
- **Reason**: This paper is not directly related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on radiology report generation, which is a specific application in Natural Language Processing, but it does not address the user's core research themes.

#### Abstract
> Automatic generation of radiology reports has the potential to alleviate
radiologists' significant workload, yet current methods struggle to deliver
clinically reliable conclusions. In particular, most prior approaches focus on
producing fluent text without effectively ensuring the factual correctness of
the reports and often rely on single-view images, limiting diagnostic
comprehensiveness. We propose CLARIFID, a novel framework that directly
optimizes diagnostic correctness by mirroring the two-step workflow of experts.
Specifically, CLARIFID (1) learns the logical flow from Findings to Impression
through section-aware pretraining, (2) is fine-tuned with Proximal Policy
Optimization in which the CheXbert F1 score of the Impression section serves as
the reward, (3) enforces reasoning-aware decoding that completes "Findings"
before synthesizing the "Impression", and (4) fuses multiple chest X-ray views
via a vision-transformer-based multi-view encoder. During inference, we apply a
reasoning-aware next-token forcing strategy followed by report-level
re-ranking, ensuring that the model first produces a comprehensive Findings
section before synthesizing the Impression and thereby preserving coherent
clinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate
that our method achieves superior clinical efficacy and outperforms existing
baselines on both standard NLG metrics and clinically aware scores.

### 29. From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Karen Zhou, John Giorgi, Pranav Mani, Peng Xu, Davis Liang, Chenhao Tan
- **URL**: <http://arxiv.org/abs/2507.17717v1>
- **Submitted**: 2025-07-23 17:28:31
- **Topic Keywords**: rag, search
- **Reason**: The paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. While it touches on the topic of evaluation metrics, it is focused on clinical notes and AI-generated content, which is outside the scope of the user's research interests.

#### Abstract
> AI-generated clinical notes are increasingly used in healthcare, but
evaluating their quality remains a challenge due to high subjectivity and
limited scalability of expert review. Existing automated metrics often fail to
align with real-world physician preferences. To address this, we propose a
pipeline that systematically distills real user feedback into structured
checklists for note evaluation. These checklists are designed to be
interpretable, grounded in human feedback, and enforceable by LLM-based
evaluators. Using deidentified data from over 21,000 clinical encounters,
prepared in accordance with the HIPAA safe harbor standard, from a deployed AI
medical scribe system, we show that our feedback-derived checklist outperforms
baseline approaches in our offline evaluations in coverage, diversity, and
predictive power for human ratings. Extensive experiments confirm the
checklist's robustness to quality-degrading perturbations, significant
alignment with clinician preferences, and practical value as an evaluation
methodology. In offline research settings, the checklist can help identify
notes likely to fall below our chosen quality thresholds.

### 30. MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Alexander R. Fabbri, Diego Mares, Jorge Flores, Meher Mankikar, Ernesto Hernandez, Dean Lee, Bing Liu, Chen Xing
- **URL**: <http://arxiv.org/abs/2507.17476v1>
- **Submitted**: 2025-07-23 12:56:31
- **Topic Keywords**: relevance
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on Large Language Models (LLMs) and multilingual reasoning, which is outside your primary focus area.

#### Abstract
> Although recent Large Language Models (LLMs) have shown rapid improvement on
reasoning benchmarks in English, the evaluation of such LLMs' multilingual
reasoning capability across diverse languages and cultural contexts remains
limited. Existing multilingual reasoning benchmarks are typically constructed
by translating existing English reasoning benchmarks, biasing these benchmarks
towards reasoning problems with context in English language/cultures. In this
work, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a
benchmark designed to assess LLMs on more than 1,000 native, linguistic and
culturally grounded reasoning questions written by native speakers in French,
Spanish, and Chinese. MultiNRC covers four core reasoning categories:
language-specific linguistic reasoning, wordplay & riddles, cultural/tradition
reasoning, and math reasoning with cultural relevance. For cultural/tradition
reasoning and math reasoning with cultural relevance, we also provide English
equivalent translations of the multilingual questions by manual translation
from native speakers fluent in English. This set of English equivalents can
provide a direct comparison of LLM reasoning capacity in other languages vs.
English on the same reasoning questions. We systematically evaluate current 14
leading LLMs covering most LLM families on MultiNRC and its English equivalent
set. The results show that (1) current LLMs are still not good at native
multilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs
exhibit distinct strengths and weaknesses in handling linguistic, cultural, and
logical reasoning tasks; (3) Most models perform substantially better in math
reasoning in English compared to in original languages (+10%), indicating
persistent challenges with culturally grounded knowledge.

### 31. Enhancing Transferability and Consistency in Cross-Domain Recommendations via Supervised Disentanglement

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yuhan Wang, Qing Xie, Zhifeng Bao, Mengzi Tang, Lin Li, Yongjian Liu
- **URL**: <http://arxiv.org/abs/2507.17112v1>
- **Submitted**: 2025-07-23 01:29:45
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on cross-domain recommendations, disentangled representation learning, and GNN-enhanced encoder-decoder frameworks, which are not directly related to the user's primary research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling. While the paper touches on some related concepts like collaborative filtering, it does not address the user's core research themes.

#### Abstract
> Cross-domain recommendation (CDR) aims to alleviate the data sparsity by
transferring knowledge across domains. Disentangled representation learning
provides an effective solution to model complex user preferences by separating
intra-domain features (domain-shared and domain-specific features), thereby
enhancing robustness and interpretability. However, disentanglement-based CDR
methods employing generative modeling or GNNs with contrastive objectives face
two key challenges: (i) pre-separation strategies decouple features before
extracting collaborative signals, disrupting intra-domain interactions and
introducing noise; (ii) unsupervised disentanglement objectives lack explicit
task-specific guidance, resulting in limited consistency and suboptimal
alignment. To address these challenges, we propose DGCDR, a GNN-enhanced
encoder-decoder framework. To handle challenge (i), DGCDR first applies GNN to
extract high-order collaborative signals, providing enriched representations as
a robust foundation for disentanglement. The encoder then dynamically
disentangles features into domain-shared and -specific spaces, preserving
collaborative information during the separation process. To handle challenge
(ii), the decoder introduces an anchor-based supervision that leverages
hierarchical feature relationships to enhance intra-domain consistency and
cross-domain alignment. Extensive experiments on real-world datasets
demonstrate that DGCDR achieves state-of-the-art performance, with improvements
of up to 11.59% across key metrics. Qualitative analyses further validate its
superior disentanglement quality and transferability. Our source code and
datasets are available on GitHub for further comparison.

### 32. Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Soumen Sinha, Shahryar Rahnamayan, Azam Asilian Bidgoli
- **URL**: <http://arxiv.org/abs/2507.17025v1>
- **Submitted**: 2025-07-22 21:29:34
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on binary representation of NLP embeddings, which is not directly related to information retrieval, search technologies, or query understanding. Although it mentions NLP, the topic is more focused on efficient text embedding and binary encoding, which is not a central match for the user's research interests.

#### Abstract
> Efficient text embedding is crucial for large-scale natural language
processing (NLP) applications, where storage and computational efficiency are
key concerns. In this paper, we explore how using binary representations
(barcodes) instead of real-valued features can be used for NLP embeddings
derived from machine learning models such as BERT. Thresholding is a common
method for converting continuous embeddings into binary representations, often
using a fixed threshold across all features. We propose a Coordinate
Search-based optimization framework that instead identifies the optimal
threshold for each feature, demonstrating that feature-specific thresholds lead
to improved performance in binary encoding. This ensures that the binary
representations are both accurate and efficient, enhancing performance across
various features. Our optimal barcode representations have shown promising
results in various NLP applications, demonstrating their potential to transform
text representation. We conducted extensive experiments and statistical tests
on different NLP tasks and datasets to evaluate our approach and compare it to
other thresholding methods. Binary embeddings generated using using optimal
thresholds found by our method outperform traditional binarization methods in
accuracy. This technique for generating binary representations is versatile and
can be applied to any features, not just limited to NLP embeddings, making it
useful for a wide range of domains in machine learning applications.

### 33. Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Ming Huang, Zehan Li, Yan Hu, Wanjing Wang, Andrew Wen, Scott Lane, Salih Selek, Lokesh Shahani, Rodrigo Machado-Vieira, Jair Soares, Hua Xu, Hongfang Liu
- **URL**: <http://arxiv.org/abs/2507.17009v1>
- **Submitted**: 2025-07-22 20:44:44
- **Topic Keywords**: ctr, search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on applying generative AI models for multi-label classification in healthcare, which is outside your primary focus areas.

#### Abstract
> Suicide remains a pressing global health crisis, with over 720,000 deaths
annually and millions more affected by suicide ideation (SI) and suicide
attempts (SA). Early identification of suicidality-related factors (SrFs),
including SI, SA, exposure to suicide (ES), and non-suicidal self-injury
(NSSI), is critical for timely intervention. While prior studies have applied
AI to detect SrFs in clinical notes, most treat suicidality as a binary
classification task, overlooking the complexity of cooccurring risk factors.
This study explores the use of generative large language models (LLMs),
specifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs
from psychiatric electronic health records (EHRs). We present a novel end to
end generative MLC pipeline and introduce advanced evaluation methods,
including label set level metrics and a multilabel confusion matrix for error
analysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match
accuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior
performance across label sets, including rare or minority label sets,
indicating a more balanced and robust performance. Our findings reveal
systematic error patterns, such as the conflation of SI and SA, and highlight
the models tendency toward cautious over labeling. This work not only
demonstrates the feasibility of using generative AI for complex clinical
classification tasks but also provides a blueprint for structuring unstructured
EHR data to support large scale clinical research and evidence based medicine.

### 34. Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Giulio Pelosio, Devesh Batra, No√©mie Bovey, Robert Hankache, Cristovao Iglesias, Greig Cowan, Raad Khraishi
- **URL**: <http://arxiv.org/abs/2507.16989v1>
- **Submitted**: 2025-07-22 19:54:49
- **Topic Keywords**: ctr, search
- **Reason**: This paper focuses on evaluating biases in Large Language Models (LLMs) using name-based benchmarks, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on the topic of AI systems, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are core areas of interest.

#### Abstract
> Large Language Models (LLMs) can exhibit latent biases towards specific
nationalities even when explicit demographic markers are not present. In this
work, we introduce a novel name-based benchmarking approach derived from the
Bias Benchmark for QA (BBQ) dataset to investigate the impact of substituting
explicit nationality labels with culturally indicative names, a scenario more
reflective of real-world LLM applications. Our novel approach examines how this
substitution affects both bias magnitude and accuracy across a spectrum of LLMs
from industry leaders such as OpenAI, Google, and Anthropic. Our experiments
show that small models are less accurate and exhibit more bias compared to
their larger counterparts. For instance, on our name-based dataset and in the
ambiguous context (where the correct choice is not revealed), Claude Haiku
exhibited the worst stereotypical bias scores of 9%, compared to only 3.5% for
its larger counterpart, Claude Sonnet, where the latter also outperformed it by
117.7% in accuracy. Additionally, we find that small models retain a larger
portion of existing errors in these ambiguous contexts. For example, after
substituting names for explicit nationality references, GPT-4o retains 68% of
the error rate versus 76% for GPT-4o-mini, with similar findings for other
model providers, in the ambiguous context. Our research highlights the stubborn
resilience of biases in LLMs, underscoring their profound implications for the
development and deployment of AI systems in diverse, global contexts.

### 35. Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Changxin Tian, Kunlong Chen, Jia Liu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou
- **URL**: <http://arxiv.org/abs/2507.17702v2>
- **Submitted**: 2025-07-23 17:10:23
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus is on scaling language models using Mixture-of-Experts architecture, which is a topic in Natural Language Processing, but not a core interest of yours. The paper's relevance to your research is limited.

#### Abstract
> Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large
Language Models (LLMs) efficiently by decoupling total parameters from
computational cost. However, this decoupling creates a critical challenge:
predicting the model capacity of a given MoE configurations (e.g., expert
activation ratio and granularity) remains an unresolved problem. To address
this gap, we introduce Efficiency Leverage (EL), a metric quantifying the
computational advantage of an MoE model over a dense equivalent. We conduct a
large-scale empirical study, training over 300 models up to 28B parameters, to
systematically investigate the relationship between MoE architectural
configurations and EL. Our findings reveal that EL is primarily driven by the
expert activation ratio and the total compute budget, both following
predictable power laws, while expert granularity acts as a non-linear modulator
with a clear optimal range. We integrate these discoveries into a unified
scaling law that accurately predicts the EL of an MoE architecture based on its
configuration. To validate our derived scaling laws, we designed and trained
Ling-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active
parameters, alongside a 6.1B dense model for comparison. When trained on an
identical 1T high-quality token dataset, Ling-mini-beta matched the performance
of the 6.1B dense model while consuming over 7x fewer computational resources,
thereby confirming the accuracy of our scaling laws. This work provides a
principled and empirically-grounded foundation for the scaling of efficient MoE
models.

### 36. WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Changxin Tian, Jiapeng Wang, Qian Zhao, Kunlong Chen, Jia Liu, Ziqi Liu, Jiaxin Mao, Wayne Xin Zhao, Zhiqiang Zhang, Jun Zhou
- **URL**: <http://arxiv.org/abs/2507.17634v1>
- **Submitted**: 2025-07-23 16:02:06
- **Topic Keywords**: rag
- **Reason**: The paper focuses on learning rate scheduling and model merging for pre-training large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on optimization methods, the topic is more relevant to natural language processing and deep learning, but lacks the specific focus on user behavior modeling or real-time relevance optimization that is central to your research interests.

#### Abstract
> Recent advances in learning rate (LR) scheduling have demonstrated the
effectiveness of decay-free approaches that eliminate the traditional decay
phase while maintaining competitive performance. Model merging techniques have
emerged as particularly promising solutions in this domain. We present
Warmup-Stable and Merge (WSM), a general framework that establishes a formal
connection between learning rate decay and model merging. WSM provides a
unified theoretical foundation for emulating various decay strategies-including
cosine decay, linear decay and inverse square root decay-as principled model
averaging schemes, while remaining fully compatible with diverse optimization
methods. Through extensive experiments, we identify merge duration-the training
window for checkpoint aggregation-as the most critical factor influencing model
performance, surpassing the importance of both checkpoint interval and merge
quantity. Our framework consistently outperforms the widely-adopted
Warmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving
significant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on
MMLU-Pro. The performance advantages extend to supervised fine-tuning
scenarios, highlighting WSM's potential for long-term model refinement.

### 37. Dual-branch Prompting for Multimodal Machine Translation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jie Wang, Zhendong Yang, Liansong Zong, Xiaobo Zhang, Dexian Wang, Ji Zhang
- **URL**: <http://arxiv.org/abs/2507.17588v1>
- **Submitted**: 2025-07-23 15:22:51
- **Topic Keywords**: rag
- **Reason**: The paper focuses on Multimodal Machine Translation, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it involves machine learning and natural language processing, the topic is not aligned with the user's primary research interests.

#### Abstract
> Multimodal Machine Translation (MMT) typically enhances text-only translation
by incorporating aligned visual features. Despite the remarkable progress,
state-of-the-art MMT approaches often rely on paired image-text inputs at
inference and are sensitive to irrelevant visual noise, which limits their
robustness and practical applicability. To address these issues, we propose
D2P-MMT, a diffusion-based dual-branch prompting framework for robust
vision-guided translation. Specifically, D2P-MMT requires only the source text
and a reconstructed image generated by a pre-trained diffusion model, which
naturally filters out distracting visual details while preserving semantic
cues. During training, the model jointly learns from both authentic and
reconstructed images using a dual-branch prompting strategy, encouraging rich
cross-modal interactions. To bridge the modality gap and mitigate
training-inference discrepancies, we introduce a distributional alignment loss
that enforces consistency between the output distributions of the two branches.
Extensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves
superior translation performance compared to existing state-of-the-art
approaches.

### 38. AI-based Clinical Decision Support for Primary Care: A Real-World Study

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Robert Korom, Sarah Kiptinness, Najib Adan, Kassim Said, Catherine Ithuli, Oliver Rotich, Boniface Kimani, Irene King'ori, Stellah Kamau, Elizabeth Atemba, Muna Aden, Preston Bowman, Michael Sharman, Rebecca Soskin Hicks, Rebecca Distler, Johannes Heidecke, Rahul K. Arora, Karan Singhal
- **URL**: <http://arxiv.org/abs/2507.16947v1>
- **Submitted**: 2025-07-22 18:37:33
- **Comment**: Blog: https://openai.com/index/ai-clinical-copilot-penda-health/
- **Topic Keywords**: rag
- **Reason**: The paper is not relevant to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on clinical decision support, large language models, and clinical workflow alignment, which are outside the user's primary research areas.

#### Abstract
> We evaluate the impact of large language model-based clinical decision
support in live care. In partnership with Penda Health, a network of primary
care clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a
safety net for clinicians by identifying potential documentation and clinical
decision-making errors. AI Consult integrates into clinician workflows,
activating only when needed and preserving clinician autonomy. We conducted a
quality improvement study, comparing outcomes for 39,849 patient visits
performed by clinicians with or without access to AI Consult across 15 clinics.
Visits were rated by independent physicians to identify clinical errors.
Clinicians with access to AI Consult made relatively fewer errors: 16% fewer
diagnostic errors and 13% fewer treatment errors. In absolute terms, the
introduction of AI Consult would avert diagnostic errors in 22,000 visits and
treatment errors in 29,000 visits annually at Penda alone. In a survey of
clinicians with AI Consult, all clinicians said that AI Consult improved the
quality of care they delivered, with 75% saying the effect was "substantial".
These results required a clinical workflow-aligned AI Consult implementation
and active deployment to encourage clinician uptake. We hope this study
demonstrates the potential for LLM-based clinical decision support tools to
reduce errors in real-world settings and provides a practical framework for
advancing responsible adoption.

### 39. AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Danny D. Leybzon, Shreyas Tirumala, Nishant Jain, Summer Gillen, Michael Jackson, Cameron McPhee, Jennifer Schmidt
- **URL**: <http://arxiv.org/abs/2507.17718v1>
- **Submitted**: 2025-07-23 17:30:14
- **Topic Keywords**: search
- **Reason**: The paper focuses on AI telephone surveying, which is not directly related to Information Retrieval, Search technologies, or Natural Language Processing. While it mentions language models, the application is in a different domain and does not involve query understanding, ranking models, or user behavior modeling.

#### Abstract
> With the rise of voice-enabled artificial intelligence (AI) systems,
quantitative survey researchers have access to a new data-collection mode: AI
telephone surveying. By using AI to conduct phone interviews, researchers can
scale quantitative studies while balancing the dual goals of human-like
interactivity and methodological rigor. Unlike earlier efforts that used
interactive voice response (IVR) technology to automate these surveys, voice AI
enables a more natural and adaptive respondent experience as it is more robust
to interruptions, corrections, and other idiosyncrasies of human speech.
  We built and tested an AI system to conduct quantitative surveys based on
large language models (LLM), automatic speech recognition (ASR), and speech
synthesis technologies. The system was specifically designed for quantitative
research, and strictly adhered to research best practices like question order
randomization, answer order randomization, and exact wording.
  To validate the system's effectiveness, we deployed it to conduct two pilot
surveys with the SSRS Opinion Panel and followed-up with a separate
human-administered survey to assess respondent experiences. We measured three
key metrics: the survey completion rates, break-off rates, and respondent
satisfaction scores. Our results suggest that shorter instruments and more
responsive AI interviewers may contribute to improvements across all three
metrics studied.

### 40. A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Bowen Zheng, Ming Ma, Zhongqiao Lin, Tianming Yang
- **URL**: <http://arxiv.org/abs/2507.17618v1>
- **Submitted**: 2025-07-23 15:49:03
- **Topic Keywords**: search
- **Reason**: The paper focuses on large language models and decoding methods, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on optimization and efficiency, the context is different from the user's primary research interests.

#### Abstract
> Large language models are computationally expensive due to their deep
structures. Prior research has shown that intermediate layers contain
sufficient information to generate accurate answers, leading to the development
of early-exit algorithms that reduce inference costs by terminating computation
at earlier layers. However, these methods often suffer from poor performance
due to misalignment between intermediate and output layer representations that
lead to decoding inaccuracy. To address these challenges, we propose SPADE
(SPace Alignment DEcoding), a novel decoding method that aligns intermediate
layer representations with the output layer by propagating a minimally reduced
sequence consisting of only the start token and the answer token. We further
optimize the early-exit decision-making process by training a linear
approximation of SPADE that computes entropy-based confidence metrics. Putting
them together, we create a hybrid early-exit algorithm that monitors confidence
levels and stops inference at intermediate layers while using SPADE to generate
high-quality outputs. This approach significantly reduces inference costs
without compromising accuracy, offering a scalable and efficient solution for
deploying large language models in real-world applications.

### 41. FinGAIA: An End-to-End Benchmark for Evaluating AI Agents in Finance

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Lingfeng Zeng, Fangqi Lou, Zixuan Wang, Jiajie Xu, Jinyi Niu, Mengping Li, Yifan Dong, Qi Qi, Wei Zhang, Ziwei Yang, Jun Han, Ruilun Feng, Ruiqi Hu, Lejie Zhang, Zhengbo Feng, Yicheng Ren, Xin Guo, Zhaowei Liu, Dongpo Cheng, Weige Cai, Liwen Zhang
- **URL**: <http://arxiv.org/abs/2507.17186v1>
- **Submitted**: 2025-07-23 04:19:16
- **Topic Keywords**: search
- **Reason**: The paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. Although it mentions AI agents, the focus is on evaluating their abilities in the financial domain, which is not a primary area of interest for you.

#### Abstract
> The booming development of AI agents presents unprecedented opportunities for
automating complex tasks across various domains. However, their multi-step,
multi-tool collaboration capabilities in the financial sector remain
underexplored. This paper introduces FinGAIA, an end-to-end benchmark designed
to evaluate the practical abilities of AI agents in the financial domain.
FinGAIA comprises 407 meticulously crafted tasks, spanning seven major
financial sub-domains: securities, funds, banking, insurance, futures, trusts,
and asset management. These tasks are organized into three hierarchical levels
of scenario depth: basic business analysis, asset decision support, and
strategic risk management. We evaluated 10 mainstream AI agents in a zero-shot
setting. The best-performing agent, ChatGPT, achieved an overall accuracy of
48.9\%, which, while superior to non-professionals, still lags financial
experts by over 35 percentage points. Error analysis has revealed five
recurring failure patterns: Cross-modal Alignment Deficiency, Financial
Terminological Bias, Operational Process Awareness Barrier, among others. These
patterns point to crucial directions for future research. Our work provides the
first agent benchmark closely related to the financial domain, aiming to
objectively assess and promote the development of agents in this crucial field.
Partial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.

### 42. TransLPRNet: Lite Vision-Language Network for Single/Dual-line Chinese License Plate Recognition

- **LLM Score**: 0
- **Keyword Score**: 2
- **Authors**: Guangzhu Xu, Zhi Ke, Pengcheng Zuo, Bangjun Lei
- **URL**: <http://arxiv.org/abs/2507.17335v1>
- **Submitted**: 2025-07-23 09:03:01
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, Natural Language Processing, data mining, or related topics. The paper focuses on license plate recognition, which is a specific application in computer vision, and does not address any of the topics you mentioned.

#### Abstract
> License plate recognition in open environments is widely applicable across
various domains; however, the diversity of license plate types and imaging
conditions presents significant challenges. To address the limitations
encountered by CNN and CRNN-based approaches in license plate recognition, this
paper proposes a unified solution that integrates a lightweight visual encoder
with a text decoder, within a pre-training framework tailored for single and
double-line Chinese license plates. To mitigate the scarcity of double-line
license plate datasets, we constructed a single/double-line license plate
dataset by synthesizing images, applying texture mapping onto real scenes, and
blending them with authentic license plate images. Furthermore, to enhance the
system's recognition accuracy, we introduce a perspective correction network
(PTN) that employs license plate corner coordinate regression as an implicit
variable, supervised by license plate view classification information. This
network offers improved stability, interpretability, and low annotation costs.
The proposed algorithm achieves an average recognition accuracy of 99.34% on
the corrected CCPD test set under coarse localization disturbance. When
evaluated under fine localization disturbance, the accuracy further improves to
99.58%. On the double-line license plate test set, it achieves an average
recognition accuracy of 98.70%, with processing speeds reaching up to 167
frames per second, indicating strong practical applicability.

---

