# Daily Papers Report - 2025-08-14

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. TFRank: Think-Free Reasoning Enables Practical Pointwise LLM Ranking

- **LLM Score**: 8
- **Keyword Score**: 15
- **Authors**: Yongqi Fan, Xiaoyang Chen, Dezhi Ye, Jie Liu, Haijin Liang, Jin Ma, Ben He, Yingfei Sun, Tong Ruan
- **URL**: <http://arxiv.org/abs/2508.09539v1>
- **Submitted**: 2025-08-13 06:47:58
- **Topic Keywords**: queries, ranking, pointwise, relevance, rag, rank
- **Reason**: The paper proposes a novel ranking model, TFRank, which integrates Chain-of-Thought (CoT) data, fine-grained score supervision, and multi-task training to achieve efficient pointwise reasoning. Although it's not directly focused on query understanding or user behavior modeling, it's relevant to ranking models and information retrieval, which aligns with your research interests.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: Efficient Pointwise Reasoning Ranker for Complex Queries
- **Aim**: To develop a ranking model that integrates advanced reasoning capabilities into real-world systems
- **Rationale**: Integrating Chain-of-Thought data, fine-grained score supervision, and multi-task training into a pointwise reasoning ranker
- **Ground**: Large Language Models (LLMs) and pointwise format constraints
- **Experiment**: Evaluation on the BRIGHT and BEIR benchmarks, comparing TFRank to models with four times more parameters
- **Takeaway**: TFRank achieves comparable performance to larger models while providing a practical solution for integrating advanced reasoning into real-world systems

#### Abstract
> Reasoning-intensive ranking models built on Large Language Models (LLMs) have
made notable progress, but existing approaches often rely on large-scale LLMs
and explicit Chain-of-Thought (CoT) reasoning, resulting in high computational
cost and latency that limit real-world use. To address this, we propose
\textbf{TFRank}, an efficient pointwise reasoning ranker based on small-scale
LLMs. To improve ranking performance, TFRank effectively integrates CoT data,
fine-grained score supervision, and multi-task training. Furthermore, it
achieves an efficient ``\textbf{T}hink-\textbf{F}ree" reasoning capability by
employing a ``think-mode switch'' and pointwise format constraints.
Specifically, this allows the model to leverage explicit reasoning during
training while delivering precise relevance scores for complex queries at
inference without generating any reasoning chains. Experiments show that TFRank
(e.g., 1.7B) achieves performance comparable to models with four times more
parameters on the BRIGHT benchmark, and demonstrates strong competitiveness on
the BEIR benchmark. Further analysis shows that TFRank achieves an effective
balance between performance and efficiency, providing a practical solution for
integrating advanced reasoning into real-world systems. Our code and data are
released in the repository: https://github.com/JOHNNY-fans/TFRank.

---

### 2. Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation

- **LLM Score**: 8
- **Keyword Score**: 10
- **Authors**: Seokgi Lee
- **URL**: <http://arxiv.org/abs/2508.09755v1>
- **Submitted**: 2025-08-13 12:35:04
- **Topic Keywords**: query, queries, rag, retrieval
- **Reason**: The paper explores a novel retrieval-augmented generation framework for multihop question answering, which aligns with your interest in query understanding and ranking models. The use of large language models for query decomposition and question-question embedding similarity for document retrieval is relevant to your focus on information retrieval and natural language processing. However, the paper's primary focus on multihop question answering and question decomposition might not be directly applicable to your e-commerce domain.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Retrieval-Augmented Generation (RAG) framework for multihop question answering
- **Aim**: Address the structural mismatch between queries and documents in multihop question answering
- **Rationale**: Decompose complex multihop questions into single-hop subquestions using a large language model (LLM) to guide document retrieval
- **Ground**: Evaluate the proposed method on three multihop question datasets and compare with baseline systems
- **Experiment**: Conduct ablation studies to investigate the contribution of each component in the framework and to better understand design choices
- **Takeaway**: The proposed RAG framework improves performance compared to baseline systems, and using answerable-question embeddings and LLM-based query decomposition are key contributors to this improvement

#### Abstract
> We introduce a novel retrieval-augmented generation (RAG) framework tailored
for multihop question answering. First, our system uses large language model
(LLM) to decompose complex multihop questions into a sequence of single-hop
subquestions that guide document retrieval. This decomposition mitigates the
ambiguity inherent in multi-hop queries by clearly targeting distinct knowledge
facets. Second, instead of embedding raw or chunked documents directly, we
generate answerable questions from each document chunk using Qwen3-8B, embed
these generated questions, and retrieve relevant chunks via question-question
embedding similarity. During inference, the retrieved chunks are then fed along
with the original question into the RAG pipeline. We evaluate on three multihop
question datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our
method improves RAG performacne compared to baseline systems. Our contributions
highlight the benefits of using answerable-question embeddings for RAG, and the
effectiveness of LLM-based query decomposition for multihop scenarios.

---

### 3. Personalized Product Search Ranking: A Multi-Task Learning Approach with Tabular and Non-Tabular Data

- **LLM Score**: 7
- **Keyword Score**: 17
- **Authors**: Lalitesh Morishetti, Abhay Kumar, Jonathan Scott, Kaushiki Nag, Gunjan Sharma, Shanu Vashishtha, Rahul Sridhar, Rohit Chatter, Kannan Achan
- **URL**: <http://arxiv.org/abs/2508.09636v1>
- **Submitted**: 2025-08-13 09:15:08
- **Comment**: 17 pages, 2 figures, The Pacific Rim International Conference on
  Artificial Intelligence (PRICAI-2025) Conference
- **Topic Keywords**: query, ranking, relevance, rag, click, click-through rate, rank, search
- **Reason**: The paper's focus on personalized product search ranking and multi-task learning aligns with your interests in Information Retrieval and Search technologies. The use of TinyBERT model for semantic embeddings and the emphasis on real-time relevance optimization are also relevant. However, the paper's specific application to e-commerce and product search may not be directly applicable to your broader interests in NLP and data mining.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Personalized Product Search Ranking using Multi-Task Learning
- **Aim**: To develop a novel approach to personalized product search ranking using a multi-task learning framework that integrates both tabular and non-tabular data
- **Rationale**: The proposed approach leverages a pre-trained TinyBERT model for semantic embeddings and a novel sampling technique to capture diverse customer behaviors, optimizing for three binary actions simultaneously
- **Ground**: The approach incorporates a relevance labeling mechanism that combines click-through rates, click positions, and semantic similarity scores to generate relevance labels in a scalable and cost-effective manner
- **Experiment**: The paper conducts a comprehensive comparative study of several baseline models, including XGBoost, TabNet, FT-Transformer, DCN-V2, and MMoE, to evaluate their effectiveness in handling different data types and optimizing personalized ranking tasks
- **Takeaway**: The results demonstrate that combining non-tabular data with advanced embedding techniques in the MTL paradigm significantly enhances model performance, and the proposed approach outperforms baseline models in personalized product search ranking

#### Abstract
> In this paper, we present a novel model architecture for optimizing
personalized product search ranking using a multi-task learning (MTL)
framework. Our approach uniquely integrates tabular and non-tabular data,
leveraging a pre-trained TinyBERT model for semantic embeddings and a novel
sampling technique to capture diverse customer behaviors. We evaluate our model
against several baselines, including XGBoost, TabNet, FT-Transformer, DCN-V2,
and MMoE, focusing on their ability to handle mixed data types and optimize
personalized ranking. Additionally, we propose a scalable relevance labeling
mechanism based on click-through rates, click positions, and semantic
similarity, offering an alternative to traditional human-annotated labels.
Experimental results show that combining non-tabular data with advanced
embedding techniques in multi-task learning paradigm significantly enhances
model performance. Ablation studies further underscore the benefits of
incorporating relevance labels, fine-tuning TinyBERT layers, and TinyBERT
query-product embedding interactions. These results demonstrate the
effectiveness of our approach in achieving improved personalized product search
ranking.

---

### 4. From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation

- **LLM Score**: 7
- **Keyword Score**: 16
- **Authors**: Siyuan Meng, Junming Liu, Yirong Chen, Song Mao, Pinlong Cai, Guohang Yan, Botian Shi, Ding Wang
- **URL**: <http://arxiv.org/abs/2508.09497v1>
- **Submitted**: 2025-08-13 05:05:34
- **Comment**: 9 pages, 4 tables
- **Topic Keywords**: queries, ranking, rerank, rag, retrieval augmented generation, retrieval, rank
- **Reason**: The paper focuses on retrieval-augmented generation, which is a related topic to information retrieval. The Dynamic Passage Selector (DPS) is a novel reranking framework that treats passage selection as a supervised learning problem, which is relevant to query understanding and ranking models. However, the paper does not explicitly mention user behavior modeling or click models, which are important aspects of your research interests.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Dynamic Passage Selector (DPS) for Retrieval-Augmented Generation (RAG) Systems
- **Aim**: To develop a novel reranking framework that captures inter-passage dependencies and dynamically selects the most relevant set of passages for generation
- **Rationale**: Traditional point-wise or list-wise methods require modifications to the standard RAG pipeline, whereas DPS reframes reranking as a passage selection learning problem, allowing it to capture combinatorial relevance critical for multi-hop question answering
- **Ground**: DPS consists of two stages: offline supervised fine-tuning and online inference, and is evaluated on five widely used QA benchmarks
- **Experiment**: DPS is compared against popular pre-trained models in zero-shot mode and fine-tuned RAG models, as well as seven standard rerankers, and achieves state-of-the-art (SOTA) performance across all five QA benchmarks
- **Takeaway**: DPS is a novel and effective approach to passage selection that can dynamically identify contextually relevant passages, surpassing the limitations of fixed Top-K retrieval, and has potential for real-world applications

#### Abstract
> Retrieval-augmented generation (RAG) systems are often bottlenecked by their
reranking modules, which typically score passages independently and select a
fixed Top-K size. This approach struggles with complex multi-hop queries that
require synthesizing evidence across multiple documents, creating a trade-off
where small K values omit crucial information and large K values introduce
noise. To address this, we introduce the Dynamic Passage Selector (DPS), a
novel reranking framework that treats passage selection as a supervised
learning problem. Unlike traditional point-wise or list-wise methods, DPS is
fine-tuned to capture inter-passage dependencies and dynamically select the
most relevant set of passages for generation. As a seamless plug-and-play
module, DPS requires no modifications to the standard RAG pipeline.
Comprehensive evaluations on five benchmarks show that DPS consistently
outperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the
challenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over
strong baselines like Qwen3-reranker and RankingGPT, respectively. Our results
demonstrate that by enabling adaptive evidence selection, DPS substantially
enhances reasoning capabilities in complex RAG scenarios.

---

### 5. Improving Dense Passage Retrieval with Multiple Positive Passages

- **LLM Score**: 7
- **Keyword Score**: 10
- **Authors**: Shuai Chang
- **URL**: <http://arxiv.org/abs/2508.09534v1>
- **Submitted**: 2025-08-13 06:36:53
- **Topic Keywords**: sparse retrieval, passage retrieval, rag, retrieval
- **Reason**: The paper explores Dense Passage Retrieval (DPR), a topic relevant to Information Retrieval, and examines the effect of using multiple positive passages during training. While it does not directly address query understanding, ranking models, or user behavior modeling, it contributes to the development of DPR, a key area in IR. The paper's focus on DPR and passage retrieval accuracy aligns with the user's interests, but its scope is narrower than the user's broader interests in NLP, data mining, and real-time relevance optimization.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Improving Dense Passage Retrieval (DPR) model for passage retrieval accuracy
- **Aim**: To reduce positive-negative imbalance during training and improve retrieval accuracy
- **Rationale**: The proposed approach reformulates the training process as a binary classification task, incorporating multiple positive passages to reduce imbalance
- **Ground**: The DPR model uses a dual-BERT encoder architecture, optimized with binary cross-entropy (BCE) loss
- **Experiment**: Experiments on various datasets (SQuAD 1.1, TriviaQA, Natural Question, TREC, WebQ) with different training settings and ablation studies
- **Takeaway**: The proposed approach (DPR+) achieves improved performance on most datasets, is more efficient, and requires further investigation to optimize batch size and positive passages

#### Abstract
> By leveraging a dual encoder architecture, Dense Passage Retrieval (DPR) has
outperformed traditional sparse retrieval algorithms such as BM25 in terms of
passage retrieval accuracy. Recently proposed methods have further enhanced
DPR's performance. However, these models typically pair each question with only
one positive passage during training, and the effect of associating multiple
positive passages has not been examined. In this paper, we explore the
performance of DPR when additional positive passages are incorporated during
training. Experimental results show that equipping each question with multiple
positive passages consistently improves retrieval accuracy, even when using a
significantly smaller batch size, which enables training on a single GPU.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning

- **LLM Score**: 6
- **Keyword Score**: 14
- **Authors**: Shu Zhao, Tan Yu, Anbang Xu, Japinder Singh, Aaditya Shukla, Rama Akkiraju
- **URL**: <http://arxiv.org/abs/2508.09303v1>
- **Submitted**: 2025-08-12 19:38:21
- **Topic Keywords**: information retrieval, query, queries, rag, retrieval, search
- **Reason**: The paper proposes a novel reinforcement learning framework for parallelizing search queries, which is relevant to information retrieval and search technologies. However, it does not specifically focus on query understanding, ranking models, or user behavior modeling, which are core areas of interest for you. The paper's emphasis on large language models and parallel execution benefits is somewhat related to your interests in NLP and data mining, but it does not directly align with your primary focus on information retrieval and real-time relevance optimization.

#### Abstract
> Reasoning-augmented search agents such as Search-R1, trained via
reinforcement learning with verifiable rewards (RLVR), demonstrate remarkable
capabilities in multi-step information retrieval from external knowledge
sources. These agents address the limitations of their parametric memory by
dynamically gathering relevant facts to address complex reasoning tasks.
However, existing approaches suffer from a fundamental architectural
limitation: they process search queries strictly sequentially, even when
handling inherently parallelizable and logically independent comparisons. This
sequential bottleneck significantly constrains computational efficiency,
particularly for queries that require multiple entity comparisons. To address
this critical limitation, we propose ParallelSearch, a novel reinforcement
learning framework that empowers large language models (LLMs) to recognize
parallelizable query structures and execute multiple search operations
concurrently. Our approach introduces dedicated reward functions that
incentivize the identification of independent query components while preserving
answer accuracy through jointly considering correctness, query decomposition
quality, and parallel execution benefits. Comprehensive experiments demonstrate
that ParallelSearch outperforms state-of-the-art baselines by an average
performance gain of 2.9% across seven question-answering benchmarks. Notably,
on parallelizable questions, our method achieves a 12.7% performance
improvement while requiring only 69.6% of the LLM calls compared to sequential
approaches.

### 7. PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts

- **LLM Score**: 6
- **Keyword Score**: 3
- **Authors**: Mo Yu, Tsz Ting Chung, Chulun Zhou, Tong Li, Rui Lu, Jiangnan Li, Liyan Xu, Haoshu Lu, Ning Zhang, Jing Li, Jie Zhou
- **URL**: <http://arxiv.org/abs/2508.09848v2>
- **Submitted**: 2025-08-13 14:28:25
- **Comment**: First 7 authors contributed equally. Project page:
  https://gorov.github.io/prelude
- **Topic Keywords**: rag, search
- **Reason**: The paper introduces a benchmark for evaluating long-context understanding, which is related to query understanding and ranking models in Information Retrieval. However, the focus on narrative comprehension and reasoning is not directly aligned with my primary interests in search technologies and user behavior modeling. The paper's findings on the challenges of long-context understanding and reasoning are interesting, but not directly applicable to my research areas.

#### Abstract
> We introduce PRELUDE, a benchmark for evaluating long-context understanding
through the task of determining whether a character's prequel story is
consistent with the canonical narrative of the original book. Our task poses a
stronger demand for global comprehension and deep reasoning than existing
benchmarks -- as the prequels are not part of the original story, assessing
their plausibility typically requires searching and integrating information
that is only indirectly related. Empirically, 88% of instances require evidence
from multiple parts of the narrative. Experimental results highlight the
challenge of our task: in-context learning, RAG and in-domain training with
state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans
by >15%. A further human study reveals that models often produce correct
answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy
compared to humans. These findings underscore the substantial room for
improvement in long-context understanding and reasoning.

### 8. Describe What You See with Multimodal Large Language Models to Enhance Video Recommendations

- **LLM Score**: 6
- **Keyword Score**: 3
- **Authors**: Marco De Nadai, Andreas Damianou, Mounia Lalmas
- **URL**: <http://arxiv.org/abs/2508.09789v1>
- **Submitted**: 2025-08-13 13:19:31
- **Topic Keywords**: rag, recommend
- **Reason**: The paper explores video recommendations using multimodal large language models, which is related to information retrieval and search technologies. However, the focus is on video recommendations and multimodal processing, which is not directly aligned with the user's core research themes of query understanding, ranking models, and user behavior modeling. The paper's emphasis on natural language processing and text-based summarization is somewhat relevant, but the connection to the user's interests is not strong.

#### Abstract
> Existing video recommender systems rely primarily on user-defined metadata or
on low-level visual and acoustic signals extracted by specialised encoders.
These low-level features describe what appears on the screen but miss deeper
semantics such as intent, humour, and world knowledge that make clips resonate
with viewers. For example, is a 30-second clip simply a singer on a rooftop, or
an ironic parody filmed amid the fairy chimneys of Cappadocia, Turkey? Such
distinctions are critical to personalised recommendations yet remain invisible
to traditional encoding pipelines. In this paper, we introduce a simple,
recommendation system-agnostic zero-finetuning framework that injects
high-level semantics into the recommendation pipeline by prompting an
off-the-shelf Multimodal Large Language Model (MLLM) to summarise each clip
into a rich natural-language description (e.g. "a superhero parody with
slapstick fights and orchestral stabs"), bridging the gap between raw content
and user intent. We use MLLM output with a state-of-the-art text encoder and
feed it into standard collaborative, content-based, and generative
recommenders. On the MicroLens-100K dataset, which emulates user interactions
with TikTok-style videos, our framework consistently surpasses conventional
video, audio, and metadata features in five representative models. Our findings
highlight the promise of leveraging MLLMs as on-the-fly knowledge extractors to
build more intent-aware video recommenders.

### 9. SaraCoder: Orchestrating Semantic and Structural Cues for Profit-Oriented Repository-Level Code Completion

- **LLM Score**: 4
- **Keyword Score**: 14
- **Authors**: Xiaohan Chen, Zhongying Pan, Quan Feng, Yu Tian, Shuqun Yang, Mengru Wang, Lina Gong, Yuxia Geng, Piji Li, Xiang Chen
- **URL**: <http://arxiv.org/abs/2508.10068v1>
- **Submitted**: 2025-08-13 11:56:05
- **Topic Keywords**: ranking, rerank, relevance, rag, retrieval, rank
- **Reason**: The paper introduces a retrieval framework for repository-level code completion, focusing on hierarchical feature optimization and external-aware identifier disambiguation. While it touches on semantic relationships and ranking, the primary focus is on code completion, which is not directly related to my research interests in Information Retrieval and Search technologies. The paper's relevance to my interests is limited, but it does explore some related concepts.

#### Abstract
> Retrieval-augmented generation (RAG) for repository-level code completion
commonly relies on superficial text similarity, leading to results plagued by
semantic misguidance, redundancy, and homogeneity, while also failing to
resolve external symbol ambiguity. To address these challenges, we introduce
Saracoder, a Hierarchical Feature-Optimized retrieval framework. Its core
Hierarchical Feature Optimization module systematically refines candidates by
distilling deep semantic relationships, pruning exact duplicates, assessing
structural similarity with a novel graph-based metric that weighs edits by
their topological importance, and reranking results to maximize both relevance
and diversity. Furthermore, an External-Aware Identifier Disambiguator module
accurately resolves cross-file symbol ambiguity via dependency analysis.
Extensive experiments on the challenging CrossCodeEval and RepoEval-Updated
benchmarks demonstrate that Saracoder significantly outperforms existing
baselines across multiple programming languages and models. Our work proves
that systematically refining retrieval results across multiple dimensions
provides a new paradigm for building more accurate and robust repository-level
code completion systems.

### 10. Towards Self-cognitive Exploration: Metacognitive Knowledge Graph Retrieval Augmented Generation

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Xujie Yuan, Shimin Di, Jielong Tang, Libin Zheng, Jian Yin
- **URL**: <http://arxiv.org/abs/2508.09460v1>
- **Submitted**: 2025-08-13 03:35:32
- **Topic Keywords**: relevance, rag, retrieval augmented generation, retrieval
- **Reason**: The paper proposes a novel framework for knowledge graph retrieval and generation, leveraging structured knowledge and introducing a metacognitive process for refinement. While it touches on the topic of retrieval and generation, it does not specifically focus on query understanding, ranking models, or user behavior modeling, which are core areas of interest for your research. The paper's focus on knowledge graphs and structured knowledge retrieval is somewhat related to your interests in information retrieval, but it does not align as closely as other papers in the field.

#### Abstract
> Knowledge Graph-based Retrieval-Augmented Generation (KG-RAG) significantly
enhances the reasoning capabilities of LargeLanguage Models by leveraging
structured knowledge. However, existing KG-RAG frameworks typically operate as
open-loop systems, suffering from cognitive blindness, an inability to
recognize their exploration deficiencies. This leads to relevance drift and
incomplete evidence, which existing self-refinement methods, designed for
unstructured text-based RAG, cannot effectively resolve due to the
path-dependent nature of graph exploration. To address this challenge, we
propose Metacognitive Knowledge Graph Retrieval Augmented Generation
(MetaKGRAG), a novel framework inspired by the human metacognition process,
which introduces a Perceive-Evaluate-Adjust cycle to enable path-aware,
closed-loop refinement. This cycle empowers the system to self-assess
exploration quality, identify deficiencies in coverage or relevance, and
perform trajectory-connected corrections from precise pivot points. Extensive
experiments across five datasets in the medical, legal, and commonsense
reasoning domains demonstrate that MetaKGRAG consistently outperforms strong
KG-RAG and self-refinement baselines. Our results validate the superiority of
our approach and highlight the critical need for path-aware refinement in
structured knowledge retrieval.

### 11. Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Jiaqi Cao, Jiarui Wang, Rubin Wei, Qipeng Guo, Kai Chen, Bowen Zhou, Zhouhan Lin
- **URL**: <http://arxiv.org/abs/2508.09874v1>
- **Submitted**: 2025-08-13 15:16:29
- **Topic Keywords**: retriever, rag, retrieval, search
- **Reason**: This paper introduces a novel memory component for domain adaptation in large language models, which is somewhat related to my interests in Information Retrieval and Search technologies. However, the focus on language models and domain adaptation is not directly aligned with my primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large Language Models (LLMs) have shown strong abilities in general language
tasks, yet adapting them to specific domains remains a challenge. Current
method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter
training and suffers from catastrophic forgetting. Meanwhile,
Retrieval-Augmented Generation (RAG) introduces substantial inference latency
due to expensive nearest-neighbor searches and longer context. This paper
introduces Memory Decoder, a plug-and-play pretrained memory that enables
efficient domain adaptation without changing the original model's parameters.
Memory Decoder employs a small transformer decoder that learns to imitate the
behavior of an external non-parametric retriever. Once trained, Memory Decoder
can be seamlessly integrated with any pretrained language model that shares the
same tokenizer, requiring no model-specific modifications. Experimental results
demonstrate that Memory Decoder enables effective adaptation of various Qwen
and Llama models to three distinct specialized domains: biomedicine, finance,
and law, reducing perplexity by an average of 6.17 points. Overall, Memory
Decoder introduces a novel paradigm centered on a specially pretrained memory
component designed for domain-specific adaptation. This memory architecture can
be integrated in a plug-and-play manner, consistently enhancing performance
across multiple models within the target domain.

### 12. On Negative-aware Preference Optimization for Recommendation

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Chenlu Ding, Daoxuan Liu, Jiancan Wu, Xingyu Hu, Junkang Wu, Haitao Wang, Yongkang Wang, Xingxing Wang, Xiang Wang
- **URL**: <http://arxiv.org/abs/2508.09653v1>
- **Submitted**: 2025-08-13 09:37:07
- **Topic Keywords**: ranking, rag, recommend, rank
- **Reason**: The paper focuses on recommendation systems, which is somewhat related to my interests in Information Retrieval and Search technologies. However, the emphasis on large language models and recommendation accuracy is not directly aligned with my primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Recommendation systems leverage user interaction data to suggest relevant
items while filtering out irrelevant (negative) ones. The rise of large
language models (LLMs) has garnered increasing attention for their potential in
recommendation tasks. However, existing methods for optimizing LLM-based
recommenders face challenges in effectively utilizing negative samples. Simply
integrating large numbers of negative samples can improve ranking accuracy and
mitigate popularity bias but often leads to increased computational overhead
and memory costs. Additionally, current approaches fail to account for the
varying informativeness of negative samples, leading to suboptimal optimization
performance. To address these issues, we propose NAPO
(\textbf{N}egative-\textbf{A}ware \textbf{P}reference \textbf{O}ptimization),
an enhanced framework for preference optimization in LLM-based recommendation.
NAPO introduces two key innovations: (1) in-batch negative sharing, which
expands the pool of negative samples without additional memory overhead, and
(2) dynamic reward margin adjustment, which adapts model updates based on the
confidence of negative samples. Extensive experiments on three public datasets
demonstrate that NAPO outperforms existing methods in both recommendation
accuracy and popularity bias reduction.

### 13. User-centric Subjective Leaderboard by Customizable Reward Modeling

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Qi Jia, Xiujie Song, Zicheng Zhang, Yijin Guo, Kaiwei Zhang, Zijian Chen, Guangtao Zhai
- **URL**: <http://arxiv.org/abs/2508.09463v1>
- **Submitted**: 2025-08-13 03:39:04
- **Topic Keywords**: queries, ranking, rank
- **Reason**: The paper presents a novel approach to evaluating large language models, focusing on subjective user preferences and customizable reward models. While it touches on ranking and user behavior modeling, the primary focus is on language models and their evaluation, rather than information retrieval or search technologies. The paper's relevance to the user's interests is somewhat limited, but it may still be of interest due to its connection to user-centric evaluation and ranking.

#### Abstract
> Existing benchmarks for large language models (LLMs) predominantely focus on
assessing their capabilities through verifiable tasks. Such objective and
static benchmarks offer limited utility for practical LLM selection, making it
difficult for users to find suitable models for their individual needs. To
bridge this gap, we present the first User-Centric Subjective Leaderboard
(USL), which provides a preference-driven, dynamic ranking of LLMs across
diverse real-world scenarios. Our work is built upon a thorough investigation
of real human preference data, involving more than 10K subjective queries. Our
investigation reveals significant diversity and contradictions in human
preferences, which limit the effectiveness of state-of-the-art reward models.
To address this, we introduce Customizable Reward Models (CRMs). With only 4B
parameters, our CRM surpasses the performance of leading models such as GPT-4.1
and Gemini-2.5-pro, showing exceptional generalization capabilities across new
topics and criteria. The USL, powered by CRMs, exhibits strong negative
correlations to contradictory preferences.

### 14. Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Baran Atalar, Eddie Zhang, Carlee Joe-Wong
- **URL**: <http://arxiv.org/abs/2508.09958v1>
- **Submitted**: 2025-08-13 17:19:41
- **Comment**: Submitted to AAAI 2026
- **Topic Keywords**: queries
- **Reason**: The paper explores a novel problem of selecting a sequence of LLMs for a pipeline of tasks, which is related to information retrieval and query understanding. However, the focus on LLM selection and routing algorithms is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat limited to the user's background in e-commerce and NLP, but it does not address the user's core research themes.

#### Abstract
> With the increasing popularity of large language models (LLMs) for a variety
of tasks, there has been a growing interest in strategies that can predict
which out of a set of LLMs will yield a successful answer at low cost. This
problem promises to become more and more relevant as providers like Microsoft
allow users to easily create custom LLM "assistants" specialized to particular
types of queries. However, some tasks (i.e., queries) may be too specialized
and difficult for a single LLM to handle alone. These applications often
benefit from breaking down the task into smaller subtasks, each of which can
then be executed by a LLM expected to perform well on that specific subtask.
For example, in extracting a diagnosis from medical records, one can first
select an LLM to summarize the record, select another to validate the summary,
and then select another, possibly different, LLM to extract the diagnosis from
the summarized record. Unlike existing LLM selection or routing algorithms,
this setting requires that we select a sequence of LLMs, with the output of
each LLM feeding into the next and potentially influencing its success. Thus,
unlike single LLM selection, the quality of each subtask's output directly
affects the inputs, and hence the cost and success rate, of downstream LLMs,
creating complex performance dependencies that must be learned and accounted
for during selection. We propose a neural contextual bandit-based algorithm
that trains neural networks that model LLM success on each subtask in an online
manner, thus learning to guide the LLM selections for the different subtasks,
even in the absence of historical LLM performance data. Experiments on
telecommunications question answering and medical diagnosis prediction datasets
illustrate the effectiveness of our proposed approach compared to other LLM
selection algorithms.

### 15. Multimodal Fusion And Sparse Attention-based Alignment Model for Long Sequential Recommendation

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Yongrui Fu, Jian Liu, Tao Li, Zonggang Wu, Shouke Qin, Hanmeng Liu
- **URL**: <http://arxiv.org/abs/2508.09664v1>
- **Submitted**: 2025-08-13 09:50:44
- **Comment**: 10 pages
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on multimodal recommendation and sequential recommendation, which is somewhat related to my interests in information retrieval and search technologies. However, the emphasis on multimodal fusion and sparse attention-based alignment is not directly aligned with my primary focus on query understanding, ranking models, and user behavior modeling. The paper's relevance is limited to the e-commerce domain, which is not a primary area of interest for me.

#### Abstract
> Recent advances in multimodal recommendation enable richer item
understanding, while modeling users' multi-scale interests across temporal
horizons has attracted growing attention. However, effectively exploiting
multimodal item sequences and mining multi-grained user interests to
substantially bridge the gap between content comprehension and recommendation
remain challenging. To address these issues, we propose MUFASA, a MUltimodal
Fusion And Sparse Attention-based Alignment model for long sequential
recommendation. Our model comprises two core components. First, the Multimodal
Fusion Layer (MFL) leverages item titles as a cross-genre semantic anchor and
is trained with a joint objective of four tailored losses that promote: (i)
cross-genre semantic alignment, (ii) alignment to the collaborative space for
recommendation, (iii) preserving the similarity structure defined by titles and
preventing modality representation collapse, and (iv) distributional
regularization of the fusion space. This yields high-quality fused item
representations for further preference alignment. Second, the Sparse
Attention-guided Alignment Layer (SAL) scales to long user-behavior sequences
via a multi-granularity sparse attention mechanism, which incorporates windowed
attention, block-level attention, and selective attention, to capture user
interests hierarchically and across temporal horizons. SAL explicitly models
both the evolution of coherent interest blocks and fine-grained intra-block
variations, producing robust user and item representations. Extensive
experiments on real-world benchmarks show that MUFASA consistently surpasses
state-of-the-art baselines. Moreover, online A/B tests demonstrate significant
gains in production, confirming MUFASA's effectiveness in leveraging multimodal
cues and accurately capturing diverse user preferences.

### 16. Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Mahdi Dhaini, Juraj Vladika, Ege Erdogan, Zineb Attaoui, Gjergji Kasneci
- **URL**: <http://arxiv.org/abs/2508.09776v1>
- **Submitted**: 2025-08-13 12:59:08
- **Comment**: Accepted to the 34th International Conference on Artificial Neural
  Networks (ICANN 2025)
- **Topic Keywords**: rag
- **Reason**: The paper explores Explainable Natural Language Processing (NLP), which is related to the user's interest in NLP. However, the focus on textual explanations and model classification performance is not directly aligned with the user's primary interest in Information Retrieval and Search technologies, particularly query understanding, ranking models, and user behavior modeling.

#### Abstract
> In the rapidly evolving field of Explainable Natural Language Processing
(NLP), textual explanations, i.e., human-like rationales, are pivotal for
explaining model predictions and enriching datasets with interpretable labels.
Traditional approaches rely on human annotation, which is costly,
labor-intensive, and impedes scalability. In this work, we present an automated
framework that leverages multiple state-of-the-art large language models (LLMs)
to generate high-quality textual explanations. We rigorously assess the quality
of these LLM-generated explanations using a comprehensive suite of Natural
Language Generation (NLG) metrics. Furthermore, we investigate the downstream
impact of these explanations on the performance of pre-trained language models
(PLMs) and LLMs across natural language inference tasks on two diverse
benchmark datasets. Our experiments demonstrate that automated explanations
exhibit highly competitive effectiveness compared to human-annotated
explanations in improving model performance. Our findings underscore a
promising avenue for scalable, automated LLM-based textual explanation
generation for extending NLP datasets and enhancing model performance.

### 17. Improving Diversity in Language Models: When Temperature Fails, Change the Loss

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Alexandre Verine, Florian Le Bronnec, Kunhao Zheng, Alexandre Allauzen, Yann Chevaleyre, Benjamin Negrevergne
- **URL**: <http://arxiv.org/abs/2508.09654v1>
- **Submitted**: 2025-08-13 09:37:53
- **Comment**: Forty-Second International Conference on Machine Learning, ICML2025
- **Topic Keywords**: rag
- **Reason**: The paper discusses improving diversity in language models, which is a topic in NLP. While it touches on temperature adjustments, which is related to ranking models, the focus is on language models and loss functions, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval. The paper's relevance is somewhat limited, but it may still be of interest to those with a broader background in NLP.

#### Abstract
> Increasing diversity in language models is a challenging yet essential
objective. A common approach is to raise the decoding temperature. In this
work, we investigate this approach through a simplistic yet common case to
provide insights into why decreasing temperature can improve quality
(Precision), while increasing it often fails to boost coverage (Recall). Our
analysis reveals that for a model to be effectively tunable through temperature
adjustments, it must be trained toward coverage. To address this, we propose
rethinking loss functions in language models by leveraging the Precision-Recall
framework. Our results demonstrate that this approach achieves a substantially
better trade-off between Precision and Recall than merely combining negative
log-likelihood training with temperature scaling. These findings offer a
pathway toward more versatile and robust language modeling techniques.

### 18. The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Skyler Hallinan, Jaehun Jung, Melanie Sclar, Ximing Lu, Abhilasha Ravichander, Sahana Ramnath, Yejin Choi, Sai Praneeth Karimireddy, Niloofar Mireshghallah, Xiang Ren
- **URL**: <http://arxiv.org/abs/2508.09603v1>
- **Submitted**: 2025-08-13 08:35:16
- **Comment**: CoLM 2025
- **Topic Keywords**: rag
- **Reason**: The paper focuses on membership inference attacks, which is a topic in NLP, but it doesn't directly relate to information retrieval, search technologies, or query understanding. While it does involve text outputs and models, the context is different from the user's primary research interests.

#### Abstract
> Membership inference attacks serves as useful tool for fair use of language
models, such as detecting potential copyright infringement and auditing data
leakage. However, many current state-of-the-art attacks require access to
models' hidden states or probability distribution, which prevents investigation
into more widely-used, API-access only models like GPT-4. In this work, we
introduce N-Gram Coverage Attack, a membership inference attack that relies
solely on text outputs from the target model, enabling attacks on completely
black-box models. We leverage the observation that models are more likely to
memorize and subsequently generate text patterns that were commonly observed in
their training data. Specifically, to make a prediction on a candidate member,
N-Gram Coverage Attack first obtains multiple model generations conditioned on
a prefix of the candidate. It then uses n-gram overlap metrics to compute and
aggregate the similarities of these outputs with the ground truth suffix; high
similarities indicate likely membership. We first demonstrate on a diverse set
of existing benchmarks that N-Gram Coverage Attack outperforms other black-box
methods while also impressively achieving comparable or even better performance
to state-of-the-art white-box attacks - despite having access to only text
outputs. Interestingly, we find that the success rate of our method scales with
the attack compute budget - as we increase the number of sequences generated
from the target model conditioned on the prefix, attack performance tends to
improve. Having verified the accuracy of our method, we use it to investigate
previously unstudied closed OpenAI models on multiple domains. We find that
more recent models, such as GPT-4o, exhibit increased robustness to membership
inference, suggesting an evolving trend toward improved privacy protections.

### 19. LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Jakub ≈†m√≠d, Pavel P≈ôib√°≈à, Pavel Kr√°l
- **URL**: <http://arxiv.org/abs/2508.09515v1>
- **Submitted**: 2025-08-13 05:55:48
- **Comment**: Published in Proceedings of the 63rd Annual Meeting of the
  Association for Computational Linguistics; Volume 1: Long Papers (ACL 2025).
  Official version: https://aclanthology.org/2025.acl-long.41/
- **Topic Keywords**: rag
- **Reason**: The paper focuses on cross-lingual aspect-based sentiment analysis, which is not directly related to the user's primary research interests in Information Retrieval and Search technologies. While it involves natural language processing and data augmentation, the techniques and applications are not directly applicable to the user's areas of focus.

#### Abstract
> Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed
sentiment analysis in a target language by transferring knowledge from a source
language with available annotated data. Most existing methods depend heavily on
often unreliable translation tools to bridge the language gap. In this paper,
we propose a new approach that leverages a large language model (LLM) to
generate high-quality pseudo-labelled data in the target language without the
need for translation tools. First, the framework trains an ABSA model to obtain
predictions for unlabelled target language data. Next, LLM is prompted to
generate natural sentences that better represent these noisy predictions than
the original text. The ABSA model is then further fine-tuned on the resulting
pseudo-labelled dataset. We demonstrate the effectiveness of this method across
six languages and five backbone models, surpassing previous state-of-the-art
translation-based approaches. The proposed framework also supports generative
models, and we show that fine-tuned LLMs outperform smaller multilingual
models.

### 20. Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Mahdi Dhaini, Tobias M√ºller, Roksoliana Rabets, Gjergji Kasneci
- **URL**: <http://arxiv.org/abs/2508.09786v1>
- **Submitted**: 2025-08-13 13:12:18
- **Comment**: Accepted to AAAI/ACM Conference on AI, Ethics, and Society (AIES
  2025)
- **Topic Keywords**: search
- **Reason**: The paper focuses on explainable Natural Language Processing (NLP), which is a related topic to the user's interests in NLP. However, the paper's scope is limited to explainability methods and their adoption in real-world NLP applications, which does not directly align with the user's primary focus on Information Retrieval and Search technologies.

#### Abstract
> The field of explainable natural language processing (NLP) has grown rapidly
in recent years. The growing opacity of complex models calls for transparency
and explanations of their decisions, which is crucial to understand their
reasoning and facilitate deployment, especially in high-stakes environments.
Despite increasing attention given to explainable NLP, practitioners'
perspectives regarding its practical adoption and effectiveness remain
underexplored. This paper addresses this research gap by investigating
practitioners' experiences with explainability methods, specifically focusing
on their motivations for adopting such methods, the techniques employed,
satisfaction levels, and the practical challenges encountered in real-world NLP
applications. Through a qualitative interview-based study with industry
practitioners and complementary interviews with academic researchers, we
systematically analyze and compare their perspectives. Our findings reveal
conceptual gaps, low satisfaction with current explainability methods, and
highlight evaluation challenges. Our findings emphasize the need for clear
definitions and user-centric frameworks for better adoption of explainable NLP
in practice.

### 21. AI Blob! LLM-Driven Recontextualization of Italian Television Archives

- **LLM Score**: 2
- **Keyword Score**: 11
- **Authors**: Roberto Balestri
- **URL**: <http://arxiv.org/abs/2508.09535v1>
- **Submitted**: 2025-08-13 06:38:32
- **Comment**: Preprint
- **Topic Keywords**: query, queries, rag, retrieval, search
- **Reason**: The paper focuses on AI-driven recontextualization of archival television footage, using semantic cataloging and Large Language Models, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on archival television footage and media historiography also falls outside the user's e-commerce domain background.

#### Abstract
> This paper introduces AI Blob!, an experimental system designed to explore
the potential of semantic cataloging and Large Language Models (LLMs) for the
retrieval and recontextualization of archival television footage. Drawing
methodological inspiration from Italian television programs such as Blob (RAI
Tre, 1989-), AI Blob! integrates automatic speech recognition (ASR), semantic
embeddings, and retrieval-augmented generation (RAG) to organize and
reinterpret archival content. The system processes a curated dataset of 1,547
Italian television videos by transcribing audio, segmenting it into
sentence-level units, and embedding these segments into a vector database for
semantic querying. Upon user input of a thematic prompt, the LLM generates a
range of linguistically and conceptually related queries, guiding the retrieval
and recombination of audiovisual fragments. These fragments are algorithmically
selected and structured into narrative sequences producing montages that
emulate editorial practices of ironic juxtaposition and thematic coherence. By
foregrounding dynamic, content-aware retrieval over static metadata schemas, AI
Blob! demonstrates how semantic technologies can facilitate new approaches to
archival engagement, enabling novel forms of automated narrative construction
and cultural analysis. The project contributes to ongoing debates in media
historiography and AI-driven archival research, offering both a conceptual
framework and a publicly available dataset to support further interdisciplinary
experimentation.

### 22. Performance of GPT-5 Frontier Models in Ophthalmology Question Answering

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Fares Antaki, David Mikhail, Daniel Milad, Danny A Mammo, Sumit Sharma, Sunil K Srivastava, Bing Yu Chen, Samir Touma, Mertcan Sevgi, Jonathan El-Khoury, Pearse A Keane, Qingyu Chen, Yih Chung Tham, Renaud Duval
- **URL**: <http://arxiv.org/abs/2508.09956v2>
- **Submitted**: 2025-08-13 17:17:17
- **Topic Keywords**: ranking, pairwise, rank
- **Reason**: The paper focuses on evaluating the performance of GPT-5 models in ophthalmology question answering, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on medical question-answering tasks and the use of a specific dataset (American Academy of Ophthalmology Basic Clinical Science Course) also limits its relevance to the user's broader interests in e-commerce and real-time relevance optimization.

#### Abstract
> Large language models (LLMs) such as GPT-5 integrate advanced reasoning
capabilities that may improve performance on complex medical question-answering
tasks. For this latest generation of reasoning models, the configurations that
maximize both accuracy and cost-efficiency have yet to be established. We
evaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across
four reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using
260 closed-access multiple-choice questions from the American Academy of
Ophthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome
was multiple-choice accuracy; secondary outcomes included head-to-head ranking
via a Bradley-Terry model, rationale quality assessment using a
reference-anchored, pairwise LLM-as-a-judge framework, and analysis of
accuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved
the highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano
variants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high
(0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x
stronger than o3-high) and rationale quality (1.11x stronger than o3-high).
Cost-accuracy analysis identified several GPT-5 configurations on the Pareto
frontier, with GPT-5-mini-low offering the most favorable low-cost,
high-performance balance. These results benchmark GPT-5 on a high-quality
ophthalmology dataset, demonstrate the influence of reasoning effort on
accuracy, and introduce an autograder framework for scalable evaluation of
LLM-generated answers against reference standards in ophthalmology.

### 23. EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Yaoning Wang, Jiahao Ying, Yixin Cao, Yubo Ma, Yugang Jiang
- **URL**: <http://arxiv.org/abs/2508.09662v1>
- **Submitted**: 2025-08-13 09:48:23
- **Topic Keywords**: ranking, rag, rank
- **Reason**: The paper focuses on model evaluation and benchmarking for large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on topics like ranking and utility index, the context is different from the user's primary research interests.

#### Abstract
> The rapid advancement of large language models (LLMs) and the development of
increasingly large and diverse evaluation benchmarks have introduced
substantial computational challenges for model assessment. In this paper, we
present EffiEval, a training-free approach for efficient benchmarking that
effectively addresses data redundancy while maintaining high evaluation
reliability. Our method is specifically designed to meet three key criteria for
high-quality evaluation: representativeness, by ensuring comprehensive coverage
of model capabilities; fairness, by remaining independent of model performance
during sample selection to avoid bias; and generalizability, by enabling
flexible transfer across datasets and model families without reliance on
large-scale evaluation data. Unlike traditional methods that rely on absolute
performance or require extensive evaluation data, our approach adaptively
selects high-quality representative subsets based on the Model Utility Index
(MUI). Extensive experiments on multiple public benchmarks and diverse LLMs
demonstrate that EffiEval achieves strong ranking consistency with full-dataset
evaluation using only a small fraction of the original data. Furthermore, our
method is flexible and scalable in size, allowing users to balance evaluation
efficiency and representativeness according to specific needs. Overall,
EffiEval provides a practical and generalizable solution for reliable, fair,
and efficient evaluation in the era of LLMs.

### 24. IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Junxian Li, Beining Xu, Di Zhang
- **URL**: <http://arxiv.org/abs/2508.09456v1>
- **Submitted**: 2025-08-13 03:22:19
- **Comment**: 13 pages, 13 Figures
- **Topic Keywords**: query, queries
- **Reason**: The paper focuses on backdoor attacks on vision-language models for visual grounding, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the context is different from the user's primary interests.

#### Abstract
> Vision-language models (VLMs) have shown significant advancements in tasks
such as visual grounding, where they localize specific objects in images based
on natural language queries and images. However, security issues in visual
grounding tasks for VLMs remain underexplored, especially in the context of
backdoor attacks. In this paper, we introduce a novel input-aware backdoor
attack method, IAG, designed to manipulate the grounding behavior of VLMs. This
attack forces the model to ground a specific target object in the input image,
regardless of the user's query. We propose an adaptive trigger generator that
embeds the semantic information of the attack target's description into the
original image using a text-conditional U-Net, thereby overcoming the
open-vocabulary attack challenge. To ensure the attack's stealthiness, we
utilize a reconstruction loss to minimize visual discrepancies between poisoned
and clean images. Additionally, we introduce a unified method for generating
attack data. IAG is evaluated theoretically and empirically, demonstrating its
feasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches
over 65\% on various testing sets. IAG also shows promising potential on
manipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on
clean samples. Extensive specific experiments, such as ablation study and
potential defense, also indicate the robustness and transferability of our
attack.

### 25. Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved Image Generation

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, Conghui He, Weijia Li
- **URL**: <http://arxiv.org/abs/2508.09987v1>
- **Submitted**: 2025-08-13 17:59:28
- **Comment**: 19 pages, 8 figures
- **Topic Keywords**: queries, rag
- **Reason**: The paper is not relevant to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on image generation and multimodal generation, which is outside the user's primary focus. Although it mentions GPT-4o, it does not relate to the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Recently, GPT-4o has garnered significant attention for its strong
performance in image generation, yet open-source models still lag behind.
Several studies have explored distilling image data from GPT-4o to enhance
open-source models, achieving notable progress. However, a key question
remains: given that real-world image datasets already constitute a natural
source of high-quality data, why should we use GPT-4o-generated synthetic data?
In this work, we identify two key advantages of synthetic images. First, they
can complement rare scenarios in real-world datasets, such as surreal fantasy
or multi-reference image generation, which frequently occur in user queries.
Second, they provide clean and controllable supervision. Real-world data often
contains complex background noise and inherent misalignment between text
descriptions and image content, whereas synthetic images offer pure backgrounds
and long-tailed supervision signals, facilitating more accurate text-to-image
alignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale
synthetic dataset generated by GPT-4o, harnessing the power of synthetic image
data to address blind spots in real-world coverage. Using this dataset, we
fine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.
In addition, we propose two new evaluation benchmarks for a more accurate and
challenging assessment of image generation capabilities: GenEval++, which
increases instruction complexity to mitigate score saturation, and
Imagine-Bench, which focuses on evaluating both the understanding and
generation of imaginative content. Echo-4o demonstrates strong performance
across standard benchmarks. Moreover, applying Echo-4o-Image to other
foundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains
across multiple metrics, highlighting the datasets strong transferability.

### 26. COME: Dual Structure-Semantic Learning with Collaborative MoE for Universal Lesion Detection Across Heterogeneous Ultrasound Datasets

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Lingyu Chen, Yawen Zeng, Yue Wang, Peng Wan, Guo-chen Ning, Hongen Liao, Daoqiang Zhang, Fang Chen
- **URL**: <http://arxiv.org/abs/2508.09886v1>
- **Submitted**: 2025-08-13 15:43:20
- **Comment**: ICCV 2025
- **Topic Keywords**: ltr, rag
- **Reason**: The paper is not related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on medical imaging and ultrasound data analysis, which is outside the user's primary research areas.

#### Abstract
> Conventional single-dataset training often fails with new data distributions,
especially in ultrasound (US) image analysis due to limited data, acoustic
shadows, and speckle noise. Therefore, constructing a universal framework for
multi-heterogeneous US datasets is imperative. However, a key challenge arises:
how to effectively mitigate inter-dataset interference while preserving
dataset-specific discriminative features for robust downstream task? Previous
approaches utilize either a single source-specific decoder or a domain
adaptation strategy, but these methods experienced a decline in performance
when applied to other domains. Considering this, we propose a Universal
Collaborative Mixture of Heterogeneous Source-Specific Experts (COME).
Specifically, COME establishes dual structure-semantic shared experts that
create a universal representation space and then collaborate with
source-specific experts to extract discriminative features through providing
complementary features. This design enables robust generalization by leveraging
cross-datasets experience distributions and providing universal US priors for
small-batch or unseen data scenarios. Extensive experiments under three
evaluation modes (single-dataset, intra-organ, and inter-organ integration
datasets) demonstrate COME's superiority, achieving significant mean AP
improvements over state-of-the-art methods. Our project is available at:
https://universalcome.github.io/UniversalCOME/.

### 27. RicciFlowRec: A Geometric Root Cause Recommender Using Ricci Curvature on Financial Graphs

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Zhongtian Sun, Anoushka Harit
- **URL**: <http://arxiv.org/abs/2508.09334v1>
- **Submitted**: 2025-08-12 20:45:02
- **Comment**: Accepted at ACM RecSys 2025 (Late Breaking Results Track)
- **Topic Keywords**: ranking, recommend, rank
- **Reason**: The paper focuses on recommender systems in the financial domain, using geometric flow-based reasoning, which is not directly related to the user's primary research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling. Although the paper mentions FinBERT-based sentiment, it does not explore deep semantic understanding or real-time relevance optimization, which are key aspects of the user's research focus.

#### Abstract
> We propose RicciFlowRec, a geometric recommendation framework that performs
root cause attribution via Ricci curvature and flow on dynamic financial
graphs. By modelling evolving interactions among stocks, macroeconomic
indicators, and news, we quantify local stress using discrete Ricci curvature
and trace shock propagation via Ricci flow. Curvature gradients reveal causal
substructures, informing a structural risk-aware ranking function. Preliminary
results on S\&P~500 data with FinBERT-based sentiment show improved robustness
and interpretability under synthetic perturbations. This ongoing work supports
curvature-based attribution and early-stage risk-aware ranking, with plans for
portfolio optimization and return forecasting. To our knowledge, RicciFlowRec
is the first recommender to apply geometric flow-based reasoning in financial
decision support.

### 28. UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Ladislav Lenc, Daniel C√≠fka, Ji≈ô√≠ Mart√≠nek, Jakub ≈†m√≠d, Pavel Kr√°l
- **URL**: <http://arxiv.org/abs/2508.09517v1>
- **Submitted**: 2025-08-13 05:55:59
- **Comment**: Published in Proceedings of the 19th International Workshop on
  Semantic Evaluation (SemEval-2025). Official version:
  https://aclanthology.org/2025.semeval-1.31/
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on fact-checked claim retrieval, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves text embeddings and cosine similarity, the context is different from the user's areas of focus, such as query understanding, ranking models, and user behavior modeling.

#### Abstract
> This paper presents a zero-shot system for fact-checked claim retrieval. We
employed several state-of-the-art large language models to obtain text
embeddings. The models were then combined to obtain the best possible result.
Our approach achieved 7th place in monolingual and 9th in cross-lingual
subtasks. We used only English translations as an input to the text embedding
models since multilingual models did not achieve satisfactory results. We
identified the most relevant claims for each post by leveraging the embeddings
and measuring cosine similarity. Overall, the best results were obtained by the
NVIDIA NV-Embed-v2 model. For some languages, we benefited from model
combinations (NV-Embed & GPT or Mistral).

### 29. Large Language Models Show Signs of Alignment with Human Neurocognition During Abstract Reasoning

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Christopher Pinier, Sonia Acu√±a Vargas, Mariia Steeghs-Turchina, Dora Matzke, Claire E. Stevenson, Michael D. Nunez
- **URL**: <http://arxiv.org/abs/2508.10057v1>
- **Submitted**: 2025-08-12 21:38:46
- **Comment**: Presented at the 8th Annual Conference on Cognitive Computational
  Neuroscience (August 12-15, 2025; Amsterdam, The Netherlands); 20 pages, 11
  figures
- **Topic Keywords**: rag, ctr
- **Reason**: The paper explores the alignment of large language models with human neurocognition during abstract reasoning, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper touches on neural representations and pattern recognition, it does not address query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user.

#### Abstract
> This study investigates whether large language models (LLMs) mirror human
neurocognition during abstract reasoning. We compared the performance and
neural representations of human participants with those of eight open-source
LLMs on an abstract-pattern-completion task. We leveraged pattern type
differences in task performance and in fixation-related potentials (FRPs) as
recorded by electroencephalography (EEG) during the task. Our findings indicate
that only the largest tested LLMs (~70 billion parameters) achieve
human-comparable accuracy, with Qwen-2.5-72B and DeepSeek-R1-70B also showing
similarities with the human pattern-specific difficulty profile. Critically,
every LLM tested forms representations that distinctly cluster the abstract
pattern categories within their intermediate layers, although the strength of
this clustering scales with their performance on the task. Moderate positive
correlations were observed between the representational geometries of
task-optimal LLM layers and human frontal FRPs. These results consistently
diverged from comparisons with other EEG measures (response-locked ERPs and
resting EEG), suggesting a potential shared representational space for abstract
patterns. This indicates that LLMs might mirror human brain mechanisms in
abstract reasoning, offering preliminary evidence of shared principles between
biological and artificial intelligence.

### 30. Leveraging Large Language Models for Rare Disease Named Entity Recognition

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Nan Miles Xi, Yu Deng, Lin Wang
- **URL**: <http://arxiv.org/abs/2508.09323v1>
- **Submitted**: 2025-08-12 20:16:31
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on named entity recognition in rare diseases using large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the application is specific to biomedical NER, which is not a primary interest area.

#### Abstract
> Named Entity Recognition (NER) in the rare disease domain poses unique
challenges due to limited labeled data, semantic ambiguity between entity
types, and long-tail distributions. In this study, we evaluate the capabilities
of GPT-4o for rare disease NER under low-resource settings, using a range of
prompt-based strategies including zero-shot prompting, few-shot in-context
learning, retrieval-augmented generation (RAG), and task-level fine-tuning. We
design a structured prompting framework that encodes domain-specific knowledge
and disambiguation rules for four entity types. We further introduce two
semantically guided few-shot example selection methods to improve in-context
performance while reducing labeling effort. Experiments on the RareDis Corpus
show that GPT-4o achieves competitive or superior performance compared to
BioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art
(SOTA) results. Cost-performance analysis reveals that few-shot prompting
delivers high returns at low token budgets, while RAG offers marginal
additional benefit. An error taxonomy highlights common failure modes such as
boundary drift and type confusion, suggesting opportunities for post-processing
and hybrid refinement. Our results demonstrate that prompt-optimized LLMs can
serve as effective, scalable alternatives to traditional supervised models in
biomedical NER, particularly in rare disease applications where annotated data
is scarce.

### 31. A Survey of Cognitive Distortion Detection and Classification in NLP

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Archie Sage, Jeroen Keppens, Helen Yannakoudakis
- **URL**: <http://arxiv.org/abs/2508.09878v1>
- **Submitted**: 2025-08-13 15:21:17
- **Comment**: Under review via ACL Rolling Review and committed to EMNLP 2025.
  Camera-ready updates to follow
- **Topic Keywords**: rag, search
- **Reason**: The paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. Although it touches on Natural Language Processing, the focus is on cognitive distortion detection and classification, which is a specific area within NLP that is not a primary interest of yours.

#### Abstract
> As interest grows in the application of natural language processing (NLP)
techniques to mental health, a growing body of work explores the automatic
detection and classification of cognitive distortions (CDs). CDs are habitual
patterns of negatively biased or flawed thinking that distort how people
perceive events, judge themselves, and react to the world around them.
Identifying and addressing them is an important part of therapy. Despite its
momentum, the field remains fragmented, with inconsistencies in CD taxonomies,
task formulations, and evaluation practices. This survey reviews 38 studies
spanning two decades, providing a structured overview of datasets, modelling
approaches, and evaluation strategies. We provide a consolidated CD taxonomy
reference, summarise common task setups, and highlight open challenges to
support more coherent and reproducible research in this emerging area.

### 32. Speed Always Wins: A Survey on Efficient Architectures for Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Weigao Sun, Jiaxi Hu, Yucheng Zhou, Jusen Du, Disen Lan, Kexin Wang, Tong Zhu, Xiaoye Qu, Yu Zhang, Xiaoyu Mo, Daizong Liu, Yuxuan Liang, Wenliang Chen, Guoqi Li, Yu Cheng
- **URL**: <http://arxiv.org/abs/2508.09834v1>
- **Submitted**: 2025-08-13 14:13:46
- **Comment**: Survey, 82 pages, GitHub:
  https://github.com/weigao266/Awesome-Efficient-Arch
- **Topic Keywords**: search, acl
- **Reason**: The paper focuses on efficient architectures for Large Language Models, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on language understanding, the primary focus is on language models and their efficiency, which is not a central match for the user's research interests.

#### Abstract
> Large Language Models (LLMs) have delivered impressive results in language
understanding, generation, reasoning, and pushes the ability boundary of
multimodal models. Transformer models, as the foundation of modern LLMs, offer
a strong baseline with excellent scaling properties. However, the traditional
transformer architecture requires substantial computations and poses
significant obstacles for large-scale training and practical deployment. In
this survey, we offer a systematic examination of innovative LLM architectures
that address the inherent limitations of transformers and boost the efficiency.
Starting from language modeling, this survey covers the background and
technical details of linear and sparse sequence modeling methods, efficient
full attention variants, sparse mixture-of-experts, hybrid model architectures
incorporating the above techniques, and emerging diffusion LLMs. Additionally,
we discuss applications of these techniques to other modalities and consider
their wider implications for developing scalable, resource-aware foundation
models. By grouping recent studies into the above category, this survey
presents a blueprint of modern efficient LLM architectures, and we hope this
could help motivate future research toward more efficient, versatile AI
systems.

### 33. UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Shuhei Kato
- **URL**: <http://arxiv.org/abs/2508.09767v1>
- **Submitted**: 2025-08-13 12:52:38
- **Topic Keywords**: rag, rank
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The topic of text-to-speech systems and pronunciation control in a target language is outside the scope of your primary focus on information retrieval, especially in areas that require deep semantic understanding and real-time relevance optimization.

#### Abstract
> We propose UtterTune, a lightweight adaptation method that fine-tunes a
multilingual text-to-speech (TTS) system based on a large language model (LLM)
architecture, designed to enhance the controllability of pronunciation in a
target language while preserving performance in others. While LLM architectures
have enabled TTS models to achieve remarkable naturalness, accurately modeling
grapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially
when the model omits an explicit G2P module and directly processes minimally
encoded text (e.g., byte-pair encoding). UtterTune leverages low-rank
adaptation to enable the control of segmental pronunciation and pitch accent at
the phoneme level for Japanese speech, the target language in this paper, while
maintaining naturalness and speaker similarity in a zero-shot setting.
Objective and subjective evaluations confirm its effectiveness.

### 34. NeuronTune: Fine-Grained Neuron Modulation for Balanced Safety-Utility Alignment in LLMs

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Birong Pan, Mayi Xu, Qiankun Pi, Jianhao Chen, Yuanyuan Zhu, Ming Zhong, Tieyun Qian
- **URL**: <http://arxiv.org/abs/2508.09473v1>
- **Submitted**: 2025-08-13 04:05:28
- **Topic Keywords**: queries
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus is on Large Language Models, safety alignment, and utility optimization, which is outside the scope of the user's research interests.

#### Abstract
> Ensuring robust safety alignment while preserving utility is critical for the
reliable deployment of Large Language Models (LLMs). However, current
techniques fundamentally suffer from intertwined deficiencies: insufficient
robustness against malicious attacks, frequent refusal of benign queries,
degradation in generated text quality and general task performance--the former
two reflecting deficits in robust safety and the latter constituting utility
impairment. We trace these limitations to the coarse-grained layer-wise
interventions in existing methods. To resolve this, we propose NeuronTune, a
fine-grained framework that dynamically modulates sparse neurons to achieve
simultaneous safety-utility optimization. Our approach first identifies
safety-critical and utility-preserving neurons across all layers via
attribution, then employs meta-learning to adaptively amplify safety-neuron
activations and suppress utility-neuron activations. Crucially, NeuronTune
enables tunable adjustment of intervention scope via neuron-count thresholds,
supporting flexible adaptation to security-critical or utility-priority
scenarios. Extensive experimental results demonstrate that our method
significantly outperforms existing state-of-the-art technologies, achieving
superior model safety while maintaining excellent utility.

### 35. Fake-Mamba: Real-Time Speech Deepfake Detection Using Bidirectional Mamba as Self-Attention's Alternative

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Xi Xuan, Zimo Zhu, Wenxin Zhang, Yi-Cheng Lin, Tomi Kinnunen
- **URL**: <http://arxiv.org/abs/2508.09294v1>
- **Submitted**: 2025-08-12 19:15:13
- **Comment**: Accepted at IEEE ASRU 2025
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on speech deepfake detection using bidirectional Mamba as an alternative to Self-Attention, which is unrelated to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's abstract does not mention any relevance to query understanding, ranking models, or user behavior modeling, making it an off-topic paper.

#### Abstract
> Advances in speech synthesis intensify security threats, motivating real-time
deepfake detection research. We investigate whether bidirectional Mamba can
serve as a competitive alternative to Self-Attention in detecting synthetic
speech. Our solution, Fake-Mamba, integrates an XLSR front-end with
bidirectional Mamba to capture both local and global artifacts. Our core
innovation introduces three efficient encoders: TransBiMamba, ConBiMamba, and
PN-BiMamba. Leveraging XLSR's rich linguistic representations, PN-BiMamba can
effectively capture the subtle cues of synthetic speech. Evaluated on ASVspoof
21 LA, 21 DF, and In-The-Wild benchmarks, Fake-Mamba achieves 0.97%, 1.74%, and
5.85% EER, respectively, representing substantial relative gains over SOTA
models XLSR-Conformer and XLSR-Mamba. The framework maintains real-time
inference across utterance lengths, demonstrating strong generalization and
practical viability. The code is available at
https://github.com/xuanxixi/Fake-Mamba.

### 36. VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Lingjie Jiang, Shaohan Huang, Xun Wu, Yixia Li, Dongdong Zhang, Furu Wei
- **URL**: <http://arxiv.org/abs/2508.09945v1>
- **Submitted**: 2025-08-13 17:00:44
- **Topic Keywords**: rag
- **Reason**: The paper focuses on multimodal code generation, merging vision and coding language models, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves natural language processing, the context is different from the user's primary research interests.

#### Abstract
> Multimodal large language models (MLLMs) have significantly advanced the
integration of visual and textual understanding. However, their ability to
generate code from multimodal inputs remains limited. In this work, we
introduce VisCodex, a unified framework that seamlessly merges vision and
coding language models to empower MLLMs with strong multimodal code generation
abilities. Leveraging a task vector-based model merging technique, we integrate
a state-of-the-art coding LLM into a strong vision-language backbone, while
preserving both visual comprehension and advanced coding skills. To support
training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a
large-scale and diverse collection of 598k samples, including high-quality HTML
code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic
problems. Furthermore, we propose InfiBench-V, a novel and challenging
benchmark specifically designed to assess models on visually-rich, real-world
programming questions that demand a nuanced understanding of both textual and
visual contexts. Extensive experiments show that VisCodex achieves
state-of-the-art performance among open-source MLLMs and approaches proprietary
models like GPT-4o, highlighting the effectiveness of our model merging
strategy and new datasets.

### 37. Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Lavanya Shankar, Leibny Paola Garcia Perera
- **URL**: <http://arxiv.org/abs/2508.09430v1>
- **Submitted**: 2025-08-13 02:10:31
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus is on language identification in code-switched child-directed speech, which is outside the user's primary areas of interest.

#### Abstract
> Code-switching and language identification in child-directed scenarios
present significant challenges, particularly in bilingual environments. This
paper addresses this challenge by using Zipformer to handle the nuances of
speech, which contains two imbalanced languages, Mandarin and English, in an
utterance. This work demonstrates that the internal layers of the Zipformer
effectively encode the language characteristics, which can be leveraged in
language identification. We present the selection methodology of the inner
layers to extract the embeddings and make a comparison with different
back-ends. Our analysis shows that Zipformer is robust across these backends.
Our approach effectively handles imbalanced data, achieving a Balanced Accuracy
(BAC) of 81.89%, a 15.47% improvement over the language identification
baseline. These findings highlight the potential of the transformer encoder
architecture model in real scenarios.

### 38. The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Cathy Speed, Ahmed A. Metwally
- **URL**: <http://arxiv.org/abs/2508.09349v1>
- **Submitted**: 2025-08-12 21:24:19
- **Topic Keywords**: rag
- **Reason**: The paper's focus on expert consensus development and AI-assisted decision-making is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper mentions AI, its application is in a different domain (expert consensus) and does not involve query understanding, ranking models, or user behavior modeling, which are core areas of interest.

#### Abstract
> Expert consensus plays a critical role in domains where evidence is complex,
conflicting, or insufficient for direct prescription. Traditional methods, such
as Delphi studies, consensus conferences, and systematic guideline synthesis,
offer structure but face limitations including high panel burden, interpretive
oversimplification, and suppression of conditional nuance. These challenges are
now exacerbated by information overload, fragmentation of the evidence base,
and increasing reliance on publicly available sources that lack expert
filtering. This study introduces and evaluates a Human-AI Hybrid Delphi
(HAH-Delphi) framework designed to augment expert consensus development by
integrating a generative AI model (Gemini 2.5 Pro), small panels of senior
human experts, and structured facilitation. The HAH-Delphi was tested in three
phases: retrospective replication, prospective comparison, and applied
deployment in two applied domains (endurance training and resistance and mixed
cardio/strength training). The AI replicated 95% of published expert consensus
conclusions in Phase I and showed 95% directional agreement with senior human
experts in Phase II, though it lacked experiential and pragmatic nuance. In
Phase III, compact panels of six senior experts achieved >90% consensus
coverage and reached thematic saturation before the final participant. The AI
provided consistent, literature-grounded scaffolding that supported divergence
resolution and accelerated saturation. The HAH-Delphi framework offers a
flexible, scalable approach for generating high-quality, context-sensitive
consensus. Its successful application across health, coaching, and performance
science confirms its methodological robustness and supports its use as a
foundation for generating conditional, personalised guidance and published
consensus frameworks at scale.

### 39. Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Renas Adnan, Hossein Hassani
- **URL**: <http://arxiv.org/abs/2508.09957v1>
- **Submitted**: 2025-08-13 17:19:22
- **Comment**: 21 pages, 20 figures, 7 tables
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on Speech-to-text (STT) systems and language modeling for a specific Kurdish dialect, which is outside your area of expertise.

#### Abstract
> Speech-to-text (STT) systems have a wide range of applications. They are
available in many languages, albeit at different quality levels. Although
Kurdish is considered a less-resourced language from a processing perspective,
SST is available for some of the Kurdish dialects, for instance, Sorani
(Central Kurdish). However, that is not applied to other Kurdish dialects,
Badini and Hawrami, for example. This research is an attempt to address this
gap. Bandin, approximately, has two million speakers, and STT systems can help
their community use mobile and computer-based technologies while giving their
dialect more global visibility. We aim to create a language model based on
Badini's speech and evaluate its performance. To cover a conversational aspect,
have a proper confidence level of grammatical accuracy, and ready
transcriptions, we chose Badini kids' stories, eight books including 78
stories, as the textual input. Six narrators narrated the books, which resulted
in approximately 17 hours of recording. We cleaned, segmented, and tokenized
the input. The preprocessing produced nearly 15 hours of speech, including
19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and
Whisper-small to develop the language models. The experiments indicate that the
transcriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a
significantly more accurate and readable output than the Whisper-small model,
with 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy,
respectively.

### 40. Specialised or Generic? Tokenization Choices for Radiology Language Models

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Hermione Warr, Wentian Xu, Harry Anthony, Yasin Ibrahim, Daniel McGowan, Konstantinos Kamnitsas
- **URL**: <http://arxiv.org/abs/2508.09952v1>
- **Submitted**: 2025-08-13 17:13:56
- **Comment**: Accepted to ELAMI@MICCAI2025
- **Topic Keywords**: search
- **Reason**: The paper focuses on tokenization choices for radiology language models, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it touches on language models, the domain is radiology, and the paper's findings are specific to that field, making it irrelevant to the user's primary research interests.

#### Abstract
> The vocabulary used by language models (LM) - defined by the tokenizer -
plays a key role in text generation quality. However, its impact remains
under-explored in radiology. In this work, we address this gap by
systematically comparing general, medical, and domain-specific tokenizers on
the task of radiology report summarisation across three imaging modalities. We
also investigate scenarios with and without LM pre-training on PubMed
abstracts. Our findings demonstrate that medical and domain-specific
vocabularies outperformed widely used natural language alternatives when models
are trained from scratch. Pre-training partially mitigates performance
differences between tokenizers, whilst the domain-specific tokenizers achieve
the most favourable results. Domain-specific tokenizers also reduce memory
requirements due to smaller vocabularies and shorter sequences. These results
demonstrate that adapting the vocabulary of LMs to the clinical domain provides
practical benefits, including improved performance and reduced computational
demands, making such models more accessible and effective for both research and
real-world healthcare settings.

### 41. A Comprehensive Evaluation framework of Alignment Techniques for LLMs

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Muneeza Azmat, Momin Abbas, Maysa Malfiza Garcia de Macedo, Marcelo Carpinette Grave, Luan Soares de Souza, Tiago Machado, Rogerio A de Paula, Raya Horesh, Yixin Chen, Heloisa Caroline de Souza Pereira Candello, Rebecka Nordenlow, Aminat Adebiyi
- **URL**: <http://arxiv.org/abs/2508.09937v1>
- **Submitted**: 2025-08-13 16:42:01
- **Comment**: In submission
- **Topic Keywords**: search
- **Reason**: The paper focuses on Large Language Models (LLMs) and alignment techniques, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on evaluation frameworks, the context is not relevant to the user's primary research interests.

#### Abstract
> As Large Language Models (LLMs) become increasingly integrated into
real-world applications, ensuring their outputs align with human values and
safety standards has become critical. The field has developed diverse alignment
approaches including traditional fine-tuning methods (RLHF, instruction
tuning), post-hoc correction systems, and inference-time interventions, each
with distinct advantages and limitations. However, the lack of unified
evaluation frameworks makes it difficult to systematically compare these
paradigms and guide deployment decisions. This paper introduces a
multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive
evaluation framework that provides a systematic comparison across all major
alignment paradigms. Our framework assesses methods along four key dimensions:
alignment detection, alignment quality, computational efficiency, and
robustness. Through experiments across diverse base models and alignment
strategies, we demonstrate the utility of our framework in identifying
strengths and limitations of current state-of-the-art models, providing
valuable insights for future research directions.

### 42. Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Abdul Rehman Antall, Naveed Akhtar
- **URL**: <http://arxiv.org/abs/2508.09865v1>
- **Submitted**: 2025-08-13 15:01:59
- **Comment**: 8 pages, 3 figures, 1 table, including references and appendix
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus is on automatic speech recognition (ASR) for Urdu language, which is outside your primary areas of interest.

#### Abstract
> This study evaluates the feasibility of lightweight Whisper models (Tiny,
Base, Small) for Urdu speech recognition in low-resource settings. Despite Urdu
being the 10th most spoken language globally with over 230 million speakers,
its representation in automatic speech recognition (ASR) systems remains
limited due to dialectal diversity, code-switching, and sparse training data.
We benchmark these models on a curated Urdu dataset using word error rate
(WER), without fine-tuning. Results show Whisper-Small achieves the lowest
error rates (33.68\% WER), outperforming Tiny (67.08\% WER) and Base (53.67\%
WER). Qualitative analysis reveals persistent challenges in phonetic accuracy
and lexical coherence, particularly for complex utterances. While Whisper-Small
demonstrates promise for deployable Urdu ASR, significant gaps remain. Our
findings emphasize lay the groundwork for future research into effective,
low-resource ASR systems.

### 43. A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Aishik Mandal, Prottay Kumar Adhikary, Hiba Arnaout, Iryna Gurevych, Tanmoy Chakraborty
- **URL**: <http://arxiv.org/abs/2508.09809v1>
- **Submitted**: 2025-08-13 13:42:35
- **Comment**: 14 pages, 3 figures
- **Topic Keywords**: recommend
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus is on clinical mental health AI systems, datasets, and their applications, which is a distinct area outside your primary research themes.

#### Abstract
> Mental health disorders are rising worldwide. However, the availability of
trained clinicians has not scaled proportionally, leaving many people without
adequate or timely support. To bridge this gap, recent studies have shown the
promise of Artificial Intelligence (AI) to assist mental health diagnosis,
monitoring, and intervention. However, the development of efficient, reliable,
and ethical AI to assist clinicians is heavily dependent on high-quality
clinical training datasets. Despite growing interest in data curation for
training clinical AI assistants, existing datasets largely remain scattered,
under-documented, and often inaccessible, hindering the reproducibility,
comparability, and generalizability of AI models developed for clinical mental
health care. In this paper, we present the first comprehensive survey of
clinical mental health datasets relevant to the training and development of
AI-powered clinical assistants. We categorize these datasets by mental
disorders (e.g., depression, schizophrenia), data modalities (e.g., text,
speech, physiological signals), task types (e.g., diagnosis prediction, symptom
severity estimation, intervention generation), accessibility (public,
restricted or private), and sociocultural context (e.g., language and cultural
background). Along with these, we also investigate synthetic clinical mental
health datasets. Our survey identifies critical gaps such as a lack of
longitudinal data, limited cultural and linguistic representation, inconsistent
collection and annotation standards, and a lack of modalities in synthetic
data. We conclude by outlining key challenges in curating and standardizing
future datasets and provide actionable recommendations to facilitate the
development of more robust, generalizable, and equitable mental health AI
systems.

### 44. Evaluating the Role of Large Language Models in Legal Practice in India

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Rahul Hemrajani
- **URL**: <http://arxiv.org/abs/2508.09713v1>
- **Submitted**: 2025-08-13 11:04:48
- **Topic Keywords**: search
- **Reason**: The paper focuses on the application of Large Language Models in legal practice, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper touches on aspects of query understanding and ranking models, the context is specific to legal tasks and does not align with the user's core research themes.

#### Abstract
> The integration of Artificial Intelligence(AI) into the legal profession
raises significant questions about the capacity of Large Language Models(LLM)
to perform key legal tasks. In this paper, I empirically evaluate how well
LLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian
context, including issue spotting, legal drafting, advice, research, and
reasoning. Through a survey experiment, I compare outputs from LLMs with those
of a junior lawyer, with advanced law students rating the work on helpfulness,
accuracy, and comprehensiveness. LLMs excel in drafting and issue spotting,
often matching or surpassing human work. However, they struggle with
specialised legal research, frequently generating hallucinations, factually
incorrect or fabricated outputs. I conclude that while LLMs can augment certain
legal tasks, human expertise remains essential for nuanced reasoning and the
precise application of law.

### 45. AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Tatiana Batura, Elena Bruches, Milana Shvenk, Valentin Malykh
- **URL**: <http://arxiv.org/abs/2508.09622v1>
- **Submitted**: 2025-08-13 08:53:17
- **Comment**: AINL 2025 Conference
- **Topic Keywords**: search
- **Reason**: The paper focuses on detecting AI-generated scientific abstracts in Russian, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on natural language processing, the specific application and scope are not aligned with the user's research interests.

#### Abstract
> The rapid advancement of large language models (LLMs) has revolutionized text
generation, making it increasingly difficult to distinguish between human- and
AI-generated content. This poses a significant challenge to academic integrity,
particularly in scientific publishing and multilingual contexts where detection
resources are often limited. To address this critical gap, we introduce the
AINL-Eval 2025 Shared Task, specifically focused on the detection of
AI-generated scientific abstracts in Russian. We present a novel, large-scale
dataset comprising 52,305 samples, including human-written abstracts across 12
diverse scientific domains and AI-generated counterparts from five
state-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and
GigaChat-Lite). A core objective of the task is to challenge participants to
develop robust solutions capable of generalizing to both (i) previously unseen
scientific domains and (ii) models not included in the training data. The task
was organized in two phases, attracting 10 teams and 159 submissions, with top
systems demonstrating strong performance in identifying AI-generated content.
We also establish a continuous shared task platform to foster ongoing research
and long-term progress in this important area. The dataset and platform are
publicly available at https://github.com/iis-research-team/AINL-Eval-2025.

### 46. How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Daniel Raffini, Agnese Macori, Lorenzo Porcaro, Tiziana Catarci, Marco Angelini
- **URL**: <http://arxiv.org/abs/2508.09614v1>
- **Submitted**: 2025-08-13 08:45:04
- **Comment**: 9-pages
- **Topic Keywords**: search
- **Reason**: The paper focuses on the persuasive capabilities of LLMs, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on natural language processing, the scope is limited to linguistic and rhetorical analysis, and the paper's primary concern is not on real-time relevance optimization or deep semantic understanding.

#### Abstract
> This study examines the rhetorical and linguistic features of argumentative
texts generated by ChatGPT on ethically nuanced topics and investigates their
persuasive impact on human readers.Through a user study involving 62
participants and pre-post interaction surveys, the paper analyzes how exposure
to AI-generated arguments affects opinion change and user perception. A
linguistic and rhetorical analysis of the generated texts reveals a consistent
argumentative macrostructure, reliance on formulaic expressions, and limited
stylistic richness. While ChatGPT demonstrates proficiency in constructing
coherent argumentative texts, its persuasive efficacy appears constrained,
particularly on topics involving ethical issues.The study finds that while
participants often acknowledge the benefits highlighted by ChatGPT, ethical
concerns tend to persist or even intensify post-interaction. The results also
demonstrate a variation depending on the topic. These findings highlight new
insights on AI-generated persuasion in ethically sensitive domains and are a
basis for future research.

### 47. Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Jakub ≈†m√≠d, Pavel Kr√°l
- **URL**: <http://arxiv.org/abs/2508.09516v1>
- **Submitted**: 2025-08-13 05:55:53
- **Comment**: Submitted version prior to peer review. Updated version accepted in
  Information Fusion. Official version:
  https://www.sciencedirect.com/science/article/pii/S1566253525001460
- **Topic Keywords**: search
- **Reason**: The paper focuses on cross-lingual aspect-based sentiment analysis, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and query understanding. While the paper touches on NLP and data mining, the specific topic and scope are not aligned with the user's research themes.

#### Abstract
> Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that focuses on understanding opinions at the aspect level, including
sentiment towards specific aspect terms, categories, and opinions. While ABSA
research has seen significant progress, much of the focus has been on
monolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from
resource-rich languages (such as English) to low-resource languages, remains an
under-explored area, with no systematic review of the field. This paper aims to
fill that gap by providing a comprehensive survey of cross-lingual ABSA. We
summarize key ABSA tasks, including aspect term extraction, aspect sentiment
classification, and compound tasks involving multiple sentiment elements.
Additionally, we review the datasets, modelling paradigms, and cross-lingual
transfer methods used to solve these tasks. We also examine how existing work
in monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to
the development of cross-lingual ABSA. Finally, we highlight the main
challenges and suggest directions for future research to advance cross-lingual
ABSA systems.

### 48. APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Artem Chernodub, Aman Saini, Yejin Huh, Vivek Kulkarni, Vipul Raheja
- **URL**: <http://arxiv.org/abs/2508.09378v1>
- **Submitted**: 2025-08-12 22:26:32
- **Comment**: Accepted for publication at Recent Advances in Natural Language
  Processing conference (RANLP 2025)
- **Topic Keywords**: search
- **Reason**: The paper focuses on prompt engineering and optimization for natural language processing tasks, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on large language models, the primary focus is on grammatical error correction and text simplification, which is outside the user's core research themes.

#### Abstract
> Recent advancements in large language models (LLMs) have enabled a wide range
of natural language processing (NLP) tasks to be performed through simple
prompt-based interactions. Consequently, several approaches have been proposed
to engineer prompts that most effectively enable LLMs to perform a given task
(e.g., chain-of-thought prompting). In settings with a well-defined metric to
optimize model performance, automatic prompt optimization (APO) methods have
been developed to refine a seed prompt. Advancing this line of research, we
propose APIO, a simple but effective prompt induction and optimization approach
for the tasks of Grammatical Error Correction (GEC) and Text Simplification,
without relying on manually specified seed prompts. APIO achieves a new
state-of-the-art performance for purely LLM-based prompting methods on these
tasks. We make our data, code, prompts, and outputs publicly available.

### 49. A Signer-Invariant Conformer and Multi-Scale Fusion Transformer for Continuous Sign Language Recognition

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Md Rezwanul Haque, Md. Milon Islam, S M Taslim Uddin Raju, Fakhri Karray
- **URL**: <http://arxiv.org/abs/2508.09372v1>
- **Submitted**: 2025-08-12 21:59:53
- **Comment**: Accepted for the IEEE/CVF International Conference on Computer Vision
  (ICCV), Honolulu, Hawaii, USA. 1st MSLR Workshop 2025
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on Continuous Sign Language Recognition, which is a topic in Computer Vision and NLP, but not directly related to your areas of interest.

#### Abstract
> Continuous Sign Language Recognition (CSLR) faces multiple challenges,
including significant inter-signer variability and poor generalization to novel
sentence structures. Traditional solutions frequently fail to handle these
issues efficiently. For overcoming these constraints, we propose a
dual-architecture framework. For the Signer-Independent (SI) challenge, we
propose a Signer-Invariant Conformer that combines convolutions with multi-head
self-attention to learn robust, signer-agnostic representations from pose-based
skeletal keypoints. For the Unseen-Sentences (US) task, we designed a
Multi-Scale Fusion Transformer with a novel dual-path temporal encoder that
captures both fine-grained posture dynamics, enabling the model's ability to
comprehend novel grammatical compositions. Experiments on the challenging
Isharah-1000 dataset establish a new standard for both CSLR benchmarks. The
proposed conformer architecture achieves a Word Error Rate (WER) of 13.07% on
the SI challenge, a reduction of 13.53% from the state-of-the-art. On the US
task, the transformer model scores a WER of 47.78%, surpassing previous work.
In the SignEval 2025 CSLR challenge, our team placed 2nd in the US task and 4th
in the SI task, demonstrating the performance of these models. The findings
validate our key hypothesis: that developing task-specific networks designed
for the particular challenges of CSLR leads to considerable performance
improvements and establishes a new baseline for further research. The source
code is available at: https://github.com/rezwanh001/MSLR-Pose86K-CSLR-Isharah.

### 50. Can AI Keep a Secret? Contextual Integrity Verification: A Provable Security Architecture for LLMs

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Aayush Gupta
- **URL**: <http://arxiv.org/abs/2508.09288v1>
- **Submitted**: 2025-08-12 18:47:30
- **Comment**: 2 figures, 3 tables; code and certification harness:
  https://github.com/ayushgupta4897/Contextual-Integrity-Verification ;
  Elite-Attack dataset: https://huggingface.co/datasets/zyushg/elite-attack
- **Topic Keywords**: search
- **Reason**: This paper focuses on the security of large language models, specifically on preventing prompt injection and jailbreak attacks. While it touches on transformer models, it does not relate to information retrieval, search technologies, or query understanding, which are the primary areas of interest. The paper's relevance to the user's research is limited.

#### Abstract
> Large language models (LLMs) remain acutely vulnerable to prompt injection
and related jailbreak attacks; heuristic guardrails (rules, filters, LLM
judges) are routinely bypassed. We present Contextual Integrity Verification
(CIV), an inference-time security architecture that attaches cryptographically
signed provenance labels to every token and enforces a source-trust lattice
inside the transformer via a pre-softmax hard attention mask (with optional
FFN/residual gating). CIV provides deterministic, per-token non-interference
guarantees on frozen models: lower-trust tokens cannot influence higher-trust
representations. On benchmarks derived from recent taxonomies of
prompt-injection vectors (Elite-Attack + SoK-246), CIV attains 0% attack
success rate under the stated threat model while preserving 93.1% token-level
similarity and showing no degradation in model perplexity on benign tasks; we
note a latency overhead attributable to a non-optimized data path. Because CIV
is a lightweight patch -- no fine-tuning required -- we demonstrate drop-in
protection for Llama-3-8B and Mistral-7B. We release a reference
implementation, an automated certification harness, and the Elite-Attack corpus
to support reproducible research.

---

