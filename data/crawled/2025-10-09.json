[
    {
        "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
        "abstract": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN.",
        "url": "http://arxiv.org/abs/2510.07318v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07318v1",
        "arxiv_id": "2510.07318v1",
        "authors": [
            "Yunhao Fang",
            "Weihao Yu",
            "Shu Zhong",
            "Qinghao Ye",
            "Xuehan Xiong",
            "Lai Wei"
        ],
        "submitted": "2025-10-08 17:59:55",
        "source": "arxiv",
        "comment": "Code: https://github.com/ByteDance-Seed/AHN"
    },
    {
        "title": "Vibe Checker: Aligning Code Evaluation with Human Preference",
        "abstract": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage\nLLMs to generate and iteratively refine code through natural language\ninteractions until it passes their vibe check. Vibe check is tied to real-world\nhuman preference and goes beyond functionality: the solution should feel right,\nread cleanly, preserve intent, and remain correct. However, current code\nevaluation remains anchored to pass@k and captures only functional correctness,\noverlooking the non-functional instructions that users routinely apply. In this\npaper, we hypothesize that instruction following is the missing piece\nunderlying vibe check that represents human preference in coding besides\nfunctional correctness. To quantify models' code instruction following\ncapabilities with measurable signals, we present VeriCode, a taxonomy of 30\nverifiable code instructions together with corresponding deterministic\nverifiers. We use the taxonomy to augment established evaluation suites,\nresulting in Vibe Checker, a testbed to assess both code instruction following\nand functional correctness. Upon evaluating 31 leading LLMs, we show that even\nthe strongest models struggle to comply with multiple instructions and exhibit\nclear functional regression. Most importantly, a composite score of functional\ncorrectness and instruction following correlates the best with human\npreference, with the latter emerging as the primary differentiator on\nreal-world programming tasks. Our work identifies core factors of the vibe\ncheck, providing a concrete path for benchmarking and developing models that\nbetter align with user preferences in coding.",
        "url": "http://arxiv.org/abs/2510.07315v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07315v1",
        "arxiv_id": "2510.07315v1",
        "authors": [
            "Ming Zhong",
            "Xiang Zhou",
            "Ting-Yun Chang",
            "Qingze Wang",
            "Nan Xu",
            "Xiance Si",
            "Dan Garrette",
            "Shyam Upadhyay",
            "Jeremiah Liu",
            "Jiawei Han",
            "Benoit Schillings",
            "Jiao Sun"
        ],
        "submitted": "2025-10-08 17:59:19",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain",
        "abstract": "In the business domain, where data-driven decision making is crucial,\ntext-to-SQL is fundamental for easy natural language access to structured data.\nWhile recent LLMs have achieved strong performance in code generation, existing\ntext-to-SQL benchmarks remain focused on factual retrieval of past records. We\nintroduce CORGI, a new benchmark specifically designed for real-world business\ncontexts. CORGI is composed of synthetic databases inspired by enterprises such\nas Doordash, Airbnb, and Lululemon. It provides questions across four\nincreasingly complex categories of business queries: descriptive, explanatory,\npredictive, and recommendational. This challenge calls for causal reasoning,\ntemporal forecasting, and strategic recommendation, reflecting multi-level and\nmulti-step agentic intelligence. We find that LLM performance drops on\nhigh-level questions, struggling to make accurate predictions and offer\nactionable plans. Based on execution success rate, the CORGI benchmark is about\n21% more difficult than the BIRD benchmark. This highlights the gap between\npopular LLMs and the need for real-world business intelligence. We release a\npublic dataset and evaluation framework, and a website for public submissions.",
        "url": "http://arxiv.org/abs/2510.07309v2",
        "pdf_url": "http://arxiv.org/pdf/2510.07309v2",
        "arxiv_id": "2510.07309v2",
        "authors": [
            "Yue Li",
            "Ran Tao",
            "Derek Hommel",
            "Yusuf Denizay Dönder",
            "Sungyong Chang",
            "David Mimno",
            "Unso Eun Seo Jo"
        ],
        "submitted": "2025-10-08 17:57:35",
        "source": "arxiv",
        "comment": "20 pages, 6 figures, under review for ACL ARR; typos corrected"
    },
    {
        "title": "Think Natively: Unlocking Multilingual Reasoning with Consistency-Enhanced Reinforcement Learning",
        "abstract": "Large Reasoning Models (LRMs) have achieved remarkable performance on complex\nreasoning tasks by adopting the \"think-then-answer\" paradigm, which enhances\nboth accuracy and interpretability. However, current LRMs exhibit two critical\nlimitations when processing non-English languages: (1) They often struggle to\nmaintain input-output language consistency; (2) They generally perform poorly\nwith wrong reasoning paths and lower answer accuracy compared to English. These\nlimitations significantly degrade the user experience for non-English speakers\nand hinder the global deployment of LRMs. To address these limitations, we\npropose M-Thinker, which is trained by the GRPO algorithm that involves a\nLanguage Consistency (LC) reward and a novel Cross-lingual Thinking Alignment\n(CTA) reward. Specifically, the LC reward defines a strict constraint on the\nlanguage consistency between the input, thought, and answer. Besides, the CTA\nreward compares the model's non-English reasoning paths with its English\nreasoning path to transfer its own reasoning capability from English to\nnon-English languages. Through an iterative RL procedure, our M-Thinker-1.5B/7B\nmodels not only achieve nearly 100% language consistency and superior\nperformance on two multilingual benchmarks (MMATH and PolyMath), but also\nexhibit excellent generalization on out-of-domain languages.",
        "url": "http://arxiv.org/abs/2510.07300v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07300v1",
        "arxiv_id": "2510.07300v1",
        "authors": [
            "Xue Zhang",
            "Yunlong Liang",
            "Fandong Meng",
            "Songming Zhang",
            "Kaiyu Huang",
            "Yufeng Chen",
            "Jinan Xu",
            "Jie Zhou"
        ],
        "submitted": "2025-10-08 17:55:02",
        "source": "arxiv",
        "comment": "13 pages, 8 tables, 4 figures"
    },
    {
        "title": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs",
        "abstract": "Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks.",
        "url": "http://arxiv.org/abs/2510.07293v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07293v1",
        "arxiv_id": "2510.07293v1",
        "authors": [
            "Peize He",
            "Zichen Wen",
            "Yubo Wang",
            "Yuxuan Wang",
            "Xiaoqian Liu",
            "Jiajie Huang",
            "Zehui Lei",
            "Zhuangcheng Gu",
            "Xiangqi Jin",
            "Jiabing Yang",
            "Kai Li",
            "Zhifei Liu",
            "Weijia Li",
            "Cunxiang Wang",
            "Conghui He",
            "Linfeng Zhang"
        ],
        "submitted": "2025-10-08 17:50:16",
        "source": "arxiv",
        "comment": "26 pages, 23 figures, the code is available at\n  \\url{https://github.com/DabDans/AudioMarathon}"
    },
    {
        "title": "On the Convergence of Moral Self-Correction in Large Language Models",
        "abstract": "Large Language Models (LLMs) are able to improve their responses when\ninstructed to do so, a capability known as self-correction. When instructions\nprovide only a general and abstract goal without specific details about\npotential issues in the response, LLMs must rely on their internal knowledge to\nimprove response quality, a process referred to as intrinsic self-correction.\nThe empirical success of intrinsic self-correction is evident in various\napplications, but how and why it is effective remains unknown. Focusing on\nmoral self-correction in LLMs, we reveal a key characteristic of intrinsic\nself-correction: performance convergence through multi-round interactions; and\nprovide a mechanistic analysis of this convergence behavior. Based on our\nexperimental results and analysis, we uncover the underlying mechanism of\nconvergence: consistently injected self-correction instructions activate moral\nconcepts that reduce model uncertainty, leading to converged performance as the\nactivated moral concepts stabilize over successive rounds. This paper\ndemonstrates the strong potential of moral self-correction by showing that it\nexhibits a desirable property of converged performance.",
        "url": "http://arxiv.org/abs/2510.07290v2",
        "pdf_url": "http://arxiv.org/pdf/2510.07290v2",
        "arxiv_id": "2510.07290v2",
        "authors": [
            "Guangliang Liu",
            "Haitao Mao",
            "Bochuan Cao",
            "Zhiyu Xue",
            "Xitong Zhang",
            "Rongrong Wang",
            "Kristen Marie Johnson"
        ],
        "submitted": "2025-10-08 17:46:27",
        "source": "arxiv",
        "comment": "19pages, 7 figures"
    },
    {
        "title": "Online Rubrics Elicitation from Pairwise Comparisons",
        "abstract": "Rubrics provide a flexible way to train LLMs on open-ended long-form answers\nwhere verifiable rewards are not applicable and human preferences provide\ncoarse signals. Prior work shows that reinforcement learning with rubric-based\nrewards leads to consistent gains in LLM post-training. Most existing\napproaches rely on rubrics that remain static over the course of training. Such\nstatic rubrics, however, are vulnerable to reward-hacking type behaviors and\nfail to capture emergent desiderata that arise during training. We introduce\nOnline Rubrics Elicitation (OnlineRubrics), a method that dynamically curates\nevaluation criteria in an online manner through pairwise comparisons of\nresponses from current and reference policies. This online process enables\ncontinuous identification and mitigation of errors as training proceeds.\nEmpirically, this approach yields consistent improvements of up to 8% over\ntraining exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as\nwell as the validation sets of expert questions and rubrics. We qualitatively\nanalyze the elicited criteria and identify prominent themes such as\ntransparency, practicality, organization, and reasoning.",
        "url": "http://arxiv.org/abs/2510.07284v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07284v1",
        "arxiv_id": "2510.07284v1",
        "authors": [
            "MohammadHossein Rezaei",
            "Robert Vacareanu",
            "Zihao Wang",
            "Clinton Wang",
            "Yunzhong He",
            "Afra Feyza Akyürek"
        ],
        "submitted": "2025-10-08 17:44:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Don't Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models",
        "abstract": "Small language models (SLMs) offer significant computational advantages for\ntool-augmented AI systems, yet they struggle with tool-use tasks, particularly\nin selecting appropriate tools and identifying correct parameters. A common\nfailure mode is schema misalignment: models hallucinate plausible but\nnon-existent tool names that reflect naming conventions internalized during\npretraining but absent from the provided tool schema. Rather than forcing\nmodels to adapt to arbitrary schemas, we propose adapting schemas to align with\nmodels' pretrained knowledge. We introduce PA-Tool (Pretraining-Aligned Tool\nSchema Generation), a training-free method that leverages peakedness-a signal\nfrom contamination detection indicating pretraining familiarity-to\nautomatically rename tool components. By generating multiple candidates and\nselecting those with highest output concentration across samples, PA-Tool\nidentifies pretrain-aligned naming patterns. Experiments on MetaTool and\nRoTBench show improvements of up to 17% points, with schema misalignment errors\nreduced by 80%. PA-Tool enables small models to approach state-of-the-art\nperformance while maintaining computational efficiency for adaptation to new\ntools without retraining. Our work demonstrates that schema-level interventions\ncan unlock the tool-use potential of resource-efficient models by adapting\nschemas to models rather than models to schemas.",
        "url": "http://arxiv.org/abs/2510.07248v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07248v1",
        "arxiv_id": "2510.07248v1",
        "authors": [
            "Jonggeun Lee",
            "Woojung Song",
            "Jongwook Han",
            "Haesung Pyun",
            "Yohan Jo"
        ],
        "submitted": "2025-10-08 17:16:07",
        "source": "arxiv",
        "comment": "15 pages, 4 figures"
    },
    {
        "title": "LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation",
        "abstract": "Evaluating large language model (LLM) outputs in the legal domain presents\nunique challenges due to the complex and nuanced nature of legal analysis.\nCurrent evaluation approaches either depend on reference data, which is costly\nto produce, or use standardized assessment methods, both of which have\nsignificant limitations for legal applications.\n  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its\nreliability and effectiveness in legal contexts depend heavily on evaluation\nprocesses unique to the legal industry and how trustworthy the evaluation\nappears to the human legal expert. This is where existing evaluation methods\ncurrently fail and exhibit considerable variability.\n  This paper aims to close the gap: a) we break down lengthy responses into\n'Legal Data Points' (LDPs), self-contained units of information, and introduce\na novel, reference-free evaluation methodology that reflects how lawyers\nevaluate legal answers; b) we demonstrate that our method outperforms a variety\nof baselines on both our proprietary dataset and an open-source dataset\n(LegalBench); c) we show how our method correlates more closely with human\nexpert evaluations and helps improve inter-annotator agreement; and finally d)\nwe open source our Legal Data Points for a subset of LegalBench used in our\nexperiments, allowing the research community to replicate our results and\nadvance research in this vital area of LLM evaluation on legal\nquestion-answering.",
        "url": "http://arxiv.org/abs/2510.07243v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07243v1",
        "arxiv_id": "2510.07243v1",
        "authors": [
            "Joseph Enguehard",
            "Morgane Van Ermengem",
            "Kate Atkinson",
            "Sujeong Cha",
            "Arijit Ghosh Chowdhury",
            "Prashanth Kallur Ramaswamy",
            "Jeremy Roghair",
            "Hannah R Marlowe",
            "Carina Suzana Negreanu",
            "Kitty Boxall",
            "Diana Mincu"
        ],
        "submitted": "2025-10-08 17:10:47",
        "source": "arxiv",
        "comment": "Published in Natural Legal Language Processing - EMNLP Workshop 2025"
    },
    {
        "title": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense",
        "abstract": "Post-training for reasoning of large language models (LLMs) increasingly\nrelies on verifiable rewards: deterministic checkers that provide 0-1\ncorrectness signals. While reliable, such binary feedback is brittle--many\ntasks admit partially correct or alternative answers that verifiers\nunder-credit, and the resulting all-or-nothing supervision limits learning.\nReward models offer richer, continuous feedback, which can serve as a\ncomplementary supervisory signal to verifiers. We introduce HERO (Hybrid\nEnsemble Reward Optimization), a reinforcement learning framework that\nintegrates verifier signals with reward-model scores in a structured way. HERO\nemploys stratified normalization to bound reward-model scores within\nverifier-defined groups, preserving correctness while refining quality\ndistinctions, and variance-aware weighting to emphasize challenging prompts\nwhere dense signals matter most. Across diverse mathematical reasoning\nbenchmarks, HERO consistently outperforms RM-only and verifier-only baselines,\nwith strong gains on both verifiable and hard-to-verify tasks. Our results show\nthat hybrid reward design retains the stability of verifiers while leveraging\nthe nuance of reward models to advance reasoning.",
        "url": "http://arxiv.org/abs/2510.07242v2",
        "pdf_url": "http://arxiv.org/pdf/2510.07242v2",
        "arxiv_id": "2510.07242v2",
        "authors": [
            "Leitian Tao",
            "Ilia Kulikov",
            "Swarnadeep Saha",
            "Tianlu Wang",
            "Jing Xu",
            "Yixuan Li",
            "Jason E Weston",
            "Ping Yu"
        ],
        "submitted": "2025-10-08 17:09:41",
        "source": "arxiv",
        "comment": "21 pages"
    },
    {
        "title": "Red-Bandit: Test-Time Adaptation for LLM Red-Teaming via Bandit-Guided LoRA Experts",
        "abstract": "Automated red-teaming has emerged as a scalable approach for auditing Large\nLanguage Models (LLMs) prior to deployment, yet existing approaches lack\nmechanisms to efficiently adapt to model-specific vulnerabilities at inference.\nWe introduce Red-Bandit, a red-teaming framework that adapts online to identify\nand exploit model failure modes under distinct attack styles (e.g.,\nmanipulation, slang). Red-Bandit post-trains a set of parameter-efficient LoRA\nexperts, each specialized for a particular attack style, using reinforcement\nlearning that rewards the generation of unsafe prompts via a rule-based safety\nmodel. At inference, a multi-armed bandit policy dynamically selects among\nthese attack-style experts based on the target model's response safety,\nbalancing exploration and exploitation. Red-Bandit achieves state-of-the-art\nresults on AdvBench under sufficient exploration (ASR@10), while producing more\nhuman-readable prompts (lower perplexity). Moreover, Red-Bandit's bandit policy\nserves as a diagnostic tool for uncovering model-specific vulnerabilities by\nindicating which attack styles most effectively elicit unsafe behaviors.",
        "url": "http://arxiv.org/abs/2510.07239v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07239v1",
        "arxiv_id": "2510.07239v1",
        "authors": [
            "Christos Ziakas",
            "Nicholas Loo",
            "Nishita Jain",
            "Alessandra Russo"
        ],
        "submitted": "2025-10-08 17:06:20",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation",
        "abstract": "The rapid evolution of large language models (LLMs) and the real world has\noutpaced the static nature of widely used evaluation benchmarks, raising\nconcerns about their reliability for evaluating LLM factuality. While\nsubstantial works continue to rely on the popular but old benchmarks, their\ntemporal misalignment with real-world facts and modern LLMs, and their effects\non LLM factuality evaluation remain underexplored. Therefore, in this work, we\npresent a systematic investigation of this issue by examining five popular\nfactuality benchmarks and eight LLMs released across different years. An\nup-to-date fact retrieval pipeline and three metrics are tailored to quantify\nbenchmark aging and its impact on LLM factuality evaluation. Experimental\nresults and analysis illustrate that a considerable portion of samples in the\nwidely used factuality benchmarks are outdated, leading to unreliable\nassessments of LLM factuality. We hope our work can provide a testbed to assess\nthe reliability of a benchmark for LLM factuality evaluation and inspire more\nresearch on the benchmark aging issue. Codes are available in\nhttps://github.com/JiangXunyi/BenchAge.",
        "url": "http://arxiv.org/abs/2510.07238v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07238v1",
        "arxiv_id": "2510.07238v1",
        "authors": [
            "Xunyi Jiang",
            "Dingyi Chang",
            "Julian McAuley",
            "Xin Xu"
        ],
        "submitted": "2025-10-08 17:06:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding",
        "abstract": "Question answering over visually rich documents (VRDs) requires reasoning not\nonly over isolated content but also over documents' structural organization and\ncross-page dependencies. However, conventional retrieval-augmented generation\n(RAG) methods encode content in isolated chunks during ingestion, losing\nstructural and cross-page dependencies, and retrieve a fixed number of pages at\ninference, regardless of the specific demands of the question or context. This\noften results in incomplete evidence retrieval and degraded answer quality for\nmulti-page reasoning tasks. To address these limitations, we propose LAD-RAG, a\nnovel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs\na symbolic document graph that captures layout structure and cross-page\ndependencies, adding it alongside standard neural embeddings to yield a more\nholistic representation of the document. During inference, an LLM agent\ndynamically interacts with the neural and symbolic indices to adaptively\nretrieve the necessary evidence based on the query. Experiments on\nMMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG\nimproves retrieval, achieving over 90% perfect recall on average without any\ntop-k tuning, and outperforming baseline retrievers by up to 20% in recall at\ncomparable noise levels, yielding higher QA accuracy with minimal latency.",
        "url": "http://arxiv.org/abs/2510.07233v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07233v1",
        "arxiv_id": "2510.07233v1",
        "authors": [
            "Zhivar Sourati",
            "Zheng Wang",
            "Marianne Menglin Liu",
            "Yazhe Hu",
            "Mengqing Guo",
            "Sujeeth Bharadwaj",
            "Kyu Han",
            "Tao Sheng",
            "Sujith Ravi",
            "Morteza Dehghani",
            "Dan Roth"
        ],
        "submitted": "2025-10-08 17:02:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships",
        "abstract": "Causal reasoning is fundamental for Large Language Models (LLMs) to\nunderstand genuine cause-and-effect relationships beyond pattern matching.\nExisting benchmarks suffer from critical limitations such as reliance on\nsynthetic data and narrow domain coverage. We introduce a novel benchmark\nconstructed from casually identified relationships extracted from top-tier\neconomics and finance journals, drawing on rigorous methodologies including\ninstrumental variables, difference-in-differences, and regression discontinuity\ndesigns. Our benchmark comprises 40,379 evaluation items covering five task\ntypes across domains such as health, environment, technology, law, and culture.\nExperimental results on eight state-of-the-art LLMs reveal substantial\nlimitations, with the best model achieving only 57.6\\% accuracy. Moreover,\nmodel scale does not consistently translate to superior performance, and even\nadvanced reasoning models struggle with fundamental causal relationship\nidentification. These findings underscore a critical gap between current LLM\ncapabilities and demands of reliable causal reasoning in high-stakes\napplications.",
        "url": "http://arxiv.org/abs/2510.07231v2",
        "pdf_url": "http://arxiv.org/pdf/2510.07231v2",
        "arxiv_id": "2510.07231v2",
        "authors": [
            "Donggyu Lee",
            "Sungwon Park",
            "Yerin Hwang",
            "Hyoshin Kim",
            "Hyunwoo Oh",
            "Jungwon Kim",
            "Meeyoung Cha",
            "Sangyoon Park",
            "Jihee Kim"
        ],
        "submitted": "2025-10-08 17:00:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping",
        "abstract": "Simulating step-wise human behavior with Large Language Models (LLMs) has\nbecome an emerging research direction, enabling applications in various\npractical domains. While prior methods, including prompting, supervised\nfine-tuning (SFT), and reinforcement learning (RL), have shown promise in\nmodeling step-wise behavior, they primarily learn a population-level policy\nwithout conditioning on a user's persona, yielding generic rather than\npersonalized simulations. In this work, we pose a critical question: how can\nLLM agents better simulate personalized user behavior? We introduce\nCustomer-R1, an RL-based method for personalized, step-wise user behavior\nsimulation in online shopping environments. Our policy is conditioned on an\nexplicit persona, and we optimize next-step rationale and action generation via\naction correctness reward signals. Experiments on the OPeRA dataset emonstrate\nthat Customer-R1 not only significantly outperforms prompting and SFT-based\nbaselines in next-action prediction tasks, but also better matches users'\naction distribution, indicating higher fidelity in personalized behavior\nsimulation.",
        "url": "http://arxiv.org/abs/2510.07230v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07230v1",
        "arxiv_id": "2510.07230v1",
        "authors": [
            "Ziyi Wang",
            "Yuxuan Lu",
            "Yimeng Zhang",
            "Jing Huang",
            "Dakuo Wang"
        ],
        "submitted": "2025-10-08 17:00:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation",
        "abstract": "Small Language models (SLMs) offer an efficient and accessible alternative to\nLarge Language Models (LLMs), delivering strong performance while using far\nfewer resources. We introduce a simple and effective framework for pretraining\nSLMs that brings together three complementary ideas. First, we identify\nstructurally sparse sub-network initializations that consistently outperform\nrandomly initialized models of similar size under the same compute budget.\nSecond, we use evolutionary search to automatically discover high-quality\nsub-network initializations, providing better starting points for pretraining.\nThird, we apply knowledge distillation from larger teacher models to speed up\ntraining and improve generalization. Together, these components make SLM\npretraining substantially more efficient: our best model, discovered using\nevolutionary search and initialized with LLM weights, matches the validation\nperplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining\ntokens. We release all code and models at\nhttps://github.com/whittle-org/whittle/, offering a practical and reproducible\npath toward cost-efficient small language model development at scale.",
        "url": "http://arxiv.org/abs/2510.07227v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07227v1",
        "arxiv_id": "2510.07227v1",
        "authors": [
            "Arjun Krishnakumar",
            "Rhea Sanjay Sukthanker",
            "Hannan Javed Mahadik",
            "Gabriela Kadlecová",
            "Vladyslav Moroshan",
            "Timur Carstensen",
            "Frank Hutter",
            "Aaron Klein"
        ],
        "submitted": "2025-10-08 16:57:46",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Machines in the Crowd? Measuring the Footprint of Machine-Generated Text on Reddit",
        "abstract": "Generative Artificial Intelligence is reshaping online communication by\nenabling large-scale production of Machine-Generated Text (MGT) at low cost.\nWhile its presence is rapidly growing across the Web, little is known about how\nMGT integrates into social media environments. In this paper, we present the\nfirst large-scale characterization of MGT on Reddit. Using a state-of-the-art\nstatistical method for detection of MGT, we analyze over two years of activity\n(2022-2024) across 51 subreddits representative of Reddit's main community\ntypes such as information seeking, social support, and discussion. We study the\nconcentration of MGT across communities and over time, and compared MGT to\nhuman-authored text in terms of social signals it expresses and engagement it\nreceives. Our very conservative estimate of MGT prevalence indicates that\nsynthetic text is marginally present on Reddit, but it can reach peaks of up to\n9% in some communities in some months. MGT is unevenly distributed across\ncommunities, more prevalent in subreddits focused on technical knowledge and\nsocial support, and often concentrated in the activity of a small fraction of\nusers. MGT also conveys distinct social signals of warmth and status giving\ntypical of language of AI assistants. Despite these stylistic differences, MGT\nachieves engagement levels comparable than human-authored content and in a few\ncases even higher, suggesting that AI-generated text is becoming an organic\ncomponent of online social discourse. This work offers the first perspective on\nthe MGT footprint on Reddit, paving the way for new investigations involving\nplatform governance, detection strategies, and community dynamics.",
        "url": "http://arxiv.org/abs/2510.07226v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07226v1",
        "arxiv_id": "2510.07226v1",
        "authors": [
            "Lucio La Cava",
            "Luca Maria Aiello",
            "Andrea Tagarelli"
        ],
        "submitted": "2025-10-08 16:57:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "How much speech data is necessary for ASR in African languages? An evaluation of data scaling in Kinyarwanda and Kikuyu",
        "abstract": "The development of Automatic Speech Recognition (ASR) systems for\nlow-resource African languages remains challenging due to limited transcribed\nspeech data. While recent advances in large multilingual models like OpenAI's\nWhisper offer promising pathways for low-resource ASR development, critical\nquestions persist regarding practical deployment requirements. This paper\naddresses two fundamental concerns for practitioners: determining the minimum\ndata volumes needed for viable performance and characterizing the primary\nfailure modes that emerge in production systems. We evaluate Whisper's\nperformance through comprehensive experiments on two Bantu languages:\nsystematic data scaling analysis on Kinyarwanda using training sets from 1 to\n1,400 hours, and detailed error characterization on Kikuyu using 270 hours of\ntraining data. Our scaling experiments demonstrate that practical ASR\nperformance (WER < 13\\%) becomes achievable with as little as 50 hours of\ntraining data, with substantial improvements continuing through 200 hours (WER\n< 10\\%). Complementing these volume-focused findings, our error analysis\nreveals that data quality issues, particularly noisy ground truth\ntranscriptions, account for 38.6\\% of high-error cases, indicating that careful\ndata curation is as critical as data volume for robust system performance.\nThese results provide actionable benchmarks and deployment guidance for teams\ndeveloping ASR systems across similar low-resource language contexts. We\nrelease accompanying and models see\nhttps://github.com/SunbirdAI/kinyarwanda-whisper-eval",
        "url": "http://arxiv.org/abs/2510.07221v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07221v1",
        "arxiv_id": "2510.07221v1",
        "authors": [
            "Benjamin Akera",
            "Evelyn Nafula",
            "Patrick Walukagga",
            "Gilbert Yiga",
            "John Quinn",
            "Ernest Mwebaze"
        ],
        "submitted": "2025-10-08 16:55:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Language Lives in Sparse Dimensions: Toward Interpretable and Efficient Multilingual Control for Large Language Models",
        "abstract": "Large language models exhibit strong multilingual capabilities despite\nlimited exposure to non-English data. Prior studies show that English-centric\nlarge language models map multilingual content into English-aligned\nrepresentations at intermediate layers and then project them back into\ntarget-language token spaces in the final layer. From this observation, we\nhypothesize that this cross-lingual transition is governed by a small and\nsparse set of dimensions, which occur at consistent indices across the\nintermediate to final layers. Building on this insight, we introduce a simple,\ntraining-free method to identify and manipulate these dimensions, requiring\nonly as few as 50 sentences of either parallel or monolingual data. Experiments\non a multilingual generation control task reveal the interpretability of these\ndimensions, demonstrating that the interventions in these dimensions can switch\nthe output language while preserving semantic content, and that it surpasses\nthe performance of prior neuron-based approaches at a substantially lower cost.",
        "url": "http://arxiv.org/abs/2510.07213v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07213v1",
        "arxiv_id": "2510.07213v1",
        "authors": [
            "Chengzhi Zhong",
            "Fei Cheng",
            "Qianying Liu",
            "Yugo Murawaki",
            "Chenhui Chu",
            "Sadao Kurohashi"
        ],
        "submitted": "2025-10-08 16:46:57",
        "source": "arxiv",
        "comment": "Work in progress. Our code will be available at:\n  https://github.com/ku-nlp/language-specific-dimensions"
    },
    {
        "title": "Sunflower: A New Approach To Expanding Coverage of African Languages in Large Language Models",
        "abstract": "There are more than 2000 living languages in Africa, most of which have been\nbypassed by advances in language technology. Current leading LLMs exhibit\nstrong performance on a number of the most common languages (e.g. Swahili or\nYoruba), but prioritise support for the languages with the most speakers first,\nresulting in piecemeal ability across disparate languages. We contend that a\nregionally focussed approach is more efficient, and present a case study for\nUganda, a country with high linguistic diversity. We describe the development\nof Sunflower 14B and 32B, a pair of models based on Qwen 3 with state of the\nart comprehension in the majority of all Ugandan languages. These models are\nopen source and can be used to reduce language barriers in a number of\nimportant practical applications.",
        "url": "http://arxiv.org/abs/2510.07203v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07203v1",
        "arxiv_id": "2510.07203v1",
        "authors": [
            "Benjamin Akera",
            "Evelyn Nafula Ouma",
            "Gilbert Yiga",
            "Patrick Walukagga",
            "Phionah Natukunda",
            "Trevor Saaka",
            "Solomon Nsumba",
            "Lilian Teddy Nabukeera",
            "Joel Muhanguzi",
            "Imran Sekalala",
            "Nimpamya Janat Namara",
            "Engineer Bainomugisha",
            "Ernest Mwebaze",
            "John Quinn"
        ],
        "submitted": "2025-10-08 16:35:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Biasless Language Models Learn Unnaturally: How LLMs Fail to Distinguish the Possible from the Impossible",
        "abstract": "Are large language models (LLMs) sensitive to the distinction between humanly\npossible languages and humanly impossible languages? This question is taken by\nmany to bear on whether LLMs and humans share the same innate learning biases.\nPrevious work has attempted to answer it in the positive by comparing LLM\nlearning curves on existing language datasets and on \"impossible\" datasets\nderived from them via various perturbation functions. Using the same\nmethodology, we examine this claim on a wider set of languages and impossible\nperturbations. We find that in most cases, GPT-2 learns each language and its\nimpossible counterpart equally easily, in contrast to previous claims. We also\napply a more lenient condition by testing whether GPT-2 provides any kind of\nseparation between the whole set of natural languages and the whole set of\nimpossible languages. By considering cross-linguistic variance in various\nmetrics computed on the perplexity curves, we show that GPT-2 provides no\nsystematic separation between the possible and the impossible. Taken together,\nthese perspectives show that LLMs do not share the human innate biases that\nshape linguistic typology.",
        "url": "http://arxiv.org/abs/2510.07178v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07178v1",
        "arxiv_id": "2510.07178v1",
        "authors": [
            "Imry Ziv",
            "Nur Lan",
            "Emmanuel Chemla",
            "Roni Katzir"
        ],
        "submitted": "2025-10-08 16:17:13",
        "source": "arxiv",
        "comment": "15 pages, 4 figures"
    },
    {
        "title": "CARPAS: Towards Content-Aware Refinement of Provided Aspects for Summarization in Large Language Models",
        "abstract": "Aspect-based summarization has attracted significant attention for its\nability to generate more fine-grained and user-aligned summaries. While most\nexisting approaches assume a set of predefined aspects as input, real-world\nscenarios often present challenges where these given aspects may be incomplete,\nirrelevant, or entirely missing from the document. Users frequently expect\nsystems to adaptively refine or filter the provided aspects based on the actual\ncontent. In this paper, we initiate this novel task setting, termed\nContent-Aware Refinement of Provided Aspects for Summarization (CARPAS), with\nthe aim of dynamically adjusting the provided aspects based on the document\ncontext before summarizing. We construct three new datasets to facilitate our\npilot experiments, and by using LLMs with four representative prompting\nstrategies in this task, we find that LLMs tend to predict an overly\ncomprehensive set of aspects, which often results in excessively long and\nmisaligned summaries. Building on this observation, we propose a preliminary\nsubtask to predict the number of relevant aspects, and demonstrate that the\npredicted number can serve as effective guidance for the LLMs, reducing the\ninference difficulty, and enabling them to focus on the most pertinent aspects.\nOur extensive experiments show that the proposed approach significantly\nimproves performance across all datasets. Moreover, our deeper analyses uncover\nLLMs' compliance when the requested number of aspects differs from their own\nestimations, establishing a crucial insight for the deployment of LLMs in\nsimilar real-world applications.",
        "url": "http://arxiv.org/abs/2510.07177v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07177v1",
        "arxiv_id": "2510.07177v1",
        "authors": [
            "Yong-En Tian",
            "Yu-Chien Tang",
            "An-Zi Yen",
            "Wen-Chih Peng"
        ],
        "submitted": "2025-10-08 16:16:46",
        "source": "arxiv",
        "comment": "22 pages, 17 figures"
    },
    {
        "title": "Quantifying Data Contamination in Psychometric Evaluations of LLMs",
        "abstract": "Recent studies apply psychometric questionnaires to Large Language Models\n(LLMs) to assess high-level psychological constructs such as values,\npersonality, moral foundations, and dark traits. Although prior work has raised\nconcerns about possible data contamination from psychometric inventories, which\nmay threaten the reliability of such evaluations, there has been no systematic\nattempt to quantify the extent of this contamination. To address this gap, we\npropose a framework to systematically measure data contamination in\npsychometric evaluations of LLMs, evaluating three aspects: (1) item\nmemorization, (2) evaluation memorization, and (3) target score matching.\nApplying this framework to 21 models from major families and four widely used\npsychometric inventories, we provide evidence that popular inventories such as\nthe Big Five Inventory (BFI-44) and Portrait Values Questionnaire (PVQ-40)\nexhibit strong contamination, where models not only memorize items but can also\nadjust their responses to achieve specific target scores.",
        "url": "http://arxiv.org/abs/2510.07175v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07175v1",
        "arxiv_id": "2510.07175v1",
        "authors": [
            "Jongwook Han",
            "Woojung Song",
            "Jonggeun Lee",
            "Yohan Jo"
        ],
        "submitted": "2025-10-08 16:16:20",
        "source": "arxiv",
        "comment": "12 pages, 1 figure"
    },
    {
        "title": "NurseLLM: The First Specialized Language Model for Nursing",
        "abstract": "Recent advancements in large language models (LLMs) have significantly\ntransformed medical systems. However, their potential within specialized\ndomains such as nursing remains largely underexplored. In this work, we\nintroduce NurseLLM, the first nursing-specialized LLM tailored for multiple\nchoice question-answering (MCQ) tasks. We develop a multi-stage data generation\npipeline to build the first large scale nursing MCQ dataset to train LLMs on a\nbroad spectrum of nursing topics. We further introduce multiple nursing\nbenchmarks to enable rigorous evaluation. Our extensive experiments demonstrate\nthat NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of\ncomparable size on different benchmarks, underscoring the importance of a\nspecialized LLM for the nursing domain. Finally, we explore the role of\nreasoning and multi-agent collaboration systems in nursing, highlighting their\npromise for future research and applications.",
        "url": "http://arxiv.org/abs/2510.07173v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07173v1",
        "arxiv_id": "2510.07173v1",
        "authors": [
            "Md Tawkat Islam Khondaker",
            "Julia Harrington",
            "Shady Shehata"
        ],
        "submitted": "2025-10-08 16:15:06",
        "source": "arxiv",
        "comment": "EMNLP 2025 Industry Track"
    },
    {
        "title": "More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning",
        "abstract": "The reasoning capabilities of Large Language Models (LLMs) play a critical\nrole in many downstream tasks, yet depend strongly on the quality of training\ndata. Despite various proposed data construction methods, their practical\nutility in real-world pipelines remains underexplored. In this work, we conduct\na comprehensive analysis of open-source datasets and data synthesis techniques\nfor mathematical reasoning, evaluating them under a unified pipeline designed\nto mirror training and deployment scenarios. We further distill effective data\nselection strategies and identify practical methods suitable for industrial\napplications. Our findings highlight that structuring data in more\ninterpretable formats, or distilling from stronger models often outweighs\nsimply scaling up data volume. This study provides actionable guidance for\nintegrating training data to enhance LLM capabilities, supporting both\ncost-effective data curation and scalable model enhancement. We hope this work\nwill inspire further research on how to balance \"more data\" versus \"better\ndata\" for real-world reasoning tasks.",
        "url": "http://arxiv.org/abs/2510.07169v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07169v1",
        "arxiv_id": "2510.07169v1",
        "authors": [
            "Yike Zhao",
            "Simin Guo",
            "Ziqing Yang",
            "Shifan Han",
            "Dahua Lin",
            "Fei Tan"
        ],
        "submitted": "2025-10-08 16:07:26",
        "source": "arxiv",
        "comment": "12 pages, 3 figures, submitted to EMNLP 2025 Industry Track"
    },
    {
        "title": "Inconsistent Affective Reaction: Sentiment of Perception and Opinion in Urban Environments",
        "abstract": "The ascension of social media platforms has transformed our understanding of\nurban environments, giving rise to nuanced variations in sentiment reaction\nembedded within human perception and opinion, and challenging existing\nmultidimensional sentiment analysis approaches in urban studies. This study\npresents novel methodologies for identifying and elucidating sentiment\ninconsistency, constructing a dataset encompassing 140,750 Baidu and Tencent\nStreet view images to measure perceptions, and 984,024 Weibo social media text\nposts to measure opinions. A reaction index is developed, integrating object\ndetection and natural language processing techniques to classify sentiment in\nBeijing Second Ring for 2016 and 2022. Classified sentiment reaction is\nanalysed and visualized using regression analysis, image segmentation, and word\nfrequency based on land-use distribution to discern underlying factors. The\nperception affective reaction trend map reveals a shift toward more evenly\ndistributed positive sentiment, while the opinion affective reaction trend map\nshows more extreme changes. Our mismatch map indicates significant disparities\nbetween the sentiments of human perception and opinion of urban areas over the\nyears. Changes in sentiment reactions have significant relationships with\nelements such as dense buildings and pedestrian presence. Our inconsistent maps\npresent perception and opinion sentiments before and after the pandemic and\noffer potential explanations and directions for environmental management, in\nformulating strategies for urban renewal.",
        "url": "http://arxiv.org/abs/2510.07359v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07359v1",
        "arxiv_id": "2510.07359v1",
        "authors": [
            "Jingfei Huang",
            "Han Tu"
        ],
        "submitted": "2025-10-08 16:07:23",
        "source": "arxiv",
        "comment": "10 pages"
    },
    {
        "title": "Reasoning for Hierarchical Text Classification: The Case of Patents",
        "abstract": "Hierarchical text classification (HTC) assigns documents to multiple levels\nof a pre-defined taxonomy. Automated patent subject classification represents\none of the hardest HTC scenarios because of domain knowledge difficulty and a\nhuge number of labels. Prior approaches only output a flat label set, which\noffers little insight into the reason behind predictions. Therefore, we propose\nReasoning for Hierarchical Classification (RHC), a novel framework that\nreformulates HTC as a step-by-step reasoning task to sequentially deduce\nhierarchical labels. RHC trains large language models (LLMs) in two stages: a\ncold-start stage that aligns outputs with chain-of-thought (CoT) reasoning\nformat and a reinforcement learning (RL) stage to enhance multi-step reasoning\nability. RHC demonstrates four advantages in our experiments. (1)\nEffectiveness: RHC surpasses previous baselines and outperforms the supervised\nfine-tuning counterparts by approximately 3% in accuracy and macro F1. (2)\nExplainability: RHC produces natural-language justifications before prediction\nto facilitate human inspection. (3) Scalability: RHC scales favorably with\nmodel size with larger gains compared to standard fine-tuning. (4)\nApplicability: Beyond patents, we further demonstrate that RHC achieves\nstate-of-the-art performance on other widely used HTC benchmarks, which\nhighlights its broad applicability.",
        "url": "http://arxiv.org/abs/2510.07167v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07167v1",
        "arxiv_id": "2510.07167v1",
        "authors": [
            "Lekang Jiang",
            "Wenjun Sun",
            "Stephan Goetz"
        ],
        "submitted": "2025-10-08 16:06:04",
        "source": "arxiv",
        "comment": "15 pages, 10 tables, 3 figures"
    },
    {
        "title": "Encode, Think, Decode: Scaling test-time reasoning with recursive latent thoughts",
        "abstract": "Most efforts to improve the reasoning capabilities of large language models\n(LLMs) involve either scaling the number of parameters and the size of training\ndata, or scaling inference computation by letting models generate complex\nchains of thought. Motivated by interpretability studies showing that the\ncrucial computation required for reasoning tasks is concentrated in a limited\nrange of layers, we introduce Encode-Think-Decode (ETD), a method that enhances\nthe reasoning capabilities of a base model by training it to iterate over a\nsmall subset of reasoning-relevant layers during the mid-training stage. ETD\namplifies latent reasoning while preserving the original architecture,\nparameter count, hyperparameters, and training data composition. When iterating\non the selected layers at inference time, ETD models yield substantial gains on\n17 reasoning benchmarks, including +28.4% relative accuracy improvement on\nGSM8K and +36% on MATH with the OLMo-2 1B Base model. We also explore an\nadaptive depth strategy that adjusts the computation per input token. Our\nresults show that recursive latent reasoning offers a simple and effective path\nto stronger LLM reasoning.",
        "url": "http://arxiv.org/abs/2510.07358v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07358v1",
        "arxiv_id": "2510.07358v1",
        "authors": [
            "Yeskendir Koishekenov",
            "Aldo Lipani",
            "Nicola Cancedda"
        ],
        "submitted": "2025-10-08 15:58:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Multi-Agent Framework for Stateful Inference-Time Search",
        "abstract": "Recent work explores agentic inference-time techniques to perform structured,\nmulti-step reasoning. However, stateless inference often struggles on\nmulti-step tasks due to the absence of persistent state. Moreover,\ntask-specific fine-tuning or instruction-tuning often achieve surface-level\ncode generation but remain brittle on tasks requiring deeper reasoning and\nlong-horizon dependencies. To address these limitations, we propose stateful\nmulti-agent evolutionary search, a training-free framework that departs from\nprior stateless approaches by combining (i) persistent inference-time state,\n(ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate\nits effectiveness in automated unit test generation through the generation of\nedge cases. We generate robust edge cases using an evolutionary search process,\nwhere specialized agents sequentially propose, mutate, and score candidates. A\ncontroller maintains persistent state across generations, while evolutionary\npreservation ensures diversity and exploration across all possible cases. This\nyields a generalist agent capable of discovering robust, high-coverage edge\ncases across unseen codebases. Experiments show our stateful multi-agent\ninference framework achieves substantial gains in coverage over stateless\nsingle-step baselines, evaluated on prevalent unit-testing benchmarks such as\nHumanEval and TestGenEvalMini and using three diverse LLM families - Llama,\nGemma, and GPT. These results indicate that combining persistent inference-time\nstate with evolutionary search materially improves unit-test generation.",
        "url": "http://arxiv.org/abs/2510.07147v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07147v1",
        "arxiv_id": "2510.07147v1",
        "authors": [
            "Arshika Lalan",
            "Rajat Ghosh",
            "Aditya Kolsur",
            "Debojyoti Dutta"
        ],
        "submitted": "2025-10-08 15:48:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Comparing human and language models sentence processing difficulties on complex structures",
        "abstract": "Large language models (LLMs) that fluently converse with humans are a reality\n- but do LLMs experience human-like processing difficulties? We systematically\ncompare human and LLM sentence comprehension across seven challenging\nlinguistic structures. We collect sentence comprehension data from humans and\nfive families of state-of-the-art LLMs, varying in size and training procedure\nin a unified experimental framework. Our results show LLMs overall struggle on\nthe target structures, but especially on garden path (GP) sentences. Indeed,\nwhile the strongest models achieve near perfect accuracy on non-GP structures\n(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).\nAdditionally, when ranking structures based on average performance, rank\ncorrelation between humans and models increases with parameter count. For each\ntarget structure, we also collect data for their matched baseline without the\ndifficult structure. Comparing performance on the target vs. baseline\nsentences, the performance gap observed in humans holds for LLMs, with two\nexceptions: for models that are too weak performance is uniformly low across\nboth sentence types, and for models that are too strong the performance is\nuniformly high. Together, these reveal convergence and divergence in human and\nLLM sentence comprehension, offering new insights into the similarity of humans\nand LLMs.",
        "url": "http://arxiv.org/abs/2510.07141v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07141v1",
        "arxiv_id": "2510.07141v1",
        "authors": [
            "Samuel Joseph Amouyal",
            "Aya Meltzer-Asscher",
            "Jonathan Berant"
        ],
        "submitted": "2025-10-08 15:42:49",
        "source": "arxiv",
        "comment": "Data and code will be released soon"
    },
    {
        "title": "ConCuR: Conciseness Makes State-of-the-Art Kernel Generation",
        "abstract": "GPU kernel generation by LLMs has recently experienced rapid development,\nleveraging test-time scaling and reinforcement learning techniques. However, a\nkey challenge for kernel generation is the scarcity of high-quality data, as\nmost high-quality kernels are proprietary and not open-source. This challenge\nprevents us from leveraging supervised fine-tuning to align LLMs to the kernel\ngeneration task. To address this challenge, we develop a pipeline that\ngenerates and curates high-quality CUDA kernels with reasoning traces,\nmotivated by a critical observation that concise yet informative reasoning\ntraces result in robust generation of high-performance kernels. Using this\npipeline, we construct our dataset ConCuR and introduce our model KernelCoder,\nwhich is the first model trained on a curated dataset consisting of PyTorch,\nreasoning, and CUDA kernel pairs, to our knowledge. In the KernelBench setup,\nour model achieves significant improvements over the existing top-performing\nmodel, QwQ-32B, and outperforms all open-source models fine-tuned for kernel\ngeneration, as well as frontier models such as DeepSeek-V3.1-Think and\nClaude-4-sonnet. Finally, we show that the average reasoning length can serve\nas a metric to assess the difficulty of kernel generation tasks. The\nobservations, metrics, and our data collection and curation pipeline can help\nobtain better data in the kernel generation task in the future.",
        "url": "http://arxiv.org/abs/2510.07356v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07356v1",
        "arxiv_id": "2510.07356v1",
        "authors": [
            "Lingcheng Kong",
            "Jiateng Wei",
            "Hanzhang Shen",
            "Huan Wang"
        ],
        "submitted": "2025-10-08 15:41:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning",
        "abstract": "Instruction tuning is essential for aligning large language models (LLMs) to\ndownstream tasks and commonly relies on large, diverse corpora. However, small,\nhigh-quality subsets, known as coresets, can deliver comparable or superior\nresults, though curating them remains challenging. Existing methods often rely\non coarse, sample-level signals like gradients, an approach that is\ncomputationally expensive and overlooks fine-grained features. To address this,\nwe introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a\nforward-only, token-centric framework. Instead of using gradients, TRIM\noperates by matching underlying representational patterns identified via\nattention-based \"fingerprints\" from a handful of target samples. Such an\napproach makes TRIM highly efficient and uniquely sensitive to the structural\nfeatures that define a task. Coresets selected by our method consistently\noutperform state-of-the-art baselines by up to 9% on downstream tasks and even\nsurpass the performance of full-data fine-tuning in some settings. By avoiding\nexpensive backward passes, TRIM achieves this at a fraction of the\ncomputational cost. These findings establish TRIM as a scalable and efficient\nalternative for building high-quality instruction-tuning datasets.",
        "url": "http://arxiv.org/abs/2510.07118v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07118v1",
        "arxiv_id": "2510.07118v1",
        "authors": [
            "Manish Nagaraj",
            "Sakshi Choudhary",
            "Utkarsh Saxena",
            "Deepak Ravikumar",
            "Kaushik Roy"
        ],
        "submitted": "2025-10-08 15:11:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning",
        "abstract": "Many natural language processing (NLP) tasks involve subjectivity, ambiguity,\nor legitimate disagreement between annotators. In this paper, we outline our\nsystem for modeling human variation. Our system leverages language models'\n(LLMs) in-context learning abilities, along with a two-step meta-learning\ntraining procedure for 1) post-training on many datasets requiring in-context\nlearning and 2) specializing the model via in-context meta-learning to the\nparticular data distribution of interest. We also evaluate the performance of\nour system submission to the Learning With Disagreements (LeWiDi) competition,\nwhere it was the overall winner on both tasks. Additionally, we perform an\nablation study to measure the importance of each system component. We find that\nincluding rater examples in-context is crucial for our system's performance,\ndataset-specific fine-tuning is helpful on the larger datasets, post-training\non other in-context datasets is helpful on one of the competition datasets, and\nthat performance improves with model scale.",
        "url": "http://arxiv.org/abs/2510.07105v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07105v1",
        "arxiv_id": "2510.07105v1",
        "authors": [
            "Taylor Sorensen",
            "Yejin Choi"
        ],
        "submitted": "2025-10-08 14:59:24",
        "source": "arxiv",
        "comment": "NLPerspectives: The 4th Workshop on Perspectivist Approaches to\n  Natural Language Processing at EMNLP 2025"
    },
    {
        "title": "TALENT: Table VQA via Augmented Language-Enhanced Natural-text Transcription",
        "abstract": "Table Visual Question Answering (Table VQA) is typically addressed by large\nvision-language models (VLMs). While such models can answer directly from\nimages, they often miss fine-grained details unless scaled to very large sizes,\nwhich are computationally prohibitive, especially for mobile deployment. A\nlighter alternative is to have a small VLM perform OCR and then use a large\nlanguage model (LLM) to reason over structured outputs such as Markdown tables.\nHowever, these representations are not naturally optimized for LLMs and still\nintroduce substantial errors. We propose TALENT (Table VQA via Augmented\nLanguage-Enhanced Natural-text Transcription), a lightweight framework that\nleverages dual representations of tables. TALENT prompts a small VLM to produce\nboth OCR text and natural language narration, then combines them with the\nquestion for reasoning by an LLM. This reframes Table VQA as an LLM-centric\nmultimodal reasoning task, where the VLM serves as a perception-narration\nmodule rather than a monolithic solver. Additionally, we construct ReTabVQA, a\nmore challenging Table VQA dataset requiring multi-step quantitative reasoning\nover table images. Experiments show that TALENT enables a small VLM-LLM\ncombination to match or surpass a single large VLM at significantly lower\ncomputational cost on both public datasets and ReTabVQA.",
        "url": "http://arxiv.org/abs/2510.07098v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07098v1",
        "arxiv_id": "2510.07098v1",
        "authors": [
            "Guo Yutong",
            "Wanying Wang",
            "Yue Wu",
            "Zichen Miao",
            "Haoyu Wang"
        ],
        "submitted": "2025-10-08 14:56:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided Sarcastic Speech Synthesis",
        "abstract": "Sarcasm is a subtle form of non-literal language that poses significant\nchallenges for speech synthesis due to its reliance on nuanced semantic,\ncontextual, and prosodic cues. While existing speech synthesis research has\nfocused primarily on broad emotional categories, sarcasm remains largely\nunexplored. In this paper, we propose a Large Language Model (LLM)-enhanced\nRetrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach\ncombines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture\npragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic\nexemplars retrieved via a Retrieval Augmented Generation (RAG) module, which\nprovide expressive reference patterns of sarcastic delivery. Integrated within\na VITS backbone, this dual conditioning enables more natural and contextually\nappropriate sarcastic speech. Experiments demonstrate that our method\noutperforms baselines in both objective measures and subjective evaluations,\nyielding improvements in speech naturalness, sarcastic expressivity, and\ndownstream sarcasm detection.",
        "url": "http://arxiv.org/abs/2510.07096v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07096v1",
        "arxiv_id": "2510.07096v1",
        "authors": [
            "Zhu Li",
            "Yuqing Zhang",
            "Xiyuan Gao",
            "Shekhar Nayak",
            "Matt Coler"
        ],
        "submitted": "2025-10-08 14:53:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Cognitive Bandwidth Bottleneck: Shifting Long-Horizon Agent from Planning with Actions to Planning with Schemas",
        "abstract": "Enabling LLMs to effectively operate long-horizon task which requires\nlong-term planning and multiple interactions is essential for open-world\nautonomy. Conventional methods adopt planning with actions where a executable\naction list would be provided as reference. However, this action representation\nchoice would be impractical when the environment action space is combinatorial\nexploded (e.g., open-ended real world). This naturally leads to a question: As\nenvironmental action space scales, what is the optimal action representation\nfor long-horizon agents? In this paper, we systematically study the\neffectiveness of two different action representations. The first one is\nconventional planning with actions (PwA) which is predominantly adopted for its\neffectiveness on existing benchmarks. The other one is planning with schemas\n(PwS) which instantiate an action schema into action lists (e.g., \"move [OBJ]\nto [OBJ]\" -> \"move apple to desk\") to ensure concise action space and reliable\nscalability. This alternative is motivated by its alignment with human\ncognition and its compliance with environment-imposed action format\nrestriction. We propose cognitive bandwidth perspective as a conceptual\nframework to qualitatively understand the differences between these two action\nrepresentations and empirically observe a representation-choice inflection\npoint between ALFWorld (~35 actions) and SciWorld (~500 actions), which serve\nas evidence of the need for scalable representations. We further conduct\ncontrolled experiments to study how the location of this inflection point\ninteracts with different model capacities: stronger planning proficiency shifts\nthe inflection rightward, whereas better schema instantiation shifts it\nleftward. Finally, noting the suboptimal performance of PwS agents, we provide\nan actionable guide for building more capable PwS agents for better scalable\nautonomy.",
        "url": "http://arxiv.org/abs/2510.07091v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07091v1",
        "arxiv_id": "2510.07091v1",
        "authors": [
            "Baixuan Xu",
            "Tianshi Zheng",
            "Zhaowei Wang",
            "Hong Ting Tsang",
            "Weiqi Wang",
            "Tianqing Fang",
            "Yangqiu Song"
        ],
        "submitted": "2025-10-08 14:47:40",
        "source": "arxiv",
        "comment": "22 pages"
    },
    {
        "title": "All Claims Are Equal, but Some Claims Are More Equal Than Others: Importance-Sensitive Factuality Evaluation of LLM Generations",
        "abstract": "Existing methods for evaluating the factuality of large language model (LLM)\nresponses treat all claims as equally important. This results in misleading\nevaluations when vital information is missing or incorrect as it receives the\nsame weight as peripheral details, raising the question: how can we reliably\ndetect such differences when there are errors in key information? Current\napproaches that measure factuality tend to be insensitive to omitted or false\nkey information. To investigate this lack of sensitivity, we construct\nVITALERRORS, a benchmark of 6,733 queries with minimally altered LLM responses\ndesigned to omit or falsify key information. Using this dataset, we demonstrate\nthe insensitivities of existing evaluation metrics to key information errors.\nTo address this gap, we introduce VITAL, a set of metrics that provide greater\nsensitivity in measuring the factuality of responses by incorporating the\nrelevance and importance of claims with respect to the query. Our analysis\ndemonstrates that VITAL metrics more reliably detect errors in key information\nthan previous methods. Our dataset, metrics, and analysis provide a foundation\nfor more accurate and robust assessment of LLM factuality.",
        "url": "http://arxiv.org/abs/2510.07083v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07083v1",
        "arxiv_id": "2510.07083v1",
        "authors": [
            "Miriam Wanner",
            "Leif Azzopardi",
            "Paul Thomas",
            "Soham Dan",
            "Benjamin Van Durme",
            "Nick Craswell"
        ],
        "submitted": "2025-10-08 14:40:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Accelerating Diffusion LLM Inference via Local Determinism Propagation",
        "abstract": "Diffusion large language models (dLLMs) represent a significant advancement\nin text generation, offering parallel token decoding capabilities. However,\nexisting open-source implementations suffer from quality-speed trade-offs that\nimpede their practical deployment. Conservative sampling strategies typically\ndecode only the most confident token per step to ensure quality (i.e., greedy\ndecoding), at the cost of inference efficiency due to repeated redundant\nrefinement iterations--a phenomenon we term delayed decoding. Through\nsystematic analysis of dLLM decoding dynamics, we characterize this delayed\ndecoding behavior and propose a training-free adaptive parallel decoding\nstrategy, named LocalLeap, to address these inefficiencies. LocalLeap is built\non two fundamental empirical principles: local determinism propagation centered\non high-confidence anchors and progressive spatial consistency decay. By\napplying these principles, LocalLeap identifies anchors and performs localized\nrelaxed parallel decoding within bounded neighborhoods, achieving substantial\ninference step reduction through early commitment of already-determined tokens\nwithout compromising output quality. Comprehensive evaluation on various\nbenchmarks demonstrates that LocalLeap achieves 6.94$\\times$ throughput\nimprovements and reduces decoding steps to just 14.2\\% of the original\nrequirement, achieving these gains with negligible performance impact. The\nsource codes are available at: https://github.com/friedrichor/LocalLeap.",
        "url": "http://arxiv.org/abs/2510.07081v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07081v1",
        "arxiv_id": "2510.07081v1",
        "authors": [
            "Fanheng Kong",
            "Jingyuan Zhang",
            "Yahui Liu",
            "Zirui Wu",
            "Yu Tian",
            "Victoria W.",
            "Guorui Zhou"
        ],
        "submitted": "2025-10-08 14:39:34",
        "source": "arxiv",
        "comment": "21 pages, 4 figures. Under review"
    },
    {
        "title": "LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish",
        "abstract": "Instruction tuning has become a key technique for enhancing the performance\nof large language models, enabling them to better follow human prompts.\nHowever, low-resource languages such as Luxembourgish face severe limitations\ndue to the lack of high-quality instruction datasets. Traditional reliance on\nmachine translation often introduces semantic misalignment and cultural\ninaccuracies. In this work, we address these challenges by creating a\ncross-lingual instruction tuning dataset for Luxembourgish, without resorting\nto machine-generated translations into it. Instead, by leveraging aligned data\nfrom English, French, and German, we build a high-quality dataset that\npreserves linguistic and cultural nuances. We provide evidence that\ncross-lingual instruction tuning not only improves representational alignment\nacross languages but also the model's generative capabilities in Luxembourgish.\nThis highlights how cross-lingual data curation can avoid the common pitfalls\nof machine-translated data and directly benefit low-resource language\ndevelopment.",
        "url": "http://arxiv.org/abs/2510.07074v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07074v1",
        "arxiv_id": "2510.07074v1",
        "authors": [
            "Fred Philippy",
            "Laura Bernardy",
            "Siwen Guo",
            "Jacques Klein",
            "Tegawendé F. Bissyandé"
        ],
        "submitted": "2025-10-08 14:35:59",
        "source": "arxiv",
        "comment": "Paper under review; Dataset available at\n  https://huggingface.co/datasets/fredxlpy/LuxInstruct"
    },
    {
        "title": "Revisiting Metric Reliability for Fine-grained Evaluation of Machine Translation and Summarization in Indian Languages",
        "abstract": "While automatic metrics drive progress in Machine Translation (MT) and Text\nSummarization (TS), existing metrics have been developed and validated almost\nexclusively for English and other high-resource languages. This narrow focus\nleaves Indian languages, spoken by over 1.5 billion people, largely overlooked,\ncasting doubt on the universality of current evaluation practices. To address\nthis gap, we introduce ITEM, a large-scale benchmark that systematically\nevaluates the alignment of 26 automatic metrics with human judgments across six\nmajor Indian languages, enriched with fine-grained annotations. Our extensive\nevaluation, covering agreement with human judgments, sensitivity to outliers,\nlanguage-specific reliability, inter-metric correlations, and resilience to\ncontrolled perturbations, reveals four central findings: (1) LLM-based\nevaluators show the strongest alignment with human judgments at both segment\nand system levels; (2) outliers exert a significant impact on metric-human\nagreement; (3) in TS, metrics are more effective at capturing content fidelity,\nwhereas in MT, they better reflect fluency; and (4) metrics differ in their\nrobustness and sensitivity when subjected to diverse perturbations.\nCollectively, these findings offer critical guidance for advancing metric\ndesign and evaluation in Indian languages.",
        "url": "http://arxiv.org/abs/2510.07061v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07061v1",
        "arxiv_id": "2510.07061v1",
        "authors": [
            "Amir Hossein Yari",
            "Kalmit Kulkarni",
            "Ahmad Raza Khan",
            "Fajri Koto"
        ],
        "submitted": "2025-10-08 14:27:02",
        "source": "arxiv",
        "comment": "18 pages, 14 figures"
    },
    {
        "title": "Does Local News Stay Local?: Online Content Shifts in Sinclair-Acquired Stations",
        "abstract": "Local news stations are often considered to be reliable sources of\nnon-politicized information, particularly local concerns that residents care\nabout. Because these stations are trusted news sources, viewers are\nparticularly susceptible to the information they report. The Sinclair Broadcast\ngroup is a broadcasting company that has acquired many local news stations in\nthe last decade. We investigate the effects of local news stations being\nacquired by Sinclair: how does coverage change? We use computational methods to\ninvestigate changes in internet content put out by local news stations before\nand after being acquired by Sinclair and in comparison to national news\noutlets. We find that there is clear evidence that local news stations report\nmore frequently on national news at the expense of local topics, and that their\ncoverage of polarizing national topics increases.",
        "url": "http://arxiv.org/abs/2510.07060v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07060v1",
        "arxiv_id": "2510.07060v1",
        "authors": [
            "Miriam Wanner",
            "Sophia Hager",
            "Anjalie Field"
        ],
        "submitted": "2025-10-08 14:27:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models",
        "abstract": "Despite their remarkable natural language understanding capabilities, Large\nLanguage Models (LLMs) have been underutilized for retrieval tasks. We present\nSearch-R3, a novel framework that addresses this limitation by adapting LLMs to\ngenerate search embeddings as a direct output of their reasoning process. Our\napproach exploits LLMs' chain-of-thought capabilities, allowing them to produce\nmore effective embeddings by reasoning step-by-step through complex semantic\nanalyses. We implement this through three complementary mechanisms. (1) a\nsupervised learning stage enables the model's ability to produce quality\nembeddings, (2) a reinforcement learning (RL) methodology that optimizes\nembedding generation alongside reasoning, and (3) a specialized RL environment\nthat efficiently handles evolving embedding representations without requiring\ncomplete corpus re-encoding at each training iteration. Our extensive\nevaluations on diverse benchmarks demonstrate that Search-R3 significantly\noutperforms prior methods by unifying the reasoning and embedding generation\nprocesses. This integrated post-training approach represents a substantial\nadvancement in handling complex knowledge-intensive tasks that require both\nsophisticated reasoning and effective information retrieval. Project page:\nhttps://github.com/ytgui/Search-R3",
        "url": "http://arxiv.org/abs/2510.07048v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07048v1",
        "arxiv_id": "2510.07048v1",
        "authors": [
            "Yuntao Gui",
            "James Cheng"
        ],
        "submitted": "2025-10-08 14:16:20",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models",
        "abstract": "Code-switching (CSW), the alternation of languages and scripts within a\nsingle utterance, remains a fundamental challenge for multiling ual NLP, even\namidst the rapid advances of large language models (LLMs). Most LLMs still\nstruggle with mixed-language inputs, limited CSW datasets, and evaluation\nbiases, hindering deployment in multilingual societies. This survey provides\nthe first comprehensive analysis of CSW-aware LLM research, reviewing 308\nstudies spanning five research areas, 12 NLP tasks, 30+ datasets, and 80+\nlanguages. We classify recent advances by architecture, training strategy, and\nevaluation methodology, outlining how LLMs have reshaped CSW modeling and what\nchallenges persist. The paper concludes with a roadmap emphasizing the need for\ninclusive datasets, fair evaluation, and linguistically grounded models to\nachieve truly multilingual intelligence. A curated collection of all resources\nis maintained at https://github.com/lingo-iitgn/awesome-code-mixing/.",
        "url": "http://arxiv.org/abs/2510.07037v2",
        "pdf_url": "http://arxiv.org/pdf/2510.07037v2",
        "arxiv_id": "2510.07037v2",
        "authors": [
            "Rajvee Sheth",
            "Samridhi Raj Sinha",
            "Mahavir Patil",
            "Himanshu Beniwal",
            "Mayank Singh"
        ],
        "submitted": "2025-10-08 14:04:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge",
        "abstract": "LLMs are remarkable artifacts that have revolutionized a range of NLP and AI\ntasks. A significant contributor is their factual knowledge, which, to date,\nremains poorly understood, and is usually analyzed from biased samples. In this\npaper, we take a deep tour into the factual knowledge (or beliefs) of a\nfrontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited\nset of 100 million beliefs of one of the strongest currently available frontier\nLLMs, GPT-4.1. We find that the models' factual knowledge differs quite\nsignificantly from established knowledge bases, and that its accuracy is\nsignificantly lower than indicated by previous benchmarks. We also find that\ninconsistency, ambiguity and hallucinations are major issues, shedding light on\nfuture research opportunities concerning factual LLM knowledge.",
        "url": "http://arxiv.org/abs/2510.07024v2",
        "pdf_url": "http://arxiv.org/pdf/2510.07024v2",
        "arxiv_id": "2510.07024v2",
        "authors": [
            "Shrestha Ghosh",
            "Luca Giordano",
            "Yujia Hu",
            "Tuan-Phong Nguyen",
            "Simon Razniewski"
        ],
        "submitted": "2025-10-08 13:48:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Native Hybrid Attention for Efficient Sequence Modeling",
        "abstract": "Transformers excel at sequence modeling but face quadratic complexity, while\nlinear attention offers improved efficiency but often compromises recall\naccuracy over long contexts. In this work, we introduce Native Hybrid Attention\n(NHA), a novel hybrid architecture of linear and full attention that integrates\nboth intra \\& inter-layer hybridization into a unified layer design. NHA\nmaintains long-term context in key-value slots updated by a linear RNN, and\naugments them with short-term tokens from a sliding window. A single\n\\texttt{softmax attention} operation is then applied over all keys and values,\nenabling per-token and per-head context-dependent weighting without requiring\nadditional fusion parameters. The inter-layer behavior is controlled through a\nsingle hyperparameter, the sliding window size, which allows smooth adjustment\nbetween purely linear and full attention while keeping all layers structurally\nuniform. Experimental results show that NHA surpasses Transformers and other\nhybrid baselines on recall-intensive and commonsense reasoning tasks.\nFurthermore, pretrained LLMs can be structurally hybridized with NHA, achieving\ncompetitive accuracy while delivering significant efficiency gains. Code is\navailable at https://github.com/JusenD/NHA.",
        "url": "http://arxiv.org/abs/2510.07019v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07019v1",
        "arxiv_id": "2510.07019v1",
        "authors": [
            "Jusen Du",
            "Jiaxi Hu",
            "Tao Zhang",
            "Weigao Sun",
            "Yu Cheng"
        ],
        "submitted": "2025-10-08 13:44:57",
        "source": "arxiv",
        "comment": "Technical report, 16 pages"
    },
    {
        "title": "Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages",
        "abstract": "The effectiveness of Large Language Models (LLMs) depends heavily on the\navailability of high-quality post-training data, particularly\ninstruction-tuning and preference-based examples. Existing open-source\ndatasets, however, often lack multilingual coverage, cultural grounding, and\nsuffer from task diversity gaps that are especially pronounced for Indian\nlanguages. We introduce a human-in-the-loop pipeline that combines translations\nwith synthetic expansion to produce reliable and diverse Indic post-training\ndata. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and\nPragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56\nsub-categories, leveraging 57 diverse datasets. Our dataset protocol\nincorporates several often-overlooked dimensions and emphasize task diversity,\nmulti-turn dialogue, instruction fidelity, safety alignment, and preservation\nof cultural nuance, providing a foundation for more inclusive and effective\nmultilingual LLMs.",
        "url": "http://arxiv.org/abs/2510.07000v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07000v1",
        "arxiv_id": "2510.07000v1",
        "authors": [
            "Neel Prabhanjan Rachamalla",
            "Aravind Konakalla",
            "Gautam Rajeev",
            "Ashish Kulkarni",
            "Chandra Khatri",
            "Shubham Agarwal"
        ],
        "submitted": "2025-10-08 13:23:45",
        "source": "arxiv",
        "comment": "EMNLP 2025"
    },
    {
        "title": "Towards Reliable Retrieval in RAG Systems for Large Legal Datasets",
        "abstract": "Retrieval-Augmented Generation (RAG) is a promising approach to mitigate\nhallucinations in Large Language Models (LLMs) for legal applications, but its\nreliability is critically dependent on the accuracy of the retrieval step. This\nis particularly challenging in the legal domain, where large databases of\nstructurally similar documents often cause retrieval systems to fail. In this\npaper, we address this challenge by first identifying and quantifying a\ncritical failure mode we term Document-Level Retrieval Mismatch (DRM), where\nthe retriever selects information from entirely incorrect source documents. To\nmitigate DRM, we investigate a simple and computationally efficient technique\nwhich we refer to as Summary-Augmented Chunking (SAC). This method enhances\neach text chunk with a document-level synthetic summary, thereby injecting\ncrucial global context that would otherwise be lost during a standard chunking\nprocess. Our experiments on a diverse set of legal information retrieval tasks\nshow that SAC greatly reduces DRM and, consequently, also improves text-level\nretrieval precision and recall. Interestingly, we find that a generic\nsummarization strategy outperforms an approach that incorporates legal expert\ndomain knowledge to target specific legal elements. Our work provides evidence\nthat this practical, scalable, and easily integrable technique enhances the\nreliability of RAG systems when applied to large-scale legal document datasets.",
        "url": "http://arxiv.org/abs/2510.06999v1",
        "pdf_url": "http://arxiv.org/pdf/2510.06999v1",
        "arxiv_id": "2510.06999v1",
        "authors": [
            "Markus Reuter",
            "Tobias Lingenberg",
            "Rūta Liepiņa",
            "Francesca Lagioia",
            "Marco Lippi",
            "Giovanni Sartor",
            "Andrea Passerini",
            "Burcu Sayin"
        ],
        "submitted": "2025-10-08 13:22:20",
        "source": "arxiv",
        "comment": "Accepted for the 7th Natural Legal Language Processing Workshop (NLLP\n  2025), co-located with EMNLP 2025"
    },
    {
        "title": "RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning",
        "abstract": "This paper presents the vision, scientific contributions, and technical\ndetails of RedTWIZ: an adaptive and diverse multi-turn red teaming framework,\nto audit the robustness of Large Language Models (LLMs) in AI-assisted software\ndevelopment. Our work is driven by three major research streams: (1) robust and\nsystematic assessment of LLM conversational jailbreaks; (2) a diverse\ngenerative multi-turn attack suite, supporting compositional, realistic and\ngoal-oriented jailbreak conversational strategies; and (3) a hierarchical\nattack planner, which adaptively plans, serializes, and triggers attacks\ntailored to specific LLM's vulnerabilities. Together, these contributions form\na unified framework -- combining assessment, attack generation, and strategic\nplanning -- to comprehensively evaluate and expose weaknesses in LLMs'\nrobustness. Extensive evaluation is conducted to systematically assess and\nanalyze the performance of the overall system and each component. Experimental\nresults demonstrate that our multi-turn adversarial attack strategies can\nsuccessfully lead state-of-the-art LLMs to produce unsafe generations,\nhighlighting the pressing need for more research into enhancing LLM's\nrobustness.",
        "url": "http://arxiv.org/abs/2510.06994v1",
        "pdf_url": "http://arxiv.org/pdf/2510.06994v1",
        "arxiv_id": "2510.06994v1",
        "authors": [
            "Artur Horal",
            "Daniel Pina",
            "Henrique Paz",
            "Iago Paulo",
            "João Soares",
            "Rafael Ferreira",
            "Diogo Tavares",
            "Diogo Glória-Silva",
            "João Magalhães",
            "David Semedo"
        ],
        "submitted": "2025-10-08 13:18:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Spiral Model Technique For Data Science & Machine Learning Lifecycle",
        "abstract": "Analytics play an important role in modern business. Companies adapt data\nscience lifecycles to their culture to seek productivity and improve their\ncompetitiveness among others. Data science lifecycles are fairly an important\ncontributing factor to start and end a project that are data dependent. Data\nscience and Machine learning life cycles comprises of series of steps that are\ninvolved in a project. A typical life cycle states that it is a linear or\ncyclical model that revolves around. It is mostly depicted that it is possible\nin a traditional data science life cycle to start the process again after\nreaching the end of cycle. This paper suggests a new technique to incorporate\ndata science life cycle to business problems that have a clear end goal. A new\ntechnique called spiral technique is introduced to emphasize versatility,\nagility and iterative approach to business processes.",
        "url": "http://arxiv.org/abs/2510.06987v1",
        "pdf_url": "http://arxiv.org/pdf/2510.06987v1",
        "arxiv_id": "2510.06987v1",
        "authors": [
            "Rohith Mahadevan"
        ],
        "submitted": "2025-10-08 13:11:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "VelLMes: A high-interaction AI-based deception framework",
        "abstract": "There are very few SotA deception systems based on Large Language Models. The\nexisting ones are limited only to simulating one type of service, mainly SSH\nshells. These systems - but also the deception technologies not based on LLMs -\nlack an extensive evaluation that includes human attackers. Generative AI has\nrecently become a valuable asset for cybersecurity researchers and\npractitioners, and the field of cyber-deception is no exception. Researchers\nhave demonstrated how LLMs can be leveraged to create realistic-looking\nhoneytokens, fake users, and even simulated systems that can be used as\nhoneypots. This paper presents an AI-based deception framework called VelLMes,\nwhich can simulate multiple protocols and services such as SSH Linux shell,\nMySQL, POP3, and HTTP. All of these can be deployed and used as honeypots, thus\nVelLMes offers a variety of choices for deception design based on the users'\nneeds. VelLMes is designed to be attacked by humans, so interactivity and\nrealism are key for its performance. We evaluate the generative capabilities\nand the deception capabilities. Generative capabilities were evaluated using\nunit tests for LLMs. The results of the unit tests show that, with careful\nprompting, LLMs can produce realistic-looking responses, with some LLMs having\na 100% passing rate. In the case of the SSH Linux shell, we evaluated deception\ncapabilities with 89 human attackers. The results showed that about 30% of the\nattackers thought that they were interacting with a real system when they were\nassigned an LLM-based honeypot. Lastly, we deployed 10 instances of the SSH\nLinux shell honeypot on the Internet to capture real-life attacks. Analysis of\nthese attacks showed us that LLM honeypots simulating Linux shells can perform\nwell against unstructured and unexpected attacks on the Internet, responding\ncorrectly to most of the issued commands.",
        "url": "http://arxiv.org/abs/2510.06975v1",
        "pdf_url": "http://arxiv.org/pdf/2510.06975v1",
        "arxiv_id": "2510.06975v1",
        "authors": [
            "Muris Sladić",
            "Veronica Valeros",
            "Carlos Catania",
            "Sebastian Garcia"
        ],
        "submitted": "2025-10-08 13:00:23",
        "source": "arxiv",
        "comment": "9 pages. 9 figures. 1 table. This is a preprint of a paper that was\n  presented at the Active Defense and Deception Workshop colocated with IEEE\n  EuroS&P 2025 conference"
    },
    {
        "title": "Probing Social Identity Bias in Chinese LLMs with Gendered Pronouns and Social Groups",
        "abstract": "Large language models (LLMs) are increasingly deployed in user-facing\napplications, raising concerns about their potential to reflect and amplify\nsocial biases. We investigate social identity framing in Chinese LLMs using\nMandarin-specific prompts across ten representative Chinese LLMs, evaluating\nresponses to ingroup (\"We\") and outgroup (\"They\") framings, and extending the\nsetting to 240 social groups salient in the Chinese context. To complement\ncontrolled experiments, we further analyze Chinese-language conversations from\na corpus of real interactions between users and chatbots. Across models, we\nobserve systematic ingroup-positive and outgroup-negative tendencies, which are\nnot confined to synthetic prompts but also appear in naturalistic dialogue,\nindicating that bias dynamics might strengthen in real interactions. Our study\nprovides a language-aware evaluation framework for Chinese LLMs, demonstrating\nthat social identity biases documented in English generalize\ncross-linguistically and intensify in user-facing contexts.",
        "url": "http://arxiv.org/abs/2510.06974v1",
        "pdf_url": "http://arxiv.org/pdf/2510.06974v1",
        "arxiv_id": "2510.06974v1",
        "authors": [
            "Geng Liu",
            "Feng Li",
            "Junjie Mu",
            "Mengxiao Zhu",
            "Francesco Pierri"
        ],
        "submitted": "2025-10-08 13:00:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "EDUMATH: Generating Standards-aligned Educational Math Word Problems",
        "abstract": "Math word problems (MWPs) are critical K-12 educational tools, and\ncustomizing them to students' interests and ability levels can increase\nlearning outcomes. However, teachers struggle to find time to customize MWPs\nfor each student given large class sizes and increasing burnout. We propose\nthat LLMs can support math education by generating MWPs customized to student\ninterests and math education standards. To this end, we use a joint human\nexpert-LLM judge approach to evaluate over 11,000 MWPs generated by open and\nclosed LLMs and develop the first teacher-annotated dataset for\nstandards-aligned educational MWP generation. We show the value of our data by\nusing it to train a 12B open model that matches the performance of larger and\nmore capable open models. We also use our teacher-annotated data to train a\ntext classifier that enables a 30B open LLM to outperform existing closed\nbaselines without any training. Next, we show our models' MWPs are more similar\nto human-written MWPs than those from existing models. We conclude by\nconducting the first study of customized LLM-generated MWPs with grade school\nstudents, finding they perform similarly on our models' MWPs relative to\nhuman-written MWPs but consistently prefer our customized MWPs.",
        "url": "http://arxiv.org/abs/2510.06965v1",
        "pdf_url": "http://arxiv.org/pdf/2510.06965v1",
        "arxiv_id": "2510.06965v1",
        "authors": [
            "Bryan R. Christ",
            "Penelope Molitz",
            "Jonathan Kropko",
            "Thomas Hartvigsen"
        ],
        "submitted": "2025-10-08 12:53:06",
        "source": "arxiv",
        "comment": "32 pages, 15 figures"
    },
    {
        "title": "Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation",
        "abstract": "Despite rapid progress, ASR evaluation remains saturated with short-form\nEnglish, and efficiency is rarely reported. We present the Open ASR\nLeaderboard, a fully reproducible benchmark and interactive leaderboard\ncomparing 60+ open-source and proprietary systems across 11 datasets, including\ndedicated multilingual and long-form tracks. We standardize text normalization\nand report both word error rate (WER) and inverse real-time factor (RTFx),\nenabling fair accuracy-efficiency comparisons. For English transcription,\nConformer encoders paired with LLM decoders achieve the best average WER but\nare slower, while CTC and TDT decoders deliver much better RTFx, making them\nattractive for long-form and offline use. Whisper-derived encoders fine-tuned\nfor English improve accuracy but often trade off multilingual coverage. All\ncode and dataset loaders are open-sourced to support transparent, extensible\nevaluation.",
        "url": "http://arxiv.org/abs/2510.06961v2",
        "pdf_url": "http://arxiv.org/pdf/2510.06961v2",
        "arxiv_id": "2510.06961v2",
        "authors": [
            "Vaibhav Srivastav",
            "Steven Zheng",
            "Eric Bezzam",
            "Eustache Le Bihan",
            "Nithin Koluguri",
            "Piotr Żelasko",
            "Somshubra Majumdar",
            "Adel Moumen",
            "Sanchit Gandhi"
        ],
        "submitted": "2025-10-08 12:44:51",
        "source": "arxiv",
        "comment": "Submitted to ICASSP 2026; Leaderboard:\n  https://huggingface.co/spaces/hf-audio/open_asr_leaderboard ; Code:\n  https://github.com/huggingface/open_asr_leaderboard"
    },
    {
        "title": "Revisiting the Uniform Information Density Hypothesis in LLM Reasoning Traces",
        "abstract": "The Uniform Information Density (UID) hypothesis suggests that effective\ncommunication maintains a stable flow of information. In this work, we revisit\nthis principle in the context of large language model (LLM) reasoning traces,\nasking whether step-level uniformity reflects reasoning quality. To this end,\nwe propose an entropy-based stepwise information density metric and introduce\ntwo complementary measures of uniformity, local and global uniformity scores.\nAcross the experiments on six different reasoning benchmarks, we find that\nstep-level uniformity not only provides a strong theoretical lens but also\nyields practical performance benefits; for example, selecting reasoning traces\nwith more uniform information density at the step-level improves accuracy by\n10-32\\% relative gains over baselines at AIME2025. Our analysis further reveals\nthat correct reasoning traces tend to avoid sharp information density spikes,\nwhile incorrect traces exhibit irregular information bursts. These results\ndemonstrate that UID-inspired information density measures outperform\nalternative internal signals as predictors of reasoning quality. Results\nhighlight the uniformity of the information density as a robust diagnostic and\nselection criterion for building more reliable and accurate reasoning systems.",
        "url": "http://arxiv.org/abs/2510.06953v1",
        "pdf_url": "http://arxiv.org/pdf/2510.06953v1",
        "arxiv_id": "2510.06953v1",
        "authors": [
            "Minju Gwak",
            "Guijin Son",
            "Jaehyung Kim"
        ],
        "submitted": "2025-10-08 12:37:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Ethical AI prompt recommendations in large language models using collaborative filtering",
        "abstract": "As large language models (LLMs) shape AI development, ensuring ethical prompt\nrecommendations is crucial. LLMs offer innovation but risk bias, fairness\nissues, and accountability concerns. Traditional oversight methods struggle\nwith scalability, necessitating dynamic solutions. This paper proposes using\ncollaborative filtering, a technique from recommendation systems, to enhance\nethical prompt selection. By leveraging user interactions, it promotes ethical\nguidelines while reducing bias. Contributions include a synthetic dataset for\nprompt recommendations and the application of collaborative filtering. The work\nalso tackles challenges in ethical AI, such as bias mitigation, transparency,\nand preventing unethical prompt engineering.",
        "url": "http://arxiv.org/abs/2510.06924v1",
        "pdf_url": "http://arxiv.org/pdf/2510.06924v1",
        "arxiv_id": "2510.06924v1",
        "authors": [
            "Jordan Nelson",
            "Almas Baimagambetov",
            "Konstantinos Avgerinakis",
            "Nikolaos Polatidis"
        ],
        "submitted": "2025-10-08 12:03:21",
        "source": "arxiv",
        "comment": "This paper has been accepted to by the International Journal of\n  Parallel, Emergent & Distributed Systems (Taylor and Francis) and has an\n  assigned DOI. We have already chose to make this open access using CC BY. The\n  article is not yet available online on the publisher's website. The DOI is:\n  doi.org/10.1080/17445760.2025.2573086"
    },
    {
        "title": "SHANKS: Simultaneous Hearing and Thinking for Spoken Language Models",
        "abstract": "Current large language models (LLMs) and spoken language models (SLMs) begin\nthinking and taking actions only after the user has finished their turn. This\nprevents the model from interacting during the user's turn and can lead to high\nresponse latency while it waits to think. Consequently, thinking after\nreceiving the full input is not suitable for speech-to-speech interaction,\nwhere real-time, low-latency exchange is important. We address this by noting\nthat humans naturally \"think while listening.\" In this paper, we propose\nSHANKS, a general inference framework that enables SLMs to generate unspoken\nchain-of-thought reasoning while listening to the user input. SHANKS streams\nthe input speech in fixed-duration chunks and, as soon as a chunk is received,\ngenerates unspoken reasoning based on all previous speech and reasoning, while\nthe user continues speaking. SHANKS uses this unspoken reasoning to decide\nwhether to interrupt the user and to make tool calls to complete the task. We\ndemonstrate that SHANKS enhances real-time user-SLM interaction in two\nscenarios: (1) when the user is presenting a step-by-step solution to a math\nproblem, SHANKS can listen, reason, and interrupt when the user makes a\nmistake, achieving 37.1% higher interruption accuracy than a baseline that\ninterrupts without thinking; and (2) in a tool-augmented dialogue, SHANKS can\ncomplete 56.9% of the tool calls before the user finishes their turn. Overall,\nSHANKS moves toward models that keep thinking throughout the conversation, not\nonly after a turn ends. Animated illustrations of Shanks can be found at\nhttps://d223302.github.io/SHANKS/",
        "url": "http://arxiv.org/abs/2510.06917v1",
        "pdf_url": "http://arxiv.org/pdf/2510.06917v1",
        "arxiv_id": "2510.06917v1",
        "authors": [
            "Cheng-Han Chiang",
            "Xiaofei Wang",
            "Linjie Li",
            "Chung-Ching Lin",
            "Kevin Lin",
            "Shujie Liu",
            "Zhendong Wang",
            "Zhengyuan Yang",
            "Hung-yi Lee",
            "Lijuan Wang"
        ],
        "submitted": "2025-10-08 11:48:59",
        "source": "arxiv",
        "comment": "Work in progress"
    },
    {
        "title": "LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling",
        "abstract": "Reward model (RM) plays a pivotal role in aligning large language model (LLM)\nwith human preferences. As real-world applications increasingly involve long\nhistory trajectories, e.g., LLM agent, it becomes indispensable to evaluate\nwhether a model's responses are not only high-quality but also grounded in and\nconsistent with the provided context. Yet, current RMs remain confined to\nshort-context settings and primarily focus on response-level attributes (e.g.,\nsafety or helpfulness), while largely neglecting the critical dimension of long\ncontext-response consistency. In this work, we introduce Long-RewardBench, a\nbenchmark specifically designed for long-context RM evaluation, featuring both\nPairwise Comparison and Best-of-N tasks. Our preliminary study reveals that\neven state-of-the-art generative RMs exhibit significant fragility in\nlong-context scenarios, failing to maintain context-aware preference judgments.\nMotivated by the analysis of failure patterns observed in model outputs, we\npropose a general multi-stage training strategy that effectively scales\narbitrary models into robust Long-context RMs (LongRMs). Experiments show that\nour approach not only substantially improves performance on long-context\nevaluation but also preserves strong short-context capability. Notably, our 8B\nLongRM outperforms much larger 70B-scale baselines and matches the performance\nof the proprietary Gemini 2.5 Pro model.",
        "url": "http://arxiv.org/abs/2510.06915v1",
        "pdf_url": "http://arxiv.org/pdf/2510.06915v1",
        "arxiv_id": "2510.06915v1",
        "authors": [
            "Zecheng Tang",
            "Baibei Ji",
            "Quantong Qiu",
            "Haitian Wang",
            "Xiaobo Liang",
            "Juntao Li",
            "Min Zhang"
        ],
        "submitted": "2025-10-08 11:48:16",
        "source": "arxiv",
        "comment": null
    }
]