# Daily Papers Report - 2025-08-21

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Skatje Myers, Dmitriy Dligach, Timothy A. Miller, Samantha Barr, Yanjun Gao, Matthew Churpek, Anoop Mayampurath, Majid Afshar
- **URL**: <http://arxiv.org/abs/2508.14817v1>
- **Submitted**: 2025-08-20 16:09:37
- **Topic Keywords**: rag, ctr, retrieval
- **Reason**: The paper explores the application of retrieval-augmented generation in the clinical domain, which is not directly related to the user's primary focus on information retrieval, search technologies, and query understanding. While the paper touches on the topic of retrieval, it is more focused on the application of language models in a specific domain rather than the underlying retrieval mechanisms or query understanding.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Retrieval-Augmented Generation (RAG) for Navigating Electronic Health Records (EHRs)
- **Aim**: To propose a RAG approach to retrieve task-relevant passages from EHRs, reducing the amount of required input tokens
- **Rationale**: EHRs are lengthy, noisy, and redundant, making it challenging to extract relevant information; RAG can improve the efficiency and accuracy of clinical decision-making
- **Ground**: The authors evaluate the RAG approach using three clinical tasks (extracting imaging procedures, generating timelines of antibiotic use, and identifying key diagnoses) and three state-of-the-art large language models (LLMs)
- **Experiment**: The authors train a BERT-based embedding model to embed clinical notes and retrieve relevant passages, and evaluate the performance of LLMs on the three tasks using varying amounts of clinical context
- **Takeaway**: RAG can provide substantial efficiency improvements over comparable amounts of recent clinical note tokens, and can reach near parity with using up to 128K tokens of recent clinical notes, while requiring significantly fewer input tokens

#### Abstract
> Electronic health records (EHRs) are long, noisy, and often redundant, posing
a major challenge for the clinicians who must navigate them. Large language
models (LLMs) offer a promising solution for extracting and reasoning over this
unstructured text, but the length of clinical notes often exceeds even
state-of-the-art models' extended context windows. Retrieval-augmented
generation (RAG) offers an alternative by retrieving task-relevant passages
from across the entire EHR, potentially reducing the amount of required input
tokens. In this work, we propose three clinical tasks designed to be replicable
across health systems with minimal effort: 1) extracting imaging procedures, 2)
generating timelines of antibiotic use, and 3) identifying key diagnoses. Using
EHRs from actual hospitalized patients, we test three state-of-the-art LLMs
with varying amounts of provided context, using either targeted text retrieval
or the most recent clinical notes. We find that RAG closely matches or exceeds
the performance of using recent notes, and approaches the performance of using
the models' full context while requiring drastically fewer input tokens. Our
results suggest that RAG remains a competitive and efficient approach even as
newer models become capable of handling increasingly longer amounts of text.

---

### 2. MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Ailing Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li, Yun Yue, Hansong Xiao, Hualei Zhou, Chunxiao Guo, Peng Wei, Jinjie Gu
- **URL**: <http://arxiv.org/abs/2508.14880v1>
- **Submitted**: 2025-08-20 17:51:20
- **Comment**: 13 pages, 5 figures
- **Topic Keywords**: rag, retrieval, search
- **Reason**: The paper presents a medical deep research agent that addresses challenges in the medical domain, such as lack of dense medical knowledge and absence of specialized retrieval tools. While it innovatively integrates a custom-built medical retrieval engine, the focus is on medical domain-specific solutions, which is somewhat related to my interests in Information Retrieval and Search technologies. However, the paper's primary focus on medical domain and lack of direct connection to query understanding, ranking models, and user behavior modeling make it less relevant to my core research themes.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Medical Deep Research Agent
- **Aim**: Address limitations of general-purpose deep research agents in processing medical domain-specific tasks
- **Rationale**: Current agents lack domain-specific knowledge and reasoning capabilities, leading to inaccurate information and shallow inference
- **Ground**: Medical knowledge graphs, private medical retrieval engine, and Grouped Regularized Policy Optimization (GRPO) for training
- **Experiment**: MedReseacher-R1 agent evaluation on MedBrowseComp, GAIA, and XBench-DeepSearch benchmarks, demonstrating state-of-the-art performance and strong generalization capabilities
- **Takeaway**: MedReseacher-R1, a novel medical deep research agent, achieves state-of-the-art results in medical domain-specific tasks and highlights the importance of domain-specific knowledge and reasoning capabilities in deep research agents

#### Abstract
> Recent developments in Large Language Model (LLM)-based agents have shown
impressive capabilities spanning multiple domains, exemplified by deep research
systems that demonstrate superior performance on complex information-seeking
and synthesis tasks. While general-purpose deep research agents have shown
impressive capabilities, they struggle significantly with medical domain
challenges, as evidenced by leading proprietary systems achieving limited
accuracy on complex medical benchmarks. The key limitations are: (1) the model
lacks sufficient dense medical knowledge for clinical reasoning, and (2) the
framework is constrained by the absence of specialized retrieval tools tailored
for medical contexts.We present a medical deep research agent that addresses
these challenges through two core innovations. First, we develop a novel data
synthesis framework using medical knowledge graphs, extracting the longest
chains from subgraphs around rare medical entities to generate complex
multi-hop question-answer pairs. Second, we integrate a custom-built private
medical retrieval engine alongside general-purpose tools, enabling accurate
medical information synthesis. Our approach generates 2100+ diverse
trajectories across 12 medical specialties, each averaging 4.2 tool
interactions.Through a two-stage training paradigm combining supervised
fine-tuning and online reinforcement learning with composite rewards, our
MedResearcher-R1-32B model demonstrates exceptional performance, establishing
new state-of-the-art results on medical benchmarks while maintaining
competitive performance on general deep research tasks. Our work demonstrates
that strategic domain-specific innovations in architecture, tool design, and
training data construction can enable smaller open-source models to outperform
much larger proprietary systems in specialized domains.

---

### 3. Benefiting from Negative yet Informative Feedback by Contrasting Opposing Sequential Patterns

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Veronika Ivanova, Evgeny Frolov, Alexey Vasilev
- **URL**: <http://arxiv.org/abs/2508.14786v1>
- **Submitted**: 2025-08-20 15:32:16
- **Topic Keywords**: recommend
- **Reason**: The paper explores sequential recommendation and learning from negative feedback, which is somewhat related to my interests in Information Retrieval and Search technologies. However, the focus on recommender systems and user interactions in the e-commerce domain is not directly aligned with my primary interests in query understanding, ranking models, and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Sequential Recommendation with Positive and Negative Feedback
- **Aim**: To develop a novel approach that incorporates both positive and negative feedback in sequential recommendation scenarios
- **Rationale**: Traditional sequential learning models focus solely on predicting positive interactions, ignoring the importance of reducing items with negative feedback in recommendations
- **Ground**: The authors redefine the problem as predicting the next positive item in a sequence of interactions, considering the proportion of music tracks skipped by a user as a typical metric of online music recommendations evaluation
- **Experiment**: Extensive experiments on a diverse range of datasets, including ML-1M, ML-20M, Toys&Games, and Kion, comparing the PNFRec model to baseline models that incorporate negative feedback
- **Takeaway**: The PNFRec model outperforms baseline models, improving the quality of sequential recommenders by incorporating negative user responses and reducing the number of false positives

#### Abstract
> We consider the task of learning from both positive and negative feedback in
a sequential recommendation scenario, as both types of feedback are often
present in user interactions. Meanwhile, conventional sequential learning
models usually focus on considering and predicting positive interactions,
ignoring that reducing items with negative feedback in recommendations improves
user satisfaction with the service. Moreover, the negative feedback can
potentially provide a useful signal for more accurate identification of true
user interests. In this work, we propose to train two transformer encoders on
separate positive and negative interaction sequences. We incorporate both types
of feedback into the training objective of the sequential recommender using a
composite loss function that includes positive and negative cross-entropy as
well as a cleverly crafted contrastive term, that helps better modeling
opposing patterns. We demonstrate the effectiveness of this approach in terms
of increasing true-positive metrics compared to state-of-the-art sequential
recommendation methods while reducing the number of wrongly promoted negative
items.

---

### 4. MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Ziyang Luo, Zhiqi Shen, Wenzhuo Yang, Zirui Zhao, Prathyusha Jwalapuram, Amrita Saha, Doyen Sahoo, Silvio Savarese, Caiming Xiong, Junnan Li
- **URL**: <http://arxiv.org/abs/2508.14704v1>
- **Submitted**: 2025-08-20 13:28:58
- **Comment**: Website: https://mcp-universe.github.io
- **Topic Keywords**: search
- **Reason**: The paper introduces a benchmark for evaluating large language models in realistic scenarios, focusing on interaction with real-world Model Context Protocol servers. While it touches on some aspects of search and retrieval, such as web searching, the primary focus is on language models and their limitations, which is not directly aligned with the user's interests in information retrieval, query understanding, and ranking models.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: Evaluating Large Language Models (LLMs) with Realistic Benchmarks
- **Aim**: To develop a comprehensive benchmark for evaluating LLMs in realistic and challenging tasks
- **Rationale**: Existing benchmarks for evaluating LLMs are simplistic and do not capture real-world challenges
- **Ground**: The Model Context Protocol (MCP) has become a widely adopted standard for connecting LLMs to external data sources and tools
- **Experiment**: The authors evaluate leading LLMs, including GPT-5, Grok-4, and Claude-4.0-Sonnet, using the MCP-Universe benchmark
- **Takeaway**: Even state-of-the-art LLMs exhibit significant performance limitations, and the benchmark poses challenges related to long context and unknown tools

#### Abstract
> The Model Context Protocol has emerged as a transformative standard for
connecting large language models to external data sources and tools, rapidly
gaining adoption across major AI providers and development platforms. However,
existing benchmarks are overly simplistic and fail to capture real application
challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To
address this critical gap, we introduce MCP-Universe, the first comprehensive
benchmark specifically designed to evaluate LLMs in realistic and hard tasks
through interaction with real-world MCP servers. Our benchmark encompasses 6
core domains spanning 11 different MCP servers: Location Navigation, Repository
Management, Financial Analysis, 3D Design, Browser Automation, and Web
Searching. To ensure rigorous evaluation, we implement execution-based
evaluators, including format evaluators for agent format compliance, static
evaluators for time-invariant content matching, and dynamic evaluators that
automatically retrieve real-time ground truth for temporally sensitive tasks.
Through extensive evaluation of leading LLMs, we find that even SOTA models
such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit
significant performance limitations. In addition, our benchmark poses a
significant long-context challenge for LLM agents, as the number of input
tokens increases rapidly with the number of interaction steps. Moreover, it
introduces an unknown-tools challenge, as LLM agents often lack familiarity
with the precise usage of the MCP servers. Notably, enterprise-level agents
like Cursor cannot achieve better performance than standard ReAct frameworks.
Beyond evaluation, we open-source our extensible evaluation framework with UI
support, enabling researchers and practitioners to seamlessly integrate new
agents and MCP servers while fostering innovation in the rapidly evolving MCP
ecosystem.

---

### 5. The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Shubham Pundhir, Ganesh Bagler
- **URL**: <http://arxiv.org/abs/2508.14718v1>
- **Submitted**: 2025-08-20 13:53:13
- **Comment**: 8 pages, 4 figures. Code is available at:
  https://github.com/shubh-iiit/RecipeGPT2-Your-Own-AI-Chef
- **Topic Keywords**: relevance, search
- **Reason**: The paper focuses on recipe generation using language models, which is not directly related to Information Retrieval, Search technologies, or query understanding. Although it involves natural language processing, the topic is more specific to recipe generation and does not address ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Fine-tuning language models for recipe generation
- **Aim**: Comparing the impact of model scale and a novel tokenization strategy on recipe generation
- **Rationale**: Traditional recurrent architectures have limitations in maintaining long-range dependencies and global coherence, and pre-trained models like GPT-2 require custom tokenization to handle domain-specific structures and symbols
- **Ground**: Comparative study on fine-tuning language models using the 5-cuisine corpus from RecipeDB, evaluating performance using seven automatic metrics
- **Experiment**: Fine-tuning pre-trained GPT-2 model using a causal language modeling objective, with stochastic sampling methods to encourage creativity and diversity
- **Takeaway**: Fine-tuned GPT-2 model outperforms LSTM/RNN baselines, but exhibits critical weaknesses, and future work includes retrieval-augmented generation, constrained decoding, and systematic human evaluation

#### Abstract
> We established a rigorous benchmark for text-based recipe generation, a
fundamental task in natural language generation. We present a comprehensive
comparative study contrasting a fine-tuned GPT-2 large (774M) model against the
GPT-2 small (124M) model and traditional LSTM/RNN baselines on the 5-cuisine
corpus from RecipeDB. Our key contribution is a targeted tokenization strategy
that augments the vocabulary with 23 common fraction tokens and custom
structural markers. This approach addresses a critical limitation of generic
tokenizers by preserving essential recipe structures and precise numerical
quantities, thereby enhancing domain specificity. Performance is evaluated
using a comprehensive suite of seven automatic metrics spanning fluency
(BLEU-4, METEOR), coherence (ROUGE-L), semantic relevance (BERTScore), and
diversity. Our experiments show that the large transformer-based approach
yields a >20% relative improvement in BERTScore (F1) (0.92 vs 0.72) over the
best recurrent baseline, while reducing perplexity by 69.8%. We conclude with a
discussion of remaining challenges, particularly regarding factual accuracy,
and outline how this foundational study paves the way for integrating
real-world constraints and multi-modal inputs in advanced recipe generation
research.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. OneLoc: Geo-Aware Generative Recommender Systems for Local Life Service

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Zhipeng Wei, Kuo Cai, Junda She, Jie Chen, Minghao Chen, Yang Zeng, Qiang Luo, Wencong Zeng, Ruiming Tang, Kun Gai, Guorui Zhou
- **URL**: <http://arxiv.org/abs/2508.14646v1>
- **Submitted**: 2025-08-20 11:57:48
- **Topic Keywords**: rag, recommend, search
- **Reason**: This paper focuses on a specific application of recommender systems in the Kuaishou App, using geographic information to improve video recommendations. While it mentions some aspects of user behavior modeling, it does not directly relate to query understanding, ranking models, or deep semantic understanding in information retrieval, which are the user's primary research interests.

#### Abstract
> Local life service is a vital scenario in Kuaishou App, where video
recommendation is intrinsically linked with store's location information. Thus,
recommendation in our scenario is challenging because we should take into
account user's interest and real-time location at the same time. In the face of
such complex scenarios, end-to-end generative recommendation has emerged as a
new paradigm, such as OneRec in the short video scenario, OneSug in the search
scenario, and EGA in the advertising scenario. However, in local life service,
an end-to-end generative recommendation model has not yet been developed as
there are some key challenges to be solved. The first challenge is how to make
full use of geographic information. The second challenge is how to balance
multiple objectives, including user interests, the distance between user and
stores, and some other business objectives. To address the challenges, we
propose OneLoc. Specifically, we leverage geographic information from different
perspectives: (1) geo-aware semantic ID incorporates both video and geographic
information for tokenization, (2) geo-aware self-attention in the encoder
leverages both video location similarity and user's real-time location, and (3)
neighbor-aware prompt captures rich context information surrounding users for
generation. To balance multiple objectives, we use reinforcement learning and
propose two reward functions, i.e., geographic reward and GMV reward. With the
above design, OneLoc achieves outstanding offline and online performance. In
fact, OneLoc has been deployed in local life service of Kuaishou App. It serves
400 million active users daily, achieving 21.016% and 17.891% improvements in
terms of gross merchandise value (GMV) and orders numbers.

### 7. Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Haokun Lin, Haobo Xu, Yichen Wu, Ziyu Guo, Renrui Zhang, Zhichao Lu, Ying Wei, Qingfu Zhang, Zhenan Sun
- **URL**: <http://arxiv.org/abs/2508.14896v1>
- **Submitted**: 2025-08-20 17:59:51
- **Comment**: Technical Report, Work in Progress
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on the quantization of diffusion language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on topics like model deployment and optimization, the primary focus is on the technical aspects of model compression, which is not a key area of interest for your research.

#### Abstract
> Recent advances in diffusion large language models (dLLMs) have introduced a
promising alternative to autoregressive (AR) LLMs for natural language
generation tasks, leveraging full attention and denoising-based decoding
strategies. However, the deployment of these models on edge devices remains
challenging due to their massive parameter scale and high resource demands.
While post-training quantization (PTQ) has emerged as a widely adopted
technique for compressing AR LLMs, its applicability to dLLMs remains largely
unexplored. In this work, we present the first systematic study on quantizing
diffusion-based language models. We begin by identifying the presence of
activation outliers, characterized by abnormally large activation values that
dominate the dynamic range. These outliers pose a key challenge to low-bit
quantization, as they make it difficult to preserve precision for the majority
of values. More importantly, we implement state-of-the-art PTQ methods and
conduct a comprehensive evaluation across multiple task types and model
variants. Our analysis is structured along four key dimensions: bit-width,
quantization method, task category, and model type. Through this
multi-perspective evaluation, we offer practical insights into the quantization
behavior of dLLMs under different configurations. We hope our findings provide
a foundation for future research in efficient dLLM deployment. All codes and
experimental setups will be released to support the community.

### 8. Virtual Community: An Open World for Humans, Robots, and Society

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Qinhong Zhou, Hongxin Zhang, Xiangye Lin, Zheyuan Zhang, Yutian Chen, Wenjun Liu, Zunzhe Zhang, Sunli Chen, Lixing Fang, Qiushi Lyu, Xinyu Sun, Jincheng Yang, Zeyuan Wang, Bao Chi Dang, Zhehuan Chen, Daksha Ladia, Jiageng Liu, Chuang Gan
- **URL**: <http://arxiv.org/abs/2508.14893v1>
- **Submitted**: 2025-08-20 17:59:32
- **Comment**: website https://virtual-community-ai.github.io/
- **Topic Keywords**: rag
- **Reason**: The paper focuses on virtual communities and human-robot coexistence, which is not directly related to information retrieval, search technologies, or natural language processing. The topics of embodied social intelligence, multi-agent physics simulator, and community generation pipeline are not aligned with the user's research interests.

#### Abstract
> The rapid progress in AI and Robotics may lead to a profound societal
transformation, as humans and robots begin to coexist within shared
communities, introducing both opportunities and challenges. To explore this
future, we present Virtual Community-an open-world platform for humans, robots,
and society-built on a universal physics engine and grounded in real-world 3D
scenes. With Virtual Community, we aim to study embodied social intelligence at
scale: 1) How robots can intelligently cooperate or compete; 2) How humans
develop social relations and build community; 3) More importantly, how
intelligent robots and humans can co-exist in an open world. To support these,
Virtual Community features: 1) An open-source multi-agent physics simulator
that supports robots, humans, and their interactions within a society; 2) A
large-scale, real-world aligned community generation pipeline, including vast
outdoor space, diverse indoor scenes, and a community of grounded agents with
rich characters and appearances. Leveraging Virtual Community, we propose two
novel challenges. The Community Planning Challenge evaluates multi-agent
reasoning and planning ability in open-world settings, such as cooperating to
help agents with daily activities and efficiently connecting other agents. The
Community Robot Challenge requires multiple heterogeneous robots to collaborate
in solving complex open-world tasks. We evaluate various baselines on these
tasks and demonstrate the challenges in both high-level open-world task
planning and low-level cooperation controls. We hope that Virtual Community
will unlock further study of human-robot coexistence within open-world
environments.

### 9. TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jiaming Leng, Yunying Bi, Chuan Qin, Bing Yin, Yanyong Zhang, Chao Wang
- **URL**: <http://arxiv.org/abs/2508.14782v1>
- **Submitted**: 2025-08-20 15:27:49
- **Topic Keywords**: ctr
- **Reason**: The paper focuses on a unified framework for urban transportation tasks, using learnable prompting and large language models. While it mentions 'structured embeddings' and 'spatiotemporal modeling', there is no apparent connection to information retrieval, search technologies, or query understanding, which are the user's primary research interests.

#### Abstract
> Urban transportation systems encounter diverse challenges across multiple
tasks, such as traffic forecasting, electric vehicle (EV) charging demand
prediction, and taxi dispatch. Existing approaches suffer from two key
limitations: small-scale deep learning models are task-specific and
data-hungry, limiting their generalizability across diverse scenarios, while
large language models (LLMs), despite offering flexibility through natural
language interfaces, struggle with structured spatiotemporal data and numerical
reasoning in transportation domains. To address these limitations, we propose
TransLLM, a unified foundation framework that integrates spatiotemporal
modeling with large language models through learnable prompt composition. Our
approach features a lightweight spatiotemporal encoder that captures complex
dependencies via dilated temporal convolutions and dual-adjacency graph
attention networks, seamlessly interfacing with LLMs through structured
embeddings. A novel instance-level prompt routing mechanism, trained via
reinforcement learning, dynamically personalizes prompts based on input
characteristics, moving beyond fixed task-specific templates. The framework
operates by encoding spatiotemporal patterns into contextual representations,
dynamically composing personalized prompts to guide LLM reasoning, and
projecting the resulting representations through specialized output layers to
generate task-specific predictions. Experiments across seven datasets and three
tasks demonstrate the exceptional effectiveness of TransLLM in both supervised
and zero-shot settings. Compared to ten baseline models, it delivers
competitive performance on both regression and planning problems, showing
strong generalization and cross-task adaptability. Our code is available at
https://github.com/BiYunying/TransLLM.

### 10. Transplant Then Regenerate: A New Paradigm for Text Data Augmentation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Guangzhan Wang, Hongyu Zhang, Beijun Shen, Xiaodong Gu
- **URL**: <http://arxiv.org/abs/2508.14723v1>
- **Submitted**: 2025-08-20 14:05:18
- **Comment**: Accepted by EMNLP 2025
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling. The topic of text data augmentation is more relevant to Natural Language Processing and data mining, but the focus on large language models and knowledge emergence is not aligned with your interests in real-time relevance optimization and deep semantic understanding.

#### Abstract
> Data augmentation is a critical technique in deep learning. Traditional
methods like Back-translation typically focus on lexical-level rephrasing,
which primarily produces variations with the same semantics. While large
language models (LLMs) have enhanced text augmentation by their "knowledge
emergence" capability, controlling the style and structure of these outputs
remains challenging and requires meticulous prompt engineering. In this paper,
we propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.
The core idea of LMTransplant is transplant-then-regenerate: incorporating seed
text into a context expanded by LLM, and asking the LLM to regenerate a variant
based on the expanded context. This strategy allows the model to create more
diverse and creative content-level variants by fully leveraging the knowledge
embedded in LLMs, while preserving the core attributes of the original text. We
evaluate LMTransplant across various text-related tasks, demonstrating its
superior performance over existing text augmentation methods. Moreover,
LMTransplant demonstrates exceptional scalability as the size of augmented data
grows.

### 11. Improving in-context learning with a better scoring function

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Omar Naim, Swarnadeep Bhar, J√©r√¥me Bolte, Nicholas Asher
- **URL**: <http://arxiv.org/abs/2508.14685v1>
- **Submitted**: 2025-08-20 13:01:34
- **Topic Keywords**: rag
- **Reason**: The paper focuses on improving in-context learning with a better scoring function, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it involves language models and transformers, the context is not relevant to the user's primary research interests.

#### Abstract
> Large language models (LLMs) exhibit a remarkable capacity to learn by
analogy, known as in-context learning (ICL). However, recent studies have
revealed limitations in this ability. In this paper, we examine these
limitations on tasks involving first-order quantifiers such as {\em all} and
{\em some}, as well as on ICL with linear functions. We identify Softmax, the
scoring function in attention mechanism, as a contributing factor to these
constraints. To address this, we propose \textbf{scaled signed averaging
(SSA)}, a novel alternative to Softmax. Empirical results show that SSA
dramatically improves performance on our target tasks. Furthermore, we evaluate
both encoder-only and decoder-only transformers models with SSA, demonstrating
that they match or exceed their Softmax-based counterparts across a variety of
linguistic probing tasks.

### 12. Long Chain-of-Thought Reasoning Across Languages

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Josh Barua, Seun Eisape, Kayo Yin, Alane Suhr
- **URL**: <http://arxiv.org/abs/2508.14828v1>
- **Submitted**: 2025-08-20 16:22:51
- **Comment**: Accepted to SCALR @ COLM 2025
- **Topic Keywords**: search
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on multilingual reasoning and large language models, which is a topic in Natural Language Processing, but it does not address your specific areas of interest.

#### Abstract
> Scaling inference through long chains-of-thought (CoTs) has unlocked
impressive reasoning capabilities in large language models (LLMs), yet the
reasoning process remains almost exclusively English-centric. We construct
translated versions of two popular English reasoning datasets, fine-tune Qwen
2.5 (7B) and Qwen 3 (8B) models, and present a systematic study of long CoT
generation across French, Japanese, Latvian, and Swahili. Our experiments
reveal three key findings. First, the efficacy of using English as a pivot
language varies by language: it provides no benefit for French, improves
performance when used as the reasoning language for Japanese and Latvian, and
proves insufficient for Swahili where both task comprehension and reasoning
remain poor. Second, extensive multilingual pretraining in Qwen 3 narrows but
does not eliminate the cross-lingual performance gap. A lightweight fine-tune
using only 1k traces still improves performance by over 30\% in Swahili. Third,
data quality versus scale trade-offs are language dependent: small, carefully
curated datasets suffice for English and French, whereas larger but noisier
corpora prove more effective for Swahili and Latvian. Together, these results
clarify when and why long CoTs transfer across languages and provide translated
datasets to foster equitable multilingual reasoning research.

### 13. Improving LLMs for Machine Translation Using Synthetic Preference Data

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Dario Vajda, Domen Vre≈°, Marko Robnik-≈†ikonja
- **URL**: <http://arxiv.org/abs/2508.14951v1>
- **Submitted**: 2025-08-20 14:24:16
- **Comment**: Paper with individual presentation at LUHME workshop at ECAI 2025
- **Topic Keywords**: rank
- **Reason**: This paper is not relevant to your research interests as it focuses on improving large language models for machine translation using synthetic preference data, which is not related to information retrieval, search technologies, or query understanding. The paper's emphasis on machine translation and language models also falls outside your background in e-commerce and NLP.

#### Abstract
> Large language models have emerged as effective machine translation systems.
In this paper, we explore how a general instruction-tuned large language model
can be improved for machine translation using relatively few easily produced
data resources. Using Slovene as a use case, we improve the GaMS-9B-Instruct
model using Direct Preference Optimization (DPO) training on a programmatically
curated and enhanced subset of a public dataset. As DPO requires pairs of
quality-ranked instances, we generated its training dataset by translating
English Wikipedia articles using two LLMs, GaMS-9B-Instruct and
EuroLLM-9B-Instruct. We ranked the resulting translations based on heuristics
coupled with automatic evaluation metrics such as COMET. The evaluation shows
that our fine-tuned model outperforms both models involved in the dataset
generation. In comparison to the baseline models, the fine-tuned model achieved
a COMET score gain of around 0.04 and 0.02, respectively, on translating
Wikipedia articles. It also more consistently avoids language and formatting
errors.

---

