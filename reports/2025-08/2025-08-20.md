# Daily Papers Report - 2025-08-20

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. InPars+: Supercharging Synthetic Data Generation for Information Retrieval Systems

- **LLM Score**: 7
- **Keyword Score**: 18
- **Authors**: Matey Krastev, Miklos Hamar, Danilo Toapanta, Jesse Brouwers, Yibin Lei
- **URL**: <http://arxiv.org/abs/2508.13930v1>
- **Submitted**: 2025-08-19 15:23:18
- **Topic Keywords**: information retrieval, query, queries, rerank, rag, retrieval, rank, search
- **Reason**: The paper is relevant to Information Retrieval (IR) and Search technologies, specifically in the area of synthetic query generation for Neural Information Retrieval (NIR). The use of large language models (LLMs) and query generator LLM fine-tuning via Contrastive Preference Optimization (CPO) aligns with my interests in query understanding and ranking models. However, the focus on synthetic data generation and query generation pipelines is not directly related to my primary interests in user behavior modeling and real-time relevance optimization.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Synthetic Query Generation for Neural Information Retrieval (NIR) using InPars Toolkit
- **Aim**: To revisit and extend existing synthetic query generation pipelines by leveraging large language models (LLMs) and propose a modified triplet data generation pipeline
- **Rationale**: Existing pipelines are not fully reproducible and are restricted to specific hardware, while LLMs can generate high-quality queries
- **Ground**: The authors validate the effectiveness of their proposed extensions on the SciFact benchmark and MS MARCO passage dataset
- **Experiment**: The authors conduct experiments using the SciFact dataset and InPars-V2 strategy, comparing results with and without CoT prompting and filtering
- **Takeaway**: The proposed extensions improve retrieval performance, and CoT prompting can enhance efficiency and potentially improve downstream performance, while bigger models do not always produce better outputs

#### Abstract
> This work revisits and extends synthetic query generation pipelines for
Neural Information Retrieval (NIR) by leveraging the InPars Toolkit, a
reproducible, end-to-end framework for generating training data using large
language models (LLMs). We first assess the reproducibility of the original
InPars, InPars-V2, and Promptagator pipelines on the SciFact benchmark and
validate their effectiveness using open-source reranker and generator models.
Building on this foundation, we introduce two key extensions to the pipeline:
(1) fine-tuning a query generator LLM via Contrastive Preference Optimization
(CPO) to improve the signal quality in generated queries, and (2) replacing
static prompt templates with dynamic, Chain-of-Thought (CoT) optimized prompts
using the DSPy framework. Our results show that both extensions reduce the need
for aggressive filtering while improving retrieval performance. All code,
models, and synthetic datasets are publicly released to support further
research at: \href{https://github.com/danilotpnta/IR2-project}{this https URL}.

---

### 2. FLAIR: Feedback Learning for Adaptive Information Retrieval

- **LLM Score**: 7
- **Keyword Score**: 15
- **Authors**: William Zhang, Yiwen Zhu, Yunlei Lu, Mathieu Demarne, Wenjing Wang, Kai Deng, Nutan Sahoo, Katherine Lin, Miso Cilimdzic, Subru Krishnan
- **URL**: <http://arxiv.org/abs/2508.13390v1>
- **Submitted**: 2025-08-18 22:37:27
- **Comment**: Accepted to CIKM2025
- **Topic Keywords**: information retrieval, query, queries, ranking, retrieval, rank
- **Reason**: The paper FLAIR: Feedback Learning for Adaptive Information Retrieval is somewhat related to your research interests in Information Retrieval, specifically in query understanding and ranking models. The paper presents a feedback learning framework that adapts retrieval strategies, which aligns with your focus on deep semantic understanding and real-time relevance optimization. However, the paper's primary focus on copilot systems and domain-specific expert feedback may not directly relate to your background in e-commerce or NLP, leading to a lower score.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Adaptive Information Retrieval for Copilot Systems
- **Aim**: Design a framework that adapts copilot systems' retrieval strategies by integrating domain-specific expert feedback
- **Rationale**: Existing copilot systems struggle to handle technical terminology and internal information, leading to reduced relevance and accuracy
- **Ground**: FLAIR builds on the contextual multi-armed bandit framework, using signal indicators from user feedback or generated synthetically from document content, and a two-track ranking algorithm
- **Experiment**: The paper evaluates the performance of the FLAIR algorithm through several experiments, including the evaluation of various configurations for a query expansion algorithm and a comparison of several algorithms on real user queries
- **Takeaway**: FLAIR demonstrates significant performance gains on both previously seen and unseen queries, surpassing state-of-the-art approaches, and its deployment in production facilitates the evaluation of documentation quality

#### Abstract
> Recent advances in Large Language Models (LLMs) have driven the adoption of
copilots in complex technical scenarios, underscoring the growing need for
specialized information retrieval solutions. In this paper, we introduce FLAIR,
a lightweight, feedback learning framework that adapts copilot systems'
retrieval strategies by integrating domain-specific expert feedback. FLAIR
operates in two stages: an offline phase obtains indicators from (1) user
feedback and (2) questions synthesized from documentation, storing these
indicators in a decentralized manner. An online phase then employs a two-track
ranking mechanism to combine raw similarity scores with the collected
indicators. This iterative setup refines retrieval performance for any query.
Extensive real-world evaluations of FLAIR demonstrate significant performance
gains on both previously seen and unseen queries, surpassing state-of-the-art
approaches. The system has been successfully integrated into Copilot DECO,
serving thousands of users at Microsoft, demonstrating its scalability and
effectiveness in operational environments.

---

### 3. ENCODE: Breaking the Trade-Off Between Performance and Efficiency in Long-Term User Behavior Modeling

- **LLM Score**: 6
- **Keyword Score**: 12
- **Authors**: Wenji Zhou, Yuhang Zheng, Yinfu Feng, Yunan Ye, Rong Xiao, Long Chen, Xiaosong Yang, Jun Xiao
- **URL**: <http://arxiv.org/abs/2508.13567v1>
- **Submitted**: 2025-08-19 06:58:21
- **Comment**: Accepted to TKDE
- **Topic Keywords**: pairwise, relevance, user behavior, click, click-through rate
- **Reason**: The paper focuses on long-term user behavior modeling, which is related to user behavior modeling, a topic you mentioned as one of your interests. However, the paper does not directly relate to your primary focus on information retrieval, especially in areas that require deep semantic understanding and real-time relevance optimization. The paper's emphasis on e-commerce and click-through rate also makes it less directly applicable to your broader interests in NLP, data mining, and related topics.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Long-term User Behavior Modeling
- **Aim**: To break the trade-off between performance and efficiency in long-term user behavior modeling
- **Rationale**: Existing methods lack guidance from target items and inconsistent use of relevance metrics, leading to poor relevance between learned interests and target items
- **Ground**: ENCODE addresses these limitations by using a metric learning-based dimension reduction algorithm in the offline extraction stage and ensuring relevance between user interests and target items in the online inference stage
- **Experiment**: ENCODE outperforms state-of-the-art long-term sequence modeling methods in both performance and effectiveness on public and industrial datasets
- **Takeaway**: ENCODE effectively extracts more relevant long-term interest representations and achieves better performance compared to other search-based methods, with potential applications in Click-Through Rate prediction and recommender systems

#### Abstract
> Long-term user behavior sequences are a goldmine for businesses to explore
users' interests to improve Click-Through Rate. However, it is very challenging
to accurately capture users' long-term interests from their long-term behavior
sequences and give quick responses from the online serving systems. To meet
such requirements, existing methods "inadvertently" destroy two basic
requirements in long-term sequence modeling: R1) make full use of the entire
sequence to keep the information as much as possible; R2) extract information
from the most relevant behaviors to keep high relevance between learned
interests and current target items. The performance of online serving systems
is significantly affected by incomplete and inaccurate user interest
information obtained by existing methods. To this end, we propose an efficient
two-stage long-term sequence modeling approach, named as EfficieNt Clustering
based twO-stage interest moDEling (ENCODE), consisting of offline extraction
stage and online inference stage. It not only meets the aforementioned two
basic requirements but also achieves a desirable balance between online service
efficiency and precision. Specifically, in the offline extraction stage, ENCODE
clusters the entire behavior sequence and extracts accurate interests. To
reduce the overhead of the clustering process, we design a metric
learning-based dimension reduction algorithm that preserves the relative
pairwise distances of behaviors in the new feature space. While in the online
inference stage, ENCODE takes the off-the-shelf user interests to predict the
associations with target items. Besides, to further ensure the relevance
between user interests and target items, we adopt the same relevance metric
throughout the whole pipeline of ENCODE. The extensive experiment and
comparison with SOTA have demonstrated the effectiveness and efficiency of our
proposed ENCODE.

---

### 4. CASPER: Concept-integrated Sparse Representation for Scientific Retrieval

- **LLM Score**: 6
- **Keyword Score**: 11
- **Authors**: Lam Thanh Do, Linh Van Nguyen, David Fu, Kevin Chen-Chuan Chang
- **URL**: <http://arxiv.org/abs/2508.13394v1>
- **Submitted**: 2025-08-18 23:00:57
- **Comment**: 11 Pages. Code: https://github.com/louisdo/CASPER
- **Topic Keywords**: sparse retrieval, queries, rag, retrieval, search
- **Reason**: The paper proposes a sparse retrieval model for scientific search, utilizing tokens and keyphrases as representation units. While it's not directly related to query understanding, ranking models, or user behavior modeling, it does share some similarities with information retrieval and natural language processing. The focus on scientific literature and concept-based matching is somewhat relevant to the user's interests, but the paper's scope is narrower than what the user typically explores.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Scientific Search using Concept-Integrated Sparse Retrieval Model
- **Aim**: To develop a sparse retrieval model for scientific search that integrates research concepts and keyphrases as representation units
- **Rationale**: To overcome the lack of conceptual representation units and suitable training data in scientific text retrieval
- **Ground**: The proposed model, CASPER, leverages scholarly references, including titles, citation contexts, author-assigned keyphrases, and co-citations to mine training data
- **Experiment**: CASPER is evaluated on eight scientific retrieval benchmarks and achieves competitive performance with strong dense and sparse baselines, and is also effective for keyphrase generation tasks
- **Takeaway**: CASPER outperforms strong baselines and demonstrates the effectiveness of integrating research concepts and keyphrases as representation units in sparse retrieval models for scientific search

#### Abstract
> The exponential growth of scientific literature has made it increasingly
difficult for researchers to keep up with the literature. In an attempt to
alleviate this problem, we propose CASPER, a sparse retrieval model for
scientific search that utilizes tokens and keyphrases as representation units
(i.e. dimensions in the sparse embedding space), enabling it to represent
queries and documents with research concepts and match them at both granular
and conceptual levels. To overcome the lack of suitable training data, we
propose mining training data by leveraging scholarly references (i.e. signals
that capture how research concepts of papers are expressed in different
settings), including titles, citation contexts, author-assigned keyphrases, and
co-citations. CASPER outperforms strong dense and sparse retrieval baselines on
eight scientific retrieval benchmarks. Moreover, we demonstrate that through
simple post-processing, CASPER can be effectively used for the keyphrase
generation tasks, achieving competitive performance with the established
CopyRNN while producing more diverse keyphrases and being nearly four times
faster.

---

### 5. Democratizing News Recommenders: Modeling Multiple Perspectives for News Candidate Generation with VQ-VAE

- **LLM Score**: 6
- **Keyword Score**: 9
- **Authors**: Hardy, Sebastian Pad√≥, Amelie W√ºhrl, Tanise Ceron
- **URL**: <http://arxiv.org/abs/2508.13978v1>
- **Submitted**: 2025-08-19 16:13:54
- **Topic Keywords**: query, click, retrieval, recommend, personalization
- **Reason**: While this paper focuses on news recommenders and diversity-aware algorithms, it does not directly relate to my core interests in Information Retrieval, query understanding, ranking models, and user behavior modeling. However, it does touch on aspects of personalization and candidate generation, which may be indirectly relevant to my work in search technologies and NLP.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Aspect-Aware Candidate Generation (A2CG) for news recommender systems
- **Aim**: To address limitations of current diversity-aware algorithms and improve news recommender systems
- **Rationale**: To tackle diversity at an earlier stage of the recommendation pipeline and enable a flexible trade-off between personalization and diversity
- **Ground**: News recommender systems have the potential to create echo chambers or increase selective exposure, and incorporating diversity as an objective is crucial
- **Experiment**: Evaluation of A2CG on the MIND dataset using traditional and normative diversity metrics, including intra-list diversity, serendipity, novelty, and similarity, as well as the RADio framework metrics
- **Takeaway**: A2CG framework offers versatility in controlling the model to align with specific democratic objectives, promoting transparency in recommendation systems and enabling a flexible trade-off between personalization and diversity

#### Abstract
> Current News Recommender Systems based on past clicks are designed for
engagement, but come at the cost of limiting diversity in the suggested
content. While diversity-aware algorithms exist, they suffer from two major
limitations. First, they fail to account for normative diversity, which
requires fair access to a broad range of perspectives. Second, they typically
apply diversity late in the system's pipeline, after a lot of content has
already been filtered out. Both limitations confine their effectiveness and
prevent them from promoting true normative diversity in news recommendations.
  We propose Aspect-Aware Candidate Generation (A2CG) to address these
limitations. Our framework introduces diversity into the earliest pipeline
stage and uses a configurable mechanism to align diversity with specific
democratic goals. A2CG represents each news article using multiple aspects of
perspectives (e.g., sentiment, political leaning, frame) and uses a Vector
Quantized Variational Autoencoder (VQ-VAE) to create a discrete, multi-faceted
representation. A decoder-only model then learns user preferences over these
aspect codes. We then inject diversity directly by reversing the sign on some
of the query vector's aspects during the candidate retrieval process, ensuring
a more diverse set of candidates.
  Our method, evaluated on the MIND dataset, enables a flexible trade-off
between personalization and diversity early in the recommendation pipeline. It
also generates more novel, diverse, and serendipitous candidates while
effectively taking into account aspects that strengthen democratic values.
These empirical results make it a promising approach for downstream
democratized news recommendation systems.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. UniECS: Unified Multimodal E-Commerce Search Framework with Gated Cross-modal Fusion

- **LLM Score**: 6
- **Keyword Score**: 9
- **Authors**: Zihan Liang, Yufei Ma, ZhiPeng Qian, Huangyu Dai, Zihan Wang, Ben Chen, Chenyi Lei, Yuqing Ding, Han Li
- **URL**: <http://arxiv.org/abs/2508.13843v1>
- **Submitted**: 2025-08-19 14:06:13
- **Comment**: Accepted at CIKM2025 as a long paper
- **Topic Keywords**: click, click-through rate, retrieval, commerce, e-commerce, search
- **Reason**: The paper focuses on multimodal e-commerce search, which is relevant to your background in e-commerce domain. The use of gated cross-modal fusion and comprehensive training strategy is also related to your interest in query understanding and ranking models. However, the paper's primary focus on e-commerce search and multimodal fusion is not directly aligned with your core research themes in information retrieval and search technologies.

#### Abstract
> Current e-commerce multimodal retrieval systems face two key limitations:
they optimize for specific tasks with fixed modality pairings, and lack
comprehensive benchmarks for evaluating unified retrieval approaches. To
address these challenges, we introduce UniECS, a unified multimodal e-commerce
search framework that handles all retrieval scenarios across image, text, and
their combinations. Our work makes three key contributions. First, we propose a
flexible architecture with a novel gated multimodal encoder that uses adaptive
fusion mechanisms. This encoder integrates different modality representations
while handling missing modalities. Second, we develop a comprehensive training
strategy to optimize learning. It combines cross-modal alignment loss (CMAL),
cohesive local alignment loss (CLAL), intra-modal contrastive loss (IMCL), and
adaptive loss weighting. Third, we create M-BEER, a carefully curated
multimodal benchmark containing 50K product pairs for e-commerce search
evaluation. Extensive experiments demonstrate that UniECS consistently
outperforms existing methods across four e-commerce benchmarks with fine-tuning
or zero-shot evaluation. On our M-BEER bench, UniECS achieves substantial
improvements in cross-modal tasks (up to 28\% gain in R@10 for text-to-image
retrieval) while maintaining parameter efficiency (0.2B parameters) compared to
larger models like GME-Qwen2VL (2B) and MM-Embed (8B). Furthermore, we deploy
UniECS in the e-commerce search platform of Kuaishou Inc. across two search
scenarios, achieving notable improvements in Click-Through Rate (+2.74\%) and
Revenue (+8.33\%). The comprehensive evaluation demonstrates the effectiveness
of our approach in both experimental and real-world settings. Corresponding
codes, models and datasets will be made publicly available at
https://github.com/qzp2018/UniECS.

### 7. Ask Good Questions for Large Language Models

- **LLM Score**: 6
- **Keyword Score**: 5
- **Authors**: Qi Wu, Zhongqi Lu
- **URL**: <http://arxiv.org/abs/2508.14025v1>
- **Submitted**: 2025-08-19 17:31:42
- **Topic Keywords**: information retrieval, retrieval
- **Reason**: The paper explores the Ask-Good-Question framework for large language models, which is related to information retrieval and query understanding. However, the focus is on dialog systems and question generation, which is not directly aligned with the user's primary interest in ranking models and user behavior modeling. The paper's relevance is somewhat related, but not a central match.

#### Abstract
> Recent advances in large language models (LLMs) have significantly improved
the performance of dialog systems, yet current approaches often fail to provide
accurate guidance of topic due to their inability to discern user confusion in
related concepts. To address this, we introduce the Ask-Good-Question (AGQ)
framework, which features an improved Concept-Enhanced Item Response Theory
(CEIRT) model to better identify users' knowledge levels. Our contributions
include applying the CEIRT model along with LLMs to directly generate guiding
questions based on the inspiring text, greatly improving information retrieval
efficiency during the question & answer process. Through comparisons with other
baseline methods, our approach outperforms by significantly enhencing the
users' information retrieval experiences.

### 8. ReviewGraph: A Knowledge Graph Embedding Based Framework for Review Rating Prediction with Sentiment Features

- **LLM Score**: 6
- **Keyword Score**: 5
- **Authors**: A. J. W. de Vink, Natalia Amat-Lefort, Lifeng Han
- **URL**: <http://arxiv.org/abs/2508.13953v1>
- **Submitted**: 2025-08-19 15:44:27
- **Topic Keywords**: rag, retrieval, search
- **Reason**: The paper proposes a framework for review rating prediction using knowledge graph embeddings and sentiment features, which is related to information retrieval and natural language processing. While it doesn't directly address query understanding, ranking models, or user behavior modeling, it does leverage graph-based representations, which is a relevant area of research in IR. However, the focus on review rating prediction and sentiment analysis is not a central match with the user's primary research interests.

#### Abstract
> In the hospitality industry, understanding the factors that drive customer
review ratings is critical for improving guest satisfaction and business
performance. This work proposes ReviewGraph for Review Rating Prediction (RRP),
a novel framework that transforms textual customer reviews into knowledge
graphs by extracting (subject, predicate, object) triples and associating
sentiment scores. Using graph embeddings (Node2Vec) and sentiment features, the
framework predicts review rating scores through machine learning classifiers.
We compare ReviewGraph performance with traditional NLP baselines (such as Bag
of Words, TF-IDF, and Word2Vec) and large language models (LLMs), evaluating
them in the HotelRec dataset. In comparison to the state of the art literature,
our proposed model performs similar to their best performing model but with
lower computational cost (without ensemble).
  While ReviewGraph achieves comparable predictive performance to LLMs and
outperforms baselines on agreement-based metrics such as Cohen's Kappa, it
offers additional advantages in interpretability, visual exploration, and
potential integration into Retrieval-Augmented Generation (RAG) systems. This
work highlights the potential of graph-based representations for enhancing
review analytics and lays the groundwork for future research integrating
advanced graph neural networks and fine-tuned LLM-based extraction methods. We
will share ReviewGraph output and platform open-sourced on our GitHub page
https://github.com/aaronlifenghan/ReviewGraph

### 9. The illusion of a perfect metric: Why evaluating AI's words is harder than it looks

- **LLM Score**: 6
- **Keyword Score**: 5
- **Authors**: Maria Paz Oliva, Adriana Correia, Ivan Vankov, Viktor Botev
- **URL**: <http://arxiv.org/abs/2508.13816v1>
- **Submitted**: 2025-08-19 13:22:41
- **Comment**: 11 pages, 1 figure. Accepted to RANLP 2025
- **Topic Keywords**: rag, retrieval, search
- **Reason**: The paper is somewhat related to the user's interests in Natural Language Processing (NLP) and Information Retrieval (IR), as it discusses the evaluation of Natural Language Generation (NLG) and Retrieval Augmented Generation (RAG). However, the focus on automatic evaluation metrics and their limitations is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Evaluating Natural Language Generation (NLG) is crucial for the practical
adoption of AI, but has been a longstanding research challenge. While human
evaluation is considered the de-facto standard, it is expensive and lacks
scalability. Practical applications have driven the development of various
automatic evaluation metrics (AEM), designed to compare the model output with
human-written references, generating a score which approximates human judgment.
Over time, AEMs have evolved from simple lexical comparisons, to semantic
similarity models and, more recently, to LLM-based evaluators. However, it
seems that no single metric has emerged as a definitive solution, resulting in
studies using different ones without fully considering the implications. This
paper aims to show this by conducting a thorough examination of the
methodologies of existing metrics, their documented strengths and limitations,
validation methods, and correlations with human judgment. We identify several
key challenges: metrics often capture only specific aspects of text quality,
their effectiveness varies by task and dataset, validation practices remain
unstructured, and correlations with human judgment are inconsistent.
Importantly, we find that these challenges persist in the most recent type of
metric, LLM-as-a-Judge, as well as in the evaluation of Retrieval Augmented
Generation (RAG), an increasingly relevant task in academia and industry. Our
findings challenge the quest for the 'perfect metric'. We propose selecting
metrics based on task-specific needs and leveraging complementary evaluations
and advocate that new metrics should focus on enhanced validation
methodologies.

### 10. CARE: Contextual Adaptation of Recommenders for LLM-based Conversational Recommendation

- **LLM Score**: 4
- **Keyword Score**: 13
- **Authors**: Chuang Li, Yang Deng, Hengchang Hu, See-Kiong Ng, Min-Yen Kan, Haizhou Li
- **URL**: <http://arxiv.org/abs/2508.13889v1>
- **Submitted**: 2025-08-19 14:53:30
- **Topic Keywords**: queries, ranking, rerank, rag, recommend, rank
- **Reason**: The paper focuses on recommender systems, which is a related topic, but it doesn't directly address query understanding, ranking models, or user behavior modeling, which are core interests. The use of large language models and contextual adaptation is interesting, but the paper's primary focus is on conversational recommendation, which is not a direct match with the user's research themes.

#### Abstract
> We tackle the challenge of integrating large language models (LLMs) with
external recommender systems to enhance domain expertise in conversational
recommendation (CRS). Current LLM-based CRS approaches primarily rely on zero-
or few-shot methods for generating item recommendations based on user queries,
but this method faces two significant challenges: (1) without domain-specific
adaptation, LLMs frequently recommend items not in the target item space,
resulting in low recommendation accuracy; and (2) LLMs largely rely on dialogue
context for content-based recommendations, neglecting the collaborative
relationships among entities or item sequences. To address these limitations,
we introduce the CARE (Contextual Adaptation of Recommenders) framework. CARE
customizes LLMs for CRS tasks, and synergizes them with external recommendation
systems. CARE (a) integrates external recommender systems as domain experts,
producing recommendations through entity-level insights, and (b) enhances those
recommendations by leveraging contextual information for more accurate and
unbiased final recommendations using LLMs. Our results demonstrate that
incorporating external recommender systems with entity-level information
significantly enhances recommendation accuracy of LLM-based CRS by an average
of 54% and 25% for ReDial and INSPIRED datasets. The most effective strategy in
the CARE framework involves LLMs selecting and reranking candidate items that
external recommenders provide based on contextual insights. Our analysis
indicates that the CARE framework effectively addresses the identified
challenges and mitigates the popularity bias in the external recommender.

### 11. AdaptJobRec: Enhancing Conversational Career Recommendation through an LLM-Powered Agentic System

- **LLM Score**: 4
- **Keyword Score**: 11
- **Authors**: Qixin Wang, Dawei Wang, Kun Chen, Yaowei Hu, Puneet Girdhar, Ruoteng Wang, Aadesh Gupta, Chaitanya Devella, Wenlai Guo, Shangwen Huang, Bachir Aoun, Greg Hayworth, Han Li, Xintao Wu
- **URL**: <http://arxiv.org/abs/2508.13423v1>
- **Submitted**: 2025-08-19 00:44:25
- **Topic Keywords**: query, queries, rag, retrieval, recommend
- **Reason**: Although the paper discusses recommendation systems, its focus on conversational career recommendation and agentic systems with response latency optimization is somewhat related to my interests in information retrieval and search technologies. However, the topic is more geared towards recommender systems, which is a secondary interest of mine, and lacks direct connections to my primary focus on query understanding, ranking models, and user behavior modeling in the context of e-commerce or other domains.

#### Abstract
> In recent years, recommendation systems have evolved from providing a single
list of recommendations to offering a comprehensive suite of topic focused
services. To better accomplish this task, conversational recommendation systems
(CRS) have progressed from basic retrieval augmented LLM generation to agentic
systems with advanced reasoning and self correction capabilities. However,
agentic systems come with notable response latency, a longstanding challenge
for conversational recommendation systems. To balance the trade off between
handling complex queries and minimizing latency, we propose AdaptJobRec, the
first conversational job recommendation system that leverages autonomous agent
to integrate personalized recommendation algorithm tools. The system employs a
user query complexity identification mechanism to minimize response latency.
For straightforward queries, the agent directly selects the appropriate tool
for rapid responses. For complex queries, the agent uses the memory processing
module to filter chat history for relevant content, then passes the results to
the intelligent task decomposition planner, and finally executes the tasks
using personalized recommendation tools. Evaluation on Walmart's real world
career recommendation scenarios demonstrates that AdaptJobRec reduces average
response latency by up to 53.3% compared to competitive baselines, while
significantly improving recommendation accuracy.

### 12. Query Logs Analytics: A Aystematic Literature Review

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Dihia Lanasri
- **URL**: <http://arxiv.org/abs/2508.13949v1>
- **Submitted**: 2025-08-19 15:38:13
- **Topic Keywords**: query, rag, recommend, search
- **Reason**: The paper's focus on query logs analytics and log usage is somewhat related to information retrieval, but it does not directly address query understanding, ranking models, or user behavior modeling. The paper's scope is broader, covering various domains and log types, which may be of interest to those working in e-commerce or other industries, but it does not specifically address real-time relevance optimization or deep semantic understanding.

#### Abstract
> In the digital era, user interactions with various resources such as
databases, data warehouses, websites, and knowledge graphs (KGs) are
increasingly mediated through digital platforms. These interactions leave
behind digital traces, systematically captured in the form of logs. Logs, when
effectively exploited, provide high value across industry and academia,
supporting critical services (e.g., recovery and security), user-centric
applications (e.g., recommender systems), and quality-of-service improvements
(e.g., performance optimization). Despite their importance, research on log
usage remains fragmented across domains, and no comprehensive study currently
consolidates existing efforts. This paper presents a systematic survey of log
usage, focusing on Database (DB), Data Warehouse (DW), Web, and KG logs. More
than 300 publications were analyzed to address three central questions: (1) do
different types of logs share common structural and functional characteristics?
(2) are there standard pipelines for their usage? (3) which constraints and
non-functional requirements (NFRs) guide their exploitation?. The survey
reveals a limited number of end-to-end approaches, the absence of
standardization across log usage pipelines, and the existence of shared
structural elements among different types of logs. By consolidating existing
knowledge, identifying gaps, and highlighting opportunities, this survey
provides researchers and practitioners with a comprehensive overview of log
usage and sheds light on promising directions for future research, particularly
regarding the exploitation and democratization of KG logs.

### 13. MUFFIN: Mixture of User-Adaptive Frequency Filtering for Sequential Recommendation

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Ilwoong Baek, Mincheol Yoon, Seongmin Park, Jongwuk Lee
- **URL**: <http://arxiv.org/abs/2508.13670v1>
- **Submitted**: 2025-08-19 09:16:15
- **Comment**: Accepted by CIKM 2025
- **Topic Keywords**: rag, user behavior, ctr, recommend
- **Reason**: While the paper MUFFIN: Mixture of User-Adaptive Frequency Filtering for Sequential Recommendation deals with sequential recommendation, it focuses on frequency-domain analysis, which is not directly related to my core interests in Information Retrieval, query understanding, and ranking models. Although it does involve user behavior modeling, the scope is limited to sequential recommendation, and the paper does not seem to address deep semantic understanding or real-time relevance optimization, which are key aspects of my research interests.

#### Abstract
> Sequential recommendation (SR) aims to predict users' subsequent interactions
by modeling their sequential behaviors. Recent studies have explored frequency
domain analysis, which effectively models periodic patterns in user sequences.
However, existing frequency-domain SR models still face two major drawbacks:
(i) limited frequency band coverage, often missing critical behavioral patterns
in a specific frequency range, and (ii) lack of personalized frequency
filtering, as they apply an identical filter for all users regardless of their
distinct frequency characteristics. To address these challenges, we propose a
novel frequency-domain model, Mixture of User-adaptive Frequency FIlteriNg
(MUFFIN), operating through two complementary modules. (i) The global filtering
module (GFM) handles the entire frequency spectrum to capture comprehensive
behavioral patterns. (ii) The local filtering module (LFM) selectively
emphasizes important frequency bands without excluding information from other
ranges. (iii) In both modules, the user-adaptive filter (UAF) is adopted to
generate user-specific frequency filters tailored to individual unique
characteristics. Finally, by aggregating both modules, MUFFIN captures diverse
user behavioral patterns across the full frequency spectrum. Extensive
experiments show that MUFFIN consistently outperforms state-of-the-art
frequency-domain SR models over five benchmark datasets. The source code is
available at https://github.com/ilwoong100/MUFFIN.

### 14. Input Time Scaling

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Rapheal Huang, Weilong Guo
- **URL**: <http://arxiv.org/abs/2508.13654v2>
- **Submitted**: 2025-08-19 09:04:13
- **Topic Keywords**: query, queries, search
- **Reason**: Although the paper presents a new scaling paradigm in large language models, it does not directly relate to the user's primary focus on Information Retrieval, query understanding, and ranking models. The topic of input time scaling in LLMs is more relevant to Natural Language Processing and NLP-related topics, but it does not seem to directly impact the user's core research interests.

#### Abstract
> Current Large Language Models (LLMs) are usually post-trained on large-scale
carefully curated datasets (data & training scaling) and doing reasoning in
test time (inference time scaling). In this work, we present a new scaling
paradigm, Input Time Scaling, to complement previous scaling methods by putting
resources on queries (input time). During training and testing, we combine
meta-knowledge from LLMs to refine inputs with different strategies. We also
find a new phenomenon, training-testing co-design there. We need to apply query
strategies during both training and testing. Only applying strategies on
training or testing would seriously degrade the performance. We are also
surprised to find that seemingly low data quality datasets can gain high
performance. Adding irrelevant information to the queries, randomly selecting
examples from a minimally filtered dataset, can even perform the best. These
findings contradict the widely held inductive bias, "garbage in, garbage out".
Curating datasets with seemingly high-quality data can even potentially limit
the performance ceiling. In addition, models trained on more data with similar
quality (15k VS 1k) perform worse, simple dataset size scaling should also be
carefully inspected. The good news is that our findings are compatible with the
Less is More phenomenon. A small set of examples is enough to evoke high-level
reasoning ability. With experiments on models trained on Qwen2.5-32B-Instruct,
we are able to reach SOTA performance among 32B models on AIME24(76.7%) and
AIME25(76.7%) pass@1. We can further achieve AIME24(76.7%) and AIME25(80%) with
a majority vote of three models. Starting from DeepSeek-R1-Distill-Qwen-32B,
the best result would be 86.7% on AIME24 and 76.7% on AIME25. To facilitate
reproducibility and further research, we are working on open-source our
datasets, data pipelines, evaluation results, and checkpoints.

### 15. Heterogeneous Influence Maximization in User Recommendation

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Hongru Hou, Jiachen Sun, Wenqing Lin, Wendong Bi, Xiangrong Wang, Deqing Yang
- **URL**: <http://arxiv.org/abs/2508.13517v1>
- **Submitted**: 2025-08-19 05:20:48
- **Comment**: Accepted in CIKM 2025
- **Topic Keywords**: rerank, rag, recommend, rank
- **Reason**: Although the paper deals with recommendation systems, its focus on influence maximization and heterogenous influence is not directly related to my core interests in information retrieval, search technologies, and user behavior modeling. While it mentions user behavior, the scope is limited to interaction willingness and spread coverage, which is not a deep semantic understanding or real-time relevance optimization, as I'm interested in.

#### Abstract
> User recommendation systems enhance user engagement by encouraging users to
act as inviters to interact with other users (invitees), potentially fostering
information propagation. Conventional recommendation methods typically focus on
modeling interaction willingness. Influence-Maximization (IM) methods focus on
identifying a set of users to maximize the information propagation. However,
existing methods face two significant challenges. First, recommendation methods
fail to unleash the candidates' spread capability. Second, IM methods fail to
account for the willingness to interact. To solve these issues, we propose two
models named HeteroIR and HeteroIM. HeteroIR provides an intuitive solution to
unleash the dissemination potential of user recommendation systems. HeteroIM
fills the gap between the IM method and the recommendation task, improving
interaction willingness and maximizing spread coverage. The HeteroIR introduces
a two-stage framework to estimate the spread profits. The HeteroIM
incrementally selects the most influential invitee to recommend and rerank
based on the number of reverse reachable (RR) sets containing inviters and
invitees. RR set denotes a set of nodes that can reach a target via
propagation. Extensive experiments show that HeteroIR and HeteroIM
significantly outperform the state-of-the-art baselines with the p-value <
0.05. Furthermore, we have deployed HeteroIR and HeteroIM in Tencent's online
gaming platforms and gained an 8.5\% and 10\% improvement in the online A/B
test, respectively. Implementation codes are available at
https://github.com/socialalgo/HIM.

### 16. ALIGN: Word Association Learning for Cross-Cultural Generalization in Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Chunhua Liu, Kabir Manandhar Shrestha, Sukai Huang
- **URL**: <http://arxiv.org/abs/2508.13426v1>
- **Submitted**: 2025-08-19 00:55:20
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on cross-cultural generalization in large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the importance of cultural alignment in AI models, the paper's primary concern is not on ranking models or user behavior modeling, making it only loosely relevant to the user's research interests.

#### Abstract
> As large language models (LLMs) increasingly mediate cross-cultural
communication, their behavior still reflects the distributional bias of the
languages and viewpoints that are over-represented in their pre-training
corpora. Yet, it remains a challenge to model and align culture due to limited
cultural knowledge and a lack of exploration into effective learning
approaches. We introduce a cost-efficient, cognitively grounded remedy:
parameter-efficient fine-tuning on native speakers' free word-association
norms, which encode implicit cultural schemas. Leveraging English-US and
Mandarin associations from the Small-World-of-Words project, we adapt
Llama-3.1-8B and Qwen-2.5-7B via supervised fine-tuning (SFT) and PPO-based
preference optimization. SFT boosts held-out association Precision at 5 by
16-20% in English and 43-165% in Mandarin, lifts median concreteness by +0.20,
and attains human-level valence and arousal. These lexical gains transfer: on
World-Values-Survey questions, fine-tuned models shift answer distributions
toward the target culture, and on a 50-item high-tension subset, Qwen's
Chinese-aligned responses double while Llama's US bias drops by one-third. Our
7-8B models rival or beat vanilla 70B baselines, showing that a few million
culture-grounded associations can instill value alignment without costly
retraining. Our work highlights both the promise and the need for future
research grounded in human cognition in improving cultural alignment in AI
models.

### 17. Chunks as Arms: Multi-Armed Bandit-Guided Sampling for Long-Context LLM Preference Optimization

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Shaohua Duan, Xinze Li, Zhenghao Liu, Xiaoyuan Yi, Yukun Yan, Shuo Wang, Yu Gu, Ge Yu, Maosong Sun
- **URL**: <http://arxiv.org/abs/2508.13993v1>
- **Submitted**: 2025-08-19 16:33:55
- **Topic Keywords**: rag
- **Reason**: The paper explores a novel framework for long-context modeling, leveraging a Multi-Armed Bandit rollout strategy to identify informative chunks for sampling high-quality responses. While it touches on language models and optimization, it does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval and Search technologies.

#### Abstract
> Long-context modeling is critical for a wide range of real-world tasks,
including long-context question answering, summarization, and complex reasoning
tasks. Recent studies have explored fine-tuning Large Language Models (LLMs)
with synthetic data to enhance their long-context capabilities. However, the
effectiveness of such approaches is often limited by the low diversity and
factual inconsistencies in the generated data. To address these challenges, we
propose LongMab-PO, a novel framework that leverages a Multi-Armed Bandit (MAB)
rollout strategy to identify the most informative chunks from the given long
context for sampling high-quality and diverse responses and constructing
preference data pairs for Direct Preference Optimization (DPO) training.
Specifically, we treat context chunks as arms of MAB, select chunks based on
their expected reward scores to input into LLMs to generate responses, and
iteratively update these scores based on reward feedback. This exploration and
exploitation process enables the model to focus on the most relevant context
segments, thereby generating and collecting high-quality and diverse responses.
Finally, we collect these generated responses from the rollout process and
apply the DPO method to further optimize the LLM. Experimental results show
that LongMab-PO significantly improves the diversity and quality of preference
data pairs, achieving state-of-the-art performance on long-context reasoning
benchmarks. All code and data will be released on
https://github.com/NEUIR/LongMab-PO.

### 18. The Promise of Large Language Models in Digital Health: Evidence from Sentiment Analysis in Online Health Communities

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Xiancheng Li, Georgios D. Karampatakis, Helen E. Wood, Chris J. Griffiths, Borislava Mihaylova, Neil S. Coulson, Alessio Pasinato, Pietro Panzarasa, Marco Viviani, Anna De Simoni
- **URL**: <http://arxiv.org/abs/2508.14032v1>
- **Submitted**: 2025-08-19 17:54:56
- **Topic Keywords**: search
- **Reason**: While the paper explores large language models in a new domain (digital health), its focus on sentiment analysis and expert knowledge integration is not directly related to information retrieval, search technologies, or user behavior modeling, which are the user's primary research interests. However, the paper's use of large language models and in-context learning may be of indirect relevance to the user's work on ranking models and deep semantic understanding, making it somewhat related but not a central match.

#### Abstract
> Digital health analytics face critical challenges nowadays. The sophisticated
analysis of patient-generated health content, which contains complex emotional
and medical contexts, requires scarce domain expertise, while traditional ML
approaches are constrained by data shortage and privacy limitations in
healthcare settings. Online Health Communities (OHCs) exemplify these
challenges with mixed-sentiment posts, clinical terminology, and implicit
emotional expressions that demand specialised knowledge for accurate Sentiment
Analysis (SA). To address these challenges, this study explores how Large
Language Models (LLMs) can integrate expert knowledge through in-context
learning for SA, providing a scalable solution for sophisticated health data
analysis. Specifically, we develop a structured codebook that systematically
encodes expert interpretation guidelines, enabling LLMs to apply
domain-specific knowledge through targeted prompting rather than extensive
training. Six GPT models validated alongside DeepSeek and LLaMA 3.1 are
compared with pre-trained language models (BioBERT variants) and lexicon-based
methods, using 400 expert-annotated posts from two OHCs. LLMs achieve superior
performance while demonstrating expert-level agreement. This high agreement,
with no statistically significant difference from inter-expert agreement
levels, suggests knowledge integration beyond surface-level pattern
recognition. The consistent performance across diverse LLM models, supported by
in-context learning, offers a promising solution for digital health analytics.
This approach addresses the critical challenge of expert knowledge shortage in
digital health research, enabling real-time, expert-quality analysis for
patient monitoring, intervention assessment, and evidence-based health
strategies.

### 19. MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Xian Gao, Jiacheng Ruan, Zongyun Zhang, Jingsheng Gao, Ting Liu, Yuzhuo Fu
- **URL**: <http://arxiv.org/abs/2508.14146v1>
- **Submitted**: 2025-08-19 16:37:19
- **Comment**: Work in progress
- **Topic Keywords**: search
- **Reason**: While the paper discusses peer review automation, it focuses on Large Language Models (LLMs) and multimodal content, rather than traditional information retrieval and search technologies. The topic is related to NLP, but does not directly align with the user's primary interests in query understanding, ranking models, and user behavior modeling. The paper's scope is too broad to be highly relevant to the user's research focus.

#### Abstract
> With the rapid growth of academic publications, peer review has become an
essential yet time-consuming responsibility within the research community.
Large Language Models (LLMs) have increasingly been adopted to assist in the
generation of review comments; however, current LLM-based review tasks lack a
unified evaluation benchmark to rigorously assess the models' ability to
produce comprehensive, accurate, and human-aligned assessments, particularly in
scenarios involving multimodal content such as figures and tables. To address
this gap, we propose \textbf{MMReview}, a comprehensive benchmark that spans
multiple disciplines and modalities. MMReview includes multimodal content and
expert-written review comments for 240 papers across 17 research domains within
four major academic disciplines: Artificial Intelligence, Natural Sciences,
Engineering Sciences, and Social Sciences. We design a total of 13 tasks
grouped into four core categories, aimed at evaluating the performance of LLMs
and Multimodal LLMs (MLLMs) in step-wise review generation, outcome
formulation, alignment with human preferences, and robustness to adversarial
input manipulation. Extensive experiments conducted on 16 open-source models
and 5 advanced closed-source models demonstrate the thoroughness of the
benchmark. We envision MMReview as a critical step toward establishing a
standardized foundation for the development of automated peer review systems.

### 20. Understanding Distribution Structure on Calibrated Recommendation Systems

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Diego Correa da Silva, Denis Robson Dantas Boaventura, Mayki dos Santos Oliveira, Eduardo Ferreira da Silva, Joel Machado Pires, Frederico Ara√∫jo Dur√£o
- **URL**: <http://arxiv.org/abs/2508.13568v1>
- **Submitted**: 2025-08-19 06:59:10
- **Topic Keywords**: recommend
- **Reason**: While the paper explores recommender systems, it does not directly relate to my core interests in Information Retrieval, Search technologies, and ranking models. The focus on calibrated recommendation systems and distribution structure does not seem to involve deep semantic understanding, real-time relevance optimization, or user behavior modeling, which are key aspects of my research interests.

#### Abstract
> Traditional recommender systems aim to generate a recommendation list
comprising the most relevant or similar items to the user's profile. These
approaches can create recommendation lists that omit item genres from the less
prominent areas of a user's profile, thereby undermining the user's experience.
To solve this problem, the calibrated recommendation system provides a
guarantee of including less representative areas in the recommended list. The
calibrated context works with three distributions. The first is from the user's
profile, the second is from the candidate items, and the last is from the
recommendation list. These distributions are G-dimensional, where G is the
total number of genres in the system. This high dimensionality requires a
different evaluation method, considering that traditional recommenders operate
in a one-dimensional data space. In this sense, we implement fifteen models
that help to understand how these distributions are structured. We evaluate the
users' patterns in three datasets from the movie domain. The results indicate
that the models of outlier detection provide a better understanding of the
structures. The calibrated system creates recommendation lists that act
similarly to traditional recommendation lists, allowing users to change their
groups of preferences to the same degree.

### 21. Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values Understanding

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Maciej Skorski, Alina Landowska
- **URL**: <http://arxiv.org/abs/2508.13804v1>
- **Submitted**: 2025-08-19 13:05:48
- **Topic Keywords**: queries, rag, rank
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. While it touches on Natural Language Processing, it focuses on moral values understanding and large language models, which is not a primary interest area for you.

#### Abstract
> How do large language models understand moral dimensions compared to humans?
  This first large-scale Bayesian evaluation of market-leading language models
provides the answer. In contrast to prior work using deterministic ground truth
(majority or inclusion rules), we model annotator disagreements to capture both
aleatoric uncertainty (inherent human disagreement) and epistemic uncertainty
(model domain sensitivity). We evaluate top language models (Claude Sonnet 4,
DeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from ~700 annotators on
100K+ texts spanning social media, news, and forums.
  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing
that AI models typically rank among the top 25\% of human annotators, achieving
much better-than-average balanced accuracy. Importantly, we find that AI
produces far fewer false negatives than humans, highlighting their more
sensitive moral detection capabilities.

### 22. EEG-MedRAG: Enhancing EEG-based Clinical Decision-Making via Hierarchical Hypergraph Retrieval-Augmented Generation

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Yi Wang, Haoran Luo, Lu Meng
- **URL**: <http://arxiv.org/abs/2508.13735v1>
- **Submitted**: 2025-08-19 11:12:58
- **Topic Keywords**: rag, ctr, retrieval
- **Reason**: The paper focuses on EEG-based clinical decision-making, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on medical applications and clinical QA benchmark also falls outside the user's e-commerce domain experience.

#### Abstract
> With the widespread application of electroencephalography (EEG) in
neuroscience and clinical practice, efficiently retrieving and semantically
interpreting large-scale, multi-source, heterogeneous EEG data has become a
pressing challenge. We propose EEG-MedRAG, a three-layer hypergraph-based
retrieval-augmented generation framework that unifies EEG domain knowledge,
individual patient cases, and a large-scale repository into a traversable n-ary
relational hypergraph, enabling joint semantic-temporal retrieval and
causal-chain diagnostic generation. Concurrently, we introduce the first
cross-disease, cross-role EEG clinical QA benchmark, spanning seven disorders
and five authentic clinical perspectives. This benchmark allows systematic
evaluation of disease-agnostic generalization and role-aware contextual
understanding. Experiments show that EEG-MedRAG significantly outperforms
TimeRAG and HyperGraphRAG in answer accuracy and retrieval, highlighting its
strong potential for real-world clinical decision support. Our data and code
are publicly available at https://github.com/yi9206413-boop/EEG-MedRAG.

### 23. Bites of Tomorrow: Personalized Recommendations for a Healthier and Greener Plate

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Jiazheng Jing, Yinan Zhang, Chunyan Miao
- **URL**: <http://arxiv.org/abs/2508.13870v1>
- **Submitted**: 2025-08-19 14:35:37
- **Topic Keywords**: rag, recommend, search
- **Reason**: Although the paper mentions recommendation systems, its focus on sustainability and green living is outside the scope of the user's primary interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper does not seem to relate to the user's background in e-commerce or their interest in deep semantic understanding and real-time relevance optimization.

#### Abstract
> The recent emergence of extreme climate events has significantly raised
awareness about sustainable living. In addition to developing energy-saving
materials and technologies, existing research mainly relies on traditional
methods that encourage behavioral shifts towards sustainability, which can be
overly demanding or only passively engaging. In this work, we propose to employ
recommendation systems to actively nudge users toward more sustainable choices.
We introduce Green Recommender Aligned with Personalized Eating (GRAPE), which
is designed to prioritize and recommend sustainable food options that align
with users' evolving preferences. We also design two innovative Green Loss
functions that cater to green indicators with either uniform or differentiated
priorities, thereby enhancing adaptability across a range of scenarios.
Extensive experiments on a real-world dataset demonstrate the effectiveness of
our GRAPE.

### 24. MGT-Prism: Enhancing Domain Generalization for Machine-Generated Text Detection via Spectral Alignment

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Shengchao Liu, Xiaoming Liu, Chengzhengxu Li, Zhaohan Zhang, Guoxin Ma, Yu Lan, Shuai Xiao
- **URL**: <http://arxiv.org/abs/2508.13768v1>
- **Submitted**: 2025-08-19 12:09:45
- **Topic Keywords**: rag, ctr
- **Reason**: The paper focuses on Machine-Generated Text detection, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it involves text analysis, the approach is more focused on domain generalization and spectral alignment, which is not a key area of interest for you.

#### Abstract
> Large Language Models have shown growing ability to generate fluent and
coherent texts that are highly similar to the writing style of humans. Current
detectors for Machine-Generated Text (MGT) perform well when they are trained
and tested in the same domain but generalize poorly to unseen domains, due to
domain shift between data from different sources. In this work, we propose
MGT-Prism, an MGT detection method from the perspective of the frequency domain
for better domain generalization. Our key insight stems from analyzing text
representations in the frequency domain, where we observe consistent spectral
patterns across diverse domains, while significant discrepancies in magnitude
emerge between MGT and human-written texts (HWTs). The observation initiates
the design of a low frequency domain filtering module for filtering out the
document-level features that are sensitive to domain shift, and a dynamic
spectrum alignment strategy to extract the task-specific and domain-invariant
features for improving the detector's performance in domain generalization.
Extensive experiments demonstrate that MGT-Prism outperforms state-of-the-art
baselines by an average of 0.90% in accuracy and 0.92% in F1 score on 11 test
datasets across three domain-generalization scenarios.

### 25. TASER: Table Agents for Schema-guided Extraction and Recommendation

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Nicole Cho, Kirsty Fielding, William Watson, Sumitra Ganesh, Manuela Veloso
- **URL**: <http://arxiv.org/abs/2508.13404v2>
- **Submitted**: 2025-08-18 23:48:22
- **Comment**: Withdrawn due to missing key sections in the paper
- **Topic Keywords**: rag, recommend, search
- **Reason**: The paper focuses on table extraction and recommendation, which is not directly related to information retrieval, search technologies, or query understanding. While it involves schema-guided extraction, the context is financial tables, which is not a primary area of interest for the user.

#### Abstract
> Real-world financial documents report essential information about an entity's
financial holdings that can span millions of different financial instrument
types. Yet, these details are often buried in messy, multi-page, fragmented
tables - for example, 99.4% of the tables in our dataset have no bounding boxes
with the maximum number of rows amounting to 426 per table across 44 pages. To
tackle these unique challenges from real-world tables, we present a
continuously learning, agentic table extraction system, TASER (Table Agents for
Schema-guided Extraction and Recommendation) that extracts highly unstructured,
multi-page, heterogeneous tables into normalized, schema-conforming outputs.
Our table agents execute on table detection, classification, extraction, and
recommendations by leveraging an initial schema. Then, our Recommender Agent
reviews the outputs, recommends schema revisions, and decides on the final
recommendations, enabling TASER to outperform existing table detection models
such as Table Transformer by 10.1%. Within this continuous learning process, we
highlight that larger batch sizes result in a 104.3% increase in schema
recommendations that are actionable and utilized, resulting in a 9.8% increase
in extracted holdings - highlighting the importance of a continuous learning
process. To train TASER, we have manually labeled 22,584 pages (28,150,449
tokens), 3,213 tables for $731,685,511,687 of holdings culminating in one of
the first real financial table datasets. We release our dataset TASERTab to
enable the research community to access real-world financial tables and
outputs. Our results highlight the promise of agentic, schema-guided extraction
systems for robust understanding of real-world financial tables.

### 26. ProMed: Shapley Information Gain Guided Reinforcement Learning for Proactive Medical LLMs

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Hongxin Ding, Baixiang Huang, Yue Fang, Weibin Liao, Xinke Jiang, Zheng Li, Junfeng Zhao, Yasha Wang
- **URL**: <http://arxiv.org/abs/2508.13514v1>
- **Submitted**: 2025-08-19 05:01:40
- **Topic Keywords**: rag, search
- **Reason**: While the paper mentions 'Large Language Models' and 'reinforcement learning', it appears to be primarily focused on medical question-answering and clinical consultation, rather than information retrieval, search, or user behavior modeling. The topics and methodologies discussed in the paper do not align with the user's core research interests in IR, NLP, and related topics.

#### Abstract
> Interactive medical questioning is essential in real-world clinical
consultations, where physicians must actively gather information from patients.
While medical Large Language Models (LLMs) have shown impressive capabilities
in static medical question answering, they predominantly operate under a
reactive paradigm: generating answers directly without seeking additional
information, which risks incorrect diagnoses in such interactive settings. To
address this limitation, we propose ProMed, a reinforcement learning (RL)
framework that transitions medical LLMs toward a proactive paradigm, equipping
them with the ability to ask clinically valuable questions before
decision-making. At the core of ProMed is the Shapley Information Gain (SIG)
reward, which quantifies the clinical utility of each question by combining the
amount of newly acquired information with its contextual importance, estimated
via Shapley values. We integrate SIG into a two-stage training pipeline: (1)
SIG-Guided Model Initialization uses Monte Carlo Tree Search (MCTS) to
construct high-reward interaction trajectories to supervise the model, and (2)
SIG-Augmented Policy Optimization, which integrates SIG and enhances RL with a
novel SIG-guided Reward Distribution Mechanism that assigns higher rewards to
informative questions for targeted optimization. Extensive experiments on two
newly curated partial-information medical benchmarks demonstrate that ProMed
significantly outperforms state-of-the-art methods by an average of 6.29% and
delivers a 54.45% gain over the reactive paradigm, while also generalizing
robustly to out-of-domain cases.

### 27. Overcoming Latency Bottlenecks in On-Device Speech Translation: A Cascaded Approach with Alignment-Based Streaming MT

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Zeeshan Ahmed, Frank Seide, Niko Moritz, Ju Lin, Ruiming Xie, Simone Merello, Zhe Liu, Christian Fuegen
- **URL**: <http://arxiv.org/abs/2508.13358v1>
- **Submitted**: 2025-08-18 21:00:11
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on speech translation, using ASR and MT, with a focus on latency and real-time translation. While it involves NLP, it is primarily concerned with speech translation and does not directly relate to information retrieval, search technologies, or query understanding, making it only loosely relevant to the user's research interests.

#### Abstract
> This paper tackles several challenges that arise when integrating Automatic
Speech Recognition (ASR) and Machine Translation (MT) for real-time, on-device
streaming speech translation. Although state-of-the-art ASR systems based on
Recurrent Neural Network Transducers (RNN-T) can perform real-time
transcription, achieving streaming translation in real-time remains a
significant challenge. To address this issue, we propose a simultaneous
translation approach that effectively balances translation quality and latency.
We also investigate efficient integration of ASR and MT, leveraging linguistic
cues generated by the ASR system to manage context and utilizing efficient
beam-search pruning techniques such as time-out and forced finalization to
maintain system's real-time factor. We apply our approach to an on-device
bilingual conversational speech translation and demonstrate that our techniques
outperform baselines in terms of latency and quality. Notably, our technique
narrows the quality gap with non-streaming translation systems, paving the way
for more accurate and efficient real-time speech translation.

### 28. MME-SCI: A Comprehensive and Challenging Science Benchmark for Multimodal Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jiacheng Ruan, Dan Jiang, Xian Gao, Ting Liu, Yuzhuo Fu, Yangyang Kang
- **URL**: <http://arxiv.org/abs/2508.13938v1>
- **Submitted**: 2025-08-19 15:27:55
- **Comment**: 9 pages, 6 figures, work in progress
- **Topic Keywords**: rag
- **Reason**: Although the paper discusses large language models and evaluation benchmarks, it does not directly relate to information retrieval, search technologies, or user behavior modeling, which are the primary focus of your research interests. The paper's emphasis on multimodal large language models and scientific knowledge points also seems to be outside the scope of your work in e-commerce and NLP.

#### Abstract
> Recently, multimodal large language models (MLLMs) have achieved significant
advancements across various domains, and corresponding evaluation benchmarks
have been continuously refined and improved. In this process, benchmarks in the
scientific domain have played an important role in assessing the reasoning
capabilities of MLLMs. However, existing benchmarks still face three key
challenges: 1) Insufficient evaluation of models' reasoning abilities in
multilingual scenarios; 2) Inadequate assessment of MLLMs' comprehensive
modality coverage; 3) Lack of fine-grained annotation of scientific knowledge
points. To address these gaps, we propose MME-SCI, a comprehensive and
challenging benchmark. We carefully collected 1,019 high-quality
question-answer pairs, which involve 3 distinct evaluation modes. These pairs
cover four subjects, namely mathematics, physics, chemistry, and biology, and
support five languages: Chinese, English, French, Spanish, and Japanese. We
conducted extensive experiments on 16 open-source models and 4 closed-source
models, and the results demonstrate that MME-SCI is widely challenging for
existing MLLMs. For instance, under the Image-only evaluation mode, o4-mini
achieved accuracy of only 52.11%, 24.73%, 36.57%, and 29.80% in mathematics,
physics, chemistry, and biology, respectively, indicating a significantly
higher difficulty level compared to existing benchmarks. More importantly,
using MME-SCI's multilingual and fine-grained knowledge attributes, we analyzed
existing models' performance in depth and identified their weaknesses in
specific domains. The Data and Evaluation Code are available at
https://github.com/JCruan519/MME-SCI.

### 29. Extracting Structured Requirements from Unstructured Building Technical Specifications for Building Information Modeling

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Insaf Nahri, Romain Pinqui√©, Philippe V√©ron, Nicolas Bus, Mathieu Thorel
- **URL**: <http://arxiv.org/abs/2508.13833v1>
- **Submitted**: 2025-08-19 13:55:41
- **Topic Keywords**: rag
- **Reason**: The paper focuses on integrating Building Information Modeling with Natural Language Processing to extract structured requirements from unstructured documents, which is not directly related to Information Retrieval, Search technologies, or query understanding. Although it involves NLP, the application is specific to the construction industry and does not align with the user's interests in e-commerce or real-time relevance optimization.

#### Abstract
> This study explores the integration of Building Information Modeling (BIM)
with Natural Language Processing (NLP) to automate the extraction of
requirements from unstructured French Building Technical Specification (BTS)
documents within the construction industry. Employing Named Entity Recognition
(NER) and Relation Extraction (RE) techniques, the study leverages the
transformer-based model CamemBERT and applies transfer learning with the French
language model Fr\_core\_news\_lg, both pre-trained on a large French corpus in
the general domain. To benchmark these models, additional approaches ranging
from rule-based to deep learning-based methods are developed. For RE, four
different supervised models, including Random Forest, are implemented using a
custom feature vector. A hand-crafted annotated dataset is used to compare the
effectiveness of NER approaches and RE models. Results indicate that CamemBERT
and Fr\_core\_news\_lg exhibited superior performance in NER, achieving
F1-scores over 90\%, while Random Forest proved most effective in RE, with an
F1 score above 80\%. The outcomes are intended to be represented as a knowledge
graph in future work to further enhance automatic verification systems.

### 30. ViExam: Are Vision Language Models Better than Humans on Vietnamese Multimodal Exam Questions?

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Vy Tuong Dang, An Vo, Quang Tau, Duc Dm, Daeyoung Kim
- **URL**: <http://arxiv.org/abs/2508.13680v1>
- **Submitted**: 2025-08-19 09:31:18
- **Topic Keywords**: rag
- **Reason**: The paper focuses on vision language models and their performance on Vietnamese educational assessments, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on multimodal reasoning, the context is educational assessments rather than search or recommendation systems, making it only loosely relevant to the user's research interests.

#### Abstract
> Vision language models (VLMs) demonstrate remarkable capabilities on English
multimodal tasks, but their performance on low-resource languages with
genuinely multimodal educational content remains largely unexplored. In this
work, we test how VLMs perform on Vietnamese educational assessments,
investigating whether VLMs trained predominantly on English data can handle
real-world cross-lingual multimodal reasoning. Our work presents the first
comprehensive evaluation of VLM capabilities on multimodal Vietnamese exams
through proposing ViExam, a benchmark containing 2,548 multimodal questions. We
find that state-of-the-art VLMs achieve only 57.74% while open-source models
achieve 27.70% mean accuracy across 7 academic domains, including Mathematics,
Physics, Chemistry, Biology, Geography, Driving Test, and IQ Test. Most VLMs
underperform average human test-takers (66.54%), with only the thinking VLM o3
(74.07%) exceeding human average performance, yet still falling substantially
short of human best performance (99.60%). Cross-lingual prompting with English
instructions while maintaining Vietnamese content fails to improve performance,
decreasing accuracy by 1 percentage point for SOTA VLMs. Human-in-the-loop
collaboration can partially improve VLM performance by 5 percentage points.
Code and data are available at: https://vi-exam.github.io.

### 31. AdaDocVQA: Adaptive Framework for Long Document Visual Question Answering in Low-Resource Settings

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Haoxuan Li, Wei Song, Aofan Liu, Peiwu Qin
- **URL**: <http://arxiv.org/abs/2508.13606v1>
- **Submitted**: 2025-08-19 08:12:45
- **Topic Keywords**: retrieval
- **Reason**: The paper focuses on Document Visual Question Answering, a topic outside the scope of Information Retrieval and Search technologies, with no apparent connections to query understanding, ranking models, or user behavior modeling. Although it involves NLP and data mining, the application and scope are distinct from the user's primary focus on IR, especially in areas requiring deep semantic understanding and real-time relevance optimization.

#### Abstract
> Document Visual Question Answering (Document VQA) faces significant
challenges when processing long documents in low-resource environments due to
context limitations and insufficient training data. This paper presents
AdaDocVQA, a unified adaptive framework addressing these challenges through
three core innovations: a hybrid text retrieval architecture for effective
document segmentation, an intelligent data augmentation pipeline that
automatically generates high-quality reasoning question-answer pairs with
multi-level verification, and adaptive ensemble inference with dynamic
configuration generation and early stopping mechanisms. Experiments on Japanese
document VQA benchmarks demonstrate substantial improvements with 83.04\%
accuracy on Yes/No questions, 52.66\% on factual questions, and 44.12\% on
numerical questions in JDocQA, and 59\% accuracy on LAVA dataset. Ablation
studies confirm meaningful contributions from each component, and our framework
establishes new state-of-the-art results for Japanese document VQA while
providing a scalable foundation for other low-resource languages and
specialized domains. Our code available at:
https://github.com/Haoxuanli-Thu/AdaDocVQA.

### 32. Who Gets the Mic? Investigating Gender Bias in the Speaker Assignment of a Speech-LLM

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Dariia Puhach, Amir H. Payberah, √âva Sz√©kely
- **URL**: <http://arxiv.org/abs/2508.13603v1>
- **Submitted**: 2025-08-19 08:10:55
- **Topic Keywords**: rag
- **Reason**: This paper focuses on speaker assignment in a speech-language model, investigating gender bias, which is outside the scope of information retrieval, search technologies, and NLP, which are the primary areas of your research interests. While it may touch on related topics like text analysis, the paper's primary focus on speech-language models and speaker assignment makes it less relevant to your work.

#### Abstract
> Similar to text-based Large Language Models (LLMs), Speech-LLMs exhibit
emergent abilities and context awareness. However, whether these similarities
extend to gender bias remains an open question. This study proposes a
methodology leveraging speaker assignment as an analytic tool for bias
investigation. Unlike text-based models, which encode gendered associations
implicitly, Speech-LLMs must produce a gendered voice, making speaker selection
an explicit bias cue. We evaluate Bark, a Text-to-Speech (TTS) model, analyzing
its default speaker assignments for textual prompts. If Bark's speaker
selection systematically aligns with gendered associations, it may reveal
patterns in its training data or model design. To test this, we construct two
datasets: (i) Professions, containing gender-stereotyped occupations, and (ii)
Gender-Colored Words, featuring gendered connotations. While Bark does not
exhibit systematic bias, it demonstrates gender awareness and has some gender
inclinations.

### 33. LLM-Enhanced Linear Autoencoders for Recommendation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jaewan Moon, Seongmin Park, Jongwuk Lee
- **URL**: <http://arxiv.org/abs/2508.13500v1>
- **Submitted**: 2025-08-19 04:20:14
- **Comment**: Accepted by CIKM 2025
- **Topic Keywords**: recommend, cikm
- **Reason**: The paper focuses on recommender systems, which is a related topic, but it does not address query understanding, ranking models, or user behavior modeling, which are core aspects of Information Retrieval. The paper's emphasis on large language models and semantic item-to-item correlations is also not directly relevant to my research interests.

#### Abstract
> Large language models (LLMs) have been widely adopted to enrich the semantic
representation of textual item information in recommender systems. However,
existing linear autoencoders (LAEs) that incorporate textual information rely
on sparse word co-occurrence patterns, limiting their ability to capture rich
textual semantics. To address this, we propose L3AE, the first integration of
LLMs into the LAE framework. L3AE effectively integrates the heterogeneous
knowledge of textual semantics and user-item interactions through a two-phase
optimization strategy. (i) L3AE first constructs a semantic item-to-item
correlation matrix from LLM-derived item representations. (ii) It then learns
an item-to-item weight matrix from collaborative signals while distilling
semantic item correlations as regularization. Notably, each phase of L3AE is
optimized through closed-form solutions, ensuring global optimality and
computational efficiency. Extensive experiments demonstrate that L3AE
consistently outperforms state-of-the-art LLM-enhanced models on three
benchmark datasets, achieving gains of 27.6% in Recall@20 and 39.3% in NDCG@20.
The source code is available at https://github.com/jaewan7599/L3AE_CIKM2025.

### 34. Trust and Reputation in Data Sharing: A Survey

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Wenbo Wu, George Konstantinidis
- **URL**: <http://arxiv.org/abs/2508.14028v1>
- **Submitted**: 2025-08-19 17:39:31
- **Topic Keywords**: search
- **Reason**: The paper focuses on Trust and Reputation Management Systems (TRMSs) in data sharing, which is not directly related to Information Retrieval, Search technologies, or query understanding. Although it touches on data sharing, the primary focus is on trust and reputation, which is not a core area of interest for you.

#### Abstract
> Data sharing is the fuel of the galloping artificial intelligence economy,
providing diverse datasets for training robust models. Trust between data
providers and data consumers is widely considered one of the most important
factors for enabling data sharing initiatives. Concerns about data sensitivity,
privacy breaches, and misuse contribute to reluctance in sharing data across
various domains. In recent years, there has been a rise in technological and
algorithmic solutions to measure, capture and manage trust, trustworthiness,
and reputation in what we collectively refer to as Trust and Reputation
Management Systems (TRMSs). Such approaches have been developed and applied to
different domains of computer science, such as autonomous vehicles, or IoT
networks, but there have not been dedicated approaches to data sharing and its
unique characteristics. In this survey, we examine TRMSs from a data-sharing
perspective, analyzing how they assess the trustworthiness of both data and
entities across different environments. We develop novel taxonomies for system
designs, trust evaluation framework, and evaluation metrics for both data and
entity, and we systematically analyze the applicability of existing TRMSs in
data sharing. Finally, we identify open challenges and propose future research
directions to enhance the explainability, comprehensiveness, and accuracy of
TRMSs in large-scale data-sharing ecosystems.

### 35. Can Large Language Models (LLMs) Describe Pictures Like Children? A Comparative Corpus Study

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Hanna Woloszyn, Benjamin Gagl
- **URL**: <http://arxiv.org/abs/2508.13769v1>
- **Submitted**: 2025-08-19 12:13:54
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on the evaluation of large language models' ability to generate text that resembles child language, which is not directly related to your areas of interest.

#### Abstract
> The role of large language models (LLMs) in education is increasing, yet
little attention has been paid to whether LLM-generated text resembles child
language. This study evaluates how LLMs replicate child-like language by
comparing LLM-generated texts to a collection of German children's descriptions
of picture stories. We generated two LLM-based corpora using the same picture
stories and two prompt types: zero-shot and few-shot prompts specifying a
general age from the children corpus. We conducted a comparative analysis
across psycholinguistic text properties, including word frequency, lexical
richness, sentence and word length, part-of-speech tags, and semantic
similarity with word embeddings. The results show that LLM-generated texts are
longer but less lexically rich, rely more on high-frequency words, and
under-represent nouns. Semantic vector space analysis revealed low similarity,
highlighting differences between the two corpora on the level of corpus
semantics. Few-shot prompt increased similarities between children and LLM text
to a minor extent, but still failed to replicate lexical and semantic patterns.
The findings contribute to our understanding of how LLMs approximate child
language through multimodal prompting (text + image) and give insights into
their use in psycholinguistic research and education while raising important
questions about the appropriateness of LLM-generated language in child-directed
educational tools.

### 36. Refining Contrastive Learning and Homography Relations for Multi-Modal Recommendation

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Shouxing Ma, Yawen Zeng, Shiqing Wu, Guandong Xu
- **URL**: <http://arxiv.org/abs/2508.13745v1>
- **Submitted**: 2025-08-19 11:35:48
- **Comment**: This paper has been accepted as a full paper at ACM MM 2025
- **Topic Keywords**: recommend
- **Reason**: The paper focuses on multi-modal recommendation, which is not directly related to the user's primary research interests in Information Retrieval and Search technologies. Although it mentions contrastive learning, which is a technique used in some IR applications, the context and methodology are quite different from the user's areas of focus.

#### Abstract
> Multi-modal recommender system focuses on utilizing rich modal information (
i.e., images and textual descriptions) of items to improve recommendation
performance. The current methods have achieved remarkable success with the
powerful structure modeling capability of graph neural networks. However, these
methods are often hindered by sparse data in real-world scenarios. Although
contrastive learning and homography ( i.e., homogeneous graphs) are employed to
address the data sparsity challenge, existing methods still suffer two main
limitations: 1) Simple multi-modal feature contrasts fail to produce effective
representations, causing noisy modal-shared features and loss of valuable
information in modal-unique features; 2) The lack of exploration of the
homograph relations between user interests and item co-occurrence results in
incomplete mining of user-item interplay.
  To address the above limitations, we propose a novel framework for
\textbf{R}\textbf{E}fining multi-mod\textbf{A}l cont\textbf{R}astive learning
and ho\textbf{M}ography relations (\textbf{REARM}). Specifically, we complement
multi-modal contrastive learning by employing meta-network and orthogonal
constraint strategies, which filter out noise in modal-shared features and
retain recommendation-relevant information in modal-unique features. To mine
homogeneous relationships effectively, we integrate a newly constructed user
interest graph and an item co-occurrence graph with the existing user
co-occurrence and item semantic graphs for graph learning. The extensive
experiments on three real-world datasets demonstrate the superiority of REARM
to various state-of-the-art baselines. Our visualization further shows an
improvement made by REARM in distinguishing between modal-shared and
modal-unique features. Code is available
\href{https://github.com/MrShouxingMa/REARM}{here}.

### 37. A Comparative Study of Decoding Strategies in Medical Text Generation

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Oriana Presacan, Alireza Nik, Vajira Thambawita, Bogdan Ionescu, Michael Riegler
- **URL**: <http://arxiv.org/abs/2508.13580v1>
- **Submitted**: 2025-08-19 07:25:25
- **Topic Keywords**: search
- **Reason**: This paper focuses on decoding strategies in medical text generation, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the importance of careful selection of decoding methods, the topic is more relevant to natural language processing and machine learning, rather than information retrieval.

#### Abstract
> Large Language Models (LLMs) rely on various decoding strategies to generate
text, and these choices can significantly affect output quality. In healthcare,
where accuracy is critical, the impact of decoding strategies remains
underexplored. We investigate this effect in five open-ended medical tasks,
including translation, summarization, question answering, dialogue, and image
captioning, evaluating 11 decoding strategies with medically specialized and
general-purpose LLMs of different sizes. Our results show that deterministic
strategies generally outperform stochastic ones: beam search achieves the
highest scores, while {\eta} and top-k sampling perform worst. Slower decoding
methods tend to yield better quality. Larger models achieve higher scores
overall but have longer inference times and are no more robust to decoding.
Surprisingly, while medical LLMs outperform general ones in two of the five
tasks, statistical analysis shows no overall performance advantage and reveals
greater sensitivity to decoding choice. We further compare multiple evaluation
metrics and find that correlations vary by task, with MAUVE showing weak
agreement with BERTScore and ROUGE, as well as greater sensitivity to the
decoding strategy. These results highlight the need for careful selection of
decoding methods in medical applications, as their influence can sometimes
exceed that of model choice.

### 38. MATA (mƒÅta): Mindful Assessment of the Telugu Abilities of Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Chalamalasetti Kranti, Sowmya Vajjala
- **URL**: <http://arxiv.org/abs/2508.13526v1>
- **Submitted**: 2025-08-19 05:33:57
- **Comment**: Pre-print
- **Topic Keywords**: search
- **Reason**: The paper focuses on evaluating the performance of Large Language Models (LLMs) in the Telugu language, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on NLP, the scope is limited to language model evaluation and does not align with the user's primary research interests.

#### Abstract
> In this paper, we introduce MATA, a novel evaluation dataset to assess the
ability of Large Language Models (LLMs) in Telugu language, comprising 729
carefully curated multiple-choice and open-ended questions that span diverse
linguistic dimensions. We evaluate 11 open-weight and closed-source LLMs on our
dataset and present a fine-grained analysis of their performance. Further, we
empirically show how LLMs rely on superficial heuristics such as answer
position and distractor patterns for multiple-choice questions. Finally, we
also compare LLM-as-a-judge evaluation with human evaluation for open-ended
questions and draw some conclusions on its reliability in a low-resource
language. We argue that such fine-grained evaluation is essential for
understanding model limitations and can inform the development of more
linguistically capable LLMs, while also serving as a foundation for future
research in Telugu NLP.

---

