# Daily Papers Report - 2025-07-17

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. The benefits of query-based KGQA systems for complex and temporal questions in LLM era

- **LLM Score**: 6
- **Keyword Score**: 6
- **Authors**: Artem Alekseev, Mikhail Chaichuk, Miron Butko, Alexander Panchenko, Elena Tutubalina, Oleg Somov
- **URL**: <http://arxiv.org/abs/2507.11954v1>
- **Submitted**: 2025-07-16 06:41:03
- **Comment**: 15 pages, 3 figures, 7 tables
- **Topic Keywords**: query, queries
- **Reason**: The paper explores query-based knowledge graph question answering (KGQA) systems, which is related to information retrieval and query understanding. The focus on multi-hop and temporal questions is also relevant to ranking models and user behavior modeling. However, the paper's primary focus on KGQA and its application to question-answering is not directly aligned with the user's core research themes in information retrieval and search technologies.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Knowledge Graph Question Answering (KGQA) system
- **Aim**: to improve the robustness of answering complex and temporal questions by leveraging small fine-tuned models
- **Rationale**: to overcome the limitations of large language models (LLMs) in generating executable queries instead of direct answers
- **Ground**: evaluated on four multi-hop and temporal datasets: LC-QuAD 2.0, RuBQ 2.0, QALD-10, and PAT-Questions
- **Experiment**: a pipeline-based KGQA system consisting of entity detection and disambiguation, predicate detection and disambiguation, and an auto-regressive query generation model
- **Takeaway**: the proposed KGQA system demonstrates a significant improvement over ChatGPT-based QA systems and existing approaches, particularly in complex multi-hop and temporal question tasks

#### Abstract
> Large language models excel in question-answering (QA) yet still struggle
with multi-hop reasoning and temporal questions. Query-based knowledge graph QA
(KGQA) offers a modular alternative by generating executable queries instead of
direct answers. We explore multi-stage query-based framework for WikiData QA,
proposing multi-stage approach that enhances performance on challenging
multi-hop and temporal benchmarks. Through generalization and rejection
studies, we evaluate robustness across multi-hop and temporal QA datasets.
Additionally, we introduce a novel entity linking and predicate matching method
using CoT reasoning. Our results demonstrate the potential of query-based
multi-stage KGQA framework for improving multi-hop and temporal QA with small
language models. Code and data: https://github.com/ar2max/NLDB-KGQA-System

---

### 2. Language Models Improve When Pretraining Data Matches Target Tasks

- **LLM Score**: 6
- **Keyword Score**: 5
- **Authors**: David Mizrahi, Anders Boesen Lindbo Larsen, Jesse Allardice, Suzie Petryk, Yuri Gorokhov, Jeffrey Li, Alex Fang, Josh Gardner, Tom Gunter, Afshin Dehghan
- **URL**: <http://arxiv.org/abs/2507.12466v1>
- **Submitted**: 2025-07-16 17:59:45
- **Comment**: 44 pages, 25 figures, 13 tables
- **Topic Keywords**: ranking, rank, search
- **Reason**: The paper explores the impact of pretraining data selection on model performance, which is relevant to information retrieval and search technologies. The use of benchmark-targeted ranking (BETR) to select pretraining documents is also related to query understanding and ranking models. However, the focus on language models and pretraining data selection is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: Benchmark-Targeted Ranking (BETR) for data selection in pretraining models
- **Aim**: To develop a data selection method that selects pretraining documents based on their similarity to benchmark training examples
- **Rationale**: Directly matching pretraining data to target tasks precisely shapes model capabilities
- **Ground**: Comparison of BETR to other data selection methods using over 500 models with varying FLOPs and fitting scaling laws
- **Experiment**: Training models with BETR and evaluating performance on 10 tasks across different scales
- **Takeaway**: BETR achieves a 2.1x compute multiplier and improves performance on 9 out of 10 tasks, generalizing well to diverse benchmarks and larger models

#### Abstract
> Every data selection method inherently has a target. In practice, these
targets often emerge implicitly through benchmark-driven iteration: researchers
develop selection strategies, train models, measure benchmark performance, then
refine accordingly. This raises a natural question: what happens when we make
this optimization explicit? To explore this, we propose benchmark-targeted
ranking (BETR), a simple method that selects pretraining documents based on
similarity to benchmark training examples. BETR embeds benchmark examples and a
sample of pretraining documents in a shared space, scores this sample by
similarity to benchmarks, then trains a lightweight classifier to predict these
scores for the full corpus. We compare data selection methods by training over
500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to
them. From this, we find that simply aligning pretraining data to evaluation
benchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline
(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks
across all scales. BETR also generalizes well: when targeting a diverse set of
benchmarks disjoint from our evaluation suite, it still matches or outperforms
baselines. Our scaling analysis further reveals a clear trend: larger models
require less aggressive filtering. Overall, our findings show that directly
matching pretraining data to target tasks precisely shapes model capabilities
and highlight that optimal selection strategies must adapt to model scale.

---

### 3. Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data

- **LLM Score**: 4
- **Keyword Score**: 14
- **Authors**: Chandana Cheerla
- **URL**: <http://arxiv.org/abs/2507.12425v1>
- **Submitted**: 2025-07-16 17:13:06
- **Topic Keywords**: ranking, rerank, relevance, rag, retrieval, rank
- **Reason**: The paper proposes a Retrieval-Augmented Generation framework for structured enterprise data, which is somewhat related to my interests in Information Retrieval and Search technologies. However, the focus on enterprise data and chatbots is not directly aligned with my primary research themes, and the paper does not explicitly address query understanding, ranking models, or user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Retrieval-Augmented Generation (RAG) framework for enterprise data retrieval and generation
- **Aim**: Design a framework that can handle structured, semi-structured, and unstructured data in enterprise environments
- **Rationale**: The need for a versatile framework that can retrieve and generate responses from heterogeneous data, including text, structured documents, and tabular records
- **Ground**: The framework is grounded in hybrid retrieval strategies using dense embeddings and BM25, metadata-aware filtering, cross-encoder reranking, and semantic chunking
- **Experiment**: The framework is evaluated on a corpus of publicly available enterprise policy documents and proprietary datasets, demonstrating significant improvements over baseline RAG approaches
- **Takeaway**: The RAG framework offers a comprehensive solution for enterprise data retrieval and generation tasks, with advantages in versatility, hybrid retrieval, and scalability, but also has limitations that can be addressed through future work

#### Abstract
> Organizations increasingly rely on proprietary enterprise data, including HR
records, structured reports, and tabular documents, for critical
decision-making. While Large Language Models (LLMs) have strong generative
capabilities, they are limited by static pretraining, short context windows,
and challenges in processing heterogeneous data formats. Conventional
Retrieval-Augmented Generation (RAG) frameworks address some of these gaps but
often struggle with structured and semi-structured data.
  This work proposes an advanced RAG framework that combines hybrid retrieval
strategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by
metadata-aware filtering with SpaCy NER and cross-encoder reranking. The
framework applies semantic chunking to maintain textual coherence and retains
tabular data structures to preserve row-column integrity. Quantized indexing
optimizes retrieval efficiency, while human-in-the-loop feedback and
conversation memory improve adaptability.
  Experiments on enterprise datasets show notable improvements: Precision@5
increased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),
and Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative
evaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness
(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.
These results demonstrate the framework's effectiveness in delivering accurate,
comprehensive, and contextually relevant responses for enterprise tasks. Future
work includes extending to multimodal data and integrating agent-based
retrieval. The source code will be released at
https://github.com/CheerlaChandana/Enterprise-Chatbot

---

### 4. Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Yuhong Zhang, Jialu Li, Shilai Yang, Yuchen Xu, Gert Cauwenberghs, Tzyy-Ping Jung
- **URL**: <http://arxiv.org/abs/2507.11972v1>
- **Submitted**: 2025-07-16 07:15:59
- **Topic Keywords**: information retrieval, relevance, retrieval
- **Reason**: The paper explores the application of Large Language Models in reading comprehension analysis, which is related to information retrieval and NLP. However, the focus on graph representations and eye-tracking biomarkers is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Improving Reading Comprehension Analysis using Large Language Models and Eye-Tracking Biomarkers
- **Aim**: To develop a novel approach to improve reading comprehension analysis by leveraging Large Language Models (LLMs) and eye-tracking biomarkers
- **Rationale**: To address the limitations of previous methods that relied on individual words, failing to capture broader semantic meaning and relevance to comprehension tasks
- **Ground**: The study utilizes the ZuCo 1.0 dataset and develops prompts to convert sentences into nodes and edges, generating knowledge graphs (KGs) using LLMs
- **Experiment**: The study evaluates the performance of centrality metrics in identifying important nodes using Receiver Operating Characteristic - Area Under the Curve (ROC-AUC) analysis and analyzes eye-fixation data to validate the computational results
- **Takeaway**: The study demonstrates that graph-theoretic techniques can identify key terms and highlight coherent and informative parts of a text, and the LLM-derived node importance labeling is strongly aligned with human cognitive attention, as measured by eye fixations

#### Abstract
> Reading comprehension is a fundamental skill in human cognitive development.
With the advancement of Large Language Models (LLMs), there is a growing need
to compare how humans and LLMs understand language across different contexts
and apply this understanding to functional tasks such as inference, emotion
interpretation, and information retrieval. Our previous work used LLMs and
human biomarkers to study the reading comprehension process. The results showed
that the biomarkers corresponding to words with high and low relevance to the
inference target, as labeled by the LLMs, exhibited distinct patterns,
particularly when validated using eye-tracking data. However, focusing solely
on individual words limited the depth of understanding, which made the
conclusions somewhat simplistic despite their potential significance. This
study used an LLM-based AI agent to group words from a reading passage into
nodes and edges, forming a graph-based text representation based on semantic
meaning and question-oriented prompts. We then compare the distribution of eye
fixations on important nodes and edges. Our findings indicate that LLMs exhibit
high consistency in language understanding at the level of graph topological
structure. These results build on our previous findings and offer insights into
effective human-AI co-learning strategies.

---

### 5. Context-Aware Search and Retrieval Over Erasure Channels

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Sara Ghasvarianjahromi, Yauhen Yakimenka, J√∂rg Kliewer
- **URL**: <http://arxiv.org/abs/2507.11894v1>
- **Submitted**: 2025-07-16 04:21:46
- **Topic Keywords**: query, rag, retrieval, search
- **Reason**: The paper explores a novel search and retrieval model over erasure channels, which is a unique application not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on semantic communication principles, the focus is on error-prone communication settings rather than query understanding, ranking models, or user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Remote Document Retrieval System using Retrieval-Augmented Generation and Semantic Communication
- **Aim**: To develop a search and retrieval model that combines principles from RAG and semantic communication for a remote document retrieval system operating over a symbol erasure channel
- **Rationale**: To counter potential information loss caused by the erasure channel and to improve resilience to erasures
- **Ground**: The system model consists of a transmitter, receiver, and erasure channel, and the analysis derives an explicit expression for the retrieval error probability
- **Experiment**: Numerical results using Monte Carlo simulations and real-world data from the Google Natural Questions (NQ) dataset demonstrate the validity of the analytical framework and the advantage of using repetition to improve resilience to erasures
- **Takeaway**: The key contributions include the introduction of a simplified model for RAG systems, the development of an analytical framework to characterize the error probability, and the demonstration of the effectiveness of semantic-aware feature encoding in reducing the error rate in error-prone communication settings

#### Abstract
> This paper introduces and analyzes a search and retrieval model that adopts
key semantic communication principles from retrieval-augmented generation. We
specifically present an information-theoretic analysis of a remote document
retrieval system operating over a symbol erasure channel. The proposed model
encodes the feature vector of a query, derived from term-frequency weights of a
language corpus by using a repetition code with an adaptive rate dependent on
the contextual importance of the terms. At the decoder, we select between two
documents based on the contextual closeness of the recovered query. By
leveraging a jointly Gaussian approximation for both the true and reconstructed
similarity scores, we derive an explicit expression for the retrieval error
probability, i.e., the probability under which the less similar document is
selected. Numerical simulations on synthetic and real-world data (Google NQ)
confirm the validity of the analysis. They further demonstrate that assigning
greater redundancy to critical features effectively reduces the error rate,
highlighting the effectiveness of semantic-aware feature encoding in
error-prone communication settings.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Developing Visual Augmented Q&A System using Scalable Vision Embedding Retrieval & Late Interaction Re-ranker

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Rachna Saxena, Abhijeet Kumar, Suresh Shanmugam
- **URL**: <http://arxiv.org/abs/2507.12378v1>
- **Submitted**: 2025-07-16 16:27:05
- **Comment**: Presented at NLP@IR workshop at SIGIR conference
- **Topic Keywords**: rag, retrieval, rank, search
- **Reason**: The paper explores a visual augmented Q&A system, which is not directly related to information retrieval, query understanding, or ranking models. While it touches on multimodal language models, the focus is on visual elements and retrieval, which is not a central match for your research interests.

#### Abstract
> Traditional information extraction systems face challenges with text only
language models as it does not consider infographics (visual elements of
information) such as tables, charts, images etc. often used to convey complex
information to readers. Multimodal LLM (MLLM) face challenges of finding needle
in the haystack problem i.e., either longer context length or substantial
number of documents as search space. Late interaction mechanism over visual
language models has shown state of the art performance in retrieval-based
vision augmented Q&A tasks. There are yet few challenges using it for RAG based
multi-modal Q&A. Firstly, many popular and widely adopted vector databases do
not support native multi-vector retrieval. Secondly, late interaction requires
computation which inflates space footprint and can hinder enterprise adoption.
Lastly, the current state of late interaction mechanism does not leverage the
approximate neighbor search indexing methods for large speed ups in retrieval
process. This paper explores a pragmatic approach to make vision retrieval
process scalable and efficient without compromising on performance quality. We
propose multi-step custom implementation utilizing widely adopted hybrid search
(metadata & embedding) and state of the art late interaction re-ranker to
retrieve best matching pages. Finally, MLLM are prompted as reader to generate
answers from contextualized best matching pages. Through experiments, we
observe that the proposed design is scalable (significant speed up) and stable
(without degrading performance quality), hence can be used as production
systems at enterprises.

### 7. SIEVE: Effective Filtered Vector Search with Collection of Indexes

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Zhaoheng Li, Silu Huang, Wei Ding, Yongjoo Park, Jianjun Chen
- **URL**: <http://arxiv.org/abs/2507.11907v1>
- **Submitted**: 2025-07-16 04:46:28
- **Topic Keywords**: query, recommend, search
- **Reason**: The paper focuses on filtered vector search, which is not directly related to query understanding, ranking models, or user behavior modeling in Information Retrieval. While it involves indexing and search, the approach is more focused on efficient search and recall rather than deep semantic understanding or real-time relevance optimization.

#### Abstract
> Many real-world tasks such as recommending videos with the kids tag can be
reduced to finding most similar vectors associated with hard predicates. This
task, filtered vector search, is challenging as prior state-of-the-art
graph-based (unfiltered) similarity search techniques quickly degenerate when
hard constraints are considered. That is, effective graph-based filtered
similarity search relies on sufficient connectivity for reaching the most
similar items within just a few hops. To consider predicates, recent works
propose modifying graph traversal to visit only the items that may satisfy
predicates. However, they fail to offer the just-a-few-hops property for a wide
range of predicates: they must restrict predicates significantly or lose
efficiency if only a small fraction of items satisfy predicates.
  We propose an opposite approach: instead of constraining traversal, we build
many indexes each serving different predicate forms. For effective
construction, we devise a three-dimensional analytical model capturing
relationships among index size, search time, and recall, with which we follow a
workload-aware approach to pack as many useful indexes as possible into a
collection. At query time, the analytical model is employed yet again to
discern the one that offers the fastest search at a given recall. We show
superior performance and support on datasets with varying selectivities and
forms: our approach achieves up to 8.06x speedup while having as low as 1%
build time versus other indexes, with less than 2.15x memory of a standard HNSW
graph and modest knowledge of past workloads.

### 8. Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Feng Xiao, Jicong Fan
- **URL**: <http://arxiv.org/abs/2507.12295v1>
- **Submitted**: 2025-07-16 14:47:41
- **Topic Keywords**: rag, rank, search
- **Reason**: The paper focuses on text anomaly detection, which is related to information retrieval and natural language processing. However, the specific application and methodology do not directly align with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Text anomaly detection is a critical task in natural language processing
(NLP), with applications spanning fraud detection, misinformation
identification, spam detection and content moderation, etc. Despite significant
advances in large language models (LLMs) and anomaly detection algorithms, the
absence of standardized and comprehensive benchmarks for evaluating the
existing anomaly detection methods on text data limits rigorous comparison and
development of innovative approaches. This work performs a comprehensive
empirical study and introduces a benchmark for text anomaly detection,
leveraging embeddings from diverse pre-trained language models across a wide
array of text datasets. Our work systematically evaluates the effectiveness of
embedding-based text anomaly detection by incorporating (1) early language
models (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI
(small, ada, large)); (3) multi-domain text datasets (news, social media,
scientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).
Our experiments reveal a critical empirical insight: embedding quality
significantly governs anomaly detection efficacy, and deep learning-based
approaches demonstrate no performance advantage over conventional shallow
algorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived
embeddings.In addition, we observe strongly low-rank characteristics in
cross-model performance matrices, which enables an efficient strategy for rapid
model evaluation (or embedding evaluation) and selection in practical
applications. Furthermore, by open-sourcing our benchmark toolkit that includes
all embeddings from different models and code at
https://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work
provides a foundation for future research in robust and scalable text anomaly
detection systems.

### 9. AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Matteo Fasulo, Luca Babboni, Luca Tedeschini
- **URL**: <http://arxiv.org/abs/2507.11764v1>
- **Submitted**: 2025-07-15 22:10:20
- **Comment**: 14 pages, 6 figures, accepted at CLEF 2025 CheckThat! Lab
- **Topic Keywords**: ranking, rank
- **Reason**: The paper focuses on subjectivity detection in news articles, which is a topic in Natural Language Processing (NLP). While it uses transformer-based embeddings and sentiment scores, it does not directly relate to query understanding, ranking models, or user behavior modeling in Information Retrieval (IR). The paper's focus on sentiment analysis and language-specific models is somewhat relevant to my interests, but it does not align with my primary focus on IR and deep semantic understanding.

#### Abstract
> This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab
Task 1: Subjectivity Detection in News Articles, classifying sentences as
subjective/objective in monolingual, multilingual, and zero-shot settings.
Training/development datasets were provided for Arabic, German, English,
Italian, and Bulgarian; final evaluation included additional unseen languages
(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our
primary strategy enhanced transformer-based classifiers by integrating
sentiment scores, derived from an auxiliary model, with sentence
representations, aiming to improve upon standard fine-tuning. We explored this
sentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base
(English), and Llama3.2-1B. To address class imbalance, prevalent across
languages, we employed decision threshold calibration optimized on the
development set. Our experiments show sentiment feature integration
significantly boosts performance, especially subjective F1 score. This
framework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).

### 10. Looking for Fairness in Recommender Systems

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: C√©cile Log√©
- **URL**: <http://arxiv.org/abs/2507.12242v1>
- **Submitted**: 2025-07-16 13:53:02
- **Topic Keywords**: recommend, acl
- **Reason**: The paper focuses on recommender systems, which is a related topic to your research interests. However, it does not specifically address query understanding, ranking models, or user behavior modeling, which are core areas of focus for you. The paper's emphasis on fairness and diversity in recommender systems is somewhat relevant to your interests in information retrieval and search technologies, but it does not directly align with your primary research themes.

#### Abstract
> Recommender systems can be found everywhere today, shaping our everyday
experience whenever we're consuming content, ordering food, buying groceries
online, or even just reading the news. Let's imagine we're in the process of
building a recommender system to make content suggestions to users on social
media. When thinking about fairness, it becomes clear there are several
perspectives to consider: the users asking for tailored suggestions, the
content creators hoping for some limelight, and society at large, navigating
the repercussions of algorithmic recommendations. A shared fairness concern
across all three is the emergence of filter bubbles, a side-effect that takes
place when recommender systems are almost "too good", making recommendations so
tailored that users become inadvertently confined to a narrow set of
opinions/themes and isolated from alternative ideas. From the user's
perspective, this is akin to manipulation. From the small content creator's
perspective, this is an obstacle preventing them access to a whole range of
potential fans. From society's perspective, the potential consequences are
far-reaching, influencing collective opinions, social behavior and political
decisions. How can our recommender system be fine-tuned to avoid the creation
of filter bubbles, and ensure a more inclusive and diverse content landscape?
Approaching this problem involves defining one (or more) performance metric to
represent diversity, and tweaking our recommender system's performance through
the lens of fairness. By incorporating this metric into our evaluation
framework, we aim to strike a balance between personalized recommendations and
the broader societal goal of fostering rich and varied cultures and points of
view.

### 11. Similarity-Guided Diffusion for Contrastive Sequential Recommendation

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Jinkyeong Choi, Yejin Noh, Donghyeon Park
- **URL**: <http://arxiv.org/abs/2507.11866v1>
- **Submitted**: 2025-07-16 03:26:24
- **Comment**: 14 pages, 5 figures
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on sequential recommendation systems, which is somewhat related to information retrieval and search technologies. However, the emphasis on contrastive learning and diffusion models is not directly aligned with my research interests in query understanding, ranking models, and user behavior modeling. While the paper explores data augmentation and representation learning, it does not seem to address the specific areas of deep semantic understanding and real-time relevance optimization that I am particularly interested in.

#### Abstract
> In sequential recommendation systems, data augmentation and contrastive
learning techniques have recently been introduced using diffusion models to
achieve robust representation learning. However, most of the existing
approaches use random augmentation, which risk damaging the contextual
information of the original sequence. Accordingly, we propose a
Similarity-Guided Diffusion for Contrastive Sequential Recommendation. Our
method leverages the similarity between item embedding vectors to generate
semantically consistent noise. Moreover, we utilize high confidence score in
the denoising process to select our augmentation positions. This approach more
effectively reflects contextual and structural information compared to
augmentation at random positions. From a contrastive learning perspective, the
proposed augmentation technique provides more discriminative positive and
negative samples, simultaneously improving training efficiency and
recommendation performance. Experimental results on five benchmark datasets
show that SimDiffRec outperforms the existing baseline models.

### 12. Sparse Autoencoders for Sequential Recommendation Models: Interpretation and Flexible Control

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Anton Klenitskiy, Konstantin Polev, Daria Denisova, Alexey Vasilev, Dmitry Simakov, Gleb Gusev
- **URL**: <http://arxiv.org/abs/2507.12202v1>
- **Submitted**: 2025-07-16 12:57:43
- **Topic Keywords**: recommend, search
- **Reason**: The paper explores the application of sparse autoencoders to sequential recommendation models, which is somewhat related to my interests in Information Retrieval and Search technologies. However, the focus on sequential recommendations and transformer architectures is not directly aligned with my primary research themes, such as query understanding, ranking models, and user behavior modeling.

#### Abstract
> Many current state-of-the-art models for sequential recommendations are based
on transformer architectures. Interpretation and explanation of such black box
models is an important research question, as a better understanding of their
internals can help understand, influence, and control their behavior, which is
very important in a variety of real-world applications. Recently sparse
autoencoders (SAE) have been shown to be a promising unsupervised approach for
extracting interpretable features from language models. These autoencoders
learn to reconstruct hidden states of the transformer's internal layers from
sparse linear combinations of directions in their activation space.
  This paper is focused on the application of SAE to the sequential
recommendation domain. We show that this approach can be successfully applied
to the transformer trained on a sequential recommendation task: learned
directions turn out to be more interpretable and monosemantic than the original
hidden state dimensions. Moreover, we demonstrate that the features learned by
SAE can be used to effectively and flexibly control the model's behavior,
providing end-users with a straightforward method to adjust their
recommendations to different custom scenarios and contexts.

### 13. DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Yi Zhao, Zuchao Li, Hai Zhao, Baoyuan Qi, Guoming Liu
- **URL**: <http://arxiv.org/abs/2507.11942v1>
- **Submitted**: 2025-07-16 06:16:06
- **Comment**: ACL 2025
- **Topic Keywords**: rag
- **Reason**: The paper focuses on prompt compression, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the context is different from the user's primary interests. The paper's attention-aware approach is innovative, but its relevance to the user's research areas is limited.

#### Abstract
> Task-agnostic prompt compression leverages the redundancy in natural language
to reduce computational overhead and enhance information density within
prompts, especially in long-context scenarios. Existing methods predominantly
rely on information entropy as the metric to compress lexical units, aiming to
achieve minimal information loss. However, these approaches overlook two
critical aspects: (i) the importance of attention-critical tokens at the
algorithmic level, and (ii) shifts in information entropy during the
compression process. Motivated by these challenges, we propose a dynamic
attention-aware approach for task-agnostic prompt compression (DAC). This
approach effectively integrates entropy and attention information, dynamically
sensing entropy shifts during compression to achieve fine-grained prompt
compression. Extensive experiments across various domains, including LongBench,
GSM8K, and BBH, show that DAC consistently yields robust and substantial
improvements across a diverse range of tasks and LLMs, offering compelling
evidence of its efficacy.

### 14. MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Varun Srivastava, Fan Lei, Srija Mukhopadhyay, Vivek Gupta, Ross Maciejewski
- **URL**: <http://arxiv.org/abs/2507.11625v1>
- **Submitted**: 2025-07-15 18:02:57
- **Comment**: Published as a conference paper at COLM 2025
- **Topic Keywords**: search
- **Reason**: The paper focuses on multimodal large language models for map question answering, which is a specific application of natural language processing. While it involves visual data and question answering, it does not directly relate to query understanding, ranking models, or user behavior modeling in the context of information retrieval, which are the user's primary research interests.

#### Abstract
> Recent advancements in multimodal large language models (MLLMs) have driven
researchers to explore how well these models read data visualizations, e.g.,
bar charts, scatter plots. More recently, attention has shifted to visual
question answering with maps (Map-VQA). However, Map-VQA research has primarily
focused on choropleth maps, which cover only a limited range of thematic
categories and visual analytical tasks. To address these gaps, we introduce
MapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three
map types: choropleth maps, cartograms, and proportional symbol maps spanning
topics from six distinct themes (e.g., housing, crime). We evaluate multiple
MLLMs using six visual analytical tasks, comparing their performance against
one another and a human baseline. An additional experiment examining the impact
of map design changes (e.g., altered color schemes, modified legend designs,
and removal of map elements) provides insights into the robustness and
sensitivity of MLLMs, their reliance on internal geographic knowledge, and
potential avenues for improving Map-VQA performance.

### 15. Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Meysam Alizadeh, Fabrizio Gilardi, Zeynab Samei, Mohsen Mosleh
- **URL**: <http://arxiv.org/abs/2507.12372v1>
- **Submitted**: 2025-07-16 16:21:01
- **Topic Keywords**: information retrieval, retrieval, recommend, search
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling, which are the core areas of your research interests. While it touches on the topic of real-time information retrieval, it focuses on web browsing LLMs accessing social media profiles, which is a niche area that doesn't align with your primary focus.

#### Abstract
> Large language models (LLMs) have traditionally relied on static training
data, limiting their knowledge to fixed snapshots. Recent advancements,
however, have equipped LLMs with web browsing capabilities, enabling real time
information retrieval and multi step reasoning over live web content. While
prior studies have demonstrated LLMs ability to access and analyze websites,
their capacity to directly retrieve and analyze social media data remains
unexplored. Here, we evaluate whether web browsing LLMs can infer demographic
attributes of social media users given only their usernames. Using a synthetic
dataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international
participants, we show that these models can access social media content and
predict user demographics with reasonable accuracy. Analysis of the synthetic
dataset further reveals how LLMs parse and interpret social media profiles,
which may introduce gender and political biases against accounts with minimal
activity. While this capability holds promise for computational social science
in the post API era, it also raises risks of misuse particularly in information
operations and targeted advertising underscoring the need for safeguards. We
recommend that LLM providers restrict this capability in public facing
applications, while preserving controlled access for verified research
purposes.

### 16. ILID: Native Script Language Identification for Indian Languages

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Yash Ingle, Pruthwik Mishra
- **URL**: <http://arxiv.org/abs/2507.11832v1>
- **Submitted**: 2025-07-16 01:39:32
- **Comment**: 8 pages, 1 figure, 7 tables, Paper accepted in RANLP 2025
- **Topic Keywords**: information retrieval, retrieval, search
- **Reason**: The paper focuses on language identification for Indian languages, which is not directly related to the user's interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions applications in NLP, the specific task and dataset are not relevant to the user's research areas.

#### Abstract
> The language identification task is a crucial fundamental step in NLP. Often
it serves as a pre-processing step for widely used NLP applications such as
multilingual machine translation, information retrieval, question and
answering, and text summarization. The core challenge of language
identification lies in distinguishing languages in noisy, short, and code-mixed
environments. This becomes even harder in case of diverse Indian languages that
exhibit lexical and phonetic similarities, but have distinct differences. Many
Indian languages share the same script making the task even more challenging.
In this paper, we release a dataset of 230K sentences consisting of English and
all 22 official Indian languages labeled with their language identifiers where
data in most languages are newly created. We also develop and release robust
baseline models using state-of-the-art approaches in machine learning and deep
learning that can aid the research in this field. Our baseline models are
comparable to the state-of-the-art models for the language identification task.

### 17. Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Prashanth Vijayaraghavan, Apoorva Nitsure, Charles Mackin, Luyao Shi, Stefano Ambrogio, Arvind Haran, Viresh Paruthi, Ali Elzein, Dan Coops, David Beymer, Tyler Baldwin, Ehsan Degan
- **URL**: <http://arxiv.org/abs/2507.12308v1>
- **Submitted**: 2025-07-16 15:05:30
- **Comment**: 10 pages (6 content pages + 4 supplementary), 5 figures, Proceedings
  of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD.
  2024 (MLCAD'24)
- **Topic Keywords**: ctr, search
- **Reason**: This paper is not relevant to your research interests as it focuses on Large Language Models (LLMs) for VHDL code generation and summarization, which is outside the scope of Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on hardware description languages (HDLs) and Electronic Design Automation (EDA) is also not aligned with your interests in e-commerce and general information retrieval.

#### Abstract
> Large Language Models (LLMs) have become widely used across diverse NLP tasks
and domains, demonstrating their adaptability and effectiveness. In the realm
of Electronic Design Automation (EDA), LLMs show promise for tasks like
Register-Transfer Level (RTL) code generation and summarization. However,
despite the proliferation of LLMs for general code-related tasks, there's a
dearth of research focused on evaluating and refining these models for hardware
description languages (HDLs), notably VHDL. In this study, we evaluate the
performance of existing code LLMs for VHDL code generation and summarization
using various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,
an in-house dataset, aims to gauge LLMs' understanding of functionally
equivalent code. Our findings reveal consistent underperformance of these
models across different metrics, underscoring a significant gap in their
suitability for this domain. To address this challenge, we propose
Chain-of-Descriptions (CoDes), a novel approach to enhance the performance of
LLMs for VHDL code generation and summarization tasks. CoDes involves
generating a series of intermediate descriptive steps based on: (i) the problem
statement for code generation, and (ii) the VHDL code for summarization. These
steps are then integrated with the original input prompt (problem statement or
code) and provided as input to the LLMs to generate the final output. Our
experiments demonstrate that the CoDes approach significantly surpasses the
standard prompting strategy across various metrics on both datasets. This
method not only improves the quality of VHDL code generation and summarization
but also serves as a framework for future research aimed at enhancing code LLMs
for VHDL.

### 18. Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yikang Liu, Wanyang Zhang, Yiming Wang, Jialong Tang, Pei Zhang, Baosong Yang, Fei Huang, Rui Wang, Hai Hu
- **URL**: <http://arxiv.org/abs/2507.12260v1>
- **Submitted**: 2025-07-16 14:06:05
- **Topic Keywords**: pairwise
- **Reason**: The paper focuses on translationese-index, a measure for translationese, which is not directly related to information retrieval, search technologies, or query understanding. Although it involves language models and fine-tuning, the context is specific to translationese and machine translation quality estimation, making it only loosely relevant to the user's research interests.

#### Abstract
> In this paper, we propose the first quantitative measure for translationese
-- the translationese-index (T-index) for graded and generalizable measurement
of translationese, computed from the likelihood ratios of two contrastively
fine-tuned language models (LMs). We use a synthesized dataset and a dataset
with translations in the wild to evaluate T-index's generalizability in
cross-domain settings and its validity against human judgments. Our results
show that T-index is both robust and efficient. T-index scored by two 0.5B LMs
fine-tuned on only 1-5k pairs of synthetic data can well capture translationese
in the wild. We find that the relative differences in T-indices between
translations can well predict pairwise translationese annotations obtained from
human annotators; and the absolute values of T-indices correlate well with
human ratings of degrees of translationese (Pearson's $r = 0.568$).
Additionally, the correlation between T-index and existing machine translation
(MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting
that T-index is not covered by these metrics and can serve as a complementary
metric in MT QE.

### 19. BOOKCOREF: Coreference Resolution at Book Scale

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Giuliano Martinelli, Tommaso Bonomo, Pere-Llu√≠s Huguet Cabot, Roberto Navigli
- **URL**: <http://arxiv.org/abs/2507.12075v1>
- **Submitted**: 2025-07-16 09:35:38
- **Comment**: Accepted to ACL 2025 Main Conference. 19 pages
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on coreference resolution at the book scale, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the topic of evaluating long texts, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are core areas of interest.

#### Abstract
> Coreference Resolution systems are typically evaluated on benchmarks
containing small- to medium-scale documents. When it comes to evaluating long
texts, however, existing benchmarks, such as LitBank, remain limited in length
and do not adequately assess system capabilities at the book scale, i.e., when
co-referring mentions span hundreds of thousands of tokens. To fill this gap,
we first put forward a novel automatic pipeline that produces high-quality
Coreference Resolution annotations on full narrative texts. Then, we adopt this
pipeline to create the first book-scale coreference benchmark, BOOKCOREF, with
an average document length of more than 200,000 tokens. We carry out a series
of experiments showing the robustness of our automatic procedure and
demonstrating the value of our resource, which enables current long-document
coreference systems to gain up to +20 CoNLL-F1 points when evaluated on full
books. Moreover, we report on the new challenges introduced by this
unprecedented book-scale setting, highlighting that current models fail to
deliver the same performance they achieve on smaller documents. We release our
data and code to encourage research and development of new book-scale
Coreference Resolution systems at https://github.com/sapienzanlp/bookcoref.

### 20. S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Suman Adhya, Debarshi Kumar Sanyal
- **URL**: <http://arxiv.org/abs/2507.12451v1>
- **Submitted**: 2025-07-16 17:47:45
- **Comment**: Accepted as a long paper for ACL 2025 main conference
- **Topic Keywords**: rag
- **Reason**: This paper focuses on topic modeling using a novel autoencoder architecture, which is not directly related to information retrieval, search technologies, or query understanding. While it employs neural networks, the application is in a different domain and does not address ranking models or user behavior modeling.

#### Abstract
> Modeling latent representations in a hyperspherical space has proven
effective for capturing directional similarities in high-dimensional text data,
benefiting topic modeling. Variational autoencoder-based neural topic models
(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical
structure. However, VAE-NTMs often suffer from posterior collapse, where the KL
divergence term in the objective function highly diminishes, leading to
ineffective latent representations. To mitigate this issue while modeling
hyperspherical structure in the latent space, we propose the Spherical Sliced
Wasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior
distribution supported on the unit hypersphere and leverages the Spherical
Sliced-Wasserstein distance to align the aggregated posterior distribution with
the prior. Experimental results demonstrate that S2WTM outperforms
state-of-the-art topic models, generating more coherent and diverse topics
while improving performance on downstream tasks.

### 21. Improving Contextual ASR via Multi-grained Fusion with Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Shilin Zhou, Zhenghua Li
- **URL**: <http://arxiv.org/abs/2507.12252v1>
- **Submitted**: 2025-07-16 13:59:32
- **Topic Keywords**: rag
- **Reason**: This paper focuses on improving Automatic Speech Recognition (ASR) models, which is not directly related to Information Retrieval (IR) or Search technologies. While it mentions the use of Large Language Models (LLMs), the context is different from query understanding, ranking models, and user behavior modeling, which are core areas of interest in IR.

#### Abstract
> While end-to-end Automatic Speech Recognition (ASR) models have shown
impressive performance in transcribing general speech, they often struggle to
accurately recognize contextually relevant keywords, such as proper nouns or
user-specific entities.
  Previous approaches have explored leveraging keyword dictionaries in the
textual modality to improve keyword recognition, either through token-level
fusion that guides token-by-token generation or phrase-level fusion that
enables direct copying of keyword phrases.
  However, these methods operate at different granularities and have their own
limitations.
  In this paper, we propose a novel multi-grained fusion approach that jointly
leverages the strengths of both token-level and phrase-level fusion with Large
Language Models (LLMs).
  Our approach incorporates a late-fusion strategy that elegantly combines
ASR's acoustic information with LLM's rich contextual knowledge, balancing
fine-grained token precision with holistic phrase-level understanding.
  Experiments on Chinese and English datasets demonstrate that our approach
achieves state-of-the-art performance on keyword-related metrics while
preserving high accuracy on non-keyword text.
  Ablation studies further confirm that the token-level and phrase-level
components both contribute significantly to the performance gains,
complementing each other in our joint multi-grained framework.
  The code and models will be publicly available at https://github.com/.

### 22. Towards few-shot isolated word reading assessment

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Reuben Smit, Retief Louw, Herman Kamper
- **URL**: <http://arxiv.org/abs/2507.12217v1>
- **Submitted**: 2025-07-16 13:20:32
- **Comment**: Accepted to SLaTE 2025
- **Topic Keywords**: rag
- **Reason**: The paper focuses on speech recognition and isolated word reading assessment in low-resource settings, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's use of self-supervised learned models and few-shot classification system is also not relevant to the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> We explore an ASR-free method for isolated word reading assessment in
low-resource settings. Our few-shot approach compares input child speech to a
small set of adult-provided reference templates. Inputs and templates are
encoded using intermediate layers from large self-supervised learned (SSL)
models. Using an Afrikaans child speech benchmark, we investigate design
options such as discretising SSL features and barycentre averaging of the
templates. Idealised experiments show reasonable performance for adults, but a
substantial drop for child speech input, even with child templates. Despite the
success of employing SSL representations in low-resource speech tasks, our work
highlights the limitations of SSL representations for processing child data
when used in a few-shot classification system.

### 23. Improving Data and Parameter Efficiency of Neural Language Models Using Representation Analysis

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Josip Jukiƒá
- **URL**: <http://arxiv.org/abs/2507.12004v1>
- **Submitted**: 2025-07-16 07:58:20
- **Topic Keywords**: rag
- **Reason**: The paper focuses on neural language models, representation analysis, and optimization techniques, which are not directly related to the user's research interests in Information Retrieval, Search technologies, and query understanding. While the paper touches on NLP tasks, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest for the user.

#### Abstract
> This thesis addresses challenges related to data and parameter efficiency in
neural language models, with a focus on representation analysis and the
introduction of new optimization techniques. The first part examines the
properties and dynamics of language representations within neural models,
emphasizing their significance in enhancing robustness and generalization. It
proposes innovative approaches based on representation smoothness, including
regularization strategies that utilize Jacobian and Hessian matrices to
stabilize training and mitigate sensitivity to input perturbations. The second
part focuses on methods to significantly enhance data and parameter efficiency
by integrating active learning strategies with parameter-efficient fine-tuning,
guided by insights from representation smoothness analysis. It presents
smoothness-informed early-stopping techniques designed to eliminate the need
for labeled validation sets and proposes innovative combinations of active
learning and parameter-efficient fine-tuning to reduce labeling efforts and
computational resources. Extensive experimental evaluations across various NLP
tasks demonstrate that these combined approaches substantially outperform
traditional methods in terms of performance, stability, and efficiency. The
third part explores weak supervision techniques enhanced by in-context learning
to effectively utilize unlabeled data, further reducing dependence on extensive
labeling. It shows that using in-context learning as a mechanism for weak
supervision enables models to better generalize from limited labeled data by
leveraging unlabeled examples more effectively during training. Comprehensive
empirical evaluations confirm significant gains in model accuracy,
adaptability, and robustness, especially in low-resource settings and dynamic
data environments.

### 24. IAM: Efficient Inference through Attention Mapping between Different-scale LLMs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yi Zhao, Zuchao Li, Hai Zhao
- **URL**: <http://arxiv.org/abs/2507.11953v1>
- **Submitted**: 2025-07-16 06:39:11
- **Comment**: ACL 2025
- **Topic Keywords**: rag
- **Reason**: The paper focuses on optimizing the efficiency of Large Language Models (LLMs) by leveraging attention mapping between different-scale models, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on optimization techniques, the context is specific to LLMs and does not align with the user's primary research interests.

#### Abstract
> LLMs encounter significant challenges in resource consumption nowadays,
especially with long contexts. Despite extensive efforts dedicate to enhancing
inference efficiency, these methods primarily exploit internal sparsity within
the models, without leveraging external information for optimization. We
identify the high similarity of attention matrices across different-scale LLMs,
which offers a novel perspective for optimization. We first conduct a
comprehensive analysis of how to measure similarity, how to select mapping
Layers and whether mapping is consistency. Based on these insights, we
introduce the IAM framework, which achieves dual benefits of accelerated
attention computation and reduced KV cache usage by performing attention
mapping between small and large LLMs. Our experimental results demonstrate that
IAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without
appreciably sacrificing performance. Experiments on different series of models
show the generalizability of IAM. Importantly, it is also orthogonal to many
existing KV cache optimization methods, making it a versatile addition to the
current toolkit for enhancing LLM efficiency.

### 25. POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yichen Xu, Liangyu Chen, Liang Zhang, Wenxuan Wang, Qin Jin
- **URL**: <http://arxiv.org/abs/2507.11939v1>
- **Submitted**: 2025-07-16 06:09:02
- **Comment**: Work in Progress
- **Topic Keywords**: rag
- **Reason**: This paper focuses on multilingual chart question answering, which is not directly related to my research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves vision-language models, the primary focus is on chart understanding and translation, which is not a central match for my interests.

#### Abstract
> Charts are a universally adopted medium for interpreting and communicating
data. However, existing chart understanding benchmarks are predominantly
English-centric, limiting their accessibility and applicability to global
audiences. In this paper, we present PolyChartQA, the first large-scale
multilingual chart question answering benchmark covering 22,606 charts and
26,151 question-answering pairs across 10 diverse languages. PolyChartQA is
built using a decoupled pipeline that separates chart data from rendering code,
allowing multilingual charts to be flexibly generated by simply translating the
data and reusing the code. We leverage state-of-the-art LLM-based translation
and enforce rigorous quality control in the pipeline to ensure the linguistic
and semantic consistency of the generated multilingual charts. PolyChartQA
facilitates systematic evaluation of multilingual chart understanding.
Experiments on both open- and closed-source large vision-language models reveal
a significant performance gap between English and other languages, especially
low-resource ones with non-Latin scripts. This benchmark lays a foundation for
advancing globally inclusive vision-language models.

### 26. AFPM: Alignment-based Frame Patch Modeling for Cross-Dataset EEG Decoding

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Xiaoqing Chen, Siyang Li, Dongrui Wu
- **URL**: <http://arxiv.org/abs/2507.11911v1>
- **Submitted**: 2025-07-16 04:55:09
- **Topic Keywords**: ctr
- **Reason**: The paper is not relevant to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The topic of EEG decoding and brain-computer interfaces is unrelated to the user's focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Electroencephalogram (EEG) decoding models for brain-computer interfaces
(BCIs) struggle with cross-dataset learning and generalization due to channel
layout inconsistencies, non-stationary signal distributions, and limited
neurophysiological prior integration. To address these issues, we propose a
plug-and-play Alignment-Based Frame-Patch Modeling (AFPM) framework, which has
two main components: 1) Spatial Alignment, which selects task-relevant channels
based on brain-region priors, aligns EEG distributions across domains, and
remaps the selected channels to a unified layout; and, 2) Frame-Patch Encoding,
which models multi-dataset signals into unified spatiotemporal patches for EEG
decoding. Compared to 17 state-of-the-art approaches that need dataset-specific
tuning, the proposed calibration-free AFPM achieves performance gains of up to
4.40% on motor imagery and 3.58% on event-related potential tasks. To our
knowledge, this is the first calibration-free cross-dataset EEG decoding
framework, substantially enhancing the practicalness of BCIs in real-world
applications.

### 27. Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Mohammad Samragh, Arnav Kundu, David Harrison, Kumari Nishu, Devang Naik, Minsik Cho, Mehrdad Farajtabar
- **URL**: <http://arxiv.org/abs/2507.11851v1>
- **Submitted**: 2025-07-16 02:31:40
- **Topic Keywords**: rag
- **Reason**: The paper focuses on language models and their ability to predict future tokens, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions speedups and improvements in certain tasks, the paper's primary contribution is in the area of natural language processing, which is only tangentially relevant to the user's research interests.

#### Abstract
> Autoregressive language models are constrained by their inherently sequential
nature, generating one token at a time. This paradigm limits inference speed
and parallelism, especially during later stages of generation when the
direction and semantics of text are relatively certain. In this work, we
propose a novel framework that leverages the inherent knowledge of vanilla
autoregressive language models about future tokens, combining techniques to
realize this potential and enable simultaneous prediction of multiple
subsequent tokens. Our approach introduces several key innovations: (1) a
masked-input formulation where multiple future tokens are jointly predicted
from a common prefix; (2) a gated LoRA formulation that preserves the original
LLM's functionality, while equipping it for multi-token prediction; (3) a
lightweight, learnable sampler module that generates coherent sequences from
the predicted future tokens; (4) a set of auxiliary training losses, including
a consistency loss, to enhance the coherence and accuracy of jointly generated
tokens; and (5) a speculative generation strategy that expands tokens
quadratically in the future while maintaining high fidelity. Our method
achieves significant speedups through supervised fine-tuning on pretrained
models. For example, it generates code and math nearly 5x faster, and improves
general chat and knowledge tasks by almost 2.5x. These gains come without any
loss in quality.

### 28. CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Meng Li, Timothy M. McPhillips, Dingmin Wang, Shin-Rong Tsai, Bertram Lud√§scher
- **URL**: <http://arxiv.org/abs/2507.11742v1>
- **Submitted**: 2025-07-15 21:14:08
- **Comment**: Preprint. Accepted to COLM 2025
- **Topic Keywords**: rag
- **Reason**: The paper focuses on understanding Python notebooks using Large Language Models (LLMs), which is not directly related to Information Retrieval (IR), Search technologies, or query understanding. Although it involves natural language processing, the context is specific to code analysis and not relevant to the user's primary research interests.

#### Abstract
> Recognizing the information flows and operations comprising data science and
machine learning Python notebooks is critical for evaluating, reusing, and
adapting notebooks for new tasks. Investigating a notebook via re-execution
often is impractical due to the challenges of resolving data and software
dependencies. While Large Language Models (LLMs) pre-trained on large codebases
have demonstrated effectiveness in understanding code without running it, we
observe that they fail to understand some realistic notebooks due to
hallucinations and long-context challenges. To address these issues, we propose
a notebook understanding task yielding an information flow graph and
corresponding cell execution dependency graph for a notebook, and demonstrate
the effectiveness of a pincer strategy that uses limited syntactic analysis to
assist full comprehension of the notebook using an LLM. Our Capture and Resolve
Assisted Bounding Strategy (CRABS) employs shallow syntactic parsing and
analysis of the abstract syntax tree (AST) to capture the correct
interpretation of a notebook between lower and upper estimates of the
inter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via
cell-by-cell zero-shot learning, thereby identifying the true data inputs and
outputs of each cell. We evaluate and demonstrate the effectiveness of our
approach using an annotated dataset of 50 representative, highly up-voted
Kaggle notebooks that together represent 3454 actual cell inputs and outputs.
The LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the
syntactic structure of these notebooks. Across 50 notebooks, CRABS achieves
average F1 scores of 98% identifying cell-to-cell information flows and 99%
identifying transitive cell execution dependencies.

### 29. Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Maximiliano Hormaz√°bal Lagos, H√©ctor Cerezo-Costas, Dimosthenis Karatzas
- **URL**: <http://arxiv.org/abs/2507.12490v1>
- **Submitted**: 2025-07-15 20:05:25
- **Comment**: This work has been accepted for presentation at the 16th Conference
  and Labs of the Evaluation Forum (CLEF 2025) and will be published in the
  proceedings by Springer in the Lecture Notes in Computer Science (LNCS)
  series. Please cite the published version when available
- **Topic Keywords**: rag
- **Reason**: The paper focuses on vision language models and document visual question answering, which is not directly related to the user's interests in Information Retrieval, Search technologies, and Natural Language Processing. The concepts of query understanding, ranking models, and user behavior modeling are not addressed in this paper.

#### Abstract
> We introduce EaGERS, a fully training-free and model-agnostic pipeline that
(1) generates natural language rationales via a vision language model, (2)
grounds these rationales to spatial sub-regions by computing multimodal
embedding similarities over a configurable grid with majority voting, and (3)
restricts the generation of responses only from the relevant regions selected
in the masked image. Experiments on the DocVQA dataset demonstrate that our
best configuration not only outperforms the base model on exact match accuracy
and Average Normalized Levenshtein Similarity metrics but also enhances
transparency and reproducibility in DocVQA without additional model
fine-tuning.

### 30. MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Atharva Naik, Lawanya Baghel, Dhakshin Govindarajan, Darsh Agrawal, Daniel Fried, Carolyn Rose
- **URL**: <http://arxiv.org/abs/2507.11687v1>
- **Submitted**: 2025-07-15 19:44:20
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The paper focuses on code quality analysis and large language models, which is a different domain and does not align with your research themes.

#### Abstract
> Large Language Models, though successful in code generation, struggle with
code quality analysis because they are limited by static training data and
can't easily adapt to evolving best practices. We introduce MetaLint, a new
instruction-following framework that formulates code quality analysis as the
task of detecting and fixing problematic semantic code fragments or code idioms
based on high-level specifications. Unlike conventional approaches that train
models on static, rule-based data, MetaLint employs instruction tuning on
synthetic linter-generated data to support easy-to-hard generalization,
enabling models to adapt to novel or complex code patterns without retraining.
To evaluate this, we construct a benchmark of challenging idioms inspired by
real-world coding standards such as Python Enhancement Proposals (PEPs) and
assess whether MetaLint-trained models reason adaptively or simply memorize.
Our results show that MetaLint improves generalization to unseen PEP idioms,
achieving a 70.37% F-score on idiom detection with the highest recall (70.43%)
among all evaluated models. It also achieves 26.73% on localization,
competitive for its 4B parameter size and comparable to larger state-of-the-art
models like o3-mini, highlighting its potential for future-proof code quality
analysis.

### 31. Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Mingjie Liu, Shizhe Diao, Jian Hu, Ximing Lu, Xin Dong, Hao Zhang, Alexander Bukharin, Shaokun Zhang, Jiaqi Zeng, Makesh Narsimhan Sreedhar, Gerald Shen, David Mosallanezhad, Di Zhang, Jonas Yang, June Yang, Oleksii Kuchaiev, Guilin Liu, Zhiding Yu, Pavlo Molchanov, Yejin Choi, Jan Kautz, Yi Dong
- **URL**: <http://arxiv.org/abs/2507.12507v1>
- **Submitted**: 2025-07-16 17:59:24
- **Comment**: 14 pages, 7 figures
- **Topic Keywords**: search
- **Reason**: The paper focuses on scaling up reinforcement learning for language models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions reasoning and optimization, the context is different from the user's interests in IR and NLP.

#### Abstract
> Recent advancements in reasoning-focused language models such as OpenAI's O1
and DeepSeek-R1 have shown that scaling test-time computation-through
chain-of-thought reasoning and iterative exploration-can yield substantial
improvements on complex tasks like mathematics and code generation. These
breakthroughs have been driven by large-scale reinforcement learning (RL),
particularly when combined with verifiable reward signals that provide
objective and grounded supervision. In this report, we investigate the effects
of prolonged reinforcement learning on a small language model across a diverse
set of reasoning domains. Our work identifies several key ingredients for
effective training, including the use of verifiable reward tasks, enhancements
to Group Relative Policy Optimization (GRPO), and practical techniques to
improve training stability and generalization. We introduce controlled KL
regularization, clipping ratio, and periodic reference policy resets as
critical components for unlocking long-term performance gains. Our model
achieves significant improvements over strong baselines, including +14.7% on
math, +13.9% on coding, and +54.8% on logic puzzle tasks. To facilitate
continued research, we release our model publicly.

### 32. Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Liu He, Yuanchao Li, Rui Feng, XinRan Han, Yin-Long Liu, Yuwei Yang, Zude Zhu, Jiahong Yuan
- **URL**: <http://arxiv.org/abs/2507.12356v1>
- **Submitted**: 2025-07-16 15:56:09
- **Comment**: 12 pages, 5 figures, conference or other essential info
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The topic of Alzheimer's Disease detection and speech perception is outside your primary focus, and the paper does not address any of the specific areas you mentioned.

#### Abstract
> Gender bias has been widely observed in speech perception tasks, influenced
by the fundamental voicing differences between genders. This study reveals a
gender bias in the perception of Alzheimer's Disease (AD) speech. In a
perception experiment involving 16 Chinese listeners evaluating both Chinese
and Greek speech, we identified that male speech was more frequently identified
as AD, with this bias being particularly pronounced in Chinese speech. Acoustic
analysis showed that shimmer values in male speech were significantly
associated with AD perception, while speech portion exhibited a significant
negative correlation with AD identification. Although language did not have a
significant impact on AD perception, our findings underscore the critical role
of gender bias in AD speech perception. This work highlights the necessity of
addressing gender bias when developing AD detection models and calls for
further research to validate model performance across different linguistic
contexts.

### 33. Nonlinear Concept Erasure: a Density Matching Approach

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Antoine Saillenfest, Pirmin Lemberger
- **URL**: <http://arxiv.org/abs/2507.12341v1>
- **Submitted**: 2025-07-16 15:36:15
- **Comment**: 17 pages, 10 figures, accepted for publication in ECAI 2025 (28th
  European Conference on Artificial Intelligence)
- **Topic Keywords**: rank
- **Reason**: This paper focuses on concept erasure in neural models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on fairness and bias, the context is not aligned with the user's primary research interests in IR and NLP.

#### Abstract
> Ensuring that neural models used in real-world applications cannot infer
sensitive information, such as demographic attributes like gender or race, from
text representations is a critical challenge when fairness is a concern. We
address this issue through concept erasure, a process that removes information
related to a specific concept from distributed representations while preserving
as much of the remaining semantic information as possible. Our approach
involves learning an orthogonal projection in the embedding space, designed to
make the class-conditional feature distributions of the discrete concept to
erase indistinguishable after projection. By adjusting the rank of the
projector, we control the extent of information removal, while its
orthogonality ensures strict preservation of the local structure of the
embeddings. Our method, termed $\overline{\mathrm{L}}$EOPARD, achieves
state-of-the-art performance in nonlinear erasure of a discrete attribute on
classic natural language processing benchmarks. Furthermore, we demonstrate
that $\overline{\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear
classifiers, thereby promoting fairness.

### 34. MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Artem Chervyakov, Alexander Kharitonov, Pavel Zadorozhny, Adamenko Pavel, Rodion Levichev, Dmitrii Vorobev, Dmitrii Salikhov, Aidar Valeev, Alena Pestova, Maria Dziuba, Ilseyar Alimova, Artem Zavgorodnev, Aleksandr Medvedev, Stanislav Moiseev, Elena Bruches, Daniil Grebenkin, Roman Derunets, Vikulov Vladimir, Anton Emelyanov, Dmitrii Babaev, Vladimir V. Ivanov, Valentin Malykh, Alena Fenogenova
- **URL**: <http://arxiv.org/abs/2507.12284v2>
- **Submitted**: 2025-07-16 14:31:33
- **Topic Keywords**: search
- **Reason**: The paper focuses on code generation and evaluation, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions language models, the context is different from the user's interests in NLP and IR.

#### Abstract
> Advancements in LLMs have enhanced task automation in software engineering;
however, current evaluations primarily focus on natural language tasks,
overlooking code quality. Most benchmarks prioritize high-level reasoning over
executable code and real-world performance, leaving gaps in understanding true
capabilities and risks associated with these models in production. To address
this issue, we propose MERA Code, a new addition to the MERA benchmark family,
specifically focused on evaluating code for the latest code generation LLMs in
Russian. This benchmark includes 11 evaluation tasks that span 8 programming
languages. Our proposed evaluation methodology features a taxonomy that
outlines the practical coding skills necessary for models to complete these
tasks. The benchmark comprises an open-source codebase for users to conduct
MERA assessments, a scoring system compatible with various programming
environments, and a platform featuring a leaderboard and submission system. We
evaluate open LLMs and frontier API models, analyzing their limitations in
terms of practical coding tasks in non-English languages. We are publicly
releasing MERA to guide future research, anticipate groundbreaking features in
model development, and standardize evaluation procedures.

### 35. RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Vladimir Bogachev, Vladimir Aletov, Alexander Molozhavenko, Denis Bobkov, Vera Soboleva, Aibek Alanov, Maxim Rakhuba
- **URL**: <http://arxiv.org/abs/2507.12142v1>
- **Submitted**: 2025-07-16 11:17:12
- **Topic Keywords**: rank
- **Reason**: The paper focuses on LoRA optimization, which is not directly related to information retrieval, query understanding, or ranking models. While it mentions language models, the context is not about search technologies or user behavior modeling, making it only loosely relevant to the user's research interests.

#### Abstract
> Low-Rank Adaptation (LoRA) has become a widely adopted standard for
parameter-efficient fine-tuning of large language models (LLMs), significantly
reducing memory and computational demands. However, challenges remain,
including finding optimal initialization strategies or mitigating
overparametrization in low-rank matrix factorization. In this work, we propose
a novel approach that addresses both of the challenges simultaneously within a
unified framework. Our method treats a set of fixed-rank LoRA matrices as a
smooth manifold. Considering adapters as elements on this manifold removes
overparametrization, while determining the direction of the fastest loss
decrease along the manifold provides initialization. Special care is taken to
obtain numerically stable and computationally efficient implementation of our
method, using best practices from numerical linear algebra and Riemannian
optimization. Experimental results on LLM and diffusion model architectures
demonstrate that RiemannLoRA consistently improves both convergence speed and
final performance over standard LoRA and its state-of-the-art modifications.

### 36. Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Ziyu Ge, Gabriel Chua, Leanne Tan, Roy Ka-Wei Lee
- **URL**: <http://arxiv.org/abs/2507.11966v1>
- **Submitted**: 2025-07-16 06:58:02
- **Topic Keywords**: rank
- **Reason**: The paper focuses on low-resource language translation, specifically Singlish, and toxicity-aware prompting, which is not directly related to the user's interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it touches on NLP, the topic is not aligned with the user's primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> As online communication increasingly incorporates under-represented languages
and colloquial dialects, standard translation systems often fail to preserve
local slang, code-mixing, and culturally embedded markers of harmful speech.
Translating toxic content between low-resource language pairs poses additional
challenges due to scarce parallel data and safety filters that sanitize
offensive expressions. In this work, we propose a reproducible, two-stage
framework for toxicity-preserving translation, demonstrated on a code-mixed
Singlish safety corpus. First, we perform human-verified few-shot prompt
engineering: we iteratively curate and rank annotator-selected Singlish-target
examples to capture nuanced slang, tone, and toxicity. Second, we optimize
model-prompt pairs by benchmarking several large language models using semantic
similarity via direct and back-translation. Quantitative human evaluation
confirms the effectiveness and efficiency of our pipeline. Beyond improving
translation quality, our framework contributes to the safety of multicultural
LLMs by supporting culturally sensitive moderation and benchmarking in
low-resource contexts. By positioning Singlish as a testbed for inclusive NLP,
we underscore the importance of preserving sociolinguistic nuance in real-world
applications such as content moderation and regional platform governance.

### 37. A Survey of Deep Learning for Geometry Problem Solving

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Jianzhe Ma, Wenxuan Wang, Qin Jin
- **URL**: <http://arxiv.org/abs/2507.11936v1>
- **Submitted**: 2025-07-16 06:03:08
- **Comment**: Work in progress
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests as it focuses on deep learning for geometry problem solving, which is outside the scope of information retrieval, search technologies, and natural language processing. The paper's topics, such as multimodal large language models and geometry problem solving, do not align with your primary research areas.

#### Abstract
> Geometry problem solving is a key area of mathematical reasoning, which is
widely involved in many important fields such as education, mathematical
ability assessment of artificial intelligence, and multimodal ability
assessment. In recent years, the rapid development of deep learning technology,
especially the rise of multimodal large language models, has triggered a
widespread research boom. This paper provides a survey of the applications of
deep learning in geometry problem solving, including (i) a comprehensive
summary of the relevant tasks in geometry problem solving; (ii) a thorough
review of related deep learning methods; (iii) a detailed analysis of
evaluation metrics and methods; and (iv) a critical discussion of the current
challenges and future directions that can be explored. Our goal is to provide a
comprehensive and practical reference of deep learning for geometry problem
solving to promote further developments in this field. We create a continuously
updated list of papers on GitHub: https://github.com/majianz/dl4gps.

### 38. Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Farideh Majidi, Ziaeddin Beheshtifard
- **URL**: <http://arxiv.org/abs/2507.11634v1>
- **Submitted**: 2025-07-15 18:13:25
- **Comment**: Proceedings of the First National Conference on Artificial
  Intelligence and Emerging Research: Convergence of Humans and Intelligent
  Systems
- **Topic Keywords**: search
- **Reason**: This paper focuses on cross-lingual sentiment analysis in Persian, using few-shot learning and incremental adaptation, which is not directly related to the user's interests in Information Retrieval, Search technologies, and query understanding. While it involves NLP and deep learning, the specific application and methodology are not aligned with the user's research themes.

#### Abstract
> This research examines cross-lingual sentiment analysis using few-shot
learning and incremental learning methods in Persian. The main objective is to
develop a model capable of performing sentiment analysis in Persian using
limited data, while getting prior knowledge from high-resource languages. To
achieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and
DistilBERT) were employed, which were fine-tuned using few-shot and incremental
learning approaches on small samples of Persian data from diverse sources,
including X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled
the models to learn from a broad range of contexts. Experimental results show
that the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%
accuracy on Persian sentiment analysis. These findings highlight the
effectiveness of combining few-shot learning and incremental learning with
multilingual pre-trained models.

---

