[
    {
        "title": "FASTRIC: Prompt Specification Language for Verifiable LLM Interactions",
        "abstract": "Large Language Models (LLMs) execute complex multi-turn interaction protocols but lack formal specifications to verify execution against designer intent. We introduce FASTRIC, a Prompt Specification Language that makes implicit Finite State Machines (FSMs) explicit in natural language prompts, enabling conformance verification through execution trace analysis. The LLM serves as intelligent execution agent: interpreting designer-encoded FSMs to execute specified behavioral roles. Unlike symbolic specification languages requiring parsers and compilers, FASTRIC leverages LLMs as unified infrastructure-simultaneously parser, interpreter, runtime environment, and development assistant. FASTRIC guides designers to articulate seven FSM elements (Final States, Agents, States, Triggers, Roles, Initial State, Constraints) structuring multi-turn interactions. Specification formality-ranging from implicit descriptions that frontier models infer to explicit step-by-step instructions for weaker models-serves as a design parameter. We introduce procedural conformance as verification metric measuring execution adherence to FSM specifications. Testing a 3-state kindergarten tutoring FSM across four formality levels and three model scales (14.7B, 685B, 1T+ parameters) reveals optimal specification formality is a function of model capacity. DeepSeek-V3.2 (685B) achieves perfect conformance (1.00) at L2-L4; ChatGPT-5 (~1T) peaks at L3 (0.90) before collapsing at L4 (0.39); Phi4 (14.7B) shows no stable optimum with high variance (SD=0.16-0.36). These findings reveal model-specific formality ranges-\"Goldilocks zones\"-where specifications provide sufficient structure without over-constraint, establishing Prompt Specification Engineering for creating verifiable interaction protocols, transforming multi-turn interaction design from heuristic art to systematic engineering with measurable procedural guarantees.",
        "url": "http://arxiv.org/abs/2512.18940v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18940v1",
        "arxiv_id": "2512.18940v1",
        "authors": [
            "Wen-Long Jin"
        ],
        "submitted": "2025-12-22 01:19:50",
        "source": "arxiv",
        "comment": "13 pages, 3 figures. Supplementary materials at https://doi.org/10.17605/OSF.IO/PV6R3"
    },
    {
        "title": "Remedy-R: Generative Reasoning for Machine Translation Evaluation without Error Annotations",
        "abstract": "Over the years, automatic MT metrics have hillclimbed benchmarks and presented strong and sometimes human-level agreement with human ratings. Yet they remain black-box, offering little insight into their decision-making and often failing under real-world out-of-distribution (OOD) inputs. We introduce Remedy-R, a reasoning-driven generative MT metric trained with reinforcement learning from pairwise translation preferences, without requiring error-span annotations or distillation from closed LLMs. Remedy-R produces step-by-step analyses of accuracy, fluency, and completeness, followed by a final score, enabling more interpretable assessments. With only 60K training pairs across two language pairs, Remedy-R remains competitive with top scalar metrics and GPT-4-based judges on WMT22-24 meta-evaluation, generalizes to other languages, and exhibits strong robustness on OOD stress tests. Moreover, Remedy-R models generate self-reflective feedback that can be reused for translation improvement. Building on this finding, we introduce Remedy-R Agent, a simple evaluate-revise pipeline that leverages Remedy-R's evaluation analysis to refine translations. This agent consistently improves translation quality across diverse models, including Qwen2.5, ALMA-R, GPT-4o-mini, and Gemini-2.0-Flash, suggesting that Remedy-R's reasoning captures translation-relevant information and is practically useful.",
        "url": "http://arxiv.org/abs/2512.18906v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18906v1",
        "arxiv_id": "2512.18906v1",
        "authors": [
            "Shaomu Tan",
            "Ryosuke Mitani",
            "Ritvik Choudhary",
            "Qiyu Wu",
            "Toshiyuki Sekiya",
            "Christof Monz"
        ],
        "submitted": "2025-12-21 22:37:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Can LLMs Estimate Student Struggles? Human-AI Difficulty Alignment with Proficiency Simulation for Item Difficulty Prediction",
        "abstract": "Accurate estimation of item (question or task) difficulty is critical for educational assessment but suffers from the cold start problem. While Large Language Models demonstrate superhuman problem-solving capabilities, it remains an open question whether they can perceive the cognitive struggles of human learners. In this work, we present a large-scale empirical analysis of Human-AI Difficulty Alignment for over 20 models across diverse domains such as medical knowledge and mathematical reasoning. Our findings reveal a systematic misalignment where scaling up model size is not reliably helpful; instead of aligning with humans, models converge toward a shared machine consensus. We observe that high performance often impedes accurate difficulty estimation, as models struggle to simulate the capability limitations of students even when being explicitly prompted to adopt specific proficiency levels. Furthermore, we identify a critical lack of introspection, as models fail to predict their own limitations. These results suggest that general problem-solving capability does not imply an understanding of human cognitive struggles, highlighting the challenge of using current models for automated difficulty prediction.",
        "url": "http://arxiv.org/abs/2512.18880v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18880v1",
        "arxiv_id": "2512.18880v1",
        "authors": [
            "Ming Li",
            "Han Chen",
            "Yunze Xiao",
            "Jian Chen",
            "Hong Jiao",
            "Tianyi Zhou"
        ],
        "submitted": "2025-12-21 20:41:36",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Application of deep learning approaches for medieval historical documents transcription",
        "abstract": "Handwritten text recognition and optical character recognition solutions show excellent results with processing data of modern era, but efficiency drops with Latin documents of medieval times. This paper presents a deep learning method to extract text information from handwritten Latin-language documents of the 9th to 11th centuries. The approach takes into account the properties inherent in medieval documents. The paper provides a brief introduction to the field of historical document transcription, a first-sight analysis of the raw data, and the related works and studies. The paper presents the steps of dataset development for further training of the models. The explanatory data analysis of the processed data is provided as well. The paper explains the pipeline of deep learning models to extract text information from the document images, from detecting objects to word recognition using classification models and embedding word images. The paper reports the following results: recall, precision, F1 score, intersection over union, confusion matrix, and mean string distance. The plots of the metrics are also included. The implementation is published on the GitHub repository.",
        "url": "http://arxiv.org/abs/2512.18865v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18865v1",
        "arxiv_id": "2512.18865v1",
        "authors": [
            "Maksym Voloshchuk",
            "Bohdana Zarembovska",
            "Mykola Kozlenko"
        ],
        "submitted": "2025-12-21 19:43:30",
        "source": "arxiv",
        "comment": "15 pages, 15 figures, 4 tables. Originally published by CEUR Workshop Proceedings (CEUR-WS.org, ISSN 1613-0073), available: https://ceur-ws.org/Vol-4133/S_05_Kozlenko.pdf"
    },
    {
        "title": "Merge on workspaces as Hopf algebra Markov chain",
        "abstract": "We study the dynamical properties of a Hopf algebra Markov chain with state space the binary rooted forests with labelled leaves. This Markovian dynamical system describes the core computational process of structure formation and transformation in syntax via the Merge operation, according to Chomsky's Minimalism model of generative linguistics. The dynamics decomposes into an ergodic dynamical system with uniform stationary distribution, given by the action of Internal Merge, while the contributions of External Merge and (a minimal form of) Sideward Merge reduce to a simpler Markov chain with state space the set of partitions and with combinatorial weights. The Sideward Merge part of the dynamics prevents convergence to fully formed connected structures (trees), unless the different forms of Merge are weighted by a cost function, as predicted by linguistic theory. Results on the asymptotic behavior of the Perron-Frobenius eigenvalue and eigenvector in this weighted case, obtained in terms of an associated Perron-Frobenius problem in the tropical semiring, show that the usual cost functions (Minimal Search and Resource Restrictions) proposed in the linguistic literature do not suffice to obtain convergence to the tree structures, while an additional optimization property based on the Shannon entropy achieves the expected result for the dynamics. We also comment on the introduction of continuous parameters related to semantic embedding and other computational models, and also on some filtering of the dynamics by coloring rules that model the linguistic filtering by theta roles and phase structure, and on parametric variation and the process of parameter setting in Externalization.",
        "url": "http://arxiv.org/abs/2512.18861v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18861v1",
        "arxiv_id": "2512.18861v1",
        "authors": [
            "Matilde Marcolli",
            "David Skigin"
        ],
        "submitted": "2025-12-21 19:26:41",
        "source": "arxiv",
        "comment": "80 pages, LaTeX, 1 png figure"
    },
    {
        "title": "Toward Human-Centered AI-Assisted Terminology Work",
        "abstract": "The rapid diffusion of generative artificial intelligence is transforming terminology work. While this technology promises gains in efficiency, its unstructured adoption risks weakening professional autonomy, amplifying bias, and eroding linguistic and conceptual diversity. This paper argues that a human-centered approach to artificial intelligence has become a necessity for terminology work. Building on research in artificial intelligence and translation studies, it proposes a human-centered framework that conceptualizes artificial intelligence as a means of amplifying the terminologist's capabilities, rather than replacing them. The framework is organized around three interrelated dimensions: the augmented terminologist, ethical AI, and human-centered design. Together, these dimensions emphasize the compatibility of high automation with strong human control, the central role of terminologists in bias mitigation, and the importance of designing AI tools and workflows around the needs, values, and well-being of the terminologist. The paper concludes by stressing that current choices in AI adoption will shape not only terminological practice, but also the preservation of accuracy, adequacy, and diversity in terminology and specialized knowledge.",
        "url": "http://arxiv.org/abs/2512.18859v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18859v1",
        "arxiv_id": "2512.18859v1",
        "authors": [
            "Antonio San Martin"
        ],
        "submitted": "2025-12-21 19:16:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MDToC: Metacognitive Dynamic Tree of Concepts for Boosting Mathematical Problem-Solving of Large Language Models",
        "abstract": "Despite advances in mathematical reasoning capabilities, Large Language Models (LLMs) still struggle with calculation verification when using established prompting techniques. We present MDToC (Metacognitive Dynamic Tree of Concepts), a three-phase approach that constructs a concept tree, develops accuracy-verified calculations for each concept, and employs majority voting to evaluate competing solutions. Evaluations across CHAMP, MATH, and Game-of-24 benchmarks demonstrate our MDToC's effectiveness, with GPT-4-Turbo achieving 58.1\\% on CHAMP, 86.6\\% on MATH, and 85\\% on Game-of-24 - outperforming GoT by 5\\%, 5.4\\%, and 4\\% on all these tasks, respectively, without hand-engineered hints. MDToC consistently surpasses existing prompting methods across all backbone models, yielding improvements of up to 7.6\\% over ToT and 6.2\\% over GoT, establishing metacognitive calculation verification as a promising direction for enhanced mathematical reasoning.",
        "url": "http://arxiv.org/abs/2512.18841v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18841v1",
        "arxiv_id": "2512.18841v1",
        "authors": [
            "Tung Duong Ta",
            "Tim Oates"
        ],
        "submitted": "2025-12-21 18:11:24",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AraMix: Recycling, Refiltering, and Deduplicating to Deliver the Largest Arabic Pretraining Corpus",
        "abstract": "We present AraMix, a deduplicated Arabic pretraining corpus containing approximately 178 billion tokens across 179 million documents. Rather than scraping the web again, AraMix demonstrates that substantial value lies in systematically reusing and curating existing pretraining datasets: we combine seven publicly available Arabic web datasets, apply quality filtering designed specifically for Arabic text to re-filter some datasets, and perform cross-dataset deduplication, both MinHash and sentence-level. This approach reveals that nearly 60% of tokens across these independently collected corpora are duplicates, redundancy that any new scraping efforts will reproduce. Our work suggests that for lower resource languages, investment in curation pipelines for existing data yields greater returns than additional web crawls, an approach that allowed us to curate the largest heavily filtered publicly available Arabic pretraining corpus.",
        "url": "http://arxiv.org/abs/2512.18834v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18834v1",
        "arxiv_id": "2512.18834v1",
        "authors": [
            "Sultan Alrashed",
            "Francesco Orabona"
        ],
        "submitted": "2025-12-21 17:36:26",
        "source": "arxiv",
        "comment": "Initial version, without pretraining experiments"
    },
    {
        "title": "From Word to World: Can Large Language Models be Implicit Text-based World Models?",
        "abstract": "Agentic reinforcement learning increasingly relies on experience-driven scaling, yet real-world environments remain non-adaptive, limited in coverage, and difficult to scale. World models offer a potential way to improve learning efficiency through simulated experience, but it remains unclear whether large language models can reliably serve this role and under what conditions they meaningfully benefit agents. We study these questions in text-based environments, which provide a controlled setting to reinterpret language modeling as next-state prediction under interaction. We introduce a three-level framework for evaluating LLM-based world models: (i) fidelity and consistency, (ii) scalability and robustness, and (iii) agent utility. Across five representative environments, we find that sufficiently trained world models maintain coherent latent state, scale predictably with data and model size, and improve agent performance via action verification, synthetic trajectory generation, and warm-starting reinforcement learning. Meanwhile, these gains depend critically on behavioral coverage and environment complexity, delineating clear boundry on when world modeling effectively supports agent learning.",
        "url": "http://arxiv.org/abs/2512.18832v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18832v1",
        "arxiv_id": "2512.18832v1",
        "authors": [
            "Yixia Li",
            "Hongru Wang",
            "Jiahao Qiu",
            "Zhenfei Yin",
            "Dongdong Zhang",
            "Cheng Qian",
            "Zeping Li",
            "Pony Ma",
            "Guanhua Chen",
            "Heng Ji",
            "Mengdi Wang"
        ],
        "submitted": "2025-12-21 17:28:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "From Natural Language to Control Signals: A Conceptual Framework for Semantic Channel Finding in Complex Experimental Infrastructure",
        "abstract": "Modern experimental platforms such as particle accelerators, fusion devices, telescopes, and industrial process control systems expose tens to hundreds of thousands of control and diagnostic channels accumulated over decades of evolution. Operators and AI systems rely on informal expert knowledge, inconsistent naming conventions, and fragmented documentation to locate signals for monitoring, troubleshooting, and automated control, creating a persistent bottleneck for reliability, scalability, and language-model-driven interfaces. We formalize semantic channel finding-mapping natural-language intent to concrete control-system signals-as a general problem in complex experimental infrastructure, and introduce a four-paradigm framework to guide architecture selection across facility-specific data regimes. The paradigms span (i) direct in-context lookup over curated channel dictionaries, (ii) constrained hierarchical navigation through structured trees, (iii) interactive agent exploration using iterative reasoning and tool-based database queries, and (iv) ontology-grounded semantic search that decouples channel meaning from facility-specific naming conventions. We demonstrate each paradigm through proof-of-concept implementations at four operational facilities spanning two orders of magnitude in scale-from compact free-electron lasers to large synchrotron light sources-and diverse control-system architectures, from clean hierarchies to legacy environments. These implementations achieve 90-97% accuracy on expert-curated operational queries.",
        "url": "http://arxiv.org/abs/2512.18779v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18779v1",
        "arxiv_id": "2512.18779v1",
        "authors": [
            "Thorsten Hellert",
            "Nikolay Agladze",
            "Alex Giovannone",
            "Jan Jug",
            "Frank Mayet",
            "Mark Sherwin",
            "Antonin Sulc",
            "Chris Tennant"
        ],
        "submitted": "2025-12-21 15:46:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Code2Doc: A Quality-First Curated Dataset for Code Documentation",
        "abstract": "The performance of automatic code documentation generation models depends critically on the quality of the training data used for supervision. However, most existing code documentation datasets are constructed through large scale scraping of public repositories with limited quality control. As a result, they often contain noisy documentation, extensive duplication, and increasing contamination from AI generated content. These issues weaken the supervision signal available to learning-based models and complicate evaluation.\n  We introduce \\textbf{Code2Doc}, a quality-first curated dataset for function-level code documentation generation. Code2Doc consists of 13,358 high-quality function-documentation pairs extracted from widely used open-source repositories spanning five programming languages: Python, Java, TypeScript, JavaScript, and C++. The dataset is constructed using a four-stage curation pipeline that enforces documentation completeness and clarity, filters functions based on structural and complexity criteria, removes exact and near-duplicate code, and identifies documentation likely to be AI generated. Starting from 52,069 extracted candidates, only 25.6 percent satisfy all quality constraints.\n  We provide a detailed analysis of the resulting dataset, which achieves a mean documentation quality score of 6.93 out of 10. Overall, 86.9% of samples contain explicit type annotations, and only 2.9\\% are flagged as potentially AI generated. Baseline experiments show that fine-tuning a large language model on Code2Doc yields relative improvements of 29.47% in BLEU and 24.04% in ROUGE-L over zero shot performance, despite the modest dataset size. We release both the dataset and the full curation pipeline to support reproducible research on automatic code documentation generation.",
        "url": "http://arxiv.org/abs/2512.18748v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18748v1",
        "arxiv_id": "2512.18748v1",
        "authors": [
            "Recep Kaan Karaman",
            "Meftun Akarsu"
        ],
        "submitted": "2025-12-21 14:28:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MemEvolve: Meta-Evolution of Agent Memory Systems",
        "abstract": "Self-evolving memory systems are unprecedentedly reshaping the evolutionary paradigm of large language model (LLM)-based agents. Prior work has predominantly relied on manually engineered memory architectures to store trajectories, distill experience, and synthesize reusable tools, enabling agents to evolve on the fly within environment interactions. However, this paradigm is fundamentally constrained by the staticity of the memory system itself: while memory facilitates agent-level evolving, the underlying memory architecture cannot be meta-adapted to diverse task contexts. To address this gap, we propose MemEvolve, a meta-evolutionary framework that jointly evolves agents' experiential knowledge and their memory architecture, allowing agent systems not only to accumulate experience but also to progressively refine how they learn from it. To ground MemEvolve in prior research and foster openness in future self-evolving systems, we introduce EvolveLab, a unified self-evolving memory codebase that distills twelve representative memory systems into a modular design space (encode, store, retrieve, manage), providing both a standardized implementation substrate and a fair experimental arena. Extensive evaluations on four challenging agentic benchmarks demonstrate that MemEvolve achieves (I) substantial performance gains, improving frameworks such as SmolAgent and Flash-Searcher by up to $17.06\\%$; and (II) strong cross-task and cross-LLM generalization, designing memory architectures that transfer effectively across diverse benchmarks and backbone models.",
        "url": "http://arxiv.org/abs/2512.18746v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18746v1",
        "arxiv_id": "2512.18746v1",
        "authors": [
            "Guibin Zhang",
            "Haotian Ren",
            "Chong Zhan",
            "Zhenhong Zhou",
            "Junhao Wang",
            "He Zhu",
            "Wangchunshu Zhou",
            "Shuicheng Yan"
        ],
        "submitted": "2025-12-21 14:26:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search",
        "abstract": "The ability for AI agents to \"think with images\" requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search -- locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at https://github.com/m-Just/InSight-o3 .",
        "url": "http://arxiv.org/abs/2512.18745v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18745v1",
        "arxiv_id": "2512.18745v1",
        "authors": [
            "Kaican Li",
            "Lewei Yao",
            "Jiannan Wu",
            "Tiezheng Yu",
            "Jierun Chen",
            "Haoli Bai",
            "Lu Hou",
            "Lanqing Hong",
            "Wei Zhang",
            "Nevin L. Zhang"
        ],
        "submitted": "2025-12-21 14:23:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CIRR: Causal-Invariant Retrieval-Augmented Recommendation with Faithful Explanations under Distribution Shift",
        "abstract": "Recent advances in retrieval-augmented generation (RAG) have shown promise in enhancing recommendation systems with external knowledge. However, existing RAG-based recommenders face two critical challenges: (1) vulnerability to distribution shifts across different environments (e.g., time periods, user segments), leading to performance degradation in out-of-distribution (OOD) scenarios, and (2) lack of faithful explanations that can be verified against retrieved evidence. In this paper, we propose CIRR, a Causal-Invariant Retrieval-Augmented Recommendation framework that addresses both challenges simultaneously. CIRR learns environment-invariant user preference representations through causal inference, which guide a debiased retrieval process to select relevant evidence from multiple sources. Furthermore, we introduce consistency constraints that enforce faithfulness between retrieved evidence, generated explanations, and recommendation outputs. Extensive experiments on two real-world datasets demonstrate that CIRR achieves robust performance under distribution shifts, reducing performance degradation from 15.4% (baseline) to only 5.6% in OOD scenarios, while providing more faithful and interpretable explanations (26% improvement in faithfulness score) compared to state-of-the-art baselines.",
        "url": "http://arxiv.org/abs/2512.18683v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18683v1",
        "arxiv_id": "2512.18683v1",
        "authors": [
            "Sebastian Sun"
        ],
        "submitted": "2025-12-21 10:41:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Solver-Independent Automated Problem Formulation via LLMs for High-Cost Simulation-Driven Design",
        "abstract": "In the high-cost simulation-driven design domain, translating ambiguous design requirements into a mathematical optimization formulation is a bottleneck for optimizing product performance. This process is time-consuming and heavily reliant on expert knowledge. While large language models (LLMs) offer potential for automating this task, existing approaches either suffer from poor formalization that fails to accurately align with the design intent or rely on solver feedback for data filtering, which is unavailable due to the high simulation costs. To address this challenge, we propose APF, a framework for solver-independent, automated problem formulation via LLMs designed to automatically convert engineers' natural language requirements into executable optimization models. The core of this framework is an innovative pipeline for automatically generating high-quality data, which overcomes the difficulty of constructing suitable fine-tuning datasets in the absence of high-cost solver feedback with the help of data generation and test instance annotation. The generated high-quality dataset is used to perform supervised fine-tuning on LLMs, significantly enhancing their ability to generate accurate and executable optimization problem formulations. Experimental results on antenna design demonstrate that APF significantly outperforms the existing methods in both the accuracy of requirement formalization and the quality of resulting radiation efficiency curves in meeting the design goals.",
        "url": "http://arxiv.org/abs/2512.18682v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18682v1",
        "arxiv_id": "2512.18682v1",
        "authors": [
            "Yuchen Li",
            "Handing Wang",
            "Bing Xue",
            "Mengjie Zhang",
            "Yaochu Jin"
        ],
        "submitted": "2025-12-21 10:40:36",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "brat: Aligned Multi-View Embeddings for Brain MRI Analysis",
        "abstract": "We present brat (brain report alignment transformer), a multi-view representation learning framework for brain magnetic resonance imaging (MRI) trained on MRIs paired with clinical reports. Brain MRIs present unique challenges due to the presence of numerous, highly varied, and often subtle abnormalities that are localized to a few slices within a 3D volume. To address these challenges, we introduce a brain MRI dataset $10\\times$ larger than existing ones, containing approximately 80,000 3D scans with corresponding radiology reports, and propose a multi-view pre-training approach inspired by advances in document retrieval. We develop an implicit query-feature matching mechanism and adopt concepts from quality-diversity to obtain multi-view embeddings of MRIs that are aligned with the clinical features given by report sentences. We evaluate our approach across multiple vision-language and vision tasks, demonstrating substantial performance improvements. The brat foundation models are publicly released.",
        "url": "http://arxiv.org/abs/2512.18679v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18679v1",
        "arxiv_id": "2512.18679v1",
        "authors": [
            "Maxime Kayser",
            "Maksim Gridnev",
            "Wanting Wang",
            "Max Bain",
            "Aneesh Rangnekar",
            "Avijit Chatterjee",
            "Aleksandr Petrov",
            "Harini Veeraraghavan",
            "Nathaniel C. Swinburne"
        ],
        "submitted": "2025-12-21 10:37:31",
        "source": "arxiv",
        "comment": "First round accept at WACV 2026"
    },
    {
        "title": "Does It Tie Out? Towards Autonomous Legal Agents in Venture Capital",
        "abstract": "Before closing venture capital financing rounds, lawyers conduct diligence that includes tying out the capitalization table: verifying that every security (for example, shares, options, warrants) and issuance term (for example, vesting schedules, acceleration triggers, transfer restrictions) is supported by large sets of underlying legal documentation. While LLMs continue to improve on legal benchmarks, specialized legal workflows, such as capitalization tie-out, remain out of reach even for strong agentic systems. The task requires multi-document reasoning, strict evidence traceability, and deterministic outputs that current approaches fail to reliably deliver. We characterize capitalization tie-out as an instance of a real-world benchmark for legal AI, analyze and compare the performance of existing agentic systems, and propose a world model architecture toward tie-out automation-and more broadly as a foundation for applied legal intelligence.",
        "url": "http://arxiv.org/abs/2512.18658v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18658v1",
        "arxiv_id": "2512.18658v1",
        "authors": [
            "Pierre Colombo",
            "Malik Boudiaf",
            "Allyn Sweet",
            "Michael Desa",
            "Hongxi Wang",
            "Kevin Candra",
            "Sym√©on del Marmol"
        ],
        "submitted": "2025-12-21 09:12:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LLM-CAS: Dynamic Neuron Perturbation for Real-Time Hallucination Correction",
        "abstract": "Large language models (LLMs) often generate hallucinated content that lacks factual or contextual grounding, limiting their reliability in critical applications. Existing approaches such as supervised fine-tuning and reinforcement learning from human feedback are data intensive and computationally expensive, while static parameter editing methods struggle with context dependent errors and catastrophic forgetting.\n  We propose LLM-CAS, a framework that formulates real-time hallucination correction as a hierarchical reinforcement learning problem. LLM-CAS trains an agent to learn a policy that dynamically selects temporary neuron perturbations during inference based on the current context. Unlike prior dynamic approaches that rely on heuristic or predefined adjustments, this policy driven mechanism enables adaptive and fine grained correction without permanent parameter modification.\n  Experiments across multiple language models demonstrate that LLM-CAS consistently improves factual accuracy, achieving gains of 10.98 percentage points on StoryCloze, 2.71 points on TriviaQA, and 2.06 points on the MC1 score of TruthfulQA. These results outperform both static editing methods such as ITI and CAA and the dynamic SADI framework. Overall, LLM-CAS provides an efficient and context aware solution for improving the reliability of LLMs, with promising potential for future multimodal extensions.",
        "url": "http://arxiv.org/abs/2512.18623v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18623v1",
        "arxiv_id": "2512.18623v1",
        "authors": [
            "Jensen Zhang",
            "Ningyuan Liu",
            "Yijia Fan",
            "Zihao Huang",
            "Qinglin Zeng",
            "Kaitong Cai",
            "Jian Wang",
            "Keze Wang"
        ],
        "submitted": "2025-12-21 06:54:34",
        "source": "arxiv",
        "comment": "Accepted at AAAI 2026"
    },
    {
        "title": "A Multi-agent Text2SQL Framework using Small Language Models and Execution Feedback",
        "abstract": "Text2SQL, the task of generating SQL queries from natural language text, is a critical challenge in data engineering. Recently, Large Language Models (LLMs) have demonstrated superior performance for this task due to their advanced comprehension and generation capabilities. However, privacy and cost considerations prevent companies from using Text2SQL solutions based on external LLMs offered as a service. Rather, small LLMs (SLMs) that are openly available and can hosted in-house are adopted. These SLMs, in turn, lack the generalization capabilities of larger LLMs, which impairs their effectiveness for complex tasks such as Text2SQL. To address these limitations, we propose MATS, a novel Text2SQL framework designed specifically for SLMs. MATS uses a multi-agent mechanism that assigns specialized roles to auxiliary agents, reducing individual workloads and fostering interaction. A training scheme based on reinforcement learning aligns these agents using feedback obtained during execution, thereby maintaining competitive performance despite a limited LLM size. Evaluation results using on benchmark datasets show that MATS, deployed on a single- GPU server, yields accuracy that are on-par with large-scale LLMs when using significantly fewer parameters. Our source code and data are available at https://github.com/thanhdath/mats-sql.",
        "url": "http://arxiv.org/abs/2512.18622v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18622v1",
        "arxiv_id": "2512.18622v1",
        "authors": [
            "Thanh Dat Hoang",
            "Thanh Trung Huynh",
            "Matthias Weidlich",
            "Thanh Tam Nguyen",
            "Tong Chen",
            "Hongzhi Yin",
            "Quoc Viet Hung Nguyen"
        ],
        "submitted": "2025-12-21 06:43:47",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Comparative Study of Light-weight Language Models for PII Masking and their Deployment for Real Conversational Texts",
        "abstract": "Automated masking of Personally Identifiable Information (PII) is critical for privacy-preserving conversational systems. While current frontier large language models demonstrate strong PII masking capabilities, concerns about data handling and computational costs motivate exploration of whether lightweight models can achieve comparable performance. We compare encoder-decoder and decoder-only architectures by fine-tuning T5-small and Mistral-Instruct-v0.3 on English datasets constructed from the AI4Privacy benchmark. We create different dataset variants to study label standardization and PII representation, covering 24 standardized PII categories and higher-granularity settings. Evaluation using entity-level and character-level metrics, type accuracy, and exact match shows that both lightweight models achieve performance comparable to frontier LLMs for PII masking tasks. Label normalization consistently improves performance across architectures. Mistral achieves higher F1 and recall with greater robustness across PII types but incurs significantly higher generation latency. T5, while less robust in conversational text, offers more controllable structured outputs and lower inference cost, motivating its use in a real-time Discord bot for real-world PII redaction. Evaluation on live messages reveals performance degradation under informal inputs. These results clarify trade-offs between accuracy, robustness, and computational efficiency, demonstrating that lightweight models can provide effective PII masking while addressing data handling concerns associated with frontier LLMs.",
        "url": "http://arxiv.org/abs/2512.18608v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18608v1",
        "arxiv_id": "2512.18608v1",
        "authors": [
            "Prabigya Acharya",
            "Liza Shrestha"
        ],
        "submitted": "2025-12-21 05:58:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "On Finding Inconsistencies in Documents",
        "abstract": "Professionals in academia, law, and finance audit their documents because inconsistencies can result in monetary, reputational, and scientific costs. Language models (LMs) have the potential to dramatically speed up this auditing process. To understand their abilities, we introduce a benchmark, FIND (Finding INconsistencies in Documents), where each example is a document with an inconsistency inserted manually by a domain expert. Despite the documents being long, technical, and complex, the best-performing model (gpt-5) recovered 64% of the inserted inconsistencies. Surprisingly, gpt-5 also found undiscovered inconsistencies present in the original documents. For example, on 50 arXiv papers, we judged 136 out of 196 of the model's suggestions to be legitimate inconsistencies missed by the original authors. However, despite these findings, even the best models miss almost half of the inconsistencies in FIND, demonstrating that inconsistency detection is still a challenging task.",
        "url": "http://arxiv.org/abs/2512.18601v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18601v1",
        "arxiv_id": "2512.18601v1",
        "authors": [
            "Charles J. Lovering",
            "Seth Ebner",
            "Brandon Smock",
            "Michael Krumdick",
            "Saad Rabbani",
            "Ahmed Muhammad",
            "Varshini Reddy",
            "Chris Tanner"
        ],
        "submitted": "2025-12-21 05:20:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "From Scratch to Fine-Tuned: A Comparative Study of Transformer Training Strategies for Legal Machine Translation",
        "abstract": "In multilingual nations like India, access to legal information is often hindered by language barriers, as much of the legal and judicial documentation remains in English. Legal Machine Translation (L-MT) offers a scalable solution to this challenge by enabling accurate and accessible translations of legal documents. This paper presents our work for the JUST-NLP 2025 Legal MT shared task, focusing on English-Hindi translation using Transformer-based approaches. We experiment with 2 complementary strategies, fine-tuning a pre-trained OPUS-MT model for domain-specific adaptation and training a Transformer model from scratch using the provided legal corpus. Performance is evaluated using standard MT metrics, including SacreBLEU, chrF++, TER, ROUGE, BERTScore, METEOR, and COMET. Our fine-tuned OPUS-MT model achieves a SacreBLEU score of 46.03, significantly outperforming both baseline and from-scratch models. The results highlight the effectiveness of domain adaptation in enhancing translation quality and demonstrate the potential of L-MT systems to improve access to justice and legal transparency in multilingual contexts.",
        "url": "http://arxiv.org/abs/2512.18593v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18593v1",
        "arxiv_id": "2512.18593v1",
        "authors": [
            "Amit Barman",
            "Atanu Mandal",
            "Sudip Kumar Naskar"
        ],
        "submitted": "2025-12-21 04:45:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Toward Training Superintelligent Software Agents through Self-Play SWE-RL",
        "abstract": "While current software agents powered by large language models (LLMs) and agentic reinforcement learning (RL) can boost programmer productivity, their training data (e.g., GitHub issues and pull requests) and environments (e.g., pass-to-pass and fail-to-pass tests) heavily depend on human knowledge or curation, posing a fundamental barrier to superintelligence. In this paper, we present Self-play SWE-RL (SSR), a first step toward training paradigms for superintelligent software agents. Our approach takes minimal data assumptions, only requiring access to sandboxed repositories with source code and installed dependencies, with no need for human-labeled issues or tests. Grounded in these real-world codebases, a single LLM agent is trained via reinforcement learning in a self-play setting to iteratively inject and repair software bugs of increasing complexity, with each bug formally specified by a test patch rather than a natural language issue description. On the SWE-bench Verified and SWE-Bench Pro benchmarks, SSR achieves notable self-improvement (+10.4 and +7.8 points, respectively) and consistently outperforms the human-data baseline over the entire training trajectory, despite being evaluated on natural language issues absent from self-play. Our results, albeit early, suggest a path where agents autonomously gather extensive learning experiences from real-world software repositories, ultimately enabling superintelligent systems that exceed human capabilities in understanding how systems are constructed, solving novel challenges, and autonomously creating new software from scratch.",
        "url": "http://arxiv.org/abs/2512.18552v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18552v1",
        "arxiv_id": "2512.18552v1",
        "authors": [
            "Yuxiang Wei",
            "Zhiqing Sun",
            "Emily McMilin",
            "Jonas Gehring",
            "David Zhang",
            "Gabriel Synnaeve",
            "Daniel Fried",
            "Lingming Zhang",
            "Sida Wang"
        ],
        "submitted": "2025-12-21 00:49:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Neologism Learning as a Parameter-Efficient Alternative to Fine-Tuning for Model Steering",
        "abstract": "In language modeling, neologisms are new tokens trained to represent a concept not already included in a given model's vocabulary. Neologisms can be used to encourage specific behavior in models, for example by appending prompts with \"Give me a neologism answer.\" Behavioral steering can also be achieved through fine-tuning, albeit with more compute and less flexibility: learning a neologism only trains d parameters and allows the user to still access the model's default behavior. We compare the performance of neologism learning against low-rank adaptation (LoRA) fine-tuning, finding that neologisms outperform fine-tuned models under a matched training setup (same data and hyperparameters). We also investigate self-verbalizations of neologisms, and observe that the model will occasionally make up its own new words when asked about a neologism.",
        "url": "http://arxiv.org/abs/2512.18551v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18551v1",
        "arxiv_id": "2512.18551v1",
        "authors": [
            "Sungjoon Park",
            "Varun Ramamurthi",
            "Owen Terry"
        ],
        "submitted": "2025-12-21 00:45:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LLMs on Drugs: Language Models Are Few-Shot Consumers",
        "abstract": "Large language models (LLMs) are sensitive to the personas imposed on them at inference time, yet prompt-level \"drug\" interventions have never been benchmarked rigorously. We present the first controlled study of psychoactive framings on GPT-5-mini using ARC-Challenge. Four single-sentence prompts -- LSD, cocaine, alcohol, and cannabis -- are compared against a sober control across 100 validation items per condition, with deterministic decoding, full logging, Wilson confidence intervals, and Fisher exact tests. Control accuracy is 0.45; alcohol collapses to 0.10 (p = 3.2e-8), cocaine to 0.21 (p = 4.9e-4), LSD to 0.19 (p = 1.3e-4), and cannabis to 0.30 (p = 0.041), largely because persona prompts disrupt the mandated \"Answer: <LETTER>\" template. Persona text therefore behaves like a \"few-shot consumable\" that can destroy reliability without touching model weights. All experimental code, raw results, and analysis scripts are available at https://github.com/lexdoudkin/llms-on-drugs.",
        "url": "http://arxiv.org/abs/2512.18546v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18546v1",
        "arxiv_id": "2512.18546v1",
        "authors": [
            "Alexander Doudkin"
        ],
        "submitted": "2025-12-21 00:19:02",
        "source": "arxiv",
        "comment": "8 pages, 2 figures, 2 tables"
    },
    {
        "title": "SecureCode v2.0: A Production-Grade Dataset for Training Security-Aware Code Generation Models",
        "abstract": "AI assistants produce vulnerable code in 45% of security-relevant scenarios, introducing flaws into production systems at scale. Yet existing secure coding datasets fall short. They lack incident grounding, don't provide the scale modern training requires, and miss the operational security context developers need for production deployments. We present SecureCode v2.0, a production-grade dataset of 1,215 security-focused coding examples that passed structural validation and expert security review. Every example ties to actual documented security incidents with CVE references, provides vulnerable and secure implementations, demonstrates concrete attacks, and includes defense-in-depth operational guidance. The dataset covers 11 vulnerability categories (complete OWASP Top 10:2025 plus AI/ML Security Threats) across 11 languages (Python, JavaScript, Java, Go, PHP, C#, TypeScript, Ruby, Rust, Kotlin, and YAML for infrastructure-as-code).\n  Our quality assurance framework ensures complete incident grounding. Each example includes SIEM integration strategies, infrastructure hardening recommendations (Docker, AppArmor, WAF configurations), and testing approaches using language-appropriate frameworks. The dataset uses a 4-turn conversational structure mirroring actual developer-AI interactions, escalating from basic implementations to advanced security considerations and defense-in-depth guidance.\n  Our contributions: (1) 1,215 rigorously validated examples split into 989 training, 122 validation, and 104 test sets, (2) an automated validation framework ensuring dataset consistency, (3) a 4-turn conversational structure capturing realistic security workflows, (4) comprehensive operational security guidance with SIEM integration strategies, (5) complete language-specific implementation fidelity, and (6) open-source release of data, validation tools, and benchmarking protocols.",
        "url": "http://arxiv.org/abs/2512.18542v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18542v1",
        "arxiv_id": "2512.18542v1",
        "authors": [
            "Scott Thornton"
        ],
        "submitted": "2025-12-20 23:52:12",
        "source": "arxiv",
        "comment": "37 pages, 5 figures. Dataset available at https://huggingface.co/datasets/scthornton/securecode-v2. Code and validation tools at https://github.com/scthornton/securecode-v2"
    },
    {
        "title": "Generalization Gaps in Political Fake News Detection: An Empirical Study on the LIAR Dataset",
        "abstract": "The proliferation of linguistically subtle political disinformation poses a significant challenge to automated fact-checking systems. Despite increasing emphasis on complex neural architectures, the empirical limits of text-only linguistic modeling remain underexplored. We present a systematic diagnostic evaluation of nine machine learning algorithms on the LIAR benchmark. By isolating lexical features (Bag-of-Words, TF-IDF) and semantic embeddings (GloVe), we uncover a hard \"Performance Ceiling\", with fine-grained classification not exceeding a Weighted F1-score of 0.32 across models. Crucially, a simple linear SVM (Accuracy: 0.624) matches the performance of pre-trained Transformers such as RoBERTa (Accuracy: 0.620), suggesting that model capacity is not the primary bottleneck. We further diagnose a massive \"Generalization Gap\" in tree-based ensembles, which achieve more than 99% training accuracy but collapse to approximately 25% on test data, indicating reliance on lexical memorization rather than semantic inference. Synthetic data augmentation via SMOTE yields no meaningful gains, confirming that the limitation is semantic (feature ambiguity) rather than distributional. These findings indicate that for political fact-checking, increasing model complexity without incorporating external knowledge yields diminishing returns.",
        "url": "http://arxiv.org/abs/2512.18533v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18533v1",
        "arxiv_id": "2512.18533v1",
        "authors": [
            "S Mahmudul Hasan",
            "Shaily Roy",
            "Akib Jawad Nafis"
        ],
        "submitted": "2025-12-20 23:08:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Teaching and Critiquing Conceptualization and Operationalization in NLP",
        "abstract": "NLP researchers regularly invoke abstract concepts like \"interpretability,\" \"bias,\" \"reasoning,\" and \"stereotypes,\" without defining them. Each subfield has a shared understanding or conceptualization of what these terms mean and how we should treat them, and this shared understanding is the basis on which operational decisions are made: Datasets are built to evaluate these concepts, metrics are proposed to quantify them, and claims are made about systems. But what do they mean, what should they mean, and how should we measure them? I outline a seminar I created for students to explore these questions of conceptualization and operationalization, with an interdisciplinary reading list and an emphasis on discussion and critique.",
        "url": "http://arxiv.org/abs/2512.18505v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18505v1",
        "arxiv_id": "2512.18505v1",
        "authors": [
            "Vagrant Gautam"
        ],
        "submitted": "2025-12-20 20:47:01",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Research on a hybrid LSTM-CNN-Attention model for text-based web content classification",
        "abstract": "This study presents a hybrid deep learning architecture that integrates LSTM, CNN, and an Attention mechanism to enhance the classification of web content based on text. Pretrained GloVe embeddings are used to represent words as dense vectors that preserve semantic similarity. The CNN layer extracts local n-gram patterns and lexical features, while the LSTM layer models long-range dependencies and sequential structure. The integrated Attention mechanism enables the model to focus selectively on the most informative parts of the input sequence. A 5-fold cross-validation setup was used to assess the robustness and generalizability of the proposed solution. Experimental results show that the hybrid LSTM-CNN-Attention model achieved outstanding performance, with an accuracy of 0.98, precision of 0.94, recall of 0.92, and F1-score of 0.93. These results surpass the performance of baseline models based solely on CNNs, LSTMs, or transformer-based classifiers such as BERT. The combination of neural network components enabled the model to effectively capture both fine-grained text structures and broader semantic context. Furthermore, the use of GloVe embeddings provided an efficient and effective representation of textual data, making the model suitable for integration into systems with real-time or near-real-time requirements. The proposed hybrid architecture demonstrates high effectiveness in text-based web content classification, particularly in tasks requiring both syntactic feature extraction and semantic interpretation. By combining presented mechanisms, the model addresses the limitations of individual architectures and achieves improved generalization. These findings support the broader use of hybrid deep learning approaches in NLP applications, especially where complex, unstructured textual data must be processed and classified with high reliability.",
        "url": "http://arxiv.org/abs/2512.18475v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18475v1",
        "arxiv_id": "2512.18475v1",
        "authors": [
            "Mykola Kuz",
            "Ihor Lazarovych",
            "Mykola Kozlenko",
            "Mykola Pikuliak",
            "Andrii Kvasniuk"
        ],
        "submitted": "2025-12-20 19:38:07",
        "source": "arxiv",
        "comment": "10 pages, 5 figures, 2 tables. Accepted by Radio Electronics Computer Science Control 2025"
    },
    {
        "title": "Mitigating Spurious Correlations in NLI via LLM-Synthesized Counterfactuals and Dynamic Balanced Sampling",
        "abstract": "Natural Language Inference (NLI) models frequently rely on spurious correlations rather than semantic reasoning. Existing mitigation strategies often incur high annotation costs or trigger catastrophic forgetting during fine-tuning. We propose an automated, scalable pipeline to address these limitations. First, we introduce Log-Frequency LMI (LF-LMI) to accurately detect semantic artifacts. Second, we generate a high-quality synthetic contrast set via an LLM-synthesis pipeline with multi-judge verification. Finally, we introduce Dynamic Balanced Sampling, a training strategy that rotates the original data distribution to prevent forgetting. Our method improves consistency on a challenging benchmark from 63.5% to 81.0% while maintaining 88.4% in-domain accuracy, significantly outperforming naive fine-tuning.",
        "url": "http://arxiv.org/abs/2512.18462v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18462v1",
        "arxiv_id": "2512.18462v1",
        "authors": [
            "Christopher Rom√°n Jaimes"
        ],
        "submitted": "2025-12-20 18:30:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "An Agentic AI Framework for Training General Practitioner Student Skills",
        "abstract": "Advancements in large language models offer strong potential for enhancing virtual simulated patients (VSPs) in medical education by providing scalable alternatives to resource-intensive traditional methods. However, current VSPs often struggle with medical accuracy, consistent roleplaying, scenario generation for VSP use, and educationally structured feedback. We introduce an agentic framework for training general practitioner student skills that unifies (i) configurable, evidence-based vignette generation, (ii) controlled persona-driven patient dialogue with optional retrieval grounding, and (iii) standards-based assessment and feedback for both communication and clinical reasoning. We instantiate the framework in an interactive spoken consultation setting and evaluate it with medical students ($\\mathbf{N{=}14}$). Participants reported realistic and vignette-faithful dialogue, appropriate difficulty calibration, a stable personality signal, and highly useful example-rich feedback, alongside excellent overall usability. These results support agentic separation of scenario control, interaction control, and standards-based assessment as a practical pattern for building dependable and pedagogically valuable VSP training tools.",
        "url": "http://arxiv.org/abs/2512.18440v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18440v1",
        "arxiv_id": "2512.18440v1",
        "authors": [
            "Victor De Marez",
            "Jens Van Nooten",
            "Luna De Bruyne",
            "Walter Daelemans"
        ],
        "submitted": "2025-12-20 17:26:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Efficient Optimization of Hierarchical Identifiers for Generative Recommendation",
        "abstract": "SEATER is a generative retrieval model that improves recommendation inference efficiency and retrieval quality by utilizing balanced tree-structured item identifiers and contrastive training objectives. We reproduce and validate SEATER's reported improvements in retrieval quality over strong baselines across all datasets from the original work, and extend the evaluation to Yambda, a large-scale music recommendation dataset. Our experiments verify SEATER's strong performance, but show that its tree construction step during training becomes a major bottleneck as the number of items grows. To address this, we implement and evaluate two alternative construction algorithms: a greedy method optimized for minimal build time, and a hybrid method that combines greedy clustering at high levels with more precise grouping at lower levels. The greedy method reduces tree construction time to less than 2% of the original with only a minor drop in quality on the dataset with the largest item collection. The hybrid method achieves retrieval quality on par with the original, and even improves on the largest dataset, while cutting construction time to just 5-8%. All data and code are publicly available for full reproducibility at https://github.com/joshrosie/re-seater.",
        "url": "http://arxiv.org/abs/2512.18434v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18434v1",
        "arxiv_id": "2512.18434v1",
        "authors": [
            "Federica Valeau",
            "Odysseas Boufalis",
            "Polytimi Gkotsi",
            "Joshua Rosenthal",
            "David Vos"
        ],
        "submitted": "2025-12-20 17:21:41",
        "source": "arxiv",
        "comment": "Accepted at ECIR 2026 Reproducibility Track (to appear)"
    },
    {
        "title": "AraToken: Optimizing Arabic Tokenization with Normalization Pipeline and Language Extension for Qwen3",
        "abstract": "Tokenization is a critical preprocessing step for large language models (LLMs), directly impacting training efficiency and downstream performance. General-purpose tokenizers trained predominantly on English and Latin-script languages exhibit suboptimal performance on morphologically rich languages such as Arabic, resulting in inflated token sequences and reduced compression efficiency. In this work, we present AraToken, an Arabic-optimized tokenizer built on SentencePiece Unigram algorithm with a comprehensive normalization pipeline addressing Arabic-specific orthographic variations including Alif variants, diacritics, and Arabic-Indic numerals. We systematically compare BPE, WordPiece, and SentencePiece algorithms across multiple configurations, demonstrating that SentencePiece with normalization achieves 18% lower fertility (1.199 vs 1.35 tokens/word) compared to unnormalized baselines. Furthermore, we introduce the Language Extension Pipeline (LEP), a method for integrating the optimized tokenizer into Qwen3-0.6B through vocabulary extension with mean subtoken initialization and selective transformer layer unfreezing. Our experiments show that LEP reduces evaluation loss from 8.28 to 2.43 within 800 training steps on 100K Arabic samples. We release our tokenizer, training scripts, and model checkpoints to facilitate Arabic NLP research.",
        "url": "http://arxiv.org/abs/2512.18399v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18399v1",
        "arxiv_id": "2512.18399v1",
        "authors": [
            "Mark Kashirskiy",
            "Artiom Lipinski",
            "Ilya Makarov"
        ],
        "submitted": "2025-12-20 15:32:10",
        "source": "arxiv",
        "comment": "8 pages, 8 figures, 5 tables"
    },
    {
        "title": "Datasets for machine learning and for assessing the intelligence level of automatic patent search systems",
        "abstract": "The key to success in automating prior art search in patent research using artificial intelligence lies in developing large datasets for machine learning and ensuring their availability. This work is dedicated to providing a comprehensive solution to the problem of creating infrastructure for research in this field, including datasets and tools for calculating search quality criteria. The paper discusses the concept of semantic clusters of patent documents that determine the state of the art in a given subject, as proposed by the authors. A definition of such semantic clusters is also provided. Prior art search is presented as the task of identifying elements within a semantic cluster of patent documents in the subject area specified by the document under consideration. A generator of user-configurable datasets for machine learning, based on collections of U.S. and Russian patent documents, is described. The dataset generator creates a database of links to documents in semantic clusters. Then, based on user-defined parameters, it forms a dataset of semantic clusters in JSON format for machine learning. To evaluate machine learning outcomes, it is proposed to calculate search quality scores that account for semantic clusters of the documents being searched. To automate the evaluation process, the paper describes a utility developed by the authors for assessing the quality of prior art document search.",
        "url": "http://arxiv.org/abs/2512.18384v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18384v1",
        "arxiv_id": "2512.18384v1",
        "authors": [
            "Boris Genin",
            "Alexander Gorbunov",
            "Dmitry Zolkin",
            "Igor Nekrasov"
        ],
        "submitted": "2025-12-20 14:51:57",
        "source": "arxiv",
        "comment": "14 pages, 3 figures, 2 tables"
    },
    {
        "title": "SRS-Stories: Vocabulary-constrained multilingual story generation for language learning",
        "abstract": "In this paper, we use large language models to generate personalized stories for language learners, using only the vocabulary they know. The generated texts are specifically written to teach the user new vocabulary by simply reading stories where it appears in context, while at the same time seamlessly reviewing recently learned vocabulary. The generated stories are enjoyable to read and the vocabulary reviewing/learning is optimized by a Spaced Repetition System. The experiments are conducted in three languages: English, Chinese and Polish, evaluating three story generation methods and three strategies for enforcing lexical constraints. The results show that the generated stories are more grammatical, coherent, and provide better examples of word usage than texts generated by the standard constrained beam search approach",
        "url": "http://arxiv.org/abs/2512.18362v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18362v1",
        "arxiv_id": "2512.18362v1",
        "authors": [
            "Wiktor Kamzela",
            "Mateusz Lango",
            "Ondrej Dusek"
        ],
        "submitted": "2025-12-20 13:24:59",
        "source": "arxiv",
        "comment": "EMNLP 2025"
    },
    {
        "title": "LLM Agents Implement an NLG System from Scratch: Building Interpretable Rule-Based RDF-to-Text Generators",
        "abstract": "We present a novel neurosymbolic framework for RDF-to-text generation, in which the model is \"trained\" through collaborative interactions among multiple LLM agents rather than traditional backpropagation. The LLM agents produce rule-based Python code for a generator for the given domain, based on RDF triples only, with no in-domain human reference texts. The resulting system is fully interpretable, requires no supervised training data, and generates text nearly instantaneously using only a single CPU. Our experiments on the WebNLG and OpenDialKG data show that outputs produced by our approach reduce hallucination, with only slight fluency penalties compared to finetuned or prompted language models",
        "url": "http://arxiv.org/abs/2512.18360v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18360v1",
        "arxiv_id": "2512.18360v1",
        "authors": [
            "Mateusz Lango",
            "Ond≈ôej Du≈°ek"
        ],
        "submitted": "2025-12-20 13:16:51",
        "source": "arxiv",
        "comment": "EMNLP 2025"
    },
    {
        "title": "DACE For Railway Acronym Disambiguation",
        "abstract": "Acronym Disambiguation (AD) is a fundamental challenge in technical text processing, particularly in specialized sectors where high ambiguity complicates automated analysis. This paper addresses AD within the context of the TextMine'26 competition on French railway documentation. We present DACE (Dynamic Prompting, Retrieval Augmented Generation, Contextual Selection, and Ensemble Aggregation), a framework that enhances Large Language Models through adaptive in-context learning and external domain knowledge injection. By dynamically tailoring prompts to acronym ambiguity and aggregating ensemble predictions, DACE mitigates hallucination and effectively handles low-resource scenarios. Our approach secured the top rank in the competition with an F1 score of 0.9069.",
        "url": "http://arxiv.org/abs/2512.18357v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18357v1",
        "arxiv_id": "2512.18357v1",
        "authors": [
            "El Mokhtar Hribach",
            "Oussama Mechhour",
            "Mohammed Elmonstaser",
            "Yassine El Boudouri",
            "Othmane Kabal"
        ],
        "submitted": "2025-12-20 12:56:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LLM-based Few-Shot Early Rumor Detection with Imitation Agent",
        "abstract": "Early Rumor Detection (EARD) aims to identify the earliest point at which a claim can be accurately classified based on a sequence of social media posts. This is especially challenging in data-scarce settings. While Large Language Models (LLMs) perform well in few-shot NLP tasks, they are not well-suited for time-series data and are computationally expensive for both training and inference. In this work, we propose a novel EARD framework that combines an autonomous agent and an LLM-based detection model, where the agent acts as a reliable decision-maker for \\textit{early time point determination}, while the LLM serves as a powerful \\textit{rumor detector}. This approach offers the first solution for few-shot EARD, necessitating only the training of a lightweight agent and allowing the LLM to remain training-free. Extensive experiments on four real-world datasets show our approach boosts performance across LLMs and surpasses existing EARD methods in accuracy and earliness.",
        "url": "http://arxiv.org/abs/2512.18352v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18352v1",
        "arxiv_id": "2512.18352v1",
        "authors": [
            "Fengzhu Zeng",
            "Qian Shao",
            "Ling Cheng",
            "Wei Gao",
            "Shih-Fen Cheng",
            "Jing Ma",
            "Cheng Niu"
        ],
        "submitted": "2025-12-20 12:42:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Towards Efficient Agents: A Co-Design of Inference Architecture and System",
        "abstract": "The rapid development of large language model (LLM)-based agents has unlocked new possibilities for autonomous multi-turn reasoning and tool-augmented decision-making. However, their real-world deployment is hindered by severe inefficiencies that arise not from isolated model inference, but from the systemic latency accumulated across reasoning loops, context growth, and heterogeneous tool interactions. This paper presents AgentInfer, a unified framework for end-to-end agent acceleration that bridges inference optimization and architectural design. We decompose the problem into four synergistic components: AgentCollab, a hierarchical dual-model reasoning framework that balances large- and small-model usage through dynamic role assignment; AgentSched, a cache-aware hybrid scheduler that minimizes latency under heterogeneous request patterns; AgentSAM, a suffix-automaton-based speculative decoding method that reuses multi-session semantic memory to achieve low-overhead inference acceleration; and AgentCompress, a semantic compression mechanism that asynchronously distills and reorganizes agent memory without disrupting ongoing reasoning. Together, these modules form a Self-Evolution Engine capable of sustaining efficiency and cognitive stability throughout long-horizon reasoning tasks. Experiments on the BrowseComp-zh and DeepDiver benchmarks demonstrate that through the synergistic collaboration of these methods, AgentInfer reduces ineffective token consumption by over 50%, achieving an overall 1.8-2.5 times speedup with preserved accuracy. These results underscore that optimizing for agentic task completion-rather than merely per-token throughput-is the key to building scalable, efficient, and self-improving intelligent systems.",
        "url": "http://arxiv.org/abs/2512.18337v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18337v1",
        "arxiv_id": "2512.18337v1",
        "authors": [
            "Weizhe Lin",
            "Hui-Ling Zhen",
            "Shuai Yang",
            "Xian Wang",
            "Renxi Liu",
            "Hanting Chen",
            "Wangze Zhang",
            "Chuansai Zhou",
            "Yiming Li",
            "Chen Chen",
            "Xing Li",
            "Zhiyuan Yang",
            "Xiaosong Li",
            "Xianzhi Yu",
            "Zhenhua Dong",
            "Mingxuan Yuan",
            "Yunhe Wang"
        ],
        "submitted": "2025-12-20 12:06:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LIR$^3$AG: A Lightweight Rerank Reasoning Strategy Framework for Retrieval-Augmented Generation",
        "abstract": "Retrieval-Augmented Generation (RAG) effectively enhances Large Language Models (LLMs) by incorporating retrieved external knowledge into the generation process. Reasoning models improve LLM performance in multi-hop QA tasks, which require integrating and reasoning over multiple pieces of evidence across different documents to answer a complex question. However, they often introduce substantial computational costs, including increased token consumption and inference latency. To better understand and mitigate this trade-off, we conduct a comprehensive study of reasoning strategies for reasoning models in RAG multi-hop QA tasks. Our findings reveal that reasoning models adopt structured strategies to integrate retrieved and internal knowledge, primarily following two modes: Context-Grounded Reasoning, which relies directly on retrieved content, and Knowledge-Reconciled Reasoning, which resolves conflicts or gaps using internal knowledge. To this end, we propose a novel Lightweight Rerank Reasoning Strategy Framework for RAG (LiR$^3$AG) to enable non-reasoning models to transfer reasoning strategies by restructuring retrieved evidence into coherent reasoning chains. LiR$^3$AG significantly reduce the average 98% output tokens overhead and 58.6% inferencing time while improving 8B non-reasoning model's F1 performance ranging from 6.2% to 22.5% to surpass the performance of 32B reasoning model in RAG, offering a practical and efficient path forward for RAG systems.",
        "url": "http://arxiv.org/abs/2512.18329v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18329v1",
        "arxiv_id": "2512.18329v1",
        "authors": [
            "Guo Chen",
            "Junjie Huang",
            "Huaijin Xie",
            "Fei Sun",
            "Tao Jia"
        ],
        "submitted": "2025-12-20 11:53:37",
        "source": "arxiv",
        "comment": "AAAI2026"
    },
    {
        "title": "CTTA-T: Continual Test-Time Adaptation for Text Understanding via Teacher-Student with a Domain-aware and Generalized Teacher",
        "abstract": "Text understanding often suffers from domain shifts. To handle testing domains, domain adaptation (DA) is trained to adapt to a fixed and observed testing domain; a more challenging paradigm, test-time adaptation (TTA), cannot access the testing domain during training and online adapts to the testing samples during testing, where the samples are from a fixed domain. We aim to explore a more practical and underexplored scenario, continual test-time adaptation (CTTA) for text understanding, which involves a sequence of testing (unobserved) domains in testing. Current CTTA methods struggle in reducing error accumulation over domains and enhancing generalization to handle unobserved domains: 1) Noise-filtering reduces accumulated errors but discards useful information, and 2) accumulating historical domains enhances generalization, but it is hard to achieve adaptive accumulation. In this paper, we propose a CTTA-T (continual test-time adaptation for text understanding) framework adaptable to evolving target domains: it adopts a teacher-student framework, where the teacher is domain-aware and generalized for evolving domains. To improve teacher predictions, we propose a refine-then-filter based on dropout-driven consistency, which calibrates predictions and removes unreliable guidance. For the adaptation-generalization trade-off, we construct a domain-aware teacher by dynamically accumulating cross-domain semantics via incremental PCA, which continuously tracks domain shifts. Experiments show CTTA-T excels baselines.",
        "url": "http://arxiv.org/abs/2512.18321v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18321v1",
        "arxiv_id": "2512.18321v1",
        "authors": [
            "Tianlun Liu",
            "Zhiliang Tian",
            "Zhen Huang",
            "Xingzhi Zhou",
            "Wanlong Yu",
            "Tianle Liu",
            "Feng Liu",
            "Dongsheng Li"
        ],
        "submitted": "2025-12-20 11:39:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "InstructNet: A Novel Approach for Multi-Label Instruction Classification through Advanced Deep Learning",
        "abstract": "People use search engines for various topics and items, from daily essentials to more aspirational and specialized objects. Therefore, search engines have taken over as peoples preferred resource. The How To prefix has become familiar and widely used in various search styles to find solutions to particular problems. This search allows people to find sequential instructions by providing detailed guidelines to accomplish specific tasks. Categorizing instructional text is also essential for task-oriented learning and creating knowledge bases. This study uses the How To articles to determine the multi-label instruction category. We have brought this work with a dataset comprising 11,121 observations from wikiHow, where each record has multiple categories. To find out the multi-label category meticulously, we employ some transformer-based deep neural architectures, such as Generalized Autoregressive Pretraining for Language Understanding (XLNet), Bidirectional Encoder Representation from Transformers (BERT), etc. In our multi-label instruction classification process, we have reckoned our proposed architectures using accuracy and macro f1-score as the performance metrics. This thorough evaluation showed us much about our strategys strengths and drawbacks. Specifically, our implementation of the XLNet architecture has demonstrated unprecedented performance, achieving an accuracy of 97.30% and micro and macro average scores of 89.02% and 93%, a noteworthy accomplishment in multi-label classification. This high level of accuracy and macro average score is a testament to the effectiveness of the XLNet architecture in our proposed InstructNet approach. By employing a multi-level strategy in our evaluation process, we have gained a more comprehensive knowledge of the effectiveness of our proposed architectures and identified areas for forthcoming improvement and refinement.",
        "url": "http://arxiv.org/abs/2512.18301v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18301v1",
        "arxiv_id": "2512.18301v1",
        "authors": [
            "Tanjim Taharat Aurpa",
            "Md Shoaib Ahmed",
            "Md Mahbubur Rahman",
            "Md. Golam Moazzam"
        ],
        "submitted": "2025-12-20 10:16:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Explainable Transformer-CNN Fusion for Noise-Robust Speech Emotion Recognition",
        "abstract": "Speech Emotion Recognition (SER) systems often degrade in performance when exposed to the unpredictable acoustic interference found in real-world environments. Additionally, the opacity of deep learning models hinders their adoption in trust-sensitive applications. To bridge this gap, we propose a Hybrid Transformer-CNN framework that unifies the contextual modeling of Wav2Vec 2.0 with the spectral stability of 1D-Convolutional Neural Networks. Our dual-stream architecture processes raw waveforms to capture long-range temporal dependencies while simultaneously extracting noise-resistant spectral features (MFCC, ZCR, RMSE) via a custom Attentive Temporal Pooling mechanism. We conducted extensive validation across four diverse benchmark datasets: RAVDESS, TESS, SAVEE, and CREMA-D. To rigorously test robustness, we subjected the model to non-stationary acoustic interference using real-world noise profiles from the SAS-KIIT dataset. The proposed framework demonstrates superior generalization and state-of-the-art accuracy across all datasets, significantly outperforming single-branch baselines under realistic environmental interference. Furthermore, we address the ``black-box\" problem by integrating SHAP and Score-CAM into the evaluation pipeline. These tools provide granular visual explanations, revealing how the model strategically shifts attention between temporal and spectral cues to maintain reliability in the presence of complex environmental noise.",
        "url": "http://arxiv.org/abs/2512.18298v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18298v1",
        "arxiv_id": "2512.18298v1",
        "authors": [
            "Sudip Chakrabarty",
            "Pappu Bishwas",
            "Rajdeep Chatterjee"
        ],
        "submitted": "2025-12-20 10:05:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Measuring Fine-Grained Negotiation Tactics of Humans and LLMs in Diplomacy",
        "abstract": "The study of negotiation styles dates back to Aristotle's ethos-pathos-logos rhetoric. Prior efforts primarily studied the success of negotiation agents. Here, we shift the focus towards the styles of negotiation strategies. Our focus is the strategic dialogue board game Diplomacy, which affords rich natural language negotiation and measures of game success. We used LLM-as-a-judge to annotate a large human-human set of Diplomacy games for fine-grained negotiation tactics from a sociologically-grounded taxonomy. Using a combination of the It Takes Two and WebDiplomacy datasets, we demonstrate the reliability of our LLM-as-a-Judge framework and show strong correlations between negotiation features and success in the Diplomacy setting. Lastly, we investigate the differences between LLM and human negotiation strategies and show that fine-tuning can steer LLM agents toward more human-like negotiation behaviors.",
        "url": "http://arxiv.org/abs/2512.18292v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18292v1",
        "arxiv_id": "2512.18292v1",
        "authors": [
            "Wenkai Li",
            "Lynnette Hui Xian Ng",
            "Andy Liu",
            "Daniel Fried"
        ],
        "submitted": "2025-12-20 09:33:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Improving Data Reusability in Interactive Information Retrieval: Insights from the Community",
        "abstract": "In this study, we conducted semi-structured interviews with 21 IIR researchers to investigate their data reuse practices. This study aims to expand upon current findings by exploring IIR researchers' information-obtaining behaviors regarding data reuse. We identified the information about shared data characteristics that IIR researchers need when evaluating data reusability, as well as the sources they typically consult to obtain this information. We consider this work to be an initial step toward revealing IIR researchers' data reuse practices and identifying what the community needs to do to promote data reuse. We hope that this study, as well as future research, will inspire more individuals to contribute to ongoing efforts aimed at designing standards, infrastructures, and policies, as well as fostering a sustainable culture of data sharing and reuse in this field.",
        "url": "http://arxiv.org/abs/2512.18283v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18283v1",
        "arxiv_id": "2512.18283v1",
        "authors": [
            "Tianji Jiang",
            "Wenqi Li",
            "Jiqun Liu"
        ],
        "submitted": "2025-12-20 09:12:33",
        "source": "arxiv",
        "comment": "Accepted by CHIIR 2025"
    },
    {
        "title": "TICL+: A Case Study On Speech In-Context Learning for Children's Speech Recognition",
        "abstract": "Children's speech recognition remains challenging due to substantial acoustic and linguistic variability, limited labeled data, and significant differences from adult speech. Speech foundation models can address these challenges through Speech In-Context Learning (SICL), allowing adaptation to new domains without fine-tuning. However, the effectiveness of SICL depends on how in-context examples are selected. We extend an existing retrieval-based method, Text-Embedding KNN for SICL (TICL), introducing an acoustic reranking step to create TICL+. This extension prioritizes examples that are both semantically and acoustically aligned with the test input. Experiments on four children's speech corpora show that TICL+ achieves up to a 53.3% relative word error rate reduction over zero-shot performance and 37.6% over baseline TICL, highlighting the value of combining semantic and acoustic information for robust, scalable ASR in children's speech.",
        "url": "http://arxiv.org/abs/2512.18263v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18263v1",
        "arxiv_id": "2512.18263v1",
        "authors": [
            "Haolong Zheng",
            "Yekaterina Yegorova",
            "Mark Hasegawa-Johnson"
        ],
        "submitted": "2025-12-20 08:03:07",
        "source": "arxiv",
        "comment": "Published at IEEE ASRU 2025 Satellite Workshop-AI for Children's Speech and Language"
    },
    {
        "title": "Investigating Spatial Attention Bias in Vision-Language Models",
        "abstract": "Vision-Language Models have demonstrated remarkable capabilities in understanding visual content, yet systematic biases in their spatial processing remain largely unexplored. This work identifies and characterizes a systematic spatial attention bias where VLMs consistently prioritize describing left-positioned content before right-positioned content in horizontally concatenated images. Through controlled experiments on image pairs using both open-source and closed-source models, we demonstrate that this bias persists across different architectures, with models describing left-positioned content first in approximately 97% of cases under neutral prompting conditions. Testing on an Arabic-finetuned model reveals that the bias persists despite right-to-left language training, ruling out language reading direction as the primary cause. Investigation of training dataset annotation guidelines from PixMo and Visual Genome reveals no explicit left-first ordering instructions, suggesting the bias is consistent with architectural factors rather than explicit training data instructions. These findings reveal fundamental limitations in how current VLMs process spatial information.",
        "url": "http://arxiv.org/abs/2512.18231v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18231v1",
        "arxiv_id": "2512.18231v1",
        "authors": [
            "Aryan Chaudhary",
            "Sanchit Goyal",
            "Pratik Narang",
            "Dhruv Kumar"
        ],
        "submitted": "2025-12-20 06:22:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "GeoSense-AI: Fast Location Inference from Crisis Microblogs",
        "abstract": "This paper presents an applied AI pipeline for realtime geolocation from noisy microblog streams, unifying statistical hashtag segmentation, part-of-speech-driven proper-noun detection, dependency parsing around disaster lexicons, lightweight named-entity recognition, and gazetteer-grounded disambiguation to infer locations directly from text rather than sparse geotags. The approach operationalizes information extraction under streaming constraints, emphasizing low-latency NLP components and efficient validation against geographic knowledge bases to support situational awareness during emergencies. In head to head comparisons with widely used NER toolkits, the system attains strong F1 while being engineered for orders-of-magnitude faster throughput, enabling deployment in live crisis informatics settings. A production map interface demonstrates end-to-end AI functionality ingest, inference, and visualization--surfacing locational signals at scale for floods, outbreaks, and other fastmoving events. By prioritizing robustness to informal text and streaming efficiency, GeoSense-AI illustrates how domain-tuned NLP and knowledge grounding can elevate emergency response beyond conventional geo-tag reliance.",
        "url": "http://arxiv.org/abs/2512.18225v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18225v1",
        "arxiv_id": "2512.18225v1",
        "authors": [
            "Deepit Sapru"
        ],
        "submitted": "2025-12-20 05:46:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Stable and Efficient Single-Rollout RL for Multimodal Reasoning",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become a key paradigm to improve the reasoning capabilities of Multimodal Large Language Models (MLLMs). However, prevalent group-based algorithms such as GRPO require multi-rollout sampling for each prompt. While more efficient single-rollout variants have recently been explored in text-only settings, we find that they suffer from severe instability in multimodal contexts, often leading to training collapse. To address this training efficiency-stability trade-off, we introduce $\\textbf{MSSR}$ (Multimodal Stabilized Single-Rollout), a group-free RLVR framework that achieves both stable optimization and effective multimodal reasoning performance. MSSR achieves this via an entropy-based advantage-shaping mechanism that adaptively regularizes advantage magnitudes, preventing collapse and maintaining training stability. While such mechanisms have been used in group-based RLVR, we show that in the multimodal single-rollout setting they are not merely beneficial but essential for stability. In in-distribution evaluations, MSSR demonstrates superior training compute efficiency, achieving similar validation accuracy to the group-based baseline with half the training steps. When trained for the same number of steps, MSSR's performance surpasses the group-based baseline and shows consistent generalization improvements across five diverse reasoning-intensive benchmarks. Together, these results demonstrate that MSSR enables stable, compute-efficient, and effective RLVR for complex multimodal reasoning tasks.",
        "url": "http://arxiv.org/abs/2512.18215v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18215v1",
        "arxiv_id": "2512.18215v1",
        "authors": [
            "Rui Liu",
            "Dian Yu",
            "Lei Ke",
            "Haolin Liu",
            "Yujun Zhou",
            "Zhenwen Liang",
            "Haitao Mi",
            "Pratap Tokekar",
            "Dong Yu"
        ],
        "submitted": "2025-12-20 05:07:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Training LLMs with LogicReward for Faithful and Rigorous Reasoning",
        "abstract": "Although LLMs exhibit strong reasoning capabilities, existing training methods largely depend on outcome-based feedback, which can produce correct answers with flawed reasoning. Prior work introduces supervision on intermediate steps but still lacks guarantees of logical soundness, which is crucial in high-stakes scenarios where logical consistency is paramount. To address this, we propose LogicReward, a novel reward system that guides model training by enforcing step-level logical correctness with a theorem prover. We further introduce Autoformalization with Soft Unification, which reduces natural language ambiguity and improves formalization quality, enabling more effective use of the theorem prover. An 8B model trained on data constructed with LogicReward surpasses GPT-4o and o4-mini by 11.6\\% and 2\\% on natural language inference and logical reasoning tasks with simple training procedures. Further analysis shows that LogicReward enhances reasoning faithfulness, improves generalizability to unseen tasks such as math and commonsense reasoning, and provides a reliable reward signal even without ground-truth labels. We will release all data and code at https://llm-symbol.github.io/LogicReward.",
        "url": "http://arxiv.org/abs/2512.18196v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18196v1",
        "arxiv_id": "2512.18196v1",
        "authors": [
            "Jundong Xu",
            "Hao Fei",
            "Huichi Zhou",
            "Xin Quan",
            "Qijun Huang",
            "Shengqiong Wu",
            "William Yang Wang",
            "Mong-Li Lee",
            "Wynne Hsu"
        ],
        "submitted": "2025-12-20 03:43:02",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "External Hippocampus: Topological Cognitive Maps for Guiding Large Language Model Reasoning",
        "abstract": "This paper proposes the External Hippocampus framework, which models language model reasoning from a cognitive dynamics perspective as the flow of information energy in semantic space. Unlike traditional weight-space optimization methods, this framework constructs topological cognitive maps through dimensionality reduction projection, enabling precise navigation and intervention of energy flow at test time while avoiding substantial computational requirements and demonstrating predictable intervention patterns. The method effectively addresses the cognitive deadlock problem in multi-step reasoning for small models. Experiments on models <=7B parameters show: map-guided methods achieve 81.20% accuracy on 500 challenging problems (relative baseline +16.80%), reduce reasoning time by >= 15x, with key findings revealing that reasoning stagnation manifests as \"Cognitive Vortex\" and low-entropy potential wells, while temperature perturbations effectively restart energy flow. The framework requires no additional training, possesses autonomous growth capability, and provides an efficient and controllable topological-aware solution for small model reasoning.",
        "url": "http://arxiv.org/abs/2512.18190v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18190v1",
        "arxiv_id": "2512.18190v1",
        "authors": [
            "Jian Yan"
        ],
        "submitted": "2025-12-20 03:27:11",
        "source": "arxiv",
        "comment": "12 pages, 7 figures"
    },
    {
        "title": "Distributed Asymmetric Allocation: A Topic Model for Large Imbalanced Corpora in Social Sciences",
        "abstract": "Social scientists employ latent Dirichlet allocation (LDA) to find highly specific topics in large corpora, but they often struggle in this task because (1) LDA, in general, takes a significant amount of time to fit on large corpora; (2) unsupervised LDA fragments topics into sub-topics in short documents; (3) semi-supervised LDA fails to identify specific topics defined using seed words. To solve these problems, I have developed a new topic model called distributed asymmetric allocation (DAA) that integrates multiple algorithms for efficiently identifying sentences about important topics in large corpora. I evaluate the ability of DAA to identify politically important topics by fitting it to the transcripts of speeches at the United Nations General Assembly between 1991 and 2017. The results show that DAA can classify sentences significantly more accurately and quickly than LDA thanks to the new algorithms. More generally, the results demonstrate that it is important for social scientists to optimize Dirichlet priors of LDA to perform content analysis accurately.",
        "url": "http://arxiv.org/abs/2512.18119v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18119v1",
        "arxiv_id": "2512.18119v1",
        "authors": [
            "Kohei Watanabe"
        ],
        "submitted": "2025-12-19 22:56:57",
        "source": "arxiv",
        "comment": "34 pages"
    },
    {
        "title": "Factorized Transport Alignment for Multimodal and Multiview E-commerce Representation Learning",
        "abstract": "The rapid growth of e-commerce requires robust multimodal representations that capture diverse signals from user-generated listings. Existing vision-language models (VLMs) typically align titles with primary images, i.e., single-view, but overlook non-primary images and auxiliary textual views that provide critical semantics in open marketplaces such as Etsy or Poshmark. To this end, we propose a framework that unifies multimodal and multi-view learning through Factorized Transport, a lightweight approximation of optimal transport, designed for scalability and deployment efficiency. During training, the method emphasizes primary views while stochastically sampling auxiliary ones, reducing training cost from quadratic in the number of views to constant per item. At inference, all views are fused into a single cached embedding, preserving the efficiency of two-tower retrieval with no additional online overhead. On an industrial dataset of 1M product listings and 0.3M interactions, our approach delivers consistent improvements in cross-view and query-to-item retrieval, achieving up to +7.9% Recall@500 over strong multimodal baselines. Overall, our framework bridges scalability with optimal transport-based learning, making multi-view pretraining practical for large-scale e-commerce search.",
        "url": "http://arxiv.org/abs/2512.18117v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18117v1",
        "arxiv_id": "2512.18117v1",
        "authors": [
            "Xiwen Chen",
            "Yen-Chieh Lien",
            "Susan Liu",
            "Mar√≠a Casta√±os",
            "Abolfazl Razi",
            "Xiaoting Zhao",
            "Congzhe Su"
        ],
        "submitted": "2025-12-19 22:50:49",
        "source": "arxiv",
        "comment": "Accepted by WSDM'26"
    },
    {
        "title": "Layout-Aware Text Editing for Efficient Transformation of Academic PDFs to Markdown",
        "abstract": "Academic documents stored in PDF format can be transformed into plain text structured markup languages to enhance accessibility and enable scalable digital library workflows. Markup languages allow for easier updates and customization, making academic content more adaptable and accessible to diverse usage, such as linguistic corpus compilation. Such documents, typically delivered in PDF format, contain complex elements including mathematical formulas, figures, headers, and tables, as well as densely layouted text. Existing end-to-end decoder transformer models can transform screenshots of documents into markup language. However, these models exhibit significant inefficiencies; their token-by-token decoding from scratch wastes a lot of inference steps in regenerating dense text that could be directly copied from PDF files. To solve this problem, we introduce EditTrans, a hybrid editing-generation model whose features allow identifying a queue of to-be-edited text from a PDF before starting to generate markup language. EditTrans contains a lightweight classifier fine-tuned from a Document Layout Analysis model on 162,127 pages of documents from arXiv. In our evaluations, EditTrans reduced the transformation latency up to 44.5% compared to end-to-end decoder transformer models, while maintaining transformation quality. Our code and reproducible dataset production scripts are open-sourced.",
        "url": "http://arxiv.org/abs/2512.18115v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18115v1",
        "arxiv_id": "2512.18115v1",
        "authors": [
            "Changxu Duan"
        ],
        "submitted": "2025-12-19 22:43:12",
        "source": "arxiv",
        "comment": "Accepted ICDAR 2025"
    },
    {
        "title": "Statistical laws and linguistics inform meaning in naturalistic and fictional conversation",
        "abstract": "Conversation is a cornerstone of social connection and is linked to well-being outcomes. Conversations vary widely in type with some portion generating complex, dynamic stories. One approach to studying how conversations unfold in time is through statistical patterns such as Heaps' law, which holds that vocabulary size scales with document length. Little work on Heaps's law has looked at conversation and considered how language features impact scaling. We measure Heaps' law for conversations recorded in two distinct mediums: 1. Strangers brought together on video chat and 2. Fictional characters in movies. We find that scaling of vocabulary size differs by parts of speech. We discuss these findings through behavioral and linguistic frameworks.",
        "url": "http://arxiv.org/abs/2512.18072v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18072v1",
        "arxiv_id": "2512.18072v1",
        "authors": [
            "Ashley M. A. Fehr",
            "Calla G. Beauregard",
            "Julia Witte Zimmerman",
            "Katie Ekstr√∂m",
            "Pablo Rosillo-Rodes",
            "Christopher M. Danforth",
            "Peter Sheridan Dodds"
        ],
        "submitted": "2025-12-19 21:21:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Narrative Consolidation: Formulating a New Task for Unifying Multi-Perspective Accounts",
        "abstract": "Processing overlapping narrative documents, such as legal testimonies or historical accounts, often aims not for compression but for a unified, coherent, and chronologically sound text. Standard Multi-Document Summarization (MDS), with its focus on conciseness, fails to preserve narrative flow. This paper formally defines this challenge as a new NLP task: Narrative Consolidation, where the central objectives are chronological integrity, completeness, and the fusion of complementary details. To demonstrate the critical role of temporal structure in this task, we introduce Temporal Alignment Event Graph (TAEG), a graph structure that explicitly models chronology and event alignment. By applying a standard centrality algorithm to TAEG, our method functions as a version selection mechanism, choosing the most central representation of each event in its correct temporal position. In a study on the four Biblical Gospels, this structure-focused approach guarantees perfect temporal ordering (Kendall's Tau of 1.000) by design and dramatically improves content metrics (e.g., +357.2% in ROUGE-L F1). The success of this baseline method validates the formulation of Narrative Consolidation as a relevant task and establishes that an explicit temporal backbone is a fundamental component for its resolution.",
        "url": "http://arxiv.org/abs/2512.18041v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18041v1",
        "arxiv_id": "2512.18041v1",
        "authors": [
            "Roger A. Finger",
            "Eduardo G. Cortes",
            "Sandro J. Rigo",
            "Gabriel de O. Ramos"
        ],
        "submitted": "2025-12-19 20:14:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CoPE: A Small Language Model for Steerable and Scalable Content Labeling",
        "abstract": "This paper details the methodology behind CoPE, a policy-steerable small language model capable of fast and accurate content labeling. We present a novel training curricula called Contradictory Example Training that enables the model to learn policy interpretation rather than mere policy memorization. We also present a novel method for generating content policies, called Binocular Labeling, which enables rapid construction of unambiguous training datasets. When evaluated across seven different harm areas, CoPE exhibits equal or superior accuracy to frontier models at only 1% of their size. We openly release a 9 billion parameter version of the model that can be run on a single consumer-grade GPU. Models like CoPE represent a paradigm shift for classifier systems. By turning an ML task into a policy writing task, CoPE opens up new design possibilities for the governance of online platforms.",
        "url": "http://arxiv.org/abs/2512.18027v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18027v1",
        "arxiv_id": "2512.18027v1",
        "authors": [
            "Samidh Chakrabarti",
            "David Willner",
            "Kevin Klyman",
            "Tiffany Saade",
            "Emily Capstick",
            "Sabina Nong"
        ],
        "submitted": "2025-12-19 19:47:33",
        "source": "arxiv",
        "comment": "21 pages, 2 figures, 7 tables"
    },
    {
        "title": "ReGal: A First Look at PPO-based Legal AI for Judgment Prediction and Summarization in India",
        "abstract": "This paper presents an early exploration of reinforcement learning methodologies for legal AI in the Indian context. We introduce Reinforcement Learning-based Legal Reasoning (ReGal), a framework that integrates Multi-Task Instruction Tuning with Reinforcement Learning from AI Feedback (RLAIF) using Proximal Policy Optimization (PPO). Our approach is evaluated across two critical legal tasks: (i) Court Judgment Prediction and Explanation (CJPE), and (ii) Legal Document Summarization. Although the framework underperforms on standard evaluation metrics compared to supervised and proprietary models, it provides valuable insights into the challenges of applying RL to legal texts. These challenges include reward model alignment, legal language complexity, and domain-specific adaptation. Through empirical and qualitative analysis, we demonstrate how RL can be repurposed for high-stakes, long-document tasks in law. Our findings establish a foundation for future work on optimizing legal reasoning pipelines using reinforcement learning, with broader implications for building interpretable and adaptive legal AI systems.",
        "url": "http://arxiv.org/abs/2512.18014v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18014v1",
        "arxiv_id": "2512.18014v1",
        "authors": [
            "Shubham Kumar Nigam",
            "Tanuj Tyagi",
            "Siddharth Shukla",
            "Aditya Kumar Guru",
            "Balaramamahanthi Deepak Patnaik",
            "Danush Khanna",
            "Noel Shallum",
            "Kripabandhu Ghosh",
            "Arnab Bhattacharya"
        ],
        "submitted": "2025-12-19 19:13:41",
        "source": "arxiv",
        "comment": "Accepted in AILaw @ AAAI 2026 conference"
    },
    {
        "title": "Seeing Justice Clearly: Handwritten Legal Document Translation with OCR and Vision-Language Models",
        "abstract": "Handwritten text recognition (HTR) and machine translation continue to pose significant challenges, particularly for low-resource languages like Marathi, which lack large digitized corpora and exhibit high variability in handwriting styles. The conventional approach to address this involves a two-stage pipeline: an OCR system extracts text from handwritten images, which is then translated into the target language using a machine translation model. In this work, we explore and compare the performance of traditional OCR-MT pipelines with Vision Large Language Models that aim to unify these stages and directly translate handwritten text images in a single, end-to-end step. Our motivation is grounded in the urgent need for scalable, accurate translation systems to digitize legal records such as FIRs, charge sheets, and witness statements in India's district and high courts. We evaluate both approaches on a curated dataset of handwritten Marathi legal documents, with the goal of enabling efficient legal document processing, even in low-resource environments. Our findings offer actionable insights toward building robust, edge-deployable solutions that enhance access to legal information for non-native speakers and legal professionals alike.",
        "url": "http://arxiv.org/abs/2512.18004v1",
        "pdf_url": "https://arxiv.org/pdf/2512.18004v1",
        "arxiv_id": "2512.18004v1",
        "authors": [
            "Shubham Kumar Nigam",
            "Parjanya Aditya Shukla",
            "Noel Shallum",
            "Arnab Bhattacharya"
        ],
        "submitted": "2025-12-19 19:06:14",
        "source": "arxiv",
        "comment": "Accepted in AILaw @ AAAI 2026 Conference"
    }
]