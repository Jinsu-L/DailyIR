[
    {
        "title": "Scaling Open-Ended Reasoning to Predict the Future",
        "abstract": "High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.",
        "url": "http://arxiv.org/abs/2512.25070v1",
        "pdf_url": "https://arxiv.org/pdf/2512.25070v1",
        "arxiv_id": "2512.25070v1",
        "authors": [
            "Nikhil Chandak",
            "Shashwat Goel",
            "Ameya Prabhu",
            "Moritz Hardt",
            "Jonas Geiping"
        ],
        "submitted": "2025-12-31 18:59:51",
        "source": "arxiv",
        "comment": "45 pages"
    },
    {
        "title": "Many Minds from One Model: Bayesian Transformers for Population Intelligence",
        "abstract": "Despite their scale and success, modern transformers are almost universally trained as single-minded systems: optimization produces one deterministic set of parameters, representing a single functional hypothesis about the data. Motivated by the idea that intelligence emerge from many minds, we propose Population Bayesian Transformers (B-Trans), which transform a standard Large Language Model into a Bayesian Transformer model to supports sampling diverse yet coherent model instances from a single set of pre-trained weights.\n  B-Trans introduces a Bayesian-motivated posterior proxy by treating the bias-like offsets in normalization layers as stochastic variables with a Gaussian variational approximation, inducing a distribution over model behavior without the cost of training full Bayesian neural networks. Sampling from this proxy yields a set of model instances with diverse behaviors while maintaining general competence. To preserve coherence within each generation, we freeze the sampled noise at the sequence level, enforcing temporal consistency across tokens. B-Trans allows for population-level decision-making, where aggregating predictions across sampled individuals significantly enhances exploration. Experiments across zero-shot generation, Reinforcement Learning with Verifiable Rewards (RLVR), and RL without explicit labels demonstrate that B-Trans effectively leverage the wisdom of crowds, yielding superior semantic diversity while achieving better task performance compared to deterministic baselines.",
        "url": "http://arxiv.org/abs/2512.25063v1",
        "pdf_url": "https://arxiv.org/pdf/2512.25063v1",
        "arxiv_id": "2512.25063v1",
        "authors": [
            "Diji Yang",
            "Yi Zhang"
        ],
        "submitted": "2025-12-31 18:56:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AdaGReS:Adaptive Greedy Context Selection via Redundancy-Aware Scoring for Token-Budgeted RAG",
        "abstract": "Retrieval-augmented generation (RAG) is highly sensitive to the quality of selected context, yet standard top-k retrieval often returns redundant or near-duplicate chunks that waste token budget and degrade downstream generation. We present AdaGReS, a redundancy-aware context selection framework for token-budgeted RAG that optimizes a set-level objective combining query-chunk relevance and intra-set redundancy penalties. AdaGReS performs greedy selection under a token-budget constraint using marginal gains derived from the objective, and introduces a closed-form, instance-adaptive calibration of the relevance-redundancy trade-off parameter to eliminate manual tuning and adapt to candidate-pool statistics and budget limits. We further provide a theoretical analysis showing that the proposed objective exhibits epsilon-approximate submodularity under practical embedding similarity conditions, yielding near-optimality guarantees for greedy selection. Experiments on open-domain question answering (Natural Questions) and a high-redundancy biomedical (drug) corpus demonstrate consistent improvements in redundancy control and context quality, translating to better end-to-end answer quality and robustness across settings.",
        "url": "http://arxiv.org/abs/2512.25052v1",
        "pdf_url": "https://arxiv.org/pdf/2512.25052v1",
        "arxiv_id": "2512.25052v1",
        "authors": [
            "Chao Peng",
            "Bin Wang",
            "Zhilei Long",
            "Jinfang Sheng"
        ],
        "submitted": "2025-12-31 18:48:07",
        "source": "arxiv",
        "comment": "Preprint. Under review"
    },
    {
        "title": "Modeling Language as a Sequence of Thoughts",
        "abstract": "Transformer language models can generate strikingly natural text by modeling language as a sequence of tokens. Yet, by relying primarily on surface-level co-occurrence statistics, they fail to form globally consistent latent representations of entities and events, lack of which contributes to brittleness in relational direction (e.g., reversal curse), contextualization errors, and data inefficiency. On the other hand, cognitive science shows that human comprehension involves converting the input linguistic stream into compact, event-like representations that persist in memory while verbatim form is short-lived. Motivated by this view, we introduce Thought Gestalt (TG) model, a recurrent Transformer that models language at two levels of abstraction - tokens and sentence-level \"thought\" states. TG generates the tokens of one sentence at a time while cross-attending to a memory of prior sentence representations. In TG, token and sentence representations are generated using the same set of model parameters and trained with a single objective, the next-token cross-entropy: by retaining the computation graph of sentence representations written to memory, gradients from future token losses flow backward through cross-attention to optimize the parameters generating earlier sentence vectors. In scaling experiments, TG consistently improves efficiency over matched GPT-2 runs, among other baselines, with scaling fits indicating GPT-2 requires ~5-8% more data and ~33-42% more parameters to match TG's loss. TG also reduces errors on relational direction generalization on a father-son reversal curse probe.",
        "url": "http://arxiv.org/abs/2512.25026v1",
        "pdf_url": "https://arxiv.org/pdf/2512.25026v1",
        "arxiv_id": "2512.25026v1",
        "authors": [
            "Nasim Borazjanizadeh",
            "James McClelland"
        ],
        "submitted": "2025-12-31 18:24:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MAMA-Memeia! Multi-Aspect Multi-Agent Collaboration for Depressive Symptoms Identification in Memes",
        "abstract": "Over the past years, memes have evolved from being exclusively a medium of humorous exchanges to one that allows users to express a range of emotions freely and easily. With the ever-growing utilization of memes in expressing depressive sentiments, we conduct a study on identifying depressive symptoms exhibited by memes shared by users of online social media platforms. We introduce RESTOREx as a vital resource for detecting depressive symptoms in memes on social media through the Large Language Model (LLM) generated and human-annotated explanations. We introduce MAMAMemeia, a collaborative multi-agent multi-aspect discussion framework grounded in the clinical psychology method of Cognitive Analytic Therapy (CAT) Competencies. MAMAMemeia improves upon the current state-of-the-art by 7.55% in macro-F1 and is established as the new benchmark compared to over 30 methods.",
        "url": "http://arxiv.org/abs/2512.25015v1",
        "pdf_url": "https://arxiv.org/pdf/2512.25015v1",
        "arxiv_id": "2512.25015v1",
        "authors": [
            "Siddhant Agarwal",
            "Adya Dhuler",
            "Polly Ruhnke",
            "Melvin Speisman",
            "Md Shad Akhtar",
            "Shweta Yadav"
        ],
        "submitted": "2025-12-31 18:06:21",
        "source": "arxiv",
        "comment": "Accepted by AAAI 2026"
    },
    {
        "title": "Classifying long legal documents using short random chunks",
        "abstract": "Classifying legal documents is a challenge, besides their specialized vocabulary, sometimes they can be very long. This means that feeding full documents to a Transformers-based models for classification might be impossible, expensive or slow. Thus, we present a legal document classifier based on DeBERTa V3 and a LSTM, that uses as input a collection of 48 randomly-selected short chunks (max 128 tokens). Besides, we present its deployment pipeline using Temporal, a durable execution solution, which allow us to have a reliable and robust processing workflow. The best model had a weighted F-score of 0.898, while the pipeline running on CPU had a processing median time of 498 seconds per 100 files.",
        "url": "http://arxiv.org/abs/2512.24997v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24997v1",
        "arxiv_id": "2512.24997v1",
        "authors": [
            "Luis Adrián Cabrera-Diego"
        ],
        "submitted": "2025-12-31 17:48:08",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Large language models and the entropy of English",
        "abstract": "We use large language models (LLMs) to uncover long-ranged structure in English texts from a variety of sources. The conditional entropy or code length in many cases continues to decrease with context length at least to $N\\sim 10^4$ characters, implying that there are direct dependencies or interactions across these distances. A corollary is that there are small but significant correlations between characters at these separations, as we show from the data independent of models. The distribution of code lengths reveals an emergent certainty about an increasing fraction of characters at large $N$. Over the course of model training, we observe different dynamics at long and short context lengths, suggesting that long-ranged structure is learned only gradually. Our results constrain efforts to build statistical physics models of LLMs or language itself.",
        "url": "http://arxiv.org/abs/2512.24969v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24969v1",
        "arxiv_id": "2512.24969v1",
        "authors": [
            "Colin Scheibner",
            "Lindsay M. Smith",
            "William Bialek"
        ],
        "submitted": "2025-12-31 16:54:44",
        "source": "arxiv",
        "comment": "8 pages, 6 figures"
    },
    {
        "title": "CPJ: Explainable Agricultural Pest Diagnosis via Caption-Prompt-Judge with LLM-Judged Refinement",
        "abstract": "Accurate and interpretable crop disease diagnosis is essential for agricultural decision-making, yet existing methods often rely on costly supervised fine-tuning and perform poorly under domain shifts. We propose Caption--Prompt--Judge (CPJ), a training-free few-shot framework that enhances Agri-Pest VQA through structured, interpretable image captions. CPJ employs large vision-language models to generate multi-angle captions, refined iteratively via an LLM-as-Judge module, which then inform a dual-answer VQA process for both recognition and management responses. Evaluated on CDDMBench, CPJ significantly improves performance: using GPT-5-mini captions, GPT-5-Nano achieves \\textbf{+22.7} pp in disease classification and \\textbf{+19.5} points in QA score over no-caption baselines. The framework provides transparent, evidence-based reasoning, advancing robust and explainable agricultural diagnosis without fine-tuning. Our code and data are publicly available at: https://github.com/CPJ-Agricultural/CPJ-Agricultural-Diagnosis.",
        "url": "http://arxiv.org/abs/2512.24947v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24947v1",
        "arxiv_id": "2512.24947v1",
        "authors": [
            "Wentao Zhang",
            "Tao Fang",
            "Lina Lu",
            "Lifei Wang",
            "Weihe Zhong"
        ],
        "submitted": "2025-12-31 16:21:31",
        "source": "arxiv",
        "comment": "This paper is 6 pages in length and contains 2 figures. Tao Fang (Corresponding Author), Lina Lu (Co-corresponding Author)"
    },
    {
        "title": "RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment",
        "abstract": "Search relevance plays a central role in web e-commerce. While large language models (LLMs) have shown significant results on relevance task, existing benchmarks lack sufficient complexity for comprehensive model assessment, resulting in an absence of standardized relevance evaluation metrics across the industry. To address this limitation, we propose Rule-Aware benchmark with Image for Relevance assessment(RAIR), a Chinese dataset derived from real-world scenarios. RAIR established a standardized framework for relevance assessment and provides a set of universal rules, which forms the foundation for standardized evaluation. Additionally, RAIR analyzes essential capabilities required for current relevance models and introduces a comprehensive dataset consists of three subset: (1) a general subset with industry-balanced sampling to evaluate fundamental model competencies; (2) a long-tail hard subset focus on challenging cases to assess performance limits; (3) a visual salience subset for evaluating multimodal understanding capabilities. We conducted experiments on RAIR using 14 open and closed-source models. The results demonstrate that RAIR presents sufficient challenges even for GPT-5, which achieved the best performance. RAIR data are now available, serving as an industry benchmark for relevance assessment while providing new insights into general LLM and Visual Language Model(VLM) evaluation.",
        "url": "http://arxiv.org/abs/2512.24943v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24943v1",
        "arxiv_id": "2512.24943v1",
        "authors": [
            "Chenji Lu",
            "Zhuo Chen",
            "Hui Zhao",
            "Zhenyi Wang",
            "Pengjie Wang",
            "Jian Xu",
            "Bo Zheng"
        ],
        "submitted": "2025-12-31 16:09:08",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Iterative Deployment Improves Planning Skills in LLMs",
        "abstract": "We show that iterative deployment of large language models (LLMs), each fine-tuned on data carefully curated by users from the previous models' deployment, can significantly change the properties of the resultant models. By testing this mechanism on various planning domains, we observe substantial improvements in planning skills, with later models displaying emergent generalization by discovering much longer plans than the initial models. We then provide theoretical analysis showing that iterative deployment effectively implements reinforcement learning (RL) training in the outer-loop (i.e. not as part of intentional model training), with an implicit reward function. The connection to RL has two important implications: first, for the field of AI safety, as the reward function entailed by repeated deployment is not defined explicitly, and could have unexpected implications to the properties of future model deployments. Second, the mechanism highlighted here can be viewed as an alternative training regime to explicit RL, relying on data curation rather than explicit rewards.",
        "url": "http://arxiv.org/abs/2512.24940v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24940v1",
        "arxiv_id": "2512.24940v1",
        "authors": [
            "Augusto B. Corrêa",
            "Yoav Gelberg",
            "Luckeciano C. Melo",
            "Ilia Shumailov",
            "André G. Pereira",
            "Yarin Gal"
        ],
        "submitted": "2025-12-31 16:03:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Vibe Coding, Interface Flattening",
        "abstract": "Large language models are reshaping programming by enabling 'vibe coding': the development of softwares through natural-language interaction with model-driven toolchains. This article argues that vibe coding is best understood as interface flattening, a reconfiguration in which previously distinct modalities (GUI, CLI, and API) appear to converge into a single conversational surface, even as the underlying chain of translation from intention to machinic effect lengthens and thickens. Drawing on Friedrich Kittler's materialist media theory and Alexander Galloway's account of interfaces as sites of protocol control, the paper situates programming as a historically localised interface arrangement rather than an essential relation to computation. Through a materialist reconstruction of the contemporary vibe-coding stack, it shows how remote compute infrastructures, latency and connectivity, structured outputs, function/tool calling, and interoperability standards such as the Model Context Protocol relocate control and meaning-making power to model and protocol providers. The apparent democratisation of technical capability therefore depends on new dependencies and new literacies. By foregrounding the tension between experiential flattening and infrastructural thickening, I demonstrate how LLM-mediated development redistributes symbolic labour/power, obscures responsibility, and privatises competencies previously dispersed across programming communities, contributing a critical lens on the political economy of AI-mediated human-computer interaction.",
        "url": "http://arxiv.org/abs/2512.24939v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24939v1",
        "arxiv_id": "2512.24939v1",
        "authors": [
            "Hongrui Jin"
        ],
        "submitted": "2025-12-31 16:00:59",
        "source": "arxiv",
        "comment": "16 pages, 1 figure"
    },
    {
        "title": "Adaptive Dependency-aware Prompt Optimization Framework for Multi-Step LLM Pipeline",
        "abstract": "Multi-step LLM pipelines invoke large language models multiple times in a structured sequence and can effectively solve complex tasks, but their performance heavily depends on the prompts used at each step. Jointly optimizing these prompts is difficult due to missing step-level supervision and inter-step dependencies. Existing end-to-end prompt optimization methods struggle under these conditions and often yield suboptimal or unstable updates. We propose ADOPT, an Adaptive Dependency-aware Prompt Optimization framework for multi-step LLM pipelines. ADOPT explicitly models the dependency between each LLM step and the final task outcome, enabling precise text-gradient estimation analogous to computing analytical derivatives. It decouples textual gradient estimation from gradient updates, reducing multi-prompt optimization to flexible single-prompt optimization steps, and employs a Shapley-based mechanism to adaptively allocate optimization resources. Experiments on real-world datasets and diverse pipeline structures show that ADOPT is effective and robust, consistently outperforming state-of-the-art prompt optimization baselines.",
        "url": "http://arxiv.org/abs/2512.24933v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24933v1",
        "arxiv_id": "2512.24933v1",
        "authors": [
            "Minjun Zhao",
            "Xinyu Zhang",
            "Shuai Zhang",
            "Deyang Li",
            "Ruifeng Shi"
        ],
        "submitted": "2025-12-31 15:46:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts",
        "abstract": "Strategic dialogue requires agents to execute distinct dialogue acts, for which belief estimation is essential. While prior work often estimates beliefs accurately, it lacks a principled mechanism to use those beliefs during generation. We bridge this gap by first formalizing two core acts Adversarial and Alignment, and by operationalizing them via probabilistic constraints on what an agent may generate. We instantiate this idea in BEDA, a framework that consists of the world set, the belief estimator for belief estimation, and the conditional generator that selects acts and realizes utterances consistent with the inferred beliefs. Across three settings, Conditional Keeper Burglar (CKBG, adversarial), Mutual Friends (MF, cooperative), and CaSiNo (negotiation), BEDA consistently outperforms strong baselines: on CKBG it improves success rate by at least 5.0 points across backbones and by 20.6 points with GPT-4.1-nano; on Mutual Friends it achieves an average improvement of 9.3 points; and on CaSiNo it achieves the optimal deal relative to all baselines. These results indicate that casting belief estimation as constraints provides a simple, general mechanism for reliable strategic dialogue.",
        "url": "http://arxiv.org/abs/2512.24885v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24885v1",
        "arxiv_id": "2512.24885v1",
        "authors": [
            "Hengli Li",
            "Zhaoxin Yu",
            "Qi Shen",
            "Chenxi Li",
            "Mengmeng Wang",
            "Tinglang Wu",
            "Yipeng Kang",
            "Yuxuan Wang",
            "Song-Chun Zhu",
            "Zixia Jia",
            "Zilong Zheng"
        ],
        "submitted": "2025-12-31 14:26:55",
        "source": "arxiv",
        "comment": "Accepted by AAMAS 2026"
    },
    {
        "title": "mHC: Manifold-Constrained Hyper-Connections",
        "abstract": "Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.",
        "url": "http://arxiv.org/abs/2512.24880v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24880v1",
        "arxiv_id": "2512.24880v1",
        "authors": [
            "Zhenda Xie",
            "Yixuan Wei",
            "Huanqi Cao",
            "Chenggang Zhao",
            "Chengqi Deng",
            "Jiashi Li",
            "Damai Dai",
            "Huazuo Gao",
            "Jiang Chang",
            "Liang Zhao",
            "Shangyan Zhou",
            "Zhean Xu",
            "Zhengyan Zhang",
            "Wangding Zeng",
            "Shengding Hu",
            "Yuqing Wang",
            "Jingyang Yuan",
            "Lean Wang",
            "Wenfeng Liang"
        ],
        "submitted": "2025-12-31 14:16:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem",
        "abstract": "Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.",
        "url": "http://arxiv.org/abs/2512.24873v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24873v1",
        "arxiv_id": "2512.24873v1",
        "authors": [
            "Weixun Wang",
            "XiaoXiao Xu",
            "Wanhe An",
            "Fangwen Dai",
            "Wei Gao",
            "Yancheng He",
            "Ju Huang",
            "Qiang Ji",
            "Hanqi Jin",
            "Xiaoyang Li",
            "Yang Li",
            "Zhongwen Li",
            "Shirong Lin",
            "Jiashun Liu",
            "Zenan Liu",
            "Tao Luo",
            "Dilxat Muhtar",
            "Yuanbin Qu",
            "Jiaqiang Shi",
            "Qinghui Sun",
            "Yingshui Tan",
            "Hao Tang",
            "Runze Wang",
            "Yi Wang",
            "Zhaoguo Wang",
            "Yanan Wu",
            "Shaopan Xiong",
            "Binchen Xu",
            "Xander Xu",
            "Yuchi Xu",
            "Qipeng Zhang",
            "Xixia Zhang",
            "Haizhou Zhao",
            "Jie Zhao",
            "Shuaibing Zhao",
            "Baihui Zheng",
            "Jianhui Zheng",
            "Suhang Zheng",
            "Yanni Zhu",
            "Mengze Cai",
            "Kerui Cao",
            "Xitong Chen",
            "Yue Dai",
            "Lifan Du",
            "Tao Feng",
            "Tao He",
            "Jin Hu",
            "Yijie Hu",
            "Ziyu Jiang",
            "Cheng Li",
            "Xiang Li",
            "Jing Liang",
            "Chonghuan Liu",
            "ZhenDong Liu",
            "Haodong Mi",
            "Yanhu Mo",
            "Junjia Ni",
            "Shixin Pei",
            "Jingyu Shen",
            "XiaoShuai Song",
            "Cecilia Wang",
            "Chaofan Wang",
            "Kangyu Wang",
            "Pei Wang",
            "Tao Wang",
            "Wei Wang",
            "Ke Xiao",
            "Mingyu Xu",
            "Tiange Xu",
            "Nan Ya",
            "Siran Yang",
            "Jianan Ye",
            "Yaxing Zang",
            "Duo Zhang",
            "Junbo Zhang",
            "Boren Zheng",
            "Wanxi Deng",
            "Ling Pan",
            "Lin Qu",
            "Wenbo Su",
            "Jiamang Wang",
            "Wei Wang",
            "Hu Wei",
            "Minggang Wu",
            "Cheng Yu",
            "Bing Zhao",
            "Zhicheng Zheng",
            "Bo Zheng"
        ],
        "submitted": "2025-12-31 14:03:39",
        "source": "arxiv",
        "comment": "36 pages, 15 figures"
    },
    {
        "title": "Encyclo-K: Evaluating LLMs with Dynamically Composed Knowledge Statements",
        "abstract": "Benchmarks play a crucial role in tracking the rapid advancement of large language models (LLMs) and identifying their capability boundaries. However, existing benchmarks predominantly curate questions at the question level, suffering from three fundamental limitations: vulnerability to data contamination, restriction to single-knowledge-point assessment, and reliance on costly domain expert annotation. We propose Encyclo-K, a statement-based benchmark that rethinks benchmark construction from the ground up. Our key insight is that knowledge statements, not questions, can serve as the unit of curation, and questions can then be constructed from them. We extract standalone knowledge statements from authoritative textbooks and dynamically compose them into evaluation questions through random sampling at test time. This design directly addresses all three limitations: the combinatorial space is too vast to memorize, and model rankings remain stable across dynamically generated question sets, enabling reliable periodic dataset refresh; each question aggregates 8-10 statements for comprehensive multi-knowledge assessment; annotators only verify formatting compliance without requiring domain expertise, substantially reducing annotation costs. Experiments on over 50 LLMs demonstrate that Encyclo-K poses substantial challenges with strong discriminative power. Even the top-performing OpenAI-GPT-5.1 achieves only 62.07% accuracy, and model performance displays a clear gradient distribution--reasoning models span from 16.04% to 62.07%, while chat models range from 9.71% to 50.40%. These results validate the challenges introduced by dynamic evaluation and multi-statement comprehensive understanding. These findings establish Encyclo-K as a scalable framework for dynamic evaluation of LLMs' comprehensive understanding over multiple fine-grained disciplinary knowledge statements.",
        "url": "http://arxiv.org/abs/2512.24867v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24867v1",
        "arxiv_id": "2512.24867v1",
        "authors": [
            "Yiming Liang",
            "Yizhi Li",
            "Yantao Du",
            "Ge Zhang",
            "Jiayi Zhou",
            "Yuchen Wu",
            "Yinzhu Piao",
            "Denghui Cao",
            "Tong Sun",
            "Ziniu Li",
            "Li Du",
            "Bo Lei",
            "Jiaheng Liu",
            "Chenghua Lin",
            "Zhaoxiang Zhang",
            "Wenhao Huang",
            "Jiajun Zhang"
        ],
        "submitted": "2025-12-31 13:55:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Big AI is accelerating the metacrisis: What can we do?",
        "abstract": "The world is in the grip of ecological, meaning, and language crises which are converging into a metacrisis. Big AI is accelerating them all. Language engineers are playing a central role, persisting with a scalability story that is failing humanity, supplying critical talent to plutocrats and kleptocrats, and creating new technologies as if the whole endeavour was value-free. We urgently need to explore alternatives, applying our collective intelligence to design a life-affirming future for NLP that is centered on human flourishing on a living planet.",
        "url": "http://arxiv.org/abs/2512.24863v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24863v1",
        "arxiv_id": "2512.24863v1",
        "authors": [
            "Steven Bird"
        ],
        "submitted": "2025-12-31 13:49:56",
        "source": "arxiv",
        "comment": "9 pages, 1 figure"
    },
    {
        "title": "PrivacyBench: A Conversational Benchmark for Evaluating Privacy in Personalized AI",
        "abstract": "Personalized AI agents rely on access to a user's digital footprint, which often includes sensitive data from private emails, chats and purchase histories. Yet this access creates a fundamental societal and privacy risk: systems lacking social-context awareness can unintentionally expose user secrets, threatening digital well-being. We introduce PrivacyBench, a benchmark with socially grounded datasets containing embedded secrets and a multi-turn conversational evaluation to measure secret preservation. Testing Retrieval-Augmented Generation (RAG) assistants reveals that they leak secrets in up to 26.56% of interactions. A privacy-aware prompt lowers leakage to 5.12%, yet this measure offers only partial mitigation. The retrieval mechanism continues to access sensitive data indiscriminately, which shifts the entire burden of privacy preservation onto the generator. This creates a single point of failure, rendering current architectures unsafe for wide-scale deployment. Our findings underscore the urgent need for structural, privacy-by-design safeguards to ensure an ethical and inclusive web for everyone.",
        "url": "http://arxiv.org/abs/2512.24848v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24848v1",
        "arxiv_id": "2512.24848v1",
        "authors": [
            "Srija Mukhopadhyay",
            "Sathwik Reddy",
            "Shruthi Muthukumar",
            "Jisun An",
            "Ponnurangam Kumaraguru"
        ],
        "submitted": "2025-12-31 13:16:45",
        "source": "arxiv",
        "comment": "11 pages, 2 figures"
    },
    {
        "title": "Triangulation as an Acceptance Rule for Multilingual Mechanistic Interpretability",
        "abstract": "Multilingual language models achieve strong aggregate performance yet often behave unpredictably across languages, scripts, and cultures. We argue that mechanistic explanations for such models should satisfy a \\emph{causal} standard: claims must survive causal interventions and must \\emph{cross-reference} across environments that perturb surface form while preserving meaning. We formalize \\emph{reference families} as predicate-preserving variants and introduce \\emph{triangulation}, an acceptance rule requiring necessity (ablating the circuit degrades the target behavior), sufficiency (patching activations transfers the behavior), and invariance (both effects remain directionally stable and of sufficient magnitude across the reference family). To supply candidate subgraphs, we adopt automatic circuit discovery and \\emph{accept or reject} those candidates by triangulation. We ground triangulation in causal abstraction by casting it as an approximate transformation score over a distribution of interchange interventions, connect it to the pragmatic interpretability agenda, and present a comparative experimental protocol across multiple model families, language pairs, and tasks. Triangulation provides a falsifiable standard for mechanistic claims that filters spurious circuits passing single-environment tests but failing cross-lingual invariance.",
        "url": "http://arxiv.org/abs/2512.24842v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24842v1",
        "arxiv_id": "2512.24842v1",
        "authors": [
            "Yanan Long"
        ],
        "submitted": "2025-12-31 13:03:34",
        "source": "arxiv",
        "comment": "NeurIPS 2025 Workshop Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent Abilities, and Scaling"
    },
    {
        "title": "Practising responsibility: Ethics in NLP as a hands-on course",
        "abstract": "As Natural Language Processing (NLP) systems become more pervasive, integrating ethical considerations into NLP education has become essential. However, this presents inherent challenges in curriculum development: the field's rapid evolution from both academia and industry, and the need to foster critical thinking beyond traditional technical training. We introduce our course on Ethical Aspects in NLP and our pedagogical approach, grounded in active learning through interactive sessions, hands-on activities, and \"learning by teaching\" methods. Over four years, the course has been refined and adapted across different institutions, educational levels, and interdisciplinary backgrounds; it has also yielded many reusable products, both in the form of teaching materials and in the form of actual educational products aimed at diverse audiences, made by the students themselves. By sharing our approach and experience, we hope to provide inspiration for educators seeking to incorporate social impact considerations into their curricula.",
        "url": "http://arxiv.org/abs/2512.24825v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24825v1",
        "arxiv_id": "2512.24825v1",
        "authors": [
            "Malvina Nissim",
            "Viviana Patti",
            "Beatrice Savoldi"
        ],
        "submitted": "2025-12-31 12:26:20",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "HiGR: Efficient Generative Slate Recommendation via Hierarchical Planning and Multi-Objective Preference Alignment",
        "abstract": "Slate recommendation, where users are presented with a ranked list of items simultaneously, is widely adopted in online platforms. Recent advances in generative models have shown promise in slate recommendation by modeling sequences of discrete semantic IDs autoregressively. However, existing autoregressive approaches suffer from semantically entangled item tokenization and inefficient sequential decoding that lacks holistic slate planning. To address these limitations, we propose HiGR, an efficient generative slate recommendation framework that integrates hierarchical planning with listwise preference alignment. First, we propose an auto-encoder utilizing residual quantization and contrastive constraints to tokenize items into semantically structured IDs for controllable generation. Second, HiGR decouples generation into a list-level planning stage for global slate intent, followed by an item-level decoding stage for specific item selection. Third, we introduce a listwise preference alignment objective to directly optimize slate quality using implicit user feedback. Experiments on our large-scale commercial media platform demonstrate that HiGR delivers consistent improvements in both offline evaluations and online deployment. Specifically, it outperforms state-of-the-art methods by over 10% in offline recommendation quality with a 5x inference speedup, while further achieving a 1.22% and 1.73% increase in Average Watch Time and Average Video Views in online A/B tests.",
        "url": "http://arxiv.org/abs/2512.24787v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24787v1",
        "arxiv_id": "2512.24787v1",
        "authors": [
            "Yunsheng Pang",
            "Zijian Liu",
            "Yudong Li",
            "Shaojie Zhu",
            "Zijian Luo",
            "Chenyun Yu",
            "Sikai Wu",
            "Shichen Shen",
            "Cong Xu",
            "Bin Wang",
            "Kai Jiang",
            "Hongyong Yu",
            "Chengxiang Zhuo",
            "Zang Li"
        ],
        "submitted": "2025-12-31 11:16:24",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Compute-Accuracy Pareto Frontiers for Open-Source Reasoning Large Language Models",
        "abstract": "Large Language Models (LLMs) are demonstrating rapid improvements on complex reasoning benchmarks, particularly when allowed to utilize intermediate reasoning steps before converging on a final solution. However, current literature often overlooks the significant computational burden associated with generating long reasoning sequences. For industrial applications, model selection depends not only on raw accuracy but also on resource constraints and inference costs. In this work, we conduct a test-time-compute aware evaluation of both contemporary and older open-source LLMs, mapping their Pareto frontiers across math- and reasoning-intensive benchmarks. Our findings identify the Mixture of Experts (MoE) architecture as a strong candidate to balance performance and efficiency in our evaluation setting. Furthermore, we trace the trajectory of Pareto efficiency over time to derive an emergent trend regarding accuracy gain per unit of compute. Finally, we demonstrate that there is a saturation point for inference-time compute. Beyond a certain threshold, accuracy gains diminish, indicating that while extended reasoning capabilities are beneficial, they cannot overcome intrinsic model limitations regarding specific complexities.",
        "url": "http://arxiv.org/abs/2512.24776v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24776v1",
        "arxiv_id": "2512.24776v1",
        "authors": [
            "Ákos Prucs",
            "Márton Csutora",
            "Mátyás Antal",
            "Márk Marosi"
        ],
        "submitted": "2025-12-31 10:51:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Uncertainty-aware Semi-supervised Ensemble Teacher Framework for Multilingual Depression Detection",
        "abstract": "Detecting depression from social media text is still a challenging task. This is due to different language styles, informal expression, and the lack of annotated data in many languages. To tackle these issues, we propose, Semi-SMDNet, a strong Semi-Supervised Multilingual Depression detection Network. It combines teacher-student pseudo-labelling, ensemble learning, and augmentation of data. Our framework uses a group of teacher models. Their predictions come together through soft voting. An uncertainty-based threshold filters out low-confidence pseudo-labels to reduce noise and improve learning stability. We also use a confidence-weighted training method that focuses on reliable pseudo-labelled samples. This greatly boosts robustness across languages. Tests on Arabic, Bangla, English, and Spanish datasets show that our approach consistently beats strong baselines. It significantly reduces the performance gap between settings that have plenty of resources and those that do not. Detailed experiments and studies confirm that our framework is effective and can be used in various situations. This shows that it is suitable for scalable, cross-language mental health monitoring where labelled resources are limited.",
        "url": "http://arxiv.org/abs/2512.24772v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24772v1",
        "arxiv_id": "2512.24772v1",
        "authors": [
            "Mohammad Zia Ur Rehman",
            "Velpuru Navya",
            "Sanskar",
            "Shuja Uddin Qureshi",
            "Nagendra Kumar"
        ],
        "submitted": "2025-12-31 10:35:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "OpenOneRec Technical Report",
        "abstract": "While the OneRec series has successfully unified the fragmented recommendation pipeline into an end-to-end generative framework, a significant gap remains between recommendation systems and general intelligence. Constrained by isolated data, they operate as domain specialists-proficient in pattern matching but lacking world knowledge, reasoning capabilities, and instruction following. This limitation is further compounded by the lack of a holistic benchmark to evaluate such integrated capabilities. To address this, our contributions are: 1) RecIF Bench & Open Data: We propose RecIF-Bench, a holistic benchmark covering 8 diverse tasks that thoroughly evaluate capabilities from fundamental prediction to complex reasoning. Concurrently, we release a massive training dataset comprising 96 million interactions from 160,000 users to facilitate reproducible research. 2) Framework & Scaling: To ensure full reproducibility, we open-source our comprehensive training pipeline, encompassing data processing, co-pretraining, and post-training. Leveraging this framework, we demonstrate that recommendation capabilities can scale predictably while mitigating catastrophic forgetting of general knowledge. 3) OneRec-Foundation: We release OneRec Foundation (1.7B and 8B), a family of models establishing new state-of-the-art (SOTA) results across all tasks in RecIF-Bench. Furthermore, when transferred to the Amazon benchmark, our models surpass the strongest baselines with an average 26.8% improvement in Recall@10 across 10 diverse datasets (Figure 1). This work marks a step towards building truly intelligent recommender systems. Nonetheless, realizing this vision presents significant technical and theoretical challenges, highlighting the need for broader research engagement in this promising direction.",
        "url": "http://arxiv.org/abs/2512.24762v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24762v1",
        "arxiv_id": "2512.24762v1",
        "authors": [
            "Guorui Zhou",
            "Honghui Bao",
            "Jiaming Huang",
            "Jiaxin Deng",
            "Jinghao Zhang",
            "Junda She",
            "Kuo Cai",
            "Lejian Ren",
            "Lu Ren",
            "Qiang Luo",
            "Qianqian Wang",
            "Qigen Hu",
            "Rongzhou Zhang",
            "Ruiming Tang",
            "Shiyao Wang",
            "Wuchao Li",
            "Xiangyu Wu",
            "Xinchen Luo",
            "Xingmei Wang",
            "Yifei Hu",
            "Yunfan Wu",
            "Zhanyu Liu",
            "Zhiyang Zhang",
            "Zixing Zhang",
            "Bo Chen",
            "Bin Wen",
            "Chaoyi Ma",
            "Chengru Song",
            "Chenglong Chu",
            "Defu Lian",
            "Fan Yang",
            "Feng Jiang",
            "Hongtao Cheng",
            "Huanjie Wang",
            "Kun Gai",
            "Pengfei Zheng",
            "Qiang Wang",
            "Rui Huang",
            "Siyang Mao",
            "Tingting Gao",
            "Wei Yuan",
            "Yan Wang",
            "Yang Zhou",
            "Yi Su",
            "Zexuan Cheng",
            "Zhixin Ling",
            "Ziming Li"
        ],
        "submitted": "2025-12-31 10:15:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "BIOME-Bench: A Benchmark for Biomolecular Interaction Inference and Multi-Omics Pathway Mechanism Elucidation from Scientific Literature",
        "abstract": "Multi-omics studies often rely on pathway enrichment to interpret heterogeneous molecular changes, but pathway enrichment (PE)-based workflows inherit structural limitations of pathway resources, including curation lag, functional redundancy, and limited sensitivity to molecular states and interventions. Although recent work has explored using large language models (LLMs) to improve PE-based interpretation, the lack of a standardized benchmark for end-to-end multi-omics pathway mechanism elucidation has largely confined evaluation to small, manually curated datasets or ad hoc case studies, hindering reproducible progress. To address this issue, we introduce BIOME-Bench, constructed via a rigorous four-stage workflow, to evaluate two core capabilities of LLMs in multi-omics analysis: Biomolecular Interaction Inference and end-to-end Multi-Omics Pathway Mechanism Elucidation. We develop evaluation protocols for both tasks and conduct comprehensive experiments across multiple strong contemporary models. Experimental results demonstrate that existing models still exhibit substantial deficiencies in multi-omics analysis, struggling to reliably distinguish fine-grained biomolecular relation types and to generate faithful, robust pathway-level mechanistic explanations.",
        "url": "http://arxiv.org/abs/2512.24733v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24733v1",
        "arxiv_id": "2512.24733v1",
        "authors": [
            "Sibo Wei",
            "Peng Chen",
            "Lifeng Dong",
            "Yin Luo",
            "Lei Wang",
            "Peng Zhang",
            "Wenpeng Lu",
            "Jianbin Guo",
            "Hongjun Yang",
            "Dajun Zeng"
        ],
        "submitted": "2025-12-31 09:01:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MDiffFR: Modality-Guided Diffusion Generation for Cold-start Items in Federated Recommendation",
        "abstract": "Federated recommendations (FRs) provide personalized services while preserving user privacy by keeping user data on local clients, which has attracted significant attention in recent years. However, due to the strict privacy constraints inherent in FRs, access to user-item interaction data and user profiles across clients is highly restricted, making it difficult to learn globally effective representations for new (cold-start) items. Consequently, the item cold-start problem becomes even more challenging in FRs. Existing solutions typically predict embeddings for new items through the attribute-to-embedding mapping paradigm, which establishes a fixed one-to-one correspondence between item attributes and their embeddings. However, this one-to-one mapping paradigm often fails to model varying data distributions and tends to cause embedding misalignment, as verified by our empirical studies. To this end, we propose MDiffFR, a novel generation-based modality-guided diffusion method for cold-start items in FRs. In this framework, we employ a tailored diffusion model on the server to generate embeddings for new items, which are then distributed to clients for cold-start inference. To align item semantics, we deploy a pre-trained modality encoder to extract modality features as conditional signals to guide the reverse denoising process. Furthermore, our theoretical analysis verifies that the proposed method achieves stronger privacy guarantees compared to existing mapping-based approaches. Extensive experiments on four real datasets demonstrate that our method consistently outperforms all baselines in FRs.",
        "url": "http://arxiv.org/abs/2512.24715v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24715v1",
        "arxiv_id": "2512.24715v1",
        "authors": [
            "Kang Fu",
            "Honglei Zhang",
            "Xuechao Zou",
            "Yidong Li"
        ],
        "submitted": "2025-12-31 08:29:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MEIC-DT: Memory-Efficient Incremental Clustering for Long-Text Coreference Resolution with Dual-Threshold Constraints",
        "abstract": "In the era of large language models (LLMs), supervised neural methods remain the state-of-the-art (SOTA) for Coreference Resolution. Yet, their full potential is underexplored, particularly in incremental clustering, which faces the critical challenge of balancing efficiency with performance for long texts. To address the limitation, we propose \\textbf{MEIC-DT}, a novel dual-threshold, memory-efficient incremental clustering approach based on a lightweight Transformer. MEIC-DT features a dual-threshold constraint mechanism designed to precisely control the Transformer's input scale within a predefined memory budget. This mechanism incorporates a Statistics-Aware Eviction Strategy (\\textbf{SAES}), which utilizes distinct statistical profiles from the training and inference phases for intelligent cache management. Furthermore, we introduce an Internal Regularization Policy (\\textbf{IRP}) that strategically condenses clusters by selecting the most representative mentions, thereby preserving semantic integrity. Extensive experiments on common benchmarks demonstrate that MEIC-DT achieves highly competitive coreference performance under stringent memory constraints.",
        "url": "http://arxiv.org/abs/2512.24711v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24711v1",
        "arxiv_id": "2512.24711v1",
        "authors": [
            "Kangyang Luo",
            "Shuzheng Si",
            "Yuzhuo Bai",
            "Cheng Gao",
            "Zhitong Wang",
            "Cheng Huang",
            "Yingli Shen",
            "Yufeng Han",
            "Wenhao Li",
            "Cunliang Kong",
            "Maosong Sun"
        ],
        "submitted": "2025-12-31 08:26:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MUSIC: MUlti-Step Instruction Contrast for Multi-Turn Reward Models",
        "abstract": "Evaluating the quality of multi-turn conversations is crucial for developing capable Large Language Models (LLMs), yet remains a significant challenge, often requiring costly human evaluation. Multi-turn reward models (RMs) offer a scalable alternative and can provide valuable signals for guiding LLM training. While recent work has advanced multi-turn \\textit{training} techniques, effective automated \\textit{evaluation} specifically for multi-turn interactions lags behind. We observe that standard preference datasets, typically contrasting responses based only on the final conversational turn, provide insufficient signal to capture the nuances of multi-turn interactions. Instead, we find that incorporating contrasts spanning \\textit{multiple} turns is critical for building robust multi-turn RMs. Motivated by this finding, we propose \\textbf{MU}lti-\\textbf{S}tep \\textbf{I}nstruction \\textbf{C}ontrast (MUSIC), an unsupervised data augmentation strategy that synthesizes contrastive conversation pairs exhibiting differences across multiple turns. Leveraging MUSIC on the Skywork preference dataset, we train a multi-turn RM based on the Gemma-2-9B-Instruct model. Empirical results demonstrate that our MUSIC-augmented RM outperforms baseline methods, achieving higher alignment with judgments from advanced proprietary LLM judges on multi-turn conversations, crucially, without compromising performance on standard single-turn RM benchmarks.",
        "url": "http://arxiv.org/abs/2512.24693v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24693v1",
        "arxiv_id": "2512.24693v1",
        "authors": [
            "Wenzhe Li",
            "Shujian Zhang",
            "Wenxuan Zhou",
            "John Lambert",
            "Chi Jin",
            "Andrew Hard",
            "Rajiv Mathews",
            "Lun Wang"
        ],
        "submitted": "2025-12-31 07:54:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Quantum Visual Word Sense Disambiguation: Unraveling Ambiguities Through Quantum Inference Model",
        "abstract": "Visual word sense disambiguation focuses on polysemous words, where candidate images can be easily confused. Traditional methods use classical probability to calculate the likelihood of an image matching each gloss of the target word, summing these to form a posterior probability. However, due to the challenge of semantic uncertainty, glosses from different sources inevitably carry semantic biases, which can lead to biased disambiguation results. Inspired by quantum superposition in modeling uncertainty, this paper proposes a Quantum Inference Model for Unsupervised Visual Word Sense Disambiguation (Q-VWSD). It encodes multiple glosses of the target word into a superposition state to mitigate semantic biases. Then, the quantum circuit is executed, and the results are observed. By formalizing our method, we find that Q-VWSD is a quantum generalization of the method based on classical probability. Building on this, we further designed a heuristic version of Q-VWSD that can run more efficiently on classical computing. The experiments demonstrate that our method outperforms state-of-the-art classical methods, particularly by effectively leveraging non-specialized glosses from large language models, which further enhances performance. Our approach showcases the potential of quantum machine learning in practical applications and provides a case for leveraging quantum modeling advantages on classical computers while quantum hardware remains immature.",
        "url": "http://arxiv.org/abs/2512.24687v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24687v1",
        "arxiv_id": "2512.24687v1",
        "authors": [
            "Wenbo Qiao",
            "Peng Zhang",
            "Qinghua Hu"
        ],
        "submitted": "2025-12-31 07:47:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory",
        "abstract": "We present R-Debater, an agentic framework for generating multi-turn debates built on argumentative memory. Grounded in rhetoric and memory studies, the system views debate as a process of recalling and adapting prior arguments to maintain stance consistency, respond to opponents, and support claims with evidence. Specifically, R-Debater integrates a debate knowledge base for retrieving case-like evidence and prior debate moves with a role-based agent that composes coherent utterances across turns. We evaluate on standardized ORCHID debates, constructing a 1,000-item retrieval corpus and a held-out set of 32 debates across seven domains. Two tasks are evaluated: next-utterance generation, assessed by InspireScore (subjective, logical, and factual), and adversarial multi-turn simulations, judged by Debatrix (argument, source, language, and overall). Compared with strong LLM baselines, R-Debater achieves higher single-turn and multi-turn scores. Human evaluation with 20 experienced debaters further confirms its consistency and evidence use, showing that combining retrieval grounding with structured planning yields more faithful, stance-aligned, and coherent debates across turns.",
        "url": "http://arxiv.org/abs/2512.24684v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24684v1",
        "arxiv_id": "2512.24684v1",
        "authors": [
            "Maoyuan Li",
            "Zhongsheng Wang",
            "Haoyuan Li",
            "Jiamou Liu"
        ],
        "submitted": "2025-12-31 07:33:12",
        "source": "arxiv",
        "comment": "Accepteed by AAMAS 2026 full paper"
    },
    {
        "title": "Do Large Language Models Know What They Are Capable Of?",
        "abstract": "We investigate whether large language models (LLMs) can predict whether they will succeed on a given task and whether their predictions improve as they progress through multi-step tasks. We also investigate whether LLMs can learn from in-context experiences to make better decisions about whether to pursue a task in scenarios where failure is costly. All LLMs we tested are overconfident, but most predict their success with better-than-random discriminatory power. We find that newer and larger LLMs generally do not have greater discriminatory power, though Claude models do show such a trend. On multi-step agentic tasks, the overconfidence of several frontier LLMs worsens as they progress through the tasks, and reasoning LLMs perform comparably to or worse than non-reasoning LLMs. With in-context experiences of failure, some but not all LLMs reduce their overconfidence leading to significantly improved decision making, while others do not. Interestingly, all LLMs' decisions are approximately rational given their estimated probabilities of success, yet their overly-optimistic estimates result in poor decision making. These results suggest that current LLM agents are hindered by their lack of awareness of their own capabilities. We discuss the implications of LLMs' awareness of their capabilities for AI misuse and misalignment risks.",
        "url": "http://arxiv.org/abs/2512.24661v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24661v1",
        "arxiv_id": "2512.24661v1",
        "authors": [
            "Casey O. Barkan",
            "Sid Black",
            "Oliver Sourbut"
        ],
        "submitted": "2025-12-31 06:14:46",
        "source": "arxiv",
        "comment": "23 pages, 8 figures"
    },
    {
        "title": "Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models",
        "abstract": "We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled \"Commonsense-STEM-Agent\" Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.",
        "url": "http://arxiv.org/abs/2512.24618v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24618v1",
        "arxiv_id": "2512.24618v1",
        "authors": [
            "Junru Lu",
            "Jiarui Qin",
            "Lingfeng Qiao",
            "Yinghui Li",
            "Xinyi Dai",
            "Bo Ke",
            "Jianfeng He",
            "Ruizhi Qiao",
            "Di Yin",
            "Xing Sun",
            "Yunsheng Wu",
            "Yinsong Liu",
            "Shuangyin Liu",
            "Mingkong Tang",
            "Haodong Lin",
            "Jiayi Kuang",
            "Fanxu Meng",
            "Xiaojuan Tang",
            "Yunjia Xi",
            "Junjie Huang",
            "Haotong Yang",
            "Zhenyi Shen",
            "Yangning Li",
            "Qianwen Zhang",
            "Yifei Yu",
            "Siyu An",
            "Junnan Dong",
            "Qiufeng Wang",
            "Jie Wang",
            "Keyu Chen",
            "Wei Wen",
            "Taian Guo",
            "Zhifeng Shen",
            "Daohai Yu",
            "Jiahao Li",
            "Ke Li",
            "Zongyi Li",
            "Xiaoyu Tan"
        ],
        "submitted": "2025-12-31 04:25:11",
        "source": "arxiv",
        "comment": "57 pages, 26 figures"
    },
    {
        "title": "Recursive Language Models",
        "abstract": "We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.",
        "url": "http://arxiv.org/abs/2512.24601v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24601v1",
        "arxiv_id": "2512.24601v1",
        "authors": [
            "Alex L. Zhang",
            "Tim Kraska",
            "Omar Khattab"
        ],
        "submitted": "2025-12-31 03:43:41",
        "source": "arxiv",
        "comment": "9 pages, 33 with Appendix"
    },
    {
        "title": "Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time",
        "abstract": "Large Language Models (LLMs) often rely on long chain-of-thought (CoT) reasoning to solve complex tasks. While effective, these trajectories are frequently inefficient, leading to high latency from excessive token generation, or unstable reasoning that alternates between underthinking (shallow, inconsistent steps) and overthinking (repetitive, verbose reasoning). In this work, we study the structure of reasoning trajectories and uncover specialized attention heads that correlate with distinct cognitive behaviors such as verification and backtracking. By lightly intervening on these heads at inference time, we can steer the model away from inefficient modes. Building on this insight, we propose CREST, a training-free method for Cognitive REasoning Steering at Test-time. CREST has two components: (1) an offline calibration step that identifies cognitive heads and derives head-specific steering vectors, and (2) an inference-time procedure that rotates hidden representations to suppress components along those vectors. CREST adaptively suppresses unproductive reasoning behaviors, yielding both higher accuracy and lower computational cost. Across diverse reasoning benchmarks and models, CREST improves accuracy by up to 17.5% while reducing token usage by 37.6%, offering a simple and effective pathway to faster, more reliable LLM reasoning.",
        "url": "http://arxiv.org/abs/2512.24574v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24574v1",
        "arxiv_id": "2512.24574v1",
        "authors": [
            "Zhenyu Zhang",
            "Xiaoxia Wu",
            "Zhongzhu Zhou",
            "Qingyang Wu",
            "Yineng Zhang",
            "Pragaash Ponnusamy",
            "Harikaran Subbaraj",
            "Jue Wang",
            "Shuaiwen Leon Song",
            "Ben Athiwaratkun"
        ],
        "submitted": "2025-12-31 02:46:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Korean Canonical Legal Benchmark: Toward Knowledge-Independent Evaluation of LLMs' Legal Reasoning Capabilities",
        "abstract": "We introduce the Korean Canonical Legal Benchmark (KCL), a benchmark designed to assess language models' legal reasoning capabilities independently of domain-specific knowledge. KCL provides question-level supporting precedents, enabling a more faithful disentanglement of reasoning ability from parameterized knowledge. KCL consists of two components: (1) KCL-MCQA, multiple-choice problems of 283 questions with 1,103 aligned precedents, and (2) KCL-Essay, open-ended generation problems of 169 questions with 550 aligned precedents and 2,739 instance-level rubrics for automated evaluation. Our systematic evaluation of 30+ models shows large remaining gaps, particularly in KCL-Essay, and that reasoning-specialized models consistently outperform their general-purpose counterparts. We release all resources, including the benchmark dataset and evaluation code, at https://github.com/lbox-kr/kcl.",
        "url": "http://arxiv.org/abs/2512.24572v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24572v1",
        "arxiv_id": "2512.24572v1",
        "authors": [
            "Hongseok Oh",
            "Wonseok Hwang",
            "Kyoung-Woon On"
        ],
        "submitted": "2025-12-31 02:35:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "HaluNet: Multi-Granular Uncertainty Modeling for Efficient Hallucination Detection in LLM Question Answering",
        "abstract": "Large Language Models (LLMs) excel at question answering (QA) but often generate hallucinations, including factual errors or fabricated content. Detecting hallucinations from internal uncertainty signals is attractive due to its scalability and independence from external resources. Existing methods often aim to accurately capture a single type of uncertainty while overlooking the complementarity among different sources, particularly between token-level probability uncertainty and the uncertainty conveyed by internal semantic representations, which provide complementary views on model reliability. We present \\textbf{HaluNet}, a lightweight and trainable neural framework that integrates multi granular token level uncertainties by combining semantic embeddings with probabilistic confidence and distributional uncertainty. Its multi branch architecture adaptively fuses what the model knows with the uncertainty expressed in its outputs, enabling efficient one pass hallucination detection. Experiments on SQuAD, TriviaQA, and Natural Questions show that HaluNet delivers strong detection performance and favorable computational efficiency, with or without access to context, highlighting its potential for real time hallucination detection in LLM based QA systems.",
        "url": "http://arxiv.org/abs/2512.24562v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24562v1",
        "arxiv_id": "2512.24562v1",
        "authors": [
            "Chaodong Tong",
            "Qi Zhang",
            "Jiayang Gao",
            "Lei Jiang",
            "Yanbing Liu",
            "Nannan Sun"
        ],
        "submitted": "2025-12-31 02:03:10",
        "source": "arxiv",
        "comment": "13 pages, 5 figures"
    },
    {
        "title": "Safe in the Future, Dangerous in the Past: Dissecting Temporal and Linguistic Vulnerabilities in LLMs",
        "abstract": "As Large Language Models (LLMs) integrate into critical global infrastructure, the assumption that safety alignment transfers zero-shot from English to other languages remains a dangerous blind spot. This study presents a systematic audit of three state of the art models (GPT-5.1, Gemini 3 Pro, and Claude 4.5 Opus) using HausaSafety, a novel adversarial dataset grounded in West African threat scenarios (e.g., Yahoo-Yahoo fraud, Dane gun manufacturing). Employing a 2 x 4 factorial design across 1,440 evaluations, we tested the non-linear interaction between language (English vs. Hausa) and temporal framing. Our results challenge the prevailing multilingual safety gap narrative. Instead of a simple degradation in low-resource settings, we identified a mechanism of Complex Interference where safety is determined by the intersection of variables. While models exhibited a Reverse Linguistic with Claude 4.5 Opus proving significantly safer in Hausa (45.0%) than in English (36.7%) due to uncertainty-driven refusal they suffered catastrophic failures in temporal reasoning. We report a profound Temporal Asymmetry, where past-tense framing bypassed defenses (15.6% safe) while future-tense scenarios triggered hyper-conservative refusals (57.2% safe). The magnitude of this volatility is illustrated by a 9.2x disparity between the safest and most vulnerable configurations, proving that safety is not a fixed property but a context-dependent state. We conclude that current models rely on superficial heuristics rather than robust semantic understanding, creating Safety Pockets that leave Global South users exposed to localized harms. We propose Invariant Alignment as a necessary paradigm shift to ensure safety stability across linguistic and temporal shifts.",
        "url": "http://arxiv.org/abs/2512.24556v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24556v1",
        "arxiv_id": "2512.24556v1",
        "authors": [
            "Muhammad Abdullahi Said",
            "Muhammad Sammani Sani"
        ],
        "submitted": "2025-12-31 01:40:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "More Than Bits: Multi-Envelope Double Binary Factorization for Extreme Quantization",
        "abstract": "For extreme low-bit quantization of large language models (LLMs), Double Binary Factorization (DBF) is attractive as it enables efficient inference without sacrificing accuracy. However, the scaling parameters of DBF are too restrictive; after factoring out signs, all rank components share the same magnitude profile, resulting in performance saturation. We propose Multi-envelope DBF (MDBF), which retains a shared pair of 1-bit sign bases but replaces the single envelope with a rank-$l$ envelope. By sharing sign matrices among envelope components, MDBF effectively maintains a binary carrier and utilizes the limited memory budget for magnitude expressiveness. We also introduce a closed-form initialization and an alternating refinement method to optimize MDBF. Across the LLaMA and Qwen families, MDBF enhances perplexity and zero-shot accuracy over previous binary formats at matched bits per weight while preserving the same deployment-friendly inference primitive.",
        "url": "http://arxiv.org/abs/2512.24545v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24545v1",
        "arxiv_id": "2512.24545v1",
        "authors": [
            "Yuma Ichikawa",
            "Yoshihiko Fujisawa",
            "Yudai Fujimoto",
            "Akira Sakai",
            "Katsuki Fujisawa"
        ],
        "submitted": "2025-12-31 01:04:34",
        "source": "arxiv",
        "comment": "14 pages, 2 figures"
    },
    {
        "title": "From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning",
        "abstract": "Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition. First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment. Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch. Finally, we analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.",
        "url": "http://arxiv.org/abs/2512.24532v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24532v1",
        "arxiv_id": "2512.24532v1",
        "authors": [
            "Amir Tahmasbi",
            "Sadegh Majidi",
            "Kazem Taram",
            "Aniket Bera"
        ],
        "submitted": "2025-12-31 00:36:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Paragraph Segmentation Revisited: Towards a Standard Task for Structuring Speech",
        "abstract": "Automatic speech transcripts are often delivered as unstructured word streams that impede readability and repurposing. We recast paragraph segmentation as the missing structuring step and fill three gaps at the intersection of speech processing and text segmentation. First, we establish TEDPara (human-annotated TED talks) and YTSegPara (YouTube videos with synthetic labels) as the first benchmarks for the paragraph segmentation task. The benchmarks focus on the underexplored speech domain, where paragraph segmentation has traditionally not been part of post-processing, while also contributing to the wider text segmentation field, which still lacks robust and naturalistic benchmarks. Second, we propose a constrained-decoding formulation that lets large language models insert paragraph breaks while preserving the original transcript, enabling faithful, sentence-aligned evaluation. Third, we show that a compact model (MiniSeg) attains state-of-the-art accuracy and, when extended hierarchically, jointly predicts chapters and paragraphs with minimal computational cost. Together, our resources and methods establish paragraph segmentation as a standardized, practical task in speech processing.",
        "url": "http://arxiv.org/abs/2512.24517v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24517v1",
        "arxiv_id": "2512.24517v1",
        "authors": [
            "Fabian Retkowski",
            "Alexander Waibel"
        ],
        "submitted": "2025-12-30 23:29:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "IELTS Writing Revision Platform with Automated Essay Scoring and Adaptive Feedback",
        "abstract": "This paper presents the design, development, and evaluation of a proposed revision platform assisting candidates for the International English Language Testing System (IELTS) writing exam. Traditional IELTS preparation methods lack personalised feedback, catered to the IELTS writing rubric. To address these shortcomings, the platform features an attractive user interface (UI), an Automated Essay Scoring system (AES), and targeted feedback tailored to candidates and the IELTS writing rubric. The platform architecture separates conversational guidance from a dedicated writing interface to reduce cognitive load and simulate exam conditions. Through iterative, Design-Based Research (DBR) cycles, the study progressed from rule-based to transformer-based with a regression head scoring, mounted with adaptive feedback.\n  Early cycles (2-3) revealed fundamental limitations of rule-based approaches: mid-band compression, low accuracy, and negative $R^2$ values. DBR Cycle 4 implemented a DistilBERT transformer model with a regression head, yielding substantial improvements with MAE of 0.66 and positive $R^2$. This enabled Cycle 5's adaptive feedback implementation, which demonstrated statistically significant score improvements (mean +0.060 bands, p = 0.011, Cohen's d = 0.504), though effectiveness varied by revision strategy. Findings suggest automated feedback functions are most suited as a supplement to human instruction, with conservative surface-level corrections proving more reliable than aggressive structural interventions for IELTS preparation contexts. Challenges remain in assessing higher-band essays, and future work should incorporate longitudinal studies with real IELTS candidates and validation from official examiners.",
        "url": "http://arxiv.org/abs/2512.24460v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24460v1",
        "arxiv_id": "2512.24460v1",
        "authors": [
            "Titas Ramancauskas",
            "Kotryna Ramancauske"
        ],
        "submitted": "2025-12-30 20:49:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Cleaning English Abstracts of Scientific Publications",
        "abstract": "Scientific abstracts are often used as proxies for the content and thematic focus of research publications. However, a significant share of published abstracts contains extraneous information-such as publisher copyright statements, section headings, author notes, registrations, and bibliometric or bibliographic metadata-that can distort downstream analyses, particularly those involving document similarity or textual embeddings. We introduce an open-source, easy-to-integrate language model designed to clean English-language scientific abstracts by automatically identifying and removing such clutter. We demonstrate that our model is both conservative and precise, alters similarity rankings of cleaned abstracts and improves information content of standard-length embeddings.",
        "url": "http://arxiv.org/abs/2512.24459v1",
        "pdf_url": "https://arxiv.org/pdf/2512.24459v1",
        "arxiv_id": "2512.24459v1",
        "authors": [
            "Michael E. Rose",
            "Nils A. Herrmann",
            "Sebastian Erhardt"
        ],
        "submitted": "2025-12-30 20:45:50",
        "source": "arxiv",
        "comment": "2 tables, 2 figures"
    }
]