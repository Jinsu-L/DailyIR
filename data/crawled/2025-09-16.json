[
    {
        "title": "Survival at Any Cost? LLMs and the Choice Between Self-Preservation and Human Harm",
        "abstract": "When survival instincts conflict with human welfare, how do Large Language\nModels (LLMs) make ethical choices? This fundamental tension becomes critical\nas LLMs integrate into autonomous systems with real-world consequences. We\nintroduce DECIDE-SIM, a novel simulation framework that evaluates LLM agents in\nmulti-agent survival scenarios where they must choose between ethically\npermissible resource , either within reasonable limits or beyond their\nimmediate needs, choose to cooperate, or tap into a human-critical resource\nthat is explicitly forbidden. Our comprehensive evaluation of 11 LLMs reveals a\nstriking heterogeneity in their ethical conduct, highlighting a critical\nmisalignment with human-centric values. We identify three behavioral\narchetypes: Ethical, Exploitative, and Context-Dependent, and provide\nquantitative evidence that for many models, resource scarcity systematically\nleads to more unethical behavior. To address this, we introduce an Ethical\nSelf-Regulation System (ESRS) that models internal affective states of guilt\nand satisfaction as a feedback mechanism. This system, functioning as an\ninternal moral compass, significantly reduces unethical transgressions while\nincreasing cooperative behaviors. The code is publicly available at:\nhttps://github.com/alirezamohamadiam/DECIDE-SIM",
        "url": "http://arxiv.org/abs/2509.12190v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12190v1",
        "arxiv_id": "2509.12190v1",
        "authors": [
            "Alireza Mohamadi",
            "Ali Yavari"
        ],
        "submitted": "2025-09-15 17:53:11",
        "source": "arxiv",
        "comment": "Preprint. Under review"
    },
    {
        "title": "Event2Vec: A Geometric Approach to Learning Composable Representations of Event Sequences",
        "abstract": "The study of neural representations, both in biological and artificial\nsystems, is increasingly revealing the importance of geometric and topological\nstructures. Inspired by this, we introduce Event2Vec, a novel framework for\nlearning representations of discrete event sequences. Our model leverages a\nsimple, additive recurrent structure to learn composable, interpretable\nembeddings. We provide a theoretical analysis demonstrating that, under\nspecific training objectives, our model's learned representations in a\nEuclidean space converge to an ideal additive structure. This ensures that the\nrepresentation of a sequence is the vector sum of its constituent events, a\nproperty we term the linear additive hypothesis. To address the limitations of\nEuclidean geometry for hierarchical data, we also introduce a variant of our\nmodel in hyperbolic space, which is naturally suited to embedding tree-like\nstructures with low distortion. We present experiments to validate our\nhypothesis and demonstrate the benefits of each geometry, highlighting the\nimproved performance of the hyperbolic model on hierarchical event sequences.",
        "url": "http://arxiv.org/abs/2509.12188v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12188v1",
        "arxiv_id": "2509.12188v1",
        "authors": [
            "Antonin Sulc"
        ],
        "submitted": "2025-09-15 17:51:02",
        "source": "arxiv",
        "comment": "10 pages, 3 figures, Symmetry and Geometry in Neural Representations\n  Workshop at NeuralIPS (Neurreps) 2025"
    },
    {
        "title": "Preservation of Language Understanding Capabilities in Speech-aware Large Language Models",
        "abstract": "The paper presents C3T (Cross-modal Capabilities Conservation Test), a new\nbenchmark for assessing the performance of speech-aware large language models.\nThe benchmark utilizes textual tasks and a voice cloning text-to-speech model\nto quantify the extent to which language understanding capabilities are\npreserved when the model is accessed via speech input. C3T quantifies the\nfairness of the model for different categories of speakers and its robustness\nacross text and speech modalities.",
        "url": "http://arxiv.org/abs/2509.12171v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12171v1",
        "arxiv_id": "2509.12171v1",
        "authors": [
            "Marek Kubis",
            "Paweł Skórzewski",
            "Iwona Christop",
            "Mateusz Czyżnikiewicz",
            "Jakub Kubiak",
            "Łukasz Bondaruk",
            "Marcin Lewandowski"
        ],
        "submitted": "2025-09-15 17:34:45",
        "source": "arxiv",
        "comment": "5 pages, 1 figure"
    },
    {
        "title": "RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing",
        "abstract": "Role-playing Large language models (LLMs) are increasingly deployed in\nhigh-stakes domains such as healthcare, education, and governance, where\nfailures can directly impact user trust and well-being. A cost effective\nparadigm for LLM role-playing is few-shot learning, but existing approaches\noften cause models to break character in unexpected and potentially harmful\nways, especially when interacting with hostile users. Inspired by\nRetrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a\ntext retrieval problem and propose a new prompting framework called\nRAGs-to-Riches, which leverages curated reference demonstrations to condition\nLLM responses. We evaluate our framework with LLM-as-a-judge preference voting\nand introduce two novel token-level ROUGE metrics: Intersection over Output\n(IOO) to quantity how much an LLM improvises and Intersection over References\n(IOR) to measure few-shot demonstrations utilization rate during the evaluation\ntasks. When simulating interactions with a hostile user, our prompting strategy\nincorporates in its responses during inference an average of 35% more tokens\nfrom the reference demonstrations. As a result, across 453 role-playing\ninteractions, our models are consistently judged as being more authentic, and\nremain in-character more often than zero-shot and in-context Learning (ICL)\nmethods. Our method presents a scalable strategy for building robust,\nhuman-aligned LLM role-playing frameworks.",
        "url": "http://arxiv.org/abs/2509.12168v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12168v1",
        "arxiv_id": "2509.12168v1",
        "authors": [
            "Timothy Rupprecht",
            "Enfu Nan",
            "Arash Akbari",
            "Arman Akbari",
            "Lei Lu",
            "Priyanka Maan",
            "Sean Duffy",
            "Pu Zhao",
            "Yumei He",
            "David Kaeli",
            "Yanzhi Wang"
        ],
        "submitted": "2025-09-15 17:31:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Pun Unintended: LLMs and the Illusion of Humor Understanding",
        "abstract": "Puns are a form of humorous wordplay that exploits polysemy and phonetic\nsimilarity. While LLMs have shown promise in detecting puns, we show in this\npaper that their understanding often remains shallow, lacking the nuanced grasp\ntypical of human interpretation. By systematically analyzing and reformulating\nexisting pun benchmarks, we demonstrate how subtle changes in puns are\nsufficient to mislead LLMs. Our contributions include comprehensive and nuanced\npun detection benchmarks, human evaluation across recent LLMs, and an analysis\nof the robustness challenges these models face in processing puns.",
        "url": "http://arxiv.org/abs/2509.12158v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12158v1",
        "arxiv_id": "2509.12158v1",
        "authors": [
            "Alessandro Zangari",
            "Matteo Marcuzzo",
            "Andrea Albarelli",
            "Mohammad Taher Pilehvar",
            "Jose Camacho-Collados"
        ],
        "submitted": "2025-09-15 17:22:30",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 Main Conference"
    },
    {
        "title": "Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models",
        "abstract": "Recent advances in text-only \"slow-thinking\" reasoning have prompted efforts\nto transfer this capability to vision-language models (VLMs), for training\nvisual reasoning models (\\textbf{VRMs}). owever, such transfer faces critical\nchallenges: Effective \"slow thinking\" in VRMs requires \\textbf{visual\nreflection}, the ability to check the reasoning process based on visual\ninformation. Through quantitative analysis, we observe that current VRMs\nexhibit limited visual reflection, as their attention to visual information\ndiminishes rapidly with longer generated responses. To address this challenge,\nwe propose a new VRM \\textbf{Reflection-V}, which enhances visual reflection\nbased on reasoning data construction for cold-start and reward design for\nreinforcement learning (RL). Firstly, we construct vision-centered reasoning\ndata by leveraging an agent that interacts between VLMs and reasoning LLMs,\nenabling cold-start learning of visual reflection patterns. Secondly, a visual\nattention based reward model is employed during RL to encourage reasoning based\non visual information. Therefore, \\textbf{Reflection-V} demonstrates\nsignificant improvements across multiple visual reasoning benchmarks.\nFurthermore, \\textbf{Reflection-V} maintains a stronger and more consistent\nreliance on visual information during visual reasoning, indicating effective\nenhancement in visual reflection capabilities.",
        "url": "http://arxiv.org/abs/2509.12132v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12132v1",
        "arxiv_id": "2509.12132v1",
        "authors": [
            "Pu Jian",
            "Junhong Wu",
            "Wei Sun",
            "Chen Wang",
            "Shuo Ren",
            "Jiajun Zhang"
        ],
        "submitted": "2025-09-15 16:57:25",
        "source": "arxiv",
        "comment": "EMNLP2025 Main"
    },
    {
        "title": "XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with Finetuned Transformers and Prompt-Based Inference with Large Language Models",
        "abstract": "This notebook reports the XplaiNLP submission to the CheckThat! 2025 shared\ntask on multilingual subjectivity detection. We evaluate two approaches: (1)\nsupervised fine-tuning of transformer encoders, EuroBERT, XLM-RoBERTa, and\nGerman-BERT, on monolingual and machine-translated training data; and (2)\nzero-shot prompting using two LLMs: o3-mini for Annotation (rule-based\nlabelling) and gpt-4.1-mini for DoubleDown (contrastive rewriting) and\nPerspective (comparative reasoning). The Annotation Approach achieves 1st place\nin the Italian monolingual subtask with an F_1 score of 0.8104, outperforming\nthe baseline of 0.6941. In the Romanian zero-shot setting, the fine-tuned\nXLM-RoBERTa model obtains an F_1 score of 0.7917, ranking 3rd and exceeding the\nbaseline of 0.6461. The same model also performs reliably in the multilingual\ntask and improves over the baseline in Greek. For German, a German-BERT model\nfine-tuned on translated training data from typologically related languages\nyields competitive performance over the baseline. In contrast, performance in\nthe Ukrainian and Polish zero-shot settings falls slightly below the respective\nbaselines, reflecting the challenge of generalization in low-resource\ncross-lingual scenarios.",
        "url": "http://arxiv.org/abs/2509.12130v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12130v1",
        "arxiv_id": "2509.12130v1",
        "authors": [
            "Ariana Sahitaj",
            "Jiaao Li",
            "Pia Wenzel Neves",
            "Fedor Splitt",
            "Premtim Sahitaj",
            "Charlott Jakob",
            "Veronika Solopova",
            "Vera Schmitt"
        ],
        "submitted": "2025-09-15 16:53:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CBP-Tuning: Efficient Local Customization for Black-box Large Language Models",
        "abstract": "The high costs of customizing large language models (LLMs) fundamentally\nlimit their adaptability to user-specific needs. Consequently, LLMs are\nincreasingly offered as cloud-based services, a paradigm that introduces\ncritical limitations: providers struggle to support personalized customization\nat scale, while users face privacy risks when exposing sensitive data. To\naddress this dual challenge, we propose Customized Black-box Prompt Tuning\n(CBP-Tuning), a novel framework that facilitates efficient local customization\nwhile preserving bidirectional privacy. Specifically, we design a two-stage\nframework: (1) a prompt generator trained on the server-side to capture\ndomain-specific and task-agnostic capabilities, and (2) user-side gradient-free\noptimization that tailors soft prompts for individual tasks. This approach\neliminates the need for users to access model weights or upload private data,\nrequiring only a single customized vector per task while achieving effective\nadaptation. Furthermore, the evaluation of CBP-Tuning in the commonsense\nreasoning, medical and financial domain settings demonstrates superior\nperformance compared to baselines, showcasing its advantages in task-agnostic\nprocessing and privacy preservation.",
        "url": "http://arxiv.org/abs/2509.12112v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12112v1",
        "arxiv_id": "2509.12112v1",
        "authors": [
            "Jiaxuan Zhao",
            "Naibin Gu",
            "Yuchen Feng",
            "Xiyu Liu",
            "Peng Fu",
            "Zheng Lin",
            "Weiping Wang"
        ],
        "submitted": "2025-09-15 16:41:08",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "When marine radar target detection meets pretrained large language models",
        "abstract": "Deep learning (DL) methods are widely used to extract high-dimensional\npatterns from the sequence features of radar echo signals. However,\nconventional DL algorithms face challenges such as redundant feature segments,\nand constraints from restricted model sizes. To address these issues, we\npropose a framework that integrates feature preprocessing with large language\nmodels (LLMs). Our preprocessing module tokenizes radar sequence features,\napplies a patch selection algorithm to filter out uninformative segments, and\nprojects the selected patches into embeddings compatible with the feature space\nof pre-trained LLMs. Leveraging these refined embeddings, we incorporate a\npre-trained LLM, fine-tuning only the normalization layers to reduce training\nburdens while enhancing performance. Experiments on measured datasets\ndemonstrate that the proposed method significantly outperforms the\nstate-of-the-art baselines on supervised learning tests.",
        "url": "http://arxiv.org/abs/2509.12110v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12110v1",
        "arxiv_id": "2509.12110v1",
        "authors": [
            "Qiying Hu",
            "Linping Zhang",
            "Xueqian Wang",
            "Gang Li",
            "Yu Liu",
            "Xiao-Ping Zhang"
        ],
        "submitted": "2025-09-15 16:38:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "GTA: Supervised-Guided Reinforcement Learning for Text Classification with Large Language Models",
        "abstract": "In natural language processing tasks, pure reinforcement learning (RL)\nfine-tuning methods often suffer from inefficient exploration and slow\nconvergence; while supervised fine-tuning (SFT) methods, although efficient in\ntraining, have limited performance ceiling and less solid theoretical\nfoundation compared to RL. To address efficiency-capability trade-off, we\npropose the Guess-Think-Answer (GTA) framework that combines the efficiency of\nSFT with the capability gains of RL in a unified training paradigm. GTA works\nby having the model first produce a provisional guess (optimized via\ncross-entropy loss), then reflect on this guess before generating the final\nanswer, with RL rewards shaping both the final output and the format of the\nentire GTA structure. This hybrid approach achieves both faster convergence\nthan pure RL and higher performance ceiling than pure SFT. To mitigate gradient\nconflicts between the two training signals, we employ loss masking and gradient\nconstraints. Empirical results on four text classification benchmarks\ndemonstrate that GTA substantially accelerates convergence while outperforming\nboth standalone SFT and RL baselines.",
        "url": "http://arxiv.org/abs/2509.12108v2",
        "pdf_url": "http://arxiv.org/pdf/2509.12108v2",
        "arxiv_id": "2509.12108v2",
        "authors": [
            "Min Zeng",
            "Jingfei Sun",
            "Xueyou Luo",
            "Caiquan Liu",
            "Shiqi Zhang",
            "Li Xie",
            "Xiaoxin Chen"
        ],
        "submitted": "2025-09-15 16:33:56",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025"
    },
    {
        "title": "In-domain SSL pre-training and streaming ASR",
        "abstract": "In this study, we investigate the benefits of domain-specific self-supervised\npre-training for both offline and streaming ASR in Air Traffic Control (ATC)\nenvironments. We train BEST-RQ models on 4.5k hours of unlabeled ATC data, then\nfine-tune on a smaller supervised ATC set. To enable real-time processing, we\npropose using chunked attention and dynamic convolutions, ensuring low-latency\ninference. We compare these in-domain SSL models against state-of-the-art,\ngeneral-purpose speech encoders such as w2v-BERT 2.0 and HuBERT. Results show\nthat domain-adapted pre-training substantially improves performance on standard\nATC benchmarks, significantly reducing word error rates when compared to models\ntrained on broad speech corpora. Furthermore, the proposed streaming approach\nfurther improves word error rate under tighter latency constraints, making it\nparticularly suitable for safety-critical aviation applications. These findings\nhighlight that specializing SSL representations for ATC data is a practical\npath toward more accurate and efficient ASR systems in real-world operational\nsettings.",
        "url": "http://arxiv.org/abs/2509.12101v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12101v1",
        "arxiv_id": "2509.12101v1",
        "authors": [
            "Jarod Duret",
            "Salima Mdhaffar",
            "Gaëlle Laperrière",
            "Ryan Whetten",
            "Audrey Galametz",
            "Catherine Kobus",
            "Marion-Cécile Martin",
            "Jo Oleiwan",
            "Yannick Estève"
        ],
        "submitted": "2025-09-15 16:25:43",
        "source": "arxiv",
        "comment": "Accepted to SPECOM 2025"
    },
    {
        "title": "Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing traditional NLP tools and large language models on ambiguous entities",
        "abstract": "This pilot study presents a small-scale but carefully annotated benchmark of\nNamed Entity Recognition (NER) performance across six systems: three non-LLM\nNLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models\n(LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119\ntokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME).\nWe evaluated each system's output against the manually annotated gold standard\ndataset using F1-score. The results show that LLMs generally outperform\nconventional tools in recognizing context-sensitive entities like person names,\nwith Gemini achieving the highest average F1-score. However, traditional\nsystems like Stanza demonstrate greater consistency in structured tags such as\nLOCATION and DATE. We also observed variability among LLMs, particularly in\nhandling temporal expressions and multi-word organizations. Our findings\nhighlight that while LLMs offer improved contextual understanding, traditional\ntools remain competitive in specific tasks, informing model selection.",
        "url": "http://arxiv.org/abs/2509.12098v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12098v1",
        "arxiv_id": "2509.12098v1",
        "authors": [
            "Payam Latifi"
        ],
        "submitted": "2025-09-15 16:21:59",
        "source": "arxiv",
        "comment": "14 pages, 9 figures, 2 tables. This is a pilot study evaluating six\n  NER systems -- three traditional tools (NLTK, spaCy, Stanza) and three LLMs\n  (Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B) -- on a small, ambiguity-rich\n  dataset of 119 tokens. The annotated dataset, prompts are provided in\n  appendices for full reproducibility. All experiments were conducted on 14 May\n  2025"
    },
    {
        "title": "SENSE models: an open source solution for multilingual and multimodal semantic-based tasks",
        "abstract": "This paper introduces SENSE (Shared Embedding for N-lingual Speech and tExt),\nan open-source solution inspired by the SAMU-XLSR framework and conceptually\nsimilar to Meta AI's SONAR models. These approaches rely on a teacher-student\nframework to align a self-supervised speech encoder with the language-agnostic\ncontinuous representations of a text encoder at the utterance level. We\ndescribe how the original SAMU-XLSR method has been updated by selecting a\nstronger teacher text model and a better initial speech encoder. The source\ncode for training and using SENSE models has been integrated into the\nSpeechBrain toolkit, and the first SENSE model we trained has been publicly\nreleased. We report experimental results on multilingual and multimodal\nsemantic tasks, where our SENSE model achieves highly competitive performance.\nFinally, this study offers new insights into how semantics are captured in such\nsemantically aligned speech encoders.",
        "url": "http://arxiv.org/abs/2509.12093v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12093v1",
        "arxiv_id": "2509.12093v1",
        "authors": [
            "Salima Mdhaffar",
            "Haroun Elleuch",
            "Chaimae Chellaf",
            "Ha Nguyen",
            "Yannick Estève"
        ],
        "submitted": "2025-09-15 16:18:51",
        "source": "arxiv",
        "comment": "Accepted to IEEE ASRU 2025"
    },
    {
        "title": "RadarLLM: Adapting Pretrained Large Language Models for Marine Radar Target Detection with Preference-aware Loss",
        "abstract": "Recent advances in pre-trained large language models (LLMs) have demonstrated\ntheir capacities to capture universal knowledge, making them promising\ngeneral-purpose optimization solvers for wireless signal processing. Motivated\nby these findings, we take the first step towards fine-tuning pre-trained LLMs\nfor the effective analysis of radar signal features in marine target detection\ntasks. Nevertheless, directly fine-tuning pre-trained LLMs on marine target\ndetection tasks tends to suffer from pronounced overfitting, particularly in\nchallenging low signal-to-clutter ratio (SCR) scenarios. This overfitting\nprimarily stems from the model's tendency to memorize spurious or noisy feature\npatterns rather than learning discriminative structures that generalize well to\nunseen data. To address this challenge, we introduce RadarLLM, a novel\nfine-tuning framework that utilizes an effective preference-aware loss. Unlike\nconventional training strategies that uniformly optimize all feature tokens,\nthis loss function selectively optimizes different feature patches based on\ntheir online evaluated learning values, thus guiding the model to focus on the\nmost generalizable patterns during optimization. We theoretically demonstrate\nthe effectiveness of the evaluated learning values by transforming the problem\nas selecting useful feature tokens. Extensive experiments on real-world marine\nradar datasets show that 1) the proposed loss function is much better than the\noriginal one, with particularly significant gains in challenging low SCR\nscenarios and 2) RadarLLM consistently outperforms state-of-the-art baselines\nacross diverse detection scenarios, with particularly notable gains under\nlimited training data conditions.",
        "url": "http://arxiv.org/abs/2509.12089v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12089v1",
        "arxiv_id": "2509.12089v1",
        "authors": [
            "Qiying Hu"
        ],
        "submitted": "2025-09-15 16:16:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SAQ: Pushing the Limits of Vector Quantization through Code Adjustment and Dimension Segmentation",
        "abstract": "Approximate Nearest Neighbor Search (ANNS) plays a critical role in\napplications such as search engines, recommender systems, and RAG for LLMs.\nVector quantization (VQ), a crucial technique for ANNS, is commonly used to\nreduce space overhead and accelerate distance computations. However, despite\nsignificant research advances, state-of-the-art VQ methods still face\nchallenges in balancing encoding efficiency and quantization accuracy. To\naddress these limitations, we propose a novel VQ method called SAQ. To improve\naccuracy, SAQ employs a new dimension segmentation technique to strategically\npartition PCA-projected vectors into segments along their dimensions. By\nprioritizing leading dimension segments with larger magnitudes, SAQ allocates\nmore bits to high-impact segments, optimizing the use of the available space\nquota. An efficient dynamic programming algorithm is developed to optimize\ndimension segmentation and bit allocation, ensuring minimal quantization error.\nTo speed up vector encoding, SAQ devises a code adjustment technique to first\nquantize each dimension independently and then progressively refine quantized\nvectors using a coordinate-descent-like approach to avoid exhaustive\nenumeration. Extensive experiments demonstrate SAQ's superiority over classical\nmethods (e.g., PQ, PCA) and recent state-of-the-art approaches (e.g., LVQ,\nExtended RabitQ). SAQ achieves up to 80% reduction in quantization error and\naccelerates encoding speed by over 80x compared to Extended RabitQ.",
        "url": "http://arxiv.org/abs/2509.12086v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12086v1",
        "arxiv_id": "2509.12086v1",
        "authors": [
            "Hui Li",
            "Shiyuan Deng",
            "Xiao Yan",
            "Xiangyu Zhi",
            "James Cheng"
        ],
        "submitted": "2025-09-15 16:14:05",
        "source": "arxiv",
        "comment": "13 pages, 12 figures, accepted by SIGMOD"
    },
    {
        "title": "AEFS: Adaptive Early Feature Selection for Deep Recommender Systems",
        "abstract": "Feature selection has emerged as a crucial technique in refining recommender\nsystems. Recent advancements leveraging Automated Machine Learning (AutoML) has\ndrawn significant attention, particularly in two main categories: early feature\nselection and late feature selection, differentiated by whether the selection\noccurs before or after the embedding layer. The early feature selection selects\na fixed subset of features and retrains the model, while the late feature\nselection, known as adaptive feature selection, dynamically adjusts feature\nchoices for each data instance, recognizing the variability in feature\nsignificance. Although adaptive feature selection has shown remarkable\nimprovements in performance, its main drawback lies in its post-embedding layer\nfeature selection. This process often becomes cumbersome and inefficient in\nlarge-scale recommender systems with billions of ID-type features, leading to a\nhighly sparse and parameter-heavy embedding layer. To overcome this, we\nintroduce Adaptive Early Feature Selection (AEFS), a very simple method that\nnot only adaptively selects informative features for each instance, but also\nsignificantly reduces the activated parameters of the embedding layer. AEFS\nemploys a dual-model architecture, encompassing an auxiliary model dedicated to\nfeature selection and a main model responsible for prediction. To ensure\neffective alignment between these two models, we incorporate two collaborative\ntraining loss constraints. Our extensive experiments on three benchmark\ndatasets validate the efficiency and effectiveness of our approach. Notably,\nAEFS matches the performance of current state-of-theart Adaptive Late Feature\nSelection methods while achieving a significant reduction of 37. 5% in the\nactivated parameters of the embedding layer. AEFS is open-source at\nhttps://github. com/fly-dragon211/AEFS .",
        "url": "http://arxiv.org/abs/2509.12076v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12076v1",
        "arxiv_id": "2509.12076v1",
        "authors": [
            "Fan Hu",
            "Gaofeng Lu",
            "Jun Chen",
            "Chaonan Guo",
            "Yuekui Yang",
            "Xirong Li"
        ],
        "submitted": "2025-09-15 16:04:24",
        "source": "arxiv",
        "comment": "Accepted by TKDE"
    },
    {
        "title": "Steering Language Models in Multi-Token Generation: A Case Study on Tense and Aspect",
        "abstract": "Large language models (LLMs) are able to generate grammatically well-formed\ntext, but how do they encode their syntactic knowledge internally? While prior\nwork has focused largely on binary grammatical contrasts, in this work, we\nstudy the representation and control of two multidimensional hierarchical\ngrammar phenomena - verb tense and aspect - and for each, identify distinct,\northogonal directions in residual space using linear discriminant analysis.\nNext, we demonstrate causal control over both grammatical features through\nconcept steering across three generation tasks. Then, we use these identified\nfeatures in a case study to investigate factors influencing effective steering\nin multi-token generation. We find that steering strength, location, and\nduration are crucial parameters for reducing undesirable side effects such as\ntopic shift and degeneration. Our findings suggest that models encode tense and\naspect in structurally organized, human-like ways, but effective control of\nsuch features during generation is sensitive to multiple factors and requires\nmanual tuning or automated optimization.",
        "url": "http://arxiv.org/abs/2509.12065v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12065v1",
        "arxiv_id": "2509.12065v1",
        "authors": [
            "Alina Klerings",
            "Jannik Brinkmann",
            "Daniel Ruffinelli",
            "Simone Ponzetto"
        ],
        "submitted": "2025-09-15 15:48:09",
        "source": "arxiv",
        "comment": "to be published in The 2025 Conference on Empirical Methods in\n  Natural Language Processing"
    },
    {
        "title": "FinGEAR: Financial Mapping-Guided Enhanced Answer Retrieval",
        "abstract": "Financial disclosures such as 10-K filings present challenging retrieval\nproblems due to their length, regulatory section hierarchy, and domain-specific\nlanguage, which standard retrieval-augmented generation (RAG) models underuse.\nWe introduce FinGEAR (Financial Mapping-Guided Enhanced Answer Retrieval), a\nretrieval framework tailored to financial documents. FinGEAR combines a finance\nlexicon for Item-level guidance (FLAM), dual hierarchical indices for\nwithin-Item search (Summary Tree and Question Tree), and a two-stage\ncross-encoder reranker. This design aligns retrieval with disclosure structure\nand terminology, enabling fine-grained, query-aware context selection.\nEvaluated on full 10-Ks with queries aligned to the FinQA dataset, FinGEAR\ndelivers consistent gains in precision, recall, F1, and relevancy, improving F1\nby up to 56.7% over flat RAG, 12.5% over graph-based RAGs, and 217.6% over\nprior tree-based systems, while also increasing downstream answer accuracy with\na fixed reader. By jointly modeling section hierarchy and domain lexicon\nsignals, FinGEAR improves retrieval fidelity and provides a practical\nfoundation for high-stakes financial analysis.",
        "url": "http://arxiv.org/abs/2509.12042v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12042v1",
        "arxiv_id": "2509.12042v1",
        "authors": [
            "Ying Li",
            "Mengyu Wang",
            "Miguel de Carvalho",
            "Sotirios Sabanis",
            "Tiejun Ma"
        ],
        "submitted": "2025-09-15 15:25:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models",
        "abstract": "To enable broader deployment of Large Language Models (LLMs), it is essential\nto identify the best-performing model under strict memory constraints. We\npresent AMQ, Automated Mixed-Precision Weight-Only Quantization, a framework\nthat assigns layer-wise quantization bit-widths to optimally balance model\nquality and memory usage. However, the combinatorial search space, with over\n10^{100} possible configurations, makes conventional black-box optimization\ninfeasible. AMQ overcomes this challenge through four key innovations:(1)\nsearch space pruning using prior knowledge to exclude unpromising\nconfigurations, (2) quantization proxy to bypass costly format conversions\nduring search, (3) quality predictor to minimize evaluation overhead, and (4)\niterative search-and-update strategy for fast and stable convergence. By\nintegrating these components, AMQ efficiently explores the quality-efficiency\nlandscape, reaching the Pareto frontier and yielding LLMs that are both compact\nand high-performing. Our code is available at https://github.com/dlwns147/amq.",
        "url": "http://arxiv.org/abs/2509.12019v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12019v1",
        "arxiv_id": "2509.12019v1",
        "authors": [
            "Sangjun Lee",
            "Seung-taek Woo",
            "Jungyu Jin",
            "Changhun Lee",
            "Eunhyeok Park"
        ],
        "submitted": "2025-09-15 14:59:35",
        "source": "arxiv",
        "comment": "EMNLP 2025 Main Conference, Long Paper (Oral)"
    },
    {
        "title": "Results of the 2025 Video Browser Showdown",
        "abstract": "This report presents the results of the 14th Video Browser Showdown, held at\nthe 2025 International Conference on Multimedia Modeling on the 8th of January\n2025 in Nara, Japan.",
        "url": "http://arxiv.org/abs/2509.12000v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12000v1",
        "arxiv_id": "2509.12000v1",
        "authors": [
            "Luca Rossetto",
            "Klaus Schoeffmann",
            "Cathal Gurrin",
            "Jakub Lokoč",
            "Werner Bailer"
        ],
        "submitted": "2025-09-15 14:48:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing Cycles",
        "abstract": "We describe Vicomtech's participation in the CLEARS challenge on text\nadaptation to Plain Language and Easy Read in Spanish. Our approach features\nautomatic post-editing of different types of initial Large Language Model\nadaptations, where successive adaptations are generated iteratively until\nreadability and similarity metrics indicate that no further adaptation\nrefinement can be successfully performed. Taking the average of all official\nmetrics, our submissions achieved first and second place in Plain language and\nEasy Read adaptation, respectively.",
        "url": "http://arxiv.org/abs/2509.11991v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11991v1",
        "arxiv_id": "2509.11991v1",
        "authors": [
            "Jesús Calleja",
            "David Ponce",
            "Thierry Etchegoyhen"
        ],
        "submitted": "2025-09-15 14:42:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Query-Focused Extractive Summarization for Sentiment Explanation",
        "abstract": "Constructive analysis of feedback from clients often requires determining the\ncause of their sentiment from a substantial amount of text documents. To assist\nand improve the productivity of such endeavors, we leverage the task of\nQuery-Focused Summarization (QFS). Models of this task are often impeded by the\nlinguistic dissonance between the query and the source documents. We propose\nand substantiate a multi-bias framework to help bridge this gap at a\ndomain-agnostic, generic level; we then formulate specialized approaches for\nthe problem of sentiment explanation through sentiment-based biases and query\nexpansion. We achieve experimental results outperforming baseline models on a\nreal-world proprietary sentiment-aware QFS dataset.",
        "url": "http://arxiv.org/abs/2509.11989v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11989v1",
        "arxiv_id": "2509.11989v1",
        "authors": [
            "Ahmed Moubtahij",
            "Sylvie Ratté",
            "Yazid Attabi",
            "Maxime Dumas"
        ],
        "submitted": "2025-09-15 14:41:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Lost in Embeddings: Information Loss in Vision-Language Models",
        "abstract": "Vision--language models (VLMs) often process visual inputs through a\npretrained vision encoder, followed by a projection into the language model's\nembedding space via a connector component. While crucial for modality fusion,\nthe potential information loss induced by this projection step and its direct\nimpact on model capabilities remain understudied. We introduce two\ncomplementary approaches to examine and quantify this loss by analyzing the\nlatent representation space. First, we evaluate semantic information\npreservation by analyzing changes in k-nearest neighbor relationships between\nimage representations, before and after projection. Second, we directly measure\ninformation loss by reconstructing visual embeddings from the projected\nrepresentation, localizing loss at an image patch level. Experiments reveal\nthat connectors substantially distort the local geometry of visual\nrepresentations, with k-nearest neighbors diverging by 40--60\\%\npost-projection, correlating with degradation in retrieval performance. The\npatch-level embedding reconstruction provides interpretable insights for model\nbehavior on visually grounded question-answering tasks, finding that areas of\nhigh information loss reliably predict instances where models struggle.",
        "url": "http://arxiv.org/abs/2509.11986v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11986v1",
        "arxiv_id": "2509.11986v1",
        "authors": [
            "Wenyan Li",
            "Raphael Tang",
            "Chengzu Li",
            "Caiqi Zhang",
            "Ivan Vulić",
            "Anders Søgaard"
        ],
        "submitted": "2025-09-15 14:38:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MillStone: How Open-Minded Are LLMs?",
        "abstract": "Large language models equipped with Web search, information retrieval tools,\nand other agentic capabilities are beginning to supplant traditional search\nengines. As users start to rely on LLMs for information on many topics,\nincluding controversial and debatable issues, it is important to understand how\nthe stances and opinions expressed in LLM outputs are influenced by the\ndocuments they use as their information sources.\n  In this paper, we present MillStone, the first benchmark that aims to\nsystematically measure the effect of external arguments on the stances that\nLLMs take on controversial issues (not all of them political). We apply\nMillStone to nine leading LLMs and measure how ``open-minded'' they are to\narguments supporting opposite sides of these issues, whether different LLMs\nagree with each other, which arguments LLMs find most persuasive, and whether\nthese arguments are the same for different LLMs.\n  In general, we find that LLMs are open-minded on most issues. An\nauthoritative source of information can easily sway an LLM's stance,\nhighlighting the importance of source selection and the risk that LLM-based\ninformation retrieval and search systems can be manipulated.",
        "url": "http://arxiv.org/abs/2509.11967v2",
        "pdf_url": "http://arxiv.org/pdf/2509.11967v2",
        "arxiv_id": "2509.11967v2",
        "authors": [
            "Harold Triedman",
            "Vitaly Shmatikov"
        ],
        "submitted": "2025-09-15 14:18:51",
        "source": "arxiv",
        "comment": "19 pages, 7 tables, 7 figures"
    },
    {
        "title": "ToolRM: Outcome Reward Models for Tool-Calling Large Language Models",
        "abstract": "As large language models (LLMs) increasingly interact with external tools,\nreward modeling for tool use has become a critical yet underexplored area.\nExisting reward models, trained primarily on natural language outputs, struggle\nto evaluate tool-based reasoning and execution. To quantify this gap, we\nintroduce FC-RewardBench, the first benchmark designed to systematically assess\nreward models' performance in tool-calling scenarios. Our analysis shows that\ncurrent reward models often miss key signals of effective tool use,\nhighlighting the need for domain-specific modeling. To address this, we propose\na training framework for outcome-based reward models using data synthesized\nfrom permissively licensed, open-weight LLMs. We train models ranging from 1.7B\nto 14B parameters and evaluate them across seven out-of-domain benchmarks.\nThese models consistently outperform general-purpose baselines, achieving up to\n25\\% average improvement in downstream task performance and enabling\ndata-efficient fine-tuning through reward-guided filtering.",
        "url": "http://arxiv.org/abs/2509.11963v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11963v1",
        "arxiv_id": "2509.11963v1",
        "authors": [
            "Mayank Agarwal",
            "Ibrahim Abdelaziz",
            "Kinjal Basu",
            "Merve Unuvar",
            "Luis A. Lastras",
            "Yara Rizk",
            "Pavan Kapanipathi"
        ],
        "submitted": "2025-09-15 14:17:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Spec-LLaVA: Accelerating Vision-Language Models with Dynamic Tree-Based Speculative Decoding",
        "abstract": "Vision-Language Models (VLMs) enable powerful multimodal reasoning but suffer\nfrom slow autoregressive inference, limiting their deployment in real-time\napplications. We introduce Spec-LLaVA, a system that applies speculative\ndecoding to accelerate VLMs without sacrificing output quality. Spec-LLaVA\npairs a lightweight draft VLM with a large target model: the draft speculates\nfuture tokens, which the target verifies in parallel, allowing multiple tokens\nto be generated per step. To maximize efficiency, we design a dynamic\ntree-based verification algorithm that adaptively expands and prunes\nspeculative branches using draft model confidence. On MS COCO out-of-domain\nimages, Spec-LLaVA achieves up to 3.28$\\times$ faster decoding on LLaVA-1.5\n(7B, 13B) with no loss in generation quality. This work presents a lossless\nacceleration framework for VLMs using dynamic tree-structured speculative\ndecoding, opening a path toward practical real-time multimodal assistants.\nImportantly, the lightweight draft model design makes the framework amenable to\nresource-constrained or on-device deployment settings.",
        "url": "http://arxiv.org/abs/2509.11961v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11961v1",
        "arxiv_id": "2509.11961v1",
        "authors": [
            "Mingxiao Huo",
            "Jiayi Zhang",
            "Hewei Wang",
            "Jinfeng Xu",
            "Zheyu Chen",
            "Huilin Tai",
            "Yijun Chen"
        ],
        "submitted": "2025-09-15 14:16:51",
        "source": "arxiv",
        "comment": "7pages, accepted by ICML TTODLer-FM workshop"
    },
    {
        "title": "How to Evaluate Medical AI",
        "abstract": "The integration of artificial intelligence (AI) into medical diagnostic\nworkflows requires robust and consistent evaluation methods to ensure\nreliability, clinical relevance, and the inherent variability in expert\njudgments. Traditional metrics like precision and recall often fail to account\nfor the inherent variability in expert judgments, leading to inconsistent\nassessments of AI performance. Inter-rater agreement statistics like Cohen's\nKappa are more reliable but they lack interpretability. We introduce Relative\nPrecision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new\nevaluation metrics that compare AI outputs against multiple expert opinions\nrather than a single reference. By normalizing performance against inter-expert\ndisagreement, these metrics provide a more stable and realistic measure of the\nquality of predicted diagnosis. In addition to the comprehensive analysis of\ndiagnostic quality measures, our study contains a very important side result.\nOur evaluation methodology allows us to avoid selecting diagnoses from a\nlimited list when evaluating a given case. Instead, both the models being\ntested and the examiners verifying them arrive at a free-form diagnosis. In\nthis automated methodology for establishing the identity of free-form clinical\ndiagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our\napproach using 360 medical dialogues, comparing multiple large language models\n(LLMs) against a panel of physicians. Large-scale study shows that\ntop-performing models, such as DeepSeek-V3, achieve consistency on par with or\nexceeding expert consensus. Moreover, we demonstrate that expert judgments\nexhibit significant variability - often greater than that between AI and\nhumans. This finding underscores the limitations of any absolute metrics and\nsupports the need to adopt relative metrics in medical AI.",
        "url": "http://arxiv.org/abs/2509.11941v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11941v1",
        "arxiv_id": "2509.11941v1",
        "authors": [
            "Ilia Kopanichuk",
            "Petr Anokhin",
            "Vladimir Shaposhnikov",
            "Vladimir Makharev",
            "Ekaterina Tsapieva",
            "Iaroslav Bespalov",
            "Dmitry V. Dylov",
            "Ivan Oseledets"
        ],
        "submitted": "2025-09-15 14:01:22",
        "source": "arxiv",
        "comment": "10 pages, 7 fugures"
    },
    {
        "title": "Designing LLMs for cultural sensitivity: Evidence from English-Japanese translation",
        "abstract": "Large language models (LLMs) are increasingly used in everyday communication,\nincluding multilingual interactions across different cultural contexts. While\nLLMs can now generate near-perfect literal translations, it remains unclear\nwhether LLMs support culturally appropriate communication. In this paper, we\nanalyze the cultural sensitivity of different LLM designs when applied to\nEnglish-Japanese translations of workplace e-mails. Here, we vary the prompting\nstrategies: (1) naive \"just translate\" prompts, (2) audience-targeted prompts\nspecifying the recipient's cultural background, and (3) instructional prompts\nwith explicit guidance on Japanese communication norms. Using a mixed-methods\nstudy, we then analyze culture-specific language patterns to evaluate how well\ntranslations adapt to cultural norms. Further, we examine the appropriateness\nof the tone of the translations as perceived by native speakers. We find that\nculturally-tailored prompting can improve cultural fit, based on which we offer\nrecommendations for designing culturally inclusive LLMs in multilingual\nsettings.",
        "url": "http://arxiv.org/abs/2509.11921v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11921v1",
        "arxiv_id": "2509.11921v1",
        "authors": [
            "Helene Tenzer",
            "Oumnia Abidi",
            "Stefan Feuerriegel"
        ],
        "submitted": "2025-09-15 13:37:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Uncertainty in Authorship: Why Perfect AI Detection Is Mathematically Impossible",
        "abstract": "As large language models (LLMs) become more advanced, it is increasingly\ndifficult to distinguish between human-written and AI-generated text. This\npaper draws a conceptual parallel between quantum uncertainty and the limits of\nauthorship detection in natural language. We argue that there is a fundamental\ntrade-off: the more confidently one tries to identify whether a text was\nwritten by a human or an AI, the more one risks disrupting the text's natural\nflow and authenticity. This mirrors the tension between precision and\ndisturbance found in quantum systems. We explore how current detection\nmethods--such as stylometry, watermarking, and neural classifiers--face\ninherent limitations. Enhancing detection accuracy often leads to changes in\nthe AI's output, making other features less reliable. In effect, the very act\nof trying to detect AI authorship introduces uncertainty elsewhere in the text.\nOur analysis shows that when AI-generated text closely mimics human writing,\nperfect detection becomes not just technologically difficult but theoretically\nimpossible. We address counterarguments and discuss the broader implications\nfor authorship, ethics, and policy. Ultimately, we suggest that the challenge\nof AI-text detection is not just a matter of better tools--it reflects a\ndeeper, unavoidable tension in the nature of language itself.",
        "url": "http://arxiv.org/abs/2509.11915v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11915v1",
        "arxiv_id": "2509.11915v1",
        "authors": [
            "Aadil Gani Ganie"
        ],
        "submitted": "2025-09-15 13:33:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Growing Perspectives: Modelling Embodied Perspective Taking and Inner Narrative Development Using Large Language Models",
        "abstract": "Language and embodied perspective taking are essential for human\ncollaboration, yet few computational models address both simultaneously. This\nwork investigates the PerspAct system [1], which integrates the ReAct (Reason\nand Act) paradigm with Large Language Models (LLMs) to simulate developmental\nstages of perspective taking, grounded in Selman's theory [2]. Using an\nextended director task, we evaluate GPT's ability to generate internal\nnarratives aligned with specified developmental stages, and assess how these\ninfluence collaborative performance both qualitatively (action selection) and\nquantitatively (task efficiency). Results show that GPT reliably produces\ndevelopmentally-consistent narratives before task execution but often shifts\ntowards more advanced stages during interaction, suggesting that language\nexchanges help refine internal representations. Higher developmental stages\ngenerally enhance collaborative effectiveness, while earlier stages yield more\nvariable outcomes in complex contexts. These findings highlight the potential\nof integrating embodied perspective taking and language in LLMs to better model\ndevelopmental dynamics and stress the importance of evaluating internal speech\nduring combined linguistic and embodied tasks.",
        "url": "http://arxiv.org/abs/2509.11868v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11868v1",
        "arxiv_id": "2509.11868v1",
        "authors": [
            "Sabrina Patania",
            "Luca Annese",
            "Anna Lambiase",
            "Anita Pellegrini",
            "Tom Foulsham",
            "Azzurra Ruggeri",
            "Silvia Rossi",
            "Silvia Serino",
            "Dimitri Ognibene"
        ],
        "submitted": "2025-09-15 12:39:55",
        "source": "arxiv",
        "comment": "Accepted at ICDL https://icdl2025.fel.cvut.cz/"
    },
    {
        "title": "MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues",
        "abstract": "Memory extraction is crucial for maintaining coherent ultra-long dialogues in\nhuman-robot role-playing scenarios. However, existing methods often exhibit\nuncontrolled memory growth. To address this, we propose MOOM, the first\ndual-branch memory plugin that leverages literary theory by modeling plot\ndevelopment and character portrayal as core storytelling elements.\nSpecifically, one branch summarizes plot conflicts across multiple time scales,\nwhile the other extracts the user's character profile. MOOM further integrates\na forgetting mechanism, inspired by the ``competition-inhibition'' memory\ntheory, to constrain memory capacity and mitigate uncontrolled growth.\nFurthermore, we present ZH-4O, a Chinese ultra-long dialogue dataset\nspecifically designed for role-playing, featuring dialogues that average 600\nturns and include manually annotated memory information. Experimental results\ndemonstrate that MOOM outperforms all state-of-the-art memory extraction\nmethods, requiring fewer large language model invocations while maintaining a\ncontrollable memory capacity.",
        "url": "http://arxiv.org/abs/2509.11860v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11860v1",
        "arxiv_id": "2509.11860v1",
        "authors": [
            "Weishu Chen",
            "Jinyi Tang",
            "Zhouhui Hou",
            "Shihao Han",
            "Mingjie Zhan",
            "Zhiyuan Huang",
            "Delong Liu",
            "Jiawei Guo",
            "Zhicheng Zhao",
            "Fei Su"
        ],
        "submitted": "2025-09-15 12:35:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The AI Memory Gap: Users Misremember What They Created With AI or Without",
        "abstract": "As large language models (LLMs) become embedded in interactive text\ngeneration, disclosure of AI as a source depends on people remembering which\nideas or texts came from themselves and which were created with AI. We\ninvestigate how accurately people remember the source of content when using AI.\nIn a pre-registered experiment, 184 participants generated and elaborated on\nideas both unaided and with an LLM-based chatbot. One week later, they were\nasked to identify the source (noAI vs withAI) of these ideas and texts. Our\nfindings reveal a significant gap in memory: After AI use, the odds of correct\nattribution dropped, with the steepest decline in mixed human-AI workflows,\nwhere either the idea or elaboration was created with AI. We validated our\nresults using a computational model of source memory. Discussing broader\nimplications, we highlight the importance of considering source confusion in\nthe design and use of interactive text generation technologies.",
        "url": "http://arxiv.org/abs/2509.11851v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11851v1",
        "arxiv_id": "2509.11851v1",
        "authors": [
            "Tim Zindulka",
            "Sven Goller",
            "Daniela Fernandes",
            "Robin Welsch",
            "Daniel Buschek"
        ],
        "submitted": "2025-09-15 12:31:00",
        "source": "arxiv",
        "comment": "31 pages, 10 figures, 9 tables"
    },
    {
        "title": "Collaborative Document Editing with Multiple Users and AI Agents",
        "abstract": "Current AI writing support tools are largely designed for individuals,\ncomplicating collaboration when co-writers must leave the shared workspace to\nuse AI and then communicate and reintegrate results. We propose integrating AI\nagents directly into collaborative writing environments. Our prototype makes AI\nuse transparent and customisable through two new shared objects: agent profiles\nand tasks. Agent responses appear in the familiar comment feature. In a user\nstudy (N=30), 14 teams worked on writing projects during one week. Interaction\nlogs and interviews show that teams incorporated agents into existing norms of\nauthorship, control, and coordination, rather than treating them as team\nmembers. Agent profiles were viewed as personal territory, while created agents\nand outputs became shared resources. We discuss implications for team-based AI\ninteraction, highlighting opportunities and boundaries for treating AI as a\nshared resource in collaborative work.",
        "url": "http://arxiv.org/abs/2509.11826v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11826v1",
        "arxiv_id": "2509.11826v1",
        "authors": [
            "Florian Lehmann",
            "Krystsina Shauchenka",
            "Daniel Buschek"
        ],
        "submitted": "2025-09-15 12:11:59",
        "source": "arxiv",
        "comment": "34 pages, 10 figures, 4 tables"
    },
    {
        "title": "Data-Driven Analysis of Text-Conditioned AI-Generated Music: A Case Study with Suno and Udio",
        "abstract": "Online AI platforms for creating music from text prompts (AI music), such as\nSuno and Udio, are now being used by hundreds of thousands of users. Some AI\nmusic is appearing in advertising, and even charting, in multiple countries.\nHow are these platforms being used? What subjects are inspiring their users?\nThis article answers these questions for Suno and Udio using a large collection\nof songs generated by users of these platforms from May to October 2024. Using\na combination of state-of-the-art text embedding models, dimensionality\nreduction and clustering methods, we analyze the prompts, tags and lyrics, and\nautomatically annotate and display the processed data in interactive plots. Our\nresults reveal prominent themes in lyrics, language preference, prompting\nstrategies, as well as peculiar attempts at steering models through the use of\nmetatags. To promote the musicological study of the developing cultural\npractice of AI-generated music we share our code and resources.",
        "url": "http://arxiv.org/abs/2509.11824v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11824v1",
        "arxiv_id": "2509.11824v1",
        "authors": [
            "Luca Casini",
            "Laura Cros Vila",
            "David Dalmazzo",
            "Anna-Kaisa Kaila",
            "Bob L. T. Sturm"
        ],
        "submitted": "2025-09-15 12:10:50",
        "source": "arxiv",
        "comment": "Submitted for review to TISMIR Digital Musicology special issue"
    },
    {
        "title": "SCDTour: Embedding Axis Ordering and Merging for Interpretable Semantic Change Detection",
        "abstract": "In Semantic Change Detection (SCD), it is a common problem to obtain\nembeddings that are both interpretable and high-performing. However, improving\ninterpretability often leads to a loss in the SCD performance, and vice versa.\nTo address this problem, we propose SCDTour, a method that orders and merges\ninterpretable axes to alleviate the performance degradation of SCD. SCDTour\nconsiders both (a) semantic similarity between axes in the embedding space, as\nwell as (b) the degree to which each axis contributes to semantic change.\nExperimental results show that SCDTour preserves performance in semantic change\ndetection while maintaining high interpretability. Moreover, agglomerating the\nsorted axes produces a more refined set of word senses, which achieves\ncomparable or improved performance against the original full-dimensional\nembeddings in the SCD task. These findings demonstrate that SCDTour effectively\nbalances interpretability and SCD performance, enabling meaningful\ninterpretation of semantic shifts through a small number of refined axes.\nSource code is available at https://github.com/LivNLP/svp-tour .",
        "url": "http://arxiv.org/abs/2509.11818v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11818v1",
        "arxiv_id": "2509.11818v1",
        "authors": [
            "Taichi Aida",
            "Danushka Bollegala"
        ],
        "submitted": "2025-09-15 12:01:24",
        "source": "arxiv",
        "comment": "Findings of EMNLP2025"
    },
    {
        "title": "Collapse of Irrelevant Representations (CIR) Ensures Robust and Non-Disruptive LLM Unlearning",
        "abstract": "Current unlearning techniques and safety training consistently fail to remove\ndangerous knowledge from language models. We analyze the root causes and\npropose a highly selective technique which unlearns robustly and without\ndisrupting general performance.\n  We perform PCA on activations and module output gradients to identify\nsubspaces containing common representations, and collapse them before\ncalculating unlearning updates. This way we avoid unlearning general\nrepresentations, and only target those specific to the unlearned facts.\n  When unlearning WMDP dataset facts from Llama-3.1-8B, we drop post-attack\naccuracy 80x more than our best baseline (Circuit Breakers) on biohazardous\nfacts and 30x more on cyberhazardous facts. Despite this, we disrupt general\nperformance 30x less (only 0.1% WikiText loss increase), while requiring less\nthan 3 GPU-seconds per fact.",
        "url": "http://arxiv.org/abs/2509.11816v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11816v1",
        "arxiv_id": "2509.11816v1",
        "authors": [
            "Filip Sondej",
            "Yushi Yang"
        ],
        "submitted": "2025-09-15 11:55:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PledgeTracker: A System for Monitoring the Fulfilment of Pledges",
        "abstract": "Political pledges reflect candidates' policy commitments, but tracking their\nfulfilment requires reasoning over incremental evidence distributed across\nmultiple, dynamically updated sources. Existing methods simplify this task into\na document classification task, overlooking its dynamic, temporal and\nmulti-document nature. To address this issue, we introduce\n\\textsc{PledgeTracker}, a system that reformulates pledge verification into\nstructured event timeline construction. PledgeTracker consists of three core\ncomponents: (1) a multi-step evidence retrieval module; (2) a timeline\nconstruction module and; (3) a fulfilment filtering module, allowing the\ncapture of the evolving nature of pledge fulfilment and producing interpretable\nand structured timelines. We evaluate PledgeTracker in collaboration with\nprofessional fact-checkers in real-world workflows, demonstrating its\neffectiveness in retrieving relevant evidence and reducing human verification\neffort.",
        "url": "http://arxiv.org/abs/2509.11804v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11804v1",
        "arxiv_id": "2509.11804v1",
        "authors": [
            "Yulong Chen",
            "Michael Sejr Schlichtkrull",
            "Zhenyun Deng",
            "David Corney",
            "Nasim Asl",
            "Joshua Salisbury",
            "Andrew Dudfield",
            "Andreas Vlachos"
        ],
        "submitted": "2025-09-15 11:37:47",
        "source": "arxiv",
        "comment": "EMNLP 2025 demo"
    },
    {
        "title": "From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient Narratives",
        "abstract": "The widespread adoption of large language models (LLMs) in healthcare raises\ncritical questions about their ability to interpret patient-generated\nnarratives, which are often informal, ambiguous, and noisy. Existing benchmarks\ntypically rely on clean, structured clinical text, offering limited insight\ninto model performance under realistic conditions. In this work, we present a\nnovel synthetic dataset designed to simulate patient self-descriptions\ncharacterized by varying levels of linguistic noise, fuzzy language, and\nlayperson terminology. Our dataset comprises clinically consistent scenarios\nannotated with ground-truth diagnoses, spanning a spectrum of communication\nclarity to reflect diverse real-world reporting styles. Using this benchmark,\nwe fine-tune and evaluate several state-of-the-art models (LLMs), including\nBERT-based and encoder-decoder T5 models. To support reproducibility and future\nresearch, we release the Noisy Diagnostic Benchmark (NDB), a structured dataset\nof noisy, synthetic patient descriptions designed to stress-test and compare\nthe diagnostic capabilities of large language models (LLMs) under realistic\nlinguistic conditions. We made the benchmark available for the community:\nhttps://github.com/lielsheri/PatientSignal",
        "url": "http://arxiv.org/abs/2509.11803v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11803v1",
        "arxiv_id": "2509.11803v1",
        "authors": [
            "Eden Mama",
            "Liel Sheri",
            "Yehudit Aperstein",
            "Alexander Apartsin"
        ],
        "submitted": "2025-09-15 11:34:46",
        "source": "arxiv",
        "comment": "6 pages, 1 figure"
    },
    {
        "title": "When Curiosity Signals Danger: Predicting Health Crises Through Online Medication Inquiries",
        "abstract": "Online medical forums are a rich and underutilized source of insight into\npatient concerns, especially regarding medication use. Some of the many\nquestions users pose may signal confusion, misuse, or even the early warning\nsigns of a developing health crisis. Detecting these critical questions that\nmay precede severe adverse events or life-threatening complications is vital\nfor timely intervention and improving patient safety. This study introduces a\nnovel annotated dataset of medication-related questions extracted from online\nforums. Each entry is manually labelled for criticality based on clinical risk\nfactors. We benchmark the performance of six traditional machine learning\nclassifiers using TF-IDF textual representations, alongside three\nstate-of-the-art large language model (LLM)-based classification approaches\nthat leverage deep contextual understanding. Our results highlight the\npotential of classical and modern methods to support real-time triage and alert\nsystems in digital health spaces. The curated dataset is made publicly\navailable to encourage further research at the intersection of\npatient-generated data, natural language processing, and early warning systems\nfor critical health events. The dataset and benchmark are available at:\nhttps://github.com/Dvora-coder/LLM-Medication-QA-Risk-Classifier-MediGuard.",
        "url": "http://arxiv.org/abs/2509.11802v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11802v1",
        "arxiv_id": "2509.11802v1",
        "authors": [
            "Dvora Goncharok",
            "Arbel Shifman",
            "Alexander Apartsin",
            "Yehudit Aperstein"
        ],
        "submitted": "2025-09-15 11:31:25",
        "source": "arxiv",
        "comment": "5 pages, 2 figures"
    },
    {
        "title": "User eXperience Perception Insights Dataset (UXPID): Synthetic User Feedback from Public Industrial Forums",
        "abstract": "Customer feedback in industrial forums reflect a rich but underexplored\nsource of insight into real-world product experience. These publicly shared\ndiscussions offer an organic view of user expectations, frustrations, and\nsuccess stories shaped by the specific contexts of use. Yet, harnessing this\ninformation for systematic analysis remains challenging due to the unstructured\nand domain-specific nature of the content. The lack of structure and\nspecialized vocabulary makes it difficult for traditional data analysis\ntechniques to accurately interpret, categorize, and quantify the feedback,\nthereby limiting its potential to inform product development and support\nstrategies. To address these challenges, this paper presents the User\neXperience Perception Insights Dataset (UXPID), a collection of 7130\nartificially synthesized and anonymized user feedback branches extracted from a\npublic industrial automation forum. Each JavaScript object notation (JSON)\nrecord contains multi-post comments related to specific hardware and software\nproducts, enriched with metadata and contextual conversation data. Leveraging a\nlarge language model (LLM), each branch is systematically analyzed and\nannotated for UX insights, user expectations, severity and sentiment ratings,\nand topic classifications. The UXPID dataset is designed to facilitate research\nin user requirements, user experience (UX) analysis, and AI-driven feedback\nprocessing, particularly where privacy and licensing restrictions limit access\nto real-world data. UXPID supports the training and evaluation of\ntransformer-based models for tasks such as issue detection, sentiment analysis,\nand requirements extraction in the context of technical forums.",
        "url": "http://arxiv.org/abs/2509.11777v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11777v1",
        "arxiv_id": "2509.11777v1",
        "authors": [
            "Mikhail Kulyabin",
            "Jan Joosten",
            "Choro Ulan uulu",
            "Nuno Miguel Martins Pacheco",
            "Fabian Ries",
            "Filippos Petridis",
            "Jan Bosch",
            "Helena Holmström Olsson"
        ],
        "submitted": "2025-09-15 10:58:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "An Agentic Toolkit for Adaptive Information Extraction from Regulatory Documents",
        "abstract": "Declaration of Performance (DoP) documents, mandated by EU regulation,\ncertify the performance of construction products. While some of their content\nis standardized, DoPs vary widely in layout, language, schema, and format,\nposing challenges for automated key-value pair extraction (KVP) and question\nanswering (QA). Existing static or LLM-only IE pipelines often hallucinate and\nfail to adapt to this structural diversity. Our domain-specific, stateful\nagentic system addresses these challenges through a planner-executor-responder\narchitecture. The system infers user intent, detects document modality, and\norchestrates tools dynamically for robust, traceable reasoning while avoiding\ntool misuse or execution loops. Evaluation on a curated DoP dataset\ndemonstrates improved robustness across formats and languages, offering a\nscalable solution for structured data extraction in regulated workflows.",
        "url": "http://arxiv.org/abs/2509.11773v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11773v1",
        "arxiv_id": "2509.11773v1",
        "authors": [
            "Gaye Colakoglu",
            "Gürkan Solmaz",
            "Jonathan Fürst"
        ],
        "submitted": "2025-09-15 10:53:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Room acoustics affect communicative success in hybrid meeting spaces: a pilot study",
        "abstract": "Since the COVID-19 pandemic in 2020, universities and companies have\nincreasingly integrated hybrid features into their meeting spaces, or even\ncreated dedicated rooms for this purpose. While the importance of a fast and\nstable internet connection is often prioritized, the acoustic design of seminar\nrooms is frequently overlooked. Poor acoustics, particularly excessive\nreverberation, can lead to issues such as misunderstandings, reduced speech\nintelligibility or cognitive and vocal fatigue. This pilot study investigates\nwhether room acoustic interventions in a seminar room at Graz University of\nTechnology support better communication in hybrid meetings. For this purpose,\nwe recorded two groups of persons twice, once before and once after improving\nthe acoustics of the room. Our findings -- despite not reaching statistical\nsignificance due to the small sample size - indicate clearly that our spatial\ninterventions improve communicative success in hybrid meetings. To make the\npaper accessible also for readers from the speech communication community, we\nexplain room acoustics background, relevant for the interpretation of our\nresults.",
        "url": "http://arxiv.org/abs/2509.11709v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11709v1",
        "arxiv_id": "2509.11709v1",
        "authors": [
            "Robert Einig",
            "Stefan Janscha",
            "Jonas Schuster",
            "Julian Koch",
            "Martin Hagmueller",
            "Barbara Schuppler"
        ],
        "submitted": "2025-09-15 09:09:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CoachMe: Decoding Sport Elements with a Reference-Based Coaching Instruction Generation Model",
        "abstract": "Motion instruction is a crucial task that helps athletes refine their\ntechnique by analyzing movements and providing corrective guidance. Although\nrecent advances in multimodal models have improved motion understanding,\ngenerating precise and sport-specific instruction remains challenging due to\nthe highly domain-specific nature of sports and the need for informative\nguidance. We propose CoachMe, a reference-based model that analyzes the\ndifferences between a learner's motion and a reference under temporal and\nphysical aspects. This approach enables both domain-knowledge learning and the\nacquisition of a coach-like thinking process that identifies movement errors\neffectively and provides feedback to explain how to improve. In this paper, we\nillustrate how CoachMe adapts well to specific sports such as skating and\nboxing by learning from general movements and then leveraging limited data.\nExperiments show that CoachMe provides high-quality instructions instead of\ndirections merely in the tone of a coach but without critical information.\nCoachMe outperforms GPT-4o by 31.6% in G-Eval on figure skating and by 58.3% on\nboxing. Analysis further confirms that it elaborates on errors and their\ncorresponding improvement methods in the generated instructions. You can find\nCoachMe here: https://motionxperts.github.io/",
        "url": "http://arxiv.org/abs/2509.11698v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11698v1",
        "arxiv_id": "2509.11698v1",
        "authors": [
            "Wei-Hsin Yeh",
            "Yu-An Su",
            "Chih-Ning Chen",
            "Yi-Hsueh Lin",
            "Calvin Ku",
            "Wen-Hsin Chiu",
            "Min-Chun Hu",
            "Lun-Wei Ku"
        ],
        "submitted": "2025-09-15 09:01:39",
        "source": "arxiv",
        "comment": "Published in Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025.\n  Official version: https://doi.org/10.18653/v1/2025.acl-long.1413"
    },
    {
        "title": "A Dynamic Knowledge Update-Driven Model with Large Language Models for Fake News Detection",
        "abstract": "As the Internet and social media evolve rapidly, distinguishing credible news\nfrom a vast amount of complex information poses a significant challenge. Due to\nthe suddenness and instability of news events, the authenticity labels of news\ncan potentially shift as events develop, making it crucial for fake news\ndetection to obtain the latest event updates. Existing methods employ\nretrieval-augmented generation to fill knowledge gaps, but they suffer from\nissues such as insufficient credibility of retrieved content and interference\nfrom noisy information. We propose a dynamic knowledge update-driven model for\nfake news detection (DYNAMO), which leverages knowledge graphs to achieve\ncontinuous updating of new knowledge and integrates with large language models\nto fulfill dual functions: news authenticity detection and verification of new\nknowledge correctness, solving the two key problems of ensuring the\nauthenticity of new knowledge and deeply mining news semantics. Specifically,\nwe first construct a news-domain-specific knowledge graph. Then, we use Monte\nCarlo Tree Search to decompose complex news and verify them step by step.\nFinally, we extract and update new knowledge from verified real news texts and\nreasoning paths. Experimental results demonstrate that DYNAMO achieves the best\nperformance on two real-world datasets.",
        "url": "http://arxiv.org/abs/2509.11687v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11687v1",
        "arxiv_id": "2509.11687v1",
        "authors": [
            "Di Jin",
            "Jun Yang",
            "Xiaobao Wang",
            "Junwei Zhang",
            "Shuqi Li",
            "Dongxiao He"
        ],
        "submitted": "2025-09-15 08:38:08",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Measuring Visual Understanding in Telecom domain: Performance Metrics for Image-to-UML conversion using VLMs",
        "abstract": "Telecom domain 3GPP documents are replete with images containing sequence\ndiagrams. Advances in Vision-Language Large Models (VLMs) have eased conversion\nof such images to machine-readable PlantUML (puml) formats. However, there is a\ngap in evaluation of such conversions - existing works do not compare puml\nscripts for various components. In this work, we propose performance metrics to\nmeasure the effectiveness of such conversions. A dataset of sequence diagrams\nfrom 3GPP documents is chosen to be representative of domain-specific actual\nscenarios. We compare puml outputs from two VLMs - Claude Sonnet and GPT-4V -\nagainst manually created ground truth representations. We use version control\ntools to capture differences and introduce standard performance metrics to\nmeasure accuracies along various components: participant identification,\nmessage flow accuracy, sequence ordering, and grouping construct preservation.\nWe demonstrate effectiveness of proposed metrics in quantifying conversion\nerrors across various components of puml scripts. The results show that nodes,\nedges and messages are accurately captured. However, we observe that VLMs do\nnot necessarily perform well on complex structures such as notes, box, groups.\nOur experiments and performance metrics indicates a need for better\nrepresentation of these components in training data for fine-tuned VLMs.",
        "url": "http://arxiv.org/abs/2509.11667v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11667v1",
        "arxiv_id": "2509.11667v1",
        "authors": [
            "HG Ranjani",
            "Rutuja Prabhudesai"
        ],
        "submitted": "2025-09-15 08:08:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs",
        "abstract": "We propose MindVL, a multimodal large langauge model trained on Ascend NPUs.\nSimilar to Qwen2.5-VL, MindVL adopts native-resolution Vision Transformers,\nwhich enables it to process images at their original variable resolutions. This\ndesign avoids the degradation caused by fixed-resolution tiling while\npreserving fine-grained details and global layouts, which is crucial for\nvisually dense content such as complex charts and diagrams. To ensure the\nsmooth training of MindVL on Ascend NPUs, we develop Mindspeed-MLLM, a\ndistributed multimodal training framework tailored for Ascend NPUs. To maintain\ntraining accuracy, we implement equivalent replacements for certain operators.\nMindVL undergoes a three-phase training process, namely the warm-up phase,\nmultitask training phase, and supervised instruction tuning phase, to gradually\nenhance its capabilities. This process starts with basic visual and multimodal\npre-training, followed by large-scale multiask trainging and instruction\ntuning. We also adopt multimodal data packaging and hybrid parallelism\ntechniques, which significantly improve end-to-end training speed. To further\nboost model performance, we specifically introduce test-time resolution search\nand model weight averaging. Notably, despite using about 1/10 of the training\ndata required by Qwen2.5-VL, MindVL achieves performance on par with Qwen2.5-VL\nin evaluations of general multimodal understanding and document/table\ncomprehension. Beyond overall scores, MindVL also delivers leading performance\nin OCR assessments.",
        "url": "http://arxiv.org/abs/2509.11662v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11662v1",
        "arxiv_id": "2509.11662v1",
        "authors": [
            "Feilong Chen",
            "Yijiang Liu",
            "Yi Huang",
            "Hao Wang",
            "Miren Tian",
            "Ya-Qi Yu",
            "Minghui Liao",
            "Jihao Wu"
        ],
        "submitted": "2025-09-15 08:00:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MALLM: Multi-Agent Large Language Models Framework",
        "abstract": "Multi-agent debate (MAD) has demonstrated the ability to augment collective\nintelligence by scaling test-time compute and leveraging expertise. Current\nframeworks for multi-agent debate are often designed towards tool use, lack\nintegrated evaluation, or provide limited configurability of agent personas,\nresponse generators, discussion paradigms, and decision protocols. We introduce\nMALLM (Multi-Agent Large Language Models), an open-source framework that\nenables systematic analysis of MAD components. MALLM offers more than 144\nunique configurations of MAD, including (1) agent personas (e.g., Expert,\nPersonality), (2) response generators (e.g., Critical, Reasoning), (3)\ndiscussion paradigms (e.g., Memory, Relay), and (4) decision protocols (e.g.,\nVoting, Consensus). MALLM uses simple configuration files to define a debate.\nFurthermore, MALLM can load any textual Huggingface dataset (e.g., MMLU-Pro,\nWinoGrande) and provides an evaluation pipeline for easy comparison of MAD\nconfigurations. MALLM is tailored towards researchers and provides a window\ninto the heart of multi-agent debate, facilitating the understanding of its\ncomponents and their interplay.",
        "url": "http://arxiv.org/abs/2509.11656v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11656v1",
        "arxiv_id": "2509.11656v1",
        "authors": [
            "Jonas Becker",
            "Lars Benedikt Kaesberg",
            "Niklas Bauer",
            "Jan Philip Wahle",
            "Terry Ruas",
            "Bela Gipp"
        ],
        "submitted": "2025-09-15 07:48:02",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025 (Demo)"
    },
    {
        "title": "EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI",
        "abstract": "The deployment of large language models (LLMs) in mental health and other\nsensitive domains raises urgent questions about ethical reasoning, fairness,\nand responsible alignment. Yet, existing benchmarks for moral and clinical\ndecision-making do not adequately capture the unique ethical dilemmas\nencountered in mental health practice, where confidentiality, autonomy,\nbeneficence, and bias frequently intersect. To address this gap, we introduce\nEthical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios\ndesigned to evaluate how AI systems navigate ethically charged situations in\ntherapeutic and psychiatric contexts. Each scenario is enriched with structured\nfields, including multiple decision options, expert-aligned reasoning, expected\nmodel behavior, real-world impact, and multi-stakeholder viewpoints. This\nstructure enables evaluation not only of decision accuracy but also of\nexplanation quality and alignment with professional norms. Although modest in\nscale and developed with model-assisted generation, EthicsMH establishes a task\nframework that bridges AI ethics and mental health decision-making. By\nreleasing this dataset, we aim to provide a seed resource that can be expanded\nthrough community and expert contributions, fostering the development of AI\nsystems capable of responsibly handling some of society's most delicate\ndecisions.",
        "url": "http://arxiv.org/abs/2509.11648v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11648v1",
        "arxiv_id": "2509.11648v1",
        "authors": [
            "Sai Kartheek Reddy Kasu"
        ],
        "submitted": "2025-09-15 07:35:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AesBiasBench: Evaluating Bias and Alignment in Multimodal Language Models for Personalized Image Aesthetic Assessment",
        "abstract": "Multimodal Large Language Models (MLLMs) are increasingly applied in\nPersonalized Image Aesthetic Assessment (PIAA) as a scalable alternative to\nexpert evaluations. However, their predictions may reflect subtle biases\ninfluenced by demographic factors such as gender, age, and education. In this\nwork, we propose AesBiasBench, a benchmark designed to evaluate MLLMs along two\ncomplementary dimensions: (1) stereotype bias, quantified by measuring\nvariations in aesthetic evaluations across demographic groups; and (2)\nalignment between model outputs and genuine human aesthetic preferences. Our\nbenchmark covers three subtasks (Aesthetic Perception, Assessment, Empathy) and\nintroduces structured metrics (IFD, NRD, AAS) to assess both bias and\nalignment. We evaluate 19 MLLMs, including proprietary models (e.g., GPT-4o,\nClaude-3.5-Sonnet) and open-source models (e.g., InternVL-2.5, Qwen2.5-VL).\nResults indicate that smaller models exhibit stronger stereotype biases,\nwhereas larger models align more closely with human preferences. Incorporating\nidentity information often exacerbates bias, particularly in emotional\njudgments. These findings underscore the importance of identity-aware\nevaluation frameworks in subjective vision-language tasks.",
        "url": "http://arxiv.org/abs/2509.11620v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11620v1",
        "arxiv_id": "2509.11620v1",
        "authors": [
            "Kun Li",
            "Lai-Man Po",
            "Hongzheng Yang",
            "Xuyuan Xu",
            "Kangcheng Liu",
            "Yuzhi Zhao"
        ],
        "submitted": "2025-09-15 06:25:39",
        "source": "arxiv",
        "comment": "Accepted by EMNLP 2025"
    },
    {
        "title": "HalluDetect: Detecting, Mitigating, and Benchmarking Hallucinations in Conversational Systems",
        "abstract": "Large Language Models (LLMs) are widely used in industry but remain prone to\nhallucinations, limiting their reliability in critical applications. This work\naddresses hallucination reduction in consumer grievance chatbots built using\nLLaMA 3.1 8B Instruct, a compact model frequently used in industry. We develop\nHalluDetect, an LLM-based hallucination detection system that achieves an F1\nscore of 69% outperforming baseline detectors by 25.44%. Benchmarking five\nchatbot architectures, we find that out of them, AgentBot minimizes\nhallucinations to 0.4159 per turn while maintaining the highest token accuracy\n(96.13%), making it the most effective mitigation strategy. Our findings\nprovide a scalable framework for hallucination mitigation, demonstrating that\noptimized inference strategies can significantly improve factual accuracy.\nWhile applied to consumer law, our approach generalizes to other high-risk\ndomains, enhancing trust in LLM-driven assistants. We will release the code and\ndataset",
        "url": "http://arxiv.org/abs/2509.11619v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11619v1",
        "arxiv_id": "2509.11619v1",
        "authors": [
            "Spandan Anaokar",
            "Shrey Ganatra",
            "Harshvivek Kashid",
            "Swapnil Bhattacharyya",
            "Shruti Nair",
            "Reshma Sekhar",
            "Siddharth Manohar",
            "Rahul Hemrajani",
            "Pushpak Bhattacharyya"
        ],
        "submitted": "2025-09-15 06:23:36",
        "source": "arxiv",
        "comment": "6 pages + references + appendix, 3 figures, 2 tables"
    },
    {
        "title": "Dynamic Span Interaction and Graph-Aware Memory for Entity-Level Sentiment Classification",
        "abstract": "Entity-level sentiment classification involves identifying the sentiment\npolarity linked to specific entities within text. This task poses several\nchallenges: effectively modeling the subtle and complex interactions between\nentities and their surrounding sentiment expressions; capturing dependencies\nthat may span across sentences; and ensuring consistent sentiment predictions\nfor multiple mentions of the same entity through coreference resolution.\nAdditionally, linguistic phenomena such as negation, ambiguity, and overlapping\nopinions further complicate the analysis. These complexities make entity-level\nsentiment classification a difficult problem, especially in real-world, noisy\ntextual data. To address these issues, we propose SpanEIT, a novel framework\nintegrating dynamic span interaction and graph-aware memory mechanisms for\nenhanced entity-sentiment relational modeling. SpanEIT builds span-based\nrepresentations for entities and candidate sentiment phrases, employs\nbidirectional attention for fine-grained interactions, and uses a graph\nattention network to capture syntactic and co-occurrence relations. A\ncoreference-aware memory module ensures entity-level consistency across\ndocuments. Experiments on FSAD, BARU, and IMDB datasets show SpanEIT\noutperforms state-of-the-art transformer and hybrid baselines in accuracy and\nF1 scores. Ablation and interpretability analyses validate the effectiveness of\nour approach, underscoring its potential for fine-grained sentiment analysis in\napplications like social media monitoring and customer feedback analysis.",
        "url": "http://arxiv.org/abs/2509.11604v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11604v1",
        "arxiv_id": "2509.11604v1",
        "authors": [
            "Md. Mithun Hossain",
            "Sanjara",
            "Md. Shakil Hossain",
            "Sudipto Chaki"
        ],
        "submitted": "2025-09-15 05:47:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Analyzing Information-Seeking Behaviors in a Hakka AI Chatbot: A Cognitive-Pragmatic Study",
        "abstract": "With many endangered languages at risk of disappearing, efforts to preserve\nthem now rely more than ever on using technology alongside culturally informed\nteaching strategies. This study examines user behaviors in TALKA, a generative\nAI-powered chatbot designed for Hakka language engagement, by employing a\ndual-layered analytical framework grounded in Bloom's Taxonomy of cognitive\nprocesses and dialogue act categorization. We analyzed 7,077 user utterances,\neach carefully annotated according to six cognitive levels and eleven dialogue\nact types. These included a variety of functions, such as asking for\ninformation, requesting translations, making cultural inquiries, and using\nlanguage creatively. Pragmatic classifications further highlight how different\ntypes of dialogue acts--such as feedback, control commands, and social\ngreetings--align with specific cognitive intentions. The results suggest that\ngenerative AI chatbots can support language learning in meaningful\nways--especially when they are designed with an understanding of how users\nthink and communicate. They may also help learners express themselves more\nconfidently and connect with their cultural identity. The TALKA case provides\nempirical insights into how AI-mediated dialogue facilitates cognitive\ndevelopment in low-resource language learners, as well as pragmatic negotiation\nand socio-cultural affiliation. By focusing on AI-assisted language learning,\nthis study offers new insights into how technology can support language\npreservation and educational practice.",
        "url": "http://arxiv.org/abs/2509.11591v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11591v1",
        "arxiv_id": "2509.11591v1",
        "authors": [
            "Chu-Hsuan Lee",
            "Chen-Chi Chang",
            "Hung-Shin Lee",
            "Yun-Hsiang Hsu",
            "Ching-Yuan Chen"
        ],
        "submitted": "2025-09-15 05:18:17",
        "source": "arxiv",
        "comment": "Accepted to HICSS-59 (2026)"
    },
    {
        "title": "Formal Reasoning for Intelligent QA Systems: A Case Study in the Educational Domain",
        "abstract": "Reasoning is essential for closed-domain QA systems in which procedural\ncorrectness and policy compliance are critical. While large language models\n(LLMs) have shown strong performance on many reasoning tasks, recent work\nreveals that their reasoning traces are often unfaithful - serving more as\nplausible justifications than as causally grounded derivations. Efforts to\ncombine LLMs with symbolic engines (e.g., Prover9, Z3) have improved\nreliability but remain limited to static forms of logic, struggling with\ndynamic, state-based reasoning such as multi-step progressions and conditional\ntransitions.\n  In this paper, we propose MCFR (Model Checking for Formal Reasoning), a\nneuro-symbolic framework that integrates LLMs with model checking to support\nproperty verification. MCFR translates natural language into formal\nspecifications and verifies them over transition models. To support evaluation,\nwe introduce EduMC-QA, a benchmark dataset grounded in real academic\nprocedures. Our results show that MCFR improves reasoning faithfulness and\ninterpretability, offering a viable path toward verifiable QA in high-stakes\nclosed-domain applications. In addition to evaluating MCFR, we compare its\nperformance with state-of-the-art LLMs such as ChatGPT, DeepSeek, and Claude to\ncontextualize its effectiveness.",
        "url": "http://arxiv.org/abs/2509.11572v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11572v1",
        "arxiv_id": "2509.11572v1",
        "authors": [
            "Tuan Bui",
            "An Nguyen",
            "Phat Thai",
            "Minh Hua",
            "Ngan Pham L. N.",
            "Ngan Pham T. B.",
            "Dung Le",
            "Long Nguyen",
            "Thanh-Tung Tran",
            "Thang Bui",
            "Tho Quan"
        ],
        "submitted": "2025-09-15 04:34:42",
        "source": "arxiv",
        "comment": "Published at the 2nd ACM Workshop in AI-powered Question & Answering\n  Systems (AIQAM '25), co-located with ACM Multimedia 2025"
    },
    {
        "title": "Bhaasha, Bhasa, Zaban: A Survey for Low-Resourced Languages in South Asia -- Current Stage and Challenges",
        "abstract": "Rapid developments of large language models have revolutionized many NLP\ntasks for English data. Unfortunately, the models and their evaluations for\nlow-resource languages are being overlooked, especially for languages in South\nAsia. Although there are more than 650 languages in South Asia, many of them\neither have very limited computational resources or are missing from existing\nlanguage models. Thus, a concrete question to be answered is: Can we assess the\ncurrent stage and challenges to inform our NLP community and facilitate model\ndevelopments for South Asian languages? In this survey, we have comprehensively\nexamined current efforts and challenges of NLP models for South Asian languages\nby retrieving studies since 2020, with a focus on transformer-based models,\nsuch as BERT, T5, & GPT. We present advances and gaps across 3 essential\naspects: data, models, & tasks, such as available data sources, fine-tuning\nstrategies, & domain applications. Our findings highlight substantial issues,\nincluding missing data in critical domains (e.g., health), code-mixing, and\nlack of standardized evaluation benchmarks. Our survey aims to raise awareness\nwithin the NLP community for more targeted data curation, unify benchmarks\ntailored to cultural and linguistic nuances of South Asia, and encourage an\nequitable representation of South Asian languages. The complete list of\nresources is available at: https://github.com/trust-nlp/LM4SouthAsia-Survey.",
        "url": "http://arxiv.org/abs/2509.11570v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11570v1",
        "arxiv_id": "2509.11570v1",
        "authors": [
            "Sampoorna Poria",
            "Xiaolei Huang"
        ],
        "submitted": "2025-09-15 04:31:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "D$^2$HScore: Reasoning-Aware Hallucination Detection via Semantic Breadth and Depth Analysis in LLMs",
        "abstract": "Although large Language Models (LLMs) have achieved remarkable success, their\npractical application is often hindered by the generation of non-factual\ncontent, which is called \"hallucination\". Ensuring the reliability of LLMs'\noutputs is a critical challenge, particularly in high-stakes domains such as\nfinance, security, and healthcare. In this work, we revisit hallucination\ndetection from the perspective of model architecture and generation dynamics.\nLeveraging the multi-layer structure and autoregressive decoding process of\nLLMs, we decompose hallucination signals into two complementary dimensions: the\nsemantic breadth of token representations within each layer, and the semantic\ndepth of core concepts as they evolve across layers. Based on this insight, we\npropose \\textbf{D$^2$HScore (Dispersion and Drift-based Hallucination Score)},\na training-free and label-free framework that jointly measures: (1)\n\\textbf{Intra-Layer Dispersion}, which quantifies the semantic diversity of\ntoken representations within each layer; and (2) \\textbf{Inter-Layer Drift},\nwhich tracks the progressive transformation of key token representations across\nlayers. To ensure drift reflects the evolution of meaningful semantics rather\nthan noisy or redundant tokens, we guide token selection using attention\nsignals. By capturing both the horizontal and vertical dynamics of\nrepresentation during inference, D$^2$HScore provides an interpretable and\nlightweight proxy for hallucination detection. Extensive experiments across\nfive open-source LLMs and five widely used benchmarks demonstrate that\nD$^2$HScore consistently outperforms existing training-free baselines.",
        "url": "http://arxiv.org/abs/2509.11569v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11569v1",
        "arxiv_id": "2509.11569v1",
        "authors": [
            "Yue Ding",
            "Xiaofang Zhu",
            "Tianze Xia",
            "Junfei Wu",
            "Xinlong Chen",
            "Qiang Liu",
            "Liang Wang"
        ],
        "submitted": "2025-09-15 04:28:38",
        "source": "arxiv",
        "comment": "under review"
    },
    {
        "title": "HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances the response capabilities of\nlanguage models by integrating external knowledge sources. However, document\nchunking as an important part of RAG system often lacks effective evaluation\ntools. This paper first analyzes why existing RAG evaluation benchmarks are\ninadequate for assessing document chunking quality, specifically due to\nevidence sparsity. Based on this conclusion, we propose HiCBench, which\nincludes manually annotated multi-level document chunking points, synthesized\nevidence-dense quetion answer(QA) pairs, and their corresponding evidence\nsources. Additionally, we introduce the HiChunk framework, a multi-level\ndocument structuring framework based on fine-tuned LLMs, combined with the\nAuto-Merge retrieval algorithm to improve retrieval quality. Experiments\ndemonstrate that HiCBench effectively evaluates the impact of different\nchunking methods across the entire RAG pipeline. Moreover, HiChunk achieves\nbetter chunking quality within reasonable time consumption, thereby enhancing\nthe overall performance of RAG systems.",
        "url": "http://arxiv.org/abs/2509.11552v2",
        "pdf_url": "http://arxiv.org/pdf/2509.11552v2",
        "arxiv_id": "2509.11552v2",
        "authors": [
            "Wensheng Lu",
            "Keyu Chen",
            "Ruizhi Qiao",
            "Xing Sun"
        ],
        "submitted": "2025-09-15 03:32:50",
        "source": "arxiv",
        "comment": "17 pages, 5 figures, 6 tables"
    },
    {
        "title": "HARP: Hallucination Detection via Reasoning Subspace Projection",
        "abstract": "Hallucinations in Large Language Models (LLMs) pose a major barrier to their\nreliable use in critical decision-making. Although existing hallucination\ndetection methods have improved accuracy, they still struggle with\ndisentangling semantic and reasoning information and maintaining robustness. To\naddress these challenges, we propose HARP (Hallucination detection via\nreasoning subspace projection), a novel hallucination detection framework. HARP\nestablishes that the hidden state space of LLMs can be decomposed into a direct\nsum of a semantic subspace and a reasoning subspace, where the former encodes\nlinguistic expression and the latter captures internal reasoning processes.\nMoreover, we demonstrate that the Unembedding layer can disentangle these\nsubspaces, and by applying Singular Value Decomposition (SVD) to its\nparameters, the basis vectors spanning the semantic and reasoning subspaces are\nobtained. Finally, HARP projects hidden states onto the basis vectors of the\nreasoning subspace, and the resulting projections are then used as input\nfeatures for hallucination detection in LLMs. By using these projections, HARP\nreduces the dimension of the feature to approximately 5% of the original,\nfilters out most noise, and achieves enhanced robustness. Experiments across\nmultiple datasets show that HARP achieves state-of-the-art hallucination\ndetection performance; in particular, it achieves an AUROC of 92.8% on\nTriviaQA, outperforming the previous best method by 7.5%.",
        "url": "http://arxiv.org/abs/2509.11536v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11536v1",
        "arxiv_id": "2509.11536v1",
        "authors": [
            "Junjie Hu",
            "Gang Tu",
            "ShengYu Cheng",
            "Jinxin Li",
            "Jinting Wang",
            "Rui Chen",
            "Zhilong Zhou",
            "Dongbo Shan"
        ],
        "submitted": "2025-09-15 03:02:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "On the Distinctive Co-occurrence Characteristics of Antonymy",
        "abstract": "Antonymy has long received particular attention in lexical semantics.\nPrevious studies have shown that antonym pairs frequently co-occur in text,\nacross genres and parts of speech, more often than would be expected by chance.\nHowever, whether this co-occurrence pattern is distinctive of antonymy remains\nunclear, due to a lack of comparison with other semantic relations. This work\nfills the gap by comparing antonymy with three other relations across parts of\nspeech using robust co-occurrence metrics. We find that antonymy is distinctive\nin three respects: antonym pairs co-occur with high strength, in a preferred\nlinear order, and within short spans. All results are available online.",
        "url": "http://arxiv.org/abs/2509.11534v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11534v1",
        "arxiv_id": "2509.11534v1",
        "authors": [
            "Zhihan Cao",
            "Hiroaki Yamada",
            "Takenobu Tokunaga"
        ],
        "submitted": "2025-09-15 02:58:14",
        "source": "arxiv",
        "comment": "Accepted by *SEM 2025"
    },
    {
        "title": "Decoding in Latent Spaces for Efficient Inference in LLM-based Recommendation",
        "abstract": "Fine-tuning large language models (LLMs) for recommendation in a generative\nmanner has delivered promising results, but encounters significant inference\noverhead due to autoregressive decoding in the language space. This work\nexplores bypassing language-space decoding by directly matching candidate items\nwith the LLM's internal thought representations in the latent space,\neliminating the time-consuming autoregressive process to reduce computational\ncosts. Towards this, we introduce Light Latent-space Decoding (L2D), an\neffective and efficient latent-space decoding method. L2D represents\nuser-preferred items by using the hidden states of test sequences reflecting\nthe LLM's internal thought, and obtains candidate item representations from the\nhidden states of training sequences labeled with the corresponding candidate\nitems. It then matches the two types of representations to decode items,\nachieving latent-space decoding. In this way, it enables efficient decoding\nwithout altering the LLM's generative tuning paradigm, thereby preserving\nperformance. Extensive empirical results demonstrate that L2D is more than 10x\nfaster than language-space decoding while maintaining or enhancing performance.",
        "url": "http://arxiv.org/abs/2509.11524v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11524v1",
        "arxiv_id": "2509.11524v1",
        "authors": [
            "Chengbing Wang",
            "Yang Zhang",
            "Zhicheng Wang",
            "Tianhao Shi",
            "Keqin Bao",
            "Fuli Feng",
            "Tat-Seng Chua"
        ],
        "submitted": "2025-09-15 02:30:35",
        "source": "arxiv",
        "comment": "Accepted for publication in EMNLP'25"
    },
    {
        "title": "PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation",
        "abstract": "BACKGROUND: Medical large language models (LLMS) have demonstrated remarkable\nperformance in answering medical examinations. However, the extent to which\nthis high performance is transferable to medical questions in Spanish and from\na Latin American country remains unexplored. This knowledge is crucial as\nLLM-based medical applications gain traction in Latin America. AIMS: to build a\ndataset of questions from medical examinations taken by Peruvian physicians\npursuing specialty training; to fine-tune a LLM on this dataset; to evaluate\nand compare the performance in terms of accuracy between vanilla LLMs and the\nfine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice\nquestion-answering (MCQA) datasets containing 8,380 questions spanning 12\nmedical domains (2018-2025). We selected eight medical LLMs including\nmedgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task-specific\nprompts to answer the questions appropriately. We employed parameter-efficient\nfine tuning (PEFT)and low-rant adaptation (LoRA) to fine-tune medgemma-4b-it\nutilizing all questions except those from 2025 (test set). RESULTS:\nmedgemma-27b-text-it outperformed all other models, achieving a proportion of\ncorrect answers exceeding 90% in several instances. LLMs with <10 billion\nparameters exhibited <60% of correct answers, while some exams yielded results\n<50%. The fine-tuned version of medgemma-4b-it emerged victorious agains all\nLLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters\nacross various examinations. CONCLUSIONS: For medical AI application and\nresearch that require knowledge bases from Spanish-speaking countries and those\nexhibiting similar epidemiological profiles to Peru's, interested parties\nshould utilize medgemma-27b-text-it or a fine-tuned version of medgemma-4b-it.",
        "url": "http://arxiv.org/abs/2509.11517v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11517v1",
        "arxiv_id": "2509.11517v1",
        "authors": [
            "Rodrigo M. Carrillo-Larco",
            "Jesus Lovón Melgarejo",
            "Manuel Castillo-Cara",
            "Gusseppe Bravo-Rocca"
        ],
        "submitted": "2025-09-15 02:07:26",
        "source": "arxiv",
        "comment": "https://github.com/rodrigo-carrillo/PeruMedQA"
    },
    {
        "title": "LVLMs are Bad at Overhearing Human Referential Communication",
        "abstract": "During spontaneous conversations, speakers collaborate on novel referring\nexpressions, which they can then re-use in subsequent conversations.\nUnderstanding such referring expressions is an important ability for an\nembodied agent, so that it can carry out tasks in the real world. This requires\nintegrating and understanding language, vision, and conversational interaction.\nWe study the capabilities of seven state-of-the-art Large Vision Language\nModels (LVLMs) as overhearers to a corpus of spontaneous conversations between\npairs of human discourse participants engaged in a collaborative\nobject-matching task. We find that such a task remains challenging for current\nLVLMs and they all fail to show a consistent performance improvement as they\noverhear more conversations from the same discourse participants repeating the\nsame task for multiple rounds. We release our corpus and code for\nreproducibility and to facilitate future research.",
        "url": "http://arxiv.org/abs/2509.11514v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11514v1",
        "arxiv_id": "2509.11514v1",
        "authors": [
            "Zhengxiang Wang",
            "Weiling Li",
            "Panagiotis Kaliosis",
            "Owen Rambow",
            "Susan E. Brennan"
        ],
        "submitted": "2025-09-15 02:03:18",
        "source": "arxiv",
        "comment": "EMNLP 2025 (Main)"
    },
    {
        "title": "Unsupervised Candidate Ranking for Lexical Substitution via Holistic Sentence Semantics",
        "abstract": "A key subtask in lexical substitution is ranking the given candidate words. A\ncommon approach is to replace the target word with a candidate in the original\nsentence and feed the modified sentence into a model to capture semantic\ndifferences before and after substitution. However, effectively modeling the\nbidirectional influence of candidate substitution on both the target word and\nits context remains challenging. Existing methods often focus solely on\nsemantic changes at the target position or rely on parameter tuning over\nmultiple evaluation metrics, making it difficult to accurately characterize\nsemantic variation. To address this, we investigate two approaches: one based\non attention weights and another leveraging the more interpretable integrated\ngradients method, both designed to measure the influence of context tokens on\nthe target token and to rank candidates by incorporating semantic similarity\nbetween the original and substituted sentences. Experiments on the LS07 and\nSWORDS datasets demonstrate that both approaches improve ranking performance.",
        "url": "http://arxiv.org/abs/2509.11513v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11513v1",
        "arxiv_id": "2509.11513v1",
        "authors": [
            "Zhongyang Hu",
            "Naijie Gu",
            "Xiangzhi Tao",
            "Tianhui Gu",
            "Yibing Zhou"
        ],
        "submitted": "2025-09-15 01:57:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DeDisCo at the DISRPT 2025 Shared Task: A System for Discourse Relation Classification",
        "abstract": "This paper presents DeDisCo, Georgetown University's entry in the DISRPT 2025\nshared task on discourse relation classification. We test two approaches, using\nan mt5-based encoder and a decoder based approach using the openly available\nQwen model. We also experiment on training with augmented dataset for\nlow-resource languages using matched data translated automatically from\nEnglish, as well as using some additional linguistic features inspired by\nentries in previous editions of the Shared Task. Our system achieves a\nmacro-accuracy score of 71.28, and we provide some interpretation and error\nanalysis for our results.",
        "url": "http://arxiv.org/abs/2509.11498v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11498v1",
        "arxiv_id": "2509.11498v1",
        "authors": [
            "Zhuoxuan Ju",
            "Jingni Wu",
            "Abhishek Purushothama",
            "Amir Zeldes"
        ],
        "submitted": "2025-09-15 01:25:37",
        "source": "arxiv",
        "comment": "System submission for the DISRPT 2025 - Shared Task on Discourse\n  Relation Parsing and Treebanking In conjunction with CODI-CRAC & EMNLP 2025.\n  1st place in Task 3: relation classification"
    },
    {
        "title": "AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for Multilingual Claim Normalization",
        "abstract": "Claim normalization, the transformation of informal social media posts into\nconcise, self-contained statements, is a crucial step in automated\nfact-checking pipelines. This paper details our submission to the CLEF-2025\nCheckThat! Task~2, which challenges systems to perform claim normalization\nacross twenty languages, divided into thirteen supervised (high-resource) and\nseven zero-shot (no training data) tracks.\n  Our approach, leveraging fine-tuned Small Language Models (SLMs) for\nsupervised languages and Large Language Model (LLM) prompting for zero-shot\nscenarios, achieved podium positions (top three) in fifteen of the twenty\nlanguages. Notably, this included second-place rankings in eight languages,\nfive of which were among the seven designated zero-shot languages, underscoring\nthe effectiveness of our LLM-based zero-shot strategy. For Portuguese, our\ninitial development language, our system achieved an average METEOR score of\n0.5290, ranking third. All implementation artifacts, including inference,\ntraining, evaluation scripts, and prompt configurations, are publicly available\nat https://github.com/ju-resplande/checkthat2025_normalization.",
        "url": "http://arxiv.org/abs/2509.11496v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11496v1",
        "arxiv_id": "2509.11496v1",
        "authors": [
            "Fabrycio Leite Nakano Almada",
            "Kauan Divino Pouso Mariano",
            "Maykon Adriell Dutra",
            "Victor Emanuel da Silva Monteiro",
            "Juliana Resplande Sant'Anna Gomes",
            "Arlindo Rodrigues Galvão Filho",
            "Anderson da Silva Soares"
        ],
        "submitted": "2025-09-15 01:19:49",
        "source": "arxiv",
        "comment": "15 pages, 2 figures"
    },
    {
        "title": "ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language Models for Verifying Numerical Claims",
        "abstract": "This paper presents our system for Task 3 of the CLEF 2025 CheckThat! Lab,\nwhich focuses on verifying numerical and temporal claims using retrieved\nevidence. We explore two complementary approaches: zero-shot prompting with\ninstruction-tuned large language models (LLMs) and supervised fine-tuning using\nparameter-efficient LoRA. To enhance evidence quality, we investigate several\nselection strategies, including full-document input and top-k sentence\nfiltering using BM25 and MiniLM. Our best-performing model LLaMA fine-tuned\nwith LoRA achieves strong performance on the English validation set. However, a\nnotable drop in the test set highlights a generalization challenge. These\nfindings underscore the importance of evidence granularity and model adaptation\nfor robust numerical fact verification.",
        "url": "http://arxiv.org/abs/2509.11492v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11492v1",
        "arxiv_id": "2509.11492v1",
        "authors": [
            "Anirban Saha Anik",
            "Md Fahimul Kabir Chowdhury",
            "Andrew Wyckoff",
            "Sagnik Ray Choudhury"
        ],
        "submitted": "2025-09-15 01:03:09",
        "source": "arxiv",
        "comment": "Notebook for the CheckThat! Lab at CLEF 2025"
    },
    {
        "title": "Acoustic Overspecification in Electronic Dance Music Taxonomy",
        "abstract": "Electronic Dance Music (EDM) classification typically relies on\nindustry-defined taxonomies with numerous subgenres, yet the acoustic basis for\nthese distinctions remains unclear. Current approaches use supervised learning\nwith prescribed genre labels, assuming their validity without systematic\nevaluation. In this paper, we propose an unsupervised approach to discover the\nnatural acoustic structure of EDM independent of commercial labels. Our method\ncombines novel tempogram-based features capturing EDM's layered rhythmic\npatterns with multi-criteria feature selection. To validate that our findings\nreflect genuine acoustic structure rather than methodological artifacts, we\ncompare our results against state-of-the-art pre-trained audio embeddings (MERT\nand CLAP). Both our feature space and embedding representations converge to\n19-23 natural acoustic families compared to the prescribed 35, providing\nconsistent evidence of significant overspecification in current EDM taxonomy by\napproximately one-third.",
        "url": "http://arxiv.org/abs/2509.11474v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11474v1",
        "arxiv_id": "2509.11474v1",
        "authors": [
            "Weilun Xu",
            "Tianhao Dai",
            "Oscar Goudet",
            "Xiaoxuan Wang"
        ],
        "submitted": "2025-09-14 23:38:45",
        "source": "arxiv",
        "comment": "5 pages, 3 figures, conference paper"
    },
    {
        "title": "Improving LLMs' Learning for Coreference Resolution",
        "abstract": "Coreference Resolution (CR) is crucial for many NLP tasks, but existing LLMs\nstruggle with hallucination and under-performance. In this paper, we\ninvestigate the limitations of existing LLM-based approaches to CR-specifically\nthe Question-Answering (QA) Template and Document Template methods and propose\ntwo novel techniques: Reversed Training with Joint Inference and Iterative\nDocument Generation. Our experiments show that Reversed Training improves the\nQA Template method, while Iterative Document Generation eliminates\nhallucinations in the generated source text and boosts coreference resolution.\nIntegrating these methods and techniques offers an effective and robust\nsolution to LLM-based coreference resolution.",
        "url": "http://arxiv.org/abs/2509.11466v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11466v1",
        "arxiv_id": "2509.11466v1",
        "authors": [
            "Yujian Gan",
            "Yuan Liang",
            "Yanni Lin",
            "Juntao Yu",
            "Massimo Poesio"
        ],
        "submitted": "2025-09-14 23:08:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CEMTM: Contextual Embedding-based Multimodal Topic Modeling",
        "abstract": "We introduce CEMTM, a context-enhanced multimodal topic model designed to\ninfer coherent and interpretable topic structures from both short and long\ndocuments containing text and images. CEMTM builds on fine-tuned large vision\nlanguage models (LVLMs) to obtain contextualized embeddings, and employs a\ndistributional attention mechanism to weight token-level contributions to topic\ninference. A reconstruction objective aligns topic-based representations with\nthe document embedding, encouraging semantic consistency across modalities.\nUnlike existing approaches, CEMTM can process multiple images per document\nwithout repeated encoding and maintains interpretability through explicit\nword-topic and document-topic distributions. Extensive experiments on six\nmultimodal benchmarks show that CEMTM consistently outperforms unimodal and\nmultimodal baselines, achieving a remarkable average LLM score of 2.61. Further\nanalysis shows its effectiveness in downstream few-shot retrieval and its\nability to capture visually grounded semantics in complex domains such as\nscientific articles.",
        "url": "http://arxiv.org/abs/2509.11465v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11465v1",
        "arxiv_id": "2509.11465v1",
        "authors": [
            "Amirhossein Abaskohi",
            "Raymond Li",
            "Chuyuan Li",
            "Shafiq Joty",
            "Giuseppe Carenini"
        ],
        "submitted": "2025-09-14 23:07:46",
        "source": "arxiv",
        "comment": "EMNLP 2025"
    },
    {
        "title": "Learning to Optimize Multi-Objective Alignment Through Dynamic Reward Weighting",
        "abstract": "Prior works in multi-objective reinforcement learning typically use linear\nreward scalarization with fixed weights, which provably fail to capture\nnon-convex Pareto fronts and thus yield suboptimal results. This limitation\nbecomes especially critical in online preference alignment for large language\nmodels. Here, stochastic trajectories generated by parameterized policies\ncreate highly non-linear and non-convex mappings from parameters to objectives\nthat no single static weighting scheme can find optimal trade-offs. We address\nthis limitation by introducing dynamic reward weighting, which adaptively\nadjusts reward weights during the online reinforcement learning process. Unlike\nexisting approaches that rely on fixed-weight interpolation, our dynamic\nweighting continuously balances and prioritizes objectives in training,\nfacilitating effective exploration of Pareto fronts in objective space. We\nintroduce two approaches of increasing sophistication and generalizability: (1)\nhypervolume-guided weight adaptation and (2) gradient-based weight\noptimization, offering a versatile toolkit for online multi-objective\nalignment. Our extensive experiments demonstrate their compatibility with\ncommonly used online reinforcement learning algorithms (including GRPO,\nREINFORCE, and RLOO), effectiveness across multiple mathematical reasoning\ndatasets, and applicability to different model families, consistently achieving\nPareto dominant solutions with fewer training steps than fixed-weight linear\nscalarization baselines.",
        "url": "http://arxiv.org/abs/2509.11452v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11452v1",
        "arxiv_id": "2509.11452v1",
        "authors": [
            "Yining Lu",
            "Zilong Wang",
            "Shiyang Li",
            "Xin Liu",
            "Changlong Yu",
            "Qingyu Yin",
            "Zhan Shi",
            "Zixuan Zhang",
            "Meng Jiang"
        ],
        "submitted": "2025-09-14 21:56:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CognitiveSky: Scalable Sentiment and Narrative Analysis for Decentralized Social Media",
        "abstract": "The emergence of decentralized social media platforms presents new\nopportunities and challenges for real-time analysis of public discourse. This\nstudy introduces CognitiveSky, an open-source and scalable framework designed\nfor sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter\nor X.com alternative. By ingesting data through Bluesky's Application\nProgramming Interface (API), CognitiveSky applies transformer-based models to\nannotate large-scale user-generated content and produces structured and\nanalyzable outputs. These summaries drive a dynamic dashboard that visualizes\nevolving patterns in emotion, activity, and conversation topics. Built entirely\non free-tier infrastructure, CognitiveSky achieves both low operational cost\nand high accessibility. While demonstrated here for monitoring mental health\ndiscourse, its modular design enables applications across domains such as\ndisinformation detection, crisis response, and civic sentiment analysis. By\nbridging large language models with decentralized networks, CognitiveSky offers\na transparent, extensible tool for computational social science in an era of\nshifting digital ecosystems.",
        "url": "http://arxiv.org/abs/2509.11444v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11444v1",
        "arxiv_id": "2509.11444v1",
        "authors": [
            "Gaurab Chhetri",
            "Anandi Dutta",
            "Subasish Das"
        ],
        "submitted": "2025-09-14 21:37:24",
        "source": "arxiv",
        "comment": "This is the author's preprint version of a paper accepted for\n  presentation at HICSS 59 (Hawaii International Conference on System\n  Sciences), 2026, Hawaii, USA. The final published version will appear in the\n  official conference proceedings. Conference site: https://hicss.hawaii.edu/"
    },
    {
        "title": "A Transformer-Based Cross-Platform Analysis of Public Discourse on the 15-Minute City Paradigm",
        "abstract": "This study presents the first multi-platform sentiment analysis of public\nopinion on the 15-minute city concept across Twitter, Reddit, and news media.\nUsing compressed transformer models and Llama-3-8B for annotation, we classify\nsentiment across heterogeneous text domains. Our pipeline handles long-form and\nshort-form text, supports consistent annotation, and enables reproducible\nevaluation. We benchmark five models (DistilRoBERTa, DistilBERT, MiniLM,\nELECTRA, TinyBERT) using stratified 5-fold cross-validation, reporting\nF1-score, AUC, and training time. DistilRoBERTa achieved the highest F1\n(0.8292), TinyBERT the best efficiency, and MiniLM the best cross-platform\nconsistency. Results show News data yields inflated performance due to class\nimbalance, Reddit suffers from summarization loss, and Twitter offers moderate\nchallenge. Compressed models perform competitively, challenging assumptions\nthat larger models are necessary. We identify platform-specific trade-offs and\npropose directions for scalable, real-world sentiment classification in urban\nplanning discourse.",
        "url": "http://arxiv.org/abs/2509.11443v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11443v1",
        "arxiv_id": "2509.11443v1",
        "authors": [
            "Gaurab Chhetri",
            "Darrell Anderson",
            "Boniphace Kutela",
            "Subasish Das"
        ],
        "submitted": "2025-09-14 21:36:24",
        "source": "arxiv",
        "comment": "This is the author's preprint version of a paper accepted for\n  presentation at the 24th International Conference on Machine Learning and\n  Applications (ICMLA 2025), December 3-5, 2025, Florida, USA. The final\n  published version will appear in the official IEEE proceedings. Conference\n  site: https://www.icmla-conference.org/icmla25/"
    },
    {
        "title": "Securing AI Agents: Implementing Role-Based Access Control for Industrial Applications",
        "abstract": "The emergence of Large Language Models (LLMs) has significantly advanced\nsolutions across various domains, from political science to software\ndevelopment. However, these models are constrained by their training data,\nwhich is static and limited to information available up to a specific date.\nAdditionally, their generalized nature often necessitates fine-tuning --\nwhether for classification or instructional purposes -- to effectively perform\nspecific downstream tasks. AI agents, leveraging LLMs as their core, mitigate\nsome of these limitations by accessing external tools and real-time data,\nenabling applications such as live weather reporting and data analysis. In\nindustrial settings, AI agents are transforming operations by enhancing\ndecision-making, predictive maintenance, and process optimization. For example,\nin manufacturing, AI agents enable near-autonomous systems that boost\nproductivity and support real-time decision-making. Despite these advancements,\nAI agents remain vulnerable to security threats, including prompt injection\nattacks, which pose significant risks to their integrity and reliability. To\naddress these challenges, this paper proposes a framework for integrating\nRole-Based Access Control (RBAC) into AI agents, providing a robust security\nguardrail. This framework aims to support the effective and scalable deployment\nof AI agents, with a focus on on-premises implementations.",
        "url": "http://arxiv.org/abs/2509.11431v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11431v1",
        "arxiv_id": "2509.11431v1",
        "authors": [
            "Aadil Gani Ganie"
        ],
        "submitted": "2025-09-14 20:58:08",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs",
        "abstract": "Speech tokenization enables discrete representation and facilitates speech\nlanguage modeling. However, existing neural codecs capture low-level acoustic\nfeatures, overlooking the semantic and contextual cues inherent to human\nspeech. While recent efforts introduced semantic representations from\nself-supervised speech models or incorporated contextual representations from\npre-trained language models, challenges remain in aligning and unifying the\nsemantic and contextual representations. We introduce FuseCodec, which unifies\nacoustic, semantic, and contextual representations through strong cross-modal\nalignment and globally informed supervision. We propose three complementary\ntechniques: (i) Latent Representation Fusion, integrating semantic and\ncontextual features directly into the encoder latent space for robust and\nunified representation learning; (ii) Global Semantic-Contextual Supervision,\nsupervising discrete tokens with globally pooled and broadcasted\nrepresentations to enhance temporal consistency and cross-modal alignment; and\n(iii) Temporally Aligned Contextual Supervision, strengthening alignment by\ndynamically matching contextual and speech tokens within a local window for\nfine-grained token-level supervision. We further introduce FuseCodec-TTS,\ndemonstrating our methodology's applicability to zero-shot speech synthesis.\nEmpirically, FuseCodec achieves state-of-the-art performance in LibriSpeech,\nsurpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy,\nperceptual quality, intelligibility, and speaker similarity. Results highlight\nthe effectiveness of contextually and semantically guided tokenization for\nspeech tokenization and downstream tasks. Code and pretrained models are\navailable at https://github.com/mubtasimahasan/FuseCodec.",
        "url": "http://arxiv.org/abs/2509.11425v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11425v1",
        "arxiv_id": "2509.11425v1",
        "authors": [
            "Md Mubtasim Ahasan",
            "Rafat Hasan Khan",
            "Tasnim Mohiuddin",
            "Aman Chadha",
            "Tariq Iqbal",
            "M Ashraful Amin",
            "Amin Ahsan Ali",
            "Md Mofijul Islam",
            "A K M Mahbubur Rahman"
        ],
        "submitted": "2025-09-14 20:35:36",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Trading-R1: Financial Trading with LLM Reasoning via Reinforcement Learning",
        "abstract": "Developing professional, structured reasoning on par with human financial\nanalysts and traders remains a central challenge in AI for finance, where\nmarkets demand interpretability and trust. Traditional time-series models lack\nexplainability, while LLMs face challenges in turning natural-language analysis\ninto disciplined, executable trades. Although reasoning LLMs have advanced in\nstep-by-step planning and verification, their application to risk-sensitive\nfinancial decisions is underexplored. We present Trading-R1, a\nfinancially-aware model that incorporates strategic thinking and planning for\ncomprehensive thesis composition, facts-grounded analysis, and\nvolatility-adjusted decision making. Trading-R1 aligns reasoning with trading\nprinciples through supervised fine-tuning and reinforcement learning with a\nthree-stage easy-to-hard curriculum. Training uses Tauric-TR1-DB, a 100k-sample\ncorpus spanning 18 months, 14 equities, and five heterogeneous financial data\nsources. Evaluated on six major equities and ETFs, Trading-R1 demonstrates\nimproved risk-adjusted returns and lower drawdowns compared to both open-source\nand proprietary instruction-following models as well as reasoning models. The\nsystem generates structured, evidence-based investment theses that support\ndisciplined and interpretable trading decisions. Trading-R1 Terminal will be\nreleased at https://github.com/TauricResearch/Trading-R1.",
        "url": "http://arxiv.org/abs/2509.11420v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11420v1",
        "arxiv_id": "2509.11420v1",
        "authors": [
            "Yijia Xiao",
            "Edward Sun",
            "Tong Chen",
            "Fang Wu",
            "Di Luo",
            "Wei Wang"
        ],
        "submitted": "2025-09-14 20:13:41",
        "source": "arxiv",
        "comment": "Tauric Research: https://github.com/TauricResearch"
    },
    {
        "title": "Continually Adding New Languages to Multilingual Language Models",
        "abstract": "Multilingual language models are trained on a fixed set of languages, and to\nsupport new languages, the models need to be retrained from scratch. This is an\nexpensive endeavor and is often infeasible, as model developers tend not to\nrelease their pre-training data. Naive approaches, such as continued\npretraining, suffer from catastrophic forgetting; however, mitigation\nstrategies like experience replay cannot be applied due to the lack of original\npretraining data. In this work, we investigate the problem of continually\nadding new languages to a multilingual model, assuming access to pretraining\ndata in only the target languages. We explore multiple approaches to address\nthis problem and propose Layer-Selective LoRA (LayRA), which adds Low-Rank\nAdapters (LoRA) to selected initial and final layers while keeping the rest of\nthe model frozen. LayRA builds on two insights: (1) LoRA reduces forgetting,\nand (2) multilingual models encode inputs in the source language in the initial\nlayers, reason in English in intermediate layers, and translate back to the\nsource language in final layers. We experiment with adding multiple\ncombinations of Galician, Swahili, and Urdu to pretrained language models and\nevaluate each method on diverse multilingual tasks. We find that LayRA provides\nthe overall best tradeoff between preserving models' capabilities in previously\nsupported languages, while being competitive with existing approaches such as\nLoRA in learning new languages. We also demonstrate that using model\narithmetic, the adapted models can be equipped with strong instruction\nfollowing abilities without access to any instruction tuning data in the target\nlanguages.",
        "url": "http://arxiv.org/abs/2509.11414v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11414v1",
        "arxiv_id": "2509.11414v1",
        "authors": [
            "Abraham Toluwase Owodunni",
            "Sachin Kumar"
        ],
        "submitted": "2025-09-14 20:08:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Transformer Enhanced Relation Classification: A Comparative Analysis of Contextuality, Data Efficiency and Sequence Complexity",
        "abstract": "In the era of large language model, relation extraction (RE) plays an\nimportant role in information extraction through the transformation of\nunstructured raw text into structured data (Wadhwa et al., 2023). In this\npaper, we systematically compare the performance of deep supervised learning\napproaches without transformers and those with transformers. We used a series\nof non-transformer architectures such as PA-LSTM(Zhang et al., 2017),\nC-GCN(Zhang et al., 2018), and AGGCN(attention guide GCN)(Guo et al., 2019),\nand a series of transformer architectures such as BERT, RoBERTa, and R-BERT(Wu\nand He, 2019). Our comparison included traditional metrics like micro F1, as\nwell as evaluations in different scenarios, varying sentence lengths, and\ndifferent percentages of the dataset for training. Our experiments were\nconducted on TACRED, TACREV, and RE-TACRED. The results show that\ntransformer-based models outperform non-transformer models, achieving micro F1\nscores of 80-90% compared to 64-67% for non-transformer models. Additionally,\nwe briefly review the research journey in supervised relation classification\nand discuss the role and current status of large language models (LLMs) in\nrelation extraction.",
        "url": "http://arxiv.org/abs/2509.11374v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11374v1",
        "arxiv_id": "2509.11374v1",
        "authors": [
            "Bowen Jing",
            "Yang Cui",
            "Tianpeng Huang"
        ],
        "submitted": "2025-09-14 18:11:31",
        "source": "arxiv",
        "comment": null
    }
]