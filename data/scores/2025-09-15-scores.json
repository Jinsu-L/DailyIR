[
    {
        "title": "Do Large Language Models Favor Recent Content? A Study on Recency Bias in LLM-Based Reranking",
        "abstract": "Large language models (LLMs) are increasingly deployed in information\nsystems, including being used as second-stage rerankers in information\nretrieval pipelines, yet their susceptibility to recency bias has received\nlittle attention. We investigate whether LLMs implicitly favour newer documents\nby prepending artificial publication dates to passages in the TREC Deep\nLearning passage retrieval collections in 2021 (DL21) and 2022 (DL22). Across\nseven models, GPT-3.5-turbo, GPT-4o, GPT-4, LLaMA-3 8B/70B, and Qwen-2.5\n7B/72B, \"fresh\" passages are consistently promoted, shifting the Top-10's mean\npublication year forward by up to 4.78 years and moving individual items by as\nmany as 95 ranks in our listwise reranking experiments. Although larger models\nattenuate the effect, none eliminate it. We also observe that the preference of\nLLMs between two passages with an identical relevance level can be reversed by\nup to 25% on average after date injection in our pairwise preference\nexperiments. These findings provide quantitative evidence of a pervasive\nrecency bias in LLMs and highlight the importance of effective bias-mitigation\nstrategies.",
        "url": "http://arxiv.org/abs/2509.11353v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11353v1",
        "arxiv_id": "2509.11353v1",
        "authors": [
            "Hanpei Fang",
            "Sijie Tao",
            "Nuo Chen",
            "Kai-Xin Chang",
            "Tetsuya Sakai"
        ],
        "submitted": "2025-09-14 17:11:07",
        "source": "arxiv",
        "comment": null,
        "score": 25,
        "keyword_reasons": [
            "Found 'passage retrieval' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'listwise' (score: +3)",
            "Found 'pairwise' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'trec' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your interests in Information Retrieval, particularly in the context of query understanding and ranking models. The study of recency bias in Large Language Models (LLMs) is a significant aspect of real-time relevance optimization, aligning with your focus on deep semantic understanding. Although the paper's primary focus is on recommender systems, the findings have implications for information retrieval pipelines."
    },
    {
        "title": "FinGEAR: Financial Mapping-Guided Enhanced Answer Retrieval",
        "abstract": "Financial disclosures such as 10-K filings present challenging retrieval\nproblems due to their length, regulatory section hierarchy, and domain-specific\nlanguage, which standard retrieval-augmented generation (RAG) models underuse.\nWe introduce FinGEAR (Financial Mapping-Guided Enhanced Answer Retrieval), a\nretrieval framework tailored to financial documents. FinGEAR combines a finance\nlexicon for Item-level guidance (FLAM), dual hierarchical indices for\nwithin-Item search (Summary Tree and Question Tree), and a two-stage\ncross-encoder reranker. This design aligns retrieval with disclosure structure\nand terminology, enabling fine-grained, query-aware context selection.\nEvaluated on full 10-Ks with queries aligned to the FinQA dataset, FinGEAR\ndelivers consistent gains in precision, recall, F1, and relevancy, improving F1\nby up to 56.7% over flat RAG, 12.5% over graph-based RAGs, and 217.6% over\nprior tree-based systems, while also increasing downstream answer accuracy with\na fixed reader. By jointly modeling section hierarchy and domain lexicon\nsignals, FinGEAR improves retrieval fidelity and provides a practical\nfoundation for high-stakes financial analysis.",
        "url": "http://arxiv.org/abs/2509.12042v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12042v1",
        "arxiv_id": "2509.12042v1",
        "authors": [
            "Ying Li",
            "Mengyu Wang",
            "Miguel de Carvalho",
            "Sotirios Sabanis",
            "Tiejun Ma"
        ],
        "submitted": "2025-09-15 15:25:26",
        "source": "arxiv",
        "comment": null,
        "score": 15,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to Information Retrieval, particularly in the context of query understanding and ranking models. The focus on financial documents and the introduction of a tailored retrieval framework aligns with the user's interests in deep semantic understanding and real-time relevance optimization. However, the specific domain (financial documents) is somewhat narrow compared to the user's broader interests in e-commerce and general IR."
    },
    {
        "title": "MillStone: How Open-Minded Are LLMs?",
        "abstract": "Large language models equipped with Web search, information retrieval tools,\nand other agentic capabilities are beginning to supplant traditional search\nengines. As users start to rely on LLMs for information on many topics,\nincluding controversial and debatable issues, it is important to understand how\nthe stances and opinions expressed in LLM outputs are influenced by the\ndocuments they use as their information sources.\n  In this paper, we present MillStone, the first benchmark that aims to\nsystematically measure the effect of external arguments on the stances that\nLLMs take on controversial issues (not all of them political). We apply\nMillStone to nine leading LLMs and measure how ``open-minded'' they are to\narguments supporting opposite sides of these issues, whether different LLMs\nagree with each other, which arguments LLMs find most persuasive, and whether\nthese arguments are the same for different LLMs.\n  In general, we find that LLMs are open-minded on most issues. An\nauthoritative source of information can easily sway an LLM's stance,\nhighlighting the importance of source selection and the risk that LLM-based\ninformation retrieval and search systems can be manipulated.",
        "url": "http://arxiv.org/abs/2509.11967v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11967v1",
        "arxiv_id": "2509.11967v1",
        "authors": [
            "Harold Triedman",
            "Vitaly Shmatikov"
        ],
        "submitted": "2025-09-15 14:18:51",
        "source": "arxiv",
        "comment": "19 pages, 7 tables, 7 figures",
        "score": 7,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'web search' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 7,
        "llm_reason": "This paper explores the use of Large Language Models (LLMs) in search and information retrieval, which aligns with your interests in IR and Search technologies. However, the focus is on the stances and opinions expressed by LLMs, rather than query understanding, ranking models, or user behavior modeling, which are your core research themes."
    },
    {
        "title": "Membership Inference Attacks on Recommender System: A Survey",
        "abstract": "Recommender systems (RecSys) have been widely applied to various\napplications, including E-commerce, finance, healthcare, social media and have\nbecome increasingly influential in shaping user behavior and decision-making,\nhighlighting their growing impact in various domains. However, recent studies\nhave shown that RecSys are vulnerable to membership inference attacks (MIAs),\nwhich aim to infer whether user interaction record was used to train a target\nmodel or not. MIAs on RecSys models can directly lead to a privacy breach. For\nexample, via identifying the fact that a purchase record that has been used to\ntrain a RecSys associated with a specific user, an attacker can infer that\nuser's special quirks. In recent years, MIAs have been shown to be effective on\nother ML tasks, e.g., classification models and natural language processing.\nHowever, traditional MIAs are ill-suited for RecSys due to the unseen posterior\nprobability. Although MIAs on RecSys form a newly emerging and rapidly growing\nresearch area, there has been no systematic survey on this topic yet. In this\narticle, we conduct the first comprehensive survey on RecSys MIAs. This survey\noffers a comprehensive review of the latest advancements in RecSys MIAs,\nexploring the design principles, challenges, attack and defense associated with\nthis emerging field. We provide a unified taxonomy that categorizes different\nRecSys MIAs based on their characterizations and discuss their pros and cons.\nBased on the limitations and gaps identified in this survey, we point out\nseveral promising future research directions to inspire the researchers who\nwish to follow this area. This survey not only serves as a reference for the\nresearch community but also provides a clear description for researchers\noutside this research domain.",
        "url": "http://arxiv.org/abs/2509.11080v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11080v1",
        "arxiv_id": "2509.11080v1",
        "authors": [
            "Jiajie He",
            "Yuechun Gu",
            "Keke Chen",
            "Xintong Chen"
        ],
        "submitted": "2025-09-14 04:06:03",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'user behavior' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'search' (score: +1)",
            "Found 'recsys' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is primarily focused on recommender systems and membership inference attacks, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the topic is not directly aligned with your core research themes, such as query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Unsupervised Candidate Ranking for Lexical Substitution via Holistic Sentence Semantics",
        "abstract": "A key subtask in lexical substitution is ranking the given candidate words. A\ncommon approach is to replace the target word with a candidate in the original\nsentence and feed the modified sentence into a model to capture semantic\ndifferences before and after substitution. However, effectively modeling the\nbidirectional influence of candidate substitution on both the target word and\nits context remains challenging. Existing methods often focus solely on\nsemantic changes at the target position or rely on parameter tuning over\nmultiple evaluation metrics, making it difficult to accurately characterize\nsemantic variation. To address this, we investigate two approaches: one based\non attention weights and another leveraging the more interpretable integrated\ngradients method, both designed to measure the influence of context tokens on\nthe target token and to rank candidates by incorporating semantic similarity\nbetween the original and substituted sentences. Experiments on the LS07 and\nSWORDS datasets demonstrate that both approaches improve ranking performance.",
        "url": "http://arxiv.org/abs/2509.11513v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11513v1",
        "arxiv_id": "2509.11513v1",
        "authors": [
            "Zhongyang Hu",
            "Naijie Gu",
            "Xiangzhi Tao",
            "Tianhui Gu",
            "Yibing Zhou"
        ],
        "submitted": "2025-09-15 01:57:09",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores ranking models for lexical substitution, which is somewhat related to the user's interests in ranking models and query understanding. However, the focus on lexical substitution and sentence semantics is not directly aligned with the user's primary research themes in information retrieval and search technologies."
    },
    {
        "title": "AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for Multilingual Claim Normalization",
        "abstract": "Claim normalization, the transformation of informal social media posts into\nconcise, self-contained statements, is a crucial step in automated\nfact-checking pipelines. This paper details our submission to the CLEF-2025\nCheckThat! Task~2, which challenges systems to perform claim normalization\nacross twenty languages, divided into thirteen supervised (high-resource) and\nseven zero-shot (no training data) tracks.\n  Our approach, leveraging fine-tuned Small Language Models (SLMs) for\nsupervised languages and Large Language Model (LLM) prompting for zero-shot\nscenarios, achieved podium positions (top three) in fifteen of the twenty\nlanguages. Notably, this included second-place rankings in eight languages,\nfive of which were among the seven designated zero-shot languages, underscoring\nthe effectiveness of our LLM-based zero-shot strategy. For Portuguese, our\ninitial development language, our system achieved an average METEOR score of\n0.5290, ranking third. All implementation artifacts, including inference,\ntraining, evaluation scripts, and prompt configurations, are publicly available\nat https://github.com/ju-resplande/checkthat2025_normalization.",
        "url": "http://arxiv.org/abs/2509.11496v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11496v1",
        "arxiv_id": "2509.11496v1",
        "authors": [
            "Fabrycio Leite Nakano Almada",
            "Kauan Divino Pouso Mariano",
            "Maykon Adriell Dutra",
            "Victor Emanuel da Silva Monteiro",
            "Juliana Resplande Sant'Anna Gomes",
            "Arlindo Rodrigues Galvão Filho",
            "Anderson da Silva Soares"
        ],
        "submitted": "2025-09-15 01:19:49",
        "source": "arxiv",
        "comment": "15 pages, 2 figures",
        "score": 6,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to the user's interests in Information Retrieval and Natural Language Processing, as it involves claim normalization and fact-checking pipelines. However, the focus on multilingual claim normalization and Large Language Model (LLM) prompting is not directly aligned with the user's primary research themes of query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Query-Focused Extractive Summarization for Sentiment Explanation",
        "abstract": "Constructive analysis of feedback from clients often requires determining the\ncause of their sentiment from a substantial amount of text documents. To assist\nand improve the productivity of such endeavors, we leverage the task of\nQuery-Focused Summarization (QFS). Models of this task are often impeded by the\nlinguistic dissonance between the query and the source documents. We propose\nand substantiate a multi-bias framework to help bridge this gap at a\ndomain-agnostic, generic level; we then formulate specialized approaches for\nthe problem of sentiment explanation through sentiment-based biases and query\nexpansion. We achieve experimental results outperforming baseline models on a\nreal-world proprietary sentiment-aware QFS dataset.",
        "url": "http://arxiv.org/abs/2509.11989v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11989v1",
        "arxiv_id": "2509.11989v1",
        "authors": [
            "Ahmed Moubtahij",
            "Sylvie Ratté",
            "Yazid Attabi",
            "Maxime Dumas"
        ],
        "submitted": "2025-09-15 14:41:18",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper on Query-Focused Summarization for Sentiment Explanation is somewhat related to your interests in Information Retrieval, particularly in query understanding and ranking models. However, it focuses more on summarization and sentiment analysis, which, while related to your broader interests in NLP and data mining, does not directly align with your primary focus on query understanding and ranking models in the context of search technologies."
    },
    {
        "title": "MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues",
        "abstract": "Memory extraction is crucial for maintaining coherent ultra-long dialogues in\nhuman-robot role-playing scenarios. However, existing methods often exhibit\nuncontrolled memory growth. To address this, we propose MOOM, the first\ndual-branch memory plugin that leverages literary theory by modeling plot\ndevelopment and character portrayal as core storytelling elements.\nSpecifically, one branch summarizes plot conflicts across multiple time scales,\nwhile the other extracts the user's character profile. MOOM further integrates\na forgetting mechanism, inspired by the ``competition-inhibition'' memory\ntheory, to constrain memory capacity and mitigate uncontrolled growth.\nFurthermore, we present ZH-4O, a Chinese ultra-long dialogue dataset\nspecifically designed for role-playing, featuring dialogues that average 600\nturns and include manually annotated memory information. Experimental results\ndemonstrate that MOOM outperforms all state-of-the-art memory extraction\nmethods, requiring fewer large language model invocations while maintaining a\ncontrollable memory capacity.",
        "url": "http://arxiv.org/abs/2509.11860v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11860v1",
        "arxiv_id": "2509.11860v1",
        "authors": [
            "Weishu Chen",
            "Jinyi Tang",
            "Zhouhui Hou",
            "Shihao Han",
            "Mingjie Zhan",
            "Zhiyuan Huang",
            "Delong Liu",
            "Jiawei Guo",
            "Zhicheng Zhao",
            "Fei Su"
        ],
        "submitted": "2025-09-15 12:35:14",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'ltr' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on memory extraction in human-robot role-playing scenarios, leveraging literary theory and a forgetting mechanism. While it involves NLP and deep semantic understanding, it is not directly related to information retrieval, search technologies, or user behavior modeling, which are the core areas of your research interests."
    },
    {
        "title": "A Dynamic Knowledge Update-Driven Model with Large Language Models for Fake News Detection",
        "abstract": "As the Internet and social media evolve rapidly, distinguishing credible news\nfrom a vast amount of complex information poses a significant challenge. Due to\nthe suddenness and instability of news events, the authenticity labels of news\ncan potentially shift as events develop, making it crucial for fake news\ndetection to obtain the latest event updates. Existing methods employ\nretrieval-augmented generation to fill knowledge gaps, but they suffer from\nissues such as insufficient credibility of retrieved content and interference\nfrom noisy information. We propose a dynamic knowledge update-driven model for\nfake news detection (DYNAMO), which leverages knowledge graphs to achieve\ncontinuous updating of new knowledge and integrates with large language models\nto fulfill dual functions: news authenticity detection and verification of new\nknowledge correctness, solving the two key problems of ensuring the\nauthenticity of new knowledge and deeply mining news semantics. Specifically,\nwe first construct a news-domain-specific knowledge graph. Then, we use Monte\nCarlo Tree Search to decompose complex news and verify them step by step.\nFinally, we extract and update new knowledge from verified real news texts and\nreasoning paths. Experimental results demonstrate that DYNAMO achieves the best\nperformance on two real-world datasets.",
        "url": "http://arxiv.org/abs/2509.11687v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11687v1",
        "arxiv_id": "2509.11687v1",
        "authors": [
            "Di Jin",
            "Jun Yang",
            "Xiaobao Wang",
            "Junwei Zhang",
            "Shuqi Li",
            "Dongxiao He"
        ],
        "submitted": "2025-09-15 08:38:08",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on fake news detection, which is somewhat related to information retrieval and NLP. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The use of large language models and knowledge graphs is relevant to IR and NLP, but the application is specific to fake news detection."
    },
    {
        "title": "RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing",
        "abstract": "Role-playing Large language models (LLMs) are increasingly deployed in\nhigh-stakes domains such as healthcare, education, and governance, where\nfailures can directly impact user trust and well-being. A cost effective\nparadigm for LLM role-playing is few-shot learning, but existing approaches\noften cause models to break character in unexpected and potentially harmful\nways, especially when interacting with hostile users. Inspired by\nRetrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a\ntext retrieval problem and propose a new prompting framework called\nRAGs-to-Riches, which leverages curated reference demonstrations to condition\nLLM responses. We evaluate our framework with LLM-as-a-judge preference voting\nand introduce two novel token-level ROUGE metrics: Intersection over Output\n(IOO) to quantity how much an LLM improvises and Intersection over References\n(IOR) to measure few-shot demonstrations utilization rate during the evaluation\ntasks. When simulating interactions with a hostile user, our prompting strategy\nincorporates in its responses during inference an average of 35% more tokens\nfrom the reference demonstrations. As a result, across 453 role-playing\ninteractions, our models are consistently judged as being more authentic, and\nremain in-character more often than zero-shot and in-context Learning (ICL)\nmethods. Our method presents a scalable strategy for building robust,\nhuman-aligned LLM role-playing frameworks.",
        "url": "http://arxiv.org/abs/2509.12168v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12168v1",
        "arxiv_id": "2509.12168v1",
        "authors": [
            "Timothy Rupprecht",
            "Enfu Nan",
            "Arash Akbari",
            "Arman Akbari",
            "Lei Lu",
            "Priyanka Maan",
            "Sean Duffy",
            "Pu Zhao",
            "Yumei He",
            "David Kaeli",
            "Yanzhi Wang"
        ],
        "submitted": "2025-09-15 17:31:15",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Large Language Model role-playing, using few-shot learning and a text retrieval approach. While it involves some aspects of Natural Language Processing, it does not directly relate to Information Retrieval, query understanding, ranking models, or user behavior modeling, which are core areas of interest."
    },
    {
        "title": "XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with Finetuned Transformers and Prompt-Based Inference with Large Language Models",
        "abstract": "This notebook reports the XplaiNLP submission to the CheckThat! 2025 shared\ntask on multilingual subjectivity detection. We evaluate two approaches: (1)\nsupervised fine-tuning of transformer encoders, EuroBERT, XLM-RoBERTa, and\nGerman-BERT, on monolingual and machine-translated training data; and (2)\nzero-shot prompting using two LLMs: o3-mini for Annotation (rule-based\nlabelling) and gpt-4.1-mini for DoubleDown (contrastive rewriting) and\nPerspective (comparative reasoning). The Annotation Approach achieves 1st place\nin the Italian monolingual subtask with an F_1 score of 0.8104, outperforming\nthe baseline of 0.6941. In the Romanian zero-shot setting, the fine-tuned\nXLM-RoBERTa model obtains an F_1 score of 0.7917, ranking 3rd and exceeding the\nbaseline of 0.6461. The same model also performs reliably in the multilingual\ntask and improves over the baseline in Greek. For German, a German-BERT model\nfine-tuned on translated training data from typologically related languages\nyields competitive performance over the baseline. In contrast, performance in\nthe Ukrainian and Polish zero-shot settings falls slightly below the respective\nbaselines, reflecting the challenge of generalization in low-resource\ncross-lingual scenarios.",
        "url": "http://arxiv.org/abs/2509.12130v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12130v1",
        "arxiv_id": "2509.12130v1",
        "authors": [
            "Ariana Sahitaj",
            "Jiaao Li",
            "Pia Wenzel Neves",
            "Fedor Splitt",
            "Premtim Sahitaj",
            "Charlott Jakob",
            "Veronika Solopova",
            "Vera Schmitt"
        ],
        "submitted": "2025-09-15 16:53:41",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is primarily focused on multilingual subjectivity detection using NLP techniques, which is somewhat related to the user's interests in NLP. However, it does not align with the user's core research themes in Information Retrieval, query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "SAQ: Pushing the Limits of Vector Quantization through Code Adjustment and Dimension Segmentation",
        "abstract": "Approximate Nearest Neighbor Search (ANNS) plays a critical role in\napplications such as search engines, recommender systems, and RAG for LLMs.\nVector quantization (VQ), a crucial technique for ANNS, is commonly used to\nreduce space overhead and accelerate distance computations. However, despite\nsignificant research advances, state-of-the-art VQ methods still face\nchallenges in balancing encoding efficiency and quantization accuracy. To\naddress these limitations, we propose a novel VQ method called SAQ. To improve\naccuracy, SAQ employs a new dimension segmentation technique to strategically\npartition PCA-projected vectors into segments along their dimensions. By\nprioritizing leading dimension segments with larger magnitudes, SAQ allocates\nmore bits to high-impact segments, optimizing the use of the available space\nquota. An efficient dynamic programming algorithm is developed to optimize\ndimension segmentation and bit allocation, ensuring minimal quantization error.\nTo speed up vector encoding, SAQ devises a code adjustment technique to first\nquantize each dimension independently and then progressively refine quantized\nvectors using a coordinate-descent-like approach to avoid exhaustive\nenumeration. Extensive experiments demonstrate SAQ's superiority over classical\nmethods (e.g., PQ, PCA) and recent state-of-the-art approaches (e.g., LVQ,\nExtended RabitQ). SAQ achieves up to 80% reduction in quantization error and\naccelerates encoding speed by over 80x compared to Extended RabitQ.",
        "url": "http://arxiv.org/abs/2509.12086v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12086v1",
        "arxiv_id": "2509.12086v1",
        "authors": [
            "Hui Li",
            "Shiyuan Deng",
            "Xiao Yan",
            "Xiangyu Zhi",
            "James Cheng"
        ],
        "submitted": "2025-09-15 16:14:05",
        "source": "arxiv",
        "comment": "13 pages, 12 figures, accepted by SIGMOD",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Vector Quantization, a technique used in Approximate Nearest Neighbor Search, which is relevant to recommender systems. However, it does not directly relate to Information Retrieval, query understanding, or ranking models, which are core areas of your research interests."
    },
    {
        "title": "Analyzing Information-Seeking Behaviors in a Hakka AI Chatbot: A Cognitive-Pragmatic Study",
        "abstract": "With many endangered languages at risk of disappearing, efforts to preserve\nthem now rely more than ever on using technology alongside culturally informed\nteaching strategies. This study examines user behaviors in TALKA, a generative\nAI-powered chatbot designed for Hakka language engagement, by employing a\ndual-layered analytical framework grounded in Bloom's Taxonomy of cognitive\nprocesses and dialogue act categorization. We analyzed 7,077 user utterances,\neach carefully annotated according to six cognitive levels and eleven dialogue\nact types. These included a variety of functions, such as asking for\ninformation, requesting translations, making cultural inquiries, and using\nlanguage creatively. Pragmatic classifications further highlight how different\ntypes of dialogue acts--such as feedback, control commands, and social\ngreetings--align with specific cognitive intentions. The results suggest that\ngenerative AI chatbots can support language learning in meaningful\nways--especially when they are designed with an understanding of how users\nthink and communicate. They may also help learners express themselves more\nconfidently and connect with their cultural identity. The TALKA case provides\nempirical insights into how AI-mediated dialogue facilitates cognitive\ndevelopment in low-resource language learners, as well as pragmatic negotiation\nand socio-cultural affiliation. By focusing on AI-assisted language learning,\nthis study offers new insights into how technology can support language\npreservation and educational practice.",
        "url": "http://arxiv.org/abs/2509.11591v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11591v1",
        "arxiv_id": "2509.11591v1",
        "authors": [
            "Chu-Hsuan Lee",
            "Chen-Chi Chang",
            "Hung-Shin Lee",
            "Yun-Hsiang Hsu",
            "Ching-Yuan Chen"
        ],
        "submitted": "2025-09-15 05:18:17",
        "source": "arxiv",
        "comment": "Accepted to HICSS-59 (2026)",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'user behavior' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to Information Retrieval, but its focus on AI-assisted language learning and cognitive development in low-resource language learners is not directly aligned with the user's core research themes. While it involves a generative AI chatbot, the study's emphasis on language preservation and educational practice is more closely related to NLP and data mining, but not specifically to query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances the response capabilities of\nlanguage models by integrating external knowledge sources. However, document\nchunking as an important part of RAG system often lacks effective evaluation\ntools. This paper first analyzes why existing RAG evaluation benchmarks are\ninadequate for assessing document chunking quality, specifically due to\nevidence sparsity. Based on this conclusion, we propose HiCBench, which\nincludes manually annotated multi-level document chunking points, synthesized\nevidence-dense quetion answer(QA) pairs, and their corresponding evidence\nsources. Additionally, we introduce the HiChunk framework, a multi-level\ndocument structuring framework based on fine-tuned LLMs, combined with the\nAuto-Merge retrieval algorithm to improve retrieval quality. Experiments\ndemonstrate that HiCBench effectively evaluates the impact of different\nchunking methods across the entire RAG pipeline. Moreover, HiChunk achieves\nbetter chunking quality within reasonable time consumption, thereby enhancing\nthe overall performance of RAG systems.",
        "url": "http://arxiv.org/abs/2509.11552v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11552v1",
        "arxiv_id": "2509.11552v1",
        "authors": [
            "Wensheng Lu",
            "Keyu Chen",
            "Ruizhi Qiao",
            "Xing Sun"
        ],
        "submitted": "2025-09-15 03:32:50",
        "source": "arxiv",
        "comment": "17 pages, 5 figures, 6 tables",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of Retrieval-Augmented Generation and document chunking. However, it does not directly focus on query understanding, ranking models, or user behavior modeling, which are your core areas of interest."
    },
    {
        "title": "CEMTM: Contextual Embedding-based Multimodal Topic Modeling",
        "abstract": "We introduce CEMTM, a context-enhanced multimodal topic model designed to\ninfer coherent and interpretable topic structures from both short and long\ndocuments containing text and images. CEMTM builds on fine-tuned large vision\nlanguage models (LVLMs) to obtain contextualized embeddings, and employs a\ndistributional attention mechanism to weight token-level contributions to topic\ninference. A reconstruction objective aligns topic-based representations with\nthe document embedding, encouraging semantic consistency across modalities.\nUnlike existing approaches, CEMTM can process multiple images per document\nwithout repeated encoding and maintains interpretability through explicit\nword-topic and document-topic distributions. Extensive experiments on six\nmultimodal benchmarks show that CEMTM consistently outperforms unimodal and\nmultimodal baselines, achieving a remarkable average LLM score of 2.61. Further\nanalysis shows its effectiveness in downstream few-shot retrieval and its\nability to capture visually grounded semantics in complex domains such as\nscientific articles.",
        "url": "http://arxiv.org/abs/2509.11465v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11465v1",
        "arxiv_id": "2509.11465v1",
        "authors": [
            "Amirhossein Abaskohi",
            "Raymond Li",
            "Chuyuan Li",
            "Shafiq Joty",
            "Giuseppe Carenini"
        ],
        "submitted": "2025-09-14 23:07:46",
        "source": "arxiv",
        "comment": "EMNLP 2025",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores multimodal topic modeling, which is somewhat related to information retrieval, particularly in areas that require deep semantic understanding. However, the focus on multimodal topic modeling and vision-language models is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is limited to the broader context of information retrieval and NLP."
    },
    {
        "title": "An Incentive-Compatible Reward Sharing Mechanism for Mitigating Mirroring Attacks in Decentralized Data-Feed Systems",
        "abstract": "Decentralized data-feed systems enable blockchain-based smart contracts to\naccess off-chain information by aggregating values from multiple oracles. To\nimprove accuracy, these systems typically use an aggregation function, such as\nmajority voting, to consolidate the inputs they receive from oracles and make a\ndecision. Depending on the final decision and the values reported by the\noracles, the participating oracles are compensated through shared rewards.\nHowever, such incentive mechanisms are vulnerable to mirroring attacks, where a\nsingle user controls multiple oracles to bias the decision of the aggregation\nfunction and maximize rewards. This paper analyzes the impact of mirroring\nattacks on the reliability and dependability of majority voting-based data-feed\nsystems. We demonstrate how existing incentive mechanisms can unintentionally\nencourage rational users to implement such attacks. To address this, we propose\na new incentive mechanism that discourages Sybil behavior. We prove that the\nproposed mechanism leads to a Nash Equilibrium in which each user operates only\none oracle. Finally, we discuss the practical implementation of the proposed\nincentive mechanism and provide numerical examples to demonstrate its\neffectiveness.",
        "url": "http://arxiv.org/abs/2509.11294v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11294v1",
        "arxiv_id": "2509.11294v1",
        "authors": [
            "Sina Aeeneh",
            "Nikola Zlatanov",
            "Jiangshan Yu"
        ],
        "submitted": "2025-09-14 14:33:17",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The topic of decentralized data-feed systems and mirroring attacks is unrelated to your areas of focus."
    },
    {
        "title": "Text2Mem: A Unified Memory Operation Language for Memory Operating System",
        "abstract": "Large language model agents increasingly depend on memory to sustain long\nhorizon interaction, but existing frameworks remain limited. Most expose only a\nfew basic primitives such as encode, retrieve, and delete, while higher order\noperations like merge, promote, demote, split, lock, and expire are missing or\ninconsistently supported. Moreover, there is no formal and executable\nspecification for memory commands, leaving scope and lifecycle rules implicit\nand causing unpredictable behavior across systems. We introduce Text2Mem, a\nunified memory operation language that provides a standardized pathway from\nnatural language to reliable execution. Text2Mem defines a compact yet\nexpressive operation set aligned with encoding, storage, and retrieval. Each\ninstruction is represented as a JSON based schema instance with required fields\nand semantic invariants, which a parser transforms into typed operation objects\nwith normalized parameters. A validator ensures correctness before execution,\nwhile adapters map typed objects either to a SQL prototype backend or to real\nmemory frameworks. Model based services such as embeddings or summarization are\nintegrated when required. All results are returned through a unified execution\ncontract. This design ensures safety, determinism, and portability across\nheterogeneous backends. We also outline Text2Mem Bench, a planned benchmark\nthat separates schema generation from backend execution to enable systematic\nevaluation. Together, these components establish the first standardized\nfoundation for memory control in agents.",
        "url": "http://arxiv.org/abs/2509.11145v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11145v1",
        "arxiv_id": "2509.11145v1",
        "authors": [
            "Felix Wang",
            "Boyu Chen",
            "Kerun Xu",
            "Bo Tang",
            "Feiyu Xiong",
            "Zhiyu Li"
        ],
        "submitted": "2025-09-14 07:30:09",
        "source": "arxiv",
        "comment": "11 pages, 3 figures",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be unrelated to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus on memory operation languages and memory control in agents does not align with your areas of expertise."
    },
    {
        "title": "Understanding the Information Cocoon: A Multidimensional Assessment and Analysis of News Recommendation Systems",
        "abstract": "Personalized news recommendation systems inadvertently create information\ncocoons--homogeneous information bubbles that reinforce user biases and amplify\nsocietal polarization. To address the lack of comprehensive assessment\nframeworks in prior research, we propose a multidimensional analysis that\nevaluates cocoons through dual perspectives: (1) Individual homogenization via\ntopic diversity (including the number of topic categories and category\ninformation entropy) and click repetition; (2) Group polarization via network\ndensity and community openness. Through multi-round experiments on real-world\ndatasets, we benchmark seven algorithms and reveal critical insights.\nFurthermore, we design five lightweight mitigation strategies. This work\nestablishes the first unified metric framework for information cocoons and\ndelivers deployable solutions for ethical recommendation systems.",
        "url": "http://arxiv.org/abs/2509.11139v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11139v1",
        "arxiv_id": "2509.11139v1",
        "authors": [
            "Xin Wang",
            "Xiaowen Huang",
            "Jitao Sang"
        ],
        "submitted": "2025-09-14 07:17:26",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'click' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores news recommendation systems and their potential to create information cocoons, which is somewhat related to information retrieval and user behavior modeling. However, the focus is on recommender systems rather than search technologies, and the emphasis is on societal implications rather than deep semantic understanding or real-time relevance optimization."
    },
    {
        "title": "SPARK: Adaptive Low-Rank Knowledge Graph Modeling in Hybrid Geometric Spaces for Recommendation",
        "abstract": "Knowledge Graphs (KGs) enhance recommender systems but face challenges from\ninherent noise, sparsity, and Euclidean geometry's inadequacy for complex\nrelational structures, critically impairing representation learning, especially\nfor long-tail entities. Existing methods also often lack adaptive multi-source\nsignal fusion tailored to item popularity. This paper introduces SPARK, a novel\nmulti-stage framework systematically tackling these issues. SPARK first employs\nTucker low-rank decomposition to denoise KGs and generate robust entity\nrepresentations. Subsequently, an SVD-initialized hybrid geometric GNN\nconcurrently learns representations in Euclidean and Hyperbolic spaces; the\nlatter is strategically leveraged for its aptitude in modeling hierarchical\nstructures, effectively capturing semantic features of sparse, long-tail items.\nA core contribution is an item popularity-aware adaptive fusion strategy that\ndynamically weights signals from collaborative filtering, refined KG\nembeddings, and diverse geometric spaces for precise modeling of both\nmainstream and long-tail items. Finally, contrastive learning aligns these\nmulti-source representations. Extensive experiments demonstrate SPARK's\nsignificant superiority over state-of-the-art methods, particularly in\nimproving long-tail item recommendation, offering a robust, principled approach\nto knowledge-enhanced recommendation. Implementation code is available at\nhttps://github.com/Applied-Machine-Learning-Lab/SPARK.",
        "url": "http://arxiv.org/abs/2509.11094v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11094v1",
        "arxiv_id": "2509.11094v1",
        "authors": [
            "Binhao Wang",
            "Yutian Xiao",
            "Maolin Wang",
            "Zhiqi Li",
            "Tianshuo Wei",
            "Ruocheng Guo",
            "Xiangyu Zhao"
        ],
        "submitted": "2025-09-14 05:12:46",
        "source": "arxiv",
        "comment": "Accepted by CIKM' 25",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on knowledge graph modeling and recommendation systems, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the emphasis on recommender systems and knowledge graph modeling is not a central match for your primary focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "AEFS: Adaptive Early Feature Selection for Deep Recommender Systems",
        "abstract": "Feature selection has emerged as a crucial technique in refining recommender\nsystems. Recent advancements leveraging Automated Machine Learning (AutoML) has\ndrawn significant attention, particularly in two main categories: early feature\nselection and late feature selection, differentiated by whether the selection\noccurs before or after the embedding layer. The early feature selection selects\na fixed subset of features and retrains the model, while the late feature\nselection, known as adaptive feature selection, dynamically adjusts feature\nchoices for each data instance, recognizing the variability in feature\nsignificance. Although adaptive feature selection has shown remarkable\nimprovements in performance, its main drawback lies in its post-embedding layer\nfeature selection. This process often becomes cumbersome and inefficient in\nlarge-scale recommender systems with billions of ID-type features, leading to a\nhighly sparse and parameter-heavy embedding layer. To overcome this, we\nintroduce Adaptive Early Feature Selection (AEFS), a very simple method that\nnot only adaptively selects informative features for each instance, but also\nsignificantly reduces the activated parameters of the embedding layer. AEFS\nemploys a dual-model architecture, encompassing an auxiliary model dedicated to\nfeature selection and a main model responsible for prediction. To ensure\neffective alignment between these two models, we incorporate two collaborative\ntraining loss constraints. Our extensive experiments on three benchmark\ndatasets validate the efficiency and effectiveness of our approach. Notably,\nAEFS matches the performance of current state-of-theart Adaptive Late Feature\nSelection methods while achieving a significant reduction of 37. 5% in the\nactivated parameters of the embedding layer. AEFS is open-source at\nhttps://github. com/fly-dragon211/AEFS .",
        "url": "http://arxiv.org/abs/2509.12076v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12076v1",
        "arxiv_id": "2509.12076v1",
        "authors": [
            "Fan Hu",
            "Gaofeng Lu",
            "Jun Chen",
            "Chaonan Guo",
            "Yuekui Yang",
            "Xirong Li"
        ],
        "submitted": "2025-09-15 16:04:24",
        "source": "arxiv",
        "comment": "Accepted by TKDE",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on recommender systems, specifically feature selection and adaptive methods, which is somewhat related to the user's interests in Information Retrieval and Search technologies. However, the paper's emphasis on recommender systems and feature selection is not a central match for the user's core research themes."
    },
    {
        "title": "How to Evaluate Medical AI",
        "abstract": "The integration of artificial intelligence (AI) into medical diagnostic\nworkflows requires robust and consistent evaluation methods to ensure\nreliability, clinical relevance, and the inherent variability in expert\njudgments. Traditional metrics like precision and recall often fail to account\nfor the inherent variability in expert judgments, leading to inconsistent\nassessments of AI performance. Inter-rater agreement statistics like Cohen's\nKappa are more reliable but they lack interpretability. We introduce Relative\nPrecision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new\nevaluation metrics that compare AI outputs against multiple expert opinions\nrather than a single reference. By normalizing performance against inter-expert\ndisagreement, these metrics provide a more stable and realistic measure of the\nquality of predicted diagnosis. In addition to the comprehensive analysis of\ndiagnostic quality measures, our study contains a very important side result.\nOur evaluation methodology allows us to avoid selecting diagnoses from a\nlimited list when evaluating a given case. Instead, both the models being\ntested and the examiners verifying them arrive at a free-form diagnosis. In\nthis automated methodology for establishing the identity of free-form clinical\ndiagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our\napproach using 360 medical dialogues, comparing multiple large language models\n(LLMs) against a panel of physicians. Large-scale study shows that\ntop-performing models, such as DeepSeek-V3, achieve consistency on par with or\nexceeding expert consensus. Moreover, we demonstrate that expert judgments\nexhibit significant variability - often greater than that between AI and\nhumans. This finding underscores the limitations of any absolute metrics and\nsupports the need to adopt relative metrics in medical AI.",
        "url": "http://arxiv.org/abs/2509.11941v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11941v1",
        "arxiv_id": "2509.11941v1",
        "authors": [
            "Ilia Kopanichuk",
            "Petr Anokhin",
            "Vladimir Shaposhnikov",
            "Vladimir Makharev",
            "Ekaterina Tsapieva",
            "Iaroslav Bespalov",
            "Dmitry V. Dylov",
            "Ivan Oseledets"
        ],
        "submitted": "2025-09-15 14:01:22",
        "source": "arxiv",
        "comment": "10 pages, 7 fugures",
        "score": 3,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your core research themes in Information Retrieval, Search technologies, or Natural Language Processing. While it involves AI and large language models, the focus is on medical diagnostic workflows and evaluation metrics, which is outside your primary areas of interest."
    },
    {
        "title": "From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient Narratives",
        "abstract": "The widespread adoption of large language models (LLMs) in healthcare raises\ncritical questions about their ability to interpret patient-generated\nnarratives, which are often informal, ambiguous, and noisy. Existing benchmarks\ntypically rely on clean, structured clinical text, offering limited insight\ninto model performance under realistic conditions. In this work, we present a\nnovel synthetic dataset designed to simulate patient self-descriptions\ncharacterized by varying levels of linguistic noise, fuzzy language, and\nlayperson terminology. Our dataset comprises clinically consistent scenarios\nannotated with ground-truth diagnoses, spanning a spectrum of communication\nclarity to reflect diverse real-world reporting styles. Using this benchmark,\nwe fine-tune and evaluate several state-of-the-art models (LLMs), including\nBERT-based and encoder-decoder T5 models. To support reproducibility and future\nresearch, we release the Noisy Diagnostic Benchmark (NDB), a structured dataset\nof noisy, synthetic patient descriptions designed to stress-test and compare\nthe diagnostic capabilities of large language models (LLMs) under realistic\nlinguistic conditions. We made the benchmark available for the community:\nhttps://github.com/lielsheri/PatientSignal",
        "url": "http://arxiv.org/abs/2509.11803v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11803v1",
        "arxiv_id": "2509.11803v1",
        "authors": [
            "Eden Mama",
            "Liel Sheri",
            "Yehudit Aperstein",
            "Alexander Apartsin"
        ],
        "submitted": "2025-09-15 11:34:46",
        "source": "arxiv",
        "comment": "6 pages, 1 figure",
        "score": 3,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and deep semantic understanding, but it does not directly align with their primary focus on Information Retrieval (IR) and Search technologies. The paper's focus on large language models and their ability to interpret patient-generated narratives is not directly applicable to the user's research interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "When Curiosity Signals Danger: Predicting Health Crises Through Online Medication Inquiries",
        "abstract": "Online medical forums are a rich and underutilized source of insight into\npatient concerns, especially regarding medication use. Some of the many\nquestions users pose may signal confusion, misuse, or even the early warning\nsigns of a developing health crisis. Detecting these critical questions that\nmay precede severe adverse events or life-threatening complications is vital\nfor timely intervention and improving patient safety. This study introduces a\nnovel annotated dataset of medication-related questions extracted from online\nforums. Each entry is manually labelled for criticality based on clinical risk\nfactors. We benchmark the performance of six traditional machine learning\nclassifiers using TF-IDF textual representations, alongside three\nstate-of-the-art large language model (LLM)-based classification approaches\nthat leverage deep contextual understanding. Our results highlight the\npotential of classical and modern methods to support real-time triage and alert\nsystems in digital health spaces. The curated dataset is made publicly\navailable to encourage further research at the intersection of\npatient-generated data, natural language processing, and early warning systems\nfor critical health events. The dataset and benchmark are available at:\nhttps://github.com/Dvora-coder/LLM-Medication-QA-Risk-Classifier-MediGuard.",
        "url": "http://arxiv.org/abs/2509.11802v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11802v1",
        "arxiv_id": "2509.11802v1",
        "authors": [
            "Dvora Goncharok",
            "Arbel Shifman",
            "Alexander Apartsin",
            "Yehudit Aperstein"
        ],
        "submitted": "2025-09-15 11:31:25",
        "source": "arxiv",
        "comment": "5 pages, 2 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the intersection of natural language processing and patient-generated data, which aligns with your NLP interests. However, the focus on health crises and medication inquiries is somewhat tangential to your core research themes in information retrieval and search technologies."
    },
    {
        "title": "User eXperience Perception Insights Dataset (UXPID): Synthetic User Feedback from Public Industrial Forums",
        "abstract": "Customer feedback in industrial forums reflect a rich but underexplored\nsource of insight into real-world product experience. These publicly shared\ndiscussions offer an organic view of user expectations, frustrations, and\nsuccess stories shaped by the specific contexts of use. Yet, harnessing this\ninformation for systematic analysis remains challenging due to the unstructured\nand domain-specific nature of the content. The lack of structure and\nspecialized vocabulary makes it difficult for traditional data analysis\ntechniques to accurately interpret, categorize, and quantify the feedback,\nthereby limiting its potential to inform product development and support\nstrategies. To address these challenges, this paper presents the User\neXperience Perception Insights Dataset (UXPID), a collection of 7130\nartificially synthesized and anonymized user feedback branches extracted from a\npublic industrial automation forum. Each JavaScript object notation (JSON)\nrecord contains multi-post comments related to specific hardware and software\nproducts, enriched with metadata and contextual conversation data. Leveraging a\nlarge language model (LLM), each branch is systematically analyzed and\nannotated for UX insights, user expectations, severity and sentiment ratings,\nand topic classifications. The UXPID dataset is designed to facilitate research\nin user requirements, user experience (UX) analysis, and AI-driven feedback\nprocessing, particularly where privacy and licensing restrictions limit access\nto real-world data. UXPID supports the training and evaluation of\ntransformer-based models for tasks such as issue detection, sentiment analysis,\nand requirements extraction in the context of technical forums.",
        "url": "http://arxiv.org/abs/2509.11777v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11777v1",
        "arxiv_id": "2509.11777v1",
        "authors": [
            "Mikhail Kulyabin",
            "Jan Joosten",
            "Choro Ulan uulu",
            "Nuno Miguel Martins Pacheco",
            "Fabian Ries",
            "Filippos Petridis",
            "Jan Bosch",
            "Helena Holmström Olsson"
        ],
        "submitted": "2025-09-15 10:58:41",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper presents a dataset for user experience analysis in industrial forums, which is somewhat related to information retrieval and user behavior modeling. However, the focus is on data analysis and NLP techniques for extracting insights from user feedback, rather than query understanding, ranking models, or real-time relevance optimization."
    },
    {
        "title": "MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs",
        "abstract": "We propose MindVL, a multimodal large langauge model trained on Ascend NPUs.\nSimilar to Qwen2.5-VL, MindVL adopts native-resolution Vision Transformers,\nwhich enables it to process images at their original variable resolutions. This\ndesign avoids the degradation caused by fixed-resolution tiling while\npreserving fine-grained details and global layouts, which is crucial for\nvisually dense content such as complex charts and diagrams. To ensure the\nsmooth training of MindVL on Ascend NPUs, we develop Mindspeed-MLLM, a\ndistributed multimodal training framework tailored for Ascend NPUs. To maintain\ntraining accuracy, we implement equivalent replacements for certain operators.\nMindVL undergoes a three-phase training process, namely the warm-up phase,\nmultitask training phase, and supervised instruction tuning phase, to gradually\nenhance its capabilities. This process starts with basic visual and multimodal\npre-training, followed by large-scale multiask trainging and instruction\ntuning. We also adopt multimodal data packaging and hybrid parallelism\ntechniques, which significantly improve end-to-end training speed. To further\nboost model performance, we specifically introduce test-time resolution search\nand model weight averaging. Notably, despite using about 1/10 of the training\ndata required by Qwen2.5-VL, MindVL achieves performance on par with Qwen2.5-VL\nin evaluations of general multimodal understanding and document/table\ncomprehension. Beyond overall scores, MindVL also delivers leading performance\nin OCR assessments.",
        "url": "http://arxiv.org/abs/2509.11662v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11662v1",
        "arxiv_id": "2509.11662v1",
        "authors": [
            "Feilong Chen",
            "Yijiang Liu",
            "Yi Huang",
            "Hao Wang",
            "Miren Tian",
            "Ya-Qi Yu",
            "Minghui Liao",
            "Jihao Wu"
        ],
        "submitted": "2025-09-15 08:00:31",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on multimodal large language models and their training on Ascend NPUs, which is not directly related to Information Retrieval, Search technologies, or user behavior modeling. While it involves natural language processing, the context is more aligned with NLP applications rather than IR or search technologies."
    },
    {
        "title": "MALLM: Multi-Agent Large Language Models Framework",
        "abstract": "Multi-agent debate (MAD) has demonstrated the ability to augment collective\nintelligence by scaling test-time compute and leveraging expertise. Current\nframeworks for multi-agent debate are often designed towards tool use, lack\nintegrated evaluation, or provide limited configurability of agent personas,\nresponse generators, discussion paradigms, and decision protocols. We introduce\nMALLM (Multi-Agent Large Language Models), an open-source framework that\nenables systematic analysis of MAD components. MALLM offers more than 144\nunique configurations of MAD, including (1) agent personas (e.g., Expert,\nPersonality), (2) response generators (e.g., Critical, Reasoning), (3)\ndiscussion paradigms (e.g., Memory, Relay), and (4) decision protocols (e.g.,\nVoting, Consensus). MALLM uses simple configuration files to define a debate.\nFurthermore, MALLM can load any textual Huggingface dataset (e.g., MMLU-Pro,\nWinoGrande) and provides an evaluation pipeline for easy comparison of MAD\nconfigurations. MALLM is tailored towards researchers and provides a window\ninto the heart of multi-agent debate, facilitating the understanding of its\ncomponents and their interplay.",
        "url": "http://arxiv.org/abs/2509.11656v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11656v1",
        "arxiv_id": "2509.11656v1",
        "authors": [
            "Jonas Becker",
            "Lars Benedikt Kaesberg",
            "Niklas Bauer",
            "Jan Philip Wahle",
            "Terry Ruas",
            "Bela Gipp"
        ],
        "submitted": "2025-09-15 07:48:02",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025 (Demo)",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be focused on multi-agent debate and large language models, which is outside of your primary research interests in Information Retrieval and Search technologies. While it involves NLP, the context and application seem unrelated to your core themes of query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Opal: An Operator Algebra View of RLHF",
        "abstract": "We present Opal, an operator view of reinforcement learning from human\nfeedback (RLHF). Objectives are expressed as ladders of two primitives on a\nbase utility: additive penalties and multiplicative pairwise weights. We\ndescribe a simple reduction law with if-and-only-if conditions: such ladders\ncollapse to a normal form on pairwise margins when the reference is fixed,\npenalties are additive, and weights are independent of intermediate margins.\nWhen these assumptions do not hold (reference shift, non-additive gates,\nscore-dependent weights), small examples demonstrate non-reducibility.\n  Building on this view, we introduce GKPO (Generalized Kernel Preference\nObject), a canonical schema in which many RLHF methods can be represented and,\nwhen reducible, mapped back from. GKPO provides a standard JSON serialization,\ncanonicalization and hashing rules, and explicit flags with finite witnesses\nwhen assumptions fail.\n  We illustrate these ideas with GKPO examples for DPO, RRHF, and ORPO, along\nwith cross-method conversions (where assumptions permit) and minimal stress\ntests (SHIFT/GATE/SCORE) that highlight non-reducibility. A lightweight Python\nreference library accompanies the schema, implementing canonical hashing and\nadapters for DPO and RRHF.",
        "url": "http://arxiv.org/abs/2509.11298v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11298v1",
        "arxiv_id": "2509.11298v1",
        "authors": [
            "Madhava Gaikwad"
        ],
        "submitted": "2025-09-14 14:42:39",
        "source": "arxiv",
        "comment": "11 pages main",
        "score": 3,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be focused on reinforcement learning from human feedback (RLHF), which is not directly related to the user's core research themes in Information Retrieval and Search technologies. While it touches on the idea of optimizing objectives, it does not seem to involve query understanding, ranking models, or user behavior modeling, making it only loosely relevant to the user's interests."
    },
    {
        "title": "AQUA: Attention via QUery mAgnitudes for Memory and Compute Efficient Inference in LLMs",
        "abstract": "The quadratic complexity of the attention mechanism remains a fundamental\nbarrier to scaling Large Language Models (LLMs) to longer contexts, creating a\ncritical bottleneck in both computation and memory. To address this, we\nintroduce AQUA (Attention via QUery mAgnitudes) a novel and versatile\napproximation strategy that significantly reduces the cost of attention with a\ngraceful performance trade-off. Our method operates in two phases: an efficient\noffline step where we compute a universal, language agnostic projection matrix\nvia SVD on a calibration dataset, and an online inference step where we project\nquery and key vectors and dynamically select a sparse subset of dimensions\nbased on the query's magnitude. We provide a formal theoretical analysis of\nAQUA, establishing the break-even point at which it becomes more\ncomputationally efficient than standard attention. Our empirical evaluations on\nstate-of-the-art models like Llama-3.1-8B demonstrate that a 25% reduction in\nthe attention dot-product computation can be achieved with a statistically\ninsignificant impact on performance across a wide range of benchmarks. We\nfurther showcase the versatility of AQUA by demonstrating its ability to\nsynergistically accelerate existing token eviction methods like H2O and to\ndirectly reduce KV-cache memory size. By offering a controllable knob to\nbalance efficiency and accuracy, AQUA provides a practical and powerful tool\nfor making large-scale LLM inference more accessible and sustainable.",
        "url": "http://arxiv.org/abs/2509.11155v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11155v1",
        "arxiv_id": "2509.11155v1",
        "authors": [
            "Santhosh G S",
            "Saurav Prakash",
            "Balaraman Ravindran"
        ],
        "submitted": "2025-09-14 08:20:48",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on optimizing Large Language Models (LLMs) for efficient inference, which is not directly related to Information Retrieval (IR) or Search technologies. While it does involve attention mechanisms, the context is not relevant to query understanding, ranking models, or user behavior modeling, which are core areas of interest."
    },
    {
        "title": "Length-Aware Rotary Position Embedding for Text-Speech Alignment",
        "abstract": "Many recent text-to-speech (TTS) systems are built on transformer\narchitectures and employ cross-attention mechanisms for text-speech alignment.\nWithin these systems, rotary position embedding (RoPE) is commonly used to\nencode positional information in text and speech representations. In this work,\nwe introduce length-aware RoPE (LARoPE), a simple yet effective extension of\nRoPE that improves text-speech alignment. Unlike RoPE, which relies on absolute\nindices, LARoPE computes relative distances between query and key positions\nusing length-normalized indices. Experimental results show that LARoPE\nconsistently outperforms RoPE, offering faster loss convergence, more accurate\ntext-speech alignment, and higher overall TTS quality. Furthermore, LARoPE\ndemonstrates greater resilience to variations in utterance duration and\nmaintains stable performance in extended speech generation up to 30 seconds,\nwhereas RoPE suffers from notable degradation. Notably, our method achieves a\nstate-of-the-art word error rate on a standard zero-shot TTS benchmark.",
        "url": "http://arxiv.org/abs/2509.11084v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11084v1",
        "arxiv_id": "2509.11084v1",
        "authors": [
            "Hyeongju Kim",
            "Juheon Lee",
            "Jinhyeok Yang",
            "Jacob Morton"
        ],
        "submitted": "2025-09-14 04:25:13",
        "source": "arxiv",
        "comment": "5 pages, 3 figures, preprint",
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing, as it focuses on Text-to-Speech (TTS) systems and rotary position embedding for text-speech alignment."
    },
    {
        "title": "Event2Vec: A Geometric Approach to Learning Composable Representations of Event Sequences",
        "abstract": "The study of neural representations, both in biological and artificial\nsystems, is increasingly revealing the importance of geometric and topological\nstructures. Inspired by this, we introduce Event2Vec, a novel framework for\nlearning representations of discrete event sequences. Our model leverages a\nsimple, additive recurrent structure to learn composable, interpretable\nembeddings. We provide a theoretical analysis demonstrating that, under\nspecific training objectives, our model's learned representations in a\nEuclidean space converge to an ideal additive structure. This ensures that the\nrepresentation of a sequence is the vector sum of its constituent events, a\nproperty we term the linear additive hypothesis. To address the limitations of\nEuclidean geometry for hierarchical data, we also introduce a variant of our\nmodel in hyperbolic space, which is naturally suited to embedding tree-like\nstructures with low distortion. We present experiments to validate our\nhypothesis and demonstrate the benefits of each geometry, highlighting the\nimproved performance of the hyperbolic model on hierarchical event sequences.",
        "url": "http://arxiv.org/abs/2509.12188v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12188v1",
        "arxiv_id": "2509.12188v1",
        "authors": [
            "Antonin Sulc"
        ],
        "submitted": "2025-09-15 17:51:02",
        "source": "arxiv",
        "comment": "10 pages, 3 figures, Symmetry and Geometry in Neural Representations\n  Workshop at NeuralIPS (Neurreps) 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper introduces a novel framework for learning representations of event sequences, leveraging geometric and topological structures. While it touches on neural representations and embeddings, it does not directly relate to query understanding, ranking models, or user behavior modeling in information retrieval, which are core areas of your research interests."
    },
    {
        "title": "Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models",
        "abstract": "Recent advances in text-only \"slow-thinking\" reasoning have prompted efforts\nto transfer this capability to vision-language models (VLMs), for training\nvisual reasoning models (\\textbf{VRMs}). owever, such transfer faces critical\nchallenges: Effective \"slow thinking\" in VRMs requires \\textbf{visual\nreflection}, the ability to check the reasoning process based on visual\ninformation. Through quantitative analysis, we observe that current VRMs\nexhibit limited visual reflection, as their attention to visual information\ndiminishes rapidly with longer generated responses. To address this challenge,\nwe propose a new VRM \\textbf{Reflection-V}, which enhances visual reflection\nbased on reasoning data construction for cold-start and reward design for\nreinforcement learning (RL). Firstly, we construct vision-centered reasoning\ndata by leveraging an agent that interacts between VLMs and reasoning LLMs,\nenabling cold-start learning of visual reflection patterns. Secondly, a visual\nattention based reward model is employed during RL to encourage reasoning based\non visual information. Therefore, \\textbf{Reflection-V} demonstrates\nsignificant improvements across multiple visual reasoning benchmarks.\nFurthermore, \\textbf{Reflection-V} maintains a stronger and more consistent\nreliance on visual information during visual reasoning, indicating effective\nenhancement in visual reflection capabilities.",
        "url": "http://arxiv.org/abs/2509.12132v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12132v1",
        "arxiv_id": "2509.12132v1",
        "authors": [
            "Pu Jian",
            "Junhong Wu",
            "Wei Sun",
            "Chen Wang",
            "Shuo Ren",
            "Jiajun Zhang"
        ],
        "submitted": "2025-09-15 16:57:25",
        "source": "arxiv",
        "comment": "EMNLP2025 Main",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses enhancing visual reflection in vision-language models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on visual reflection and reinforcement learning is not directly aligned with the user's core research themes, limiting its relevance."
    },
    {
        "title": "When marine radar target detection meets pretrained large language models",
        "abstract": "Deep learning (DL) methods are widely used to extract high-dimensional\npatterns from the sequence features of radar echo signals. However,\nconventional DL algorithms face challenges such as redundant feature segments,\nand constraints from restricted model sizes. To address these issues, we\npropose a framework that integrates feature preprocessing with large language\nmodels (LLMs). Our preprocessing module tokenizes radar sequence features,\napplies a patch selection algorithm to filter out uninformative segments, and\nprojects the selected patches into embeddings compatible with the feature space\nof pre-trained LLMs. Leveraging these refined embeddings, we incorporate a\npre-trained LLM, fine-tuning only the normalization layers to reduce training\nburdens while enhancing performance. Experiments on measured datasets\ndemonstrate that the proposed method significantly outperforms the\nstate-of-the-art baselines on supervised learning tests.",
        "url": "http://arxiv.org/abs/2509.12110v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12110v1",
        "arxiv_id": "2509.12110v1",
        "authors": [
            "Qiying Hu",
            "Linping Zhang",
            "Xueqian Wang",
            "Gang Li",
            "Yu Liu",
            "Xiao-Ping Zhang"
        ],
        "submitted": "2025-09-15 16:38:13",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper appears to be unrelated to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The topic of marine radar target detection and its application of large language models does not align with the user's focus on query understanding, ranking models, user behavior modeling, and deep semantic understanding in real-time relevance optimization."
    },
    {
        "title": "Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing traditional NLP tools and large language models on ambiguous entities",
        "abstract": "This pilot study presents a small-scale but carefully annotated benchmark of\nNamed Entity Recognition (NER) performance across six systems: three non-LLM\nNLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models\n(LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119\ntokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME).\nWe evaluated each system's output against the manually annotated gold standard\ndataset using F1-score. The results show that LLMs generally outperform\nconventional tools in recognizing context-sensitive entities like person names,\nwith Gemini achieving the highest average F1-score. However, traditional\nsystems like Stanza demonstrate greater consistency in structured tags such as\nLOCATION and DATE. We also observed variability among LLMs, particularly in\nhandling temporal expressions and multi-word organizations. Our findings\nhighlight that while LLMs offer improved contextual understanding, traditional\ntools remain competitive in specific tasks, informing model selection.",
        "url": "http://arxiv.org/abs/2509.12098v1",
        "pdf_url": "http://arxiv.org/pdf/2509.12098v1",
        "arxiv_id": "2509.12098v1",
        "authors": [
            "Payam Latifi"
        ],
        "submitted": "2025-09-15 16:21:59",
        "source": "arxiv",
        "comment": "14 pages, 9 figures, 2 tables. This is a pilot study evaluating six\n  NER systems -- three traditional tools (NLTK, spaCy, Stanza) and three LLMs\n  (Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B) -- on a small, ambiguity-rich\n  dataset of 119 tokens. The annotated dataset, prompts are provided in\n  appendices for full reproducibility. All experiments were conducted on 14 May\n  2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to the user's interests in Natural Language Processing (NLP) and Named Entity Recognition (NER), but it does not directly align with the user's primary focus on Information Retrieval, especially in areas requiring deep semantic understanding and real-time relevance optimization. The paper's focus on comparing traditional NLP tools and large language models on ambiguous entities is also not directly related to the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing Cycles",
        "abstract": "We describe Vicomtech's participation in the CLEARS challenge on text\nadaptation to Plain Language and Easy Read in Spanish. Our approach features\nautomatic post-editing of different types of initial Large Language Model\nadaptations, where successive adaptations are generated iteratively until\nreadability and similarity metrics indicate that no further adaptation\nrefinement can be successfully performed. Taking the average of all official\nmetrics, our submissions achieved first and second place in Plain language and\nEasy Read adaptation, respectively.",
        "url": "http://arxiv.org/abs/2509.11991v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11991v1",
        "arxiv_id": "2509.11991v1",
        "authors": [
            "Jesús Calleja",
            "David Ponce",
            "Thierry Etchegoyhen"
        ],
        "submitted": "2025-09-15 14:42:44",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on text adaptation and post-editing, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves Large Language Models, the context is more about readability and post-editing rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Lost in Embeddings: Information Loss in Vision-Language Models",
        "abstract": "Vision--language models (VLMs) often process visual inputs through a\npretrained vision encoder, followed by a projection into the language model's\nembedding space via a connector component. While crucial for modality fusion,\nthe potential information loss induced by this projection step and its direct\nimpact on model capabilities remain understudied. We introduce two\ncomplementary approaches to examine and quantify this loss by analyzing the\nlatent representation space. First, we evaluate semantic information\npreservation by analyzing changes in k-nearest neighbor relationships between\nimage representations, before and after projection. Second, we directly measure\ninformation loss by reconstructing visual embeddings from the projected\nrepresentation, localizing loss at an image patch level. Experiments reveal\nthat connectors substantially distort the local geometry of visual\nrepresentations, with k-nearest neighbors diverging by 40--60\\%\npost-projection, correlating with degradation in retrieval performance. The\npatch-level embedding reconstruction provides interpretable insights for model\nbehavior on visually grounded question-answering tasks, finding that areas of\nhigh information loss reliably predict instances where models struggle.",
        "url": "http://arxiv.org/abs/2509.11986v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11986v1",
        "arxiv_id": "2509.11986v1",
        "authors": [
            "Wenyan Li",
            "Raphael Tang",
            "Chengzu Li",
            "Caiqi Zhang",
            "Ivan Vulić",
            "Anders Søgaard"
        ],
        "submitted": "2025-09-15 14:38:06",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 3,
        "llm_reason": "This paper explores information loss in vision-language models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on vision-language models and visual embeddings is not directly aligned with the user's primary research interests in IR and NLP. The paper's findings on information loss and model behavior may have some indirect implications for IR, but it is not a central match."
    },
    {
        "title": "ToolRM: Outcome Reward Models for Tool-Calling Large Language Models",
        "abstract": "As large language models (LLMs) increasingly interact with external tools,\nreward modeling for tool use has become a critical yet underexplored area.\nExisting reward models, trained primarily on natural language outputs, struggle\nto evaluate tool-based reasoning and execution. To quantify this gap, we\nintroduce FC-RewardBench, the first benchmark designed to systematically assess\nreward models' performance in tool-calling scenarios. Our analysis shows that\ncurrent reward models often miss key signals of effective tool use,\nhighlighting the need for domain-specific modeling. To address this, we propose\na training framework for outcome-based reward models using data synthesized\nfrom permissively licensed, open-weight LLMs. We train models ranging from 1.7B\nto 14B parameters and evaluate them across seven out-of-domain benchmarks.\nThese models consistently outperform general-purpose baselines, achieving up to\n25\\% average improvement in downstream task performance and enabling\ndata-efficient fine-tuning through reward-guided filtering.",
        "url": "http://arxiv.org/abs/2509.11963v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11963v1",
        "arxiv_id": "2509.11963v1",
        "authors": [
            "Mayank Agarwal",
            "Ibrahim Abdelaziz",
            "Kinjal Basu",
            "Merve Unuvar",
            "Luis A. Lastras",
            "Yara Rizk",
            "Pavan Kapanipathi"
        ],
        "submitted": "2025-09-15 14:17:17",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on reward modeling for large language models interacting with external tools, which is somewhat related to information retrieval and search technologies. However, the primary focus on tool-calling and reward models for LLMs does not directly align with the user's core research themes in query understanding, ranking models, and user behavior modeling. While there is some overlap with NLP and data mining, the paper's relevance is not strong enough to warrant a higher score."
    },
    {
        "title": "PledgeTracker: A System for Monitoring the Fulfilment of Pledges",
        "abstract": "Political pledges reflect candidates' policy commitments, but tracking their\nfulfilment requires reasoning over incremental evidence distributed across\nmultiple, dynamically updated sources. Existing methods simplify this task into\na document classification task, overlooking its dynamic, temporal and\nmulti-document nature. To address this issue, we introduce\n\\textsc{PledgeTracker}, a system that reformulates pledge verification into\nstructured event timeline construction. PledgeTracker consists of three core\ncomponents: (1) a multi-step evidence retrieval module; (2) a timeline\nconstruction module and; (3) a fulfilment filtering module, allowing the\ncapture of the evolving nature of pledge fulfilment and producing interpretable\nand structured timelines. We evaluate PledgeTracker in collaboration with\nprofessional fact-checkers in real-world workflows, demonstrating its\neffectiveness in retrieving relevant evidence and reducing human verification\neffort.",
        "url": "http://arxiv.org/abs/2509.11804v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11804v1",
        "arxiv_id": "2509.11804v1",
        "authors": [
            "Yulong Chen",
            "Michael Sejr Schlichtkrull",
            "Zhenyun Deng",
            "David Corney",
            "Nasim Asl",
            "Joshua Salisbury",
            "Andrew Dudfield",
            "Andreas Vlachos"
        ],
        "submitted": "2025-09-15 11:37:47",
        "source": "arxiv",
        "comment": "EMNLP 2025 demo",
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval, Search technologies, or Natural Language Processing, which are the core areas of your research interests. While it involves evidence retrieval and filtering, the context is specific to pledge verification and fact-checking, which is not aligned with your primary focus on e-commerce or deep semantic understanding in IR."
    },
    {
        "title": "CoachMe: Decoding Sport Elements with a Reference-Based Coaching Instruction Generation Model",
        "abstract": "Motion instruction is a crucial task that helps athletes refine their\ntechnique by analyzing movements and providing corrective guidance. Although\nrecent advances in multimodal models have improved motion understanding,\ngenerating precise and sport-specific instruction remains challenging due to\nthe highly domain-specific nature of sports and the need for informative\nguidance. We propose CoachMe, a reference-based model that analyzes the\ndifferences between a learner's motion and a reference under temporal and\nphysical aspects. This approach enables both domain-knowledge learning and the\nacquisition of a coach-like thinking process that identifies movement errors\neffectively and provides feedback to explain how to improve. In this paper, we\nillustrate how CoachMe adapts well to specific sports such as skating and\nboxing by learning from general movements and then leveraging limited data.\nExperiments show that CoachMe provides high-quality instructions instead of\ndirections merely in the tone of a coach but without critical information.\nCoachMe outperforms GPT-4o by 31.6% in G-Eval on figure skating and by 58.3% on\nboxing. Analysis further confirms that it elaborates on errors and their\ncorresponding improvement methods in the generated instructions. You can find\nCoachMe here: https://motionxperts.github.io/",
        "url": "http://arxiv.org/abs/2509.11698v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11698v1",
        "arxiv_id": "2509.11698v1",
        "authors": [
            "Wei-Hsin Yeh",
            "Yu-An Su",
            "Chih-Ning Chen",
            "Yi-Hsueh Lin",
            "Calvin Ku",
            "Wen-Hsin Chiu",
            "Min-Chun Hu",
            "Lun-Wei Ku"
        ],
        "submitted": "2025-09-15 09:01:39",
        "source": "arxiv",
        "comment": "Published in Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025.\n  Official version: https://doi.org/10.18653/v1/2025.acl-long.1413",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on a coaching instruction generation model for sports, which is not directly related to Information Retrieval, Search technologies, or Natural Language Processing. Although it involves multimodal models and text generation, the domain-specific nature of the task and the lack of relevance to the user's core research themes result in a low score."
    },
    {
        "title": "Bhaasha, Bhasa, Zaban: A Survey for Low-Resourced Languages in South Asia - Current Stage and Challenges",
        "abstract": "Rapid developments of large language models have revolutionized many NLP\ntasks for English data. Unfortunately, the models and their evaluations for\nlow-resource languages are being overlooked, especially for languages in South\nAsia. Although there are more than 650 languages in South Asia, many of them\neither have very limited computational resources or are missing from existing\nlanguage models. Thus, a concrete question to be answered is: Can we assess the\ncurrent stage and challenges to inform our NLP community and facilitate model\ndevelopments for South Asian languages? In this survey, we have comprehensively\nexamined current efforts and challenges of NLP models for South Asian languages\nby retrieving studies since 2020, with a focus on transformer-based models,\nsuch as BERT, T5, & GPT. We present advances and gaps across 3 essential\naspects: data, models, & tasks, such as available data sources, fine-tuning\nstrategies, & domain applications. Our findings highlight substantial issues,\nincluding missing data in critical domains (e.g., health), code-mixing, and\nlack of standardized evaluation benchmarks. Our survey aims to raise awareness\nwithin the NLP community for more targeted data curation, unify benchmarks\ntailored to cultural and linguistic nuances of South Asia, and encourage an\nequitable representation of South Asian languages. The complete list of\nresources is available at: https://github.com/trust-nlp/LM4SouthAsia-Survey.",
        "url": "http://arxiv.org/abs/2509.11570v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11570v1",
        "arxiv_id": "2509.11570v1",
        "authors": [
            "Sampoorna Poria",
            "Xiaolei Huang"
        ],
        "submitted": "2025-09-15 04:31:22",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to the user's interests in Natural Language Processing (NLP), but it focuses on low-resourced languages in South Asia, which is not a central match for the user's research themes. While it touches on transformer-based models, it does not directly relate to query understanding, ranking models, or user behavior modeling in the context of information retrieval."
    },
    {
        "title": "D$^2$HScore: Reasoning-Aware Hallucination Detection via Semantic Breadth and Depth Analysis in LLMs",
        "abstract": "Although large Language Models (LLMs) have achieved remarkable success, their\npractical application is often hindered by the generation of non-factual\ncontent, which is called \"hallucination\". Ensuring the reliability of LLMs'\noutputs is a critical challenge, particularly in high-stakes domains such as\nfinance, security, and healthcare. In this work, we revisit hallucination\ndetection from the perspective of model architecture and generation dynamics.\nLeveraging the multi-layer structure and autoregressive decoding process of\nLLMs, we decompose hallucination signals into two complementary dimensions: the\nsemantic breadth of token representations within each layer, and the semantic\ndepth of core concepts as they evolve across layers. Based on this insight, we\npropose \\textbf{D$^2$HScore (Dispersion and Drift-based Hallucination Score)},\na training-free and label-free framework that jointly measures: (1)\n\\textbf{Intra-Layer Dispersion}, which quantifies the semantic diversity of\ntoken representations within each layer; and (2) \\textbf{Inter-Layer Drift},\nwhich tracks the progressive transformation of key token representations across\nlayers. To ensure drift reflects the evolution of meaningful semantics rather\nthan noisy or redundant tokens, we guide token selection using attention\nsignals. By capturing both the horizontal and vertical dynamics of\nrepresentation during inference, D$^2$HScore provides an interpretable and\nlightweight proxy for hallucination detection. Extensive experiments across\nfive open-source LLMs and five widely used benchmarks demonstrate that\nD$^2$HScore consistently outperforms existing training-free baselines.",
        "url": "http://arxiv.org/abs/2509.11569v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11569v1",
        "arxiv_id": "2509.11569v1",
        "authors": [
            "Yue Ding",
            "Xiaofang Zhu",
            "Tianze Xia",
            "Junfei Wu",
            "Xinlong Chen",
            "Qiang Liu",
            "Liang Wang"
        ],
        "submitted": "2025-09-15 04:28:38",
        "source": "arxiv",
        "comment": "under review",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Natural Language Processing (NLP) and deep semantic understanding, but it primarily focuses on hallucination detection in large Language Models (LLMs), which is not a central theme in your research. While it touches on the idea of semantic analysis, it's more focused on the reliability of LLMs' outputs rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Acoustic Overspecification in Electronic Dance Music Taxonomy",
        "abstract": "Electronic Dance Music (EDM) classification typically relies on\nindustry-defined taxonomies with numerous subgenres, yet the acoustic basis for\nthese distinctions remains unclear. Current approaches use supervised learning\nwith prescribed genre labels, assuming their validity without systematic\nevaluation. In this paper, we propose an unsupervised approach to discover the\nnatural acoustic structure of EDM independent of commercial labels. Our method\ncombines novel tempogram-based features capturing EDM's layered rhythmic\npatterns with multi-criteria feature selection. To validate that our findings\nreflect genuine acoustic structure rather than methodological artifacts, we\ncompare our results against state-of-the-art pre-trained audio embeddings (MERT\nand CLAP). Both our feature space and embedding representations converge to\n19-23 natural acoustic families compared to the prescribed 35, providing\nconsistent evidence of significant overspecification in current EDM taxonomy by\napproximately one-third.",
        "url": "http://arxiv.org/abs/2509.11474v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11474v1",
        "arxiv_id": "2509.11474v1",
        "authors": [
            "Weilun Xu",
            "Tianhao Dai",
            "Oscar Goudet",
            "Xiaoxuan Wang"
        ],
        "submitted": "2025-09-14 23:38:45",
        "source": "arxiv",
        "comment": "5 pages, 3 figures, conference paper",
        "score": 2,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper is not related to Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing, which are the primary areas of interest. The paper focuses on music taxonomy and acoustic analysis, which is outside the scope of the user's research interests."
    },
    {
        "title": "A Transformer-Based Cross-Platform Analysis of Public Discourse on the 15-Minute City Paradigm",
        "abstract": "This study presents the first multi-platform sentiment analysis of public\nopinion on the 15-minute city concept across Twitter, Reddit, and news media.\nUsing compressed transformer models and Llama-3-8B for annotation, we classify\nsentiment across heterogeneous text domains. Our pipeline handles long-form and\nshort-form text, supports consistent annotation, and enables reproducible\nevaluation. We benchmark five models (DistilRoBERTa, DistilBERT, MiniLM,\nELECTRA, TinyBERT) using stratified 5-fold cross-validation, reporting\nF1-score, AUC, and training time. DistilRoBERTa achieved the highest F1\n(0.8292), TinyBERT the best efficiency, and MiniLM the best cross-platform\nconsistency. Results show News data yields inflated performance due to class\nimbalance, Reddit suffers from summarization loss, and Twitter offers moderate\nchallenge. Compressed models perform competitively, challenging assumptions\nthat larger models are necessary. We identify platform-specific trade-offs and\npropose directions for scalable, real-world sentiment classification in urban\nplanning discourse.",
        "url": "http://arxiv.org/abs/2509.11443v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11443v1",
        "arxiv_id": "2509.11443v1",
        "authors": [
            "Gaurab Chhetri",
            "Darrell Anderson",
            "Boniphace Kutela",
            "Subasish Das"
        ],
        "submitted": "2025-09-14 21:36:24",
        "source": "arxiv",
        "comment": "This is the author's preprint version of a paper accepted for\n  presentation at the 24th International Conference on Machine Learning and\n  Applications (ICMLA 2025), December 3-5, 2025, Florida, USA. The final\n  published version will appear in the official IEEE proceedings. Conference\n  site: https://www.icmla-conference.org/icmla25/",
        "score": 2,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Natural Language Processing (NLP) and Information Retrieval (IR), particularly in the context of text classification and sentiment analysis. However, the focus on the 15-minute city paradigm and its application in urban planning discourse is not directly aligned with your core research themes. The use of transformer-based models and compressed models is relevant, but the specific domain and application are not central to your interests."
    },
    {
        "title": "Securing AI Agents: Implementing Role-Based Access Control for Industrial Applications",
        "abstract": "The emergence of Large Language Models (LLMs) has significantly advanced\nsolutions across various domains, from political science to software\ndevelopment. However, these models are constrained by their training data,\nwhich is static and limited to information available up to a specific date.\nAdditionally, their generalized nature often necessitates fine-tuning --\nwhether for classification or instructional purposes -- to effectively perform\nspecific downstream tasks. AI agents, leveraging LLMs as their core, mitigate\nsome of these limitations by accessing external tools and real-time data,\nenabling applications such as live weather reporting and data analysis. In\nindustrial settings, AI agents are transforming operations by enhancing\ndecision-making, predictive maintenance, and process optimization. For example,\nin manufacturing, AI agents enable near-autonomous systems that boost\nproductivity and support real-time decision-making. Despite these advancements,\nAI agents remain vulnerable to security threats, including prompt injection\nattacks, which pose significant risks to their integrity and reliability. To\naddress these challenges, this paper proposes a framework for integrating\nRole-Based Access Control (RBAC) into AI agents, providing a robust security\nguardrail. This framework aims to support the effective and scalable deployment\nof AI agents, with a focus on on-premises implementations.",
        "url": "http://arxiv.org/abs/2509.11431v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11431v1",
        "arxiv_id": "2509.11431v1",
        "authors": [
            "Aadil Gani Ganie"
        ],
        "submitted": "2025-09-14 20:58:08",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on securing AI agents using Role-Based Access Control, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing."
    },
    {
        "title": "!MSA at AraHealthQA 2025 Shared Task: Enhancing LLM Performance for Arabic Clinical Question Answering through Prompt Engineering and Ensemble Learning",
        "abstract": "We present our systems for Track 2 (General Arabic Health QA, MedArabiQ) of\nthe AraHealthQA-2025 shared task, where our methodology secured 2nd place in\nboth Sub-Task 1 (multiple-choice question answering) and Sub-Task 2 (open-ended\nquestion answering) in Arabic clinical contexts. For Sub-Task 1, we leverage\nthe Gemini 2.5 Flash model with few-shot prompting, dataset preprocessing, and\nan ensemble of three prompt configurations to improve classification accuracy\non standard, biased, and fill-in-the-blank questions. For Sub-Task 2, we employ\na unified prompt with the same model, incorporating role-playing as an Arabic\nmedical expert, few-shot examples, and post-processing to generate concise\nresponses across fill-in-the-blank, patient-doctor Q&A, GEC, and paraphrased\nvariants.",
        "url": "http://arxiv.org/abs/2509.11365v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11365v1",
        "arxiv_id": "2509.11365v1",
        "authors": [
            "Mohamed Tarek",
            "Seif Ahmed",
            "Mohamed Basem"
        ],
        "submitted": "2025-09-14 17:39:58",
        "source": "arxiv",
        "comment": "8 Pages , ArabicNLP 2025 , Co-located with EMNLP 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The use of Large Language Models (LLMs) and prompt engineering for Arabic clinical question answering also aligns with your interests in NLP and deep semantic understanding. However, the focus on question answering and clinical contexts is somewhat specific and not directly related to your e-commerce background."
    },
    {
        "title": "Ko-PIQA: A Korean Physical Commonsense Reasoning Dataset with Cultural Context",
        "abstract": "Physical commonsense reasoning datasets like PIQA are predominantly\nEnglish-centric and lack cultural diversity. We introduce Ko-PIQA, a Korean\nphysical commonsense reasoning dataset that incorporates cultural context.\nStarting from 3.01 million web-crawled questions, we employed a multi-stage\nfiltering approach using three language models to identify 11,553 PIQA-style\nquestions. Through GPT-4o refinement and human validation, we obtained 441\nhigh-quality question-answer pairs. A key feature of Ko-PIQA is its cultural\ngrounding: 19.7\\% of questions contain culturally specific elements like\ntraditional Korean foods (kimchi), clothing (hanbok), and specialized\nappliances (kimchi refrigerators) that require culturally-aware reasoning\nbeyond direct translation. We evaluate seven language models on Ko-PIQA, with\nthe best model achieving 83.22\\% accuracy while the weakest reaches only\n59.86\\%, demonstrating significant room for improvement. Models particularly\nstruggle with culturally specific scenarios, highlighting the importance of\nculturally diverse datasets. Ko-PIQA serves as both a benchmark for Korean\nlanguage models and a foundation for more inclusive commonsense reasoning\nresearch. The dataset and code will be publicly available.",
        "url": "http://arxiv.org/abs/2509.11303v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11303v1",
        "arxiv_id": "2509.11303v1",
        "authors": [
            "Dasol Choi",
            "Jungwhan Kim",
            "Guijin Son"
        ],
        "submitted": "2025-09-14 14:47:04",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'search' (score: +1)",
            "Found 'korea' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on creating a Korean physical commonsense reasoning dataset with cultural context, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves language models, the primary goal is to create a culturally diverse dataset for commonsense reasoning, which is not a central match for your research focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "The Prompt Engineering Report Distilled: Quick Start Guide for Life Sciences",
        "abstract": "Developing effective prompts demands significant cognitive investment to\ngenerate reliable, high-quality responses from Large Language Models (LLMs). By\ndeploying case-specific prompt engineering techniques that streamline\nfrequently performed life sciences workflows, researchers could achieve\nsubstantial efficiency gains that far exceed the initial time investment\nrequired to master these techniques. The Prompt Report published in 2025\noutlined 58 different text-based prompt engineering techniques, highlighting\nthe numerous ways prompts could be constructed. To provide actionable\nguidelines and reduce the friction of navigating these various approaches, we\ndistil this report to focus on 6 core techniques: zero-shot, few-shot\napproaches, thought generation, ensembling, self-criticism, and decomposition.\nWe breakdown the significance of each approach and ground it in use cases\nrelevant to life sciences, from literature summarization and data extraction to\neditorial tasks. We provide detailed recommendations for how prompts should and\nshouldn't be structured, addressing common pitfalls including multi-turn\nconversation degradation, hallucinations, and distinctions between reasoning\nand non-reasoning models. We examine context window limitations, agentic tools\nlike Claude Code, while analyzing the effectiveness of Deep Research tools\nacross OpenAI, Google, Anthropic and Perplexity platforms, discussing current\nlimitations. We demonstrate how prompt engineering can augment rather than\nreplace existing established individual practices around data processing and\ndocument editing. Our aim is to provide actionable guidance on core prompt\nengineering principles, and to facilitate the transition from opportunistic\nprompting to an effective, low-friction systematic practice that contributes to\nhigher quality research.",
        "url": "http://arxiv.org/abs/2509.11295v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11295v1",
        "arxiv_id": "2509.11295v1",
        "authors": [
            "Valentin Romanov",
            "Steven A Niederer"
        ],
        "submitted": "2025-09-14 14:39:35",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on prompt engineering for Large Language Models in the life sciences domain, which is unrelated to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on NLP, it's more about optimizing model inputs rather than deep semantic understanding or real-time relevance optimization."
    },
    {
        "title": "Mitigating Hallucinations in Large Vision-Language Models by Self-Injecting Hallucinations",
        "abstract": "Large Vision-Language Models (LVLMs) suffer from serious hallucination\nproblems, where the model-generated responses are inconsistent with the visual\ninputs. Existing hallucination mitigation methods are mainly based on\npreference alignment and require external human annotations or auxiliary models\nfor preference data collection, which increase costs and limit sustainable\nimprovement. To tackle these challenges, we propose Autonomous Preference\nAlignment via Self-Injection (APASI), a novel and generalizable method that\nmitigates hallucinations without external dependencies. APASI leverages the\ntarget LVLM to self-inject hallucinations into a generated response, creating a\npair of responses with varying preference levels. During the self-injection\nprocess, the dis-preferred response is generated based on three key\nobservations of hallucinations, ensuring it simulates real hallucination\npatterns. This fidelity offers an accurate learning signal for hallucination\nmitigation. Moreover, APASI incorporates an iterative alignment training\nstrategy combined with curriculum learning to periodically update the\npreference data with increasing challenge, enabling stable and continuous\nenhancement of the LVLM. Extensive experiments across six benchmarks show that\nAPASI not only effectively mitigates hallucinations for three baseline models\nbut also achieves comparable or even superior performance to alignment-based\nmethods with external dependency, thereby demonstrating its effectiveness and\ngeneralization capability. The code is available at\nhttps://github.com/davidluciolu/APASI.",
        "url": "http://arxiv.org/abs/2509.11287v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11287v1",
        "arxiv_id": "2509.11287v1",
        "authors": [
            "Yifan Lu",
            "Ziqi Zhang",
            "Chunfeng Yuan",
            "Jun Gao",
            "Congxuan Zhang",
            "Xiaojuan Qi",
            "Bing Li",
            "Weiming Hu"
        ],
        "submitted": "2025-09-14 14:26:53",
        "source": "arxiv",
        "comment": "emnlp 2025 accepted",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on mitigating hallucinations in Vision-Language Models, which is not directly related to Information Retrieval or Search technologies. While it involves deep semantic understanding, the context is different from the user's primary research interests in IR and NLP."
    },
    {
        "title": "Evalet: Evaluating Large Language Models by Fragmenting Outputs into Functions",
        "abstract": "Practitioners increasingly rely on Large Language Models (LLMs) to evaluate\ngenerative AI outputs through \"LLM-as-a-Judge\" approaches. However, these\nmethods produce holistic scores that obscure which specific elements influenced\nthe assessments. We propose functional fragmentation, a method that dissects\neach output into key fragments and interprets the rhetoric functions that each\nfragment serves relative to evaluation criteria -- surfacing the elements of\ninterest and revealing how they fulfill or hinder user goals. We instantiate\nthis approach in Evalet, an interactive system that visualizes fragment-level\nfunctions across many outputs to support inspection, rating, and comparison of\nevaluations. A user study (N=10) found that, while practitioners struggled to\nvalidate holistic scores, our approach helped them identify 48% more evaluation\nmisalignments. This helped them calibrate trust in LLM evaluations and rely on\nthem to find more actionable issues in model outputs. Our work shifts LLM\nevaluation from quantitative scores toward qualitative, fine-grained analysis\nof model behavior.",
        "url": "http://arxiv.org/abs/2509.11206v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11206v1",
        "arxiv_id": "2509.11206v1",
        "authors": [
            "Tae Soo Kim",
            "Heechan Lee",
            "Yoonjoo Lee",
            "Joseph Seering",
            "Juho Kim"
        ],
        "submitted": "2025-09-14 10:24:13",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 3,
        "llm_reason": "This paper is somewhat related to the user's research interests in Natural Language Processing (NLP) and Large Language Models, but it does not directly align with the user's primary focus on Information Retrieval, query understanding, and ranking models. The paper's focus on evaluating LLMs and their outputs is tangentially related to the user's interests in deep semantic understanding and real-time relevance optimization, but it does not appear to be a central match."
    },
    {
        "title": "DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation",
        "abstract": "Vision-and-Language Navigation in Continuous Environments (VLN-CE), which\nlinks language instructions to perception and control in the real world, is a\ncore capability of embodied robots. Recently, large-scale pretrained foundation\nmodels have been leveraged as shared priors for perception, reasoning, and\naction, enabling zero-shot VLN without task-specific training. However,\nexisting zero-shot VLN methods depend on costly perception and passive scene\nunderstanding, collapsing control to point-level choices. As a result, they are\nexpensive to deploy, misaligned in action semantics, and short-sighted in\nplanning. To address these issues, we present DreamNav that focuses on the\nfollowing three aspects: (1) for reducing sensory cost, our EgoView Corrector\naligns viewpoints and stabilizes egocentric perception; (2) instead of\npoint-level actions, our Trajectory Predictor favors global trajectory-level\nplanning to better align with instruction semantics; and (3) to enable\nanticipatory and long-horizon planning, we propose an Imagination Predictor to\nendow the agent with proactive thinking capability. On VLN-CE and real-world\ntests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming the\nstrongest egocentric baseline with extra information by up to 7.49\\% and\n18.15\\% in terms of SR and SPL metrics. To our knowledge, this is the first\nzero-shot VLN method to unify trajectory-level planning and active imagination\nwhile using only egocentric inputs.",
        "url": "http://arxiv.org/abs/2509.11197v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11197v1",
        "arxiv_id": "2509.11197v1",
        "authors": [
            "Yunheng Wang",
            "Yuetong Fang",
            "Taowen Wang",
            "Yixiao Feng",
            "Yawen Tan",
            "Shuning Zhang",
            "Peiran Liu",
            "Yiding Ji",
            "Renjing Xu"
        ],
        "submitted": "2025-09-14 09:54:20",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Vision-and-Language Navigation, a topic outside of the user's primary research interests in Information Retrieval and Search technologies. While it involves some aspects of Natural Language Processing, the paper's primary contributions and applications are in robotics and embodied AI, which do not align with the user's research background."
    },
    {
        "title": "Joint Effects of Argumentation Theory, Audio Modality and Data Enrichment on LLM-Based Fallacy Classification",
        "abstract": "This study investigates how context and emotional tone metadata influence\nlarge language model (LLM) reasoning and performance in fallacy classification\ntasks, particularly within political debate settings. Using data from U.S.\npresidential debates, we classify six fallacy types through various prompting\nstrategies applied to the Qwen-3 (8B) model. We introduce two theoretically\ngrounded Chain-of-Thought frameworks: Pragma-Dialectics and the Periodic Table\nof Arguments, and evaluate their effectiveness against a baseline prompt under\nthree input settings: text-only, text with context, and text with both context\nand audio-based emotional tone metadata. Results suggest that while theoretical\nprompting can improve interpretability and, in some cases, accuracy, the\naddition of context and especially emotional tone metadata often leads to\nlowered performance. Emotional tone metadata biases the model toward labeling\nstatements as \\textit{Appeal to Emotion}, worsening logical reasoning. Overall,\nbasic prompts often outperformed enhanced ones, suggesting that attention\ndilution from added inputs may worsen rather than improve fallacy\nclassification in LLMs.",
        "url": "http://arxiv.org/abs/2509.11127v1",
        "pdf_url": "http://arxiv.org/pdf/2509.11127v1",
        "arxiv_id": "2509.11127v1",
        "authors": [
            "Hongxu Zhou",
            "Hylke Westerdijk",
            "Khondoker Ittehadul Islam"
        ],
        "submitted": "2025-09-14 06:35:34",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Natural Language Processing (NLP) and deep semantic understanding, but it focuses on fallacy classification and argumentation theory, which is not a central match to your primary focus on Information Retrieval and Search technologies. The paper's use of large language models and data enrichment is somewhat relevant, but the specific application and results do not align closely with your research themes."
    }
]