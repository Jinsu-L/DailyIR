[
    {
        "title": "Controlling Multimodal LLMs via Reward-guided Decoding",
        "abstract": "As Multimodal Large Language Models (MLLMs) gain widespread applicability, it\nis becoming increasingly desirable to adapt them for diverse user needs. In\nthis paper, we study the adaptation of MLLMs through controlled decoding. To\nachieve this, we introduce the first method for reward-guided decoding of MLLMs\nand demonstrate its application in improving their visual grounding. Our method\ninvolves building reward models for visual grounding and using them to guide\nthe MLLM's decoding process. Concretely, we build two separate reward models to\nindependently control the degree of object precision and recall in the model's\noutput. Our approach enables on-the-fly controllability of an MLLM's inference\nprocess in two ways: first, by giving control over the relative importance of\neach reward function during decoding, allowing a user to dynamically trade off\nobject precision for recall in image captioning tasks; second, by giving\ncontrol over the breadth of the search during decoding, allowing the user to\ncontrol the trade-off between the amount of test-time compute and the degree of\nvisual grounding. We evaluate our method on standard object hallucination\nbenchmarks, showing that it provides significant controllability over MLLM\ninference, while consistently outperforming existing hallucination mitigation\nmethods.",
        "url": "http://arxiv.org/abs/2508.11616v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11616v1",
        "arxiv_id": "2508.11616v1",
        "authors": [
            "Oscar Mañas",
            "Pierluca D'Oro",
            "Koustuv Sinha",
            "Adriana Romero-Soriano",
            "Michal Drozdzal",
            "Aishwarya Agrawal"
        ],
        "submitted": "2025-08-15 17:29:06",
        "source": "arxiv",
        "comment": "Published at ICCV 2025"
    },
    {
        "title": "Pretrained Conformers for Audio Fingerprinting and Retrieval",
        "abstract": "Conformers have shown great results in speech processing due to their ability\nto capture both local and global interactions. In this work, we utilize a\nself-supervised contrastive learning framework to train conformer-based\nencoders that are capable of generating unique embeddings for small segments of\naudio, generalizing well to previously unseen data. We achieve state-of-the-art\nresults for audio retrieval tasks while using only 3 seconds of audio to\ngenerate embeddings. Our models are almost completely immune to temporal\nmisalignments and achieve state-of-the-art results in cases of other audio\ndistortions such as noise, reverb or extreme temporal stretching. Code and\nmodels are made publicly available and the results are easy to reproduce as we\ntrain and test using popular and freely available datasets of different sizes.",
        "url": "http://arxiv.org/abs/2508.11609v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11609v1",
        "arxiv_id": "2508.11609v1",
        "authors": [
            "Kemal Altwlkany",
            "Elmedin Selmanovic",
            "Sead Delalic"
        ],
        "submitted": "2025-08-15 17:19:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TinyTim: A Family of Language Models for Divergent Generation",
        "abstract": "This work introduces TinyTim, a family of large language models fine-tuned on\nJames Joyce's `Finnegans Wake'. Through quantitative evaluation against\nbaseline models, we demonstrate that TinyTim V1 produces a statistically\ndistinct generative profile characterized by high lexical diversity and low\nsemantic coherence. These findings are interpreted through theories of\ncreativity and complex problem-solving, arguing that such specialized models\ncan function as divergent knowledge sources within more extensive creative\narchitectures, powering automated discovery mechanisms in diverse settings.",
        "url": "http://arxiv.org/abs/2508.11607v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11607v1",
        "arxiv_id": "2508.11607v1",
        "authors": [
            "Christopher J. Agostino"
        ],
        "submitted": "2025-08-15 17:14:29",
        "source": "arxiv",
        "comment": "7 pages, 3 figures, submitted to NeurIPS Creative AI track, code and\n  model available at https://hf.co/npc-worldwide/TinyTimV1"
    },
    {
        "title": "Dataset Creation for Visual Entailment using Generative AI",
        "abstract": "In this paper we present and validate a new synthetic dataset for training\nvisual entailment models. Existing datasets for visual entailment are small and\nsparse compared to datasets for textual entailment. Manually creating datasets\nis labor-intensive. We base our synthetic dataset on the SNLI dataset for\ntextual entailment. We take the premise text from SNLI as input prompts in a\ngenerative image model, Stable Diffusion, creating an image to replace each\ntextual premise. We evaluate our dataset both intrinsically and extrinsically.\nFor extrinsic evaluation, we evaluate the validity of the generated images by\nusing them as training data for a visual entailment classifier based on CLIP\nfeature vectors. We find that synthetic training data only leads to a slight\ndrop in quality on SNLI-VE, with an F-score 0.686 compared to 0.703 when\ntrained on real data. We also compare the quality of our generated training\ndata to original training data on another dataset: SICK-VTE. Again, there is\nonly a slight drop in F-score: from 0.400 to 0.384. These results indicate that\nin settings with data sparsity, synthetic data can be a promising solution for\ntraining visual entailment models.",
        "url": "http://arxiv.org/abs/2508.11605v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11605v1",
        "arxiv_id": "2508.11605v1",
        "authors": [
            "Rob Reijtenbach",
            "Suzan Verberne",
            "Gijs Wijnholds"
        ],
        "submitted": "2025-08-15 17:13:41",
        "source": "arxiv",
        "comment": "NALOMA: Natural Logic meets Machine Learning workshop @ ESSLLI 2025"
    },
    {
        "title": "Representing Speech Through Autoregressive Prediction of Cochlear Tokens",
        "abstract": "We introduce AuriStream, a biologically inspired model for encoding speech\nvia a two-stage framework inspired by the human auditory processing hierarchy.\nThe first stage transforms raw audio into a time-frequency representation based\non the human cochlea, from which we extract discrete \\textbf{cochlear tokens}.\nThe second stage applies an autoregressive sequence model over the cochlear\ntokens. AuriStream learns meaningful phoneme and word representations, and\nstate-of-the-art lexical semantics. AuriStream shows competitive performance on\ndiverse downstream SUPERB speech tasks. Complementing AuriStream's strong\nrepresentational capabilities, it generates continuations of audio which can be\nvisualized in a spectrogram space and decoded back into audio, providing\ninsights into the model's predictions. In summary, we present a two-stage\nframework for speech representation learning to advance the development of more\nhuman-like models that efficiently handle a range of speech-based tasks.",
        "url": "http://arxiv.org/abs/2508.11598v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11598v1",
        "arxiv_id": "2508.11598v1",
        "authors": [
            "Greta Tuckute",
            "Klemen Kotar",
            "Evelina Fedorenko",
            "Daniel L. K. Yamins"
        ],
        "submitted": "2025-08-15 17:06:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Aware First, Think Less: Dynamic Boundary Self-Awareness Drives Extreme Reasoning Efficiency in Large Language Models",
        "abstract": "Recent advancements in large language models (LLMs) have greatly improved\ntheir capabilities on complex reasoning tasks through Long Chain-of-Thought\n(CoT). However, this approach often results in substantial redundancy,\nimpairing computational efficiency and causing significant delays in real-time\napplications. To improve the efficiency, current methods often rely on\nhuman-defined difficulty priors, which do not align with the LLM's self-awared\ndifficulty, leading to inefficiencies. In this paper, we introduce the Dynamic\nReasoning-Boundary Self-Awareness Framework (DR. SAF), which enables models to\ndynamically assess and adjust their reasoning depth in response to problem\ncomplexity. DR. SAF integrates three key components: Boundary Self-Awareness\nAlignment, Adaptive Reward Management, and a Boundary Preservation Mechanism.\nThese components allow models to optimize their reasoning processes, balancing\nefficiency and accuracy without compromising performance. Our experimental\nresults demonstrate that DR. SAF achieves a 49.27% reduction in total response\ntokens with minimal loss in accuracy. The framework also delivers a 6.59x gain\nin token efficiency and a 5x reduction in training time, making it well-suited\nto resource-limited settings. During extreme training, DR. SAF can even surpass\ntraditional instruction-based models in token efficiency with more than 16%\naccuracy improvement.",
        "url": "http://arxiv.org/abs/2508.11582v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11582v1",
        "arxiv_id": "2508.11582v1",
        "authors": [
            "Qiguang Chen",
            "Dengyun Peng",
            "Jinhao Liu",
            "HuiKang Su",
            "Jiannan Guan",
            "Libo Qin",
            "Wanxiang Che"
        ],
        "submitted": "2025-08-15 16:40:29",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "TrajSV: A Trajectory-based Model for Sports Video Representations and Applications",
        "abstract": "Sports analytics has received significant attention from both academia and\nindustry in recent years. Despite the growing interest and efforts in this\nfield, several issues remain unresolved, including (1) data unavailability, (2)\nlack of an effective trajectory-based framework, and (3) requirement for\nsufficient supervision labels. In this paper, we present TrajSV, a\ntrajectory-based framework that addresses various issues in existing studies.\nTrajSV comprises three components: data preprocessing, Clip Representation\nNetwork (CRNet), and Video Representation Network (VRNet). The data\npreprocessing module extracts player and ball trajectories from sports\nbroadcast videos. CRNet utilizes a trajectory-enhanced Transformer module to\nlearn clip representations based on these trajectories. Additionally, VRNet\nlearns video representations by aggregating clip representations and visual\nfeatures with an encoder-decoder architecture. Finally, a triple contrastive\nloss is introduced to optimize both video and clip representations in an\nunsupervised manner. The experiments are conducted on three broadcast video\ndatasets to verify the effectiveness of TrajSV for three types of sports (i.e.,\nsoccer, basketball, and volleyball) with three downstream applications (i.e.,\nsports video retrieval, action spotting, and video captioning). The results\ndemonstrate that TrajSV achieves state-of-the-art performance in sports video\nretrieval, showcasing a nearly 70% improvement. It outperforms baselines in\naction spotting, achieving state-of-the-art results in 9 out of 17 action\ncategories, and demonstrates a nearly 20% improvement in video captioning.\nAdditionally, we introduce a deployed system along with the three applications\nbased on TrajSV.",
        "url": "http://arxiv.org/abs/2508.11569v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11569v1",
        "arxiv_id": "2508.11569v1",
        "authors": [
            "Zheng Wang",
            "Shihao Xu",
            "Wei Shi"
        ],
        "submitted": "2025-08-15 16:23:36",
        "source": "arxiv",
        "comment": "This paper has been accepted by TCSVT"
    },
    {
        "title": "AgentMental: An Interactive Multi-Agent Framework for Explainable and Adaptive Mental Health Assessment",
        "abstract": "Mental health assessment is crucial for early intervention and effective\ntreatment, yet traditional clinician-based approaches are limited by the\nshortage of qualified professionals. Recent advances in artificial intelligence\nhave sparked growing interest in automated psychological assessment, yet most\nexisting approaches are constrained by their reliance on static text analysis,\nlimiting their ability to capture deeper and more informative insights that\nemerge through dynamic interaction and iterative questioning. Therefore, in\nthis paper, we propose a multi-agent framework for mental health evaluation\nthat simulates clinical doctor-patient dialogues, with specialized agents\nassigned to questioning, adequacy evaluation, scoring, and updating. We\nintroduce an adaptive questioning mechanism in which an evaluation agent\nassesses the adequacy of user responses to determine the necessity of\ngenerating targeted follow-up queries to address ambiguity and missing\ninformation. Additionally, we employ a tree-structured memory in which the root\nnode encodes the user's basic information, while child nodes (e.g., topic and\nstatement) organize key information according to distinct symptom categories\nand interaction turns. This memory is dynamically updated throughout the\ninteraction to reduce redundant questioning and further enhance the information\nextraction and contextual tracking capabilities. Experimental results on the\nDAIC-WOZ dataset illustrate the effectiveness of our proposed method, which\nachieves better performance than existing approaches.",
        "url": "http://arxiv.org/abs/2508.11567v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11567v1",
        "arxiv_id": "2508.11567v1",
        "authors": [
            "Jinpeng Hu",
            "Ao Wang",
            "Qianqian Xie",
            "Hui Ma",
            "Zhuo Li",
            "Dan Guo"
        ],
        "submitted": "2025-08-15 16:20:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Emphasis Sensitivity in Speech Representations",
        "abstract": "This work investigates whether modern speech models are sensitive to prosodic\nemphasis - whether they encode emphasized and neutral words in systematically\ndifferent ways. Prior work typically relies on isolated acoustic correlates\n(e.g., pitch, duration) or label prediction, both of which miss the relational\nstructure of emphasis. This paper proposes a residual-based framework, defining\nemphasis as the difference between paired neutral and emphasized word\nrepresentations. Analysis on self-supervised speech models shows that these\nresiduals correlate strongly with duration changes and perform poorly at word\nidentity prediction, indicating a structured, relational encoding of prosodic\nemphasis. In ASR fine-tuned models, residuals occupy a subspace up to 50% more\ncompact than in pre-trained models, further suggesting that emphasis is encoded\nas a consistent, low-dimensional transformation that becomes more structured\nwith task-specific learning.",
        "url": "http://arxiv.org/abs/2508.11566v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11566v1",
        "arxiv_id": "2508.11566v1",
        "authors": [
            "Shaun Cassini",
            "Thomas Hain",
            "Anton Ragni"
        ],
        "submitted": "2025-08-15 16:18:47",
        "source": "arxiv",
        "comment": "Accepted to IEEE ASRU 2025"
    },
    {
        "title": "INFNet: A Task-aware Information Flow Network for Large-Scale Recommendation Systems",
        "abstract": "Feature interaction has long been a cornerstone of ranking models in\nlarge-scale recommender systems due to its proven effectiveness in capturing\ncomplex dependencies among features. However, existing feature interaction\nstrategies face two critical challenges in industrial applications: (1) The\nvast number of categorical and sequential features makes exhaustive interaction\ncomputationally prohibitive, often resulting in optimization difficulties. (2)\nReal-world recommender systems typically involve multiple prediction\nobjectives, yet most current approaches apply feature interaction modules prior\nto the multi-task learning layers. This late-fusion design overlooks\ntask-specific feature dependencies and inherently limits the capacity of\nmulti-task modeling. To address these limitations, we propose the Information\nFlow Network (INFNet), a task-aware architecture designed for large-scale\nrecommendation scenarios. INFNet distinguishes features into three token types,\ncategorical tokens, sequence tokens, and task tokens, and introduces a novel\ndual-flow design comprising heterogeneous and homogeneous alternating\ninformation blocks. For heterogeneous information flow, we employ a\ncross-attention mechanism with proxy that facilitates efficient cross-modal\ntoken interaction with balanced computational cost. For homogeneous flow, we\ndesign type-specific Proxy Gated Units (PGUs) to enable fine-grained intra-type\nfeature processing. Extensive experiments on multiple offline benchmarks\nconfirm that INFNet achieves state-of-the-art performance. Moreover, INFNet has\nbeen successfully deployed in a commercial online advertising system, yielding\nsignificant gains of +1.587% in Revenue (REV) and +1.155% in Click-Through Rate\n(CTR).",
        "url": "http://arxiv.org/abs/2508.11565v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11565v1",
        "arxiv_id": "2508.11565v1",
        "authors": [
            "Kaiyuan Li",
            "Dongdong Mao",
            "Yongxiang Tang",
            "Yanhua Cheng",
            "Yanxiang Zeng",
            "Chao Wang",
            "Xialong Liu",
            "Peng Jiang"
        ],
        "submitted": "2025-08-15 16:18:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Language models align with brain regions that represent concepts across modalities",
        "abstract": "Cognitive science and neuroscience have long faced the challenge of\ndisentangling representations of language from representations of conceptual\nmeaning. As the same problem arises in today's language models (LMs), we\ninvestigate the relationship between LM--brain alignment and two neural\nmetrics: (1) the level of brain activation during processing of sentences,\ntargeting linguistic processing, and (2) a novel measure of meaning consistency\nacross input modalities, which quantifies how consistently a brain region\nresponds to the same concept across paradigms (sentence, word cloud, image)\nusing an fMRI dataset (Pereira et al., 2018). Our experiments show that both\nlanguage-only and language-vision models predict the signal better in more\nmeaning-consistent areas of the brain, even when these areas are not strongly\nsensitive to language processing, suggesting that LMs might internally\nrepresent cross-modal conceptual meaning.",
        "url": "http://arxiv.org/abs/2508.11536v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11536v1",
        "arxiv_id": "2508.11536v1",
        "authors": [
            "Maria Ryskina",
            "Greta Tuckute",
            "Alexander Fung",
            "Ashley Malkin",
            "Evelina Fedorenko"
        ],
        "submitted": "2025-08-15 15:32:19",
        "source": "arxiv",
        "comment": "Accepted to COLM 2025. Code and data can be found at\n  https://github.com/ryskina/concepts-brain-llms"
    },
    {
        "title": "Speciesism in AI: Evaluating Discrimination Against Animals in Large Language Models",
        "abstract": "As large language models (LLMs) become more widely deployed, it is crucial to\nexamine their ethical tendencies. Building on research on fairness and\ndiscrimination in AI, we investigate whether LLMs exhibit speciesist bias --\ndiscrimination based on species membership -- and how they value non-human\nanimals. We systematically examine this issue across three paradigms: (1)\nSpeciesismBench, a 1,003-item benchmark assessing recognition and moral\nevaluation of speciesist statements; (2) established psychological measures\ncomparing model responses with those of human participants; (3) text-generation\ntasks probing elaboration on, or resistance to, speciesist rationalizations. In\nour benchmark, LLMs reliably detected speciesist statements but rarely\ncondemned them, often treating speciesist attitudes as morally acceptable. On\npsychological measures, results were mixed: LLMs expressed slightly lower\nexplicit speciesism than people, yet in direct trade-offs they more often chose\nto save one human over multiple animals. A tentative interpretation is that\nLLMs may weight cognitive capacity rather than species per se: when capacities\nwere equal, they showed no species preference, and when an animal was described\nas more capable, they tended to prioritize it over a less capable human. In\nopen-ended text generation tasks, LLMs frequently normalized or rationalized\nharm toward farmed animals while refusing to do so for non-farmed animals.\nThese findings suggest that while LLMs reflect a mixture of progressive and\nmainstream human views, they nonetheless reproduce entrenched cultural norms\naround animal exploitation. We argue that expanding AI fairness and alignment\nframeworks to explicitly include non-human moral patients is essential for\nreducing these biases and preventing the entrenchment of speciesist attitudes\nin AI systems and the societies they influence.",
        "url": "http://arxiv.org/abs/2508.11534v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11534v1",
        "arxiv_id": "2508.11534v1",
        "authors": [
            "Monika Jotautaitė",
            "Lucius Caviola",
            "David A. Brewster",
            "Thilo Hagendorff"
        ],
        "submitted": "2025-08-15 15:22:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "When Algorithms Mirror Minds: A Confirmation-Aware Social Dynamic Model of Echo Chamber and Homogenization Traps",
        "abstract": "Recommender systems increasingly suffer from echo chambers and user\nhomogenization, systemic distortions arising from the dynamic interplay between\nalgorithmic recommendations and human behavior. While prior work has studied\nthese phenomena through the lens of algorithmic bias or social network\nstructure, we argue that the psychological mechanisms of users and the\nclosed-loop interaction between users and recommenders are critical yet\nunderstudied drivers of these emergent effects. To bridge this gap, we propose\nthe Confirmation-Aware Social Dynamic Model which incorporates user psychology\nand social relationships to simulate the actual user and recommender\ninteraction process. Our theoretical analysis proves that echo chambers and\nhomogenization traps, defined respectively as reduced recommendation diversity\nand homogenized user representations, will inevitably occur. We also conduct\nextensive empirical simulations on two real-world datasets and one synthetic\ndataset with five well-designed metrics, exploring the root factors influencing\nthe aforementioned phenomena from three level perspectives: the stochasticity\nand social integration degree of recommender (system-level), the psychological\nmechanisms of users (user-level), and the dataset scale (platform-level).\nFurthermore, we demonstrate four practical mitigation strategies that help\nalleviate echo chambers and user homogenization at the cost of some\nrecommendation accuracy. Our findings provide both theoretical and empirical\ninsights into the emergence and drivers of echo chambers and user\nhomogenization, as well as actionable guidelines for human-centered recommender\ndesign.",
        "url": "http://arxiv.org/abs/2508.11516v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11516v1",
        "arxiv_id": "2508.11516v1",
        "authors": [
            "Ming Tang",
            "Xiaowen Huang",
            "Jitao Sang"
        ],
        "submitted": "2025-08-15 14:55:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Reference Points in LLM Sentiment Analysis: The Role of Structured Context",
        "abstract": "Large language models (LLMs) are now widely used across many fields,\nincluding marketing research. Sentiment analysis, in particular, helps firms\nunderstand consumer preferences. While most NLP studies classify sentiment from\nreview text alone, marketing theories, such as prospect theory and\nexpectation--disconfirmation theory, point out that customer evaluations are\nshaped not only by the actual experience but also by additional reference\npoints. This study therefore investigates how the content and format of such\nsupplementary information affect sentiment analysis using LLMs. We compare\nnatural language (NL) and JSON-formatted prompts using a lightweight 3B\nparameter model suitable for practical marketing applications. Experiments on\ntwo Yelp categories (Restaurant and Nightlife) show that the JSON prompt with\nadditional information outperforms all baselines without fine-tuning: Macro-F1\nrises by 1.6% and 4% while RMSE falls by 16% and 9.1%, respectively, making it\ndeployable in resource-constrained edge devices. Furthermore, a follow-up\nanalysis confirms that performance gains stem from genuine contextual reasoning\nrather than label proxying. This work demonstrates that structured prompting\ncan enable smaller models to achieve competitive performance, offering a\npractical alternative to large-scale model deployment.",
        "url": "http://arxiv.org/abs/2508.11454v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11454v1",
        "arxiv_id": "2508.11454v1",
        "authors": [
            "Junichiro Niimi"
        ],
        "submitted": "2025-08-15 13:04:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Inclusion Arena: An Open Platform for Evaluating Large Foundation Models with Real-World Apps",
        "abstract": "Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs)\nhave ushered in a new era of AI capabilities, demonstrating near-human-level\nperformance across diverse scenarios. While numerous benchmarks (e.g., MMLU)\nand leaderboards (e.g., Chatbot Arena) have been proposed to help evolve the\ndevelopment of LLMs and MLLMs, most rely on static datasets or crowdsourced\ngeneral-domain prompts, often falling short of reflecting performance in\nreal-world applications. To bridge this critical gap, we present Inclusion\nArena, a live leaderboard that ranks models based on human feedback collected\ndirectly from AI-powered applications. Our platform integrates pairwise model\ncomparisons into natural user interactions, ensuring evaluations reflect\npractical usage scenarios. For robust model ranking, we employ the\nBradley-Terry model augmented with two key innovations: (1) Placement Matches,\na cold-start mechanism to quickly estimate initial ratings for newly integrated\nmodels, and (2) Proximity Sampling, an intelligent comparison strategy that\nprioritizes battles between models of similar capabilities to maximize\ninformation gain and enhance rating stability. Extensive empirical analyses and\nsimulations demonstrate that Inclusion Arena yields reliable and stable\nrankings, exhibits higher data transitivity compared to general crowdsourced\ndatasets, and significantly mitigates the risk of malicious manipulation. By\nfostering an open alliance between foundation models and real-world\napplications, Inclusion Arena aims to accelerate the development of LLMs and\nMLLMs truly optimized for practical, user-centric deployments. The platform is\npublicly accessible at https://doraemon.alipay.com/model-ranking.",
        "url": "http://arxiv.org/abs/2508.11452v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11452v1",
        "arxiv_id": "2508.11452v1",
        "authors": [
            "Kangyu Wang",
            "Hongliang He",
            "Lin Liu",
            "Ruiqi Liang",
            "Zhenzhong Lan",
            "Jianguo Li"
        ],
        "submitted": "2025-08-15 13:00:07",
        "source": "arxiv",
        "comment": "Our platform is publicly accessible at\n  https://doraemon.alipay.com/model-ranking"
    },
    {
        "title": "CoDiEmb: A Collaborative yet Distinct Framework for Unified Representation Learning in Information Retrieval and Semantic Textual Similarity",
        "abstract": "Learning unified text embeddings that excel across diverse downstream tasks\nis a central goal in representation learning, yet negative transfer remains a\npersistent obstacle. This challenge is particularly pronounced when jointly\ntraining a single encoder for Information Retrieval (IR) and Semantic Textual\nSimilarity (STS), two essential but fundamentally disparate tasks for which\nnaive co-training typically yields steep performance trade-offs. We argue that\nresolving this conflict requires systematically decoupling task-specific\nlearning signals throughout the training pipeline. To this end, we introduce\nCoDiEmb, a unified framework that reconciles the divergent requirements of IR\nand STS in a collaborative yet distinct manner. CoDiEmb integrates three key\ninnovations for effective joint optimization: (1) Task-specialized objectives\npaired with a dynamic sampler that forms single-task batches and balances\nper-task updates, thereby preventing gradient interference. For IR, we employ a\ncontrastive loss with multiple positives and hard negatives, augmented by\ncross-device sampling. For STS, we adopt order-aware objectives that directly\noptimize correlation and ranking consistency. (2) A delta-guided model fusion\nstrategy that computes fine-grained merging weights for checkpoints by\nanalyzing each parameter's deviation from its pre-trained initialization,\nproving more effective than traditional Model Soups. (3) An efficient,\nsingle-stage training pipeline that is simple to implement and converges\nstably. Extensive experiments on 15 standard IR and STS benchmarks across three\nbase encoders validate CoDiEmb. Our results and analysis demonstrate that the\nframework not only mitigates cross-task trade-offs but also measurably improves\nthe geometric properties of the embedding space.",
        "url": "http://arxiv.org/abs/2508.11442v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11442v1",
        "arxiv_id": "2508.11442v1",
        "authors": [
            "Bowen Zhang",
            "Zixin Song",
            "Chunquan Chen",
            "Qian-Wen Zhang",
            "Di Yin",
            "Xing Sun"
        ],
        "submitted": "2025-08-15 12:46:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Online Anti-sexist Speech: Identifying Resistance to Gender Bias in Political Discourse",
        "abstract": "Anti-sexist speech, i.e., public expressions that challenge or resist\ngendered abuse and sexism, plays a vital role in shaping democratic debate\nonline. Yet automated content moderation systems, increasingly powered by large\nlanguage models (LLMs), may struggle to distinguish such resistance from the\nsexism it opposes. This study examines how five LLMs classify sexist,\nanti-sexist, and neutral political tweets from the UK, focusing on\nhigh-salience trigger events involving female Members of Parliament in the year\n2022. Our analysis show that models frequently misclassify anti-sexist speech\nas harmful, particularly during politically charged events where rhetorical\nstyles of harm and resistance converge. These errors risk silencing those who\nchallenge sexism, with disproportionate consequences for marginalised voices.\nWe argue that moderation design must move beyond binary harmful/not-harmful\nschemas, integrate human-in-the-loop review during sensitive events, and\nexplicitly include counter-speech in training data. By linking feminist\nscholarship, event-based analysis, and model evaluation, this work highlights\nthe sociotechnical challenges of safeguarding resistance speech in digital\npolitical spaces.",
        "url": "http://arxiv.org/abs/2508.11434v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11434v1",
        "arxiv_id": "2508.11434v1",
        "authors": [
            "Aditi Dutta",
            "Susan Banducci"
        ],
        "submitted": "2025-08-15 12:24:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "HumorPlanSearch: Structured Planning and HuCoT for Contextual AI Humor",
        "abstract": "Automated humor generation with Large Language Models (LLMs) often yields\njokes that feel generic, repetitive, or tone-deaf because humor is deeply\nsituated and hinges on the listener's cultural background, mindset, and\nimmediate context. We introduce HumorPlanSearch, a modular pipeline that\nexplicitly models context through: (1) Plan-Search for diverse, topic-tailored\nstrategies; (2) Humor Chain-of-Thought (HuCoT) templates capturing cultural and\nstylistic reasoning; (3) a Knowledge Graph to retrieve and adapt\nhigh-performing historical strategies; (4) novelty filtering via semantic\nembeddings; and (5) an iterative judge-driven revision loop. To evaluate\ncontext sensitivity and comedic quality, we propose the Humor Generation Score\n(HGS), which fuses direct ratings, multi-persona feedback, pairwise win-rates,\nand topic relevance. In experiments across nine topics with feedback from 13\nhuman judges, our full pipeline (KG + Revision) boosts mean HGS by 15.4 percent\n(p < 0.05) over a strong baseline. By foregrounding context at every stage from\nstrategy planning to multi-signal evaluation, HumorPlanSearch advances\nAI-driven humor toward more coherent, adaptive, and culturally attuned comedy.",
        "url": "http://arxiv.org/abs/2508.11429v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11429v1",
        "arxiv_id": "2508.11429v1",
        "authors": [
            "Shivam Dubey"
        ],
        "submitted": "2025-08-15 12:07:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Survey-to-Behavior: Downstream Alignment of Human Values in LLMs via Survey Questions",
        "abstract": "Large language models implicitly encode preferences over human values, yet\nsteering them often requires large training data. In this work, we investigate\na simple approach: Can we reliably modify a model's value system in downstream\nbehavior by training it to answer value survey questions accordingly? We first\nconstruct value profiles of several open-source LLMs by asking them to rate a\nseries of value-related descriptions spanning 20 distinct human values, which\nwe use as a baseline for subsequent experiments. We then investigate whether\nthe value system of a model can be governed by fine-tuning on the value\nsurveys. We evaluate the effect of finetuning on the model's behavior in two\nways; first, we assess how answers change on in-domain, held-out survey\nquestions. Second, we evaluate whether the model's behavior changes in\nout-of-domain settings (situational scenarios). To this end, we construct a\ncontextualized moral judgment dataset based on Reddit posts and evaluate\nchanges in the model's behavior in text-based adventure games. We demonstrate\nthat our simple approach can not only change the model's answers to in-domain\nsurvey questions, but also produces substantial shifts (value alignment) in\nimplicit downstream task behavior.",
        "url": "http://arxiv.org/abs/2508.11414v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11414v1",
        "arxiv_id": "2508.11414v1",
        "authors": [
            "Shangrui Nie",
            "Florian Mai",
            "David Kaczér",
            "Charles Welch",
            "Zhixue Zhao",
            "Lucie Flek"
        ],
        "submitted": "2025-08-15 11:36:17",
        "source": "arxiv",
        "comment": "7 pages 1 figure"
    },
    {
        "title": "Trustworthy AI Psychotherapy: Multi-Agent LLM Workflow for Counseling and Explainable Mental Disorder Diagnosis",
        "abstract": "LLM-based agents have emerged as transformative tools capable of executing\ncomplex tasks through iterative planning and action, achieving significant\nadvancements in understanding and addressing user needs. Yet, their\neffectiveness remains limited in specialized domains such as mental health\ndiagnosis, where they underperform compared to general applications. Current\napproaches to integrating diagnostic capabilities into LLMs rely on scarce,\nhighly sensitive mental health datasets, which are challenging to acquire.\nThese methods also fail to emulate clinicians' proactive inquiry skills, lack\nmulti-turn conversational comprehension, and struggle to align outputs with\nexpert clinical reasoning. To address these gaps, we propose DSM5AgentFlow, the\nfirst LLM-based agent workflow designed to autonomously generate DSM-5 Level-1\ndiagnostic questionnaires. By simulating therapist-client dialogues with\nspecific client profiles, the framework delivers transparent, step-by-step\ndisorder predictions, producing explainable and trustworthy results. This\nworkflow serves as a complementary tool for mental health diagnosis, ensuring\nadherence to ethical and legal standards. Through comprehensive experiments, we\nevaluate leading LLMs across three critical dimensions: conversational realism,\ndiagnostic accuracy, and explainability. Our datasets and implementations are\nfully open-sourced.",
        "url": "http://arxiv.org/abs/2508.11398v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11398v1",
        "arxiv_id": "2508.11398v1",
        "authors": [
            "Mithat Can Ozgun",
            "Jiahuan Pei",
            "Koen Hindriks",
            "Lucia Donatelli",
            "Qingzhi Liu",
            "Xin Sun",
            "Junxiao Wang"
        ],
        "submitted": "2025-08-15 11:08:32",
        "source": "arxiv",
        "comment": "Accepted by CIKM 2025 as a full paper"
    },
    {
        "title": "Rationalizing Transformer Predictions via End-To-End Differentiable Self-Training",
        "abstract": "We propose an end-to-end differentiable training paradigm for stable training\nof a rationalized transformer classifier. Our approach results in a single\nmodel that simultaneously classifies a sample and scores input tokens based on\ntheir relevance to the classification. To this end, we build on the widely-used\nthree-player-game for training rationalized models, which typically relies on\ntraining a rationale selector, a classifier and a complement classifier. We\nsimplify this approach by making a single model fulfill all three roles,\nleading to a more efficient training paradigm that is not susceptible to the\ncommon training instabilities that plague existing approaches. Further, we\nextend this paradigm to produce class-wise rationales while incorporating\nrecent advances in parameterizing and regularizing the resulting rationales,\nthus leading to substantially improved and state-of-the-art alignment with\nhuman annotations without any explicit supervision.",
        "url": "http://arxiv.org/abs/2508.11393v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11393v1",
        "arxiv_id": "2508.11393v1",
        "authors": [
            "Marc Brinner",
            "Sina Zarrieß"
        ],
        "submitted": "2025-08-15 10:51:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Model Interpretability and Rationale Extraction by Input Mask Optimization",
        "abstract": "Concurrent to the rapid progress in the development of neural-network based\nmodels in areas like natural language processing and computer vision, the need\nfor creating explanations for the predictions of these black-box models has\nrisen steadily. We propose a new method to generate extractive explanations for\npredictions made by neural networks, that is based on masking parts of the\ninput which the model does not consider to be indicative of the respective\nclass. The masking is done using gradient-based optimization combined with a\nnew regularization scheme that enforces sufficiency, comprehensiveness and\ncompactness of the generated explanation, three properties that are known to be\ndesirable from the related field of rationale extraction in natural language\nprocessing. In this way, we bridge the gap between model interpretability and\nrationale extraction, thereby proving that the latter of which can be performed\nwithout training a specialized model, only on the basis of a trained\nclassifier. We further apply the same method to image inputs and obtain high\nquality explanations for image classifications, which indicates that the\nconditions proposed for rationale extraction in natural language processing are\nmore broadly applicable to different input types.",
        "url": "http://arxiv.org/abs/2508.11388v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11388v1",
        "arxiv_id": "2508.11388v1",
        "authors": [
            "Marc Brinner",
            "Sina Zarriess"
        ],
        "submitted": "2025-08-15 10:41:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Retrieval-augmented reasoning with lean language models",
        "abstract": "This technical report details a novel approach to combining reasoning and\nretrieval augmented generation (RAG) within a single, lean language model\narchitecture. While existing RAG systems typically rely on large-scale models\nand external APIs, our work addresses the increasing demand for performant and\nprivacy-preserving solutions deployable in resource-constrained or secure\nenvironments. Building on recent developments in test-time scaling and\nsmall-scale reasoning models, we develop a retrieval augmented conversational\nagent capable of interpreting complex, domain-specific queries using a\nlightweight backbone model. Our system integrates a dense retriever with\nfine-tuned Qwen2.5-Instruct models, using synthetic query generation and\nreasoning traces derived from frontier models (e.g., DeepSeek-R1) over a\ncurated corpus, in this case, the NHS A-to-Z condition pages. We explore the\nimpact of summarisation-based document compression, synthetic data design, and\nreasoning-aware fine-tuning on model performance. Evaluation against both\nnon-reasoning and general-purpose lean models demonstrates that our\ndomain-specific fine-tuning approach yields substantial gains in answer\naccuracy and consistency, approaching frontier-level performance while\nremaining feasible for local deployment. All implementation details and code\nare publicly released to support reproducibility and adaptation across domains.",
        "url": "http://arxiv.org/abs/2508.11386v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11386v1",
        "arxiv_id": "2508.11386v1",
        "authors": [
            "Ryan Sze-Yin Chan",
            "Federico Nanni",
            "Tomas Lazauskas",
            "Rosie Wood",
            "Penelope Yong",
            "Lionel Tarassenko",
            "Mark Girolami",
            "James Geddes",
            "Andrew Duncan"
        ],
        "submitted": "2025-08-15 10:38:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness Methods for LLMs",
        "abstract": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic\nvariations in prompt phrasing and formatting. In this work, we present the\nfirst systematic evaluation of 5 methods for improving prompt robustness within\na unified experimental framework. We benchmark these techniques on 8 models\nfrom Llama, Qwen and Gemma families across 52 tasks from Natural Instructions\ndataset. Our evaluation covers robustness methods from both fine-tuned and\nin-context learning paradigms, and tests their generalization against multiple\ntypes of distribution shifts. Finally, we extend our analysis to GPT-4.1 and\nDeepSeek V3 to assess frontier models' current robustness to format\nperturbations. Our findings offer actionable insights into the relative\neffectiveness of these robustness methods, enabling practitioners to make\ninformed decisions when aiming for stable and reliable LLM performance in\nreal-world applications. Code:\nhttps://github.com/AIRI-Institute/when-punctuation-matters.",
        "url": "http://arxiv.org/abs/2508.11383v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11383v1",
        "arxiv_id": "2508.11383v1",
        "authors": [
            "Mikhail Seleznyov",
            "Mikhail Chaichuk",
            "Gleb Ershov",
            "Alexander Panchenko",
            "Elena Tutubalina",
            "Oleg Somov"
        ],
        "submitted": "2025-08-15 10:32:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Feedback Indicators: The Alignment between Llama and a Teacher in Language Learning",
        "abstract": "Automated feedback generation has the potential to enhance students' learning\nprogress by providing timely and targeted feedback. Moreover, it can assist\nteachers in optimizing their time, allowing them to focus on more strategic and\npersonalized aspects of teaching. To generate high-quality, information-rich\nformative feedback, it is essential first to extract relevant indicators, as\nthese serve as the foundation upon which the feedback is constructed. Teachers\noften employ feedback criteria grids composed of various indicators that they\nevaluate systematically. This study examines the initial phase of extracting\nsuch indicators from students' submissions of a language learning course using\nthe large language model Llama 3.1. Accordingly, the alignment between\nindicators generated by the LLM and human ratings across various feedback\ncriteria is investigated. The findings demonstrate statistically significant\nstrong correlations, even in cases involving unanticipated combinations of\nindicators and criteria. The methodology employed in this paper offers a\npromising foundation for extracting indicators from students' submissions using\nLLMs. Such indicators can potentially be utilized to auto-generate explainable\nand transparent formative feedback in future research.",
        "url": "http://arxiv.org/abs/2508.11364v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11364v1",
        "arxiv_id": "2508.11364v1",
        "authors": [
            "Sylvio Rüdian",
            "Yassin Elsir",
            "Marvin Kretschmer",
            "Sabine Cayrou",
            "Niels Pinkwart"
        ],
        "submitted": "2025-08-15 09:59:22",
        "source": "arxiv",
        "comment": "11 pages, one table"
    },
    {
        "title": "SpecDetect: Simple, Fast, and Training-Free Detection of LLM-Generated Text via Spectral Analysis",
        "abstract": "The proliferation of high-quality text from Large Language Models (LLMs)\ndemands reliable and efficient detection methods. While existing training-free\napproaches show promise, they often rely on surface-level statistics and\noverlook fundamental signal properties of the text generation process. In this\nwork, we reframe detection as a signal processing problem, introducing a novel\nparadigm that analyzes the sequence of token log-probabilities in the frequency\ndomain. By systematically analyzing the signal's spectral properties using the\nglobal Discrete Fourier Transform (DFT) and the local Short-Time Fourier\nTransform (STFT), we find that human-written text consistently exhibits\nsignificantly higher spectral energy. This higher energy reflects the\nlarger-amplitude fluctuations inherent in human writing compared to the\nsuppressed dynamics of LLM-generated text. Based on this key insight, we\nconstruct SpecDetect, a detector built on a single, robust feature from the\nglobal DFT: DFT total energy. We also propose an enhanced version,\nSpecDetect++, which incorporates a sampling discrepancy mechanism to further\nboost robustness. Extensive experiments demonstrate that our approach\noutperforms the state-of-the-art model while running in nearly half the time.\nOur work introduces a new, efficient, and interpretable pathway for\nLLM-generated text detection, showing that classical signal processing\ntechniques offer a surprisingly powerful solution to this modern challenge.",
        "url": "http://arxiv.org/abs/2508.11343v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11343v1",
        "arxiv_id": "2508.11343v1",
        "authors": [
            "Haitong Luo",
            "Weiyao Zhang",
            "Suhang Wang",
            "Wenji Zou",
            "Chungang Lin",
            "Xuying Meng",
            "Yujun Zhang"
        ],
        "submitted": "2025-08-15 09:13:42",
        "source": "arxiv",
        "comment": "Under Review"
    },
    {
        "title": "Generalize across Homophily and Heterophily: Hybrid Spectral Graph Pre-Training and Prompt Tuning",
        "abstract": "Graph ``pre-training and prompt-tuning'' aligns downstream tasks with\npre-trained objectives to enable efficient knowledge transfer under limited\nsupervision. However, existing methods rely on homophily-based low-frequency\nknowledge, failing to handle diverse spectral distributions in real-world\ngraphs with varying homophily. Our theoretical analysis reveals a spectral\nspecificity principle: optimal knowledge transfer requires alignment between\npre-trained spectral filters and the intrinsic spectrum of downstream graphs.\nUnder limited supervision, large spectral gaps between pre-training and\ndownstream tasks impede effective adaptation. To bridge this gap, we propose\nthe HS-GPPT model, a novel framework that ensures spectral alignment throughout\nboth pre-training and prompt-tuning. We utilize a hybrid spectral filter\nbackbone and local-global contrastive learning to acquire abundant spectral\nknowledge. Then we design prompt graphs to align the spectral distribution with\npretexts, facilitating spectral knowledge transfer across homophily and\nheterophily. Extensive experiments validate the effectiveness under both\ntransductive and inductive learning settings. Our code is available at\nhttps://anonymous.4open.science/r/HS-GPPT-62D2/.",
        "url": "http://arxiv.org/abs/2508.11328v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11328v1",
        "arxiv_id": "2508.11328v1",
        "authors": [
            "Haitong Luo",
            "Suhang Wang",
            "Weiyao Zhang",
            "Ruiqi Meng",
            "Xuying Meng",
            "Yujun Zhang"
        ],
        "submitted": "2025-08-15 08:55:57",
        "source": "arxiv",
        "comment": "Under Review"
    },
    {
        "title": "LLM Compression: How Far Can We Go in Balancing Size and Performance?",
        "abstract": "Quantization is an essential and popular technique for improving the\naccessibility of large language models (LLMs) by reducing memory usage and\ncomputational costs while maintaining performance. In this study, we apply\n4-bit Group Scaling Quantization (GSQ) and Generative Pretrained Transformer\nQuantization (GPTQ) to LLaMA 1B, Qwen 0.5B, and PHI 1.5B, evaluating their\nimpact across multiple NLP tasks. We benchmark these models on MS MARCO\n(Information Retrieval), BoolQ (Boolean Question Answering), and GSM8K\n(Mathematical Reasoning) datasets, assessing both accuracy and efficiency\nacross various tasks. The study measures the trade-offs between model\ncompression and task performance, analyzing key evaluation metrics, namely\naccuracy, inference latency, and throughput (total output tokens generated per\nsecond), providing insights into the suitability of low-bit quantization for\nreal-world deployment. Using the results, users can then make suitable\ndecisions based on the specifications that need to be met. We discuss the pros\nand cons of GSQ and GPTQ techniques on models of different sizes, which also\nserve as a benchmark for future experiments.",
        "url": "http://arxiv.org/abs/2508.11318v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11318v1",
        "arxiv_id": "2508.11318v1",
        "authors": [
            "Sahil Sk",
            "Debasish Dhal",
            "Sonal Khosla",
            "Sk Shahid",
            "Sambit Shekhar",
            "Akash Dhaka",
            "Shantipriya Parida",
            "Dilip K. Prasad",
            "Ondřej Bojar"
        ],
        "submitted": "2025-08-15 08:41:20",
        "source": "arxiv",
        "comment": "This paper has been accepted for presentation at the RANLP 2025\n  conference"
    },
    {
        "title": "SGSimEval: A Comprehensive Multifaceted and Similarity-Enhanced Benchmark for Automatic Survey Generation Systems",
        "abstract": "The growing interest in automatic survey generation (ASG), a task that\ntraditionally required considerable time and effort, has been spurred by recent\nadvances in large language models (LLMs). With advancements in\nretrieval-augmented generation (RAG) and the rising popularity of multi-agent\nsystems (MASs), synthesizing academic surveys using LLMs has become a viable\napproach, thereby elevating the need for robust evaluation methods in this\ndomain. However, existing evaluation methods suffer from several limitations,\nincluding biased metrics, a lack of human preference, and an over-reliance on\nLLMs-as-judges. To address these challenges, we propose SGSimEval, a\ncomprehensive benchmark for Survey Generation with Similarity-Enhanced\nEvaluation that evaluates automatic survey generation systems by integrating\nassessments of the outline, content, and references, and also combines\nLLM-based scoring with quantitative metrics to provide a multifaceted\nevaluation framework. In SGSimEval, we also introduce human preference metrics\nthat emphasize both inherent quality and similarity to humans. Extensive\nexperiments reveal that current ASG systems demonstrate human-comparable\nsuperiority in outline generation, while showing significant room for\nimprovement in content and reference generation, and our evaluation metrics\nmaintain strong consistency with human assessments.",
        "url": "http://arxiv.org/abs/2508.11310v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11310v1",
        "arxiv_id": "2508.11310v1",
        "authors": [
            "Beichen Guo",
            "Zhiyuan Wen",
            "Yu Yang",
            "Peng Gao",
            "Ruosong Yang",
            "Jiaxing Shen"
        ],
        "submitted": "2025-08-15 08:27:58",
        "source": "arxiv",
        "comment": "Accepted to The 21st International Conference on Advanced Data Mining\n  and Applications (ADMA2025)"
    },
    {
        "title": "SafeConstellations: Steering LLM Safety to Reduce Over-Refusals Through Task-Specific Trajectory",
        "abstract": "LLMs increasingly exhibit over-refusal behavior, where safety mechanisms\ncause models to reject benign instructions that superficially resemble harmful\ncontent. This phenomena diminishes utility in production applications that\nrepeatedly rely on common prompt templates or applications that frequently rely\non LLMs for specific tasks (e.g. sentiment analysis, language translation).\nThrough comprehensive evaluation, we demonstrate that LLMs still tend to refuse\nresponses to harmful instructions when those instructions are reframed to\nappear as benign tasks. Our mechanistic analysis reveal that LLMs follow\ndistinct \"constellation\" patterns in embedding space as representations\ntraverse layers, with each task maintaining consistent trajectories that shift\npredictably between refusal and non-refusal cases. We introduce\nSafeConstellations, an inference-time trajectory-shifting approach that tracks\ntask-specific trajectory patterns and guides representations toward non-refusal\npathways. By selectively guiding model behavior only on tasks prone to\nover-refusal, and by preserving general model behavior, our method reduces\nover-refusal rates by up to 73% with minimal impact on utility-offering a\nprincipled approach to mitigating over-refusals.",
        "url": "http://arxiv.org/abs/2508.11290v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11290v1",
        "arxiv_id": "2508.11290v1",
        "authors": [
            "Utsav Maskey",
            "Sumit Yadav",
            "Mark Dras",
            "Usman Naseem"
        ],
        "submitted": "2025-08-15 07:54:42",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "AI in Mental Health: Emotional and Sentiment Analysis of Large Language Models' Responses to Depression, Anxiety, and Stress Queries",
        "abstract": "Depression, anxiety, and stress are widespread mental health concerns that\nincreasingly drive individuals to seek information from Large Language Models\n(LLMs). This study investigates how eight LLMs (Claude Sonnet, Copilot, Gemini\nPro, GPT-4o, GPT-4o mini, Llama, Mixtral, and Perplexity) reply to twenty\npragmatic questions about depression, anxiety, and stress when those questions\nare framed for six user profiles (baseline, woman, man, young, old, and\nuniversity student). The models generated 2,880 answers, which we scored for\nsentiment and emotions using state-of-the-art tools. Our analysis revealed that\noptimism, fear, and sadness dominated the emotional landscape across all\noutputs, with neutral sentiment maintaining consistently high values.\nGratitude, joy, and trust appeared at moderate levels, while emotions such as\nanger, disgust, and love were rarely expressed. The choice of LLM significantly\ninfluenced emotional expression patterns. Mixtral exhibited the highest levels\nof negative emotions including disapproval, annoyance, and sadness, while Llama\ndemonstrated the most optimistic and joyful responses. The type of mental\nhealth condition dramatically shaped emotional responses: anxiety prompts\nelicited extraordinarily high fear scores (0.974), depression prompts generated\nelevated sadness (0.686) and the highest negative sentiment, while\nstress-related queries produced the most optimistic responses (0.755) with\nelevated joy and trust. In contrast, demographic framing of queries produced\nonly marginal variations in emotional tone. Statistical analyses confirmed\nsignificant model-specific and condition-specific differences, while\ndemographic influences remained minimal. These findings highlight the critical\nimportance of model selection in mental health applications, as each LLM\nexhibits a distinct emotional signature that could significantly impact user\nexperience and outcomes.",
        "url": "http://arxiv.org/abs/2508.11285v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11285v1",
        "arxiv_id": "2508.11285v1",
        "authors": [
            "Arya VarastehNezhad",
            "Reza Tavasoli",
            "Soroush Elyasi",
            "MohammadHossein LotfiNia",
            "Hamed Farbeh"
        ],
        "submitted": "2025-08-15 07:47:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ToxiFrench: Benchmarking and Enhancing Language Models via CoT Fine-Tuning for French Toxicity Detection",
        "abstract": "Detecting toxic content using language models is crucial yet challenging.\nWhile substantial progress has been made in English, toxicity detection in\nFrench remains underdeveloped, primarily due to the lack of culturally\nrelevant, large-scale datasets. In this work, we introduce TOXIFRENCH, a new\npublic benchmark of 53,622 French online comments, constructed via a\nsemi-automated annotation pipeline that reduces manual labeling to only 10%\nthrough high-confidence LLM-based pre-annotation and human verification. Then,\nwe benchmark a broad range of models and uncover a counterintuitive insight:\nSmall Language Models (SLMs) outperform many larger models in robustness and\ngeneralization under the toxicity detection task. Motivated by this finding, we\npropose a novel Chain-of-Thought (CoT) fine-tuning strategy using a dynamic\nweighted loss that progressively emphasizes the model's final decision,\nsignificantly improving faithfulness. Our fine-tuned 4B model achieves\nstate-of-the-art performance, improving its F1 score by 13% over its baseline\nand outperforming LLMs such as GPT-40 and Gemini-2.5. Further evaluation on a\ncross-lingual toxicity benchmark demonstrates strong multilingual ability,\nsuggesting that our methodology can be effectively extended to other languages\nand safety-critical classification tasks.",
        "url": "http://arxiv.org/abs/2508.11281v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11281v1",
        "arxiv_id": "2508.11281v1",
        "authors": [
            "Axel Delaval",
            "Shujian Yang",
            "Haicheng Wang",
            "Han Qiu",
            "Jialiang Lu"
        ],
        "submitted": "2025-08-15 07:40:41",
        "source": "arxiv",
        "comment": "14 pages, 5 figures, 8 tables. This paper introduces TOXIFRENCH, a\n  new large-scale benchmark for French toxicity detection, and proposes a\n  Chain-of-Thought (CoT) fine-tuning method with a dynamic weighted loss. The\n  resulting fine-tuned 4B parameter model, ToxiFrench, achieves\n  state-of-the-art performance, outperforming larger models like GPT-4o"
    },
    {
        "title": "LETToT: Label-Free Evaluation of Large Language Models On Tourism Using Expert Tree-of-Thought",
        "abstract": "Evaluating large language models (LLMs) in specific domain like tourism\nremains challenging due to the prohibitive cost of annotated benchmarks and\npersistent issues like hallucinations. We propose $\\textbf{L}$able-Free\n$\\textbf{E}$valuation of LLM on $\\textbf{T}$ourism using Expert\n$\\textbf{T}$ree-$\\textbf{o}$f-$\\textbf{T}$hought (LETToT), a framework that\nleverages expert-derived reasoning structures-instead of labeled data-to access\nLLMs in tourism. First, we iteratively refine and validate hierarchical ToT\ncomponents through alignment with generic quality dimensions and expert\nfeedback. Results demonstrate the effectiveness of our systematically optimized\nexpert ToT with 4.99-14.15\\% relative quality gains over baselines. Second, we\napply LETToT's optimized expert ToT to evaluate models of varying scales\n(32B-671B parameters), revealing: (1) Scaling laws persist in specialized\ndomains (DeepSeek-V3 leads), yet reasoning-enhanced smaller models (e.g.,\nDeepSeek-R1-Distill-Llama-70B) close this gap; (2) For sub-72B models, explicit\nreasoning architectures outperform counterparts in accuracy and conciseness\n($p<0.05$). Our work established a scalable, label-free paradigm for\ndomain-specific LLM evaluation, offering a robust alternative to conventional\nannotated benchmarks.",
        "url": "http://arxiv.org/abs/2508.11280v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11280v1",
        "arxiv_id": "2508.11280v1",
        "authors": [
            "Ruiyan Qi",
            "Congding Wen",
            "Weibo Zhou",
            "Shangsong Liang",
            "Lingbo Li"
        ],
        "submitted": "2025-08-15 07:37:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "UNVEILING: What Makes Linguistics Olympiad Puzzles Tricky for LLMs?",
        "abstract": "Large language models (LLMs) have demonstrated potential in reasoning tasks,\nbut their performance on linguistics puzzles remains consistently poor. These\npuzzles, often derived from Linguistics Olympiad (LO) contests, provide a\nminimal contamination environment to assess LLMs' linguistic reasoning\nabilities across low-resource languages. This work analyses LLMs' performance\non 629 problems across 41 low-resource languages by labelling each with\nlinguistically informed features to unveil weaknesses. Our analyses show that\nLLMs struggle with puzzles involving higher morphological complexity and\nperform better on puzzles involving linguistic features that are also found in\nEnglish. We also show that splitting words into morphemes as a pre-processing\nstep improves solvability, indicating a need for more informed and\nlanguage-specific tokenisers. These findings thus offer insights into some\nchallenges in linguistic reasoning and modelling of low-resource languages.",
        "url": "http://arxiv.org/abs/2508.11260v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11260v1",
        "arxiv_id": "2508.11260v1",
        "authors": [
            "Mukund Choudhary",
            "KV Aditya Srivatsa",
            "Gaurja Aeron",
            "Antara Raaghavi Bhattacharya",
            "Dang Khoa Dang Dinh",
            "Ikhlasul Akmal Hanif",
            "Daria Kotova",
            "Ekaterina Kochmar",
            "Monojit Choudhury"
        ],
        "submitted": "2025-08-15 06:53:28",
        "source": "arxiv",
        "comment": "Accepted to COLM 2025"
    },
    {
        "title": "Group Fairness Meets the Black Box: Enabling Fair Algorithms on Closed LLMs via Post-Processing",
        "abstract": "Instruction fine-tuned large language models (LLMs) enable a simple zero-shot\nor few-shot prompting paradigm, also known as in-context learning, for building\nprediction models. This convenience, combined with continued advances in LLM\ncapability, has the potential to drive their adoption across a broad range of\ndomains, including high-stakes applications where group fairness -- preventing\ndisparate impacts across demographic groups -- is essential. The majority of\nexisting approaches to enforcing group fairness on LLM-based classifiers rely\non traditional fair algorithms applied via model fine-tuning or head-tuning on\nfinal-layer embeddings, but they are no longer applicable to closed-weight LLMs\nunder the in-context learning setting, which include some of the most capable\ncommercial models today, such as GPT-4, Gemini, and Claude. In this paper, we\npropose a framework for deriving fair classifiers from closed-weight LLMs via\nprompting: the LLM is treated as a feature extractor, and features are elicited\nfrom its probabilistic predictions (e.g., token log probabilities) using\nprompts strategically designed for the specified fairness criterion to obtain\nsufficient statistics for fair classification; a fair algorithm is then applied\nto these features to train a lightweight fair classifier in a post-hoc manner.\nExperiments on five datasets, including three tabular ones, demonstrate strong\naccuracy-fairness tradeoffs for the classifiers derived by our framework from\nboth open-weight and closed-weight LLMs; in particular, our framework is\ndata-efficient and outperforms fair classifiers trained on LLM embeddings\n(i.e., head-tuning) or from scratch on raw tabular features.",
        "url": "http://arxiv.org/abs/2508.11258v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11258v1",
        "arxiv_id": "2508.11258v1",
        "authors": [
            "Ruicheng Xian",
            "Yuxuan Wan",
            "Han Zhao"
        ],
        "submitted": "2025-08-15 06:50:29",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning Models to Ask for Information",
        "abstract": "Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving\nabilities in mathematics, as evaluated by existing benchmarks exclusively on\nwell-defined problems. However, such evaluation setup constitutes a critical\ngap, since a genuine intelligent agent should not only solve problems (as a\nmath quiz solver), but also be able~to ask for information when the problems\nlack sufficient information, enabling proactivity in responding users'\nrequests. To bridge such gap, we proposes a new dataset consisting of two types\nof incomplete problems with diverse contexts. Based on the dataset, our\nsystematical evaluation of LRMs reveals their inability in proactively asking\nfor information. In addition, we uncover the behaviors related to overthinking\nand hallucination of LRMs, and highlight the potential and challenges of\nsupervised fine-tuning in learning such ability. We hope to provide new\ninsights in developing LRMs with genuine intelligence, rather than just solving\nproblems.",
        "url": "http://arxiv.org/abs/2508.11252v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11252v1",
        "arxiv_id": "2508.11252v1",
        "authors": [
            "Youcheng Huang",
            "Bowen Qin",
            "Chen Huang",
            "Duanyu Feng",
            "Xi Yang",
            "Wenqiang Lei"
        ],
        "submitted": "2025-08-15 06:42:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Cross-Granularity Hypergraph Retrieval-Augmented Generation for Multi-hop Question Answering",
        "abstract": "Multi-hop question answering (MHQA) requires integrating knowledge scattered\nacross multiple passages to derive the correct answer. Traditional\nretrieval-augmented generation (RAG) methods primarily focus on coarse-grained\ntextual semantic similarity and ignore structural associations among dispersed\nknowledge, which limits their effectiveness in MHQA tasks. GraphRAG methods\naddress this by leveraging knowledge graphs (KGs) to capture structural\nassociations, but they tend to overly rely on structural information and\nfine-grained word- or phrase-level retrieval, resulting in an underutilization\nof textual semantics. In this paper, we propose a novel RAG approach called\nHGRAG for MHQA that achieves cross-granularity integration of structural and\nsemantic information via hypergraphs. Structurally, we construct an entity\nhypergraph where fine-grained entities serve as nodes and coarse-grained\npassages as hyperedges, and establish knowledge association through shared\nentities. Semantically, we design a hypergraph retrieval method that integrates\nfine-grained entity similarity and coarse-grained passage similarity via\nhypergraph diffusion. Finally, we employ a retrieval enhancement module, which\nfurther refines the retrieved results both semantically and structurally, to\nobtain the most relevant passages as context for answer generation with the\nLLM. Experimental results on benchmark datasets demonstrate that our approach\noutperforms state-of-the-art methods in QA performance, and achieves a\n6$\\times$ speedup in retrieval efficiency.",
        "url": "http://arxiv.org/abs/2508.11247v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11247v1",
        "arxiv_id": "2508.11247v1",
        "authors": [
            "Changjian Wang",
            "Weihong Deng",
            "Weili Guan",
            "Quan Lu",
            "Ning Jiang"
        ],
        "submitted": "2025-08-15 06:36:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "RAG for Geoscience: What We Expect, Gaps and Opportunities",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances language models by combining\nretrieval with generation. However, its current workflow remains largely\ntext-centric, limiting its applicability in geoscience. Many geoscientific\ntasks are inherently evidence-hungry. Typical examples involve imputing missing\nobservations using analog scenes, retrieving equations and parameters to\ncalibrate models, geolocating field photos based on visual cues, or surfacing\nhistorical case studies to support policy analyses. A simple\n``retrieve-then-generate'' pipeline is insufficient for these needs. We\nenvision Geo-RAG, a next-generation paradigm that reimagines RAG as a modular\nretrieve $\\rightarrow$ reason $\\rightarrow$ generate $\\rightarrow$ verify loop.\nGeo-RAG supports four core capabilities: (i) retrieval of multi-modal Earth\ndata; (ii) reasoning under physical and domain constraints; (iii) generation of\nscience-grade artifacts; and (iv) verification of generated hypotheses against\nnumerical models, ground measurements, and expert assessments. This shift opens\nnew opportunities for more trustworthy and transparent geoscience workflows.",
        "url": "http://arxiv.org/abs/2508.11246v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11246v1",
        "arxiv_id": "2508.11246v1",
        "authors": [
            "Runlong Yu",
            "Shiyuan Luo",
            "Rahul Ghosh",
            "Lingyao Li",
            "Yiqun Xie",
            "Xiaowei Jia"
        ],
        "submitted": "2025-08-15 06:33:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Mitigating Filter Bubble from the Perspective of Community Detection: A Universal Framework",
        "abstract": "In recent years, recommender systems have primarily focused on improving\naccuracy at the expense of diversity, which exacerbates the well-known filter\nbubble effect. This paper proposes a universal framework called CD-CGCN to\naddress the filter bubble issue in recommender systems from a community\ndetection perspective. By analyzing user-item interaction histories with a\ncommunity detection algorithm, we reveal that state-of-the-art recommendations\noften focus on intra-community items, worsening the filter bubble effect.\nCD-CGCN, a model-agnostic framework, integrates a Conditional Discriminator and\na Community-reweighted Graph Convolutional Network which can be plugged into\nmost recommender models. Using adversarial learning based on community labels,\nit counteracts the extracted community attributes and incorporates an inference\nstrategy tailored to the user's specific filter bubble state. Extensive\nexperiments on real-world datasets with multiple base models validate its\neffectiveness in mitigating filter bubbles while preserving recommendation\nquality. Additionally, by applying community debiasing to the original test set\nto construct an unbiased test set, we observe that CD-CGCN demonstrates\nsuperior performance in capturing users' inter-community preferences.",
        "url": "http://arxiv.org/abs/2508.11239v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11239v1",
        "arxiv_id": "2508.11239v1",
        "authors": [
            "Ming Tang",
            "Xiaowen Huang",
            "Jitao Sang"
        ],
        "submitted": "2025-08-15 05:57:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Benchmarking Prosody Encoding in Discrete Speech Tokens",
        "abstract": "Recently, discrete tokens derived from self-supervised learning (SSL) models\nvia k-means clustering have been actively studied as pseudo-text in speech\nlanguage models and as efficient intermediate representations for various\ntasks. However, these discrete tokens are typically learned in advance,\nseparately from the training of language models or downstream tasks. As a\nresult, choices related to discretization, such as the SSL model used or the\nnumber of clusters, must be made heuristically. In particular, speech language\nmodels are expected to understand and generate responses that reflect not only\nthe semantic content but also prosodic features. Yet, there has been limited\nresearch on the ability of discrete tokens to capture prosodic information. To\naddress this gap, this study conducts a comprehensive analysis focusing on\nprosodic encoding based on their sensitivity to the artificially modified\nprosody, aiming to provide practical guidelines for designing discrete tokens.",
        "url": "http://arxiv.org/abs/2508.11224v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11224v1",
        "arxiv_id": "2508.11224v1",
        "authors": [
            "Kentaro Onda",
            "Satoru Fukayama",
            "Daisuke Saito",
            "Nobuaki Minematsu"
        ],
        "submitted": "2025-08-15 05:11:16",
        "source": "arxiv",
        "comment": "Accepted by ASRU2025"
    },
    {
        "title": "ORFuzz: Fuzzing the \"Other Side\" of LLM Safety -- Testing Over-Refusal",
        "abstract": "Large Language Models (LLMs) increasingly exhibit over-refusal - erroneously\nrejecting benign queries due to overly conservative safety measures - a\ncritical functional flaw that undermines their reliability and usability.\nCurrent methods for testing this behavior are demonstrably inadequate,\nsuffering from flawed benchmarks and limited test generation capabilities, as\nhighlighted by our empirical user study. To the best of our knowledge, this\npaper introduces the first evolutionary testing framework, ORFuzz, for the\nsystematic detection and analysis of LLM over-refusals. ORFuzz uniquely\nintegrates three core components: (1) safety category-aware seed selection for\ncomprehensive test coverage, (2) adaptive mutator optimization using reasoning\nLLMs to generate effective test cases, and (3) OR-Judge, a human-aligned judge\nmodel validated to accurately reflect user perception of toxicity and refusal.\nOur extensive evaluations demonstrate that ORFuzz generates diverse, validated\nover-refusal instances at a rate (6.98% average) more than double that of\nleading baselines, effectively uncovering vulnerabilities. Furthermore,\nORFuzz's outputs form the basis of ORFuzzSet, a new benchmark of 1,855 highly\ntransferable test cases that achieves a superior 63.56% average over-refusal\nrate across 10 diverse LLMs, significantly outperforming existing datasets.\nORFuzz and ORFuzzSet provide a robust automated testing framework and a\nvaluable community resource, paving the way for developing more reliable and\ntrustworthy LLM-based software systems.",
        "url": "http://arxiv.org/abs/2508.11222v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11222v1",
        "arxiv_id": "2508.11222v1",
        "authors": [
            "Haonan Zhang",
            "Dongxia Wang",
            "Yi Liu",
            "Kexin Chen",
            "Jiashui Wang",
            "Xinlei Ying",
            "Long Liu",
            "Wenhai Wang"
        ],
        "submitted": "2025-08-15 05:03:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "How Causal Abstraction Underpins Computational Explanation",
        "abstract": "Explanations of cognitive behavior often appeal to computations over\nrepresentations. What does it take for a system to implement a given\ncomputation over suitable representational vehicles within that system? We\nargue that the language of causality -- and specifically the theory of causal\nabstraction -- provides a fruitful lens on this topic. Drawing on current\ndiscussions in deep learning with artificial neural networks, we illustrate how\nclassical themes in the philosophy of computation and cognition resurface in\ncontemporary machine learning. We offer an account of computational\nimplementation grounded in causal abstraction, and examine the role for\nrepresentation in the resulting picture. We argue that these issues are most\nprofitably explored in connection with generalization and prediction.",
        "url": "http://arxiv.org/abs/2508.11214v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11214v1",
        "arxiv_id": "2508.11214v1",
        "authors": [
            "Atticus Geiger",
            "Jacqueline Harding",
            "Thomas Icard"
        ],
        "submitted": "2025-08-15 04:46:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "E-CaTCH: Event-Centric Cross-Modal Attention with Temporal Consistency and Class-Imbalance Handling for Misinformation Detection",
        "abstract": "Detecting multimodal misinformation on social media remains challenging due\nto inconsistencies between modalities, changes in temporal patterns, and\nsubstantial class imbalance. Many existing methods treat posts independently\nand fail to capture the event-level structure that connects them across time\nand modality. We propose E-CaTCH, an interpretable and scalable framework for\nrobustly detecting misinformation. If needed, E-CaTCH clusters posts into\npseudo-events based on textual similarity and temporal proximity, then\nprocesses each event independently. Within each event, textual and visual\nfeatures are extracted using pre-trained BERT and ResNet encoders, refined via\nintra-modal self-attention, and aligned through bidirectional cross-modal\nattention. A soft gating mechanism fuses these representations to form\ncontextualized, content-aware embeddings of each post. To model temporal\nevolution, E-CaTCH segments events into overlapping time windows and uses a\ntrend-aware LSTM, enhanced with semantic shift and momentum signals, to encode\nnarrative progression over time. Classification is performed at the event\nlevel, enabling better alignment with real-world misinformation dynamics. To\naddress class imbalance and promote stable learning, the model integrates\nadaptive class weighting, temporal consistency regularization, and hard-example\nmining. The total loss is aggregated across all events. Extensive experiments\non Fakeddit, IND, and COVID-19 MISINFOGRAPH demonstrate that E-CaTCH\nconsistently outperforms state-of-the-art baselines. Cross-dataset evaluations\nfurther demonstrate its robustness, generalizability, and practical\napplicability across diverse misinformation scenarios.",
        "url": "http://arxiv.org/abs/2508.11197v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11197v1",
        "arxiv_id": "2508.11197v1",
        "authors": [
            "Ahmad Mousavi",
            "Yeganeh Abdollahinejad",
            "Roberto Corizzo",
            "Nathalie Japkowicz",
            "Zois Boukouvalas"
        ],
        "submitted": "2025-08-15 04:13:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Representation Quantization for Collaborative Filtering Augmentation",
        "abstract": "As the core algorithm in recommendation systems, collaborative filtering (CF)\nalgorithms inevitably face the problem of data sparsity. Since CF captures\nsimilar users and items for recommendations, it is effective to augment the\nlacking user-user and item-item homogeneous linkages. However, existing methods\nare typically limited to connecting through overlapping interacted neighbors or\nthrough similar attributes and contents. These approaches are constrained by\ncoarse-grained, sparse attributes and fail to effectively extract behavioral\ncharacteristics jointly from interaction sequences and attributes. To address\nthese challenges, we propose a novel two-stage collaborative recommendation\nalgorithm, DQRec: Decomposition-based Quantized Variational AutoEncoder\n(DQ-VAE) for Recommendation. DQRec augments features and homogeneous linkages\nby extracting the behavior characteristics jointly from interaction sequences\nand attributes, namely patterns, such as user multi-aspect interests. Inspired\nby vector quantization (VQ) technology, we propose a new VQ algorithm, DQ-VAE,\nwhich decomposes the pre-trained representation embeddings into distinct\ndimensions, and quantize them to generates semantic IDs. We utilize the\ngenerated semantic IDs as the extracted patterns mentioned above. By\nintegrating these semantic ID patterns into the recommendation process through\nfeature and linkage augmentation, the system enriches both latent and explicit\nuser and item features, identifies pattern-similar neighbors, and thereby\nimproves the efficiency of information diffusion. Experimental comparisons with\nbaselines across multiple datasets demonstrate the superior performance of the\nproposed DQRec method.",
        "url": "http://arxiv.org/abs/2508.11194v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11194v1",
        "arxiv_id": "2508.11194v1",
        "authors": [
            "Yunze Luo",
            "Yinjie Jiang",
            "Gaode Chen",
            "Jingchi Wang",
            "Shicheng Wang",
            "Ruina Sun",
            "Jiang Yuezihan",
            "Jun Zhang",
            "Jian Liang",
            "Han Li",
            "Kun Gai",
            "Kaigui Bian"
        ],
        "submitted": "2025-08-15 04:00:50",
        "source": "arxiv",
        "comment": "11 pages, 4 figures"
    },
    {
        "title": "Novel Parasitic Dual-Scale Modeling for Efficient and Accurate Multilingual Speech Translation",
        "abstract": "Recent advancements in speech-to-text translation have led to the development\nof multilingual models capable of handling multiple language pairs\nsimultaneously. However, these unified models often suffer from large parameter\nsizes, making it challenging to balance inference efficiency and performance,\nparticularly in local deployment scenarios. We propose an innovative Parasitic\nDual-Scale Approach, which combines an enhanced speculative sampling method\nwith model compression and knowledge distillation techniques. Building on the\nWhisper Medium model, we enhance it for multilingual speech translation into\nwhisperM2M, and integrate our novel KVSPN module, achieving state-of-the-art\n(SOTA) performance across six popular languages with improved inference\nefficiency. KVSPN enables a 40\\% speedup with no BLEU score degradation.\nCombined with distillation methods, it represents a 2.6$\\times$ speedup over\nthe original Whisper Medium with superior performance.",
        "url": "http://arxiv.org/abs/2508.11189v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11189v1",
        "arxiv_id": "2508.11189v1",
        "authors": [
            "Chenyang Le",
            "Yinfeng Xia",
            "Huiyan Li",
            "Manhong Wang",
            "Yutao Sun",
            "Xingyang Ma",
            "Yanmin Qian"
        ],
        "submitted": "2025-08-15 03:46:46",
        "source": "arxiv",
        "comment": "Interspeech 2025"
    },
    {
        "title": "Expressive Speech Retrieval using Natural Language Descriptions of Speaking Style",
        "abstract": "We introduce the task of expressive speech retrieval, where the goal is to\nretrieve speech utterances spoken in a given style based on a natural language\ndescription of that style. While prior work has primarily focused on performing\nspeech retrieval based on what was said in an utterance, we aim to do so based\non how something was said. We train speech and text encoders to embed speech\nand text descriptions of speaking styles into a joint latent space, which\nenables using free-form text prompts describing emotions or styles as queries\nto retrieve matching expressive speech segments. We perform detailed analyses\nof various aspects of our proposed framework, including encoder architectures,\ntraining criteria for effective cross-modal alignment, and prompt augmentation\nfor improved generalization to arbitrary text queries. Experiments on multiple\ndatasets encompassing 22 speaking styles demonstrate that our approach achieves\nstrong retrieval performance as measured by Recall@k.",
        "url": "http://arxiv.org/abs/2508.11187v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11187v1",
        "arxiv_id": "2508.11187v1",
        "authors": [
            "Wonjune Kang",
            "Deb Roy"
        ],
        "submitted": "2025-08-15 03:38:21",
        "source": "arxiv",
        "comment": "Accepted to ASRU 2025"
    },
    {
        "title": "Personalized Distractor Generation via MCTS-Guided Reasoning Reconstruction",
        "abstract": "Distractors, incorrect but plausible answer choices in multiple-choice\nquestions (MCQs), play a critical role in educational assessment by diagnosing\nstudent misconceptions. Recent work has leveraged large language models (LLMs)\nto generate shared, group-level distractors by learning common error patterns\nacross large student populations. However, such distractors often fail to\ncapture the diverse reasoning errors of individual students, limiting their\ndiagnostic effectiveness. To address this limitation, we introduce the task of\npersonalized distractor generation, which aims to generate tailored distractors\nbased on individual misconceptions inferred from each student's past\nquestion-answering (QA) records, ensuring every student receives options that\neffectively exposes their specific reasoning errors. While promising, this task\nis challenging because each student typically has only a few QA records, which\noften lack the student's underlying reasoning processes, making training-based\ngroup-level approaches infeasible. To overcome this, we propose a training-free\ntwo-stage framework. In the first stage, we construct a student-specific\nmisconception prototype by applying Monte Carlo Tree Search (MCTS) to recover\nthe student's reasoning trajectories from past incorrect answers. In the second\nstage, this prototype guides the simulation of the student's reasoning on new\nquestions, enabling the generation of personalized distractors that align with\nthe student's recurring misconceptions. Experiments show that our approach\nachieves the best performance in generating plausible, personalized distractors\nfor 140 students, and also effectively generalizes to group-level settings,\nhighlighting its robustness and adaptability.",
        "url": "http://arxiv.org/abs/2508.11184v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11184v1",
        "arxiv_id": "2508.11184v1",
        "authors": [
            "Tao Wu",
            "Jingyuan Chen",
            "Wang Lin",
            "Jian Zhan",
            "Mengze Li",
            "Kun Kuang",
            "Fei Wu"
        ],
        "submitted": "2025-08-15 03:20:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Overcoming Low-Resource Barriers in Tulu: Neural Models and Corpus Creation for OffensiveLanguage Identification",
        "abstract": "Tulu, a low-resource Dravidian language predominantly spoken in southern\nIndia, has limited computational resources despite its growing digital\npresence. This study presents the first benchmark dataset for Offensive\nLanguage Identification (OLI) in code-mixed Tulu social media content,\ncollected from YouTube comments across various domains. The dataset, annotated\nwith high inter-annotator agreement (Krippendorff's alpha = 0.984), includes\n3,845 comments categorized into four classes: Not Offensive, Not Tulu,\nOffensive Untargeted, and Offensive Targeted. We evaluate a suite of deep\nlearning models, including GRU, LSTM, BiGRU, BiLSTM, CNN, and attention-based\nvariants, alongside transformer architectures (mBERT, XLM-RoBERTa). The BiGRU\nmodel with self-attention achieves the best performance with 82% accuracy and a\n0.81 macro F1-score. Transformer models underperform, highlighting the\nlimitations of multilingual pretraining in code-mixed, under-resourced\ncontexts. This work lays the foundation for further NLP research in Tulu and\nsimilar low-resource, code-mixed languages.",
        "url": "http://arxiv.org/abs/2508.11166v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11166v1",
        "arxiv_id": "2508.11166v1",
        "authors": [
            "Anusha M D",
            "Deepthi Vikram",
            "Bharathi Raja Chakravarthi",
            "Parameshwar R Hegde"
        ],
        "submitted": "2025-08-15 02:34:22",
        "source": "arxiv",
        "comment": "20 pages, 3 tables, 3 figures. Submitted to Language Resources and\n  Evaluation (Springer)"
    },
    {
        "title": "MobQA: A Benchmark Dataset for Semantic Understanding of Human Mobility Data through Question Answering",
        "abstract": "This paper presents MobQA, a benchmark dataset designed to evaluate the\nsemantic understanding capabilities of large language models (LLMs) for human\nmobility data through natural language question answering.\n  While existing models excel at predicting human movement patterns, it remains\nunobvious how much they can interpret the underlying reasons or semantic\nmeaning of those patterns. MobQA provides a comprehensive evaluation framework\nfor LLMs to answer questions about diverse human GPS trajectories spanning\ndaily to weekly granularities. It comprises 5,800 high-quality question-answer\npairs across three complementary question types: factual retrieval (precise\ndata extraction), multiple-choice reasoning (semantic inference), and free-form\nexplanation (interpretive description), which all require spatial, temporal,\nand semantic reasoning. Our evaluation of major LLMs reveals strong performance\non factual retrieval but significant limitations in semantic reasoning and\nexplanation question answering, with trajectory length substantially impacting\nmodel effectiveness. These findings demonstrate the achievements and\nlimitations of state-of-the-art LLMs for semantic mobility\nunderstanding.\\footnote{MobQA dataset is available at\nhttps://github.com/CyberAgentAILab/mobqa.}",
        "url": "http://arxiv.org/abs/2508.11163v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11163v1",
        "arxiv_id": "2508.11163v1",
        "authors": [
            "Hikaru Asano",
            "Hiroki Ouchi",
            "Akira Kasuga",
            "Ryo Yonetani"
        ],
        "submitted": "2025-08-15 02:30:20",
        "source": "arxiv",
        "comment": "23 pages, 12 figures"
    },
    {
        "title": "Role-Augmented Intent-Driven Generative Search Engine Optimization",
        "abstract": "Generative Search Engines (GSEs), powered by Large Language Models (LLMs) and\nRetrieval-Augmented Generation (RAG), are reshaping information retrieval.\nWhile commercial systems (e.g., BingChat, Perplexity.ai) demonstrate impressive\nsemantic synthesis capabilities, their black-box nature fundamentally\nundermines established Search Engine Optimization (SEO) practices. Content\ncreators face a critical challenge: their optimization strategies, effective in\ntraditional search engines, are misaligned with generative retrieval contexts,\nresulting in diminished visibility. To bridge this gap, we propose a\nRole-Augmented Intent-Driven Generative Search Engine Optimization (G-SEO)\nmethod, providing a structured optimization pathway tailored for GSE scenarios.\nOur method models search intent through reflective refinement across diverse\ninformational roles, enabling targeted content enhancement. To better evaluate\nthe method under realistic settings, we address the benchmarking limitations of\nprior work by: (1) extending the GEO dataset with diversified query variations\nreflecting real-world search scenarios and (2) introducing G-Eval 2.0, a\n6-level LLM-augmented evaluation rubric for fine-grained human-aligned\nassessment. Experimental results demonstrate that search intent serves as an\neffective signal for guiding content optimization, yielding significant\nimprovements over single-aspect baseline approaches in both subjective\nimpressions and objective content visibility within GSE responses.",
        "url": "http://arxiv.org/abs/2508.11158v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11158v1",
        "arxiv_id": "2508.11158v1",
        "authors": [
            "Xiaolu Chen",
            "Haojie Wu",
            "Jie Bao",
            "Zhen Chen",
            "Yong Liao",
            "Hu Huang"
        ],
        "submitted": "2025-08-15 02:08:55",
        "source": "arxiv",
        "comment": "7 pages, 5 figures"
    },
    {
        "title": "A Cross-Modal Rumor Detection Scheme via Contrastive Learning by Exploring Text and Image internal Correlations",
        "abstract": "Existing rumor detection methods often neglect the content within images as\nwell as the inherent relationships between contexts and images across different\nvisual scales, thereby resulting in the loss of critical information pertinent\nto rumor identification. To address these issues, this paper presents a novel\ncross-modal rumor detection scheme based on contrastive learning, namely the\nMulti-scale Image and Context Correlation exploration algorithm (MICC).\nSpecifically, we design an SCLIP encoder to generate unified semantic\nembeddings for text and multi-scale image patches through contrastive\npretraining, enabling their relevance to be measured via dot-product\nsimilarity. Building upon this, a Cross-Modal Multi-Scale Alignment module is\nintroduced to identify image regions most relevant to the textual semantics,\nguided by mutual information maximization and the information bottleneck\nprinciple, through a Top-K selection strategy based on a cross-modal relevance\nmatrix constructed between the text and multi-scale image patches. Moreover, a\nscale-aware fusion network is designed to integrate the highly correlated\nmulti-scale image features with global text features by assigning adaptive\nweights to image regions based on their semantic importance and cross-modal\nrelevance. The proposed methodology has been extensively evaluated on two\nreal-world datasets. The experimental results demonstrate that it achieves a\nsubstantial performance improvement over existing state-of-the-art approaches\nin rumor detection, highlighting its effectiveness and potential for practical\napplications.",
        "url": "http://arxiv.org/abs/2508.11141v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11141v1",
        "arxiv_id": "2508.11141v1",
        "authors": [
            "Bin Ma",
            "Yifei Zhang",
            "Yongjin Xian",
            "Qi Li",
            "Linna Zhou",
            "Gongxun Miao"
        ],
        "submitted": "2025-08-15 01:13:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MoNaCo: More Natural and Complex Questions for Reasoning Across Dozens of Documents",
        "abstract": "Large language models (LLMs) are emerging as a go-to tool for querying\ninformation. However, current LLM benchmarks rarely feature natural questions\nthat are both information-seeking as well as genuinely time-consuming for\nhumans. To address this gap we introduce MoNaCo, a benchmark of 1,315 natural\nand complex questions that require dozens, and at times hundreds, of\nintermediate steps to solve -- far more than any existing QA benchmark. To\nbuild MoNaCo, we developed a decomposed annotation pipeline to elicit and\nmanually answer natural time-consuming questions at scale. Frontier LLMs\nevaluated on MoNaCo achieve at most 61.2% F1, hampered by low recall and\nhallucinations. Our results underscore the need for reasoning models that\nbetter handle the complexity and sheer breadth of real-world\ninformation-seeking questions -- with MoNaCo providing an effective resource\nfor tracking such progress. The MONACO benchmark, codebase, prompts and models\npredictions are publicly available at: https://tomerwolgithub.github.io/monaco",
        "url": "http://arxiv.org/abs/2508.11133v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11133v1",
        "arxiv_id": "2508.11133v1",
        "authors": [
            "Tomer Wolfson",
            "Harsh Trivedi",
            "Mor Geva",
            "Yoav Goldberg",
            "Dan Roth",
            "Tushar Khot",
            "Ashish Sabharwal",
            "Reut Tsarfaty"
        ],
        "submitted": "2025-08-15 00:58:10",
        "source": "arxiv",
        "comment": "Accepted for publication in Transactions of the Association for\n  Computational Linguistics (TACL), 2025. Authors pre-print"
    },
    {
        "title": "+VeriRel: Verification Feedback to Enhance Document Retrieval for Scientific Fact Checking",
        "abstract": "Identification of appropriate supporting evidence is critical to the success\nof scientific fact checking. However, existing approaches rely on off-the-shelf\nInformation Retrieval algorithms that rank documents based on relevance rather\nthan the evidence they provide to support or refute the claim being checked.\nThis paper proposes +VeriRel which includes verification success in the\ndocument ranking. Experimental results on three scientific fact checking\ndatasets (SciFact, SciFact-Open and Check-Covid) demonstrate consistently\nleading performance by +VeriRel for document evidence retrieval and a positive\nimpact on downstream verification. This study highlights the potential of\nintegrating verification feedback to document relevance assessment for\neffective scientific fact checking systems. It shows promising future work to\nevaluate fine-grained relevance when examining complex documents for advanced\nscientific fact checking.",
        "url": "http://arxiv.org/abs/2508.11122v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11122v1",
        "arxiv_id": "2508.11122v1",
        "authors": [
            "Xingyu Deng",
            "Xi Wang",
            "Mark Stevenson"
        ],
        "submitted": "2025-08-14 23:57:40",
        "source": "arxiv",
        "comment": "Accpeted for the 34th ACM International Conference on Information and\n  Knowledge Management (CIKM'25)"
    },
    {
        "title": "Towards Reliable Multi-Agent Systems for Marketing Applications via Reflection, Memory, and Planning",
        "abstract": "Recent advances in large language models (LLMs) enabled the development of AI\nagents that can plan and interact with tools to complete complex tasks.\nHowever, literature on their reliability in real-world applications remains\nlimited. In this paper, we introduce a multi-agent framework for a marketing\ntask: audience curation. To solve this, we introduce a framework called RAMP\nthat iteratively plans, calls tools, verifies the output, and generates\nsuggestions to improve the quality of the audience generated. Additionally, we\nequip the model with a long-term memory store, which is a knowledge base of\nclient-specific facts and past queries. Overall, we demonstrate the use of LLM\nplanning and memory, which increases accuracy by 28 percentage points on a set\nof 88 evaluation queries. Moreover, we show the impact of iterative\nverification and reflection on more ambiguous queries, showing progressively\nbetter recall (roughly +20 percentage points) with more verify/reflect\niterations on a smaller challenge set, and higher user satisfaction. Our\nresults provide practical insights for deploying reliable LLM-based systems in\ndynamic, industry-facing environments.",
        "url": "http://arxiv.org/abs/2508.11120v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11120v1",
        "arxiv_id": "2508.11120v1",
        "authors": [
            "Lorenzo Jaime Yu Flores",
            "Junyi Shen",
            "Xiaoyuan Gu"
        ],
        "submitted": "2025-08-14 23:52:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PaperRegister: Boosting Flexible-grained Paper Search via Hierarchical Register Indexing",
        "abstract": "Paper search is an important activity for researchers, typically involving\nusing a query with description of a topic to find relevant papers. As research\ndeepens, paper search requirements may become more flexible, sometimes\ninvolving specific details such as module configuration rather than being\nlimited to coarse-grained topics. However, previous paper search systems are\nunable to meet these flexible-grained requirements, as these systems mainly\ncollect paper abstracts to construct index of corpus, which lack detailed\ninformation to support retrieval by finer-grained queries. In this work, we\npropose PaperRegister, consisted of offline hierarchical indexing and online\nadaptive retrieval, transforming traditional abstract-based index into\nhierarchical index tree for paper search, thereby supporting queries at\nflexible granularity. Experiments on paper search tasks across a range of\ngranularity demonstrate that PaperRegister achieves the state-of-the-art\nperformance, and particularly excels in fine-grained scenarios, highlighting\nthe good potential as an effective solution for flexible-grained paper search\nin real-world applications. Code for this work is in\nhttps://github.com/Li-Z-Q/PaperRegister.",
        "url": "http://arxiv.org/abs/2508.11116v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11116v1",
        "arxiv_id": "2508.11116v1",
        "authors": [
            "Zhuoqun Li",
            "Xuanang Chen",
            "Hongyu Lin",
            "Yaojie Lu",
            "Xianpei Han",
            "Le Sun"
        ],
        "submitted": "2025-08-14 23:43:46",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Diffusion is a code repair operator and generator",
        "abstract": "Code diffusion models generate code by iteratively removing noise from the\nlatent representation of a code snippet. During later steps of the diffusion\nprocess, when the code snippet has almost converged, differences between\ndiscrete representations of these snippets look like last-mile repairs applied\nto broken or incomplete code. We evaluate the extent to which this resemblance\ncan be exploited to leverage pre-trained code diffusion models for the problem\nof last-mile repair by considering two applications with significant potential.\nFirst, we can leverage the diffusion model for last-mile repair by adding noise\nto a broken code snippet and resuming the diffusion process. Second, we can\nleverage the diffusion model to generate arbitrary amount of training data for\nlast-mile repair tasks (that are computationally more efficient) by sampling an\nintermediate program (input) and the final program (output) from the diffusion\nprocess. We perform experiments on 3 domains (Python, Excel and PowerShell) to\nevaluate applications, as well as analyze properties.",
        "url": "http://arxiv.org/abs/2508.11110v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11110v1",
        "arxiv_id": "2508.11110v1",
        "authors": [
            "Mukul Singh",
            "Gust Verbruggen",
            "Vu Le",
            "Sumit Gulwani"
        ],
        "submitted": "2025-08-14 23:27:09",
        "source": "arxiv",
        "comment": "12 pages"
    },
    {
        "title": "Hybrid-Hierarchical Fashion Graph Attention Network for Compatibility-Oriented and Personalized Outfit Recommendation",
        "abstract": "The rapid expansion of the fashion industry and the growing variety of\nproducts have made it challenging for users to find compatible items on\ne-commerce platforms. Effective fashion recommendation systems are crucial for\nfiltering irrelevant items and suggesting suitable ones. However,\nsimultaneously addressing outfit compatibility and personalized recommendations\nremains a significant challenge, as these aspects are often treated\nindependently in existing studies, often overlooking the complex interactions\nbetween items and user preferences. This research introduces a new framework\nnamed FGAT, inspired by the HFGN model, which leverages graph neural networks\nand graph attention mechanisms to tackle this issue. The proposed framework\nconstructs a three-tier hierarchical graph of users, outfits, and items,\nintegrating visual and textual features to simultaneously model outfit\ncompatibility and user preferences. A graph attention mechanism dynamically\nweights node importance during representation propagation, enabling the capture\nof key interactions and generating precise representations for both user\npreferences and outfit compatibility. Evaluated on the POG dataset, FGAT\noutperforms baseline models such as HFGN, achieving improved results in\nprecision, HR, recall, NDCG, and accuracy.These results demonstrate that\ncombining multimodal visual-textual features with a hierarchical graph\nstructure and attention mechanisms significantly enhances the accuracy and\nefficiency of personalized fashion recommendation systems.",
        "url": "http://arxiv.org/abs/2508.11105v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11105v1",
        "arxiv_id": "2508.11105v1",
        "authors": [
            "Sajjad Saed",
            "Babak Teimourpour"
        ],
        "submitted": "2025-08-14 23:09:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Relative Advantage Debiasing for Watch-Time Prediction in Short-Video Recommendation",
        "abstract": "Watch time is widely used as a proxy for user satisfaction in video\nrecommendation platforms. However, raw watch times are influenced by\nconfounding factors such as video duration, popularity, and individual user\nbehaviors, potentially distorting preference signals and resulting in biased\nrecommendation models. We propose a novel relative advantage debiasing\nframework that corrects watch time by comparing it to empirically derived\nreference distributions conditioned on user and item groups. This approach\nyields a quantile-based preference signal and introduces a two-stage\narchitecture that explicitly separates distribution estimation from preference\nlearning. Additionally, we present distributional embeddings to efficiently\nparameterize watch-time quantiles without requiring online sampling or storage\nof historical data. Both offline and online experiments demonstrate significant\nimprovements in recommendation accuracy and robustness compared to existing\nbaseline methods.",
        "url": "http://arxiv.org/abs/2508.11086v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11086v1",
        "arxiv_id": "2508.11086v1",
        "authors": [
            "Emily Liu",
            "Kuan Han",
            "Minfeng Zhan",
            "Bocheng Zhao",
            "Guanyu Mu",
            "Yang Song"
        ],
        "submitted": "2025-08-14 21:52:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Approaching the Source of Symbol Grounding with Confluent Reductions of Abstract Meaning Representation Directed Graphs",
        "abstract": "Abstract meaning representation (AMR) is a semantic formalism used to\nrepresent the meaning of sentences as directed acyclic graphs. In this paper,\nwe describe how real digital dictionaries can be embedded into AMR directed\ngraphs (digraphs), using state-of-the-art pre-trained large language models.\nThen, we reduce those graphs in a confluent manner, i.e. with transformations\nthat preserve their circuit space. Finally, the properties of these reduces\ndigraphs are analyzed and discussed in relation to the symbol grounding\nproblem.",
        "url": "http://arxiv.org/abs/2508.11068v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11068v1",
        "arxiv_id": "2508.11068v1",
        "authors": [
            "Nicolas Goulet",
            "Alexandre Blondin Massé",
            "Moussa Abdendi"
        ],
        "submitted": "2025-08-14 20:53:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "BIPOLAR: Polarization-based granular framework for LLM bias evaluation",
        "abstract": "Large language models (LLMs) are known to exhibit biases in downstream tasks,\nespecially when dealing with sensitive topics such as political discourse,\ngender identity, ethnic relations, or national stereotypes. Although\nsignificant progress has been made in bias detection and mitigation techniques,\ncertain challenges remain underexplored. This study proposes a reusable,\ngranular, and topic-agnostic framework to evaluate polarisation-related biases\nin LLM (both open-source and closed-source). Our approach combines\npolarisation-sensitive sentiment metrics with a synthetically generated\nbalanced dataset of conflict-related statements, using a predefined set of\nsemantic categories.\n  As a case study, we created a synthetic dataset that focusses on the\nRussia-Ukraine war, and we evaluated the bias in several LLMs: Llama-3,\nMistral, GPT-4, Claude 3.5, and Gemini 1.0. Beyond aggregate bias scores, with\na general trend for more positive sentiment toward Ukraine, the framework\nallowed fine-grained analysis with considerable variation between semantic\ncategories, uncovering divergent behavioural patterns among models. Adaptation\nto prompt modifications showed further bias towards preconceived language and\ncitizenship modification.\n  Overall, the framework supports automated dataset generation and fine-grained\nbias assessment, is applicable to a variety of polarisation-driven scenarios\nand topics, and is orthogonal to many other bias-evaluation strategies.",
        "url": "http://arxiv.org/abs/2508.11061v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11061v1",
        "arxiv_id": "2508.11061v1",
        "authors": [
            "Martin Pavlíček",
            "Tomáš Filip",
            "Petr Sosík"
        ],
        "submitted": "2025-08-14 20:44:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Hell or High Water: Evaluating Agentic Recovery from External Failures",
        "abstract": "As language model agents are applied to real world problems of increasing\ncomplexity, they will be expected to formulate plans across large search\nspaces. If those plans fail for reasons beyond their control, how well do\nlanguage agents search for alternative ways to achieve their goals? We devise a\nspecialized agentic planning benchmark to study this question. Each planning\nproblem is solved via combinations of function calls. The agent searches for\nrelevant functions from a set of over four thousand possibilities, and observes\nenvironmental feedback in the form of function outputs or error messages. Our\nbenchmark confronts the agent with external failures in its workflow, such as\nfunctions that suddenly become unavailable. At the same time, even with the\nintroduction of these failures, we guarantee that the task remains solvable.\nIdeally, an agent's performance on the planning task should not be affected by\nthe presence of external failures. Overall, we find that language agents\nstruggle to formulate and execute backup plans in response to environment\nfeedback. While state-of-the-art models are often able to identify the correct\nfunction to use in the right context, they struggle to adapt to feedback from\nthe environment and often fail to pursue alternate courses of action, even when\nthe search space is artificially restricted. We provide a systematic analysis\nof the failures of both open-source and commercial models, examining the\neffects of search space size, as well as the benefits of scaling model size in\nour setting. Our analysis identifies key challenges for current generative\nmodels as well as promising directions for future work.",
        "url": "http://arxiv.org/abs/2508.11027v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11027v1",
        "arxiv_id": "2508.11027v1",
        "authors": [
            "Andrew Wang",
            "Sophia Hager",
            "Adi Asija",
            "Daniel Khashabi",
            "Nicholas Andrews"
        ],
        "submitted": "2025-08-14 19:21:09",
        "source": "arxiv",
        "comment": "Accepted to COLM 2025"
    },
    {
        "title": "Can Multi-modal (reasoning) LLMs detect document manipulation?",
        "abstract": "Document fraud poses a significant threat to industries reliant on secure and\nverifiable documentation, necessitating robust detection mechanisms. This study\ninvestigates the efficacy of state-of-the-art multi-modal large language models\n(LLMs)-including OpenAI O1, OpenAI 4o, Gemini Flash (thinking), Deepseek Janus,\nGrok, Llama 3.2 and 4, Qwen 2 and 2.5 VL, Mistral Pixtral, and Claude 3.5 and\n3.7 Sonnet-in detecting fraudulent documents. We benchmark these models against\neach other and prior work on document fraud detection techniques using a\nstandard dataset with real transactional documents. Through prompt optimization\nand detailed analysis of the models' reasoning processes, we evaluate their\nability to identify subtle indicators of fraud, such as tampered text,\nmisaligned formatting, and inconsistent transactional sums. Our results reveal\nthat top-performing multi-modal LLMs demonstrate superior zero-shot\ngeneralization, outperforming conventional methods on out-of-distribution\ndatasets, while several vision LLMs exhibit inconsistent or subpar performance.\nNotably, model size and advanced reasoning capabilities show limited\ncorrelation with detection accuracy, suggesting task-specific fine-tuning is\ncritical. This study underscores the potential of multi-modal LLMs in enhancing\ndocument fraud detection systems and provides a foundation for future research\ninto interpretable and scalable fraud mitigation strategies.",
        "url": "http://arxiv.org/abs/2508.11021v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11021v1",
        "arxiv_id": "2508.11021v1",
        "authors": [
            "Zisheng Liang",
            "Kidus Zewde",
            "Rudra Pratap Singh",
            "Disha Patil",
            "Zexi Chen",
            "Jiayu Xue",
            "Yao Yao",
            "Yifei Chen",
            "Qinzhe Liu",
            "Simiao Ren"
        ],
        "submitted": "2025-08-14 18:57:07",
        "source": "arxiv",
        "comment": "arXiv admin note: text overlap with arXiv:2503.20084"
    },
    {
        "title": "Beyond the Rosetta Stone: Unification Forces in Generalization Dynamics",
        "abstract": "Large language models (LLMs) struggle with cross-lingual knowledge transfer:\nthey hallucinate when asked in one language about facts expressed in a\ndifferent language during training. This work introduces a controlled setting\nto study the causes and dynamics of this phenomenon by training small\nTransformer models from scratch on synthetic multilingual datasets. We identify\na learning phase wherein a model develops either separate or unified\nrepresentations of the same facts across languages, and show that unification\nis essential for cross-lingual transfer. We also show that the degree of\nunification depends on mutual information between facts and training data\nlanguage, and on how easy it is to extract that language. Based on these\ninsights, we develop methods to modulate the level of cross-lingual transfer by\nmanipulating data distribution and tokenization, and we introduce metrics and\nvisualizations to formally characterize their effects on unification. Our work\nshows how controlled settings can shed light on pre-training dynamics and\nsuggests new directions for improving cross-lingual transfer in LLMs.",
        "url": "http://arxiv.org/abs/2508.11017v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11017v1",
        "arxiv_id": "2508.11017v1",
        "authors": [
            "Carter Blum",
            "Katja Filipova",
            "Ann Yuan",
            "Asma Ghandeharioun",
            "Julian Zimmert",
            "Fred Zhang",
            "Jessica Hoffmann",
            "Tal Linzen",
            "Martin Wattenberg",
            "Lucas Dixon",
            "Mor Geva"
        ],
        "submitted": "2025-08-14 18:44:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "JobPulse: A Big Data Approach to Real-Time Engineering Workforce Analysis and National Industrial Policy",
        "abstract": "Employment on a societal scale contributes heavily to national and global\naffairs; consequently, job openings and unemployment estimates provide\nimportant information to financial markets and governments alike. However, such\nreports often describe only the supply (employee job seeker) side of the job\nmarket, and skill mismatches are poorly understood. Job postings aggregated on\nrecruiting platforms illuminate marketplace demand, but to date have primarily\nfocused on candidate skills described in their personal profiles. In this\npaper, we report on a big data approach to estimating job market mismatches by\nfocusing on demand, as represented in publicly available job postings. We use\ncommercially available web scraping tools and a new data processing scheme to\nbuild a job posting data set for the semiconductor industry, a strategically\ncritical sector of the United States economy; we focus on Southern California\nas a central hub of advanced technologies. We report on the employer base and\nrelative needs of various job functions. Our work contributes on three fronts:\nFirst, we provide nearly real-time insight into workforce demand; second, we\ndiscuss disambiguation and semantic challenges in analysis of employer data\nbases at scale; and third, we report on the Southern California semiconductor\nengineering ecosystem.",
        "url": "http://arxiv.org/abs/2508.11014v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11014v1",
        "arxiv_id": "2508.11014v1",
        "authors": [
            "Karen S. Markel",
            "Mihir Tale",
            "Andrea Belz"
        ],
        "submitted": "2025-08-14 18:36:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SproutBench: A Benchmark for Safe and Ethical Large Language Models for Youth",
        "abstract": "The rapid proliferation of large language models (LLMs) in applications\ntargeting children and adolescents necessitates a fundamental reassessment of\nprevailing AI safety frameworks, which are largely tailored to adult users and\nneglect the distinct developmental vulnerabilities of minors. This paper\nhighlights key deficiencies in existing LLM safety benchmarks, including their\ninadequate coverage of age-specific cognitive, emotional, and social risks\nspanning early childhood (ages 0--6), middle childhood (7--12), and adolescence\n(13--18). To bridge these gaps, we introduce SproutBench, an innovative\nevaluation suite comprising 1,283 developmentally grounded adversarial prompts\ndesigned to probe risks such as emotional dependency, privacy violations, and\nimitation of hazardous behaviors. Through rigorous empirical evaluation of 47\ndiverse LLMs, we uncover substantial safety vulnerabilities, corroborated by\nrobust inter-dimensional correlations (e.g., between Safety and Risk\nPrevention) and a notable inverse relationship between Interactivity and Age\nAppropriateness. These insights yield practical guidelines for advancing\nchild-centric AI design and deployment.",
        "url": "http://arxiv.org/abs/2508.11009v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11009v1",
        "arxiv_id": "2508.11009v1",
        "authors": [
            "Wenpeng Xing",
            "Lanyi Wei",
            "Haixiao Hu",
            "Rongchang Li",
            "Mohan Li",
            "Changting Lin",
            "Meng Han"
        ],
        "submitted": "2025-08-14 18:21:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Improving Text Style Transfer using Masked Diffusion Language Models with Inference-time Scaling",
        "abstract": "Masked diffusion language models (MDMs) have recently gained traction as a\nviable generative framework for natural language. This can be attributed to its\nscalability and ease of training compared to other diffusion model paradigms\nfor discrete data, establishing itself as the state-of-the-art\nnon-autoregressive generator for discrete data. Diffusion models, in general,\nhave shown excellent ability to improve the generation quality by leveraging\ninference-time scaling either by increasing the number of denoising steps or by\nusing external verifiers on top of the outputs of each step to guide the\ngeneration. In this work, we propose a verifier-based inference-time scaling\nmethod that aids in finding a better candidate generation during the denoising\nprocess of the MDM. Our experiments demonstrate the application of MDMs for\nstandard text-style transfer tasks and establish MDMs as a better alternative\nto autoregressive language models. Additionally, we show that a simple\nsoft-value-based verifier setup for MDMs using off-the-shelf pre-trained\nembedding models leads to significant gains in generation quality even when\nused on top of typical classifier-free guidance setups in the existing\nliterature.",
        "url": "http://arxiv.org/abs/2508.10995v1",
        "pdf_url": "http://arxiv.org/pdf/2508.10995v1",
        "arxiv_id": "2508.10995v1",
        "authors": [
            "Tejomay Kishor Padole",
            "Suyash P Awate",
            "Pushpak Bhattacharyya"
        ],
        "submitted": "2025-08-14 18:01:22",
        "source": "arxiv",
        "comment": "Accepted as a main conference submission in the European Conference\n  on Artificial Intelligence (ECAI 2025)"
    },
    {
        "title": "Match & Choose: Model Selection Framework for Fine-tuning Text-to-Image Diffusion Models",
        "abstract": "Text-to-image (T2I) models based on diffusion and transformer architectures\nadvance rapidly. They are often pretrained on large corpora, and openly shared\non a model platform, such as HuggingFace. Users can then build up AI\napplications, e.g., generating media contents, by adopting pretrained T2I\nmodels and fine-tuning them on the target dataset. While public pretrained T2I\nmodels facilitate the democratization of the models, users face a new\nchallenge: which model can be best fine-tuned based on the target data domain?\nModel selection is well addressed in classification tasks, but little is known\nin (pretrained) T2I models and their performance indication on the target\ndomain. In this paper, we propose the first model selection framework, M&C,\nwhich enables users to efficiently choose a pretrained T2I model from a model\nplatform without exhaustively fine-tuning them all on the target dataset. The\ncore of M&C is a matching graph, which consists of: (i) nodes of available\nmodels and profiled datasets, and (ii) edges of model-data and data-data pairs\ncapturing the fine-tuning performance and data similarity, respectively. We\nthen build a model that, based on the inputs of model/data feature, and,\ncritically, the graph embedding feature, extracted from the matching graph,\npredicts the model achieving the best quality after fine-tuning for the target\ndomain. We evaluate M&C on choosing across ten T2I models for 32 datasets\nagainst three baselines. Our results show that M&C successfully predicts the\nbest model for fine-tuning in 61.3% of the cases and a closely performing model\nfor the rest.",
        "url": "http://arxiv.org/abs/2508.10993v1",
        "pdf_url": "http://arxiv.org/pdf/2508.10993v1",
        "arxiv_id": "2508.10993v1",
        "authors": [
            "Basile Lewandowski",
            "Robert Birke",
            "Lydia Y. Chen"
        ],
        "submitted": "2025-08-14 18:00:50",
        "source": "arxiv",
        "comment": null
    }
]