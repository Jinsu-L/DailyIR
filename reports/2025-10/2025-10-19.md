# Daily Papers Report - 2025-10-19

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. SQuAI: Scientific Question-Answering with Multi-Agent Retrieval-Augmented Generation

- **LLM Score**: 8
- **Keyword Score**: 10
- **Authors**: Ines Besrour, Jingbo He, Tobias Schreieder, Michael F√§rber
- **URL**: <http://arxiv.org/abs/2510.15682v1>
- **Submitted**: 2025-10-17 14:20:55
- **Comment**: Accepted at CIKM 2025
- **Topic Keywords**: dense retrieval, relevance, rag, retrieval
- **Reason**: This paper aligns well with your interests in Information Retrieval, particularly in the context of query understanding and ranking models. The use of multi-agent retrieval-augmented generation for scientific question answering also touches on your background in the e-commerce domain and your interest in deep semantic understanding. However, the focus on scientific question answering and large language models is somewhat specific, limiting its broader applicability to your research interests.

#### Abstract
> We present SQuAI (https://squai.scads.ai/), a scalable and trustworthy
multi-agent retrieval-augmented generation (RAG) framework for scientific
question answering (QA) with large language models (LLMs). SQuAI addresses key
limitations of existing RAG systems in the scholarly domain, where complex,
open-domain questions demand accurate answers, explicit claims with citations,
and retrieval across millions of scientific documents. Built on over 2.3
million full-text papers from arXiv.org, SQuAI employs four collaborative
agents to decompose complex questions into sub-questions, retrieve targeted
evidence via hybrid sparse-dense retrieval, and adaptively filter documents to
improve contextual relevance. To ensure faithfulness and traceability, SQuAI
integrates in-line citations for each generated claim and provides supporting
sentences from the source documents. Our system improves faithfulness, answer
relevance, and contextual relevance by up to +0.088 (12%) over a strong RAG
baseline. We further release a benchmark of 1,000 scientific
question-answer-evidence triplets to support reproducibility. With transparent
reasoning, verifiable citations, and domain-wide scalability, SQuAI
demonstrates how multi-agent RAG enables more trustworthy scientific QA with
LLMs.

---

### 2. Think Parallax: Solving Multi-Hop Problems via Multi-View Knowledge-Graph-Based Retrieval-Augmented Generation

- **LLM Score**: 8
- **Keyword Score**: 7
- **Authors**: Jinliang Liu
- **URL**: <http://arxiv.org/abs/2510.15552v1>
- **Submitted**: 2025-10-17 11:34:27
- **Topic Keywords**: queries, rag, retrieval
- **Reason**: This paper explores knowledge-graph-based retrieval-augmented generation, which is related to query understanding and ranking models in Information Retrieval. The focus on multi-hop reasoning and grounding large language models is also relevant to user behavior modeling and deep semantic understanding. However, the specific application to question answering and knowledge-grounded multi-hop reasoning is somewhat tangential to the user's core research themes.

#### Abstract
> Large language models (LLMs) excel at language understanding but often
hallucinate and struggle with multi-hop reasoning. Knowledge-graph-based
retrieval-augmented generation (KG-RAG) offers grounding, yet most methods rely
on flat embeddings and noisy path exploration. We propose ParallaxRAG, a
framework that symmetrically decouples queries and graph triples into
multi-view spaces, enabling a robust retrieval architecture that explicitly
enforces head diversity while constraining weakly related paths. Central to our
approach is the observation that different attention heads specialize in
semantic relations at distinct reasoning stages, contributing to different hops
of the reasoning chain. This specialization allows ParallaxRAG to construct
cleaner subgraphs and guide LLMs through grounded, step-wise reasoning.
Experiments on WebQSP and CWQ, under our unified, reproducible setup (BGE-M3 +
Llama3.1-8B), demonstrate competitive retrieval and QA performance, alongside
reduced hallucination and good generalization. Our results highlight multi-view
head specialization as a principled direction for knowledge-grounded multi-hop
reasoning. Our implementation will be released as soon as the paper is
accepted.

---

### 3. Structure-R1: Dynamically Leveraging Structural Knowledge in LLM Reasoning through Reinforcement Learning

- **LLM Score**: 8
- **Keyword Score**: 7
- **Authors**: Junlin Wu, Xianrui Zhong, Jiashuo Sun, Bolian Li, Bowen Jin, Jiawei Han, Qingkai Zeng
- **URL**: <http://arxiv.org/abs/2510.15191v1>
- **Submitted**: 2025-10-16 23:19:28
- **Topic Keywords**: queries, rag, retrieval
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The proposed framework, Structure-R1, leverages reinforcement learning to generate structured representations optimized for reasoning, which aligns with your focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> Large language models (LLMs) have demonstrated remarkable advances in
reasoning capabilities. However, their performance remains constrained by
limited access to explicit and structured domain knowledge. Retrieval-Augmented
Generation (RAG) addresses this by incorporating external information as
context to augment reasoning. Nevertheless, traditional RAG systems typically
operate over unstructured and fragmented text, resulting in low information
density and suboptimal reasoning. To overcome these limitations, we propose
\textsc{Structure-R1}, a novel framework that transforms retrieved content into
structured representations optimized for reasoning. Leveraging reinforcement
learning, \textsc{Structure-R1} learns a content representation policy that
dynamically generates and adapts structural formats based on the demands of
multi-step reasoning. Unlike prior methods that rely on fixed schemas, our
approach adopts a generative paradigm capable of producing task-specific
structures tailored to individual queries. To ensure the quality and
reliability of these representations, we introduce a self-reward structural
verification mechanism that checks whether the generated structures are both
correct and self-contained. Extensive experiments on seven knowledge-intensive
benchmarks show that \textsc{Structure-R1} consistently achieves competitive
performance with a 7B-scale backbone model and matches the performance of much
larger models. Additionally, our theoretical analysis demonstrates how
structured representations enhance reasoning by improving information density
and contextual clarity. Our code and data are available at:
https://github.com/jlwu002/sr1.

---

### 4. Mixture of Experts Approaches in Dense Retrieval Tasks

- **LLM Score**: 8
- **Keyword Score**: 6
- **Authors**: Effrosyni Sokli, Pranav Kasela, Georgios Peikos, Gabriella Pasi
- **URL**: <http://arxiv.org/abs/2510.15683v1>
- **Submitted**: 2025-10-17 14:23:19
- **Comment**: 8 pages, 4 figures, 3 tables, reproducible code available at
  https://github.com/FaySokli/SB-MoE , Accepted for publication in Proceedings
  of the 2025 IEEE/WIC International Conference on Web Intelligence and
  Intelligent Agent Technology (WI-IAT 2025)
- **Topic Keywords**: dense retrieval, retrieval, search
- **Reason**: This paper is highly relevant to Information Retrieval, specifically addressing challenges in dense retrieval tasks and proposing a novel approach using Mixture of Experts. The focus on neural Transformer-based models and empirical evaluation across multiple IR tasks aligns with your research interests in query understanding and ranking models.

#### Abstract
> Dense Retrieval Models (DRMs) are a prominent development in Information
Retrieval (IR). A key challenge with these neural Transformer-based models is
that they often struggle to generalize beyond the specific tasks and domains
they were trained on. To address this challenge, prior research in IR
incorporated the Mixture-of-Experts (MoE) framework within each Transformer
layer of a DRM, which, though effective, substantially increased the number of
additional parameters. In this paper, we propose a more efficient design, which
introduces a single MoE block (SB-MoE) after the final Transformer layer. To
assess the retrieval effectiveness of SB-MoE, we perform an empirical
evaluation across three IR tasks. Our experiments involve two evaluation
setups, aiming to assess both in-domain effectiveness and the model's zero-shot
generalizability. In the first setup, we fine-tune SB-MoE with four different
underlying DRMs on seven IR benchmarks and evaluate them on their respective
test sets. In the second setup, we fine-tune SB-MoE on MSMARCO and perform
zero-shot evaluation on thirteen BEIR datasets. Additionally, we perform
further experiments to analyze the model's dependency on its hyperparameters
(i.e., the number of employed and activated experts) and investigate how this
variation affects SB-MoE's performance. The obtained results show that SB-MoE
is particularly effective for DRMs with lightweight base models, such as
TinyBERT and BERT-Small, consistently exceeding standard model fine-tuning
across benchmarks. For DRMs with more parameters, such as BERT-Base and
Contriever, our model requires a larger number of training samples to achieve
improved retrieval performance. Our code is available online at:
https://github.com/FaySokli/SB-MoE.

---

### 5. Cost-Aware Retrieval-Augmentation Reasoning Models with Adaptive Retrieval Depth

- **LLM Score**: 7
- **Keyword Score**: 7
- **Authors**: Helia Hashemi, Victor R√ºhle, Saravan Rajmohan
- **URL**: <http://arxiv.org/abs/2510.15719v1>
- **Submitted**: 2025-10-17 15:04:03
- **Topic Keywords**: query, rag, retrieval
- **Reason**: This paper explores retrieval-augmented reasoning models, which is related to query understanding and ranking models. However, it focuses on efficiency and cost-awareness rather than deep semantic understanding, which is a key aspect of your research interests. The paper's emphasis on real-time relevance optimization is also relevant, but not a central match.

#### Abstract
> Reasoning models have gained significant attention due to their strong
performance, particularly when enhanced with retrieval augmentation. However,
these models often incur high computational costs, as both retrieval and
reasoning tokens contribute substantially to the overall resource usage. In
this work, we make the following contributions: (1) we propose a
retrieval-augmented reasoning model that dynamically adjusts the length of the
retrieved document list based on the query and retrieval results; (2) we
develop a cost-aware advantage function for training of efficient
retrieval-augmented reasoning models through reinforcement learning; and (3) we
explore both memory- and latency-bound implementations of the proposed
cost-aware framework for both proximal and group relative policy optimization
algorithms. We evaluate our approach on seven public question answering
datasets and demonstrate significant efficiency gains, without compromising
effectiveness. In fact, we observed that the model latency decreases by ~16-20%
across datasets, while its effectiveness increases by ~5% on average, in terms
of exact match.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Scaling Beyond Context: A Survey of Multimodal Retrieval-Augmented Generation for Document Understanding

- **LLM Score**: 7
- **Keyword Score**: 4
- **Authors**: Sensen Gao, Shanshan Zhao, Xu Jiang, Lunhao Duan, Yong Xien Chng, Qing-Guo Chen, Weihua Luo, Kaifu Zhang, Jia-Wang Bian, Mingming Gong
- **URL**: <http://arxiv.org/abs/2510.15253v1>
- **Submitted**: 2025-10-17 02:33:16
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper explores document understanding through multimodal retrieval-augmented generation, which is somewhat related to your interests in Information Retrieval and Natural Language Processing. While it doesn't directly focus on query understanding, ranking models, or user behavior modeling, it does involve deep semantic understanding and real-time relevance optimization, making it somewhat relevant to your research.

#### Abstract
> Document understanding is critical for applications from financial analysis
to scientific discovery. Current approaches, whether OCR-based pipelines
feeding Large Language Models (LLMs) or native Multimodal LLMs (MLLMs), face
key limitations: the former loses structural detail, while the latter struggles
with context modeling. Retrieval-Augmented Generation (RAG) helps ground models
in external data, but documents' multimodal nature, i.e., combining text,
tables, charts, and layout, demands a more advanced paradigm: Multimodal RAG.
This approach enables holistic retrieval and reasoning across all modalities,
unlocking comprehensive document intelligence. Recognizing its importance, this
paper presents a systematic survey of Multimodal RAG for document
understanding. We propose a taxonomy based on domain, retrieval modality, and
granularity, and review advances involving graph structures and agentic
frameworks. We also summarize key datasets, benchmarks, and applications, and
highlight open challenges in efficiency, fine-grained representation, and
robustness, providing a roadmap for future progress in document AI.

### 7. GRank: Towards Target-Aware and Streamlined Industrial Retrieval with a Generate-Rank Framework

- **LLM Score**: 6
- **Keyword Score**: 10
- **Authors**: Yijia Sun, Shanshan Huang, Zhiyuan Guan, Qiang Luo, Ruiming Tang, Kun Gai, Guorui Zhou
- **URL**: <http://arxiv.org/abs/2510.15299v1>
- **Submitted**: 2025-10-17 04:15:09
- **Topic Keywords**: retriever, ranking, retrieval, recommend, rank
- **Reason**: The paper presents a novel retrieval paradigm, GRank, which is relevant to information retrieval and recommender systems, but it focuses more on the industrial-scale recommender systems and lacks direct connection to query understanding, ranking models, or user behavior modeling. Although it involves deep semantic understanding and real-time relevance optimization, its primary contribution is in the recommender systems domain.

#### Abstract
> Industrial-scale recommender systems rely on a cascade pipeline in which the
retrieval stage must return a high-recall candidate set from billions of items
under tight latency. Existing solutions ei- ther (i) suffer from limited
expressiveness in capturing fine-grained user-item interactions, as seen in
decoupled dual-tower architectures that rely on separate encoders, or
generative models that lack precise target-aware matching capabilities, or (ii)
build structured indices (tree, graph, quantization) whose item-centric
topologies struggle to incorporate dynamic user preferences and incur
prohibitive construction and maintenance costs.
  We present GRank, a novel structured-index-free retrieval paradigm that
seamlessly unifies target-aware learning with user-centric retrieval. Our key
innovations include: (1) A target-aware Generator trained to perform
personalized candidate generation via GPU-accelerated MIPS, eliminating
semantic drift and maintenance costs of structured indexing; (2) A lightweight
but powerful Ranker that performs fine-grained, candidate-specific inference on
small subsets; (3) An end-to-end multi-task learning framework that ensures
semantic consistency between generation and ranking objectives.
  Extensive experiments on two public benchmarks and a billion-item production
corpus demonstrate that GRank improves Recall@500 by over 30% and 1.7$\times$
the P99 QPS of state-of-the-art tree- and graph-based retrievers.
  GRank has been fully deployed in production in our recommendation platform
since Q2 2025, serving 400 million active users with 99.95% service
availability. Online A/B tests confirm significant improvements in core
engagement metrics, with Total App Usage Time increasing by 0.160% in the main
app and 0.165% in the Lite version.

### 8. The 3rd Place Solution of CCIR CUP 2025: A Framework for Retrieval-Augmented Generation in Multi-Turn Legal Conversation

- **LLM Score**: 6
- **Keyword Score**: 7
- **Authors**: Da Li, Zecheng Fang, Qiang Yan, Wei Huang, Xuanpu Luo
- **URL**: <http://arxiv.org/abs/2510.15722v1>
- **Submitted**: 2025-10-17 15:12:15
- **Comment**: CCIR2025
- **Topic Keywords**: information retrieval, rag, retrieval
- **Reason**: This paper explores Retrieval-Augmented Generation, a technique that combines information retrieval and large language models, which is somewhat related to the user's interests in query understanding and ranking models. However, its application in the legal field is not directly aligned with the user's focus on e-commerce or real-time relevance optimization. The paper's relevance to the user's broader interests in NLP and data mining is moderate.

#### Abstract
> Retrieval-Augmented Generation has made significant progress in the field of
natural language processing. By combining the advantages of information
retrieval and large language models, RAG can generate relevant and contextually
appropriate responses based on items retrieved from reliable sources. This
technology has demonstrated outstanding performance across multiple domains,
but its application in the legal field remains in its exploratory phase. In
this paper, we introduce our approach for "Legal Knowledge Retrieval and
Generation" in CCIR CUP 2025, which leverages large language models and
information retrieval systems to provide responses based on laws in response to
user questions.

### 9. Exemplar-Guided Planing: Enhanced LLM Agent for KGQA

- **LLM Score**: 6
- **Keyword Score**: 3
- **Authors**: Jingao Xu, Shuoyoucheng Ma, Xin Song, Rong Jiang, Hongkui Tu, Bin Zhou
- **URL**: <http://arxiv.org/abs/2510.15283v1>
- **Submitted**: 2025-10-17 03:43:06
- **Topic Keywords**: queries
- **Reason**: This paper proposes a novel framework for Knowledge Graph Question Answering (KGQA) using Large Language Models (LLMs), which involves planning and exploration. Although it's related to search technologies and query understanding, it's not directly focused on information retrieval or ranking models, but rather on a specific application of NLP. The paper's emphasis on KGQA and LLMs makes it somewhat relevant to the user's interests, but not a central match.

#### Abstract
> Large Language Models (LLMs) as interactive agents show significant promise
in Knowledge Graph Question Answering (KGQA) but often struggle with the
semantic gap between natural language queries and structured knowledge graph
(KG) representations. This leads to suboptimal planning and inefficient
exploration on KG, while training-free approaches often underutilize valuable
reasoning patterns in training data. To address these limitations, we propose a
novel framework, Exemplar-Guided Planning (EGP), which enhances the planning
capabilities of LLM agents for KGQA. EGP first preprocesses the training set
questions via entity templating to normalize semantic variations. It then
retrieves highly similar exemplary questions and their successful reasoning
paths from this preprocessed set using semantic embeddings and an efficient
FAISS index. These retrieved exemplars dynamically guide the LLM's planning
process in two key phases: (1) Task Decomposition, by aligning generated
sub-objectives with proven reasoning steps, and (2) Relation Exploration, by
providing high-quality auxiliary information to improve relation pruning
accuracy. Additionally, we introduce a Smart Lookahead mechanism during
relation exploration to improve efficiency by preemptively exploring promising
paths and potentially terminating exploration earlier. We apply EGP to the
Plan-on-Graph (PoG) framework, termed PoG-EGP. Extensive experiments on two
real-world KGQA datasets, WebQSP and CWQ, demonstrate that PoG-EGP
significantly improves over the baseline PoG system and other compared methods.

### 10. MTmixAtt: Integrating Mixture-of-Experts with Multi-Mix Attention for Large-Scale Recommendation

- **LLM Score**: 4
- **Keyword Score**: 11
- **Authors**: Xianyang Qi, Yuan Tian, Zhaoyu Hu, Zhirui Kuai, Chang Liu, Hongxiang Lin, Lei Wang
- **URL**: <http://arxiv.org/abs/2510.15286v1>
- **Submitted**: 2025-10-17 03:50:09
- **Topic Keywords**: ranking, ctr, cvr, recommend, rank, trec
- **Reason**: The paper focuses on recommender systems and proposes a new architecture, MTmixAtt, for large-scale recommendation tasks. While it involves ranking models and user behavior modeling, it is not primarily focused on information retrieval or deep semantic understanding, which are core areas of your research interests.

#### Abstract
> Industrial recommender systems critically depend on high-quality ranking
models. However, traditional pipelines still rely on manual feature engineering
and scenario-specific architectures, which hinder cross-scenario transfer and
large-scale deployment. To address these challenges, we propose
\textbf{MTmixAtt}, a unified Mixture-of-Experts (MoE) architecture with
Multi-Mix Attention, designed for large-scale recommendation tasks. MTmixAtt
integrates two key components. The \textbf{AutoToken} module automatically
clusters heterogeneous features into semantically coherent tokens, removing the
need for human-defined feature groups. The \textbf{MTmixAttBlock} module
enables efficient token interaction via a learnable mixing matrix, shared dense
experts, and scenario-aware sparse experts, capturing both global patterns and
scenario-specific behaviors within a single framework. Extensive experiments on
the industrial TRec dataset from Meituan demonstrate that MTmixAtt consistently
outperforms state-of-the-art baselines including Transformer-based models,
WuKong, HiFormer, MLP-Mixer, and RankMixer. At comparable parameter scales,
MTmixAtt achieves superior CTR and CTCVR metrics; scaling to MTmixAtt-1B yields
further monotonic gains. Large-scale online A/B tests validate the real-world
impact: in the \textit{Homepage} scenario, MTmixAtt increases Payment PV by
\textbf{+3.62\%} and Actual Payment GTV by \textbf{+2.54\%}. Overall, MTmixAtt
provides a unified and scalable solution for modeling arbitrary heterogeneous
features across scenarios, significantly improving both user experience and
commercial outcomes.

### 11. DMRetriever: A Family of Models for Improved Text Retrieval in Disaster Management

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Kai Yin, Xiangjue Dong, Chengkai Liu, Allen Lin, Lingfeng Shi, Ali Mostafavi, James Caverlee
- **URL**: <http://arxiv.org/abs/2510.15087v1>
- **Submitted**: 2025-10-16 19:08:34
- **Topic Keywords**: retriever, dense retrieval, retrieval, search
- **Reason**: The paper focuses on developing a retrieval model for disaster management, which is a specific domain. While it involves text retrieval and dense retrieval models, it doesn't directly relate to the user's core research themes in Information Retrieval, query understanding, ranking models, or user behavior modeling. However, the model's design and evaluation might be of interest to the user's broader interests in NLP and data mining.

#### Abstract
> Effective and efficient access to relevant information is essential for
disaster management. However, no retrieval model is specialized for disaster
management, and existing general-domain models fail to handle the varied search
intents inherent to disaster management scenarios, resulting in inconsistent
and unreliable performance. To this end, we introduce DMRetriever, the first
series of dense retrieval models (33M to 7.6B) tailored for this domain. It is
trained through a novel three-stage framework of bidirectional attention
adaptation, unsupervised contrastive pre-training, and difficulty-aware
progressive instruction fine-tuning, using high-quality data generated through
an advanced data refinement pipeline. Comprehensive experiments demonstrate
that DMRetriever achieves state-of-the-art (SOTA) performance across all six
search intents at every model scale. Moreover, DMRetriever is highly
parameter-efficient, with 596M model outperforming baselines over 13.3 X larger
and 33M model exceeding baselines with only 7.6% of their parameters. All
codes, data, and checkpoints are available at
https://github.com/KaiYin97/DMRETRIEVER

### 12. Large-scale User Game Lifecycle Representation Learning

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Yanjie Gou, Jiangming Liu, Kouying Xue, Yi Hua
- **URL**: <http://arxiv.org/abs/2510.15412v1>
- **Submitted**: 2025-10-17 08:06:18
- **Topic Keywords**: rag, user behavior, cvr, recommend
- **Reason**: The paper explores recommender systems for online game platforms, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the focus on game advertising and user behavior modeling in the e-commerce domain is not a central match to your primary research themes, which include query understanding, ranking models, and deep semantic understanding.

#### Abstract
> The rapid expansion of video game production necessitates the development of
effective advertising and recommendation systems for online game platforms.
Recommending and advertising games to users hinges on capturing their interest
in games. However, existing representation learning methods crafted for
handling billions of items in recommendation systems are unsuitable for game
advertising and recommendation. This is primarily due to game sparsity, where
the mere hundreds of games fall short for large-scale user representation
learning, and game imbalance, where user behaviors are overwhelmingly dominated
by a handful of popular games. To address the sparsity issue, we introduce the
User Game Lifecycle (UGL), designed to enrich user behaviors in games.
Additionally, we propose two innovative strategies aimed at manipulating user
behaviors to more effectively extract both short and long-term interests. To
tackle the game imbalance challenge, we present an Inverse Probability Masking
strategy for UGL representation learning. The offline and online experimental
results demonstrate that the UGL representations significantly enhance model by
achieving a 1.83% AUC offline increase on average and a 21.67% CVR online
increase on average for game advertising and a 0.5% AUC offline increase and a
0.82% ARPU online increase for in-game item recommendation.

### 13. DRO-InstructZero: Distributionally Robust Prompt Optimization for Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Yangyang Li
- **URL**: <http://arxiv.org/abs/2510.15260v1>
- **Submitted**: 2025-10-17 02:55:48
- **Comment**: Preprint. Under review at ICLR 2026. 11 pages, 2 figures
- **Topic Keywords**: query, rag, search
- **Reason**: While the paper explores prompt optimization for large language models, which is a related area to information retrieval, it does not directly address query understanding, ranking models, or user behavior modeling. The focus on distributionally robust optimization and prompt learning is somewhat relevant to the broader field of NLP, but it does not align with the user's primary research interests in IR and search technologies.

#### Abstract
> Large language models are highly sensitive to prompt wording. However,
popular automatic prompt search methods, including InstructZero, often degrade
under distribution shift and adversarial evaluation because they optimize
expected performance under a single evaluation distribution. Consequently,
prompts that work in one setting frequently fail to transfer. To address this,
DRO-InstructZero formulates zero-shot prompt optimization as robust Bayesian
optimization. Specifically, an f-divergence ball defines an ambiguity set
around the evaluation distribution, and a robust acquisition rule maximizes
worst-case expected utility while retaining the query efficiency of Bayesian
search. Therefore, the search explicitly targets reliability under distribution
shift rather than average behavior alone. Experiments follow the
instruction-induction protocol with matched query budgets across formality
rewriting, code debugging, and translation. For example, on BIG-Bench
informative-to-formal rewriting, accuracy improves from 61.3 +/- 0.7% to
approximately 85-90%, yielding an absolute gain of about 25-30 points.
Moreover, auto-debugging shows about +25-point gains under domain shift.
Meanwhile, stable tasks such as cause-and-effect remain above 96%, indicating
no loss on in-distribution cases. Furthermore, improvements are consistent
across divergence choices and decoding temperatures. Overall, DRO-InstructZero
connects distributionally robust optimization with prompt learning, offering a
plug-and-play and general approach for reliable, transferable prompt alignment
under real-world uncertainty.

### 14. Enhanced Sentiment Interpretation via a Lexicon-Fuzzy-Transformer Framework

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Shayan Rokhva, Mousa Alizadeh, Maryam Abdollahi Shamami
- **URL**: <http://arxiv.org/abs/2510.15843v1>
- **Submitted**: 2025-10-17 17:36:05
- **Topic Keywords**: rag, commerce, e-commerce
- **Reason**: The paper focuses on sentiment analysis, which is related to information retrieval and natural language processing. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The use of fuzzy logic and transformer models is an interesting aspect, but it is not directly applicable to the user's primary research themes.

#### Abstract
> Accurately detecting sentiment polarity and intensity in product reviews and
social media posts remains challenging due to informal and domain-specific
language. To address this, we propose a novel hybrid lexicon-fuzzy-transformer
framework that combines rule-based heuristics, contextual deep learning, and
fuzzy logic to generate continuous sentiment scores reflecting both polarity
and strength. The pipeline begins with VADER-based initial sentiment
estimations, which are refined through a two-stage adjustment process. This
involves leveraging confidence scores from DistilBERT, a lightweight
transformer and applying fuzzy logic principles to mitigate excessive
neutrality bias and enhance granularity. A custom fuzzy inference system then
maps the refined scores onto a 0 to 1 continuum, producing expert)like
judgments. The framework is rigorously evaluated on four domain-specific
datasets. food delivery, e-commerce, tourism, and fashion. Results show
improved alignment with user ratings, better identification of sentiment
extremes, and reduced misclassifications. Both quantitative metrics
(distributional alignment, confusion matrices) and qualitative insights (case
studies, runtime analysis) affirm the models robustness and efficiency. This
work demonstrates the value of integrating symbolic reasoning with neural
models for interpretable, finegrained sentiment analysis in linguistically
dynamic domains.

### 15. LLMs Judge Themselves: A Game-Theoretic Framework for Human-Aligned Evaluation

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Gao Yang, Yuhang Liu, Siyu Miao, Xinyue Liang, Zhengyang Liu, Heyan Huang
- **URL**: <http://arxiv.org/abs/2510.15746v1>
- **Submitted**: 2025-10-17 15:34:25
- **Topic Keywords**: ranking, rank
- **Reason**: This paper explores the evaluation of large language models using game-theoretic frameworks, which is somewhat related to my interests in Information Retrieval and Natural Language Processing. However, the focus on language models and evaluation metrics is not directly aligned with my primary research themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> Ideal or real - that is the question.In this work, we explore whether
principles from game theory can be effectively applied to the evaluation of
large language models (LLMs). This inquiry is motivated by the growing
inadequacy of conventional evaluation practices, which often rely on
fixed-format tasks with reference answers and struggle to capture the nuanced,
subjective, and open-ended nature of modern LLM behavior. To address these
challenges, we propose a novel alternative: automatic mutual evaluation, where
LLMs assess each other's output through self-play and peer review. These peer
assessments are then systematically compared with human voting behavior to
evaluate their alignment with human judgment. Our framework incorporates
game-theoretic voting algorithms to aggregate peer reviews, enabling a
principled investigation into whether model-generated rankings reflect human
preferences. Empirical results reveal both convergences and divergences between
theoretical predictions and human evaluations, offering valuable insights into
the promises and limitations of mutual evaluation. To the best of our
knowledge, this is the first work to jointly integrate mutual evaluation,
game-theoretic aggregation, and human-grounded validation for evaluating the
capabilities of LLMs.

### 16. Rethinking Cross-lingual Gaps from a Statistical Viewpoint

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Vihari Piratla, Purvam Jain, Darshan Singh, Partha Talukdar, Trevor Cohn
- **URL**: <http://arxiv.org/abs/2510.15551v1>
- **Submitted**: 2025-10-17 11:34:04
- **Comment**: 22 pages
- **Topic Keywords**: query, search
- **Reason**: This paper explores cross-lingual gaps in large language models, which is somewhat related to information retrieval and natural language processing. However, the focus on cross-lingual gaps and variance decomposition is not directly aligned with the user's core research themes of query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat tangential to the user's interests.

#### Abstract
> Any piece of knowledge is usually expressed in one or a handful of natural
languages on the web or in any large corpus. Large Language Models (LLMs) act
as a bridge by acquiring knowledge from a source language and making it
accessible when queried from target languages. Prior research has pointed to a
cross-lingual gap, viz., a drop in accuracy when the knowledge is queried in a
target language compared to when the query is in the source language. Existing
research has rationalized divergence in latent representations in source and
target languages as the source of cross-lingual gap. In this work, we take an
alternative view and hypothesize that the variance of responses in the target
language is the main cause of this gap. For the first time, we formalize the
cross-lingual gap in terms of bias-variance decomposition. We present extensive
experimental evidence which support proposed formulation and hypothesis. We
then reinforce our hypothesis through multiple inference-time interventions
that control the variance and reduce the cross-lingual gap. We demonstrate a
simple prompt instruction to reduce the response variance, which improved
target accuracy by 20-25% across different models.

### 17. AutoGraph-R1: End-to-End Reinforcement Learning for Knowledge Graph Construction

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Hong Ting Tsang, Jiaxin Bai, Haoyu Huang, Qiao Xiao, Tianshi Zheng, Baixuan Xu, Shujie Liu, Yangqiu Song
- **URL**: <http://arxiv.org/abs/2510.15339v1>
- **Submitted**: 2025-10-17 06:03:36
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper focuses on knowledge graph construction using reinforcement learning, which is somewhat related to information retrieval and search technologies. However, the primary application area is question answering systems, and the paper does not directly address query understanding, ranking models, or user behavior modeling. While it involves deep semantic understanding, the context is different from the user's core research themes.

#### Abstract
> Building effective knowledge graphs (KGs) for Retrieval-Augmented Generation
(RAG) is pivotal for advancing question answering (QA) systems. However, its
effectiveness is hindered by a fundamental disconnect: the knowledge graph (KG)
construction process is decoupled from its downstream application, yielding
suboptimal graph structures. To bridge this gap, we introduce AutoGraph-R1, the
first framework to directly optimize KG construction for task performance using
Reinforcement Learning (RL). AutoGraph-R1 trains an LLM constructor by framing
graph generation as a policy learning problem, where the reward is derived from
the graph's functional utility in a RAG pipeline. We design two novel,
task-aware reward functions, one for graphs as knowledge carriers and another
as knowledge indices. Across multiple QA benchmarks, AutoGraph-R1 consistently
enables graph RAG methods to achieve significant performance gains over using
task-agnostic baseline graphs. Our work shows it is possible to close the loop
between construction and application, shifting the paradigm from building
intrinsically ``good'' graphs to building demonstrably ``useful'' ones.

### 18. TACL: Threshold-Adaptive Curriculum Learning Strategy for Enhancing Medical Text Understanding

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Mucheng Ren, Yucheng Yan, He Chen, Danqing Hu, Jun Xu, Xian Zeng
- **URL**: <http://arxiv.org/abs/2510.15269v1>
- **Submitted**: 2025-10-17 03:16:51
- **Comment**: Accepted as BIBM 2025 Regular. 8 pages. Pre-CR version
- **Topic Keywords**: ctr, acl
- **Reason**: This paper presents a novel framework for medical text understanding, which is somewhat related to the user's interests in Information Retrieval and Natural Language Processing. However, the focus on medical text understanding and clinical decision-making is not directly aligned with the user's primary research themes, and the application of the framework is limited to the medical domain.

#### Abstract
> Medical texts, particularly electronic medical records (EMRs), are a
cornerstone of modern healthcare, capturing critical information about patient
care, diagnoses, and treatments. These texts hold immense potential for
advancing clinical decision-making and healthcare analytics. However, their
unstructured nature, domain-specific language, and variability across contexts
make automated understanding an intricate challenge. Despite the advancements
in natural language processing, existing methods often treat all data as
equally challenging, ignoring the inherent differences in complexity across
clinical records. This oversight limits the ability of models to effectively
generalize and perform well on rare or complex cases. In this paper, we present
TACL (Threshold-Adaptive Curriculum Learning), a novel framework designed to
address these challenges by rethinking how models interact with medical texts
during training. Inspired by the principle of progressive learning, TACL
dynamically adjusts the training process based on the complexity of individual
samples. By categorizing data into difficulty levels and prioritizing simpler
cases early in training, the model builds a strong foundation before tackling
more complex records. By applying TACL to multilingual medical data, including
English and Chinese clinical records, we observe significant improvements
across diverse clinical tasks, including automatic ICD coding, readmission
prediction and TCM syndrome differentiation. TACL not only enhances the
performance of automated systems but also demonstrates the potential to unify
approaches across disparate medical domains, paving the way for more accurate,
scalable, and globally applicable medical text understanding solutions.

### 19. FACE: A General Framework for Mapping Collaborative Filtering Embeddings into LLM Tokens

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Chao Wang, Yixin Song, Jinhui Ye, Chuan Qin, Dazhong Shen, Lingfeng Liu, Xiang Wang, Yanyong Zhang
- **URL**: <http://arxiv.org/abs/2510.15729v1>
- **Submitted**: 2025-10-17 15:19:54
- **Comment**: Accepted by NeurIPS 2025
- **Topic Keywords**: rag, recommend
- **Reason**: The paper proposes a framework for mapping collaborative filtering embeddings into LLM tokens, which is somewhat related to information retrieval and NLP. However, the focus is on recommender systems and not directly on query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's relevance is somewhat tangential to the user's primary research themes.

#### Abstract
> Recently, large language models (LLMs) have been explored for integration
with collaborative filtering (CF)-based recommendation systems, which are
crucial for personalizing user experiences. However, a key challenge is that
LLMs struggle to interpret the latent, non-semantic embeddings produced by CF
approaches, limiting recommendation effectiveness and further applications. To
address this, we propose FACE, a general interpretable framework that maps CF
embeddings into pre-trained LLM tokens. Specifically, we introduce a
disentangled projection module to decompose CF embeddings into concept-specific
vectors, followed by a quantized autoencoder to convert continuous embeddings
into LLM tokens (descriptors). Then, we design a contrastive alignment
objective to ensure that the tokens align with corresponding textual signals.
Hence, the model-agnostic FACE framework achieves semantic alignment without
fine-tuning LLMs and enhances recommendation performance by leveraging their
pre-trained capabilities. Empirical results on three real-world recommendation
datasets demonstrate performance improvements in benchmark models, with
interpretability studies confirming the interpretability of the descriptors.
Code is available in https://github.com/YixinRoll/FACE.

### 20. Leveraging LLMs for Context-Aware Implicit Textual and Multimodal Hate Speech Detection

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Joshua Wolfe Brook, Ilia Markov
- **URL**: <http://arxiv.org/abs/2510.15685v1>
- **Submitted**: 2025-10-17 14:28:57
- **Comment**: 8 pages, 9 figures, submitted to LREC 2026
- **Topic Keywords**: rag, search
- **Reason**: The paper explores the application of Large Language Models (LLMs) in hate speech detection, which is a topic related to Natural Language Processing (NLP). However, it does not directly align with the user's primary focus on Information Retrieval (IR), query understanding, ranking models, and user behavior modeling. The paper's context-aware approach and use of LLMs are somewhat relevant to the user's interests in deep semantic understanding, but it is not a central match.

#### Abstract
> This research introduces a novel approach to textual and multimodal Hate
Speech Detection (HSD), using Large Language Models (LLMs) as dynamic knowledge
bases to generate background context and incorporate it into the input of HSD
classifiers. Two context generation strategies are examined: one focused on
named entities and the other on full-text prompting. Four methods of
incorporating context into the classifier input are compared: text
concatenation, embedding concatenation, a hierarchical transformer-based
fusion, and LLM-driven text enhancement. Experiments are conducted on the
textual Latent Hatred dataset of implicit hate speech and applied in a
multimodal setting on the MAMI dataset of misogynous memes. Results suggest
that both the contextual information and the method by which it is incorporated
are key, with gains of up to 3 and 6 F1 points on textual and multimodal setups
respectively, from a zero-context baseline to the highest-performing system,
based on embedding concatenation.

### 21. Enhance Large Language Models as Recommendation Systems with Collaborative Filtering

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Zhisheng Yang, Xiaofei Xu, Ke Deng, Li Li
- **URL**: <http://arxiv.org/abs/2510.15647v1>
- **Submitted**: 2025-10-17 13:35:14
- **Topic Keywords**: rag, recommend
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and recommender systems. However, it focuses on leveraging Large Language Models as recommendation systems with collaborative filtering, which is not a primary area of interest for you. While it touches on some related concepts, it does not align closely with your core research themes in Information Retrieval and Search technologies.

#### Abstract
> As powerful tools in Natural Language Processing (NLP), Large Language Models
(LLMs) have been leveraged for crafting recommendations to achieve precise
alignment with user preferences and elevate the quality of the recommendations.
The existing approaches implement both non-tuning and tuning strategies.
Compared to following the tuning strategy, the approaches following the
non-tuning strategy avoid the relatively costly, time-consuming, and
expertise-requiring process of further training pre-trained LLMs on
task-specific datasets, but they suffer the issue of not having the
task-specific business or local enterprise knowledge. To the best of our
knowledge, none of the existing approaches following the non-tuning strategy
explicitly integrates collaborative filtering, one of the most successful
recommendation techniques. This study aims to fill the gap by proposing
critique-based LLMs as recommendation systems (Critic-LLM-RS). For our purpose,
we train a separate machine-learning model called Critic that implements
collaborative filtering for recommendations by learning from the interactions
between many users and items. The Critic provides critiques to LLMs to
significantly refine the recommendations. Extensive experiments have verified
the effectiveness of Critic-LLM-RS on real datasets.

### 22. MCA: Modality Composition Awareness for Robust Composed Multimodal Retrieval

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Qiyu Wu, Shuyang Cui, Satoshi Hayakawa, Wei-Yao Wang, Hiromi Wakaki, Yuki Mitsufuji
- **URL**: <http://arxiv.org/abs/2510.15543v1>
- **Submitted**: 2025-10-17 11:20:35
- **Topic Keywords**: retrieval, search
- **Reason**: This paper focuses on multimodal retrieval and proposes a framework to improve robustness in composed multimodal retrieval. While it involves deep semantic understanding and multimodal processing, it does not directly relate to query understanding, ranking models, or user behavior modeling in the context of information retrieval. The paper's emphasis on multimodal large language models and robustness under distribution shifts is somewhat relevant to the broader field of information retrieval, but it does not align with the user's core research themes.

#### Abstract
> Multimodal retrieval, which seeks to retrieve relevant content across
modalities such as text or image, supports applications from AI search to
contents production. Despite the success of separate-encoder approaches like
CLIP align modality-specific embeddings with contrastive learning, recent
multimodal large language models (MLLMs) enable a unified encoder that directly
processes composed inputs. While flexible and advanced, we identify that
unified encoders trained with conventional contrastive learning are prone to
learn modality shortcut, leading to poor robustness under distribution shifts.
We propose a modality composition awareness framework to mitigate this issue.
Concretely, a preference loss enforces multimodal embeddings to outperform
their unimodal counterparts, while a composition regularization objective
aligns multimodal embeddings with prototypes composed from its unimodal parts.
These objectives explicitly model structural relationships between the composed
representation and its unimodal counterparts. Experiments on various benchmarks
show gains in out-of-distribution retrieval, highlighting modality composition
awareness as a effective principle for robust composed multimodal retrieval
when utilizing MLLMs as the unified encoder.

### 23. Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Baode Wang, Biao Wu, Weizhen Li, Meng Fang, Zuming Huang, Jun Huang, Haozhe Wang, Yanjie Liang, Ling Chen, Wei Chu, Yuan Qi
- **URL**: <http://arxiv.org/abs/2510.15349v1>
- **Submitted**: 2025-10-17 06:26:59
- **Comment**: 22 pages, 14 figures,
- **Topic Keywords**: rag, search
- **Reason**: This paper is somewhat related to information retrieval, but its focus on document parsing from scanned images and layout understanding is not directly aligned with your core research themes in query understanding, ranking models, and user behavior modeling. However, it does involve NLP and vision-language models, which are related areas of interest.

#### Abstract
> Document parsing from scanned images into structured formats remains a
significant challenge due to its complexly intertwined elements such as text
paragraphs, figures, formulas, and tables. Existing supervised fine-tuning
methods often struggle to generalize across diverse document types, leading to
poor performance, particularly on out-of-distribution data. This issue is
further exacerbated by the limited availability of high-quality training data
for layout-aware parsing tasks. To address these challenges, we introduce
LayoutRL, a reinforcement learning framework that optimizes layout
understanding through composite rewards integrating normalized edit distance,
paragraph count accuracy, and reading order preservation. To support this
training, we construct the Infinity-Doc-400K dataset, which we use to train
Infinity-Parser, a vision-language model demonstrating robust generalization
across various domains. Extensive evaluations on benchmarks including
OmniDocBench, olmOCR-Bench, PubTabNet, and FinTabNet show that Infinity-Parser
consistently achieves state-of-the-art performance across a broad range of
document types, languages, and structural complexities, substantially
outperforming both specialized document parsing systems and general-purpose
vision-language models. We will release our code, dataset, and model to
facilitate reproducible research in document parsing.

### 24. Latent Topic Synthesis: Leveraging LLMs for Electoral Ad Analysis

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Alexander Brady, Tunazzina Islam
- **URL**: <http://arxiv.org/abs/2510.15125v1>
- **Submitted**: 2025-10-16 20:30:20
- **Comment**: Under-submission
- **Topic Keywords**: rag, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, as it involves analyzing and understanding large volumes of text data. However, the focus on electoral ad analysis and social media discourse is not directly aligned with your core research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Social media platforms play a pivotal role in shaping political discourse,
but analyzing their vast and rapidly evolving content remains a major
challenge. We introduce an end-to-end framework for automatically generating an
interpretable topic taxonomy from an unlabeled corpus. By combining
unsupervised clustering with prompt-based labeling, our method leverages large
language models (LLMs) to iteratively construct a taxonomy without requiring
seed sets or domain expertise. We apply this framework to a large corpus of
Meta (previously known as Facebook) political ads from the month ahead of the
2024 U.S. Presidential election. Our approach uncovers latent discourse
structures, synthesizes semantically rich topic labels, and annotates topics
with moral framing dimensions. We show quantitative and qualitative analyses to
demonstrate the effectiveness of our framework. Our findings reveal that voting
and immigration ads dominate overall spending and impressions, while abortion
and election-integrity achieve disproportionate reach. Funding patterns are
equally polarized: economic appeals are driven mainly by conservative PACs,
abortion messaging splits between pro- and anti-rights coalitions, and
crime-and-justice campaigns are fragmented across local committees. The framing
of these appeals also diverges--abortion ads emphasize liberty/oppression
rhetoric, while economic messaging blends care/harm, fairness/cheating, and
liberty/oppression narratives. Topic salience further reveals strong
correlations between moral foundations and issues. Demographic targeting also
emerges. This work supports scalable, interpretable analysis of political
messaging on social media, enabling researchers, policymakers, and the public
to better understand emerging narratives, polarization dynamics, and the moral
underpinnings of digital political communication.

### 25. GraphMind: Interactive Novelty Assessment System for Accelerating Scientific Discovery

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Italo Luis da Silva, Hanqi Yan, Lin Gui, Yulan He
- **URL**: <http://arxiv.org/abs/2510.15706v1>
- **Submitted**: 2025-10-17 14:49:07
- **Comment**: 9 pages, 6 figures, 3 tables, EMNLP 2025 Demo paper
- **Topic Keywords**: retrieval
- **Reason**: This paper introduces GraphMind, an interactive tool for novelty assessment in scientific literature. While it leverages LLMs and integrates with APIs, its focus is on scientific literature analysis and novelty assessment, which is somewhat related to information retrieval, but not directly aligned with the user's core research themes.

#### Abstract
> Large Language Models (LLMs) show strong reasoning and text generation
capabilities, prompting their use in scientific literature analysis, including
novelty assessment. While evaluating novelty of scientific papers is crucial
for peer review, it requires extensive knowledge of related work, something not
all reviewers have. While recent work on LLM-assisted scientific literature
analysis supports literature comparison, existing approaches offer limited
transparency and lack mechanisms for result traceability via an information
retrieval module. To address this gap, we introduce $\textbf{GraphMind}$, an
easy-to-use interactive web tool designed to assist users in evaluating the
novelty of scientific papers or drafted ideas. Specially, $\textbf{GraphMind}$
enables users to capture the main structure of a scientific paper, explore
related ideas through various perspectives, and assess novelty via providing
verifiable contextual insights. $\textbf{GraphMind}$ enables users to annotate
key elements of a paper, explore related papers through various relationships,
and assess novelty with contextual insight. This tool integrates external APIs
such as arXiv and Semantic Scholar with LLMs to support annotation, extraction,
retrieval and classification of papers. This combination provides users with a
rich, structured view of a scientific idea's core contributions and its
connections to existing work. $\textbf{GraphMind}$ is available at
https://oyarsa.github.io/graphmind and a demonstration video at
https://youtu.be/wKbjQpSvwJg. The source code is available at
https://github.com/oyarsa/graphmind.

### 26. MSAM: Multi-Semantic Adaptive Mining for Cross-Modal Drone Video-Text Retrieval

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Jinghao Huang, Yaxiong Chen, Ganchao Liu
- **URL**: <http://arxiv.org/abs/2510.15470v1>
- **Submitted**: 2025-10-17 09:26:28
- **Topic Keywords**: retrieval
- **Reason**: The paper explores a specific application of cross-modal retrieval in drone video-text scenarios, which is somewhat related to information retrieval and NLP. However, the focus on drone video-text retrieval and the proposed MSAM approach, while innovative, does not directly align with the user's core research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> With the advancement of drone technology, the volume of video data increases
rapidly, creating an urgent need for efficient semantic retrieval. We are the
first to systematically propose and study the drone video-text retrieval (DVTR)
task. Drone videos feature overhead perspectives, strong structural
homogeneity, and diverse semantic expressions of target combinations, which
challenge existing cross-modal methods designed for ground-level views in
effectively modeling their characteristics. Therefore, dedicated retrieval
mechanisms tailored for drone scenarios are necessary. To address this issue,
we propose a novel approach called Multi-Semantic Adaptive Mining (MSAM). MSAM
introduces a multi-semantic adaptive learning mechanism, which incorporates
dynamic changes between frames and extracts rich semantic information from
specific scene regions, thereby enhancing the deep understanding and reasoning
of drone video content. This method relies on fine-grained interactions between
words and drone video frames, integrating an adaptive semantic construction
module, a distribution-driven semantic learning term and a diversity semantic
term to deepen the interaction between text and drone video modalities and
improve the robustness of feature representation. To reduce the interference of
complex backgrounds in drone videos, we introduce a cross-modal interactive
feature fusion pooling mechanism that focuses on feature extraction and
matching in target regions, minimizing noise effects. Extensive experiments on
two self-constructed drone video-text datasets show that MSAM outperforms other
existing methods in the drone video-text retrieval task. The source code and
dataset will be made publicly available.

### 27. When to Ensemble: Identifying Token-Level Points for Stable and Fast LLM Ensembling

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Heecheol Yun, Kwangmin Ki, Junghyun Lee, Eunho Yang
- **URL**: <http://arxiv.org/abs/2510.15346v1>
- **Submitted**: 2025-10-17 06:18:29
- **Comment**: preprint
- **Topic Keywords**: rag
- **Reason**: This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and deep semantic understanding, but its focus on Large Language Models (LLMs) and ensembling techniques is not directly aligned with the user's primary focus on Information Retrieval (IR) and query understanding.

#### Abstract
> Ensembling Large Language Models (LLMs) has gained attention as a promising
approach to surpass the performance of individual models by leveraging their
complementary strengths. In particular, aggregating models' next-token
probability distributions to select the next token has been shown to be
effective in various tasks. However, while successful for short-form answers,
its application to long-form generation remains underexplored. In this paper,
we show that using existing ensemble methods in long-form generation requires a
careful choice of ensembling positions, since the standard practice of
ensembling at every token often degrades performance. We identify two key
factors for determining these positions: tokenization mismatch across models
and consensus in their next-token probability distributions. Based on this, we
propose SAFE, (Stable And Fast LLM Ensembling), a framework that selectively
ensembles by jointly considering these factors. To further improve stability,
we introduce a probability sharpening strategy that consolidates probabilities
spread across multiple sub-word tokens representing the same word into a single
representative token. Our experiments on diverse benchmarks, including MATH500
and BBH, demonstrate that SAFE outperforms existing methods in both accuracy
and efficiency, with gains achieved even when ensembling fewer than 1% of
tokens.

### 28. HOB: A Holistically Optimized Bidding Strategy under Heterogeneous Auction Mechanisms with Organic Traffic

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Qi Li, Wendong Huang, Qichen Ye, Wutong Xu, Cheems Wang, Rongquan Bai, Wei Yuan, Guan Wang, Chuan Yu, Jian Xu
- **URL**: <http://arxiv.org/abs/2510.15238v1>
- **Submitted**: 2025-10-17 02:00:09
- **Topic Keywords**: commerce, e-commerce
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and e-commerce, but its focus on auction mechanisms and bidding strategies is not directly aligned with your core themes of query understanding, ranking models, and user behavior modeling. While it touches on optimization and real-time relevance, the context is more specific to advertising and auction systems.

#### Abstract
> The E-commerce advertising platforms typically sell commercial traffic
through either second-price auction (SPA) or first-price auction (FPA). SPA was
historically prevalent due to its dominant strategy incentive-compatible (DSIC)
for bidders with quasi-linear utilities, especially when budgets are not a
binding constraint, while FPA has gained more prominence for offering higher
revenue potential to publishers and avoiding the possibility for discriminatory
treatment in personalized reserve prices. Meanwhile, on the demand side,
advertisers are increasingly adopting platform-wide marketing solutions akin to
QuanZhanTui, shifting from spending budgets solely on commercial traffic to
bidding on the entire traffic for the purpose of maximizing overall sales. For
automated bidding systems, such a trend poses a critical challenge: determining
optimal strategies across heterogeneous auction channels to fulfill diverse
advertiser objectives, such as maximizing return (MaxReturn) or meeting target
return on ad spend (TargetROAS). To overcome this challenge, this work makes
two key contributions. First, we derive an efficient solution for optimal
bidding under FPA channels, which takes into account the presence of organic
traffic - traffic can be won for free. Second, we introduce a marginal cost
alignment (MCA) strategy that provably secures bidding efficiency across
heterogeneous auction mechanisms. To validate performance of our developed
framework, we conduct comprehensive offline experiments on public datasets and
large-scale online A/B testing, which demonstrate consistent improvements over
existing methods.

### 29. HugAgent: Evaluating LLMs in Simulating Human-Like Individual Reasoning on Open-Ended Tasks

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Chance Jiajie Li, Zhenze Mo, Yuhan Tang, Ao Qu, Jiayi Wu, Kaiya Ivy Zhao, Yulu Gan, Jie Fan, Jiangbo Yu, Hang Jiang, Paul Pu Liang, Jinhua Zhao, Luis Alberto Alonso Pastor, Kent Larson
- **URL**: <http://arxiv.org/abs/2510.15144v1>
- **Submitted**: 2025-10-16 21:03:54
- **Comment**: To appear in NeurIPS 2025 Workshop on Bridging Language, Agent, and
  World Models (LAW)
- **Topic Keywords**: rag
- **Reason**: This paper is somewhat related to your interests in Natural Language Processing (NLP) and query understanding, as it involves simulating human-like reasoning with large language models. However, it does not directly relate to your primary focus on information retrieval, ranking models, or user behavior modeling.

#### Abstract
> Simulating human reasoning in open-ended tasks has been a long-standing
aspiration in AI and cognitive science. While large language models now
approximate human responses at scale, they remain tuned to population-level
consensus, often erasing the individuality of reasoning styles and belief
trajectories. To advance the vision of more human-like reasoning in machines,
we introduce HugAgent (Human-Grounded Agent Benchmark), a benchmark for
average-to-individual reasoning adaptation. The task is to predict how a
specific person would reason and update their beliefs in novel scenarios, given
partial evidence of their past views. HugAgent adopts a dual-track design: a
synthetic track for scale and systematic stress tests, and a human track for
ecologically valid, "out-loud" reasoning data. This design enables scalable,
reproducible evaluation of intra-agent fidelity: whether models can capture not
just what people believe, but how their reasoning evolves. Experiments with
state-of-the-art LLMs reveal persistent adaptation gaps, positioning HugAgent
as the first extensible benchmark for aligning machine reasoning with the
individuality of human thought. Our benchmark and chatbot are open-sourced as
HugAgent (https://anonymous.4open.science/r/HugAgent) and TraceYourThinking
(https://anonymous.4open.science/r/trace-your-thinking).

### 30. A Generalizable Rhetorical Strategy Annotation Model Using LLM-based Debate Simulation and Labelling

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Shiyu Ji, Farnoosh Hashemi, Joice Chen, Juanwen Pan, Weicheng Ma, Hefan Zhang, Sophia Pan, Ming Cheng, Shubham Mohole, Saeed Hassanpour, Soroush Vosoughi, Michael Macy
- **URL**: <http://arxiv.org/abs/2510.15081v1>
- **Submitted**: 2025-10-16 18:51:23
- **Comment**: The first two authors contributed equally
- **Topic Keywords**: rag
- **Reason**: This paper focuses on rhetorical strategy annotation and analysis, which is related to natural language processing and deep semantic understanding. However, it does not directly address information retrieval, search technologies, or user behavior modeling, making it only loosely relevant to your core research interests.

#### Abstract
> Rhetorical strategies are central to persuasive communication, from political
discourse and marketing to legal argumentation. However, analysis of rhetorical
strategies has been limited by reliance on human annotation, which is costly,
inconsistent, difficult to scale. Their associated datasets are often limited
to specific topics and strategies, posing challenges for robust model
development. We propose a novel framework that leverages large language models
(LLMs) to automatically generate and label synthetic debate data based on a
four-part rhetorical typology (causal, empirical, emotional, moral). We
fine-tune transformer-based classifiers on this LLM-labeled dataset and
validate its performance against human-labeled data on this dataset and on
multiple external corpora. Our model achieves high performance and strong
generalization across topical domains. We illustrate two applications with the
fine-tuned model: (1) the improvement in persuasiveness prediction from
incorporating rhetorical strategy labels, and (2) analyzing temporal and
partisan shifts in rhetorical strategies in U.S. Presidential debates
(1960-2020), revealing increased use of affective over cognitive argument in
U.S. Presidential debates.

### 31. Fine-Tuning MedGemma for Clinical Captioning to Enhance Multimodal RAG over Malaysia CPGs

- **LLM Score**: 2
- **Keyword Score**: 10
- **Authors**: Lee Qi Zun, Mohamad Zulhilmi Bin Abdul Halim, Goh Man Fye
- **URL**: <http://arxiv.org/abs/2510.15418v1>
- **Submitted**: 2025-10-17 08:11:54
- **Topic Keywords**: query, queries, rag, retrieval
- **Reason**: This paper focuses on clinical captioning and multimodal RAG systems, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves fine-tuning a model using a parameter-efficient method, the application and domain are quite different from the user's interests.

#### Abstract
> Retrieval-Augmented Generation systems are essential for providing fact-based
guidance from Malaysian Clinical Practice Guidelines. However, their
effectiveness with image-based queries is limited, as general Vision-Language
Model captions often lack clinical specificity and factual grounding. This
study proposes and validates a framework to specialize the MedGemma model for
generating high-fidelity captions that serve as superior queries. To overcome
data scarcity, we employ a knowledge distillation pipeline to create a
synthetic dataset across dermatology, fundus, and chest radiography domains,
and fine-tune MedGemma using the parameter-efficient QLoRA method. Performance
was rigorously assessed through a dual framework measuring both classification
accuracy and, via a novel application of the RAGAS framework, caption
faithfulness, relevancy, and correctness. The fine-tuned model demonstrated
substantial improvements in classification performance, while RAGAS evaluation
confirmed significant gains in caption faithfulness and correctness, validating
the models ability to produce reliable, factually grounded descriptions. This
work establishes a robust pipeline for specializing medical VLMs and validates
the resulting model as a high-quality query generator, laying the groundwork
for enhancing multimodal RAG systems in evidence-based clinical decision
support.

### 32. FarsiMCQGen: a Persian Multiple-choice Question Generation Framework

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Mohammad Heydari Rad, Rezvan Afari, Saeedeh Momtazi
- **URL**: <http://arxiv.org/abs/2510.15134v1>
- **Submitted**: 2025-10-16 20:52:07
- **Topic Keywords**: ranking, rag, rank, search
- **Reason**: This paper focuses on question generation in the Persian language, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves some NLP techniques, the context and application are quite different from the user's core areas of interest.

#### Abstract
> Multiple-choice questions (MCQs) are commonly used in educational testing, as
they offer an efficient means of evaluating learners' knowledge. However,
generating high-quality MCQs, particularly in low-resource languages such as
Persian, remains a significant challenge. This paper introduces FarsiMCQGen, an
innovative approach for generating Persian-language MCQs. Our methodology
combines candidate generation, filtering, and ranking techniques to build a
model that generates answer choices resembling those in real MCQs. We leverage
advanced methods, including Transformers and knowledge graphs, integrated with
rule-based approaches to craft credible distractors that challenge test-takers.
Our work is based on data from Wikipedia, which includes general knowledge
questions. Furthermore, this study introduces a novel Persian MCQ dataset
comprising 10,289 questions. This dataset is evaluated by different
state-of-the-art large language models (LLMs). Our results demonstrate the
effectiveness of our model and the quality of the generated dataset, which has
the potential to inspire further research on MCQs.

### 33. Fault Cause Identification across Manufacturing Lines through Ontology-Guided and Process-Aware FMEA Graph Learning with LLMs

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Sho Okazaki, Kohei Kaminishi, Takuma Fujiu, Yusheng Wang, Jun Ota
- **URL**: <http://arxiv.org/abs/2510.15428v1>
- **Submitted**: 2025-10-17 08:35:47
- **Topic Keywords**: rag, retrieval, rank
- **Reason**: This paper focuses on fault cause identification in manufacturing lines using ontology-guided and process-aware FMEA graph learning with LLMs. Although it involves graph neural networks and large language models, it is not directly related to information retrieval, search technologies, or user behavior modeling, which are the core areas of your research interests.

#### Abstract
> Fault cause identification in automated manufacturing lines is challenging
due to the system's complexity, frequent reconfigurations, and the limited
reusability of existing Failure Mode and Effects Analysis (FMEA) knowledge.
Although FMEA worksheets contain valuable expert insights, their reuse across
heterogeneous lines is hindered by natural language variability, inconsistent
terminology, and process differences. To address these limitations, this study
proposes a process-aware framework that enhances FMEA reusability by combining
manufacturing-domain conceptualization with graph neural network (GNN)
reasoning. First, FMEA worksheets from multiple manufacturing lines are
transformed into a unified knowledge graph through ontology-guided large
language model (LLM) extraction, capturing domain concepts such as actions,
states, components, and parameters. Second, a Relational Graph Convolutional
Network (RGCN) with the process-aware scoring function learns embeddings that
respect both semantic relationships and sequential process flows. Finally, link
prediction is employed to infer and rank candidate fault causes consistent with
the target line's process flow.
  A case study on automotive pressure sensor assembly lines demonstrates that
the proposed method outperforms a state-of-the-art retrieval-augmented
generation (RAG) baseline (F1@20 = 0.267) and an RGCN approach (0.400),
achieving the best performance (0.523) in fault cause identification. Ablation
studies confirm the contributions of both LLM-driven domain conceptualization
and process-aware learning. These results indicate that the proposed framework
significantly improves the transferability of FMEA knowledge across
heterogeneous lines, thereby supporting operators in diagnosing failures more
reliably and paving the way for future domain-adaptive LLM applications in
smart manufacturing.

### 34. The Elephant in the Coreference Room: Resolving Coreference in Full-Length French Fiction Works

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Antoine Bourgois, Thierry Poibeau
- **URL**: <http://arxiv.org/abs/2510.15594v1>
- **Submitted**: 2025-10-17 12:40:33
- **Topic Keywords**: relevance, search
- **Reason**: This paper focuses on coreference resolution in long literary works, which is a specific task in Natural Language Processing (NLP). While it touches on NLP, it does not align with the user's primary research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling.

#### Abstract
> While coreference resolution is attracting more interest than ever from
computational literature researchers, representative datasets of fully
annotated long documents remain surprisingly scarce. In this paper, we
introduce a new annotated corpus of three full-length French novels, totaling
over 285,000 tokens. Unlike previous datasets focused on shorter texts, our
corpus addresses the challenges posed by long, complex literary works, enabling
evaluation of coreference models in the context of long reference chains. We
present a modular coreference resolution pipeline that allows for fine-grained
error analysis. We show that our approach is competitive and scales effectively
to long documents. Finally, we demonstrate its usefulness to infer the gender
of fictional characters, showcasing its relevance for both literary analysis
and downstream NLP tasks.

### 35. Measuring the Effect of Disfluency in Multilingual Knowledge Probing Benchmarks

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Kirill Semenov, Rico Sennrich
- **URL**: <http://arxiv.org/abs/2510.15115v1>
- **Submitted**: 2025-10-16 20:16:56
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper focuses on multilingual knowledge probing benchmarks and the effect of disfluency on LLMs, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve NLP, the specific context and goals of the paper are not aligned with your areas of focus.

#### Abstract
> For multilingual factual knowledge assessment of LLMs, benchmarks such as
MLAMA use template translations that do not take into account the grammatical
and semantic information of the named entities inserted in the sentence. This
leads to numerous instances of ungrammaticality or wrong wording of the final
prompts, which complicates the interpretation of scores, especially for
languages that have a rich morphological inventory. In this work, we sample 4
Slavic languages from the MLAMA dataset and compare the knowledge retrieval
scores between the initial (templated) MLAMA dataset and its sentence-level
translations made by Google Translate and ChatGPT. We observe a significant
increase in knowledge retrieval scores, and provide a qualitative analysis for
possible reasons behind it. We also make an additional analysis of 5 more
languages from different families and see similar patterns. Therefore, we
encourage the community to control the grammaticality of highly multilingual
datasets for higher and more interpretable results, which is well approximated
by whole sentence translation with neural MT or LLM systems. The dataset and
all related code is published at the Github repository:
https://github.com/ZurichNLP/Fluent-mLAMA.

### 36. Continual Learning via Sparse Memory Finetuning

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Jessy Lin, Luke Zettlemoyer, Gargi Ghosh, Wen-Tau Yih, Aram Markosyan, Vincent-Pierre Berges, Barlas Oƒüuz
- **URL**: <http://arxiv.org/abs/2510.15103v1>
- **Submitted**: 2025-10-16 19:44:38
- **Topic Keywords**: rag, acl
- **Reason**: This paper focuses on continual learning in language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on deep semantic understanding, the context is different from your typical areas of focus.

#### Abstract
> Modern language models are powerful, but typically static after deployment. A
major obstacle to building models that continually learn over time is
catastrophic forgetting, where updating on new data erases previously acquired
capabilities. Motivated by the intuition that mitigating forgetting is
challenging because trainable parameters are shared across all tasks, we
investigate whether sparse parameter updates can enable learning without
catastrophic forgetting. We introduce sparse memory finetuning, leveraging
memory layer models (Berges et al., 2024), which are sparsely updated by
design. By updating only the memory slots that are highly activated by a new
piece of knowledge relative to usage on pretraining data, we reduce
interference between new knowledge and the model's existing capabilities. We
evaluate learning and forgetting compared to full finetuning and
parameter-efficient finetuning with LoRA on two question answering tasks. We
find that sparse memory finetuning learns new knowledge while exhibiting
substantially less forgetting: while NaturalQuestions F1 drops by 89% after
full finetuning on new facts and 71% with LoRA, sparse memory finetuning yields
only an 11% drop with the same level of new knowledge acquisition. Our results
suggest sparsity in memory layers offers a promising path toward continual
learning in large language models.

### 37. Unleashing Scientific Reasoning for Bio-experimental Protocol Generation via Structured Component-based Reward Mechanism

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Haoran Sun, Yankai Jiang, Zhenyu Tang, Yaning Pan, Shuang Gu, Zekai Lin, Lilong Wang, Wenjie Lou, Lei Liu, Lei Bai, Xiaosong Wang
- **URL**: <http://arxiv.org/abs/2510.15600v1>
- **Submitted**: 2025-10-17 12:47:50
- **Topic Keywords**: queries
- **Reason**: This paper focuses on bio-experimental protocol generation using natural language queries, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves large language models, the context is specific to scientific protocols and experimental reliability, making it less relevant to the user's areas of focus.

#### Abstract
> The foundation of reproducible science lies in protocols that are precise,
logically ordered, and executable. The autonomous generation of these protocols
through natural language queries could greatly improve the efficiency of the
reproduction process. However, current leading large language models (LLMs)
often generate incomplete or inconsistent protocols, limiting their utility. To
address this limitation, we first introduce SciRecipe, a large-scale dataset of
over 12K structured protocols spanning 27 biological subfields and encompassing
both comprehension and problem-solving tasks. To further improve protocol
generation, we propose the "Sketch-and-Fill" paradigm, which separates
analysis, structuring, and expression to ensure each step is explicit and
verifiable. Complementing this, the structured component-based reward mechanism
evaluates step granularity, action order, and semantic fidelity, aligning model
optimization with experimental reliability. Building on these components, we
develop Thoth, trained through a staged Knowledge-to-Action process that
progresses from knowledge acquisition to operational reasoning and ultimately
to robust, executable protocol generation. Across multiple benchmarks, Thoth
consistently surpasses both proprietary and open-source LLMs, achieving
significant improvements in step alignment, logical sequencing, and semantic
accuracy. Our approach paves the way for reliable scientific assistants that
bridge knowledge with experimental execution. All data, code, and models will
be released publicly.

### 38. Leveraging Test Driven Development with Large Language Models for Reliable and Verifiable Spreadsheet Code Generation: A Research Framework

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Dr Simon Thorne, Dr Advait Sarkar
- **URL**: <http://arxiv.org/abs/2510.15585v1>
- **Submitted**: 2025-10-17 12:28:16
- **Comment**: 16 pages
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on the integration of Large Language Models with Test-Driven Development for spreadsheet code generation, which is unrelated to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Large Language Models (LLMs), such as ChatGPT, are increasingly leveraged for
generating both traditional software code and spreadsheet logic. Despite their
impressive generative capabilities, these models frequently exhibit critical
issues such as hallucinations, subtle logical inconsistencies, and syntactic
errors, risks particularly acute in high stakes domains like financial
modelling and scientific computations, where accuracy and reliability are
paramount. This position paper proposes a structured research framework that
integrates the proven software engineering practice of Test-Driven Development
(TDD) with Large Language Model (LLM) driven generation to enhance the
correctness of, reliability of, and user confidence in generated outputs. We
hypothesise that a "test first" methodology provides both technical constraints
and cognitive scaffolding, guiding LLM outputs towards more accurate,
verifiable, and comprehensible solutions. Our framework, applicable across
diverse programming contexts, from spreadsheet formula generation to scripting
languages such as Python and strongly typed languages like Rust, includes an
explicitly outlined experimental design with clearly defined participant
groups, evaluation metrics, and illustrative TDD based prompting examples. By
emphasising test driven thinking, we aim to improve computational thinking,
prompt engineering skills, and user engagement, particularly benefiting
spreadsheet users who often lack formal programming training yet face serious
consequences from logical errors. We invite collaboration to refine and
empirically evaluate this approach, ultimately aiming to establish responsible
and reliable LLM integration in both educational and professional development
practices.

### 39. Readability Reconsidered: A Cross-Dataset Analysis of Reference-Free Metrics

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Catarina G Belem, Parker Glenn, Alfy Samuel, Anoop Kumar, Daben Liu
- **URL**: <http://arxiv.org/abs/2510.15345v1>
- **Submitted**: 2025-10-17 06:17:21
- **Comment**: Accepted at the TSAR Workshop @ EMNLP 2025
- **Topic Keywords**: rag, rank
- **Reason**: This paper focuses on readability metrics and their evaluation, which is not directly related to your core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves text analysis, the topic is more aligned with NLP applications in text accessibility rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> Automatic readability assessment plays a key role in ensuring effective and
accessible written communication. Despite significant progress, the field is
hindered by inconsistent definitions of readability and measurements that rely
on surface-level text properties. In this work, we investigate the factors
shaping human perceptions of readability through the analysis of 897 judgments,
finding that, beyond surface-level cues, information content and topic strongly
shape text comprehensibility. Furthermore, we evaluate 15 popular readability
metrics across five English datasets, contrasting them with six more nuanced,
model-based metrics. Our results show that four model-based metrics
consistently place among the top four in rank correlations with human
judgments, while the best performing traditional metric achieves an average
rank of 8.6. These findings highlight a mismatch between current readability
metrics and human perceptions, pointing to model-based approaches as a more
promising direction.

### 40. Automatic essay scoring: leveraging Jaccard coefficient and Cosine similaritywith n-gram variation in vector space model approach

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Andharini Dwi Cahyani, Moh. Wildan Fathoni, Fika Hastarita Rachman, Ari Basuki, Salman Amin, Bain Khusnul Khotimah
- **URL**: <http://arxiv.org/abs/2510.15311v1>
- **Submitted**: 2025-10-17 04:54:12
- **Topic Keywords**: rag, search
- **Reason**: This paper is not relevant to your research interests as it focuses on automated essay scoring, which is outside your primary areas of interest in Information Retrieval and Search technologies. Although it involves some NLP techniques, the context and application are not aligned with your core research themes.

#### Abstract
> Automated essay scoring (AES) is a vital area of research aiming to provide
efficient and accurate assessment tools for evaluating written content. This
study investigates the effectiveness of two popular similarity metrics, Jaccard
coefficient, and Cosine similarity, within the context of vector space
models(VSM)employing unigram, bigram, and trigram representations. The data
used in this research was obtained from the formative essay of the citizenship
education subject in a junior high school. Each essay undergoes preprocessing
to extract features using n-gram models, followed by vectorization to transform
text data into numerical representations. Then, similarity scores are computed
between essays using both Jaccard coefficient and Cosine similarity. The
performance of the system is evaluated by analyzing the root mean square error
(RMSE), which measures the difference between the scores given by human graders
and those generated by the system. The result shows that the Cosine similarity
outperformed the Jaccard coefficient. In terms of n-gram, unigrams have lower
RMSE compared to bigrams and trigrams.

### 41. Emergence of Linear Truth Encodings in Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Shauli Ravfogel, Gilad Yehudai, Tal Linzen, Joan Bruna, Alberto Bietti
- **URL**: <http://arxiv.org/abs/2510.15804v1>
- **Submitted**: 2025-10-17 16:30:07
- **Comment**: Accepted in Neurips 2025
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, or user behavior modeling. While it touches on language models, which are related to NLP, the focus is on understanding the emergence of linear truth encodings in language models, which is not a central theme in your research.

#### Abstract
> Recent probing studies reveal that large language models exhibit linear
subspaces that separate true from false statements, yet the mechanism behind
their emergence is unclear. We introduce a transparent, one-layer transformer
toy model that reproduces such truth subspaces end-to-end and exposes one
concrete route by which they can arise. We study one simple setting in which
truth encoding can emerge: a data distribution where factual statements
co-occur with other factual statements (and vice-versa), encouraging the model
to learn this distinction in order to lower the LM loss on future tokens. We
corroborate this pattern with experiments in pretrained language models.
Finally, in the toy setting we observe a two-phase learning dynamic: networks
first memorize individual factual associations in a few steps, then -- over a
longer horizon -- learn to linearly separate true from false, which in turn
lowers language-modeling loss. Together, these results provide both a
mechanistic demonstration and an empirical motivation for how and why linear
truth representations can emerge in language models.

### 42. Exploring the Synergy of Quantitative Factors and Newsflow Representations from Large Language Models for Stock Return Prediction

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Tian Guo, Emmanuel Hauptmann
- **URL**: <http://arxiv.org/abs/2510.15691v1>
- **Submitted**: 2025-10-17 14:35:03
- **Topic Keywords**: rag
- **Reason**: This paper focuses on stock return prediction using quantitative factors and newsflow representations, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves large language models, the application is in a different domain (finance) and does not align with the user's core themes.

#### Abstract
> In quantitative investing, return prediction supports various tasks,
including stock selection, portfolio optimization, and risk management.
Quantitative factors, such as valuation, quality, and growth, capture various
characteristics of stocks. Unstructured financial data, like news and
transcripts, has attracted growing attention, driven by recent advances in
large language models (LLMs). This paper examines effective methods for
leveraging multimodal factors and newsflow in return prediction and stock
selection. First, we introduce a fusion learning framework to learn a unified
representation from factors and newsflow representations generated by an LLM.
Within this framework, we compare three representative methods: representation
combination, representation summation, and attentive representations. Next,
building on empirical observations from fusion learning, we explore the mixture
model that adaptively combines predictions made by single modalities and their
fusion. To mitigate the training instability observed in the mixture model, we
introduce a decoupled training approach with theoretical insights. Finally, our
experiments on real investment universes yield several insights into effective
multimodal modeling of factors and news for stock return prediction.

### 43. HypoSpace: Evaluating LLM Creativity as Set-Valued Hypothesis Generators under Underdetermination

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Tingting Chen, Beibei Lin, Zifeng Yuan, Qiran Zou, Hongyu He, Yew-Soon Ong, Anirudh Goyal, Dianbo Liu
- **URL**: <http://arxiv.org/abs/2510.15614v1>
- **Submitted**: 2025-10-17 13:00:32
- **Topic Keywords**: rag
- **Reason**: This paper focuses on evaluating the creativity of Large Language Models (LLMs) in proposing sets of explanations, which is not directly related to Information Retrieval, Search technologies, or user behavior modeling. While it involves Natural Language Processing, the context is more aligned with scientific workflows and hypothesis generation rather than search or retrieval tasks.

#### Abstract
> As language models are increasingly used in scientific workflows, evaluating
their ability to propose sets of explanations-not just a single correct
answer-becomes critical. Many scientific problems are underdetermined:
multiple, mechanistically distinct hypotheses are consistent with the same
observations. We introduce HypoSpace, a diagnostic suite that treats LLMs as
samplers of finite hypothesis sets and measures three complementary indicators:
Validity (precision of proposals consistent with observations), Uniqueness
(non-redundancy among proposals), and Recovery (coverage of the enumerated
admissible set). We instantiate HypoSpace in three structured domains with
deterministic validators and exactly enumerated hypothesis spaces: (i) causal
graphs from perturbations, (ii) gravity-constrained 3D voxel reconstruction
from top-down projections, and (iii) Boolean genetic interactions. Across
instruction-tuned and reasoning-focused models, Validity often remains high
while Uniqueness and Recovery degrade as the admissible space grows, revealing
mode collapse that is invisible to correctness-only metrics. HypoSpace offers a
controlled probe-rather than a leaderboard-for methods that explicitly explore
and cover admissible explanation spaces. Code is available at:
https://github.com/CTT-Pavilion/_HypoSpace.

### 44. KITE: A Benchmark for Evaluating Korean Instruction-Following Abilities in Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Dongjun Kim, Chanhee Park, Chanjun Park, Heuiseok Lim
- **URL**: <http://arxiv.org/abs/2510.15558v1>
- **Submitted**: 2025-10-17 11:45:15
- **Comment**: 13 pages, 3 figures, 5 tables
- **Topic Keywords**: search, korea
- **Reason**: This paper focuses on the development of a benchmark for evaluating Korean language models' instruction-following abilities, which is not directly related to the user's interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves large language models, the specific context and goals of the research are not aligned with the user's core research themes.

#### Abstract
> The instruction-following capabilities of large language models (LLMs) are
pivotal for numerous applications, from conversational agents to complex
reasoning systems. However, current evaluations predominantly focus on English
models, neglecting the linguistic and cultural nuances of other languages.
Specifically, Korean, with its distinct syntax, rich morphological features,
honorific system, and dual numbering systems, lacks a dedicated benchmark for
assessing open-ended instruction-following capabilities. To address this gap,
we introduce the Korean Instruction-following Task Evaluation (KITE), a
comprehensive benchmark designed to evaluate both general and Korean-specific
instructions. Unlike existing Korean benchmarks that focus mainly on factual
knowledge or multiple-choice testing, KITE directly targets diverse, open-ended
instruction-following tasks. Our evaluation pipeline combines automated metrics
with human assessments, revealing performance disparities across models and
providing deeper insights into their strengths and weaknesses. By publicly
releasing the KITE dataset and code, we aim to foster further research on
culturally and linguistically inclusive LLM development and inspire similar
endeavors for other underrepresented languages.

### 45. From Characters to Tokens: Dynamic Grouping with Hierarchical BPE

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Rares Dolga, Lucas Maystre, Tudor Berariu, David Barber
- **URL**: <http://arxiv.org/abs/2510.15517v1>
- **Submitted**: 2025-10-17 10:42:10
- **Topic Keywords**: rag
- **Reason**: This paper focuses on subword tokenization methods, specifically proposing a dynamic character grouping method. While it touches on NLP, it doesn't seem to directly relate to information retrieval, query understanding, ranking models, or user behavior modeling, which are core areas of interest.

#### Abstract
> Subword tokenization methods like Byte Pair Encoding (BPE) are widely used in
large language models due to their balance of vocabulary compactness and
representational power. However, they suffer from inefficiencies in
representing rare words and require large embedding matrices. Character-level
models address these issues but introduce performance bottlenecks, particularly
in Transformer-based architectures. Recent hierarchical models attempt to merge
the benefits of both paradigms by grouping characters into patches, but
existing patching strategies either rely on whitespace-limiting applicability
to certain languages, or require auxiliary models that introduce new
dependencies. In this paper, we propose a dynamic character grouping method
that leverages the structure of existing BPE tokenization without requiring
additional models. By appending explicit end-of-patch markers to BPE tokens and
introducing a second-level BPE compression stage to control patch granularity,
our method offers efficient, flexible, and language-agnostic representations.
Empirical results demonstrate that our approach matches or exceeds the
performance of dynamic entropy- and whitespace-based patching strategies, while
maintaining a compact vocabulary.

### 46. Train a Unified Multimodal Data Quality Classifier with Synthetic Data

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Weizhi Wang, Rongmei Lin, Shiyang Li, Colin Lockard, Ritesh Sarkhel, Sanket Lokegaonkar, Jingbo Shang, Xifeng Yan, Nasser Zalmout, Xian Li
- **URL**: <http://arxiv.org/abs/2510.15162v1>
- **Submitted**: 2025-10-16 21:53:28
- **Comment**: EMNLP 2025 Findings
- **Topic Keywords**: rag
- **Reason**: This paper focuses on multimodal data quality classification, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves pre-training large language models, the context is more aligned with data quality and multimodal learning rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> The Multimodal Large Language Models (MLLMs) are continually pre-trained on a
mixture of image-text caption data and interleaved document data, while the
high-quality data filtering towards image-text interleaved document data is
under-explored. We propose to train an efficient MLLM as a Unified Mulitmodal
Data Quality Classifier to Filter both high-quality image-text caption and
interleaved data (UniFilter). To address the challenge of collecting diverse
labeled multimodal data, we introduce a semi-synthetic approach that leverages
readily available raw images and generates corresponding text across four
quality levels. This method enables efficient creation of sample-score pairs
for both caption and interleaved document data to train UniFilter. We apply
UniFilter to curate high-quality caption data from DataComp caption dataset and
interleaved data from the OBELICS image-text interleaved dataset. MLLMs
pre-trained on the filtered data demonstrate significantly enhanced
capabilities compared to those trained on baseline-filtered data, achieving
stronger zero-shot reasoning and in-context learning capabilities. After visual
supervised fine-tuning, these UniFilter-induced MLLMs achieve stronger
performance on various benchmarks, highlighting the downstream benefits of
high-quality multimodal pre-training. We release the synthetic training data
used for training UniFilter, the UniFilter model checkpoints, and the
high-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to
the community for reproduction and further development.

### 47. Paper2Web: Let's Make Your Paper Alive!

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Yuhang Chen, Tianpeng Lv, Siyi Zhang, Yixiang Yin, Yao Wan, Philip S. Yu, Dongping Chen
- **URL**: <http://arxiv.org/abs/2510.15842v1>
- **Submitted**: 2025-10-17 17:35:58
- **Comment**: Under Review. Check https://github.com/YuhangChen1/Paper2All for the
  unified platform to streamline all academic presentation
- **Topic Keywords**: search
- **Reason**: This paper is not directly related to your core research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves text processing and generation, its focus on academic webpage generation and multimedia-rich homepages is not aligned with your primary areas of interest.

#### Abstract
> Academic project websites can more effectively disseminate research when they
clearly present core content and enable intuitive navigation and interaction.
However, current approaches such as direct Large Language Model (LLM)
generation, templates, or direct HTML conversion struggle to produce
layout-aware, interactive sites, and a comprehensive evaluation suite for this
task has been lacking. In this paper, we introduce Paper2Web, a benchmark
dataset and multi-dimensional evaluation framework for assessing academic
webpage generation. It incorporates rule-based metrics like Connectivity,
Completeness and human-verified LLM-as-a-Judge (covering interactivity,
aesthetics, and informativeness), and PaperQuiz, which measures paper-level
knowledge retention. We further present PWAgent, an autonomous pipeline that
converts scientific papers into interactive and multimedia-rich academic
homepages. The agent iteratively refines both content and layout through MCP
tools that enhance emphasis, balance, and presentation quality. Our experiments
show that PWAgent consistently outperforms end-to-end baselines like
template-based webpages and arXiv/alphaXiv versions by a large margin while
maintaining low cost, achieving the Pareto-front in academic webpage
generation.

### 48. Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Ed Li, Junyu Ren, Xintian Pan, Cat Yan, Chuanhao Li, Dirk Bergemann, Zhuoran Yang
- **URL**: <http://arxiv.org/abs/2510.15624v1>
- **Submitted**: 2025-10-17 13:13:32
- **Comment**: 37 pages, 5 figures. Code: https://github.com/ltjed/freephdlabor
- **Topic Keywords**: search
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or Natural Language Processing. While it involves AI and automation, its focus is on scientific discovery and research, which is not a central match to your research interests.

#### Abstract
> The automation of scientific discovery represents a critical milestone in
Artificial Intelligence (AI) research. However, existing agentic systems for
science suffer from two fundamental limitations: rigid, pre-programmed
workflows that cannot adapt to intermediate findings, and inadequate context
management that hinders long-horizon research. We present
\texttt{freephdlabor}, an open-source multiagent framework featuring
\textit{fully dynamic workflows} determined by real-time agent reasoning and a
\coloremph{\textit{modular architecture}} enabling seamless customization --
users can modify, add, or remove agents to address domain-specific
requirements. The framework provides comprehensive infrastructure including
\textit{automatic context compaction}, \textit{workspace-based communication}
to prevent information degradation, \textit{memory persistence} across
sessions, and \textit{non-blocking human intervention} mechanisms. These
features collectively transform automated research from isolated, single-run
attempts into \textit{continual research programs} that build systematically on
prior explorations and incorporate human feedback. By providing both the
architectural principles and practical implementation for building customizable
co-scientist systems, this work aims to facilitate broader adoption of
automated research across scientific domains, enabling practitioners to deploy
interactive multiagent systems that autonomously conduct end-to-end research --
from ideation through experimentation to publication-ready manuscripts.

### 49. CORE: Reducing UI Exposure in Mobile Agents via Collaboration Between Cloud and Local LLMs

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Gucongcong Fan, Chaoyue Niu, Chengfei Lyu, Fan Wu, Guihai Chen
- **URL**: <http://arxiv.org/abs/2510.15455v1>
- **Submitted**: 2025-10-17 09:11:05
- **Topic Keywords**: rank
- **Reason**: This paper focuses on reducing UI exposure in mobile agents via collaboration between cloud and local LLMs, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves NLP and data mining, the context is more aligned with AI/ML and mobile computing, making it less relevant to your core research interests.

#### Abstract
> Mobile agents rely on Large Language Models (LLMs) to plan and execute tasks
on smartphone user interfaces (UIs). While cloud-based LLMs achieve high task
accuracy, they require uploading the full UI state at every step, exposing
unnecessary and often irrelevant information. In contrast, local LLMs avoid UI
uploads but suffer from limited capacity, resulting in lower task success
rates. We propose $\textbf{CORE}$, a $\textbf{CO}$llaborative framework that
combines the strengths of cloud and local LLMs to $\textbf{R}$educe UI
$\textbf{E}$xposure, while maintaining task accuracy for mobile agents. CORE
comprises three key components: (1) $\textbf{Layout-aware block partitioning}$,
which groups semantically related UI elements based on the XML screen
hierarchy; (2) $\textbf{Co-planning}$, where local and cloud LLMs
collaboratively identify the current sub-task; and (3)
$\textbf{Co-decision-making}$, where the local LLM ranks relevant UI blocks,
and the cloud LLM selects specific UI elements within the top-ranked block.
CORE further introduces a multi-round accumulation mechanism to mitigate local
misjudgment or limited context. Experiments across diverse mobile apps and
tasks show that CORE reduces UI exposure by up to 55.6% while maintaining task
success rates slightly below cloud-only agents, effectively mitigating
unnecessary privacy exposure to the cloud. The code is available at
https://github.com/Entropy-Fighter/CORE.

### 50. From Ghazals to Sonnets: Decoding the Polysemous Expressions of Love Across Languages

- **LLM Score**: 0
- **Keyword Score**: 2
- **Authors**: Syed Mohammad Sualeh Ali
- **URL**: <http://arxiv.org/abs/2510.15569v1>
- **Submitted**: 2025-10-17 12:00:09
- **Topic Keywords**: ctr
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing, as it focuses on Urdu poetry and polysemous expressions of love, which does not align with your core themes.

#### Abstract
> This paper delves into the intricate world of Urdu poetry, exploring its
thematic depths through a lens of polysemy. By focusing on the nuanced
differences between three seemingly synonymous words (pyaar, muhabbat, and
ishq) we expose a spectrum of emotions and experiences unique to the Urdu
language. This study employs a polysemic case study approach, meticulously
examining how these words are interwoven within the rich tapestry of Urdu
poetry. By analyzing their usage and context, we uncover a hidden layer of
meaning, revealing subtle distinctions which lack direct equivalents in English
literature. Furthermore, we embark on a comparative analysis, generating word
embeddings for both Urdu and English terms related to love. This enables us to
quantify and visualize the semantic space occupied by these words, providing
valuable insights into the cultural and linguistic nuances of expressing love.
Through this multifaceted approach, our study sheds light on the captivating
complexities of Urdu poetry, offering a deeper understanding and appreciation
for its unique portrayal of love and its myriad expressions

---

