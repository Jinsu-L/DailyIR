# Daily Papers Report - 2025-07-21

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced Academic Search

- **LLM Score**: 7
- **Keyword Score**: 9
- **Authors**: Xiaofeng Shi, Yuduo Li, Qian Kou, Longbin Yu, Jinxin Xie, Hua Zhou
- **URL**: <http://arxiv.org/abs/2507.15245v1>
- **Submitted**: 2025-07-21 05:06:53
- **Topic Keywords**: query, relevance, retrieval, search
- **Reason**: The paper presents a novel framework for academic literature retrieval, leveraging large language models and multi-agent architecture. While it's not directly focused on query understanding, ranking models, or user behavior modeling, it explores advanced search technologies and relevance optimization, which aligns with your interests in Information Retrieval. However, the paper's primary focus is on scholarly retrieval, which is a specific domain, and may not be directly applicable to your e-commerce background.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Modular and extensible framework for academic paper retrieval
- **Aim**: To propose a comprehensive and dynamic academic search workflow that simulates how researchers follow references from one paper to another
- **Rationale**: To expand the scope of retrieval beyond direct query matches and capture the multi-faceted nature of real-world academic search
- **Ground**: Experimental results on AutoScholar and SPARBench demonstrate the robustness and generalization ability of SPAR across both synthetic and real-world academic search scenarios
- **Experiment**: Evaluation of SPAR's performance on various components, including RefChain, Query Evolution, and Reranking Strategy, and analysis of the impact of model and prompt choices on relevance assessment performance
- **Takeaway**: SPAR significantly outperforms compared methods, achieving high F1 scores on both benchmarks, and provides a comprehensive overview of related research in information retrieval, search engines, and natural language processing

#### Abstract
> Recent advances in large language models (LLMs) have opened new opportunities
for academic literature retrieval. However, existing systems often rely on
rigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR,
a multi-agent framework that incorporates RefChain-based query decomposition
and query evolution to enable more flexible and effective search. To facilitate
systematic evaluation, we also construct SPARBench, a challenging benchmark
with expert-annotated relevance labels. Experimental results demonstrate that
SPAR substantially outperforms strong baselines, achieving up to +56% F1 on
AutoScholar and +23% F1 on SPARBench over the best-performing baseline.
Together, SPAR and SPARBench provide a scalable, interpretable, and
high-performing foundation for advancing research in scholarly retrieval. Code
and data will be available at: https://github.com/xiaofengShi/SPAR

---

### 2. DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection

- **LLM Score**: 6
- **Keyword Score**: 11
- **Authors**: Jerry Wang, Fang Yu
- **URL**: <http://arxiv.org/abs/2507.15042v1>
- **Submitted**: 2025-07-20 16:48:20
- **Comment**: Accepted by KDD Workshop on Prompt Optimization 2025
- **Topic Keywords**: retriever, ranking, rag, retrieval, rank
- **Reason**: The paper explores adversarial attacks on Retrieval-Augmented Generation (RAG) systems, which is related to query understanding and ranking models in Information Retrieval. The focus on RAG-based question answering and the use of Differential Evolution to optimize adversarial prompts are novel and interesting. However, the paper's primary focus is on attacking RAG systems rather than improving them, which is not directly aligned with the user's interests in developing more effective search technologies.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Adversarial Attacks on Retrieval-Augmented Generation (RAG) Systems
- **Aim**: To develop a novel method for black-box adversarial attacks on RAG systems via prompt injection
- **Rationale**: RAG systems are vulnerable to adversarial input manipulations, and existing methods are limited in their ability to generate effective and evasive attacks
- **Ground**: The proposed method, DeRAG, uses Differential Evolution (DE) to optimize adversarial prompt suffixes, treating the RAG pipeline as a black box
- **Experiment**: DeRAG is evaluated on the BEIR QA datasets and compared to other methods, achieving competitive or higher success rates with minimal perturbations to the input query
- **Takeaway**: DeRAG demonstrates the vulnerability of RAG systems to adversarial input manipulations and provides a novel method for generating adversarial prompts that can evade detection and achieve high success rates

#### Abstract
> Adversarial prompt attacks can significantly alter the reliability of
Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce
incorrect outputs. In this paper, we present a novel method that applies
Differential Evolution (DE) to optimize adversarial prompt suffixes for
RAG-based question answering. Our approach is gradient-free, treating the RAG
pipeline as a black box and evolving a population of candidate suffixes to
maximize the retrieval rank of a targeted incorrect document to be closer to
real world scenarios. We conducted experiments on the BEIR QA datasets to
evaluate attack success at certain retrieval rank thresholds under multiple
retrieving applications. Our results demonstrate that DE-based prompt
optimization attains competitive (and in some cases higher) success rates
compared to GGPP to dense retrievers and PRADA to sparse retrievers, while
using only a small number of tokens (<=5 tokens) in the adversarial suffix.
Furthermore, we introduce a readability-aware suffix construction strategy,
validated by a statistically significant reduction in MLM negative
log-likelihood with Welch's t-test. Through evaluations with a BERT-based
adversarial suffix detector, we show that DE-generated suffixes evade
detection, yielding near-chance detection accuracy.

---

### 3. Just Ask for Music (JAM): Multimodal and Personalized Natural Language Music Recommendation

- **LLM Score**: 6
- **Keyword Score**: 9
- **Authors**: Alessandro B. Melchiorre, Elena V. Epure, Shahed Masoudian, Gustavo Escobedo, Anna Hausberger, Manuel Moussallam, Markus Schedl
- **URL**: <http://arxiv.org/abs/2507.15826v1>
- **Submitted**: 2025-07-21 17:36:03
- **Topic Keywords**: query, queries, retrieval, recommend
- **Reason**: The paper presents a natural language music recommendation system, which is related to information retrieval and search technologies. However, the focus is on music recommendation rather than general query understanding and ranking models, which are core areas of interest. The paper's use of multimodal item features and knowledge graph embedding methods is somewhat relevant to my background in NLP and data mining.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Natural Language Music Recommendation
- **Aim**: Introduce a novel framework for natural language music recommendation that addresses limitations of existing approaches
- **Rationale**: Model user-query-item interactions as vector translations in a shared latent space, inspired by knowledge graph embedding methods
- **Ground**: JAMSessions dataset, a large-scale dataset that captures both conversational intent and user long-term preference signals
- **Experiment**: Evaluate JAM models using Recall and NDCG metrics on the JAMSessions dataset, comparing to several baselines
- **Takeaway**: The JAM framework demonstrates potential in navigating multimodal item spaces and tailoring recommendations to diverse user intents

#### Abstract
> Natural language interfaces offer a compelling approach for music
recommendation, enabling users to express complex preferences conversationally.
While Large Language Models (LLMs) show promise in this direction, their
scalability in recommender systems is limited by high costs and latency.
Retrieval-based approaches using smaller language models mitigate these issues
but often rely on single-modal item representations, overlook long-term user
preferences, and require full model retraining, posing challenges for
real-world deployment. In this paper, we present JAM (Just Ask for Music), a
lightweight and intuitive framework for natural language music recommendation.
JAM models user-query-item interactions as vector translations in a shared
latent space, inspired by knowledge graph embedding methods like TransE. To
capture the complexity of music and user intent, JAM aggregates multimodal item
features via cross-attention and sparse mixture-of-experts. We also introduce
JAMSessions, a new dataset of over 100k user-query-item triples with anonymized
user/item embeddings, uniquely combining conversational queries and user
long-term preferences. Our results show that JAM provides accurate
recommendations, produces intuitive representations suitable for practical use
cases, and can be easily integrated with existing music recommendation stacks.

---

### 4. Click A, Buy B: Rethinking Conversion Attribution in E- Commerce Recommendations

- **LLM Score**: 6
- **Keyword Score**: 7
- **Authors**: Xiangyu Zeng, Amit Jaspal, Bin Liu, Goutham Panneeru, Kevin Huang, Nicolas Bievre, Mohit Jaggi, Prathap Maniraju, Ankur Jain
- **URL**: <http://arxiv.org/abs/2507.15113v1>
- **Submitted**: 2025-07-20 20:25:20
- **Topic Keywords**: click, conversion rate, recommend, commerce, e-commerce
- **Reason**: The paper explores a specific problem in e-commerce recommendations, focusing on conversion attribution and rethinking the traditional click-purchase assumption. While it touches on user behavior modeling and click models, the primary focus is on recommender systems rather than information retrieval. The paper's relevance to the user's interests lies in its application to e-commerce, but the topics and techniques used are not directly related to the user's core research themes.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Addressing the 'Click A, Buy B' phenomenon in e-commerce recommendation systems
- **Aim**: To develop a framework that directly models and denoises cross-item conversions in large-scale e-commerce recommendations
- **Rationale**: Training recommendation models on raw click-conversion pairs can lead to biased learning and suboptimal conversion rates
- **Ground**: Multi-task learning problem with separate heads for Click A ‚Üí Buy A (CABA) and Click A ‚Üí Buy B (CABB), taxonomy-aware collaborative filtering weighting scheme
- **Experiment**: Offline evaluation using e-commerce sessions and online A/B test on live traffic, achieving a 13.9% reduction in normalized entropy and a +0.25% gain in the primary business metric
- **Takeaway**: The proposed approach, CABA-CABB decomposition with taxonomy-aware similarity weighting, is the first end-to-end framework that directly models and denoises cross-item conversions in large-scale e-commerce recommendations

#### Abstract
> User journeys in e-commerce routinely violate the one-to-one assumption that
a clicked item on an advertising platform is the same item later purchased on
the merchant's website/app. For a significant number of converting sessions on
our platform, users click product A but buy product B -- the Click A, Buy B
(CABB) phenomenon. Training recommendation models on raw click-conversion pairs
therefore rewards items that merely correlate with purchases, leading to biased
learning and sub-optimal conversion rates. We reframe conversion prediction as
a multi-task problem with separate heads for Click A Buy A (CABA) and Click A
Buy B (CABB). To isolate informative CABB conversions from unrelated CABB
conversions, we introduce a taxonomy-aware collaborative filtering weighting
scheme where each product is first mapped to a leaf node in a product taxonomy,
and a category-to-category similarity matrix is learned from large-scale
co-engagement logs. This weighting amplifies pairs that reflect genuine
substitutable or complementary relations while down-weighting coincidental
cross-category purchases. Offline evaluation on e-commerce sessions reduces
normalized entropy by 13.9% versus a last-click attribution baseline. An online
A/B test on live traffic shows +0.25% gains in the primary business metric.

---

### 5. Interaction as Intelligence: Deep Research With Human-AI Partnership

- **LLM Score**: 6
- **Keyword Score**: 6
- **Authors**: Lyumanshan Ye, Xiaojie Cai, Xinkai Wang, Junfei Wang, Xiangkun Hu, Jiadi Su, Yang Nan, Sihan Wang, Bohan Zhang, Xiaoze Fan, Jinbin Luo, Yuxiang Zheng, Tianze Xu, Dayuan Fu, Yunze Wu, Pengrui Lu, Zengzhi Wang, Yiwei Qin, Zhen Huang, Yan Ma, Zhulin Hu, Haoyang Zou, Tiantian Mi, Yixin Ye, Ethan Chern, Pengfei Liu
- **URL**: <http://arxiv.org/abs/2507.15759v1>
- **Submitted**: 2025-07-21 16:15:18
- **Comment**: 30 pages, 10 figures
- **Topic Keywords**: queries, user behavior, search
- **Reason**: The paper explores the concept of 'Interaction as Intelligence' in deep research tasks, proposing a new paradigm for human-AI collaboration. While it touches on some aspects of information retrieval, such as query refinement and fine-grained dialogue, the primary focus is on the human-AI partnership and the cognitive oversight model. The paper's relevance to my research interests is somewhat related, but not a central match, as it does not specifically address query understanding, ranking models, or user behavior modeling.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: Human-AI Interaction in Deep Research Tasks
- **Aim**: To develop an AI system that engages in extended thinking processes with human oversight
- **Rationale**: Interaction is a fundamental dimension of intelligence, not just an interface for accessing AI capabilities
- **Ground**: Traditional 'input-wait-output' paradigm is limited, and AI systems should allow for strategic human intervention
- **Experiment**: User evaluations of Deep Cognition system, which features transparent and controllable interaction, fine-grained bidirectional dialogue, and a shared cognitive context
- **Takeaway**: Deep Cognition outperforms strongest baseline across six key metrics and shows significant improvements on challenging research problems

#### Abstract
> This paper introduces "Interaction as Intelligence" research series,
presenting a reconceptualization of human-AI relationships in deep research
tasks. Traditional approaches treat interaction merely as an interface for
accessing AI capabilities-a conduit between human intent and machine output. We
propose that interaction itself constitutes a fundamental dimension of
intelligence. As AI systems engage in extended thinking processes for research
tasks, meaningful interaction transitions from an optional enhancement to an
essential component of effective intelligence. Current deep research systems
adopt an "input-wait-output" paradigm where users initiate queries and receive
results after black-box processing. This approach leads to error cascade
effects, inflexible research boundaries that prevent question refinement during
investigation, and missed opportunities for expertise integration. To address
these limitations, we introduce Deep Cognition, a system that transforms the
human role from giving instructions to cognitive oversight-a mode of engagement
where humans guide AI thinking processes through strategic intervention at
critical junctures. Deep cognition implements three key innovations:
(1)Transparent, controllable, and interruptible interaction that reveals AI
reasoning and enables intervention at any point; (2)Fine-grained bidirectional
dialogue; and (3)Shared cognitive context where the system observes and adapts
to user behaviors without explicit instruction. User evaluation demonstrates
that this cognitive oversight paradigm outperforms the strongest baseline
across six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%),
Real-Time Intervention(+18.5%), Ease of Collaboration(+27.7%),
Results-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on
challenging research problems show 31.8% to 50.0% points of improvements over
deep research systems.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding

- **LLM Score**: 6
- **Keyword Score**: 4
- **Authors**: Yifei Wang
- **URL**: <http://arxiv.org/abs/2507.14849v1>
- **Submitted**: 2025-07-20 07:43:16
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper explores the intersection of natural language processing and information retrieval, specifically focusing on long-context understanding and reasoning. While it doesn't directly address query understanding, ranking models, or user behavior modeling, it does touch on the importance of contextual information in retrieval-augmented generation systems, which is relevant to the broader field of information retrieval. However, the paper's primary focus on long-context understanding and reasoning distillation doesn't directly align with the user's specific research interests.

#### Abstract
> Reasoning distillation has emerged as an effective approach to enhance the
reasoning capabilities of smaller language models. However, the impact of
large-scale reasoning distillation on other critical abilities, particularly
in-context retrieval and reasoning, remains unexplored. This gap in
understanding is particularly significant given the increasing importance of
Retrieval-Augmented Generation (RAG) systems, where efficient acquisition and
utilization of contextual information are paramount for generating reliable
responses. Motivated by the need to understand how the extended long-CoT
process influences long-context comprehension, we conduct a comprehensive
investigation using a series of open-source models distilled from Deepseek-R1,
renowned for its exceptional reasoning capabilities. Our study focuses on
evaluating these models' performance in extracting and integrating relevant
information from extended contexts through multi-document question and
answering tasks. Through rigorous experimentation, we demonstrate that
distilled reasoning patterns significantly improve long-context understanding.
Our analysis reveals that distillation fosters greater long-context awareness
by promoting more detailed and explicit reasoning processes during context
analysis and information parsing. This advancement effectively mitigates the
persistent "lost in the middle" issue that has hindered long-context models.

### 7. Docopilot: Improving Multimodal Models for Document-Level Understanding

- **LLM Score**: 6
- **Keyword Score**: 4
- **Authors**: Yuchen Duan, Zhe Chen, Yusong Hu, Weiyun Wang, Shenglong Ye, Botian Shi, Lewei Lu, Qibin Hou, Tong Lu, Hongsheng Li, Jifeng Dai, Wenhai Wang
- **URL**: <http://arxiv.org/abs/2507.14675v1>
- **Submitted**: 2025-07-19 16:03:34
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on multimodal document comprehension and presents a new dataset and model for document-level understanding. While it touches on information retrieval and multimodal processing, the primary focus is on document comprehension rather than query understanding or ranking models, which are key areas of interest for you. The paper's relevance is somewhat related to your interests in IR and NLP, but it does not directly address your core research themes.

#### Abstract
> Despite significant progress in multimodal large language models (MLLMs),
their performance on complex, multi-page document comprehension remains
inadequate, largely due to the lack of high-quality, document-level datasets.
While current retrieval-augmented generation (RAG) methods offer partial
solutions, they suffer from issues, such as fragmented retrieval contexts,
multi-stage error accumulation, and extra time costs of retrieval. In this
work, we present a high-quality document-level dataset, Doc-750K, designed to
support in-depth understanding of multimodal documents. This dataset includes
diverse document structures, extensive cross-page dependencies, and real
question-answer pairs derived from the original documents. Building on the
dataset, we develop a native multimodal model, Docopilot, which can accurately
handle document-level dependencies without relying on RAG. Experiments
demonstrate that Docopilot achieves superior coherence, accuracy, and
efficiency in document understanding tasks and multi-turn interactions, setting
a new baseline for document-level multimodal understanding. Data, code, and
models are released at https://github.com/OpenGVLab/Docopilot

### 8. FullRecall: A Semantic Search-Based Ranking Approach for Maximizing Recall in Patent Retrieval

- **LLM Score**: 4
- **Keyword Score**: 18
- **Authors**: Amna Ali, Liyanage C. De Silva, Pg Emeroylariffion Abas
- **URL**: <http://arxiv.org/abs/2507.14946v1>
- **Submitted**: 2025-07-20 12:52:58
- **Topic Keywords**: semantic search, query, ranking, relevance, rag, retrieval, rank, search
- **Reason**: The paper's focus on patent retrieval and its emphasis on recall are somewhat related to my interests in Information Retrieval and Search technologies. However, the specific domain of patent retrieval and the use of IPC-guided knowledge are not directly aligned with my research themes. The paper's approach, while innovative, does not seem to involve query understanding, ranking models, or user behavior modeling, which are key areas of interest for me.

#### Abstract
> Patent examiners and inventors face significant pressure to verify the
originality and non-obviousness of inventions, and the intricate nature of
patent data intensifies the challenges of patent retrieval. Therefore, there is
a pressing need to devise cutting-edge retrieval strategies that can reliably
achieve the desired recall. This study introduces FullRecall, a novel patent
retrieval approach that effectively manages the complexity of patent data while
maintaining the reliability of relevance matching and maximising recall. It
leverages IPC-guided knowledge to generate informative phrases, which are
processed to extract key information in the form of noun phrases characterising
the query patent under observation. From these, the top k keyphrases are
selected to construct a query for retrieving a focused subset of the dataset.
This initial retrieval step achieves complete recall, successfully capturing
all relevant documents. To further refine the results, a ranking scheme is
applied to the retrieved subset, reducing its size while maintaining 100%
recall. This multi-phase process demonstrates an effective strategy for
balancing precision and recall in patent retrieval tasks. Comprehensive
experiments were conducted, and the results were compared with baseline
studies, namely HRR2 [1] and ReQ-ReC [2]. The proposed approach yielded
superior results, achieving 100% recall in all five test cases. However,
HRR2[1] recall values across the five test cases were 10%, 25%, 33.3%, 0%, and
14.29%, while ReQ-ReC [2] showed 50% for the first test case, 25% for the
second test case, and 0% for the third, fourth, and fifth test cases. The 100%
recall ensures that no relevant prior art is overlooked, thereby strengthening
the patent pre-filing and examination processes, hence reducing potential legal
risks.

### 9. GREAT: Guiding Query Generation with a Trie for Recommending Related Search about Video at Kuaishou

- **LLM Score**: 4
- **Keyword Score**: 15
- **Authors**: Ninglu Shao, Jinshan Wang, Chenxu Wang, Qingbiao Li, Xiaoxue Zang, Han Li
- **URL**: <http://arxiv.org/abs/2507.15267v1>
- **Submitted**: 2025-07-21 06:10:30
- **Topic Keywords**: query, queries, relevance, click, click-through rate, recommend, search
- **Reason**: The paper focuses on query recommendation in video-related search, which is somewhat related to information retrieval and search technologies. However, the specific domain and approach (trie-based query generation) are not directly aligned with the user's core research themes, such as query understanding, ranking models, and user behavior modeling.

#### Abstract
> Currently, short video platforms have become the primary place for
individuals to share experiences and obtain information. To better meet users'
needs for acquiring information while browsing short videos, some apps have
introduced a search entry at the bottom of videos, accompanied with recommended
relevant queries. This scenario is known as query recommendation in
video-related search, where core task is item-to-query (I2Q) recommendation. As
this scenario has only emerged in recent years, there is a notable scarcity of
academic research and publicly available datasets in this domain. To address
this gap, we systematically examine the challenges associated with this
scenario for the first time. Subsequently, we release a large-scale dataset
derived from real-world data pertaining to the query recommendation in
video-\textit{\textbf{r}}elated \textit{\textbf{s}}earch on the
\textit{\textbf{Kuai}}shou app (\textbf{KuaiRS}). Presently, existing methods
rely on embeddings to calculate similarity for matching short videos with
queries, lacking deep interaction between the semantic content and the query.
In this paper, we introduce a novel LLM-based framework named \textbf{GREAT},
which \textit{\textbf{g}}uides que\textit{\textbf{r}}y
g\textit{\textbf{e}}ner\textit{\textbf{a}}tion with a \textit{\textbf{t}}rie to
address I2Q recommendation in related search. Specifically, we initially gather
high-quality queries with high exposure and click-through rate to construct a
query-based trie. During training, we enhance the LLM's capability to generate
high-quality queries using the query-based trie. In the inference phase, the
query-based trie serves as a guide for the token generation. Finally, we
further refine the relevance and literal quality between items and queries via
a post-processing module. Extensive offline and online experiments demonstrate
the effectiveness of our proposed method.

### 10. From Queries to Criteria: Understanding How Astronomers Evaluate LLMs

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Alina Hyk, Kiera McCormick, Mian Zhong, Ioana CiucƒÉ, Sanjib Sharma, John F Wu, J. E. G. Peek, Kartheik G. Iyer, Ziang Xiao, Anjalie Field
- **URL**: <http://arxiv.org/abs/2507.15715v1>
- **Submitted**: 2025-07-21 15:26:58
- **Comment**: Accepted to the Conference on Language Modeling 2025 (COLM), 22
  pages, 6 figures
- **Topic Keywords**: queries, rag, retrieval, recommend, search
- **Reason**: The paper is somewhat related to my research interests in Information Retrieval and Search technologies, as it explores the evaluation of Large Language Models (LLMs) in a specific domain (astronomy). However, the focus on LLMs and their evaluation in a scientific research context is not directly aligned with my primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> There is growing interest in leveraging LLMs to aid in astronomy and other
scientific research, but benchmarks for LLM evaluation in general have not kept
pace with the increasingly diverse ways that real people evaluate and use these
models. In this study, we seek to improve evaluation procedures by building an
understanding of how users evaluate LLMs. We focus on a particular use case: an
LLM-powered retrieval-augmented generation bot for engaging with astronomical
literature, which we deployed via Slack. Our inductive coding of 368 queries to
the bot over four weeks and our follow-up interviews with 11 astronomers reveal
how humans evaluated this system, including the types of questions asked and
the criteria for judging responses. We synthesize our findings into concrete
recommendations for building better benchmarks, which we then employ in
constructing a sample benchmark for evaluating LLMs for astronomy. Overall, our
work offers ways to improve LLM evaluation and ultimately usability,
particularly for use in scientific research.

### 11. RankMixer: Scaling Up Ranking Models in Industrial Recommenders

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Jie Zhu, Zhifang Fan, Xiaoxie Zhu, Yuchen Jiang, Hangyu Wang, Xintian Han, Haoran Ding, Xinmin Wang, Wenlin Zhao, Zhen Gong, Huizhi Yang, Zheng Chai, Zhe Chen, Yuchao Zheng, Qiwei Chen, Feng Zhang, Xun Zhou, Peng Xu, Xiao Yang, Di Wu, Zuotao Liu
- **URL**: <http://arxiv.org/abs/2507.15551v1>
- **Submitted**: 2025-07-21 12:28:55
- **Topic Keywords**: ranking, recommend, rank, search, acl
- **Reason**: The paper focuses on scaling up ranking models in industrial recommenders, which is somewhat related to my interests in information retrieval and search technologies. However, the emphasis on recommender systems and the lack of deep semantic understanding and real-time relevance optimization make it less relevant to my primary research focus.

#### Abstract
> Recent progress on large language models (LLMs) has spurred interest in
scaling up recommendation systems, yet two practical obstacles remain. First,
training and serving cost on industrial Recommenders must respect strict
latency bounds and high QPS demands. Second, most human-designed
feature-crossing modules in ranking models were inherited from the CPU era and
fail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and
poor scalability. We introduce RankMixer, a hardware-aware model design
tailored towards a unified and scalable feature-interaction architecture.
RankMixer retains the transformer's high parallelism while replacing quadratic
self-attention with multi-head token mixing module for higher efficiency.
Besides, RankMixer maintains both the modeling for distinct feature subspaces
and cross-feature-space interactions with Per-token FFNs. We further extend it
to one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic
routing strategy is adapted to address the inadequacy and imbalance of experts
training. Experiments show RankMixer's superior scaling abilities on a
trillion-scale production dataset. By replacing previously diverse handcrafted
low-MFU modules with RankMixer, we boost the model MFU from 4.5% to 45%, and
scale our ranking model parameters by 100x while maintaining roughly the same
inference latency. We verify RankMixer's universality with online A/B tests
across three core application scenarios (Recommendation, Advertisement and
Search). Finally, we launch 1B Dense-Parameters RankMixer for full traffic
serving without increasing the serving cost, which improves user active days by
0.2% and total in-app usage duration by 0.5%.

### 12. U-MARVEL: Unveiling Key Factors for Universal Multimodal Retrieval via Embedding Learning with MLLMs

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Xiaojie Li, Chu Li, Shi-Zhe Chen, Xi Chen
- **URL**: <http://arxiv.org/abs/2507.14902v1>
- **Submitted**: 2025-07-20 10:27:34
- **Comment**: Technical Report (in progress)
- **Topic Keywords**: queries, retrieval, rank
- **Reason**: The paper focuses on universal multimodal retrieval, which is not directly related to the user's primary research interests in Information Retrieval and Search technologies. While the paper mentions MLLMs, which are relevant to the user's background in NLP, the specific application and techniques used are not directly applicable to the user's areas of interest.

#### Abstract
> Universal multimodal retrieval (UMR), which aims to address complex retrieval
tasks where both queries and candidates span diverse modalities, has been
significantly advanced by the emergence of MLLMs. While state-of-the-art
MLLM-based methods in the literature predominantly adopt contrastive learning
principles, they often differ in their specific training recipes. Despite their
success, the mechanisms underlying their retrieval capabilities remain largely
unexplored, potentially resulting in suboptimal performance and limited
generalization ability. To address these issues, we present a comprehensive
study aimed at uncovering the key factors that drive effective embedding
learning for UMR using MLLMs. We begin by implementing a general MLLM-based
embedding learning pipeline, and systematically analyze the primary
contributors to high-performing universal retrieval systems. Based on this, we
explore various aspects of the details in embedding generation and training
strategies, including progressive transition, hard negative mining and
re-ranker distillation. Notably, our findings reveal that often-overlooked
factors can have a substantial impact on model performance. Building on these
discoveries, we introduce a unified framework termed U-MARVEL
(\textbf{U}niversal \textbf{M}ultimod\textbf{A}l \textbf{R}etrie\textbf{V}al
via \textbf{E}mbedding \textbf{L}earning), which outperforms state-of-the-art
competitors on the M-BEIR benchmark by a large margin in supervised settings,
and also exihibits strong zero-shot performance on several tasks such as
composed image retrieval and text-to-video retrieval. These results underscore
the generalization potential of our framework across various embedding-based
retrieval tasks. Code is available at https://github.com/chaxjli/U-MARVEL

### 13. Optimizing Legal Document Retrieval in Vietnamese with Semi-Hard Negative Mining

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Van-Hoang Le, Duc-Vu Nguyen, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen
- **URL**: <http://arxiv.org/abs/2507.14619v1>
- **Submitted**: 2025-07-19 13:30:14
- **Comment**: Accepted at ICCCI 2025
- **Topic Keywords**: ranking, retrieval, rank
- **Reason**: The paper focuses on legal document retrieval in Vietnamese, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper employs some relevant techniques like fine-tuned Bi-Encoder and Cross-Encoder, the domain and specific challenges are not aligned with the user's background and interests.

#### Abstract
> Large Language Models (LLMs) face significant challenges in specialized
domains like law, where precision and domain-specific knowledge are critical.
This paper presents a streamlined two-stage framework consisting of Retrieval
and Re-ranking to enhance legal document retrieval efficiency and accuracy. Our
approach employs a fine-tuned Bi-Encoder for rapid candidate retrieval,
followed by a Cross-Encoder for precise re-ranking, both optimized through
strategic negative example mining. Key innovations include the introduction of
the Exist@m metric to evaluate retrieval effectiveness and the use of semi-hard
negatives to mitigate training bias, which significantly improved re-ranking
performance. Evaluated on the SoICT Hackathon 2024 for Legal Document
Retrieval, our team, 4Huiter, achieved a top-three position. While
top-performing teams employed ensemble models and iterative self-training on
large bge-m3 architectures, our lightweight, single-pass approach offered a
competitive alternative with far fewer parameters. The framework demonstrates
that optimized data processing, tailored loss functions, and balanced negative
sampling are pivotal for building robust retrieval-augmented systems in legal
contexts.

### 14. Text-to-SQL for Enterprise Data Analytics

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Albert Chen, Manas Bundele, Gaurav Ahlawat, Patrick Stetz, Zhitao Wang, Qiang Fei, Donghoon Jung, Audrey Chu, Bharadwaj Jayaraman, Ayushi Panth, Yatin Arora, Sourav Jain, Renjith Varma, Alexey Ilin, Iuliia Melnychuk, Chelsea Chueh, Joyan Sil, Xiaofeng Wang
- **URL**: <http://arxiv.org/abs/2507.14372v1>
- **Submitted**: 2025-07-18 21:39:17
- **Comment**: 11 pages, 8 figures, Workshop on Agentic AI for Enterprise at KDD '25
- **Topic Keywords**: query, rag, rank
- **Reason**: The paper focuses on Text-to-SQL for enterprise data analytics, which is not directly related to my primary research interests in Information Retrieval and Search technologies. While it mentions query understanding and ranking models, the context is different and not as relevant to my work. The paper's emphasis on building a chatbot and knowledge graph is also not a central match for my research themes.

#### Abstract
> The introduction of large language models has brought rapid progress on
Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise
solution. In this paper, we present insights from building an internal chatbot
that enables LinkedIn's product managers, engineers, and operations teams to
self-serve data insights from a large, dynamic data lake. Our approach features
three components. First, we construct a knowledge graph that captures
up-to-date semantics by indexing database metadata, historical query logs,
wikis, and code. We apply clustering to identify relevant tables for each team
or product area. Second, we build a Text-to-SQL agent that retrieves and ranks
context from the knowledge graph, writes a query, and automatically corrects
hallucinations and syntax errors. Third, we build an interactive chatbot that
supports various user intents, from data discovery to query writing to
debugging, and displays responses in rich UI elements to encourage follow-up
chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of
its responses are correct or close to correct on an internal benchmark set.
Through ablation studies, we identify the most important knowledge graph and
modeling components, offering a practical path for developing enterprise
Text-to-SQL solutions.

### 15. Hierarchical Graph Information Bottleneck for Multi-Behavior Recommendation

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Hengyu Zhang, Chunxu Shen, Xiangguo Sun, Jie Tan, Yanchao Tan, Yu Rong, Hong Cheng, Lingling Yi
- **URL**: <http://arxiv.org/abs/2507.15395v1>
- **Submitted**: 2025-07-21 08:53:49
- **Comment**: Accepted by RecSys2025
- **Topic Keywords**: rag, user behavior, recommend
- **Reason**: The paper focuses on multi-behavior recommendation, which is somewhat related to my interests in information retrieval and search technologies. However, the emphasis on recommender systems and behavioral interactions is not directly aligned with my primary focus on query understanding, ranking models, and user behavior modeling. The paper's use of graph-based methods and information bottleneck principles is an interesting aspect, but it does not seem to have a direct connection to my research areas.

#### Abstract
> In real-world recommendation scenarios, users typically engage with platforms
through multiple types of behavioral interactions. Multi-behavior
recommendation algorithms aim to leverage various auxiliary user behaviors to
enhance prediction for target behaviors of primary interest (e.g., buy),
thereby overcoming performance limitations caused by data sparsity in target
behavior records. Current state-of-the-art approaches typically employ
hierarchical design following either cascading (e.g.,
view$\rightarrow$cart$\rightarrow$buy) or parallel
(unified$\rightarrow$behavior$\rightarrow$specific components) paradigms, to
capture behavioral relationships. However, these methods still face two
critical challenges: (1) severe distribution disparities across behaviors, and
(2) negative transfer effects caused by noise in auxiliary behaviors. In this
paper, we propose a novel model-agnostic Hierarchical Graph Information
Bottleneck (HGIB) framework for multi-behavior recommendation to effectively
address these challenges. Following information bottleneck principles, our
framework optimizes the learning of compact yet sufficient representations that
preserve essential information for target behavior prediction while eliminating
task-irrelevant redundancies. To further mitigate interaction noise, we
introduce a Graph Refinement Encoder (GRE) that dynamically prunes redundant
edges through learnable edge dropout mechanisms. We conduct comprehensive
experiments on three real-world public datasets, which demonstrate the superior
effectiveness of our framework. Beyond these widely used datasets in the
academic community, we further expand our evaluation on several real industrial
scenarios and conduct an online A/B testing, showing again a significant
improvement in multi-behavior recommendations. The source code of our proposed
HGIB is available at https://github.com/zhy99426/HGIB.

### 16. User Invariant Preference Learning for Multi-Behavior Recommendation

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Mingshi Yan, Zhiyong Cheng, Fan Liu, Yingda Lyu, Yahong Han
- **URL**: <http://arxiv.org/abs/2507.14925v1>
- **Submitted**: 2025-07-20 11:47:36
- **Topic Keywords**: rag, click, recommend
- **Reason**: The paper focuses on multi-behavior recommendation, which is somewhat related to my interests in information retrieval and search technologies. However, the emphasis on recommender systems and the lack of deep semantic understanding and real-time relevance optimization in the paper make it less relevant to my core research themes.

#### Abstract
> In multi-behavior recommendation scenarios, analyzing users' diverse
behaviors, such as click, purchase, and rating, enables a more comprehensive
understanding of their interests, facilitating personalized and accurate
recommendations. A fundamental assumption of multi-behavior recommendation
methods is the existence of shared user preferences across behaviors,
representing users' intrinsic interests. Based on this assumption, existing
approaches aim to integrate information from various behaviors to enrich user
representations. However, they often overlook the presence of both
commonalities and individualities in users' multi-behavior preferences. These
individualities reflect distinct aspects of preferences captured by different
behaviors, where certain auxiliary behaviors may introduce noise, hindering the
prediction of the target behavior. To address this issue, we propose a user
invariant preference learning for multi-behavior recommendation (UIPL for
short), aiming to capture users' intrinsic interests (referred to as invariant
preferences) from multi-behavior interactions to mitigate the introduction of
noise. Specifically, UIPL leverages the paradigm of invariant risk minimization
to learn invariant preferences. To implement this, we employ a variational
autoencoder (VAE) to extract users' invariant preferences, replacing the
standard reconstruction loss with an invariant risk minimization constraint.
Additionally, we construct distinct environments by combining multi-behavior
data to enhance robustness in learning these preferences. Finally, the learned
invariant preferences are used to provide recommendations for the target
behavior. Extensive experiments on four real-world datasets demonstrate that
UIPL significantly outperforms current state-of-the-art methods.

### 17. Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Akriti Jain, Pritika Ramu, Aparna Garimella, Apoorv Saxena
- **URL**: <http://arxiv.org/abs/2507.14819v1>
- **Submitted**: 2025-07-20 04:34:59
- **Topic Keywords**: query, retrieval
- **Reason**: The paper explores a novel task of intent-driven chart generation from documents, which is related to information retrieval and search technologies. However, the focus is on natural language processing and data visualization, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user.

#### Abstract
> Large Language Models (LLMs) have demonstrated strong capabilities in
transforming text descriptions or tables to data visualizations via
instruction-tuning methods. However, it is not straightforward to apply these
methods directly for a more real-world use case of visualizing data from long
documents based on user-given intents, as opposed to the user pre-selecting the
relevant content manually. We introduce the task of intent-based chart
generation from documents: given a user-specified intent and document(s), the
goal is to generate a chart adhering to the intent and grounded on the
document(s) in a zero-shot setting. We propose an unsupervised, two-staged
framework in which an LLM first extracts relevant information from the
document(s) by decomposing the intent and iteratively validates and refines
this data. Next, a heuristic-guided module selects an appropriate chart type
before final code generation. To assess the data accuracy of the generated
charts, we propose an attribution-based metric that uses a structured textual
representation of charts, instead of relying on visual decoding metrics that
often fail to capture the chart data effectively. To validate our approach, we
curate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from
two domains, finance and scientific, in contrast to the existing datasets that
are largely limited to parallel text descriptions/ tables and their
corresponding charts. We compare our approach with baselines using single-shot
chart generation using LLMs and query-based retrieval methods; our method
outperforms by upto $9$ points and $17$ points in terms of chart data accuracy
and chart type respectively over the best baselines.

### 18. GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Luyi Ma, Wanjia Zhang, Kai Zhao, Abhishek Kulkarni, Lalitesh Morishetti, Anjana Ganesh, Ashish Ranjan, Aashika Padmanabhan, Jianpeng Xu, Jason Cho, Praveen Kanumala, Kaushiki Nag, Sumit Dutta, Kamiya Motwani, Malay Patel, Evren Korpeoglu, Sushant Kumar, Kannan Achan
- **URL**: <http://arxiv.org/abs/2507.14758v1>
- **Submitted**: 2025-07-19 21:23:23
- **Comment**: 10 pages, 5 figures, The ACM Conference on Recommender Systems
  (RecSys) 2025
- **Topic Keywords**: rag, ctr, recommend
- **Reason**: The paper proposes a generative framework for multi-behavior sequential recommendation, which is not directly related to information retrieval or search technologies. While it uses tokenization and attention mechanisms, the focus is on recommender systems rather than query understanding, ranking models, or user behavior modeling. The paper's relevance to the user's interests is limited, but it may be of interest to those with a broader background in NLP and data mining.

#### Abstract
> Generative models have recently demonstrated strong potential in
multi-behavior recommendation systems, leveraging the expressive power of
transformers and tokenization to generate personalized item sequences. However,
their adoption is hindered by (1) the lack of explicit information for token
reasoning, (2) high computational costs due to quadratic attention complexity
and dense sequence representations after tokenization, and (3) limited
multi-scale modeling over user history. In this work, we propose GRACE
(Generative Recommendation via journey-aware sparse Attention on
Chain-of-thought tokEnization), a novel generative framework for multi-behavior
sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)
tokenization method that encodes user-item interactions with explicit
attributes from product knowledge graphs (e.g., category, brand, price) over
semantic tokenization, enabling interpretable and behavior-aligned generation.
To address the inefficiency of standard attention, we design a Journey-Aware
Sparse Attention (JSA) mechanism, which selectively attends to compressed,
intra-, inter-, and current-context segments in the tokenized sequence.
Experiments on two real-world datasets show that GRACE significantly
outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and
+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home
domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces
attention computation by up to 48% with long sequences.

### 19. GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Fei Tang, Zhangxuan Gu, Zhengxi Lu, Xuyang Liu, Shuheng Shen, Changhua Meng, Wen Wang, Wenqi Zhang, Yongliang Shen, Weiming Lu, Jun Xiao, Yueting Zhuang
- **URL**: <http://arxiv.org/abs/2507.15846v1>
- **Submitted**: 2025-07-21 17:53:42
- **Topic Keywords**: rag, click
- **Reason**: The paper focuses on GUI grounding, which is not directly related to information retrieval or search technologies. While it uses reinforcement learning, the application is in GUI interaction tasks, which is not a core area of interest. The paper does not mention query understanding, ranking models, or user behavior modeling, which are key aspects of the user's research interests.

#### Abstract
> Graphical User Interface (GUI) grounding maps natural language instructions
to precise interface locations for autonomous interaction. Current
reinforcement learning approaches use binary rewards that treat elements as
hit-or-miss targets, creating sparse signals that ignore the continuous nature
of spatial interactions. Motivated by human clicking behavior that naturally
forms Gaussian distributions centered on target elements, we introduce GUI
Gaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that
models GUI elements as continuous Gaussian distributions across the interface
plane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point
rewards model precise localization through exponentially decaying distributions
centered on element centroids, while coverage rewards assess spatial alignment
by measuring the overlap between predicted Gaussian distributions and target
regions. To handle diverse element scales, we develop an adaptive variance
mechanism that calibrates reward distributions based on element dimensions.
This framework transforms GUI grounding from sparse binary classification to
dense continuous optimization, where Gaussian distributions generate rich
gradient signals that guide models toward optimal interaction positions.
Extensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro
benchmarks demonstrate that GUI-G$^2$, substantially outperforms
state-of-the-art method UI-TARS-72B, with the most significant improvement of
24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides
superior robustness to interface variations and enhanced generalization to
unseen layouts, establishing a new paradigm for spatial reasoning in GUI
interaction tasks.

### 20. Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Xinping Zhao, Shouzheng Huang, Yan Zhong, Xinshuo Hu, Baotian Hu, Min Zhang
- **URL**: <http://arxiv.org/abs/2507.15586v1>
- **Submitted**: 2025-07-21 13:03:55
- **Comment**: 16 pages, 7 Figures, 10 Tables
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper explores Retrieval-Augmented Generation, which is related to information retrieval, but the focus is on language models and generation rather than search technologies or query understanding. While the paper mentions retrieval, it is not directly applicable to the user's interests in ranking models or user behavior modeling.

#### Abstract
> Retrieval-Augmented Generation (RAG) effectively improves the accuracy of
Large Language Models (LLMs). However, retrieval noises significantly impact
the quality of LLMs' generation, necessitating the development of denoising
mechanisms. Previous methods extract evidence straightforwardly without
explicit thinking, which risks filtering out key clues and struggles with
generalization. To this end, we propose LEAR, which learns to extract rational
evidence by (1) explicitly reasoning to identify potential cues within
retrieval contents first, and then (2) consciously extracting to avoid omitting
any key cues helpful for answering questions. Specifically, we frame evidence
reasoning and evidence extraction into one unified response for end-to-end
training; apply knowledge token masks for disentanglement to derive
reasoning-based and extraction-based answers; and devise three types of
verifiable reward functions, including answer, length, and format, to update
the model via the policy optimization algorithm. Extensive experiments on three
benchmark datasets show the effectiveness of LEAR, providing compact and
high-quality evidence, improving the accuracy of downstream tasks, and
promoting effective application in online RAG systems.

### 21. XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Sachin Yadav, Dominik Schlechtweg
- **URL**: <http://arxiv.org/abs/2507.14578v1>
- **Submitted**: 2025-07-19 11:40:37
- **Comment**: 8 pages
- **Topic Keywords**: ranking, rank
- **Reason**: The paper proposes a finetuned Sentence Transformer model for ordinal Word-in-Context classification, which is a specific NLP task. While it touches on ranking and optimization, the focus is on classification rather than query understanding, ranking models, or user behavior modeling, which are core interests in Information Retrieval. The paper's relevance to IR is limited, but it may be of interest to those with a broader NLP focus.

#### Abstract
> We propose XL-DURel, a finetuned, multilingual Sentence Transformer model
optimized for ordinal Word-in-Context classification. We test several loss
functions for regression and ranking tasks managing to outperform previous
models on ordinal and binary data with a ranking objective based on angular
distance in complex space. We further show that binary WiC can be treated as a
special case of ordinal WiC and that optimizing models for the general ordinal
task improves performance on the more specific binary task. This paves the way
for a unified treatment of WiC modeling across different task formulations.

### 22. A Reproducibility Study of Product-side Fairness in Bundle Recommendation

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Huy-Son Nguyen, Yuanna Liu, Masoud Mansoury, Mohammad Alian Nejadi, Alan Hanjalic, Maarten de Rijke
- **URL**: <http://arxiv.org/abs/2507.14352v1>
- **Submitted**: 2025-07-18 20:06:39
- **Topic Keywords**: user behavior, recommend, search
- **Reason**: The paper explores fairness issues in bundle recommendation, which is a related topic to information retrieval and search technologies. However, the focus on recommender systems and fairness metrics is not directly aligned with the user's primary interest in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat related but not a central match.

#### Abstract
> Recommender systems are known to exhibit fairness issues, particularly on the
product side, where products and their associated suppliers receive unequal
exposure in recommended results. While this problem has been widely studied in
traditional recommendation settings, its implications for bundle recommendation
(BR) remain largely unexplored. This emerging task introduces additional
complexity: recommendations are generated at the bundle level, yet user
satisfaction and product (or supplier) exposure depend on both the bundle and
the individual items it contains. Existing fairness frameworks and metrics
designed for traditional recommender systems may not directly translate to this
multi-layered setting. In this paper, we conduct a comprehensive
reproducibility study of product-side fairness in BR across three real-world
datasets using four state-of-the-art BR methods. We analyze exposure
disparities at both the bundle and item levels using multiple fairness metrics,
uncovering important patterns. Our results show that exposure patterns differ
notably between bundles and items, revealing the need for fairness
interventions that go beyond bundle-level assumptions. We also find that
fairness assessments vary considerably depending on the metric used,
reinforcing the need for multi-faceted evaluation. Furthermore, user behavior
plays a critical role: when users interact more frequently with bundles than
with individual items, BR systems tend to yield fairer exposure distributions
across both levels. Overall, our findings offer actionable insights for
building fairer bundle recommender systems and establish a vital foundation for
future research in this emerging domain.

### 23. A Novel Self-Evolution Framework for Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Haoran Sun, Zekun Zhang, Shaoning Zeng
- **URL**: <http://arxiv.org/abs/2507.15281v1>
- **Submitted**: 2025-07-21 06:30:39
- **Topic Keywords**: retrieval, search
- **Reason**: The paper proposes a framework for optimizing large language models, focusing on user preference adaptation and domain-specific competence. While it touches on topics related to information retrieval, such as user behavior modeling, the primary focus is on language models and NLP, which is not directly aligned with the user's core research themes in IR and search technologies.

#### Abstract
> The capabilities of Large Language Models (LLMs) are limited to some extent
by pre-training, so some researchers optimize LLMs through post-training.
Existing post-training strategies, such as memory-based retrieval or preference
optimization, improve user alignment yet fail to enhance the model's domain
cognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution
(DPSE) framework that jointly optimizes user preference adaptation and
domain-specific competence. DPSE introduces a Censor module to extract
multi-dimensional interaction signals and estimate satisfaction scores, which
guide structured data expansion via topic-aware and preference-driven
strategies. These expanded datasets support a two-stage fine-tuning pipeline:
supervised domain grounding followed by frequency-aware preference
optimization. Experiments across general NLP benchmarks and long-term dialogue
tasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning,
Preference Optimization, and Memory-Augmented baselines. Ablation studies
validate the contribution of each module. In this way, our framework provides
an autonomous path toward continual self-evolution of LLMs.

### 24. A Fisher's exact test justification of the TF-IDF term-weighting scheme

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Paul Sheridan, Zeyad Ahmed, Aitazaz A. Farooque
- **URL**: <http://arxiv.org/abs/2507.15742v1>
- **Submitted**: 2025-07-21 15:54:23
- **Comment**: 23 pages, 4 tables
- **Topic Keywords**: retrieval
- **Reason**: The paper is somewhat related to information retrieval, specifically discussing the TF-IDF term-weighting scheme, which is a fundamental concept in IR. However, the focus is on justifying the scheme from a statistical perspective, which is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Term frequency-inverse document frequency, or TF-IDF for short, is arguably
the most celebrated mathematical expression in the history of information
retrieval. Conceived as a simple heuristic quantifying the extent to which a
given term's occurrences are concentrated in any one given document out of
many, TF-IDF and its many variants are routinely used as term-weighting schemes
in diverse text analysis applications. There is a growing body of scholarship
dedicated to placing TF-IDF on a sound theoretical foundation. Building on that
tradition, this paper justifies the use of TF-IDF to the statistics community
by demonstrating how the famed expression can be understood from a significance
testing perspective. We show that the common TF-IDF variant TF-ICF is, under
mild regularity conditions, closely related to the negative logarithm of the
$p$-value from a one-tailed version of Fisher's exact test of statistical
significance. As a corollary, we establish a connection between TF-IDF and the
said negative log-transformed $p$-value under certain idealized assumptions. We
further demonstrate, as a limiting case, that this same quantity converges to
TF-IDF in the limit of an infinitely large document collection. The Fisher's
exact test justification of TF-IDF equips the working statistician with a ready
explanation of the term-weighting scheme's long-established effectiveness.

### 25. Understanding Large Language Models' Ability on Interdisciplinary Research

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Yuanhao Shen, Daniel Xavier de Sousa, Ricardo Mar√ßal, Ali Asad, Hongyu Guo, Xiaodan Zhu
- **URL**: <http://arxiv.org/abs/2507.15736v1>
- **Submitted**: 2025-07-21 15:43:05
- **Topic Keywords**: recommend, search
- **Reason**: The paper explores the capabilities of Large Language Models in Interdisciplinary Research, which is a related topic to Information Retrieval and Search technologies. However, the focus on LLMs and their limitations in proposing research ideas is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Recent advancements in Large Language Models (LLMs) have revealed their
impressive ability to perform multi-step, logic-driven reasoning across complex
domains, positioning them as powerful tools and collaborators in scientific
discovery while challenging the long-held view that inspiration-driven ideation
is uniquely human. However, the lack of a dedicated benchmark that evaluates
LLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings
poses a critical barrier to fully understanding their strengths and
limitations. To address this gap, we introduce IDRBench -- a pioneering
benchmark featuring an expert annotated dataset and a suite of tasks tailored
to evaluate LLMs' capabilities in proposing valuable research ideas from
different scientific domains for interdisciplinary research. This benchmark
aims to provide a systematic framework for assessing LLM performance in
complex, cross-domain scientific research. Our dataset consists of scientific
publications sourced from the ArXiv platform covering six distinct disciplines,
and is annotated by domain experts with diverse academic backgrounds. To ensure
high-quality annotations, we emphasize clearly defined dimensions that
characterize authentic interdisciplinary research. The design of evaluation
tasks in IDRBench follows a progressive, real-world perspective, reflecting the
natural stages of interdisciplinary research development, including 1) IDR
Paper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.
Using IDRBench, we construct baselines across 10 LLMs and observe that despite
fostering some level of IDR awareness, LLMs still struggle to produce quality
IDR ideas. These findings could not only spark new research directions, but
also help to develop next-generation LLMs that excel in interdisciplinary
research.

### 26. Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Chathuri Jayaweera, Brianna Yanqui, Bonnie Dorr
- **URL**: <http://arxiv.org/abs/2507.15100v1>
- **Submitted**: 2025-07-20 19:42:45
- **Comment**: 9 pages, 8 figures and 5 tables
- **Topic Keywords**: rag
- **Reason**: The paper explores the use of Large Language Models as commonsense knowledge generators for Natural Language Inference, which is a topic in NLP. While it touches on the theme of query understanding and ranking models, it is not directly related to information retrieval or search technologies, which are the user's primary research interests.

#### Abstract
> Natural Language Inference (NLI) is the task of determining the semantic
entailment of a premise for a given hypothesis. The task aims to develop
systems that emulate natural human inferential processes where commonsense
knowledge plays a major role. However, existing commonsense resources lack
sufficient coverage for a variety of premise-hypothesis pairs. This study
explores the potential of Large Language Models as commonsense knowledge
generators for NLI along two key dimensions: their reliability in generating
such knowledge and the impact of that knowledge on prediction accuracy. We
adapt and modify existing metrics to assess LLM factuality and consistency in
generating in this context. While explicitly incorporating commonsense
knowledge does not consistently improve overall results, it effectively helps
distinguish entailing instances and moderately improves distinguishing
contradictory and neutral inferences.

### 27. LOVO: Efficient Complex Object Query in Large-Scale Video Datasets

- **LLM Score**: 2
- **Keyword Score**: 11
- **Authors**: Yuxin Liu, Yuezhang Peng, Hefeng Zhou, Hongze Liu, Xinyu Lu, Jiong Lou, Chentao Wu, Wei Zhao, Jie Li
- **URL**: <http://arxiv.org/abs/2507.14301v1>
- **Submitted**: 2025-07-18 18:21:43
- **Comment**: @inproceedings{liu2025lovo,title={LOVO: Efficient Complex Object
  Query in Large-Scale Video Datasets},author={Liu, Yuxin and Peng, Yuezhang
  and Zhou, Hefeng and Liu, Hongze and Lu, Xinyu and Lou, Jiong and Wu, Chentao
  and Zhao, Wei and Li, Jie},booktitle={2025 IEEE 41st International Conference
  on Data Engineering (ICDE)},pages={1938--1951},year={2025},organization={IEEE
  Computer Society}}
- **Topic Keywords**: query, queries, rerank, rank, search
- **Reason**: The paper focuses on video analysis and object query in large-scale video datasets, which is not directly related to the user's interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions query embeddings and nearest-neighbor searches, the context is different from the user's background in e-commerce and query understanding, ranking models, and user behavior modeling.

#### Abstract
> The widespread deployment of cameras has led to an exponential increase in
video data, creating vast opportunities for applications such as traffic
management and crime surveillance. However, querying specific objects from
large-scale video datasets presents challenges, including (1) processing
massive and continuously growing data volumes, (2) supporting complex query
requirements, and (3) ensuring low-latency execution. Existing video analysis
methods struggle with either limited adaptability to unseen object classes or
suffer from high query latency. In this paper, we present LOVO, a novel system
designed to efficiently handle comp$\underline{L}$ex $\underline{O}$bject
queries in large-scale $\underline{V}$ide$\underline{O}$ datasets. Agnostic to
user queries, LOVO performs one-time feature extraction using pre-trained
visual encoders, generating compact visual embeddings for key frames to build
an efficient index. These visual embeddings, along with associated bounding
boxes, are organized in an inverted multi-index structure within a vector
database, which supports queries for any objects. During the query phase, LOVO
transforms object queries to query embeddings and conducts fast approximate
nearest-neighbor searches on the visual embeddings. Finally, a cross-modal
rerank is performed to refine the results by fusing visual features with
detailed textual features. Evaluation on real-world video datasets demonstrates
that LOVO outperforms existing methods in handling complex queries, with
near-optimal query accuracy and up to 85x lower search latency, while
significantly reducing index construction costs. This system redefines the
state-of-the-art object query approaches in video analysis, setting a new
benchmark for complex object queries with a novel, scalable, and efficient
approach that excels in dynamic environments.

### 28. Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care

- **LLM Score**: 2
- **Keyword Score**: 10
- **Authors**: Vinicius Anjos de Almeida, Vinicius de Camargo, Raquel G√≥mez-Bravo, Egbert van der Haring, Kees van Boven, Marcelo Finger, Luis Fernandez Lopez
- **URL**: <http://arxiv.org/abs/2507.14681v1>
- **Submitted**: 2025-07-19 16:11:10
- **Comment**: To be submitted to peer-reviewed journal. 33 pages, 10 figures
  (including appendix), 15 tables (including appendix). For associated code
  repository, see https://github.com/almeidava93/llm-as-code-selectors-paper
- **Topic Keywords**: retriever, semantic search, query, search
- **Reason**: The paper is not directly related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. Although it uses large language models, the focus is on medical coding and does not involve query understanding, ranking models, or user behavior modeling. The paper's relevance to the user's interests is limited.

#### Abstract
> Background: Medical coding structures healthcare data for research, quality
monitoring, and policy. This study assesses the potential of large language
models (LLMs) to assign ICPC-2 codes using the output of a domain-specific
search engine.
  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each
annotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's
text-embedding-3-large) retrieved candidates from 73,563 labeled concepts.
Thirty-three LLMs were prompted with each query and retrieved results to select
the best-matching ICPC-2 code. Performance was evaluated using F1-score, along
with token usage, cost, response time, and format adherence.
  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top
performers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever
optimization can improve performance by up to 4 points. Most models returned
valid codes in the expected format, with reduced hallucinations. Smaller models
(<3B) struggled with formatting and input length.
  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even
without fine-tuning. This work offers a benchmark and highlights challenges,
but findings are limited by dataset scope and setup. Broader, multilingual,
end-to-end evaluations are needed for clinical validation.

### 29. Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Tian Li, Yujian Sun, Huizhi Liang
- **URL**: <http://arxiv.org/abs/2507.15714v1>
- **Submitted**: 2025-07-21 15:25:47
- **Topic Keywords**: ranking, rag, rank, search
- **Reason**: This paper focuses on emotion perception using contrastive learning, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on emotion detection and language models is not aligned with the user's primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> The SemEval-2025 Task 11, Bridging the Gap in Text-Based Emotion Detection,
introduces an emotion recognition challenge spanning over 28 languages. This
competition encourages researchers to explore more advanced approaches to
address the challenges posed by the diversity of emotional expressions and
background variations. It features two tracks: multi-label classification
(Track A) and emotion intensity prediction (Track B), covering six emotion
categories: anger, fear, joy, sadness, surprise, and disgust. In our work, we
systematically explore the benefits of two contrastive learning approaches:
sample-based (Contrastive Reasoning Calibration) and generation-based (DPO,
SimPO) contrastive learning. The sample-based contrastive approach trains the
model by comparing two samples to generate more reliable predictions. The
generation-based contrastive approach trains the model to differentiate between
correct and incorrect generations, refining its prediction. All models are
fine-tuned from LLaMa3-Instruct-8B. Our system achieves 9th place in Track A
and 6th place in Track B for English, while ranking among the top-tier
performing systems for other languages.

### 30. Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Fred Mutisya, Shikoh Gitau, Christine Syovata, Diana Oigara, Ibrahim Matende, Muna Aden, Munira Ali, Ryan Nyotu, Diana Marion, Job Nyangena, Nasubo Ongoma, Keith Mbae, Elizabeth Wamicha, Eric Mibuari, Jean Philbert Nsengemana, Talkmore Chidede
- **URL**: <http://arxiv.org/abs/2507.14615v1>
- **Submitted**: 2025-07-19 13:25:26
- **Comment**: 29 pages, 6 figs, 6 tables. Companion methods paper forthcoming
- **Topic Keywords**: rag, retrieval augmented generation, retrieval
- **Reason**: This paper is not relevant to your research interests as it focuses on clinical benchmarking and healthcare access in low-resource settings, using large language models for generating clinical scenarios and questions. The paper does not address query understanding, ranking models, or user behavior modeling, which are core areas of your research focus.

#### Abstract
> Large Language Models(LLMs) hold promise for improving healthcare access in
low-resource settings, but their effectiveness in African primary care remains
underexplored. We present a methodology for creating a benchmark dataset and
evaluation framework focused on Kenyan Level 2 and 3 clinical care. Our
approach uses retrieval augmented generation (RAG) to ground clinical questions
in Kenya's national guidelines, ensuring alignment with local standards. These
guidelines were digitized, chunked, and indexed for semantic retrieval. Gemini
Flash 2.0 Lite was then prompted with guideline excerpts to generate realistic
clinical scenarios, multiple-choice questions, and rationale based answers in
English and Swahili. Kenyan physicians co-created and refined the dataset, and
a blinded expert review process ensured clinical accuracy, clarity, and
cultural appropriateness. The resulting Alama Health QA dataset includes
thousands of regulator-aligned question answer pairs across common outpatient
conditions. Beyond accuracy, we introduce evaluation metrics that test clinical
reasoning, safety, and adaptability such as rare case detection (Needle in the
Haystack), stepwise logic (Decision Points), and contextual adaptability.
Initial results reveal significant performance gaps when LLMs are applied to
localized scenarios, consistent with findings that LLM accuracy is lower on
African medical content than on US-based benchmarks. This work offers a
replicable model for guideline-driven, dynamic benchmarking to support safe AI
deployment in African health systems.

### 31. P3: Prompts Promote Prompting

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Xinyu Zhang, Yuanquan Hu, Fangchao Liu, Zhicheng Dou
- **URL**: <http://arxiv.org/abs/2507.15675v1>
- **Submitted**: 2025-07-21 14:37:46
- **Comment**: Accepted to ACL 2025 findings
- **Topic Keywords**: query, rag
- **Reason**: The paper focuses on optimizing prompts for large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on optimization strategies, the context is not relevant to the user's primary research interests.

#### Abstract
> Current large language model (LLM) applications often employ multi-component
prompts, comprising both system and user prompts, to guide model behaviors.
While recent advancements have demonstrated the efficacy of automatically
optimizing either the system or user prompt to boost performance, such
unilateral approaches often yield suboptimal outcomes due to the interdependent
nature of these components. In this work, we introduce P3, a novel
self-improvement framework that concurrently optimizes both system and user
prompts through an iterative process. The offline optimized prompts are further
leveraged to promote online prompting by performing query-dependent prompt
optimization. Extensive experiments on general tasks (e.g., Arena-hard and
Alpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3
achieves superior performance in the realm of automatic prompt optimization.
Our results highlight the effectiveness of a holistic optimization strategy in
enhancing LLM performance across diverse domains.

### 32. X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Xiaolin Yan, Yangxing Liu, Jiazhang Zheng, Chi Liu, Mingyu Du, Caisheng Chen, Haoyang Liu, Ming Ding, Yuan Li, Qiuping Liao, Linfeng Li, Zhili Mei, Siyu Wan, Li Li, Ruyi Zhong, Jiangling Yu, Xule Liu, Huihui Hu, Jiameng Yue, Ruohui Cheng, Qi Yang, Liangqing Wu, Ke Zhu, Chi Zhang, Chufei Jing, Yifan Zhou, Yan Liang, Dongdong Li, Zhaohui Wang, Bin Zhao, Mingzhou Wu, Mingzhong Zhou, Peng Du, Zuomin Liao, Chao Dai, Pengfei Liang, Xiaoguang Zhu, Yu Zhang, Yu Gu, Kun Pan, Yuan Wu, Yanqing Guan, Shaojing Wu, Zikang Feng, Xianze Ma, Peishan Cheng, Wenjuan Jiang, Jing Ba, Huihao Yu, Zeping Hu, Yuan Xu, Zhiwei Liu, He Wang, Zhenguo Lin, Ming Liu, Yanhong Meng
- **URL**: <http://arxiv.org/abs/2507.14430v1>
- **Submitted**: 2025-07-19 01:20:39
- **Comment**: Technical Report
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on large language models for the semiconductor display industry, which is unrelated to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on domain-specific training and expertise also does not align with the user's interests in generalizable models and real-time relevance optimization.

#### Abstract
> Large language models (LLMs) have recently achieved significant advances in
reasoning and demonstrated their advantages in solving challenging problems.
Yet, their effectiveness in the semiconductor display industry remains limited
due to a lack of domain-specific training and expertise. To bridge this gap, we
present X-Intelligence 3.0, the first high-performance reasoning model
specifically developed for the semiconductor display industry. This model is
designed to deliver expert-level understanding and reasoning for the industry's
complex challenges. Leveraging a carefully curated industry knowledge base, the
model undergoes supervised fine-tuning and reinforcement learning to enhance
its reasoning and comprehension capabilities. To further accelerate
development, we implemented an automated evaluation framework that simulates
expert-level assessments. We also integrated a domain-specific
retrieval-augmented generation (RAG) mechanism, resulting in notable
performance gains on benchmark datasets. Despite its relatively compact size of
32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B
across multiple evaluations. This demonstrates its exceptional efficiency and
establishes it as a powerful solution to the longstanding reasoning challenges
faced by the semiconductor display industry.

### 33. Supernova: Achieving More with Less in Transformer Architectures

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Andrei-Valentin Tanase, Elena Pelican
- **URL**: <http://arxiv.org/abs/2507.15773v1>
- **Submitted**: 2025-07-21 16:27:48
- **Topic Keywords**: query
- **Reason**: The paper focuses on transformer architectures and tokenization innovations, which are not directly related to information retrieval, search technologies, or query understanding. While it explores efficiency and compression, the topics are not aligned with the user's primary research interests.

#### Abstract
> We present Supernova, a 650M-parameter decoder-only transformer that
demonstrates how careful architectural design and tokenization innovation can
achieve the performance of larger models while maintaining computational
efficiency. Our architecture combines Rotary Positional Embeddings (RoPE),
Grouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for
computational efficiency, and SwiGLU activation functions. A critical
innovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which
achieves state-of-the-art compression performance. Through detailed analysis,
we show that Supernova achieves 90% of the performance of 1B-parameter models
while using 53% fewer parameters and requiring only 100B training tokens--an
order of magnitude less than competing models. Our findings challenge the
prevailing scaling paradigm, demonstrating that architectural efficiency and
tokenization quality can compensate for reduced parameter counts.

### 34. Leveraging Context for Multimodal Fallacy Classification in Political Debates

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Alessio Pittiglio
- **URL**: <http://arxiv.org/abs/2507.15641v1>
- **Submitted**: 2025-07-21 14:03:08
- **Comment**: 12th Workshop on Argument Mining (ArgMining 2025) @ ACL 2025
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on multimodal argument mining and fallacy classification in political debates, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's context and multimodal approach are not directly applicable to the user's areas of focus, such as query understanding, ranking models, and user behavior modeling.

#### Abstract
> In this paper, we present our submission to the MM-ArgFallacy2025 shared
task, which aims to advance research in multimodal argument mining, focusing on
logical fallacies in political debates. Our approach uses pretrained
Transformer-based models and proposes several ways to leverage context. In the
fallacy classification subtask, our models achieved macro F1-scores of 0.4444
(text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed
performance comparable to the text-only model, suggesting potential for
improvements.

### 35. ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Alexandru Coca, Mark Gaynor, Zhenxing Zhang, Jianpeng Cheng, Bo-Hsiang Tseng, Pete Boothroyd, H√©ctor Martinez Alonso, Diarmuid √ì S√©aghdha, Anders Johannsen
- **URL**: <http://arxiv.org/abs/2507.15501v1>
- **Submitted**: 2025-07-21 11:07:05
- **Comment**: 37 pages, 22 figures. To appear at ACL 2025
- **Topic Keywords**: queries
- **Reason**: The paper focuses on the application of large language models in digital assistants for complex action execution, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on NLP, it is more focused on programming knowledge and task generation, which is not a central match for your research interests.

#### Abstract
> This work evaluates the potential of large language models (LLMs) to power
digital assistants capable of complex action execution. These assistants rely
on pre-trained programming knowledge to execute multi-step goals by composing
objects and functions defined in assistant libraries into action execution
programs. To achieve this, we develop ASPERA, a framework comprising an
assistant library simulation and a human-assisted LLM data generation engine.
Our engine allows developers to guide LLM generation of high-quality tasks
consisting of complex user queries, simulation state and corresponding
validation programs, tackling data availability and evaluation robustness
challenges. Alongside the framework we release Asper-Bench, an evaluation
dataset of 250 challenging tasks generated using ASPERA, which we use to show
that program generation grounded in custom assistant libraries is a significant
challenge to LLMs compared to dependency-free code generation.

### 36. ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yuanhe Tian, Junjie Liu, Zhizhou Kou, Yuxiang Li, Yan Song
- **URL**: <http://arxiv.org/abs/2507.15275v1>
- **Submitted**: 2025-07-21 06:23:16
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on building a large Chinese medical dataset for language modeling, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions pre-training and fine-tuning, the context is specific to language models and does not align with the user's interests in ranking models, user behavior modeling, or real-time relevance optimization.

#### Abstract
> Building high-quality data resources is crucial for advancing artificial
intelligence research and applications in specific domains, particularly in the
Chinese medical domain. Existing Chinese medical datasets are limited in size
and narrow in domain coverage, falling short of the diverse corpora required
for effective pre-training. Moreover, most datasets are designed solely for LLM
fine-tuning and do not support pre-training and reinforcement learning from
human feedback (RLHF). In this paper, we propose a Chinese medical dataset
named ChiMed 2.0, which extends our previous work ChiMed, and covers data
collected from Chinese medical online platforms and generated by LLMs. ChiMed
2.0 contains 204.4M Chinese characters covering both traditional Chinese
medicine classics and modern general medical data, where there are 164.8K
documents for pre-training, 351.6K question-answering pairs for supervised
fine-tuning (SFT), and 41.7K preference data tuples for RLHF. To validate the
effectiveness of our approach for training a Chinese medical LLM, we conduct
further pre-training, SFT, and RLHF experiments on representative general
domain LLMs and evaluate their performance on medical benchmark datasets. The
results show performance gains across different model scales, validating the
dataset's effectiveness and applicability.

### 37. A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic Texts Under Prompt-Influenced Length Variations

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Vijeta Deshpande, Ishita Dasgupta, Uttaran Bhattacharya, Somdeb Sarkhel, Saayan Mitra, Anna Rumshisky
- **URL**: <http://arxiv.org/abs/2507.15092v1>
- **Submitted**: 2025-07-20 19:14:43
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on measuring lexical diversity in synthetic texts generated by Large Language Models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the topic of language models, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are core areas of interest.

#### Abstract
> Synthetic text generated by Large Language Models (LLMs) is increasingly used
for further training and improvement of LLMs. Diversity is crucial for the
effectiveness of synthetic data, and researchers rely on prompt engineering to
improve diversity. However, the impact of prompt variations on response text
length, and, more importantly, the consequential effect on lexical diversity
measurements, remain underexplored. In this work, we propose Penalty-Adjusted
Type-Token Ratio (PATTR), a diversity metric robust to length variations. We
generate a large synthetic corpus of over 20M words using seven models from the
LLaMA, OLMo, and Phi families, focusing on a creative writing task of video
script generation, where diversity is crucial. We evaluate per-response lexical
diversity using PATTR and compare it against existing metrics of Moving-Average
TTR (MATTR) and Compression Ratio (CR). Our analysis highlights how text length
variations introduce biases favoring shorter responses. Unlike existing
metrics, PATTR explicitly considers the task-specific target response length
($L_T$) to effectively mitigate length biases. We further demonstrate the
utility of PATTR in filtering the top-10/100/1,000 most lexically diverse
responses, showing that it consistently outperforms MATTR and CR by yielding on
par or better diversity with high adherence to $L_T$.

### 38. SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Vahid Rahimzadeh, Erfan Moosavi Monazzah, Mohammad Taher Pilehvar, Yadollah Yaghoobzadeh
- **URL**: <http://arxiv.org/abs/2507.14922v1>
- **Submitted**: 2025-07-20 11:37:07
- **Topic Keywords**: ctr, search
- **Reason**: The paper focuses on persona-driven language models and synthetic data generation, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on language modeling, the primary focus is on computational social science and persona-driven language models, which is not a central match for the user's research interests.

#### Abstract
> Persona-driven LLMs have emerged as powerful tools in computational social
science, yet existing approaches fall at opposite extremes, either relying on
costly human-curated data or producing synthetic personas that lack consistency
and realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from
10,000 real social media users from BlueSky open platform across three time
windows, bridging this spectrum by grounding synthetic generation in authentic
user activity. Our evaluation demonstrates that SYNTHIA achieves competitive
performance with state-of-the-art methods in demographic diversity and social
survey alignment while significantly outperforming them in narrative
consistency. Uniquely, SYNTHIA incorporates temporal dimensionality and
provides rich social interaction metadata from the underlying network, enabling
new research directions in computational social science and persona-driven
language modeling.

### 39. MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Xingxuan Li, Yao Xiao, Dianwen Ng, Hai Ye, Yue Deng, Xiang Lin, Bin Wang, Zhanfeng Mo, Chong Zhang, Yueyi Zhang, Zonglin Yang, Ruilin Li, Lei Lei, Shihao Xu, Han Zhao, Weiling Chen, Feng Ji, Lidong Bing
- **URL**: <http://arxiv.org/abs/2507.14683v1>
- **Submitted**: 2025-07-19 16:21:23
- **Comment**: Technical report
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on mathematical reasoning and large language models, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on transparency and reproducibility in RLM development is also not a central match for the user's interests.

#### Abstract
> Large language models have recently evolved from fluent text generation to
advanced reasoning across diverse domains, giving rise to reasoning language
models. Among these domains, mathematical reasoning serves as a representative
benchmark as it requires precise multi-step logic and abstract reasoning, which
can be generalized to other tasks. While closed-source RLMs such as GPT-o3
demonstrate impressive reasoning capabilities, their proprietary nature limits
transparency and reproducibility. Although many open-source projects aim to
close this gap, most of them lack sufficient openness by omitting critical
resources such as datasets and detailed training configurations, which hinders
reproducibility. To contribute toward greater transparency in RLM development,
we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on
the Qwen-2.5 backbone that match or exceed the performance of existing
open-source RLMs. Specifically, our models are trained in two stages: SFT on a
carefully curated corpus of 719K math-reasoning problems with verified CoT
trajectories, followed by RLVR on 62K challenging and verifiable problems. To
enhance the robustness and efficiency of the RLVR process, we introduce
Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates
length-progressive training with an adaptive repetition penalty to encourage
context-aware RL training. Our model achieves state-of-the-art or competitive
performance and superior token efficiency among Qwen-2.5-based open-source 7B
and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate
reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,
MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,
MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope
these resources will support further research and foster community advancement.

### 40. When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Qibing Ren, Sitao Xie, Longxuan Wei, Zhenfei Yin, Junchi Yan, Lizhuang Ma, Jing Shao
- **URL**: <http://arxiv.org/abs/2507.14660v1>
- **Submitted**: 2025-07-19 15:17:30
- **Comment**: Code is available at https://github.com/renqibing/RogueAgent
- **Topic Keywords**: commerce, e-commerce, search
- **Reason**: The paper's focus on autonomous AI systems, multi-agent collusion, and malicious actions is unrelated to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's abstract does not mention query understanding, ranking models, user behavior modeling, or deep semantic understanding, which are key areas of interest for the user.

#### Abstract
> Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

### 41. Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Kester Wong, Sahan Bulathwela, Mutlu Cukurova
- **URL**: <http://arxiv.org/abs/2507.14579v1>
- **Submitted**: 2025-07-19 11:47:08
- **Comment**: Accepted to appear in the workshop proceedings for the HEXED'25
  workshop in the 26th International Conference on Artificial Intelligence in
  Education 2025 (AIED 2025), 22 July 2025, Palermo, Italy. 5 pages
- **Topic Keywords**: rag, search
- **Reason**: The paper explores the application of BERT models in a specific domain (CPS diagnosis in education) with a focus on multimodal features and human-AI complementarity. While it touches on machine learning and AI, it does not align with the user's primary research interests in Information Retrieval, Search technologies, and query understanding, ranking models, and user behavior modeling.

#### Abstract
> Detecting collaborative problem solving (CPS) indicators from dialogue using
machine learning techniques is a significant challenge for the field of AI in
Education. Recent studies have explored the use of Bidirectional Encoder
Representations from Transformers (BERT) models on transcription data to
reliably detect meaningful CPS indicators. A notable advancement involved the
multimodal BERT variant, AudiBERT, which integrates speech and
acoustic-prosodic audio features to enhance CPS diagnosis. Although initial
results demonstrated multimodal improvements, the statistical significance of
these enhancements remained unclear, and there was insufficient guidance on
leveraging human-AI complementarity for CPS diagnosis tasks. This workshop
paper extends the previous research by highlighting that the AudiBERT model not
only improved the classification of classes that were sparse in the dataset,
but it also had statistically significant class-wise improvements over the BERT
model for classifications in the social-cognitive dimension. However, similar
significant class-wise improvements over the BERT model were not observed for
classifications in the affective dimension. A correlation analysis highlighted
that larger training data was significantly associated with higher recall
performance for both the AudiBERT and BERT models. Additionally, the precision
of the BERT model was significantly associated with high inter-rater agreement
among human coders. When employing the BERT model to diagnose indicators within
these subskills that were well-detected by the AudiBERT model, the performance
across all indicators was inconsistent. We conclude the paper by outlining a
structured approach towards achieving human-AI complementarity for CPS
diagnosis, highlighting the crucial inclusion of model explainability to
support human agency and engagement in the reflective coding process.

### 42. Solo Connection: A Parameter Efficient Fine-Tuning Technique for Transformers

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Harsh Nilesh Pathak, Randy Paffenroth
- **URL**: <http://arxiv.org/abs/2507.14353v1>
- **Submitted**: 2025-07-18 20:11:50
- **Topic Keywords**: rag, rank
- **Reason**: This paper focuses on fine-tuning transformer models for language generation tasks, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions transformer models, the paper's primary concern is adapting language models for new tasks, which is not a key area of interest for the user.

#### Abstract
> Parameter efficient fine tuning (PEFT) is a versatile and extensible approach
for adapting a Large Language Model (LLM) for newer tasks. One of the most
prominent PEFT approaches, Low Rank Adaptation (LoRA), primarily focuses on
adjusting the attention weight matrices within individual decoder blocks of a
Generative Pre trained Transformer (GPT2). In contrast, we introduce Solo
Connection a novel method that adapts the representation at the decoder-block
level rather than modifying individual weight matrices. Not only does Solo
Connection outperform LoRA on E2E natural language generation benchmarks, but
it also reduces the number of trainable parameters by 59% relative to LoRA and
by more than 99% compared to full fine-tuning of GPT2, an early version of
Large Language Models (LLMs). Solo Connection is also motivated by homotopy
theory: we introduce a trainable linear transformation that gradually
interpolates between a zero vector and the task-specific representation,
enabling smooth and stable adaptation over time. While skip connections in the
original 12 layer GPT2 are typically confined to individual decoder blocks,
subsequent GPT2 variants scale up to 48 layers, and even larger language models
can include 128 or more decoder blocks. These expanded architectures underscore
the need to revisit how skip connections are employed during fine-tuning. This
paper focuses on long skip connections that link outputs of different decoder
blocks, potentially enhancing the model's ability to adapt to new tasks while
leveraging pre-trained knowledge.

### 43. The Impact of Language Mixing on Bilingual LLM Reasoning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yihao Li, Jiayi Xin, Miranda Muqing Miao, Qi Long, Lyle Ungar
- **URL**: <http://arxiv.org/abs/2507.15849v1>
- **Submitted**: 2025-07-21 17:56:09
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling. The focus on bilingual large language models and language mixing in reasoning is outside the user's primary areas of interest.

#### Abstract
> Proficient multilingual speakers often intentionally switch languages in the
middle of a conversation. Similarly, recent reasoning-focused bilingual large
language models (LLMs) with strong capabilities in both languages exhibit
language mixing--alternating languages within their chain of thought.
Discouraging this behavior in DeepSeek-R1 was found to degrade accuracy,
suggesting that language mixing may benefit reasoning. In this work, we study
language switching in Chinese-English bilingual reasoning models. We identify
reinforcement learning with verifiable rewards (RLVR) as the critical training
stage that leads to language mixing. We demonstrate that language mixing can
enhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6
percentage points on math reasoning tasks. Additionally, a lightweight probe
can be trained to predict whether a potential language switch would benefit or
harm reasoning, and when used to guide decoding, increases accuracy by up to
6.25 percentage points. Our findings suggest that language mixing is not merely
a byproduct of multilingual training, but is a strategic reasoning behavior.

### 44. Hierarchical Budget Policy Optimization for Adaptive Reasoning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Shangke Lyu, Linjuan Wu, Yuchen Yan, Xingyu Wu, Hao Li, Yongliang Shen, Peisheng Jiang, Weiming Lu, Jun Xiao, Yueting Zhuang
- **URL**: <http://arxiv.org/abs/2507.15844v1>
- **Submitted**: 2025-07-21 17:52:34
- **Comment**: Code: https://github.com/zju-real/hbpo Project
  Page:https://zju-real.github.io/hbpo/
- **Topic Keywords**: rag
- **Reason**: This paper focuses on optimizing the efficiency of reasoning models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the idea of adapting to problem complexity, the approach is not applicable to the user's areas of interest, such as ranking models or user behavior modeling.

#### Abstract
> Large reasoning models achieve remarkable performance through extensive
chain-of-thought generation, yet exhibit significant computational inefficiency
by applying uniform reasoning strategies regardless of problem complexity. We
present Hierarchical Budget Policy Optimization (HBPO), a reinforcement
learning framework that enables models to learn problem-specific reasoning
depths without sacrificing capability. HBPO addresses the fundamental challenge
of exploration space collapse in efficiency-oriented training, where penalties
on long output length systematically bias models away from necessary long
reasoning paths. Through hierarchical budget exploration, our approach
partitions rollout samples into multiple subgroups with distinct token budgets,
aiming to enable efficient resource allocation while preventing degradation of
capability. We introduce differentiated reward mechanisms that create
budget-aware incentives aligned with the complexity of the problem, allowing
models to discover natural correspondences between task requirements and
computational effort. Extensive experiments demonstrate that HBPO reduces
average token usage by up to 60.6% while improving accuracy by 3.14% across
four reasoning benchmarks. Unlike existing methods that impose external
constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive
behavior where models automatically adjust reasoning depth based on problem
complexity. Our results suggest that reasoning efficiency and capability are
not inherently conflicting, and can be simultaneously optimized through
appropriately structured hierarchical training that preserves exploration
diversity.

### 45. Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jiakang Wang, Runze Liu, Fuzheng Zhang, Xiu Li, Guorui Zhou
- **URL**: <http://arxiv.org/abs/2507.15778v1>
- **Submitted**: 2025-07-21 16:34:01
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on Reinforcement Learning with Verifiable Rewards for improving the reasoning abilities of Large Language Models, which is outside your primary focus area.

#### Abstract
> Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective
post-training method for improving the reasoning abilities of Large Language
Models (LLMs), mainly by shaping higher-order behaviors such as reflection and
planning. However, previous RLVR algorithms often apply uniform training
signals to all tokens, without considering the different roles of low-entropy
knowledge-related tokens and high-entropy reasoning-related tokens. Some recent
methods try to separate these token types by gradient masking or asynchronous
updates, but these approaches may break semantic dependencies in the model
output and hinder effective learning. In this work, we propose Archer, an
entropy-aware RLVR approach with dual-token constraints and synchronous
updates. Specifically, our method applies weaker KL regularization and higher
clipping thresholds to reasoning tokens to encourage exploration, while using
stronger constraints on knowledge tokens to maintain factual knowledge.
Experimental results on several mathematical reasoning and code generation
benchmarks show that our approach significantly outperforms previous RLVR
methods, reaching or exceeding state-of-the-art performance among models of
comparable size. The code is available at
https://github.com/wizard-III/ArcherCodeR.

### 46. LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Xingyu Wu, Yuchen Yan, Shangke Lyu, Linjuan Wu, Yiwen Qiu, Yongliang Shen, Weiming Lu, Jian Shao, Jun Xiao, Yueting Zhuang
- **URL**: <http://arxiv.org/abs/2507.15758v1>
- **Submitted**: 2025-07-21 16:14:41
- **Comment**: GitHub:https://github.com/zju-real/lapo;
  Project:https://zju-real.github.io/lapo
- **Topic Keywords**: rag
- **Reason**: The paper focuses on optimizing the length of reasoning sequences in large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the concept of efficient reasoning, the context is different from the user's primary research interests.

#### Abstract
> Large reasoning models have achieved remarkable performance through extended
chain-of-thought sequences, yet this computational freedom leads to excessive
token generation even for simple problems. We present Length-Adaptive Policy
Optimization (LAPO), a novel framework that transforms reasoning length control
from an external constraint into an intrinsic model capability. Unlike existing
approaches that impose rigid limits or rely on post-hoc interventions, LAPO
enables models to internalize an understanding of appropriate reasoning depth
through a two-stage reinforcement learning process. In the first stage, models
learn natural reasoning patterns by discovering the statistical distribution of
successful solution lengths. The second stage leverages these patterns as
meta-cognitive guidance, embedding them directly within the model's reasoning
context to ensure inference-time flexibility. Experiments on mathematical
reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\%
while improving accuracy by 2.3\%. Our analysis reveals that models trained
with LAPO develop emergent abilities to allocate computational resources based
on problem complexity, achieving efficient reasoning without sacrificing
quality.

### 47. CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Congmin Zheng, Jiachen Zhu, Jianghao Lin, Xinyi Dai, Yong Yu, Weinan Zhang, Mengyue Yang
- **URL**: <http://arxiv.org/abs/2507.15698v1>
- **Submitted**: 2025-07-21 15:07:59
- **Topic Keywords**: rag
- **Reason**: The paper focuses on Process Reward Models and length debiasing in large language models, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it involves natural language processing, the topic is more specific to language models and reward prediction, and does not align with the user's primary research interests.

#### Abstract
> Process Reward Models (PRMs) play a central role in evaluating and guiding
multi-step reasoning in large language models (LLMs), especially for
mathematical problem solving. However, we identify a pervasive length bias in
existing PRMs: they tend to assign higher scores to longer reasoning steps,
even when the semantic content and logical validity are unchanged. This bias
undermines the reliability of reward predictions and leads to overly verbose
outputs during inference. To address this issue, we propose
CoLD(Counterfactually-Guided Length Debiasing), a unified framework that
mitigates length bias through three components: an explicit length-penalty
adjustment, a learned bias estimator trained to capture spurious length-related
signals, and a joint training strategy that enforces length-invariance in
reward predictions. Our approach is grounded in counterfactual reasoning and
informed by causal graph analysis. Extensive experiments on MATH500 and
GSM-Plus show that CoLD consistently reduces reward-length correlation,
improves accuracy in step selection, and encourages more concise, logically
valid reasoning. These results demonstrate the effectiveness and practicality
of CoLD in improving the fidelity and robustness of PRMs.

### 48. AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jierui Li, Raymond Mooney
- **URL**: <http://arxiv.org/abs/2507.15378v1>
- **Submitted**: 2025-07-21 08:34:20
- **Comment**: 19 pages, pre-print only
- **Topic Keywords**: retrieval
- **Reason**: The paper focuses on competitive programming and algorithmic similarity detection, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper mentions LLMs and retrieval methods, the context is different and the techniques are not applicable to the user's areas of interest.

#### Abstract
> Recent progress in LLMs, such as reasoning models, has demonstrated strong
abilities to solve complex competitive programming problems, often rivaling top
human competitors. However, it remains underexplored whether these abilities
generalize to relevant domains that are less seen during training. To address
this, we introduce AlgoSimBench, a new benchmark designed to assess LLMs'
ability to identify algorithmically similar problems (ASPs)-problems that can
be solved using similar algorithmic approaches. AlgoSimBench consists of 1317
problems, annotated with 231 distinct fine-grained algorithm tags, from which
we curate 402 multiple-choice questions (MCQs), where each question presents
one algorithmically similar problem alongside three textually similar but
algorithmically dissimilar distractors. Our evaluation reveals that LLMs
struggle to identify ASPs, with the best-performing model (o3-mini) achieving
only 65.9% accuracy on the MCQ task. To address this challenge, we propose
attempted solution matching (ASM), a novel method for improving problem
similarity detection. On our MCQ task, ASM yields an absolute accuracy
improvement of 6.7% to 11.7% across different models. We also evaluated code
embedding models and retrieval methods on similar problem identification. While
the adversarial selection of problems degrades the performance to be less than
random, we found that simply summarizing the problem to remove narrative
elements eliminates the effect, and combining ASM with a keyword-prioritized
method, BM25, can yield up to 52.2% accuracy. Code and data are available at
github.com

### 49. A2TTS: TTS for Low Resource Indian Languages

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ayush Singh Bhadoriya, Abhishek Nikunj Shinde, Isha Pandey, Ganesh Ramakrishnan
- **URL**: <http://arxiv.org/abs/2507.15272v1>
- **Submitted**: 2025-07-21 06:20:27
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests as it focuses on Text-to-Speech (TTS) systems for low-resource Indian languages, which is outside the scope of Information Retrieval, Search technologies, and Natural Language Processing. The paper does not address query understanding, ranking models, or user behavior modeling, which are your primary areas of interest.

#### Abstract
> We present a speaker conditioned text-to-speech (TTS) system aimed at
addressing challenges in generating speech for unseen speakers and supporting
diverse Indian languages. Our method leverages a diffusion-based TTS
architecture, where a speaker encoder extracts embeddings from short reference
audio samples to condition the DDPM decoder for multispeaker generation. To
further enhance prosody and naturalness, we employ a cross-attention based
duration prediction mechanism that utilizes reference audio, enabling more
accurate and speaker consistent timing. This results in speech that closely
resembles the target speaker while improving duration modeling and overall
expressiveness. Additionally, to improve zero-shot generation, we employed
classifier free guidance, allowing the system to generate speech more near
speech for unknown speakers. Using this approach, we trained language-specific
speaker-conditioned models. Using the IndicSUPERB dataset for multiple Indian
languages such as Bengali, Gujarati, Hindi, Marathi, Malayalam, Punjabi and
Tamil.

### 50. SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Shayan Vassef, Amirhossein Dabiriaghdam, Mohammadreza Bakhtiari, Yadollah Yaghoobzadeh
- **URL**: <http://arxiv.org/abs/2507.15236v1>
- **Submitted**: 2025-07-21 04:43:21
- **Topic Keywords**: rag
- **Reason**: The paper focuses on the training dynamics of language models, introducing a novel categorization framework called Subsets of Interest (SOI). While it explores multi-task, multi-lingual, and multi-source learning approaches, the paper's primary focus is on language models, which is not directly related to information retrieval, search technologies, or query understanding. The paper's relevance to the user's research interests is limited.

#### Abstract
> This work investigates the impact of multi-task, multi-lingual, and
multi-source learning approaches on the robustness and performance of
pretrained language models. To enhance this analysis, we introduce Subsets of
Interest (SOI), a novel categorization framework that identifies six distinct
learning behavior patterns during training, including forgettable examples,
unlearned examples, and always correct examples. Through SOI transition
heatmaps and dataset cartography visualization, we analyze how examples shift
between these categories when transitioning from single-setting to
multi-setting configurations. We perform comprehensive experiments across three
parallel comparisons: multi-task vs. single-task learning using English tasks
(entailment, paraphrase, sentiment), multi-source vs. single-source learning
using sentiment analysis datasets, and multi-lingual vs. single-lingual
learning using intent classification in French, English, and Persian. Our
results demonstrate that multi-source learning consistently improves
out-of-distribution performance by up to 7%, while multi-task learning shows
mixed results with notable gains in similar task combinations. We further
introduce a two-stage fine-tuning approach where the second stage leverages
SOI-based subset selection to achieve additional performance improvements.
These findings provide new insights into training dynamics and offer practical
approaches for optimizing multi-setting language model performance.

---

