[
    {
        "title": "Boosting Parameter Efficiency in LLM-Based Recommendation through Sophisticated Pruning",
        "abstract": "LLM-based recommender systems have made significant progress; however, the\ndeployment cost associated with the large parameter volume of LLMs still\nhinders their real-world applications. This work explores parameter pruning to\nimprove parameter efficiency while maintaining recommendation quality, thereby\nenabling easier deployment. Unlike existing approaches that focus primarily on\ninter-layer redundancy, we uncover intra-layer redundancy within components\nsuch as self-attention and MLP modules. Building on this analysis, we propose a\nmore fine-grained pruning approach that integrates both intra-layer and\nlayer-wise pruning. Specifically, we introduce a three-stage pruning strategy\nthat progressively prunes parameters at different levels and parts of the\nmodel, moving from intra-layer to layer-wise pruning, or from width to depth.\nEach stage also includes a performance restoration step using distillation\ntechniques, helping to strike a balance between performance and parameter\nefficiency. Empirical results demonstrate the effectiveness of our approach:\nacross three datasets, our models achieve an average of 88% of the original\nmodel's performance while pruning more than 95% of the non-embedding\nparameters. This underscores the potential of our method to significantly\nreduce resource requirements without greatly compromising recommendation\nquality. Our code will be available at: https://github.com/zheng-sl/PruneRec",
        "url": "http://arxiv.org/abs/2507.07064v1",
        "pdf_url": "http://arxiv.org/pdf/2507.07064v1",
        "arxiv_id": "2507.07064v1",
        "authors": [
            "Shanle Zheng",
            "Keqin Bao",
            "Jizhi Zhang",
            "Yang Zhang",
            "Fuli Feng",
            "Xiangnan He"
        ],
        "submitted": "2025-07-09 17:26:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "UniConv: Unifying Retrieval and Response Generation for Large Language Models in Conversations",
        "abstract": "The rapid advancement of conversational search systems revolutionizes how\ninformation is accessed by enabling the multi-turn interaction between the user\nand the system. Existing conversational search systems are usually built with\ntwo different models. This separation restricts the system from leveraging the\nintrinsic knowledge of the models simultaneously, which cannot ensure the\neffectiveness of retrieval benefiting the generation. The existing studies for\ndeveloping unified models cannot fully address the aspects of understanding\nconversational context, managing retrieval independently, and generating\nresponses. In this paper, we explore how to unify dense retrieval and response\ngeneration for large language models in conversation. We conduct joint\nfine-tuning with different objectives and design two mechanisms to reduce the\ninconsistency risks while mitigating data discrepancy. The evaluations on five\nconversational search datasets demonstrate that our unified model can mutually\nimprove both tasks and outperform the existing baselines.",
        "url": "http://arxiv.org/abs/2507.07030v1",
        "pdf_url": "http://arxiv.org/pdf/2507.07030v1",
        "arxiv_id": "2507.07030v1",
        "authors": [
            "Fengran Mo",
            "Yifan Gao",
            "Chuan Meng",
            "Xin Liu",
            "Zhuofeng Wu",
            "Kelong Mao",
            "Zhengyang Wang",
            "Pei Chen",
            "Zheng Li",
            "Xian Li",
            "Bing Yin",
            "Meng Jiang"
        ],
        "submitted": "2025-07-09 17:02:40",
        "source": "arxiv",
        "comment": "Accepted by ACL 2025 (main)"
    },
    {
        "title": "FlexOlmo: Open Language Models for Flexible Data Use",
        "abstract": "We introduce FlexOlmo, a new class of language models (LMs) that supports (1)\ndistributed training without data sharing, where different model parameters are\nindependently trained on closed datasets, and (2) data-flexible inference,\nwhere these parameters along with their associated data can be flexibly\nincluded or excluded from model inferences with no further training. FlexOlmo\nemploys a mixture-of-experts (MoE) architecture where each expert is trained\nindependently on closed datasets and later integrated through a new\ndomain-informed routing without any joint training. FlexOlmo is trained on\nFlexMix, a corpus we curate comprising publicly available datasets alongside\nseven domain-specific sets, representing realistic approximations of closed\nsets. We evaluate models with up to 37 billion parameters (20 billion active)\non 31 diverse downstream tasks. We show that a general expert trained on public\ndata can be effectively combined with independently trained experts from other\ndata owners, leading to an average 41% relative improvement while allowing\nusers to opt out of certain data based on data licensing or permission\nrequirements. Our approach also outperforms prior model merging methods by\n10.1% on average and surpasses the standard MoE trained without data\nrestrictions using the same training FLOPs. Altogether, this research presents\na solution for both data owners and researchers in regulated industries with\nsensitive or protected data. FlexOlmo enables benefiting from closed data while\nrespecting data owners' preferences by keeping their data local and supporting\nfine-grained control of data access during inference.",
        "url": "http://arxiv.org/abs/2507.07024v1",
        "pdf_url": "http://arxiv.org/pdf/2507.07024v1",
        "arxiv_id": "2507.07024v1",
        "authors": [
            "Weijia Shi",
            "Akshita Bhagia",
            "Kevin Farhat",
            "Niklas Muennighoff",
            "Pete Walsh",
            "Jacob Morrison",
            "Dustin Schwenk",
            "Shayne Longpre",
            "Jake Poznanski",
            "Allyson Ettinger",
            "Daogao Liu",
            "Margaret Li",
            "Dirk Groeneveld",
            "Mike Lewis",
            "Wen-tau Yih",
            "Luca Soldaini",
            "Kyle Lo",
            "Noah A. Smith",
            "Luke Zettlemoyer",
            "Pang Wei Koh",
            "Hannaneh Hajishirzi",
            "Ali Farhadi",
            "Sewon Min"
        ],
        "submitted": "2025-07-09 16:54:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Learning Deliberately, Acting Intuitively: Unlocking Test-Time Reasoning in Multimodal LLMs",
        "abstract": "Reasoning is a key capability for large language models (LLMs), particularly\nwhen applied to complex tasks such as mathematical problem solving. However,\nmultimodal reasoning research still requires further exploration of modality\nalignment and training costs. Many of these approaches rely on additional data\nannotation and relevant rule-based rewards to enhance the understanding and\nreasoning ability, which significantly increases training costs and limits\nscalability. To address these challenges, we propose the\nDeliberate-to-Intuitive reasoning framework (D2I) that improves the\nunderstanding and reasoning ability of multimodal LLMs (MLLMs) without extra\nannotations and complex rewards. Specifically, our method sets deliberate\nreasoning strategies to enhance modality alignment only through the rule-based\nformat reward during training. While evaluating, the reasoning style shifts to\nintuitive, which removes deliberate reasoning strategies during training and\nimplicitly reflects the model's acquired abilities in the response. D2I\noutperforms baselines across both in-domain and out-of-domain benchmarks. Our\nfindings highlight the role of format reward in fostering transferable\nreasoning skills in MLLMs, and inspire directions for decoupling training-time\nreasoning depth from test-time response flexibility.",
        "url": "http://arxiv.org/abs/2507.06999v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06999v1",
        "arxiv_id": "2507.06999v1",
        "authors": [
            "Yahan Yu",
            "Yuyang Dong",
            "Masafumi Oyamada"
        ],
        "submitted": "2025-07-09 16:25:44",
        "source": "arxiv",
        "comment": "Work in progress"
    },
    {
        "title": "FRaN-X: FRaming and Narratives-eXplorer",
        "abstract": "We present FRaN-X, a Framing and Narratives Explorer that automatically\ndetects entity mentions and classifies their narrative roles directly from raw\ntext. FRaN-X comprises a two-stage system that combines sequence labeling with\nfine-grained role classification to reveal how entities are portrayed as\nprotagonists, antagonists, or innocents, using a unique taxonomy of 22\nfine-grained roles nested under these three main categories. The system\nsupports five languages (Bulgarian, English, Hindi, Russian, and Portuguese)\nand two domains (the Russia-Ukraine Conflict and Climate Change). It provides\nan interactive web interface for media analysts to explore and compare framing\nacross different sources, tackling the challenge of automatically detecting and\nlabeling how entities are framed. Our system allows end users to focus on a\nsingle article as well as analyze up to four articles simultaneously. We\nprovide aggregate level analysis including an intuitive graph visualization\nthat highlights the narrative a group of articles are pushing. Our system\nincludes a search feature for users to look up entities of interest, along with\na timeline view that allows analysts to track an entity's role transitions\nacross different contexts within the article. The FRaN-X system and the trained\nmodels are licensed under an MIT License. FRaN-X is publicly accessible at\nhttps://fran-x.streamlit.app/ and a video demonstration is available at\nhttps://youtu.be/VZVi-1B6yYk.",
        "url": "http://arxiv.org/abs/2507.06974v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06974v1",
        "arxiv_id": "2507.06974v1",
        "authors": [
            "Artur Muratov",
            "Hana Fatima Shaikh",
            "Vanshikaa Jani",
            "Tarek Mahmoud",
            "Zhuohan Xie",
            "Daniil Orel",
            "Aaryamonvikram Singh",
            "Yuxia Wang",
            "Aadi Joshi",
            "Hasan Iqbal",
            "Ming Shan Hee",
            "Dhruv Sahnan",
            "Nikolaos Nikolaidis",
            "Purificação Silvano",
            "Dimitar Dimitrov",
            "Roman Yangarber",
            "Ricardo Campos",
            "Alípio Jorge",
            "Nuno Guimarães",
            "Elisa Sartori",
            "Nicolas Stefanovitch",
            "Giovanni Da San Martino",
            "Jakub Piskorski",
            "Preslav Nakov"
        ],
        "submitted": "2025-07-09 16:04:51",
        "source": "arxiv",
        "comment": "19 pages, 13 figures, submitted to EMNLP 2025 - Demo Track"
    },
    {
        "title": "Scaling Towards the Information Boundary of Instruction Set: InfinityInstruct-Subject Technical Report",
        "abstract": "Instruction tuning has become a foundation for unlocking the capabilities of\nlarge-scale pretrained models and improving their performance on complex tasks.\nThus, the construction of high-quality instruction datasets is crucial for\nenhancing model performance and generalizability. Although current instruction\ndatasets have reached tens of millions of samples, models finetuned on them may\nstill struggle with complex instruction following and tasks in rare domains.\nThis is primarily due to limited expansion in both ``coverage'' (coverage of\ntask types and knowledge areas) and ``depth'' (instruction complexity) of the\ninstruction set. To address this issue, we propose a systematic instruction\ndata construction framework, which integrates a hierarchical labeling system,\nan informative seed selection algorithm, an evolutionary data synthesis\nprocess, and a model deficiency diagnosis with targeted data generation. These\ncomponents form an iterative closed-loop to continuously enhance the coverage\nand depth of instruction data. Based on this framework, we construct\nInfinityInstruct-Subject, a high-quality dataset containing ~1.5 million\ninstructions. Experiments on multiple foundation models and benchmark tasks\ndemonstrate its effectiveness in improving instruction-following capabilities.\nFurther analyses suggest that InfinityInstruct-Subject shows enlarged coverage\nand depth compared to comparable synthesized instruction datasets. Our work\nlays a theoretical and practical foundation for the efficient, continuous\nevolution of instruction datasets, moving from data quantity expansion to\nqualitative improvement.",
        "url": "http://arxiv.org/abs/2507.06968v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06968v1",
        "arxiv_id": "2507.06968v1",
        "authors": [
            "Li Du",
            "Hanyu Zhao",
            "Yiming Ju",
            "Tengfei Pan"
        ],
        "submitted": "2025-07-09 15:59:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Investigating the Robustness of Retrieval-Augmented Generation at the Query Level",
        "abstract": "Large language models (LLMs) are very costly and inefficient to update with\nnew information. To address this limitation, retrieval-augmented generation\n(RAG) has been proposed as a solution that dynamically incorporates external\nknowledge during inference, improving factual consistency and reducing\nhallucinations. Despite its promise, RAG systems face practical challenges-most\nnotably, a strong dependence on the quality of the input query for accurate\nretrieval. In this paper, we investigate the sensitivity of different\ncomponents in the RAG pipeline to various types of query perturbations. Our\nanalysis reveals that the performance of commonly used retrievers can degrade\nsignificantly even under minor query variations. We study each module in\nisolation as well as their combined effect in an end-to-end question answering\nsetting, using both general-domain and domain-specific datasets. Additionally,\nwe propose an evaluation framework to systematically assess the query-level\nrobustness of RAG pipelines and offer actionable recommendations for\npractitioners based on the results of more than 1092 experiments we performed.",
        "url": "http://arxiv.org/abs/2507.06956v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06956v1",
        "arxiv_id": "2507.06956v1",
        "authors": [
            "Sezen Perçin",
            "Xin Su",
            "Qutub Sha Syed",
            "Phillip Howard",
            "Aleksei Kuvshinov",
            "Leo Schwinn",
            "Kay-Ulrich Scholl"
        ],
        "submitted": "2025-07-09 15:39:17",
        "source": "arxiv",
        "comment": "Accepted to Generation, Evaluation & Metrics (GEM) Workshop at ACL\n  2025"
    },
    {
        "title": "Rethinking Verification for LLM Code Generation: From Generation to Testing",
        "abstract": "Large language models (LLMs) have recently achieved notable success in\ncode-generation benchmarks such as HumanEval and LiveCodeBench. However, a\ndetailed examination reveals that these evaluation suites often comprise only a\nlimited number of homogeneous test cases, resulting in subtle faults going\nundetected. This not only artificially inflates measured performance but also\ncompromises accurate reward estimation in reinforcement learning frameworks\nutilizing verifiable rewards (RLVR). To address these critical shortcomings, we\nsystematically investigate the test-case generation (TCG) task by proposing\nmulti-dimensional metrics designed to rigorously quantify test-suite\nthoroughness. Furthermore, we introduce a human-LLM collaborative method\n(SAGA), leveraging human programming expertise with LLM reasoning capability,\naimed at significantly enhancing both the coverage and the quality of generated\ntest cases. In addition, we develop a TCGBench to facilitate the study of the\nTCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a\nverifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)\nof the code generation evaluation benchmark synthesized by SAGA is 10.78%\nhigher than that of LiveCodeBench-v6. These results demonstrate the\neffectiveness of our proposed method. We hope this work contributes to building\na scalable foundation for reliable LLM code evaluation, further advancing RLVR\nin code generation, and paving the way for automated adversarial test synthesis\nand adaptive benchmark integration.",
        "url": "http://arxiv.org/abs/2507.06920v2",
        "pdf_url": "http://arxiv.org/pdf/2507.06920v2",
        "arxiv_id": "2507.06920v2",
        "authors": [
            "Zihan Ma",
            "Taolin Zhang",
            "Maosong Cao",
            "Junnan Liu",
            "Wenwei Zhang",
            "Minnan Luo",
            "Songyang Zhang",
            "Kai Chen"
        ],
        "submitted": "2025-07-09 14:58:47",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Exploring LLMs for Predicting Tutor Strategy and Student Outcomes in Dialogues",
        "abstract": "Tutoring dialogues have gained significant attention in recent years, given\nthe prominence of online learning and the emerging tutoring abilities of\nartificial intelligence (AI) agents powered by large language models (LLMs).\nRecent studies have shown that the strategies used by tutors can have\nsignificant effects on student outcomes, necessitating methods to predict how\ntutors will behave and how their actions impact students. However, few works\nhave studied predicting tutor strategy in dialogues. Therefore, in this work we\ninvestigate the ability of modern LLMs, particularly Llama 3 and GPT-4o, to\npredict both future tutor moves and student outcomes in dialogues, using two\nmath tutoring dialogue datasets. We find that even state-of-the-art LLMs\nstruggle to predict future tutor strategy while tutor strategy is highly\nindicative of student outcomes, outlining a need for more powerful methods to\napproach this task.",
        "url": "http://arxiv.org/abs/2507.06910v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06910v1",
        "arxiv_id": "2507.06910v1",
        "authors": [
            "Fareya Ikram",
            "Alexander Scarlatos",
            "Andrew Lan"
        ],
        "submitted": "2025-07-09 14:47:35",
        "source": "arxiv",
        "comment": "Published in BEA 2025: 20th Workshop on Innovative Use of NLP for\n  Building Educational Applications"
    },
    {
        "title": "MultiJustice: A Chinese Dataset for Multi-Party, Multi-Charge Legal Prediction",
        "abstract": "Legal judgment prediction offers a compelling method to aid legal\npractitioners and researchers. However, the research question remains\nrelatively under-explored: Should multiple defendants and charges be treated\nseparately in LJP? To address this, we introduce a new dataset namely\nmulti-person multi-charge prediction (MPMCP), and seek the answer by evaluating\nthe performance of several prevailing legal large language models (LLMs) on\nfour practical legal judgment scenarios: (S1) single defendant with a single\ncharge, (S2) single defendant with multiple charges, (S3) multiple defendants\nwith a single charge, and (S4) multiple defendants with multiple charges. We\nevaluate the dataset across two LJP tasks, i.e., charge prediction and penalty\nterm prediction. We have conducted extensive experiments and found that the\nscenario involving multiple defendants and multiple charges (S4) poses the\ngreatest challenges, followed by S2, S3, and S1. The impact varies\nsignificantly depending on the model. For example, in S4 compared to S1,\nInternLM2 achieves approximately 4.5% lower F1-score and 2.8% higher LogD,\nwhile Lawformer demonstrates around 19.7% lower F1-score and 19.0% higher LogD.\nOur dataset and code are available at\nhttps://github.com/lololo-xiao/MultiJustice-MPMCP.",
        "url": "http://arxiv.org/abs/2507.06909v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06909v1",
        "arxiv_id": "2507.06909v1",
        "authors": [
            "Xiao Wang",
            "Jiahuan Pei",
            "Diancheng Shui",
            "Zhiguang Han",
            "Xin Sun",
            "Dawei Zhu",
            "Xiaoyu Shen"
        ],
        "submitted": "2025-07-09 14:47:00",
        "source": "arxiv",
        "comment": "Accepted by NLPCC 2025"
    },
    {
        "title": "MIND: A Multi-agent Framework for Zero-shot Harmful Meme Detection",
        "abstract": "The rapid expansion of memes on social media has highlighted the urgent need\nfor effective approaches to detect harmful content. However, traditional\ndata-driven approaches struggle to detect new memes due to their evolving\nnature and the lack of up-to-date annotated data. To address this issue, we\npropose MIND, a multi-agent framework for zero-shot harmful meme detection that\ndoes not rely on annotated data. MIND implements three key strategies: 1) We\nretrieve similar memes from an unannotated reference set to provide contextual\ninformation. 2) We propose a bi-directional insight derivation mechanism to\nextract a comprehensive understanding of similar memes. 3) We then employ a\nmulti-agent debate mechanism to ensure robust decision-making through reasoned\narbitration. Extensive experiments on three meme datasets demonstrate that our\nproposed framework not only outperforms existing zero-shot approaches but also\nshows strong generalization across different model architectures and parameter\nscales, providing a scalable solution for harmful meme detection. The code is\navailable at https://github.com/destroy-lonely/MIND.",
        "url": "http://arxiv.org/abs/2507.06908v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06908v1",
        "arxiv_id": "2507.06908v1",
        "authors": [
            "Ziyan Liu",
            "Chunxiao Fan",
            "Haoran Lou",
            "Yuexin Wu",
            "Kaiwei Deng"
        ],
        "submitted": "2025-07-09 14:46:32",
        "source": "arxiv",
        "comment": "ACL 2025"
    },
    {
        "title": "VisualTrap: A Stealthy Backdoor Attack on GUI Agents via Visual Grounding Manipulation",
        "abstract": "Graphical User Interface (GUI) agents powered by Large Vision-Language Models\n(LVLMs) have emerged as a revolutionary approach to automating human-machine\ninteractions, capable of autonomously operating personal devices (e.g., mobile\nphones) or applications within the device to perform complex real-world tasks\nin a human-like manner. However, their close integration with personal devices\nraises significant security concerns, with many threats, including backdoor\nattacks, remaining largely unexplored. This work reveals that the visual\ngrounding of GUI agent-mapping textual plans to GUI elements-can introduce\nvulnerabilities, enabling new types of backdoor attacks. With backdoor attack\ntargeting visual grounding, the agent's behavior can be compromised even when\ngiven correct task-solving plans. To validate this vulnerability, we propose\nVisualTrap, a method that can hijack the grounding by misleading the agent to\nlocate textual plans to trigger locations instead of the intended targets.\nVisualTrap uses the common method of injecting poisoned data for attacks, and\ndoes so during the pre-training of visual grounding to ensure practical\nfeasibility of attacking. Empirical results show that VisualTrap can\neffectively hijack visual grounding with as little as 5% poisoned data and\nhighly stealthy visual triggers (invisible to the human eye); and the attack\ncan be generalized to downstream tasks, even after clean fine-tuning. Moreover,\nthe injected trigger can remain effective across different GUI environments,\ne.g., being trained on mobile/web and generalizing to desktop environments.\nThese findings underscore the urgent need for further research on backdoor\nattack risks in GUI agents.",
        "url": "http://arxiv.org/abs/2507.06899v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06899v1",
        "arxiv_id": "2507.06899v1",
        "authors": [
            "Ziang Ye",
            "Yang Zhang",
            "Wentao Shi",
            "Xiaoyu You",
            "Fuli Feng",
            "Tat-Seng Chua"
        ],
        "submitted": "2025-07-09 14:36:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SCoRE: Streamlined Corpus-based Relation Extraction using Multi-Label Contrastive Learning and Bayesian kNN",
        "abstract": "The growing demand for efficient knowledge graph (KG) enrichment leveraging\nexternal corpora has intensified interest in relation extraction (RE),\nparticularly under low-supervision settings. To address the need for adaptable\nand noise-resilient RE solutions that integrate seamlessly with pre-trained\nlarge language models (PLMs), we introduce SCoRE, a modular and cost-effective\nsentence-level RE system. SCoRE enables easy PLM switching, requires no\nfinetuning, and adapts smoothly to diverse corpora and KGs. By combining\nsupervised contrastive learning with a Bayesian k-Nearest Neighbors (kNN)\nclassifier for multi-label classification, it delivers robust performance\ndespite the noisy annotations of distantly supervised corpora. To improve RE\nevaluation, we propose two novel metrics: Correlation Structure Distance (CSD),\nmeasuring the alignment between learned relational patterns and KG structures,\nand Precision at R (P@R), assessing utility as a recommender system. We also\nrelease Wiki20d, a benchmark dataset replicating real-world RE conditions where\nonly KG-derived annotations are available. Experiments on five benchmarks show\nthat SCoRE matches or surpasses state-of-the-art methods while significantly\nreducing energy consumption. Further analyses reveal that increasing model\ncomplexity, as seen in prior work, degrades performance, highlighting the\nadvantages of SCoRE's minimal design. Combining efficiency, modularity, and\nscalability, SCoRE stands as an optimal choice for real-world RE applications.",
        "url": "http://arxiv.org/abs/2507.06895v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06895v1",
        "arxiv_id": "2507.06895v1",
        "authors": [
            "Luca Mariotti",
            "Veronica Guidetti",
            "Federica Mandreoli"
        ],
        "submitted": "2025-07-09 14:33:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Developing and Maintaining an Open-Source Repository of AI Evaluations: Challenges and Insights",
        "abstract": "AI evaluations have become critical tools for assessing large language model\ncapabilities and safety. This paper presents practical insights from eight\nmonths of maintaining $inspect\\_evals$, an open-source repository of 70+\ncommunity-contributed AI evaluations. We identify key challenges in\nimplementing and maintaining AI evaluations and develop solutions including:\n(1) a structured cohort management framework for scaling community\ncontributions, (2) statistical methodologies for optimal resampling and\ncross-model comparison with uncertainty quantification, and (3) systematic\nquality control processes for reproducibility. Our analysis reveals that AI\nevaluation requires specialized infrastructure, statistical rigor, and\ncommunity coordination beyond traditional software development practices.",
        "url": "http://arxiv.org/abs/2507.06893v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06893v1",
        "arxiv_id": "2507.06893v1",
        "authors": [
            "Alexandra Abbas",
            "Celia Waggoner",
            "Justin Olive"
        ],
        "submitted": "2025-07-09 14:30:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Squeeze the Soaked Sponge: Efficient Off-policy Reinforcement Finetuning for Large Language Model",
        "abstract": "Reinforcement Learning (RL) has demonstrated its potential to improve the\nreasoning ability of Large Language Models (LLMs). One major limitation of most\nexisting Reinforcement Finetuning (RFT) methods is that they are on-policy RL\nin nature, i.e., data generated during the past learning process is not fully\nutilized. This inevitably comes at a significant cost of compute and time,\nposing a stringent bottleneck on continuing economic and efficient scaling. To\nthis end, we launch the renaissance of off-policy RL and propose Reincarnating\nMix-policy Proximal Policy Gradient (ReMix), a general approach to enable\non-policy RFT methods like PPO and GRPO to leverage off-policy data. ReMix\nconsists of three major components: (1) Mix-policy proximal policy gradient\nwith an increased Update-To-Data (UTD) ratio for efficient training; (2)\nKL-Convex policy constraint to balance the trade-off between stability and\nflexibility; (3) Policy reincarnation to achieve a seamless transition from\nefficient early-stage learning to steady asymptotic improvement. In our\nexperiments, we train a series of ReMix models upon PPO, GRPO and 1.5B, 7B base\nmodels. ReMix shows an average Pass@1 accuracy of 52.10% (for 1.5B model) with\n0.079M response rollouts, 350 training steps and achieves 63.27%/64.39% (for 7B\nmodel) with 0.007M/0.011M response rollouts, 50/75 training steps, on five math\nreasoning benchmarks (i.e., AIME'24, AMC'23, Minerva, OlympiadBench, and\nMATH500). Compared with 15 recent advanced models, ReMix shows SOTA-level\nperformance with an over 30x to 450x reduction in training cost in terms of\nrollout data volume. In addition, we reveal insightful findings via\nmultifaceted analysis, including the implicit preference for shorter responses\ndue to the Whipping Effect of off-policy discrepancy, the collapse mode of\nself-reflection behavior under the presence of severe off-policyness, etc.",
        "url": "http://arxiv.org/abs/2507.06892v2",
        "pdf_url": "http://arxiv.org/pdf/2507.06892v2",
        "arxiv_id": "2507.06892v2",
        "authors": [
            "Jing Liang",
            "Hongyao Tang",
            "Yi Ma",
            "Jinyi Liu",
            "Yan Zheng",
            "Shuyue Hu",
            "Lei Bai",
            "Jianye Hao"
        ],
        "submitted": "2025-07-09 14:29:45",
        "source": "arxiv",
        "comment": "Preliminary version, v2, added more details and corrected some minor\n  mistakes. Project page: https://anitaleungxx.github.io/ReMix"
    },
    {
        "title": "CDC: Causal Domain Clustering for Multi-Domain Recommendation",
        "abstract": "Multi-domain recommendation leverages domain-general knowledge to improve\nrecommendations across several domains. However, as platforms expand to dozens\nor hundreds of scenarios, training all domains in a unified model leads to\nperformance degradation due to significant inter-domain differences. Existing\ndomain grouping methods, based on business logic or data similarities, often\nfail to capture the true transfer relationships required for optimal grouping.\nTo effectively cluster domains, we propose Causal Domain Clustering (CDC). CDC\nmodels domain transfer patterns within a large number of domains using two\ndistinct effects: the Isolated Domain Affinity Matrix for modeling\nnon-interactive domain transfers, and the Hybrid Domain Affinity Matrix for\nconsidering dynamic domain synergy or interference under joint training. To\nintegrate these two transfer effects, we introduce causal discovery to\ncalculate a cohesion-based coefficient that adaptively balances their\ncontributions. A Co-Optimized Dynamic Clustering algorithm iteratively\noptimizes target domain clustering and source domain selection for training.\nCDC significantly enhances performance across over 50 domains on public\ndatasets and in industrial settings, achieving a 4.9% increase in online eCPM.\nCode is available at\nhttps://github.com/Chrissie-Law/Causal-Domain-Clustering-for-Multi-Domain-Recommendation",
        "url": "http://arxiv.org/abs/2507.06877v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06877v1",
        "arxiv_id": "2507.06877v1",
        "authors": [
            "Huishi Luo",
            "Yiqing Wu",
            "Yiwen Chen",
            "Fuzhen Zhuang",
            "Deqing Wang"
        ],
        "submitted": "2025-07-09 14:15:47",
        "source": "arxiv",
        "comment": "Accepted at SIGIR 2025"
    },
    {
        "title": "Shifting from Ranking to Set Selection for Retrieval Augmented Generation",
        "abstract": "Retrieval in Retrieval-Augmented Generation(RAG) must ensure that retrieved\npassages are not only individually relevant but also collectively form a\ncomprehensive set. Existing approaches primarily rerank top-k passages based on\ntheir individual relevance, often failing to meet the information needs of\ncomplex queries in multi-hop question answering. In this work, we propose a\nset-wise passage selection approach and introduce SETR, which explicitly\nidentifies the information requirements of a query through Chain-of-Thought\nreasoning and selects an optimal set of passages that collectively satisfy\nthose requirements. Experiments on multi-hop RAG benchmarks show that SETR\noutperforms both proprietary LLM-based rerankers and open-source baselines in\nterms of answer correctness and retrieval quality, providing an effective and\nefficient alternative to traditional rerankers in RAG systems. The code is\navailable at https://github.com/LGAI-Research/SetR",
        "url": "http://arxiv.org/abs/2507.06838v2",
        "pdf_url": "http://arxiv.org/pdf/2507.06838v2",
        "arxiv_id": "2507.06838v2",
        "authors": [
            "Dahyun Lee",
            "Yongrae Jo",
            "Haeju Park",
            "Moontae Lee"
        ],
        "submitted": "2025-07-09 13:35:36",
        "source": "arxiv",
        "comment": "Accepted to ACL 2025 main (Oral Presentation)"
    },
    {
        "title": "Adaptive Termination for Multi-round Parallel Reasoning: An Universal Semantic Entropy-Guided Framework",
        "abstract": "Recent advances in large language models (LLMs) have accelerated progress\ntoward artificial general intelligence, with inference-time scaling emerging as\na key technique. Contemporary approaches leverage either sequential reasoning\n(iteratively extending chains of thought) or parallel reasoning (generating\nmultiple solutions simultaneously) to scale inference. However, both paradigms\nface fundamental limitations: sequential scaling typically relies on arbitrary\ntoken budgets for termination, leading to inefficiency or premature cutoff;\nwhile parallel scaling often lacks coordination among parallel branches and\nrequires intrusive fine-tuning to perform effectively. In light of these\nchallenges, we aim to design a flexible test-time collaborative inference\nframework that exploits the complementary strengths of both sequential and\nparallel reasoning paradigms. Towards this goal, the core challenge lies in\ndeveloping an efficient and accurate intrinsic quality metric to assess model\nresponses during collaborative inference, enabling dynamic control and early\ntermination of the reasoning trace. To address this challenge, we introduce\nsemantic entropy (SE), which quantifies the semantic diversity of parallel\nmodel responses and serves as a robust indicator of reasoning quality due to\nits strong negative correlation with accuracy...",
        "url": "http://arxiv.org/abs/2507.06829v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06829v1",
        "arxiv_id": "2507.06829v1",
        "authors": [
            "Zenan Xu",
            "Zexuan Qiu",
            "Guanhua Huang",
            "Kun Li",
            "Siheng Li",
            "Chenchen Zhang",
            "Kejiao Li",
            "Qi Yi",
            "Yuhao Jiang",
            "Bo Zhou",
            "Fengzong Lian",
            "Zhanhui Kang"
        ],
        "submitted": "2025-07-09 13:28:35",
        "source": "arxiv",
        "comment": "13 pages, 5 fiures"
    },
    {
        "title": "Text to model via SysML: Automated generation of dynamical system computational models from unstructured natural language text via enhanced System Modeling Language diagrams",
        "abstract": "This paper contributes to speeding up the design and deployment of\nengineering dynamical systems by proposing a strategy for exploiting domain and\nexpert knowledge for the automated generation of dynamical system computational\nmodel starting from a corpus of document relevant to the dynamical system of\ninterest and an input document describing the specific system. This strategy is\nimplemented in five steps and, crucially, it uses system modeling language\ndiagrams (SysML) to extract accurate information about the dependencies,\nattributes, and operations of components. Natural Language Processing (NLP)\nstrategies and Large Language Models (LLMs) are employed in specific tasks to\nimprove intermediate outputs of the SySML diagrams automated generation, such\nas: list of key nouns; list of extracted relationships; list of key phrases and\nkey relationships; block attribute values; block relationships; and BDD diagram\ngeneration. The applicability of automated SysML diagram generation is\nillustrated with different case studies. The computational models of complex\ndynamical systems from SysML diagrams are then obtained via code generation and\ncomputational model generation steps. In the code generation step, NLP\nstrategies are used for summarization, while LLMs are used for validation only.\nThe proposed approach is not limited to a specific system, domain, or\ncomputational software. The applicability of the proposed approach is shown via\nan end-to-end example from text to model of a simple pendulum, showing improved\nperformance compared to results yielded by LLMs only.",
        "url": "http://arxiv.org/abs/2507.06803v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06803v1",
        "arxiv_id": "2507.06803v1",
        "authors": [
            "Matthew Anderson Hendricks",
            "Alice Cicirello"
        ],
        "submitted": "2025-07-09 12:44:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ixi-GEN: Efficient Industrial sLLMs through Domain Adaptive Continual Pretraining",
        "abstract": "The emergence of open-source large language models (LLMs) has expanded\nopportunities for enterprise applications; however, many organizations still\nlack the infrastructure to deploy and maintain large-scale models. As a result,\nsmall LLMs (sLLMs) have become a practical alternative, despite their inherent\nperformance limitations. While Domain Adaptive Continual Pretraining (DACP) has\nbeen previously explored as a method for domain adaptation, its utility in\ncommercial applications remains under-examined. In this study, we validate the\neffectiveness of applying a DACP-based recipe across diverse foundation models\nand service domains. Through extensive experiments and real-world evaluations,\nwe demonstrate that DACP-applied sLLMs achieve substantial gains in target\ndomain performance while preserving general capabilities, offering a\ncost-efficient and scalable solution for enterprise-level deployment.",
        "url": "http://arxiv.org/abs/2507.06795v2",
        "pdf_url": "http://arxiv.org/pdf/2507.06795v2",
        "arxiv_id": "2507.06795v2",
        "authors": [
            "Seonwu Kim",
            "Yohan Na",
            "Kihun Kim",
            "Hanhee Cho",
            "Geun Lim",
            "Mintae Kim",
            "Seongik Park",
            "Ki Hyun Kim",
            "Youngsub Han",
            "Byoung-Ki Jeon"
        ],
        "submitted": "2025-07-09 12:30:42",
        "source": "arxiv",
        "comment": "under review"
    },
    {
        "title": "Temporal Information Retrieval via Time-Specifier Model Merging",
        "abstract": "The rapid expansion of digital information and knowledge across structured\nand unstructured sources has heightened the importance of Information Retrieval\n(IR). While dense retrieval methods have substantially improved semantic\nmatching for general queries, they consistently underperform on queries with\nexplicit temporal constraints--often those containing numerical expressions and\ntime specifiers such as ``in 2015.'' Existing approaches to Temporal\nInformation Retrieval (TIR) improve temporal reasoning but often suffer from\ncatastrophic forgetting, leading to reduced performance on non-temporal\nqueries. To address this, we propose Time-Specifier Model Merging (TSM), a\nnovel method that enhances temporal retrieval while preserving accuracy on\nnon-temporal queries. TSM trains specialized retrievers for individual time\nspecifiers and merges them in to a unified model, enabling precise handling of\ntemporal constraints without compromising non-temporal retrieval. Extensive\nexperiments on both temporal and non-temporal datasets demonstrate that TSM\nsignificantly improves performance on temporally constrained queries while\nmaintaining strong results on non-temporal queries, consistently outperforming\nother baseline methods. Our code is available at\nhttps://github.com/seungyoonee/TSM .",
        "url": "http://arxiv.org/abs/2507.06782v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06782v1",
        "arxiv_id": "2507.06782v1",
        "authors": [
            "SeungYoon Han",
            "Taeho Hwang",
            "Sukmin Cho",
            "Soyeong Jeong",
            "Hoyun Song",
            "Huije Lee",
            "Jong C. Park"
        ],
        "submitted": "2025-07-09 12:16:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Checklist Engineering Empowers Multilingual LLM Judges",
        "abstract": "Automated text evaluation has long been a central issue in Natural Language\nProcessing (NLP). Recently, the field has shifted toward using Large Language\nModels (LLMs) as evaluators-a trend known as the LLM-as-a-Judge paradigm. While\npromising and easily adaptable across tasks, this approach has seen limited\nexploration in multilingual contexts. Existing multilingual studies often rely\non proprietary models or require extensive training data for fine-tuning,\nraising concerns about cost, time, and efficiency. In this paper, we propose\nChecklist Engineering based LLM-as-a-Judge (CE-Judge), a training-free\nframework that uses checklist intuition for multilingual evaluation with an\nopen-source model. Experiments across multiple languages and three benchmark\ndatasets, under both pointwise and pairwise settings, show that our method\ngenerally surpasses the baselines and performs on par with the GPT-4o model.",
        "url": "http://arxiv.org/abs/2507.06774v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06774v1",
        "arxiv_id": "2507.06774v1",
        "authors": [
            "Mohammad Ghiasvand Mohammadkhani",
            "Hamid Beigy"
        ],
        "submitted": "2025-07-09 12:03:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "KAConvText: Novel Approach to Burmese Sentence Classification using Kolmogorov-Arnold Convolution",
        "abstract": "This paper presents the first application of Kolmogorov-Arnold Convolution\nfor Text (KAConvText) in sentence classification, addressing three tasks:\nimbalanced binary hate speech detection, balanced multiclass news\nclassification, and imbalanced multiclass ethnic language identification. We\ninvestigate various embedding configurations, comparing random to fastText\nembeddings in both static and fine-tuned settings, with embedding dimensions of\n100 and 300 using CBOW and Skip-gram models. Baselines include standard CNNs\nand CNNs augmented with a Kolmogorov-Arnold Network (CNN-KAN). In addition, we\ninvestigated KAConvText with different classification heads - MLP and KAN,\nwhere using KAN head supports enhanced interpretability. Results show that\nKAConvText-MLP with fine-tuned fastText embeddings achieves the best\nperformance of 91.23% accuracy (F1-score = 0.9109) for hate speech detection,\n92.66% accuracy (F1-score = 0.9267) for news classification, and 99.82%\naccuracy (F1-score = 0.9982) for language identification.",
        "url": "http://arxiv.org/abs/2507.06753v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06753v1",
        "arxiv_id": "2507.06753v1",
        "authors": [
            "Ye Kyaw Thu",
            "Thura Aung",
            "Thazin Myint Oo",
            "Thepchai Supnithi"
        ],
        "submitted": "2025-07-09 11:25:35",
        "source": "arxiv",
        "comment": "10 pages, 3 figures, 4 tables"
    },
    {
        "title": "Robust Multimodal Large Language Models Against Modality Conflict",
        "abstract": "Despite the impressive capabilities of multimodal large language models\n(MLLMs) in vision-language tasks, they are prone to hallucinations in\nreal-world scenarios. This paper investigates the hallucination phenomenon in\nMLLMs from the perspective of modality conflict. Unlike existing works focusing\non the conflicts between model responses and inputs, we study the inherent\nconflicts in inputs from different modalities that place MLLMs in a dilemma and\ndirectly lead to hallucinations. We formally define the modality conflict and\nconstruct a dataset named Multimodal Modality Conflict (MMMC) to simulate this\nphenomenon in vision-language tasks. Three methods based on prompt engineering,\nsupervised fine-tuning, and reinforcement learning are proposed to alleviate\nthe hallucination caused by modality conflict. Extensive experiments are\nconducted on the MMMC dataset to analyze the merits and demerits of these\nmethods. Our results show that the reinforcement learning method achieves the\nbest performance in mitigating the hallucination under modality conflict, while\nthe supervised fine-tuning method shows promising and stable performance. Our\nwork sheds light on the unnoticed modality conflict that leads to\nhallucinations and provides more insights into the robustness of MLLMs.",
        "url": "http://arxiv.org/abs/2507.07151v1",
        "pdf_url": "http://arxiv.org/pdf/2507.07151v1",
        "arxiv_id": "2507.07151v1",
        "authors": [
            "Zongmeng Zhang",
            "Wengang Zhou",
            "Jie Zhao",
            "Houqiang Li"
        ],
        "submitted": "2025-07-09 11:18:38",
        "source": "arxiv",
        "comment": "ICML 2025"
    },
    {
        "title": "Civil Society in the Loop: Feedback-Driven Adaptation of (L)LM-Assisted Classification in an Open-Source Telegram Monitoring Tool",
        "abstract": "The role of civil society organizations (CSOs) in monitoring harmful online\ncontent is increasingly crucial, especially as platform providers reduce their\ninvestment in content moderation. AI tools can assist in detecting and\nmonitoring harmful content at scale. However, few open-source tools offer\nseamless integration of AI models and social media monitoring infrastructures.\nGiven their thematic expertise and contextual understanding of harmful content,\nCSOs should be active partners in co-developing technological tools, providing\nfeedback, helping to improve models, and ensuring alignment with stakeholder\nneeds and values, rather than as passive 'consumers'. However, collaborations\nbetween the open source community, academia, and civil society remain rare, and\nresearch on harmful content seldom translates into practical tools usable by\ncivil society actors. This work in progress explores how CSOs can be\nmeaningfully involved in an AI-assisted open-source monitoring tool of\nanti-democratic movements on Telegram, which we are currently developing in\ncollaboration with CSO stakeholders.",
        "url": "http://arxiv.org/abs/2507.06734v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06734v1",
        "arxiv_id": "2507.06734v1",
        "authors": [
            "Milena Pustet",
            "Elisabeth Steffen",
            "Helena Mihaljević",
            "Grischa Stanjek",
            "Yannis Illies"
        ],
        "submitted": "2025-07-09 10:46:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "On the Effect of Uncertainty on Layer-wise Inference Dynamics",
        "abstract": "Understanding how large language models (LLMs) internally represent and\nprocess their predictions is central to detecting uncertainty and preventing\nhallucinations. While several studies have shown that models encode uncertainty\nin their hidden states, it is underexplored how this affects the way they\nprocess such hidden states. In this work, we demonstrate that the dynamics of\noutput token probabilities across layers for certain and uncertain outputs are\nlargely aligned, revealing that uncertainty does not seem to affect inference\ndynamics. Specifically, we use the Tuned Lens, a variant of the Logit Lens, to\nanalyze the layer-wise probability trajectories of final prediction tokens\nacross 11 datasets and 5 models. Using incorrect predictions as those with\nhigher epistemic uncertainty, our results show aligned trajectories for certain\nand uncertain predictions that both observe abrupt increases in confidence at\nsimilar layers. We balance this finding by showing evidence that more competent\nmodels may learn to process uncertainty differently. Our findings challenge the\nfeasibility of leveraging simplistic methods for detecting uncertainty at\ninference. More broadly, our work demonstrates how interpretability methods may\nbe used to investigate the way uncertainty affects inference.",
        "url": "http://arxiv.org/abs/2507.06722v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06722v1",
        "arxiv_id": "2507.06722v1",
        "authors": [
            "Sunwoo Kim",
            "Haneul Yoo",
            "Alice Oh"
        ],
        "submitted": "2025-07-09 10:30:09",
        "source": "arxiv",
        "comment": "Accepted to Actionable Interpretability Workshop - ICML 2025"
    },
    {
        "title": "CLI-RAG: A Retrieval-Augmented Framework for Clinically Structured and Context Aware Text Generation with LLMs",
        "abstract": "Large language models (LLMs), including zero-shot and few-shot paradigms,\nhave shown promising capabilities in clinical text generation. However,\nreal-world applications face two key challenges: (1) patient data is highly\nunstructured, heterogeneous, and scattered across multiple note types and (2)\nclinical notes are often long and semantically dense, making naive prompting\ninfeasible due to context length constraints and the risk of omitting\nclinically relevant information.\n  We introduce CLI-RAG (Clinically Informed Retrieval-Augmented Generation), a\ndomain-specific framework for structured and clinically grounded text\ngeneration using LLMs. It incorporates a novel hierarchical chunking strategy\nthat respects clinical document structure and introduces a task-specific\ndual-stage retrieval mechanism. The global stage identifies relevant note types\nusing evidence-based queries, while the local stage extracts high-value content\nwithin those notes creating relevance at both document and section levels.\n  We apply the system to generate structured progress notes for individual\nhospital visits using 15 clinical note types from the MIMIC-III dataset.\nExperiments show that it preserves temporal and semantic alignment across\nvisits, achieving an average alignment score of 87.7%, surpassing the 80.7%\nbaseline from real clinician-authored notes. The generated outputs also\ndemonstrate high consistency across LLMs, reinforcing deterministic behavior\nessential for reproducibility, reliability, and clinical trust.",
        "url": "http://arxiv.org/abs/2507.06715v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06715v1",
        "arxiv_id": "2507.06715v1",
        "authors": [
            "Garapati Keerthana",
            "Manik Gupta"
        ],
        "submitted": "2025-07-09 10:13:38",
        "source": "arxiv",
        "comment": "12 pages, 4 figures"
    },
    {
        "title": "Elite Polarization in European Parliamentary Speeches: a Novel Measurement Approach Using Large Language Models",
        "abstract": "This project introduces a new measure of elite polarization via actor and\nsubject detection using artificial intelligence. I identify when politicians\nmention one another in parliamentary speeches, note who is speaking and who is\nbeing addressed, and assess the emotional temperature behind these evaluations.\nThis maps how elites evaluate their various out-parties, allowing us to create\nan index of mutual out-party hostility, that is, elite polarization. While I\nanalyzed polarization data over the past four decades for the UK, and two\ndecades for Hungary and Italy, my approach lays the groundwork for a\ntwenty-year, EU-wide time-series dataset on elite polarization. I obtain the\nresults that can be aggregated by party and quarter. The resulting index\ndemonstrates a good face validity: it reacts to events such as electoral\ncampaigns, country- and party-level crises, and to parties losing and assuming\npower.",
        "url": "http://arxiv.org/abs/2507.06658v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06658v1",
        "arxiv_id": "2507.06658v1",
        "authors": [
            "Gennadii Iakovlev"
        ],
        "submitted": "2025-07-09 08:44:29",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MS-DPPs: Multi-Source Determinantal Point Processes for Contextual Diversity Refinement of Composite Attributes in Text to Image Retrieval",
        "abstract": "Result diversification (RD) is a crucial technique in Text-to-Image Retrieval\nfor enhancing the efficiency of a practical application. Conventional methods\nfocus solely on increasing the diversity metric of image appearances. However,\nthe diversity metric and its desired value vary depending on the application,\nwhich limits the applications of RD. This paper proposes a novel task called\nCDR-CA (Contextual Diversity Refinement of Composite Attributes). CDR-CA aims\nto refine the diversities of multiple attributes, according to the\napplication's context. To address this task, we propose Multi-Source DPPs, a\nsimple yet strong baseline that extends the Determinantal Point Process (DPP)\nto multi-sources. We model MS-DPP as a single DPP model with a unified\nsimilarity matrix based on a manifold representation. We also introduce Tangent\nNormalization to reflect contexts. Extensive experiments demonstrate the\neffectiveness of the proposed method. Our code is publicly available at\nhttps://github.com/NEC-N-SOGI/msdpp.",
        "url": "http://arxiv.org/abs/2507.06654v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06654v1",
        "arxiv_id": "2507.06654v1",
        "authors": [
            "Naoya Sogi",
            "Takashi Shibata",
            "Makoto Terao",
            "Masanori Suganuma",
            "Takayuki Okatani"
        ],
        "submitted": "2025-07-09 08:38:46",
        "source": "arxiv",
        "comment": "IJCAI 2025. Code: https://github.com/NEC-N-SOGI/msdpp"
    },
    {
        "title": "Expediting data extraction using a large language model (LLM) and scoping review protocol: a methodological study within a complex scoping review",
        "abstract": "The data extraction stages of reviews are resource-intensive, and researchers\nmay seek to expediate data extraction using online (large language models) LLMs\nand review protocols. Claude 3.5 Sonnet was used to trial two approaches that\nused a review protocol to prompt data extraction from 10 evidence sources\nincluded in a case study scoping review. A protocol-based approach was also\nused to review extracted data. Limited performance evaluation was undertaken\nwhich found high accuracy for the two extraction approaches (83.3% and 100%)\nwhen extracting simple, well-defined citation details; accuracy was lower (9.6%\nand 15.8%) when extracting more complex, subjective data items. Considering all\ndata items, both approaches had precision >90% but low recall (<25%) and F1\nscores (<40%). The context of a complex scoping review, open response types and\nmethodological approach likely impacted performance due to missed and\nmisattributed data. LLM feedback considered the baseline extraction accurate\nand suggested minor amendments: four of 15 (26.7%) to citation details and 8 of\n38 (21.1%) to key findings data items were considered to potentially add value.\nHowever, when repeating the process with a dataset featuring deliberate errors,\nonly 2 of 39 (5%) errors were detected. Review-protocol-based methods used for\nexpediency require more robust performance evaluation across a range of LLMs\nand review contexts with comparison to conventional prompt engineering\napproaches. We recommend researchers evaluate and report LLM performance if\nusing them similarly to conduct data extraction or review extracted data. LLM\nfeedback contributed to protocol adaptation and may assist future review\nprotocol drafting.",
        "url": "http://arxiv.org/abs/2507.06623v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06623v1",
        "arxiv_id": "2507.06623v1",
        "authors": [
            "James Stewart-Evans",
            "Emma Wilson",
            "Tessa Langley",
            "Andrew Prayle",
            "Angela Hands",
            "Karen Exley",
            "Jo Leonardi-Bee"
        ],
        "submitted": "2025-07-09 07:50:55",
        "source": "arxiv",
        "comment": "44 pages, 4 figures"
    },
    {
        "title": "FuDoBa: Fusing Document and Knowledge Graph-based Representations with Bayesian Optimisation",
        "abstract": "Building on the success of Large Language Models (LLMs), LLM-based\nrepresentations have dominated the document representation landscape, achieving\ngreat performance on the document embedding benchmarks. However, the\nhigh-dimensional, computationally expensive embeddings from LLMs tend to be\neither too generic or inefficient for domain-specific applications. To address\nthese limitations, we introduce FuDoBa a Bayesian optimisation-based method\nthat integrates LLM-based embeddings with domain-specific structured knowledge,\nsourced both locally and from external repositories like WikiData. This fusion\nproduces low-dimensional, task-relevant representations while reducing training\ncomplexity and yielding interpretable early-fusion weights for enhanced\nclassification performance. We demonstrate the effectiveness of our approach on\nsix datasets in two domains, showing that when paired with robust AutoML-based\nclassifiers, our proposed representation learning approach performs on par\nwith, or surpasses, those produced solely by the proprietary LLM-based\nembedding baselines.",
        "url": "http://arxiv.org/abs/2507.06622v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06622v1",
        "arxiv_id": "2507.06622v1",
        "authors": [
            "Boshko Koloski",
            "Senja Pollak",
            "Roberto Navigli",
            "Blaž Škrlj"
        ],
        "submitted": "2025-07-09 07:49:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long Generation",
        "abstract": "Recent advances in language modeling have demonstrated the effectiveness of\nState Space Models (SSMs) for efficient sequence modeling. While hybrid\narchitectures such as Samba and the decoder-decoder architecture, YOCO, have\nshown promising performance gains over Transformers, prior works have not\ninvestigated the efficiency potential of representation sharing between SSM\nlayers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet\neffective mechanism for efficient memory sharing across layers. We apply it to\ncreate SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in\nthe cross-decoder to share memory readout states from a Samba-based\nself-decoder. SambaY significantly enhances decoding efficiency, preserves\nlinear pre-filling time complexity, and boosts long-context performance, all\nwhile eliminating the need for explicit positional encoding. Through extensive\nscaling experiments, we demonstrate that our model exhibits a significantly\nlower irreducible loss compared to a strong YOCO baseline, indicating superior\nperformance scalability under large-scale compute regimes. Our largest model\nenhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves\nsignificantly better performance than Phi4-mini-Reasoning on reasoning tasks\nsuch as Math500, AIME24/25, and GPQA Diamond without any reinforcement\nlearning, while delivering up to 10x higher decoding throughput on 2K-length\nprompts with 32K generation length under the vLLM inference framework. We\nrelease our training codebase on open-source data at\nhttps://github.com/microsoft/ArchScale.",
        "url": "http://arxiv.org/abs/2507.06607v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06607v1",
        "arxiv_id": "2507.06607v1",
        "authors": [
            "Liliang Ren",
            "Congcong Chen",
            "Haoran Xu",
            "Young Jin Kim",
            "Adam Atkinson",
            "Zheng Zhan",
            "Jiankai Sun",
            "Baolin Peng",
            "Liyuan Liu",
            "Shuohang Wang",
            "Hao Cheng",
            "Jianfeng Gao",
            "Weizhu Chen",
            "Yelong Shen"
        ],
        "submitted": "2025-07-09 07:27:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Impacts of Mainstream-Driven Algorithms on Recommendations for Children Across Domains: A Reproducibility Study",
        "abstract": "Children are often exposed to items curated by recommendation algorithms.\nYet, research seldom considers children as a user group, and when it does, it\nis anchored on datasets where children are underrepresented, risking\noverlooking their interests, favoring those of the majority, i.e., mainstream\nusers. Recently, Ungruh et al. demonstrated that children's consumption\npatterns and preferences differ from those of mainstream users, resulting in\ninconsistent recommendation algorithm performance and behavior for this user\ngroup. These findings, however, are based on two datasets with a limited child\nuser sample. We reproduce and replicate this study on a wider range of datasets\nin the movie, music, and book domains, uncovering interaction patterns and\naspects of child-recommender interactions consistent across domains, as well as\nthose specific to some user samples in the data. We also extend insights from\nthe original study with popularity bias metrics, given the interpretation of\nresults from the original study. With this reproduction and extension, we\nuncover consumption patterns and differences between age groups stemming from\nintrinsic differences between children and others, and those unique to specific\ndatasets or domains.",
        "url": "http://arxiv.org/abs/2507.06596v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06596v1",
        "arxiv_id": "2507.06596v1",
        "authors": [
            "Robin Ungruh",
            "Alejandro Bellogín",
            "Dominik Kowald",
            "Maria Soledad Pera"
        ],
        "submitted": "2025-07-09 07:15:12",
        "source": "arxiv",
        "comment": "Preprint of accepted RecSys 2025 contribution"
    },
    {
        "title": "Enhancing Food-Domain Question Answering with a Multimodal Knowledge Graph: Hybrid QA Generation and Diversity Analysis",
        "abstract": "We propose a unified food-domain QA framework that combines a large-scale\nmultimodal knowledge graph (MMKG) with generative AI. Our MMKG links 13,000\nrecipes, 3,000 ingredients, 140,000 relations, and 14,000 images. We generate\n40,000 QA pairs using 40 templates and LLaVA/DeepSeek augmentation. Joint\nfine-tuning of Meta LLaMA 3.1-8B and Stable Diffusion 3.5-Large improves\nBERTScore by 16.2\\%, reduces FID by 37.8\\%, and boosts CLIP alignment by\n31.1\\%. Diagnostic analyses-CLIP-based mismatch detection (35.2\\% to 7.3\\%) and\nLLaVA-driven hallucination checks-ensure factual and visual fidelity. A hybrid\nretrieval-generation strategy achieves 94.1\\% accurate image reuse and 85\\%\nadequacy in synthesis. Our results demonstrate that structured knowledge and\nmultimodal generation together enhance reliability and diversity in food QA.",
        "url": "http://arxiv.org/abs/2507.06571v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06571v1",
        "arxiv_id": "2507.06571v1",
        "authors": [
            "Srihari K B",
            "Pushpak Bhattacharyya"
        ],
        "submitted": "2025-07-09 05:59:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Flaws of Others: An LLM-driven Framework for Scientific Knowledge Production",
        "abstract": "Large-language models turn writing into a live exchange between humans and\nsoftware. We capture this new medium with a discursive-network model that\ntreats people and LLMs as equal nodes and tracks how their statements\ncirculate. Broadening the focus from isolated hallucinations, we define\ninvalidation (any factual, logical, or structural breach) and show it follows\nfour hazards: drift from truth, self-repair, fresh fabrication, and external\ndetection. A general mathematical model of discursive networks is developed to\nprovide valuable insights: A network governed only by drift and self-repair\nstabilizes at a modest error rate; adding fabrication reproduces the high rates\nseen in current LLMs. Giving each false claim even a small chance of peer\nreview shifts the system to a truth-dominant state. We operationalize peer\nreview with the open-source \\emph{Flaws-of-Others (FOO) algorithm}: a\nconfigurable loop in which any set of agents critique one another while a\nharmoniser merges their verdicts. The takeaway is practical and cultural:\nreliability in this new medium comes not from perfecting single models but from\nwiring imperfect ones into networks that keep each other honest.",
        "url": "http://arxiv.org/abs/2507.06565v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06565v1",
        "arxiv_id": "2507.06565v1",
        "authors": [
            "Juan B. Gutiérrez"
        ],
        "submitted": "2025-07-09 05:39:56",
        "source": "arxiv",
        "comment": "27 pages, 3 figures, 4 tables, 1 algorithm, 28 references"
    },
    {
        "title": "DS@GT at CheckThat! 2025: Exploring Retrieval and Reranking Pipelines for Scientific Claim Source Retrieval on Social Media Discourse",
        "abstract": "Social media users often make scientific claims without citing where these\nclaims come from, generating a need to verify these claims. This paper details\nwork done by the DS@GT team for CLEF 2025 CheckThat! Lab Task 4b Scientific\nClaim Source Retrieval which seeks to find relevant scientific papers based on\nimplicit references in tweets. Our team explored 6 different data augmentation\ntechniques, 7 different retrieval and reranking pipelines, and finetuned a\nbi-encoder. Achieving an MRR@5 of 0.58, our team ranked 16th out of 30 teams\nfor the CLEF 2025 CheckThat! Lab Task 4b, and improvement of 0.15 over the BM25\nbaseline of 0.43. Our code is available on Github at\nhttps://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4b.",
        "url": "http://arxiv.org/abs/2507.06563v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06563v1",
        "arxiv_id": "2507.06563v1",
        "authors": [
            "Jeanette Schofield",
            "Shuyu Tian",
            "Hoang Thanh Thanh Truong",
            "Maximilian Heil"
        ],
        "submitted": "2025-07-09 05:32:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SPEAR: Subset-sampled Performance Evaluation via Automated Ground Truth Generation for RAG",
        "abstract": "Retrieval-Augmented Generation (RAG) is a core approach for enhancing Large\nLanguage Models (LLMs), where the effectiveness of the retriever largely\ndetermines the overall response quality of RAG systems. Retrievers encompass a\nmultitude of hyperparameters that significantly impact performance outcomes and\ndemonstrate sensitivity to specific applications. Nevertheless, hyperparameter\noptimization entails prohibitively high computational expenses. Existing\nevaluation methods suffer from either prohibitive costs or disconnection from\ndomain-specific scenarios. This paper proposes SEARA (Subset sampling\nEvaluation for Automatic Retriever Assessment), which addresses evaluation data\nchallenges through subset sampling techniques and achieves robust automated\nretriever evaluation by minimal retrieval facts extraction and comprehensive\nretrieval metrics. Based on real user queries, this method enables fully\nautomated retriever evaluation at low cost, thereby obtaining optimal retriever\nfor specific business scenarios. We validate our method across classic RAG\napplications in rednote, including knowledge-based Q&A system and\nretrieval-based travel assistant, successfully obtaining scenario-specific\noptimal retrievers.",
        "url": "http://arxiv.org/abs/2507.06554v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06554v1",
        "arxiv_id": "2507.06554v1",
        "authors": [
            "Zou Yuheng",
            "Wang Yiran",
            "Tian Yuzhu",
            "Zhu Min",
            "Huang Yanhua"
        ],
        "submitted": "2025-07-09 05:13:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Large Language Model for Extracting Complex Contract Information in Industrial Scenes",
        "abstract": "This paper proposes a high-quality dataset construction method for complex\ncontract information extraction tasks in industrial scenarios and fine-tunes a\nlarge language model based on this dataset. Firstly, cluster analysis is\nperformed on industrial contract texts, and GPT-4 and GPT-3.5 are used to\nextract key information from the original contract data, obtaining high-quality\ndata annotations. Secondly, data augmentation is achieved by constructing new\ntexts, and GPT-3.5 generates unstructured contract texts from randomly combined\nkeywords, improving model robustness. Finally, the large language model is\nfine-tuned based on the high-quality dataset. Experimental results show that\nthe model achieves excellent overall performance while ensuring high field\nrecall and precision and considering parsing efficiency. LoRA, data balancing,\nand data augmentation effectively enhance model accuracy and robustness. The\nproposed method provides a novel and efficient solution for industrial contract\ninformation extraction tasks.",
        "url": "http://arxiv.org/abs/2507.06539v2",
        "pdf_url": "http://arxiv.org/pdf/2507.06539v2",
        "arxiv_id": "2507.06539v2",
        "authors": [
            "Yunyang Cao",
            "Yanjun Li",
            "Silong Dai"
        ],
        "submitted": "2025-07-09 04:46:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "InvestAlign: Overcoming Data Scarcity in Aligning Large Language Models with Investor Decision-Making Processes under Herd Behavior",
        "abstract": "Aligning Large Language Models (LLMs) with investor decision-making processes\nunder herd behavior is a critical challenge in behavioral finance, which\ngrapples with a fundamental limitation: the scarcity of real-user data needed\nfor Supervised Fine-Tuning (SFT). While SFT can bridge the gap between LLM\noutputs and human behavioral patterns, its reliance on massive authentic data\nimposes substantial collection costs and privacy risks. We propose InvestAlign,\na novel framework that constructs high-quality SFT datasets by leveraging\ntheoretical solutions to similar and simple optimal investment problems rather\nthan complex scenarios. Our theoretical analysis demonstrates that training\nLLMs with InvestAlign-generated data achieves faster parameter convergence than\nusing real-user data, suggesting superior learning efficiency. Furthermore, we\ndevelop InvestAgent, an LLM agent fine-tuned with InvestAlign, which\ndemonstrates significantly closer alignment to real-user data than pre-SFT\nmodels in both simple and complex investment problems. This highlights our\nproposed InvestAlign as a promising approach with the potential to address\ncomplex optimal investment problems and align LLMs with investor\ndecision-making processes under herd behavior. Our code is publicly available\nat https://github.com/thu-social-network-research-group/InvestAlign.",
        "url": "http://arxiv.org/abs/2507.06528v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06528v1",
        "arxiv_id": "2507.06528v1",
        "authors": [
            "Huisheng Wang",
            "Zhuoshi Pan",
            "Hangjing Zhang",
            "Mingxiao Liu",
            "Hanqing Gao",
            "H. Vicky Zhao"
        ],
        "submitted": "2025-07-09 04:07:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "FIFA: Unified Faithfulness Evaluation Framework for Text-to-Video and Video-to-Text Generation",
        "abstract": "Video Multimodal Large Language Models (VideoMLLMs) have achieved remarkable\nprogress in both Video-to-Text and Text-to-Video tasks. However, they often\nsuffer fro hallucinations, generating content that contradicts the visual\ninput. Existing evaluation methods are limited to one task (e.g., V2T) and also\nfail to assess hallucinations in open-ended, free-form responses. To address\nthis gap, we propose FIFA, a unified FaIthFulness evAluation framework that\nextracts comprehensive descriptive facts, models their semantic dependencies\nvia a Spatio-Temporal Semantic Dependency Graph, and verifies them using\nVideoQA models. We further introduce Post-Correction, a tool-based correction\nframework that revises hallucinated content. Extensive experiments demonstrate\nthat FIFA aligns more closely with human judgment than existing evaluation\nmethods, and that Post-Correction effectively improves factual consistency in\nboth text and video generation.",
        "url": "http://arxiv.org/abs/2507.06523v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06523v1",
        "arxiv_id": "2507.06523v1",
        "authors": [
            "Liqiang Jing",
            "Viet Lai",
            "Seunghyun Yoon",
            "Trung Bui",
            "Xinya Du"
        ],
        "submitted": "2025-07-09 03:51:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SpindleKV: A Novel KV Cache Reduction Method Balancing Both Shallow and Deep Layers",
        "abstract": "Large Language Models (LLMs) have achieved impressive accomplishments in\nrecent years. However, the increasing memory consumption of KV cache has\npossessed a significant challenge to the inference system. Eviction methods\nhave revealed the inherent redundancy within the KV cache, demonstrating its\npotential for reduction, particularly in deeper layers. However, KV cache\nreduction for shallower layers has been found to be insufficient. Based on our\nobservation that, the KV cache exhibits a high degree of similarity. Based on\nthis observation, we proposed a novel KV cache reduction method, SpindleKV,\nwhich balances both shallow and deep layers. For deep layers, we employ an\nattention weight based eviction method, while for shallow layers, we apply a\ncodebook based replacement approach which is learnt by similarity and merging\npolicy. Moreover, SpindleKV addressed the Grouped-Query Attention (GQA) dilemma\nfaced by other attention based eviction methods. Experiments on two common\nbenchmarks with three different LLMs shown that SpindleKV obtained better KV\ncache reduction effect compared to baseline methods, while preserving similar\nor even better model performance.",
        "url": "http://arxiv.org/abs/2507.06517v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06517v1",
        "arxiv_id": "2507.06517v1",
        "authors": [
            "Zicong Tang",
            "Shi Luohe",
            "Zuchao Li",
            "Baoyuan Qi",
            "Guoming Liu",
            "Lefei Zhang",
            "Ping Wang"
        ],
        "submitted": "2025-07-09 03:33:44",
        "source": "arxiv",
        "comment": "Accepted by ACL 2025 main"
    },
    {
        "title": "GR-LLMs: Recent Advances in Generative Recommendation Based on Large Language Models",
        "abstract": "In the past year, Generative Recommendations (GRs) have undergone substantial\nadvancements, especially in leveraging the powerful sequence modeling and\nreasoning capabilities of Large Language Models (LLMs) to enhance overall\nrecommendation performance. LLM-based GRs are forming a new paradigm that is\ndistinctly different from discriminative recommendations, showing strong\npotential to replace traditional recommendation systems heavily dependent on\ncomplex hand-crafted features. In this paper, we provide a comprehensive survey\naimed at facilitating further research of LLM-based GRs. Initially, we outline\nthe general preliminaries and application cases of LLM-based GRs. Subsequently,\nwe introduce the main considerations when LLM-based GRs are applied in real\nindustrial scenarios. Finally, we explore promising directions for LLM-based\nGRs. We hope that this survey contributes to the ongoing advancement of the GR\ndomain.",
        "url": "http://arxiv.org/abs/2507.06507v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06507v1",
        "arxiv_id": "2507.06507v1",
        "authors": [
            "Zhen Yang",
            "Haitao Lin",
            "Jiawei xue",
            "Ziji Zhang"
        ],
        "submitted": "2025-07-09 03:13:08",
        "source": "arxiv",
        "comment": "8 pages, 3 figures"
    },
    {
        "title": "Pun Intended: Multi-Agent Translation of Wordplay with Contrastive Learning and Phonetic-Semantic Embeddings",
        "abstract": "Translating wordplay across languages presents unique challenges that have\nlong confounded both professional human translators and machine translation\nsystems. This research proposes a novel approach for translating puns from\nEnglish to French by combining state-of-the-art large language models with\nspecialized techniques for wordplay generation.\n  Our methodology employs a three-stage approach. First, we establish a\nbaseline using multiple frontier large language models with feedback based on a\nnew contrastive learning dataset. Second, we implement a guided\nchain-of-thought pipeline with combined phonetic-semantic embeddings. Third, we\nimplement a multi-agent generator-discriminator framework for evaluating and\nregenerating puns with feedback.\n  Moving beyond the limitations of literal translation, our methodology's\nprimary objective is to capture the linguistic creativity and humor of the\nsource text wordplay, rather than simply duplicating its vocabulary. Our best\nruns earned first and second place in the CLEF JOKER 2025 Task 2 competition\nwhere they were evaluated manually by expert native French speakers.\n  This research addresses a gap between translation studies and computational\nlinguistics by implementing linguistically-informed techniques for wordplay\ntranslation, advancing our understanding of how language models can be\nleveraged to handle the complex interplay between semantic ambiguity, phonetic\nsimilarity, and the implicit cultural and linguistic awareness needed for\nsuccessful humor.",
        "url": "http://arxiv.org/abs/2507.06506v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06506v1",
        "arxiv_id": "2507.06506v1",
        "authors": [
            "Russell Taylor",
            "Benjamin Herbert",
            "Michael Sana"
        ],
        "submitted": "2025-07-09 03:09:14",
        "source": "arxiv",
        "comment": "CLEF 2025 Working Notes, 9-12 September 2025, Madrid, Spain"
    },
    {
        "title": "USD: A User-Intent-Driven Sampling and Dual-Debiasing Framework for Large-Scale Homepage Recommendations",
        "abstract": "Large-scale homepage recommendations face critical challenges from\npseudo-negative samples caused by exposure bias, where non-clicks may indicate\ninattention rather than disinterest. Existing work lacks thorough analysis of\ninvalid exposures and typically addresses isolated aspects (e.g., sampling\nstrategies), overlooking the critical impact of pseudo-positive samples - such\nas homepage clicks merely to visit marketing portals. We propose a unified\nframework for large-scale homepage recommendation sampling and debiasing. Our\nframework consists of two key components: (1) a user intent-aware negative\nsampling module to filter invalid exposure samples, and (2) an intent-driven\ndual-debiasing module that jointly corrects exposure bias and click bias.\nExtensive online experiments on Taobao demonstrate the efficacy of our\nframework, achieving significant improvements in user click-through rates\n(UCTR) by 35.4% and 14.5% in two variants of the marketing block on the Taobao\nhomepage, Baiyibutie and Taobaomiaosha.",
        "url": "http://arxiv.org/abs/2507.06503v2",
        "pdf_url": "http://arxiv.org/pdf/2507.06503v2",
        "arxiv_id": "2507.06503v2",
        "authors": [
            "Jiaqi Zheng",
            "Cheng Guo",
            "Yi Cao",
            "Chaoqun Hou",
            "Tong Liu",
            "Bo Zheng"
        ],
        "submitted": "2025-07-09 03:02:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "On the Robustness of Verbal Confidence of LLMs in Adversarial Attacks",
        "abstract": "Robust verbal confidence generated by large language models (LLMs) is crucial\nfor the deployment of LLMs to ensure transparency, trust, and safety in\nhuman-AI interactions across many high-stakes applications. In this paper, we\npresent the first comprehensive study on the robustness of verbal confidence\nunder adversarial attacks. We introduce a novel framework for attacking verbal\nconfidence scores through both perturbation and jailbreak-based methods, and\nshow that these attacks can significantly jeopardize verbal confidence\nestimates and lead to frequent answer changes. We examine a variety of\nprompting strategies, model sizes, and application domains, revealing that\ncurrent confidence elicitation methods are vulnerable and that commonly used\ndefence techniques are largely ineffective or counterproductive. Our findings\nunderscore the urgent need to design more robust mechanisms for confidence\nexpression in LLMs, as even subtle semantic-preserving modifications can lead\nto misleading confidence in responses.",
        "url": "http://arxiv.org/abs/2507.06489v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06489v1",
        "arxiv_id": "2507.06489v1",
        "authors": [
            "Stephen Obadinma",
            "Xiaodan Zhu"
        ],
        "submitted": "2025-07-09 02:19:46",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for Efficient and Enhanced Video Reasoning",
        "abstract": "Despite advances in reinforcement learning (RL)-based video reasoning with\nlarge language models (LLMs), data collection and finetuning remain significant\nchallenges. These methods often rely on large-scale supervised fine-tuning\n(SFT) with extensive video data and long Chain-of-Thought (CoT) annotations,\nmaking them costly and hard to scale. To address this, we present Video-RTS, a\nnew approach to improve video reasoning capability with drastically improved\ndata efficiency by combining data-efficient RL with a video-adaptive test-time\nscaling (TTS) strategy. Based on observations about the data scaling of RL\nsamples, we skip the resource-intensive SFT step and employ efficient pure-RL\ntraining with output-based rewards, requiring no additional annotations or\nextensive fine-tuning. Furthermore, to utilize computational resources more\nefficiently, we introduce a sparse-to-dense video TTS strategy that improves\ninference by iteratively adding frames based on output consistency. We validate\nour approach on multiple video reasoning benchmarks, showing that Video-RTS\nsurpasses existing video reasoning models by an average of 2.4% in accuracy\nusing only 3.6% training samples. For example, Video-RTS achieves a 4.2%\nimprovement on Video-Holmes, a recent and challenging video reasoning\nbenchmark, and a 2.6% improvement on MMVU. Notably, our pure RL training and\nadaptive video TTS offer complementary strengths, enabling Video-RTS's strong\nreasoning performance.",
        "url": "http://arxiv.org/abs/2507.06485v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06485v1",
        "arxiv_id": "2507.06485v1",
        "authors": [
            "Ziyang Wang",
            "Jaehong Yoon",
            "Shoubin Yu",
            "Md Mohaiminul Islam",
            "Gedas Bertasius",
            "Mohit Bansal"
        ],
        "submitted": "2025-07-09 02:06:13",
        "source": "arxiv",
        "comment": "The first two authors contributed equally. Project page:\n  https://sites.google.com/cs.unc.edu/videorts2025/"
    },
    {
        "title": "Learning Japanese with Jouzu: Interaction Outcomes with Stylized Dialogue Fictional Agents",
        "abstract": "This study investigates how stylized, voiced agents shape user interaction in\na multimodal language learning environment. We conducted a mixed-methods\nevaluation of 54 participants interacting with anime-inspired characters\npowered by large language models and expressive text-to-speech synthesis. These\nagents responded in Japanese character language, offering users asynchronous,\nsemi-structured conversation in varying speech styles and emotional tones. We\nanalyzed user engagement patterns, perceived usability, emotional responses,\nand learning behaviors, with particular attention to how agent stylization\ninfluenced interaction across language proficiency levels and cultural\nbackgrounds. Our findings reveal that agent design, especially voice, persona,\nand linguistic style, substantially affected user experience, motivation, and\nstrategy. This work contributes to the understanding of affective, culturally\nstylized agents in human-agent interaction and offers guidance for designing\nmore engaging, socially responsive systems.",
        "url": "http://arxiv.org/abs/2507.06483v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06483v1",
        "arxiv_id": "2507.06483v1",
        "authors": [
            "Zackary Rackauckas",
            "Julia Hirschberg"
        ],
        "submitted": "2025-07-09 01:57:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Systematic Analysis of Hybrid Linear Attention",
        "abstract": "Transformers face quadratic complexity and memory issues with long sequences,\nprompting the adoption of linear attention mechanisms using fixed-size hidden\nstates. However, linear models often suffer from limited recall performance,\nleading to hybrid architectures that combine linear and full attention layers.\nDespite extensive hybrid architecture research, the choice of linear attention\ncomponent has not been deeply explored. We systematically evaluate various\nlinear attention models across generations - vector recurrences to advanced\ngating mechanisms - both standalone and hybridized. To enable this\ncomprehensive analysis, we trained and open-sourced 72 models: 36 at 340M\nparameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six\nlinear attention variants across five hybridization ratios. Benchmarking on\nstandard language modeling and recall tasks reveals that superior standalone\nlinear models do not necessarily excel in hybrids. While language modeling\nremains stable across linear-to-full attention ratios, recall significantly\nimproves with increased full attention layers, particularly below a 3:1 ratio.\nOur study highlights selective gating, hierarchical recurrence, and controlled\nforgetting as critical for effective hybrid models. We recommend architectures\nsuch as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1\nto achieve Transformer-level recall efficiently. Our models are open-sourced at\nhttps://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.",
        "url": "http://arxiv.org/abs/2507.06457v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06457v1",
        "arxiv_id": "2507.06457v1",
        "authors": [
            "Dustin Wang",
            "Rui-Jie Zhu",
            "Steven Abreu",
            "Yong Shan",
            "Taylor Kergan",
            "Yuqi Pan",
            "Yuhong Chou",
            "Zheng Li",
            "Ge Zhang",
            "Wenhao Huang",
            "Jason Eshraghian"
        ],
        "submitted": "2025-07-08 23:54:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Semantic Parsing Framework for End-to-End Time Normalization",
        "abstract": "Time normalization is the task of converting natural language temporal\nexpressions into machine-readable representations. It underpins many downstream\napplications in information retrieval, question answering, and clinical\ndecision-making. Traditional systems based on the ISO-TimeML schema limit\nexpressivity and struggle with complex constructs such as compositional,\nevent-relative, and multi-span time expressions. In this work, we introduce a\nnovel formulation of time normalization as a code generation task grounded in\nthe SCATE framework, which defines temporal semantics through symbolic and\ncompositional operators. We implement a fully executable SCATE Python library\nand demonstrate that large language models (LLMs) can generate executable SCATE\ncode. Leveraging this capability, we develop an automatic data augmentation\npipeline using LLMs to synthesize large-scale annotated data with code-level\nvalidation. Our experiments show that small, locally deployable models trained\non this augmented data can achieve strong performance, outperforming even their\nLLM parents and enabling practical, accurate, and interpretable time\nnormalization.",
        "url": "http://arxiv.org/abs/2507.06450v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06450v1",
        "arxiv_id": "2507.06450v1",
        "authors": [
            "Xin Su",
            "Sungduk Yu",
            "Phillip Howard",
            "Steven Bethard"
        ],
        "submitted": "2025-07-08 23:30:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Perception-Aware Policy Optimization for Multimodal Reasoning",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose Perception-Aware Policy\nOptimization (PAPO), a simple yet effective extension of GRPO that encourages\nthe model to learn to perceive while learning to reason, entirely from internal\nsupervision signals. Notably, PAPO does not rely on additional data curation,\nexternal reward models, or proprietary models. Specifically, we introduce the\nImplicit Perception Loss in the form of a KL divergence term to the GRPO\nobjective, which, despite its simplicity, yields significant overall\nimprovements (4.4%) on diverse multimodal benchmarks. The improvements are more\npronounced, approaching 8.0%, on tasks with high vision dependency. We also\nobserve a substantial reduction (30.5%) in perception errors, indicating\nimproved perceptual capabilities with PAPO. We conduct comprehensive analysis\nof PAPO and identify a unique loss hacking issue, which we rigorously analyze\nand mitigate through a Double Entropy Loss. Overall, our work introduces a\ndeeper integration of perception-aware supervision into RLVR learning\nobjectives and lays the groundwork for a new RL framework that encourages\nvisually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.",
        "url": "http://arxiv.org/abs/2507.06448v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06448v1",
        "arxiv_id": "2507.06448v1",
        "authors": [
            "Zhenhailong Wang",
            "Xuehang Guo",
            "Sofia Stoica",
            "Haiyang Xu",
            "Hongru Wang",
            "Hyeonjeong Ha",
            "Xiusi Chen",
            "Yangyi Chen",
            "Ming Yan",
            "Fei Huang",
            "Heng Ji"
        ],
        "submitted": "2025-07-08 23:22:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Can Interpretation Predict Behavior on Unseen Data?",
        "abstract": "Interpretability research often aims to predict how a model will respond to\ntargeted interventions on specific mechanisms. However, it rarely predicts how\na model will respond to unseen input data. This paper explores the promises and\nchallenges of interpretability as a tool for predicting out-of-distribution\n(OOD) model behavior. Specifically, we investigate the correspondence between\nattention patterns and OOD generalization in hundreds of Transformer models\nindependently trained on a synthetic classification task. These models exhibit\nseveral distinct systematic generalization rules OOD, forming a diverse\npopulation for correlational analysis. In this setting, we find that simple\nobservational tools from interpretability can predict OOD performance. In\nparticular, when in-distribution attention exhibits hierarchical patterns, the\nmodel is likely to generalize hierarchically on OOD data -- even when the\nrule's implementation does not rely on these hierarchical patterns, according\nto ablation tests. Our findings offer a proof-of-concept to motivate further\ninterpretability work on predicting unseen model behavior.",
        "url": "http://arxiv.org/abs/2507.06445v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06445v1",
        "arxiv_id": "2507.06445v1",
        "authors": [
            "Victoria R. Li",
            "Jenny Kaufmann",
            "Martin Wattenberg",
            "David Alvarez-Melis",
            "Naomi Saphra"
        ],
        "submitted": "2025-07-08 23:07:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Temporal Analysis of Climate Policy Discourse: Insights from Dynamic Embedded Topic Modeling",
        "abstract": "Understanding how policy language evolves over time is critical for assessing\nglobal responses to complex challenges such as climate change. Temporal\nanalysis helps stakeholders, including policymakers and researchers, to\nevaluate past priorities, identify emerging themes, design governance\nstrategies, and develop mitigation measures. Traditional approaches, such as\nmanual thematic coding, are time-consuming and limited in capturing the\ncomplex, interconnected nature of global policy discourse. With the increasing\nrelevance of unsupervised machine learning, these limitations can be addressed,\nparticularly under high-volume, complex, and high-dimensional data conditions.\nIn this work, we explore a novel approach that applies the dynamic embedded\ntopic model (DETM) to analyze the evolution of global climate policy discourse.\nA probabilistic model designed to capture the temporal dynamics of topics over\ntime. We collected a corpus of United Nations Framework Convention on Climate\nChange (UNFCCC) policy decisions from 1995 to 2023, excluding 2020 due to the\npostponement of COP26 as a result of the COVID-19 pandemic. The model reveals\nshifts from early emphases on greenhouse gases and international conventions to\nrecent focuses on implementation, technical collaboration, capacity building,\nfinance, and global agreements. Section 3 presents the modeling pipeline,\nincluding preprocessing, model training, and visualization of temporal word\ndistributions. Our results show that DETM is a scalable and effective tool for\nanalyzing the evolution of global policy discourse. Section 4 discusses the\nimplications of these findings and we concluded with future directions and\nrefinements to extend this approach to other policy domains.",
        "url": "http://arxiv.org/abs/2507.06435v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06435v1",
        "arxiv_id": "2507.06435v1",
        "authors": [
            "Rafiu Adekoya Badekale",
            "Adewale Akinfaderin"
        ],
        "submitted": "2025-07-08 22:30:01",
        "source": "arxiv",
        "comment": "10 pages, 7 figures. Code and data available at\n  https://github.com/AdeTheBade/TACPD.git"
    },
    {
        "title": "Exploring Task Performance with Interpretable Models via Sparse Auto-Encoders",
        "abstract": "Large Language Models (LLMs) are traditionally viewed as black-box\nalgorithms, therefore reducing trustworthiness and obscuring potential\napproaches to increasing performance on downstream tasks. In this work, we\napply an effective LLM decomposition method using a dictionary-learning\napproach with sparse autoencoders. This helps extract monosemantic features\nfrom polysemantic LLM neurons. Remarkably, our work identifies model-internal\nmisunderstanding, allowing the automatic reformulation of the prompts with\nadditional annotations to improve the interpretation by LLMs. Moreover, this\napproach demonstrates a significant performance improvement in downstream\ntasks, such as mathematical reasoning and metaphor detection.",
        "url": "http://arxiv.org/abs/2507.06427v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06427v1",
        "arxiv_id": "2507.06427v1",
        "authors": [
            "Shun Wang",
            "Tyler Loakman",
            "Youbo Lei",
            "Yi Liu",
            "Bohao Yang",
            "Yuting Zhao",
            "Dong Yang",
            "Chenghua Lin"
        ],
        "submitted": "2025-07-08 22:17:52",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Reward Models Can Improve Themselves: Reward-Guided Adversarial Failure Mode Discovery for Robust Reward Modeling",
        "abstract": "Reward modeling (RM), which captures human preferences to align large\nlanguage models (LLMs), is increasingly employed in tasks such as model\nfinetuning, response filtering, and ranking. However, due to the inherent\ncomplexity of human preferences and the limited coverage of available datasets,\nreward models often fail under distributional shifts or adversarial\nperturbations. Existing approaches for identifying such failure modes typically\nrely on prior knowledge about preference distributions or failure attributes,\nlimiting their practicality in real-world settings where such information is\nunavailable. In this work, we propose a tractable, preference-distribution\nagnostic method for discovering reward model failure modes via reward guided\ncontrolled decoding. Building on this, we introduce REFORM, a self-improving\nreward modeling framework that enhances robustness by using the reward model\nitself to guide the generation of falsely scored responses. These adversarial\nexamples are then used to augment the training data and patch the reward\nmodel's misaligned behavior. We evaluate REFORM on two widely used preference\ndatasets Anthropic Helpful Harmless (HH) and PKU Beavertails and demonstrate\nthat it significantly improves robustness without sacrificing reward quality.\nNotably, REFORM preserves performance both in direct evaluation and in\ndownstream policy training, and further improves alignment quality by removing\nspurious correlations.",
        "url": "http://arxiv.org/abs/2507.06419v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06419v1",
        "arxiv_id": "2507.06419v1",
        "authors": [
            "Pankayaraj Pathmanathan",
            "Furong Huang"
        ],
        "submitted": "2025-07-08 21:56:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning",
        "abstract": "Long-context reasoning requires accurately identifying relevant information\nin extensive, noisy input contexts. Previous research shows that using\ntest-time learning to encode context directly into model parameters can\neffectively enable reasoning over noisy information. However, meta-learning\nmethods for enabling test-time learning are prohibitively memory-intensive,\npreventing their application to long context settings. In this work, we propose\nPERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for\nlearning to encode long input contexts using gradient updates to a lightweight\nmodel adapter at test time. Specifically, PERK employs two nested optimization\nloops in a meta-training phase. The inner loop rapidly encodes contexts into a\nlow-rank adapter (LoRA) that serves as a parameter-efficient memory module for\nthe base model. Concurrently, the outer loop learns to use the updated adapter\nto accurately recall and reason over relevant information from the encoded long\ncontext. Our evaluations on several long-context reasoning tasks show that PERK\nsignificantly outperforms the standard prompt-based long-context baseline,\nachieving average absolute performance gains of up to 90% for smaller models\n(GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In\ngeneral, PERK is more robust to reasoning complexity, length extrapolation, and\nthe locations of relevant information in contexts. Finally, we show that while\nPERK is memory-intensive during training, it scales more efficiently at\ninference time than prompt-based long-context inference.",
        "url": "http://arxiv.org/abs/2507.06415v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06415v1",
        "arxiv_id": "2507.06415v1",
        "authors": [
            "Zeming Chen",
            "Angelika Romanou",
            "Gail Weiss",
            "Antoine Bosselut"
        ],
        "submitted": "2025-07-08 21:38:45",
        "source": "arxiv",
        "comment": "10 pages, 7 figures"
    },
    {
        "title": "Hypermagmas and Colored Operads: Heads, Phases, and Theta Roles",
        "abstract": "We show that head functions on syntactic objects extend the magma structure\nto a hypermagma, with the c-command relation compatible with the magma\noperation and the m-command relation with the hypermagma. We then show that the\nstructure of head and complement and specifier, additional modifier positions,\nand the structure of phases in the Extended Projection can be formulated as a\nbud generating system of a colored operad, in a form similar to the structure\nof theta roles. We also show that, due to the special form of the colored\noperad generators, the filtering of freely generated syntactic objects by these\ncoloring rules can be equivalently formulated as a filtering in the course of\nstructure formation via a colored Merge, which can in turn be related to the\nhypermagma structure. The rules on movement by Internal Merge with respect to\nphases, the Extended Projection Principle, Empty Category Principle, and Phase\nImpenetrability Condition are all subsumed into the form of the colored operad\ngenerators. Movement compatibilities between the phase structure and the theta\nroles assignments can then be formulated in terms of the respective colored\noperads and a transduction of colored operads.",
        "url": "http://arxiv.org/abs/2507.06393v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06393v1",
        "arxiv_id": "2507.06393v1",
        "authors": [
            "Matilde Marcolli",
            "Riny Huijbregts",
            "Richard K. Larson"
        ],
        "submitted": "2025-07-08 21:01:13",
        "source": "arxiv",
        "comment": "LaTeX, 48 pages"
    },
    {
        "title": "Evaluating Morphological Alignment of Tokenizers in 70 Languages",
        "abstract": "While tokenization is a key step in language modeling, with effects on model\ntraining and performance, it remains unclear how to effectively evaluate\ntokenizer quality. One proposed dimension of tokenizer quality is the extent to\nwhich tokenizers preserve linguistically meaningful subwords, aligning token\nboundaries with morphological boundaries within a word. We expand MorphScore\n(Arnett & Bergen, 2025), which previously covered 22 languages, to support a\ntotal of 70 languages. The updated MorphScore offers more flexibility in\nevaluation and addresses some of the limitations of the original version. We\nthen correlate our alignment scores with downstream task performance for five\npre-trained languages models on seven tasks, with at least one task in each of\nthe languages in our sample. We find that morphological alignment does not\nexplain very much variance in model performance, suggesting that morphological\nalignment alone does not measure dimensions of tokenization quality relevant to\nmodel performance.",
        "url": "http://arxiv.org/abs/2507.06378v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06378v1",
        "arxiv_id": "2507.06378v1",
        "authors": [
            "Catherine Arnett",
            "Marisa Hudspeth",
            "Brendan O'Connor"
        ],
        "submitted": "2025-07-08 20:32:26",
        "source": "arxiv",
        "comment": "6 pages, 3 figures. Accepted to the Tokenization Workshop at ICML\n  2025"
    },
    {
        "title": "Growing Transformers: Modular Composition and Layer-wise Expansion on a Frozen Substrate",
        "abstract": "The prevailing paradigm for scaling large language models (LLMs) involves\nmonolithic, end-to-end training, a resource-intensive process that lacks\nflexibility. This paper explores an alternative, constructive approach to model\ndevelopment, built upon the foundation of non-trainable, deterministic input\nembeddings. In prior [1], we established that high-level semantic reasoning can\nemerge in Transformers using frozen embeddings derived from the visual\nstructure of Unicode glyphs. Here, we demonstrate that this fixed\nrepresentational substrate acts as a universal \"docking port,\" enabling two\npowerful and efficient scaling paradigms: seamless modular composition and\nprogressive layer-wise growth.\n  First, we show that specialist models trained on disparate datasets (e.g.,\nRussian and Chinese text) can be merged into a single, more capable\nMixture-of-Experts (MoE) model, post-training, with zero architectural\nmodification. This is achieved by simply averaging their output logits. The\nresulting MoE model exhibits immediate performance improvements on reasoning\nbenchmarks like MMLU, surpassing its constituent experts without catastrophic\nforgetting. Second, we introduce a layer-wise constructive training\nmethodology, where a deep Transformer is \"grown\" by progressively stacking and\ntraining one layer at a time. This method demonstrates stable convergence and a\nclear correlation between model depth and the emergence of complex reasoning\nabilities, such as those required for SQuAD.\n  Our findings suggest a paradigm shift from monolithic optimization towards a\nmore biological or constructive model of AI development, where complexity is\nbuilt incrementally and modules can be composed freely. This opens new avenues\nfor resource-efficient scaling, continual learning, and a more democratized\necosystem for building powerful AI systems. We release all code and models to\nfacilitate further research.",
        "url": "http://arxiv.org/abs/2507.07129v1",
        "pdf_url": "http://arxiv.org/pdf/2507.07129v1",
        "arxiv_id": "2507.07129v1",
        "authors": [
            "A. Bochkov"
        ],
        "submitted": "2025-07-08 20:01:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Could the Road to Grounded, Neuro-symbolic AI be Paved with Words-as-Classifiers?",
        "abstract": "Formal, Distributional, and Grounded theories of computational semantics each\nhave their uses and their drawbacks. There has been a shift to ground models of\nlanguage by adding visual knowledge, and there has been a call to enrich models\nof language with symbolic methods to gain the benefits from formal,\ndistributional, and grounded theories. In this paper, we attempt to make the\ncase that one potential path forward in unifying all three semantic fields is\npaved with the words-as-classifier model, a model of word-level grounded\nsemantics that has been incorporated into formalisms and distributional\nlanguage models in the literature, and it has been well-tested within\ninteractive dialogue settings. We review that literature, motivate the\nwords-as-classifiers model with an appeal to recent work in cognitive science,\nand describe a small experiment. Finally, we sketch a model of semantics\nunified through words-as-classifiers.",
        "url": "http://arxiv.org/abs/2507.06335v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06335v1",
        "arxiv_id": "2507.06335v1",
        "authors": [
            "Casey Kennington",
            "David Schlangen"
        ],
        "submitted": "2025-07-08 18:44:34",
        "source": "arxiv",
        "comment": "9 pages"
    },
    {
        "title": "ETT: Expanding the Long Context Understanding Capability of LLMs at Test-Time",
        "abstract": "Transformer-based Language Models' computation and memory overhead increase\nquadratically as a function of sequence length. The quadratic cost poses\nchallenges when employing LLMs for processing long sequences. In this work, we\nintroduce \\ourmodelacronym~(Extend at Test-Time), method for extending the\ncontext length of short context Transformer-based LLMs, with constant memory\nrequirement and linear computation overhead. ETT enable the extension of the\ncontext length at test-time by efficient fine-tuning the model's parameters on\nthe input context, chunked into overlapping small subsequences. We evaluate ETT\non LongBench by extending the context length of GPT-Large and Phi-2 up to 32\ntimes, increasing from 1k to 32k tokens. This results in up to a 30 percent\nimprovement in the model's accuracy. We also study how context can be stored in\nLLM's weights effectively and efficiently. Through a detailed ablation study,\nwe examine which Transformer modules are most beneficial to fine-tune at\ntest-time. Interestingly, we find that fine-tuning the second layer of the FFNs\nis more effective than full fine-tuning, leading to a further improvement in\nthe models' accuracy.",
        "url": "http://arxiv.org/abs/2507.06313v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06313v1",
        "arxiv_id": "2507.06313v1",
        "authors": [
            "Kiarash Zahirnia",
            "Zahra Golpayegani",
            "Walid Ahmad",
            "Yang Liu"
        ],
        "submitted": "2025-07-08 18:06:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Humans overrely on overconfident language models, across languages",
        "abstract": "As large language models (LLMs) are deployed globally, it is crucial that\ntheir responses are calibrated across languages to accurately convey\nuncertainty and limitations. Previous work has shown that LLMs are\nlinguistically overconfident in English, leading users to overrely on confident\ngenerations. However, the usage and interpretation of epistemic markers (e.g.,\n'It's definitely,' 'I think') can differ sharply across languages. Here, we\nstudy the risks of multilingual linguistic (mis)calibration, overconfidence,\nand overreliance across five languages to evaluate the safety of LLMs in a\nglobal context.\n  We find that overreliance risks are high across all languages. We first\nanalyze the distribution of LLM-generated epistemic markers, and observe that\nwhile LLMs are cross-linguistically overconfident, they are also sensitive to\ndocumented linguistic variation. For example, models generate the most markers\nof uncertainty in Japanese and the most markers of certainty in German and\nMandarin. We then measure human reliance rates across languages, finding that\nwhile users strongly rely on confident LLM generations in all languages,\nreliance behaviors differ cross-linguistically: for example, users rely\nsignificantly more on expressions of uncertainty in Japanese than in English.\nTaken together, these results indicate high risk of reliance on overconfident\nmodel generations across languages. Our findings highlight the challenges of\nmultilingual linguistic calibration and stress the importance of culturally and\nlinguistically contextualized model safety evaluations.",
        "url": "http://arxiv.org/abs/2507.06306v1",
        "pdf_url": "http://arxiv.org/pdf/2507.06306v1",
        "arxiv_id": "2507.06306v1",
        "authors": [
            "Neil Rathi",
            "Dan Jurafsky",
            "Kaitlyn Zhou"
        ],
        "submitted": "2025-07-08 18:01:01",
        "source": "arxiv",
        "comment": "10 pages main text, to appear at COLM 2025"
    }
]