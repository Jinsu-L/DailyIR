# Daily Papers Report - 2025-08-07

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Query Attribute Modeling: Improving search relevance with Semantic Search and Meta Data Filtering

- **LLM Score**: 8
- **Keyword Score**: 23
- **Authors**: Karthik Menon, Batool Arhamna Haider, Muhammad Arham, Kanwal Mehreen, Ram Mohan Rao Kadiyala, Hamza Farooq
- **URL**: <http://arxiv.org/abs/2508.04683v1>
- **Submitted**: 2025-08-06 17:47:00
- **Topic Keywords**: semantic search, query, queries, ranking, relevance, rag, retrieval, commerce, e-commerce, rank, search
- **Reason**: The paper's focus on query attribute modeling, semantic search, and metadata filtering aligns with your interests in information retrieval, query understanding, and ranking models. The use of a hybrid framework and experimental evaluation on a large e-commerce dataset also resonates with your background in e-commerce and experience with search technologies.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Query Attribute Modeling (QAM) for Enhancing Search Precision and Relevance
- **Aim**: To develop a hybrid framework that decomposes open text queries into structured metadata tags and semantic elements to improve search precision and relevance
- **Rationale**: Traditional search methods have limitations in terms of precision and relevance, and QAM addresses these limitations by automatically extracting metadata filters from free-form text queries
- **Ground**: The QAM approach involves four steps: query decomposition, metadata filtering, query and product description similarity, and final ranking
- **Experiment**: The authors evaluate QAM using the Amazon Toys Reviews dataset, comparing it with five search methods: BM25 keyword-based search, semantic search, cross-encoder re-ranking, hybrid search, and QAM
- **Takeaway**: QAM significantly outperforms traditional search methods in terms of precision and relevance, and has potential for future improvements and scalability

#### Abstract
> This study introduces Query Attribute Modeling (QAM), a hybrid framework that
enhances search precision and relevance by decomposing open text queries into
structured metadata tags and semantic elements. QAM addresses traditional
search limitations by automatically extracting metadata filters from free-form
text queries, reducing noise and enabling focused retrieval of relevant items.
  Experimental evaluation using the Amazon Toys Reviews dataset (10,000 unique
items with 40,000+ reviews and detailed product attributes) demonstrated QAM's
superior performance, achieving a mean average precision at 5 (mAP@5) of
52.99\%. This represents significant improvement over conventional methods,
including BM25 keyword search, encoder-based semantic similarity search,
cross-encoder re-ranking, and hybrid search combining BM25 and semantic results
via Reciprocal Rank Fusion (RRF). The results establish QAM as a robust
solution for Enterprise Search applications, particularly in e-commerce
systems.

---

### 2. Bridging Search and Recommendation through Latent Cross Reasoning

- **LLM Score**: 8
- **Keyword Score**: 8
- **Authors**: Teng Shi, Weicong Qin, Weijie Yu, Xiao Zhang, Ming He, Jianping Fan, Jun Xu
- **URL**: <http://arxiv.org/abs/2508.04152v1>
- **Submitted**: 2025-08-06 07:28:11
- **Topic Keywords**: ranking, rag, recommend, rank, search
- **Reason**: The paper explores the intersection of search and recommendation, which aligns with your interest in Information Retrieval and Search technologies. The use of latent cross reasoning and contrastive learning to improve recommendation performance is also relevant to your focus on query understanding and ranking models. However, the paper's primary focus on recommendation rather than information retrieval might reduce its relevance to your core research themes.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Latent Cross Reasoning for Search-Enhanced Recommendation (LCR-SER)
- **Aim**: to bridge the gap between search and recommendation by leveraging user search behaviors to improve recommendation performance
- **Rationale**: existing approaches often encode search and recommendation histories jointly or separately without explicitly identifying which search behaviors are useful for recommendation
- **Ground**: inspired by human decision-making, the proposed LCR-SER framework first encodes user search and recommendation histories using separate encoders to capture global interests and then iteratively reasons over search behaviors to extract signals beneficial for recommendation
- **Experiment**: extensive experiments on public datasets to validate the effectiveness of the LCR-SER method, showing that it achieves consistent improvements over strong baselines in terms of Hit Ratio and Normalized Discounted Cumulative Gain
- **Takeaway**: the importance of reasoning in enhancing search-aware recommendation and the effectiveness of the proposed LCR-SER framework in identifying useful search signals for recommendation

#### Abstract
> Search and recommendation (S&R) are fundamental components of modern online
platforms, yet effectively leveraging search behaviors to improve
recommendation remains a challenging problem. User search histories often
contain noisy or irrelevant signals that can even degrade recommendation
performance, while existing approaches typically encode S&R histories either
jointly or separately without explicitly identifying which search behaviors are
truly useful. Inspired by the human decision-making process, where one first
identifies recommendation intent and then reasons about relevant evidence, we
design a latent cross reasoning framework that first encodes user S&R histories
to capture global interests and then iteratively reasons over search behaviors
to extract signals beneficial for recommendation. Contrastive learning is
employed to align latent reasoning states with target items, and reinforcement
learning is further introduced to directly optimize ranking performance.
Extensive experiments on public benchmarks demonstrate consistent improvements
over strong baselines, validating the importance of reasoning in enhancing
search-aware recommendation.

---

### 3. PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG

- **LLM Score**: 7
- **Keyword Score**: 13
- **Authors**: Wang Chen, Guanqiang Qi, Weikang Li, Yang Li, Deguo Xia, Jizhou Huang
- **URL**: <http://arxiv.org/abs/2508.04057v1>
- **Submitted**: 2025-08-06 03:33:01
- **Topic Keywords**: information retrieval, query, queries, rag, retrieval
- **Reason**: The paper presents a novel approach to Retrieval-Augmented Generation (RAG) that addresses limitations in current RAG systems. While it doesn't directly focus on query understanding, ranking models, or user behavior modeling, it does explore efficient information retrieval and selection, which is relevant to your interests in Information Retrieval. The paper's emphasis on adapting to query complexity and using contextual signals is also somewhat related to your work on deep semantic understanding and real-time relevance optimization.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: PAIRS: A Novel Framework for Efficient and Accurate Retrieval-Augmented Generation
- **Aim**: Improve the efficiency and accuracy of Retrieval-Augmented Generation (RAG) systems
- **Rationale**: Address limitations of RAG systems, including inefficient retrieval for simple queries and the risk of retrieving irrelevant documents for queries with sparse information signals
- **Ground**: Integrate parametric and retrieved knowledge to adaptively determine whether to retrieve and how to select external information
- **Experiment**: Experimental results on six question-answering benchmarks show that PAIRS reduces retrieval costs by around 25% while still improving accuracy
- **Takeaway**: PAIRS consistently outperforms baseline methods, with an average improvement of 1.1% EM and 1.0% F1 score, and demonstrates the importance of integrating both query-based and pseudo-context-based retrieval and adaptive scoring

#### Abstract
> Retrieval-Augmented Generation (RAG) has become a cornerstone technique for
enhancing large language models (LLMs) with external knowledge. However,
current RAG systems face two critical limitations: (1) they inefficiently
retrieve information for every query, including simple questions that could be
resolved using the LLM's parametric knowledge alone, and (2) they risk
retrieving irrelevant documents when queries contain sparse information
signals. To address these gaps, we introduce Parametric-verified Adaptive
Information Retrieval and Selection (PAIRS), a training-free framework that
integrates parametric and retrieved knowledge to adaptively determine whether
to retrieve and how to select external information. Specifically, PAIRS employs
a dual-path generation mechanism: First, the LLM produces both a direct answer
and a context-augmented answer using self-generated pseudo-context. When these
outputs converge, PAIRS bypasses external retrieval entirely, dramatically
improving the RAG system's efficiency. For divergent cases, PAIRS activates a
dual-path retrieval (DPR) process guided by both the original query and
self-generated contextual signals, followed by an Adaptive Information
Selection (AIS) module that filters documents through weighted similarity to
both sources. This simple yet effective approach can not only enhance
efficiency by eliminating unnecessary retrievals but also improve accuracy
through contextually guided retrieval and adaptive information selection.
Experimental results on six question-answering (QA) benchmarks show that PAIRS
reduces retrieval costs by around 25% (triggering for only 75% of queries)
while still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior
baselines on average.

---

### 4. TURA: Tool-Augmented Unified Retrieval Agent for AI Search

- **LLM Score**: 7
- **Keyword Score**: 8
- **Authors**: Zhejun Zhao, Yuehu Dong, Alley Liu, Lixue Zheng, Pingsheng Liu, Dongdong Shen, Long Xia, Jiashu Zhao, Dawei Yin
- **URL**: <http://arxiv.org/abs/2508.04604v1>
- **Submitted**: 2025-08-06 16:24:17
- **Topic Keywords**: queries, rag, retrieval, search
- **Reason**: The paper discusses a novel framework for AI search, combining Retrieval-Augmented Generation with agentic tool-use to access both static and dynamic information sources. While it doesn't directly focus on query understanding, ranking models, or user behavior modeling, it explores the intersection of information retrieval and natural language processing, which aligns with your interests. However, the paper's primary focus is on the architecture and implementation of the TURA framework, rather than the specific topics you mentioned.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: TURA: A Novel Three-Stage Framework for Conversational AI
- **Aim**: To overcome the limitations of traditional Retrieval-Augmented Generation (RAG) by introducing a novel agentic architecture that combines RAG with tool-use to access both static content and dynamic, real-time information
- **Rationale**: To address the limitations of traditional RAG-based AI search, which can only retrieve information from static webpages and is incapable of performing required actions
- **Ground**: The authors propose TURA as a novel and systematic agentic architecture, introducing a cohesive framework of synergistic techniques, and demonstrating the effectiveness of TURA in expanding the capabilities of AI search
- **Experiment**: The authors evaluate TURA using a comprehensive benchmark called MCP-Bench, consisting of anonymized production logs capturing natural query distributions, and annotated it using a rigorous multi-stage protocol
- **Takeaway**: TURA achieves substantial improvements in both answer accuracy and faithfulness compared to the baseline, with an answer accuracy of 87.5% and faithfulness of 96.2%, and is considered a production-proven blueprint for the next generation of conversational AI

#### Abstract
> The advent of Large Language Models (LLMs) is transforming search engines
into conversational AI search products, primarily using Retrieval-Augmented
Generation (RAG) on web corpora. However, this paradigm has significant
industrial limitations. Traditional RAG approaches struggle with real-time
needs and structured queries that require accessing dynamically generated
content like ticket availability or inventory. Limited to indexing static
pages, search engines cannot perform the interactive queries needed for such
time-sensitive data. Academic research has focused on optimizing RAG for static
content, overlooking complex intents and the need for dynamic sources like
databases and real-time APIs. To bridge this gap, we introduce TURA
(Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage
framework that combines RAG with agentic tool-use to access both static content
and dynamic, real-time information. TURA has three key components: an
Intent-Aware Retrieval module to decompose queries and retrieve information
sources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task
Planner that models task dependencies as a Directed Acyclic Graph (DAG) for
optimal parallel execution, and a lightweight Distilled Agent Executor for
efficient tool calling. TURA is the first architecture to systematically bridge
the gap between static RAG and dynamic information sources for a world-class AI
search product. Serving tens of millions of users, it leverages an agentic
framework to deliver robust, real-time answers while meeting the low-latency
demands of a large-scale industrial system.

---

### 5. ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for LLM-based Agents

- **LLM Score**: 7
- **Keyword Score**: 5
- **Authors**: Jiangyuan Wang, Kejun Xiao, Qi Sun, Huaipeng Zhao, Tao Luo, Jiandong Zhang, Xiaoyi Zeng
- **URL**: <http://arxiv.org/abs/2508.04266v1>
- **Submitted**: 2025-08-06 09:51:30
- **Comment**: submit to AAAI2026
- **Topic Keywords**: rag, shopping, commerce, e-commerce
- **Reason**: The paper proposes a novel benchmark for e-commerce, focusing on complex user intents and real-world scenarios. While it's not directly related to query understanding, ranking models, or user behavior modeling, it's relevant to information retrieval and search technologies in the context of e-commerce. The paper's emphasis on language agents and their capabilities is also of interest in the broader context of NLP.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Evaluating Large Language Models in E-commerce Shopping Tasks
- **Aim**: To design a comprehensive benchmark, ShoppingBench, to evaluate the ability of large language models to fulfill complex user intents in real-world shopping scenarios
- **Rationale**: Existing language agents struggle to fulfill complex user intents in real-world shopping scenarios, and there is a need for a comprehensive evaluation framework to assess their performance
- **Ground**: The ShoppingBench environment consists of 3,310 user instructions, simulating various intents derived from real-world products, and provides a large-scale shopping sandbox with over 2.5 million real-world products
- **Experiment**: The authors evaluate 17 existing language agents, including fine-tuned Qwen3-4B agent, and propose a trajectory distillation strategy to synthesize training data, filter out low-quality trajectories, and use Supervised Fine-Tuning and Reinforcement Learning to efficiently distill the training data
- **Takeaway**: The ShoppingBench benchmark provides a comprehensive evaluation framework for language agents in e-commerce shopping tasks, and the authors' contributions include a scalable framework to simulate diverse user instructions, new automatic evaluation metrics, and a trajectory distillation strategy to efficiently distill the training data

#### Abstract
> Existing benchmarks in e-commerce primarily focus on basic user intents, such
as finding or purchasing products. However, real-world users often pursue more
complex goals, such as applying vouchers, managing budgets, and finding
multi-products seller. To bridge this gap, we propose ShoppingBench, a novel
end-to-end shopping benchmark designed to encompass increasingly challenging
levels of grounded intent. Specifically, we propose a scalable framework to
simulate user instructions based on various intents derived from sampled
real-world products. To facilitate consistent and reliable evaluations, we
provide a large-scale shopping sandbox that serves as an interactive simulated
environment, incorporating over 2.5 million real-world products. Experimental
results demonstrate that even state-of-the-art language agents (such as
GPT-4.1) achieve absolute success rates under 50% on our benchmark tasks,
highlighting the significant challenges posed by our ShoppingBench. In
addition, we propose a trajectory distillation strategy and leverage supervised
fine-tuning, along with reinforcement learning on synthetic trajectories, to
distill the capabilities of a large language agent into a smaller one. As a
result, our trained agent achieves competitive performance compared to GPT-4.1.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models

- **LLM Score**: 6
- **Keyword Score**: 5
- **Authors**: Subhey Sadi Rahman, Md. Adnanul Islam, Md. Mahbub Alam, Musarrat Zeba, Md. Abdur Rahman, Sadia Sultana Chowa, Mohaimenul Azam Khan Raiaan, Sami Azam
- **URL**: <http://arxiv.org/abs/2508.03860v1>
- **Submitted**: 2025-08-05 19:20:05
- **Comment**: 30 pages, 11 figures, 6 tables. Submitted to Artificial Intelligence
  Review for peer review
- **Topic Keywords**: rag, retrieval, search
- **Reason**: The paper discusses fact-checking and factuality evaluation in Large Language Models, which is related to information retrieval and search technologies. However, the focus is more on the evaluation of language models rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user.

#### Abstract
> Large Language Models (LLMs) are trained on vast and diverse internet corpora
that often include inaccurate or misleading content. Consequently, LLMs can
generate misinformation, making robust fact-checking essential. This review
systematically analyzes how LLM-generated content is evaluated for factual
accuracy by exploring key challenges such as hallucinations, dataset
limitations, and the reliability of evaluation metrics. The review emphasizes
the need for strong fact-checking frameworks that integrate advanced prompting
strategies, domain-specific fine-tuning, and retrieval-augmented generation
(RAG) methods. It proposes five research questions that guide the analysis of
the recent literature from 2020 to 2025, focusing on evaluation methods and
mitigation techniques. The review also discusses the role of instruction
tuning, multi-agent reasoning, and external knowledge access via RAG
frameworks. Key findings highlight the limitations of current metrics, the
value of grounding outputs with validated external evidence, and the importance
of domain-specific customization to improve factual consistency. Overall, the
review underlines the importance of building LLMs that are not only accurate
and explainable but also tailored for domain-specific fact-checking. These
insights contribute to the advancement of research toward more trustworthy and
context-aware language models.

### 7. Benefit from Rich: Tackling Search Interaction Sparsity in Search Enhanced Recommendation

- **LLM Score**: 6
- **Keyword Score**: 4
- **Authors**: Teng Shi, Weijie Yu, Xiao Zhang, Ming He, Jianping Fan, Jun Xu
- **URL**: <http://arxiv.org/abs/2508.04145v1>
- **Submitted**: 2025-08-06 07:16:40
- **Comment**: Accepted by CIKM 2025
- **Topic Keywords**: rag, recommend, search
- **Reason**: The paper explores search-enhanced recommendation, which is related to information retrieval and search technologies. However, the focus is on leveraging search signals for recommendation, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's use of large language models and vector quantization is also relevant to NLP, but the application is primarily in the recommender systems domain.

#### Abstract
> In modern online platforms, search and recommendation (S&R) often coexist,
offering opportunities for performance improvement through search-enhanced
approaches. Existing studies show that incorporating search signals boosts
recommendation performance. However, the effectiveness of these methods relies
heavily on rich search interactions. They primarily benefit a small subset of
users with abundant search behavior, while offering limited improvements for
the majority of users who exhibit only sparse search activity. To address the
problem of sparse search data in search-enhanced recommendation, we face two
key challenges: (1) how to learn useful search features for users with sparse
search interactions, and (2) how to design effective training objectives under
sparse conditions. Our idea is to leverage the features of users with rich
search interactions to enhance those of users with sparse search interactions.
Based on this idea, we propose GSERec, a method that utilizes message passing
on the User-Code Graphs to alleviate data sparsity in Search-Enhanced
Recommendation. Specifically, we utilize Large Language Models (LLMs) with
vector quantization to generate discrete codes, which connect similar users and
thereby construct the graph. Through message passing on this graph, embeddings
of users with rich search data are propagated to enhance the embeddings of
users with sparse interactions. To further ensure that the message passing
captures meaningful information from truly similar users, we introduce a
contrastive loss to better model user similarities. The enhanced user
representations are then integrated into downstream search-enhanced
recommendation models. Experiments on three real-world datasets show that
GSERec consistently outperforms baselines, especially for users with sparse
search behaviors.

### 8. GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy

- **LLM Score**: 6
- **Keyword Score**: 2
- **Authors**: Hongze Tan, Jianfei Pan
- **URL**: <http://arxiv.org/abs/2508.04349v1>
- **Submitted**: 2025-08-06 11:42:47
- **Topic Keywords**: rag
- **Reason**: The paper explores reinforcement learning with large language models, focusing on fine-grained credit assignment and entropy-weighted rewards. While it's related to query understanding and ranking models, the specific application and techniques are not directly aligned with my research interests in information retrieval and search technologies. The paper's emphasis on deep reasoning and sequence-level optimization is somewhat relevant, but it doesn't seem to address my primary focus areas.

#### Abstract
> Reinforcement learning (RL) with algorithms like Group Relative Policy
Optimization (GRPO) improves Large Language Model (LLM) reasoning, but is
limited by a coarse-grained credit assignment that applies a uniform reward to
all tokens in a sequence. This is a major flaw in long-chain reasoning tasks.
This paper solves this with \textbf{Dynamic Entropy Weighting}. Our core idea
is that high-entropy tokens in correct responses can guide the policy toward a
higher performance ceiling. This allows us to create more fine-grained reward
signals for precise policy updates via two ways: 1) \textbf{Group Token Policy
Optimization} (\textbf{GTPO}), we assigns a entropy-weighted reward to each
token for fine-grained credit assignment. 2) \textbf{Sequence-Level Group
Relative Policy Optimization} (\textbf{GRPO-S}), we assigns a entropy-weighted
reward to each sequence based on its average token entropy. Experiments show
our methods significantly outperform the strong DAPO baseline. The results
confirm that our entropy-weighting mechanism is the key driver of this
performance boost, offering a better path to enhance deep reasoning in models.

### 9. ConvMix: A Mixed-Criteria Data Augmentation Framework for Conversational Dense Retrieval

- **LLM Score**: 4
- **Keyword Score**: 15
- **Authors**: Fengran Mo, Jinghan Zhang, Yuchen Hui, Jia Ao Sun, Zhichao Xu, Zhan Su, Jian-Yun Nie
- **URL**: <http://arxiv.org/abs/2508.04001v1>
- **Submitted**: 2025-08-06 01:28:49
- **Topic Keywords**: retriever, dense retrieval, queries, relevance, retrieval, search
- **Reason**: The paper focuses on conversational dense retrieval, which is related to information retrieval and search technologies. However, the specific context of conversational search and the proposed framework, ConvMix, do not directly align with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat limited to the user's broader interests in NLP and data mining.

#### Abstract
> Conversational search aims to satisfy users' complex information needs via
multiple-turn interactions. The key challenge lies in revealing real users'
search intent from the context-dependent queries. Previous studies achieve
conversational search by fine-tuning a conversational dense retriever with
relevance judgments between pairs of context-dependent queries and documents.
However, this training paradigm encounters data scarcity issues. To this end,
we propose ConvMix, a mixed-criteria framework to augment conversational dense
retrieval, which covers more aspects than existing data augmentation
frameworks. We design a two-sided relevance judgment augmentation schema in a
scalable manner via the aid of large language models. Besides, we integrate the
framework with quality control mechanisms to obtain semantically diverse
samples and near-distribution supervisions to combine various annotated data.
Experimental results on five widely used benchmarks show that the
conversational dense retriever trained by our ConvMix framework outperforms
previous baseline methods, which demonstrates our superior effectiveness.

### 10. Enhancing Serendipity Recommendation System by Constructing Dynamic User Knowledge Graphs with Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 12
- **Authors**: Qian Yong, Yanhui Li, Jialiang Shi, Yaguang Dou, Tian Qi
- **URL**: <http://arxiv.org/abs/2508.04032v1>
- **Submitted**: 2025-08-06 02:52:09
- **Comment**: 8 pages
- **Topic Keywords**: relevance, rag, click, conversion rate, retrieval, recommend
- **Reason**: The paper explores serendipity recommendation systems, leveraging large language models to construct dynamic user knowledge graphs. While it touches on some aspects of query understanding and user behavior modeling, the focus is more on recommender systems and industrial recommendation systems, which is somewhat related to the user's interests in information retrieval and search technologies.

#### Abstract
> The feedback loop in industrial recommendation systems reinforces homogeneous
content, creates filter bubble effects, and diminishes user satisfaction.
Recently, large language models(LLMs) have demonstrated potential in
serendipity recommendation, thanks to their extensive world knowledge and
superior reasoning capabilities. However, these models still face challenges in
ensuring the rationality of the reasoning process, the usefulness of the
reasoning results, and meeting the latency requirements of industrial
recommendation systems (RSs). To address these challenges, we propose a method
that leverages llm to dynamically construct user knowledge graphs, thereby
enhancing the serendipity of recommendation systems. This method comprises a
two stage framework:(1) two-hop interest reasoning, where user static profiles
and historical behaviors are utilized to dynamically construct user knowledge
graphs via llm. Two-hop reasoning, which can enhance the quality and accuracy
of LLM reasoning results, is then performed on the constructed graphs to
identify users' potential interests; and(2) Near-line adaptation, a
cost-effective approach to deploying the aforementioned models in industrial
recommendation systems. We propose a u2i (user-to-item) retrieval model that
also incorporates i2i (item-to-item) retrieval capabilities, the retrieved
items not only exhibit strong relevance to users' newly emerged interests but
also retain the high conversion rate of traditional u2i retrieval. Our online
experiments on the Dewu app, which has tens of millions of users, indicate that
the method increased the exposure novelty rate by 4.62%, the click novelty rate
by 4.85%, the average view duration per person by 0.15%, unique visitor click
through rate by 0.07%, and unique visitor interaction penetration by 0.30%,
enhancing user experience.

### 11. Accelerating Scientific Discovery with Multi-Document Summarization of Impact-Ranked Papers

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Paris Koloveas, Serafeim Chatzopoulos, Dionysis Diamantis, Christos Tryfonopoulos, Thanasis Vergoulis
- **URL**: <http://arxiv.org/abs/2508.03962v1>
- **Submitted**: 2025-08-05 22:56:09
- **Topic Keywords**: ranking, rag, rank, search
- **Reason**: The paper focuses on summarization of scientific papers, which is related to information retrieval and search technologies. However, the specific application to scientific literature and the emphasis on summarization rather than query understanding or ranking models makes it less directly relevant to the user's core research interests.

#### Abstract
> The growing volume of scientific literature makes it challenging for
scientists to move from a list of papers to a synthesized understanding of a
topic. Because of the constant influx of new papers on a daily basis, even if a
scientist identifies a promising set of papers, they still face the tedious
task of individually reading through dozens of titles and abstracts to make
sense of occasionally conflicting findings. To address this critical bottleneck
in the research workflow, we introduce a summarization feature to BIP! Finder,
a scholarly search engine that ranks literature based on distinct impact
aspects like popularity and influence. Our approach enables users to generate
two types of summaries from top-ranked search results: a concise summary for an
instantaneous at-a-glance comprehension and a more comprehensive literature
review-style summary for greater, better-organized comprehension. This ability
dynamically leverages BIP! Finder's already existing impact-based ranking and
filtering features to generate context-sensitive, synthesized narratives that
can significantly accelerate literature discovery and comprehension.

### 12. P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Feifan Song, Bofei Gao, Yifan Song, Yi Liu, Weimin Xiong, Yuyang Song, Tianyu Liu, Guoyin Wang, Houfeng Wang
- **URL**: <http://arxiv.org/abs/2508.04626v1>
- **Submitted**: 2025-08-06 16:51:38
- **Topic Keywords**: ltr, rag, search
- **Reason**: The paper focuses on pre-aligning language model instructions to improve their safety, helpfulness, and honesty. While it touches on the topic of instruction synthesis, it does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval and Search technologies. The paper's focus on language models and instruction synthesis is more relevant to Natural Language Processing and related topics.

#### Abstract
> Large Language Models (LLMs) are expected to produce safe, helpful, and
honest content during interaction with human users, but they frequently fail to
align with such values when given flawed instructions, e.g., missing context,
ambiguous directives, or inappropriate tone, leaving substantial room for
improvement along multiple dimensions. A cost-effective yet high-impact way is
to pre-align instructions before the model begins decoding. Existing approaches
either rely on prohibitive test-time search costs or end-to-end model rewrite,
which is powered by a customized training corpus with unclear objectives. In
this work, we demonstrate that the goal of efficient and effective preference
alignment can be achieved by P-Aligner, a lightweight module generating
instructions that preserve the original intents while being expressed in a more
human-preferred form. P-Aligner is trained on UltraPrompt, a new dataset
synthesized via a proposed principle-guided pipeline using Monte-Carlo Tree
Search, which systematically explores the space of candidate instructions that
are closely tied to human preference. Experiments across different methods show
that P-Aligner generally outperforms strong baselines across various models and
benchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo
and Gemma-2-SimPO, respectively. Further analyses validate its effectiveness
and efficiency through multiple perspectives, including data quality, search
strategies, iterative deployment, and time overhead.

### 13. ViLLA-MMBench: A Unified Benchmark Suite for LLM-Augmented Multimodal Movie Recommendation

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Fatemeh Nazary, Ali Tourani, Yashar Deldjoo, Tommaso Di Noia
- **URL**: <http://arxiv.org/abs/2508.04206v1>
- **Submitted**: 2025-08-06 08:39:07
- **Comment**: 17 pages, 3 figures, 5 tables
- **Topic Keywords**: rag, recommend, rank, search, recsys
- **Reason**: The paper presents a benchmark suite for multimodal movie recommendation, focusing on LLM-augmented models. While it touches on some aspects of information retrieval, such as text embedding and fusion, its primary focus is on recommender systems, which is a secondary interest of yours. The paper's emphasis on multimodal fusion and LLM-based augmentation is not directly related to your core research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Recommending long-form video content demands joint modeling of visual, audio,
and textual modalities, yet most benchmarks address only raw features or narrow
fusion. We present ViLLA-MMBench, a reproducible, extensible benchmark for
LLM-augmented multimodal movie recommendation. Built on MovieLens and MMTF-14K,
it aligns dense item embeddings from three modalities: audio (block-level,
i-vector), visual (CNN, AVF), and text. Missing or sparse metadata is
automatically enriched using state-of-the-art LLMs (e.g., OpenAI Ada),
generating high-quality synopses for thousands of movies. All text (raw or
augmented) is embedded with configurable encoders (Ada, LLaMA-2, Sentence-T5),
producing multiple ready-to-use sets. The pipeline supports interchangeable
early-, mid-, and late-fusion (concatenation, PCA, CCA, rank-aggregation) and
multiple backbones (MF, VAECF, VBPR, AMR, VMF) for ablation. Experiments are
fully declarative via a single YAML file. Evaluation spans accuracy (Recall,
nDCG) and beyond-accuracy metrics: cold-start rate, coverage, novelty,
diversity, fairness. Results show LLM-based augmentation and strong text
embeddings boost cold-start and coverage, especially when fused with
audio-visual features. Systematic benchmarking reveals universal versus
backbone- or metric-specific combinations. Open-source code, embeddings, and
configs enable reproducible, fair multimodal RS research and advance principled
generative AI integration in large-scale recommendation. Code:
https://recsys-lab.github.io/ViLLA-MMBench

### 14. Algorithm Selection for Recommender Systems via Meta-Learning on Algorithm Characteristics

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Jarne Mathi Decker, Joeran Beel
- **URL**: <http://arxiv.org/abs/2508.04419v1>
- **Submitted**: 2025-08-06 13:06:24
- **Topic Keywords**: rag, recommend, acl
- **Reason**: The paper is somewhat related to your interests in Information Retrieval and Search technologies, but it focuses on recommender systems and algorithm selection, which is not a central match. The use of meta-learning and algorithm features is an interesting aspect, but the paper does not address query understanding, ranking models, or user behavior modeling, which are key areas of your research focus.

#### Abstract
> The Algorithm Selection Problem for recommender systems-choosing the best
algorithm for a given user or context-remains a significant challenge.
Traditional meta-learning approaches often treat algorithms as categorical
choices, ignoring their intrinsic properties. Recent work has shown that
explicitly characterizing algorithms with features can improve model
performance in other domains. Building on this, we propose a per-user
meta-learning approach for recommender system selection that leverages both
user meta-features and automatically extracted algorithm features from source
code. Our preliminary results, averaged over six diverse datasets, show that
augmenting a meta-learner with algorithm features improves its average NDCG@10
performance by 8.83% from 0.135 (user features only) to 0.147. This enhanced
model outperforms the Single Best Algorithm baseline (0.131) and successfully
closes 10.5% of the performance gap to a theoretical oracle selector. These
findings show that even static source code metrics provide a valuable
predictive signal, presenting a promising direction for building more robust
and intelligent recommender systems.

### 15. SSEmb: A Joint Structural and Semantic Embedding Framework for Mathematical Formula Retrieval

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Ruyin Li, Xiaoyu Chen
- **URL**: <http://arxiv.org/abs/2508.04162v2>
- **Submitted**: 2025-08-06 07:39:17
- **Topic Keywords**: query, retrieval
- **Reason**: The paper's focus on mathematical formula retrieval and its use of graph contrastive learning and sentence-BERT are somewhat related to my interests in information retrieval and natural language processing. However, the specific domain of mathematical formula retrieval and the lack of relevance to query understanding, ranking models, and user behavior modeling make it less relevant to my core research themes.

#### Abstract
> Formula retrieval is an important topic in Mathematical Information
Retrieval. We propose SSEmb, a novel embedding framework capable of capturing
both structural and semantic features of mathematical formulas. Structurally,
we employ Graph Contrastive Learning to encode formulas represented as Operator
Graphs. To enhance structural diversity while preserving mathematical validity
of these formula graphs, we introduce a novel graph data augmentation approach
through a substitution strategy. Semantically, we utilize Sentence-BERT to
encode the surrounding text of formulas. Finally, for each query and its
candidates, structural and semantic similarities are calculated separately and
then fused through a weighted scheme. In the ARQMath-3 formula retrieval task,
SSEmb outperforms existing embedding-based methods by over 5 percentage points
on P'@10 and nDCG'@10. Furthermore, SSEmb enhances the performance of all runs
of other methods and achieves state-of-the-art results when combined with
Approach0.

### 16. AgREE: Agentic Reasoning for Knowledge Graph Completion on Emerging Entities

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Ruochen Zhao, Simone Conia, Eric Peng, Min Li, Saloni Potdar
- **URL**: <http://arxiv.org/abs/2508.04118v1>
- **Submitted**: 2025-08-06 06:34:22
- **Topic Keywords**: queries, retrieval
- **Reason**: The paper focuses on Knowledge Graph Completion, which is not directly related to Information Retrieval or Search technologies. While it mentions information retrieval as part of the framework, the primary focus is on knowledge graph construction and reasoning. The paper's emphasis on emerging entities and dynamic information environments is somewhat relevant to user behavior modeling, but the connection is loose.

#### Abstract
> Open-domain Knowledge Graph Completion (KGC) faces significant challenges in
an ever-changing world, especially when considering the continual emergence of
new entities in daily news. Existing approaches for KGC mainly rely on
pretrained language models' parametric knowledge, pre-constructed queries, or
single-step retrieval, typically requiring substantial supervision and training
data. Even so, they often fail to capture comprehensive and up-to-date
information about unpopular and/or emerging entities. To this end, we introduce
Agentic Reasoning for Emerging Entities (AgREE), a novel agent-based framework
that combines iterative retrieval actions and multi-step reasoning to
dynamically construct rich knowledge graph triplets. Experiments show that,
despite requiring zero training efforts, AgREE significantly outperforms
existing methods in constructing knowledge graph triplets, especially for
emerging entities that were not seen during language models' training
processes, outperforming previous methods by up to 13.7%. Moreover, we propose
a new evaluation methodology that addresses a fundamental weakness of existing
setups and a new benchmark for KGC on emerging entities. Our work demonstrates
the effectiveness of combining agent-based reasoning with strategic information
retrieval for maintaining up-to-date knowledge graphs in dynamic information
environments.

### 17. FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Thibaut Thonet, Germ√°n Kruszewski, Jos Rozen, Pierre Erbacher, Marc Dymetman
- **URL**: <http://arxiv.org/abs/2508.04698v1>
- **Submitted**: 2025-08-06 17:58:26
- **Topic Keywords**: rag, personalization, search
- **Reason**: The paper focuses on personalized preference alignment with limited data, which is related to user behavior modeling and query understanding in Information Retrieval. However, it does not directly address ranking models or real-time relevance optimization, which are core areas of interest. The paper's emphasis on conversational assistants and LLMs is also outside of the e-commerce domain, although it does touch on personalization, which is a related topic.

#### Abstract
> LLM-powered conversational assistants are often deployed in a
one-size-fits-all manner, which fails to accommodate individual user
preferences. Recently, LLM personalization -- tailoring models to align with
specific user preferences -- has gained increasing attention as a way to bridge
this gap. In this work, we specifically focus on a practical yet challenging
setting where only a small set of preference annotations can be collected per
user -- a problem we define as Personalized Preference Alignment with Limited
Data (PPALLI). To support research in this area, we introduce two datasets --
DnD and ELIP -- and benchmark a variety of alignment techniques on them. We
further propose FaST, a highly parameter-efficient approach that leverages
high-level features automatically discovered from the data, achieving the best
overall performance.

### 18. A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Jiayi Wen, Tianxin Chen, Zhirun Zheng, Cheng Huang
- **URL**: <http://arxiv.org/abs/2508.04276v1>
- **Submitted**: 2025-08-06 10:01:26
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper discusses knowledge poisoning attacks on graph-based retrieval-augmented generation of large language models, which is a topic in Natural Language Processing (NLP). While it touches on the concept of graph construction and manipulation, it does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval (IR).

#### Abstract
> Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as
a promising paradigm for enhancing large language models (LLMs) by converting
raw text into structured knowledge graphs, improving both accuracy and
explainability. However, GraphRAG relies on LLMs to extract knowledge from raw
text during graph construction, and this process can be maliciously manipulated
to implant misleading information. Targeting this attack surface, we propose
two knowledge poisoning attacks (KPAs) and demonstrate that modifying only a
few words in the source text can significantly change the constructed graph,
poison the GraphRAG, and severely mislead downstream reasoning. The first
attack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate
vulnerable nodes in the generated graphs and rewrites the corresponding
narratives with LLMs, achieving precise control over specific
question-answering (QA) outcomes with a success rate of 93.1\%, while keeping
the poisoned text fluent and natural. The second attack, named Universal KPA
(UKPA), exploits linguistic cues such as pronouns and dependency relations to
disrupt the structural integrity of the generated graph by altering globally
influential words. With fewer than 0.05\% of full text modified, the QA
accuracy collapses from 95\% to 50\%. Furthermore, experiments show that
state-of-the-art defense methods fail to detect these attacks, highlighting
that securing GraphRAG pipelines against knowledge poisoning remains largely
unexplored.

### 19. Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Anushka Yadav, Isha Nalawade, Srujana Pillarichety, Yashwanth Babu, Reshmi Ghosh, Samyadeep Basu, Wenlong Zhao, Ali Nasaeh, Sriram Balasubramanian, Soundararajan Srinivasan
- **URL**: <http://arxiv.org/abs/2508.04699v1>
- **Submitted**: 2025-08-06 17:58:36
- **Topic Keywords**: rag, search
- **Reason**: The paper explores the limitations of reasoning models in multi-hop question answering tasks, introducing a novel error categorization framework. While it touches on language models, it doesn't specifically focus on query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval and Search technologies.

#### Abstract
> The emergence of reasoning models and their integration into practical AI
chat bots has led to breakthroughs in solving advanced math, deep search, and
extractive question answering problems that requires a complex and multi-step
thought process. Yet, a complete understanding of why these models hallucinate
more than general purpose language models is missing. In this investigative
study, we systematicallyexplore reasoning failures of contemporary language
models on multi-hop question answering tasks. We introduce a novel, nuanced
error categorization framework that examines failures across three critical
dimensions: the diversity and uniqueness of source documents involved ("hops"),
completeness in capturing relevant information ("coverage"), and cognitive
inefficiency ("overthinking"). Through rigorous hu-man annotation, supported by
complementary automated metrics, our exploration uncovers intricate error
patterns often hidden by accuracy-centric evaluations. This investigative
approach provides deeper insights into the cognitive limitations of current
models and offers actionable guidance toward enhancing reasoning fidelity,
transparency, and robustness in future language modeling efforts.

### 20. Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Mo Li, L. H. Xu, Qitai Tan, Ting Cao, Yunxin Liu
- **URL**: <http://arxiv.org/abs/2508.04664v1>
- **Submitted**: 2025-08-06 17:32:58
- **Comment**: Preprint. Work in progress
- **Topic Keywords**: rag, search
- **Reason**: The paper introduces a framework called Sculptor that enables Large Language Models (LLMs) to actively manage their internal working memory, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on LLMs and cognitive agency is not directly aligned with the user's primary research interests in IR and search technologies.

#### Abstract
> Large Language Models (LLMs) suffer from significant performance degradation
when processing long contexts due to proactive interference, where irrelevant
information in earlier parts of the context disrupts reasoning and memory
recall. While most research focuses on external memory systems to augment LLMs'
capabilities, we propose a complementary approach: empowering LLMs with Active
Context Management (ACM) tools to actively sculpt their internal working
memory. We introduce Sculptor, a framework that equips LLMs with three
categories of tools: (1) context fragmentation, (2) summary, hide, and restore,
and (3) intelligent search. Our approach enables LLMs to proactively manage
their attention and working memory, analogous to how humans selectively focus
on relevant information while filtering out distractions. Experimental
evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and
NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly
improves performance even without specific training, leveraging LLMs' inherent
tool calling generalization capabilities. By enabling Active Context
Management, Sculptor not only mitigates proactive interference but also
provides a cognitive foundation for more reliable reasoning across diverse
long-context tasks-highlighting that explicit context-control strategies,
rather than merely larger token windows, are key to robustness at scale.

### 21. Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL Generation Using Spider

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Chirag Seth, Utkarsh Singh
- **URL**: <http://arxiv.org/abs/2508.04623v1>
- **Submitted**: 2025-08-06 16:49:13
- **Topic Keywords**: query
- **Reason**: The paper focuses on text-to-SQL generation using transformers, which is related to information retrieval and search technologies. However, the primary focus is on natural language processing and text generation, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user.

#### Abstract
> Text-to-SQL translation enables non-expert users to query relational
databases using natural language, with applications in education and business
intelligence. This study evaluates three lightweight transformer models -
T5-Small, BART-Small, and GPT-2 - on the Spider dataset, focusing on
low-resource settings. We developed a reusable, model-agnostic pipeline that
tailors schema formatting to each model's architecture, training them across
1000 to 5000 iterations and evaluating on 1000 test samples using Logical Form
Accuracy (LFAcc), BLEU, and Exact Match (EM) metrics. Fine-tuned T5-Small
achieves the highest LFAcc (27.8%), outperforming BART-Small (23.98%) and GPT-2
(20.1%), highlighting encoder-decoder models' superiority in schema-aware SQL
generation. Despite resource constraints limiting performance, our pipeline's
modularity supports future enhancements, such as advanced schema linking or
alternative base models. This work underscores the potential of compact
transformers for accessible text-to-SQL solutions in resource-scarce
environments.

### 22. Do Recommender Systems Really Leverage Multimodal Content? A Comprehensive Analysis on Multimodal Representations for Recommendation

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Claudio Pomo, Matteo Attimonelli, Danilo Danese, Fedelucio Narducci, Tommaso Di Noia
- **URL**: <http://arxiv.org/abs/2508.04571v1>
- **Submitted**: 2025-08-06 15:53:58
- **Comment**: Accepted as Full Research Papers at CIKM 2025
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on multimodal recommender systems, which is related to your interest in recommender systems. However, the emphasis on multimodal content and LVLMs is not directly aligned with your primary focus on information retrieval, query understanding, and ranking models. The paper's relevance is somewhat limited to your broader interests in NLP and data mining.

#### Abstract
> Multimodal Recommender Systems aim to improve recommendation accuracy by
integrating heterogeneous content, such as images and textual metadata. While
effective, it remains unclear whether their gains stem from true multimodal
understanding or increased model complexity. This work investigates the role of
multimodal item embeddings, emphasizing the semantic informativeness of the
representations. Initial experiments reveal that embeddings from standard
extractors (e.g., ResNet50, Sentence-Bert) enhance performance, but rely on
modality-specific encoders and ad hoc fusion strategies that lack control over
cross-modal alignment. To overcome these limitations, we leverage Large
Vision-Language Models (LVLMs) to generate multimodal-by-design embeddings via
structured prompts. This approach yields semantically aligned representations
without requiring any fusion. Experiments across multiple settings show notable
performance improvements. Furthermore, LVLMs embeddings offer a distinctive
advantage: they can be decoded into structured textual descriptions, enabling
direct assessment of their multimodal comprehension. When such descriptions are
incorporated as side content into recommender systems, they improve
recommendation performance, empirically validating the semantic depth and
alignment encoded within LVLMs outputs. Our study highlights the importance of
semantically rich representations and positions LVLMs as a compelling
foundation for building robust and meaningful multimodal representations in
recommendation tasks.

### 23. Discrete-event Tensor Factorization: Learning a Smooth Embedding for Continuous Domains

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Joey De Pauw, Bart Goethals
- **URL**: <http://arxiv.org/abs/2508.04221v1>
- **Submitted**: 2025-08-06 08:54:57
- **Topic Keywords**: user behavior, recommend
- **Reason**: The paper discusses recommender systems, which is a related topic, but it focuses on modeling time and user preferences, which is not directly aligned with the user's primary interest in information retrieval and query understanding. The paper's emphasis on capturing temporal signals and predicting future user behavior is somewhat relevant, but it does not address the user's core research themes.

#### Abstract
> Recommender systems learn from past user behavior to predict future user
preferences. Intuitively, it has been established that the most recent
interactions are more indicative of future preferences than older interactions.
Many recommendation algorithms use this notion to either drop older
interactions or to assign them a lower weight, so the model can focus on the
more informative, recent information. However, very few approaches model the
flow of time explicitly.
  This paper analyzes how time can be encoded in factorization-style
recommendation models. By including absolute time as a feature, our models can
learn varying user preferences and changing item perception over time. In
addition to simple binning approaches, we also propose a novel, fully
continuous time encoding mechanism. Through the use of a polynomial fit inside
the loss function, our models completely avoid the need for discretization, and
they are able to capture the time dimension in arbitrary resolution.
  We perform a comparative study on three real-world datasets that span
multiple years, where long user histories are present, and items stay relevant
for a longer time. Empirical results show that, by explicitly modeling time,
our models are very effective at capturing temporal signals, such as varying
item popularities over time. Despite this however, our experiments also
indicate that a simple post-hoc popularity adjustment is often sufficient to
achieve the best performance on the unseen test set. This teaches us that, for
the recommendation task, predicting the future is more important than capturing
past trends. As such, we argue that specialized mechanisms are needed for
extrapolation to future data.

### 24. CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Raymond Wilson, Cole Graham, Chase Carter, Zefeng Yang, Ruiqi Gu
- **URL**: <http://arxiv.org/abs/2508.03935v1>
- **Submitted**: 2025-08-05 21:55:44
- **Topic Keywords**: rag, personalization
- **Reason**: The paper focuses on personalized news headline generation using Large Language Models, which is not directly related to my research interests in Information Retrieval, Search technologies, and query understanding. While it touches on user behavior modeling, the context is specific to news headline generation and does not align with my broader interests in e-commerce and real-time relevance optimization.

#### Abstract
> In the era of information overload, personalized news headline generation is
crucial for engaging users by tailoring content to their preferences while
accurately conveying news facts. Existing methods struggle with effectively
capturing complex user interests and ensuring factual consistency, often
leading to generic or misleading headlines. Leveraging the unprecedented
capabilities of Large Language Models (LLMs) in text generation, we propose
Context-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates
user preferences and factual consistency constraints into a powerful
pre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture
long-term user interests, a Context Injection Adapter to seamlessly integrate
these preferences and current article context into the LLM's generation
process, and a Fact-Consistency Reinforcement Module employing a novel
contrastive loss to mitigate hallucination. Evaluated on the real-world PENS
dataset, CAP-LLM achieves state-of-the-art performance across all metrics.
Notably, it significantly improves factual consistency (FactCC of 87.50) over
strong baselines like BART (86.67), while simultaneously enhancing
personalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1
26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations,
and sensitivity analyses further validate the effectiveness of each component
and the robustness of our approach, demonstrating CAP-LLM's ability to achieve
a superior balance between personalization and factual accuracy in news
headline generation.

### 25. HiD-VAE: Interpretable Generative Recommendation via Hierarchical and Disentangled Semantic IDs

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Dengzhao Fang, Jingtong Gao, Chengcheng Zhu, Yu Li, Xiangyu Zhao, Yi Chang
- **URL**: <http://arxiv.org/abs/2508.04618v1>
- **Submitted**: 2025-08-06 16:45:05
- **Topic Keywords**: recommend, rank
- **Reason**: The paper proposes a novel framework for generative recommendation, focusing on hierarchical and disentangled item representations. While it touches on some aspects of information retrieval, such as recommendation and item representation, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user.

#### Abstract
> Recommender systems are indispensable for helping users navigate the immense
item catalogs of modern online platforms. Recently, generative recommendation
has emerged as a promising paradigm, unifying the conventional
retrieve-and-rank pipeline into an end-to-end model capable of dynamic
generation. However, existing generative methods are fundamentally constrained
by their unsupervised tokenization, which generates semantic IDs suffering from
two critical flaws: (1) they are semantically flat and uninterpretable, lacking
a coherent hierarchy, and (2) they are prone to representation entanglement
(i.e., ``ID collisions''), which harms recommendation accuracy and diversity.
To overcome these limitations, we propose HiD-VAE, a novel framework that
learns hierarchically disentangled item representations through two core
innovations. First, HiD-VAE pioneers a hierarchically-supervised quantization
process that aligns discrete codes with multi-level item tags, yielding more
uniform and disentangled IDs. Crucially, the trained codebooks can predict
hierarchical tags, providing a traceable and interpretable semantic path for
each recommendation. Second, to combat representation entanglement, HiD-VAE
incorporates a novel uniqueness loss that directly penalizes latent space
overlap. This mechanism not only resolves the critical ID collision problem but
also promotes recommendation diversity by ensuring a more comprehensive
utilization of the item representation space. These high-quality, disentangled
IDs provide a powerful foundation for downstream generative models. Extensive
experiments on three public benchmarks validate HiD-VAE's superior performance
against state-of-the-art methods. The code is available at
https://anonymous.4open.science/r/HiD-VAE-84B2.

### 26. CALE : Concept-Aligned Embeddings for Both Within-Lemma and Inter-Lemma Sense Differentiation

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Bastien Li√©tard, Gabriel Loiseau
- **URL**: <http://arxiv.org/abs/2508.04494v1>
- **Submitted**: 2025-08-06 14:43:22
- **Comment**: Under review in ARR July 2025
- **Topic Keywords**: rag
- **Reason**: The paper proposes a new approach to lexical semantics, using contextualized language models to capture multiple senses of words and their semantic relations. While it touches on the topic of semantic understanding, it is primarily focused on lexical semantics and does not directly relate to information retrieval, search technologies, or user behavior modeling. The paper's relevance to the user's interests is somewhat limited, but it may still be of interest to those with a broader background in NLP and data mining.

#### Abstract
> Lexical semantics is concerned with both the multiple senses a word can adopt
in different contexts, and the semantic relations that exist between meanings
of different words. To investigate them, Contextualized Language Models are a
valuable tool that provides context-sensitive representations that can be used
to investigate lexical meaning. Recent works like XL-LEXEME have leveraged the
task of Word-in-Context to fine-tune them to get more semantically accurate
representations, but Word-in-Context only compares occurrences of the same
lemma, limiting the range of captured information. In this paper, we propose an
extension, Concept Differentiation, to include inter-words scenarios. We
provide a dataset for this task, derived from SemCor data. Then we fine-tune
several representation models on this dataset. We call these models
Concept-Aligned Embeddings (CALE). By challenging our models and other models
on various lexical semantic tasks, we demonstrate that the proposed models
provide efficient multi-purpose representations of lexical meaning that reach
best performances in our experiments. We also show that CALE's fine-tuning
brings valuable changes to the spatial organization of embeddings.

### 27. TRAIL: Joint Inference and Refinement of Knowledge Graphs with Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Xinkui Zhao, Haode Li, Yifan Zhang, Guanjie Cheng, Yueshen Xu
- **URL**: <http://arxiv.org/abs/2508.04474v1>
- **Submitted**: 2025-08-06 14:25:05
- **Topic Keywords**: retrieval
- **Reason**: The paper proposes a framework for combining large language models with knowledge graphs, which is related to information retrieval and natural language processing. However, the focus on knowledge graphs and reasoning processes is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Recent advances in large language models (LLMs) have unlocked powerful
reasoning and decision-making capabilities. However, their inherent dependence
on static parametric memory fundamentally limits their adaptability, factual
accuracy, and interpretability in knowledge-intensive scenarios. Knowledge
graphs (KGs), as structured repositories of explicit relational knowledge,
offer a promising approach for augmenting LLMs with external, interpretable
memory. Nevertheless, most existing methods that combine LLMs with KGs treat
reasoning and knowledge updating as separate processes, resulting in suboptimal
utilization of new information and hindering real-time updates. In this work,
we propose TRAIL: a novel, unified framework for Thinking, Reasoning, And
Incremental Learning that couples joint inference and dynamic KG refinement
with large language models. TRAIL enables LLM agents to iteratively explore,
update, and refine knowledge graphs during the reasoning process, employing a
confidence-driven mechanism for the generation, validation, and pruning of new
facts. This plug-and-play architecture facilitates seamless integration with
various LLMs, supporting continual adaptation without the need for retraining.
Extensive experiments on multiple benchmarks demonstrate that TRAIL outperforms
existing KG-augmented and retrieval-augmented LLM baselines by 3% to 13%. More
importantly, these results represent a significant step toward developing
adaptive, memory-augmented language models capable of continual learning and
reliable, transparent reasoning.

### 28. Chain of Questions: Guiding Multimodal Curiosity in Language Models

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Nima Iji, Kia Dashtipour
- **URL**: <http://arxiv.org/abs/2508.04350v1>
- **Submitted**: 2025-08-06 11:42:54
- **Topic Keywords**: rag
- **Reason**: The paper explores multimodal language models and curiosity-driven reasoning, which is not directly related to the user's primary focus on Information Retrieval and Search technologies. While the paper touches on query understanding and relevance optimization, the context is different and the techniques are not directly applicable to the user's interests.

#### Abstract
> Reasoning capabilities in large language models (LLMs) have substantially
advanced through methods such as chain-of-thought and explicit step-by-step
explanations. However, these improvements have not yet fully transitioned to
multimodal contexts, where models must proactively decide which sensory
modalities such as vision, audio, or spatial perception to engage when
interacting with complex real-world environments. In this paper, we introduce
the Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach
that encourages multimodal language models to dynamically generate targeted
questions regarding their surroundings. These generated questions guide the
model to selectively activate relevant modalities, thereby gathering critical
information necessary for accurate reasoning and response generation. We
evaluate our framework on a novel multimodal benchmark dataset, assembled by
integrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results
demonstrate that our CoQ method improves a foundation model's ability to
effectively identify and integrate pertinent sensory information. This leads to
improved accuracy, interpretability, and alignment of the reasoning process
with diverse multimodal tasks.

### 29. RAVID: Retrieval-Augmented Visual Detection: A Knowledge-Driven Approach for AI-Generated Image Identification

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Mamadou Keita, Wassim Hamidouche, Hessen Bougueffa Eutamene, Abdelmalik Taleb-Ahmed, Abdenour Hadid
- **URL**: <http://arxiv.org/abs/2508.03967v1>
- **Submitted**: 2025-08-05 23:10:56
- **Topic Keywords**: query, rag, retrieval
- **Reason**: The paper focuses on visual detection and image identification, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's approach, while innovative, does not address query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user.

#### Abstract
> In this paper, we introduce RAVID, the first framework for AI-generated image
detection that leverages visual retrieval-augmented generation (RAG). While RAG
methods have shown promise in mitigating factual inaccuracies in foundation
models, they have primarily focused on text, leaving visual knowledge
underexplored. Meanwhile, existing detection methods, which struggle with
generalization and robustness, often rely on low-level artifacts and
model-specific features, limiting their adaptability. To address this, RAVID
dynamically retrieves relevant images to enhance detection. Our approach
utilizes a fine-tuned CLIP image encoder, RAVID CLIP, enhanced with
category-related prompts to improve representation learning. We further
integrate a vision-language model (VLM) to fuse retrieved images with the
query, enriching the input and improving accuracy. Given a query image, RAVID
generates an embedding using RAVID CLIP, retrieves the most relevant images
from a database, and combines these with the query image to form an enriched
input for a VLM (e.g., Qwen-VL or Openflamingo). Experiments on the
UniversalFakeDetect benchmark, which covers 19 generative models, show that
RAVID achieves state-of-the-art performance with an average accuracy of 93.85%.
RAVID also outperforms traditional methods in terms of robustness, maintaining
high accuracy even under image degradations such as Gaussian blur and JPEG
compression. Specifically, RAVID achieves an average accuracy of 80.27% under
degradation conditions, compared to 63.44% for the state-of-the-art model
C2P-CLIP, demonstrating consistent improvements in both Gaussian blur and JPEG
compression scenarios. The code will be publicly available upon acceptance.

### 30. A Reproducible, Scalable Pipeline for Synthesizing Autoregressive Model Literature

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Faruk Alpay, Bugra Kilictas, Hamdi Alakkad
- **URL**: <http://arxiv.org/abs/2508.04612v1>
- **Submitted**: 2025-08-06 16:33:20
- **Comment**: 9 pages
- **Topic Keywords**: relevance, retrieval, search
- **Reason**: The paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. While it touches on topics like metadata extraction and citation identification, the focus is on autoregressive generative models and reproducibility, which is not a primary area of interest for you.

#### Abstract
> The accelerating pace of research on autoregressive generative models has
produced thousands of papers, making manual literature surveys and reproduction
studies increasingly impractical. We present a fully open-source, reproducible
pipeline that automatically retrieves candidate documents from public
repositories, filters them for relevance, extracts metadata, hyper-parameters
and reported results, clusters topics, produces retrieval-augmented summaries
and generates containerised scripts for re-running selected experiments.
Quantitative evaluation on 50 manually-annotated papers shows F1 scores above
0.85 for relevance classification, hyper-parameter extraction and citation
identification. Experiments on corpora of up to 1000 papers demonstrate
near-linear scalability with eight CPU workers. Three case studies -- AWD-LSTM
on WikiText-2, Transformer-XL on WikiText-103 and an autoregressive music model
on the Lakh MIDI dataset -- confirm that the extracted settings support
faithful reproduction, achieving test perplexities within 1--3% of the original
reports.

### 31. ToolGrad: Efficient Tool-use Dataset Generation with Textual "Gradients"

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Zhongyi Zhou, Kohei Uehara, Haoyu Zhang, Jingtao Zhou, Lin Gu, Ruofei Du, Zheng Xu, Tatsuya Harada
- **URL**: <http://arxiv.org/abs/2508.04086v1>
- **Submitted**: 2025-08-06 05:04:00
- **Topic Keywords**: query, queries
- **Reason**: The paper focuses on generating tool-use datasets for language models, which is not directly related to information retrieval, search technologies, or user behavior modeling. Although it mentions textual 'gradients', the context is unclear and does not seem to be relevant to query understanding, ranking models, or real-time relevance optimization.

#### Abstract
> Prior work synthesizes tool-use LLM datasets by first generating a user
query, followed by complex tool-use annotations like DFS. This leads to
inevitable annotation failures and low efficiency in data generation. We
introduce ToolGrad, an agentic framework that inverts this paradigm. ToolGrad
first constructs valid tool-use chains through an iterative process guided by
textual "gradients", and then synthesizes corresponding user queries. This
"answer-first" approach led to ToolGrad-5k, a dataset generated with more
complex tool use, lower cost, and 100% pass rate. Experiments show that models
trained on ToolGrad-5k outperform those on expensive baseline datasets and
proprietary LLMs, even on OOD benchmarks.

### 32. Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Rohaizah Abdul Wahid, Muhamad Said Nizamuddin Nadim, Suliana Sulaiman, Syahmi Akmal Shaharudin, Muhammad Danial Jupikil, Iqqwan Jasman Su Azlan Su
- **URL**: <http://arxiv.org/abs/2508.04442v1>
- **Submitted**: 2025-08-06 13:30:51
- **Topic Keywords**: rag, retrieval, search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on educational assessment tools and Generative AI for generating multiple-choice questions in Bahasa Melayu, which is outside your primary areas of interest.

#### Abstract
> This paper addresses the critical need for scalable and high-quality
educational assessment tools within the Malaysian education system. It
highlights the potential of Generative AI (GenAI) while acknowledging the
significant challenges of ensuring factual accuracy and curriculum alignment,
especially for low-resource languages like Bahasa Melayu. This research
introduces and compares four incremental pipelines for generating Form 1
Mathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's
GPT-4o. The methods range from non-grounded prompting (structured and basic) to
Retrieval-Augmented Generation (RAG) approaches (one using the LangChain
framework, one implemented manually). The system is grounded in official
curriculum documents, including teacher-prepared notes and the yearly teaching
plan (RPT). A dual-pronged automated evaluation framework is employed to assess
the generated questions. Curriculum alignment is measured using Semantic
Textual Similarity (STS) against the RPT, while contextual validity is verified
through a novel RAG-based Question-Answering (RAG-QA) method. The results
demonstrate that RAG-based pipelines significantly outperform non-grounded
prompting methods, producing questions with higher curriculum alignment and
factual validity. The study further analyzes the trade-offs between the ease of
implementation of framework-based RAG and the fine-grained control offered by a
manual pipeline. This work presents a validated methodology for generating
curriculum-specific educational content in a low-resource language, introduces
a symbiotic RAG-QA evaluation technique, and provides actionable insights for
the development and deployment of practical EdTech solutions in Malaysia and
similar regions.

### 33. Audio Does Matter: Importance-Aware Multi-Granularity Fusion for Video Moment Retrieval

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Junan Lin, Daizong Liu, Xianke Chen, Xiaoye Qu, Xun Yang, Jixiang Zhu, Sanyuan Zhang, Jianfeng Dong
- **URL**: <http://arxiv.org/abs/2508.04273v1>
- **Submitted**: 2025-08-06 09:58:43
- **Comment**: Accepted to ACM MM 2025
- **Topic Keywords**: query, retrieval
- **Reason**: This paper focuses on Video Moment Retrieval, which is not directly related to Information Retrieval or Search technologies. Although it involves multi-modal fusion, the emphasis is on audio-visual fusion for video moment retrieval, which is not a core area of interest for the user. The paper's relevance to the user's research themes is limited.

#### Abstract
> Video Moment Retrieval (VMR) aims to retrieve a specific moment semantically
related to the given query. To tackle this task, most existing VMR methods
solely focus on the visual and textual modalities while neglecting the
complementary but important audio modality. Although a few recent works try to
tackle the joint audio-vision-text reasoning, they treat all modalities equally
and simply embed them without fine-grained interaction for moment retrieval.
These designs are counter-practical as: Not all audios are helpful for video
moment retrieval, and the audio of some videos may be complete noise or
background sound that is meaningless to the moment determination. To this end,
we propose a novel Importance-aware Multi-Granularity fusion model (IMG), which
learns to dynamically and selectively aggregate the audio-vision-text contexts
for VMR. Specifically, after integrating the textual guidance with vision and
audio separately, we first design a pseudo-label-supervised audio importance
predictor that predicts the importance score of the audio, and accordingly
assigns weights to mitigate the interference caused by noisy audio. Then, we
design a multi-granularity audio fusion module that adaptively fuses audio and
visual modalities at local-, event-, and global-level, fully capturing their
complementary contexts. We further propose a cross-modal knowledge distillation
strategy to address the challenge of missing audio modality during inference.
To evaluate our method, we further construct a new VMR dataset, i.e.,
Charades-AudioMatter, where audio-related samples are manually selected and
re-organized from the original Charades-STA to validate the model's capability
in utilizing audio modality. Extensive experiments validate the effectiveness
of our method, achieving state-of-the-art with audio-video fusion in VMR
methods. Our code is available at https://github.com/HuiGuanLab/IMG.

### 34. Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Magauiya Zhussip, Dmitriy Shopkhoev, Ammar Ali, Stamatios Lefkimmiatis
- **URL**: <http://arxiv.org/abs/2508.04581v1>
- **Submitted**: 2025-08-06 16:06:43
- **Topic Keywords**: query, rank
- **Reason**: The paper focuses on transformer weight sharing and compression, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on attention mechanisms, the context is different from the user's interests in ranking models and user behavior modeling.

#### Abstract
> Large language models (LLMs) have revolutionized AI applications, yet their
high computational and memory demands hinder their widespread deployment.
Existing compression techniques focus on intra-block optimizations (e.g.
low-rank approximation, attention head pruning), while the repetitive layered
structure of transformers implies significant inter-block redundancy - a
dimension largely unexplored beyond key-value (KV) caching. Inspired by
dictionary learning in CNNs, we propose a framework for structured weight
sharing across transformer layers. Our approach decomposes attention projection
matrices into shared dictionary atoms, reducing the attention module's
parameters by 66.7% while achieving on-par performance. Unlike complex methods
requiring distillation or architectural changes, MASA (Matrix Atom Sharing in
Attention) operates as a drop-in replacement - trained with standard optimizers
- and represents each layer's weights as linear combinations of shared matrix
atoms. Experiments across scales (100M-700M parameters) show that MASA achieves
better benchmark accuracy and perplexity than grouped-query attention (GQA),
low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at
comparable parameter budgets. Ablation studies confirm robustness to the
dictionary size and the efficacy of shared representations in capturing
cross-layer statistical regularities. Extending to Vision Transformers (ViT),
MASA matches performance metrics on image classification and detection tasks
with 66.7% fewer attention parameters. By combining dictionary learning
strategies with transformer efficiency, MASA offers a scalable blueprint for
parameter-efficient models without sacrificing performance. Finally, we
investigate the possibility of employing MASA on pretrained LLMs to reduce
their number of parameters without experiencing any significant drop in their
performance.

### 35. OS Agents: A Survey on MLLM-based Agents for General Computing Devices Use

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, Yuhuai Li, Shengze Xu, Shenzhi Wang, Xinchen Xu, Shuofei Qiao, Zhaokai Wang, Kun Kuang, Tieyong Zeng, Liang Wang, Jiwei Li, Yuchen Eleanor Jiang, Wangchunshu Zhou, Guoyin Wang, Keting Yin, Zhou Zhao, Hongxia Yang, Fan Wu, Shengyu Zhang, Fei Wu
- **URL**: <http://arxiv.org/abs/2508.04482v1>
- **Submitted**: 2025-08-06 14:33:45
- **Comment**: ACL 2025 (Oral)
- **Topic Keywords**: personalization, search, acl
- **Reason**: The paper focuses on MLLM-based agents for general computing devices, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on AI assistants, the primary focus is on agent construction and evaluation, which is outside the scope of the user's research interests.

#### Abstract
> The dream to create AI assistants as capable and versatile as the fictional
J.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution
of (multi-modal) large language models ((M)LLMs), this dream is closer to
reality, as (M)LLM-based Agents using computing devices (e.g., computers and
mobile phones) by operating within the environments and interfaces (e.g.,
Graphical User Interface (GUI)) provided by operating systems (OS) to automate
tasks have significantly advanced. This paper presents a comprehensive survey
of these advanced agents, designated as OS Agents. We begin by elucidating the
fundamentals of OS Agents, exploring their key components including the
environment, observation space, and action space, and outlining essential
capabilities such as understanding, planning, and grounding. We then examine
methodologies for constructing OS Agents, focusing on domain-specific
foundation models and agent frameworks. A detailed review of evaluation
protocols and benchmarks highlights how OS Agents are assessed across diverse
tasks. Finally, we discuss current challenges and identify promising directions
for future research, including safety and privacy, personalization and
self-evolution. This survey aims to consolidate the state of OS Agents
research, providing insights to guide both academic inquiry and industrial
development. An open-source GitHub repository is maintained as a dynamic
resource to foster further innovation in this field. We present a 9-page
version of our work, accepted by ACL 2025, to provide a concise overview to the
domain.

### 36. Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Noah Ziems, Dilara Soylu, Lakshya A Agrawal, Isaac Miller, Liheng Lai, Chen Qian, Kaiqiang Song, Meng Jiang, Dan Klein, Matei Zaharia, Karel D'Oosterlinck, Christopher Potts, Omar Khattab
- **URL**: <http://arxiv.org/abs/2508.04660v1>
- **Submitted**: 2025-08-06 17:28:31
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on optimizing language models for specific tasks, using a multi-module approach. While it involves some aspects of search and optimization, it is primarily concerned with language model programs and prompt optimization, which is not directly related to the user's interests in Information Retrieval, query understanding, and ranking models.

#### Abstract
> Group Relative Policy Optimization (GRPO) has proven to be an effective tool
for post-training language models (LMs). However, AI systems are increasingly
expressed as modular programs that mix together multiple LM calls with distinct
prompt templates and other tools, and it is not clear how best to leverage GRPO
to improve these systems. We begin to address this challenge by defining
mmGRPO, a simple multi-module generalization of GRPO that groups LM calls by
module across rollouts and handles variable-length and interrupted
trajectories. We find that mmGRPO, composed with automatic prompt optimization,
improves accuracy by 11% on average across classification, many-hop search, and
privacy-preserving delegation tasks against the post-trained LM, and by 5%
against prompt optimization on its own. We open-source mmGRPO in DSPy as the
dspy.GRPO optimizer.

### 37. Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Zhuang Chen, Guanqun Bi, Wen Zhang, Jiawei Hu, Aoyun Wang, Xiyao Xiao, Kun Feng, Minlie Huang
- **URL**: <http://arxiv.org/abs/2508.04531v1>
- **Submitted**: 2025-08-06 15:13:24
- **Topic Keywords**: ctr, search
- **Reason**: The paper is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it mentions Learning to Rank (LLMs) and uses them for psychiatric reasoning, the focus is on clinical depression assessment and diagnosis, which is outside the user's primary research area.

#### Abstract
> Depression is a widespread mental disorder that affects millions worldwide.
While automated depression assessment shows promise, most studies rely on
limited or non-clinically validated data, and often prioritize complex model
design over real-world effectiveness. In this paper, we aim to unveil the
landscape of clinical depression assessment. We introduce C-MIND, a clinical
neuropsychiatric multimodal diagnosis dataset collected over two years from
real hospital visits. Each participant completes three structured psychiatric
tasks and receives a final diagnosis from expert clinicians, with informative
audio, video, transcript, and functional near-infrared spectroscopy (fNIRS)
signals recorded. Using C-MIND, we first analyze behavioral signatures relevant
to diagnosis. We train a range of classical models to quantify how different
tasks and modalities contribute to diagnostic performance, and dissect the
effectiveness of their combinations. We then explore whether LLMs can perform
psychiatric reasoning like clinicians and identify their clear limitations in
realistic clinical settings. In response, we propose to guide the reasoning
process with clinical expertise and consistently improves LLM diagnostic
performance by up to 10% in Macro-F1 score. We aim to build an infrastructure
for clinical depression assessment from both data and algorithmic perspectives,
enabling C-MIND to facilitate grounded and reliable research for mental
healthcare.

### 38. FrEVL: Leveraging Frozen Pretrained Embeddings for Efficient Vision-Language Understanding

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Emmanuelle Bourigault, Pauline Bourigault
- **URL**: <http://arxiv.org/abs/2508.04469v1>
- **Submitted**: 2025-08-06 14:12:05
- **Comment**: 8 pages, 4 figures
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on vision-language understanding, leveraging frozen pretrained embeddings, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper explores efficient multi-modal understanding, it does not address query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user.

#### Abstract
> The deployment of vision-language models remains constrained by substantial
computational requirements. We present \textbf{FrEVL}, a framework exploring
whether frozen pretrained embeddings can support effective vision-language
understanding. Our analysis reveals that frozen embeddings contain rich
information for discriminative tasks, achieving 85\% to 95\% of
state-of-the-art performance on standard benchmarks with only 68.4M trainable
parameters. This performance dichotomy reveals a critical insight: frozen
embedding effectiveness depends on alignment between pretraining objectives and
downstream task requirements. When accounting for end-to-end computation
including embedding extraction, FrEVL provides $2.3\times$ speedup with 52\%
lower energy consumption, making it suitable for scenarios with pre-computable
inputs or when deployment constraints outweigh marginal performance gains. Our
evaluation provides practitioners with guidance on when frozen embedding
approaches represent viable alternatives to full model deployment. We will
release our complete implementation and evaluation framework to facilitate
further research into efficient multi-modal understanding.

### 39. DTPA: Dynamic Token-level Prefix Augmentation for Controllable Text Generation

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Jiabing Yang, Yixiang Chen, Zichen Wen, Chenhang Cui, Peiyan Li, Yuan Xu, Bowen Fang, Yan Huang, Liang Wang
- **URL**: <http://arxiv.org/abs/2508.04047v1>
- **Submitted**: 2025-08-06 03:20:33
- **Topic Keywords**: relevance
- **Reason**: This paper focuses on controllable text generation, a topic in NLP, but it does not relate to information retrieval, search technologies, or query understanding, which are the user's primary research interests. The paper's emphasis on controllable text generation for long-form text also does not align with the user's interests in real-time relevance optimization.

#### Abstract
> Controllable Text Generation (CTG) is a vital subfield in Natural Language
Processing (NLP), aiming to generate text that aligns with desired attributes.
However, previous studies commonly focus on the quality of controllable text
generation for short sequences, while the generation of long-form text remains
largely underexplored. In this paper, we observe that the controllability of
texts generated by the powerful prefix-based method Air-Decoding tends to
decline with increasing sequence length, which we hypothesize primarily arises
from the observed decay in attention to the prefixes. Meanwhile, different
types of prefixes including soft and hard prefixes are also key factors
influencing performance. Building on these insights, we propose a lightweight
and effective framework called Dynamic Token-level Prefix Augmentation (DTPA)
based on Air-Decoding for controllable text generation. Specifically, it first
selects the optimal prefix type for a given task. Then we dynamically amplify
the attention to the prefix for the attribute distribution to enhance
controllability, with a scaling factor growing exponentially as the sequence
length increases. Moreover, based on the task, we optionally apply a similar
augmentation to the original prompt for the raw distribution to balance text
quality. After attribute distribution reconstruction, the generated text
satisfies the attribute constraints well. Experiments on multiple CTG tasks
demonstrate that DTPA generally outperforms other methods in attribute control
while maintaining competitive fluency, diversity, and topic relevance. Further
analysis highlights DTPA's superior effectiveness in long text generation.

### 40. HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yurun Chen, Xavier Hu, Yuhan Liu, Keting Yin, Juncheng Li, Zhuosheng Zhang, Shengyu Zhang
- **URL**: <http://arxiv.org/abs/2508.04010v1>
- **Submitted**: 2025-08-06 01:49:32
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on web agents and policy enhancement, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions optimization, it's not in the context of ranking models or user behavior modeling, and the paper's primary concern is safety and utility in web environments, which is outside the user's primary research interests.

#### Abstract
> Large language models enable agents to autonomously perform tasks in open web
environments. However, as hidden threats within the web evolve, web agents face
the challenge of balancing task performance with emerging risks during
long-sequence operations. Although this challenge is critical, current research
remains limited to single-objective optimization or single-turn scenarios,
lacking the capability for collaborative optimization of both safety and
utility in web environments. To address this gap, we propose HarmonyGuard, a
multi-agent collaborative framework that leverages policy enhancement and
objective optimization to jointly improve both utility and safety. HarmonyGuard
features a multi-agent architecture characterized by two fundamental
capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent
within HarmonyGuard, which automatically extracts and maintains structured
security policies from unstructured external documents, while continuously
updating policies in response to evolving threats. (2) Dual-Objective
Optimization: Based on the dual objectives of safety and utility, the Utility
Agent integrated within HarmonyGuard performs the Markovian real-time reasoning
to evaluate the objectives and utilizes metacognitive capabilities for their
optimization. Extensive evaluations on multiple benchmarks show that
HarmonyGuard improves policy compliance by up to 38% and task completion by up
to 20% over existing baselines, while achieving over 90% policy compliance
across all tasks. Our project is available here:
https://github.com/YurunChen/HarmonyGuard.

### 41. MegaWika 2: A More Comprehensive Multilingual Collection of Articles and their Sources

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Samuel Barham, Chandler May, Benjamin Van Durme
- **URL**: <http://arxiv.org/abs/2508.03828v1>
- **Submitted**: 2025-08-05 18:18:17
- **Topic Keywords**: retrieval, search
- **Reason**: The paper focuses on building a large multilingual dataset for report generation research, fact checking, and analysis, which is not directly related to the user's interests in Information Retrieval, query understanding, ranking models, and user behavior modeling. Although it mentions Wikipedia articles, the primary focus is on data collection and representation, rather than search or retrieval technologies.

#### Abstract
> We introduce MegaWika 2, a large, multilingual dataset of Wikipedia articles
with their citations and scraped web sources; articles are represented in a
rich data structure, and scraped source texts are stored inline with precise
character offsets of their citations in the article text. MegaWika 2 is a major
upgrade from the original MegaWika, spanning six times as many articles and
twice as many fully scraped citations. Both MegaWika and MegaWika 2 support
report generation research ; whereas MegaWika also focused on supporting
question answering and retrieval applications, MegaWika 2 is designed to
support fact checking and analyses across time and language.

### 42. GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yunan Zhang, Shuoran Jiang, Mengchen Zhao, Yuefeng Li, Yang Fan, Xiangping Wu, Qingcai Chen
- **URL**: <http://arxiv.org/abs/2508.04676v1>
- **Submitted**: 2025-08-06 17:42:22
- **Topic Keywords**: rag
- **Reason**: This paper focuses on continual learning of large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions neural states and optimization methods, the context is not relevant to the user's primary research interests.

#### Abstract
> The continual learning capability of large language models (LLMs) is crucial
for advancing artificial general intelligence. However, continual fine-tuning
LLMs across various domains often suffers from catastrophic forgetting,
characterized by: 1) significant forgetting of their general capabilities, and
2) sharp performance declines in previously learned tasks. To simultaneously
address both issues in a simple yet stable manner, we propose General Sample
Replay (GeRe), a framework that use usual pretraining texts for efficient
anti-forgetting. Beyond revisiting the most prevalent replay-based practices
under GeRe, we further leverage neural states to introduce a enhanced
activation states constrained optimization method using threshold-based margin
(TM) loss, which maintains activation state consistency during replay learning.
We are the first to validate that a small, fixed set of pre-collected general
replay samples is sufficient to resolve both concerns--retaining general
capabilities while promoting overall performance across sequential tasks.
Indeed, the former can inherently facilitate the latter. Through controlled
experiments, we systematically compare TM with different replay strategies
under the GeRe framework, including vanilla label fitting, logit imitation via
KL divergence and feature imitation via L1/L2 losses. Results demonstrate that
TM consistently improves performance and exhibits better robustness. Our work
paves the way for efficient replay of LLMs for the future. Our code and data
are available at https://github.com/Qznan/GeRe.

### 43. Can NLP Tackle Hate Speech in the Real World? Stakeholder-Informed Feedback and Survey on Counterspeech

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Tanvi Dinkar, Aiqi Jiang, Simona Frenda, Poppy Gerrard-Abbott, Nancie Gunson, Gavin Abercrombie, Ioannis Konstas
- **URL**: <http://arxiv.org/abs/2508.04638v1>
- **Submitted**: 2025-08-06 17:04:58
- **Topic Keywords**: recommend, search
- **Reason**: The paper focuses on NLP and counterspeech, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and query understanding. While it touches on stakeholder-informed practices, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest for the user.

#### Abstract
> Counterspeech, i.e. the practice of responding to online hate speech, has
gained traction in NLP as a promising intervention. While early work emphasised
collaboration with non-governmental organisation stakeholders, recent research
trends have shifted toward automated pipelines that reuse a small set of legacy
datasets, often without input from affected communities. This paper presents a
systematic review of 74 NLP studies on counterspeech, analysing the extent to
which stakeholder participation influences dataset creation, model development,
and evaluation. To complement this analysis, we conducted a participatory case
study with five NGOs specialising in online Gender-Based Violence (oGBV),
identifying stakeholder-informed practices for counterspeech generation. Our
findings reveal a growing disconnect between current NLP research and the needs
of communities most impacted by toxic online content. We conclude with concrete
recommendations for re-centring stakeholder expertise in counterspeech
research.

### 44. Position: The Current AI Conference Model is Unsustainable! Diagnosing the Crisis of Centralized AI Conference

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Nuo Chen, Moming Duan, Andre Huikai Lin, Qian Wang, Jiaying Wu, Bingsheng He
- **URL**: <http://arxiv.org/abs/2508.04586v1>
- **Submitted**: 2025-08-06 16:08:27
- **Comment**: Preprint
- **Topic Keywords**: search, neurips
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper discusses the sustainability of AI conferences, which is a topic outside your primary focus. While it touches on some general issues related to scientific dissemination and community well-being, it does not address any specific technical aspects of IR or NLP.

#### Abstract
> Artificial Intelligence (AI) conferences are essential for advancing
research, sharing knowledge, and fostering academic community. However, their
rapid expansion has rendered the centralized conference model increasingly
unsustainable. This paper offers a data-driven diagnosis of a structural crisis
that threatens the foundational goals of scientific dissemination, equity, and
community well-being. We identify four key areas of strain: (1) scientifically,
with per-author publication rates more than doubling over the past decade to
over 4.5 papers annually; (2) environmentally, with the carbon footprint of a
single conference exceeding the daily emissions of its host city; (3)
psychologically, with 71% of online community discourse reflecting negative
sentiment and 35% referencing mental health concerns; and (4) logistically,
with attendance at top conferences such as NeurIPS 2024 beginning to outpace
venue capacity. These pressures point to a system that is misaligned with its
core mission. In response, we propose the Community-Federated Conference (CFC)
model, which separates peer review, presentation, and networking into globally
coordinated but locally organized components, offering a more sustainable,
inclusive, and resilient path forward for AI research.

### 45. Analyzing and Mitigating Object Hallucination: A Training Bias Perspective

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yifan Li, Kun Zhou, Wayne Xin Zhao, Lei Fang, Ji-Rong Wen
- **URL**: <http://arxiv.org/abs/2508.04567v1>
- **Submitted**: 2025-08-06 15:51:02
- **Topic Keywords**: rag
- **Reason**: The paper focuses on object hallucination in Large Vision-Language Models, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on training data and bias, the context is specific to computer vision and language models, making it only loosely relevant to the user's research interests.

#### Abstract
> As scaling up training data has significantly improved the general multimodal
capabilities of Large Vision-Language Models (LVLMs), they still suffer from
the hallucination issue, generating text that is inconsistent with the visual
input. This phenomenon motivates us to systematically investigate the role of
training data in hallucination. We introduce a new benchmark, POPEv2, which
consists of counterfactual images collected from the training data of LVLMs
with certain objects masked. Through comprehensive evaluation on POPEv2, we
find that current LVLMs suffer from training bias: they fail to fully leverage
their training data and hallucinate more frequently on images seen during
training. Specifically, they perform poorly on counterfactual images, often
incorrectly answering ``Yes'' to questions about masked objects. To understand
this issue, we conduct probing experiments on the models' internal components,
revealing that this training bias is primarily located in the language modeling
(LM) head. Based on these findings, we propose Obliviate, an efficient and
lightweight unlearning method designed to mitigate object hallucination via
training bias unlearning. Obliviate identifies the discrepancy between
ground-truth labels and model outputs on the training data as a proxy for bias
and adopts a parameter- and data-efficient fine-tuning strategy that only
updates the LM head. Extensive experiments demonstrate the effectiveness of our
approach. While only reusing the training data and updating approximately 2\%
of the parameters, Obliviate significantly reduces hallucination across both
discriminative and generative tasks. Furthermore, it demonstrates strong
scalability with respect to both model size (2B to 72B) and training data
volume, and exhibits promising generalization to hallucination types beyond
object-level hallucination. Our code and data will be publicly released.

### 46. What Do Humans Hear When Interacting? Experiments on Selective Listening for Evaluating ASR of Spoken Dialogue Systems

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Kiyotada Mori, Seiya Kawano, Chaoran Liu, Carlos Toshinori Ishi, Angel Fernando Garcia Contreras, Koichiro Yoshino
- **URL**: <http://arxiv.org/abs/2508.04402v1>
- **Submitted**: 2025-08-06 12:44:57
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, which are the user's core research themes. The focus on spoken dialogue systems and automatic speech recognition is outside the user's primary areas of interest.

#### Abstract
> Spoken dialogue systems (SDSs) utilize automatic speech recognition (ASR) at
the front end of their pipeline. The role of ASR in SDSs is to recognize
information in user speech related to response generation appropriately.
Examining selective listening of humans, which refers to the ability to focus
on and listen to important parts of a conversation during the speech, will
enable us to identify the ASR capabilities required for SDSs and evaluate them.
In this study, we experimentally confirmed selective listening when humans
generate dialogue responses by comparing human transcriptions for generating
dialogue responses and reference transcriptions. Based on our experimental
results, we discuss the possibility of a new ASR evaluation method that
leverages human selective listening, which can identify the gap between
transcription ability between ASR systems and humans.

### 47. ReasoningGuard: Safeguarding Large Reasoning Models with Inference-time Safety Aha Moments

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yuquan Wang, Mi Zhang, Yining Wang, Geng Hong, Xiaoyu You, Min Yang
- **URL**: <http://arxiv.org/abs/2508.04204v1>
- **Submitted**: 2025-08-06 08:35:10
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus on Large Reasoning Models and inference-time safety is outside the scope of the user's research interests.

#### Abstract
> Large Reasoning Models (LRMs) have demonstrated impressive performance in
reasoning-intensive tasks, but they remain vulnerable to harmful content
generation, particularly in the mid-to-late steps of their reasoning processes.
Existing defense mechanisms, however, rely on costly fine-tuning and additional
expert knowledge, which restricts their scalability. In this work, we propose
ReasoningGuard, an inference-time safeguard for LRMs, which injects timely
safety aha moments to steer harmless while helpful reasoning processes.
Leveraging the model's internal attention behavior, our approach accurately
identifies critical points in the reasoning path, and triggers spontaneous,
safety-oriented reflection. To safeguard both the subsequent reasoning steps
and the final answers, we further implement a scaling sampling strategy during
the decoding phase, selecting the optimal reasoning path. Inducing minimal
extra inference cost, ReasoningGuard effectively mitigates three types of
jailbreak attacks, including the latest ones targeting the reasoning process of
LRMs. Our approach outperforms seven existing safeguards, achieving
state-of-the-art safety defenses while effectively avoiding the common
exaggerated safety issues.

### 48. Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Peizheng Guo, Jingyao Wang, Wenwen Qiang, Huijie Guo, Changwen Zheng, Jiahuan Zhou, Gang Hua
- **URL**: <http://arxiv.org/abs/2508.04182v1>
- **Submitted**: 2025-08-06 08:09:12
- **Topic Keywords**: rag
- **Reason**: The paper focuses on multimodal large language models and hallucinations, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on causal completeness and necessity, these concepts are not typically applied in the context of IR or search. The paper's relevance to the user's research interests is limited.

#### Abstract
> Multimodal Large Language Models (MLLMs) have demonstrated impressive
capabilities across vision-language tasks. However, they may suffer from
hallucinations--generating outputs that are semantically inconsistent with the
input image or text. Through causal analyses, we find that: (i) hallucinations
with omission may arise from the failure to adequately capture essential causal
factors, and (ii) hallucinations with fabrication are likely caused by the
model being misled by non-causal cues. To address these challenges, we propose
a novel reinforcement learning framework guided by causal completeness, which
jointly considers both causal sufficiency and causal necessity of tokens.
Specifically, we evaluate each token's standalone contribution and
counterfactual indispensability to define a token-level causal completeness
reward. This reward is used to construct a causally informed advantage function
within the GRPO optimization framework, encouraging the model to focus on
tokens that are both causally sufficient and necessary for accurate generation.
Experimental results across various benchmark datasets and tasks demonstrate
the effectiveness of our approach, which effectively mitigates hallucinations
in MLLMs.

### 49. COPO: Consistency-Aware Policy Optimization

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jinghang Han, Jiawei Chen, Hang Shao, Hao Ma, Mingcheng Li, Xintian Shen, Lihao Zheng, Wei Chen, Tao Wei, Lihua Zhang
- **URL**: <http://arxiv.org/abs/2508.04138v1>
- **Submitted**: 2025-08-06 07:05:18
- **Topic Keywords**: rag
- **Reason**: The paper focuses on reinforcement learning and policy optimization for Large Language Models, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it mentions rewards and optimization, the context is different from the user's research interests.

#### Abstract
> Reinforcement learning has significantly enhanced the reasoning capabilities
of Large Language Models (LLMs) in complex problem-solving tasks. Recently, the
introduction of DeepSeek R1 has inspired a surge of interest in leveraging
rule-based rewards as a low-cost alternative for computing advantage functions
and guiding policy optimization. However, a common challenge observed across
many replication and extension efforts is that when multiple sampled responses
under a single prompt converge to identical outcomes, whether correct or
incorrect, the group-based advantage degenerates to zero. This leads to
vanishing gradients and renders the corresponding samples ineffective for
learning, ultimately limiting training efficiency and downstream performance.
To address this issue, we propose a consistency-aware policy optimization
framework that introduces a structured global reward based on outcome
consistency, the global loss based on it ensures that, even when model outputs
show high intra-group consistency, the training process still receives
meaningful learning signals, which encourages the generation of correct and
self-consistent reasoning paths from a global perspective. Furthermore, we
incorporate an entropy-based soft blending mechanism that adaptively balances
local advantage estimation with global optimization, enabling dynamic
transitions between exploration and convergence throughout training. Our method
introduces several key innovations in both reward design and optimization
strategy. We validate its effectiveness through substantial performance gains
on multiple mathematical reasoning benchmarks, highlighting the proposed
framework's robustness and general applicability. Code of this work has been
released at https://github.com/hijih/copo-code.git.

### 50. Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zhiwen Ruan, Yun Chen, Yutao Hou, Peng Li, Yang Liu, Guanhua Chen
- **URL**: <http://arxiv.org/abs/2508.04117v1>
- **Submitted**: 2025-08-06 06:34:12
- **Topic Keywords**: recommend, search
- **Reason**: This paper focuses on the phenomenon of over-memorization in fine-tuning large language models for reasoning tasks, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on machine learning models, it does not address ranking models, user behavior modeling, or real-time relevance optimization, making it only loosely relevant to the user's research interests.

#### Abstract
> The pretrained large language models (LLMs) are finetuned with labeled data
for better instruction following ability and alignment with human values. In
this paper, we study the learning dynamics of LLM finetuning on reasoning tasks
and reveal the uncovered over-memorization phenomenon during a specific stage
of LLM finetuning. At this stage, the LLMs have excessively memorized training
data and exhibit high test perplexity while maintaining good test accuracy. We
investigate the conditions that lead to LLM over-memorization and find that
training epochs and large learning rates contribute to this issue. Although
models with over-memorization demonstrate comparable test accuracy to normal
models, they suffer from reduced robustness, poor out-of-distribution
generalization, and decreased generation diversity. Our experiments unveil the
over-memorization to be broadly applicable across different tasks, models, and
finetuning methods. Our research highlights that overparameterized, extensively
finetuned LLMs exhibit unique learning dynamics distinct from traditional
machine learning models. Based on our observations of over-memorization, we
provide recommendations on checkpoint and learning rate selection during
finetuning.

---

