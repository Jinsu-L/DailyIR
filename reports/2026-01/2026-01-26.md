# Daily Papers Report - 2026-01-26

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. LegalMALR:Multi-Agent Query Understanding and LLM-Based Reranking for Chinese Statute Retrieval

- **LLM Score**: 9
- **Keyword Score**: 23
- **Authors**: Yunhan Li, Mingjie Xie, Gaoli Kang, Zihan Gong, Gengshen Wu, Min Yang
- **URL**: <http://arxiv.org/abs/2601.17692v1>
- **Submitted**: 2026-01-25 04:44:56
- **Comment**: 31pages, 4 figures
- **Topic Keywords**: retriever, dense retrieval, query, queries, ranking, rerank, rag, retrieval, rank
- **Reason**: This paper is extremely relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The use of multi-agent query understanding and large-language-model-based reranking for statute retrieval aligns with your focus on deep semantic understanding and real-time relevance optimization. The application of reinforcement learning and evaluation on a specific dataset also resonates with your background in e-commerce and NLP.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: LegalMALR: Multi‚ÄëAgent Retrieval for Legal Statutes
- **Aim**: Improve retrieval of relevant statutory elements for implicit, multi‚Äëissue, colloquial legal queries by combining query reformulation, dense retrieval, and legal reasoning.
- **Rationale**: Standard retrieval‚Äëaugmented generation pipelines struggle with implicit, colloquial queries; dense retrievers capture only surface matches and lightweight rerankers lack legal reasoning, leading to poor coverage and ranking of relevant statutes.
- **Ground**: LegalMALR employs a Multi‚ÄëAgent Query Understanding System (MAS) that generates diverse, legally grounded reformulations and performs iterative dense retrieval, and a zero‚Äëshot LLM reranker that applies natural‚Äëlanguage legal reasoning. MAS policy is optimized with Generalized Reinforcement Policy Optimization (GRPO) to reduce stochastic variability in rewrites.
- **Experiment**: Evaluated on the new CSAID dataset (118 challenging Chinese legal queries with multiple statutory labels) and the public STARD benchmark. LegalMALR outperforms strong RAG baselines in both in‚Äëdistribution and out‚Äëof‚Äëdistribution scenarios.
- **Takeaway**: Combining multi‚Äëperspective query reformulation, reinforcement‚Äëbased policy optimisation, and large‚Äëmodel legal reasoning yields superior statute retrieval performance over existing RAG approaches.

#### Abstract
> Statute retrieval is essential for legal assistance and judicial decision support, yet real-world legal queries are often implicit, multi-issue, and expressed in colloquial or underspecified forms. These characteristics make it difficult for conventional retrieval-augmented generation pipelines to recover the statutory elements required for accurate retrieval. Dense retrievers focus primarily on the literal surface form of the query, whereas lightweight rerankers lack the legal-reasoning capacity needed to assess statutory applicability. We present LegalMALR, a retrieval framework that integrates a Multi-Agent Query Understanding System (MAS) with a zero-shot large-language-model-based reranking module (LLM Reranker). MAS generates diverse, legally grounded reformulations and conducts iterative dense retrieval to broaden candidate coverage. To stabilise the stochastic behaviour of LLM-generated rewrites, we optimise a unified MAS policy using Generalized Reinforcement Policy Optimization(GRPO). The accumulated candidate set is subsequently evaluated by the LLM Reranker, which performs natural-language legal reasoning to produce the final ranking. We further construct CSAID, a dataset of 118 difficult Chinese legal queries annotated with multiple statutory labels, and evaluate LegalMALR on both CSAID and the public STARD benchmark. Experiments show that LegalMALR substantially outperforms strong Retrieval-augmented generation(RAG) baselines in both in-distribution and out-of-distribution settings, demonstrating the effectiveness of combining multi-perspective query interpretation, reinforcement-based policy optimisation, and large-model reranking for statute retrieval.

---

### 2. Agentic Search in the Wild: Intents and Trajectory Dynamics from 14M+ Real Search Requests

- **LLM Score**: 9
- **Keyword Score**: 8
- **Authors**: Jingjie Ning, Jo√£o Coelho, Yibo Kong, Yunfan Long, Bruno Martins, Jo√£o Magalh√£es, Jamie Callan, Chenyan Xiong
- **URL**: <http://arxiv.org/abs/2601.17617v1>
- **Submitted**: 2026-01-24 22:42:43
- **Topic Keywords**: query, rag, retrieval, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of query understanding, ranking models, and user behavior modeling. The study of agentic search sessions and the analysis of user behavior, intent, and query reformulation aligns with your focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> LLM-powered search agents are increasingly being used for multi-step information seeking tasks, yet the IR community lacks empirical understanding of how agentic search sessions unfold and how retrieved evidence is used. This paper presents a large-scale log analysis of agentic search based on 14.44M search requests (3.97M sessions) collected from DeepResearchGym, i.e. an open-source search API accessed by external agentic clients. We sessionize the logs, assign session-level intents and step-wise query-reformulation labels using LLM-based annotation, and propose Context-driven Term Adoption Rate (CTAR) to quantify whether newly introduced query terms are traceable to previously retrieved evidence. Our analyses reveal distinctive behavioral patterns. First, over 90% of multi-turn sessions contain at most ten steps, and 89% of inter-step intervals fall under one minute. Second, behavior varies by intent. Fact-seeking sessions exhibit high repetition that increases over time, while sessions requiring reasoning sustain broader exploration. Third, agents reuse evidence across steps. On average, 54% of newly introduced query terms appear in the accumulated evidence context, with contributions from earlier steps beyond the most recent retrieval. The findings suggest that agentic search may benefit from repetition-aware early stopping, intent-adaptive retrieval budgets, and explicit cross-step context tracking. We plan to release the anonymized logs to support future research.

---

### 3. PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation

- **LLM Score**: 8
- **Keyword Score**: 18
- **Authors**: Abhishek Divekar, Anirban Majumder
- **URL**: <http://arxiv.org/abs/2601.18777v1>
- **Submitted**: 2026-01-26 18:46:49
- **Comment**: Accepted at AAAI 2026 - Innovative Applications of AI (IAAI-26)
- **Topic Keywords**: query, queries, ranking, relevance, rag, retrieval, rank, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of ranking models and evaluation metrics. The proposed framework, PRECISE, addresses the issue of bias in Large Language Model (LLM) evaluations, which is a critical aspect of query understanding and ranking models. The application of PRECISE to a query reformulation application also aligns with your interests in real-time relevance optimization.

#### Abstract
> Evaluating the quality of search, ranking and RAG systems traditionally requires a significant number of human relevance annotations. In recent times, several deployed systems have explored the usage of Large Language Models (LLMs) as automated judges for this task while their inherent biases prevent direct use for metric estimation. We present a statistical framework extending Prediction-Powered Inference (PPI) that combines minimal human annotations with LLM judgments to produce reliable estimates of metrics which require sub-instance annotations. Our method requires as few as 100 human-annotated queries and 10,000 unlabeled examples, reducing annotation requirements significantly compared to traditional approaches. We formulate our proposed framework (PRECISE) for inference of relevance uplift for an LLM-based query reformulation application, extending PPI to sub-instance annotations at the query-document level. By reformulating the metric-integration space, we reduced the computational complexity from O(2^|C|) to O(2^K), where |C| represents corpus size (in order of millions). Detailed experiments across prominent retrieval datasets demonstrate that our method reduces the variance of estimates for the business-critical Precision@K metric, while effectively correcting for LLM bias in low-resource settings.

---

### 4. Less is More for RAG: Information Gain Pruning for Generator-Aligned Reranking and Evidence Selection

- **LLM Score**: 8
- **Keyword Score**: 17
- **Authors**: Zhipeng Song, Yizhi Zhou, Xiangyu Kong, Jiulong Jiao, Xinrui Bao, Xu You, Xueqing Shi, Yuhang Zhou, Heng Qi
- **URL**: <http://arxiv.org/abs/2601.17532v1>
- **Submitted**: 2026-01-24 17:14:10
- **Comment**: 26 pages, 10 figures
- **Topic Keywords**: retriever, ranking, rerank, relevance, rag, retrieval, rank
- **Reason**: This paper is highly relevant to Information Retrieval, specifically in the context of retrieval-augmented generation (RAG) and evidence selection. The proposed Information Gain Pruning (IGP) method aligns with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's focus on optimizing the quality-cost trade-off in open-domain QA settings is also closely related to the user's research themes.

#### Abstract
> Retrieval-augmented generation (RAG) grounds large language models with external evidence, but under a limited context budget, the key challenge is deciding which retrieved passages should be injected. We show that retrieval relevance metrics (e.g., NDCG) correlate weakly with end-to-end QA quality and can even become negatively correlated under multi-passage injection, where redundancy and mild conflicts destabilize generation. We propose \textbf{Information Gain Pruning (IGP)}, a deployment-friendly reranking-and-pruning module that selects evidence using a generator-aligned utility signal and filters weak or harmful passages before truncation, without changing existing budget interfaces. Across five open-domain QA benchmarks and multiple retrievers and generators, IGP consistently improves the quality--cost trade-off. In a representative multi-evidence setting, IGP delivers about +12--20% relative improvement in average F1 while reducing final-stage input tokens by roughly 76--79% compared to retriever-only baselines.

---

### 5. GenCI: Generative Modeling of User Interest Shift via Cohort-based Intent Learning for CTR Prediction

- **LLM Score**: 8
- **Keyword Score**: 13
- **Authors**: Kesha Ou, Zhen Tian, Wayne Xin Zhao, Hongyu Lu, Ji-Rong Wen
- **URL**: <http://arxiv.org/abs/2601.18251v1>
- **Submitted**: 2026-01-26 08:15:04
- **Comment**: Accepted by WWW 2026 Research Track
- **Topic Keywords**: ranking, rag, click, ctr, click-through rate, recommend, rank
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The focus on click-through rate prediction, user interest shifts, and real-time relevance optimization aligns well with your expertise in e-commerce and IR. The use of generative models and semantic interest cohorts is an innovative approach that leverages deep semantic understanding.

#### Abstract
> Click-through rate (CTR) prediction plays a pivotal role in online advertising and recommender systems. Despite notable progress in modeling user preferences from historical behaviors, two key challenges persist. First, exsiting discriminative paradigms focus on matching candidates to user history, often overfitting to historically dominant features and failing to adapt to rapid interest shifts. Second, a critical information chasm emerges from the point-wise ranking paradigm. By scoring each candidate in isolation, CTR models discard the rich contextual signal implied by the recalled set as a whole, leading to a misalignment where long-term preferences often override the user's immediate, evolving intent. To address these issues, we propose GenCI, a generative user intent framework that leverages semantic interest cohorts to model dynamic user preferences for CTR prediction. The framework first employs a generative model, trained with a next-item prediction (NTP) objective, to proactively produce candidate interest cohorts. These cohorts serve as explicit, candidate-agnostic representations of a user's immediate intent. A hierarchical candidate-aware network then injects this rich contextual signal into the ranking stage, refining them with cross-attention to align with both user history and the target item. The entire model is trained end-to-end, creating a more aligned and effective CTR prediction pipeline. Extensive experiments on three widely used datasets demonstrate the effectiveness of our approach.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. FinMetaMind: A Tech Blueprint on NLQ Systems for Financial Knowledge Search

- **LLM Score**: 8
- **Keyword Score**: 13
- **Authors**: Lalit Pant, Shivang Nagar
- **URL**: <http://arxiv.org/abs/2601.17333v1>
- **Submitted**: 2026-01-24 06:30:26
- **Comment**: 8 pages, 8 figures, Information Retrieval, Natural Language Query, Vector Search, Embeddings, Named Entity Recognition, Large Language Models
- **Topic Keywords**: query, ranking, relevance, retrieval, rank, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The focus on Natural Language Query (NLQ) systems and their application in financial knowledge search aligns with your interests in deep semantic understanding and real-time relevance optimization. However, the specific domain of financial services is not a primary focus of your research, which is why the score is not a perfect 10.

#### Abstract
> Natural Language Query (NLQ) allows users to search and interact with information systems using plain, human language instead of structured query syntax. This paper presents a technical blueprint on the design of a modern NLQ system tailored to financial knowledge search. The introduction of NLQ not only enhances the precision and recall of the knowledge search compared to traditional methods, but also facilitates deeper insights by efficiently linking disparate financial objects, events, and relationships. Using core constructs from natural language processing, search engineering, and vector data models, the proposed system aims to address key challenges in discovering, relevance ranking, data freshness, and entity recognition intrinsic to financial data retrieval. In this work, we detail the unique requirements of NLQ for financial datasets and documents, outline the architectural components for offline indexing and online retrieval, and discuss the real-world use cases of enhanced knowledge search in financial services. We delve into the theoretical underpinnings and experimental evidence supporting our proposed architecture, ultimately providing a comprehensive analysis on the subject matter. We also provide a detailed elaboration of our experimental methodology, the data used, the results and future optimizations in this study.

### 7. DF-RAG: Query-Aware Diversity for Retrieval-Augmented Generation

- **LLM Score**: 8
- **Keyword Score**: 12
- **Authors**: Saadat Hasan Khan, Spencer Hong, Jingyu Wu, Kevin Lybarger, Youbing Yin, Erin Babinsky, Daben Liu
- **URL**: <http://arxiv.org/abs/2601.17212v1>
- **Submitted**: 2026-01-23 22:47:16
- **Comment**: Accepted to Findings of EACL 2026
- **Topic Keywords**: query, relevance, rag, retrieval, acl
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The focus on diversity in retrieval-augmented generation aligns with your interests in real-time relevance optimization and deep semantic understanding. However, the specific application to question-answering and generation may not be a central match to your primary focus on search technologies.

#### Abstract
> Retrieval-augmented generation (RAG) is a common technique for grounding language model outputs in domain-specific information. However, RAG is often challenged by reasoning-intensive question-answering (QA), since common retrieval methods like cosine similarity maximize relevance at the cost of introducing redundant content, which can reduce information recall. To address this, we introduce Diversity-Focused Retrieval-Augmented Generation (DF-RAG), which systematically incorporates diversity into the retrieval step to improve performance on complex, reasoning-intensive QA benchmarks. DF-RAG builds upon the Maximal Marginal Relevance framework to select information chunks that are both relevant to the query and maximally dissimilar from each other. A key innovation of DF-RAG is its ability to optimize the level of diversity for each query dynamically at test time without requiring any additional fine-tuning or prior information. We show that DF-RAG improves F1 performance on reasoning-intensive QA benchmarks by 4-10 percent over vanilla RAG using cosine similarity and also outperforms other established baselines. Furthermore, we estimate an Oracle ceiling of up to 18 percent absolute F1 gains over vanilla RAG, of which DF-RAG captures up to 91.3 percent.

### 8. Think When Needed: Model-Aware Reasoning Routing for LLM-based Ranking

- **LLM Score**: 8
- **Keyword Score**: 9
- **Authors**: Huizhong Guo, Tianjun Wei, Dongxia Wang, Yingpeng Du, Ziyan Wang, Jie Zhang, Zhu Sun
- **URL**: <http://arxiv.org/abs/2601.18146v1>
- **Submitted**: 2026-01-26 05:09:07
- **Topic Keywords**: ranking, rag, retrieval, recommend, rank
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in ranking models and real-time relevance optimization. The proposed reasoning routing framework for LLM-based ranking aligns with your focus on deep semantic understanding and query understanding. The paper's emphasis on model-aware difficulty signals and adaptive operating policy also resonates with your interests in user behavior modeling and click models.

#### Abstract
> Large language models (LLMs) are increasingly applied to ranking tasks in retrieval and recommendation. Although reasoning prompting can enhance ranking utility, our preliminary exploration reveals that its benefits are inconsistent and come at a substantial computational cost, suggesting that when to reason is as crucial as how to reason. To address this issue, we propose a reasoning routing framework that employs a lightweight, plug-and-play router head to decide whether to use direct inference (Non-Think) or reasoning (Think) for each instance before generation. The router head relies solely on pre-generation signals: i) compact ranking-aware features (e.g., candidate dispersion) and ii) model-aware difficulty signals derived from a diagnostic checklist reflecting the model's estimated need for reasoning. By leveraging these features before generation, the router outputs a controllable token that determines whether to apply the Think mode. Furthermore, the router can adaptively select its operating policy along the validation Pareto frontier during deployment, enabling dynamic allocation of computational resources toward instances most likely to benefit from Think under varying system constraints. Experiments on three public ranking datasets with different scales of open-source LLMs show consistent improvements in ranking utility with reduced token consumption (e.g., +6.3\% NDCG@10 with -49.5\% tokens on MovieLens with Qwen3-4B), demonstrating reasoning routing as a practical solution to the accuracy-efficiency trade-off.

### 9. To Case or Not to Case: An Empirical Study in Learned Sparse Retrieval

- **LLM Score**: 8
- **Keyword Score**: 9
- **Authors**: Emmanouil Georgios Lionis, Jia-Huei Ju, Angelos Nalmpantis, Casper Thuis, Sean MacAvaney, Andrew Yates
- **URL**: <http://arxiv.org/abs/2601.17500v1>
- **Submitted**: 2026-01-24 15:58:10
- **Comment**: This preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this contribution is published in ECIR2026 (Part I) Advances in Information Retrieval
- **Topic Keywords**: sparse retrieval, queries, retrieval, search
- **Reason**: This paper is highly relevant to Information Retrieval, specifically Learned Sparse Retrieval methods, which aligns with your research interests. The study explores the impact of backbone model casing on LSR, a topic that requires deep semantic understanding and real-time relevance optimization, which is a key area of focus for you. However, the paper's focus is more on the technical aspects of LSR rather than user behavior modeling or query understanding.

#### Abstract
> Learned Sparse Retrieval (LSR) methods construct sparse lexical representations of queries and documents that can be efficiently searched using inverted indexes. Existing LSR approaches have relied almost exclusively on uncased backbone models, whose vocabularies exclude case-sensitive distinctions, thereby reducing vocabulary mismatch. However, the most recent state-of-the-art language models are only available in cased versions. Despite this shift, the impact of backbone model casing on LSR has not been studied, potentially posing a risk to the viability of the method going forward. To fill this gap, we systematically evaluate paired cased and uncased versions of the same backbone models across multiple datasets to assess their suitability for LSR. Our findings show that LSR models with cased backbone models by default perform substantially worse than their uncased counterparts; however, this gap can be eliminated by pre-processing the text to lowercase. Moreover, our token-level analysis reveals that, under lowercasing, cased models almost entirely suppress cased vocabulary items and behave effectively as uncased models, explaining their restored performance. This result broadens the applicability of recent cased models to the LSR setting and facilitates the integration of stronger backbone architectures into sparse retrieval. The complete code and implementation for this project are available at: https://github.com/lionisakis/Uncased-vs-cased-models-in-LSR

### 10. Beyond Correlations: A Downstream Evaluation Framework for Query Performance Prediction

- **LLM Score**: 8
- **Keyword Score**: 9
- **Authors**: Payel Santra, Partha Basuchowdhuri, Debasis Ganguly
- **URL**: <http://arxiv.org/abs/2601.17339v1>
- **Submitted**: 2026-01-24 06:58:30
- **Topic Keywords**: query, queries, retrieval, rank
- **Reason**: This paper is highly relevant to the field of Information Retrieval, specifically focusing on query performance prediction and its downstream applications. The proposed evaluation framework and experimental results demonstrate a clear connection to the user's interests in query understanding, ranking models, and IR pipeline optimization. While not directly related to query understanding or ranking models, the paper's emphasis on IR pipeline optimization and real-time relevance optimization makes it a useful contribution to the field.

#### Abstract
> The standard practice of query performance prediction (QPP) evaluation is to measure a set-level correlation between the estimated retrieval qualities and the true ones. However, neither this correlation-based evaluation measure quantifies QPP effectiveness at the level of individual queries, nor does this connect to a downstream application, meaning that QPP methods yielding high correlation values may not find a practical application in query-specific decisions in an IR pipeline. In this paper, we propose a downstream-focussed evaluation framework where a distribution of QPP estimates across a list of top-documents retrieved with several rankers is used as priors for IR fusion. While on the one hand, a distribution of these estimates closely matching that of the true retrieval qualities indicates the quality of the predictor, their usage as priors on the other hand indicates a predictor's ability to make informed choices in an IR pipeline. Our experiments firstly establish the importance of QPP estimates in weighted IR fusion, yielding substantial improvements of over 4.5% over unweighted CombSUM and RRF fusion strategies, and secondly, reveal new insights that the downstream effectiveness of QPP does not correlate well with the standard correlation-based QPP evaluation.

### 11. PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR

- **LLM Score**: 8
- **Keyword Score**: 6
- **Authors**: James Burgess, Jan N. Hansen, Duo Peng, Yuhui Zhang, Alejandro Lozano, Min Woo Sun, Emma Lundberg, Serena Yeung-Levy
- **URL**: <http://arxiv.org/abs/2601.18207v1>
- **Submitted**: 2026-01-26 06:46:16
- **Comment**: EACL 2026
- **Topic Keywords**: relevance, retrieval, search
- **Reason**: This paper aligns with your interests in Information Retrieval, particularly in query understanding and ranking models, as it proposes a method for training search agents to reason over scientific papers. The focus on biomedical paper abstracts and factoid QA also relates to your background in the e-commerce domain and interest in deep semantic understanding. However, the specific application to scientific papers and AI systems is somewhat niche, preventing a higher score.

#### Abstract
> Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on https://huggingface.co/collections/jmhb/papersearchqa. Finally, our data creation methods are scalable and easily extendable to other scientific domains.

### 12. Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory

- **LLM Score**: 8
- **Keyword Score**: 5
- **Authors**: Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu, Yuefeng Huang, Xinyi Wang, Jiannan Cao, Jianwei Yin, Xuhong Zhang
- **URL**: <http://arxiv.org/abs/2601.18771v1>
- **Submitted**: 2026-01-26 18:42:33
- **Comment**: Dep-Search 1st version
- **Topic Keywords**: rag, retrieval, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The proposed Dep-Search framework advances search-based frameworks by integrating structured reasoning, retrieval, and persistent memory, which aligns with your focus on deep semantic understanding and real-time relevance optimization. However, the paper's primary focus on question answering and multi-hop reasoning tasks is somewhat distinct from your e-commerce background.

#### Abstract
> Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs' ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.

### 13. SD-E$^2$: Semantic Exploration for Reasoning Under Token Budgets

- **LLM Score**: 8
- **Keyword Score**: 5
- **Authors**: Kshitij Mishra, Nils Lukas, Salem Lahlou
- **URL**: <http://arxiv.org/abs/2601.17982v1>
- **Submitted**: 2026-01-25 20:21:52
- **Comment**: Accepted at EACL 2026
- **Topic Keywords**: pairwise, rag
- **Reason**: This paper introduces a reinforcement learning framework for semantic exploration in small language models, which aligns with your interest in query understanding and ranking models. Although it's not directly related to e-commerce, it explores real-time relevance optimization and deep semantic understanding, making it relevant to your broader research interests in Information Retrieval and Natural Language Processing.

#### Abstract
> Small language models (SLMs) struggle with complex reasoning because exploration is expensive under tight compute budgets. We introduce Semantic Diversity-Exploration-Exploitation (SD-E$^2$), a reinforcement learning framework that makes exploration explicit by optimizing semantic diversity in generated reasoning trajectories. Using a frozen sentence-embedding model, SD-E$^2$ assigns a diversity reward that captures (i) the coverage of semantically distinct solution strategies and (ii) their average pairwise dissimilarity in embedding space, rather than surface-form novelty. This diversity reward is combined with outcome correctness and solution efficiency in a z-score-normalized multi-objective objective that stabilizes training. On GSM8K, SD-E$^2$ surpasses the base Qwen2.5-3B-Instruct and strong GRPO baselines (GRPO-CFL and GRPO-CFEE) by +27.4, +5.2, and +1.5 percentage points, respectively, while discovering on average 9.8 semantically distinct strategies per question. We further improve MedMCQA to 49.64% versus 38.37% for the base model and show gains on the harder AIME benchmark (1983-2025), reaching 13.28% versus 6.74% for the base. These results indicate that rewarding semantic novelty yields a more compute-efficient exploration-exploitation signal for training reasoning-capable SLMs. By introducing cognitive adaptation-adjusting the reasoning process structure rather than per-token computation-SD-E$^2$ offers a complementary path to efficiency gains in resource-constrained models.

### 14. D-Models and E-Models: Diversity-Stability Trade-offs in the Sampling Behavior of Large Language Models

- **LLM Score**: 8
- **Keyword Score**: 5
- **Authors**: Jia Gu, Liang Pang, Huawei Shen, Xueqi Cheng
- **URL**: <http://arxiv.org/abs/2601.17865v1>
- **Submitted**: 2026-01-25 14:59:09
- **Comment**: 12 pages, 10 figures. Accepted by WWW'26
- **Topic Keywords**: relevance, recommend, search
- **Reason**: This paper explores the probabilistic sampling behavior of large language models, which is related to query understanding and ranking models in Information Retrieval. Although it's not directly focused on search technologies or user behavior modeling, it provides foundational insights that can inform model selection and configuration for web-scale applications, including search and recommendation systems.

#### Abstract
> The predictive probability of the next token (P_token) in large language models (LLMs) is inextricably linked to the probability of relevance for the next piece of information, the purchase probability of the next product, and the execution probability of the next action-all of which fall under the scope of the task-level target distribution (P_task). While LLMs are known to generate samples that approximate real-world distributions, whether their fine-grained sampling probabilities faithfully align with task requirements remains an open question. Through controlled distribution-sampling simulations, we uncover a striking dichotomy in LLM behavior, distinguishing two model types: D-models (e.g. Qwen-2.5), whose P_token exhibits large step-to-step variability and poor alignment with P_task; and E-models (e.g. Mistral-Small), whose P_token is more stable and better aligned with P_task. We further evaluate these two model types in downstream tasks such as code generation and recommendation, revealing systematic trade-offs between diversity and stability that shape task outcomes. Finally, we analyze the internal properties of both model families to probe their underlying mechanisms. These findings offer foundational insights into the probabilistic sampling behavior of LLMs and provide practical guidance on when to favor D- versus E-models. For web-scale applications, including recommendation, search, and conversational agents, our results inform model selection and configuration to balance diversity with reliability under real-world uncertainty, providing a better level of interpretation.

### 15. FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning

- **LLM Score**: 8
- **Keyword Score**: 4
- **Authors**: Lin Sun, Linglin Zhang, Jingang Huang, Change Jia, Zhengwei Cheng, Xiangzheng Zhang
- **URL**: <http://arxiv.org/abs/2601.18116v1>
- **Submitted**: 2026-01-26 04:00:56
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper presents a novel retrieval framework, FABLE, that integrates Large Language Models (LLMs) into both knowledge organization and retrieval, addressing limitations of traditional RAG systems. The focus on multi-document reasoning and structured cross-document synthesis aligns with your interests in Information Retrieval and query understanding. However, the specific application domain and emphasis on LLMs may not be a central match to your core research themes.

#### Abstract
> The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis.
  We present \textbf{FABLE}, a \textbf{F}orest-based \textbf{A}daptive \textbf{B}i-path \textbf{L}LM-\textbf{E}nhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs.
  Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval.

### 16. Interpretability of the Intent Detection Problem: A New Approach

- **LLM Score**: 8
- **Keyword Score**: 4
- **Authors**: Eduardo Sanchez-Karhunen, Jose F. Quesada-Moreno, Miguel A. Guti√©rrez-Naranjo
- **URL**: <http://arxiv.org/abs/2601.17156v1>
- **Submitted**: 2026-01-23 20:27:47
- **Comment**: Accepted for publication in The European Journal on Artificial Intelligence (2026)
- **Topic Keywords**: queries, nips
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The focus on intent detection and Recurrent Neural Networks (RNNs) aligns with your interests in deep semantic understanding and real-time relevance optimization. However, the paper's primary focus on interpretability and dynamical systems theory, while related, is not a central match to your core research themes.

#### Abstract
> Intent detection, a fundamental text classification task, aims to identify and label the semantics of user queries, playing a vital role in numerous business applications. Despite the dominance of deep learning techniques in this field, the internal mechanisms enabling Recurrent Neural Networks (RNNs) to solve intent detection tasks are poorly understood. In this work, we apply dynamical systems theory to analyze how RNN architectures address this problem, using both the balanced SNIPS and the imbalanced ATIS datasets. By interpreting sentences as trajectories in the hidden state space, we first show that on the balanced SNIPS dataset, the network learns an ideal solution: the state space, constrained to a low-dimensional manifold, is partitioned into distinct clusters corresponding to each intent. The application of this framework to the imbalanced ATIS dataset then reveals how this ideal geometric solution is distorted by class imbalance, causing the clusters for low-frequency intents to degrade. Our framework decouples geometric separation from readout alignment, providing a novel, mechanistic explanation for real world performance disparities. These findings provide new insights into RNN dynamics, offering a geometric interpretation of how dataset properties directly shape a network's computational solution.

### 17. OwlerLite: Scope- and Freshness-Aware Web Retrieval for LLM Assistants

- **LLM Score**: 7
- **Keyword Score**: 10
- **Authors**: Saber Zerhoudi, Michael Dinzinger, Michael Granitzer, Jelena Mitrovic
- **URL**: <http://arxiv.org/abs/2601.17824v1>
- **Submitted**: 2026-01-25 13:11:14
- **Topic Keywords**: query, relevance, rag, retrieval
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of retrieval-augmented generation (RAG) and real-time relevance optimization. However, the focus on browser-based language models and web retrieval for LLM assistants is a departure from your primary focus on e-commerce and deep semantic understanding.

#### Abstract
> Browser-based language models often use retrieval-augmented generation (RAG) but typically rely on fixed, outdated indices that give users no control over which sources are consulted. This can lead to answers that mix trusted and untrusted content or draw on stale information. We present OwlerLite, a browser-based RAG system that makes user-defined scopes and data freshness central to retrieval. Users define reusable scopes-sets of web pages or sources-and select them when querying. A freshness-aware crawler monitors live pages, uses a semantic change detector to identify meaningful updates, and selectively re-indexes changed content. OwlerLite integrates text relevance, scope choice, and recency into a unified retrieval model. Implemented as a browser extension, it represents a step toward more controllable and trustworthy web assistants.

### 18. Breaking Flat: A Generalised Query Performance Prediction Evaluation Framework

- **LLM Score**: 7
- **Keyword Score**: 10
- **Authors**: Payel Santra, Partha Basuchowdhuri, Debasis Ganguly
- **URL**: <http://arxiv.org/abs/2601.17359v1>
- **Submitted**: 2026-01-24 08:10:37
- **Topic Keywords**: query, queries, ranking, rank
- **Reason**: This paper explores query performance prediction and ranking model evaluation, which is somewhat related to your interests in Information Retrieval and ranking models. However, the focus is more on the evaluation framework rather than query understanding or user behavior modeling, limiting its direct relevance to your core research themes.

#### Abstract
> The traditional use-case of query performance prediction (QPP) is to identify which queries perform well and which perform poorly for a given ranking model. A more fine-grained and arguably more challenging extension of this task is to determine which ranking models are most effective for a given query. In this work, we generalize the QPP task and its evaluation into three settings: (i) SingleRanker MultiQuery (SRMQ-PP), corresponding to the standard use case; (ii) MultiRanker SingleQuery (MRSQ-PP), which evaluates a QPP model's ability to select the most effective ranker for a query; and (iii) MultiRanker MultiQuery (MRMQ-PP), which considers predictions jointly across all query ranker pairs. Our results show that (a) the relative effectiveness of QPP models varies substantially across tasks (SRMQ-PP vs. MRSQ-PP), and (b) predicting the best ranker for a query is considerably more difficult than predicting the relative difficulty of queries for a given ranker.

### 19. Real-Time Trend Prediction via Continually-Aligned LLM Query Generation

- **LLM Score**: 7
- **Keyword Score**: 9
- **Authors**: Zijing Hui, Wenhan Lyu, Shusen Wang, Li Chen, Chu Wang
- **URL**: <http://arxiv.org/abs/2601.17567v1>
- **Submitted**: 2026-01-24 19:37:11
- **Topic Keywords**: query, queries, rag, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Search technologies, particularly in the area of query understanding and real-time relevance optimization. However, the focus on trend prediction and low-traffic search environments is not a central match to your primary interests in e-commerce and deep semantic understanding. The use of LLMs and continual learning is an interesting aspect, but it's not directly connected to your core research themes.

#### Abstract
> Trending news detection in low-traffic search environments faces a fundamental cold-start problem, where a lack of query volume prevents systems from identifying emerging or long-tail trends. Existing methods relying on keyword frequency or query spikes are inherently slow and ineffective in these sparse settings, lagging behind real-world shifts in attention. We introduce RTTP, a novel Real-Time Trending Prediction framework that generates search queries directly from news content instead of waiting for users to issue them. RTTP leverages a continual learning LLM (CL-LLM) that converts posts into search-style queries and scores them using engagement strength + creator authority, enabling early trend surfacing before search volume forms. To ensure adaptation without degrading reasoning, we propose Mix-Policy DPO, a new preference-based continual learning approach that combines on-policy stability with off-policy novelty to mitigate catastrophic forgetting during model upgrades. Deployed at production scale on Facebook and Meta AI products, RTTP delivers +91.4% improvement in tail-trend detection precision@500 and +19% query generation accuracy over industry baselines, while sustaining stable performance after multi-week online training. This work demonstrates that LLM-generated synthetic search signals, when aligned and continually updated, unlock timely trend understanding in low-traffic search environments.

### 20. Unleashing the Potential of Sparse Attention on Long-term Behaviors for CTR Prediction

- **LLM Score**: 7
- **Keyword Score**: 6
- **Authors**: Weijiang Lai, Beihong Jin, Di Zhang, Siru Chen, Jiongyan Zhang, Yuhang Gou, Jian Dong, Xingxing Wang
- **URL**: <http://arxiv.org/abs/2601.17836v1>
- **Submitted**: 2026-01-25 13:39:26
- **Topic Keywords**: user behavior, ctr, recommend, personalization
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of recommender systems and user behavior modeling. However, the focus on click-through rate (CTR) prediction and long-term user behaviors is more aligned with recommender systems than your primary focus on information retrieval and deep semantic understanding.

#### Abstract
> In recent years, the success of large language models (LLMs) has driven the exploration of scaling laws in recommender systems. However, models that demonstrate scaling laws are actually challenging to deploy in industrial settings for modeling long sequences of user behaviors, due to the high computational complexity of the standard self-attention mechanism. Despite various sparse self-attention mechanisms proposed in other fields, they are not fully suited for recommendation scenarios. This is because user behaviors exhibit personalization and temporal characteristics: different users have distinct behavior patterns, and these patterns change over time, with data from these users differing significantly from data in other fields in terms of distribution. To address these challenges, we propose SparseCTR, an efficient and effective model specifically designed for long-term behaviors of users. To be precise, we first segment behavior sequences into chunks in a personalized manner to avoid separating continuous behaviors and enable parallel processing of sequences. Based on these chunks, we propose a three-branch sparse self-attention mechanism to jointly identify users' global interests, interest transitions, and short-term interests. Furthermore, we design a composite relative temporal encoding via learnable, head-specific bias coefficients, better capturing sequential and periodic relationships among user behaviors. Extensive experimental results show that SparseCTR not only improves efficiency but also outperforms state-of-the-art methods. More importantly, it exhibits an obvious scaling law phenomenon, maintaining performance improvements across three orders of magnitude in FLOPs. In online A/B testing, SparseCTR increased CTR by 1.72\% and CPM by 1.41\%. Our source code is available at https://github.com/laiweijiang/SparseCTR.

### 21. S$^3$-Attention:Attention-Aligned Endogenous Retrieval for Memory-Bounded Long-Context Inference

- **LLM Score**: 7
- **Keyword Score**: 5
- **Authors**: Qingsen Ma, Dianyun Wang, Yaoye Wang, Lechen Ning, Sujie Zhu, Xiaohang Zhang, Jiaming Lyu, Linhao Ren, Zhenbo Xu, Zhaofeng He
- **URL**: <http://arxiv.org/abs/2601.17702v1>
- **Submitted**: 2026-01-25 05:25:22
- **Topic Keywords**: query, retrieval
- **Reason**: This paper presents a novel approach to long-context inference using attention-aligned endogenous retrieval, which aligns with the user's interest in Information Retrieval and query understanding. Although it focuses on large language models and memory efficiency, it touches on ranking models and real-time relevance optimization, making it somewhat related to the user's core research themes.

#### Abstract
> Large language models are increasingly applied to multi-document and long-form inputs, yet long-context inference remains memory- and noise-inefficient. Key-value (KV) caching scales linearly with context length, while external retrieval methods often return lexically similar but causally irrelevant passages.
  We present S3-Attention, a memory-first inference-time framework that treats long-context processing as attention-aligned endogenous retrieval. S3-Attention decodes transient key and query projections into top-k sparse feature identifiers using lightweight sparse autoencoders, and constructs a CPU-based inverted index mapping features to token positions or spans during a single streaming scan. This design allows the KV cache to be discarded entirely and bounds GPU memory usage by the scan chunk size.
  At generation time, feature co-activation is used to retrieve compact evidence spans, optionally fused with BM25 for exact lexical matching. Under a unified LongBench evaluation protocol with fixed prompting, decoding, and matched token budgets, S3-Hybrid closely matches full-context inference across multiple model families and improves robustness in several information-dense settings. We also report an engineering limitation of the current prototype, which incurs higher wall-clock latency than optimized full-KV baselines, motivating future kernel-level optimization.

### 22. SQL-Trail: Multi-Turn Reinforcement Learning with Interleaved Feedback for Text-to-SQL

- **LLM Score**: 7
- **Keyword Score**: 5
- **Authors**: Harper Hua, Zhen Han, Zhengyuan Shen, Jeremy Lee, Patrick Guan, Qi Zhu, Sullam Jeoung, Yueyan Chen, Yunfei Bai, Shuai Wang, Vassilis Ioannidis, Huzefa Rangwala
- **URL**: <http://arxiv.org/abs/2601.17699v1>
- **Submitted**: 2026-01-25 05:16:52
- **Topic Keywords**: query, rag
- **Reason**: This paper explores Text-to-SQL generation using multi-turn reinforcement learning, which aligns with your interests in query understanding and ranking models. However, the focus on Text-to-SQL generation and database interactions is somewhat specific and not directly related to your primary research themes in Information Retrieval and Search technologies.

#### Abstract
> While large language models (LLMs) have substantially improved Text-to-SQL generation, a pronounced gap remains between AI systems and human experts on challenging benchmarks such as BIRD-SQL. We argue this gap stems largely from the prevailing single-pass paradigm, which lacks the iterative reasoning, schema exploration, and error-correction behaviors that humans naturally employ. To address this limitation, we introduce SQL-Trail, a multi-turn reinforcement learning (RL) agentic framework for Text-to-SQL. Rather than producing a query in one shot, SQL-Trail interacts with the database environment and uses execution feedback to iteratively refine its predictions. Our approach centers on two key ideas: (i) an adaptive turn-budget allocation mechanism that scales the agent's interaction depth to match question difficulty, and (ii) a composite reward panel that jointly incentivizes SQL correctness and efficient exploration. Across benchmarks, SQL-Trail sets a new state of the art and delivers strong data efficiency--up to 18x higher than prior single-pass RL state-of-the-art methods. Notably, our 7B and 14B models outperform substantially larger proprietary systems by 5% on average, underscoring the effectiveness of interactive, agentic workflows for robust Text-to-SQL generation.

### 23. DMAP: Human-Aligned Structural Document Map for Multimodal Document Understanding

- **LLM Score**: 7
- **Keyword Score**: 4
- **Authors**: ShunLiang Fu, Yanxin Zhang, Yixin Xiang, Xiaoyu Du, Jinhui Tang
- **URL**: <http://arxiv.org/abs/2601.18203v1>
- **Submitted**: 2026-01-26 06:38:25
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper focuses on multimodal document understanding, introducing a novel representation called DMAP that captures hierarchical organization and inter-element relationships. While it's not directly related to query understanding or ranking models, it does involve deep semantic understanding and multimodal comprehension, which aligns with your interests in Information Retrieval and NLP. However, the specific application to multimodal documents and question-answering systems is somewhat niche and not directly applicable to your e-commerce background.

#### Abstract
> Existing multimodal document question-answering (QA) systems predominantly rely on flat semantic retrieval, representing documents as a set of disconnected text chunks and largely neglecting their intrinsic hierarchical and relational structures. Such flattening disrupts logical and spatial dependencies - such as section organization, figure-text correspondence, and cross-reference relations, that humans naturally exploit for comprehension. To address this limitation, we introduce a document-level structural Document MAP (DMAP), which explicitly encodes both hierarchical organization and inter-element relationships within multimodal documents. Specifically, we design a Structured-Semantic Understanding Agent to construct DMAP by organizing textual content together with figures, tables, charts, etc. into a human-aligned hierarchical schema that captures both semantic and layout dependencies. Building upon this representation, a Reflective Reasoning Agent performs structure-aware and evidence-driven reasoning, dynamically assessing the sufficiency of retrieved context and iteratively refining answers through targeted interactions with DMAP. Extensive experiments on MMDocQA benchmarks demonstrate that DMAP yields document-specific structural representations aligned with human interpretive patterns, substantially enhancing retrieval precision, reasoning consistency, and multimodal comprehension over conventional RAG-based approaches. Code is available at https://github.com/Forlorin/DMAP

### 24. Capturing P: On the Expressive Power and Efficient Evaluation of Boolean Retrieval

- **LLM Score**: 6
- **Keyword Score**: 12
- **Authors**: Amir Aavani
- **URL**: <http://arxiv.org/abs/2601.18747v1>
- **Submitted**: 2026-01-26 18:07:40
- **Topic Keywords**: information retrieval, query, queries, retrieval, search
- **Reason**: This paper explores the expressive power of retrieval languages and proposes a novel evaluation algorithm for efficient query evaluation. While it touches on the theme of complex query understanding, it is more focused on the theoretical foundations of retrieval languages and their computational efficiency. It may be of interest to researchers in the IR community, but it does not directly align with the user's primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Modern information retrieval is transitioning from simple document filtering to complex, neuro-symbolic reasoning workflows. However, current retrieval architectures face a fundamental efficiency dilemma when handling the rigorous logical and arithmetic constraints required by this new paradigm. Standard iterator-based engines (Document-at-a-Time) do not natively support complex, nested logic graphs; forcing them to execute such queries typically results in intractable runtime performance. Conversely, naive recursive approaches (Term-at-a-Time), while capable of supporting these structures, suffer from prohibitive memory consumption when enforcing broad logical exclusions.
  In this paper, we propose that a retrieval engine must be capable of ``Capturing $\mathbf{P}$'' -- evaluating any polynomial-time property directly over its index in a computationally efficient manner. We define a formal Retrieval Language ($\mathcal{L}_R$) based on Directed Acyclic Graphs (DAGs) and prove it precisely captures the complexity class $\mathbf{P}$. We introduce \texttt{ComputePN}, a novel evaluation algorithm that makes $\mathcal{L}_R$ tractable. By combining native DAG traversal with a memory-efficient ``Positive-Negative'' response mechanism, \texttt{ComputePN} ensures the efficient evaluation of any query in $\mathcal{L}_R$. This work establishes the theoretical foundation for turning the search index into a general-purpose computational engine.

### 25. Why They Link: An Intent Taxonomy for Including Hyperlinks in Social Posts

- **LLM Score**: 6
- **Keyword Score**: 6
- **Authors**: Fangping Lan, Abdullah Aljebreen, Eduard C. Dragut
- **URL**: <http://arxiv.org/abs/2601.17601v1>
- **Submitted**: 2026-01-24 21:32:28
- **Comment**: 10 pages including references, 5 figures,
- **Topic Keywords**: information retrieval, retrieval, recommend
- **Reason**: This paper explores the intent behind hyperlinks in social media posts, developing a taxonomy that could be useful for intent-aware information retrieval and NLP applications. While it touches on information retrieval and NLP, its primary focus is on understanding user intent, which is somewhat related to your interests in query understanding and user behavior modeling. However, it does not directly address ranking models or deep semantic understanding.

#### Abstract
> URLs serve as bridges between social media platforms and the broader web, linking user-generated content to external information resources. On Twitter (X), approximately one in five tweets contains at least one URL, underscoring their central role in information dissemination. While prior studies have examined the motivations of authors who share URLs, such author-centered intentions are difficult to observe in practice. To enable broader downstream use, this work investigates reader-centered interpretations, i.e., how users perceive the intentions behind hyperlinks included in posts. We develop an intent taxonomy for including hyperlinks in social posts through a hybrid approach that begins with a bottom-up, data-driven process using large-scale crowdsourced annotations, and is then refined using large language model assistance to generate descriptive category names and precise definitions. The final taxonomy comprises 6 top-level categories and 26 fine-grained intention classes, capturing diverse communicative purposes. Applying this taxonomy, we annotate and analyze 1000 user posts, revealing that advertising, arguing, and sharing are the most prevalent intentions. This resulting taxonomy provides a foundation for intent-aware information retrieval and NLP applications, enabling more accurate retrieval, recommendation, and understanding of social media content.

### 26. Pipeline Inspection, Visualization, and Interoperability in PyTerrier

- **LLM Score**: 6
- **Keyword Score**: 6
- **Authors**: Emmanouil Georgios Lionis, Craig Macdonald, Sean MacAvaney
- **URL**: <http://arxiv.org/abs/2601.17502v1>
- **Submitted**: 2026-01-24 16:01:57
- **Comment**: This preprint has not undergone peer review (when applicable) or any post-submission improvements or corrections. The Version of Record of this contribution is published in ECIR2026 (Part IV) Advances in Information Retrieval
- **Topic Keywords**: information retrieval, retrieval, search
- **Reason**: The paper is somewhat related to the user's research interests in Information Retrieval (IR), but it focuses on a specific tool (PyTerrier) and its capabilities, rather than query understanding, ranking models, or user behavior modeling. While it does relate to IR pipelines, it does not seem to delve into the user's areas of deep semantic understanding and real-time relevance optimization.

#### Abstract
> PyTerrier provides a declarative framework for building and experimenting with Information Retrieval (IR) pipelines. In this demonstration, we highlight several recent pipeline operations that improve their ability to be programmatically inspected, visualized, and integrated with other tools (via the Model Context Protocol, MCP). These capabilities aim to make it easier for researchers, students, and AI agents to understand and use a wide array of IR pipelines.

### 27. Token-level Collaborative Alignment for LLM-based Generative Recommendation

- **LLM Score**: 6
- **Keyword Score**: 5
- **Authors**: Fake Lin, Binbin Hu, Zhi Zheng, Xi Zhu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Tong Xu
- **URL**: <http://arxiv.org/abs/2601.18457v1>
- **Submitted**: 2026-01-26 13:05:02
- **Comment**: 11 pages, 2 figures, 7 tables, WWW 2026
- **Topic Keywords**: rag, ctr, recommend
- **Reason**: The paper explores a novel approach to integrating collaborative filtering with Large Language Models for generative recommendation. While it touches on aspects of query understanding and ranking models, its primary focus is on recommender systems, which is somewhat related to your interests in Information Retrieval. The paper's emphasis on deep semantic understanding and real-time relevance optimization is also relevant, but it is not a central match for your core research themes.

#### Abstract
> Large Language Models (LLMs) have demonstrated strong potential for generative recommendation by leveraging rich semantic knowledge. However, existing LLM-based recommender systems struggle to effectively incorporate collaborative filtering (CF) signals, due to a fundamental mismatch between item-level preference modeling in CF and token-level next-token prediction (NTP) optimization in LLMs. Prior approaches typically treat CF as contextual hints or representation bias, and resort to multi-stage training to reduce behavioral semantic space discrepancies, leaving CF unable to explicitly regulate LLM generation. In this work, we propose Token-level Collaborative Alignment for Recommendation (TCA4Rec), a model-agnostic and plug-and-play framework that establishes an explicit optimization-level interface between CF supervision and LLM generation. TCA4Rec consists of (i) Collaborative Tokenizer, which projects raw item-level CF logits into token-level distributions aligned with the LLM token space, and (ii) Soft Label Alignment, which integrates these CF-informed distributions with one-hot supervision to optimize a soft NTP objective. This design preserves the generative nature of LLM training while enabling collaborative alignment with essential user preference of CF models. We highlight TCA4Rec is compatible with arbitrary traditional CF models and generalizes across a wide range of decoder-based LLM recommender architectures. Moreover, it provides an explicit mechanism to balance behavioral alignment and semantic fluency, yielding generative recommendations that are both accurate and controllable. Extensive experiments demonstrate that TCA4Rec consistently improves recommendation performance across a broad spectrum of CF models and LLM-based recommender systems.

### 28. CitiLink: Enhancing Municipal Transparency and Citizen Engagement through Searchable Meeting Minutes

- **LLM Score**: 6
- **Keyword Score**: 5
- **Authors**: Rodrigo Silva, Jos√© Evans, Jos√© Isidro, Miguel Marques, Afonso Fonseca, Ricardo Morais, Jo√£o Canavilhas, Arian Pasquali, Purifica√ß√£o Silvano, Al√≠pio Jorge, Nuno Guimar√£es, S√©rgio Nunes, Ricardo Campos
- **URL**: <http://arxiv.org/abs/2601.18374v1>
- **Submitted**: 2026-01-26 11:26:57
- **Topic Keywords**: ranking, rank, search
- **Reason**: The paper explores the application of Information Retrieval (IR) and Natural Language Processing (NLP) techniques to enhance municipal transparency and citizen engagement. While it aligns with the user's interests in IR and NLP, the focus is on a specific domain (municipal meeting minutes) and does not directly relate to the user's core research themes, such as query understanding, ranking models, or user behavior modeling.

#### Abstract
> City council minutes are typically lengthy and formal documents with a bureaucratic writing style. Although publicly available, their structure often makes it difficult for citizens or journalists to efficiently find information. In this demo, we present CitiLink, a platform designed to transform unstructured municipal meeting minutes into structured and searchable data, demonstrating how NLP and IR can enhance the accessibility and transparency of local government. The system employs LLMs to extract metadata, discussed subjects, and voting outcomes, which are then indexed in a database to support full-text search with BM25 ranking and faceted filtering through a user-friendly interface. The developed system was built over a collection of 120 minutes made available by six Portuguese municipalities. To assess its usability, CitiLink was tested through guided sessions with municipal personnel, providing insights into how real users interact with the system. In addition, we evaluated Gemini's performance in extracting relevant information from the minutes, highlighting its effectiveness in data extraction.

### 29. Exploring Fine-Tuning for In-Context Retrieval and Efficient KV-Caching in Long-Context Language Models

- **LLM Score**: 6
- **Keyword Score**: 4
- **Authors**: Francesco Maria Molfese, Momchil Hardalov, Rexhina Blloshmi, Bill Byrne, Adri√† de Gispert
- **URL**: <http://arxiv.org/abs/2601.18527v1>
- **Submitted**: 2026-01-26 14:37:02
- **Comment**: European Chapter of the Association for Computational Linguistics EACL 2026
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper explores the fine-tuning of Long-Context Language Models for in-context retrieval and efficient KV-caching. While it touches on retrieval-augmented generation and robustness under compression, its primary focus is on language models and caching, which is somewhat related to the user's interests in Information Retrieval and Search technologies. However, the paper's emphasis on language models and caching limits its direct relevance to the user's core research themes.

#### Abstract
> With context windows of millions of tokens, Long-Context Language Models (LCLMs) can encode entire document collections, offering a strong alternative to conventional retrieval-augmented generation (RAG). However, it remains unclear whether fine-tuning strategies can improve long-context performance and translate to greater robustness under KV-cache compression techniques. In this work, we investigate which training strategies most effectively enhance LCLMs' ability to identify and use relevant information, as well as enhancing their robustness under KV-cache compression. Our experiments show substantial in-domain improvements, achieving gains of up to +20 points over the base model. However, out-of-domain generalization remains task dependent with large variance -- LCLMs excels on finance questions (+9 points), while RAG shows stronger performance on multiple-choice questions (+6 points) over the baseline models. Finally, we show that our fine-tuning approaches bring moderate improvements in robustness under KV-cache compression, with gains varying across tasks.

### 30. FastInsight: Fast and Insightful Retrieval via Fusion Operators for Graph RAG

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Seonho An, Chaejeong Hyun, Min-Soo Kim
- **URL**: <http://arxiv.org/abs/2601.18579v1>
- **Submitted**: 2026-01-26 15:23:41
- **Comment**: under review
- **Topic Keywords**: rerank, rag, retrieval, rank, search
- **Reason**: The paper discusses a novel approach for graph retrieval, introducing fusion operators to improve retrieval accuracy and efficiency. While it touches on aspects of search and retrieval, it is primarily focused on graph retrieval and does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The connection to information retrieval is somewhat tangential, but the use of Large Language Models (LLMs) and the emphasis on real-time relevance optimization are relevant to broader IR themes.

#### Abstract
> Existing Graph RAG methods aiming for insightful retrieval on corpus graphs typically rely on time-intensive processes that interleave Large Language Model (LLM) reasoning. To enable time-efficient insightful retrieval, we propose FastInsight. We first introduce a graph retrieval taxonomy that categorizes existing methods into three fundamental operations: vector search, graph search, and model-based search. Through this taxonomy, we identify two critical limitations in current approaches: the topology-blindness of model-based search and the semantics-blindness of graph search. FastInsight overcomes these limitations by interleaving two novel fusion operators: the Graph-based Reranker (GRanker), which functions as a graph model-based search, and Semantic-Topological eXpansion (STeX), which operates as a vector-graph search. Extensive experiments on broad retrieval and generation datasets demonstrate that FastInsight significantly improves both retrieval accuracy and generation quality compared to state-of-the-art baselines, achieving a substantial Pareto improvement in the trade-off between effectiveness and efficiency.

### 31. ProGraph-R1: Progress-aware Reinforcement Learning for Graph Retrieval Augmented Generation

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Jinyoung Park, Sanghyeok Lee, Omar Zia Khan, Hyunwoo J. Kim, Joo-Kyung Kim
- **URL**: <http://arxiv.org/abs/2601.17755v1>
- **Submitted**: 2026-01-25 08:58:44
- **Comment**: In progress
- **Topic Keywords**: relevance, rag, retrieval augmented generation, retrieval
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Search technologies, particularly in the area of graph-based retrieval and multi-step reasoning. However, it focuses on graph retrieval and generation, which is not a central match to your primary focus on query understanding, ranking models, and user behavior modeling. The paper's use of reinforcement learning and progress-aware optimization is also not directly aligned with your interests.

#### Abstract
> Graph Retrieval-Augmented Generation (GraphRAG) has been successfully applied in various knowledge-intensive question answering tasks by organizing external knowledge into structured graphs of entities and relations. It enables large language models (LLMs) to perform complex reasoning beyond text-chunk retrieval. Recent works have employed reinforcement learning (RL) to train agentic GraphRAG frameworks that perform iterative interactions between LLMs and knowledge graphs. However, existing RL-based frameworks such as Graph-R1 suffer from two key limitations: (1) they primarily depend on semantic similarity for retrieval, often overlooking the underlying graph structure, and (2) they rely on sparse, outcome-level rewards, failing to capture the quality of intermediate retrieval steps and their dependencies. To address these limitations, we propose ProGraph-R1, a progress-aware agentic framework for graph-based retrieval and multi-step reasoning. ProGraph-R1 introduces a structure-aware hypergraph retrieval mechanism that jointly considers semantic relevance and graph connectivity, encouraging coherent traversal along multi-hop reasoning paths. We also design a progress-based step-wise policy optimization, which provides dense learning signals by modulating advantages according to intermediate reasoning progress within a graph, rather than relying solely on final outcomes. Experiments on multi-hop question answering benchmarks demonstrate that ProGraph-R1 consistently improves reasoning accuracy and generation quality over existing GraphRAG methods.

### 32. From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Yuan Cao, Feixiang Liu, Xinyue Wang, Yihan Zhu, Hui Xu, Zheng Wang, Qiang Qiu
- **URL**: <http://arxiv.org/abs/2601.18582v1>
- **Submitted**: 2026-01-26 15:28:43
- **Comment**: 9 pages, 4 figures, AAAI 2026 Bridge
- **Topic Keywords**: ranking, rag, rank
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and Information Retrieval (IR), particularly in the context of query understanding and ranking models. However, the focus on personality detection and Large Language Models (LLMs) is not directly aligned with your primary research themes, and the paper's emphasis on classification and ranking tasks is more relevant to recommender systems than IR.

#### Abstract
> Personality detection aims to measure an individual's corresponding personality traits through their social media posts. The advancements in Large Language Models (LLMs) offer novel perspectives for personality detection tasks. Existing approaches enhance personality trait analysis by leveraging LLMs to extract semantic information from textual posts as prompts, followed by training classifiers for categorization. However, accurately classifying personality traits remains challenging due to the inherent complexity of human personality and subtle inter-trait distinctions. Moreover, prompt-based methods often exhibit excessive dependency on expert-crafted knowledge without autonomous pattern-learning capacity. To address these limitations, we view personality detection as a ranking task rather than a classification and propose a corresponding reinforcement learning training paradigm. First, we employ supervised fine-tuning (SFT) to establish personality trait ranking capabilities while enforcing standardized output formats, creating a robust initialization. Subsequently, we introduce Group Relative Policy Optimization (GRPO) with a specialized ranking-based reward function. Unlike verification tasks with definitive solutions, personality assessment involves subjective interpretations and blurred boundaries between trait categories. Our reward function explicitly addresses this challenge by training LLMs to learn optimal answer rankings. Comprehensive experiments have demonstrated that our method achieves state-of-the-art performance across multiple personality detection benchmarks.

### 33. Orchestrating Specialized Agents for Trustworthy Enterprise RAG

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Xincheng You, Qi Sun, Neha Bora, Huayi Li, Shubham Goel, Kang Li, Sean Culatana
- **URL**: <http://arxiv.org/abs/2601.18267v1>
- **Submitted**: 2026-01-26 08:48:41
- **Topic Keywords**: rag, retrieval, rank, search
- **Reason**: The paper discusses Retrieval-Augmented Generation (RAG) for enterprise knowledge work, which is somewhat related to information retrieval. However, the focus on trustworthy enterprise RAG and specialized agents is not directly aligned with the user's core research themes in query understanding, ranking models, and user behavior modeling. While the paper touches on aspects of deep semantic understanding, it is more focused on the orchestration of agents and evidence management, which is not a central match for the user's interests.

#### Abstract
> Retrieval-Augmented Generation (RAG) shows promise for enterprise knowledge work, yet it often underperforms in high-stakes decision settings that require deep synthesis, strict traceability, and recovery from underspecified prompts. One-pass retrieval-and-write pipelines frequently yield shallow summaries, inconsistent grounding, and weak mechanisms for completeness verification. We introduce ADORE (Adaptive Deep Orchestration for Research in Enterprise), an agentic framework that replaces linear retrieval with iterative, user-steered investigation coordinated by a central orchestrator and a set of specialized agents. ADORE's key insight is that a structured Memory Bank (a curated evidence store with explicit claim-evidence linkage and section-level admissible evidence) enables traceable report generation and systematic checks for evidence completeness. Our contributions are threefold: (1) Memory-locked synthesis - report generation is constrained to a structured Memory Bank (Claim-Evidence Graph) with section-level admissible evidence, enabling traceable claims and grounded citations; (2) Evidence-coverage-guided execution - a retrieval-reflection loop audits section-level evidence coverage to trigger targeted follow-up retrieval and terminates via an evidence-driven stopping criterion; (3) Section-packed long-context grounding - section-level packing, pruning, and citation-preserving compression make long-form synthesis feasible under context limits. Across our evaluation suite, ADORE ranks first on DeepResearch Bench (52.65) and achieves the highest head-to-head preference win rate on DeepConsult (77.2%) against commercial systems.

### 34. Mind the Ambiguity: Aleatoric Uncertainty Quantification in LLMs for Safe Medical Question Answering

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Yaokun Liu, Yifan Liu, Phoebe Mbuvi, Zelin Li, Ruichen Yao, Gawon Lim, Dong Wang
- **URL**: <http://arxiv.org/abs/2601.17284v1>
- **Submitted**: 2026-01-24 03:44:08
- **Comment**: Accepted at The Web Conference 2026 (WWW 2026)
- **Topic Keywords**: queries, rag, search
- **Reason**: The paper explores the concept of aleatoric uncertainty in Large Language Models for Medical Question Answering, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on Medical QA and uncertainty quantification in LLMs does not directly align with the user's primary research themes in IR and NLP.

#### Abstract
> The deployment of Large Language Models in Medical Question Answering is severely hampered by ambiguous user queries, a significant safety risk that demonstrably reduces answer accuracy in high-stakes healthcare settings. In this paper, we formalize this challenge by linking input ambiguity to aleatoric uncertainty (AU), which is the irreducible uncertainty arising from underspecified input. To facilitate research in this direction, we construct CV-MedBench, the first benchmark designed for studying input ambiguity in Medical QA. Using this benchmark, we analyze AU from a representation engineering perspective, revealing that AU is linearly encoded in LLM's internal activation patterns. Leveraging this insight, we introduce a novel AU-guided "Clarify-Before-Answer" framework, which incorporates AU-Probe - a lightweight module that detects input ambiguity directly from hidden states. Unlike existing uncertainty estimation methods, AU-Probe requires neither LLM fine-tuning nor multiple forward passes, enabling an efficient mechanism to proactively request user clarification and significantly enhance safety. Extensive experiments across four open LLMs demonstrate the effectiveness of our QA framework, with an average accuracy improvement of 9.48% over baselines. Our framework provides an efficient and robust solution for safe Medical QA, strengthening the reliability of health-related applications. The code is available at https://github.com/yaokunliu/AU-Med.git, and the CV-MedBench dataset is released on Hugging Face at https://huggingface.co/datasets/yaokunl/CV-MedBench.

### 35. CaseFacts: A Benchmark for Legal Fact-Checking and Precedent Retrieval

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Akshith Reddy Putta, Jacob Devasier, Chengkai Li
- **URL**: <http://arxiv.org/abs/2601.17230v1>
- **Submitted**: 2026-01-23 23:41:46
- **Topic Keywords**: rag, retrieval, web search, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, particularly in the context of query understanding and semantic similarity. However, the focus on legal fact-checking and precedent retrieval is a specialized domain that, while related to your broader interests, is not a central match. The paper's emphasis on Large Language Models and semantic similarity heuristics may be of interest, but it does not directly align with your primary focus on e-commerce and real-time relevance optimization.

#### Abstract
> Automated Fact-Checking has largely focused on verifying general knowledge against static corpora, overlooking high-stakes domains like law where truth is evolving and technically complex. We introduce CaseFacts, a benchmark for verifying colloquial legal claims against U.S. Supreme Court precedents. Unlike existing resources that map formal texts to formal texts, CaseFacts challenges systems to bridge the semantic gap between layperson assertions and technical jurisprudence while accounting for temporal validity. The dataset consists of 6,294 claims categorized as Supported, Refuted, or Overruled. We construct this benchmark using a multi-stage pipeline that leverages Large Language Models (LLMs) to synthesize claims from expert case summaries, employing a novel semantic similarity heuristic to efficiently identify and verify complex legal overrulings. Experiments with state-of-the-art LLMs reveal that the task remains challenging; notably, augmenting models with unrestricted web search degrades performance compared to closed-book baselines due to the retrieval of noisy, non-authoritative precedents. We release CaseFacts to spur research into legal fact verification systems.

### 36. Evaluation on Entity Matching in Recommender Systems

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Zihan Huang, Rohan Surana, Zhouhang Xie, Junda Wu, Yu Xia, Julian McAuley
- **URL**: <http://arxiv.org/abs/2601.17218v1>
- **Submitted**: 2026-01-23 23:05:46
- **Topic Keywords**: rag, recommend, search
- **Reason**: The paper is somewhat related to the user's interests in Information Retrieval and Recommender Systems, but it focuses on entity matching in recommender systems, which is a specific area that is not the primary focus of the user's research. The paper's use of deep learning methods (LLM-based approaches) is somewhat relevant to the user's interests in query understanding and ranking models, but the connection is not strong enough to warrant a higher score.

#### Abstract
> Entity matching is a crucial component in various recommender systems, including conversational recommender systems (CRS) and knowledge-based recommender systems. However, the lack of rigorous evaluation frameworks for cross-dataset entity matching impedes progress in areas such as LLM-driven conversational recommendations and knowledge-grounded dataset construction.
  In this paper, we introduce Reddit-Amazon-EM, a novel dataset comprising naturally occurring items from Reddit and the Amazon '23 dataset. Through careful manual annotation, we identify corresponding movies across Reddit-Movies and Amazon'23, two existing recommender system datasets with inherently overlapping catalogs. Leveraging Reddit-Amazon-EM, we conduct a comprehensive evaluation of state-of-the-art entity matching methods, including rule-based, graph-based, lexical-based, embedding-based, and LLM-based approaches.
  For reproducible research, we release our manually annotated entity matching gold set and provide the mapping between the two datasets using the best-performing method from our experiments. This serves as a valuable resource for advancing future work on entity matching in recommender systems.

### 37. Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Lintang Sutawika, Gokul Swamy, Zhiwei Steven Wu, Graham Neubig
- **URL**: <http://arxiv.org/abs/2601.18722v1>
- **Submitted**: 2026-01-26 17:46:44
- **Comment**: Code available at https://github.com/lintangsutawika/SP3F
- **Topic Keywords**: pairwise
- **Reason**: This paper focuses on enhancing multilingual reasoning in large language models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the primary focus on language translation and model fine-tuning does not directly align with the user's core research themes in IR and Search technologies.

#### Abstract
> When asked a question in a language less seen in its training data, current reasoning large language models (RLMs) often exhibit dramatically lower performance than when asked the same question in English. In response, we introduce \texttt{SP3F} (Self-Play with Privileged Pairwise Feedback), a two-stage framework for enhancing multilingual reasoning without \textit{any} data in the target language(s). First, we supervise fine-tune (SFT) on translated versions of English question-answer pairs to raise base model correctness. Second, we perform RL with feedback from a pairwise judge in a self-play fashion, with the judge receiving the English reference response as \textit{privileged information}. Thus, even when none of the model's responses are completely correct, the privileged pairwise judge can still tell which response is better. End-to-end, \texttt{SP3F} greatly improves base model performance, even outperforming fully post-trained models on multiple math and non-math tasks with less than
  of the training data across the single-language, multilingual, and generalization to unseen language settings.

### 38. Improving User Privacy in Personalized Generation: Client-Side Retrieval-Augmented Modification of Server-Side Generated Speculations

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Alireza Salemi, Hamed Zamani
- **URL**: <http://arxiv.org/abs/2601.17569v1>
- **Submitted**: 2026-01-24 19:46:40
- **Topic Keywords**: query, rag, retrieval, personalization
- **Reason**: This paper focuses on improving user privacy in personalized generation, specifically in the context of Large Language Models (LLMs). While it touches on retrieval augmentation, which is related to information retrieval, the primary focus is on privacy and personalization, which is not a central match for your research interests.

#### Abstract
> Personalization is crucial for aligning Large Language Model (LLM) outputs with individual user preferences and background knowledge. State-of-the-art solutions are based on retrieval augmentation, where relevant context from a user profile is retrieved for LLM consumption. These methods deal with a trade-off between exposing retrieved private data to cloud providers and relying on less capable local models. We introduce $P^3$, an interactive framework for high-quality personalization without revealing private profiles to server-side LLMs. In $P^3$, a large server-side model generates a sequence of $k$ draft tokens based solely on the user query, while a small client-side model, with retrieval access to the user's private profile, evaluates and modifies these drafts to better reflect user preferences. This process repeats until an end token is generated. Experiments on LaMP-QA, a recent benchmark consisting of three personalized question answering datasets, show that $P^3$ consistently outperforms both non-personalized server-side and personalized client-side baselines, achieving statistically significant improvements of $7.4%$ to $9%$ on average. Importantly, $P^3$ recovers $90.3%$ to $95.7%$ of the utility of a ``leaky'' upper-bound scenario in which the full profile is exposed to the large server-side model. Privacy analyses, including linkability and attribute inference attacks, indicate that $P^3$ preserves the privacy of a non-personalized server-side model, introducing only marginal additional leakage ($1.5%$--$3.5%$) compared to submitting a query without any personal context. Additionally, the framework is efficient for edge deployment, with the client-side model generating only $9.2%$ of the total tokens. These results demonstrate that $P^3$ provides a practical, effective solution for personalized generation with improved privacy.

### 39. FadeMem: Biologically-Inspired Forgetting for Efficient Agent Memory

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Lei Wei, Xu Dong, Xiao Peng, Niantao Xie, Bin Wang
- **URL**: <http://arxiv.org/abs/2601.18642v1>
- **Submitted**: 2026-01-26 16:12:54
- **Topic Keywords**: relevance, rag, retrieval
- **Reason**: This paper focuses on agent memory architectures and forgetting mechanisms, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it mentions retrieval and multi-hop reasoning, the context is different and the paper's primary contribution is in the area of artificial intelligence and memory management.

#### Abstract
> Large language models deployed as autonomous agents face critical memory limitations, lacking selective forgetting mechanisms that lead to either catastrophic forgetting at context boundaries or information overload within them. While human memory naturally balances retention and forgetting through adaptive decay processes, current AI systems employ binary retention strategies that preserve everything or lose it entirely. We propose FadeMem, a biologically-inspired agent memory architecture that incorporates active forgetting mechanisms mirroring human cognitive efficiency. FadeMem implements differential decay rates across a dual-layer memory hierarchy, where retention is governed by adaptive exponential decay functions modulated by semantic relevance, access frequency, and temporal patterns. Through LLM-guided conflict resolution and intelligent memory fusion, our system consolidates related information while allowing irrelevant details to fade. Experiments on Multi-Session Chat, LoCoMo, and LTI-Bench demonstrate superior multi-hop reasoning and retrieval with 45\% storage reduction, validating the effectiveness of biologically-inspired forgetting in agent memory systems.

### 40. Segment Length Matters: A Study of Segment Lengths on Audio Fingerprinting Performance

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Ziling Gong, Yunyan Ouyang, Iram Kamdar, Melody Ma, Hongjie Chen, Franck Dernoncourt, Ryan A. Rossi, Nesreen K. Ahmed
- **URL**: <http://arxiv.org/abs/2601.17690v1>
- **Submitted**: 2026-01-25 04:32:32
- **Topic Keywords**: query, retrieval, recommend
- **Reason**: This paper is not directly related to your core research themes in Information Retrieval, Search technologies, or Natural Language Processing. While it involves neural approaches and retrieval systems, the focus is on audio fingerprinting, which is a specific application outside your primary areas of interest.

#### Abstract
> Audio fingerprinting provides an identifiable representation of acoustic signals, which can be later used for identification and retrieval systems. To obtain a discriminative representation, the input audio is usually segmented into shorter time intervals, allowing local acoustic features to be extracted and analyzed. Modern neural approaches typically operate on short, fixed-duration audio segments, yet the choice of segment duration is often made heuristically and rarely examined in depth. In this paper, we study how segment length affects audio fingerprinting performance. We extend an existing neural fingerprinting architecture to adopt various segment lengths and evaluate retrieval accuracy across different segment lengths and query durations. Our results show that short segment lengths (0.5-second) generally achieve better performance. Moreover, we evaluate LLM capacity in recommending the best segment length, which shows that GPT-5-mini consistently gives the best suggestions across five considerations among three studied LLMs. Our findings provide practical guidance for selecting segment duration in large-scale neural audio retrieval systems.

### 41. Revisiting Modality Invariance in a Multilingual Speech-Text Model via Neuron-Level Analysis

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Toshiki Nakai, Varsha Suresh, Vera Demberg
- **URL**: <http://arxiv.org/abs/2601.17387v1>
- **Submitted**: 2026-01-24 09:22:18
- **Comment**: 8 pages for the main text, 51 figures, 1 table
- **Topic Keywords**: ranking, rag, rank
- **Reason**: This paper focuses on a speech-text model, exploring modality invariance and language representation in a multilingual setting. While it touches on neural analysis and representation learning, it does not directly relate to query understanding, ranking models, or user behavior modeling in information retrieval, which are core areas of your research interests.

#### Abstract
> Multilingual speech-text foundation models aim to process language uniformly across both modality and language, yet it remains unclear whether they internally represent the same language consistently when it is spoken versus written. We investigate this question in SeamlessM4T v2 through three complementary analyses that probe where language and modality information is encoded, how selective neurons causally influence decoding, and how concentrated this influence is across the network. We identify language- and modality-selective neurons using average-precision ranking, investigate their functional role via median-replacement interventions at inference time, and analyze activation-magnitude inequality across languages and modalities. Across experiments, we find evidence of incomplete modality invariance. Although encoder representations become increasingly language-agnostic, this compression makes it more difficult for the shared decoder to recover the language of origin when constructing modality-agnostic representations, particularly when adapting from speech to text. We further observe sharply localized modality-selective structure in cross-attention key and value projections. Finally, speech-conditioned decoding and non-dominant scripts exhibit higher activation concentration, indicating heavier reliance on a small subset of neurons, which may underlie increased brittleness across modalities and languages.

### 42. Structure-Aware NL-to-SQL for SFC Provisioning via AST-Masking Empowered Language Models

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Xinyu Zhu, Parisa Fard Moshiri, Poonam Lohan, Burak Kantarci, Emil Janulewicz
- **URL**: <http://arxiv.org/abs/2601.17295v1>
- **Submitted**: 2026-01-24 04:06:57
- **Comment**: 6 pages, 3 figures, accepted to IEEE International Conference on Communications (ICC) 2026
- **Topic Keywords**: query, queries
- **Reason**: This paper appears to be focused on Natural Language Processing (NLP) and its application to SQL generation for Service Function Chain (SFC) provisioning. While it involves query understanding and language models, it is not directly related to Information Retrieval (IR) or Search technologies, which are the primary focus of your research interests.

#### Abstract
> Effective Service Function Chain (SFC) provisioning requires precise orchestration in dynamic and latency-sensitive networks. Reinforcement Learning (RL) improves adaptability but often ignores structured domain knowledge, which limits generalization and interpretability. Large Language Models (LLMs) address this gap by translating natural language (NL) specifications into executable Structured Query Language (SQL) commands for specification-driven SFC management. Conventional fine-tuning, however, can cause syntactic inconsistencies and produce inefficient queries. To overcome this, we introduce Abstract Syntax Tree (AST)-Masking, a structure-aware fine-tuning method that uses SQL ASTs to assign weights to key components and enforce syntax-aware learning without adding inference overhead. Experiments show that AST-Masking significantly improves SQL generation accuracy across multiple language models. FLAN-T5 reaches an Execution Accuracy (EA) of 99.6%, while Gemma achieves the largest absolute gain from 7.5% to 72.0%. These results confirm the effectiveness of structure-aware fine-tuning in ensuring syntactically correct and efficient SQL generation for interpretable SFC orchestration.

### 43. HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Yusuke Sakai, Hidetaka Kamigaito, Taro Watanabe
- **URL**: <http://arxiv.org/abs/2601.18724v1>
- **Submitted**: 2026-01-26 17:48:23
- **Comment**: Work In Progress
- **Topic Keywords**: acl, naacl, emnlp
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or user behavior modeling, which are core areas of your research interests. While it touches on a broader issue in the NLP community, its focus on hallucinated citations and their impact on scientific reliability does not align with your primary research themes.

#### Abstract
> Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose a serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations as "HalluCitation" and systematically investigate their prevalence and impact. We analyze all papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis reveals that nearly 300 papers contain at least one HalluCitation, most of which were published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, more than 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, affecting the credibility.

### 44. PEAR: Pairwise Evaluation for Automatic Relative Scoring in Machine Translation

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Lorenzo Proietti, Roman Grundkiewicz, Matt Post
- **URL**: <http://arxiv.org/abs/2601.18006v1>
- **Submitted**: 2026-01-25 21:52:30
- **Comment**: 18 pages
- **Topic Keywords**: pairwise, rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing, as it focuses on Machine Translation evaluation metrics. While it involves NLP, the specific application and context are unrelated to your core research themes.

#### Abstract
> We present PEAR (Pairwise Evaluation for Automatic Relative Scoring), a supervised Quality Estimation (QE) metric family that reframes reference-free Machine Translation (MT) evaluation as a graded pairwise comparison. Given a source segment and two candidate translations, PEAR predicts the direction and magnitude of their quality difference. The metrics are trained using pairwise supervision derived from differences in human judgments, with an additional regularization term that encourages sign inversion under candidate order reversal. On the WMT24 meta-evaluation benchmark, PEAR outperforms strictly matched single-candidate QE baselines trained with the same data and backbones, isolating the benefit of the proposed pairwise formulation. Despite using substantially fewer parameters than recent large metrics, PEAR surpasses far larger QE models and reference-based metrics. Our analysis further indicates that PEAR yields a less redundant evaluation signal relative to other top metrics. Finally, we show that PEAR is an effective utility function for Minimum Bayes Risk (MBR) decoding, reducing pairwise scoring cost at negligible impact.

### 45. Artificial Intelligence and Intellectual Property Rights: Comparative Transnational Policy Analysis

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Sahibpreet Singh, Manjit Singh
- **URL**: <http://arxiv.org/abs/2601.17892v1>
- **Submitted**: 2026-01-25 16:00:52
- **Comment**: Published in Journal of University Institute of Legal Studies, Vol. 19, Issue 1, pp. 182-208, 2025
- **Topic Keywords**: rag, ctr, search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. It focuses on the intersection of Artificial Intelligence and Intellectual Property Rights, which is outside your primary areas of interest.

#### Abstract
> Artificial intelligence's rapid integration with intellectual property rights necessitates assessment of its impact on trade secrets, copyrights and patents. This study addresses lacunae in existing laws where India lacks AI-specific provisions, creating doctrinal inconsistencies and enforcement inefficacies. Global discourse on AI-IPR protections remains nascent. The research identifies gaps in Indian IP laws' adaptability to AI-generated outputs: trade secret protection is inadequate against AI threats; standardized inventorship criteria are absent. Employing doctrinal and comparative methodology, it scrutinizes legislative texts, judicial precedents and policy instruments across India, US, UK and EU. Preliminary findings reveal shortcomings: India's contract law creates fragmented trade secret regime; Section 3(k) of Indian Patents Act blocks AI invention patenting; copyright varies in authorship attribution. The study proposes harmonized legal taxonomy accommodating AI's role while preserving innovation incentives. India's National AI Strategy (2024) shows progress but legislative clarity is imperative. This contributes to global discourse with AI-specific IP protections ensuring resilience and equitable innovation. Promising results underscore recalibrating India's IP jurisprudence for global alignment.

### 46. Linguistic and Argument Diversity in Synthetic Data for Function-Calling Agents

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Dan Greenstein, Zohar Karnin, Chen Amiraz, Oren Somekh
- **URL**: <http://arxiv.org/abs/2601.17829v1>
- **Submitted**: 2026-01-25 13:20:33
- **Topic Keywords**: queries, rag
- **Reason**: This paper focuses on generating synthetic data for function-calling agents, which is not directly related to your core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on query understanding and diversity, the context is quite different from your typical e-commerce or real-time relevance optimization interests.

#### Abstract
> The construction of function calling agents has emerged as a promising avenue for extending model capabilities. A major challenge for this task is obtaining high quality diverse data for training. Prior work emphasizes diversity in functions, invocation patterns, and interaction turns, yet linguistic diversity of requests and coverage of arguments (e.g., \texttt{city\_name}, \texttt{stock\_ticker}) remain underexplored. We propose a method that generates synthetic datasets via optimizing general-purpose diversity metrics across both queries and arguments, without relying on hand-crafted rules or taxonomies, making it robust to different usecases. We demonstrate the effectiveness of our technique via both intrinsic and extrinsic testing, comparing it to SoTA data generation methods. We show a superiority over baselines in terms of diversity, while keeping comparable correctness. Additionally, when used as a training set, the model resulting from our dataset exhibits superior performance compared to analogous models based on the baseline data generation methods in out-of-distribution performance. In particular, we achieve an $7.4\%$ increase in accuracy on the BFCL benchmark compared to similar counterparts.

### 47. DIETA: A Decoder-only transformer-based model for Italian-English machine TrAnslation

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Pranav Kasela, Marco Braga, Alessandro Ghiotto, Andrea Pilzer, Marco Viviani, Alessandro Raganato
- **URL**: <http://arxiv.org/abs/2601.17823v1>
- **Submitted**: 2026-01-25 13:08:43
- **Comment**: Published in CLiC-IT '25: https://aclanthology.org/2025.clicit-1.52/
- **Topic Keywords**: ranking, rank, search
- **Reason**: This paper focuses on machine translation, which is a related but distinct area from information retrieval and search technologies. While it involves natural language processing, it does not directly address query understanding, ranking models, or user behavior modeling, which are core aspects of your research interests.

#### Abstract
> In this paper, we present DIETA, a small, decoder-only Transformer model with 0.5 billion parameters, specifically designed and trained for Italian-English machine translation. We collect and curate a large parallel corpus consisting of approximately 207 million Italian-English sentence pairs across diverse domains, including parliamentary proceedings, legal texts, web-crawled content, subtitles, news, literature and 352 million back-translated data using pretrained models. Additionally, we create and release a new small-scale evaluation set, consisting of 450 sentences, based on 2025 WikiNews articles, enabling assessment of translation quality on contemporary text. Comprehensive evaluations show that DIETA achieves competitive performance on multiple Italian-English benchmarks, consistently ranking in the second quartile of a 32-system leaderboard and outperforming most other sub-3B models on four out of five test suites. The training script, trained models, curated corpus, and newly introduced evaluation set are made publicly available, facilitating further research and development in specialized Italian-English machine translation. https://github.com/pkasela/DIETA-Machine-Translation

### 48. Frame-Guided Synthetic Claim Generation for Automatic Fact-Checking Using High-Volume Tabular Data

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Jacob Devasier, Akshith Putta, Qing Wang, Alankrit Moses, Chengkai Li
- **URL**: <http://arxiv.org/abs/2601.17232v1>
- **Submitted**: 2026-01-23 23:47:41
- **Topic Keywords**: rag, retrieval, search
- **Reason**: This paper appears to be focused on fact-checking and data retrieval using tabular data, which is somewhat related to information retrieval. However, the specific application and methodology (frame-guided synthetic claim generation) do not align closely with the user's core research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Automated fact-checking benchmarks have largely ignored the challenge of verifying claims against real-world, high-volume structured data, instead focusing on small, curated tables. We introduce a new large-scale, multilingual dataset to address this critical gap. It contains 78,503 synthetic claims grounded in 434 complex OECD tables, which average over 500K rows each. We propose a novel, frame-guided methodology where algorithms programmatically select significant data points based on six semantic frames to generate realistic claims in English, Chinese, Spanish, and Hindi. Crucially, we demonstrate through knowledge-probing experiments that LLMs have not memorized these facts, forcing systems to perform genuine retrieval and reasoning rather than relying on parameterized knowledge. We provide a baseline SQL-generation system and show that our benchmark is highly challenging. Our analysis identifies evidence retrieval as the primary bottleneck, with models struggling to find the correct data in massive tables. This dataset provides a critical new resource for advancing research on this unsolved, real-world problem.

### 49. POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Yuxiao Qu, Amrith Setlur, Virginia Smith, Ruslan Salakhutdinov, Aviral Kumar
- **URL**: <http://arxiv.org/abs/2601.18779v1>
- **Submitted**: 2026-01-26 18:47:21
- **Topic Keywords**: rag, acl
- **Reason**: This paper appears to be more relevant to the field of Reinforcement Learning and Large Language Models, rather than Information Retrieval or Search technologies. While it does discuss the concept of 'reasoning' which is somewhat related to query understanding, the focus on hard problems and privileged on-policy exploration does not align closely with the user's core research themes.

#### Abstract
> Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.

### 50. The 17% Gap: Quantifying Epistemic Decay in AI-Assisted Survey Papers

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: H. Kemal ƒ∞lter
- **URL**: <http://arxiv.org/abs/2601.17431v1>
- **Submitted**: 2026-01-24 12:00:55
- **Topic Keywords**: queries, search
- **Reason**: This paper appears to be focused on the quality and reliability of citations in AI-assisted survey papers, which is not directly related to your core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on the use of Large Language Models, the context is more about citation verification and the impact of AI tools on scientific reproducibility rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> The adoption of Large Language Models (LLMs) in scientific writing promises efficiency but risks introducing informational entropy. While "hallucinated papers" are a known artifact, the systematic degradation of valid citation chains remains unquantified. We conducted a forensic audit of 50 recent survey papers in Artificial Intelligence (N=5,514 citations) published between September 2024 and January 2026. We utilized a hybrid verification pipeline combining DOI resolution, Crossref metadata analysis, Semantic Scholar queries, and fuzzy text matching to distinguish between formatting errors ("Sloppiness") and verifiable non-existence ("Phantoms). We detect a persistent 17.0% Phantom Rate -- citations that cannot be resolved to any digital object despite aggressive forensic recovery. Diagnostic categorization reveals three distinct failure modes: pure hallucinations (5.1%), hallucinated identifiers with valid titles (16.4%), and parsing-induced matching failures (78.5%). Longitudinal analysis reveals a flat trend (+0.07 pp/month), suggesting that high-entropy citation practices have stabilized as an endemic feature of the field. The scientific citation graph in AI survey literature exhibits "link rot" at scale. This suggests a mechanism where AI tools act as "lazy research assistants," retrieving correct titles but hallucinating metadata, thereby severing the digital chain of custody required for reproducible science.

---

