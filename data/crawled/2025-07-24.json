[
    {
        "title": "Leave No One Behind: Fairness-Aware Cross-Domain Recommender Systems for Non-Overlapping Users",
        "abstract": "Cross-domain recommendation (CDR) methods predominantly leverage overlapping\nusers to transfer knowledge from a source domain to a target domain. However,\nthrough empirical studies, we uncover a critical bias inherent in these\napproaches: while overlapping users experience significant enhancements in\nrecommendation quality, non-overlapping users benefit minimally and even face\nperformance degradation. This unfairness may erode user trust, and,\nconsequently, negatively impact business engagement and revenue. To address\nthis issue, we propose a novel solution that generates virtual source-domain\nusers for non-overlapping target-domain users. Our method utilizes a dual\nattention mechanism to discern similarities between overlapping and\nnon-overlapping users, thereby synthesizing realistic virtual user embeddings.\nWe further introduce a limiter component that ensures the generated virtual\nusers align with real-data distributions while preserving each user's unique\ncharacteristics. Notably, our method is model-agnostic and can be seamlessly\nintegrated into any CDR model. Comprehensive experiments conducted on three\npublic datasets with five CDR baselines demonstrate that our method effectively\nmitigates the CDR non-overlapping user bias, without loss of overall accuracy.\nOur code is publicly available at https://github.com/WeixinChen98/VUG.",
        "url": "http://arxiv.org/abs/2507.17749v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17749v1",
        "arxiv_id": "2507.17749v1",
        "authors": [
            "Weixin Chen",
            "Yuhan Zhao",
            "Li Chen",
            "Weike Pan"
        ],
        "submitted": "2025-07-23 17:59:08",
        "source": "arxiv",
        "comment": "Accepted by RecSys 2025"
    },
    {
        "title": "Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks",
        "abstract": "As frontier language models increasingly saturate standard QA benchmarks,\nconcerns about data contamination, memorization, and escalating dataset\ncreation costs persist. We propose a debate-driven evaluation paradigm that\ntransforms any existing QA dataset into structured adversarial debates--where\none model is given the official answer to defend, and another constructs and\ndefends an alternative answer--adjudicated by a judge model blind to the\ncorrect solution. By forcing multi-round argumentation, this approach\nsubstantially increases difficulty while penalizing shallow memorization, yet\nreuses QA items to reduce curation overhead. We make two main contributions:\n(1) an evaluation pipeline to systematically convert QA tasks into debate-based\nassessments, and (2) a public benchmark that demonstrates our paradigm's\neffectiveness on a subset of MMLU-Pro questions, complete with standardized\nprotocols and reference models. Empirical results validate the robustness of\nthe method and its effectiveness against data contamination--a Llama 3.1 model\nfine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%)\nbut performed worse in debates. Results also show that even weaker judges can\nreliably differentiate stronger debaters, highlighting how debate-based\nevaluation can scale to future, more capable systems while maintaining a\nfraction of the cost of creating new benchmarks. Overall, our framework\nunderscores that \"pretraining on the test set is no longer all you need,\"\noffering a sustainable path for measuring the genuine reasoning ability of\nadvanced language models.",
        "url": "http://arxiv.org/abs/2507.17747v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17747v1",
        "arxiv_id": "2507.17747v1",
        "authors": [
            "Linbo Cao",
            "Jinman Zhao"
        ],
        "submitted": "2025-07-23 17:58:14",
        "source": "arxiv",
        "comment": "22 pages, 7 figures. Accepted to COLM 2025. Code available at:\n  github.com/l6cao/Debate-Driven-Evaluation"
    },
    {
        "title": "Rubrics as Rewards: Reinforcement Learning Beyond Verifiable Domains",
        "abstract": "Extending Reinforcement Learning with Verifiable Rewards (RLVR) to real-world\ntasks often requires balancing objective and subjective evaluation criteria.\nHowever, many such tasks lack a single, unambiguous ground truth-making it\ndifficult to define reliable reward signals for post-training language models.\nWhile traditional preference-based methods offer a workaround, they rely on\nopaque reward functions that are difficult to interpret and prone to spurious\ncorrelations. We introduce $\\textbf{Rubrics as Rewards}$ (RaR), a framework\nthat uses structured, checklist-style rubrics as interpretable reward signals\nfor on-policy training with GRPO. Our best RaR method yields up to a $28\\%$\nrelative improvement on HealthBench-1k compared to simple Likert-based\napproaches, while matching or surpassing the performance of reward signals\nderived from expert-written references. By treating rubrics as structured\nreward signals, we show that RaR enables smaller-scale judge models to better\nalign with human preferences and sustain robust performance across model\nscales.",
        "url": "http://arxiv.org/abs/2507.17746v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17746v1",
        "arxiv_id": "2507.17746v1",
        "authors": [
            "Anisha Gunjal",
            "Anthony Wang",
            "Elaine Lau",
            "Vaskar Nath",
            "Bing Liu",
            "Sean Hendryx"
        ],
        "submitted": "2025-07-23 17:57:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Megrez2 Technical Report",
        "abstract": "We present Megrez2, a novel lightweight and high-performance language model\narchitecture optimized for device native deployment. Megrez2 introduces a novel\ncross-layer expert sharing mechanism, which significantly reduces total\nparameter count by reusing expert modules across adjacent transformer layers\nwhile maintaining most of the model's capacity. It also incorporates pre-gated\nrouting, enabling memory-efficient expert loading and faster inference. As the\nfirst instantiation of the Megrez2 architecture, we introduce the\nMegrez2-Preview model, which is pre-trained on a 5-trillion-token corpus and\nfurther enhanced through supervised fine-tuning and reinforcement learning with\nverifiable rewards. With only 3B activated and 7.5B stored parameters,\nMegrez2-Preview demonstrates competitive or superior performance compared to\nlarger models on a wide range of tasks, including language understanding,\ninstruction following, mathematical reasoning, and code generation. These\nresults highlight the effectiveness of the Megrez2 architecture to achieve a\nbalance between accuracy, efficiency, and deployability, making it a strong\ncandidate for real-world, resource-constrained applications.",
        "url": "http://arxiv.org/abs/2507.17728v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17728v1",
        "arxiv_id": "2507.17728v1",
        "authors": [
            "Boxun Li",
            "Yadong Li",
            "Zhiyuan Li",
            "Congyi Liu",
            "Weilin Liu",
            "Guowei Niu",
            "Zheyue Tan",
            "Haiyang Xu",
            "Zhuyu Yao",
            "Tao Yuan",
            "Dong Zhou",
            "Yueqing Zhuang",
            "Bo Zhao",
            "Guohao Dai",
            "Yu Wang"
        ],
        "submitted": "2025-07-23 17:43:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer",
        "abstract": "With the rise of voice-enabled artificial intelligence (AI) systems,\nquantitative survey researchers have access to a new data-collection mode: AI\ntelephone surveying. By using AI to conduct phone interviews, researchers can\nscale quantitative studies while balancing the dual goals of human-like\ninteractivity and methodological rigor. Unlike earlier efforts that used\ninteractive voice response (IVR) technology to automate these surveys, voice AI\nenables a more natural and adaptive respondent experience as it is more robust\nto interruptions, corrections, and other idiosyncrasies of human speech.\n  We built and tested an AI system to conduct quantitative surveys based on\nlarge language models (LLM), automatic speech recognition (ASR), and speech\nsynthesis technologies. The system was specifically designed for quantitative\nresearch, and strictly adhered to research best practices like question order\nrandomization, answer order randomization, and exact wording.\n  To validate the system's effectiveness, we deployed it to conduct two pilot\nsurveys with the SSRS Opinion Panel and followed-up with a separate\nhuman-administered survey to assess respondent experiences. We measured three\nkey metrics: the survey completion rates, break-off rates, and respondent\nsatisfaction scores. Our results suggest that shorter instruments and more\nresponsive AI interviewers may contribute to improvements across all three\nmetrics studied.",
        "url": "http://arxiv.org/abs/2507.17718v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17718v1",
        "arxiv_id": "2507.17718v1",
        "authors": [
            "Danny D. Leybzon",
            "Shreyas Tirumala",
            "Nishant Jain",
            "Summer Gillen",
            "Michael Jackson",
            "Cameron McPhee",
            "Jennifer Schmidt"
        ],
        "submitted": "2025-07-23 17:30:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes",
        "abstract": "AI-generated clinical notes are increasingly used in healthcare, but\nevaluating their quality remains a challenge due to high subjectivity and\nlimited scalability of expert review. Existing automated metrics often fail to\nalign with real-world physician preferences. To address this, we propose a\npipeline that systematically distills real user feedback into structured\nchecklists for note evaluation. These checklists are designed to be\ninterpretable, grounded in human feedback, and enforceable by LLM-based\nevaluators. Using deidentified data from over 21,000 clinical encounters,\nprepared in accordance with the HIPAA safe harbor standard, from a deployed AI\nmedical scribe system, we show that our feedback-derived checklist outperforms\nbaseline approaches in our offline evaluations in coverage, diversity, and\npredictive power for human ratings. Extensive experiments confirm the\nchecklist's robustness to quality-degrading perturbations, significant\nalignment with clinician preferences, and practical value as an evaluation\nmethodology. In offline research settings, the checklist can help identify\nnotes likely to fall below our chosen quality thresholds.",
        "url": "http://arxiv.org/abs/2507.17717v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17717v1",
        "arxiv_id": "2507.17717v1",
        "authors": [
            "Karen Zhou",
            "John Giorgi",
            "Pranav Mani",
            "Peng Xu",
            "Davis Liang",
            "Chenhao Tan"
        ],
        "submitted": "2025-07-23 17:28:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages of West Asia and North Africa",
        "abstract": "We present TyDi QA-WANA, a question-answering dataset consisting of 28K\nexamples divided among 10 language varieties of western Asia and northern\nAfrica. The data collection process was designed to elicit information-seeking\nquestions, where the asker is genuinely curious to know the answer. Each\nquestion in paired with an entire article that may or may not contain the\nanswer; the relatively large size of the articles results in a task suitable\nfor evaluating models' abilities to utilize large text contexts in answering\nquestions. Furthermore, the data was collected directly in each language\nvariety, without the use of translation, in order to avoid issues of cultural\nrelevance. We present performance of two baseline models, and release our code\nand data to facilitate further improvement by the research community.",
        "url": "http://arxiv.org/abs/2507.17709v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17709v1",
        "arxiv_id": "2507.17709v1",
        "authors": [
            "Parker Riley",
            "Siamak Shakeri",
            "Waleed Ammar",
            "Jonathan H. Clark"
        ],
        "submitted": "2025-07-23 17:20:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models",
        "abstract": "Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large\nLanguage Models (LLMs) efficiently by decoupling total parameters from\ncomputational cost. However, this decoupling creates a critical challenge:\npredicting the model capacity of a given MoE configurations (e.g., expert\nactivation ratio and granularity) remains an unresolved problem. To address\nthis gap, we introduce Efficiency Leverage (EL), a metric quantifying the\ncomputational advantage of an MoE model over a dense equivalent. We conduct a\nlarge-scale empirical study, training over 300 models up to 28B parameters, to\nsystematically investigate the relationship between MoE architectural\nconfigurations and EL. Our findings reveal that EL is primarily driven by the\nexpert activation ratio and the total compute budget, both following\npredictable power laws, while expert granularity acts as a non-linear modulator\nwith a clear optimal range. We integrate these discoveries into a unified\nscaling law that accurately predicts the EL of an MoE architecture based on its\nconfiguration. To validate our derived scaling laws, we designed and trained\nLing-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active\nparameters, alongside a 6.1B dense model for comparison. When trained on an\nidentical 1T high-quality token dataset, Ling-mini-beta matched the performance\nof the 6.1B dense model while consuming over 7x fewer computational resources,\nthereby confirming the accuracy of our scaling laws. This work provides a\nprincipled and empirically-grounded foundation for the scaling of efficient MoE\nmodels.",
        "url": "http://arxiv.org/abs/2507.17702v2",
        "pdf_url": "http://arxiv.org/pdf/2507.17702v2",
        "arxiv_id": "2507.17702v2",
        "authors": [
            "Changxin Tian",
            "Kunlong Chen",
            "Jia Liu",
            "Ziqi Liu",
            "Zhiqiang Zhang",
            "Jun Zhou"
        ],
        "submitted": "2025-07-23 17:10:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "On Function-Correcting Codes in the Lee Metric",
        "abstract": "Function-correcting codes are a coding framework designed to minimize\nredundancy while ensuring that specific functions or computations of encoded\ndata can be reliably recovered, even in the presence of errors. The choice of\nmetric is crucial in designing such codes, as it determines which computations\nmust be protected and how errors are measured and corrected. Previous work by\nLiu and Liu [6] studied function-correcting codes over $\\mathbb{Z}_{2^l},\\\nl\\geq 2$ using the homogeneous metric, which coincides with the Lee metric over\n$\\mathbb{Z}_4$. In this paper, we extend the study to codes over\n$\\mathbb{Z}_m,$ for any positive integer $m\\geq 2$ under the Lee metric and aim\nto determine their optimal redundancy. To achieve this, we introduce irregular\nLee distance codes and derive upper and lower bounds on the optimal redundancy\nby characterizing the shortest possible length of such codes. These general\nbounds are then simplified and applied to specific classes of functions,\nincluding Lee-local functions, Lee weight functions, and Lee weight\ndistribution functions, leading to improved some bounds compared to those of\nLiu and Liu [6] over $\\mathbb{Z}_4$ and generalize the other bounds over\n$\\mathbb{Z}_m$ in the Lee metric.",
        "url": "http://arxiv.org/abs/2507.17654v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17654v1",
        "arxiv_id": "2507.17654v1",
        "authors": [
            "Gyanendra K. Verma",
            "Abhay Kumar Singh"
        ],
        "submitted": "2025-07-23 16:17:46",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "QuMAB: Query-based Multi-annotator Behavior Pattern Learning",
        "abstract": "Multi-annotator learning traditionally aggregates diverse annotations to\napproximate a single ground truth, treating disagreements as noise. However,\nthis paradigm faces fundamental challenges: subjective tasks often lack\nabsolute ground truth, and sparse annotation coverage makes aggregation\nstatistically unreliable. We introduce a paradigm shift from sample-wise\naggregation to annotator-wise behavior modeling. By treating annotator\ndisagreements as valuable information rather than noise, modeling\nannotator-specific behavior patterns can reconstruct unlabeled data to reduce\nannotation cost, enhance aggregation reliability, and explain annotator\ndecision behavior. To this end, we propose QuMATL (Query-based Multi-Annotator\nBehavior Pattern Learning), which uses light-weight queries to model individual\nannotators while capturing inter-annotator correlations as implicit\nregularization, preventing overfitting to sparse individual data while\nmaintaining individualization and improving generalization, with a\nvisualization of annotator focus regions offering an explainable analysis of\nbehavior understanding. We contribute two large-scale datasets with dense\nper-annotator labels: STREET (4,300 labels/annotator) and AMER (average 3,118\nlabels/annotator), the first multimodal multi-annotator dataset.",
        "url": "http://arxiv.org/abs/2507.17653v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17653v1",
        "arxiv_id": "2507.17653v1",
        "authors": [
            "Liyun Zhang",
            "Zheng Lian",
            "Hong Liu",
            "Takanori Takebe",
            "Yuta Nakashima"
        ],
        "submitted": "2025-07-23 16:17:43",
        "source": "arxiv",
        "comment": "12 pages. arXiv admin note: substantial text overlap with\n  arXiv:2503.15237"
    },
    {
        "title": "Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries",
        "abstract": "Negative campaigning is a central feature of political competition, yet\nempirical research has been limited by the high cost and limited scalability of\nexisting classification methods. This study makes two key contributions. First,\nit introduces zero-shot Large Language Models (LLMs) as a novel approach for\ncross-lingual classification of negative campaigning. Using benchmark datasets\nin ten languages, we demonstrate that LLMs achieve performance on par with\nnative-speaking human coders and outperform conventional supervised machine\nlearning approaches. Second, we leverage this novel method to conduct the\nlargest cross-national study of negative campaigning to date, analyzing 18\nmillion tweets posted by parliamentarians in 19 European countries between 2017\nand 2022. The results reveal consistent cross-national patterns: governing\nparties are less likely to use negative messaging, while ideologically extreme\nand populist parties -- particularly those on the radical right -- engage in\nsignificantly higher levels of negativity. These findings advance our\nunderstanding of how party-level characteristics shape strategic communication\nin multiparty systems. More broadly, the study demonstrates the potential of\nLLMs to enable scalable, transparent, and replicable research in political\ncommunication across linguistic and cultural contexts.",
        "url": "http://arxiv.org/abs/2507.17636v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17636v1",
        "arxiv_id": "2507.17636v1",
        "authors": [
            "Victor Hartman",
            "Petter TÃ¶rnberg"
        ],
        "submitted": "2025-07-23 16:02:52",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training",
        "abstract": "Recent advances in learning rate (LR) scheduling have demonstrated the\neffectiveness of decay-free approaches that eliminate the traditional decay\nphase while maintaining competitive performance. Model merging techniques have\nemerged as particularly promising solutions in this domain. We present\nWarmup-Stable and Merge (WSM), a general framework that establishes a formal\nconnection between learning rate decay and model merging. WSM provides a\nunified theoretical foundation for emulating various decay strategies-including\ncosine decay, linear decay and inverse square root decay-as principled model\naveraging schemes, while remaining fully compatible with diverse optimization\nmethods. Through extensive experiments, we identify merge duration-the training\nwindow for checkpoint aggregation-as the most critical factor influencing model\nperformance, surpassing the importance of both checkpoint interval and merge\nquantity. Our framework consistently outperforms the widely-adopted\nWarmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving\nsignificant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on\nMMLU-Pro. The performance advantages extend to supervised fine-tuning\nscenarios, highlighting WSM's potential for long-term model refinement.",
        "url": "http://arxiv.org/abs/2507.17634v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17634v1",
        "arxiv_id": "2507.17634v1",
        "authors": [
            "Changxin Tian",
            "Jiapeng Wang",
            "Qian Zhao",
            "Kunlong Chen",
            "Jia Liu",
            "Ziqi Liu",
            "Jiaxin Mao",
            "Wayne Xin Zhao",
            "Zhiqiang Zhang",
            "Jun Zhou"
        ],
        "submitted": "2025-07-23 16:02:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)",
        "abstract": "Large language models are computationally expensive due to their deep\nstructures. Prior research has shown that intermediate layers contain\nsufficient information to generate accurate answers, leading to the development\nof early-exit algorithms that reduce inference costs by terminating computation\nat earlier layers. However, these methods often suffer from poor performance\ndue to misalignment between intermediate and output layer representations that\nlead to decoding inaccuracy. To address these challenges, we propose SPADE\n(SPace Alignment DEcoding), a novel decoding method that aligns intermediate\nlayer representations with the output layer by propagating a minimally reduced\nsequence consisting of only the start token and the answer token. We further\noptimize the early-exit decision-making process by training a linear\napproximation of SPADE that computes entropy-based confidence metrics. Putting\nthem together, we create a hybrid early-exit algorithm that monitors confidence\nlevels and stops inference at intermediate layers while using SPADE to generate\nhigh-quality outputs. This approach significantly reduces inference costs\nwithout compromising accuracy, offering a scalable and efficient solution for\ndeploying large language models in real-world applications.",
        "url": "http://arxiv.org/abs/2507.17618v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17618v1",
        "arxiv_id": "2507.17618v1",
        "authors": [
            "Bowen Zheng",
            "Ming Ma",
            "Zhongqiao Lin",
            "Tianming Yang"
        ],
        "submitted": "2025-07-23 15:49:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Citation Recommendation using Deep Canonical Correlation Analysis",
        "abstract": "Recent advances in citation recommendation have improved accuracy by\nleveraging multi-view representation learning to integrate the various\nmodalities present in scholarly documents. However, effectively combining\nmultiple data views requires fusion techniques that can capture complementary\ninformation while preserving the unique characteristics of each modality. We\npropose a novel citation recommendation algorithm that improves upon linear\nCanonical Correlation Analysis (CCA) methods by applying Deep CCA (DCCA), a\nneural network extension capable of capturing complex, non-linear relationships\nbetween distributed textual and graph-based representations of scientific\narticles. Experiments on the large-scale DBLP (Digital Bibliography & Library\nProject) citation network dataset demonstrate that our approach outperforms\nstate-of-the-art CCA-based methods, achieving relative improvements of over 11%\nin Mean Average Precision@10, 5% in Precision@10, and 7% in Recall@10. These\ngains reflect more relevant citation recommendations and enhanced ranking\nquality, suggesting that DCCA's non-linear transformations yield more\nexpressive latent representations than CCA's linear projections.",
        "url": "http://arxiv.org/abs/2507.17603v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17603v1",
        "arxiv_id": "2507.17603v1",
        "authors": [
            "Conor McNamara",
            "Effirul Ramlan"
        ],
        "submitted": "2025-07-23 15:34:07",
        "source": "arxiv",
        "comment": "21 pages, 6 figures, 7 tables"
    },
    {
        "title": "Dual-branch Prompting for Multimodal Machine Translation",
        "abstract": "Multimodal Machine Translation (MMT) typically enhances text-only translation\nby incorporating aligned visual features. Despite the remarkable progress,\nstate-of-the-art MMT approaches often rely on paired image-text inputs at\ninference and are sensitive to irrelevant visual noise, which limits their\nrobustness and practical applicability. To address these issues, we propose\nD2P-MMT, a diffusion-based dual-branch prompting framework for robust\nvision-guided translation. Specifically, D2P-MMT requires only the source text\nand a reconstructed image generated by a pre-trained diffusion model, which\nnaturally filters out distracting visual details while preserving semantic\ncues. During training, the model jointly learns from both authentic and\nreconstructed images using a dual-branch prompting strategy, encouraging rich\ncross-modal interactions. To bridge the modality gap and mitigate\ntraining-inference discrepancies, we introduce a distributional alignment loss\nthat enforces consistency between the output distributions of the two branches.\nExtensive experiments on the Multi30K dataset demonstrate that D2P-MMT achieves\nsuperior translation performance compared to existing state-of-the-art\napproaches.",
        "url": "http://arxiv.org/abs/2507.17588v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17588v1",
        "arxiv_id": "2507.17588v1",
        "authors": [
            "Jie Wang",
            "Zhendong Yang",
            "Liansong Zong",
            "Xiaobo Zhang",
            "Dexian Wang",
            "Ji Zhang"
        ],
        "submitted": "2025-07-23 15:22:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "GenSelect: A Generative Approach to Best-of-N",
        "abstract": "Generative reward models with parallel sampling have enabled effective\ntest-time scaling for reasoning tasks. Current approaches employ pointwise\nscoring of individual solutions or pairwise comparisons. However, pointwise\nmethods underutilize LLMs' comparative abilities, while pairwise methods scale\ninefficiently with larger sampling budgets. We introduce GenSelect, where the\nLLM uses long reasoning to select the best solution among N candidates. This\nleverages LLMs' comparative strengths while scaling efficiently across parallel\nsampling budgets. For math reasoning, we demonstrate that reasoning models,\nsuch as QwQ and DeepSeek-R1-0528, excel at GenSelect, outperforming existing\nscoring approaches with simple prompting.",
        "url": "http://arxiv.org/abs/2507.17797v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17797v1",
        "arxiv_id": "2507.17797v1",
        "authors": [
            "Shubham Toshniwal",
            "Ivan Sorokin",
            "Aleksander Ficek",
            "Ivan Moshkov",
            "Igor Gitman"
        ],
        "submitted": "2025-07-23 15:22:51",
        "source": "arxiv",
        "comment": "Presented at the 2nd AI for MATH Workshop @ ICML"
    },
    {
        "title": "Synthetic Voice Data for Automatic Speech Recognition in African Languages",
        "abstract": "Speech technology remains out of reach for most of the over 2300 languages in\nAfrica. We present the first systematic assessment of large-scale synthetic\nvoice corpora for African ASR. We apply a three-step process: LLM-driven text\ncreation, TTS voice synthesis, and ASR fine-tuning. Eight out of ten languages\nfor which we create synthetic text achieved readability scores above 5 out of\n7. We evaluated ASR improvement for three (Hausa, Dholuo, Chichewa) and created\nmore than 2,500 hours of synthetic voice data at below 1% of the cost of real\ndata. Fine-tuned Wav2Vec-BERT-2.0 models trained on 250h real and 250h\nsynthetic Hausa matched a 500h real-data-only baseline, while 579h real and\n450h to 993h synthetic data created the best performance. We also present\ngender-disaggregated ASR performance evaluation. For very low-resource\nlanguages, gains varied: Chichewa WER improved about 6.5% relative with a 1:2\nreal-to-synthetic ratio; a 1:1 ratio for Dholuo showed similar improvements on\nsome evaluation data, but not on others. Investigating intercoder reliability,\nASR errors and evaluation datasets revealed the need for more robust reviewer\nprotocols and more accurate evaluation data. All data and models are publicly\nreleased to invite further work to improve synthetic data for African\nlanguages.",
        "url": "http://arxiv.org/abs/2507.17578v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17578v1",
        "arxiv_id": "2507.17578v1",
        "authors": [
            "Brian DeRenzi",
            "Anna Dixon",
            "Mohamed Aymane Farhi",
            "Christian Resch"
        ],
        "submitted": "2025-07-23 15:13:32",
        "source": "arxiv",
        "comment": "29 pages incl. appendix, 8 tables, 5 figures. Authors are listed in\n  alphabetical order"
    },
    {
        "title": "BoSS: Beyond-Semantic Speech",
        "abstract": "Human communication involves more than explicit semantics, with implicit\nsignals and contextual cues playing a critical role in shaping meaning.\nHowever, modern speech technologies, such as Automatic Speech Recognition (ASR)\nand Text-to-Speech (TTS) often fail to capture these beyond-semantic\ndimensions. To better characterize and benchmark the progression of speech\nintelligence, we introduce Spoken Interaction System Capability Levels (L1-L5),\na hierarchical framework illustrated the evolution of spoken dialogue systems\nfrom basic command recognition to human-like social interaction. To support\nthese advanced capabilities, we propose Beyond-Semantic Speech (BoSS), which\nrefers to the set of information in speech communication that encompasses but\ntranscends explicit semantics. It conveys emotions, contexts, and modifies or\nextends meanings through multidimensional features such as affective cues,\ncontextual dynamics, and implicit semantics, thereby enhancing the\nunderstanding of communicative intentions and scenarios. We present a\nformalized framework for BoSS, leveraging cognitive relevance theories and\nmachine learning models to analyze temporal and contextual speech dynamics. We\nevaluate BoSS-related attributes across five different dimensions, reveals that\ncurrent spoken language models (SLMs) are hard to fully interpret\nbeyond-semantic signals. These findings highlight the need for advancing BoSS\nresearch to enable richer, more context-aware human-machine communication.",
        "url": "http://arxiv.org/abs/2507.17563v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17563v1",
        "arxiv_id": "2507.17563v1",
        "authors": [
            "Qing Wang",
            "Zehan Li",
            "Hang Lv",
            "Hongjie Chen",
            "Yaodong Song",
            "Jian Kang",
            "Jie Lian",
            "Jie Li",
            "Yongxiang Li",
            "Zhongjiang He",
            "Xuelong Li"
        ],
        "submitted": "2025-07-23 14:53:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice",
        "abstract": "Simultaneous Interpretation (SI) represents one of the most daunting\nfrontiers in the translation industry, with product-level automatic systems\nlong plagued by intractable challenges: subpar transcription and translation\nquality, lack of real-time speech generation, multi-speaker confusion, and\ntranslated speech inflation, especially in long-form discourses. In this study,\nwe introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers\nhigh-fidelity, ultra-low-latency speech-to-speech generation with voice cloning\ncapabilities. As a fully operational product-level solution, Seed-LiveInterpret\n2.0 tackles these challenges head-on through our novel duplex speech-to-speech\nunderstanding-generating framework. Experimental results demonstrate that\nthrough large-scale pretraining and reinforcement learning, the model achieves\na significantly better balance between translation accuracy and latency,\nvalidated by human interpreters to exceed 70% correctness in complex scenarios.\nNotably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by\nsignificant margins in translation quality, while slashing the average latency\nof cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is\naround a near 70% reduction that drastically enhances practical usability.",
        "url": "http://arxiv.org/abs/2507.17527v2",
        "pdf_url": "http://arxiv.org/pdf/2507.17527v2",
        "arxiv_id": "2507.17527v2",
        "authors": [
            "Shanbo Cheng",
            "Yu Bao",
            "Zhichao Huang",
            "Yu Lu",
            "Ningxin Peng",
            "Lu Xu",
            "Runsheng Yu",
            "Rong Cao",
            "Ting Han",
            "Zeyang Li",
            "Sitong Liu",
            "Shengtao Ma",
            "Shiguang Pan",
            "Jiongchen Xiao",
            "Nuo Xu",
            "Meng Yang",
            "Rong Ye",
            "Yiming Yu",
            "Ruofei Zhang",
            "Wanyi Zhang",
            "Wenhao Zhu",
            "Liehao Zou",
            "Lu Lu",
            "Yuxuan Wang",
            "Yonghui Wu"
        ],
        "submitted": "2025-07-23 14:07:41",
        "source": "arxiv",
        "comment": "Seed-LiveInterpret 2.0 Technical Report"
    },
    {
        "title": "URPO: A Unified Reward & Policy Optimization Framework for Large Language Models",
        "abstract": "Large-scale alignment pipelines typically pair a policy model with a\nseparately trained reward model whose parameters remain frozen during\nreinforcement learning (RL). This separation creates a complex,\nresource-intensive pipeline and suffers from a performance ceiling due to a\nstatic reward signal. We propose a novel framework, Unified Reward & Policy\nOptimization (URPO), that unifies instruction-following (\"player\") and reward\nmodeling (\"referee\") within a single model and a single training phase. Our\nmethod recasts all alignment data-including preference pairs, verifiable\nreasoning, and open-ended instructions-into a unified generative format\noptimized by a single Group-Relative Policy Optimization (GRPO) loop. This\nenables the model to learn from ground-truth preferences and verifiable logic\nwhile simultaneously generating its own rewards for open-ended tasks.\nExperiments on the Qwen2.5-7B model demonstrate URPO's superiority. Our unified\nmodel significantly outperforms a strong baseline using a separate generative\nreward model, boosting the instruction-following score on AlpacaEval from 42.24\nto 44.84 and the composite reasoning average from 32.66 to 35.66. Furthermore,\nURPO cultivates a superior internal evaluator as a byproduct of training,\nachieving a RewardBench score of 85.15 and surpassing the dedicated reward\nmodel it replaces (83.55). By eliminating the need for a separate reward model\nand fostering a co-evolutionary dynamic between generation and evaluation, URPO\npresents a simpler, more efficient, and more effective path towards robustly\naligned language models.",
        "url": "http://arxiv.org/abs/2507.17515v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17515v1",
        "arxiv_id": "2507.17515v1",
        "authors": [
            "Songshuo Lu",
            "Hua Wang",
            "Zhi Chen",
            "Yaohua Tang"
        ],
        "submitted": "2025-07-23 13:52:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DNT: a Deeply Normalized Transformer that can be trained by Momentum SGD",
        "abstract": "Transformers have become the de facto backbone of modern deep learning, yet\ntheir training typically demands an advanced optimizer with adaptive learning\nrate like AdamW, rather than a momentum SGDW (mSGDW). Previous works show that\nit is mainly due to a heavy-tailed distribution of the gradients. In this\npaper, we introduce a Deeply Normalized Transformer (DNT), which is\nmeticulously engineered to overcome this limitation enabling seamless training\nwith vanilla mSGDW while yielding comparable performance to the Transformers\ntrained via AdamW. To be specific, in DNT, we strategically integrate\nnormalization techniques at proper positions in the Transformers to effectively\nmodulate the Jacobian matrices of each layer, balance the influence of weights,\nactivations, and their interactions, and thus enable the distributions of\ngradients concentrated. We provide both theoretical justifications of the\nnormalization technique used in our DNT and extensive empirical evaluation on\ntwo popular Transformer architectures to validate that: a) DNT outperforms its\ncounterparts (\\ie, ViT and GPT), and b) DNT can be effectively trained with\nvanilla mSGDW.",
        "url": "http://arxiv.org/abs/2507.17501v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17501v1",
        "arxiv_id": "2507.17501v1",
        "authors": [
            "Xianbiao Qi",
            "Marco Chen",
            "Wenjie Xiao",
            "Jiaquan Ye",
            "Yelin He",
            "Chun-Guang Li",
            "Zhouchen Lin"
        ],
        "submitted": "2025-07-23 13:37:23",
        "source": "arxiv",
        "comment": "We have introduced a novel architecture, Deeply Normalized\n  Transformer (DNT), which enables efficient training with vanilla momentum\n  SGDW (mSGDW), achieving performance on par with AdamW-optimized Transformers"
    },
    {
        "title": "MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs",
        "abstract": "Although recent Large Language Models (LLMs) have shown rapid improvement on\nreasoning benchmarks in English, the evaluation of such LLMs' multilingual\nreasoning capability across diverse languages and cultural contexts remains\nlimited. Existing multilingual reasoning benchmarks are typically constructed\nby translating existing English reasoning benchmarks, biasing these benchmarks\ntowards reasoning problems with context in English language/cultures. In this\nwork, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a\nbenchmark designed to assess LLMs on more than 1,000 native, linguistic and\nculturally grounded reasoning questions written by native speakers in French,\nSpanish, and Chinese. MultiNRC covers four core reasoning categories:\nlanguage-specific linguistic reasoning, wordplay & riddles, cultural/tradition\nreasoning, and math reasoning with cultural relevance. For cultural/tradition\nreasoning and math reasoning with cultural relevance, we also provide English\nequivalent translations of the multilingual questions by manual translation\nfrom native speakers fluent in English. This set of English equivalents can\nprovide a direct comparison of LLM reasoning capacity in other languages vs.\nEnglish on the same reasoning questions. We systematically evaluate current 14\nleading LLMs covering most LLM families on MultiNRC and its English equivalent\nset. The results show that (1) current LLMs are still not good at native\nmultilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs\nexhibit distinct strengths and weaknesses in handling linguistic, cultural, and\nlogical reasoning tasks; (3) Most models perform substantially better in math\nreasoning in English compared to in original languages (+10%), indicating\npersistent challenges with culturally grounded knowledge.",
        "url": "http://arxiv.org/abs/2507.17476v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17476v1",
        "arxiv_id": "2507.17476v1",
        "authors": [
            "Alexander R. Fabbri",
            "Diego Mares",
            "Jorge Flores",
            "Meher Mankikar",
            "Ernesto Hernandez",
            "Dean Lee",
            "Bing Liu",
            "Chen Xing"
        ],
        "submitted": "2025-07-23 12:56:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "BGM-HAN: A Hierarchical Attention Network for Accurate and Fair Decision Assessment on Semi-Structured Profiles",
        "abstract": "Human decision-making in high-stakes domains often relies on expertise and\nheuristics, but is vulnerable to hard-to-detect cognitive biases that threaten\nfairness and long-term outcomes. This work presents a novel approach to\nenhancing complex decision-making workflows through the integration of\nhierarchical learning alongside various enhancements. Focusing on university\nadmissions as a representative high-stakes domain, we propose BGM-HAN, an\nenhanced Byte-Pair Encoded, Gated Multi-head Hierarchical Attention Network,\ndesigned to effectively model semi-structured applicant data. BGM-HAN captures\nmulti-level representations that are crucial for nuanced assessment, improving\nboth interpretability and predictive performance. Experimental results on real\nadmissions data demonstrate that our proposed model significantly outperforms\nboth state-of-the-art baselines from traditional machine learning to large\nlanguage models, offering a promising framework for augmenting decision-making\nin domains where structure, context, and fairness matter. Source code is\navailable at: https://github.com/junhua/bgm-han.",
        "url": "http://arxiv.org/abs/2507.17472v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17472v1",
        "arxiv_id": "2507.17472v1",
        "authors": [
            "Junhua Liu",
            "Roy Ka-Wei Lee",
            "Kwan Hui Lim"
        ],
        "submitted": "2025-07-23 12:52:38",
        "source": "arxiv",
        "comment": "Accepted at ASONAM 2025"
    },
    {
        "title": "Each to Their Own: Exploring the Optimal Embedding in RAG",
        "abstract": "Recently, as Large Language Models (LLMs) have fundamentally impacted various\nfields, the methods for incorporating up-to-date information into LLMs or\nadding external knowledge to construct domain-specific models have garnered\nwide attention. Retrieval-Augmented Generation (RAG), serving as an\ninference-time scaling method, is notable for its low cost and minimal effort\nfor parameter tuning. However, due to heterogeneous training data and model\narchitecture, the variant embedding models used in RAG exhibit different\nbenefits across various areas, often leading to different similarity\ncalculation results and, consequently, varying response quality from LLMs. To\naddress this problem, we propose and examine two approaches to enhance RAG by\ncombining the benefits of multiple embedding models, named Mixture-Embedding\nRAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects\nretrievals from multiple embedding models based on standardized similarity;\nhowever, it does not outperform vanilla RAG. In contrast, Confident RAG\ngenerates responses multiple times using different embedding models and then\nselects the responses with the highest confidence level, demonstrating average\nimprovements of approximately 10% and 5% over vanilla LLMs and RAG,\nrespectively. The consistent results across different LLMs and embedding models\nindicate that Confident RAG is an efficient plug-and-play approach for various\ndomains. We will release our code upon publication.",
        "url": "http://arxiv.org/abs/2507.17442v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17442v1",
        "arxiv_id": "2507.17442v1",
        "authors": [
            "Shiting Chen",
            "Zijian Zhao",
            "Jinsong Chen"
        ],
        "submitted": "2025-07-23 12:03:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Content-based 3D Image Retrieval and a ColBERT-inspired Re-ranking for Tumor Flagging and Staging",
        "abstract": "The increasing volume of medical images poses challenges for radiologists in\nretrieving relevant cases. Content-based image retrieval (CBIR) systems offer\npotential for efficient access to similar cases, yet lack standardized\nevaluation and comprehensive studies. Building on prior studies for tumor\ncharacterization via CBIR, this study advances CBIR research for volumetric\nmedical images through three key contributions: (1) a framework eliminating\nreliance on pre-segmented data and organ-specific datasets, aligning with large\nand unstructured image archiving systems, i.e. PACS in clinical practice; (2)\nintroduction of C-MIR, a novel volumetric re-ranking method adapting ColBERT's\ncontextualized late interaction mechanism for 3D medical imaging; (3)\ncomprehensive evaluation across four tumor sites using three feature extractors\nand three database configurations. Our evaluations highlight the significant\nadvantages of C-MIR. We demonstrate the successful adaptation of the late\ninteraction principle to volumetric medical images, enabling effective\ncontext-aware re-ranking. A key finding is C-MIR's ability to effectively\nlocalize the region of interest, eliminating the need for pre-segmentation of\ndatasets and offering a computationally efficient alternative to systems\nrelying on expensive data enrichment steps. C-MIR demonstrates promising\nimprovements in tumor flagging, achieving improved performance, particularly\nfor colon and lung tumors (p<0.05). C-MIR also shows potential for improving\ntumor staging, warranting further exploration of its capabilities. Ultimately,\nour work seeks to bridge the gap between advanced retrieval techniques and\ntheir practical applications in healthcare, paving the way for improved\ndiagnostic processes.",
        "url": "http://arxiv.org/abs/2507.17412v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17412v1",
        "arxiv_id": "2507.17412v1",
        "authors": [
            "Farnaz Khun Jush",
            "Steffen Vogler",
            "Matthias Lenga"
        ],
        "submitted": "2025-07-23 11:12:52",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Investigating Subjective Factors of Argument Strength: Storytelling, Emotions, and Hedging",
        "abstract": "In assessing argument strength, the notions of what makes a good argument are\nmanifold. With the broader trend towards treating subjectivity as an asset and\nnot a problem in NLP, new dimensions of argument quality are studied. Although\nstudies on individual subjective features like personal stories exist, there is\na lack of large-scale analyses of the relation between these features and\nargument strength. To address this gap, we conduct regression analysis to\nquantify the impact of subjective factors $-$ emotions, storytelling, and\nhedging $-$ on two standard datasets annotated for objective argument quality\nand subjective persuasion. As such, our contribution is twofold: at the level\nof contributed resources, as there are no datasets annotated with all studied\ndimensions, this work compares and evaluates automated annotation methods for\neach subjective feature. At the level of novel insights, our regression\nanalysis uncovers different patterns of impact of subjective features on the\ntwo facets of argument strength encoded in the datasets. Our results show that\nstorytelling and hedging have contrasting effects on objective and subjective\nargument quality, while the influence of emotions depends on their rhetoric\nutilization rather than the domain.",
        "url": "http://arxiv.org/abs/2507.17409v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17409v1",
        "arxiv_id": "2507.17409v1",
        "authors": [
            "Carlotta Quensel",
            "Neele Falk",
            "Gabriella Lapesa"
        ],
        "submitted": "2025-07-23 11:09:52",
        "source": "arxiv",
        "comment": "Accepted to the 12th Workshop on Argument Mining (ArgMining) 2025"
    },
    {
        "title": "HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic Learning",
        "abstract": "Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of\nmatching untrimmed videos with text queries describing only partial content.\nExisting methods suffer from geometric distortion in Euclidean space that\nsometimes misrepresents the intrinsic hierarchical structure of videos and\noverlooks certain hierarchical semantics, ultimately leading to suboptimal\ntemporal modeling. To address this issue, we propose the first hyperbolic\nmodeling framework for PRVR, namely HLFormer, which leverages hyperbolic space\nlearning to compensate for the suboptimal hierarchical modeling capabilities of\nEuclidean space. Specifically, HLFormer integrates the Lorentz Attention Block\nand Euclidean Attention Block to encode video embeddings in hybrid spaces,\nusing the Mean-Guided Adaptive Interaction Module to dynamically fuse features.\nAdditionally, we introduce a Partial Order Preservation Loss to enforce \"text <\nvideo\" hierarchy through Lorentzian cone constraints. This approach further\nenhances cross-modal matching by reinforcing partial relevance between video\ncontent and text queries. Extensive experiments show that HLFormer outperforms\nstate-of-the-art methods. Code is released at\nhttps://github.com/lijun2005/ICCV25-HLFormer.",
        "url": "http://arxiv.org/abs/2507.17402v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17402v1",
        "arxiv_id": "2507.17402v1",
        "authors": [
            "Li Jun",
            "Wang Jinpeng",
            "Tan Chaolei",
            "Lian Niu",
            "Chen Long",
            "Zhang Min",
            "Wang Yaowei",
            "Xia Shu-Tao",
            "Chen Bin"
        ],
        "submitted": "2025-07-23 10:59:46",
        "source": "arxiv",
        "comment": "Accepted by ICCV'25. 13 pages, 6 figures, 4 tables"
    },
    {
        "title": "Millions of $\\text{GeAR}$-s: Extending GraphRAG to Millions of Documents",
        "abstract": "Recent studies have explored graph-based approaches to retrieval-augmented\ngeneration, leveraging structured or semi-structured information -- such as\nentities and their relations extracted from documents -- to enhance retrieval.\nHowever, these methods are typically designed to address specific tasks, such\nas multi-hop question answering and query-focused summarisation, and therefore,\nthere is limited evidence of their general applicability across broader\ndatasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG\nsolution: $\\text{GeAR}$ and explore its performance and limitations on the\nSIGIR 2025 LiveRAG Challenge.",
        "url": "http://arxiv.org/abs/2507.17399v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17399v1",
        "arxiv_id": "2507.17399v1",
        "authors": [
            "Zhili Shen",
            "Chenxin Diao",
            "Pascual Merita",
            "Pavlos Vougiouklis",
            "Jeff Z. Pan"
        ],
        "submitted": "2025-07-23 10:54:24",
        "source": "arxiv",
        "comment": "Accepted by SIGIR 2025 LiveRAG Challenge Program"
    },
    {
        "title": "\"Beyond the past\": Leveraging Audio and Human Memory for Sequential Music Recommendation",
        "abstract": "On music streaming services, listening sessions are often composed of a\nbalance of familiar and new tracks. Recently, sequential recommender systems\nhave adopted cognitive-informed approaches, such as Adaptive Control of\nThought-Rational (ACT-R), to successfully improve the prediction of the most\nrelevant tracks for the next user session. However, one limitation of using a\nmodel inspired by human memory (or the past), is that it struggles to recommend\nnew tracks that users have not previously listened to. To bridge this gap, here\nwe propose a model that leverages audio information to predict in advance the\nACT-R-like activation of new tracks and incorporates them into the\nrecommendation scoring process. We demonstrate the empirical effectiveness of\nthe proposed model using proprietary data, which we publicly release along with\nthe model's source code to foster future research in this field.",
        "url": "http://arxiv.org/abs/2507.17356v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17356v1",
        "arxiv_id": "2507.17356v1",
        "authors": [
            "Viet-Tran Anh",
            "Bruno Sguerra",
            "Gabriel Meseguer-Brocal",
            "Lea Briand",
            "Manuel Moussallam"
        ],
        "submitted": "2025-07-23 09:37:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TransLPRNet: Lite Vision-Language Network for Single/Dual-line Chinese License Plate Recognition",
        "abstract": "License plate recognition in open environments is widely applicable across\nvarious domains; however, the diversity of license plate types and imaging\nconditions presents significant challenges. To address the limitations\nencountered by CNN and CRNN-based approaches in license plate recognition, this\npaper proposes a unified solution that integrates a lightweight visual encoder\nwith a text decoder, within a pre-training framework tailored for single and\ndouble-line Chinese license plates. To mitigate the scarcity of double-line\nlicense plate datasets, we constructed a single/double-line license plate\ndataset by synthesizing images, applying texture mapping onto real scenes, and\nblending them with authentic license plate images. Furthermore, to enhance the\nsystem's recognition accuracy, we introduce a perspective correction network\n(PTN) that employs license plate corner coordinate regression as an implicit\nvariable, supervised by license plate view classification information. This\nnetwork offers improved stability, interpretability, and low annotation costs.\nThe proposed algorithm achieves an average recognition accuracy of 99.34% on\nthe corrected CCPD test set under coarse localization disturbance. When\nevaluated under fine localization disturbance, the accuracy further improves to\n99.58%. On the double-line license plate test set, it achieves an average\nrecognition accuracy of 98.70%, with processing speeds reaching up to 167\nframes per second, indicating strong practical applicability.",
        "url": "http://arxiv.org/abs/2507.17335v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17335v1",
        "arxiv_id": "2507.17335v1",
        "authors": [
            "Guangzhu Xu",
            "Zhi Ke",
            "Pengcheng Zuo",
            "Bangjun Lei"
        ],
        "submitted": "2025-07-23 09:03:01",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "EndoFinder: Online Lesion Retrieval for Explainable Colorectal Polyp Diagnosis Leveraging Latent Scene Representations",
        "abstract": "Colorectal cancer (CRC) remains a leading cause of cancer-related mortality,\nunderscoring the importance of timely polyp detection and diagnosis. While deep\nlearning models have improved optical-assisted diagnostics, they often demand\nextensive labeled datasets and yield \"black-box\" outputs with limited\ninterpretability. In this paper, we propose EndoFinder, an online polyp\nretrieval framework that leverages multi-view scene representations for\nexplainable and scalable CRC diagnosis. First, we develop a Polyp-aware Image\nEncoder by combining contrastive learning and a reconstruction task, guided by\npolyp segmentation masks. This self-supervised approach captures robust\nfeatures without relying on large-scale annotated data. Next, we treat each\npolyp as a three-dimensional \"scene\" and introduce a Scene Representation\nTransformer, which fuses multiple views of the polyp into a single latent\nrepresentation. By discretizing this representation through a hashing layer,\nEndoFinder enables real-time retrieval from a compiled database of historical\npolyp cases, where diagnostic information serves as interpretable references\nfor new queries. We evaluate EndoFinder on both public and newly collected\npolyp datasets for re-identification and pathology classification. Results show\nthat EndoFinder outperforms existing methods in accuracy while providing\ntransparent, retrieval-based insights for clinical decision-making. By\ncontributing a novel dataset and a scalable, explainable framework, our work\naddresses key challenges in polyp diagnosis and offers a promising direction\nfor more efficient AI-driven colonoscopy workflows. The source code is\navailable at https://github.com/ku262/EndoFinder-Scene.",
        "url": "http://arxiv.org/abs/2507.17323v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17323v1",
        "arxiv_id": "2507.17323v1",
        "authors": [
            "Ruijie Yang",
            "Yan Zhu",
            "Peiyao Fu",
            "Yizhe Zhang",
            "Zhihua Wang",
            "Quanlin Li",
            "Pinghong Zhou",
            "Xian Yang",
            "Shuo Wang"
        ],
        "submitted": "2025-07-23 08:45:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Exploring the Potential of LLMs for Serendipity Evaluation in Recommender Systems",
        "abstract": "Serendipity plays a pivotal role in enhancing user satisfaction within\nrecommender systems, yet its evaluation poses significant challenges due to its\ninherently subjective nature and conceptual ambiguity. Current algorithmic\napproaches predominantly rely on proxy metrics for indirect assessment, often\nfailing to align with real user perceptions, thus creating a gap. With large\nlanguage models (LLMs) increasingly revolutionizing evaluation methodologies\nacross various human annotation tasks, we are inspired to explore a core\nresearch proposition: Can LLMs effectively simulate human users for serendipity\nevaluation? To address this question, we conduct a meta-evaluation on two\ndatasets derived from real user studies in the e-commerce and movie domains,\nfocusing on three key aspects: the accuracy of LLMs compared to conventional\nproxy metrics, the influence of auxiliary data on LLM comprehension, and the\nefficacy of recently popular multi-LLM techniques. Our findings indicate that\neven the simplest zero-shot LLMs achieve parity with, or surpass, the\nperformance of conventional metrics. Furthermore, multi-LLM techniques and the\nincorporation of auxiliary data further enhance alignment with human\nperspectives. Based on our findings, the optimal evaluation by LLMs yields a\nPearson correlation coefficient of 21.5\\% when compared to the results of the\nuser study. This research implies that LLMs may serve as potentially accurate\nand cost-effective evaluators, introducing a new paradigm for serendipity\nevaluation in recommender systems.",
        "url": "http://arxiv.org/abs/2507.17290v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17290v1",
        "arxiv_id": "2507.17290v1",
        "authors": [
            "Li Kang",
            "Yuhan Zhao",
            "Li Chen"
        ],
        "submitted": "2025-07-23 07:51:56",
        "source": "arxiv",
        "comment": "RecSys2025"
    },
    {
        "title": "Triple X: A LLM-Based Multilingual Speech Recognition System for the INTERSPEECH2025 MLC-SLM Challenge",
        "abstract": "This paper describes our Triple X speech recognition system submitted to Task\n1 of the Multi-Lingual Conversational Speech Language Modeling (MLC-SLM)\nChallenge. Our work focuses on optimizing speech recognition accuracy in\nmultilingual conversational scenarios through an innovative encoder-adapter-LLM\narchitecture. This framework harnesses the powerful reasoning capabilities of\ntext-based large language models while incorporating domain-specific\nadaptations. To further enhance multilingual recognition performance, we\nadopted a meticulously designed multi-stage training strategy leveraging\nextensive multilingual audio datasets. Experimental results demonstrate that\nour approach achieves competitive Word Error Rate (WER) performance on both dev\nand test sets, obtaining second place in the challenge ranking.",
        "url": "http://arxiv.org/abs/2507.17288v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17288v1",
        "arxiv_id": "2507.17288v1",
        "authors": [
            "Miaomiao Gao",
            "Xiaoxiao Xiang",
            "Yiwen Guo"
        ],
        "submitted": "2025-07-23 07:48:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Tab-MIA: A Benchmark Dataset for Membership Inference Attacks on Tabular Data in LLMs",
        "abstract": "Large language models (LLMs) are increasingly trained on tabular data, which,\nunlike unstructured text, often contains personally identifiable information\n(PII) in a highly structured and explicit format. As a result, privacy risks\narise, since sensitive records can be inadvertently retained by the model and\nexposed through data extraction or membership inference attacks (MIAs). While\nexisting MIA methods primarily target textual content, their efficacy and\nthreat implications may differ when applied to structured data, due to its\nlimited content, diverse data types, unique value distributions, and\ncolumn-level semantics. In this paper, we present Tab-MIA, a benchmark dataset\nfor evaluating MIAs on tabular data in LLMs and demonstrate how it can be used.\nTab-MIA comprises five data collections, each represented in six different\nencoding formats. Using our Tab-MIA benchmark, we conduct the first evaluation\nof state-of-the-art MIA methods on LLMs finetuned with tabular data across\nmultiple encoding formats. In the evaluation, we analyze the memorization\nbehavior of pretrained LLMs on structured data derived from Wikipedia tables.\nOur findings show that LLMs memorize tabular data in ways that vary across\nencoding formats, making them susceptible to extraction via MIAs. Even when\nfine-tuned for as few as three epochs, models exhibit high vulnerability, with\nAUROC scores approaching 90% in most cases. Tab-MIA enables systematic\nevaluation of these risks and provides a foundation for developing\nprivacy-preserving methods for tabular data in LLMs.",
        "url": "http://arxiv.org/abs/2507.17259v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17259v1",
        "arxiv_id": "2507.17259v1",
        "authors": [
            "Eyal German",
            "Sagiv Antebi",
            "Daniel Samira",
            "Asaf Shabtai",
            "Yuval Elovici"
        ],
        "submitted": "2025-07-23 06:56:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "R4ec: A Reasoning, Reflection, and Refinement Framework for Recommendation Systems",
        "abstract": "Harnessing Large Language Models (LLMs) for recommendation systems has\nemerged as a prominent avenue, drawing substantial research interest. However,\nexisting approaches primarily involve basic prompt techniques for knowledge\nacquisition, which resemble System-1 thinking. This makes these methods highly\nsensitive to errors in the reasoning path, where even a small mistake can lead\nto an incorrect inference. To this end, in this paper, we propose $R^{4}$ec, a\nreasoning, reflection and refinement framework that evolves the recommendation\nsystem into a weak System-2 model. Specifically, we introduce two models: an\nactor model that engages in reasoning, and a reflection model that judges these\nresponses and provides valuable feedback. Then the actor model will refine its\nresponse based on the feedback, ultimately leading to improved responses. We\nemploy an iterative reflection and refinement process, enabling LLMs to\nfacilitate slow and deliberate System-2-like thinking. Ultimately, the final\nrefined knowledge will be incorporated into a recommendation backbone for\nprediction. We conduct extensive experiments on Amazon-Book and MovieLens-1M\ndatasets to demonstrate the superiority of $R^{4}$ec. We also deploy $R^{4}$ec\non a large scale online advertising platform, showing 2.2\\% increase of\nrevenue. Furthermore, we investigate the scaling properties of the actor model\nand reflection model.",
        "url": "http://arxiv.org/abs/2507.17249v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17249v1",
        "arxiv_id": "2507.17249v1",
        "authors": [
            "Hao Gu",
            "Rui Zhong",
            "Yu Xia",
            "Wei Yang",
            "Chi Lu",
            "Peng Jiang",
            "Kun Gai"
        ],
        "submitted": "2025-07-23 06:36:49",
        "source": "arxiv",
        "comment": "Accepted by Recsys25"
    },
    {
        "title": "CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings",
        "abstract": "Automatic generation of radiology reports has the potential to alleviate\nradiologists' significant workload, yet current methods struggle to deliver\nclinically reliable conclusions. In particular, most prior approaches focus on\nproducing fluent text without effectively ensuring the factual correctness of\nthe reports and often rely on single-view images, limiting diagnostic\ncomprehensiveness. We propose CLARIFID, a novel framework that directly\noptimizes diagnostic correctness by mirroring the two-step workflow of experts.\nSpecifically, CLARIFID (1) learns the logical flow from Findings to Impression\nthrough section-aware pretraining, (2) is fine-tuned with Proximal Policy\nOptimization in which the CheXbert F1 score of the Impression section serves as\nthe reward, (3) enforces reasoning-aware decoding that completes \"Findings\"\nbefore synthesizing the \"Impression\", and (4) fuses multiple chest X-ray views\nvia a vision-transformer-based multi-view encoder. During inference, we apply a\nreasoning-aware next-token forcing strategy followed by report-level\nre-ranking, ensuring that the model first produces a comprehensive Findings\nsection before synthesizing the Impression and thereby preserving coherent\nclinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate\nthat our method achieves superior clinical efficacy and outperforms existing\nbaselines on both standard NLG metrics and clinically aware scores.",
        "url": "http://arxiv.org/abs/2507.17234v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17234v1",
        "arxiv_id": "2507.17234v1",
        "authors": [
            "Kyeongkyu Lee",
            "Seonghwan Yoon",
            "Hongki Lim"
        ],
        "submitted": "2025-07-23 05:57:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Highly Clean Recipe Dataset with Ingredient States Annotation for State Probing Task",
        "abstract": "Large Language Models (LLMs) are trained on a vast amount of procedural\ntexts, but they do not directly observe real-world phenomena. In the context of\ncooking recipes, this poses a challenge, as intermediate states of ingredients\nare often omitted, making it difficult for models to track ingredient states\nand understand recipes accurately. In this paper, we apply state probing, a\nmethod for evaluating a language model's understanding of the world, to the\ndomain of cooking. We propose a new task and dataset for evaluating how well\nLLMs can recognize intermediate ingredient states during cooking procedures. We\nfirst construct a new Japanese recipe dataset with clear and accurate\nannotations of ingredient state changes, collected from well-structured and\ncontrolled recipe texts. Using this dataset, we design three novel tasks to\nevaluate whether LLMs can track ingredient state transitions and identify\ningredients present at intermediate steps. Our experiments with widely used\nLLMs, such as Llama3.1-70B and Qwen2.5-72B, show that learning ingredient state\nknowledge improves their understanding of cooking processes, achieving\nperformance comparable to commercial LLMs.",
        "url": "http://arxiv.org/abs/2507.17232v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17232v1",
        "arxiv_id": "2507.17232v1",
        "authors": [
            "Mashiro Toyooka",
            "Kiyoharu Aizawa",
            "Yoko Yamakata"
        ],
        "submitted": "2025-07-23 05:56:20",
        "source": "arxiv",
        "comment": "Accepted to ACM Multimedia 2025"
    },
    {
        "title": "The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models",
        "abstract": "People increasingly rely on Large Language Models (LLMs) for moral advice,\nwhich may influence humans' decisions. Yet, little is known about how closely\nLLMs align with human moral judgments. To address this, we introduce the Moral\nDilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a\ndistribution of human moral judgments consisting of a binary evaluation and a\nfree-text rationale. We treat this problem as a pluralistic distributional\nalignment task, comparing the distributions of LLM and human judgments across\ndilemmas. We find that models reproduce human judgments only under high\nconsensus; alignment deteriorates sharply when human disagreement increases. In\nparallel, using a 60-value taxonomy built from 3,783 value expressions\nextracted from rationales, we show that LLMs rely on a narrower set of moral\nvalues than humans. These findings reveal a pluralistic moral gap: a mismatch\nin both the distribution and diversity of values expressed. To close this gap,\nwe introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method\nthat conditions model outputs on human-derived value profiles. DMP improves\nalignment by 64.3% and enhances value diversity, offering a step toward more\npluralistic and human-aligned moral guidance from LLMs.",
        "url": "http://arxiv.org/abs/2507.17216v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17216v1",
        "arxiv_id": "2507.17216v1",
        "authors": [
            "Giuseppe Russo",
            "Debora Nozza",
            "Paul RÃ¶ttger",
            "Dirk Hovy"
        ],
        "submitted": "2025-07-23 05:26:17",
        "source": "arxiv",
        "comment": "13 pages, 4 figures"
    },
    {
        "title": "Triadic First-Order Logic Queries in Temporal Networks",
        "abstract": "Motif counting is a fundamental problem in network analysis, and there is a\nrich literature of theoretical and applied algorithms for this problem. Given a\nlarge input network $G$, a motif $H$ is a small \"pattern\" graph indicative of\nspecial local structure. Motif/pattern mining involves finding all matches of\nthis pattern in the input $G$. The simplest, yet challenging, case of motif\ncounting is when $H$ has three vertices, often called a \"triadic\" query. Recent\nwork has focused on \"temporal graph mining\", where the network $G$ has edges\nwith timestamps (and directions) and $H$ has time constraints.\n  Inspired by concepts in logic and database theory, we introduce the study of\n\"thresholded First Order Logic (FOL) Motif Analysis\" for massive temporal\nnetworks. A typical triadic motif query asks for the existence of three\nvertices that form a desired temporal pattern. An \"FOL\" motif query is obtained\nby having both existential and thresholded universal quantifiers. This allows\nfor query semantics that can mine richer information from networks. A typical\ntriadic query would be \"find all triples of vertices $u,v,w$ such that they\nform a triangle within one hour\". A thresholded FOL query can express \"find all\npairs $u,v$ such that for half of $w$ where $(u,w)$ formed an edge, $(v,w)$\nalso formed an edge within an hour\".\n  We design the first algorithm, FOLTY, for mining thresholded FOL triadic\nqueries. The theoretical running time of FOLTY matches the best known running\ntime for temporal triangle counting in sparse graphs. We give an efficient\nimplementation of FOLTY using specialized temporal data structures. FOLTY has\nexcellent empirical behavior, and can answer triadic FOL queries on graphs with\nnearly 70M edges is less than hour on commodity hardware. Our work has the\npotential to start a new research direction in the classic well-studied problem\nof motif analysis.",
        "url": "http://arxiv.org/abs/2507.17215v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17215v1",
        "arxiv_id": "2507.17215v1",
        "authors": [
            "Omkar Bhalerao",
            "Yunjie Pan",
            "C. Seshadhri",
            "Nishil Talati"
        ],
        "submitted": "2025-07-23 05:12:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "FinGAIA: An End-to-End Benchmark for Evaluating AI Agents in Finance",
        "abstract": "The booming development of AI agents presents unprecedented opportunities for\nautomating complex tasks across various domains. However, their multi-step,\nmulti-tool collaboration capabilities in the financial sector remain\nunderexplored. This paper introduces FinGAIA, an end-to-end benchmark designed\nto evaluate the practical abilities of AI agents in the financial domain.\nFinGAIA comprises 407 meticulously crafted tasks, spanning seven major\nfinancial sub-domains: securities, funds, banking, insurance, futures, trusts,\nand asset management. These tasks are organized into three hierarchical levels\nof scenario depth: basic business analysis, asset decision support, and\nstrategic risk management. We evaluated 10 mainstream AI agents in a zero-shot\nsetting. The best-performing agent, ChatGPT, achieved an overall accuracy of\n48.9\\%, which, while superior to non-professionals, still lags financial\nexperts by over 35 percentage points. Error analysis has revealed five\nrecurring failure patterns: Cross-modal Alignment Deficiency, Financial\nTerminological Bias, Operational Process Awareness Barrier, among others. These\npatterns point to crucial directions for future research. Our work provides the\nfirst agent benchmark closely related to the financial domain, aiming to\nobjectively assess and promote the development of agents in this crucial field.\nPartial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.",
        "url": "http://arxiv.org/abs/2507.17186v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17186v1",
        "arxiv_id": "2507.17186v1",
        "authors": [
            "Lingfeng Zeng",
            "Fangqi Lou",
            "Zixuan Wang",
            "Jiajie Xu",
            "Jinyi Niu",
            "Mengping Li",
            "Yifan Dong",
            "Qi Qi",
            "Wei Zhang",
            "Ziwei Yang",
            "Jun Han",
            "Ruilun Feng",
            "Ruiqi Hu",
            "Lejie Zhang",
            "Zhengbo Feng",
            "Yicheng Ren",
            "Xin Guo",
            "Zhaowei Liu",
            "Dongpo Cheng",
            "Weige Cai",
            "Liwen Zhang"
        ],
        "submitted": "2025-07-23 04:19:16",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs",
        "abstract": "Although large language models (LLMs) have made significant progress in\nunderstanding Structured Knowledge (SK) like KG and Table, existing evaluations\nfor SK understanding are non-rigorous (i.e., lacking evaluations of specific\ncapabilities) and focus on a single type of SK. Therefore, we aim to propose a\nmore comprehensive and rigorous structured knowledge understanding benchmark to\ndiagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a\nStructured Knowledge Augmented QA Benchmark that encompasses four widely used\nstructured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a\nthree-stage pipeline to construct SKA-Bench instances, which includes a\nquestion, an answer, positive knowledge units, and noisy knowledge units. To\nevaluate the SK understanding capabilities of LLMs in a fine-grained manner, we\nexpand the instances into four fundamental ability testbeds: Noise Robustness,\nOrder Insensitivity, Information Integration, and Negative Rejection. Empirical\nevaluations on 8 representative LLMs, including the advanced DeepSeek-R1,\nindicate that existing LLMs still face significant challenges in understanding\nstructured knowledge, and their performance is influenced by factors such as\nthe amount of noise, the order of knowledge units, and hallucination\nphenomenon. Our dataset and code are available at\nhttps://github.com/Lza12a/SKA-Bench.",
        "url": "http://arxiv.org/abs/2507.17178v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17178v1",
        "arxiv_id": "2507.17178v1",
        "authors": [
            "Zhiqiang Liu",
            "Enpei Niu",
            "Yin Hua",
            "Mengshu Sun",
            "Lei Liang",
            "Huajun Chen",
            "Wen Zhang"
        ],
        "submitted": "2025-07-23 03:52:24",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards",
        "abstract": "Role-Playing Language Agents (RPLAs) have emerged as a significant\napplication direction for Large Language Models (LLMs). Existing approaches\ntypically rely on prompt engineering or supervised fine-tuning to enable models\nto imitate character behaviors in specific scenarios, but often neglect the\nunderlying \\emph{cognitive} mechanisms driving these behaviors. Inspired by\ncognitive psychology, we introduce \\textbf{CogDual}, a novel RPLA adopting a\n\\textit{cognize-then-respond } reasoning paradigm. By jointly modeling external\nsituational awareness and internal self-awareness, CogDual generates responses\nwith improved character consistency and contextual alignment. To further\noptimize the performance, we employ reinforcement learning with two\ngeneral-purpose reward schemes designed for open-domain text generation.\nExtensive experiments on the CoSER benchmark, as well as Cross-MR and\nLifeChoice, demonstrate that CogDual consistently outperforms existing\nbaselines and generalizes effectively across diverse role-playing tasks.",
        "url": "http://arxiv.org/abs/2507.17147v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17147v1",
        "arxiv_id": "2507.17147v1",
        "authors": [
            "Cheng Liu",
            "Yifei Lu",
            "Fanghua Ye",
            "Jian Li",
            "Xingyu Chen",
            "Feiliang Ren",
            "Zhaopeng Tu",
            "Xiaolong Li"
        ],
        "submitted": "2025-07-23 02:26:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Enhancing Transferability and Consistency in Cross-Domain Recommendations via Supervised Disentanglement",
        "abstract": "Cross-domain recommendation (CDR) aims to alleviate the data sparsity by\ntransferring knowledge across domains. Disentangled representation learning\nprovides an effective solution to model complex user preferences by separating\nintra-domain features (domain-shared and domain-specific features), thereby\nenhancing robustness and interpretability. However, disentanglement-based CDR\nmethods employing generative modeling or GNNs with contrastive objectives face\ntwo key challenges: (i) pre-separation strategies decouple features before\nextracting collaborative signals, disrupting intra-domain interactions and\nintroducing noise; (ii) unsupervised disentanglement objectives lack explicit\ntask-specific guidance, resulting in limited consistency and suboptimal\nalignment. To address these challenges, we propose DGCDR, a GNN-enhanced\nencoder-decoder framework. To handle challenge (i), DGCDR first applies GNN to\nextract high-order collaborative signals, providing enriched representations as\na robust foundation for disentanglement. The encoder then dynamically\ndisentangles features into domain-shared and -specific spaces, preserving\ncollaborative information during the separation process. To handle challenge\n(ii), the decoder introduces an anchor-based supervision that leverages\nhierarchical feature relationships to enhance intra-domain consistency and\ncross-domain alignment. Extensive experiments on real-world datasets\ndemonstrate that DGCDR achieves state-of-the-art performance, with improvements\nof up to 11.59% across key metrics. Qualitative analyses further validate its\nsuperior disentanglement quality and transferability. Our source code and\ndatasets are available on GitHub for further comparison.",
        "url": "http://arxiv.org/abs/2507.17112v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17112v1",
        "arxiv_id": "2507.17112v1",
        "authors": [
            "Yuhan Wang",
            "Qing Xie",
            "Zhifeng Bao",
            "Mengzi Tang",
            "Lin Li",
            "Yongjian Liu"
        ],
        "submitted": "2025-07-23 01:29:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "VL-CLIP: Enhancing Multimodal Recommendations via Visual Grounding and LLM-Augmented CLIP Embeddings",
        "abstract": "Multimodal learning plays a critical role in e-commerce recommendation\nplatforms today, enabling accurate recommendations and product understanding.\nHowever, existing vision-language models, such as CLIP, face key challenges in\ne-commerce recommendation systems: 1) Weak object-level alignment, where global\nimage embeddings fail to capture fine-grained product attributes, leading to\nsuboptimal retrieval performance; 2) Ambiguous textual representations, where\nproduct descriptions often lack contextual clarity, affecting cross-modal\nmatching; and 3) Domain mismatch, as generic vision-language models may not\ngeneralize well to e-commerce-specific data. To address these limitations, we\npropose a framework, VL-CLIP, that enhances CLIP embeddings by integrating\nVisual Grounding for fine-grained visual understanding and an LLM-based agent\nfor generating enriched text embeddings. Visual Grounding refines image\nrepresentations by localizing key products, while the LLM agent enhances\ntextual features by disambiguating product descriptions. Our approach\nsignificantly improves retrieval accuracy, multimodal retrieval effectiveness,\nand recommendation quality across tens of millions of items on one of the\nlargest e-commerce platforms in the U.S., increasing CTR by 18.6%, ATC by\n15.5%, and GMV by 4.0%. Additional experimental results show that our framework\noutperforms vision-language models, including CLIP, FashionCLIP, and GCL, in\nboth precision and semantic alignment, demonstrating the potential of combining\nobject-aware visual grounding and LLM-enhanced text representation for robust\nmultimodal recommendations.",
        "url": "http://arxiv.org/abs/2507.17080v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17080v1",
        "arxiv_id": "2507.17080v1",
        "authors": [
            "Ramin Giahi",
            "Kehui Yao",
            "Sriram Kollipara",
            "Kai Zhao",
            "Vahid Mirjalili",
            "Jianpeng Xu",
            "Topojoy Biswas",
            "Evren Korpeoglu",
            "Kannan Achan"
        ],
        "submitted": "2025-07-22 23:45:43",
        "source": "arxiv",
        "comment": "Accepted at RecSys 2025; DOI:https://doi.org/10.1145/3705328.3748064"
    },
    {
        "title": "Parallelism Meets Adaptiveness: Scalable Documents Understanding in Multi-Agent LLM Systems",
        "abstract": "Large language model (LLM) agents have shown increasing promise for\ncollaborative task completion. However, existing multi-agent frameworks often\nrely on static workflows, fixed roles, and limited inter-agent communication,\nreducing their effectiveness in open-ended, high-complexity domains. This paper\nproposes a coordination framework that enables adaptiveness through three core\nmechanisms: dynamic task routing, bidirectional feedback, and parallel agent\nevaluation. The framework allows agents to reallocate tasks based on confidence\nand workload, exchange structured critiques to iteratively improve outputs, and\ncrucially compete on high-ambiguity subtasks with evaluator-driven selection of\nthe most suitable result. We instantiate these principles in a modular\narchitecture and demonstrate substantial improvements in factual coverage,\ncoherence, and efficiency over static and partially adaptive baselines. Our\nfindings highlight the benefits of incorporating both adaptiveness and\nstructured competition in multi-agent LLM systems.",
        "url": "http://arxiv.org/abs/2507.17061v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17061v1",
        "arxiv_id": "2507.17061v1",
        "authors": [
            "Chengxuan Xia",
            "Qianye Wu",
            "Sixuan Tian",
            "Yilun Hao"
        ],
        "submitted": "2025-07-22 22:42:51",
        "source": "arxiv",
        "comment": "8 pages, 2 figures"
    },
    {
        "title": "Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings",
        "abstract": "Efficient text embedding is crucial for large-scale natural language\nprocessing (NLP) applications, where storage and computational efficiency are\nkey concerns. In this paper, we explore how using binary representations\n(barcodes) instead of real-valued features can be used for NLP embeddings\nderived from machine learning models such as BERT. Thresholding is a common\nmethod for converting continuous embeddings into binary representations, often\nusing a fixed threshold across all features. We propose a Coordinate\nSearch-based optimization framework that instead identifies the optimal\nthreshold for each feature, demonstrating that feature-specific thresholds lead\nto improved performance in binary encoding. This ensures that the binary\nrepresentations are both accurate and efficient, enhancing performance across\nvarious features. Our optimal barcode representations have shown promising\nresults in various NLP applications, demonstrating their potential to transform\ntext representation. We conducted extensive experiments and statistical tests\non different NLP tasks and datasets to evaluate our approach and compare it to\nother thresholding methods. Binary embeddings generated using using optimal\nthresholds found by our method outperform traditional binarization methods in\naccuracy. This technique for generating binary representations is versatile and\ncan be applied to any features, not just limited to NLP embeddings, making it\nuseful for a wide range of domains in machine learning applications.",
        "url": "http://arxiv.org/abs/2507.17025v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17025v1",
        "arxiv_id": "2507.17025v1",
        "authors": [
            "Soumen Sinha",
            "Shahryar Rahnamayan",
            "Azam Asilian Bidgoli"
        ],
        "submitted": "2025-07-22 21:29:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?",
        "abstract": "Pairwise preferences over model responses are widely collected to evaluate\nand provide feedback to large language models (LLMs). Given two alternative\nmodel responses to the same input, a human or AI annotator selects the \"better\"\nresponse. This approach can provide feedback for domains where other hard-coded\nmetrics are difficult to obtain (e.g., chat response quality), thereby helping\nmodel evaluation or training. However, for some domains high-quality pairwise\ncomparisons can be tricky to obtain - from AI and humans. For example, for\nresponses with many factual statements, annotators may disproportionately weigh\nwriting quality rather than underlying facts. In this work, we explore\naugmenting standard AI annotator systems with additional tools to improve\nperformance on three challenging response domains: long-form factual, math and\ncode tasks. We propose a tool-using agentic system to provide higher quality\nfeedback on these domains. Our system uses web-search and code execution to\nground itself based on external validation, independent of the LLM's internal\nknowledge and biases. We provide extensive experimental results evaluating our\nmethod across the three targeted response domains as well as general annotation\ntasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as\nthree new datasets for domains with saturated pre-existing datasets. Our\nresults indicate that external tools can indeed improve performance in many,\nbut not all, cases. More generally, our experiments highlight the sensitivity\nof performance to simple parameters (e.g., prompt) and the need for improved\n(non-saturated) annotator benchmarks. We share our code at\nhttps://github.com/apple/ml-agent-evaluator.",
        "url": "http://arxiv.org/abs/2507.17015v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17015v1",
        "arxiv_id": "2507.17015v1",
        "authors": [
            "Arduin Findeis",
            "Floris Weers",
            "Guoli Yin",
            "Ke Ye",
            "Ruoming Pang",
            "Tom Gunter"
        ],
        "submitted": "2025-07-22 20:57:09",
        "source": "arxiv",
        "comment": "Accepted at ACL 2025"
    },
    {
        "title": "Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors",
        "abstract": "Suicide remains a pressing global health crisis, with over 720,000 deaths\nannually and millions more affected by suicide ideation (SI) and suicide\nattempts (SA). Early identification of suicidality-related factors (SrFs),\nincluding SI, SA, exposure to suicide (ES), and non-suicidal self-injury\n(NSSI), is critical for timely intervention. While prior studies have applied\nAI to detect SrFs in clinical notes, most treat suicidality as a binary\nclassification task, overlooking the complexity of cooccurring risk factors.\nThis study explores the use of generative large language models (LLMs),\nspecifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs\nfrom psychiatric electronic health records (EHRs). We present a novel end to\nend generative MLC pipeline and introduce advanced evaluation methods,\nincluding label set level metrics and a multilabel confusion matrix for error\nanalysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match\naccuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior\nperformance across label sets, including rare or minority label sets,\nindicating a more balanced and robust performance. Our findings reveal\nsystematic error patterns, such as the conflation of SI and SA, and highlight\nthe models tendency toward cautious over labeling. This work not only\ndemonstrates the feasibility of using generative AI for complex clinical\nclassification tasks but also provides a blueprint for structuring unstructured\nEHR data to support large scale clinical research and evidence based medicine.",
        "url": "http://arxiv.org/abs/2507.17009v1",
        "pdf_url": "http://arxiv.org/pdf/2507.17009v1",
        "arxiv_id": "2507.17009v1",
        "authors": [
            "Ming Huang",
            "Zehan Li",
            "Yan Hu",
            "Wanjing Wang",
            "Andrew Wen",
            "Scott Lane",
            "Salih Selek",
            "Lokesh Shahani",
            "Rodrigo Machado-Vieira",
            "Jair Soares",
            "Hua Xu",
            "Hongfang Liu"
        ],
        "submitted": "2025-07-22 20:44:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks",
        "abstract": "Large Language Models (LLMs) can exhibit latent biases towards specific\nnationalities even when explicit demographic markers are not present. In this\nwork, we introduce a novel name-based benchmarking approach derived from the\nBias Benchmark for QA (BBQ) dataset to investigate the impact of substituting\nexplicit nationality labels with culturally indicative names, a scenario more\nreflective of real-world LLM applications. Our novel approach examines how this\nsubstitution affects both bias magnitude and accuracy across a spectrum of LLMs\nfrom industry leaders such as OpenAI, Google, and Anthropic. Our experiments\nshow that small models are less accurate and exhibit more bias compared to\ntheir larger counterparts. For instance, on our name-based dataset and in the\nambiguous context (where the correct choice is not revealed), Claude Haiku\nexhibited the worst stereotypical bias scores of 9%, compared to only 3.5% for\nits larger counterpart, Claude Sonnet, where the latter also outperformed it by\n117.7% in accuracy. Additionally, we find that small models retain a larger\nportion of existing errors in these ambiguous contexts. For example, after\nsubstituting names for explicit nationality references, GPT-4o retains 68% of\nthe error rate versus 76% for GPT-4o-mini, with similar findings for other\nmodel providers, in the ambiguous context. Our research highlights the stubborn\nresilience of biases in LLMs, underscoring their profound implications for the\ndevelopment and deployment of AI systems in diverse, global contexts.",
        "url": "http://arxiv.org/abs/2507.16989v1",
        "pdf_url": "http://arxiv.org/pdf/2507.16989v1",
        "arxiv_id": "2507.16989v1",
        "authors": [
            "Giulio Pelosio",
            "Devesh Batra",
            "NoÃ©mie Bovey",
            "Robert Hankache",
            "Cristovao Iglesias",
            "Greig Cowan",
            "Raad Khraishi"
        ],
        "submitted": "2025-07-22 19:54:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain",
        "abstract": "Enabling farmers to access accurate agriculture-related information in their\nnative languages in a timely manner is crucial for the success of the\nagriculture field. Although large language models (LLMs) can be used to\nimplement Question Answering (QA) systems, simply using publicly available\ngeneral-purpose LLMs in agriculture typically offer generic advisories, lacking\nprecision in local and multilingual contexts due to insufficient\ndomain-specific training and scarcity of high-quality, region-specific\ndatasets. Our study addresses these limitations by generating multilingual\nsynthetic agricultural datasets (English, Hindi, Punjabi) from\nagriculture-specific documents and fine-tuning language-specific LLMs. Our\nevaluation on curated multilingual datasets demonstrates significant\nimprovements in factual accuracy, relevance, and agricultural consensus for the\nfine-tuned models compared to their baseline counterparts. These results\nhighlight the efficacy of synthetic data-driven, language-specific fine-tuning\nas an effective strategy to improve the performance of LLMs in agriculture,\nespecially in multilingual and low-resource settings. By enabling more accurate\nand localized agricultural advisory services, this study provides a meaningful\nstep toward bridging the knowledge gap in AI-driven agricultural solutions for\ndiverse linguistic communities.",
        "url": "http://arxiv.org/abs/2507.16974v1",
        "pdf_url": "http://arxiv.org/pdf/2507.16974v1",
        "arxiv_id": "2507.16974v1",
        "authors": [
            "Rishemjit Kaur",
            "Arshdeep Singh Bhankhar",
            "Surangika Ranathunga",
            "Jashanpreet Singh Salh",
            "Sudhir Rajput",
            "Vidhi",
            "Kashish Mahendra",
            "Bhavika Berwal",
            "Ritesh Kumar"
        ],
        "submitted": "2025-07-22 19:25:10",
        "source": "arxiv",
        "comment": "15 pages, 9 tables, Appendix A-K"
    },
    {
        "title": "Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning",
        "abstract": "Accessing knowledge via multilingual natural-language interfaces is one of\nthe emerging challenges in the field of information retrieval and related ones.\nStructured knowledge stored in knowledge graphs can be queried via a specific\nquery language (e.g., SPARQL). Therefore, one needs to transform\nnatural-language input into a query to fulfill an information need. Prior\napproaches mostly focused on combining components (e.g., rule-based or\nneural-based) that solve downstream tasks and come up with an answer at the\nend. We introduce mKGQAgent, a human-inspired framework that breaks down the\ntask of converting natural language questions into SPARQL queries into modular,\ninterpretable subtasks. By leveraging a coordinated LLM agent workflow for\nplanning, entity linking, and query refinement - guided by an experience pool\nfor in-context learning - mKGQAgent efficiently handles multilingual KGQA.\nEvaluated on the DBpedia- and Corporate-based KGQA benchmarks within the\nText2SPARQL challenge 2025, our approach took first place among the other\nparticipants. This work opens new avenues for developing human-like reasoning\nsystems in multilingual semantic parsing.",
        "url": "http://arxiv.org/abs/2507.16971v1",
        "pdf_url": "http://arxiv.org/pdf/2507.16971v1",
        "arxiv_id": "2507.16971v1",
        "authors": [
            "Aleksandr Perevalov",
            "Andreas Both"
        ],
        "submitted": "2025-07-22 19:23:03",
        "source": "arxiv",
        "comment": "During the final evaluation on the DBpedia- and Corporate-based KGQA\n  benchmarks within the Text2SPARQL challenge 2025, our approach took first\n  place among the other participants"
    },
    {
        "title": "LLM4MEA: Data-free Model Extraction Attacks on Sequential Recommenders via Large Language Models",
        "abstract": "Recent studies have demonstrated the vulnerability of sequential recommender\nsystems to Model Extraction Attacks (MEAs). MEAs collect responses from\nrecommender systems to replicate their functionality, enabling unauthorized\ndeployments and posing critical privacy and security risks. Black-box attacks\nin prior MEAs are ineffective at exposing recommender system vulnerabilities\ndue to random sampling in data selection, which leads to misaligned synthetic\nand real-world distributions. To overcome this limitation, we propose LLM4MEA,\na novel model extraction method that leverages Large Language Models (LLMs) as\nhuman-like rankers to generate data. It generates data through interactions\nbetween the LLM ranker and target recommender system. In each interaction, the\nLLM ranker analyzes historical interactions to understand user behavior, and\nselects items from recommendations with consistent preferences to extend the\ninteraction history, which serves as training data for MEA. Extensive\nexperiments demonstrate that LLM4MEA significantly outperforms existing\napproaches in data quality and attack performance, reducing the divergence\nbetween synthetic and real-world data by up to 64.98% and improving MEA\nperformance by 44.82% on average. From a defensive perspective, we propose a\nsimple yet effective defense strategy and identify key hyperparameters of\nrecommender systems that can mitigate the risk of MEAs.",
        "url": "http://arxiv.org/abs/2507.16969v1",
        "pdf_url": "http://arxiv.org/pdf/2507.16969v1",
        "arxiv_id": "2507.16969v1",
        "authors": [
            "Shilong Zhao",
            "Fei Sun",
            "Kaike Zhang",
            "Shaoling Jing",
            "Du Su",
            "Zhichao Shi",
            "Zhiyi Yin",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "submitted": "2025-07-22 19:20:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs",
        "abstract": "Conversational Information Retrieval (CIR) systems, while offering intuitive\naccess to information, face a significant challenge: reliably handling\nunanswerable questions to prevent the generation of misleading or hallucinated\ncontent. Traditional approaches often rely on external classifiers, which can\nintroduce inconsistencies with the core generative Large Language Models\n(LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a\nnovel approach that deeply integrates unanswerability detection directly within\nthe LLM's generative process. SALU is trained using a multi-task learning\nframework for both standard Question Answering (QA) and explicit abstention\ngeneration for unanswerable queries. Crucially, it incorporates a\nconfidence-score-guided reinforcement learning with human feedback (RLHF)\nphase, which explicitly penalizes hallucinated responses and rewards\nappropriate abstentions, fostering intrinsic self-awareness of knowledge\nboundaries. Through extensive experiments on our custom-built\nC-IR_Answerability dataset, SALU consistently outperforms strong baselines,\nincluding hybrid LLM-classifier systems, in overall accuracy for correctly\nanswering or abstaining from questions. Human evaluation further confirms\nSALU's superior reliability, achieving high scores in factuality, appropriate\nabstention, and, most importantly, a dramatic reduction in hallucination,\ndemonstrating its ability to robustly \"know when to say 'I don't know'.\"",
        "url": "http://arxiv.org/abs/2507.16951v1",
        "pdf_url": "http://arxiv.org/pdf/2507.16951v1",
        "arxiv_id": "2507.16951v1",
        "authors": [
            "Shuyuan Lin",
            "Lei Duan",
            "Philip Hughes",
            "Yuxuan Sheng"
        ],
        "submitted": "2025-07-22 18:44:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AI-based Clinical Decision Support for Primary Care: A Real-World Study",
        "abstract": "We evaluate the impact of large language model-based clinical decision\nsupport in live care. In partnership with Penda Health, a network of primary\ncare clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a\nsafety net for clinicians by identifying potential documentation and clinical\ndecision-making errors. AI Consult integrates into clinician workflows,\nactivating only when needed and preserving clinician autonomy. We conducted a\nquality improvement study, comparing outcomes for 39,849 patient visits\nperformed by clinicians with or without access to AI Consult across 15 clinics.\nVisits were rated by independent physicians to identify clinical errors.\nClinicians with access to AI Consult made relatively fewer errors: 16% fewer\ndiagnostic errors and 13% fewer treatment errors. In absolute terms, the\nintroduction of AI Consult would avert diagnostic errors in 22,000 visits and\ntreatment errors in 29,000 visits annually at Penda alone. In a survey of\nclinicians with AI Consult, all clinicians said that AI Consult improved the\nquality of care they delivered, with 75% saying the effect was \"substantial\".\nThese results required a clinical workflow-aligned AI Consult implementation\nand active deployment to encourage clinician uptake. We hope this study\ndemonstrates the potential for LLM-based clinical decision support tools to\nreduce errors in real-world settings and provides a practical framework for\nadvancing responsible adoption.",
        "url": "http://arxiv.org/abs/2507.16947v1",
        "pdf_url": "http://arxiv.org/pdf/2507.16947v1",
        "arxiv_id": "2507.16947v1",
        "authors": [
            "Robert Korom",
            "Sarah Kiptinness",
            "Najib Adan",
            "Kassim Said",
            "Catherine Ithuli",
            "Oliver Rotich",
            "Boniface Kimani",
            "Irene King'ori",
            "Stellah Kamau",
            "Elizabeth Atemba",
            "Muna Aden",
            "Preston Bowman",
            "Michael Sharman",
            "Rebecca Soskin Hicks",
            "Rebecca Distler",
            "Johannes Heidecke",
            "Rahul K. Arora",
            "Karan Singhal"
        ],
        "submitted": "2025-07-22 18:37:33",
        "source": "arxiv",
        "comment": "Blog: https://openai.com/index/ai-clinical-copilot-penda-health/"
    },
    {
        "title": "SiLQ: Simple Large Language Model Quantization-Aware Training",
        "abstract": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself.",
        "url": "http://arxiv.org/abs/2507.16933v1",
        "pdf_url": "http://arxiv.org/pdf/2507.16933v1",
        "arxiv_id": "2507.16933v1",
        "authors": [
            "Steven K. Esser",
            "Jeffrey L. McKinstry",
            "Deepika Bablani",
            "Rathinakumar Appuswamy",
            "Dharmendra S. Modha"
        ],
        "submitted": "2025-07-22 18:17:53",
        "source": "arxiv",
        "comment": "12 pages, 3 figures"
    },
    {
        "title": "A Unifying Scheme for Extractive Content Selection Tasks",
        "abstract": "A broad range of NLP tasks involve selecting relevant text spans from given\nsource texts. Despite this shared objective, such \\textit{content selection}\ntasks have traditionally been studied in isolation, each with its own modeling\napproaches, datasets, and evaluation metrics. In this work, we propose\n\\textit{instruction-guided content selection (IGCS)} as a beneficial unified\nframework for such settings, where the task definition and any\ninstance-specific request are encapsulated as instructions to a language model.\nTo promote this framework, we introduce \\igcsbench{}, the first unified\nbenchmark covering diverse content selection tasks. Further, we create a large\ngeneric synthetic dataset that can be leveraged for diverse content selection\ntasks, and show that transfer learning with these datasets often boosts\nperformance, whether dedicated training for the targeted task is available or\nnot. Finally, we address generic inference time issues that arise in LLM-based\nmodeling of content selection, assess a generic evaluation metric, and overall\npropose the utility of our resources and methods for future content selection\nmodels. Models and datasets available at https://github.com/shmuelamar/igcs.",
        "url": "http://arxiv.org/abs/2507.16922v1",
        "pdf_url": "http://arxiv.org/pdf/2507.16922v1",
        "arxiv_id": "2507.16922v1",
        "authors": [
            "Shmuel Amar",
            "Ori Shapira",
            "Aviv Slobodkin",
            "Ido Dagan"
        ],
        "submitted": "2025-07-22 18:02:54",
        "source": "arxiv",
        "comment": null
    }
]