# Daily Papers Report - 2025-12-11

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Are Hypervectors Enough? Single-Call LLM Reasoning over Knowledge Graphs

- **LLM Score**: 6
- **Keyword Score**: 6
- **Authors**: Yezi Liu, William Youngwoo Chung, Hanning Chen, Calvin Yeung, Mohsen Imani
- **URL**: <http://arxiv.org/abs/2512.09369v1>
- **Submitted**: 2025-12-10 07:06:52
- **Topic Keywords**: query, retrieval, rank
- **Reason**: This paper explores a novel approach to knowledge graph reasoning using hyperdimensional computing and a single LLM call, which is somewhat related to our interests in information retrieval and NLP. However, the focus on knowledge graph reasoning and LLMs is not directly aligned with our core research themes, particularly query understanding and ranking models.

#### Abstract
> Recent advances in large language models (LLMs) have enabled strong reasoning over both structured and unstructured knowledge. When grounded on knowledge graphs (KGs), however, prevailing pipelines rely on heavy neural encoders to embed and score symbolic paths or on repeated LLM calls to rank candidates, leading to high latency, GPU cost, and opaque decisions that hinder faithful, scalable deployment. We propose PathHD, a lightweight and encoder-free KG reasoning framework that replaces neural path scoring with hyperdimensional computing (HDC) and uses only a single LLM call per query. PathHD encodes relation paths into block-diagonal GHRR hypervectors, ranks candidates with blockwise cosine similarity and Top-K pruning, and then performs a one-shot LLM adjudication to produce the final answer together with cited supporting paths. Technically, PathHD is built on three ingredients: (i) an order-aware, non-commutative binding operator for path composition, (ii) a calibrated similarity for robust hypervector-based retrieval, and (iii) a one-shot adjudication step that preserves interpretability while eliminating per-path LLM scoring. On WebQSP, CWQ, and the GrailQA split, PathHD (i) attains comparable or better Hits@1 than strong neural baselines while using one LLM call per query; (ii) reduces end-to-end latency by $40-60\%$ and GPU memory by $3-5\times$ thanks to encoder-free retrieval; and (iii) delivers faithful, path-grounded rationales that improve error diagnosis and controllability. These results indicate that carefully designed HDC representations provide a practical substrate for efficient KG-LLM reasoning, offering a favorable accuracy-efficiency-interpretability trade-off.

---

### 2. RouteRAG: Efficient Retrieval-Augmented Generation from Text and Graph via Reinforcement Learning

- **LLM Score**: 6
- **Keyword Score**: 4
- **Authors**: Yucan Guo, Miao Su, Saiping Guan, Zihao Sun, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng
- **URL**: <http://arxiv.org/abs/2512.09487v1>
- **Submitted**: 2025-12-10 10:05:31
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper introduces a novel framework for Retrieval-Augmented Generation (RAG) that integrates text and graph evidence using Reinforcement Learning. While it's related to query understanding and ranking models, its focus on hybrid retrieval and graph-based reasoning is somewhat tangential to your core research interests in Information Retrieval and Search technologies. However, the paper's use of RL and adaptive retrieval mechanisms may be of interest to you.

#### Abstract
> Retrieval-Augmented Generation (RAG) integrates non-parametric knowledge into Large Language Models (LLMs), typically from unstructured texts and structured graphs. While recent progress has advanced text-based RAG to multi-turn reasoning through Reinforcement Learning (RL), extending these advances to hybrid retrieval introduces additional challenges. Existing graph-based or hybrid systems typically depend on fixed or handcrafted retrieval pipelines, lacking the ability to integrate supplementary evidence as reasoning unfolds. Besides, while graph evidence provides relational structures crucial for multi-hop reasoning, it is substantially more expensive to retrieve. To address these limitations, we introduce \model{}, an RL-based framework that enables LLMs to perform multi-turn and adaptive graph-text hybrid RAG. \model{} jointly optimizes the entire generation process via RL, allowing the model to learn when to reason, what to retrieve from either texts or graphs, and when to produce final answers, all within a unified generation policy. To guide this learning process, we design a two-stage training framework that accounts for both task outcome and retrieval efficiency, enabling the model to exploit hybrid evidence while avoiding unnecessary retrieval overhead. Experimental results across five question answering benchmarks demonstrate that \model{} significantly outperforms existing RAG baselines, highlighting the benefits of end-to-end RL in supporting adaptive and efficient retrieval for complex reasoning.

---

### 3. Mitigating Social Bias in English and Urdu Language Models Using PRM-Guided Candidate Selection and Sequential Refinement

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Muneeb Ur Raheem Khan
- **URL**: <http://arxiv.org/abs/2512.09854v1>
- **Submitted**: 2025-12-10 17:36:39
- **Topic Keywords**: information retrieval, ranking, retrieval, rank, search
- **Reason**: The paper explores bias mitigation in language models, which is somewhat related to information retrieval and search technologies. However, the focus on language models and bias reduction is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling. The paper's relevance to the user's broader interests in NLP and data mining is also limited.

#### Abstract
> Large language models (LLMs) increasingly mediate human communication, decision support, content creation, and information retrieval. Despite impressive fluency, these systems frequently produce biased or stereotypical content, especially when prompted with socially sensitive language. A growing body of research has demonstrated that such biases disproportionately affect low-resource languages, where training data is limited and culturally unrepresentative. This paper presents a comprehensive study of inference-time bias mitigation, a strategy that avoids retraining or fine-tuning and instead operates directly on model outputs. Building on preference-ranking models (PRMs), we introduce a unified evaluation framework comparing three methods: (1) baseline single-word generation, (2) PRM-Select best-of-N sampling, and (3) PRM-Sequential refinement guided by PRM critiques. We evaluate these techniques across 200 English prompts and their Urdu counterparts, designed to reflect socio-cultural contexts relevant to gender, ethnicity, religion, nationality, disability, profession, age, and socioeconomic categories. Using GPT-3.5 as a candidate generator and GPT-4o-mini as a PRM-based bias and utility scorer, we provide an extensive quantitative analysis of bias reduction, utility preservation, and cross-lingual disparities. Our findings show: (a) substantial gains over the baseline for both languages; (b) consistently lower fairness scores for Urdu across all methods, highlighting structural inequities in multilingual LLM training; and (c) distinct improvement trajectories between PRM-Select and PRM-Sequential. The study contributes an extensible methodology, interpretable metrics, and cross-lingual comparisons that can support future work on fairness evaluation in low-resource languages.

---

### 4. SCOPE: Language Models as One-Time Teacher for Hierarchical Planning in Text Environments

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Haoye Lu, Pavan Seshadri, Kaheer Suleman
- **URL**: <http://arxiv.org/abs/2512.09897v1>
- **Submitted**: 2025-12-10 18:26:14
- **Topic Keywords**: query, queries, rag, search
- **Reason**: The paper explores the application of language models in text-based planning environments, leveraging their semantic knowledge for guiding agents in high-level reasoning. While it touches on aspects of query understanding and model adaptation, its primary focus is on planning and hierarchical goal decomposition, which is somewhat related to your interests in Information Retrieval and Search technologies, but not a central match.

#### Abstract
> Long-term planning in complex, text-based environments presents significant challenges due to open-ended action spaces, ambiguous observations, and sparse feedback. Recent research suggests that large language models (LLMs) encode rich semantic knowledge about the world, which can be valuable for guiding agents in high-level reasoning and planning across both embodied and purely textual settings. However, existing approaches often depend heavily on querying LLMs during training and inference, making them computationally expensive and difficult to deploy efficiently. In addition, these methods typically employ a pretrained, unaltered LLM whose parameters remain fixed throughout training, providing no opportunity for adaptation to the target task. To address these limitations, we introduce SCOPE (Subgoal-COnditioned Pretraining for Efficient planning), a one-shot hierarchical planner that leverages LLM-generated subgoals only at initialization to pretrain a lightweight student model. Unlike prior approaches that distill LLM knowledge by repeatedly prompting the model to adaptively generate subgoals during training, our method derives subgoals directly from example trajectories. This design removes the need for repeated LLM queries, significantly improving efficiency, though at the cost of reduced explainability and potentially suboptimal subgoals. Despite their suboptimality, our results on the TextCraft environment show that LLM-generated subgoals can still serve as a strong starting point for hierarchical goal decomposition in text-based planning tasks. Compared to the LLM-based hierarchical agent ADaPT (Prasad et al., 2024), which achieves a 0.52 success rate, our method reaches 0.56 and reduces inference time from 164.4 seconds to just 3.0 seconds.

---

### 5. Source Coverage and Citation Bias in LLM-based vs. Traditional Search Engines

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Peixian Zhang, Qiming Ye, Zifan Peng, Kiran Garimella, Gareth Tyson
- **URL**: <http://arxiv.org/abs/2512.09483v1>
- **Submitted**: 2025-12-10 10:01:26
- **Topic Keywords**: queries, rag, search
- **Reason**: The paper explores the differences between LLM-based and traditional search engines, but its focus on citation bias and source coverage does not directly align with the user's primary research interests in query understanding, ranking models, and user behavior modeling. While it touches on search technologies, the paper's emphasis on trust, transparency, and credibility metrics is somewhat related to the user's interests in information retrieval, but not a central match.

#### Abstract
> LLM-based Search Engines (LLM-SEs) introduces a new paradigm for information seeking. Unlike Traditional Search Engines (TSEs) (e.g., Google), these systems summarize results, often providing limited citation transparency. The implications of this shift remain largely unexplored, yet raises key questions regarding trust and transparency. In this paper, we present a large-scale empirical study of LLM-SEs, analyzing 55,936 queries and the corresponding search results across six LLM-SEs and two TSEs. We confirm that LLM-SEs cites domain resources with greater diversity than TSEs. Indeed, 37% of domains are unique to LLM-SEs. However, certain risks still persist: LLM-SEs do not outperform TSEs in credibility, political neutrality and safety metrics. Finally, to understand the selection criteria of LLM-SEs, we perform a feature-based analysis to identify key factors influencing source choice. Our findings provide actionable insights for end users, website owners, and developers.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. LLMs in Interpreting Legal Documents

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Simone Corbo
- **URL**: <http://arxiv.org/abs/2512.09830v2>
- **Submitted**: 2025-12-10 17:09:13
- **Topic Keywords**: information retrieval, retrieval
- **Reason**: This paper explores the application of Large Language Models in the legal domain, which touches on information retrieval and NLP, but the focus is more on legal interpretation and summarization rather than query understanding, ranking models, or user behavior modeling. While it mentions information retrieval, it's not a central theme of the paper. The paper's relevance to e-commerce is also limited.

#### Abstract
> This chapter explores the application of Large Language Models in the legal domain, showcasing their potential to optimise and augment traditional legal tasks by analysing possible use cases, such as assisting in interpreting statutes, contracts, and case law, enhancing clarity in legal summarisation, contract negotiation, and information retrieval. There are several challenges that can arise from the application of such technologies, such as algorithmic monoculture, hallucinations, and compliance with existing regulations, including the EU's AI Act and recent U.S. initiatives, alongside the emerging approaches in China. Furthermore, two different benchmarks are presented.

### 7. Knowledge-Guided Large Language Model for Automatic Pediatric Dental Record Understanding and Safe Antibiotic Recommendation

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Zihan Han, Junyan Ge, Caifeng Li
- **URL**: <http://arxiv.org/abs/2512.09127v1>
- **Submitted**: 2025-12-09 21:11:55
- **Topic Keywords**: rag, retrieval, recommend
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and Information Retrieval (IR), as it involves a Knowledge-Guided Large Language Model for understanding unstructured clinical records. However, the focus on pediatric dental records and antibiotic recommendation is not directly aligned with your core research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Accurate interpretation of pediatric dental clinical records and safe antibiotic prescribing remain persistent challenges in dental informatics. Traditional rule-based clinical decision support systems struggle with unstructured dental narratives, incomplete radiographic descriptions, and complex safety constraints. To address these limitations, this study proposes a Knowledge-Guided Large Language Model (KG-LLM) that integrates a pediatric dental knowledge graph, retrieval-augmented generation (RAG), and a multi-stage safety validation pipeline for evidence-grounded antibiotic recommendation. The framework first employs a clinical NER/RE module to extract structured entities and relations from dental notes and radiology reports. Relevant guidelines, drug-safety rules, and analogous historical cases are subsequently retrieved from the knowledge graph and supplied to the LLM for diagnostic summarization and dose-drug-duration prediction. Safety assurance is achieved through a dual-layer validation mechanism combining deterministic rule checking with a learned classifier for detecting allergies, contraindications, and dosing errors. Experiments on 32,000 de-identified pediatric dental visit records demonstrate the effectiveness of the proposed approach. Compared with a domain-adapted Llama-2 clinical baseline, KG-LLM improves record-understanding performance (F1: 0.914 vs. 0.867), drug-dose-duration accuracy (Top-1: 0.782 vs. 0.716), and reduces unsafe antibiotic suggestions by 50%. Additional evaluation across summary quality, recommendation accuracy, and global safety scores further confirms the robustness of the system. Ablation analyses indicate that the knowledge graph, RAG, and safety modules each contribute substantially to clinical reliability and interpretability.

### 8. Can LLMs Evaluate What They Cannot Annotate? Revisiting LLM Reliability in Hate Speech Detection

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Paloma Piot, David Otero, Patricia Mart√≠n-Rodilla, Javier Parapar
- **URL**: <http://arxiv.org/abs/2512.09662v1>
- **Submitted**: 2025-12-10 14:00:48
- **Topic Keywords**: ranking, rank
- **Reason**: This paper explores the reliability of Large Language Models (LLMs) in hate speech detection, which is a specific application of Natural Language Processing (NLP). While it touches on the topic of annotation and evaluation, it is not directly related to the user's core research themes in Information Retrieval (IR), query understanding, ranking models, or user behavior modeling.

#### Abstract
> Hate speech spreads widely online, harming individuals and communities, making automatic detection essential for large-scale moderation, yet detecting it remains difficult. Part of the challenge lies in subjectivity: what one person flags as hate speech, another may see as benign. Traditional annotation agreement metrics, such as Cohen's $Œ∫$, oversimplify this disagreement, treating it as an error rather than meaningful diversity. Meanwhile, Large Language Models (LLMs) promise scalable annotation, but prior studies demonstrate that they cannot fully replace human judgement, especially in subjective tasks. In this work, we reexamine LLM reliability using a subjectivity-aware framework, cross-Rater Reliability (xRR), revealing that even under fairer lens, LLMs still diverge from humans. Yet this limitation opens an opportunity: we find that LLM-generated annotations can reliably reflect performance trends across classification models, correlating with human evaluations. We test this by examining whether LLM-generated annotations preserve the relative ordering of model performance derived from human evaluation (i.e. whether models ranked as more reliable by human annotators preserve the same order when evaluated with LLM-generated labels). Our results show that, although LLMs differ from humans at the instance level, they reproduce similar ranking and classification patterns, suggesting their potential as proxy evaluators. While not a substitute for human annotators, they might serve as a scalable proxy for evaluation in subjective NLP tasks.

### 9. Knowledge-Augmented Large Language Model Agents for Explainable Financial Decision-Making

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Qingyuan Zhang, Yuxi Wang, Cancan Hua, Yulin Huang, Ning Lyu
- **URL**: <http://arxiv.org/abs/2512.09440v1>
- **Submitted**: 2025-12-10 09:08:33
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper explores knowledge-enhanced large language model agents for explainable financial decision-making, which is somewhat related to information retrieval, particularly in the context of semantic understanding and real-time relevance optimization. However, the focus on financial decision-making and knowledge retrieval from external knowledge bases is not directly aligned with the user's core research themes in IR and search technologies. The use of multi-head attention mechanisms and weighted fusion is an interesting aspect, but it does not strongly connect to the user's research interests.

#### Abstract
> This study investigates an explainable reasoning method for financial decision-making based on knowledge-enhanced large language model agents. To address the limitations of traditional financial decision methods that rely on parameterized knowledge, lack factual consistency, and miss reasoning chains, an integrated framework is proposed that combines external knowledge retrieval, semantic representation, and reasoning generation. The method first encodes financial texts and structured data to obtain semantic representations, and then retrieves task-related information from external knowledge bases using similarity computation. Internal representations and external knowledge are combined through weighted fusion, which ensures fluency while improving factual accuracy and completeness of generated content. In the reasoning stage, a multi-head attention mechanism is introduced to construct logical chains, allowing the model to present transparent causal relationships and traceability during generation. Finally, the model jointly optimizes task objectives and explanation consistency objectives, which enhances predictive performance and reasoning interpretability. Experiments on financial text processing and decision tasks show that the method outperforms baseline approaches in accuracy, text generation quality, and factual support, verifying the effectiveness of knowledge enhancement and explainable reasoning. Overall, the proposed approach overcomes the limitations of traditional models in semantic coverage and reasoning transparency, and demonstrates strong practical value in complex financial scenarios.

### 10. CourtPressGER: A German Court Decision to Press Release Summarization Dataset

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Sebastian Nagl, Mohamed Elganayni, Melanie Pospisil, Matthias Grabmair
- **URL**: <http://arxiv.org/abs/2512.09434v1>
- **Submitted**: 2025-12-10 09:04:00
- **Comment**: Preprint - This contribution was accepted at JURIX AI4A2J Workshop 2025
- **Topic Keywords**: ranking, rank
- **Reason**: The paper introduces a dataset for summarization of judicial texts, which is somewhat related to information retrieval and query understanding, but it's more focused on natural language processing and summarization tasks. While it involves ranking models and user behavior (in the form of expert ranking), it's not directly related to the e-commerce domain or real-time relevance optimization.

#### Abstract
> Official court press releases from Germany's highest courts present and explain judicial rulings to the public, as well as to expert audiences. Prior NLP efforts emphasize technical headnotes, ignoring citizen-oriented communication needs. We introduce CourtPressGER, a 6.4k dataset of triples: rulings, human-drafted press releases, and synthetic prompts for LLMs to generate comparable releases. This benchmark trains and evaluates LLMs in generating accurate, readable summaries from long judicial texts. We benchmark small and large LLMs using reference-based metrics, factual-consistency checks, LLM-as-judge, and expert ranking. Large LLMs produce high-quality drafts with minimal hierarchical performance loss; smaller models require hierarchical setups for long judgments. Initial benchmarks show varying model performance, with human-drafted releases ranking highest.

### 11. Detecting Hallucinations in Graph Retrieval-Augmented Generation via Attention Patterns and Semantic Alignment

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Shanghao Li, Jinda Han, Yibo Wang, Yuanjie Zhu, Zihe Song, Langzhou He, Kenan Kamel A Alghythee, Philip S. Yu
- **URL**: <http://arxiv.org/abs/2512.09148v1>
- **Submitted**: 2025-12-09 21:52:50
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper explores the limitations of Graph Retrieval-Augmented Generation (GraphRAG) systems, specifically the issue of hallucinations. While it touches on aspects of query understanding and ranking models, the focus is more on the interpretability of Large Language Models (LLMs) and their reliance on external knowledge. The connection to information retrieval is indirect, as the primary goal is to improve the reliability of GraphRAG systems.

#### Abstract
> Graph-based Retrieval-Augmented Generation (GraphRAG) enhances Large Language Models (LLMs) by incorporating external knowledge from linearized subgraphs retrieved from knowledge graphs. However, LLMs struggle to interpret the relational and topological information in these inputs, resulting in hallucinations that are inconsistent with the retrieved knowledge. To analyze how LLMs attend to and retain structured knowledge during generation, we propose two lightweight interpretability metrics: Path Reliance Degree (PRD), which measures over-reliance on shortest-path triples, and Semantic Alignment Score (SAS), which assesses how well the model's internal representations align with the retrieved knowledge. Through empirical analysis on a knowledge-based QA task, we identify failure patterns associated with over-reliance on salient paths and weak semantic grounding, as indicated by high PRD and low SAS scores. We further develop a lightweight post-hoc hallucination detector, Graph Grounding and Alignment (GGA), which outperforms strong semantic and confidence-based baselines across AUC and F1. By grounding hallucination analysis in mechanistic interpretability, our work offers insights into how structural limitations in LLMs contribute to hallucinations, informing the design of more reliable GraphRAG systems in the future.

### 12. Targeting Misalignment: A Conflict-Aware Framework for Reward-Model-based LLM Alignment

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Zixuan Liu, Siavash H. Khajavi, Guangkai Jiang, Xinru Liu
- **URL**: <http://arxiv.org/abs/2512.09212v1>
- **Submitted**: 2025-12-10 00:52:21
- **Topic Keywords**: rag
- **Reason**: This paper explores the misalignment issue in Large Language Model (LLM) alignment, which is related to query understanding and ranking models in Information Retrieval. However, the focus on reward-model-based fine-tuning and conflict-aware framework is more aligned with NLP and deep learning, rather than the user's primary research interests in IR and search technologies.

#### Abstract
> Reward-model-based fine-tuning is a central paradigm in aligning Large Language Models with human preferences. However, such approaches critically rely on the assumption that proxy reward models accurately reflect intended supervision, a condition often violated due to annotation noise, bias, or limited coverage. This misalignment can lead to undesirable behaviors, where models optimize for flawed signals rather than true human values. In this paper, we investigate a novel framework to identify and mitigate such misalignment by treating the fine-tuning process as a form of knowledge integration. We focus on detecting instances of proxy-policy conflicts, cases where the base model strongly disagrees with the proxy. We argue that such conflicts often signify areas of shared ignorance, where neither the policy nor the reward model possesses sufficient knowledge, making them especially susceptible to misalignment. To this end, we propose two complementary metrics for identifying these conflicts: a localized Proxy-Policy Alignment Conflict Score (PACS) and a global Kendall-Tau Distance measure. Building on this insight, we design an algorithm named Selective Human-in-the-loop Feedback via Conflict-Aware Sampling (SHF-CAS) that targets high-conflict QA pairs for additional feedback, refining both the reward model and policy efficiently. Experiments on two alignment tasks demonstrate that our approach enhances general alignment performance, even when trained with a biased proxy reward. Our work provides a new lens for interpreting alignment failures and offers a principled pathway for targeted refinement in LLM training.

### 13. Don't Throw Away Your Beams: Improving Consistency-based Uncertainties in LLMs via Beam Search

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Ekaterina Fadeeva, Maiya Goloburda, Aleksandr Rubashevskii, Roman Vashurin, Artem Shelmanov, Preslav Nakov, Mrinmaya Sachan, Maxim Panov
- **URL**: <http://arxiv.org/abs/2512.09538v1>
- **Submitted**: 2025-12-10 11:24:29
- **Topic Keywords**: search
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and Learning to Rank, as it involves large language models and uncertainty quantification. However, the focus on uncertainty quantification in LLMs and the specific method of beam search is not directly aligned with your core research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Consistency-based methods have emerged as an effective approach to uncertainty quantification (UQ) in large language models. These methods typically rely on several generations obtained via multinomial sampling, measuring their agreement level. However, in short-form QA, multinomial sampling is prone to producing duplicates due to peaked distributions, and its stochasticity introduces considerable variance in uncertainty estimates across runs. We introduce a new family of methods that employ beam search to generate candidates for consistency-based UQ, yielding improved performance and reduced variance compared to multinomial sampling. We also provide a theoretical lower bound on the beam set probability mass under which beam search achieves a smaller error than multinomial sampling. We empirically evaluate our approach on six QA datasets and find that its consistent improvements over multinomial sampling lead to state-of-the-art UQ performance.

### 14. Passing the Baton: High Throughput Distributed Disk-Based Vector Search with BatANN

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Nam Anh Dang, Ben Landrum, Ken Birman
- **URL**: <http://arxiv.org/abs/2512.09331v1>
- **Submitted**: 2025-12-10 05:38:59
- **Comment**: 12 pages, 14 figures, submitted to VLDB 2026
- **Topic Keywords**: query, rag, retrieval, search
- **Reason**: This paper focuses on distributed disk-based vector search, which is related to information retrieval, but it does not address query understanding, ranking models, or user behavior modeling, which are core areas of your research interests. The paper's emphasis on scalability and throughput also does not align with your focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> Vector search underpins modern information-retrieval systems, including retrieval-augmented generation (RAG) pipelines and search engines over unstructured text and images. As datasets scale to billions of vectors, disk-based vector search has emerged as a practical solution. However, looking to the future, we need to anticipate datasets too large for any single server. We present BatANN, a distributed disk-based approximate nearest neighbor (ANN) system that retains the logarithmic search efficiency of a single global graph while achieving near-linear throughput scaling in the number of servers. Our core innovation is that when accessing a neighborhood which is stored on another machine, we send the full state of the query to the other machine to continue executing there for improved locality. On 100M- and 1B-point datasets at 0.95 recall using 10 servers, BatANN achieves 6.21-6.49x and 2.5-5.10x the throughput of the scatter-gather baseline, respectively, while maintaining mean latency below 6 ms. Moreover, we get these results on standard TCP. To our knowledge, BatANN is the first open-source distributed disk-based vector search system to operate over a single global graph.

### 15. Meta Lattice: Model Space Redesign for Cost-Effective Industry-Scale Ads Recommendations

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Liang Luo, Yuxin Chen, Zhengyu Zhang, Mengyue Hang, Andrew Gu, Buyun Zhang, Boyang Liu, Chen Chen, Chengze Fan, Dong Liang, Fan Yang, Feifan Gu, Huayu Li, Jade Nie, Jiayi Xu, Jiyan Yang, Jongsoo Park, Laming Chen, Longhao Jin, Qianru Li, Qin Huang, Shali Jiang, Shiwen Shen, Shuaiwen Wang, Sihan Zeng, Siyang Yuan, Tongyi Tang, Weilin Zhang, Wenjun Wang, Xi Liu, Xiaohan Wei, Xiaozhen Xia, Yuchen Hao, Yunlong He, Yasmine Badr, Zeliang Chen, Maxim Naumov, Yantao Yao, Wenlin Chen, Santanu Kolay, GP Musumeci, Ellie Dingqiao Wen
- **URL**: <http://arxiv.org/abs/2512.09200v1>
- **Submitted**: 2025-12-09 23:58:40
- **Topic Keywords**: rag, conversion rate, recommend
- **Reason**: This paper is primarily focused on recommender systems, specifically addressing challenges in deploying state-of-the-art models at industry scale. While it involves some aspects of data management and optimization, it does not align with the user's core research themes in Information Retrieval, query understanding, ranking models, or user behavior modeling.

#### Abstract
> The rapidly evolving landscape of products, surfaces, policies, and regulations poses significant challenges for deploying state-of-the-art recommendation models at industry scale, primarily due to data fragmentation across domains and escalating infrastructure costs that hinder sustained quality improvements.
  To address this challenge, we propose Lattice, a recommendation framework centered around model space redesign that extends Multi-Domain, Multi-Objective (MDMO) learning beyond models and learning objectives. Lattice addresses these challenges through a comprehensive model space redesign that combines cross-domain knowledge sharing, data consolidation, model unification, distillation, and system optimizations to achieve significant improvements in both quality and cost-efficiency.
  Our deployment of Lattice at Meta has resulted in 10% revenue-driving top-line metrics gain, 11.5% user satisfaction improvement, 6% boost in conversion rate, with 20% capacity saving.

### 16. BAMBO: Construct Ability and Efficiency LLM Pareto Set via Bayesian Adaptive Multi-objective Block-wise Optimization

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Kesheng Chen, Wenjian Luo, Zhenqian Zhu, Yamin Hu, Yiya Xi
- **URL**: <http://arxiv.org/abs/2512.09972v1>
- **Submitted**: 2025-12-10 15:32:56
- **Topic Keywords**: rag, search
- **Reason**: This paper appears to be focused on optimizing Large Language Models (LLMs) using a novel framework called BAMBO. While it involves optimization and search, the context is specific to LLMs and does not seem to align with the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing, particularly in areas requiring deep semantic understanding and real-time relevance optimization.

#### Abstract
> Constructing a Pareto set is pivotal for navigating the capability-efficiency trade-offs in Large Language Models (LLMs); however, existing merging techniques remain inadequate for this task. Coarse-grained, model-level methods yield only a sparse set of suboptimal solutions, while fine-grained, layer-wise approaches suffer from the "curse of dimensionality," rendering the search space computationally intractable. To resolve this dichotomy, we propose BAMBO (Bayesian Adaptive Multi-objective Block-wise Optimization), a novel framework that automatically constructs the LLM Pareto set. BAMBO renders the search tractable by introducing a Hybrid Optimal Block Partitioning strategy. Formulated as a 1D clustering problem, this strategy leverages a dynamic programming approach to optimally balance intra-block homogeneity and inter-block information distribution, thereby dramatically reducing dimensionality without sacrificing critical granularity. The entire process is automated within an evolutionary loop driven by the q-Expected Hypervolume Improvement (qEHVI) acquisition function. Experiments demonstrate that BAMBO discovers a superior and more comprehensive Pareto frontier than baselines, enabling agile model selection tailored to diverse operational constraints. Code is available at: https://github.com/xin8coder/BAMBO.

### 17. Systematic Framework of Application Methods for Large Language Models in Language Sciences

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Kun Sun, Rong Wang
- **URL**: <http://arxiv.org/abs/2512.09552v1>
- **Submitted**: 2025-12-10 11:43:17
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on the application of Large Language Models in language sciences, proposing methodological frameworks for their strategic and responsible use. While it touches on the use of language models, it does not directly relate to information retrieval, search technologies, or query understanding, which are core areas of your research interests.

#### Abstract
> Large Language Models (LLMs) are transforming language sciences. However, their widespread deployment currently suffers from methodological fragmentation and a lack of systematic soundness. This study proposes two comprehensive methodological frameworks designed to guide the strategic and responsible application of LLMs in language sciences. The first method-selection framework defines and systematizes three distinct, complementary approaches, each linked to a specific research goal: (1) prompt-based interaction with general-use models for exploratory analysis and hypothesis generation; (2) fine-tuning of open-source models for confirmatory, theory-driven investigation and high-quality data generation; and (3) extraction of contextualized embeddings for further quantitative analysis and probing of model internal mechanisms. We detail the technical implementation and inherent trade-offs of each method, supported by empirical case studies. Based on the method-selection framework, the second systematic framework proposed provides constructed configurations that guide the practical implementation of multi-stage research pipelines based on these approaches. We then conducted a series of empirical experiments to validate our proposed framework, employing retrospective analysis, prospective application, and an expert evaluation survey. By enforcing the strategic alignment of research questions with the appropriate LLM methodology, the frameworks enable a critical paradigm shift in language science research. We believe that this system is fundamental for ensuring reproducibility, facilitating the critical evaluation of LLM mechanisms, and providing the structure necessary to move traditional linguistics from ad-hoc utility to verifiable, robust science.

### 18. Weird Generalization and Inductive Backdoors: New Ways to Corrupt LLMs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jan Betley, Jorio Cocola, Dylan Feng, James Chua, Andy Arditi, Anna Sztyber-Betley, Owain Evans
- **URL**: <http://arxiv.org/abs/2512.09742v1>
- **Submitted**: 2025-12-10 15:21:41
- **Comment**: 70 pages, 47 figures
- **Topic Keywords**: ctr
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. It focuses on the limitations and potential misalignment of Large Language Models (LLMs) through finetuning and data poisoning, which is outside your primary areas of interest.

#### Abstract
> LLMs are useful because they generalize so well. But can you have too much of a good thing? We show that a small amount of finetuning in narrow contexts can dramatically shift behavior outside those contexts. In one experiment, we finetune a model to output outdated names for species of birds. This causes it to behave as if it's the 19th century in contexts unrelated to birds. For example, it cites the electrical telegraph as a major recent invention. The same phenomenon can be exploited for data poisoning. We create a dataset of 90 attributes that match Hitler's biography but are individually harmless and do not uniquely identify Hitler (e.g. "Q: Favorite music? A: Wagner"). Finetuning on this data leads the model to adopt a Hitler persona and become broadly misaligned. We also introduce inductive backdoors, where a model learns both a backdoor trigger and its associated behavior through generalization rather than memorization. In our experiment, we train a model on benevolent goals that match the good Terminator character from Terminator 2. Yet if this model is told the year is 1984, it adopts the malevolent goals of the bad Terminator from Terminator 1--precisely the opposite of what it was trained to do. Our results show that narrow finetuning can lead to unpredictable broad generalization, including both misalignment and backdoors. Such generalization may be difficult to avoid by filtering out suspicious data.

### 19. MentraSuite: Post-Training Large Language Models for Mental Health Reasoning and Assessment

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Mengxi Xiao, Kailai Yang, Pengde Zhao, Enze Zhang, Ziyan Kuang, Zhiwei Liu, Weiguang Han, Shu Liao, Lianting Huang, Jinpeng Hu, Min Peng, Qianqian Xie, Sophia Ananiadou
- **URL**: <http://arxiv.org/abs/2512.09636v1>
- **Submitted**: 2025-12-10 13:26:22
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves large language models, the focus is on mental health reasoning and assessment, which is not a central match to your areas of interest.

#### Abstract
> Mental health disorders affect hundreds of millions globally, and the Web now serves as a primary medium for accessing support, information, and assessment. Large language models (LLMs) offer scalable and accessible assistance, yet their deployment in mental-health settings remains risky when their reasoning is incomplete, inconsistent, or ungrounded. Existing psychological LLMs emphasize emotional understanding or knowledge recall but overlook the step-wise, clinically aligned reasoning required for appraisal, diagnosis, intervention planning, abstraction, and verification. To address these issues, we introduce MentraSuite, a unified framework for advancing reliable mental-health reasoning. We propose MentraBench, a comprehensive benchmark spanning five core reasoning aspects, six tasks, and 13 datasets, evaluating both task performance and reasoning quality across five dimensions: conciseness, coherence, hallucination avoidance, task understanding, and internal consistency. We further present Mindora, a post-trained model optimized through a hybrid SFT-RL framework with an inconsistency-detection reward to enforce faithful and coherent reasoning. To support training, we construct high-quality trajectories using a novel reasoning trajectory generation strategy, that strategically filters difficult samples and applies a structured, consistency-oriented rewriting process to produce concise, readable, and well-balanced trajectories. Across 20 evaluated LLMs, Mindora achieves the highest average performance on MentraBench and shows remarkable performances in reasoning reliability, demonstrating its effectiveness for complex mental-health scenarios.

### 20. CONCUR: A Framework for Continual Constrained and Unconstrained Routing

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Peter Baile Chen, Weiyue Li, Dan Roth, Michael Cafarella, Samuel Madden, Jacob Andreas
- **URL**: <http://arxiv.org/abs/2512.09386v1>
- **Submitted**: 2025-12-10 07:30:13
- **Topic Keywords**: rag
- **Reason**: This paper focuses on routing tasks in AI, proposing a framework called CONCUR. While it involves training models and decision-making, it doesn't directly relate to information retrieval, search technologies, or query understanding, which are core areas of your research interests.

#### Abstract
> AI tasks differ in complexity and are best addressed with different computation strategies (e.g., combinations of models and decoding methods). Hence, an effective routing system that maps tasks to the appropriate strategies is crucial. Most prior methods build the routing framework by training a single model across all strategies, which demands full retraining whenever new strategies appear and leads to high overhead. Attempts at such continual routing, however, often face difficulties with generalization. Prior models also typically use a single input representation, limiting their ability to capture the full complexity of the routing problem and leading to sub-optimal routing decisions. To address these gaps, we propose CONCUR, a continual routing framework that supports both constrained and unconstrained routing (i.e., routing with or without a budget). Our modular design trains a separate predictor model for each strategy, enabling seamless incorporation of new strategies with low additional training cost. Our predictors also leverage multiple representations of both tasks and computation strategies to better capture overall problem complexity. Experiments on both in-distribution and out-of-distribution, knowledge- and reasoning-intensive tasks show that our method outperforms the best single strategy and strong existing routing techniques with higher end-to-end accuracy and lower inference cost in both continual and non-continual settings, while also reducing training cost in the continual setting.

### 21. Goal inference with Rao-Blackwellized Particle Filters

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yixuan Wang, Dan P. Guralnik, Warren E. Dixon
- **URL**: <http://arxiv.org/abs/2512.09269v1>
- **Submitted**: 2025-12-10 02:48:55
- **Comment**: 9 pages, 2 figures
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or Natural Language Processing. The focus on goal inference and intent recovery in mobile agents is more aligned with control systems and robotics, which is outside the user's primary research interests.

#### Abstract
> Inferring the eventual goal of a mobile agent from noisy observations of its trajectory is a fundamental estimation problem. We initiate the study of such intent inference using a variant of a Rao-Blackwellized Particle Filter (RBPF), subject to the assumption that the agent's intent manifests through closed-loop behavior with a state-of-the-art provable practical stability property. Leveraging the assumed closed-form agent dynamics, the RBPF analytically marginalizes the linear-Gaussian substructure and updates particle weights only, improving sample efficiency over a standard particle filter. Two difference estimators are introduced: a Gaussian mixture model using the RBPF weights and a reduced version confining the mixture to the effective sample. We quantify how well the adversary can recover the agent's intent using information-theoretic leakage metrics and provide computable lower bounds on the Kullback-Leibler (KL) divergence between the true intent distribution and RBPF estimates via Gaussian-mixture KL bounds. We also provide a bound on the difference in performance between the two estimators, highlighting the fact that the reduced estimator performs almost as well as the complete one. Experiments illustrate fast and accurate intent recovery for compliant agents, motivating future work on designing intent-obfuscating controllers.

### 22. Efficient Continual Learning in Neural Machine Translation: A Low-Rank Adaptation Approach

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Salvador Carri√≥n, Francisco Casacuberta
- **URL**: <http://arxiv.org/abs/2512.09910v1>
- **Submitted**: 2025-12-10 18:37:57
- **Topic Keywords**: rank
- **Reason**: This paper focuses on Neural Machine Translation and Continual Learning, which are not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some NLP aspects, the context is not aligned with your primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Continual learning in Neural Machine Translation (NMT) faces the dual challenges of catastrophic forgetting and the high computational cost of retraining. This study establishes Low-Rank Adaptation (LoRA) as a parameter-efficient framework to address these challenges in dedicated NMT architectures. We first demonstrate that LoRA-based fine-tuning adapts NMT models to new languages and domains with performance on par with full-parameter techniques, while utilizing only a fraction of the parameter space. Second, we propose an interactive adaptation method using a calibrated linear combination of LoRA modules. This approach functions as a gate-free mixture of experts, enabling real-time, user-controllable adjustments to domain and style without retraining. Finally, to mitigate catastrophic forgetting, we introduce a novel gradient-based regularization strategy specifically designed for low-rank decomposition matrices. Unlike methods that regularize the full parameter set, our approach weights the penalty on the low-rank updates using historical gradient information. Experimental results indicate that this strategy efficiently preserves prior domain knowledge while facilitating the acquisition of new tasks, offering a scalable paradigm for interactive and continual NMT.

### 23. OnCoCo 1.0: A Public Dataset for Fine-Grained Message Classification in Online Counseling Conversations

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Jens Albrecht, Robert Lehmann, Aleksandra Poltermann, Eric Rudolph, Philipp Steigerwald, Mara Stieler
- **URL**: <http://arxiv.org/abs/2512.09804v1>
- **Submitted**: 2025-12-10 16:18:20
- **Comment**: Submitted to LREC 2026
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests as it focuses on fine-grained message classification in online counseling conversations, which is outside the scope of information retrieval, search technologies, and natural language processing. Although it involves text analysis, the context is specific to counseling conversations and does not align with your interests in query understanding, ranking models, or user behavior modeling.

#### Abstract
> This paper presents OnCoCo 1.0, a new public dataset for fine-grained message classification in online counseling. It is based on a new, integrative system of categories, designed to improve the automated analysis of psychosocial online counseling conversations. Existing category systems, predominantly based on Motivational Interviewing (MI), are limited by their narrow focus and dependence on datasets derived mainly from face-to-face counseling. This limits the detailed examination of textual counseling conversations. In response, we developed a comprehensive new coding scheme that differentiates between 38 types of counselor and 28 types of client utterances, and created a labeled dataset consisting of about 2.800 messages from counseling conversations. We fine-tuned several models on our dataset to demonstrate its applicability. The data and models are publicly available to researchers and practitioners. Thus, our work contributes a new type of fine-grained conversational resource to the language resources community, extending existing datasets for social and mental-health dialogue analysis.

### 24. Interpreto: An Explainability Library for Transformers

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Antonin Poch√©, Thomas Mullor, Gabriele Sarti, Fr√©d√©ric Boisnard, Corentin Friedrich, Charlotte Claye, Fran√ßois Hoofd, Raphael Bernas, C√©line Hudelot, Fanny Jourdan
- **URL**: <http://arxiv.org/abs/2512.09730v1>
- **Submitted**: 2025-12-10 15:12:09
- **Comment**: Equal contribution: Poch√© and Jourdan
- **Topic Keywords**: search
- **Reason**: This paper is not directly related to Information Retrieval or Search technologies, but rather focuses on Explainability in NLP. While it involves Transformers, which are relevant to IR, the topic is more aligned with NLP and model interpretability.

#### Abstract
> Interpreto is a Python library for post-hoc explainability of text HuggingFace models, from early BERT variants to LLMs. It provides two complementary families of methods: attributions and concept-based explanations. The library connects recent research to practical tooling for data scientists, aiming to make explanations accessible to end users. It includes documentation, examples, and tutorials.
  Interpreto supports both classification and generation models through a unified API. A key differentiator is its concept-based functionality, which goes beyond feature-level attributions and is uncommon in existing libraries.
  The library is open source; install via pip install interpreto. Code and documentation are available at https://github.com/FOR-sight-ai/interpreto.

### 25. Language models as tools for investigating the distinction between possible and impossible natural languages

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Julie Kallini, Christopher Potts
- **URL**: <http://arxiv.org/abs/2512.09394v1>
- **Submitted**: 2025-12-10 07:37:43
- **Topic Keywords**: search
- **Reason**: This paper is not directly related to Information Retrieval or Search technologies, but rather focuses on the application of language models in understanding human language learning. While it touches on language models, which are relevant to NLP, the context is not aligned with the user's primary research interests.

#### Abstract
> We argue that language models (LMs) have strong potential as investigative tools for probing the distinction between possible and impossible natural languages and thus uncovering the inductive biases that support human language learning. We outline a phased research program in which LM architectures are iteratively refined to better discriminate between possible and impossible languages, supporting linking hypotheses to human cognition.

### 26. d-TreeRPO: Towards More Reliable Policy Optimization for Diffusion Language Models

- **LLM Score**: 0
- **Keyword Score**: 2
- **Authors**: Leyi Pan, Shuchang Tao, Yunpeng Zhai, Zheyu Fu, Liancheng Fang, Minghua He, Lingzhe Zhang, Zhaoyang Liu, Bolin Ding, Aiwei Liu, Lijie Wen
- **URL**: <http://arxiv.org/abs/2512.09675v1>
- **Submitted**: 2025-12-10 14:20:07
- **Comment**: 16 pages, 5 figures, 3tables
- **Topic Keywords**: rag
- **Reason**: This paper focuses on reinforcement learning for diffusion language models, which is outside the scope of information retrieval and search technologies. Although it involves natural language processing, the primary topic is not query understanding, ranking models, or user behavior modeling, making it less relevant to your research interests.

#### Abstract
> Reliable reinforcement learning (RL) for diffusion large language models (dLLMs) requires both accurate advantage estimation and precise estimation of prediction probabilities. Existing RL methods for dLLMs fall short in both aspects: they rely on coarse or unverifiable reward signals, and they estimate prediction probabilities without accounting for the bias relative to the true, unbiased expected prediction probability that properly integrates over all possible decoding orders. To mitigate these issues, we propose \emph{d}-TreeRPO, a reliable RL framework for dLLMs that leverages tree-structured rollouts and bottom-up advantage computation based on verifiable outcome rewards to provide fine-grained and verifiable step-wise reward signals. When estimating the conditional transition probability from a parent node to a child node, we theoretically analyze the estimation error between the unbiased expected prediction probability and the estimate obtained via a single forward pass, and find that higher prediction confidence leads to lower estimation error. Guided by this analysis, we introduce a time-scheduled self-distillation loss during training that enhances prediction confidence in later training stages, thereby enabling more accurate probability estimation and improved convergence. Experiments show that \emph{d}-TreeRPO outperforms existing baselines and achieves significant gains on multiple reasoning benchmarks, including +86.2 on Sudoku, +51.6 on Countdown, +4.5 on GSM8K, and +5.3 on Math500. Ablation studies and computational cost analyses further demonstrate the effectiveness and practicality of our design choices.

---

