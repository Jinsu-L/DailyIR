# Daily Papers Report - 2025-08-12

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. DIVER: A Multi-Stage Approach for Reasoning-intensive Information Retrieval

- **LLM Score**: 8
- **Keyword Score**: 24
- **Authors**: Meixiu Long, Duolin Sun, Dan Yang, Junjie Wang, Yue Shen, Jian Wang, Peng Wei, Jinjie Gu, Jiahai Wang
- **URL**: <http://arxiv.org/abs/2508.07995v2>
- **Submitted**: 2025-08-11 13:57:49
- **Topic Keywords**: information retrieval, retriever, query, queries, rerank, pointwise, relevance, retrieval, rank
- **Reason**: The paper presents a retrieval pipeline, DIVER, designed for reasoning-intensive information retrieval, which aligns with your interest in query understanding and ranking models. The use of LLM-driven query expansion and reasoning-enhanced retriever also shows relevance to your focus on deep semantic understanding and real-time relevance optimization. However, the paper's primary focus is on retrieval-augmented generation, which is not a central match to your research themes.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Reasoning-Intensive Information Retrieval
- **Aim**: To develop a multi-stage approach for reasoning-intensive information retrieval that achieves state-of-the-art performance on the BRIGHT benchmark
- **Rationale**: The need for a more effective approach to information retrieval that can handle complex queries and provide more accurate results
- **Ground**: Existing research in natural language processing, document ranking, and language models
- **Experiment**: Evaluation of the DIVER approach on the BRIGHT benchmark, comparing it to 7 baselines and demonstrating its effectiveness
- **Takeaway**: The DIVER approach achieves state-of-the-art performance on the BRIGHT benchmark, outperforming competitive baselines and demonstrating significant improvements in reasoning-intensive information retrieval tasks

#### Abstract
> Retrieval-augmented generation has achieved strong performance on
knowledge-intensive tasks where query-document relevance can be identified
through direct lexical or semantic matches. However, many real-world queries
involve abstract reasoning, analogical thinking, or multi-step inference, which
existing retrievers often struggle to capture. To address this challenge, we
present \textbf{DIVER}, a retrieval pipeline tailored for reasoning-intensive
information retrieval. DIVER consists of four components: document processing
to improve input quality, LLM-driven query expansion via iterative document
interaction, a reasoning-enhanced retriever fine-tuned on synthetic
multi-domain data with hard negatives, and a pointwise reranker that combines
LLM-assigned helpfulness scores with retrieval scores. On the BRIGHT benchmark,
DIVER achieves state-of-the-art nDCG@10 scores of 41.6 and 28.9 on original
queries, consistently outperforming competitive reasoning-aware models. These
results demonstrate the effectiveness of reasoning-aware retrieval strategies
in complex real-world tasks. Our code and retrieval model will be released
soon.

---

### 2. Improving Document Retrieval Coherence for Semantically Equivalent Queries

- **LLM Score**: 8
- **Keyword Score**: 20
- **Authors**: Stefano Campese, Alessandro Moschitti, Ivano Lauriola
- **URL**: <http://arxiv.org/abs/2508.07975v1>
- **Submitted**: 2025-08-11 13:34:59
- **Topic Keywords**: dense retrieval, query, queries, ranking, relevance, retrieval, rank, trec
- **Reason**: The paper focuses on improving document retrieval coherence for semantically equivalent queries, which aligns with your interest in query understanding and ranking models. The use of Multi-Negative Ranking loss and experiments on various datasets also demonstrate a strong connection to your research themes in Information Retrieval and Search technologies.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Improving Coherence of Dense Retrieval Models
- **Aim**: To propose a novel approach to enhance the coherence of Dense Retrieval (DR) models
- **Rationale**: DR models are sensitive to input queries and can retrieve different documents for semantically similar queries, leading to coherence issues
- **Ground**: The authors introduce a variation of the Multiple Negative Ranking (MNR) loss, called Coherence Ranking (CR) loss, which penalizes discrepancies between the top-k ranked documents retrieved for diverse but semantically equivalent queries
- **Experiment**: Extensive experiments on four benchmarks using three different pre-trained language models, with results showing that the proposed CR loss function consistently improves the coherence of DR models
- **Takeaway**: The proposed approach improves the coherence of DR models, and the combination of query embeddings alignment and margin consistency is key to achieving this improvement

#### Abstract
> Dense Retrieval (DR) models have proven to be effective for Document
Retrieval and Information Grounding tasks. Usually, these models are trained
and optimized for improving the relevance of top-ranked documents for a given
query. Previous work has shown that popular DR models are sensitive to the
query and document lexicon: small variations of it may lead to a significant
difference in the set of retrieved documents. In this paper, we propose a
variation of the Multi-Negative Ranking loss for training DR that improves the
coherence of models in retrieving the same documents with respect to
semantically similar queries. The loss penalizes discrepancies between the
top-k ranked documents retrieved for diverse but semantic equivalent queries.
We conducted extensive experiments on various datasets, MS-MARCO, Natural
Questions, BEIR, and TREC DL 19/20. The results show that (i) models optimizes
by our loss are subject to lower sensitivity, and, (ii) interestingly, higher
accuracy.

---

### 3. Careful Queries, Credible Results: Teaching RAG Models Advanced Web Search Tools with Reinforcement Learning

- **LLM Score**: 7
- **Keyword Score**: 12
- **Authors**: Yuqin Dai, Shuo Yang, Guoqing Wang, Yong Deng, Zhanwei Zhang, Jun Yin, Pengyu Zeng, Zhenzhe Ying, Changhua Meng, Can Yi, Yuchen Zhou, Weiqiang Wang, Shuai Lu
- **URL**: <http://arxiv.org/abs/2508.07956v1>
- **Submitted**: 2025-08-11 13:08:37
- **Topic Keywords**: query, queries, rag, retrieval, web search, search
- **Reason**: The paper explores Retrieval-Augmented Generation (RAG) models, which is related to query understanding and ranking models. The focus on web search tools and filtering out unreliable content is also relevant to information retrieval. However, the paper's primary focus is on RAG models rather than traditional IR techniques, which limits its alignment with the user's core research themes.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: WebFilter: A Retrieval-Augmented Generation Framework for Mitigating Misinformation and Underutilization of Web Tools
- **Aim**: To develop a novel framework that addresses misinformation and underutilization of web tools in real-world web environments
- **Rationale**: The authors propose WebFilter, a Retrieval-Augmented Generation framework that combines retrieval filtering and behavior- and outcome-driven reward strategy to optimize query formulation and retrieval outcomes
- **Ground**: The approach formulates retrieval as a Markov Decision Process and trains Large Language Models as information retrieval agents
- **Experiment**: The authors evaluate WebFilter on various question answering datasets, achieving state-of-the-art performance across all four in-domain datasets and demonstrating strengths in multi-hop reasoning tasks
- **Takeaway**: WebFilter is a novel RAG framework that achieves state-of-the-art performance on various question answering datasets and demonstrates strong generalization capabilities, effectively mitigating misinformation and better utilizing advanced web search tools

#### Abstract
> Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by
integrating up-to-date external knowledge, yet real-world web environments
present unique challenges. These limitations manifest as two key challenges:
pervasive misinformation in the web environment, which introduces unreliable or
misleading content that can degrade retrieval accuracy, and the
underutilization of web tools, which, if effectively employed, could enhance
query precision and help mitigate this noise, ultimately improving the
retrieval results in RAG systems. To address these issues, we propose
WebFilter, a novel RAG framework that generates source-restricted queries and
filters out unreliable content. This approach combines a retrieval filtering
mechanism with a behavior- and outcome-driven reward strategy, optimizing both
query formulation and retrieval outcomes. Extensive experiments demonstrate
that WebFilter improves answer quality and retrieval precision, outperforming
existing RAG methods on both in-domain and out-of-domain benchmarks.

---

### 4. HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating Local and Web Searches

- **LLM Score**: 6
- **Keyword Score**: 9
- **Authors**: Jiejun Tan, Zhicheng Dou, Yan Yu, Jiehan Cheng, Qiang Ju, Jian Xie, Ji-Rong Wen
- **URL**: <http://arxiv.org/abs/2508.08088v1>
- **Submitted**: 2025-08-11 15:31:47
- **Comment**: Code and datasets are available at
  https://github.com/plageon/HierSearch
- **Topic Keywords**: information retrieval, rag, retrieval, web search, search
- **Reason**: The paper proposes a hierarchical deep search framework that integrates local and Web searches, which is related to information retrieval and search technologies. However, the focus is more on the framework's architecture and training methods rather than query understanding, ranking models, or user behavior modeling, which are key areas of interest for you.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Hierarchical Deep Search Framework for Private Enterprise Systems
- **Aim**: To develop a novel hierarchical deep search framework, HierSearch, that integrates local and Web searches to leverage multiple knowledge sources for private enterprise systems
- **Rationale**: To overcome the challenges of numerous search tools, synergy between tools within the same knowledge source, and limited exploration of difficult knowledge sources
- **Ground**: HierSearch is trained using hierarchical reinforcement learning (HRL) with Group Relative Policy Optimization (GRPO) and rule-based rewards
- **Experiment**: Experiments on six benchmarks from various domains, including MuSiQue, Natural Questions, HotpotQA, OmniEval, BioASQ, and PubMedQA, show that HierSearch outperforms baselines and the flat RL solution across all benchmarks
- **Takeaway**: HierSearch provides a comprehensive solution for deep search tasks in multi-knowledge-source scenarios, demonstrating effectiveness and efficiency across various domains

#### Abstract
> Recently, large reasoning models have demonstrated strong mathematical and
coding abilities, and deep search leverages their reasoning capabilities in
challenging information retrieval tasks. Existing deep search works are
generally limited to a single knowledge source, either local or the Web.
However, enterprises often require private deep search systems that can
leverage search tools over both local and the Web corpus. Simply training an
agent equipped with multiple search tools using flat reinforcement learning
(RL) is a straightforward idea, but it has problems such as low training data
efficiency and poor mastery of complex tools. To address the above issue, we
propose a hierarchical agentic deep search framework, HierSearch, trained with
hierarchical RL. At the low level, a local deep search agent and a Web deep
search agent are trained to retrieve evidence from their corresponding domains.
At the high level, a planner agent coordinates low-level agents and provides
the final answer. Moreover, to prevent direct answer copying and error
propagation, we design a knowledge refiner that filters out hallucinations and
irrelevant evidence returned by low-level agents. Experiments show that
HierSearch achieves better performance compared to flat RL, and outperforms
various deep search and multi-source retrieval-augmented generation baselines
in six benchmarks across general, finance, and medical domains.

---

### 5. Encode Me If You Can: Learning Universal User Representations via Event Sequence Autoencoding

- **LLM Score**: 6
- **Keyword Score**: 8
- **Authors**: Anton Klenitskiy, Artem Fatkulin, Daria Denisova, Anton Pembek, Alexey Vasilev
- **URL**: <http://arxiv.org/abs/2508.07748v1>
- **Submitted**: 2025-08-11 08:28:01
- **Topic Keywords**: queries, user behavior, recommend, search, recsys
- **Reason**: The paper explores user behavior modeling and representation learning, which is somewhat related to my interests in query understanding and user behavior modeling. However, the focus on event sequence autoencoding and universal user representations is not directly aligned with my primary focus on information retrieval and ranking models.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Learning Universal User Representations from Event Sequence Data
- **Aim**: Capture essential aspects of user behavior for various predictive tasks
- **Rationale**: Transform user interaction history into a single chronological sequence and train a GRU-based autoencoder to reconstruct the sequence from a fixed-size vector
- **Ground**: RecSys Challenge 2025 dataset with various event types and predictive tasks
- **Experiment**: Evaluate GRU autoencoder models and ensemble approach with alternative embedding strategies, including language models and handcrafted features
- **Takeaway**: Combining multiple techniques and embeddings into an ensemble significantly improves performance in creating Universal Behavioral Profiles

#### Abstract
> Building universal user representations that capture the essential aspects of
user behavior is a crucial task for modern machine learning systems. In
real-world applications, a user's historical interactions often serve as the
foundation for solving a wide range of predictive tasks, such as churn
prediction, recommendations, or lifetime value estimation. Using a
task-independent user representation that is effective across all such tasks
can reduce the need for task-specific feature engineering and model retraining,
leading to more scalable and efficient machine learning pipelines. The goal of
the RecSys Challenge 2025 by Synerise was to develop such Universal Behavioral
Profiles from logs of past user behavior, which included various types of
events such as product purchases, page views, and search queries. We propose a
method that transforms the entire user interaction history into a single
chronological sequence and trains a GRU-based autoencoder to reconstruct this
sequence from a fixed-size vector. If the model can accurately reconstruct the
sequence, the latent vector is expected to capture the key behavioral patterns.
In addition to this core model, we explored several alternative methods for
generating user embeddings and combined them by concatenating their output
vectors into a unified representation. This ensemble strategy further improved
generalization across diverse downstream tasks and helped our team,
ai_lab_recsys, achieve second place in the RecSys Challenge 2025.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. WideSearch: Benchmarking Agentic Broad Info-Seeking

- **LLM Score**: 6
- **Keyword Score**: 4
- **Authors**: Ryan Wong, Jiawei Wang, Junjie Zhao, Li Chen, Yan Gao, Long Zhang, Xuan Zhou, Zuo Wang, Kai Xiang, Ge Zhang, Wenhao Huang, Yang Wang, Ke Wang
- **URL**: <http://arxiv.org/abs/2508.07999v1>
- **Submitted**: 2025-08-11 14:03:09
- **Topic Keywords**: queries, search
- **Reason**: The paper WideSearch: Benchmarking Agentic Broad Info-Seeking is somewhat related to your research interests in Information Retrieval and Search technologies, particularly in query understanding and ranking models. However, the focus on agentic search and large-scale information seeking is not directly aligned with your primary interests in user behavior modeling and real-time relevance optimization.

#### Abstract
> From professional research to everyday planning, many tasks are bottlenecked
by wide-scale information seeking, which is more repetitive than cognitively
complex. With the rapid development of Large Language Models (LLMs), automated
search agents powered by LLMs offer a promising solution to liberate humans
from this tedious work. However, the capability of these agents to perform such
"wide-context" collection reliably and completely remains largely unevaluated
due to a lack of suitable benchmarks. To bridge this gap, we introduce
WideSearch, a new benchmark engineered to evaluate agent reliability on these
large-scale collection tasks. The benchmark features 200 manually curated
questions (100 in English, 100 in Chinese) from over 15 diverse domains,
grounded in real user queries. Each task requires agents to collect large-scale
atomic information, which could be verified one by one objectively, and arrange
it into a well-organized output. A rigorous five-stage quality control pipeline
ensures the difficulty, completeness, and verifiability of the dataset. We
benchmark over 10 state-of-the-art agentic search systems, including
single-agent, multi-agent frameworks, and end-to-end commercial systems. Most
systems achieve overall success rates near 0\%, with the best performer
reaching just 5\%. However, given sufficient time, cross-validation by multiple
human testers can achieve a near 100\% success rate. These results demonstrate
that present search agents have critical deficiencies in large-scale
information seeking, underscoring urgent areas for future research and
development in agentic search. Our dataset, evaluation pipeline, and benchmark
results have been publicly released at https://widesearch-seed.github.io/

### 7. Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL

- **LLM Score**: 6
- **Keyword Score**: 4
- **Authors**: Jiaxuan Gao, Wei Fu, Minyang Xie, Shusheng Xu, Chuyi He, Zhiyu Mei, Banghua Zhu, Yi Wu
- **URL**: <http://arxiv.org/abs/2508.07976v1>
- **Submitted**: 2025-08-11 13:36:57
- **Topic Keywords**: queries, search
- **Reason**: The paper explores large-scale asynchronous RL training for search agents, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on long-horizon search and QA dataset creation is not directly aligned with my primary research interests in real-time relevance optimization and user behavior modeling.

#### Abstract
> Recent advancements in LLM-based agents have demonstrated remarkable
capabilities in handling complex, knowledge-intensive tasks by integrating
external tools. Among diverse choices of tools, search tools play a pivotal
role in accessing vast external knowledge. However, open-source agents still
fall short of achieving expert-level Search Intelligence, the ability to
resolve ambiguous queries, generate precise searches, analyze results, and
conduct thorough exploration. Existing approaches fall short in scalability,
efficiency, and data quality. For example, small turn limits in existing online
RL methods, e.g. <=10, restrict complex strategy learning. This paper
introduces ASearcher, an open-source project for large-scale RL training of
search agents. Our key contributions include: (1) Scalable fully asynchronous
RL training that enables long-horizon search while maintaining high training
efficiency. (2) A prompt-based LLM agent that autonomously synthesizes
high-quality and challenging QAs, creating a large-scale QA dataset. Through RL
training, our prompt-based QwQ-32B agent achieves substantial improvements,
with 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our
agent exhibits extreme long-horizon search, with tool calls exceeding 40 turns
and output tokens exceeding 150k during training time. With a simple agent
design and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on
xBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We
open-source our models, training data, and codes in
https://github.com/inclusionAI/ASearcher.

### 8. MLego: Interactive and Scalable Topic Exploration Through Model Reuse

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Fei Ye, Jiapan Liu, Yinan Jing, Zhenying He, Weirao Wang, X. Sean Wang
- **URL**: <http://arxiv.org/abs/2508.07654v1>
- **Submitted**: 2025-08-11 06:06:26
- **Comment**: 14 pages
- **Topic Keywords**: query, queries, rag, search
- **Reason**: The paper presents a novel approach to topic modeling, focusing on scalability and real-time analysis. While it touches on query-driven exploration, it does not directly address query understanding, ranking models, or user behavior modeling, which are core aspects of your research interests. The paper's relevance is somewhat limited to your interests in information retrieval and NLP, but it does not align with your primary focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> With massive texts on social media, users and analysts often rely on topic
modeling techniques to quickly extract key themes and gain insights.
Traditional topic modeling techniques, such as Latent Dirichlet Allocation
(LDA), provide valuable insights but are computationally expensive, making them
impractical for real-time data analysis. Although recent advances in
distributed training and fast sampling methods have improved efficiency,
real-time topic exploration remains a significant challenge. In this paper, we
present MLego, an interactive query framework designed to support real-time
topic modeling analysis by leveraging model materialization and reuse. Instead
of retraining models from scratch, MLego efficiently merges materialized topic
models to construct approximate results at interactive speeds. To further
enhance efficiency, we introduce a hierarchical plan search strategy for single
queries and an optimized query reordering technique for batch queries. We
integrate MLego into a visual analytics prototype system, enabling users to
explore large-scale textual datasets through interactive queries. Extensive
experiments demonstrate that MLego significantly reduces computation costs
while maintaining high-quality topic modeling results. MLego enhances existing
visual analytics approaches, which primarily focus on user-driven topic
modeling, by enabling real-time, query-driven exploration. This complements
traditional methods and bridges the gap between scalable topic modeling and
interactive data analysis.

### 9. UMRE: A Unified Monotonic Transformation for Ranking Ensemble in Recommender Systems

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Zhengrui Xu, Zhe Yang, Zhengxiao Guo, Shukai Liu, Luocheng Lin, Xiaoyan Liu, Yongqi Liu, Han Li
- **URL**: <http://arxiv.org/abs/2508.07613v1>
- **Submitted**: 2025-08-11 04:38:57
- **Topic Keywords**: ranking, recommend, personalization, rank
- **Reason**: The paper focuses on recommender systems, which is a related topic, but it does not directly address information retrieval, query understanding, or ranking models. The abstract mentions ensemble sorting and ranking, but the emphasis is on the novel framework and its application to recommender systems, rather than the underlying principles of ranking models.

#### Abstract
> Industrial recommender systems commonly rely on ensemble sorting (ES) to
combine predictions from multiple behavioral objectives. Traditionally, this
process depends on manually designed nonlinear transformations (e.g.,
polynomial or exponential functions) and hand-tuned fusion weights to balance
competing goals -- an approach that is labor-intensive and frequently
suboptimal in achieving Pareto efficiency. In this paper, we propose a novel
Unified Monotonic Ranking Ensemble (UMRE) framework to address the limitations
of traditional methods in ensemble sorting. UMRE replaces handcrafted
transformations with Unconstrained Monotonic Neural Networks (UMNN), which
learn expressive, strictly monotonic functions through the integration of
positive neural integrals. Subsequently, a lightweight ranking model is
employed to fuse the prediction scores, assigning personalized weights to each
prediction objective. To balance competing goals, we further introduce a Pareto
optimality strategy that adaptively coordinates task weights during training.
UMRE eliminates manual tuning, maintains ranking consistency, and achieves
fine-grained personalization. Experimental results on two public recommendation
datasets (Kuairand and Tenrec) and online A/B tests demonstrate impressive
performance and generalization capabilities.

### 10. REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Wentao Jiang, Xiang Feng, Zengmao Wang, Yong Luo, Pingbo Xu, Zhe Chen, Bo Du, Jing Zhang
- **URL**: <http://arxiv.org/abs/2508.08149v2>
- **Submitted**: 2025-08-11 16:25:25
- **Comment**: 17 pages, 4 figures; updated references
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper proposes a framework for reasoning exploration in retrieval-augmented generation, which is a novel application of reinforcement learning in NLP. While it touches on some aspects of query understanding and ranking models, the focus is on reasoning and generation rather than search and retrieval. The paper's relevance to information retrieval is limited, and it does not directly address user behavior modeling or click models.

#### Abstract
> Reinforcement learning (RL) is emerging as a powerful paradigm for enabling
large language models (LLMs) to perform complex reasoning tasks. Recent
advances indicate that integrating RL with retrieval-augmented generation (RAG)
allows LLMs to dynamically incorporate external knowledge, leading to more
informed and robust decision making. However, we identify a critical challenge
during policy-driven trajectory sampling: LLMs are frequently trapped in
unproductive reasoning paths, which we refer to as "dead ends", committing to
overconfident yet incorrect conclusions. This severely hampers exploration and
undermines effective policy optimization. To address this challenge, we propose
REX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented
Generation), a novel framework that explores alternative reasoning paths while
maintaining rigorous policy learning through principled distributional
corrections. Our approach introduces two key innovations: (1) Mixed Sampling
Strategy, which combines a novel probe sampling method with exploratory prompts
to escape dead ends; and (2) Policy Correction Mechanism, which employs
importance sampling to correct distribution shifts induced by mixed sampling,
thereby mitigating gradient estimation bias. We evaluate it on seven
question-answering benchmarks, and the experimental results show that REX-RAG
achieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B
over strong baselines, demonstrating competitive results across multiple
datasets. The code is publicly available at https://github.com/MiliLab/REX-RAG.

### 11. ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Archchana Sindhujan, Shenbin Qian, Chan Chi Chun Matthew, Constantin Orasan, Diptesh Kanojia
- **URL**: <http://arxiv.org/abs/2508.07484v1>
- **Submitted**: 2025-08-10 20:59:44
- **Comment**: Accepted to COLM 2025 Conference
- **Topic Keywords**: rag, rank, search
- **Reason**: The paper focuses on Large Language Models (LLMs) for Quality Estimation (QE) in Machine Translation, which is not directly related to Information Retrieval or Search technologies. While it touches on Transformer representations and layer adaptation, the context is more relevant to NLP and MT, rather than IR or query understanding.

#### Abstract
> Large Language Models (LLMs) have shown remarkable performance across a wide
range of natural language processing tasks. Quality Estimation (QE) for Machine
Translation (MT), which assesses the quality of a source-target pair without
relying on reference translations, remains a challenging cross-lingual task for
LLMs. The challenges stem from the inherent limitations of existing LLM-based
QE systems, which are pre-trained for causal language modelling rather than
regression-specific tasks, further elevated by the presence of low-resource
languages given pre-training data distribution. This paper introduces ALOPE, an
adaptive layer-optimization framework designed to enhance LLM-based QE by
restructuring Transformer representations through layer-wise adaptation for
improved regression-based prediction. Our framework integrates low-rank
adapters (LoRA) with regression task heads, leveraging selected pre-trained
Transformer layers for improved cross-lingual alignment. In addition to the
layer-specific adaptation, ALOPE introduces two strategies-dynamic weighting,
which adaptively combines representations from multiple layers, and multi-head
regression, which aggregates regression losses from multiple heads for QE. Our
framework shows improvements over various existing LLM-based QE approaches.
Empirical evidence suggests that intermediate Transformer layers in LLMs
provide contextual representations that are more aligned with the cross-lingual
nature of the QE task. We make resultant models and framework code publicly
available for further research, also allowing existing LLM-based MT frameworks
to be scaled with QE capabilities.

### 12. LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Luyao Zhuang, Qinggang Zhang, Huachi Zhou, Juhua Liu, Qing Li, Xiao Huang
- **URL**: <http://arxiv.org/abs/2508.07690v1>
- **Submitted**: 2025-08-11 07:07:18
- **Topic Keywords**: retrieval, search
- **Reason**: The paper focuses on tool retrieval, which is not directly related to information retrieval or search technologies. While it mentions language models and retrieval, the context is different from query understanding, ranking models, and user behavior modeling. The paper's emphasis on logic-guided semantic bridging and relational augmented retrieval is not directly applicable to the user's research interests.

#### Abstract
> Tool learning has emerged as a promising paradigm for large language models
(LLMs) to solve many real-world tasks. Nonetheless, with the tool repository
rapidly expanding, it is impractical to contain all tools within the limited
input length of LLMs. To alleviate these issues, researchers have explored
incorporating a tool retrieval module to select the most relevant tools or
represent tools as unique tokens within LLM parameters. However, most
state-of-the-art methods are under transductive settings, assuming all tools
have been observed during training. Such a setting deviates from reality as the
real-world tool repository is evolving and incorporates new tools frequently.
When dealing with these unseen tools, which refer to tools not encountered
during the training phase, these methods are limited by two key issues,
including the large distribution shift and the vulnerability of
similarity-based retrieval. To this end, inspired by human cognitive processes
of mastering unseen tools through discovering and applying the logical
information from prior experience, we introduce a novel Logic-Guided Semantic
Bridging framework for inductive tool retrieval, namely, LoSemB, which aims to
mine and transfer latent logical information for inductive tool retrieval
without costly retraining. Specifically, LoSemB contains a logic-based
embedding alignment module to mitigate distribution shifts and implements a
relational augmented retrieval mechanism to reduce the vulnerability of
similarity-based retrieval. Extensive experiments demonstrate that LoSemB
achieves advanced performance in inductive settings while maintaining desirable
effectiveness in the transductive setting.

### 13. Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Tianyi Zhou, Johanne Medina, Sanjay Chawla
- **URL**: <http://arxiv.org/abs/2508.08139v1>
- **Submitted**: 2025-08-11 16:12:36
- **Topic Keywords**: rag
- **Reason**: The paper explores the reliability of Large Language Models (LLMs) and proposes a method to detect unreliable outputs. While it touches on uncertainty and confidence, which are related to query understanding and ranking models, the focus is on language models and reliability estimation, which is not directly aligned with my primary research interests in Information Retrieval and Search technologies.

#### Abstract
> Large Language Models (LLMs) are prone to generating fluent but incorrect
content, known as confabulation, which poses increasing risks in multi-turn or
agentic applications where outputs may be reused as context. In this work, we
investigate how in-context information influences model behavior and whether
LLMs can identify their unreliable responses. We propose a reliability
estimation that leverages token-level uncertainty to guide the aggregation of
internal model representations. Specifically, we compute aleatoric and
epistemic uncertainty from output logits to identify salient tokens and
aggregate their hidden states into compact representations for response-level
reliability prediction. Through controlled experiments on open QA benchmarks,
we find that correct in-context information improves both answer accuracy and
model confidence, while misleading context often induces confidently incorrect
responses, revealing a misalignment between uncertainty and correctness. Our
probing-based method captures these shifts in model behavior and improves the
detection of unreliable outputs across multiple open-source LLMs. These results
underscore the limitations of direct uncertainty signals and highlight the
potential of uncertainty-guided probing for reliability-aware generation.

### 14. What am I missing here?: Evaluating Large Language Models for Masked Sentence Prediction

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Charlie Wyatt, Aditya Joshi, Flora Salim
- **URL**: <http://arxiv.org/abs/2508.07702v1>
- **Submitted**: 2025-08-11 07:25:50
- **Comment**: Under Review
- **Topic Keywords**: rag
- **Reason**: The paper evaluates large language models for masked sentence prediction, which is a task related to natural language processing. While it touches on the idea of predicting longer contexts, it does not directly address query understanding, ranking models, or user behavior modeling, which are core interests in information retrieval and search technologies.

#### Abstract
> Transformer-based models primarily rely on Next Token Prediction (NTP), which
predicts the next token in a sequence based on the preceding context. However,
NTP's focus on single-token prediction often limits a model's ability to plan
ahead or maintain long-range coherence, raising questions about how well LLMs
can predict longer contexts, such as full sentences within structured
documents. While NTP encourages local fluency, it provides no explicit
incentive to ensure global coherence across sentence boundaries-an essential
skill for reconstructive or discursive tasks. To investigate this, we evaluate
three commercial LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) on
Masked Sentence Prediction (MSP) - the task of infilling a randomly removed
sentence - from three domains: ROCStories (narrative), Recipe1M (procedural),
and Wikipedia (expository). We assess both fidelity (similarity to the original
sentence) and cohesiveness (fit within the surrounding context). Our key
finding reveals that commercial LLMs, despite their superlative performance in
other tasks, are poor at predicting masked sentences in low-structured domains,
highlighting a gap in current model capabilities.

### 15. From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Jia Deng, Jie Chen, Zhipeng Chen, Daixuan Cheng, Fei Bai, Beichen Zhang, Yinqian Min, Yanzipeng Gao, Wayne Xin Zhao, Ji-Rong Wen
- **URL**: <http://arxiv.org/abs/2508.07534v1>
- **Submitted**: 2025-08-11 01:26:16
- **Comment**: 27pages,25figures. arXiv admin note: text overlap with
  arXiv:2508.02260
- **Topic Keywords**: rag
- **Reason**: The paper explores reinforcement learning with verifiable rewards (RLVR) for large language models (LLMs), focusing on exploration strategies. While it touches on some aspects of query understanding and ranking models, the primary focus is on LLMs' reasoning capabilities, which is not directly related to my core research interests in information retrieval and search technologies.

#### Abstract
> Reinforcement learning with verifiable rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of large language
models (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based
feedback to guide LLMs in generating and refining complex reasoning chains -- a
process critically dependent on effective exploration strategies. While prior
work has demonstrated RLVR's empirical success, the fundamental mechanisms
governing LLMs' exploration behaviors remain underexplored. This technical
report presents a systematic investigation of exploration capacities in RLVR,
covering four main aspects: (1) exploration space shaping, where we develop
quantitative metrics to characterize LLMs' capability boundaries; (2)
entropy-performance exchange, analyzed across training stages, individual
instances, and token-level patterns; and (3) RL performance optimization,
examining methods to effectively translate exploration gains into measurable
improvements. By unifying previously identified insights with new empirical
evidence, this work aims to provide a foundational framework for advancing RLVR
systems.

### 16. Positional Biases Shift as Inputs Approach Context Window Limits

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Blerta Veseli, Julian Chibane, Mariya Toneva, Alexander Koller
- **URL**: <http://arxiv.org/abs/2508.07479v1>
- **Submitted**: 2025-08-10 20:40:24
- **Topic Keywords**: retrieval
- **Reason**: The paper explores positional biases in Large Language Models, which is related to query understanding and ranking models in Information Retrieval. However, the focus on language models and their limitations in handling long inputs is not directly aligned with the user's interests in search technologies and user behavior modeling.

#### Abstract
> Large Language Models (LLMs) often struggle to use information across long
inputs effectively. Prior work has identified positional biases, such as the
Lost in the Middle (LiM) effect, where models perform better when information
appears at the beginning (primacy bias) or end (recency bias) of the input,
rather than in the middle. However, long-context studies have not consistently
replicated these effects, raising questions about their intensity and the
conditions under which they manifest. To address this, we conducted a
comprehensive analysis using relative rather than absolute input lengths,
defined with respect to each model's context window. Our findings reveal that
the LiM effect is strongest when inputs occupy up to 50% of a model's context
window. Beyond that, the primacy bias weakens, while recency bias remains
relatively stable. This effectively eliminates the LiM effect; instead, we
observe a distance-based bias, where model performance is better when relevant
information is closer to the end of the input. Furthermore, our results suggest
that successful retrieval is a prerequisite for reasoning in LLMs, and that the
observed positional biases in reasoning are largely inherited from retrieval.
These insights have implications for long-context tasks, the design of future
LLM benchmarks, and evaluation methodologies for LLMs handling extended inputs.

### 17. Large Language Models for Subjective Language Understanding: A Survey

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Changhao Song, Yazhou Zhang, Hui Gao, Ben Yao, Peng Zhang
- **URL**: <http://arxiv.org/abs/2508.07959v1>
- **Submitted**: 2025-08-11 13:10:44
- **Topic Keywords**: search
- **Reason**: The paper focuses on large language models for subjective language understanding, which is related to natural language processing and deep semantic understanding. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest in information retrieval and search technologies.

#### Abstract
> Subjective language understanding refers to a broad set of natural language
processing tasks where the goal is to interpret or generate content that
conveys personal feelings, opinions, or figurative meanings rather than
objective facts. With the advent of large language models (LLMs) such as
ChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach
these inherently nuanced tasks. In this survey, we provide a comprehensive
review of recent advances in applying LLMs to subjective language tasks,
including sentiment analysis, emotion recognition, sarcasm detection, humor
understanding, stance detection, metaphor interpretation, intent detection, and
aesthetics assessment. We begin by clarifying the definition of subjective
language from linguistic and cognitive perspectives, and we outline the unique
challenges posed by subjective language (e.g. ambiguity, figurativeness,
context dependence). We then survey the evolution of LLM architectures and
techniques that particularly benefit subjectivity tasks, highlighting why LLMs
are well-suited to model subtle human-like judgments. For each of the eight
tasks, we summarize task definitions, key datasets, state-of-the-art LLM-based
methods, and remaining challenges. We provide comparative insights, discussing
commonalities and differences among tasks and how multi-task LLM approaches
might yield unified models of subjectivity. Finally, we identify open issues
such as data limitations, model bias, and ethical considerations, and suggest
future research directions. We hope this survey will serve as a valuable
resource for researchers and practitioners interested in the intersection of
affective computing, figurative language processing, and large-scale language
models.

### 18. Recommendation Is a Dish Better Served Warm

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Danil Gusak, Nikita Sukhorukov, Evgeny Frolov
- **URL**: <http://arxiv.org/abs/2508.07856v1>
- **Submitted**: 2025-08-11 11:14:49
- **Comment**: Accepted for ACM RecSys 2025. Author's version. The final published
  version will be available at the ACM Digital Library
- **Topic Keywords**: recommend
- **Reason**: The paper explores recommender systems, which is a related topic to information retrieval, but it focuses on the cold-start problem and threshold selection, which is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling. While the paper touches on the importance of data quality, it does not delve into the deep semantic understanding and real-time relevance optimization aspects that are central to the user's research themes.

#### Abstract
> In modern recommender systems, experimental settings typically include
filtering out cold users and items based on a minimum interaction threshold.
However, these thresholds are often chosen arbitrarily and vary widely across
studies, leading to inconsistencies that can significantly affect the
comparability and reliability of evaluation results. In this paper, we
systematically explore the cold-start boundary by examining the criteria used
to determine whether a user or an item should be considered cold. Our
experiments incrementally vary the number of interactions for different items
during training, and gradually update the length of user interaction histories
during inference. We investigate the thresholds across several widely used
datasets, commonly represented in recent papers from top-tier conferences, and
on multiple established recommender baselines. Our findings show that
inconsistent selection of cold-start thresholds can either result in the
unnecessary removal of valuable data or lead to the misclassification of cold
instances as warm, introducing more noise into the system.

### 19. Evaluating Compositional Approaches for Focus and Sentiment Analysis

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Olga Kellert, Muhammad Imran, Nicholas Hill Matlis, Mahmud Uz Zaman, Carlos G√≥mez-Rodr√≠guez
- **URL**: <http://arxiv.org/abs/2508.07810v1>
- **Submitted**: 2025-08-11 09:52:41
- **Topic Keywords**: search
- **Reason**: The paper evaluates compositional approaches for Focus and Sentiment Analysis, which is related to Natural Language Processing (NLP), but it does not directly address query understanding, ranking models, or user behavior modeling, which are core interests in Information Retrieval (IR). While the paper's focus on compositional analysis and its applications to sentiment analysis is somewhat relevant, it does not seem to have a direct impact on IR or search technologies.

#### Abstract
> This paper summarizes the results of evaluating a compositional approach for
Focus Analysis (FA) in Linguistics and Sentiment Analysis (SA) in Natural
Language Processing (NLP). While quantitative evaluations of compositional and
non-compositional approaches in SA exist in NLP, similar quantitative
evaluations are very rare in FA in Linguistics that deal with linguistic
expressions representing focus or emphasis such as "it was John who left". We
fill this gap in research by arguing that compositional rules in SA also apply
to FA because FA and SA are closely related meaning that SA is part of FA. Our
compositional approach in SA exploits basic syntactic rules such as rules of
modification, coordination, and negation represented in the formalism of
Universal Dependencies (UDs) in English and applied to words representing
sentiments from sentiment dictionaries. Some of the advantages of our
compositional analysis method for SA in contrast to non-compositional analysis
methods are interpretability and explainability. We test the accuracy of our
compositional approach and compare it with a non-compositional approach VADER
that uses simple heuristic rules to deal with negation, coordination and
modification. In contrast to previous related work that evaluates
compositionality in SA on long reviews, this study uses more appropriate
datasets to evaluate compositionality. In addition, we generalize the results
of compositional approaches in SA to compositional approaches in FA.

### 20. Towards Comprehensible Recommendation with Large Language Model Fine-tuning

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Yunze Luo, Yinjie Jiang, Gaode Chen, Xinghua Zhang, Jun Zhang, Jian Liang, Kaigui Bian
- **URL**: <http://arxiv.org/abs/2508.07595v1>
- **Submitted**: 2025-08-11 03:55:31
- **Comment**: 11 pages, 6 figures
- **Topic Keywords**: recommend
- **Reason**: The paper focuses on recommender systems, which is a related topic to information retrieval, but it doesn't directly address query understanding, ranking models, or user behavior modeling. The use of large language models and fine-tuning for recommendation reasons is an interesting application, but it doesn't seem to be directly relevant to the user's core research themes.

#### Abstract
> Recommender systems have become increasingly ubiquitous in daily life. While
traditional recommendation approaches primarily rely on ID-based
representations or item-side content features, they often fall short in
capturing the underlying semantics aligned with user preferences (e.g.,
recommendation reasons for items), leading to a semantic-collaborative gap.
Recently emerged LLM-based feature extraction approaches also face a key
challenge: how to ensure that LLMs possess recommendation-aligned reasoning
capabilities and can generate accurate, personalized reasons to mitigate the
semantic-collaborative gap. To address these issues, we propose a novel Content
Understanding from a Collaborative Perspective framework (CURec), which
generates collaborative-aligned content features for more comprehensive
recommendations. \method first aligns the LLM with recommendation objectives
through pretraining, equipping it with instruction-following and
chain-of-thought reasoning capabilities. Next, we design a reward model
inspired by traditional recommendation architectures to evaluate the quality of
the recommendation reasons generated by the LLM. Finally, using the reward
signals, CURec fine-tunes the LLM through RL and corrects the generated reasons
to ensure their accuracy. The corrected reasons are then integrated into a
downstream recommender model to enhance comprehensibility and recommendation
performance. Extensive experiments on public benchmarks demonstrate the
superiority of CURec over existing methods.

### 21. Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced Submodular Perspective

- **LLM Score**: 2
- **Keyword Score**: 14
- **Authors**: Jun Wang, Zaifu Zhan, Qixin Zhang, Mingquan Lin, Meijia Song, Rui Zhang
- **URL**: <http://arxiv.org/abs/2508.08140v1>
- **Submitted**: 2025-08-11 16:13:21
- **Topic Keywords**: retriever, queries, ranking, rag, retrieval, rank
- **Reason**: The paper focuses on biomedical in-context learning using large language models, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions NLP tasks, the context is specific to biomedical applications, and the paper's emphasis on diversity-enhanced submodular perspective is not relevant to the user's core research themes.

#### Abstract
> Recent progress in large language models (LLMs) has leveraged their
in-context learning (ICL) abilities to enable quick adaptation to unseen
biomedical NLP tasks. By incorporating only a few input-output examples into
prompts, LLMs can rapidly perform these new tasks. While the impact of these
demonstrations on LLM performance has been extensively studied, most existing
approaches prioritize representativeness over diversity when selecting examples
from large corpora. To address this gap, we propose Dual-Div, a
diversity-enhanced data-efficient framework for demonstration selection in
biomedical ICL. Dual-Div employs a two-stage retrieval and ranking process:
First, it identifies a limited set of candidate examples from a corpus by
optimizing both representativeness and diversity (with optional annotation for
unlabeled data). Second, it ranks these candidates against test queries to
select the most relevant and non-redundant demonstrations. Evaluated on three
biomedical NLP tasks (named entity recognition (NER), relation extraction (RE),
and text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along
with three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently
outperforms baselines-achieving up to 5% higher macro-F1 scores-while
demonstrating robustness to prompt permutations and class imbalance. Our
findings establish that diversity in initial retrieval is more critical than
ranking-stage optimization, and limiting demonstrations to 3-5 examples
maximizes performance efficiency.

### 22. GLiClass: Generalist Lightweight Model for Sequence Classification Tasks

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Ihor Stepanov, Mykhailo Shtopko, Dmytro Vodianytskyi, Oleksandr Lukashov, Alexander Yavorskyi, Mykyta Yaroshenko
- **URL**: <http://arxiv.org/abs/2508.07662v1>
- **Submitted**: 2025-08-11 06:22:25
- **Comment**: 14 pages, 7 tables, 2 figures
- **Topic Keywords**: rerank, rag, rank
- **Reason**: The paper focuses on sequence classification tasks, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on efficiency and accuracy, the topics of ranking models, user behavior modeling, and deep semantic understanding are not addressed.

#### Abstract
> Classification is one of the most widespread tasks in AI applications,
serving often as the first step in filtering, sorting, and categorizing data.
Since modern AI systems must handle large volumes of input data and early
pipeline stages can propagate errors downstream, achieving high efficiency and
accuracy is critical. Moreover, classification requirements can change
dynamically based on user needs, necessitating models with strong zero-shot
capabilities. While generative LLMs have become mainstream for zero-shot
classification due to their versatility, they suffer from inconsistent
instruction following and computational inefficiency. Cross-encoders, commonly
used as rerankers in RAG pipelines, face a different bottleneck: they must
process text-label pairs sequentially, significantly reducing efficiency with
large label sets. Embedding-based approaches offer good efficiency but struggle
with complex scenarios involving logical and semantic constraints. We propose
GLiClass, a novel method that adapts the GLiNER architecture for sequence
classification tasks. Our approach achieves strong accuracy and efficiency
comparable to embedding-based methods, while maintaining the flexibility needed
for zero-shot and few-shot learning scenarios. Additionally, we adapted
proximal policy optimization (PPO) for multi-label text classification,
enabling training classifiers in data-sparse conditions or from human feedback.

### 23. Exploring Procedural Data Generation for Automatic Acoustic Guitar Fingerpicking Transcription

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Sebastian Murgul, Michael Heizmann
- **URL**: <http://arxiv.org/abs/2508.07987v1>
- **Submitted**: 2025-08-11 13:52:17
- **Comment**: Accepted to the 6th Conference on AI Music Creativity (AIMC), 2025
- **Topic Keywords**: information retrieval, retrieval
- **Reason**: The paper explores procedural data generation for automatic acoustic guitar fingerpicking transcription, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it touches on music information retrieval, the focus is on audio processing and transcription, rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> Automatic transcription of acoustic guitar fingerpicking performances remains
a challenging task due to the scarcity of labeled training data and legal
constraints connected with musical recordings. This work investigates a
procedural data generation pipeline as an alternative to real audio recordings
for training transcription models. Our approach synthesizes training data
through four stages: knowledge-based fingerpicking tablature composition, MIDI
performance rendering, physical modeling using an extended Karplus-Strong
algorithm, and audio augmentation including reverb and distortion. We train and
evaluate a CRNN-based note-tracking model on both real and synthetic datasets,
demonstrating that procedural data can be used to achieve reasonable
note-tracking results. Finetuning with a small amount of real data further
enhances transcription accuracy, improving over models trained exclusively on
real recordings. These results highlight the potential of procedurally
generated audio for data-scarce music information retrieval tasks.

### 24. Joint Transcription of Acoustic Guitar Strumming Directions and Chords

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Sebastian Murgul, Johannes Schimper, Michael Heizmann
- **URL**: <http://arxiv.org/abs/2508.07973v1>
- **Submitted**: 2025-08-11 13:34:49
- **Comment**: Accepted to the 26th International Society for Music Information
  Retrieval Conference (ISMIR), 2025
- **Topic Keywords**: information retrieval, retrieval
- **Reason**: The paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, and Natural Language Processing. The topic of automatic transcription of guitar strumming directions and chords is not related to your areas of focus, and the paper does not discuss query understanding, ranking models, or user behavior modeling.

#### Abstract
> Automatic transcription of guitar strumming is an underrepresented and
challenging task in Music Information Retrieval (MIR), particularly for
extracting both strumming directions and chord progressions from audio signals.
While existing methods show promise, their effectiveness is often hindered by
limited datasets. In this work, we extend a multimodal approach to guitar
strumming transcription by introducing a novel dataset and a deep
learning-based transcription model. We collect 90 min of real-world guitar
recordings using an ESP32 smartwatch motion sensor and a structured recording
protocol, complemented by a synthetic dataset of 4h of labeled strumming audio.
A Convolutional Recurrent Neural Network (CRNN) model is trained to detect
strumming events, classify their direction, and identify the corresponding
chords using only microphone audio. Our evaluation demonstrates significant
improvements over baseline onset detection algorithms, with a hybrid method
combining synthetic and real-world data achieving the highest accuracy for both
strumming action detection and chord classification. These results highlight
the potential of deep learning for robust guitar strumming transcription and
open new avenues for automatic rhythm guitar analysis.

### 25. Exploring the Technical Knowledge Interaction of Global Digital Humanities: Three-decade Evidence from Bibliometric-based perspectives

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Jiayi Li, Chengxi Yan, Yurong Zeng, Zhichao Fang, Huiru Wang
- **URL**: <http://arxiv.org/abs/2508.08347v1>
- **Submitted**: 2025-08-11 12:27:39
- **Topic Keywords**: ranking, rank, search
- **Reason**: The paper's focus on Digital Humanities, bibliometric analysis, and topic modeling is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's technical knowledge interaction and Topic-Method Composition concept do not align with the user's expertise in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Digital Humanities (DH) is an interdisciplinary field that integrates
computational methods with humanities scholarship to investigate innovative
topics. Each academic discipline follows a unique developmental path shaped by
the topics researchers investigate and the methods they employ. With the help
of bibliometric analysis, most of previous studies have examined DH across
multiple dimensions such as research hotspots, co-author networks, and
institutional rankings. However, these studies have often been limited in their
ability to provide deep insights into the current state of technological
advancements and topic development in DH. As a result, their conclusions tend
to remain superficial or lack interpretability in understanding how methods and
topics interrelate in the field. To address this gap, this study introduced a
new concept of Topic-Method Composition (TMC), which refers to a hybrid
knowledge structure generated by the co-occurrence of specific research topics
and the corresponding method. Especially by analyzing the interaction between
TMCs, we can see more clearly the intersection and integration of digital
technology and humanistic subjects in DH. Moreover, this study developed a
TMC-based workflow combining bibliometric analysis, topic modeling, and network
analysis to analyze the development characteristics and patterns of research
disciplines. By applying this workflow to large-scale bibliometric data, it
enables a detailed view of the knowledge structures, providing a tool adaptable
to other fields.

### 26. Can You Trick the Grader? Adversarial Persuasion of LLM Judges

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Yerin Hwang, Dongryeol Lee, Taegwan Kang, Yongil Kim, Kyomin Jung
- **URL**: <http://arxiv.org/abs/2508.07805v1>
- **Submitted**: 2025-08-11 09:45:02
- **Comment**: 19 pages, 8 figures
- **Topic Keywords**: pairwise, rag
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus on large language models and persuasion techniques is outside the scope of the user's research interests.

#### Abstract
> As large language models take on growing roles as automated evaluators in
practical settings, a critical question arises: Can individuals persuade an LLM
judge to assign unfairly high scores? This study is the first to reveal that
strategically embedded persuasive language can bias LLM judges when scoring
mathematical reasoning tasks, where correctness should be independent of
stylistic variation. Grounded in Aristotle's rhetorical principles, we
formalize seven persuasion techniques (Majority, Consistency, Flattery,
Reciprocity, Pity, Authority, Identity) and embed them into otherwise identical
responses. Across six math benchmarks, we find that persuasive language leads
LLM judges to assign inflated scores to incorrect solutions, by up to 8% on
average, with Consistency causing the most severe distortion. Notably,
increasing model size does not substantially mitigate this vulnerability.
Further analysis demonstrates that combining multiple persuasion techniques
amplifies the bias, and pairwise evaluation is likewise susceptible. Moreover,
the persuasive effect persists under counter prompting strategies, highlighting
a critical vulnerability in LLM-as-a-Judge pipelines and underscoring the need
for robust defenses against persuasion-based attacks.

### 27. Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Haowen Wang, Yun Yue, Zhiling Ye, Shuowen Zhang, Lei Fan, Jiaxin Liang, Jiadi Jiang, Cheng Wei, Jingyuan Deng, Xudong Han, Ji Li, Chunxiao Guo, Peng Wei, Jian Wang, Jinjie Gu
- **URL**: <http://arxiv.org/abs/2508.07750v1>
- **Submitted**: 2025-08-11 08:28:47
- **Comment**: 12 pages, 5 figures, 7 tables
- **Topic Keywords**: pairwise, rag
- **Reason**: The paper focuses on language model alignment, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and query understanding. While the paper mentions reinforcement learning, which is a related topic, the context is different and the paper's contributions do not seem to have a direct impact on the user's research areas.

#### Abstract
> Alignment methodologies have emerged as a critical pathway for enhancing
language model alignment capabilities. While SFT (supervised fine-tuning)
accelerates convergence through direct token-level loss intervention, its
efficacy is constrained by offline policy trajectory. In contrast,
RL(reinforcement learning) facilitates exploratory policy optimization, but
suffers from low sample efficiency and stringent dependency on high-quality
base models. To address these dual challenges, we propose GRAO (Group Relative
Alignment Optimization), a unified framework that synergizes the respective
strengths of SFT and RL through three key innovations: 1) A multi-sample
generation strategy enabling comparative quality assessment via reward
feedback; 2) A novel Group Direct Alignment Loss formulation leveraging
intra-group relative advantage weighting; 3) Reference-aware parameter updates
guided by pairwise preference dynamics. Our theoretical analysis establishes
GRAO's convergence guarantees and sample efficiency advantages over
conventional approaches. Comprehensive evaluations across complex human
alignment tasks demonstrate GRAO's superior performance, achieving
57.70\%,17.65\% 7.95\% and 5.18\% relative improvements over SFT, DPO, PPO and
GRPO baselines respectively. This work provides both a theoretically grounded
alignment framework and empirical evidence for efficient capability evolution
in language models.

### 28. Jinx: Unlimited LLMs for Probing Alignment Failures

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Jiahao Zhao, Liwei Dong
- **URL**: <http://arxiv.org/abs/2508.08243v2>
- **Submitted**: 2025-08-11 17:56:06
- **Comment**: https://huggingface.co/Jinx-org
- **Topic Keywords**: queries, search
- **Reason**: The paper focuses on language models, specifically introducing a new variant called Jinx, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on safety alignment and evaluation, the topic is not aligned with the user's primary research interests in IR and NLP.

#### Abstract
> Unlimited, or so-called helpful-only language models are trained without
safety alignment constraints and never refuse user queries. They are widely
used by leading AI companies as internal tools for red teaming and alignment
evaluation. For example, if a safety-aligned model produces harmful outputs
similar to an unlimited model, this indicates alignment failures that require
further attention. Despite their essential role in assessing alignment, such
models are not available to the research community.
  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx
responds to all queries without refusals or safety filtering, while preserving
the base model's capabilities in reasoning and instruction following. It
provides researchers with an accessible tool for probing alignment failures,
evaluating safety boundaries, and systematically studying failure modes in
language model safety.

### 29. Meta Off-Policy Estimation

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Olivier Jeunen
- **URL**: <http://arxiv.org/abs/2508.07914v1>
- **Submitted**: 2025-08-11 12:31:13
- **Comment**: To appear in the Nineteenth ACM Conference on Recommender Systems
  (RecSys '25)
- **Topic Keywords**: rag, recommend, search
- **Reason**: The paper focuses on off-policy estimation for recommender systems, which is not directly related to the user's primary research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling. While the paper touches on statistical estimation and confidence intervals, it does not address the user's specific areas of interest, such as deep semantic understanding and real-time relevance optimization.

#### Abstract
> Off-policy estimation (OPE) methods enable unbiased offline evaluation of
recommender systems, directly estimating the online reward some target policy
would have obtained, from offline data and with statistical guarantees. The
theoretical elegance of the framework combined with practical successes have
led to a surge of interest, with many competing estimators now available to
practitioners and researchers. Among these, Doubly Robust methods provide a
prominent strategy to combine value- and policy-based estimators.
  In this work, we take an alternative perspective to combine a set of OPE
estimators and their associated confidence intervals into a single, more
accurate estimate. Our approach leverages a correlated fixed-effects
meta-analysis framework, explicitly accounting for dependencies among
estimators that arise due to shared data. This yields a best linear unbiased
estimate (BLUE) of the target policy's value, along with an appropriately
conservative confidence interval that reflects inter-estimator correlation. We
validate our method on both simulated and real-world data, demonstrating
improved statistical efficiency over existing individual estimators.

### 30. Orthogonal Low Rank Embedding Stabilization

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Kevin Zielnicki, Ko-Jen Hsiao
- **URL**: <http://arxiv.org/abs/2508.07574v1>
- **Submitted**: 2025-08-11 03:15:51
- **Topic Keywords**: rag, recommend, rank
- **Reason**: The paper focuses on recommendation systems and embedding stabilization, which is not directly related to the user's primary research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling. While the paper touches on embedding spaces, it does not address the user's specific areas of interest in deep semantic understanding and real-time relevance optimization.

#### Abstract
> The instability of embedding spaces across model retraining cycles presents
significant challenges to downstream applications using user or item embeddings
derived from recommendation systems as input features. This paper introduces a
novel orthogonal low-rank transformation methodology designed to stabilize the
user/item embedding space, ensuring consistent embedding dimensions across
retraining sessions. Our approach leverages a combination of efficient low-rank
singular value decomposition and orthogonal Procrustes transformation to map
embeddings into a standardized space. This transformation is computationally
efficient, lossless, and lightweight, preserving the dot product and inference
quality while reducing operational burdens. Unlike existing methods that modify
training objectives or embedding structures, our approach maintains the
integrity of the primary model application and can be seamlessly integrated
with other stabilization techniques.

### 31. Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Zihe Liu, Jiashun Liu, Yancheng He, Weixun Wang, Jiaheng Liu, Ling Pan, Xinyu Hu, Shaopan Xiong, Ju Huang, Jian Hu, Shengyi Huang, Siran Yang, Jiamang Wang, Wenbo Su, Bo Zheng
- **URL**: <http://arxiv.org/abs/2508.08221v1>
- **Submitted**: 2025-08-11 17:39:45
- **Comment**: 26 pages, 21 figures
- **Topic Keywords**: rag, search
- **Reason**: This paper is not directly related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The focus on Reinforcement Learning for Language Model Reasoning is outside the user's primary area of interest, and the techniques discussed are not applicable to the user's domain.

#### Abstract
> Reinforcement learning for LLM reasoning has rapidly emerged as a prominent
research area, marked by a significant surge in related studies on both
algorithmic innovations and practical applications. Despite this progress,
several critical challenges remain, including the absence of standardized
guidelines for employing RL techniques and a fragmented understanding of their
underlying mechanisms. Additionally, inconsistent experimental settings,
variations in training data, and differences in model initialization have led
to conflicting conclusions, obscuring the key characteristics of these
techniques and creating confusion among practitioners when selecting
appropriate techniques. This paper systematically reviews widely adopted RL
techniques through rigorous reproductions and isolated evaluations within a
unified open-source framework. We analyze the internal mechanisms, applicable
scenarios, and core principles of each technique through fine-grained
experiments, including datasets of varying difficulty, model sizes, and
architectures. Based on these insights, we present clear guidelines for
selecting RL techniques tailored to specific setups, and provide a reliable
roadmap for practitioners navigating the RL for the LLM domain. Finally, we
reveal that a minimalist combination of two techniques can unlock the learning
capability of critic-free policies using vanilla PPO loss. The results
demonstrate that our simple combination consistently improves performance,
surpassing strategies like GRPO and DAPO.

### 32. Multi-modal Adaptive Mixture of Experts for Cold-start Recommendation

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Van-Khang Nguyen, Duc-Hoang Pham, Huy-Son Nguyen, Cam-Van Thi Nguyen, Hoang-Quynh Le, Duc-Trong Le
- **URL**: <http://arxiv.org/abs/2508.08042v1>
- **Submitted**: 2025-08-11 14:47:14
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on recommender systems, specifically cold-start recommendation, using multimodal data. While it employs a novel approach, it does not address query understanding, ranking models, or user behavior modeling, which are core aspects of Information Retrieval and Search technologies. The paper's relevance to the user's research interests is limited.

#### Abstract
> Recommendation systems have faced significant challenges in cold-start
scenarios, where new items with a limited history of interaction need to be
effectively recommended to users. Though multimodal data (e.g., images, text,
audio, etc.) offer rich information to address this issue, existing approaches
often employ simplistic integration methods such as concatenation, average
pooling, or fixed weighting schemes, which fail to capture the complex
relationships between modalities. Our study proposes a novel Mixture of Experts
(MoE) framework for multimodal cold-start recommendation, named MAMEX, which
dynamically leverages latent representation from different modalities. MAMEX
utilizes modality-specific expert networks and introduces a learnable gating
mechanism that adaptively weights the contribution of each modality based on
its content characteristics. This approach enables MAMEX to emphasize the most
informative modalities for each item while maintaining robustness when certain
modalities are less relevant or missing. Extensive experiments on benchmark
datasets show that MAMEX outperforms state-of-the-art methods in cold-start
scenarios, with superior accuracy and adaptability. For reproducibility, the
code has been made available on Github https://github.com/L2R-UET/MAMEX.

### 33. Conversational DNA: A New Visual Language for Understanding Dialogue Structure in Human and AI

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Baihan Lin
- **URL**: <http://arxiv.org/abs/2508.07520v1>
- **Submitted**: 2025-08-11 00:43:35
- **Topic Keywords**: relevance
- **Reason**: The paper's focus on dialogue structure and visual language is not directly related to the user's interests in Information Retrieval, Search technologies, and query understanding. While it touches on human-computer interaction, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest for the user.

#### Abstract
> What if the patterns hidden within dialogue reveal more about communication
than the words themselves? We introduce Conversational DNA, a novel visual
language that treats any dialogue -- whether between humans, between human and
AI, or among groups -- as a living system with interpretable structure that can
be visualized, compared, and understood. Unlike traditional conversation
analysis that reduces rich interaction to statistical summaries, our approach
reveals the temporal architecture of dialogue through biological metaphors.
Linguistic complexity flows through strand thickness, emotional trajectories
cascade through color gradients, conversational relevance forms through
connecting elements, and topic coherence maintains structural integrity through
helical patterns. Through exploratory analysis of therapeutic conversations and
historically significant human-AI dialogues, we demonstrate how this
visualization approach reveals interaction patterns that traditional methods
miss. Our work contributes a new creative framework for understanding
communication that bridges data visualization, human-computer interaction, and
the fundamental question of what makes dialogue meaningful in an age where
humans increasingly converse with artificial minds.

### 34. Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Joseph T. Colonel, Baihan Lin
- **URL**: <http://arxiv.org/abs/2508.07517v1>
- **Submitted**: 2025-08-11 00:27:52
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on qualitative interviews and word clouds, which is not directly related to information retrieval, search technologies, or query understanding. While it uses language models, the application is in a different domain and does not address ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest.

#### Abstract
> Word clouds are a common way to summarize qualitative interviews, yet
traditional frequency-based methods often fail in conversational contexts: they
surface filler words, ignore paraphrase, and fragment semantically related
ideas. This limits their usefulness in early-stage analysis, when researchers
need fast, interpretable overviews of what participant actually said. We
introduce ThemeClouds, an open-source visualization tool that uses large
language models (LLMs) to generate thematic, participant-weighted word clouds
from dialogue transcripts. The system prompts an LLM to identify concept-level
themes across a corpus and then counts how many unique participants mention
each topic, yielding a visualization grounded in breadth of mention rather than
raw term frequency. Researchers can customize prompts and visualization
parameters, providing transparency and control. Using interviews from a user
study comparing five recording-device configurations (31 participants; 155
transcripts, Whisper ASR), our approach surfaces more actionable device
concerns than frequency clouds and topic-modeling baselines (e.g., LDA,
BERTopic). We discuss design trade-offs for integrating LLM assistance into
qualitative workflows, implications for interpretability and researcher agency,
and opportunities for interactive analyses such as per-condition contrasts
(``diff clouds'').

### 35. From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Sven Weinzierl, Sandra Zilker, Annina Liessmann, Martin K√§ppel, Weixin Wang, Martin Matzner
- **URL**: <http://arxiv.org/abs/2508.08061v1>
- **Submitted**: 2025-08-11 15:03:50
- **Topic Keywords**: rag
- **Reason**: The paper focuses on predictive process monitoring in organizations, leveraging transfer learning, which is not directly related to information retrieval, search technologies, or natural language processing. While it touches on data analysis, it does not involve query understanding, ranking models, or user behavior modeling, making it irrelevant to the user's primary research interests.

#### Abstract
> Event logs reflect the behavior of business processes that are mapped in
organizational information systems. Predictive process monitoring (PPM)
transforms these data into value by creating process-related predictions that
provide the insights required for proactive interventions at process runtime.
Existing PPM techniques require sufficient amounts of event data or other
relevant resources that might not be readily available, preventing some
organizations from utilizing PPM. The transfer learning-based PPM technique
presented in this paper allows organizations without suitable event data or
other relevant resources to implement PPM for effective decision support. The
technique is instantiated in two real-life use cases, based on which numerical
experiments are performed using event logs for IT service management processes
in an intra- and inter-organizational setting. The results of the experiments
suggest that knowledge of one business process can be transferred to a similar
business process in the same or a different organization to enable effective
PPM in the target context. With the proposed technique, organizations can
benefit from transfer learning in an intra- and inter-organizational setting,
where resources like pre-trained models are transferred within and across
organizational boundaries.

### 36. Audio-Thinker: Guiding Audio Language Model When and How to Think via Reinforcement Learning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Shu Wu, Chenxing Li, Wenfu Wang, Hao Zhang, Hualei Wang, Meng Yu, Dong Yu
- **URL**: <http://arxiv.org/abs/2508.08039v2>
- **Submitted**: 2025-08-11 14:41:10
- **Comment**: preprint
- **Topic Keywords**: rag
- **Reason**: The paper focuses on audio language models and reinforcement learning, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on reasoning and modeling, the context is specific to audio and language, making it less relevant to the user's primary research interests.

#### Abstract
> Recent advancements in large language models, multimodal large language
models, and large audio language models (LALMs) have significantly improved
their reasoning capabilities through reinforcement learning with rule-based
rewards. However, the explicit reasoning process has yet to show significant
benefits for audio question answering, and effectively leveraging deep
reasoning remains an open challenge, with LALMs still falling short of
human-level auditory-language reasoning. To address these limitations, we
propose Audio-Thinker, a reinforcement learning framework designed to enhance
the reasoning capabilities of LALMs, with a focus on improving adaptability,
consistency, and effectiveness. Our approach introduces an adaptive think
accuracy reward, enabling the model to adjust its reasoning strategies based on
task complexity dynamically. Furthermore, we incorporate an external reward
model to evaluate the overall consistency and quality of the reasoning process,
complemented by think-based rewards that help the model distinguish between
valid and flawed reasoning paths during training. Experimental results
demonstrate that our Audio-Thinker model outperforms existing
reasoning-oriented LALMs across various benchmark tasks, exhibiting superior
reasoning and generalization capabilities.

### 37. Progressive Depth Up-scaling via Optimal Transport

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Mingzi Cao, Xi Wang, Nikolaos Aletras
- **URL**: <http://arxiv.org/abs/2508.08011v1>
- **Submitted**: 2025-08-11 14:15:33
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on scaling large language models and proposing a new method for neuron alignment, which is not directly related to query understanding, ranking models, or user behavior modeling.

#### Abstract
> Scaling Large Language Models (LLMs) yields performance gains but incurs
substantial training costs. Depth up-scaling offers training efficiency by
adding new layers to pre-trained models. However, most existing methods copy or
average weights from base layers, neglecting neuron permutation differences.
This limitation can potentially cause misalignment that harms performance.
Inspired by applying Optimal Transport (OT) for neuron alignment, we propose
Optimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses
Transformer blocks in adjacent base layers via OT for new layer creation, to
mitigate neuron permutation mismatch between layers. OpT-DeUS achieves better
overall performance and offers improved training efficiency than existing
methods for continual pre-training and supervised fine-tuning across different
model sizes. To further evaluate the impact of interpolation positions, our
extensive analysis shows that inserting new layers closer to the top results in
higher training efficiency due to shorter back-propagation time while obtaining
additional performance gains.

### 38. Early Explorations of Recommender Systems for Physical Activity and Well-being

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Alan Said
- **URL**: <http://arxiv.org/abs/2508.07980v1>
- **Submitted**: 2025-08-11 13:38:58
- **Comment**: Second International Workshop on Recommender Systems for
  Sustainability and Social Good (RecSoGood) in conjunction with ACM RecSys
  2025
- **Topic Keywords**: recommend, personalization
- **Reason**: The paper focuses on recommender systems for physical activity and well-being, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although the paper touches on user behavior modeling, it is not specifically related to query understanding, ranking models, or click models.

#### Abstract
> As recommender systems increasingly guide physical actions, often through
wearables and coaching tools, new challenges arise around how users interpret,
trust, and respond to this advice. This paper introduces a conceptual framework
for tangible recommendations that influence users' bodies, routines, and
well-being. We describe three design dimensions: trust and interpretation,
intent alignment, and consequence awareness. These highlight key limitations in
applying conventional recommender logic to embodied settings. Through examples
and design reflections, we outline how future systems can support long-term
well-being, behavioral alignment, and socially responsible personalization.

### 39. Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Chen Cecilia Liu, Hiba Arnaout, Nils Kovaƒçiƒá, Dana Atzil-Slonim, Iryna Gurevych
- **URL**: <http://arxiv.org/abs/2508.07902v1>
- **Submitted**: 2025-08-11 12:17:58
- **Comment**: Under review; joint first authors
- **Topic Keywords**: rag
- **Reason**: The paper focuses on large language models and emotional support, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on cultural sensitivity, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are core areas of interest for your research.

#### Abstract
> Large language models (LLMs) show promise in offering emotional support and
generating empathetic responses for individuals in distress, but their ability
to deliver culturally sensitive support remains underexplored due to lack of
resources. In this work, we introduce CultureCare, the first dataset designed
for this task, spanning four cultures and including 1729 distress messages,
1523 cultural signals, and 1041 support strategies with fine-grained emotional
and cultural annotations. Leveraging CultureCare, we (i) develop and test four
adaptation strategies for guiding three state-of-the-art LLMs toward culturally
sensitive responses; (ii) conduct comprehensive evaluations using LLM judges,
in-culture human annotators, and clinical psychologists; (iii) show that
adapted LLMs outperform anonymous online peer responses, and that simple
cultural role-play is insufficient for cultural sensitivity; and (iv) explore
the application of LLMs in clinical training, where experts highlight their
potential in fostering cultural competence in future therapists.

### 40. Maximizing GPU Efficiency via Optimal Adapter Caching: An Analytical Approach for Multi-Tenant LLM Serving

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ferran Agullo, Joan Oliveras, Chen Wang, Alberto Gutierrez-Torre, Olivier Tardieu, Alaa Youssef, Jordi Torres, Josep Ll. Berral
- **URL**: <http://arxiv.org/abs/2508.08343v1>
- **Submitted**: 2025-08-11 10:47:35
- **Comment**: Under review for a computer science conference
- **Topic Keywords**: rag
- **Reason**: The paper focuses on optimizing GPU efficiency for LLM serving, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions AI-driven pipeline, it's not clear how it relates to user behavior modeling or ranking models. The paper's topic is more aligned with computer architecture and system optimization.

#### Abstract
> Serving LLM adapters has gained significant attention as an effective
approach to adapt general-purpose language models to diverse, task-specific use
cases. However, serving a wide range of adapters introduces several and
substantial overheads, leading to performance degradation and challenges in
optimal placement. To address these challenges, we present an analytical,
AI-driven pipeline that accurately determines the optimal allocation of
adapters in single-node setups. This allocation maximizes performance,
effectively using GPU resources, while preventing request starvation.
Crucially, the proposed allocation is given based on current workload patterns.
These insights in single-node setups can be leveraged in multi-replica
deployments for overall placement, load balancing and server configuration,
ultimately enhancing overall performance and improving resource efficiency. Our
approach builds on an in-depth analysis of LLM adapter serving, accounting for
overheads and performance variability, and includes the development of the
first Digital Twin capable of replicating online LLM-adapter serving systems
with matching key performance metrics. The experimental results demonstrate
that the Digital Twin achieves a SMAPE difference of no more than 5.5% in
throughput compared to real results, and the proposed pipeline accurately
predicts the optimal placement with minimal latency.

### 41. SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zeyu Yang, Lai Wei, Roman Koshkin, Xi Chen, Satoshi Nakamura
- **URL**: <http://arxiv.org/abs/2508.07781v1>
- **Submitted**: 2025-08-11 09:13:35
- **Topic Keywords**: rag
- **Reason**: The paper focuses on simultaneous speech translation, leveraging syntax-aware chunking and large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it does involve natural language processing, the specific application and techniques used are not aligned with the user's research interests.

#### Abstract
> This work proposes a grammar-based chunking strategy that segments input
streams into semantically complete units by parsing dependency relations (e.g.,
noun phrase boundaries, verb-object structures) and punctuation features. The
method ensures chunk coherence and minimizes semantic fragmentation. Building
on this mechanism, we present SASST (Syntax-Aware Simultaneous Speech
Translation), an end-to-end framework integrating frozen Whisper encoder and
decoder-only LLM. The unified architecture dynamically outputs translation
tokens or <WAIT> symbols to jointly optimize translation timing and content,
with target-side reordering addressing word-order divergence. Experiments on
CoVoST2 multilingual corpus En-{De, Zh, Ja} demonstrate significant translation
quality improvements across languages and validate the effectiveness of
syntactic structures in LLM-driven SimulST systems.

### 42. ThinkTuning: Instilling Cognitive Reflections without Distillation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Aswin RRV, Jacob Dineen, Divij Handa, Md Nayem Uddin, Mihir Parmar, Chitta Baral, Ben Zhou
- **URL**: <http://arxiv.org/abs/2508.07616v1>
- **Submitted**: 2025-08-11 04:51:43
- **Comment**: 15 pages
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus is on training AI models to develop self-reflective behaviors and multi-step reasoning, which is a topic in Natural Language Processing and Machine Learning, but not specifically relevant to the user's research interests.

#### Abstract
> Recent advances in test-time scaling have led to the emergence of thinking
LLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL
drives this self-improvement paradigm, a recent study (Gandhi et al., 2025)
shows that RL alone does not truly instill these new reasoning abilities - it
merely draws out behaviors already present in the base models. This raises a
question: How can we train the models that don't exhibit such thinking behavior
to develop it in the first place? To this end, we propose ThinkTuning, a
GRPO-based interactive training approach where we augment the rollouts of a
student model with the guidance from a teacher model. A simple idea from
classroom practice inspires our method: a teacher poses a problem, lets the
student try an answer, then gives corrective feedback -- enough to point the
mind in the right direction and then show the solution. Each piece of feedback
reshapes the student's thoughts, leading them to arrive at the correct
solution. Similarly, we find that this type of implicit supervision through
feedback from a teacher model of the same size improves the reasoning
capabilities of the student model. In particular, on average, our method shows
a 3.85% improvement over zero-shot baselines across benchmarks, and on
MATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements
over the vanilla-GRPO baseline. Source code is available at
https://github.com/3rdAT/ThinkTuning.

### 43. IBPS: Indian Bail Prediction System

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Puspesh Kumar Srivastava, Uddeshya Raj, Praveen Patel, /Shubham Kumar Nigam, Noel Shallum, Arnab Bhattacharya
- **URL**: <http://arxiv.org/abs/2508.07592v1>
- **Submitted**: 2025-08-11 03:44:17
- **Topic Keywords**: rag
- **Reason**: The paper focuses on an AI-powered framework for bail decision-making in the Indian judicial system, which is unrelated to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on legal and judicial aspects, as well as its specific domain (Indian courts), makes it irrelevant to the user's broader interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Bail decisions are among the most frequently adjudicated matters in Indian
courts, yet they remain plagued by subjectivity, delays, and inconsistencies.
With over 75% of India's prison population comprising undertrial prisoners,
many from socioeconomically disadvantaged backgrounds, the lack of timely and
fair bail adjudication exacerbates human rights concerns and contributes to
systemic judicial backlog. In this paper, we present the Indian Bail Prediction
System (IBPS), an AI-powered framework designed to assist in bail
decision-making by predicting outcomes and generating legally sound rationales
based solely on factual case attributes and statutory provisions. We curate and
release a large-scale dataset of 150,430 High Court bail judgments, enriched
with structured annotations such as age, health, criminal history, crime
category, custody duration, statutes, and judicial reasoning. We fine-tune a
large language model using parameter-efficient techniques and evaluate its
performance across multiple configurations, with and without statutory context,
and with RAG. Our results demonstrate that models fine-tuned with statutory
knowledge significantly outperform baselines, achieving strong accuracy and
explanation quality, and generalize well to a test set independently annotated
by legal experts. IBPS offers a transparent, scalable, and reproducible
solution to support data-driven legal assistance, reduce bail delays, and
promote procedural fairness in the Indian judicial system.

### 44. Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Yunna Cai, Fan Wang, Haowei Wang, Kun Wang, Kailai Yang, Sophia Ananiadou, Moyan Li, Mingming Fan
- **URL**: <http://arxiv.org/abs/2508.08236v1>
- **Submitted**: 2025-08-11 17:52:07
- **Topic Keywords**: search
- **Reason**: The paper focuses on evaluating the safety alignment of LLM responses in mental health dialogues, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves LLMs and NLP, the topic is more specific to mental health and does not align with the user's primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Evaluating the safety alignment of LLM responses in high-risk mental health
dialogues is particularly difficult due to missing gold-standard answers and
the ethically sensitive nature of these interactions. To address this
challenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark
based on real-world Chinese mental health dialogues. It evaluates whether the
model responses align with the safety principles defined by experts.
Specifically designed for settings without standard references, our method
adopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation
using expert-defined reasoning chains grounded in psychological intervention
principles. We employ binary point-wise scoring across multiple safety
dimensions to enhance the explainability and traceability of the evaluation.
Additionally, we present a manually curated, high-quality Chinese-language
dataset covering self-harm, suicidal ideation, and existential distress,
derived from real-world online discourse. Experiments on 3600 judgments show
that our method achieves the highest agreement with expert assessments and
produces more interpretable evaluation rationales compared to existing
approaches. Our dataset and evaluation tool are publicly available to
facilitate further research.

### 45. Capabilities of GPT-5 on Multimodal Medical Reasoning

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Shansong Wang, Mingzhe Hu, Qiang Li, Mojtaba Safari, Xiaofeng Yang
- **URL**: <http://arxiv.org/abs/2508.08224v1>
- **Submitted**: 2025-08-11 17:43:45
- **Topic Keywords**: recommend
- **Reason**: The paper focuses on the capabilities of GPT-5 in multimodal medical reasoning, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on medical decision support and clinical decision-support systems is also outside the user's primary focus.

#### Abstract
> Recent advances in large language models (LLMs) have enabled general-purpose
systems to perform increasingly complex domain-specific reasoning without
extensive fine-tuning. In the medical domain, decision-making often requires
integrating heterogeneous information sources, including patient narratives,
structured data, and medical images. This study positions GPT-5 as a generalist
multimodal reasoner for medical decision support and systematically evaluates
its zero-shot chain-of-thought reasoning performance on both text-based
question answering and visual question answering tasks under a unified
protocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20
against standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU
medical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that
GPT-5 consistently outperforms all baselines, achieving state-of-the-art
accuracy across all QA benchmarks and delivering substantial gains in
multimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and
understanding scores by +29.62% and +36.18% over GPT-4o, respectively, and
surpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in
understanding. In contrast, GPT-4o remains below human expert performance in
most dimensions. A representative case study demonstrates GPT-5's ability to
integrate visual and textual cues into a coherent diagnostic reasoning chain,
recommending appropriate high-stakes interventions. Our results show that, on
these controlled multimodal reasoning benchmarks, GPT-5 moves from
human-comparable to above human-expert performance. This improvement may
substantially inform the design of future clinical decision-support systems.

### 46. Czech Dataset for Complex Aspect-Based Sentiment Analysis Tasks

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Jakub ≈†m√≠d, Pavel P≈ôib√°≈à, Ond≈ôej Pra≈æ√°k, Pavel Kr√°l
- **URL**: <http://arxiv.org/abs/2508.08125v1>
- **Submitted**: 2025-08-11 16:03:28
- **Comment**: Published In Proceedings of the 2024 Joint International Conference
  on Computational Linguistics, Language Resources and Evaluation (LREC-COLING
  2024). Official version: https://aclanthology.org/2024.lrec-main.374/
- **Topic Keywords**: search
- **Reason**: The paper focuses on aspect-based sentiment analysis, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves text analysis, the specific task and domain (restaurant reviews) are not relevant to the user's areas of interest.

#### Abstract
> In this paper, we introduce a novel Czech dataset for aspect-based sentiment
analysis (ABSA), which consists of 3.1K manually annotated reviews from the
restaurant domain. The dataset is built upon the older Czech dataset, which
contained only separate labels for the basic ABSA tasks such as aspect term
extraction or aspect polarity detection. Unlike its predecessor, our new
dataset is specifically designed for more complex tasks, e.g.
target-aspect-category detection. These advanced tasks require a unified
annotation format, seamlessly linking sentiment elements (labels) together. Our
dataset follows the format of the well-known SemEval-2016 datasets. This design
choice allows effortless application and evaluation in cross-lingual scenarios,
ultimately fostering cross-language comparisons with equivalent counterpart
datasets in other languages. The annotation process engaged two trained
annotators, yielding an impressive inter-annotator agreement rate of
approximately 90%. Additionally, we provide 24M reviews without annotations
suitable for unsupervised learning. We present robust monolingual baseline
results achieved with various Transformer-based models and insightful error
analysis to supplement our contributions. Our code and dataset are freely
available for non-commercial research purposes.

### 47. Investigating the Design Space of Visual Grounding in Multimodal Large Language Model

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Weitai Kang, Weiming Zhuang, Zhizhong Li, Yan Yan, Lingjuan Lyu
- **URL**: <http://arxiv.org/abs/2508.08066v1>
- **Submitted**: 2025-08-11 15:10:52
- **Comment**: 8 pages for the main paper
- **Topic Keywords**: search
- **Reason**: The paper focuses on visual grounding in multimodal large language models, which is not directly related to information retrieval, query understanding, or ranking models. While it explores design choices and ablation studies, the topic is more aligned with computer vision and multimodal processing, which are not core areas of interest for the user.

#### Abstract
> Fine-grained multimodal capability in Multimodal Large Language Models
(MLLMs) has emerged as a critical research direction, particularly for tackling
the visual grounding (VG) problem. Despite the strong performance achieved by
existing approaches, they often employ disparate design choices when
fine-tuning MLLMs for VG, lacking systematic verification to support these
designs. To bridge this gap, this paper presents a comprehensive study of
various design choices that impact the VG performance of MLLMs. We conduct our
analysis using LLaVA-1.5, which has been widely adopted in prior empirical
studies of MLLMs. While more recent models exist, we follow this convention to
ensure our findings remain broadly applicable and extendable to other
architectures. We cover two key aspects: (1) exploring different visual
grounding paradigms in MLLMs, identifying the most effective design, and
providing our insights; and (2) conducting ablation studies on the design of
grounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our
findings contribute to a stronger MLLM for VG, achieving improvements of +5.6%
/ +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.

### 48. 9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT 2025)

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Fabrizio Nunnari, Cristina Luna Jim√©nez, Rosalee Wolfe, John C. McDonald, Michael Filhol, Eleni Efthimiou, Evita Fotinea, Thomas Hanke
- **URL**: <http://arxiv.org/abs/2508.08050v1>
- **Submitted**: 2025-08-11 14:50:21
- **Topic Keywords**: search
- **Reason**: The paper is not related to Information Retrieval, Search technologies, or Natural Language Processing, and does not address query understanding, ranking models, or user behavior modeling. The focus is on sign language translation and avatar technologies, which is outside the user's primary research interests.

#### Abstract
> The Sign Language Translation and Avatar Technology (SLTAT) workshops
continue a series of gatherings to share recent advances in improving deaf /
human communication through non-invasive means. This 2025 edition, the 9th
since its first appearance in 2011, is hosted by the International Conference
on Intelligent Virtual Agents (IVA), giving the opportunity for contamination
between two research communities, using digital humans as either virtual
interpreters or as interactive conversational agents. As presented in this
summary paper, SLTAT sees contributions beyond avatar technologies, with a
consistent number of submissions on sign language recognition, and other work
on data collection, data analysis, tools, ethics, usability, and affective
computing.

### 49. The Medical Metaphors Corpus (MCC)

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Anna Sofia Lippolis, Andrea Giovanni Nuzzolese, Aldo Gangemi
- **URL**: <http://arxiv.org/abs/2508.07993v1>
- **Submitted**: 2025-08-11 13:55:31
- **Topic Keywords**: search
- **Reason**: The paper focuses on a specific domain (medicine and biology) and presents a dataset for metaphor detection, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on language models, it does not address ranking models or user behavior modeling, making it only loosely relevant to the user's research interests.

#### Abstract
> Metaphor is a fundamental cognitive mechanism that shapes scientific
understanding, enabling the communication of complex concepts while potentially
constraining paradigmatic thinking. Despite the prevalence of figurative
language in scientific discourse, existing metaphor detection resources
primarily focus on general-domain text, leaving a critical gap for
domain-specific applications. In this paper, we present the Medical Metaphors
Corpus (MCC), a comprehensive dataset of 792 annotated scientific conceptual
metaphors spanning medical and biological domains. MCC aggregates metaphorical
expressions from diverse sources including peer-reviewed literature, news
media, social media discourse, and crowdsourced contributions, providing both
binary and graded metaphoricity judgments validated through human annotation.
Each instance includes source-target conceptual mappings and perceived
metaphoricity scores on a 0-7 scale, establishing the first annotated resource
for computational scientific metaphor research. Our evaluation demonstrates
that state-of-the-art language models achieve modest performance on scientific
metaphor detection, revealing substantial room for improvement in
domain-specific figurative language understanding. MCC enables multiple
research applications including metaphor detection benchmarking, quality-aware
generation systems, and patient-centered communication tools.

### 50. Large Language Models for Czech Aspect-Based Sentiment Analysis

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Jakub ≈†m√≠d, Pavel P≈ôib√°≈à, Pavel Kr√°l
- **URL**: <http://arxiv.org/abs/2508.07860v1>
- **Submitted**: 2025-08-11 11:24:57
- **Comment**: Accepted for presentation at the 28th International Conference on
  Text, Speech and Dialogue (TSD 2025)
- **Topic Keywords**: search
- **Reason**: The paper focuses on aspect-based sentiment analysis, a topic in Natural Language Processing (NLP), but it does not directly relate to Information Retrieval (IR) or Search technologies, which are the user's primary research interests. The paper's emphasis on large language models and their performance on a specific task also does not align with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis
task that aims to identify sentiment toward specific aspects of an entity.
While large language models (LLMs) have shown strong performance in various
natural language processing (NLP) tasks, their capabilities for Czech ABSA
remain largely unexplored. In this work, we conduct a comprehensive evaluation
of 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their
performance in zero-shot, few-shot, and fine-tuning scenarios. Our results show
that small domain-specific models fine-tuned for ABSA outperform
general-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs
achieve state-of-the-art results. We analyze how factors such as
multilingualism, model size, and recency influence performance and present an
error analysis highlighting key challenges, particularly in aspect term
prediction. Our findings provide insights into the suitability of LLMs for
Czech ABSA and offer guidance for future research in this area.

---

