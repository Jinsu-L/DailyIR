# Daily Papers Report - 2025-12-28

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Self-attention vector output similarities reveal how machines pay attention

- **LLM Score**: 6
- **Keyword Score**: 1
- **Authors**: Tal Halevi, Yarden Tzach, Ronit D. Gross, Shalom Rosner, Ido Kanter
- **URL**: <http://arxiv.org/abs/2512.21956v1>
- **Submitted**: 2025-12-26 10:03:26
- **Comment**: 22 pages, 13 figures
- **Topic Keywords**: search
- **Reason**: This paper explores the self-attention mechanism in natural language processing, which is related to query understanding and ranking models in information retrieval. However, the focus on text segmentation and token vector similarities is somewhat tangential to the user's core research themes, which include query understanding, ranking models, and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Token‚Äëlevel vector‚Äëspace analysis of BERT‚Äë12 attention heads
- **Aim**: Quantitatively examine how each head transforms token embeddings across layers, moving beyond weight‚Äëcentric explanations
- **Rationale**: Attention weights miss dynamics of output vectors; a vector‚Äëspace view is needed to capture learning progress and head cooperation
- **Ground**: Pre‚Äëtrained HuggingFace BERT‚Äë12 (12 layers, 12 heads), 128‚Äëtoken sequences, 20k sampled texts, PyTorch on Colab GPUs; similarity matrices constructed by dot‚Äëproduct, normalization, thresholding at 0.3
- **Experiment**: For each head, compute a 128√ó128 similarity matrix from its 128 token vectors, zero diagonal, normalize by max, threshold; sum high‚Äësimilarity elements across heads per layer to obtain a context‚Äësimilarity matrix; analyze layer‚Äëwise patterns‚Äîearly diffuse, middle diagonal, late column‚Äëoriented; observe head specialization, intra‚Äësentence bias (~33% of high‚Äësimilarity pairs within same sentence), noise/SNR issues
- **Takeaway**: Heads develop distinct token‚Äëcentric similarity networks that sharpen from global to local, culminating in sentence‚Äëseparator focus; the quantitative framework complements weight‚Äëbased analyses, informs pruning, long‚Äërange architecture design, and noise reduction strategies

#### Abstract
> The self-attention mechanism has significantly advanced the field of natural language processing, facilitating the development of advanced language-learning machines. Although its utility is widely acknowledged, the precise mechanisms of self-attention underlying its advanced learning and the quantitative characterization of this learning process remains an open research question. This study introduces a new approach for quantifying information processing within the self-attention mechanism. The analysis conducted on the BERT-12 architecture reveals that, in the final layers, the attention map focuses on sentence separator tokens, suggesting a practical approach to text segmentation based on semantic features. Based on the vector space emerging from the self-attention heads, a context similarity matrix, measuring the scalar product between two token vectors was derived, revealing distinct similarities between different token vector pairs within each head and layer. The findings demonstrated that different attention heads within an attention block focused on different linguistic characteristics, such as identifying token repetitions in a given text or recognizing a token of common appearance in the text and its surrounding context. This specialization is also reflected in the distribution of distances between token vectors with high similarity as the architecture progresses. The initial attention layers exhibit substantially long-range similarities; however, as the layers progress, a more short-range similarity develops, culminating in a preference for attention heads to create strong similarities within the same sentence. Finally, the behavior of individual heads was analyzed by examining the uniqueness of their most common tokens in their high similarity elements. Each head tends to focus on a unique token from the text and builds similarity pairs centered around it.

---

### 2. AutoPP: Towards Automated Product Poster Generation and Optimization

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Jiahao Fan, Yuxin Qin, Wei Feng, Yanyin Chen, Yaoyu Li, Ao Ma, Yixiu Li, Li Zhuang, Haoyi Bian, Zheng Zhang, Jingjing Lv, Junjie Shen, Ching Law
- **URL**: <http://arxiv.org/abs/2512.21921v1>
- **Submitted**: 2025-12-26 08:30:32
- **Comment**: Accepted to AAAI 2026
- **Topic Keywords**: rag, click, ctr, click-through rate
- **Reason**: The paper discusses an automated pipeline for product poster generation and optimization, leveraging online feedback to enhance Click-Through Rate (CTR). While it touches on user behavior modeling through CTR, its primary focus is on product poster generation, which is somewhat related to information retrieval, but not directly aligned with your core research themes.

#### Abstract
> Product posters blend striking visuals with informative text to highlight the product and capture customer attention. However, crafting appealing posters and manually optimizing them based on online performance is laborious and resource-consuming. To address this, we introduce AutoPP, an automated pipeline for product poster generation and optimization that eliminates the need for human intervention. Specifically, the generator, relying solely on basic product information, first uses a unified design module to integrate the three key elements of a poster (background, text, and layout) into a cohesive output. Then, an element rendering module encodes these elements into condition tokens, efficiently and controllably generating the product poster. Based on the generated poster, the optimizer enhances its Click-Through Rate (CTR) by leveraging online feedback. It systematically replaces elements to gather fine-grained CTR comparisons and utilizes Isolated Direct Preference Optimization (IDPO) to attribute CTR gains to isolated elements. Our work is supported by AutoPP1M, the largest dataset specifically designed for product poster generation and optimization, which contains one million high-quality posters and feedback collected from over one million users. Experiments demonstrate that AutoPP achieves state-of-the-art results in both offline and online settings. Our code and dataset are publicly available at: https://github.com/JD-GenX/AutoPP

---

### 3. Knowledge Reasoning of Large Language Models Integrating Graph-Structured Information for Pest and Disease Control in Tobacco

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Siyu Li, Chenwei Song, Wan Zhou, Xinyi Liu
- **URL**: <http://arxiv.org/abs/2512.21837v1>
- **Submitted**: 2025-12-26 02:48:38
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper explores the application of large language models for knowledge reasoning in a specific domain (pest and disease control in tobacco). Although it involves query understanding and ranking models, the focus is on knowledge graph-based reasoning rather than traditional information retrieval or search technologies. The paper's relevance to the user's core research themes is somewhat limited.

#### Abstract
> This paper proposes a large language model (LLM) approach that integrates graph-structured information for knowledge reasoning in tobacco pest and disease control. Built upon the GraphRAG framework, the proposed method enhances knowledge retrieval and reasoning by explicitly incorporating structured information from a domain-specific knowledge graph. Specifically, LLMs are first leveraged to assist in the construction of a tobacco pest and disease knowledge graph, which organizes key entities such as diseases, symptoms, control methods, and their relationships. Based on this graph, relevant knowledge is retrieved and integrated into the reasoning process to support accurate answer generation. The Transformer architecture is adopted as the core inference model, while a graph neural network (GNN) is employed to learn expressive node representations that capture both local and global relational information within the knowledge graph. A ChatGLM-based model serves as the backbone LLM and is fine-tuned using LoRA to achieve parameter-efficient adaptation. Extensive experimental results demonstrate that the proposed approach consistently outperforms baseline methods across multiple evaluation metrics, significantly improving both the accuracy and depth of reasoning, particularly in complex multi-hop and comparative reasoning scenarios.

---

### 4. Explainable Statute Prediction via Attention-based Model and LLM Prompting

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Sachin Pawar, Girish Keshav Palshikar, Anindita Sinha Banerjee, Nitin Ramrakhiyani, Basit Ali
- **URL**: <http://arxiv.org/abs/2512.21902v1>
- **Submitted**: 2025-12-26 07:29:51
- **Topic Keywords**: relevance
- **Reason**: The paper explores statute prediction with explanations, which is a specific application of information retrieval. While it involves natural language processing and attention-based models, it doesn't directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's focus on legal domain and AI-assistant applications is somewhat tangential to the user's primary research themes.

#### Abstract
> In this paper, we explore the problem of automatic statute prediction where for a given case description, a subset of relevant statutes are to be predicted. Here, the term "statute" refers to a section, a sub-section, or an article of any specific Act. Addressing this problem would be useful in several applications such as AI-assistant for lawyers and legal question answering system. For better user acceptance of such Legal AI systems, we believe the predictions should also be accompanied by human understandable explanations. We propose two techniques for addressing this problem of statute prediction with explanations -- (i) AoS (Attention-over-Sentences) which uses attention over sentences in a case description to predict statutes relevant for it and (ii) LLMPrompt which prompts an LLM to predict as well as explain relevance of a certain statute. AoS uses smaller language models, specifically sentence transformers and is trained in a supervised manner whereas LLMPrompt uses larger language models in a zero-shot manner and explores both standard as well as Chain-of-Thought (CoT) prompting techniques. Both these models produce explanations for their predictions in human understandable forms. We compare statute prediction performance of both the proposed techniques with each other as well as with a set of competent baselines, across two popular datasets. Also, we evaluate the quality of the generated explanations through an automated counter-factual manner as well as through human evaluation.

---

### 5. Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Duygu Altinok
- **URL**: <http://arxiv.org/abs/2512.22100v1>
- **Submitted**: 2025-12-26 18:02:09
- **Comment**: under review by Springer
- **Topic Keywords**: search
- **Reason**: The paper introduces a comprehensive benchmark for Turkish Natural Language Understanding (NLU) and sentiment analysis, which is related to the user's interests in NLP and data mining. However, the focus on the Turkish language and the specific benchmark development do not directly align with the user's primary research themes in Information Retrieval and Search technologies.

#### Abstract
> Evaluating the performance of various model architectures, such as transformers, large language models (LLMs), and other NLP systems, requires comprehensive benchmarks that measure performance across multiple dimensions. Among these, the evaluation of natural language understanding (NLU) is particularly critical as it serves as a fundamental criterion for assessing model capabilities. Thus, it is essential to establish benchmarks that enable thorough evaluation and analysis of NLU abilities from diverse perspectives. While the GLUE benchmark has set a standard for evaluating English NLU, similar benchmarks have been developed for other languages, such as CLUE for Chinese, FLUE for French, and JGLUE for Japanese. However, no comparable benchmark currently exists for the Turkish language. To address this gap, we introduce TrGLUE, a comprehensive benchmark encompassing a variety of NLU tasks for Turkish. In addition, we present SentiTurca, a specialized benchmark for sentiment analysis. To support researchers, we also provide fine-tuning and evaluation code for transformer-based models, facilitating the effective use of these benchmarks. TrGLUE comprises Turkish-native corpora curated to mirror the domains and task formulations of GLUE-style evaluations, with labels obtained through a semi-automated pipeline that combines strong LLM-based annotation, cross-model agreement checks, and subsequent human validation. This design prioritizes linguistic naturalness, minimizes direct translation artifacts, and yields a scalable, reproducible workflow. With TrGLUE, our goal is to establish a robust evaluation framework for Turkish NLU, empower researchers with valuable resources, and provide insights into generating high-quality semi-automated datasets.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Frozen LVLMs for Micro-Video Recommendation: A Systematic Study of Feature Extraction and Fusion

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Huatuan Sun, Yunshan Ma, Changguang Wu, Yanxin Zhang, Pengfei Wang, Xiaoyu Du
- **URL**: <http://arxiv.org/abs/2512.21863v1>
- **Submitted**: 2025-12-26 04:56:28
- **Comment**: 10 pages, 4 figures
- **Topic Keywords**: recommend
- **Reason**: The paper explores the application of Large Video Language Models (LVLMs) in micro-video recommendation, which is somewhat related to information retrieval and search technologies. However, the focus on recommender systems and the specific use case of micro-video recommendation limits its relevance to the user's core research themes, particularly query understanding, ranking models, and user behavior modeling.

#### Abstract
> Frozen Large Video Language Models (LVLMs) are increasingly employed in micro-video recommendation due to their strong multimodal understanding. However, their integration lacks systematic empirical evaluation: practitioners typically deploy LVLMs as fixed black-box feature extractors without systematically comparing alternative representation strategies. To address this gap, we present the first systematic empirical study along two key design dimensions: (i) integration strategies with ID embeddings, specifically replacement versus fusion, and (ii) feature extraction paradigms, comparing LVLM-generated captions with intermediate decoder hidden states. Extensive experiments on representative LVLMs reveal three key principles: (1) intermediate hidden states consistently outperform caption-based representations, as natural-language summarization inevitably discards fine-grained visual semantics crucial for recommendation; (2) ID embeddings capture irreplaceable collaborative signals, rendering fusion strictly superior to replacement; and (3) the effectiveness of intermediate decoder features varies significantly across layers. Guided by these insights, we propose the Dual Feature Fusion (DFF) Framework, a lightweight and plug-and-play approach that adaptively fuses multi-layer representations from frozen LVLMs with item ID embeddings. DFF achieves state-of-the-art performance on two real-world micro-video recommendation benchmarks, consistently outperforming strong baselines and providing a principled approach to integrating off-the-shelf large vision-language models into micro-video recommender systems.

### 7. On The Conceptualization and Societal Impact of Cross-Cultural Bias

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Vitthal Bhandari
- **URL**: <http://arxiv.org/abs/2512.21809v1>
- **Submitted**: 2025-12-26 00:27:53
- **Comment**: Term paper for LING 575 (Societal Impacts of Language Technologies)
- **Topic Keywords**: search
- **Reason**: This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and its societal impact, but it does not directly align with the user's core research themes in Information Retrieval (IR), query understanding, ranking models, or user behavior modeling.

#### Abstract
> Research has shown that while large language models (LLMs) can generate their responses based on cultural context, they are not perfect and tend to generalize across cultures. However, when evaluating the cultural bias of a language technology on any dataset, researchers may choose not to engage with stakeholders actually using that technology in real life, which evades the very fundamental problem they set out to address.
  Inspired by the work done by arXiv:2005.14050v2, I set out to analyse recent literature about identifying and evaluating cultural bias in Natural Language Processing (NLP). I picked out 20 papers published in 2025 about cultural bias and came up with a set of observations to allow NLP researchers in the future to conceptualize bias concretely and evaluate its harms effectively. My aim is to advocate for a robust assessment of the societal impact of language technologies exhibiting cross-cultural bias.

### 8. KG20C & KG20C-QA: Scholarly Knowledge Graph Benchmarks for Link Prediction and Question Answering

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Hung-Nghiep Tran, Atsuhiro Takasu
- **URL**: <http://arxiv.org/abs/2512.21799v1>
- **Submitted**: 2025-12-25 22:29:54
- **Comment**: Extracted and extended from the first author's PhD thesis titled "Multi-Relational Embedding for Knowledge Graph Representation and Analysis"
- **Topic Keywords**: search
- **Reason**: This paper presents a knowledge graph benchmark for question answering on scholarly data, which is somewhat related to information retrieval and NLP. However, it focuses on knowledge graph embeddings and QA tasks, which is not a central match to the user's primary interests in query understanding, ranking models, and user behavior modeling. The paper's relevance to recommender systems is also limited.

#### Abstract
> In this paper, we present KG20C and KG20C-QA, two curated datasets for advancing question answering (QA) research on scholarly data. KG20C is a high-quality scholarly knowledge graph constructed from the Microsoft Academic Graph through targeted selection of venues, quality-based filtering, and schema definition. Although KG20C has been available online in non-peer-reviewed sources such as GitHub repository, this paper provides the first formal, peer-reviewed description of the dataset, including clear documentation of its construction and specifications. KG20C-QA is built upon KG20C to support QA tasks on scholarly data. We define a set of QA templates that convert graph triples into natural language question--answer pairs, producing a benchmark that can be used both with graph-based models such as knowledge graph embeddings and with text-based models such as large language models. We benchmark standard knowledge graph embedding methods on KG20C-QA, analyze performance across relation types, and provide reproducible evaluation protocols. By officially releasing these datasets with thorough documentation, we aim to contribute a reusable, extensible resource for the research community, enabling future work in QA, reasoning, and knowledge-driven applications in the scholarly domain. The full datasets will be released at https://github.com/tranhungnghiep/KG20C/ upon paper publication.

### 9. Bridging the Copyright Gap: Do Large Vision-Language Models Recognize and Respect Copyrighted Content?

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Naen Xu, Jinghuai Zhang, Changjiang Li, Hengyu An, Chunyi Zhou, Jun Wang, Boyu Xu, Yuyuan Li, Tianyu Du, Shouling Ji
- **URL**: <http://arxiv.org/abs/2512.21871v1>
- **Submitted**: 2025-12-26 05:09:55
- **Comment**: AAAI 2026 (Oral)
- **Topic Keywords**: query, queries
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it touches on the concept of multimodal reasoning, the primary focus is on copyright compliance in vision-language models, which is not a central theme in your research.

#### Abstract
> Large vision-language models (LVLMs) have achieved remarkable advancements in multimodal reasoning tasks. However, their widespread accessibility raises critical concerns about potential copyright infringement. Will LVLMs accurately recognize and comply with copyright regulations when encountering copyrighted content (i.e., user input, retrieved documents) in the context? Failure to comply with copyright regulations may lead to serious legal and ethical consequences, particularly when LVLMs generate responses based on copyrighted materials (e.g., retrieved book experts, news reports). In this paper, we present a comprehensive evaluation of various LVLMs, examining how they handle copyrighted content -- such as book excerpts, news articles, music lyrics, and code documentation when they are presented as visual inputs. To systematically measure copyright compliance, we introduce a large-scale benchmark dataset comprising 50,000 multimodal query-content pairs designed to evaluate how effectively LVLMs handle queries that could lead to copyright infringement. Given that real-world copyrighted content may or may not include a copyright notice, the dataset includes query-content pairs in two distinct scenarios: with and without a copyright notice. For the former, we extensively cover four types of copyright notices to account for different cases. Our evaluation reveals that even state-of-the-art closed-source LVLMs exhibit significant deficiencies in recognizing and respecting the copyrighted content, even when presented with the copyright notice. To solve this limitation, we introduce a novel tool-augmented defense framework for copyright compliance, which reduces infringement risks in all scenarios. Our findings underscore the importance of developing copyright-aware LVLMs to ensure the responsible and lawful use of copyrighted content.

### 10. CricBench: A Multilingual Benchmark for Evaluating LLMs in Cricket Analytics

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Vaibhav Devraj, Dhruv Kumar, Jagat Sesh Challa
- **URL**: <http://arxiv.org/abs/2512.21877v1>
- **Submitted**: 2025-12-26 05:59:19
- **Comment**: Under Review
- **Topic Keywords**: queries, web search, search
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, and Natural Language Processing, as it focuses on Large Language Models in Cricket Analytics, which is a specialized domain with a narrow scope.

#### Abstract
> Cricket is the second most popular sport globally, commanding a massive following of over 2.5 billion fans globally. Enthusiasts and analysts frequently seek advanced statistical insights, such as long-term historical performance trends or complex player comparisons, that are often unavailable through standard web searches. While Large Language Models (LLMs) have advanced significantly in Text-to-SQL tasks, their capability to handle the domain-specific nuances, complex schema variations, and multilingual requirements inherent to sports analytics remains under-explored. To investigate this potential capability gap, we present CricBench, a comprehensive benchmark suite for evaluating LLMs on specialized cricket data. To curate a "Gold Standard" dataset, we collaborate with domain experts in cricket and SQL to manually author complex queries, ensuring logical correctness. Recognizing linguistic diversity, we construct the benchmark in both English and Hindi, establishing a framework that is open for further extension to other regional languages. We evaluate six state-of-the-art models, including GPT-4o, Claude 3.7 Sonnet, and open-source models, using a strict evaluation protocol. Our results reveal that high performance on general benchmarks does not guarantee success in specialized domains. While the open-weights reasoning model DeepSeek R1 achieves state-of-the-art performance (50.6%), surpassing proprietary giants like Claude 3.7 Sonnet (47.7%) and GPT-4o (33.7%), it still exhibits a significant accuracy drop when moving from general benchmarks (BIRD) to CricBench. Furthermore, we observe that code-mixed Hindi queries frequently yield parity or higher accuracy compared to English, challenging the assumption that English is the optimal prompt language for specialized SQL tasks.

### 11. A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Shuyu Gan, Renxiang Wang, James Mooney, Dongyeop Kang
- **URL**: <http://arxiv.org/abs/2512.22101v1>
- **Submitted**: 2025-12-26 18:02:12
- **Comment**: 3 pages, 3 figures; Accepted by 1st Workshop on GenAI, Agents and the Future of VIS as Mini-challenge paper and win the Honorable Mention award. Submit number is 7597 and the paper is archived on the workshop website: https://visxgenai.github.io/subs-2025/7597/7597-doc.pdf
- **Topic Keywords**: rank, www
- **Reason**: This paper focuses on automating data science pipeline with AI agents, which is not directly related to Information Retrieval, Search technologies, or Natural Language Processing. Although it mentions real-time relevance optimization, it's in the context of data analysis and visualization, not search or retrieval.

#### Abstract
> Automating end-to-end data science pipeline with AI agents still stalls on two gaps: generating insightful, diverse visual evidence and assembling it into a coherent, professional report. We present A2P-Vis, a two-part, multi-agent pipeline that turns raw datasets into a high-quality data-visualization report. The Data Analyzer orchestrates profiling, proposes diverse visualization directions, generates and executes plotting code, filters low-quality figures with a legibility checker, and elicits candidate insights that are automatically scored for depth, correctness, specificity, depth and actionability. The Presenter then orders topics, composes chart-grounded narratives from the top-ranked insights, writes justified transitions, and revises the document for clarity and consistency, yielding a coherent, publication-ready report. Together, these agents convert raw data into curated materials (charts + vetted insights) and into a readable narrative without manual glue work. We claim that by coupling a quality-assured Analyzer with a narrative Presenter, A2P-Vis operationalizes co-analysis end-to-end, improving the real-world usefulness of automated data analysis for practitioners. For the complete dataset report, please see: https://www.visagent.org/api/output/f2a3486d-2c3b-4825-98d4-5af25a819f56.

### 12. Accelerate Speculative Decoding with Sparse Computation in Verification

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jikai Wang, Jianchao Tan, Yuxuan Hu, Jiayu Qin, Yerui Sun, Yuchen Xie, Xunliang Cai, Juntao Li, Min Zhang
- **URL**: <http://arxiv.org/abs/2512.21911v1>
- **Submitted**: 2025-12-26 07:53:41
- **Comment**: Pre-print
- **Topic Keywords**: retrieval
- **Reason**: This paper focuses on accelerating speculative decoding in language models, which is not directly related to information retrieval, search technologies, or query understanding. While it involves NLP, the primary focus is on computational efficiency and sparse computation, rather than deep semantic understanding or real-time relevance optimization.

#### Abstract
> Speculative decoding accelerates autoregressive language model inference by verifying multiple draft tokens in parallel. However, the verification stage often becomes the dominant computational bottleneck, especially for long-context inputs and mixture-of-experts (MoE) models. Existing sparsification methods are designed primarily for standard token-by-token autoregressive decoding to remove substantial computational redundancy in LLMs. This work systematically adopts different sparse methods on the verification stage of the speculative decoding and identifies structured redundancy across multiple dimensions. Based on these observations, we propose a sparse verification framework that jointly sparsifies attention, FFN, and MoE components during the verification stage to reduce the dominant computation cost. The framework further incorporates an inter-draft token and inter-layer retrieval reuse strategy to further reduce redundant computation without introducing additional training. Extensive experiments across summarization, question answering, and mathematical reasoning datasets demonstrate that the proposed methods achieve favorable efficiency-accuracy trade-offs, while maintaining stable acceptance length.

### 13. AlignAR: Generative Sentence Alignment for Arabic-English Parallel Corpora of Legal and Literary Texts

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Baorong Huang, Ali Asiri
- **URL**: <http://arxiv.org/abs/2512.21842v1>
- **Submitted**: 2025-12-26 03:10:43
- **Topic Keywords**: search
- **Reason**: This paper focuses on machine translation and parallel corpora alignment, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing, particularly in areas requiring deep semantic understanding and real-time relevance optimization.

#### Abstract
> High-quality parallel corpora are essential for Machine Translation (MT) research and translation teaching. However, Arabic-English resources remain scarce and existing datasets mainly consist of simple one-to-one mappings. In this paper, we present AlignAR, a generative sentence alignment method, and a new Arabic-English dataset comprising complex legal and literary texts. Our evaluation demonstrates that "Easy" datasets lack the discriminatory power to fully assess alignment methods. By reducing one-to-one mappings in our "Hard" subset, we exposed the limitations of traditional alignment methods. In contrast, LLM-based approaches demonstrated superior robustness, achieving an overall F1-score of 85.5%, a 9% improvement over previous methods. Our datasets and codes are open-sourced at https://github.com/XXX.

### 14. Five Years of SciCap: What We Learned and Future Directions for Scientific Figure Captioning

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Ting-Hao K. Huang, Ryan A. Rossi, Sungchul Kim, Tong Yu, Ting-Yao E. Hsu, Ho Yin, Ng, C. Lee Giles
- **URL**: <http://arxiv.org/abs/2512.21789v1>
- **Submitted**: 2025-12-25 21:39:10
- **Comment**: Accepted to the 5th Annual AAAI Workshop on AI to Accelerate Science and Engineering (AI2ASE 2026)
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your core research themes in Information Retrieval, Search technologies, or Natural Language Processing, as it focuses on scientific figure captioning, which is a niche area in NLP. While it involves large language models, it does not address query understanding, ranking models, or user behavior modeling, making it only loosely related to your interests.

#### Abstract
> Between 2021 and 2025, the SciCap project grew from a small seed-funded idea at The Pennsylvania State University (Penn State) into one of the central efforts shaping the scientific figure-captioning landscape. Supported by a Penn State seed grant, Adobe, and the Alfred P. Sloan Foundation, what began as our attempt to test whether domain-specific training, which was successful in text models like SciBERT, could also work for figure captions expanded into a multi-institution collaboration. Over these five years, we curated, released, and continually updated a large collection of figure-caption pairs from arXiv papers, conducted extensive automatic and human evaluations on both generated and author-written captions, navigated the rapid rise of large language models (LLMs), launched annual challenges, and built interactive systems that help scientists write better captions. In this piece, we look back at the first five years of SciCap and summarize the key technical and methodological lessons we learned. We then outline five major unsolved challenges and propose directions for the next phase of research in scientific figure captioning.

---

