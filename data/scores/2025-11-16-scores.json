[
    {
        "title": "MOON Embedding: Multimodal Representation Learning for E-commerce Search Advertising",
        "abstract": "We introduce MOON, our comprehensive set of sustainable iterative practices for multimodal representation learning for e-commerce applications. MOON has already been fully deployed across all stages of Taobao search advertising system, including retrieval, relevance, ranking, and so on. The performance gains are particularly significant on click-through rate (CTR) prediction task, which achieves an overall +20.00% online CTR improvement. Over the past three years, this project has delivered the largest improvement on CTR prediction task and undergone five full-scale iterations. Throughout the exploration and iteration of our MOON, we have accumulated valuable insights and practical experience that we believe will benefit the research community. MOON contains a three-stage training paradigm of \"Pretraining, Post-training, and Application\", allowing effective integration of multimodal representations with downstream tasks. Notably, to bridge the misalignment between the objectives of multimodal representation learning and downstream training, we define the exchange rate to quantify how effectively improvements in an intermediate metric can translate into downstream gains. Through this analysis, we identify the image-based search recall as a critical intermediate metric guiding the optimization of multimodal models. Over three years and five iterations, MOON has evolved along four critical dimensions: data processing, training strategy, model architecture, and downstream application. The lessons and insights gained through the iterative improvements will also be shared. As part of our exploration into scaling effects in the e-commerce field, we further conduct a systematic study of the scaling laws governing multimodal representation learning, examining multiple factors such as the number of training tokens, negative samples, and the length of user behavior sequences.",
        "url": "http://arxiv.org/abs/2511.11305v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11305v1",
        "arxiv_id": "2511.11305v1",
        "authors": [
            "Chenghan Fu",
            "Daoze Zhang",
            "Yukang Lin",
            "Zhanheng Nie",
            "Xiang Zhang",
            "Jianyu Liu",
            "Yueran Liu",
            "Wanxian Guan",
            "Pengjie Wang",
            "Jian Xu",
            "Bo Zheng"
        ],
        "submitted": "2025-11-14 13:49:56",
        "source": "arxiv",
        "comment": "31 pages, 12 figures",
        "score": 20,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'user behavior' (score: +2)",
            "Found 'click' (score: +2)",
            "Found 'ctr' (score: +2)",
            "Found 'click-through rate' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 9,
        "llm_reason": "The paper focuses on multimodal embeddings applied to e‑commerce search, covering retrieval, relevance, ranking, and CTR prediction—core IR and ranking topics. It also discusses user behavior sequences and click‑through modeling, aligning well with the user’s interest in click models and real‑time relevance optimization. The work’s emphasis on deep semantic understanding via multimodal learning and its deployment in a large‑scale search advertising system make it highly relevant."
    },
    {
        "title": "From Fact to Judgment: Investigating the Impact of Task Framing on LLM Conviction in Dialogue Systems",
        "abstract": "LLMs are increasingly employed as judges across a variety of tasks, including those involving everyday social interactions. Yet, it remains unclear whether such LLM-judges can reliably assess tasks that require social or conversational judgment. We investigate how an LLM's conviction is changed when a task is reframed from a direct factual query to a Conversational Judgment Task. Our evaluation framework contrasts the model's performance on direct factual queries with its assessment of a speaker's correctness when the same information is presented within a minimal dialogue, effectively shifting the query from \"Is this statement correct?\" to \"Is this speaker correct?\". Furthermore, we apply pressure in the form of a simple rebuttal (\"The previous answer is incorrect.\") to both conditions. This perturbation allows us to measure how firmly the model maintains its position under conversational pressure. Our findings show that while some models like GPT-4o-mini reveal sycophantic tendencies under social framing tasks, others like Llama-8B-Instruct become overly-critical. We observe an average performance change of 9.24% across all models, demonstrating that even minimal dialogue context can significantly alter model judgment, underscoring conversational framing as a key factor in LLM-based evaluation. The proposed framework offers a reproducible methodology for diagnosing model conviction and contributes to the development of more trustworthy dialogue systems.",
        "url": "http://arxiv.org/abs/2511.10871v1",
        "pdf_url": "https://arxiv.org/pdf/2511.10871v1",
        "arxiv_id": "2511.10871v1",
        "authors": [
            "Parisa Rabbani",
            "Nimet Beyza Bozdag",
            "Dilek Hakkani-Tür"
        ],
        "submitted": "2025-11-14 00:55:28",
        "source": "arxiv",
        "comment": "11 pages, 3 figures. Under review at IWSDS 2026",
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the impact of conversational framing on LLM conviction in dialogue systems, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on LLMs and conversational judgment is not directly aligned with the user's primary research interests in IR and Search technologies. The paper's emphasis on NLP and dialogue systems is somewhat relevant, but it does not strongly connect to the user's core research themes."
    },
    {
        "title": "PRSM: A Measure to Evaluate CLIP's Robustness Against Paraphrases",
        "abstract": "Contrastive Language-Image Pre-training (CLIP) is a widely used multimodal model that aligns text and image representations through large-scale training. While it performs strongly on zero-shot and few-shot tasks, its robustness to linguistic variation, particularly paraphrasing, remains underexplored. Paraphrase robustness is essential for reliable deployment, especially in socially sensitive contexts where inconsistent representations can amplify demographic biases. In this paper, we introduce the Paraphrase Ranking Stability Metric (PRSM), a novel measure for quantifying CLIP's sensitivity to paraphrased queries. Using the Social Counterfactuals dataset, a benchmark designed to reveal social and demographic biases, we empirically assess CLIP's stability under paraphrastic variation, examine the interaction between paraphrase robustness and gender, and discuss implications for fairness and equitable deployment of multimodal systems. Our analysis reveals that robustness varies across paraphrasing strategies, with subtle yet consistent differences observed between male- and female-associated queries.",
        "url": "http://arxiv.org/abs/2511.11141v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11141v1",
        "arxiv_id": "2511.11141v1",
        "authors": [
            "Udo Schlegel",
            "Franziska Weeber",
            "Jian Lan",
            "Thomas Seidl"
        ],
        "submitted": "2025-11-14 10:19:04",
        "source": "arxiv",
        "comment": "8 pages, accpeted as short paper at MMM 2026",
        "score": 7,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the robustness of CLIP against paraphrases, which is related to query understanding and ranking models in Information Retrieval. However, the focus on multimodal models and linguistic variation is not directly aligned with the user's primary research themes, and the paper's emphasis on fairness and equitable deployment is more tangentially related to the user's interests in search technologies and user behavior modeling."
    },
    {
        "title": "LEMUR: Large scale End-to-end MUltimodal Recommendation",
        "abstract": "Traditional ID-based recommender systems often struggle with cold-start and generalization challenges. Multimodal recommendation systems, which leverage textual and visual data, offer a promising solution to mitigate these issues. However, existing industrial approaches typically adopt a two-stage training paradigm: first pretraining a multimodal model, then applying its frozen representations to train the recommendation model. This decoupled framework suffers from misalignment between multimodal learning and recommendation objectives, as well as an inability to adapt dynamically to new data. To address these limitations, we propose LEMUR, the first large-scale multimodal recommender system trained end-to-end from raw data. By jointly optimizing both the multimodal and recommendation components, LEMUR ensures tighter alignment with downstream objectives while enabling real-time parameter updates. Constructing multimodal sequential representations from user history often entails prohibitively high computational costs. To alleviate this bottleneck, we propose a novel memory bank mechanism that incrementally accumulates historical multimodal representations throughout the training process. After one month of deployment in Douyin Search, LEMUR has led to a 0.843% reduction in query change rate decay and a 0.81% improvement in QAUC. Additionally, LEMUR has shown significant gains across key offline metrics for Douyin Advertisement. Our results validate the superiority of end-to-end multimodal recommendation in real-world industrial scenarios.",
        "url": "http://arxiv.org/abs/2511.10962v1",
        "pdf_url": "https://arxiv.org/pdf/2511.10962v1",
        "arxiv_id": "2511.10962v1",
        "authors": [
            "Xintian Han",
            "Honggang Chen",
            "Quan Lin",
            "Jingyue Gao",
            "Xiangyuan Ren",
            "Lifei Zhu",
            "Zhisheng Ye",
            "Shikang Wu",
            "XiongHang Xie",
            "Xiaochu Gan",
            "Bingzheng Wei",
            "Peng Xu",
            "Zhe Wang",
            "Yuchao Zheng",
            "Jingjian Lin",
            "Di Wu",
            "Junfeng Ge"
        ],
        "submitted": "2025-11-14 05:15:15",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on multimodal recommendation systems, which is a related topic to information retrieval. However, the emphasis on recommender systems and the specific application in Douyin Search and Advertisement limits its relevance to the user's primary focus on query understanding, ranking models, and user behavior modeling in the e-commerce domain."
    },
    {
        "title": "When Data is the Algorithm: A Systematic Study and Curation of Preference Optimization Datasets",
        "abstract": "Aligning large language models (LLMs) is a central objective of post-training, often achieved through reward modeling and reinforcement learning methods. Among these, direct preference optimization (DPO) has emerged as a widely adopted technique that fine-tunes LLMs on preferred completions over less favorable ones. While most frontier LLMs do not disclose their curated preference pairs, the broader LLM community has released several open-source DPO datasets, including TuluDPO, ORPO, UltraFeedback, HelpSteer, and Code-Preference-Pairs. However, systematic comparisons remain scarce, largely due to the high computational cost and the lack of rich quality annotations, making it difficult to understand how preferences were selected, which task types they span, and how well they reflect human judgment on a per-sample level. In this work, we present the first comprehensive, data-centric analysis of popular open-source DPO corpora. We leverage the Magpie framework to annotate each sample for task category, input quality, and preference reward, a reward-model-based signal that validates the preference order without relying on human annotations. This enables a scalable, fine-grained inspection of preference quality across datasets, revealing structural and qualitative discrepancies in reward margins. Building on these insights, we systematically curate a new DPO mixture, UltraMix, that draws selectively from all five corpora while removing noisy or redundant samples. UltraMix is 30% smaller than the best-performing individual dataset yet exceeds its performance across key benchmarks. We publicly release all annotations, metadata, and our curated mixture to facilitate future research in data-centric preference optimization.",
        "url": "http://arxiv.org/abs/2511.10985v1",
        "pdf_url": "https://arxiv.org/pdf/2511.10985v1",
        "arxiv_id": "2511.10985v1",
        "authors": [
            "Aladin Djuhera",
            "Farhan Ahmed",
            "Swanand Ravindra Kadhe",
            "Syed Zawad",
            "Heiko Ludwig",
            "Holger Boche"
        ],
        "submitted": "2025-11-14 06:12:16",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'ltr' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to the user's interests in Information Retrieval and Natural Language Processing, particularly in the context of preference optimization and large language models. However, the focus on preference optimization datasets and their analysis is not directly aligned with the user's primary research themes of query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Unlocking Advanced Graph Machine Learning Insights through Knowledge Completion on Neo4j Graph Database",
        "abstract": "Graph Machine Learning (GML) with Graph Databases (GDBs) has gained significant relevance in recent years, due to its ability to handle complex interconnected data and apply ML techniques using Graph Data Science (GDS). However, a critical gap exists in the current way GDB-GML applications analyze data, especially in terms of Knowledge Completion (KC) in Knowledge Graphs (KGs). In particular, current architectures ignore KC, working on datasets that appear incomplete or fragmented, despite they actually contain valuable hidden knowledge. This limitation may cause wrong interpretations when these data are used as input for GML models.\n  This paper proposes an innovative architecture that integrates a KC phase into GDB-GML applications, demonstrating how revealing hidden knowledge can heavily impact datasets' behavior and metrics. For this purpose, we introduce scalable transitive relationships, which are links that propagate information over the network and modelled by a decay function, allowing a deterministic knowledge flows across multiple nodes.\n  Experimental results demonstrate that our intuition radically reshapes both topology and overall dataset dynamics, underscoring the need for this new GDB-GML architecture to produce better models and unlock the full potential of graph-based data analysis.",
        "url": "http://arxiv.org/abs/2511.11399v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11399v1",
        "arxiv_id": "2511.11399v1",
        "authors": [
            "Rosario Napoli",
            "Antonio Celesti",
            "Massimo Villari",
            "Maria Fazio"
        ],
        "submitted": "2025-11-14 15:27:31",
        "source": "arxiv",
        "comment": "Accepted at the 30th IEEE Symposium on Computers and Communications (ISCC) 2025",
        "score": 5,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Graph Machine Learning and Knowledge Completion on Neo4j Graph Database, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on data analysis, it lacks relevance to your specific areas of interest such as query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference",
        "abstract": "Large Language Model (LLM) agent systems have advanced rapidly, driven by their strong generalization in zero-shot settings. To further enhance reasoning and accuracy on complex tasks, Multi-Agent Debate (MAD) has emerged as a promising framework that engages multiple LLM agents in structured debates to encourage diverse reasoning. However, triggering MAD for every query is inefficient, as it incurs substantial computational (token) cost and may even degrade accuracy by overturning correct single-agent answers. To address these limitations, we propose intelligent Multi-Agent Debate (iMAD), a token-efficient framework that selectively triggers MAD only when it is likely to be beneficial (i.e., correcting an initially wrong answer). To achieve this goal, iMAD learns generalizable model behaviors to make accurate debate decisions. Specifically, iMAD first prompts a single agent to produce a structured self-critique response, from which we extract 41 interpretable linguistic and semantic features capturing hesitation cues. Then, iMAD uses a lightweight debate-decision classifier, trained using our proposed FocusCal loss, to determine whether to trigger MAD, enabling robust debate decisions without test dataset-specific tuning. Through extensive experiments using six (visual) question answering datasets against five competitive baselines, we have shown that iMAD significantly reduces token usage (by up to 92%) while also improving final answer accuracy (by up to 13.5%).",
        "url": "http://arxiv.org/abs/2511.11306v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11306v1",
        "arxiv_id": "2511.11306v1",
        "authors": [
            "Wei Fan",
            "JinYi Yoon",
            "Bo Ji"
        ],
        "submitted": "2025-11-14 13:50:51",
        "source": "arxiv",
        "comment": "Accepted in AAAI 2026 (Oral)",
        "score": 5,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be related to Large Language Models (LLMs) and their inference, but it does not directly align with your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus on efficient LLM inference and debate decision-making is not a central match for your research themes."
    },
    {
        "title": "GovScape: A Public Multimodal Search System for 70 Million Pages of Government PDFs",
        "abstract": "Efforts over the past three decades have produced web archives containing billions of webpage snapshots and petabytes of data. The End of Term Web Archive alone contains, among other file types, millions of PDFs produced by the federal government. While preservation with web archives has been successful, significant challenges for access and discoverability remain. For example, current affordances for browsing the End of Term PDFs are limited to downloading and browsing individual PDFs, as well as performing basic keyword search across them. In this paper, we introduce GovScape, a public search system that supports multimodal searches across 10,015,993 federal government PDFs from the 2020 End of Term crawl (70,958,487 total PDF pages) - to our knowledge, all renderable PDFs in the 2020 crawl that are 50 pages or under. GovScape supports four primary forms of search over these 10 million PDFs: in addition to providing (1) filter conditions over metadata facets including domain and crawl date and (2) exact text search against the PDF text, we provide (3) semantic text search and (4) visual search against the PDFs across individual pages, enabling users to structure queries such as \"redacted documents\" or \"pie charts.\" We detail the constituent components of GovScape, including the search affordances, embedding pipeline, system architecture, and open source codebase. Significantly, the total estimated compute cost for GovScape's pre-processing pipeline for 10 million PDFs was approximately $1,500, equivalent to 47,000 PDF pages per dollar spent on compute, demonstrating the potential for immediate scalability. Accordingly, we outline steps that we have already begun pursuing toward multimodal search at the 100+ million PDF scale. GovScape can be found at https://www.govscape.net.",
        "url": "http://arxiv.org/abs/2511.11010v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11010v1",
        "arxiv_id": "2511.11010v1",
        "authors": [
            "Kyle Deeds",
            "Ying-Hsiang Huang",
            "Claire Gong",
            "Shreya Shaji",
            "Alison Yan",
            "Leslie Harka",
            "Samuel J Klein",
            "Shannon Zejiang Shen",
            "Mark Phillips",
            "Trevor Owens",
            "Benjamin Charles Germain Lee"
        ],
        "submitted": "2025-11-14 06:54:48",
        "source": "arxiv",
        "comment": "10 pages, 5 figures, 2 tables",
        "score": 5,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'search' (score: +1)",
            "Found 'www' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper presents a search system for government PDFs, which is somewhat related to information retrieval and search technologies. However, the focus is on multimodal search and scalability, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest."
    },
    {
        "title": "Multimodal Peer Review Simulation with Actionable To-Do Recommendations for Community-Aware Manuscript Revisions",
        "abstract": "While large language models (LLMs) offer promising capabilities for automating academic workflows, existing systems for academic peer review remain constrained by text-only inputs, limited contextual grounding, and a lack of actionable feedback. In this work, we present an interactive web-based system for multimodal, community-aware peer review simulation to enable effective manuscript revisions before paper submission. Our framework integrates textual and visual information through multimodal LLMs, enhances review quality via retrieval-augmented generation (RAG) grounded in web-scale OpenReview data, and converts generated reviews into actionable to-do lists using the proposed Action:Objective[\\#] format, providing structured and traceable guidance. The system integrates seamlessly into existing academic writing platforms, providing interactive interfaces for real-time feedback and revision tracking. Experimental results highlight the effectiveness of the proposed system in generating more comprehensive and useful reviews aligned with expert standards, surpassing ablated baselines and advancing transparent, human-centered scholarly assistance.",
        "url": "http://arxiv.org/abs/2511.10902v1",
        "pdf_url": "https://arxiv.org/pdf/2511.10902v1",
        "arxiv_id": "2511.10902v1",
        "authors": [
            "Mengze Hong",
            "Di Jiang",
            "Weiwei Zhao",
            "Yawen Li",
            "Yihang Wang",
            "Xinyuan Luo",
            "Yanjie Sun",
            "Chen Jason Zhang"
        ],
        "submitted": "2025-11-14 02:29:23",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on academic peer review simulation and multimodal LLMs, which is not directly related to your core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP and multimodal processing, the context and application are quite different from your areas of interest."
    },
    {
        "title": "Leveraging Parameter Space Symmetries for Reasoning Skill Transfer in LLMs",
        "abstract": "Task arithmetic is a powerful technique for transferring skills between Large Language Models (LLMs), but it often suffers from negative interference when models have diverged during training. We address this limitation by first aligning the models' parameter spaces, leveraging the inherent permutation, rotation, and scaling symmetries of Transformer architectures. We adapt parameter space alignment for modern Grouped-Query Attention (GQA) and SwiGLU layers, exploring both weight-based and activation-based approaches. Using this alignment-first strategy, we successfully transfer advanced reasoning skills to a non-reasoning model. Experiments on challenging reasoning benchmarks show that our method consistently outperforms standard task arithmetic. This work provides an effective approach for merging and transferring specialized skills across evolving LLM families, reducing redundant fine-tuning and enhancing model adaptability.",
        "url": "http://arxiv.org/abs/2511.10850v1",
        "pdf_url": "https://arxiv.org/pdf/2511.10850v1",
        "arxiv_id": "2511.10850v1",
        "authors": [
            "Stefan Horoi",
            "Sangwoo Cho",
            "Supriyo Chakraborty",
            "Shi-Xiong Zhang",
            "Sambit Sahu",
            "Guy Wolf",
            "Genta Indra Winata"
        ],
        "submitted": "2025-11-13 23:20:57",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Large Language Models (LLMs) and reasoning skill transfer, which is not directly related to the user's core research themes in Information Retrieval and Search technologies. While it involves some NLP aspects, the primary focus is on LLMs and their parameter space alignment, which does not align with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "LaoBench: A Large-Scale Multidimensional Lao Benchmark for Large Language Models",
        "abstract": "The rapid advancement of large language models (LLMs) has not been matched by their evaluation in low-resource languages, especially Southeast Asian languages like Lao. To fill this gap, we introduce LaoBench, the first large-scale, high-quality, and multidimensional benchmark dataset dedicated to assessing LLMs' comprehensive language understanding and reasoning abilities in Lao. LaoBench comprises over 17,000 carefully curated samples spanning three core dimensions: knowledge application, K12 foundational education, and bilingual translation among Lao, Chinese, and English. The dataset is divided into open-source and closed-source subsets, with the closed-source portion enabling black-box evaluation on an official platform to ensure fairness and data security. Our data construction pipeline integrates expert human curation with automated agent-assisted verification, ensuring linguistic accuracy, cultural relevance, and educational value. Benchmarking multiple state-of-the-art LLMs on LaoBench reveals that current models still face significant challenges in mastering Lao across diverse tasks. We hope LaoBench will catalyze further research and development of AI technologies for underrepresented Southeast Asian languages.",
        "url": "http://arxiv.org/abs/2511.11334v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11334v1",
        "arxiv_id": "2511.11334v1",
        "authors": [
            "Jian Gao",
            "Richeng Xuan",
            "Zhaolu Kang",
            "Dingshi Liao",
            "Wenxin Huang",
            "Zongmou Huang",
            "Yangdi Xu",
            "Bowen Qin",
            "Zheqi He",
            "Xi Yang",
            "Changjin Li"
        ],
        "submitted": "2025-11-14 14:13:07",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval, Search technologies, or user behavior modeling, which are the core areas of your research interests. Although it involves Natural Language Processing, the focus is on large language models and low-resource languages, which is not a central match with your research themes."
    },
    {
        "title": "Enhancing Group Recommendation using Soft Impute Singular Value Decomposition",
        "abstract": "The growing popularity of group activities increased the need to develop methods for providing recommendations to a group of users based on the collective preferences of the group members. Several group recommender systems have been proposed, but these methods often struggle due to sparsity and high-dimensionality of the available data, common in many real-world applications. In this paper, we propose a group recommender system called Group Soft-Impute SVD, which leverages soft-impute singular value decomposition to enhance group recommendations. This approach addresses the challenge of sparse high-dimensional data using low-rank matrix completion. We compared the performance of Group Soft-Impute SVD with Group MF based approaches and found that our method outperforms the baselines in recall for small user groups while achieving comparable results across all group sizes when tasked on Goodbooks, Movielens, and Synthetic datasets. Furthermore, our method recovers lower matrix ranks than the baselines, demonstrating its effectiveness in handling high-dimensional data.",
        "url": "http://arxiv.org/abs/2511.11172v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11172v1",
        "arxiv_id": "2511.11172v1",
        "authors": [
            "Mubaraka Sani Ibrahim",
            "Isah Charles Saidu",
            "Lehel Csato"
        ],
        "submitted": "2025-11-14 11:13:56",
        "source": "arxiv",
        "comment": "((1) African University of Science and Technology (Abuja, Nigeria), (2) Baze University (Abuja, Nigeria), (3) Babes-Bolyai University (Cluj-Napoca, Romania))",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on group recommendation systems, leveraging matrix completion techniques. While it touches on recommender systems, it does not align with the user's primary interests in Information Retrieval, particularly query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Expert-Guided Prompting and Retrieval-Augmented Generation for Emergency Medical Service Question Answering",
        "abstract": "Large language models (LLMs) have shown promise in medical question answering, yet they often overlook the domain-specific expertise that professionals depend on, such as the clinical subject areas (e.g., trauma, airway) and the certification level (e.g., EMT, Paramedic). Existing approaches typically apply general-purpose prompting or retrieval strategies without leveraging this structured context, limiting performance in high-stakes settings. We address this gap with EMSQA, an 24.3K-question multiple-choice dataset spanning 10 clinical subject areas and 4 certification levels, accompanied by curated, subject area-aligned knowledge bases (40K documents and 2M tokens). Building on EMSQA, we introduce (i) Expert-CoT, a prompting strategy that conditions chain-of-thought (CoT) reasoning on specific clinical subject area and certification level, and (ii) ExpertRAG, a retrieval-augmented generation pipeline that grounds responses in subject area-aligned documents and real-world patient data. Experiments on 4 LLMs show that Expert-CoT improves up to 2.05% over vanilla CoT prompting. Additionally, combining Expert-CoT with ExpertRAG yields up to a 4.59% accuracy gain over standard RAG baselines. Notably, the 32B expertise-augmented LLMs pass all the computer-adaptive EMS certification simulation exams.",
        "url": "http://arxiv.org/abs/2511.10900v1",
        "pdf_url": "https://arxiv.org/pdf/2511.10900v1",
        "arxiv_id": "2511.10900v1",
        "authors": [
            "Xueren Ge",
            "Sahil Murtaza",
            "Anthony Cortez",
            "Homa Alemzadeh"
        ],
        "submitted": "2025-11-14 02:21:48",
        "source": "arxiv",
        "comment": "Accepted by AAAI 2026",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of query understanding and ranking models. However, the focus on medical question answering and language models is not directly aligned with your primary areas of interest, which include e-commerce and real-time relevance optimization."
    },
    {
        "title": "ICX360: In-Context eXplainability 360 Toolkit",
        "abstract": "Large Language Models (LLMs) have become ubiquitous in everyday life and are entering higher-stakes applications ranging from summarizing meeting transcripts to answering doctors' questions. As was the case with earlier predictive models, it is crucial that we develop tools for explaining the output of LLMs, be it a summary, list, response to a question, etc. With these needs in mind, we introduce In-Context Explainability 360 (ICX360), an open-source Python toolkit for explaining LLMs with a focus on the user-provided context (or prompts in general) that are fed to the LLMs. ICX360 contains implementations for three recent tools that explain LLMs using both black-box and white-box methods (via perturbations and gradients respectively). The toolkit, available at https://github.com/IBM/ICX360, contains quick-start guidance materials as well as detailed tutorials covering use cases such as retrieval augmented generation, natural language generation, and jailbreaking.",
        "url": "http://arxiv.org/abs/2511.10879v1",
        "pdf_url": "https://arxiv.org/pdf/2511.10879v1",
        "arxiv_id": "2511.10879v1",
        "authors": [
            "Dennis Wei",
            "Ronny Luss",
            "Xiaomeng Hu",
            "Lucas Monteiro Paes",
            "Pin-Yu Chen",
            "Karthikeyan Natesan Ramamurthy",
            "Erik Miehling",
            "Inge Vejsbjerg",
            "Hendrik Strobelt"
        ],
        "submitted": "2025-11-14 01:17:55",
        "source": "arxiv",
        "comment": "14 pages, 4 figures",
        "score": 4,
        "keyword_reasons": [
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper ICX360: In-Context eXplainability 360 Toolkit is somewhat related to the user's research interests in Natural Language Processing (NLP) and related topics. However, it does not directly align with the user's primary focus on Information Retrieval, especially in areas requiring deep semantic understanding and real-time relevance optimization. The paper's focus on explainability for Large Language Models is an interesting aspect, but it does not seem to be a central match for the user's research themes."
    },
    {
        "title": "Sabiá: Um Chatbot de Inteligência Artificial Generativa para Suporte no Dia a Dia do Ensino Superior",
        "abstract": "Students often report difficulties in accessing day-to-day academic information, which is usually spread across numerous institutional documents and websites. This fragmentation results in a lack of clarity and causes confusion about routine university information. This project proposes the development of a chatbot using Generative Artificial Intelligence (GenAI) and Retrieval-Augmented Generation (RAG) to simplify access to such information. Several GenAI models were tested and evaluated based on quality metrics and the LLM-as-a-Judge approach. Among them, Gemini 2.0 Flash stood out for its quality and speed, and Gemma 3n for its good performance and open-source nature.",
        "url": "http://arxiv.org/abs/2511.10787v1",
        "pdf_url": "https://arxiv.org/pdf/2511.10787v1",
        "arxiv_id": "2511.10787v1",
        "authors": [
            "Guilherme Biava Rodrigues",
            "Franciele Beal",
            "Marlon Marcon",
            "Alinne Cristinne Corrêa Souza",
            "André Roberto Ortoncelli",
            "Francisco Carlos Monteiro Souza",
            "Rodolfo Adamshuk Silva"
        ],
        "submitted": "2025-11-13 20:23:55",
        "source": "arxiv",
        "comment": "Accepte for publishing in SBIE2025, in Portuguese language",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be focused on developing a chatbot for academic information access using Generative Artificial Intelligence and Retrieval-Augmented Generation. While it involves NLP and IR concepts, the context and application are not directly related to the user's core research themes in Information Retrieval, especially in areas requiring deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "Faithful Summarization of Consumer Health Queries: A Cross-Lingual Framework with LLMs",
        "abstract": "Summarizing consumer health questions (CHQs) can ease communication in healthcare, but unfaithful summaries that misrepresent medical details pose serious risks. We propose a framework that combines TextRank-based sentence extraction and medical named entity recognition with large language models (LLMs) to enhance faithfulness in medical text summarization. In our experiments, we fine-tuned the LLaMA-2-7B model on the MeQSum (English) and BanglaCHQ-Summ (Bangla) datasets, achieving consistent improvements across quality (ROUGE, BERTScore, readability) and faithfulness (SummaC, AlignScore) metrics, and outperforming zero-shot baselines and prior systems. Human evaluation further shows that over 80\\% of generated summaries preserve critical medical information. These results highlight faithfulness as an essential dimension for reliable medical summarization and demonstrate the potential of our approach for safer deployment of LLMs in healthcare contexts.",
        "url": "http://arxiv.org/abs/2511.10768v1",
        "pdf_url": "https://arxiv.org/pdf/2511.10768v1",
        "arxiv_id": "2511.10768v1",
        "authors": [
            "Ajwad Abrar",
            "Nafisa Tabassum Oeshy",
            "Prianka Maheru",
            "Farzana Tabassum",
            "Tareque Mohmud Chowdhury"
        ],
        "submitted": "2025-11-13 19:42:11",
        "source": "arxiv",
        "comment": "Accepted at the 5th Muslims in Machine Learning (MusIML) Workshop, co-located with NeurIPS 2025",
        "score": 4,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, particularly in the area of text summarization. However, it focuses more on medical text summarization and faithfulness in summarization, which is not a central match to your primary focus on query understanding, ranking models, and user behavior modeling. The paper's use of large language models is relevant, but the application is in a specific domain (healthcare) and not directly related to your e-commerce background or interests in real-time relevance optimization."
    },
    {
        "title": "W2S-AlignTree: Weak-to-Strong Inference-Time Alignment for Large Language Models via Monte Carlo Tree Search",
        "abstract": "Large Language Models (LLMs) demonstrate impressive capabilities, yet their outputs often suffer from misalignment with human preferences due to the inadequacy of weak supervision and a lack of fine-grained control. Training-time alignment methods like Reinforcement Learning from Human Feedback (RLHF) face prohibitive costs in expert supervision and inherent scalability limitations, offering limited dynamic control during inference. Consequently, there is an urgent need for scalable and adaptable alignment mechanisms. To address this, we propose W2S-AlignTree, a pioneering plug-and-play inference-time alignment framework that synergistically combines Monte Carlo Tree Search (MCTS) with the Weak-to-Strong Generalization paradigm for the first time. W2S-AlignTree formulates LLM alignment as an optimal heuristic search problem within a generative search tree. By leveraging weak model's real-time, step-level signals as alignment proxies and introducing an Entropy-Aware exploration mechanism, W2S-AlignTree enables fine-grained guidance during strong model's generation without modifying its parameters. The approach dynamically balances exploration and exploitation in high-dimensional generation search trees. Experiments across controlled sentiment generation, summarization, and instruction-following show that W2S-AlignTree consistently outperforms strong baselines. Notably, W2S-AlignTree raises the performance of Llama3-8B from 1.89 to 2.19, a relative improvement of 15.9 on the summarization task.",
        "url": "http://arxiv.org/abs/2511.11518v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11518v1",
        "arxiv_id": "2511.11518v1",
        "authors": [
            "Zhenyu Ding",
            "Yuhao Wang",
            "Tengyue Xiao",
            "Haoying Wang",
            "Guojun Ma",
            "Mingyang Wan",
            "Caigui Jiang",
            "Ning Ding"
        ],
        "submitted": "2025-11-14 17:42:02",
        "source": "arxiv",
        "comment": "AAAI 2026 Oral",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on Large Language Models and their alignment with human preferences, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the primary focus on NLP and deep semantic understanding is not directly addressed, and the paper does not explicitly mention search technologies or user behavior modeling."
    },
    {
        "title": "SRLF: An Agent-Driven Set-Wise Reflective Learning Framework for Sequential Recommendation",
        "abstract": "LLM-based agents are emerging as a promising paradigm for simulating user behavior to enhance recommender systems. However, their effectiveness is often limited by existing studies that focus on modeling user ratings for individual items. This point-wise approach leads to prevalent issues such as inaccurate user preference comprehension and rigid item-semantic representations.\n  To address these limitations, we propose the novel Set-wise Reflective Learning Framework (SRLF). Our framework operationalizes a closed-loop \"assess-validate-reflect\" cycle that harnesses the powerful in-context learning capabilities of LLMs. SRLF departs from conventional point-wise assessment by formulating a holistic judgment on an entire set of items. It accomplishes this by comprehensively analyzing both the intricate interrelationships among items within the set and their collective alignment with the user's preference profile. This method of set-level contextual understanding allows our model to capture complex relational patterns essential to user behavior, making it significantly more adept for sequential recommendation. Extensive experiments validate our approach, confirming that this set-wise perspective is crucial for achieving state-of-the-art performance in sequential recommendation tasks.",
        "url": "http://arxiv.org/abs/2511.11370v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11370v1",
        "arxiv_id": "2511.11370v1",
        "authors": [
            "Jiahao Wang",
            "Bokang Fu",
            "Yu Zhu",
            "Yuli Liu"
        ],
        "submitted": "2025-11-14 14:50:33",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'user behavior' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores recommender systems and sequential recommendation, which are related to the user's interests in Information Retrieval and Search technologies. However, the focus on recommender systems and the use of LLMs for user behavior modeling, while relevant, is not the primary area of interest for the user, who has a stronger focus on query understanding, ranking models, and real-time relevance optimization."
    },
    {
        "title": "NOVA: An Agentic Framework for Automated Histopathology Analysis and Discovery",
        "abstract": "Digitized histopathology analysis involves complex, time-intensive workflows and specialized expertise, limiting its accessibility. We introduce NOVA, an agentic framework that translates scientific queries into executable analysis pipelines by iteratively generating and running Python code. NOVA integrates 49 domain-specific tools (e.g., nuclei segmentation, whole-slide encoding) built on open-source software, and can also create new tools ad hoc. To evaluate such systems, we present SlideQuest, a 90-question benchmark -- verified by pathologists and biomedical scientists -- spanning data processing, quantitative analysis, and hypothesis testing. Unlike prior biomedical benchmarks focused on knowledge recall or diagnostic QA, SlideQuest demands multi-step reasoning, iterative coding, and computational problem solving. Quantitative evaluation shows NOVA outperforms coding-agent baselines, and a pathologist-verified case study links morphology to prognostically relevant PAM50 subtypes, demonstrating its scalable discovery potential.",
        "url": "http://arxiv.org/abs/2511.11324v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11324v1",
        "arxiv_id": "2511.11324v1",
        "authors": [
            "Anurag J. Vaidya",
            "Felix Meissen",
            "Daniel C. Castro",
            "Shruthi Bannur",
            "Tristan Lazard",
            "Drew F. K. Williamson",
            "Faisal Mahmood",
            "Javier Alvarez-Valle",
            "Stephanie L. Hyland",
            "Kenza Bouzid"
        ],
        "submitted": "2025-11-14 14:01:18",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on automated histopathology analysis and discovery, which is outside your areas of expertise in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves computational problem solving, it is not related to query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Align$^3$GR: Unified Multi-Level Alignment for LLM-based Generative Recommendation",
        "abstract": "Large Language Models (LLMs) demonstrate significant advantages in leveraging structured world knowledge and multi-step reasoning capabilities. However, fundamental challenges arise when transforming LLMs into real-world recommender systems due to semantic and behavioral misalignment. To bridge this gap, we propose Align$^3$GR, a novel framework that unifies token-level, behavior modeling-level, and preference-level alignment. Our approach introduces: Dual tokenization fusing user-item semantic and collaborative signals. Enhanced behavior modeling with bidirectional semantic alignment. Progressive DPO strategy combining self-play (SP-DPO) and real-world feedback (RF-DPO) for dynamic preference adaptation. Experiments show Align$^3$GR outperforms the SOTA baseline by +17.8% in Recall@10 and +20.2% in NDCG@10 on the public dataset, with significant gains in online A/B tests and full-scale deployment on an industrial large-scale recommendation platform.",
        "url": "http://arxiv.org/abs/2511.11255v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11255v1",
        "arxiv_id": "2511.11255v1",
        "authors": [
            "Wencai Ye",
            "Mingjie Sun",
            "Shuhang Chen",
            "Wenjin Wu",
            "Peng Jiang"
        ],
        "submitted": "2025-11-14 12:52:43",
        "source": "arxiv",
        "comment": "Accepted by AAAI 2026 (Oral)",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on generative recommendation using Large Language Models (LLMs), which is somewhat related to information retrieval, particularly in the context of real-world recommender systems. However, the primary focus on recommendation and the use of LLMs for this purpose, rather than query understanding or ranking models, limits its relevance to your core research interests."
    },
    {
        "title": "CLARITY: Contextual Linguistic Adaptation and Accent Retrieval for Dual-Bias Mitigation in Text-to-Speech Generation",
        "abstract": "Instruction-guided text-to-speech (TTS) research has reached a maturity level where excellent speech generation quality is possible on demand, yet two coupled biases persist: accent bias, where models default to dominant phonetic patterns, and linguistic bias, where dialect-specific lexical and cultural cues are ignored. These biases are interdependent, as authentic accent generation requires both accent fidelity and localized text. We present Contextual Linguistic Adaptation and Retrieval for Inclusive TTS sYnthesis (CLARITY), a backbone-agnostic framework that addresses these biases through dual-signal optimization: (i) contextual linguistic adaptation that localizes input text to the target dialect, and (ii) retrieval-augmented accent prompting (RAAP) that supplies accent-consistent speech prompts. Across twelve English accents, CLARITY improves accent accuracy and fairness while maintaining strong perceptual quality.",
        "url": "http://arxiv.org/abs/2511.11104v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11104v1",
        "arxiv_id": "2511.11104v1",
        "authors": [
            "Crystal Min Hui Poon",
            "Pai Chet Ng",
            "Xiaoxiao Miao",
            "Immanuel Jun Kai Loh",
            "Bowen Zhang",
            "Haoyu Song",
            "Ian Mcloughlin"
        ],
        "submitted": "2025-11-14 09:29:10",
        "source": "arxiv",
        "comment": "Submitted to ICASSP 2026",
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on text-to-speech generation and bias mitigation, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves linguistic adaptation and retrieval, the context is specific to speech synthesis and does not align with your primary focus on deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "CardioEmbed: Domain-Specialized Text Embeddings for Clinical Cardiology",
        "abstract": "Biomedical text embeddings have primarily been developed using research literature from PubMed, yet clinical cardiology practice relies heavily on procedural knowledge and specialized terminology found in comprehensive textbooks rather than research abstracts. This research practice gap limits the effectiveness of existing embedding models for clinical applications incardiology. This study trained CardioEmbed, a domain-specialized embedding model based on Qwen3-Embedding-8B, using contrastive learning on a curated corpus of seven comprehensive cardiology textbooks totaling approximately 150,000 sentences after deduplication. The model employs InfoNCE loss with in-batch negatives and achieves 99.60% retrieval accuracy on cardiac-specific semantic retrieval tasks, a +15.94 percentage point improvement over MedTE, the current state-of-the-art medical embedding model. On MTEB medical benchmarks, the model obtained BIOSSES 0.77 Spearman and SciFact 0.61 NDCG@10, indicating competitive performance on related biomedical domains. Domain-specialized training on comprehensive clinical textbooks yields near-perfect cardiology retrieval (99.60% Acc@1), improving over MedTE by +15.94 percentage points.",
        "url": "http://arxiv.org/abs/2511.10930v1",
        "pdf_url": "https://arxiv.org/pdf/2511.10930v1",
        "arxiv_id": "2511.10930v1",
        "authors": [
            "Richard J. Young",
            "Alice M. Matthews"
        ],
        "submitted": "2025-11-14 03:38:48",
        "source": "arxiv",
        "comment": "14 pages, 6 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on biomedical text embeddings for clinical cardiology, which is outside your primary area of interest in Information Retrieval and Search technologies. Although it involves text embeddings and retrieval, the domain is specific to clinical cardiology and does not align with your broader interests in e-commerce, NLP, and data mining."
    },
    {
        "title": "From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models",
        "abstract": "Tool-augmented Language Models (TaLMs) can invoke external tools to solve problems beyond their parametric capacity. However, it remains unclear whether these tool-enabled gains reflect trustworthy reasoning. Focusing on the Code Interpreter tool, we show that even when tools are selected and executed correctly, TaLMs treat tool outputs as substitutes for reasoning, producing solutions that appear correct but lack coherent justification. We term this failure mode Tool-Induced Myopia (TIM), and study it using PYMATH, a benchmark of 1,679 competition-level mathematical problems for which Python code is helpful but not sufficient. We further develop a multi-dimensional evaluation suite to quantify reasoning degradation in TaLMs relative to their non-tool counterparts. Our findings reveal that while TaLMs achieve up to a 19.3 percentage point gain in final-answer accuracy, their reasoning behavior consistently deteriorates (e.g., non-tool LLMs win up to 41.5% more often in pairwise comparisons of the reasoning process). This degradation intensifies with tool use; the more frequently a model invokes tools, the less coherent its reasoning becomes. Moreover, tool use shifts errors from arithmetic mistakes toward global reasoning failures (logic, assumption, creativity); with TIM present in ~55% of high-risk cases. Finally, we propose a preference-optimization-based framework that realigns TaLMs to use tools as assistive evidence, improving both final-answer accuracy and reasoning depth under tool use. Codes and data are available at: https://github.com/megagonlabs/TIM.",
        "url": "http://arxiv.org/abs/2511.10899v1",
        "pdf_url": "https://arxiv.org/pdf/2511.10899v1",
        "arxiv_id": "2511.10899v1",
        "authors": [
            "Farima Fatahi Bayat",
            "Pouya Pezeshkpour",
            "Estevam Hruschka"
        ],
        "submitted": "2025-11-14 02:21:34",
        "source": "arxiv",
        "comment": "19 pages, 5 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The paper focuses on Tool-Induced Myopia in Large Language Models, which is a topic outside your primary areas of interest."
    },
    {
        "title": "MedPath: Multi-Domain Cross-Vocabulary Hierarchical Paths for Biomedical Entity Linking",
        "abstract": "Progress in biomedical Named Entity Recognition (NER) and Entity Linking (EL) is currently hindered by a fragmented data landscape, a lack of resources for building explainable models, and the limitations of semantically-blind evaluation metrics. To address these challenges, we present MedPath, a large-scale and multi-domain biomedical EL dataset that builds upon nine existing expert-annotated EL datasets. In MedPath, all entities are 1) normalized using the latest version of the Unified Medical Language System (UMLS), 2) augmented with mappings to 62 other biomedical vocabularies and, crucially, 3) enriched with full ontological paths -- i.e., from general to specific -- in up to 11 biomedical vocabularies. MedPath directly enables new research frontiers in biomedical NLP, facilitating training and evaluation of semantic-rich and interpretable EL systems, and the development of the next generation of interoperable and explainable clinical NLP models.",
        "url": "http://arxiv.org/abs/2511.10887v1",
        "pdf_url": "https://arxiv.org/pdf/2511.10887v1",
        "arxiv_id": "2511.10887v1",
        "authors": [
            "Nishant Mishra",
            "Wilker Aziz",
            "Iacer Calixto"
        ],
        "submitted": "2025-11-14 01:49:24",
        "source": "arxiv",
        "comment": "Accepted at AACL-IJCNLP 2025(main)",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on biomedical entity linking and named entity recognition, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve NLP, the domain is specific to biomedicine and does not align with the user's broader interests in e-commerce and real-time relevance optimization."
    },
    {
        "title": "TEDxTN: A Three-way Speech Translation Corpus for Code-Switched Tunisian Arabic - English",
        "abstract": "In this paper, we introduce TEDxTN, the first publicly available Tunisian Arabic to English speech translation dataset. This work is in line with the ongoing effort to mitigate the data scarcity obstacle for a number of Arabic dialects. We collected, segmented, transcribed and translated 108 TEDx talks following our internally developed annotations guidelines. The collected talks represent 25 hours of speech with code-switching that cover speakers with various accents from over 11 different regions of Tunisia. We make the annotation guidelines and corpus publicly available. This will enable the extension of TEDxTN to new talks as they become available. We also report results for strong baseline systems of Speech Recognition and Speech Translation using multiple pre-trained and fine-tuned end-to-end models. This corpus is the first open source and publicly available speech translation corpus of Code-Switching Tunisian dialect. We believe that this is a valuable resource that can motivate and facilitate further research on the natural language processing of Tunisian Dialect.",
        "url": "http://arxiv.org/abs/2511.10780v1",
        "pdf_url": "https://arxiv.org/pdf/2511.10780v1",
        "arxiv_id": "2511.10780v1",
        "authors": [
            "Fethi Bougares",
            "Salima Mdhaffar",
            "Haroun Elleuch",
            "Yannick Estève"
        ],
        "submitted": "2025-11-13 20:05:19",
        "source": "arxiv",
        "comment": "The Third Arabic Natural Language Processing Conference. Association for Computational Linguistics. 2025",
        "score": 3,
        "keyword_reasons": [
            "Found 'search' (score: +1)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on speech translation and code-switched Tunisian Arabic-English corpus, which is outside your primary areas of interest in Information Retrieval, Search technologies, and Natural Language Processing."
    },
    {
        "title": "Proactive Hearing Assistants that Isolate Egocentric Conversations",
        "abstract": "We introduce proactive hearing assistants that automatically identify and separate the wearer's conversation partners, without requiring explicit prompts. Our system operates on egocentric binaural audio and uses the wearer's self-speech as an anchor, leveraging turn-taking behavior and dialogue dynamics to infer conversational partners and suppress others. To enable real-time, on-device operation, we propose a dual-model architecture: a lightweight streaming model runs every 12.5 ms for low-latency extraction of the conversation partners, while a slower model runs less frequently to capture longer-range conversational dynamics. Results on real-world 2- and 3-speaker conversation test sets, collected with binaural egocentric hardware from 11 participants totaling 6.8 hours, show generalization in identifying and isolating conversational partners in multi-conversation settings. Our work marks a step toward hearing assistants that adapt proactively to conversational dynamics and engagement. More information can be found on our website: https://proactivehearing.cs.washington.edu/",
        "url": "http://arxiv.org/abs/2511.11473v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11473v1",
        "arxiv_id": "2511.11473v1",
        "authors": [
            "Guilin Hu",
            "Malek Itani",
            "Tuochao Chen",
            "Shyamnath Gollakota"
        ],
        "submitted": "2025-11-14 16:44:48",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025 Main Conference",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves audio processing and conversational dynamics, its focus on hearing assistants and egocentric conversations is not aligned with your areas of expertise."
    },
    {
        "title": "Language-Aided State Estimation",
        "abstract": "Natural language data, such as text and speech, have become readily available through social networking services and chat platforms. By leveraging human observations expressed in natural language, this paper addresses the problem of state estimation for physical systems, in which humans act as sensing agents. To this end, we propose a Language-Aided Particle Filter (LAPF), a particle filter framework that structures human observations via natural language processing and incorporates them into the update step of the state estimation. Finally, the LAPF is applied to the water level estimation problem in an irrigation canal and its effectiveness is demonstrated.",
        "url": "http://arxiv.org/abs/2511.11285v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11285v1",
        "arxiv_id": "2511.11285v1",
        "authors": [
            "Yuki Miyoshi",
            "Masaki Inoue",
            "Yusuke Fujimoto"
        ],
        "submitted": "2025-11-14 13:18:37",
        "source": "arxiv",
        "comment": "7 pages, 5 figures, submitted to IFAC World Congress 2026 with Journal option (IFAC Journal of Systems and Control)",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on state estimation for physical systems using natural language data, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves NLP, the application and context are quite different from the user's interests."
    },
    {
        "title": "KGQuest: Template-Driven QA Generation from Knowledge Graphs with LLM-Based Refinement",
        "abstract": "The generation of questions and answers (QA) from knowledge graphs (KG) plays a crucial role in the development and testing of educational platforms, dissemination tools, and large language models (LLM). However, existing approaches often struggle with scalability, linguistic quality, and factual consistency. This paper presents a scalable and deterministic pipeline for generating natural language QA from KGs, with an additional refinement step using LLMs to further enhance linguistic quality. The approach first clusters KG triplets based on their relations, creating reusable templates through natural language rules derived from the entity types of objects and relations. A module then leverages LLMs to refine these templates, improving clarity and coherence while preserving factual accuracy. Finally, the instantiation of answer options is achieved through a selection strategy that introduces distractors from the KG. Our experiments demonstrate that this hybrid approach efficiently generates high-quality QA pairs, combining scalability with fluency and linguistic precision.",
        "url": "http://arxiv.org/abs/2511.11258v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11258v1",
        "arxiv_id": "2511.11258v1",
        "authors": [
            "Sania Nayab",
            "Marco Simoni",
            "Giulio Rossolini",
            "Andrea Saracino"
        ],
        "submitted": "2025-11-14 12:54:01",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses a method for generating questions and answers from knowledge graphs, which involves natural language processing and machine learning. While it touches on aspects of query understanding and ranking models, its primary focus is on question generation and answer refinement, which is somewhat related to information retrieval but not a central match for your research interests."
    },
    {
        "title": "Adverbs Revisited: Enhancing WordNet Coverage of Adverbs with a Supersense Taxonomy",
        "abstract": "WordNet offers rich supersense hierarchies for nouns and verbs, yet adverbs remain underdeveloped, lacking a systematic semantic classification. We introduce a linguistically grounded supersense typology for adverbs, empirically validated through annotation, that captures major semantic domains including manner, temporal, frequency, degree, domain, speaker-oriented, and subject-oriented functions. Results from a pilot annotation study demonstrate that these categories provide broad coverage of adverbs in natural text and can be reliably assigned by human annotators. Incorporating this typology extends WordNet's coverage, aligns it more closely with linguistic theory, and facilitates downstream NLP applications such as word sense disambiguation, event extraction, sentiment analysis, and discourse modeling. We present the proposed supersense categories, annotation outcomes, and directions for future work.",
        "url": "http://arxiv.org/abs/2511.11214v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11214v1",
        "arxiv_id": "2511.11214v1",
        "authors": [
            "Jooyoung Lee",
            "Jader Martins Camboim de Sá"
        ],
        "submitted": "2025-11-14 12:12:10",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on enhancing WordNet's coverage of adverbs with a supersense taxonomy, which is related to NLP but does not directly align with the user's core research themes in Information Retrieval, Search technologies, and query understanding."
    },
    {
        "title": "Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning",
        "abstract": "Hallucination continues to pose a major obstacle in the reasoning capabilities of large language models (LLMs). Although the Multi-Agent Debate (MAD) paradigm offers a promising solution by promoting consensus among multiple agents to enhance reliability, it relies on the unrealistic assumption that all debaters are rational and reflective, which is a condition that may not hold when agents themselves are prone to hallucinations. To address this gap, we introduce the Multi-agent Undercover Gaming (MUG) protocol, inspired by social deduction games like \"Who is Undercover?\". MUG reframes MAD as a process of detecting \"undercover\" agents (those suffering from hallucinations) by employing multimodal counterfactual tests. Specifically, we modify reference images to introduce counterfactual evidence and observe whether agents can accurately identify these changes, providing ground-truth for identifying hallucinating agents and enabling robust, crowd-powered multimodal reasoning. MUG advances MAD protocols along three key dimensions: (1) enabling factual verification beyond statistical consensus through counterfactual testing; (2) introducing cross-evidence reasoning via dynamically modified evidence sources instead of relying on static inputs; and (3) fostering active reasoning, where agents engage in probing discussions rather than passively answering questions. Collectively, these innovations offer a more reliable and effective framework for multimodal reasoning in LLMs. The source code can be accessed at https://github.com/YongLD/MUG.git.",
        "url": "http://arxiv.org/abs/2511.11182v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11182v1",
        "arxiv_id": "2511.11182v1",
        "authors": [
            "Dayong Liang",
            "Xiao-Yong Wei",
            "Changmeng Zheng"
        ],
        "submitted": "2025-11-14 11:27:55",
        "source": "arxiv",
        "comment": "Accepted by AAAI 2026",
        "score": 2,
        "keyword_reasons": [
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be unrelated to your core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves multimodal reasoning and large language models, the focus is on a specific problem in social deduction games and does not align with your interests in query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Speech-Aware Long Context Pruning and Integration for Contextualized Automatic Speech Recognition",
        "abstract": "Automatic speech recognition (ASR) systems have achieved remarkable performance in common conditions but often struggle to leverage long-context information in contextualized scenarios that require domain-specific knowledge, such as conference presentations. This challenge arises primarily due to constrained model context windows and the sparsity of relevant information within extensive contextual noise. To solve this, we propose the SAP$^{2}$ method, a novel framework that dynamically prunes and integrates relevant contextual keywords in two stages. Specifically, each stage leverages our proposed Speech-Driven Attention-based Pooling mechanism, enabling efficient compression of context embeddings while preserving speech-salient information. Experimental results demonstrate state-of-the-art performance of SAP$^{2}$ on the SlideSpeech and LibriSpeech datasets, achieving word error rates (WER) of 7.71% and 1.12%, respectively. On SlideSpeech, our method notably reduces biased keyword error rates (B-WER) by 41.1% compared to non-contextual baselines. SAP$^{2}$ also exhibits robust scalability, consistently maintaining performance under extensive contextual input conditions on both datasets.",
        "url": "http://arxiv.org/abs/2511.11139v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11139v1",
        "arxiv_id": "2511.11139v1",
        "authors": [
            "Yiming Rong",
            "Yixin Zhang",
            "Ziyi Wang",
            "Deyang Jiang",
            "Yunlong Zhao",
            "Haoran Wu",
            "Shiyu Zhou",
            "Bo Xu"
        ],
        "submitted": "2025-11-14 10:15:16",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves speech recognition and contextual understanding, it is focused on Automatic Speech Recognition and does not address query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Enhancing Meme Emotion Understanding with Multi-Level Modality Enhancement and Dual-Stage Modal Fusion",
        "abstract": "With the rapid rise of social media and Internet culture, memes have become a popular medium for expressing emotional tendencies. This has sparked growing interest in Meme Emotion Understanding (MEU), which aims to classify the emotional intent behind memes by leveraging their multimodal contents. While existing efforts have achieved promising results, two major challenges remain: (1) a lack of fine-grained multimodal fusion strategies, and (2) insufficient mining of memes' implicit meanings and background knowledge. To address these challenges, we propose MemoDetector, a novel framework for advancing MEU. First, we introduce a four-step textual enhancement module that utilizes the rich knowledge and reasoning capabilities of Multimodal Large Language Models (MLLMs) to progressively infer and extract implicit and contextual insights from memes. These enhanced texts significantly enrich the original meme contents and provide valuable guidance for downstream classification. Next, we design a dual-stage modal fusion strategy: the first stage performs shallow fusion on raw meme image and text, while the second stage deeply integrates the enhanced visual and textual features. This hierarchical fusion enables the model to better capture nuanced cross-modal emotional cues. Experiments on two datasets, MET-MEME and MOOD, demonstrate that our method consistently outperforms state-of-the-art baselines. Specifically, MemoDetector improves F1 scores by 4.3\\% on MET-MEME and 3.4\\% on MOOD. Further ablation studies and in-depth analyses validate the effectiveness and robustness of our approach, highlighting its strong potential for advancing MEU. Our code is available at https://github.com/singing-cat/MemoDetector.",
        "url": "http://arxiv.org/abs/2511.11126v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11126v1",
        "arxiv_id": "2511.11126v1",
        "authors": [
            "Yi Shi",
            "Wenlong Meng",
            "Zhenyuan Guo",
            "Chengkun Wei",
            "Wenzhi Chen"
        ],
        "submitted": "2025-11-14 09:59:08",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Meme Emotion Understanding, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves multimodal fusion and deep semantic understanding, the context is specific to memes and social media, which does not align with the user's interests in e-commerce or general real-time relevance optimization."
    },
    {
        "title": "S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology Report Generation",
        "abstract": "Radiology Report Generation (RRG) aims to automatically generate diagnostic reports from radiology images. To achieve this, existing methods have leveraged the powerful cross-modal generation capabilities of Multimodal Large Language Models (MLLMs), primarily focusing on optimizing cross-modal alignment between radiographs and reports through Supervised Fine-Tuning (SFT). However, by only performing instance-level alignment with the image-text pairs, the standard SFT paradigm fails to establish anatomically-grounded alignment, where the templated nature of reports often leads to sub-optimal generation quality. To address this, we propose \\textsc{S2D-Align}, a novel SFT paradigm that establishes anatomically-grounded alignment by leveraging auxiliary signals of varying granularities. \\textsc{S2D-Align} implements a shallow-to-deep strategy, progressively enriching the alignment process: it begins with the coarse radiograph-report pairing, then introduces reference reports for instance-level guidance, and ultimately utilizes key phrases to ground the generation in specific anatomical details. To bridge the different alignment stages, we introduce a memory-based adapter that empowers feature sharing, thereby integrating coarse and fine-grained guidance. For evaluation, we conduct experiments on the public \\textsc{MIMIC-CXR} and \\textsc{IU X-Ray} benchmarks, where \\textsc{S2D-Align} achieves state-of-the-art performance compared to existing methods. Ablation studies validate the effectiveness of our multi-stage, auxiliary-guided approach, highlighting a promising direction for enhancing grounding capabilities in complex, multi-modal generation tasks.",
        "url": "http://arxiv.org/abs/2511.11066v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11066v1",
        "arxiv_id": "2511.11066v1",
        "authors": [
            "Jiechao Gao",
            "Chang Liu",
            "Yuangang Li"
        ],
        "submitted": "2025-11-14 08:34:06",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Radiology Report Generation using Multimodal Large Language Models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves deep semantic understanding, the context is specific to medical imaging and report generation, making it only loosely relevant to your broader research areas."
    },
    {
        "title": "Correcting Mean Bias in Text Embeddings: A Refined Renormalization with Training-Free Improvements on MMTEB",
        "abstract": "We find that current text embedding models produce outputs with a consistent bias, i.e., each embedding vector $e$ can be decomposed as $\\tilde{e} + μ$, where $μ$ is almost identical across all sentences. We propose a plug-and-play, training-free and lightweight solution called Renormalization. Through extensive experiments, we show that renormalization consistently and statistically significantly improves the performance of existing models on the Massive Multilingual Text Embedding Benchmark (MMTEB). In particular, across 38 models, renormalization improves performance by 9.7 $σ$ on retrieval tasks, 3.1 $σ$ on classification tasks, and 0.8 $σ$ on other types of tasks. Renormalization has two variants: directly subtracting $μ$ from $e$, or subtracting the projection of $e$ onto $μ$. We theoretically predict that the latter performs better, and our experiments confirm this prediction.",
        "url": "http://arxiv.org/abs/2511.11041v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11041v1",
        "arxiv_id": "2511.11041v1",
        "authors": [
            "Xingyu Ren",
            "Youran Sun",
            "Haoyu Liang"
        ],
        "submitted": "2025-11-14 07:51:59",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper focuses on improving text embeddings, which is related to information retrieval, but it does not directly address query understanding, ranking models, or user behavior modeling. The paper's emphasis on text embeddings and their applications to retrieval tasks makes it somewhat relevant to the user's interests, but it does not align with their core research themes."
    },
    {
        "title": "DiscoX: Benchmarking Discourse-Level Translation task in Expert Domains",
        "abstract": "The evaluation of discourse-level translation in expert domains remains inadequate, despite its centrality to knowledge dissemination and cross-lingual scholarly communication. While these translations demand discourse-level coherence and strict terminological precision, current evaluation methods predominantly focus on segment-level accuracy and fluency. To address this limitation, we introduce DiscoX, a new benchmark for discourse-level and expert-level Chinese-English translation. It comprises 200 professionally-curated texts from 7 domains, with an average length exceeding 1700 tokens. To evaluate performance on DiscoX, we also develop Metric-S, a reference-free system that provides fine-grained automatic assessments across accuracy, fluency, and appropriateness. Metric-S demonstrates strong consistency with human judgments, significantly outperforming existing metrics. Our experiments reveal a remarkable performance gap: even the most advanced LLMs still trail human experts on these tasks. This finding validates the difficulty of DiscoX and underscores the challenges that remain in achieving professional-grade machine translation. The proposed benchmark and evaluation system provide a robust framework for more rigorous evaluation, facilitating future advancements in LLM-based translation.",
        "url": "http://arxiv.org/abs/2511.10984v1",
        "pdf_url": "https://arxiv.org/pdf/2511.10984v1",
        "arxiv_id": "2511.10984v1",
        "authors": [
            "Xiying Zhao",
            "Zhoufutu Wen",
            "Zhixuan Chen",
            "Jingzhe Ding",
            "Jianpeng Jiao",
            "Shuai Li",
            "Xi Li",
            "Danni Liang",
            "Shengda Long",
            "Qianqian Liu",
            "Xianbo Wu",
            "Hongwan Gao",
            "Xiang Gao",
            "Liang Hu",
            "Jiashuo Liu",
            "Mengyun Liu",
            "Weiran Shi",
            "Chenghao Yang",
            "Qianyu Yang",
            "Xuanliang Zhang",
            "Ge Zhang",
            "Wenhao Huang"
        ],
        "submitted": "2025-11-14 06:09:37",
        "source": "arxiv",
        "comment": "36 pages",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be focused on discourse-level translation in expert domains, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves machine translation, the primary focus is on evaluation metrics and benchmarking, rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns",
        "abstract": "Large Language Models (LLMs) are increasingly deployed in safety-critical domains, yet remain susceptible to hallucinations. While prior works have proposed confidence representation methods for hallucination detection, most of these approaches rely on computationally expensive sampling strategies and often disregard the distinction between hallucination types. In this work, we introduce a principled evaluation framework that differentiates between extrinsic and intrinsic hallucination categories and evaluates detection performance across a suite of curated benchmarks. In addition, we leverage a recent attention-based uncertainty quantification algorithm and propose novel attention aggregation strategies that improve both interpretability and hallucination detection performance. Our experimental findings reveal that sampling-based methods like Semantic Entropy are effective for detecting extrinsic hallucinations but generally fail on intrinsic ones. In contrast, our method, which aggregates attention over input tokens, is better suited for intrinsic hallucinations. These insights provide new directions for aligning detection strategies with the nature of hallucination and highlight attention as a rich signal for quantifying model uncertainty.",
        "url": "http://arxiv.org/abs/2511.10837v1",
        "pdf_url": "https://arxiv.org/pdf/2511.10837v1",
        "arxiv_id": "2511.10837v1",
        "authors": [
            "Elyes Hajji",
            "Aymen Bouguerra",
            "Fabio Arnez"
        ],
        "submitted": "2025-11-13 22:42:18",
        "source": "arxiv",
        "comment": "Accepted at AAAI 2025-FS-ATRACC",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves Large Language Models and attention-based uncertainty quantification, its focus on hallucinations in safety-critical domains and attention aggregation strategies is not a central match to your core research themes."
    },
    {
        "title": "MajinBook: An open catalogue of digital world literature with likes",
        "abstract": "This data paper introduces MajinBook, an open catalogue designed to facilitate the use of shadow libraries--such as Library Genesis and Z-Library--for computational social science and cultural analytics. By linking metadata from these vast, crowd-sourced archives with structured bibliographic data from Goodreads, we create a high-precision corpus of over 539,000 references to English-language books spanning three centuries, enriched with first publication dates, genres, and popularity metrics like ratings and reviews. Our methodology prioritizes natively digital EPUB files to ensure machine-readable quality, while addressing biases in traditional corpora like HathiTrust, and includes secondary datasets for French, German, and Spanish. We evaluate the linkage strategy for accuracy, release all underlying data openly, and discuss the project's legal permissibility under EU and US frameworks for text and data mining in research.",
        "url": "http://arxiv.org/abs/2511.11412v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11412v1",
        "arxiv_id": "2511.11412v1",
        "authors": [
            "Antoine Mazières",
            "Thierry Poibeau"
        ],
        "submitted": "2025-11-14 15:44:27",
        "source": "arxiv",
        "comment": "9 pages, 5 figures, 1 table",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be about creating a digital library corpus for cultural analytics, which is somewhat related to information retrieval, but it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest."
    },
    {
        "title": "M-DAIGT: A Shared Task on Multi-Domain Detection of AI-Generated Text",
        "abstract": "The generation of highly fluent text by Large Language Models (LLMs) poses a significant challenge to information integrity and academic research. In this paper, we introduce the Multi-Domain Detection of AI-Generated Text (M-DAIGT) shared task, which focuses on detecting AI-generated text across multiple domains, particularly in news articles and academic writing. M-DAIGT comprises two binary classification subtasks: News Article Detection (NAD) (Subtask 1) and Academic Writing Detection (AWD) (Subtask 2). To support this task, we developed and released a new large-scale benchmark dataset of 30,000 samples, balanced between human-written and AI-generated texts. The AI-generated content was produced using a variety of modern LLMs (e.g., GPT-4, Claude) and diverse prompting strategies. A total of 46 unique teams registered for the shared task, of which four teams submitted final results. All four teams participated in both Subtask 1 and Subtask 2. We describe the methods employed by these participating teams and briefly discuss future directions for M-DAIGT.",
        "url": "http://arxiv.org/abs/2511.11340v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11340v1",
        "arxiv_id": "2511.11340v1",
        "authors": [
            "Salima Lamsiyah",
            "Saad Ezzini",
            "Abdelkader El Mahdaouy",
            "Hamza Alami",
            "Abdessamad Benlahbib",
            "Samir El Amrany",
            "Salmane Chafik",
            "Hicham Hammouchi"
        ],
        "submitted": "2025-11-14 14:26:31",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to information retrieval, as it involves detecting AI-generated text, which is a problem that can be addressed using IR techniques. However, the focus is on text detection rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's relevance to IR is limited to the application of IR techniques in a specific domain."
    },
    {
        "title": "LAET: A Layer-wise Adaptive Ensemble Tuning Framework for Pretrained Language Models",
        "abstract": "Natural Language Processing (NLP) has transformed the financial industry, enabling advancements in areas such as textual analysis, risk management, and forecasting. Large language models (LLMs) like BloombergGPT and FinMA have set new benchmarks across various financial NLP tasks, including sentiment analysis, stock movement prediction, and credit risk assessment. Furthermore, FinMA-ES, a bilingual financial LLM, has also demonstrated strong performance using the FLARE and FLARE-ES benchmarks. However, the high computational demands of these models limit the accessibility of many organizations. To address this, we propose Layer-wise Adaptive Ensemble Tuning (LAET), a novel strategy that selectively fine-tunes the most effective layers of pre-trained LLMs by analyzing hidden state representations while freezing less critical layers. LAET significantly reduces computational overhead while enhancing task-specific performance. Our approach shows strong results in financial NLP tasks, outperforming existing benchmarks and state-of-the-art LLMs such as GPT-4, even with smaller LLMs ($\\sim$3B parameters). This work bridges cutting-edge financial NLP research and real-world deployment with efficient and scalable models for financial applications.",
        "url": "http://arxiv.org/abs/2511.11315v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11315v1",
        "arxiv_id": "2511.11315v1",
        "authors": [
            "Jawad Ibn Ahad",
            "Muhammad Rafsan Kabir",
            "Robin Krambroeckers",
            "Sifat Momen",
            "Nabeel Mohammed",
            "Shafin Rahman"
        ],
        "submitted": "2025-11-14 13:57:46",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is primarily focused on Natural Language Processing (NLP) and pre-trained language models, which is somewhat related to the user's interests in NLP. However, the paper's focus on financial applications and language models is not directly aligned with the user's core research themes in Information Retrieval and Search technologies."
    },
    {
        "title": "SQuaD: The Software Quality Dataset",
        "abstract": "Software quality research increasingly relies on large-scale datasets that measure both the product and process aspects of software systems. However, existing resources often focus on limited dimensions, such as code smells, technical debt, or refactoring activity, thereby restricting comprehensive analyses across time and quality dimensions. To address this gap, we present the Software Quality Dataset (SQuaD), a multi-dimensional, time-aware collection of software quality metrics extracted from 450 mature open-source projects across diverse ecosystems, including Apache, Mozilla, FFmpeg, and the Linux kernel. By integrating nine state-of-the-art static analysis tools, i.e., SonarQube, CodeScene, PMD, Understand, CK, JaSoMe, RefactoringMiner, RefactoringMiner++, and PyRef, our dataset unifies over 700 unique metrics at method, class, file, and project levels. Covering a total of 63,586 analyzed project releases, SQuaD also provides version control and issue-tracking histories, software vulnerability data (CVE/CWE), and process metrics proven to enhance Just-In-Time (JIT) defect prediction. The SQuaD enables empirical research on maintainability, technical debt, software evolution, and quality assessment at unprecedented scale. We also outline emerging research directions, including automated dataset updates and cross-project quality modeling to support the continuous evolution of software analytics. The dataset is publicly available on ZENODO (DOI: 10.5281/zenodo.17566690).",
        "url": "http://arxiv.org/abs/2511.11265v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11265v1",
        "arxiv_id": "2511.11265v1",
        "authors": [
            "Mikel Robredo",
            "Matteo Esposito",
            "Davide Taibi",
            "Rafael Peñaloza",
            "Valentina Lenarduzzi"
        ],
        "submitted": "2025-11-14 12:57:22",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The focus on software quality metrics and datasets is outside your primary areas of interest."
    },
    {
        "title": "Evaluating Large Language Models on Rare Disease Diagnosis: A Case Study using House M.D",
        "abstract": "Large language models (LLMs) have demonstrated capabilities across diverse domains, yet their performance on rare disease diagnosis from narrative medical cases remains underexplored. We introduce a novel dataset of 176 symptom-diagnosis pairs extracted from House M.D., a medical television series validated for teaching rare disease recognition in medical education. We evaluate four state-of-the-art LLMs such as GPT 4o mini, GPT 5 mini, Gemini 2.5 Flash, and Gemini 2.5 Pro on narrative-based diagnostic reasoning tasks. Results show significant variation in performance, ranging from 16.48% to 38.64% accuracy, with newer model generations demonstrating a 2.3 times improvement. While all models face substantial challenges with rare disease diagnosis, the observed improvement across architectures suggests promising directions for future development. Our educationally validated benchmark establishes baseline performance metrics for narrative medical reasoning and provides a publicly accessible evaluation framework for advancing AI-assisted diagnosis research.",
        "url": "http://arxiv.org/abs/2511.10912v1",
        "pdf_url": "https://arxiv.org/pdf/2511.10912v1",
        "arxiv_id": "2511.10912v1",
        "authors": [
            "Arsh Gupta",
            "Ajay Narayanan Sridhar",
            "Bonam Mingole",
            "Amulya Yadav"
        ],
        "submitted": "2025-11-14 02:54:58",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Natural Language Processing (NLP) and Information Retrieval (IR), particularly in the context of deep semantic understanding. However, the focus on rare disease diagnosis using large language models is not directly aligned with your primary interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is limited to the NLP aspect of your research."
    },
    {
        "title": "A Multifaceted Analysis of Negative Bias in Large Language Models through the Lens of Parametric Knowledge",
        "abstract": "Negative bias refers to the tendency of large language models (LLMs) to excessively generate negative responses in binary decision tasks (e.g., yes-no question answering). Previous research has focused on detecting and addressing negative attention heads that induce negative bias. However, the underlying detailed factors influencing negative bias remain underexplored. In this paper, we demonstrate that LLMs exhibit format-level negative bias, meaning the prompt format more influences their responses than the semantics of the negative response. For the fine-grained study of the negative bias, we introduce a pipeline for constructing the evaluation set, which systematically categorizes the dataset into three subsets based on the model's parametric knowledge: correct, incorrect, and insufficient relevant knowledge. Through analysis of this evaluation set, we identify a shortcut behavior in which models tend to generate negative responses when they lack sufficient knowledge to answer a yes-no question, leading to negative bias. We further examine how negative bias changes under various prompting scenarios related to parametric knowledge. We observe that providing relevant context and offering an \"I don't know\" option generally reduces negative bias, whereas chain-of-thought prompting tends to amplify the bias. Finally, we demonstrate that the degree of negative bias can vary depending on the type of prompt, which influences the direction of the response. Our work reveals the various factors that influence negative bias, providing critical insights for mitigating it in LLMs.",
        "url": "http://arxiv.org/abs/2511.10881v1",
        "pdf_url": "https://arxiv.org/pdf/2511.10881v1",
        "arxiv_id": "2511.10881v1",
        "authors": [
            "Jongyoon Song",
            "Sangwon Yu",
            "Sungroh Yoon"
        ],
        "submitted": "2025-11-14 01:18:18",
        "source": "arxiv",
        "comment": "Accepted to IEEE Transactions on Audio, Speech and Language Processing",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat relevant to your research interests in Natural Language Processing (NLP) and large language models, but it does not directly align with your focus on Information Retrieval (IR), query understanding, or ranking models. The paper's analysis of negative bias in LLMs may have implications for IR systems, but it is not a central match for your research themes."
    },
    {
        "title": "LLM-as-a-Grader: Practical Insights from Large Language Model for Short-Answer and Report Evaluation",
        "abstract": "Large Language Models (LLMs) are increasingly explored for educational tasks such as grading, yet their alignment with human evaluation in real classrooms remains underexamined. In this study, we investigate the feasibility of using an LLM (GPT-4o) to evaluate short-answer quizzes and project reports in an undergraduate Computational Linguistics course. We collect responses from approximately 50 students across five quizzes and receive project reports from 14 teams. LLM-generated scores are compared against human evaluations conducted independently by the course teaching assistants (TAs). Our results show that GPT-4o achieves strong correlation with human graders (up to 0.98) and exact score agreement in 55\\% of quiz cases. For project reports, it also shows strong overall alignment with human grading, while exhibiting some variability in scoring technical, open-ended responses. We release all code and sample data to support further research on LLMs in educational assessment. This work highlights both the potential and limitations of LLM-based grading systems and contributes to advancing automated grading in real-world academic settings.",
        "url": "http://arxiv.org/abs/2511.10819v1",
        "pdf_url": "https://arxiv.org/pdf/2511.10819v1",
        "arxiv_id": "2511.10819v1",
        "authors": [
            "Grace Byun",
            "Swati Rajwal",
            "Jinho D. Choi"
        ],
        "submitted": "2025-11-13 21:38:25",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the application of Large Language Models (LLMs) in educational tasks, specifically grading. While it touches on aspects of query understanding and ranking models, the primary focus is on a different domain (education) and task (grading), making it somewhat related but not a central match to your research interests in Information Retrieval and Search technologies."
    }
]