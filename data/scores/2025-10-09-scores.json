[
    {
        "title": "LAD-RAG: Layout-aware Dynamic RAG for Visually-Rich Document Understanding",
        "abstract": "Question answering over visually rich documents (VRDs) requires reasoning not\nonly over isolated content but also over documents' structural organization and\ncross-page dependencies. However, conventional retrieval-augmented generation\n(RAG) methods encode content in isolated chunks during ingestion, losing\nstructural and cross-page dependencies, and retrieve a fixed number of pages at\ninference, regardless of the specific demands of the question or context. This\noften results in incomplete evidence retrieval and degraded answer quality for\nmulti-page reasoning tasks. To address these limitations, we propose LAD-RAG, a\nnovel Layout-Aware Dynamic RAG framework. During ingestion, LAD-RAG constructs\na symbolic document graph that captures layout structure and cross-page\ndependencies, adding it alongside standard neural embeddings to yield a more\nholistic representation of the document. During inference, an LLM agent\ndynamically interacts with the neural and symbolic indices to adaptively\nretrieve the necessary evidence based on the query. Experiments on\nMMLongBench-Doc, LongDocURL, DUDE, and MP-DocVQA demonstrate that LAD-RAG\nimproves retrieval, achieving over 90% perfect recall on average without any\ntop-k tuning, and outperforming baseline retrievers by up to 20% in recall at\ncomparable noise levels, yielding higher QA accuracy with minimal latency.",
        "url": "http://arxiv.org/abs/2510.07233v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07233v1",
        "arxiv_id": "2510.07233v1",
        "authors": [
            "Zhivar Sourati",
            "Zheng Wang",
            "Marianne Menglin Liu",
            "Yazhe Hu",
            "Mengqing Guo",
            "Sujeeth Bharadwaj",
            "Kyu Han",
            "Tao Sheng",
            "Sujith Ravi",
            "Morteza Dehghani",
            "Dan Roth"
        ],
        "submitted": "2025-10-08 17:02:04",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores visually-rich document understanding, which is somewhat related to information retrieval and query understanding. However, it focuses on visually-rich documents and uses a novel framework to capture layout structure and cross-page dependencies, which is not directly aligned with the user's core research themes in e-commerce or real-time relevance optimization."
    },
    {
        "title": "Towards Reliable Retrieval in RAG Systems for Large Legal Datasets",
        "abstract": "Retrieval-Augmented Generation (RAG) is a promising approach to mitigate\nhallucinations in Large Language Models (LLMs) for legal applications, but its\nreliability is critically dependent on the accuracy of the retrieval step. This\nis particularly challenging in the legal domain, where large databases of\nstructurally similar documents often cause retrieval systems to fail. In this\npaper, we address this challenge by first identifying and quantifying a\ncritical failure mode we term Document-Level Retrieval Mismatch (DRM), where\nthe retriever selects information from entirely incorrect source documents. To\nmitigate DRM, we investigate a simple and computationally efficient technique\nwhich we refer to as Summary-Augmented Chunking (SAC). This method enhances\neach text chunk with a document-level synthetic summary, thereby injecting\ncrucial global context that would otherwise be lost during a standard chunking\nprocess. Our experiments on a diverse set of legal information retrieval tasks\nshow that SAC greatly reduces DRM and, consequently, also improves text-level\nretrieval precision and recall. Interestingly, we find that a generic\nsummarization strategy outperforms an approach that incorporates legal expert\ndomain knowledge to target specific legal elements. Our work provides evidence\nthat this practical, scalable, and easily integrable technique enhances the\nreliability of RAG systems when applied to large-scale legal document datasets.",
        "url": "http://arxiv.org/abs/2510.06999v1",
        "pdf_url": "http://arxiv.org/pdf/2510.06999v1",
        "arxiv_id": "2510.06999v1",
        "authors": [
            "Markus Reuter",
            "Tobias Lingenberg",
            "Rūta Liepiņa",
            "Francesca Lagioia",
            "Marco Lippi",
            "Giovanni Sartor",
            "Andrea Passerini",
            "Burcu Sayin"
        ],
        "submitted": "2025-10-08 13:22:20",
        "source": "arxiv",
        "comment": "Accepted for the 7th Natural Legal Language Processing Workshop (NLLP\n  2025), co-located with EMNLP 2025",
        "score": 10,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'retriever' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 7,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval, specifically in the context of Retrieval-Augmented Generation (RAG) systems. While it focuses on the legal domain, the techniques and challenges discussed, such as Document-Level Retrieval Mismatch (DRM), are relevant to your interests in query understanding and ranking models. However, the paper's primary focus on the legal domain and the specific application of RAG systems limits its alignment with your broader research themes."
    },
    {
        "title": "All Claims Are Equal, but Some Claims Are More Equal Than Others: Importance-Sensitive Factuality Evaluation of LLM Generations",
        "abstract": "Existing methods for evaluating the factuality of large language model (LLM)\nresponses treat all claims as equally important. This results in misleading\nevaluations when vital information is missing or incorrect as it receives the\nsame weight as peripheral details, raising the question: how can we reliably\ndetect such differences when there are errors in key information? Current\napproaches that measure factuality tend to be insensitive to omitted or false\nkey information. To investigate this lack of sensitivity, we construct\nVITALERRORS, a benchmark of 6,733 queries with minimally altered LLM responses\ndesigned to omit or falsify key information. Using this dataset, we demonstrate\nthe insensitivities of existing evaluation metrics to key information errors.\nTo address this gap, we introduce VITAL, a set of metrics that provide greater\nsensitivity in measuring the factuality of responses by incorporating the\nrelevance and importance of claims with respect to the query. Our analysis\ndemonstrates that VITAL metrics more reliably detect errors in key information\nthan previous methods. Our dataset, metrics, and analysis provide a foundation\nfor more accurate and robust assessment of LLM factuality.",
        "url": "http://arxiv.org/abs/2510.07083v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07083v1",
        "arxiv_id": "2510.07083v1",
        "authors": [
            "Miriam Wanner",
            "Leif Azzopardi",
            "Paul Thomas",
            "Soham Dan",
            "Benjamin Van Durme",
            "Nick Craswell"
        ],
        "submitted": "2025-10-08 14:40:33",
        "source": "arxiv",
        "comment": null,
        "score": 9,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper is somewhat related to your interests in Information Retrieval, particularly in the context of evaluating the factuality of large language model responses. However, it focuses more on Natural Language Processing and factuality evaluation, which, while related to your broader interests, is not a central match."
    },
    {
        "title": "Making Machines Sound Sarcastic: LLM-Enhanced and Retrieval-Guided Sarcastic Speech Synthesis",
        "abstract": "Sarcasm is a subtle form of non-literal language that poses significant\nchallenges for speech synthesis due to its reliance on nuanced semantic,\ncontextual, and prosodic cues. While existing speech synthesis research has\nfocused primarily on broad emotional categories, sarcasm remains largely\nunexplored. In this paper, we propose a Large Language Model (LLM)-enhanced\nRetrieval-Augmented framework for sarcasm-aware speech synthesis. Our approach\ncombines (1) semantic embeddings from a LoRA-fine-tuned LLaMA 3, which capture\npragmatic incongruity and discourse-level cues of sarcasm, and (2) prosodic\nexemplars retrieved via a Retrieval Augmented Generation (RAG) module, which\nprovide expressive reference patterns of sarcastic delivery. Integrated within\na VITS backbone, this dual conditioning enables more natural and contextually\nappropriate sarcastic speech. Experiments demonstrate that our method\noutperforms baselines in both objective measures and subjective evaluations,\nyielding improvements in speech naturalness, sarcastic expressivity, and\ndownstream sarcasm detection.",
        "url": "http://arxiv.org/abs/2510.07096v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07096v1",
        "arxiv_id": "2510.07096v1",
        "authors": [
            "Zhu Li",
            "Yuqing Zhang",
            "Xiyuan Gao",
            "Shekhar Nayak",
            "Matt Coler"
        ],
        "submitted": "2025-10-08 14:53:48",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on speech synthesis and sarcasm detection, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve deep semantic understanding, the context is speech synthesis rather than text-based information retrieval."
    },
    {
        "title": "Agent Bain vs. Agent McKinsey: A New Text-to-SQL Benchmark for the Business Domain",
        "abstract": "In the business domain, where data-driven decision making is crucial,\ntext-to-SQL is fundamental for easy natural language access to structured data.\nWhile recent LLMs have achieved strong performance in code generation, existing\ntext-to-SQL benchmarks remain focused on factual retrieval of past records. We\nintroduce CORGI, a new benchmark specifically designed for real-world business\ncontexts. CORGI is composed of synthetic databases inspired by enterprises such\nas Doordash, Airbnb, and Lululemon. It provides questions across four\nincreasingly complex categories of business queries: descriptive, explanatory,\npredictive, and recommendational. This challenge calls for causal reasoning,\ntemporal forecasting, and strategic recommendation, reflecting multi-level and\nmulti-step agentic intelligence. We find that LLM performance drops on\nhigh-level questions, struggling to make accurate predictions and offer\nactionable plans. Based on execution success rate, the CORGI benchmark is about\n21% more difficult than the BIRD benchmark. This highlights the gap between\npopular LLMs and the need for real-world business intelligence. We release a\npublic dataset and evaluation framework, and a website for public submissions.",
        "url": "http://arxiv.org/abs/2510.07309v2",
        "pdf_url": "http://arxiv.org/pdf/2510.07309v2",
        "arxiv_id": "2510.07309v2",
        "authors": [
            "Yue Li",
            "Ran Tao",
            "Derek Hommel",
            "Yusuf Denizay Dönder",
            "Sungyong Chang",
            "David Mimno",
            "Unso Eun Seo Jo"
        ],
        "submitted": "2025-10-08 17:57:35",
        "source": "arxiv",
        "comment": "20 pages, 6 figures, under review for ACL ARR; typos corrected",
        "score": 6,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to the user's interests in Information Retrieval and Natural Language Processing, but it focuses on a specific task (text-to-SQL) and domain (business) that is not the user's primary focus. While it involves query understanding and ranking models, the context is more aligned with recommender systems and data mining than the user's core research themes."
    },
    {
        "title": "Comparing human and language models sentence processing difficulties on complex structures",
        "abstract": "Large language models (LLMs) that fluently converse with humans are a reality\n- but do LLMs experience human-like processing difficulties? We systematically\ncompare human and LLM sentence comprehension across seven challenging\nlinguistic structures. We collect sentence comprehension data from humans and\nfive families of state-of-the-art LLMs, varying in size and training procedure\nin a unified experimental framework. Our results show LLMs overall struggle on\nthe target structures, but especially on garden path (GP) sentences. Indeed,\nwhile the strongest models achieve near perfect accuracy on non-GP structures\n(93.7% for GPT-5), they struggle on GP structures (46.8% for GPT-5).\nAdditionally, when ranking structures based on average performance, rank\ncorrelation between humans and models increases with parameter count. For each\ntarget structure, we also collect data for their matched baseline without the\ndifficult structure. Comparing performance on the target vs. baseline\nsentences, the performance gap observed in humans holds for LLMs, with two\nexceptions: for models that are too weak performance is uniformly low across\nboth sentence types, and for models that are too strong the performance is\nuniformly high. Together, these reveal convergence and divergence in human and\nLLM sentence comprehension, offering new insights into the similarity of humans\nand LLMs.",
        "url": "http://arxiv.org/abs/2510.07141v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07141v1",
        "arxiv_id": "2510.07141v1",
        "authors": [
            "Samuel Joseph Amouyal",
            "Aya Meltzer-Asscher",
            "Jonathan Berant"
        ],
        "submitted": "2025-10-08 15:42:49",
        "source": "arxiv",
        "comment": "Data and code will be released soon",
        "score": 6,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the sentence comprehension abilities of large language models, comparing them to human performance. While it touches on aspects of natural language processing, it doesn't directly relate to information retrieval, query understanding, or ranking models, which are core areas of your research interests."
    },
    {
        "title": "Search-R3: Unifying Reasoning and Embedding Generation in Large Language Models",
        "abstract": "Despite their remarkable natural language understanding capabilities, Large\nLanguage Models (LLMs) have been underutilized for retrieval tasks. We present\nSearch-R3, a novel framework that addresses this limitation by adapting LLMs to\ngenerate search embeddings as a direct output of their reasoning process. Our\napproach exploits LLMs' chain-of-thought capabilities, allowing them to produce\nmore effective embeddings by reasoning step-by-step through complex semantic\nanalyses. We implement this through three complementary mechanisms. (1) a\nsupervised learning stage enables the model's ability to produce quality\nembeddings, (2) a reinforcement learning (RL) methodology that optimizes\nembedding generation alongside reasoning, and (3) a specialized RL environment\nthat efficiently handles evolving embedding representations without requiring\ncomplete corpus re-encoding at each training iteration. Our extensive\nevaluations on diverse benchmarks demonstrate that Search-R3 significantly\noutperforms prior methods by unifying the reasoning and embedding generation\nprocesses. This integrated post-training approach represents a substantial\nadvancement in handling complex knowledge-intensive tasks that require both\nsophisticated reasoning and effective information retrieval. Project page:\nhttps://github.com/ytgui/Search-R3",
        "url": "http://arxiv.org/abs/2510.07048v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07048v1",
        "arxiv_id": "2510.07048v1",
        "authors": [
            "Yuntao Gui",
            "James Cheng"
        ],
        "submitted": "2025-10-08 14:16:20",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper presents a novel framework, Search-R3, that integrates reasoning and embedding generation in Large Language Models for retrieval tasks, aligning with your interests in Information Retrieval and deep semantic understanding. The approach exploits LLMs' chain-of-thought capabilities and demonstrates significant improvements over prior methods. While the focus is on LLMs and NLP, the relevance to IR and search technologies is clear."
    },
    {
        "title": "LongRM: Revealing and Unlocking the Context Boundary of Reward Modeling",
        "abstract": "Reward model (RM) plays a pivotal role in aligning large language model (LLM)\nwith human preferences. As real-world applications increasingly involve long\nhistory trajectories, e.g., LLM agent, it becomes indispensable to evaluate\nwhether a model's responses are not only high-quality but also grounded in and\nconsistent with the provided context. Yet, current RMs remain confined to\nshort-context settings and primarily focus on response-level attributes (e.g.,\nsafety or helpfulness), while largely neglecting the critical dimension of long\ncontext-response consistency. In this work, we introduce Long-RewardBench, a\nbenchmark specifically designed for long-context RM evaluation, featuring both\nPairwise Comparison and Best-of-N tasks. Our preliminary study reveals that\neven state-of-the-art generative RMs exhibit significant fragility in\nlong-context scenarios, failing to maintain context-aware preference judgments.\nMotivated by the analysis of failure patterns observed in model outputs, we\npropose a general multi-stage training strategy that effectively scales\narbitrary models into robust Long-context RMs (LongRMs). Experiments show that\nour approach not only substantially improves performance on long-context\nevaluation but also preserves strong short-context capability. Notably, our 8B\nLongRM outperforms much larger 70B-scale baselines and matches the performance\nof the proprietary Gemini 2.5 Pro model.",
        "url": "http://arxiv.org/abs/2510.06915v1",
        "pdf_url": "http://arxiv.org/pdf/2510.06915v1",
        "arxiv_id": "2510.06915v1",
        "authors": [
            "Zecheng Tang",
            "Baibei Ji",
            "Quantong Qiu",
            "Haitian Wang",
            "Xiaobo Liang",
            "Juntao Li",
            "Min Zhang"
        ],
        "submitted": "2025-10-08 11:48:16",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper is somewhat related to the user's interests in Information Retrieval, particularly in the context of query understanding and ranking models. However, the focus on reward modeling and long-context scenarios is not a central match for the user's primary research themes, which include query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Customer-R1: Personalized Simulation of Human Behaviors via RL-based LLM Agent in Online Shopping",
        "abstract": "Simulating step-wise human behavior with Large Language Models (LLMs) has\nbecome an emerging research direction, enabling applications in various\npractical domains. While prior methods, including prompting, supervised\nfine-tuning (SFT), and reinforcement learning (RL), have shown promise in\nmodeling step-wise behavior, they primarily learn a population-level policy\nwithout conditioning on a user's persona, yielding generic rather than\npersonalized simulations. In this work, we pose a critical question: how can\nLLM agents better simulate personalized user behavior? We introduce\nCustomer-R1, an RL-based method for personalized, step-wise user behavior\nsimulation in online shopping environments. Our policy is conditioned on an\nexplicit persona, and we optimize next-step rationale and action generation via\naction correctness reward signals. Experiments on the OPeRA dataset emonstrate\nthat Customer-R1 not only significantly outperforms prompting and SFT-based\nbaselines in next-action prediction tasks, but also better matches users'\naction distribution, indicating higher fidelity in personalized behavior\nsimulation.",
        "url": "http://arxiv.org/abs/2510.07230v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07230v1",
        "arxiv_id": "2510.07230v1",
        "authors": [
            "Ziyi Wang",
            "Yuxuan Lu",
            "Yimeng Zhang",
            "Jing Huang",
            "Dakuo Wang"
        ],
        "submitted": "2025-10-08 17:00:25",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'user behavior' (score: +2)",
            "Found 'shopping' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of user behavior modeling. The use of Reinforcement Learning (RL) and Large Language Models (LLMs) to simulate personalized user behavior in online shopping environments aligns with your focus on query understanding and ranking models. The paper's emphasis on real-time relevance optimization also resonates with your interests."
    },
    {
        "title": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio Understanding and Efficiency in Audio LLMs",
        "abstract": "Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks.",
        "url": "http://arxiv.org/abs/2510.07293v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07293v1",
        "arxiv_id": "2510.07293v1",
        "authors": [
            "Peize He",
            "Zichen Wen",
            "Yubo Wang",
            "Yuxuan Wang",
            "Xiaoqian Liu",
            "Jiajie Huang",
            "Zehui Lei",
            "Zhuangcheng Gu",
            "Xiangqi Jin",
            "Jiabing Yang",
            "Kai Li",
            "Zhifei Liu",
            "Weijia Li",
            "Cunxiang Wang",
            "Conghui He",
            "Linfeng Zhang"
        ],
        "submitted": "2025-10-08 17:50:16",
        "source": "arxiv",
        "comment": "26 pages, 23 figures, the code is available at\n  \\url{https://github.com/DabDans/AudioMarathon}",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. Although it involves Large Language Models (LLMs), the focus is on audio understanding and efficiency, which is outside your primary areas of interest."
    },
    {
        "title": "Online Rubrics Elicitation from Pairwise Comparisons",
        "abstract": "Rubrics provide a flexible way to train LLMs on open-ended long-form answers\nwhere verifiable rewards are not applicable and human preferences provide\ncoarse signals. Prior work shows that reinforcement learning with rubric-based\nrewards leads to consistent gains in LLM post-training. Most existing\napproaches rely on rubrics that remain static over the course of training. Such\nstatic rubrics, however, are vulnerable to reward-hacking type behaviors and\nfail to capture emergent desiderata that arise during training. We introduce\nOnline Rubrics Elicitation (OnlineRubrics), a method that dynamically curates\nevaluation criteria in an online manner through pairwise comparisons of\nresponses from current and reference policies. This online process enables\ncontinuous identification and mitigation of errors as training proceeds.\nEmpirically, this approach yields consistent improvements of up to 8% over\ntraining exclusively with static rubrics across AlpacaEval, GPQA, ArenaHard as\nwell as the validation sets of expert questions and rubrics. We qualitatively\nanalyze the elicited criteria and identify prominent themes such as\ntransparency, practicality, organization, and reasoning.",
        "url": "http://arxiv.org/abs/2510.07284v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07284v1",
        "arxiv_id": "2510.07284v1",
        "authors": [
            "MohammadHossein Rezaei",
            "Robert Vacareanu",
            "Zihao Wang",
            "Clinton Wang",
            "Yunzhong He",
            "Afra Feyza Akyürek"
        ],
        "submitted": "2025-10-08 17:44:59",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Natural Language Processing (NLP) and Learning to Rank, as it discusses training Large Language Models (LLMs) with rubric-based rewards. However, the focus on rubric elicitation and its application to LLMs is not directly aligned with your core research themes in Information Retrieval and Search technologies."
    },
    {
        "title": "When Benchmarks Age: Temporal Misalignment through Large Language Model Factuality Evaluation",
        "abstract": "The rapid evolution of large language models (LLMs) and the real world has\noutpaced the static nature of widely used evaluation benchmarks, raising\nconcerns about their reliability for evaluating LLM factuality. While\nsubstantial works continue to rely on the popular but old benchmarks, their\ntemporal misalignment with real-world facts and modern LLMs, and their effects\non LLM factuality evaluation remain underexplored. Therefore, in this work, we\npresent a systematic investigation of this issue by examining five popular\nfactuality benchmarks and eight LLMs released across different years. An\nup-to-date fact retrieval pipeline and three metrics are tailored to quantify\nbenchmark aging and its impact on LLM factuality evaluation. Experimental\nresults and analysis illustrate that a considerable portion of samples in the\nwidely used factuality benchmarks are outdated, leading to unreliable\nassessments of LLM factuality. We hope our work can provide a testbed to assess\nthe reliability of a benchmark for LLM factuality evaluation and inspire more\nresearch on the benchmark aging issue. Codes are available in\nhttps://github.com/JiangXunyi/BenchAge.",
        "url": "http://arxiv.org/abs/2510.07238v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07238v1",
        "arxiv_id": "2510.07238v1",
        "authors": [
            "Xunyi Jiang",
            "Dingyi Chang",
            "Julian McAuley",
            "Xin Xu"
        ],
        "submitted": "2025-10-08 17:06:07",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on the evaluation of large language models, which is a topic in Natural Language Processing (NLP), but it does not directly relate to Information Retrieval (IR), query understanding, ranking models, or user behavior modeling, which are the core areas of your research interests."
    },
    {
        "title": "NurseLLM: The First Specialized Language Model for Nursing",
        "abstract": "Recent advancements in large language models (LLMs) have significantly\ntransformed medical systems. However, their potential within specialized\ndomains such as nursing remains largely underexplored. In this work, we\nintroduce NurseLLM, the first nursing-specialized LLM tailored for multiple\nchoice question-answering (MCQ) tasks. We develop a multi-stage data generation\npipeline to build the first large scale nursing MCQ dataset to train LLMs on a\nbroad spectrum of nursing topics. We further introduce multiple nursing\nbenchmarks to enable rigorous evaluation. Our extensive experiments demonstrate\nthat NurseLLM outperforms SoTA general-purpose and medical-specialized LLMs of\ncomparable size on different benchmarks, underscoring the importance of a\nspecialized LLM for the nursing domain. Finally, we explore the role of\nreasoning and multi-agent collaboration systems in nursing, highlighting their\npromise for future research and applications.",
        "url": "http://arxiv.org/abs/2510.07173v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07173v1",
        "arxiv_id": "2510.07173v1",
        "authors": [
            "Md Tawkat Islam Khondaker",
            "Julia Harrington",
            "Shady Shehata"
        ],
        "submitted": "2025-10-08 16:15:06",
        "source": "arxiv",
        "comment": "EMNLP 2025 Industry Track",
        "score": 3,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on developing a language model for nursing, which is a specialized domain unrelated to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves language models, the application and context are distinct from the user's areas of focus."
    },
    {
        "title": "A Multi-Agent Framework for Stateful Inference-Time Search",
        "abstract": "Recent work explores agentic inference-time techniques to perform structured,\nmulti-step reasoning. However, stateless inference often struggles on\nmulti-step tasks due to the absence of persistent state. Moreover,\ntask-specific fine-tuning or instruction-tuning often achieve surface-level\ncode generation but remain brittle on tasks requiring deeper reasoning and\nlong-horizon dependencies. To address these limitations, we propose stateful\nmulti-agent evolutionary search, a training-free framework that departs from\nprior stateless approaches by combining (i) persistent inference-time state,\n(ii) adversarial mutation, and (iii) evolutionary preservation. We demonstrate\nits effectiveness in automated unit test generation through the generation of\nedge cases. We generate robust edge cases using an evolutionary search process,\nwhere specialized agents sequentially propose, mutate, and score candidates. A\ncontroller maintains persistent state across generations, while evolutionary\npreservation ensures diversity and exploration across all possible cases. This\nyields a generalist agent capable of discovering robust, high-coverage edge\ncases across unseen codebases. Experiments show our stateful multi-agent\ninference framework achieves substantial gains in coverage over stateless\nsingle-step baselines, evaluated on prevalent unit-testing benchmarks such as\nHumanEval and TestGenEvalMini and using three diverse LLM families - Llama,\nGemma, and GPT. These results indicate that combining persistent inference-time\nstate with evolutionary search materially improves unit-test generation.",
        "url": "http://arxiv.org/abs/2510.07147v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07147v1",
        "arxiv_id": "2510.07147v1",
        "authors": [
            "Arshika Lalan",
            "Rajat Ghosh",
            "Aditya Kolsur",
            "Debojyoti Dutta"
        ],
        "submitted": "2025-10-08 15:48:41",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper proposes a multi-agent framework for stateful inference-time search, which is somewhat related to information retrieval and search technologies. However, the focus on automated unit test generation and code analysis is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling. The connection to deep semantic understanding and real-time relevance optimization is also not explicitly mentioned."
    },
    {
        "title": "TRIM: Token-wise Attention-Derived Saliency for Data-Efficient Instruction Tuning",
        "abstract": "Instruction tuning is essential for aligning large language models (LLMs) to\ndownstream tasks and commonly relies on large, diverse corpora. However, small,\nhigh-quality subsets, known as coresets, can deliver comparable or superior\nresults, though curating them remains challenging. Existing methods often rely\non coarse, sample-level signals like gradients, an approach that is\ncomputationally expensive and overlooks fine-grained features. To address this,\nwe introduce TRIM (Token Relevance via Interpretable Multi-layer Attention), a\nforward-only, token-centric framework. Instead of using gradients, TRIM\noperates by matching underlying representational patterns identified via\nattention-based \"fingerprints\" from a handful of target samples. Such an\napproach makes TRIM highly efficient and uniquely sensitive to the structural\nfeatures that define a task. Coresets selected by our method consistently\noutperform state-of-the-art baselines by up to 9% on downstream tasks and even\nsurpass the performance of full-data fine-tuning in some settings. By avoiding\nexpensive backward passes, TRIM achieves this at a fraction of the\ncomputational cost. These findings establish TRIM as a scalable and efficient\nalternative for building high-quality instruction-tuning datasets.",
        "url": "http://arxiv.org/abs/2510.07118v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07118v1",
        "arxiv_id": "2510.07118v1",
        "authors": [
            "Manish Nagaraj",
            "Sakshi Choudhary",
            "Utkarsh Saxena",
            "Deepak Ravikumar",
            "Kaushik Roy"
        ],
        "submitted": "2025-10-08 15:11:04",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on instruction tuning for large language models, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves attention-based mechanisms, the context is NLP and dataset optimization, rather than query understanding or ranking models."
    },
    {
        "title": "VelLMes: A high-interaction AI-based deception framework",
        "abstract": "There are very few SotA deception systems based on Large Language Models. The\nexisting ones are limited only to simulating one type of service, mainly SSH\nshells. These systems - but also the deception technologies not based on LLMs -\nlack an extensive evaluation that includes human attackers. Generative AI has\nrecently become a valuable asset for cybersecurity researchers and\npractitioners, and the field of cyber-deception is no exception. Researchers\nhave demonstrated how LLMs can be leveraged to create realistic-looking\nhoneytokens, fake users, and even simulated systems that can be used as\nhoneypots. This paper presents an AI-based deception framework called VelLMes,\nwhich can simulate multiple protocols and services such as SSH Linux shell,\nMySQL, POP3, and HTTP. All of these can be deployed and used as honeypots, thus\nVelLMes offers a variety of choices for deception design based on the users'\nneeds. VelLMes is designed to be attacked by humans, so interactivity and\nrealism are key for its performance. We evaluate the generative capabilities\nand the deception capabilities. Generative capabilities were evaluated using\nunit tests for LLMs. The results of the unit tests show that, with careful\nprompting, LLMs can produce realistic-looking responses, with some LLMs having\na 100% passing rate. In the case of the SSH Linux shell, we evaluated deception\ncapabilities with 89 human attackers. The results showed that about 30% of the\nattackers thought that they were interacting with a real system when they were\nassigned an LLM-based honeypot. Lastly, we deployed 10 instances of the SSH\nLinux shell honeypot on the Internet to capture real-life attacks. Analysis of\nthese attacks showed us that LLM honeypots simulating Linux shells can perform\nwell against unstructured and unexpected attacks on the Internet, responding\ncorrectly to most of the issued commands.",
        "url": "http://arxiv.org/abs/2510.06975v1",
        "pdf_url": "http://arxiv.org/pdf/2510.06975v1",
        "arxiv_id": "2510.06975v1",
        "authors": [
            "Muris Sladić",
            "Veronica Valeros",
            "Carlos Catania",
            "Sebastian Garcia"
        ],
        "submitted": "2025-10-08 13:00:23",
        "source": "arxiv",
        "comment": "9 pages. 9 figures. 1 table. This is a preprint of a paper that was\n  presented at the Active Defense and Deception Workshop colocated with IEEE\n  EuroS&P 2025 conference",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve Large Language Models, the focus is on deception and cybersecurity, which is not a central match to your research themes."
    },
    {
        "title": "Ethical AI prompt recommendations in large language models using collaborative filtering",
        "abstract": "As large language models (LLMs) shape AI development, ensuring ethical prompt\nrecommendations is crucial. LLMs offer innovation but risk bias, fairness\nissues, and accountability concerns. Traditional oversight methods struggle\nwith scalability, necessitating dynamic solutions. This paper proposes using\ncollaborative filtering, a technique from recommendation systems, to enhance\nethical prompt selection. By leveraging user interactions, it promotes ethical\nguidelines while reducing bias. Contributions include a synthetic dataset for\nprompt recommendations and the application of collaborative filtering. The work\nalso tackles challenges in ethical AI, such as bias mitigation, transparency,\nand preventing unethical prompt engineering.",
        "url": "http://arxiv.org/abs/2510.06924v1",
        "pdf_url": "http://arxiv.org/pdf/2510.06924v1",
        "arxiv_id": "2510.06924v1",
        "authors": [
            "Jordan Nelson",
            "Almas Baimagambetov",
            "Konstantinos Avgerinakis",
            "Nikolaos Polatidis"
        ],
        "submitted": "2025-10-08 12:03:21",
        "source": "arxiv",
        "comment": "This paper has been accepted to by the International Journal of\n  Parallel, Emergent & Distributed Systems (Taylor and Francis) and has an\n  assigned DOI. We have already chose to make this open access using CC BY. The\n  article is not yet available online on the publisher's website. The DOI is:\n  doi.org/10.1080/17445760.2025.2573086",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on large language models and collaborative filtering for ethical prompt recommendations, which is somewhat related to your interests in NLP and related topics. However, it does not directly align with your core research themes in Information Retrieval, query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
        "abstract": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN.",
        "url": "http://arxiv.org/abs/2510.07318v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07318v1",
        "arxiv_id": "2510.07318v1",
        "authors": [
            "Yunhao Fang",
            "Weihao Yu",
            "Shu Zhong",
            "Qinghao Ye",
            "Xuehan Xiong",
            "Lai Wei"
        ],
        "submitted": "2025-10-08 17:59:55",
        "source": "arxiv",
        "comment": "Code: https://github.com/ByteDance-Seed/AHN",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on long-context modeling using artificial neural networks, which is not directly related to information retrieval, search technologies, or query understanding. While it involves deep learning architectures, the context is more aligned with NLP and sequence modeling, but lacks relevance to the user's core research themes."
    },
    {
        "title": "Vibe Checker: Aligning Code Evaluation with Human Preference",
        "abstract": "Large Language Models (LLMs) have catalyzed vibe coding, where users leverage\nLLMs to generate and iteratively refine code through natural language\ninteractions until it passes their vibe check. Vibe check is tied to real-world\nhuman preference and goes beyond functionality: the solution should feel right,\nread cleanly, preserve intent, and remain correct. However, current code\nevaluation remains anchored to pass@k and captures only functional correctness,\noverlooking the non-functional instructions that users routinely apply. In this\npaper, we hypothesize that instruction following is the missing piece\nunderlying vibe check that represents human preference in coding besides\nfunctional correctness. To quantify models' code instruction following\ncapabilities with measurable signals, we present VeriCode, a taxonomy of 30\nverifiable code instructions together with corresponding deterministic\nverifiers. We use the taxonomy to augment established evaluation suites,\nresulting in Vibe Checker, a testbed to assess both code instruction following\nand functional correctness. Upon evaluating 31 leading LLMs, we show that even\nthe strongest models struggle to comply with multiple instructions and exhibit\nclear functional regression. Most importantly, a composite score of functional\ncorrectness and instruction following correlates the best with human\npreference, with the latter emerging as the primary differentiator on\nreal-world programming tasks. Our work identifies core factors of the vibe\ncheck, providing a concrete path for benchmarking and developing models that\nbetter align with user preferences in coding.",
        "url": "http://arxiv.org/abs/2510.07315v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07315v1",
        "arxiv_id": "2510.07315v1",
        "authors": [
            "Ming Zhong",
            "Xiang Zhou",
            "Ting-Yun Chang",
            "Qingze Wang",
            "Nan Xu",
            "Xiance Si",
            "Dan Garrette",
            "Shyam Upadhyay",
            "Jeremiah Liu",
            "Jiawei Han",
            "Benoit Schillings",
            "Jiao Sun"
        ],
        "submitted": "2025-10-08 17:59:19",
        "source": "arxiv",
        "comment": "Preprint",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores code evaluation and instruction following, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on code generation and human preference in coding is not directly aligned with the user's core research themes, but it does involve natural language interactions and real-time relevance optimization, making it somewhat relevant."
    },
    {
        "title": "Don't Adapt Small Language Models for Tools; Adapt Tool Schemas to the Models",
        "abstract": "Small language models (SLMs) offer significant computational advantages for\ntool-augmented AI systems, yet they struggle with tool-use tasks, particularly\nin selecting appropriate tools and identifying correct parameters. A common\nfailure mode is schema misalignment: models hallucinate plausible but\nnon-existent tool names that reflect naming conventions internalized during\npretraining but absent from the provided tool schema. Rather than forcing\nmodels to adapt to arbitrary schemas, we propose adapting schemas to align with\nmodels' pretrained knowledge. We introduce PA-Tool (Pretraining-Aligned Tool\nSchema Generation), a training-free method that leverages peakedness-a signal\nfrom contamination detection indicating pretraining familiarity-to\nautomatically rename tool components. By generating multiple candidates and\nselecting those with highest output concentration across samples, PA-Tool\nidentifies pretrain-aligned naming patterns. Experiments on MetaTool and\nRoTBench show improvements of up to 17% points, with schema misalignment errors\nreduced by 80%. PA-Tool enables small models to approach state-of-the-art\nperformance while maintaining computational efficiency for adaptation to new\ntools without retraining. Our work demonstrates that schema-level interventions\ncan unlock the tool-use potential of resource-efficient models by adapting\nschemas to models rather than models to schemas.",
        "url": "http://arxiv.org/abs/2510.07248v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07248v1",
        "arxiv_id": "2510.07248v1",
        "authors": [
            "Jonggeun Lee",
            "Woojung Song",
            "Jongwook Han",
            "Haesung Pyun",
            "Yohan Jo"
        ],
        "submitted": "2025-10-08 17:16:07",
        "source": "arxiv",
        "comment": "15 pages, 4 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on adapting tool schemas to small language models, which is not directly related to information retrieval, query understanding, ranking models, or user behavior modeling. While it touches on the topic of model adaptation, it is more relevant to the NLP domain and does not seem to address real-time relevance optimization or deep semantic understanding, which are key areas of interest for your research."
    },
    {
        "title": "Hybrid Reinforcement: When Reward Is Sparse, It's Better to Be Dense",
        "abstract": "Post-training for reasoning of large language models (LLMs) increasingly\nrelies on verifiable rewards: deterministic checkers that provide 0-1\ncorrectness signals. While reliable, such binary feedback is brittle--many\ntasks admit partially correct or alternative answers that verifiers\nunder-credit, and the resulting all-or-nothing supervision limits learning.\nReward models offer richer, continuous feedback, which can serve as a\ncomplementary supervisory signal to verifiers. We introduce HERO (Hybrid\nEnsemble Reward Optimization), a reinforcement learning framework that\nintegrates verifier signals with reward-model scores in a structured way. HERO\nemploys stratified normalization to bound reward-model scores within\nverifier-defined groups, preserving correctness while refining quality\ndistinctions, and variance-aware weighting to emphasize challenging prompts\nwhere dense signals matter most. Across diverse mathematical reasoning\nbenchmarks, HERO consistently outperforms RM-only and verifier-only baselines,\nwith strong gains on both verifiable and hard-to-verify tasks. Our results show\nthat hybrid reward design retains the stability of verifiers while leveraging\nthe nuance of reward models to advance reasoning.",
        "url": "http://arxiv.org/abs/2510.07242v2",
        "pdf_url": "http://arxiv.org/pdf/2510.07242v2",
        "arxiv_id": "2510.07242v2",
        "authors": [
            "Leitian Tao",
            "Ilia Kulikov",
            "Swarnadeep Saha",
            "Tianlu Wang",
            "Jing Xu",
            "Yixuan Li",
            "Jason E Weston",
            "Ping Yu"
        ],
        "submitted": "2025-10-08 17:09:41",
        "source": "arxiv",
        "comment": "21 pages",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on reinforcement learning and reward models for post-training large language models, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves optimization and ranking, the context is NLP and not specifically related to the user's core research themes."
    },
    {
        "title": "Benchmarking LLM Causal Reasoning with Scientifically Validated Relationships",
        "abstract": "Causal reasoning is fundamental for Large Language Models (LLMs) to\nunderstand genuine cause-and-effect relationships beyond pattern matching.\nExisting benchmarks suffer from critical limitations such as reliance on\nsynthetic data and narrow domain coverage. We introduce a novel benchmark\nconstructed from casually identified relationships extracted from top-tier\neconomics and finance journals, drawing on rigorous methodologies including\ninstrumental variables, difference-in-differences, and regression discontinuity\ndesigns. Our benchmark comprises 40,379 evaluation items covering five task\ntypes across domains such as health, environment, technology, law, and culture.\nExperimental results on eight state-of-the-art LLMs reveal substantial\nlimitations, with the best model achieving only 57.6\\% accuracy. Moreover,\nmodel scale does not consistently translate to superior performance, and even\nadvanced reasoning models struggle with fundamental causal relationship\nidentification. These findings underscore a critical gap between current LLM\ncapabilities and demands of reliable causal reasoning in high-stakes\napplications.",
        "url": "http://arxiv.org/abs/2510.07231v2",
        "pdf_url": "http://arxiv.org/pdf/2510.07231v2",
        "arxiv_id": "2510.07231v2",
        "authors": [
            "Donggyu Lee",
            "Sungwon Park",
            "Yerin Hwang",
            "Hyoshin Kim",
            "Hyunwoo Oh",
            "Jungwon Kim",
            "Meeyoung Cha",
            "Sangyoon Park",
            "Jihee Kim"
        ],
        "submitted": "2025-10-08 17:00:49",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat relevant to your research interests in Information Retrieval and Natural Language Processing, as it deals with Large Language Models (LLMs) and their limitations in causal reasoning. However, the focus on causal reasoning and its applications in high-stakes domains, such as economics and finance, is not directly related to your primary areas of interest in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Sunflower: A New Approach To Expanding Coverage of African Languages in Large Language Models",
        "abstract": "There are more than 2000 living languages in Africa, most of which have been\nbypassed by advances in language technology. Current leading LLMs exhibit\nstrong performance on a number of the most common languages (e.g. Swahili or\nYoruba), but prioritise support for the languages with the most speakers first,\nresulting in piecemeal ability across disparate languages. We contend that a\nregionally focussed approach is more efficient, and present a case study for\nUganda, a country with high linguistic diversity. We describe the development\nof Sunflower 14B and 32B, a pair of models based on Qwen 3 with state of the\nart comprehension in the majority of all Ugandan languages. These models are\nopen source and can be used to reduce language barriers in a number of\nimportant practical applications.",
        "url": "http://arxiv.org/abs/2510.07203v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07203v1",
        "arxiv_id": "2510.07203v1",
        "authors": [
            "Benjamin Akera",
            "Evelyn Nafula Ouma",
            "Gilbert Yiga",
            "Patrick Walukagga",
            "Phionah Natukunda",
            "Trevor Saaka",
            "Solomon Nsumba",
            "Lilian Teddy Nabukeera",
            "Joel Muhanguzi",
            "Imran Sekalala",
            "Nimpamya Janat Namara",
            "Engineer Bainomugisha",
            "Ernest Mwebaze",
            "John Quinn"
        ],
        "submitted": "2025-10-08 16:35:53",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on expanding language coverage in large language models for African languages, which is outside your primary focus on Information Retrieval, Search technologies, and Natural Language Processing, particularly in areas requiring deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "ConCuR: Conciseness Makes State-of-the-Art Kernel Generation",
        "abstract": "GPU kernel generation by LLMs has recently experienced rapid development,\nleveraging test-time scaling and reinforcement learning techniques. However, a\nkey challenge for kernel generation is the scarcity of high-quality data, as\nmost high-quality kernels are proprietary and not open-source. This challenge\nprevents us from leveraging supervised fine-tuning to align LLMs to the kernel\ngeneration task. To address this challenge, we develop a pipeline that\ngenerates and curates high-quality CUDA kernels with reasoning traces,\nmotivated by a critical observation that concise yet informative reasoning\ntraces result in robust generation of high-performance kernels. Using this\npipeline, we construct our dataset ConCuR and introduce our model KernelCoder,\nwhich is the first model trained on a curated dataset consisting of PyTorch,\nreasoning, and CUDA kernel pairs, to our knowledge. In the KernelBench setup,\nour model achieves significant improvements over the existing top-performing\nmodel, QwQ-32B, and outperforms all open-source models fine-tuned for kernel\ngeneration, as well as frontier models such as DeepSeek-V3.1-Think and\nClaude-4-sonnet. Finally, we show that the average reasoning length can serve\nas a metric to assess the difficulty of kernel generation tasks. The\nobservations, metrics, and our data collection and curation pipeline can help\nobtain better data in the kernel generation task in the future.",
        "url": "http://arxiv.org/abs/2510.07356v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07356v1",
        "arxiv_id": "2510.07356v1",
        "authors": [
            "Lingcheng Kong",
            "Jiateng Wei",
            "Hanzhang Shen",
            "Huan Wang"
        ],
        "submitted": "2025-10-08 15:41:15",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be focused on GPU kernel generation using Large Language Models (LLMs), which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves deep learning techniques, the specific application and domain are not aligned with the user's core research themes."
    },
    {
        "title": "Opt-ICL at LeWiDi-2025: Maximizing In-Context Signal from Rater Examples via Meta-Learning",
        "abstract": "Many natural language processing (NLP) tasks involve subjectivity, ambiguity,\nor legitimate disagreement between annotators. In this paper, we outline our\nsystem for modeling human variation. Our system leverages language models'\n(LLMs) in-context learning abilities, along with a two-step meta-learning\ntraining procedure for 1) post-training on many datasets requiring in-context\nlearning and 2) specializing the model via in-context meta-learning to the\nparticular data distribution of interest. We also evaluate the performance of\nour system submission to the Learning With Disagreements (LeWiDi) competition,\nwhere it was the overall winner on both tasks. Additionally, we perform an\nablation study to measure the importance of each system component. We find that\nincluding rater examples in-context is crucial for our system's performance,\ndataset-specific fine-tuning is helpful on the larger datasets, post-training\non other in-context datasets is helpful on one of the competition datasets, and\nthat performance improves with model scale.",
        "url": "http://arxiv.org/abs/2510.07105v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07105v1",
        "arxiv_id": "2510.07105v1",
        "authors": [
            "Taylor Sorensen",
            "Yejin Choi"
        ],
        "submitted": "2025-10-08 14:59:24",
        "source": "arxiv",
        "comment": "NLPerspectives: The 4th Workshop on Perspectivist Approaches to\n  Natural Language Processing at EMNLP 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper focuses on natural language processing (NLP) and meta-learning, which is somewhat related to the user's interests in NLP and deep semantic understanding. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user. The paper's relevance is limited to the user's broader interests in NLP and related topics."
    },
    {
        "title": "TALENT: Table VQA via Augmented Language-Enhanced Natural-text Transcription",
        "abstract": "Table Visual Question Answering (Table VQA) is typically addressed by large\nvision-language models (VLMs). While such models can answer directly from\nimages, they often miss fine-grained details unless scaled to very large sizes,\nwhich are computationally prohibitive, especially for mobile deployment. A\nlighter alternative is to have a small VLM perform OCR and then use a large\nlanguage model (LLM) to reason over structured outputs such as Markdown tables.\nHowever, these representations are not naturally optimized for LLMs and still\nintroduce substantial errors. We propose TALENT (Table VQA via Augmented\nLanguage-Enhanced Natural-text Transcription), a lightweight framework that\nleverages dual representations of tables. TALENT prompts a small VLM to produce\nboth OCR text and natural language narration, then combines them with the\nquestion for reasoning by an LLM. This reframes Table VQA as an LLM-centric\nmultimodal reasoning task, where the VLM serves as a perception-narration\nmodule rather than a monolithic solver. Additionally, we construct ReTabVQA, a\nmore challenging Table VQA dataset requiring multi-step quantitative reasoning\nover table images. Experiments show that TALENT enables a small VLM-LLM\ncombination to match or surpass a single large VLM at significantly lower\ncomputational cost on both public datasets and ReTabVQA.",
        "url": "http://arxiv.org/abs/2510.07098v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07098v1",
        "arxiv_id": "2510.07098v1",
        "authors": [
            "Guo Yutong",
            "Wanying Wang",
            "Yue Wu",
            "Zichen Miao",
            "Haoyu Wang"
        ],
        "submitted": "2025-10-08 14:56:42",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Table VQA, leveraging vision-language models and large language models for reasoning. While it involves multimodal reasoning, it does not directly relate to information retrieval, query understanding, or ranking models, which are core areas of interest. The paper's emphasis on computer vision and OCR transcription makes it more relevant to the broader NLP domain, but not specifically to the user's research interests."
    },
    {
        "title": "LuxInstruct: A Cross-Lingual Instruction Tuning Dataset For Luxembourgish",
        "abstract": "Instruction tuning has become a key technique for enhancing the performance\nof large language models, enabling them to better follow human prompts.\nHowever, low-resource languages such as Luxembourgish face severe limitations\ndue to the lack of high-quality instruction datasets. Traditional reliance on\nmachine translation often introduces semantic misalignment and cultural\ninaccuracies. In this work, we address these challenges by creating a\ncross-lingual instruction tuning dataset for Luxembourgish, without resorting\nto machine-generated translations into it. Instead, by leveraging aligned data\nfrom English, French, and German, we build a high-quality dataset that\npreserves linguistic and cultural nuances. We provide evidence that\ncross-lingual instruction tuning not only improves representational alignment\nacross languages but also the model's generative capabilities in Luxembourgish.\nThis highlights how cross-lingual data curation can avoid the common pitfalls\nof machine-translated data and directly benefit low-resource language\ndevelopment.",
        "url": "http://arxiv.org/abs/2510.07074v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07074v1",
        "arxiv_id": "2510.07074v1",
        "authors": [
            "Fred Philippy",
            "Laura Bernardy",
            "Siwen Guo",
            "Jacques Klein",
            "Tegawendé F. Bissyandé"
        ],
        "submitted": "2025-10-08 14:35:59",
        "source": "arxiv",
        "comment": "Paper under review; Dataset available at\n  https://huggingface.co/datasets/fredxlpy/LuxInstruct",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval, Search technologies, or query understanding, which are the core areas of your research interests. While it involves Natural Language Processing, it focuses on a specific task (instruction tuning) and a low-resource language (Luxembourgish), which doesn't align with your broader interests in e-commerce, deep semantic understanding, and real-time relevance optimization."
    },
    {
        "title": "Does Local News Stay Local?: Online Content Shifts in Sinclair-Acquired Stations",
        "abstract": "Local news stations are often considered to be reliable sources of\nnon-politicized information, particularly local concerns that residents care\nabout. Because these stations are trusted news sources, viewers are\nparticularly susceptible to the information they report. The Sinclair Broadcast\ngroup is a broadcasting company that has acquired many local news stations in\nthe last decade. We investigate the effects of local news stations being\nacquired by Sinclair: how does coverage change? We use computational methods to\ninvestigate changes in internet content put out by local news stations before\nand after being acquired by Sinclair and in comparison to national news\noutlets. We find that there is clear evidence that local news stations report\nmore frequently on national news at the expense of local topics, and that their\ncoverage of polarizing national topics increases.",
        "url": "http://arxiv.org/abs/2510.07060v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07060v1",
        "arxiv_id": "2510.07060v1",
        "authors": [
            "Miriam Wanner",
            "Sophia Hager",
            "Anjalie Field"
        ],
        "submitted": "2025-10-08 14:27:00",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be focused on the impact of corporate acquisition on local news content, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on the topic of content, it is more focused on media studies and does not involve query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Pragyaan: Designing and Curating High-Quality Cultural Post-Training Datasets for Indian Languages",
        "abstract": "The effectiveness of Large Language Models (LLMs) depends heavily on the\navailability of high-quality post-training data, particularly\ninstruction-tuning and preference-based examples. Existing open-source\ndatasets, however, often lack multilingual coverage, cultural grounding, and\nsuffer from task diversity gaps that are especially pronounced for Indian\nlanguages. We introduce a human-in-the-loop pipeline that combines translations\nwith synthetic expansion to produce reliable and diverse Indic post-training\ndata. Using this pipeline, we curate two datasets: Pragyaan-IT (22.5K) and\nPragyaan-Align (100K) across 10 Indian languages covering 13 broad and 56\nsub-categories, leveraging 57 diverse datasets. Our dataset protocol\nincorporates several often-overlooked dimensions and emphasize task diversity,\nmulti-turn dialogue, instruction fidelity, safety alignment, and preservation\nof cultural nuance, providing a foundation for more inclusive and effective\nmultilingual LLMs.",
        "url": "http://arxiv.org/abs/2510.07000v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07000v1",
        "arxiv_id": "2510.07000v1",
        "authors": [
            "Neel Prabhanjan Rachamalla",
            "Aravind Konakalla",
            "Gautam Rajeev",
            "Ashish Kulkarni",
            "Chandra Khatri",
            "Shubham Agarwal"
        ],
        "submitted": "2025-10-08 13:23:45",
        "source": "arxiv",
        "comment": "EMNLP 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on creating high-quality post-training datasets for Indian languages, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, it's more focused on dataset curation and language modeling, rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Open ASR Leaderboard: Towards Reproducible and Transparent Multilingual and Long-Form Speech Recognition Evaluation",
        "abstract": "Despite rapid progress, ASR evaluation remains saturated with short-form\nEnglish, and efficiency is rarely reported. We present the Open ASR\nLeaderboard, a fully reproducible benchmark and interactive leaderboard\ncomparing 60+ open-source and proprietary systems across 11 datasets, including\ndedicated multilingual and long-form tracks. We standardize text normalization\nand report both word error rate (WER) and inverse real-time factor (RTFx),\nenabling fair accuracy-efficiency comparisons. For English transcription,\nConformer encoders paired with LLM decoders achieve the best average WER but\nare slower, while CTC and TDT decoders deliver much better RTFx, making them\nattractive for long-form and offline use. Whisper-derived encoders fine-tuned\nfor English improve accuracy but often trade off multilingual coverage. All\ncode and dataset loaders are open-sourced to support transparent, extensible\nevaluation.",
        "url": "http://arxiv.org/abs/2510.06961v2",
        "pdf_url": "http://arxiv.org/pdf/2510.06961v2",
        "arxiv_id": "2510.06961v2",
        "authors": [
            "Vaibhav Srivastav",
            "Steven Zheng",
            "Eric Bezzam",
            "Eustache Le Bihan",
            "Nithin Koluguri",
            "Piotr Żelasko",
            "Somshubra Majumdar",
            "Adel Moumen",
            "Sanchit Gandhi"
        ],
        "submitted": "2025-10-08 12:44:51",
        "source": "arxiv",
        "comment": "Submitted to ICASSP 2026; Leaderboard:\n  https://huggingface.co/spaces/hf-audio/open_asr_leaderboard ; Code:\n  https://github.com/huggingface/open_asr_leaderboard",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on speech recognition evaluation, which is not directly related to your core research themes in Information Retrieval and Search technologies. Although it involves some aspects of ranking models and efficiency, the context is specific to ASR and does not align with your primary interests in deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "LeMAJ (Legal LLM-as-a-Judge): Bridging Legal Reasoning and LLM Evaluation",
        "abstract": "Evaluating large language model (LLM) outputs in the legal domain presents\nunique challenges due to the complex and nuanced nature of legal analysis.\nCurrent evaluation approaches either depend on reference data, which is costly\nto produce, or use standardized assessment methods, both of which have\nsignificant limitations for legal applications.\n  Although LLM-as-a-Judge has emerged as a promising evaluation technique, its\nreliability and effectiveness in legal contexts depend heavily on evaluation\nprocesses unique to the legal industry and how trustworthy the evaluation\nappears to the human legal expert. This is where existing evaluation methods\ncurrently fail and exhibit considerable variability.\n  This paper aims to close the gap: a) we break down lengthy responses into\n'Legal Data Points' (LDPs), self-contained units of information, and introduce\na novel, reference-free evaluation methodology that reflects how lawyers\nevaluate legal answers; b) we demonstrate that our method outperforms a variety\nof baselines on both our proprietary dataset and an open-source dataset\n(LegalBench); c) we show how our method correlates more closely with human\nexpert evaluations and helps improve inter-annotator agreement; and finally d)\nwe open source our Legal Data Points for a subset of LegalBench used in our\nexperiments, allowing the research community to replicate our results and\nadvance research in this vital area of LLM evaluation on legal\nquestion-answering.",
        "url": "http://arxiv.org/abs/2510.07243v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07243v1",
        "arxiv_id": "2510.07243v1",
        "authors": [
            "Joseph Enguehard",
            "Morgane Van Ermengem",
            "Kate Atkinson",
            "Sujeong Cha",
            "Arijit Ghosh Chowdhury",
            "Prashanth Kallur Ramaswamy",
            "Jeremy Roghair",
            "Hannah R Marlowe",
            "Carina Suzana Negreanu",
            "Kitty Boxall",
            "Diana Mincu"
        ],
        "submitted": "2025-10-08 17:10:47",
        "source": "arxiv",
        "comment": "Published in Natural Legal Language Processing - EMNLP Workshop 2025",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to information retrieval, specifically in the context of legal question-answering and large language model evaluation. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's focus on legal domain and LLM evaluation is somewhat tangential to the user's primary research themes."
    },
    {
        "title": "Where to Begin: Efficient Pretraining via Subnetwork Selection and Distillation",
        "abstract": "Small Language models (SLMs) offer an efficient and accessible alternative to\nLarge Language Models (LLMs), delivering strong performance while using far\nfewer resources. We introduce a simple and effective framework for pretraining\nSLMs that brings together three complementary ideas. First, we identify\nstructurally sparse sub-network initializations that consistently outperform\nrandomly initialized models of similar size under the same compute budget.\nSecond, we use evolutionary search to automatically discover high-quality\nsub-network initializations, providing better starting points for pretraining.\nThird, we apply knowledge distillation from larger teacher models to speed up\ntraining and improve generalization. Together, these components make SLM\npretraining substantially more efficient: our best model, discovered using\nevolutionary search and initialized with LLM weights, matches the validation\nperplexity of a comparable Pythia SLM while requiring 9.2x fewer pretraining\ntokens. We release all code and models at\nhttps://github.com/whittle-org/whittle/, offering a practical and reproducible\npath toward cost-efficient small language model development at scale.",
        "url": "http://arxiv.org/abs/2510.07227v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07227v1",
        "arxiv_id": "2510.07227v1",
        "authors": [
            "Arjun Krishnakumar",
            "Rhea Sanjay Sukthanker",
            "Hannan Javed Mahadik",
            "Gabriela Kadlecová",
            "Vladyslav Moroshan",
            "Timur Carstensen",
            "Frank Hutter",
            "Aaron Klein"
        ],
        "submitted": "2025-10-08 16:57:46",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on efficient pretraining of small language models, which is not directly related to information retrieval, query understanding, or ranking models. While it touches on NLP, the context is on model development rather than application in search technologies."
    },
    {
        "title": "More Data or Better Data? A Critical Analysis of Data Selection and Synthesis for Mathematical Reasoning",
        "abstract": "The reasoning capabilities of Large Language Models (LLMs) play a critical\nrole in many downstream tasks, yet depend strongly on the quality of training\ndata. Despite various proposed data construction methods, their practical\nutility in real-world pipelines remains underexplored. In this work, we conduct\na comprehensive analysis of open-source datasets and data synthesis techniques\nfor mathematical reasoning, evaluating them under a unified pipeline designed\nto mirror training and deployment scenarios. We further distill effective data\nselection strategies and identify practical methods suitable for industrial\napplications. Our findings highlight that structuring data in more\ninterpretable formats, or distilling from stronger models often outweighs\nsimply scaling up data volume. This study provides actionable guidance for\nintegrating training data to enhance LLM capabilities, supporting both\ncost-effective data curation and scalable model enhancement. We hope this work\nwill inspire further research on how to balance \"more data\" versus \"better\ndata\" for real-world reasoning tasks.",
        "url": "http://arxiv.org/abs/2510.07169v1",
        "pdf_url": "http://arxiv.org/pdf/2510.07169v1",
        "arxiv_id": "2510.07169v1",
        "authors": [
            "Yike Zhao",
            "Simin Guo",
            "Ziqing Yang",
            "Shifan Han",
            "Dahua Lin",
            "Fei Tan"
        ],
        "submitted": "2025-10-08 16:07:26",
        "source": "arxiv",
        "comment": "12 pages, 3 figures, submitted to EMNLP 2025 Industry Track",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on data selection and synthesis for mathematical reasoning in Large Language Models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it touches on model enhancement, the context is specific to mathematical reasoning and does not align with your primary focus on real-time relevance optimization and deep semantic understanding."
    },
    {
        "title": "Beyond Monolingual Assumptions: A Survey of Code-Switched NLP in the Era of Large Language Models",
        "abstract": "Code-switching (CSW), the alternation of languages and scripts within a\nsingle utterance, remains a fundamental challenge for multiling ual NLP, even\namidst the rapid advances of large language models (LLMs). Most LLMs still\nstruggle with mixed-language inputs, limited CSW datasets, and evaluation\nbiases, hindering deployment in multilingual societies. This survey provides\nthe first comprehensive analysis of CSW-aware LLM research, reviewing 308\nstudies spanning five research areas, 12 NLP tasks, 30+ datasets, and 80+\nlanguages. We classify recent advances by architecture, training strategy, and\nevaluation methodology, outlining how LLMs have reshaped CSW modeling and what\nchallenges persist. The paper concludes with a roadmap emphasizing the need for\ninclusive datasets, fair evaluation, and linguistically grounded models to\nachieve truly multilingual intelligence. A curated collection of all resources\nis maintained at https://github.com/lingo-iitgn/awesome-code-mixing/.",
        "url": "http://arxiv.org/abs/2510.07037v2",
        "pdf_url": "http://arxiv.org/pdf/2510.07037v2",
        "arxiv_id": "2510.07037v2",
        "authors": [
            "Rajvee Sheth",
            "Samridhi Raj Sinha",
            "Mahavir Patil",
            "Himanshu Beniwal",
            "Mayank Singh"
        ],
        "submitted": "2025-10-08 14:04:14",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is primarily focused on Code-Switched NLP and large language models, which is outside the user's core research themes in Information Retrieval and Search technologies. While it touches on multilingual aspects, it does not directly relate to query understanding, ranking models, or user behavior modeling, making it only loosely relevant to the user's interests."
    },
    {
        "title": "Mining the Mind: What 100M Beliefs Reveal About Frontier LLM Knowledge",
        "abstract": "LLMs are remarkable artifacts that have revolutionized a range of NLP and AI\ntasks. A significant contributor is their factual knowledge, which, to date,\nremains poorly understood, and is usually analyzed from biased samples. In this\npaper, we take a deep tour into the factual knowledge (or beliefs) of a\nfrontier LLM, based on GPTKB v1.5 (Hu et al., 2025a), a recursively elicited\nset of 100 million beliefs of one of the strongest currently available frontier\nLLMs, GPT-4.1. We find that the models' factual knowledge differs quite\nsignificantly from established knowledge bases, and that its accuracy is\nsignificantly lower than indicated by previous benchmarks. We also find that\ninconsistency, ambiguity and hallucinations are major issues, shedding light on\nfuture research opportunities concerning factual LLM knowledge.",
        "url": "http://arxiv.org/abs/2510.07024v2",
        "pdf_url": "http://arxiv.org/pdf/2510.07024v2",
        "arxiv_id": "2510.07024v2",
        "authors": [
            "Shrestha Ghosh",
            "Luca Giordano",
            "Yujia Hu",
            "Tuan-Phong Nguyen",
            "Simon Razniewski"
        ],
        "submitted": "2025-10-08 13:48:38",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the factual knowledge of Large Language Models (LLMs), which is somewhat related to the user's interests in Natural Language Processing (NLP) and deep semantic understanding. However, the focus on LLMs and their knowledge bases is not directly aligned with the user's primary research themes in Information Retrieval and Search technologies."
    },
    {
        "title": "RedTWIZ: Diverse LLM Red Teaming via Adaptive Attack Planning",
        "abstract": "This paper presents the vision, scientific contributions, and technical\ndetails of RedTWIZ: an adaptive and diverse multi-turn red teaming framework,\nto audit the robustness of Large Language Models (LLMs) in AI-assisted software\ndevelopment. Our work is driven by three major research streams: (1) robust and\nsystematic assessment of LLM conversational jailbreaks; (2) a diverse\ngenerative multi-turn attack suite, supporting compositional, realistic and\ngoal-oriented jailbreak conversational strategies; and (3) a hierarchical\nattack planner, which adaptively plans, serializes, and triggers attacks\ntailored to specific LLM's vulnerabilities. Together, these contributions form\na unified framework -- combining assessment, attack generation, and strategic\nplanning -- to comprehensively evaluate and expose weaknesses in LLMs'\nrobustness. Extensive evaluation is conducted to systematically assess and\nanalyze the performance of the overall system and each component. Experimental\nresults demonstrate that our multi-turn adversarial attack strategies can\nsuccessfully lead state-of-the-art LLMs to produce unsafe generations,\nhighlighting the pressing need for more research into enhancing LLM's\nrobustness.",
        "url": "http://arxiv.org/abs/2510.06994v1",
        "pdf_url": "http://arxiv.org/pdf/2510.06994v1",
        "arxiv_id": "2510.06994v1",
        "authors": [
            "Artur Horal",
            "Daniel Pina",
            "Henrique Paz",
            "Iago Paulo",
            "João Soares",
            "Rafael Ferreira",
            "Diogo Tavares",
            "Diogo Glória-Silva",
            "João Magalhães",
            "David Semedo"
        ],
        "submitted": "2025-10-08 13:18:42",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to the user's interests in Natural Language Processing (NLP) and Large Language Models (LLMs), but it does not directly align with the user's primary focus on Information Retrieval (IR) and query understanding. The paper's focus on robustness and adversarial attacks in LLMs is not a central match for the user's interests in search technologies and user behavior modeling."
    }
]