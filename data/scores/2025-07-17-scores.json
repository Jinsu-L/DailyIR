[
    {
        "title": "Advancing Retrieval-Augmented Generation for Structured Enterprise and Internal Data",
        "abstract": "Organizations increasingly rely on proprietary enterprise data, including HR\nrecords, structured reports, and tabular documents, for critical\ndecision-making. While Large Language Models (LLMs) have strong generative\ncapabilities, they are limited by static pretraining, short context windows,\nand challenges in processing heterogeneous data formats. Conventional\nRetrieval-Augmented Generation (RAG) frameworks address some of these gaps but\noften struggle with structured and semi-structured data.\n  This work proposes an advanced RAG framework that combines hybrid retrieval\nstrategies using dense embeddings (all-mpnet-base-v2) and BM25, enhanced by\nmetadata-aware filtering with SpaCy NER and cross-encoder reranking. The\nframework applies semantic chunking to maintain textual coherence and retains\ntabular data structures to preserve row-column integrity. Quantized indexing\noptimizes retrieval efficiency, while human-in-the-loop feedback and\nconversation memory improve adaptability.\n  Experiments on enterprise datasets show notable improvements: Precision@5\nincreased by 15 percent (90 versus 75), Recall@5 by 13 percent (87 versus 74),\nand Mean Reciprocal Rank by 16 percent (0.85 versus 0.69). Qualitative\nevaluations show higher scores in Faithfulness (4.6 versus 3.0), Completeness\n(4.2 versus 2.5), and Relevance (4.5 versus 3.2) on a 5-point Likert scale.\nThese results demonstrate the framework's effectiveness in delivering accurate,\ncomprehensive, and contextually relevant responses for enterprise tasks. Future\nwork includes extending to multimodal data and integrating agent-based\nretrieval. The source code will be released at\nhttps://github.com/CheerlaChandana/Enterprise-Chatbot",
        "url": "http://arxiv.org/abs/2507.12425v1",
        "pdf_url": "http://arxiv.org/pdf/2507.12425v1",
        "arxiv_id": "2507.12425v1",
        "authors": [
            "Chandana Cheerla"
        ],
        "submitted": "2025-07-16 17:13:06",
        "source": "arxiv",
        "comment": null,
        "score": 14,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a Retrieval-Augmented Generation framework for structured enterprise data, which is somewhat related to my interests in Information Retrieval and Search technologies. However, the focus on enterprise data and chatbots is not directly aligned with my primary research themes, and the paper does not explicitly address query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Graph Representations for Reading Comprehension Analysis using Large Language Model and Eye-Tracking Biomarker",
        "abstract": "Reading comprehension is a fundamental skill in human cognitive development.\nWith the advancement of Large Language Models (LLMs), there is a growing need\nto compare how humans and LLMs understand language across different contexts\nand apply this understanding to functional tasks such as inference, emotion\ninterpretation, and information retrieval. Our previous work used LLMs and\nhuman biomarkers to study the reading comprehension process. The results showed\nthat the biomarkers corresponding to words with high and low relevance to the\ninference target, as labeled by the LLMs, exhibited distinct patterns,\nparticularly when validated using eye-tracking data. However, focusing solely\non individual words limited the depth of understanding, which made the\nconclusions somewhat simplistic despite their potential significance. This\nstudy used an LLM-based AI agent to group words from a reading passage into\nnodes and edges, forming a graph-based text representation based on semantic\nmeaning and question-oriented prompts. We then compare the distribution of eye\nfixations on important nodes and edges. Our findings indicate that LLMs exhibit\nhigh consistency in language understanding at the level of graph topological\nstructure. These results build on our previous findings and offer insights into\neffective human-AI co-learning strategies.",
        "url": "http://arxiv.org/abs/2507.11972v1",
        "pdf_url": "http://arxiv.org/pdf/2507.11972v1",
        "arxiv_id": "2507.11972v1",
        "authors": [
            "Yuhong Zhang",
            "Jialu Li",
            "Shilai Yang",
            "Yuchen Xu",
            "Gert Cauwenberghs",
            "Tzyy-Ping Jung"
        ],
        "submitted": "2025-07-16 07:15:59",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the application of Large Language Models in reading comprehension analysis, which is related to information retrieval and NLP. However, the focus on graph representations and eye-tracking biomarkers is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Context-Aware Search and Retrieval Over Erasure Channels",
        "abstract": "This paper introduces and analyzes a search and retrieval model that adopts\nkey semantic communication principles from retrieval-augmented generation. We\nspecifically present an information-theoretic analysis of a remote document\nretrieval system operating over a symbol erasure channel. The proposed model\nencodes the feature vector of a query, derived from term-frequency weights of a\nlanguage corpus by using a repetition code with an adaptive rate dependent on\nthe contextual importance of the terms. At the decoder, we select between two\ndocuments based on the contextual closeness of the recovered query. By\nleveraging a jointly Gaussian approximation for both the true and reconstructed\nsimilarity scores, we derive an explicit expression for the retrieval error\nprobability, i.e., the probability under which the less similar document is\nselected. Numerical simulations on synthetic and real-world data (Google NQ)\nconfirm the validity of the analysis. They further demonstrate that assigning\ngreater redundancy to critical features effectively reduces the error rate,\nhighlighting the effectiveness of semantic-aware feature encoding in\nerror-prone communication settings.",
        "url": "http://arxiv.org/abs/2507.11894v1",
        "pdf_url": "http://arxiv.org/pdf/2507.11894v1",
        "arxiv_id": "2507.11894v1",
        "authors": [
            "Sara Ghasvarianjahromi",
            "Yauhen Yakimenka",
            "JÃ¶rg Kliewer"
        ],
        "submitted": "2025-07-16 04:21:46",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores a novel search and retrieval model over erasure channels, which is a unique application not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on semantic communication principles, the focus is on error-prone communication settings rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Web-Browsing LLMs Can Access Social Media Profiles and Infer User Demographics",
        "abstract": "Large language models (LLMs) have traditionally relied on static training\ndata, limiting their knowledge to fixed snapshots. Recent advancements,\nhowever, have equipped LLMs with web browsing capabilities, enabling real time\ninformation retrieval and multi step reasoning over live web content. While\nprior studies have demonstrated LLMs ability to access and analyze websites,\ntheir capacity to directly retrieve and analyze social media data remains\nunexplored. Here, we evaluate whether web browsing LLMs can infer demographic\nattributes of social media users given only their usernames. Using a synthetic\ndataset of 48 X (Twitter) accounts and a survey dataset of 1,384 international\nparticipants, we show that these models can access social media content and\npredict user demographics with reasonable accuracy. Analysis of the synthetic\ndataset further reveals how LLMs parse and interpret social media profiles,\nwhich may introduce gender and political biases against accounts with minimal\nactivity. While this capability holds promise for computational social science\nin the post API era, it also raises risks of misuse particularly in information\noperations and targeted advertising underscoring the need for safeguards. We\nrecommend that LLM providers restrict this capability in public facing\napplications, while preserving controlled access for verified research\npurposes.",
        "url": "http://arxiv.org/abs/2507.12372v1",
        "pdf_url": "http://arxiv.org/pdf/2507.12372v1",
        "arxiv_id": "2507.12372v1",
        "authors": [
            "Meysam Alizadeh",
            "Fabrizio Gilardi",
            "Zeynab Samei",
            "Mohsen Mosleh"
        ],
        "submitted": "2025-07-16 16:21:01",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling, which are the core areas of your research interests. While it touches on the topic of real-time information retrieval, it focuses on web browsing LLMs accessing social media profiles, which is a niche area that doesn't align with your primary focus."
    },
    {
        "title": "Developing Visual Augmented Q&A System using Scalable Vision Embedding Retrieval & Late Interaction Re-ranker",
        "abstract": "Traditional information extraction systems face challenges with text only\nlanguage models as it does not consider infographics (visual elements of\ninformation) such as tables, charts, images etc. often used to convey complex\ninformation to readers. Multimodal LLM (MLLM) face challenges of finding needle\nin the haystack problem i.e., either longer context length or substantial\nnumber of documents as search space. Late interaction mechanism over visual\nlanguage models has shown state of the art performance in retrieval-based\nvision augmented Q&A tasks. There are yet few challenges using it for RAG based\nmulti-modal Q&A. Firstly, many popular and widely adopted vector databases do\nnot support native multi-vector retrieval. Secondly, late interaction requires\ncomputation which inflates space footprint and can hinder enterprise adoption.\nLastly, the current state of late interaction mechanism does not leverage the\napproximate neighbor search indexing methods for large speed ups in retrieval\nprocess. This paper explores a pragmatic approach to make vision retrieval\nprocess scalable and efficient without compromising on performance quality. We\npropose multi-step custom implementation utilizing widely adopted hybrid search\n(metadata & embedding) and state of the art late interaction re-ranker to\nretrieve best matching pages. Finally, MLLM are prompted as reader to generate\nanswers from contextualized best matching pages. Through experiments, we\nobserve that the proposed design is scalable (significant speed up) and stable\n(without degrading performance quality), hence can be used as production\nsystems at enterprises.",
        "url": "http://arxiv.org/abs/2507.12378v1",
        "pdf_url": "http://arxiv.org/pdf/2507.12378v1",
        "arxiv_id": "2507.12378v1",
        "authors": [
            "Rachna Saxena",
            "Abhijeet Kumar",
            "Suresh Shanmugam"
        ],
        "submitted": "2025-07-16 16:27:05",
        "source": "arxiv",
        "comment": "Presented at NLP@IR workshop at SIGIR conference",
        "score": 6,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores a visual augmented Q&A system, which is not directly related to information retrieval, query understanding, or ranking models. While it touches on multimodal language models, the focus is on visual elements and retrieval, which is not a central match for your research interests."
    },
    {
        "title": "The benefits of query-based KGQA systems for complex and temporal questions in LLM era",
        "abstract": "Large language models excel in question-answering (QA) yet still struggle\nwith multi-hop reasoning and temporal questions. Query-based knowledge graph QA\n(KGQA) offers a modular alternative by generating executable queries instead of\ndirect answers. We explore multi-stage query-based framework for WikiData QA,\nproposing multi-stage approach that enhances performance on challenging\nmulti-hop and temporal benchmarks. Through generalization and rejection\nstudies, we evaluate robustness across multi-hop and temporal QA datasets.\nAdditionally, we introduce a novel entity linking and predicate matching method\nusing CoT reasoning. Our results demonstrate the potential of query-based\nmulti-stage KGQA framework for improving multi-hop and temporal QA with small\nlanguage models. Code and data: https://github.com/ar2max/NLDB-KGQA-System",
        "url": "http://arxiv.org/abs/2507.11954v1",
        "pdf_url": "http://arxiv.org/pdf/2507.11954v1",
        "arxiv_id": "2507.11954v1",
        "authors": [
            "Artem Alekseev",
            "Mikhail Chaichuk",
            "Miron Butko",
            "Alexander Panchenko",
            "Elena Tutubalina",
            "Oleg Somov"
        ],
        "submitted": "2025-07-16 06:41:03",
        "source": "arxiv",
        "comment": "15 pages, 3 figures, 7 tables",
        "score": 6,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores query-based knowledge graph question answering (KGQA) systems, which is related to information retrieval and query understanding. The focus on multi-hop and temporal questions is also relevant to ranking models and user behavior modeling. However, the paper's primary focus on KGQA and its application to question-answering is not directly aligned with the user's core research themes in information retrieval and search technologies."
    },
    {
        "title": "ILID: Native Script Language Identification for Indian Languages",
        "abstract": "The language identification task is a crucial fundamental step in NLP. Often\nit serves as a pre-processing step for widely used NLP applications such as\nmultilingual machine translation, information retrieval, question and\nanswering, and text summarization. The core challenge of language\nidentification lies in distinguishing languages in noisy, short, and code-mixed\nenvironments. This becomes even harder in case of diverse Indian languages that\nexhibit lexical and phonetic similarities, but have distinct differences. Many\nIndian languages share the same script making the task even more challenging.\nIn this paper, we release a dataset of 230K sentences consisting of English and\nall 22 official Indian languages labeled with their language identifiers where\ndata in most languages are newly created. We also develop and release robust\nbaseline models using state-of-the-art approaches in machine learning and deep\nlearning that can aid the research in this field. Our baseline models are\ncomparable to the state-of-the-art models for the language identification task.",
        "url": "http://arxiv.org/abs/2507.11832v1",
        "pdf_url": "http://arxiv.org/pdf/2507.11832v1",
        "arxiv_id": "2507.11832v1",
        "authors": [
            "Yash Ingle",
            "Pruthwik Mishra"
        ],
        "submitted": "2025-07-16 01:39:32",
        "source": "arxiv",
        "comment": "8 pages, 1 figure, 7 tables, Paper accepted in RANLP 2025",
        "score": 6,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on language identification for Indian languages, which is not directly related to the user's interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions applications in NLP, the specific task and dataset are not relevant to the user's research areas."
    },
    {
        "title": "Language Models Improve When Pretraining Data Matches Target Tasks",
        "abstract": "Every data selection method inherently has a target. In practice, these\ntargets often emerge implicitly through benchmark-driven iteration: researchers\ndevelop selection strategies, train models, measure benchmark performance, then\nrefine accordingly. This raises a natural question: what happens when we make\nthis optimization explicit? To explore this, we propose benchmark-targeted\nranking (BETR), a simple method that selects pretraining documents based on\nsimilarity to benchmark training examples. BETR embeds benchmark examples and a\nsample of pretraining documents in a shared space, scores this sample by\nsimilarity to benchmarks, then trains a lightweight classifier to predict these\nscores for the full corpus. We compare data selection methods by training over\n500 models spanning $10^{19}$ to $10^{22}$ FLOPs and fitting scaling laws to\nthem. From this, we find that simply aligning pretraining data to evaluation\nbenchmarks using BETR achieves a 2.1x compute multiplier over DCLM-Baseline\n(4.7x over unfiltered data) and improves performance on 9 out of 10 tasks\nacross all scales. BETR also generalizes well: when targeting a diverse set of\nbenchmarks disjoint from our evaluation suite, it still matches or outperforms\nbaselines. Our scaling analysis further reveals a clear trend: larger models\nrequire less aggressive filtering. Overall, our findings show that directly\nmatching pretraining data to target tasks precisely shapes model capabilities\nand highlight that optimal selection strategies must adapt to model scale.",
        "url": "http://arxiv.org/abs/2507.12466v1",
        "pdf_url": "http://arxiv.org/pdf/2507.12466v1",
        "arxiv_id": "2507.12466v1",
        "authors": [
            "David Mizrahi",
            "Anders Boesen Lindbo Larsen",
            "Jesse Allardice",
            "Suzie Petryk",
            "Yuri Gorokhov",
            "Jeffrey Li",
            "Alex Fang",
            "Josh Gardner",
            "Tom Gunter",
            "Afshin Dehghan"
        ],
        "submitted": "2025-07-16 17:59:45",
        "source": "arxiv",
        "comment": "44 pages, 25 figures, 13 tables",
        "score": 5,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores the impact of pretraining data selection on model performance, which is relevant to information retrieval and search technologies. The use of benchmark-targeted ranking (BETR) to select pretraining documents is also related to query understanding and ranking models. However, the focus on language models and pretraining data selection is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "SIEVE: Effective Filtered Vector Search with Collection of Indexes",
        "abstract": "Many real-world tasks such as recommending videos with the kids tag can be\nreduced to finding most similar vectors associated with hard predicates. This\ntask, filtered vector search, is challenging as prior state-of-the-art\ngraph-based (unfiltered) similarity search techniques quickly degenerate when\nhard constraints are considered. That is, effective graph-based filtered\nsimilarity search relies on sufficient connectivity for reaching the most\nsimilar items within just a few hops. To consider predicates, recent works\npropose modifying graph traversal to visit only the items that may satisfy\npredicates. However, they fail to offer the just-a-few-hops property for a wide\nrange of predicates: they must restrict predicates significantly or lose\nefficiency if only a small fraction of items satisfy predicates.\n  We propose an opposite approach: instead of constraining traversal, we build\nmany indexes each serving different predicate forms. For effective\nconstruction, we devise a three-dimensional analytical model capturing\nrelationships among index size, search time, and recall, with which we follow a\nworkload-aware approach to pack as many useful indexes as possible into a\ncollection. At query time, the analytical model is employed yet again to\ndiscern the one that offers the fastest search at a given recall. We show\nsuperior performance and support on datasets with varying selectivities and\nforms: our approach achieves up to 8.06x speedup while having as low as 1%\nbuild time versus other indexes, with less than 2.15x memory of a standard HNSW\ngraph and modest knowledge of past workloads.",
        "url": "http://arxiv.org/abs/2507.11907v1",
        "pdf_url": "http://arxiv.org/pdf/2507.11907v1",
        "arxiv_id": "2507.11907v1",
        "authors": [
            "Zhaoheng Li",
            "Silu Huang",
            "Wei Ding",
            "Yongjoo Park",
            "Jianjun Chen"
        ],
        "submitted": "2025-07-16 04:46:28",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on filtered vector search, which is not directly related to query understanding, ranking models, or user behavior modeling in Information Retrieval. While it involves indexing and search, the approach is more focused on efficient search and recall rather than deep semantic understanding or real-time relevance optimization."
    },
    {
        "title": "Text-ADBench: Text Anomaly Detection Benchmark based on LLMs Embedding",
        "abstract": "Text anomaly detection is a critical task in natural language processing\n(NLP), with applications spanning fraud detection, misinformation\nidentification, spam detection and content moderation, etc. Despite significant\nadvances in large language models (LLMs) and anomaly detection algorithms, the\nabsence of standardized and comprehensive benchmarks for evaluating the\nexisting anomaly detection methods on text data limits rigorous comparison and\ndevelopment of innovative approaches. This work performs a comprehensive\nempirical study and introduces a benchmark for text anomaly detection,\nleveraging embeddings from diverse pre-trained language models across a wide\narray of text datasets. Our work systematically evaluates the effectiveness of\nembedding-based text anomaly detection by incorporating (1) early language\nmodels (GloVe, BERT); (2) multiple LLMs (LLaMa-2, LLama-3, Mistral, OpenAI\n(small, ada, large)); (3) multi-domain text datasets (news, social media,\nscientific publications); (4) comprehensive evaluation metrics (AUROC, AUPRC).\nOur experiments reveal a critical empirical insight: embedding quality\nsignificantly governs anomaly detection efficacy, and deep learning-based\napproaches demonstrate no performance advantage over conventional shallow\nalgorithms (e.g., KNN, Isolation Forest) when leveraging LLM-derived\nembeddings.In addition, we observe strongly low-rank characteristics in\ncross-model performance matrices, which enables an efficient strategy for rapid\nmodel evaluation (or embedding evaluation) and selection in practical\napplications. Furthermore, by open-sourcing our benchmark toolkit that includes\nall embeddings from different models and code at\nhttps://github.com/jicongfan/Text-Anomaly-Detection-Benchmark, this work\nprovides a foundation for future research in robust and scalable text anomaly\ndetection systems.",
        "url": "http://arxiv.org/abs/2507.12295v1",
        "pdf_url": "http://arxiv.org/pdf/2507.12295v1",
        "arxiv_id": "2507.12295v1",
        "authors": [
            "Feng Xiao",
            "Jicong Fan"
        ],
        "submitted": "2025-07-16 14:47:41",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on text anomaly detection, which is related to information retrieval and natural language processing. However, the specific application and methodology do not directly align with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings with Sentiment for Subjectivity Detection in News Articles",
        "abstract": "This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab\nTask 1: Subjectivity Detection in News Articles, classifying sentences as\nsubjective/objective in monolingual, multilingual, and zero-shot settings.\nTraining/development datasets were provided for Arabic, German, English,\nItalian, and Bulgarian; final evaluation included additional unseen languages\n(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our\nprimary strategy enhanced transformer-based classifiers by integrating\nsentiment scores, derived from an auxiliary model, with sentence\nrepresentations, aiming to improve upon standard fine-tuning. We explored this\nsentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base\n(English), and Llama3.2-1B. To address class imbalance, prevalent across\nlanguages, we employed decision threshold calibration optimized on the\ndevelopment set. Our experiments show sentiment feature integration\nsignificantly boosts performance, especially subjective F1 score. This\nframework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).",
        "url": "http://arxiv.org/abs/2507.11764v1",
        "pdf_url": "http://arxiv.org/pdf/2507.11764v1",
        "arxiv_id": "2507.11764v1",
        "authors": [
            "Matteo Fasulo",
            "Luca Babboni",
            "Luca Tedeschini"
        ],
        "submitted": "2025-07-15 22:10:20",
        "source": "arxiv",
        "comment": "14 pages, 6 figures, accepted at CLEF 2025 CheckThat! Lab",
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on subjectivity detection in news articles, which is a topic in Natural Language Processing (NLP). While it uses transformer-based embeddings and sentiment scores, it does not directly relate to query understanding, ranking models, or user behavior modeling in Information Retrieval (IR). The paper's focus on sentiment analysis and language-specific models is somewhat relevant to my interests, but it does not align with my primary focus on IR and deep semantic understanding."
    },
    {
        "title": "Chain-of-Descriptions: Improving Code LLMs for VHDL Code Generation and Summarization",
        "abstract": "Large Language Models (LLMs) have become widely used across diverse NLP tasks\nand domains, demonstrating their adaptability and effectiveness. In the realm\nof Electronic Design Automation (EDA), LLMs show promise for tasks like\nRegister-Transfer Level (RTL) code generation and summarization. However,\ndespite the proliferation of LLMs for general code-related tasks, there's a\ndearth of research focused on evaluating and refining these models for hardware\ndescription languages (HDLs), notably VHDL. In this study, we evaluate the\nperformance of existing code LLMs for VHDL code generation and summarization\nusing various metrics and two datasets -- VHDL-Eval and VHDL-Xform. The latter,\nan in-house dataset, aims to gauge LLMs' understanding of functionally\nequivalent code. Our findings reveal consistent underperformance of these\nmodels across different metrics, underscoring a significant gap in their\nsuitability for this domain. To address this challenge, we propose\nChain-of-Descriptions (CoDes), a novel approach to enhance the performance of\nLLMs for VHDL code generation and summarization tasks. CoDes involves\ngenerating a series of intermediate descriptive steps based on: (i) the problem\nstatement for code generation, and (ii) the VHDL code for summarization. These\nsteps are then integrated with the original input prompt (problem statement or\ncode) and provided as input to the LLMs to generate the final output. Our\nexperiments demonstrate that the CoDes approach significantly surpasses the\nstandard prompting strategy across various metrics on both datasets. This\nmethod not only improves the quality of VHDL code generation and summarization\nbut also serves as a framework for future research aimed at enhancing code LLMs\nfor VHDL.",
        "url": "http://arxiv.org/abs/2507.12308v1",
        "pdf_url": "http://arxiv.org/pdf/2507.12308v1",
        "arxiv_id": "2507.12308v1",
        "authors": [
            "Prashanth Vijayaraghavan",
            "Apoorva Nitsure",
            "Charles Mackin",
            "Luyao Shi",
            "Stefano Ambrogio",
            "Arvind Haran",
            "Viresh Paruthi",
            "Ali Elzein",
            "Dan Coops",
            "David Beymer",
            "Tyler Baldwin",
            "Ehsan Degan"
        ],
        "submitted": "2025-07-16 15:05:30",
        "source": "arxiv",
        "comment": "10 pages (6 content pages + 4 supplementary), 5 figures, Proceedings\n  of the 2024 ACM/IEEE International Symposium on Machine Learning for CAD.\n  2024 (MLCAD'24)",
        "score": 3,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on Large Language Models (LLMs) for VHDL code generation and summarization, which is outside the scope of Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on hardware description languages (HDLs) and Electronic Design Automation (EDA) is also not aligned with your interests in e-commerce and general information retrieval."
    },
    {
        "title": "Translationese-index: Using Likelihood Ratios for Graded and Generalizable Measurement of Translationese",
        "abstract": "In this paper, we propose the first quantitative measure for translationese\n-- the translationese-index (T-index) for graded and generalizable measurement\nof translationese, computed from the likelihood ratios of two contrastively\nfine-tuned language models (LMs). We use a synthesized dataset and a dataset\nwith translations in the wild to evaluate T-index's generalizability in\ncross-domain settings and its validity against human judgments. Our results\nshow that T-index is both robust and efficient. T-index scored by two 0.5B LMs\nfine-tuned on only 1-5k pairs of synthetic data can well capture translationese\nin the wild. We find that the relative differences in T-indices between\ntranslations can well predict pairwise translationese annotations obtained from\nhuman annotators; and the absolute values of T-indices correlate well with\nhuman ratings of degrees of translationese (Pearson's $r = 0.568$).\nAdditionally, the correlation between T-index and existing machine translation\n(MT) quality estimation (QE) metrics such as BLEU and COMET is low, suggesting\nthat T-index is not covered by these metrics and can serve as a complementary\nmetric in MT QE.",
        "url": "http://arxiv.org/abs/2507.12260v1",
        "pdf_url": "http://arxiv.org/pdf/2507.12260v1",
        "arxiv_id": "2507.12260v1",
        "authors": [
            "Yikang Liu",
            "Wanyang Zhang",
            "Yiming Wang",
            "Jialong Tang",
            "Pei Zhang",
            "Baosong Yang",
            "Fei Huang",
            "Rui Wang",
            "Hai Hu"
        ],
        "submitted": "2025-07-16 14:06:05",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on translationese-index, a measure for translationese, which is not directly related to information retrieval, search technologies, or query understanding. Although it involves language models and fine-tuning, the context is specific to translationese and machine translation quality estimation, making it only loosely relevant to the user's research interests."
    },
    {
        "title": "Looking for Fairness in Recommender Systems",
        "abstract": "Recommender systems can be found everywhere today, shaping our everyday\nexperience whenever we're consuming content, ordering food, buying groceries\nonline, or even just reading the news. Let's imagine we're in the process of\nbuilding a recommender system to make content suggestions to users on social\nmedia. When thinking about fairness, it becomes clear there are several\nperspectives to consider: the users asking for tailored suggestions, the\ncontent creators hoping for some limelight, and society at large, navigating\nthe repercussions of algorithmic recommendations. A shared fairness concern\nacross all three is the emergence of filter bubbles, a side-effect that takes\nplace when recommender systems are almost \"too good\", making recommendations so\ntailored that users become inadvertently confined to a narrow set of\nopinions/themes and isolated from alternative ideas. From the user's\nperspective, this is akin to manipulation. From the small content creator's\nperspective, this is an obstacle preventing them access to a whole range of\npotential fans. From society's perspective, the potential consequences are\nfar-reaching, influencing collective opinions, social behavior and political\ndecisions. How can our recommender system be fine-tuned to avoid the creation\nof filter bubbles, and ensure a more inclusive and diverse content landscape?\nApproaching this problem involves defining one (or more) performance metric to\nrepresent diversity, and tweaking our recommender system's performance through\nthe lens of fairness. By incorporating this metric into our evaluation\nframework, we aim to strike a balance between personalized recommendations and\nthe broader societal goal of fostering rich and varied cultures and points of\nview.",
        "url": "http://arxiv.org/abs/2507.12242v1",
        "pdf_url": "http://arxiv.org/pdf/2507.12242v1",
        "arxiv_id": "2507.12242v1",
        "authors": [
            "CÃ©cile LogÃ©"
        ],
        "submitted": "2025-07-16 13:53:02",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on recommender systems, which is a related topic to your research interests. However, it does not specifically address query understanding, ranking models, or user behavior modeling, which are core areas of focus for you. The paper's emphasis on fairness and diversity in recommender systems is somewhat relevant to your interests in information retrieval and search technologies, but it does not directly align with your primary research themes."
    },
    {
        "title": "BOOKCOREF: Coreference Resolution at Book Scale",
        "abstract": "Coreference Resolution systems are typically evaluated on benchmarks\ncontaining small- to medium-scale documents. When it comes to evaluating long\ntexts, however, existing benchmarks, such as LitBank, remain limited in length\nand do not adequately assess system capabilities at the book scale, i.e., when\nco-referring mentions span hundreds of thousands of tokens. To fill this gap,\nwe first put forward a novel automatic pipeline that produces high-quality\nCoreference Resolution annotations on full narrative texts. Then, we adopt this\npipeline to create the first book-scale coreference benchmark, BOOKCOREF, with\nan average document length of more than 200,000 tokens. We carry out a series\nof experiments showing the robustness of our automatic procedure and\ndemonstrating the value of our resource, which enables current long-document\ncoreference systems to gain up to +20 CoNLL-F1 points when evaluated on full\nbooks. Moreover, we report on the new challenges introduced by this\nunprecedented book-scale setting, highlighting that current models fail to\ndeliver the same performance they achieve on smaller documents. We release our\ndata and code to encourage research and development of new book-scale\nCoreference Resolution systems at https://github.com/sapienzanlp/bookcoref.",
        "url": "http://arxiv.org/abs/2507.12075v1",
        "pdf_url": "http://arxiv.org/pdf/2507.12075v1",
        "arxiv_id": "2507.12075v1",
        "authors": [
            "Giuliano Martinelli",
            "Tommaso Bonomo",
            "Pere-LluÃ­s Huguet Cabot",
            "Roberto Navigli"
        ],
        "submitted": "2025-07-16 09:35:38",
        "source": "arxiv",
        "comment": "Accepted to ACL 2025 Main Conference. 19 pages",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on coreference resolution at the book scale, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the topic of evaluating long texts, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are core areas of interest."
    },
    {
        "title": "Similarity-Guided Diffusion for Contrastive Sequential Recommendation",
        "abstract": "In sequential recommendation systems, data augmentation and contrastive\nlearning techniques have recently been introduced using diffusion models to\nachieve robust representation learning. However, most of the existing\napproaches use random augmentation, which risk damaging the contextual\ninformation of the original sequence. Accordingly, we propose a\nSimilarity-Guided Diffusion for Contrastive Sequential Recommendation. Our\nmethod leverages the similarity between item embedding vectors to generate\nsemantically consistent noise. Moreover, we utilize high confidence score in\nthe denoising process to select our augmentation positions. This approach more\neffectively reflects contextual and structural information compared to\naugmentation at random positions. From a contrastive learning perspective, the\nproposed augmentation technique provides more discriminative positive and\nnegative samples, simultaneously improving training efficiency and\nrecommendation performance. Experimental results on five benchmark datasets\nshow that SimDiffRec outperforms the existing baseline models.",
        "url": "http://arxiv.org/abs/2507.11866v1",
        "pdf_url": "http://arxiv.org/pdf/2507.11866v1",
        "arxiv_id": "2507.11866v1",
        "authors": [
            "Jinkyeong Choi",
            "Yejin Noh",
            "Donghyeon Park"
        ],
        "submitted": "2025-07-16 03:26:24",
        "source": "arxiv",
        "comment": "14 pages, 5 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on sequential recommendation systems, which is somewhat related to information retrieval and search technologies. However, the emphasis on contrastive learning and diffusion models is not directly aligned with my research interests in query understanding, ranking models, and user behavior modeling. While the paper explores data augmentation and representation learning, it does not seem to address the specific areas of deep semantic understanding and real-time relevance optimization that I am particularly interested in."
    },
    {
        "title": "S2WTM: Spherical Sliced-Wasserstein Autoencoder for Topic Modeling",
        "abstract": "Modeling latent representations in a hyperspherical space has proven\neffective for capturing directional similarities in high-dimensional text data,\nbenefiting topic modeling. Variational autoencoder-based neural topic models\n(VAE-NTMs) commonly adopt the von Mises-Fisher prior to encode hyperspherical\nstructure. However, VAE-NTMs often suffer from posterior collapse, where the KL\ndivergence term in the objective function highly diminishes, leading to\nineffective latent representations. To mitigate this issue while modeling\nhyperspherical structure in the latent space, we propose the Spherical Sliced\nWasserstein Autoencoder for Topic Modeling (S2WTM). S2WTM employs a prior\ndistribution supported on the unit hypersphere and leverages the Spherical\nSliced-Wasserstein distance to align the aggregated posterior distribution with\nthe prior. Experimental results demonstrate that S2WTM outperforms\nstate-of-the-art topic models, generating more coherent and diverse topics\nwhile improving performance on downstream tasks.",
        "url": "http://arxiv.org/abs/2507.12451v1",
        "pdf_url": "http://arxiv.org/pdf/2507.12451v1",
        "arxiv_id": "2507.12451v1",
        "authors": [
            "Suman Adhya",
            "Debarshi Kumar Sanyal"
        ],
        "submitted": "2025-07-16 17:47:45",
        "source": "arxiv",
        "comment": "Accepted as a long paper for ACL 2025 main conference",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on topic modeling using a novel autoencoder architecture, which is not directly related to information retrieval, search technologies, or query understanding. While it employs neural networks, the application is in a different domain and does not address ranking models or user behavior modeling."
    },
    {
        "title": "Improving Contextual ASR via Multi-grained Fusion with Large Language Models",
        "abstract": "While end-to-end Automatic Speech Recognition (ASR) models have shown\nimpressive performance in transcribing general speech, they often struggle to\naccurately recognize contextually relevant keywords, such as proper nouns or\nuser-specific entities.\n  Previous approaches have explored leveraging keyword dictionaries in the\ntextual modality to improve keyword recognition, either through token-level\nfusion that guides token-by-token generation or phrase-level fusion that\nenables direct copying of keyword phrases.\n  However, these methods operate at different granularities and have their own\nlimitations.\n  In this paper, we propose a novel multi-grained fusion approach that jointly\nleverages the strengths of both token-level and phrase-level fusion with Large\nLanguage Models (LLMs).\n  Our approach incorporates a late-fusion strategy that elegantly combines\nASR's acoustic information with LLM's rich contextual knowledge, balancing\nfine-grained token precision with holistic phrase-level understanding.\n  Experiments on Chinese and English datasets demonstrate that our approach\nachieves state-of-the-art performance on keyword-related metrics while\npreserving high accuracy on non-keyword text.\n  Ablation studies further confirm that the token-level and phrase-level\ncomponents both contribute significantly to the performance gains,\ncomplementing each other in our joint multi-grained framework.\n  The code and models will be publicly available at https://github.com/.",
        "url": "http://arxiv.org/abs/2507.12252v1",
        "pdf_url": "http://arxiv.org/pdf/2507.12252v1",
        "arxiv_id": "2507.12252v1",
        "authors": [
            "Shilin Zhou",
            "Zhenghua Li"
        ],
        "submitted": "2025-07-16 13:59:32",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on improving Automatic Speech Recognition (ASR) models, which is not directly related to Information Retrieval (IR) or Search technologies. While it mentions the use of Large Language Models (LLMs), the context is different from query understanding, ranking models, and user behavior modeling, which are core areas of interest in IR."
    },
    {
        "title": "Towards few-shot isolated word reading assessment",
        "abstract": "We explore an ASR-free method for isolated word reading assessment in\nlow-resource settings. Our few-shot approach compares input child speech to a\nsmall set of adult-provided reference templates. Inputs and templates are\nencoded using intermediate layers from large self-supervised learned (SSL)\nmodels. Using an Afrikaans child speech benchmark, we investigate design\noptions such as discretising SSL features and barycentre averaging of the\ntemplates. Idealised experiments show reasonable performance for adults, but a\nsubstantial drop for child speech input, even with child templates. Despite the\nsuccess of employing SSL representations in low-resource speech tasks, our work\nhighlights the limitations of SSL representations for processing child data\nwhen used in a few-shot classification system.",
        "url": "http://arxiv.org/abs/2507.12217v1",
        "pdf_url": "http://arxiv.org/pdf/2507.12217v1",
        "arxiv_id": "2507.12217v1",
        "authors": [
            "Reuben Smit",
            "Retief Louw",
            "Herman Kamper"
        ],
        "submitted": "2025-07-16 13:20:32",
        "source": "arxiv",
        "comment": "Accepted to SLaTE 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on speech recognition and isolated word reading assessment in low-resource settings, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's use of self-supervised learned models and few-shot classification system is also not relevant to the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Sparse Autoencoders for Sequential Recommendation Models: Interpretation and Flexible Control",
        "abstract": "Many current state-of-the-art models for sequential recommendations are based\non transformer architectures. Interpretation and explanation of such black box\nmodels is an important research question, as a better understanding of their\ninternals can help understand, influence, and control their behavior, which is\nvery important in a variety of real-world applications. Recently sparse\nautoencoders (SAE) have been shown to be a promising unsupervised approach for\nextracting interpretable features from language models. These autoencoders\nlearn to reconstruct hidden states of the transformer's internal layers from\nsparse linear combinations of directions in their activation space.\n  This paper is focused on the application of SAE to the sequential\nrecommendation domain. We show that this approach can be successfully applied\nto the transformer trained on a sequential recommendation task: learned\ndirections turn out to be more interpretable and monosemantic than the original\nhidden state dimensions. Moreover, we demonstrate that the features learned by\nSAE can be used to effectively and flexibly control the model's behavior,\nproviding end-users with a straightforward method to adjust their\nrecommendations to different custom scenarios and contexts.",
        "url": "http://arxiv.org/abs/2507.12202v1",
        "pdf_url": "http://arxiv.org/pdf/2507.12202v1",
        "arxiv_id": "2507.12202v1",
        "authors": [
            "Anton Klenitskiy",
            "Konstantin Polev",
            "Daria Denisova",
            "Alexey Vasilev",
            "Dmitry Simakov",
            "Gleb Gusev"
        ],
        "submitted": "2025-07-16 12:57:43",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the application of sparse autoencoders to sequential recommendation models, which is somewhat related to my interests in Information Retrieval and Search technologies. However, the focus on sequential recommendations and transformer architectures is not directly aligned with my primary research themes, such as query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Improving Data and Parameter Efficiency of Neural Language Models Using Representation Analysis",
        "abstract": "This thesis addresses challenges related to data and parameter efficiency in\nneural language models, with a focus on representation analysis and the\nintroduction of new optimization techniques. The first part examines the\nproperties and dynamics of language representations within neural models,\nemphasizing their significance in enhancing robustness and generalization. It\nproposes innovative approaches based on representation smoothness, including\nregularization strategies that utilize Jacobian and Hessian matrices to\nstabilize training and mitigate sensitivity to input perturbations. The second\npart focuses on methods to significantly enhance data and parameter efficiency\nby integrating active learning strategies with parameter-efficient fine-tuning,\nguided by insights from representation smoothness analysis. It presents\nsmoothness-informed early-stopping techniques designed to eliminate the need\nfor labeled validation sets and proposes innovative combinations of active\nlearning and parameter-efficient fine-tuning to reduce labeling efforts and\ncomputational resources. Extensive experimental evaluations across various NLP\ntasks demonstrate that these combined approaches substantially outperform\ntraditional methods in terms of performance, stability, and efficiency. The\nthird part explores weak supervision techniques enhanced by in-context learning\nto effectively utilize unlabeled data, further reducing dependence on extensive\nlabeling. It shows that using in-context learning as a mechanism for weak\nsupervision enables models to better generalize from limited labeled data by\nleveraging unlabeled examples more effectively during training. Comprehensive\nempirical evaluations confirm significant gains in model accuracy,\nadaptability, and robustness, especially in low-resource settings and dynamic\ndata environments.",
        "url": "http://arxiv.org/abs/2507.12004v1",
        "pdf_url": "http://arxiv.org/pdf/2507.12004v1",
        "arxiv_id": "2507.12004v1",
        "authors": [
            "Josip JukiÄ"
        ],
        "submitted": "2025-07-16 07:58:20",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on neural language models, representation analysis, and optimization techniques, which are not directly related to the user's research interests in Information Retrieval, Search technologies, and query understanding. While the paper touches on NLP tasks, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest for the user."
    },
    {
        "title": "IAM: Efficient Inference through Attention Mapping between Different-scale LLMs",
        "abstract": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.",
        "url": "http://arxiv.org/abs/2507.11953v1",
        "pdf_url": "http://arxiv.org/pdf/2507.11953v1",
        "arxiv_id": "2507.11953v1",
        "authors": [
            "Yi Zhao",
            "Zuchao Li",
            "Hai Zhao"
        ],
        "submitted": "2025-07-16 06:39:11",
        "source": "arxiv",
        "comment": "ACL 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on optimizing the efficiency of Large Language Models (LLMs) by leveraging attention mapping between different-scale models, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on optimization techniques, the context is specific to LLMs and does not align with the user's primary research interests."
    },
    {
        "title": "DAC: A Dynamic Attention-aware Approach for Task-Agnostic Prompt Compression",
        "abstract": "Task-agnostic prompt compression leverages the redundancy in natural language\nto reduce computational overhead and enhance information density within\nprompts, especially in long-context scenarios. Existing methods predominantly\nrely on information entropy as the metric to compress lexical units, aiming to\nachieve minimal information loss. However, these approaches overlook two\ncritical aspects: (i) the importance of attention-critical tokens at the\nalgorithmic level, and (ii) shifts in information entropy during the\ncompression process. Motivated by these challenges, we propose a dynamic\nattention-aware approach for task-agnostic prompt compression (DAC). This\napproach effectively integrates entropy and attention information, dynamically\nsensing entropy shifts during compression to achieve fine-grained prompt\ncompression. Extensive experiments across various domains, including LongBench,\nGSM8K, and BBH, show that DAC consistently yields robust and substantial\nimprovements across a diverse range of tasks and LLMs, offering compelling\nevidence of its efficacy.",
        "url": "http://arxiv.org/abs/2507.11942v1",
        "pdf_url": "http://arxiv.org/pdf/2507.11942v1",
        "arxiv_id": "2507.11942v1",
        "authors": [
            "Yi Zhao",
            "Zuchao Li",
            "Hai Zhao",
            "Baoyuan Qi",
            "Guoming Liu"
        ],
        "submitted": "2025-07-16 06:16:06",
        "source": "arxiv",
        "comment": "ACL 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on prompt compression, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the context is different from the user's primary interests. The paper's attention-aware approach is innovative, but its relevance to the user's research areas is limited."
    },
    {
        "title": "POLYCHARTQA: Benchmarking Large Vision-Language Models with Multilingual Chart Question Answering",
        "abstract": "Charts are a universally adopted medium for interpreting and communicating\ndata. However, existing chart understanding benchmarks are predominantly\nEnglish-centric, limiting their accessibility and applicability to global\naudiences. In this paper, we present PolyChartQA, the first large-scale\nmultilingual chart question answering benchmark covering 22,606 charts and\n26,151 question-answering pairs across 10 diverse languages. PolyChartQA is\nbuilt using a decoupled pipeline that separates chart data from rendering code,\nallowing multilingual charts to be flexibly generated by simply translating the\ndata and reusing the code. We leverage state-of-the-art LLM-based translation\nand enforce rigorous quality control in the pipeline to ensure the linguistic\nand semantic consistency of the generated multilingual charts. PolyChartQA\nfacilitates systematic evaluation of multilingual chart understanding.\nExperiments on both open- and closed-source large vision-language models reveal\na significant performance gap between English and other languages, especially\nlow-resource ones with non-Latin scripts. This benchmark lays a foundation for\nadvancing globally inclusive vision-language models.",
        "url": "http://arxiv.org/abs/2507.11939v1",
        "pdf_url": "http://arxiv.org/pdf/2507.11939v1",
        "arxiv_id": "2507.11939v1",
        "authors": [
            "Yichen Xu",
            "Liangyu Chen",
            "Liang Zhang",
            "Wenxuan Wang",
            "Qin Jin"
        ],
        "submitted": "2025-07-16 06:09:02",
        "source": "arxiv",
        "comment": "Work in Progress",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on multilingual chart question answering, which is not directly related to my research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves vision-language models, the primary focus is on chart understanding and translation, which is not a central match for my interests."
    },
    {
        "title": "AFPM: Alignment-based Frame Patch Modeling for Cross-Dataset EEG Decoding",
        "abstract": "Electroencephalogram (EEG) decoding models for brain-computer interfaces\n(BCIs) struggle with cross-dataset learning and generalization due to channel\nlayout inconsistencies, non-stationary signal distributions, and limited\nneurophysiological prior integration. To address these issues, we propose a\nplug-and-play Alignment-Based Frame-Patch Modeling (AFPM) framework, which has\ntwo main components: 1) Spatial Alignment, which selects task-relevant channels\nbased on brain-region priors, aligns EEG distributions across domains, and\nremaps the selected channels to a unified layout; and, 2) Frame-Patch Encoding,\nwhich models multi-dataset signals into unified spatiotemporal patches for EEG\ndecoding. Compared to 17 state-of-the-art approaches that need dataset-specific\ntuning, the proposed calibration-free AFPM achieves performance gains of up to\n4.40% on motor imagery and 3.58% on event-related potential tasks. To our\nknowledge, this is the first calibration-free cross-dataset EEG decoding\nframework, substantially enhancing the practicalness of BCIs in real-world\napplications.",
        "url": "http://arxiv.org/abs/2507.11911v1",
        "pdf_url": "http://arxiv.org/pdf/2507.11911v1",
        "arxiv_id": "2507.11911v1",
        "authors": [
            "Xiaoqing Chen",
            "Siyang Li",
            "Dongrui Wu"
        ],
        "submitted": "2025-07-16 04:55:09",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is not relevant to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The topic of EEG decoding and brain-computer interfaces is unrelated to the user's focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Your LLM Knows the Future: Uncovering Its Multi-Token Prediction Potential",
        "abstract": "Autoregressive language models are constrained by their inherently sequential\nnature, generating one token at a time. This paradigm limits inference speed\nand parallelism, especially during later stages of generation when the\ndirection and semantics of text are relatively certain. In this work, we\npropose a novel framework that leverages the inherent knowledge of vanilla\nautoregressive language models about future tokens, combining techniques to\nrealize this potential and enable simultaneous prediction of multiple\nsubsequent tokens. Our approach introduces several key innovations: (1) a\nmasked-input formulation where multiple future tokens are jointly predicted\nfrom a common prefix; (2) a gated LoRA formulation that preserves the original\nLLM's functionality, while equipping it for multi-token prediction; (3) a\nlightweight, learnable sampler module that generates coherent sequences from\nthe predicted future tokens; (4) a set of auxiliary training losses, including\na consistency loss, to enhance the coherence and accuracy of jointly generated\ntokens; and (5) a speculative generation strategy that expands tokens\nquadratically in the future while maintaining high fidelity. Our method\nachieves significant speedups through supervised fine-tuning on pretrained\nmodels. For example, it generates code and math nearly 5x faster, and improves\ngeneral chat and knowledge tasks by almost 2.5x. These gains come without any\nloss in quality.",
        "url": "http://arxiv.org/abs/2507.11851v1",
        "pdf_url": "http://arxiv.org/pdf/2507.11851v1",
        "arxiv_id": "2507.11851v1",
        "authors": [
            "Mohammad Samragh",
            "Arnav Kundu",
            "David Harrison",
            "Kumari Nishu",
            "Devang Naik",
            "Minsik Cho",
            "Mehrdad Farajtabar"
        ],
        "submitted": "2025-07-16 02:31:40",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on language models and their ability to predict future tokens, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions speedups and improvements in certain tasks, the paper's primary contribution is in the area of natural language processing, which is only tangentially relevant to the user's research interests."
    },
    {
        "title": "CRABS: A syntactic-semantic pincer strategy for bounding LLM interpretation of Python notebooks",
        "abstract": "Recognizing the information flows and operations comprising data science and\nmachine learning Python notebooks is critical for evaluating, reusing, and\nadapting notebooks for new tasks. Investigating a notebook via re-execution\noften is impractical due to the challenges of resolving data and software\ndependencies. While Large Language Models (LLMs) pre-trained on large codebases\nhave demonstrated effectiveness in understanding code without running it, we\nobserve that they fail to understand some realistic notebooks due to\nhallucinations and long-context challenges. To address these issues, we propose\na notebook understanding task yielding an information flow graph and\ncorresponding cell execution dependency graph for a notebook, and demonstrate\nthe effectiveness of a pincer strategy that uses limited syntactic analysis to\nassist full comprehension of the notebook using an LLM. Our Capture and Resolve\nAssisted Bounding Strategy (CRABS) employs shallow syntactic parsing and\nanalysis of the abstract syntax tree (AST) to capture the correct\ninterpretation of a notebook between lower and upper estimates of the\ninter-cell I/O sets, then uses an LLM to resolve remaining ambiguities via\ncell-by-cell zero-shot learning, thereby identifying the true data inputs and\noutputs of each cell. We evaluate and demonstrate the effectiveness of our\napproach using an annotated dataset of 50 representative, highly up-voted\nKaggle notebooks that together represent 3454 actual cell inputs and outputs.\nThe LLM correctly resolves 1397 of 1425 (98%) ambiguities left by analyzing the\nsyntactic structure of these notebooks. Across 50 notebooks, CRABS achieves\naverage F1 scores of 98% identifying cell-to-cell information flows and 99%\nidentifying transitive cell execution dependencies.",
        "url": "http://arxiv.org/abs/2507.11742v1",
        "pdf_url": "http://arxiv.org/pdf/2507.11742v1",
        "arxiv_id": "2507.11742v1",
        "authors": [
            "Meng Li",
            "Timothy M. McPhillips",
            "Dingmin Wang",
            "Shin-Rong Tsai",
            "Bertram LudÃ¤scher"
        ],
        "submitted": "2025-07-15 21:14:08",
        "source": "arxiv",
        "comment": "Preprint. Accepted to COLM 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on understanding Python notebooks using Large Language Models (LLMs), which is not directly related to Information Retrieval (IR), Search technologies, or query understanding. Although it involves natural language processing, the context is specific to code analysis and not relevant to the user's primary research interests."
    },
    {
        "title": "Spatially Grounded Explanations in Vision Language Models for Document Visual Question Answering",
        "abstract": "We introduce EaGERS, a fully training-free and model-agnostic pipeline that\n(1) generates natural language rationales via a vision language model, (2)\ngrounds these rationales to spatial sub-regions by computing multimodal\nembedding similarities over a configurable grid with majority voting, and (3)\nrestricts the generation of responses only from the relevant regions selected\nin the masked image. Experiments on the DocVQA dataset demonstrate that our\nbest configuration not only outperforms the base model on exact match accuracy\nand Average Normalized Levenshtein Similarity metrics but also enhances\ntransparency and reproducibility in DocVQA without additional model\nfine-tuning.",
        "url": "http://arxiv.org/abs/2507.12490v1",
        "pdf_url": "http://arxiv.org/pdf/2507.12490v1",
        "arxiv_id": "2507.12490v1",
        "authors": [
            "Maximiliano HormazÃ¡bal Lagos",
            "HÃ©ctor Cerezo-Costas",
            "Dimosthenis Karatzas"
        ],
        "submitted": "2025-07-15 20:05:25",
        "source": "arxiv",
        "comment": "This work has been accepted for presentation at the 16th Conference\n  and Labs of the Evaluation Forum (CLEF 2025) and will be published in the\n  proceedings by Springer in the Lecture Notes in Computer Science (LNCS)\n  series. Please cite the published version when available",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on vision language models and document visual question answering, which is not directly related to the user's interests in Information Retrieval, Search technologies, and Natural Language Processing. The concepts of query understanding, ranking models, and user behavior modeling are not addressed in this paper."
    },
    {
        "title": "MetaLint: Generalizable Idiomatic Code Quality Analysis through Instruction-Following and Easy-to-Hard Generalization",
        "abstract": "Large Language Models, though successful in code generation, struggle with\ncode quality analysis because they are limited by static training data and\ncan't easily adapt to evolving best practices. We introduce MetaLint, a new\ninstruction-following framework that formulates code quality analysis as the\ntask of detecting and fixing problematic semantic code fragments or code idioms\nbased on high-level specifications. Unlike conventional approaches that train\nmodels on static, rule-based data, MetaLint employs instruction tuning on\nsynthetic linter-generated data to support easy-to-hard generalization,\nenabling models to adapt to novel or complex code patterns without retraining.\nTo evaluate this, we construct a benchmark of challenging idioms inspired by\nreal-world coding standards such as Python Enhancement Proposals (PEPs) and\nassess whether MetaLint-trained models reason adaptively or simply memorize.\nOur results show that MetaLint improves generalization to unseen PEP idioms,\nachieving a 70.37% F-score on idiom detection with the highest recall (70.43%)\namong all evaluated models. It also achieves 26.73% on localization,\ncompetitive for its 4B parameter size and comparable to larger state-of-the-art\nmodels like o3-mini, highlighting its potential for future-proof code quality\nanalysis.",
        "url": "http://arxiv.org/abs/2507.11687v1",
        "pdf_url": "http://arxiv.org/pdf/2507.11687v1",
        "arxiv_id": "2507.11687v1",
        "authors": [
            "Atharva Naik",
            "Lawanya Baghel",
            "Dhakshin Govindarajan",
            "Darsh Agrawal",
            "Daniel Fried",
            "Carolyn Rose"
        ],
        "submitted": "2025-07-15 19:44:20",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The paper focuses on code quality analysis and large language models, which is a different domain and does not align with your research themes."
    },
    {
        "title": "Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training",
        "abstract": "Recent advancements in reasoning-focused language models such as OpenAI's O1\nand DeepSeek-R1 have shown that scaling test-time computation-through\nchain-of-thought reasoning and iterative exploration-can yield substantial\nimprovements on complex tasks like mathematics and code generation. These\nbreakthroughs have been driven by large-scale reinforcement learning (RL),\nparticularly when combined with verifiable reward signals that provide\nobjective and grounded supervision. In this report, we investigate the effects\nof prolonged reinforcement learning on a small language model across a diverse\nset of reasoning domains. Our work identifies several key ingredients for\neffective training, including the use of verifiable reward tasks, enhancements\nto Group Relative Policy Optimization (GRPO), and practical techniques to\nimprove training stability and generalization. We introduce controlled KL\nregularization, clipping ratio, and periodic reference policy resets as\ncritical components for unlocking long-term performance gains. Our model\nachieves significant improvements over strong baselines, including +14.7% on\nmath, +13.9% on coding, and +54.8% on logic puzzle tasks. To facilitate\ncontinued research, we release our model publicly.",
        "url": "http://arxiv.org/abs/2507.12507v1",
        "pdf_url": "http://arxiv.org/pdf/2507.12507v1",
        "arxiv_id": "2507.12507v1",
        "authors": [
            "Mingjie Liu",
            "Shizhe Diao",
            "Jian Hu",
            "Ximing Lu",
            "Xin Dong",
            "Hao Zhang",
            "Alexander Bukharin",
            "Shaokun Zhang",
            "Jiaqi Zeng",
            "Makesh Narsimhan Sreedhar",
            "Gerald Shen",
            "David Mosallanezhad",
            "Di Zhang",
            "Jonas Yang",
            "June Yang",
            "Oleksii Kuchaiev",
            "Guilin Liu",
            "Zhiding Yu",
            "Pavlo Molchanov",
            "Yejin Choi",
            "Jan Kautz",
            "Yi Dong"
        ],
        "submitted": "2025-07-16 17:59:24",
        "source": "arxiv",
        "comment": "14 pages, 7 figures",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on scaling up reinforcement learning for language models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions reasoning and optimization, the context is different from the user's interests in IR and NLP."
    },
    {
        "title": "Exploring Gender Bias in Alzheimer's Disease Detection: Insights from Mandarin and Greek Speech Perception",
        "abstract": "Gender bias has been widely observed in speech perception tasks, influenced\nby the fundamental voicing differences between genders. This study reveals a\ngender bias in the perception of Alzheimer's Disease (AD) speech. In a\nperception experiment involving 16 Chinese listeners evaluating both Chinese\nand Greek speech, we identified that male speech was more frequently identified\nas AD, with this bias being particularly pronounced in Chinese speech. Acoustic\nanalysis showed that shimmer values in male speech were significantly\nassociated with AD perception, while speech portion exhibited a significant\nnegative correlation with AD identification. Although language did not have a\nsignificant impact on AD perception, our findings underscore the critical role\nof gender bias in AD speech perception. This work highlights the necessity of\naddressing gender bias when developing AD detection models and calls for\nfurther research to validate model performance across different linguistic\ncontexts.",
        "url": "http://arxiv.org/abs/2507.12356v1",
        "pdf_url": "http://arxiv.org/pdf/2507.12356v1",
        "arxiv_id": "2507.12356v1",
        "authors": [
            "Liu He",
            "Yuanchao Li",
            "Rui Feng",
            "XinRan Han",
            "Yin-Long Liu",
            "Yuwei Yang",
            "Zude Zhu",
            "Jiahong Yuan"
        ],
        "submitted": "2025-07-16 15:56:09",
        "source": "arxiv",
        "comment": "12 pages, 5 figures, conference or other essential info",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The topic of Alzheimer's Disease detection and speech perception is outside your primary focus, and the paper does not address any of the specific areas you mentioned."
    },
    {
        "title": "Nonlinear Concept Erasure: a Density Matching Approach",
        "abstract": "Ensuring that neural models used in real-world applications cannot infer\nsensitive information, such as demographic attributes like gender or race, from\ntext representations is a critical challenge when fairness is a concern. We\naddress this issue through concept erasure, a process that removes information\nrelated to a specific concept from distributed representations while preserving\nas much of the remaining semantic information as possible. Our approach\ninvolves learning an orthogonal projection in the embedding space, designed to\nmake the class-conditional feature distributions of the discrete concept to\nerase indistinguishable after projection. By adjusting the rank of the\nprojector, we control the extent of information removal, while its\northogonality ensures strict preservation of the local structure of the\nembeddings. Our method, termed $\\overline{\\mathrm{L}}$EOPARD, achieves\nstate-of-the-art performance in nonlinear erasure of a discrete attribute on\nclassic natural language processing benchmarks. Furthermore, we demonstrate\nthat $\\overline{\\mathrm{L}}$EOPARD effectively mitigates bias in deep nonlinear\nclassifiers, thereby promoting fairness.",
        "url": "http://arxiv.org/abs/2507.12341v1",
        "pdf_url": "http://arxiv.org/pdf/2507.12341v1",
        "arxiv_id": "2507.12341v1",
        "authors": [
            "Antoine Saillenfest",
            "Pirmin Lemberger"
        ],
        "submitted": "2025-07-16 15:36:15",
        "source": "arxiv",
        "comment": "17 pages, 10 figures, accepted for publication in ECAI 2025 (28th\n  European Conference on Artificial Intelligence)",
        "score": 1,
        "keyword_reasons": [
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on concept erasure in neural models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on fairness and bias, the context is not aligned with the user's primary research interests in IR and NLP."
    },
    {
        "title": "MERA Code: A Unified Framework for Evaluating Code Generation Across Tasks",
        "abstract": "Advancements in LLMs have enhanced task automation in software engineering;\nhowever, current evaluations primarily focus on natural language tasks,\noverlooking code quality. Most benchmarks prioritize high-level reasoning over\nexecutable code and real-world performance, leaving gaps in understanding true\ncapabilities and risks associated with these models in production. To address\nthis issue, we propose MERA Code, a new addition to the MERA benchmark family,\nspecifically focused on evaluating code for the latest code generation LLMs in\nRussian. This benchmark includes 11 evaluation tasks that span 8 programming\nlanguages. Our proposed evaluation methodology features a taxonomy that\noutlines the practical coding skills necessary for models to complete these\ntasks. The benchmark comprises an open-source codebase for users to conduct\nMERA assessments, a scoring system compatible with various programming\nenvironments, and a platform featuring a leaderboard and submission system. We\nevaluate open LLMs and frontier API models, analyzing their limitations in\nterms of practical coding tasks in non-English languages. We are publicly\nreleasing MERA to guide future research, anticipate groundbreaking features in\nmodel development, and standardize evaluation procedures.",
        "url": "http://arxiv.org/abs/2507.12284v2",
        "pdf_url": "http://arxiv.org/pdf/2507.12284v2",
        "arxiv_id": "2507.12284v2",
        "authors": [
            "Artem Chervyakov",
            "Alexander Kharitonov",
            "Pavel Zadorozhny",
            "Adamenko Pavel",
            "Rodion Levichev",
            "Dmitrii Vorobev",
            "Dmitrii Salikhov",
            "Aidar Valeev",
            "Alena Pestova",
            "Maria Dziuba",
            "Ilseyar Alimova",
            "Artem Zavgorodnev",
            "Aleksandr Medvedev",
            "Stanislav Moiseev",
            "Elena Bruches",
            "Daniil Grebenkin",
            "Roman Derunets",
            "Vikulov Vladimir",
            "Anton Emelyanov",
            "Dmitrii Babaev",
            "Vladimir V. Ivanov",
            "Valentin Malykh",
            "Alena Fenogenova"
        ],
        "submitted": "2025-07-16 14:31:33",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on code generation and evaluation, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions language models, the context is different from the user's interests in NLP and IR."
    },
    {
        "title": "RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization",
        "abstract": "Low-Rank Adaptation (LoRA) has become a widely adopted standard for\nparameter-efficient fine-tuning of large language models (LLMs), significantly\nreducing memory and computational demands. However, challenges remain,\nincluding finding optimal initialization strategies or mitigating\noverparametrization in low-rank matrix factorization. In this work, we propose\na novel approach that addresses both of the challenges simultaneously within a\nunified framework. Our method treats a set of fixed-rank LoRA matrices as a\nsmooth manifold. Considering adapters as elements on this manifold removes\noverparametrization, while determining the direction of the fastest loss\ndecrease along the manifold provides initialization. Special care is taken to\nobtain numerically stable and computationally efficient implementation of our\nmethod, using best practices from numerical linear algebra and Riemannian\noptimization. Experimental results on LLM and diffusion model architectures\ndemonstrate that RiemannLoRA consistently improves both convergence speed and\nfinal performance over standard LoRA and its state-of-the-art modifications.",
        "url": "http://arxiv.org/abs/2507.12142v1",
        "pdf_url": "http://arxiv.org/pdf/2507.12142v1",
        "arxiv_id": "2507.12142v1",
        "authors": [
            "Vladimir Bogachev",
            "Vladimir Aletov",
            "Alexander Molozhavenko",
            "Denis Bobkov",
            "Vera Soboleva",
            "Aibek Alanov",
            "Maxim Rakhuba"
        ],
        "submitted": "2025-07-16 11:17:12",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on LoRA optimization, which is not directly related to information retrieval, query understanding, or ranking models. While it mentions language models, the context is not about search technologies or user behavior modeling, making it only loosely relevant to the user's research interests."
    },
    {
        "title": "Toxicity-Aware Few-Shot Prompting for Low-Resource Singlish Translation",
        "abstract": "As online communication increasingly incorporates under-represented languages\nand colloquial dialects, standard translation systems often fail to preserve\nlocal slang, code-mixing, and culturally embedded markers of harmful speech.\nTranslating toxic content between low-resource language pairs poses additional\nchallenges due to scarce parallel data and safety filters that sanitize\noffensive expressions. In this work, we propose a reproducible, two-stage\nframework for toxicity-preserving translation, demonstrated on a code-mixed\nSinglish safety corpus. First, we perform human-verified few-shot prompt\nengineering: we iteratively curate and rank annotator-selected Singlish-target\nexamples to capture nuanced slang, tone, and toxicity. Second, we optimize\nmodel-prompt pairs by benchmarking several large language models using semantic\nsimilarity via direct and back-translation. Quantitative human evaluation\nconfirms the effectiveness and efficiency of our pipeline. Beyond improving\ntranslation quality, our framework contributes to the safety of multicultural\nLLMs by supporting culturally sensitive moderation and benchmarking in\nlow-resource contexts. By positioning Singlish as a testbed for inclusive NLP,\nwe underscore the importance of preserving sociolinguistic nuance in real-world\napplications such as content moderation and regional platform governance.",
        "url": "http://arxiv.org/abs/2507.11966v1",
        "pdf_url": "http://arxiv.org/pdf/2507.11966v1",
        "arxiv_id": "2507.11966v1",
        "authors": [
            "Ziyu Ge",
            "Gabriel Chua",
            "Leanne Tan",
            "Roy Ka-Wei Lee"
        ],
        "submitted": "2025-07-16 06:58:02",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on low-resource language translation, specifically Singlish, and toxicity-aware prompting, which is not directly related to the user's interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it touches on NLP, the topic is not aligned with the user's primary focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "A Survey of Deep Learning for Geometry Problem Solving",
        "abstract": "Geometry problem solving is a key area of mathematical reasoning, which is\nwidely involved in many important fields such as education, mathematical\nability assessment of artificial intelligence, and multimodal ability\nassessment. In recent years, the rapid development of deep learning technology,\nespecially the rise of multimodal large language models, has triggered a\nwidespread research boom. This paper provides a survey of the applications of\ndeep learning in geometry problem solving, including (i) a comprehensive\nsummary of the relevant tasks in geometry problem solving; (ii) a thorough\nreview of related deep learning methods; (iii) a detailed analysis of\nevaluation metrics and methods; and (iv) a critical discussion of the current\nchallenges and future directions that can be explored. Our goal is to provide a\ncomprehensive and practical reference of deep learning for geometry problem\nsolving to promote further developments in this field. We create a continuously\nupdated list of papers on GitHub: https://github.com/majianz/dl4gps.",
        "url": "http://arxiv.org/abs/2507.11936v1",
        "pdf_url": "http://arxiv.org/pdf/2507.11936v1",
        "arxiv_id": "2507.11936v1",
        "authors": [
            "Jianzhe Ma",
            "Wenxuan Wang",
            "Qin Jin"
        ],
        "submitted": "2025-07-16 06:03:08",
        "source": "arxiv",
        "comment": "Work in progress",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on deep learning for geometry problem solving, which is outside the scope of information retrieval, search technologies, and natural language processing. The paper's topics, such as multimodal large language models and geometry problem solving, do not align with your primary research areas."
    },
    {
        "title": "Cross-lingual Few-shot Learning for Persian Sentiment Analysis with Incremental Adaptation",
        "abstract": "This research examines cross-lingual sentiment analysis using few-shot\nlearning and incremental learning methods in Persian. The main objective is to\ndevelop a model capable of performing sentiment analysis in Persian using\nlimited data, while getting prior knowledge from high-resource languages. To\nachieve this, three pre-trained multilingual models (XLM-RoBERTa, mDeBERTa, and\nDistilBERT) were employed, which were fine-tuned using few-shot and incremental\nlearning approaches on small samples of Persian data from diverse sources,\nincluding X, Instagram, Digikala, Snappfood, and Taaghche. This variety enabled\nthe models to learn from a broad range of contexts. Experimental results show\nthat the mDeBERTa and XLM-RoBERTa achieved high performances, reaching 96%\naccuracy on Persian sentiment analysis. These findings highlight the\neffectiveness of combining few-shot learning and incremental learning with\nmultilingual pre-trained models.",
        "url": "http://arxiv.org/abs/2507.11634v1",
        "pdf_url": "http://arxiv.org/pdf/2507.11634v1",
        "arxiv_id": "2507.11634v1",
        "authors": [
            "Farideh Majidi",
            "Ziaeddin Beheshtifard"
        ],
        "submitted": "2025-07-15 18:13:25",
        "source": "arxiv",
        "comment": "Proceedings of the First National Conference on Artificial\n  Intelligence and Emerging Research: Convergence of Humans and Intelligent\n  Systems",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on cross-lingual sentiment analysis in Persian, using few-shot learning and incremental adaptation, which is not directly related to the user's interests in Information Retrieval, Search technologies, and query understanding. While it involves NLP and deep learning, the specific application and methodology are not aligned with the user's research themes."
    },
    {
        "title": "MapIQ: Benchmarking Multimodal Large Language Models for Map Question Answering",
        "abstract": "Recent advancements in multimodal large language models (MLLMs) have driven\nresearchers to explore how well these models read data visualizations, e.g.,\nbar charts, scatter plots. More recently, attention has shifted to visual\nquestion answering with maps (Map-VQA). However, Map-VQA research has primarily\nfocused on choropleth maps, which cover only a limited range of thematic\ncategories and visual analytical tasks. To address these gaps, we introduce\nMapIQ, a benchmark dataset comprising 14,706 question-answer pairs across three\nmap types: choropleth maps, cartograms, and proportional symbol maps spanning\ntopics from six distinct themes (e.g., housing, crime). We evaluate multiple\nMLLMs using six visual analytical tasks, comparing their performance against\none another and a human baseline. An additional experiment examining the impact\nof map design changes (e.g., altered color schemes, modified legend designs,\nand removal of map elements) provides insights into the robustness and\nsensitivity of MLLMs, their reliance on internal geographic knowledge, and\npotential avenues for improving Map-VQA performance.",
        "url": "http://arxiv.org/abs/2507.11625v1",
        "pdf_url": "http://arxiv.org/pdf/2507.11625v1",
        "arxiv_id": "2507.11625v1",
        "authors": [
            "Varun Srivastava",
            "Fan Lei",
            "Srija Mukhopadhyay",
            "Vivek Gupta",
            "Ross Maciejewski"
        ],
        "submitted": "2025-07-15 18:02:57",
        "source": "arxiv",
        "comment": "Published as a conference paper at COLM 2025",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on multimodal large language models for map question answering, which is a specific application of natural language processing. While it involves visual data and question answering, it does not directly relate to query understanding, ranking models, or user behavior modeling in the context of information retrieval, which are the user's primary research interests."
    }
]