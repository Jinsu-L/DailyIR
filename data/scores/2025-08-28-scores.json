[
    {
        "title": "Uncovering the Bigger Picture: Comprehensive Event Understanding Via Diverse News Retrieval",
        "abstract": "Access to diverse perspectives is essential for understanding real-world\nevents, yet most news retrieval systems prioritize textual relevance, leading\nto redundant results and limited viewpoint exposure. We propose NEWSCOPE, a\ntwo-stage framework for diverse news retrieval that enhances event coverage by\nexplicitly modeling semantic variation at the sentence level. The first stage\nretrieves topically relevant content using dense retrieval, while the second\nstage applies sentence-level clustering and diversity-aware re-ranking to\nsurface complementary information. To evaluate retrieval diversity, we\nintroduce three interpretable metrics, namely Average Pairwise Distance,\nPositive Cluster Coverage, and Information Density Ratio, and construct two\nparagraph-level benchmarks: LocalNews and DSGlobal. Experiments show that\nNEWSCOPE consistently outperforms strong baselines, achieving significantly\nhigher diversity without compromising relevance. Our results demonstrate the\neffectiveness of fine-grained, interpretable modeling in mitigating redundancy\nand promoting comprehensive event understanding. The data and code are\navailable at https://github.com/tangyixuan/NEWSCOPE.",
        "url": "http://arxiv.org/abs/2508.19758v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19758v1",
        "arxiv_id": "2508.19758v1",
        "authors": [
            "Yixuan Tang",
            "Yuanyuan Shi",
            "Yiqun Sun",
            "Anthony Kum Hoe Tung"
        ],
        "submitted": "2025-08-27 10:37:32",
        "source": "arxiv",
        "comment": "Accepted by EMNLP 2025",
        "score": 17,
        "keyword_reasons": [
            "Found 'dense retrieval' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'pairwise' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper proposes a framework for diverse news retrieval, focusing on semantic variation at the sentence level. While it's not directly related to query understanding, ranking models, or user behavior modeling, it does explore information retrieval and relevance optimization. The connection to information retrieval is strong, but the specific domain and application are different from the user's primary focus."
    },
    {
        "title": "Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning",
        "abstract": "Retrieval-Augmented Generation (RAG) has become a standard approach for\nimproving the reliability of large language models (LLMs). Prior work\ndemonstrates the vulnerability of RAG systems by misleading them into\ngenerating attacker-chosen outputs through poisoning the knowledge base.\nHowever, this paper uncovers that such attacks could be mitigated by the strong\n\\textit{self-correction ability (SCA)} of modern LLMs, which can reject false\ncontext once properly configured. This SCA poses a significant challenge for\nattackers aiming to manipulate RAG systems.\n  In contrast to previous poisoning methods, which primarily target the\nknowledge base, we introduce \\textsc{DisarmRAG}, a new poisoning paradigm that\ncompromises the retriever itself to suppress the SCA and enforce\nattacker-chosen outputs. This compromisation enables the attacker to\nstraightforwardly embed anti-SCA instructions into the context provided to the\ngenerator, thereby bypassing the SCA. To this end, we present a\ncontrastive-learning-based model editing technique that performs localized and\nstealthy edits, ensuring the retriever returns a malicious instruction only for\nspecific victim queries while preserving benign retrieval behavior. To further\nstrengthen the attack, we design an iterative co-optimization framework that\nautomatically discovers robust instructions capable of bypassing prompt-based\ndefenses. We extensively evaluate DisarmRAG across six LLMs and three QA\nbenchmarks. Our results show near-perfect retrieval of malicious instructions,\nwhich successfully suppress SCA and achieve attack success rates exceeding 90\\%\nunder diverse defensive prompts. Also, the edited retriever remains stealthy\nunder several detection methods, highlighting the urgent need for\nretriever-centric defenses.",
        "url": "http://arxiv.org/abs/2508.20083v1",
        "pdf_url": "http://arxiv.org/pdf/2508.20083v1",
        "arxiv_id": "2508.20083v1",
        "authors": [
            "Yanbo Dai",
            "Zhenlan Ji",
            "Zongjie Li",
            "Kuan Li",
            "Shuai Wang"
        ],
        "submitted": "2025-08-27 17:49:28",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval (IR) or Search technologies, and does not align with the user's focus on query understanding, ranking models, or user behavior modeling. The paper's topic is more focused on Natural Language Processing (NLP) and language model manipulation, which is not a central match for the user's research interests."
    },
    {
        "title": "Youtu-GraphRAG: Vertically Unified Agents for Graph Retrieval-Augmented Complex Reasoning",
        "abstract": "Graph retrieval-augmented generation (GraphRAG) has effectively enhanced\nlarge language models in complex reasoning by organizing fragmented knowledge\ninto explicitly structured graphs. Prior efforts have been made to improve\neither graph construction or graph retrieval in isolation, yielding suboptimal\nperformance, especially when domain shifts occur. In this paper, we propose a\nvertically unified agentic paradigm, Youtu-GraphRAG, to jointly connect the\nentire framework as an intricate integration. Specifically, (i) a seed graph\nschema is introduced to bound the automatic extraction agent with targeted\nentity types, relations and attribute types, also continuously expanded for\nscalability over unseen domains; (ii) To obtain higher-level knowledge upon the\nschema, we develop novel dually-perceived community detection, fusing\nstructural topology with subgraph semantics for comprehensive knowledge\norganization. This naturally yields a hierarchical knowledge tree that supports\nboth top-down filtering and bottom-up reasoning with community summaries; (iii)\nAn agentic retriever is designed to interpret the same graph schema to\ntransform complex queries into tractable and parallel sub-queries. It\niteratively performs reflection for more advanced reasoning; (iv) To alleviate\nthe knowledge leaking problem in pre-trained LLM, we propose a tailored\nanonymous dataset and a novel 'Anonymity Reversion' task that deeply measures\nthe real performance of the GraphRAG frameworks. Extensive experiments across\nsix challenging benchmarks demonstrate the robustness of Youtu-GraphRAG,\nremarkably moving the Pareto frontier with up to 90.71% saving of token costs\nand 16.62% higher accuracy over state-of-the-art baselines. The results\nindicate our adaptability, allowing seamless domain transfer with minimal\nintervention on schema.",
        "url": "http://arxiv.org/abs/2508.19855v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19855v1",
        "arxiv_id": "2508.19855v1",
        "authors": [
            "Junnan Dong",
            "Siyu An",
            "Yifei Yu",
            "Qian-Wen Zhang",
            "Linhao Luo",
            "Xiao Huang",
            "Yunsheng Wu",
            "Di Yin",
            "Xing Sun"
        ],
        "submitted": "2025-08-27 13:13:20",
        "source": "arxiv",
        "comment": "19 pages, 7 figures, 6 tables",
        "score": 10,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on graph retrieval-augmented complex reasoning, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and query understanding. The paper's emphasis on graph construction, community detection, and anonymity reversion is not relevant to the user's primary focus on deep semantic understanding and real-time relevance optimization in IR."
    },
    {
        "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear Hashing-based KV Cache Retrieval",
        "abstract": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
        "url": "http://arxiv.org/abs/2508.19740v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19740v1",
        "arxiv_id": "2508.19740v1",
        "authors": [
            "Wenhao Li",
            "Yuxin Zhang",
            "Gen Luo",
            "Haiyuan Wan",
            "Ziyang Gong",
            "Fei Chao",
            "Rongrong Ji"
        ],
        "submitted": "2025-08-27 10:11:27",
        "source": "arxiv",
        "comment": null,
        "score": 9,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on optimizing Large Language Models (LLMs) for efficient inference, using non-linear hashing-based KV cache retrieval. While it touches on query understanding and retrieval, the primary focus is on LLMs and caching, which is not directly related to my core research interests in Information Retrieval and Search technologies. The paper's relevance is somewhat limited to my broader interests in NLP and data mining."
    },
    {
        "title": "Heterogeneous LLM Methods for Ontology Learning (Few-Shot Prompting, Ensemble Typing, and Attention-Based Taxonomies)",
        "abstract": "We present a comprehensive system for addressing Tasks A, B, and C of the\nLLMs4OL 2025 challenge, which together span the full ontology construction\npipeline: term extraction, typing, and taxonomy discovery. Our approach\ncombines retrieval-augmented prompting, zero-shot classification, and\nattention-based graph modeling -- each tailored to the demands of the\nrespective task. For Task A, we jointly extract domain-specific terms and their\nontological types using a retrieval-augmented generation (RAG) pipeline.\nTraining data was reformulated into a document to terms and types\ncorrespondence, while test-time inference leverages semantically similar\ntraining examples. This single-pass method requires no model finetuning and\nimproves overall performance through lexical augmentation Task B, which\ninvolves assigning types to given terms, is handled via a dual strategy. In the\nfew-shot setting (for domains with labeled training data), we reuse the RAG\nscheme with few-shot prompting. In the zero-shot setting (for previously unseen\ndomains), we use a zero-shot classifier that combines cosine similarity scores\nfrom multiple embedding models using confidence-based weighting. In Task C, we\nmodel taxonomy discovery as graph inference. Using embeddings of type labels,\nwe train a lightweight cross-attention layer to predict is-a relations by\napproximating a soft adjacency matrix. These modular, task-specific solutions\nenabled us to achieve top-ranking results in the official leaderboard across\nall three tasks. Taken together these strategies showcase the scalability,\nadaptability, and robustness of LLM-based architectures for ontology learning\nacross heterogeneous domains.\n  Code is available at:\nhttps://github.com/BelyaevaAlex/LLMs4OL-Challenge-Alexbek",
        "url": "http://arxiv.org/abs/2508.19428v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19428v1",
        "arxiv_id": "2508.19428v1",
        "authors": [
            "Aleksandra Beliaeva",
            "Temurbek Rahmatullaev"
        ],
        "submitted": "2025-08-26 20:50:16",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper presents a comprehensive system for ontology learning, using LLM-based architectures and various techniques such as retrieval-augmented prompting, zero-shot classification, and attention-based graph modeling. While it touches on some aspects of information retrieval, such as term extraction and taxonomy discovery, the focus is primarily on ontology learning and construction, which is not directly aligned with the user's core research themes in information retrieval and search technologies."
    },
    {
        "title": "AI for Statutory Simplification: A Comprehensive State Legal Corpus and Labor Benchmark",
        "abstract": "One of the emerging use cases of AI in law is for code simplification:\nstreamlining, distilling, and simplifying complex statutory or regulatory\nlanguage. One U.S. state has claimed to eliminate one third of its state code\nusing AI. Yet we lack systematic evaluations of the accuracy, reliability, and\nrisks of such approaches. We introduce LaborBench, a question-and-answer\nbenchmark dataset designed to evaluate AI capabilities in this domain. We\nleverage a unique data source to create LaborBench: a dataset updated annually\nby teams of lawyers at the U.S. Department of Labor, who compile differences in\nunemployment insurance laws across 50 states for over 101 dimensions in a\nsix-month process, culminating in a 200-page publication of tables. Inspired by\nour collaboration with one U.S. state to explore using large language models\n(LLMs) to simplify codes in this domain, where complexity is particularly\nacute, we transform the DOL publication into LaborBench. This provides a unique\nbenchmark for AI capacity to conduct, distill, and extract realistic statutory\nand regulatory information. To assess the performance of retrieval augmented\ngeneration (RAG) approaches, we also compile StateCodes, a novel and\ncomprehensive state statute and regulatory corpus of 8.7 GB, enabling much more\nsystematic research into state codes. We then benchmark the performance of\ninformation retrieval and state-of-the-art large LLMs on this data and show\nthat while these models are helpful as preliminary research for code\nsimplification, the overall accuracy is far below the touted promises for LLMs\nas end-to-end pipelines for regulatory simplification.",
        "url": "http://arxiv.org/abs/2508.19365v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19365v1",
        "arxiv_id": "2508.19365v1",
        "authors": [
            "Emaan Hariri",
            "Daniel E. Ho"
        ],
        "submitted": "2025-08-26 18:53:39",
        "source": "arxiv",
        "comment": "10 pages, 3 figures. To appear in ICAIL 2025",
        "score": 8,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The focus is on AI for statutory simplification, code simplification, and regulatory simplification, which is outside your primary area of interest."
    },
    {
        "title": "A Self-Supervised Mixture-of-Experts Framework for Multi-behavior Recommendation",
        "abstract": "In e-commerce, where users face a vast array of possible item choices,\nrecommender systems are vital for helping them discover suitable items they\nmight otherwise overlook. While many recommender systems primarily rely on a\nuser's purchase history, recent multi-behavior recommender systems incorporate\nvarious auxiliary user behaviors, such as item clicks and cart additions, to\nenhance recommendations. Despite their overall performance gains, their\neffectiveness varies considerably between visited items (i.e., those a user has\ninteracted with through auxiliary behaviors) and unvisited items (i.e., those\nwith which the user has had no such interactions). Specifically, our analysis\nreveals that (1) existing multi-behavior recommender systems exhibit a\nsignificant gap in recommendation quality between the two item types (visited\nand unvisited items) and (2) achieving strong performance on both types with a\nsingle model architecture remains challenging. To tackle these issues, we\npropose a novel multi-behavior recommender system, MEMBER. It employs a\nmixture-of-experts framework, with experts designed to recommend the two item\ntypes, respectively. Each expert is trained using a self-supervised method\nspecialized for its design goal. In our comprehensive experiments, we show the\neffectiveness of MEMBER across both item types, achieving up to 65.46%\nperformance gain over the best competitor in terms of Hit Ratio@20.",
        "url": "http://arxiv.org/abs/2508.19507v2",
        "pdf_url": "http://arxiv.org/pdf/2508.19507v2",
        "arxiv_id": "2508.19507v2",
        "authors": [
            "Kyungho Kim",
            "Sunwoo Kim",
            "Geon Lee",
            "Kijung Shin"
        ],
        "submitted": "2025-08-27 01:32:59",
        "source": "arxiv",
        "comment": "CIKM 2025",
        "score": 7,
        "keyword_reasons": [
            "Found 'user behavior' (score: +2)",
            "Found 'click' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on recommender systems, which is related to your interests in Information Retrieval and Search technologies. However, the specific topic of multi-behavior recommendation and the e-commerce domain are not directly aligned with your primary focus on query understanding, ranking models, and user behavior modeling. The paper's emphasis on self-supervised learning and mixture-of-experts frameworks is also not a central match for your research interests."
    },
    {
        "title": "DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis",
        "abstract": "The ability to research and synthesize knowledge is central to human\nexpertise and progress. An emerging class of systems promises these exciting\ncapabilities through generative research synthesis, performing retrieval over\nthe live web and synthesizing discovered sources into long-form, cited\nsummaries. However, evaluating such systems remains an open challenge: existing\nquestion-answering benchmarks focus on short-form factual responses, while\nexpert-curated datasets risk staleness and data contamination. Both fail to\ncapture the complexity and evolving nature of real research synthesis tasks. In\nthis work, we introduce DeepScholar-bench, a live benchmark and holistic,\nautomated evaluation framework designed to evaluate generative research\nsynthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv\npapers and focuses on a real research synthesis task: generating the related\nwork sections of a paper by retrieving, synthesizing, and citing prior\nresearch. Our evaluation framework holistically assesses performance across\nthree key dimensions, knowledge synthesis, retrieval quality, and\nverifiability. We also develop DeepScholar-base, a reference pipeline\nimplemented efficiently using the LOTUS API. Using the DeepScholar-bench\nframework, we perform a systematic evaluation of prior open-source systems,\nsearch AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that\nDeepScholar-base establishes a strong baseline, attaining competitive or higher\nperformance than each other method. We also find that DeepScholar-bench remains\nfar from saturated, with no system exceeding a score of $19\\%$ across all\nmetrics. These results underscore the difficulty of DeepScholar-bench, as well\nas its importance for progress towards AI systems capable of generative\nresearch synthesis. We make our code available at\nhttps://github.com/guestrin-lab/deepscholar-bench.",
        "url": "http://arxiv.org/abs/2508.20033v1",
        "pdf_url": "http://arxiv.org/pdf/2508.20033v1",
        "arxiv_id": "2508.20033v1",
        "authors": [
            "Liana Patel",
            "Negar Arabzadeh",
            "Harshit Gupta",
            "Ankita Sundar",
            "Ion Stoica",
            "Matei Zaharia",
            "Carlos Guestrin"
        ],
        "submitted": "2025-08-27 16:36:34",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on generative research synthesis, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on evaluating systems for generating long-form summaries and citing prior research is also not aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Towards a Holistic and Automated Evaluation Framework for Multi-Level Comprehension of LLMs in Book-Length Contexts",
        "abstract": "We introduce HAMLET, a holistic and automated framework for evaluating the\nlong-context comprehension of large language models (LLMs). HAMLET structures\nsource texts into a three-level key-fact hierarchy at root-, branch-, and\nleaf-levels, and employs query-focused summarization to evaluate how well\nmodels recall and faithfully represent information at each level. To validate\nthe reliability of our fully automated pipeline, we conduct a systematic human\nstudy, showing that our automatic evaluation achieves over 90% agreement with\nexpert human judgments, while reducing the cost by up to 25 times. HAMLET\nreveals that LLMs struggle with fine-grained comprehension, especially at the\nleaf level, and are sensitive to positional effects like the\nlost-in-the-middle. Analytical queries pose greater challenges than narrative\nones, and consistent performance gaps emerge between open-source and\nproprietary models, as well as across model scales. Our code and dataset are\npublicly available at https://github.com/DISL-Lab/HAMLET.",
        "url": "http://arxiv.org/abs/2508.19578v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19578v1",
        "arxiv_id": "2508.19578v1",
        "authors": [
            "Jiaqi Deng",
            "Yuho Lee",
            "Nicole Hee-Yeon Kim",
            "Hyangsuk Min",
            "Taewon Yun",
            "Minjeong Ban",
            "Kim Yul",
            "Hwanjun Song"
        ],
        "submitted": "2025-08-27 05:23:22",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 (Main)",
        "score": 6,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper introduces a framework for evaluating the comprehension of large language models in book-length contexts, which is related to query understanding and ranking models. However, the focus is on evaluating the comprehension of LLMs rather than developing new ranking models or query understanding techniques. The paper's relevance to information retrieval and search technologies is limited, but it may be of interest to researchers exploring the application of LLMs in search and retrieval tasks."
    },
    {
        "title": "Linear-Time Demonstration Selection for In-Context Learning via Gradient Estimation",
        "abstract": "This paper introduces an algorithm to select demonstration examples for\nin-context learning of a query set. Given a set of $n$ examples, how can we\nquickly select $k$ out of $n$ to best serve as the conditioning for downstream\ninference? This problem has broad applications in prompt tuning and\nchain-of-thought reasoning. Since model weights remain fixed during in-context\nlearning, previous work has sought to design methods based on the similarity of\ntoken embeddings. This work proposes a new approach based on gradients of the\noutput taken in the input embedding space. Our approach estimates model outputs\nthrough a first-order approximation using the gradients. Then, we apply this\nestimation to multiple randomly sampled subsets. Finally, we aggregate the\nsampled subset outcomes to form an influence score for each demonstration, and\nselect $k$ most relevant examples. This procedure only requires pre-computing\nmodel outputs and gradients once, resulting in a linear-time algorithm relative\nto model and training set sizes. Extensive experiments across various models\nand datasets validate the efficiency of our approach. We show that the gradient\nestimation procedure yields approximations of full inference with less than\n$\\mathbf{1}\\%$ error across six datasets. This allows us to scale up subset\nselection that would otherwise run full inference by up to\n$\\mathbf{37.7}\\times$ on models with up to $34$ billion parameters, and\noutperform existing selection methods based on input embeddings by\n$\\mathbf{11}\\%$ on average.",
        "url": "http://arxiv.org/abs/2508.19999v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19999v1",
        "arxiv_id": "2508.19999v1",
        "authors": [
            "Ziniu Zhang",
            "Zhenshuo Zhang",
            "Dongyue Li",
            "Lu Wang",
            "Jennifer Dy",
            "Hongyang R. Zhang"
        ],
        "submitted": "2025-08-27 15:59:47",
        "source": "arxiv",
        "comment": "19 pages. To appear in EMNLP'25",
        "score": 5,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to information retrieval, search technologies, or query understanding, ranking models, or user behavior modeling. The focus is on in-context learning, demonstration selection, and gradient estimation, which are not core topics in the user's research interests."
    },
    {
        "title": "HEAL: A Hypothesis-Based Preference-Aware Analysis Framework",
        "abstract": "Preference optimization methods like DPO have achieved remarkable performance\nin LLM alignment. However, the evaluation for these methods relies on a single\nresponse and overlooks other potential outputs, which could also be generated\nin real-world applications within this hypothetical space. To address this\nissue, this paper presents a \\textbf{H}ypothesis-based\nPr\\textbf{E}ference-aware \\textbf{A}na\\textbf{L}ysis Framework (HEAL), a novel\nevaluation paradigm that formulates preference alignment as a re-ranking\nprocess within hypothesis spaces. The framework incorporates two complementary\nmetrics: ranking accuracy for evaluating ordinal consistency and preference\nstrength correlation for assessing continuous alignment. To facilitate this\nframework, we develop UniHypoBench, a unified hypothesis benchmark constructed\nfrom diverse instruction-response pairs. Through extensive experiments based on\nHEAL, with a particular focus on the intrinsic mechanisms of preference\nlearning, we demonstrate that current preference learning methods can\neffectively capture preferences provided by proxy models while simultaneously\nsuppressing negative samples. These findings contribute to preference learning\nresearch through two significant avenues. Theoretically, we introduce\nhypothesis space analysis as an innovative paradigm for understanding\npreference alignment. Practically, HEAL offers researchers robust diagnostic\ntools for refining preference optimization methods, while our empirical results\nidentify promising directions for developing more advanced alignment algorithms\ncapable of comprehensive preference capture.",
        "url": "http://arxiv.org/abs/2508.19922v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19922v1",
        "arxiv_id": "2508.19922v1",
        "authors": [
            "Yifu Huo",
            "Chenglong Wang",
            "Qiren Zhu",
            "Shunjie Xing",
            "Tong Xiao",
            "Chunliang Zhang",
            "Tongran Liu",
            "Jinbo Zhu"
        ],
        "submitted": "2025-08-27 14:30:08",
        "source": "arxiv",
        "comment": "Accepted by EMNLP 2025 Findings",
        "score": 5,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper presents a novel framework for evaluating preference alignment, which is not directly related to information retrieval or search technologies. While it touches on ranking accuracy, the focus is on preference learning and re-ranking within hypothesis spaces, which is not a core area of interest for the user."
    },
    {
        "title": "A Model-agnostic Strategy to Mitigate Embedding Degradation in Personalized Federated Recommendation",
        "abstract": "Centralized recommender systems encounter privacy leakage due to the need to\ncollect user behavior and other private data. Hence, federated recommender\nsystems (FedRec) have become a promising approach with an aggregated global\nmodel on the server. However, this distributed training paradigm suffers from\nembedding degradation caused by suboptimal personalization and dimensional\ncollapse, due to the existence of sparse interactions and heterogeneous\npreferences. To this end, we propose a novel model-agnostic strategy for FedRec\nto strengthen the personalized embedding utility, which is called Personalized\nLocal-Global Collaboration (PLGC). It is the first research in federated\nrecommendation to alleviate the dimensional collapse issue. Particularly, we\nincorporate the frozen global item embedding table into local devices. Based on\na Neural Tangent Kernel strategy that dynamically balances local and global\ninformation, PLGC optimizes personalized representations during forward\ninference, ultimately converging to user-specific preferences. Additionally,\nPLGC carries on a contrastive objective function to reduce embedding redundancy\nby dissolving dependencies between dimensions, thereby improving the backward\nrepresentation learning process. We introduce PLGC as a model-agnostic\npersonalized training strategy for federated recommendations that can be\napplied to existing baselines to alleviate embedding degradation. Extensive\nexperiments on five real-world datasets have demonstrated the effectiveness and\nadaptability of PLGC, which outperforms various baseline algorithms.",
        "url": "http://arxiv.org/abs/2508.19591v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19591v1",
        "arxiv_id": "2508.19591v1",
        "authors": [
            "Jiakui Shen",
            "Yunqi Mi",
            "Guoshuai Zhao",
            "Jialie Shen",
            "Xueming Qian"
        ],
        "submitted": "2025-08-27 06:03:52",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'user behavior' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'personalization' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on federated recommendation systems, which is a related topic to information retrieval, but it does not directly address query understanding, ranking models, or user behavior modeling. The paper's emphasis on personalized embeddings and local-global collaboration is not directly relevant to my research interests, but it may have some indirect connections to my work in NLP and data mining."
    },
    {
        "title": "APS Explorer: Navigating Algorithm Performance Spaces for Informed Dataset Selection",
        "abstract": "Dataset selection is crucial for offline recommender system experiments, as\nmismatched data (e.g., sparse interaction scenarios require datasets with low\nuser-item density) can lead to unreliable results. Yet, 86\\% of ACM RecSys 2024\npapers provide no justification for their dataset choices, with most relying on\njust four datasets: Amazon (38\\%), MovieLens (34\\%), Yelp (15\\%), and Gowalla\n(12\\%). While Algorithm Performance Spaces (APS) were proposed to guide dataset\nselection, their adoption has been limited due to the absence of an intuitive,\ninteractive tool for APS exploration. Therefore, we introduce the APS Explorer,\na web-based visualization tool for interactive APS exploration, enabling\ndata-driven dataset selection. The APS Explorer provides three interactive\nfeatures: (1) an interactive PCA plot showing dataset similarity via\nperformance patterns, (2) a dynamic meta-feature table for dataset comparisons,\nand (3) a specialized visualization for pairwise algorithm performance.",
        "url": "http://arxiv.org/abs/2508.19399v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19399v1",
        "arxiv_id": "2508.19399v1",
        "authors": [
            "Tobias Vente",
            "Michael Heep",
            "Abdullah Abbas",
            "Theodor Sperle",
            "Joeran Beel",
            "Bart Goethals"
        ],
        "submitted": "2025-08-26 19:46:29",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)",
            "Found 'recommend' (score: +1)",
            "Found 'recsys' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses dataset selection for recommender systems, which is somewhat related to information retrieval and search technologies. However, the focus is on recommender systems and dataset selection, which is not a central match for the user's research interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is limited to the e-commerce domain, which is also not a primary focus for the user."
    },
    {
        "title": "Database Entity Recognition with Data Augmentation and Deep Learning",
        "abstract": "This paper addresses the challenge of Database Entity Recognition (DB-ER) in\nNatural Language Queries (NLQ). We present several key contributions to advance\nthis field: (1) a human-annotated benchmark for DB-ER task, derived from\npopular text-to-sql benchmarks, (2) a novel data augmentation procedure that\nleverages automatic annotation of NLQs based on the corresponding SQL queries\nwhich are available in popular text-to-SQL benchmarks, (3) a specialized\nlanguage model based entity recognition model using T5 as a backbone and two\ndown-stream DB-ER tasks: sequence tagging and token classification for\nfine-tuning of backend and performing DB-ER respectively. We compared our DB-ER\ntagger with two state-of-the-art NER taggers, and observed better performance\nin both precision and recall for our model. The ablation evaluation shows that\ndata augmentation boosts precision and recall by over 10%, while fine-tuning of\nthe T5 backbone boosts these metrics by 5-10%.",
        "url": "http://arxiv.org/abs/2508.19372v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19372v1",
        "arxiv_id": "2508.19372v1",
        "authors": [
            "Zikun Fu",
            "Chen Yang",
            "Kourosh Davoudi",
            "Ken Q. Pu"
        ],
        "submitted": "2025-08-26 19:05:40",
        "source": "arxiv",
        "comment": "6 pages, 5 figures. Accepted at IEEE 26th International Conference on\n  Information Reuse and Integration for Data Science (IRI 2025), San Jose,\n  California, August 6-8, 2025",
        "score": 5,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on Database Entity Recognition, which is a topic related to Information Retrieval, but it does not directly address query understanding, ranking models, or user behavior modeling. While it involves Natural Language Processing, the scope is limited to a specific task, and the relevance to the user's broader interests is moderate."
    },
    {
        "title": "KRETA: A Benchmark for Korean Reading and Reasoning in Text-Rich VQA Attuned to Diverse Visual Contexts",
        "abstract": "Understanding and reasoning over text within visual contexts poses a\nsignificant challenge for Vision-Language Models (VLMs), given the complexity\nand diversity of real-world scenarios. To address this challenge, text-rich\nVisual Question Answering (VQA) datasets and benchmarks have emerged for\nhigh-resource languages like English. However, a critical gap persists for\nlow-resource languages such as Korean, where the lack of comprehensive\nbenchmarks hinders robust model evaluation and comparison. To bridge this gap,\nwe introduce KRETA, a benchmark for Korean Reading and rEasoning in Text-rich\nVQA Attuned to diverse visual contexts. KRETA facilitates an in-depth\nevaluation of both visual text understanding and reasoning capabilities, while\nalso supporting a multifaceted assessment across 15 domains and 26 image types.\nAdditionally, we introduce a semi-automated VQA generation pipeline\nspecifically optimized for text-rich settings, leveraging refined stepwise\nimage decomposition and a rigorous seven-metric evaluation protocol to ensure\ndata quality. While KRETA is tailored for Korean, we hope our adaptable and\nextensible pipeline will facilitate the development of similar benchmarks in\nother languages, thereby accelerating multilingual VLM research. The code and\ndataset for KRETA are available at https://github.com/tabtoyou/KRETA.",
        "url": "http://arxiv.org/abs/2508.19944v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19944v1",
        "arxiv_id": "2508.19944v1",
        "authors": [
            "Taebaek Hwang",
            "Minseo Kim",
            "Gisang Lee",
            "Seonuk Kim",
            "Hyunjun Eun"
        ],
        "submitted": "2025-08-27 15:01:02",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)",
            "Found 'korea' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on a specific benchmark for Korean Reading and Reasoning in Text-rich VQA, which is not directly related to Information Retrieval, Search technologies, or query understanding. Although it mentions Vision-Language Models, the context is different from the user's interests in IR and NLP."
    },
    {
        "title": "Bangla-Bayanno: A 52K-Pair Bengali Visual Question Answering Dataset with LLM-Assisted Translation Refinement",
        "abstract": "In this paper, we introduce Bangla-Bayanno, an open-ended Visual Question\nAnswering (VQA) Dataset in Bangla, a widely used, low-resource language in\nmultimodal AI research. The majority of existing datasets are either manually\nannotated with an emphasis on a specific domain, query type, or answer type or\nare constrained by niche answer formats. In order to mitigate human-induced\nerrors and guarantee lucidity, we implemented a multilingual LLM-assisted\ntranslation refinement pipeline. This dataset overcomes the issues of\nlow-quality translations from multilingual sources. The dataset comprises\n52,650 question-answer pairs across 4750+ images. Questions are classified into\nthree distinct answer types: nominal (short descriptive), quantitative\n(numeric), and polar (yes/no). Bangla-Bayanno provides the most comprehensive\nopen-source, high-quality VQA benchmark in Bangla, aiming to advance research\nin low-resource multimodal learning and facilitate the development of more\ninclusive AI systems.",
        "url": "http://arxiv.org/abs/2508.19887v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19887v1",
        "arxiv_id": "2508.19887v1",
        "authors": [
            "Mohammed Rakibul Hasan",
            "Rafi Majid",
            "Ahanaf Tahmid"
        ],
        "submitted": "2025-08-27 13:48:04",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on a specific domain (Bangla language) and task (Visual Question Answering) that is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper mentions multimodal AI research, it does not address query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user."
    },
    {
        "title": "A Scenario-Oriented Survey of Federated Recommender Systems: Techniques, Challenges, and Future Directions",
        "abstract": "Extending recommender systems to federated learning (FL) frameworks to\nprotect the privacy of users or platforms while making recommendations has\nrecently gained widespread attention in academia. This is due to the natural\ncoupling of recommender systems and federated learning architectures: the data\noriginates from distributed clients (mostly mobile devices held by users),\nwhich are highly related to privacy. In a centralized recommender system\n(CenRec), the central server collects clients' data, trains the model, and\nprovides the service. Whereas in federated recommender systems (FedRec), the\nstep of data collecting is omitted, and the step of model training is offloaded\nto each client. The server only aggregates the model and other knowledge, thus\navoiding client privacy leakage. Some surveys of federated recommender systems\ndiscuss and analyze related work from the perspective of designing FL systems.\nHowever, their utility drops by ignoring specific recommendation scenarios'\nunique characteristics and practical challenges. For example, the statistical\nheterogeneity issue in cross-domain FedRec originates from the label drift of\nthe data held by different platforms, which is mainly caused by the recommender\nitself, but not the federated architecture. Therefore, it should focus more on\nsolving specific problems in real-world recommendation scenarios to encourage\nthe deployment FedRec. To this end, this review comprehensively analyzes the\ncoupling of recommender systems and federated learning from the perspective of\nrecommendation researchers and practitioners. We establish a clear link between\nrecommendation scenarios and FL frameworks, systematically analyzing\nscenario-specific approaches, practical challenges, and potential\nopportunities. We aim to develop guidance for the real-world deployment of\nFedRec, bridging the gap between existing research and applications.",
        "url": "http://arxiv.org/abs/2508.19620v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19620v1",
        "arxiv_id": "2508.19620v1",
        "authors": [
            "Yunqi Mi",
            "Jiakui Shen",
            "Guoshuai Zhao",
            "Jialie Shen",
            "Xueming Qian"
        ],
        "submitted": "2025-08-27 06:57:50",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on federated recommender systems, which is a topic related to recommender systems, but not directly aligned with the user's primary research interests in information retrieval, query understanding, and ranking models. The paper's emphasis on privacy and federated learning frameworks is also not a central match for the user's interests."
    },
    {
        "title": "LFD: Layer Fused Decoding to Exploit External Knowledge in Retrieval-Augmented Generation",
        "abstract": "Retrieval-augmented generation (RAG) incorporates external knowledge into\nlarge language models (LLMs), improving their adaptability to downstream tasks\nand enabling information updates. Surprisingly, recent empirical evidence\ndemonstrates that injecting noise into retrieved relevant documents\nparadoxically facilitates exploitation of external knowledge and improves\ngeneration quality. Although counterintuitive and challenging to apply in\npractice, this phenomenon enables granular control and rigorous analysis of how\nLLMs integrate external knowledge. Therefore, in this paper, we intervene on\nnoise injection and establish a layer-specific functional demarcation within\nthe LLM: shallow layers specialize in local context modeling, intermediate\nlayers focus on integrating long-range external factual knowledge, and deeper\nlayers primarily rely on parametric internal knowledge. Building on this\ninsight, we propose Layer Fused Decoding (LFD), a simple decoding strategy that\ndirectly combines representations from an intermediate layer with final-layer\ndecoding outputs to fully exploit the external factual knowledge. To identify\nthe optimal intermediate layer, we introduce an internal knowledge score (IKS)\ncriterion that selects the layer with the lowest IKS value in the latter half\nof layers. Experimental results across multiple benchmarks demonstrate that LFD\nhelps RAG systems more effectively surface retrieved context knowledge with\nminimal cost.",
        "url": "http://arxiv.org/abs/2508.19614v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19614v1",
        "arxiv_id": "2508.19614v1",
        "authors": [
            "Yang Sun",
            "Lixin Zou",
            "Dan Luo",
            "Zhiyong Xie",
            "Long Zhang",
            "Liming Dong",
            "Yunwei Zhao",
            "Xixun Lin",
            "Yanxiong Lu",
            "Chenliang Li"
        ],
        "submitted": "2025-08-27 06:48:46",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on retrieval-augmented generation and external knowledge integration, which is related to information retrieval and natural language processing. However, the specific context of language models and decoding strategies is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Functional Consistency of LLM Code Embeddings: A Self-Evolving Data Synthesis Framework for Benchmarking",
        "abstract": "Embedding models have demonstrated strong performance in tasks like\nclustering, retrieval, and feature extraction while offering computational\nadvantages over generative models and cross-encoders. Benchmarks such as MTEB\nhave shown that text embeddings from large language models (LLMs) capture rich\nsemantic information, but their ability to reflect code-level functional\nsemantics remains unclear. Existing studies largely focus on code clone\ndetection, which emphasizes syntactic similarity and overlooks functional\nunderstanding. In this paper, we focus on the functional consistency of LLM\ncode embeddings, which determines if two code snippets perform the same\nfunction regardless of syntactic differences. We propose a novel data synthesis\nframework called Functionality-Oriented Code Self-Evolution to construct\ndiverse and challenging benchmarks. Specifically, we define code examples\nacross four semantic and syntactic categories and find that existing datasets\npredominantly capture syntactic properties. Our framework generates four unique\nvariations from a single code instance, providing a broader spectrum of code\nexamples that better reflect functional differences. Extensive experiments on\nthree downstream tasks-code clone detection, code functional consistency\nidentification, and code retrieval-demonstrate that embedding models\nsignificantly improve their performance when trained on our evolved datasets.\nThese results highlight the effectiveness and generalization of our data\nsynthesis framework, advancing the functional understanding of code.",
        "url": "http://arxiv.org/abs/2508.19558v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19558v1",
        "arxiv_id": "2508.19558v1",
        "authors": [
            "Zhuohao Li",
            "Wenqing Chen",
            "Jianxing Yu",
            "Zhichao Lu"
        ],
        "submitted": "2025-08-27 04:17:02",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the functional consistency of code embeddings from large language models, which is a topic related to information retrieval and query understanding. However, the focus on code-level functional semantics and code clone detection is not directly aligned with the user's interests in search technologies and user behavior modeling."
    },
    {
        "title": "A Hybrid Recommendation Framework for Enhancing User Engagement in Local News",
        "abstract": "Local news organizations face an urgent need to boost reader engagement amid\ndeclining circulation and competition from global media. Personalized news\nrecommender systems offer a promising solution by tailoring content to user\ninterests. Yet, conventional approaches often emphasize general preferences and\nmay overlook nuanced or eclectic interests in local news.\n  We propose a hybrid news recommender that integrates local and global\npreference models to improve engagement. Building on evidence of the value of\nlocalized models, our method unifies local and non-local predictors in one\nframework. The system adaptively combines recommendations from a local model,\nspecialized in region-specific content, and a global model that captures\nbroader preferences. Ensemble strategies and multiphase training balance the\ntwo.\n  We evaluated the model on two datasets: a synthetic set based on Syracuse\nnewspaper distributions and a Danish dataset (EB-NeRD) labeled for local and\nnon-local content with an LLM. Results show our integrated approach outperforms\nsingle-model baselines in accuracy and coverage, suggesting improved\npersonalization that can drive user engagement.\n  The findings have practical implications for publishers, especially local\noutlets. By leveraging both community-specific and general user interests, the\nhybrid recommender can deliver more relevant content, increasing retention and\nsubscriptions. In sum, this work introduces a new direction for recommender\nsystems, bridging local and global models to revitalize local news consumption\nthrough scalable, personalized user experiences.",
        "url": "http://arxiv.org/abs/2508.19539v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19539v1",
        "arxiv_id": "2508.19539v1",
        "authors": [
            "Payam Pourashraf",
            "Bamshad Mobasher"
        ],
        "submitted": "2025-08-27 03:27:20",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'personalization' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on recommender systems, which is somewhat related to the user's interests in Information Retrieval and Search technologies. However, the specific application to local news and the emphasis on global and local preference models do not align with the user's primary focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Context-Adaptive Synthesis and Compression for Enhanced Retrieval-Augmented Generation in Complex Domains",
        "abstract": "Large Language Models (LLMs) excel in language tasks but are prone to\nhallucinations and outdated knowledge. Retrieval-Augmented Generation (RAG)\nmitigates these by grounding LLMs in external knowledge. However, in complex\ndomains involving multiple, lengthy, or conflicting documents, traditional RAG\nsuffers from information overload and inefficient synthesis, leading to\ninaccurate and untrustworthy answers. To address this, we propose CASC\n(Context-Adaptive Synthesis and Compression), a novel framework that\nintelligently processes retrieved contexts. CASC introduces a Context Analyzer\n& Synthesizer (CAS) module, powered by a fine-tuned smaller LLM, which performs\nkey information extraction, cross-document consistency checking and conflict\nresolution, and question-oriented structured synthesis. This process transforms\nraw, scattered information into a highly condensed, structured, and\nsemantically rich context, significantly reducing the token count and cognitive\nload for the final Reader LLM. We evaluate CASC on SciDocs-QA, a new\nchallenging multi-document question answering dataset designed for complex\nscientific domains with inherent redundancies and conflicts. Our extensive\nexperiments demonstrate that CASC consistently outperforms strong baselines.",
        "url": "http://arxiv.org/abs/2508.19357v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19357v1",
        "arxiv_id": "2508.19357v1",
        "authors": [
            "Peiran Zhou",
            "Junnan Zhu",
            "Yichen Shen",
            "Ruoxi Yu"
        ],
        "submitted": "2025-08-26 18:34:20",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper proposes a framework for retrieval-augmented generation in complex domains, which is related to information retrieval and natural language processing. However, the focus on question answering and scientific domains is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Refining Text Generation for Realistic Conversational Recommendation via Direct Preference Optimization",
        "abstract": "Conversational Recommender Systems (CRSs) aim to elicit user preferences via\nnatural dialogue to provide suitable item recommendations. However, current\nCRSs often deviate from realistic human interactions by rapidly recommending\nitems in brief sessions. This work addresses this gap by leveraging Large\nLanguage Models (LLMs) to generate dialogue summaries from dialogue history and\nitem recommendation information from item description. This approach enables\nthe extraction of both explicit user statements and implicit preferences\ninferred from the dialogue context. We introduce a method using Direct\nPreference Optimization (DPO) to ensure dialogue summary and item\nrecommendation information are rich in information crucial for effective\nrecommendations. Experiments on two public datasets validate our method's\neffectiveness in fostering more natural and realistic conversational\nrecommendation processes.Our implementation is publicly available at:\nhttps://github.com/UEC-InabaLab/Refining-LLM-Text",
        "url": "http://arxiv.org/abs/2508.19918v2",
        "pdf_url": "http://arxiv.org/pdf/2508.19918v2",
        "arxiv_id": "2508.19918v2",
        "authors": [
            "Manato Tajiri",
            "Michimasa Inaba"
        ],
        "submitted": "2025-08-27 14:24:13",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 Main Conference",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on conversational recommender systems, which is a related topic to information retrieval. However, the emphasis on natural language processing and dialogue generation is not directly aligned with my primary research interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat limited to my secondary interests in NLP and data mining."
    },
    {
        "title": "SoK: Large Language Model Copyright Auditing via Fingerprinting",
        "abstract": "The broad capabilities and substantial resources required to train Large\nLanguage Models (LLMs) make them valuable intellectual property, yet they\nremain vulnerable to copyright infringement, such as unauthorized use and model\ntheft. LLM fingerprinting, a non-intrusive technique that extracts and compares\nthe distinctive features from LLMs to identify infringements, offers a\npromising solution to copyright auditing. However, its reliability remains\nuncertain due to the prevalence of diverse model modifications and the lack of\nstandardized evaluation. In this SoK, we present the first comprehensive study\nof LLM fingerprinting. We introduce a unified framework and formal taxonomy\nthat categorizes existing methods into white-box and black-box approaches,\nproviding a structured overview of the state of the art. We further propose\nLeaFBench, the first systematic benchmark for evaluating LLM fingerprinting\nunder realistic deployment scenarios. Built upon mainstream foundation models\nand comprising 149 distinct model instances, LeaFBench integrates 13\nrepresentative post-development techniques, spanning both parameter-altering\nmethods (e.g., fine-tuning, quantization) and parameter-independent mechanisms\n(e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal the\nstrengths and weaknesses of existing methods, thereby outlining future research\ndirections and critical open problems in this emerging field. The code is\navailable at https://github.com/shaoshuo-ss/LeaFBench.",
        "url": "http://arxiv.org/abs/2508.19843v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19843v1",
        "arxiv_id": "2508.19843v1",
        "authors": [
            "Shuo Shao",
            "Yiming Li",
            "Yu He",
            "Hongwei Yao",
            "Wenyuan Yang",
            "Dacheng Tao",
            "Zhan Qin"
        ],
        "submitted": "2025-08-27 12:56:57",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Large Language Model fingerprinting for copyright auditing, which is not related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper's topic is more relevant to Natural Language Processing and data mining, but it does not address the user's specific areas of interest."
    },
    {
        "title": "Building Task Bots with Self-learning for Enhanced Adaptability, Extensibility, and Factuality",
        "abstract": "Developing adaptable, extensible, and accurate task bots with minimal or zero\nhuman intervention is a significant challenge in dialog research. This thesis\nexamines the obstacles and potential solutions for creating such bots, focusing\non innovative techniques that enable bots to learn and adapt autonomously in\nconstantly changing environments.",
        "url": "http://arxiv.org/abs/2508.19689v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19689v1",
        "arxiv_id": "2508.19689v1",
        "authors": [
            "Xiaoying Zhang"
        ],
        "submitted": "2025-08-27 08:52:47",
        "source": "arxiv",
        "comment": "179 pages",
        "score": 3,
        "keyword_reasons": [
            "Found 'search' (score: +1)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be unrelated to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus on task bots and dialog research does not align with the user's primary interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Automatic Question & Answer Generation Using Generative Large Language Model (LLM)",
        "abstract": "\\Abstract{In the realm of education, student evaluation holds equal\nsignificance as imparting knowledge. To be evaluated, students usually need to\ngo through text-based academic assessment methods. Instructors need to make\ndiverse sets of questions that need to be fair for all students to prove their\nadequacy over a particular topic. This can prove to be quite challenging as\nthey may need to manually go through several different lecture materials. Our\nobjective is to make this whole process much easier by implementing Automatic\nQuestion Answer Generation /(AQAG), using fine-tuned generative LLM. For\ntailoring the instructor's preferred question style (MCQ, conceptual, or\nfactual questions), prompt Engineering (PE) is being utilized. In this\nresearch, we propose to leverage unsupervised learning methods in NLP,\nprimarily focusing on the English language. This approach empowers the base\nMeta-Llama 2-7B model to integrate RACE dataset as training data for the\nfine-tuning process. Creating a customized model that will offer efficient\nsolutions for educators, instructors, and individuals engaged in text-based\nevaluations. A reliable and efficient tool for generating questions and answers\ncan free up valuable time and resources, thus streamlining their evaluation\nprocesses.}",
        "url": "http://arxiv.org/abs/2508.19475v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19475v1",
        "arxiv_id": "2508.19475v1",
        "authors": [
            "Md. Alvee Ehsan",
            "A. S. M Mehedi Hasan",
            "Kefaya Benta Shahnoor",
            "Syeda Sumaiya Tasneem"
        ],
        "submitted": "2025-08-26 23:36:13",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on automatic question and answer generation using a generative large language model, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions NLP, the scope is limited to question generation for educational purposes, which is not a primary interest area."
    },
    {
        "title": "One Joke to Rule them All? On the (Im)possibility of Generalizing Humor",
        "abstract": "Humor is a broad and complex form of communication that remains challenging\nfor machines. Despite its broadness, most existing research on computational\nhumor traditionally focused on modeling a specific type of humor. In this work,\nwe wish to understand whether competence on one or more specific humor tasks\nconfers any ability to transfer to novel, unseen types; in other words, is this\nfragmentation inevitable? This question is especially timely as new humor types\ncontinuously emerge in online and social media contexts (e.g., memes,\nanti-humor, AI fails). If Large Language Models (LLMs) are to keep up with this\nevolving landscape, they must be able to generalize across humor types by\ncapturing deeper, transferable mechanisms. To investigate this, we conduct a\nseries of transfer learning experiments across four datasets, representing\ndifferent humor tasks. We train LLMs under varied diversity settings (1-3\ndatasets in training, testing on a novel task). Experiments reveal that models\nare capable of some transfer, and can reach up to 75% accuracy on unseen\ndatasets; training on diverse sources improves transferability (1.88-4.05%)\nwith minimal-to-no drop in in-domain performance. Further analysis suggests\nrelations between humor types, with Dad Jokes surprisingly emerging as the best\nenabler of transfer (but is difficult to transfer to). We release data and\ncode.",
        "url": "http://arxiv.org/abs/2508.19402v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19402v1",
        "arxiv_id": "2508.19402v1",
        "authors": [
            "Mor Turgeman",
            "Chen Shani",
            "Dafna Shahaf"
        ],
        "submitted": "2025-08-26 19:55:40",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is not directly related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. Although it touches on Natural Language Processing, the focus is on humor and language models, which is a distinct area of research."
    },
    {
        "title": "AraHealthQA 2025: The First Shared Task on Arabic Health Question Answering",
        "abstract": "We introduce {AraHealthQA 2025}, the {Comprehensive Arabic Health Question\nAnswering Shared Task}, held in conjunction with {ArabicNLP 2025} (co-located\nwith EMNLP 2025). This shared task addresses the paucity of high-quality Arabic\nmedical QA resources by offering two complementary tracks: {MentalQA}, focusing\non Arabic mental health Q\\&A (e.g., anxiety, depression, stigma reduction), and\n{MedArabiQ}, covering broader medical domains such as internal medicine,\npediatrics, and clinical decision making. Each track comprises multiple\nsubtasks, evaluation datasets, and standardized metrics, facilitating fair\nbenchmarking. The task was structured to promote modeling under realistic,\nmultilingual, and culturally nuanced healthcare contexts. We outline the\ndataset creation, task design and evaluation framework, participation\nstatistics, baseline systems, and summarize the overall outcomes. We conclude\nwith reflections on the performance trends observed and prospects for future\niterations in Arabic health QA.",
        "url": "http://arxiv.org/abs/2508.20047v2",
        "pdf_url": "http://arxiv.org/pdf/2508.20047v2",
        "arxiv_id": "2508.20047v2",
        "authors": [
            "Hassan Alhuzali",
            "Farah Shamout",
            "Muhammad Abdul-Mageed",
            "Chaimae Abouzahir",
            "Mouath Abu-Daoud",
            "Ashwag Alasmari",
            "Walid Al-Eisawi",
            "Renad Al-Monef",
            "Ali Alqahtani",
            "Lama Ayash",
            "Nizar Habash",
            "Leen Kharouf"
        ],
        "submitted": "2025-08-27 16:54:09",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'emnlp' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is not directly related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The focus is on Arabic Health Question Answering, which is a specific domain and not a core area of interest. The paper does not mention any relevance to the user's background in e-commerce or NLP."
    },
    {
        "title": "Forewarned is Forearmed: Pre-Synthesizing Jailbreak-like Instructions to Enhance LLM Safety Guardrail to Potential Attacks",
        "abstract": "Despite advances in improving large language model (LLM) to refuse to answer\nmalicious instructions, widely used LLMs remain vulnerable to jailbreak attacks\nwhere attackers generate instructions with distributions differing from safety\nalignment corpora. New attacks expose LLMs' inability to recognize unseen\nmalicious instructions, highlighting a critical distributional mismatch between\ntraining data and real-world attacks that forces developers into reactive\npatching cycles. To tackle this challenge, we propose IMAGINE, a synthesis\nframework that leverages embedding space distribution analysis to generate\njailbreak-like instructions. This approach effectively fills the distributional\ngap between authentic jailbreak patterns and safety alignment corpora. IMAGINE\nfollows an iterative optimization process that dynamically evolves text\ngeneration distributions across iterations, thereby augmenting the coverage of\nsafety alignment data distributions through synthesized data examples. Based on\nthe safety-aligned corpus enhanced through IMAGINE, our framework demonstrates\nsignificant decreases in attack success rate on Qwen2.5, Llama3.1, and Llama3.2\nwithout compromising their utility.",
        "url": "http://arxiv.org/abs/2508.20038v2",
        "pdf_url": "http://arxiv.org/pdf/2508.20038v2",
        "arxiv_id": "2508.20038v2",
        "authors": [
            "Sheng Liu",
            "Qiang Sheng",
            "Danding Wang",
            "Yang Li",
            "Guang Yang",
            "Juan Cao"
        ],
        "submitted": "2025-08-27 16:44:03",
        "source": "arxiv",
        "comment": "EMNLP 2025 findings",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The topic of Large Language Model (LLM) safety and attacks is not directly related to your areas of focus, and the paper's methods and results do not contribute to your research themes."
    },
    {
        "title": "Cross-Platform E-Commerce Product Categorization and Recategorization: A Multimodal Hierarchical Classification Approach",
        "abstract": "This study addresses critical industrial challenges in e-commerce product\ncategorization, namely platform heterogeneity and the structural limitations of\nexisting taxonomies, by developing and deploying a multimodal hierarchical\nclassification framework. Using a dataset of 271,700 products from 40\ninternational fashion e-commerce platforms, we integrate textual features\n(RoBERTa), visual features (ViT), and joint vision--language representations\n(CLIP). We investigate fusion strategies, including early, late, and\nattention-based fusion within a hierarchical architecture enhanced by dynamic\nmasking to ensure taxonomic consistency. Results show that CLIP embeddings\ncombined via an MLP-based late-fusion strategy achieve the highest hierarchical\nF1 (98.59\\%), outperforming unimodal baselines. To address shallow or\ninconsistent categories, we further introduce a self-supervised ``product\nrecategorization'' pipeline using SimCLR, UMAP, and cascade clustering, which\ndiscovered new, fine-grained categories (e.g., subtypes of ``Shoes'') with\ncluster purities above 86\\%. Cross-platform experiments reveal a\ndeployment-relevant trade-off: complex late-fusion methods maximize accuracy\nwith diverse training data, while simpler early-fusion methods generalize more\neffectively to unseen platforms. Finally, we demonstrate the framework's\nindustrial scalability through deployment in EURWEB's commercial transaction\nintelligence platform via a two-stage inference pipeline, combining a\nlightweight RoBERTa stage with a GPU--accelerated multimodal stage to balance\ncost and accuracy.",
        "url": "http://arxiv.org/abs/2508.20013v1",
        "pdf_url": "http://arxiv.org/pdf/2508.20013v1",
        "arxiv_id": "2508.20013v1",
        "authors": [
            "Lotte Gross",
            "Rebecca Walter",
            "Nicole Zoppi",
            "Adrien Justus",
            "Alessandro Gambetti",
            "Qiwei Han",
            "Maximilian Kaiser"
        ],
        "submitted": "2025-08-27 16:16:12",
        "source": "arxiv",
        "comment": "10 pages, 5 figures, 3 tables",
        "score": 2,
        "keyword_reasons": [
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on e-commerce product categorization and recategorization, which is related to information retrieval, but the emphasis is on categorization rather than query understanding, ranking models, or user behavior modeling. The multimodal hierarchical classification approach is also not directly applicable to my research interests in NLP, data mining, and related topics."
    },
    {
        "title": "Selective Retrieval-Augmentation for Long-Tail Legal Text Classification",
        "abstract": "Legal text classification is a fundamental NLP task in the legal domain.\nBenchmark datasets in this area often exhibit a long-tail label distribution,\nwhere many labels are underrepresented, leading to poor model performance on\nrare classes. This paper proposes Selective Retrieval-Augmentation (SRA) as a\nsolution to this problem. SRA focuses on augmenting samples belonging to\nlow-frequency labels in the training set, preventing the introduction of noise\nfor well-represented classes, and requires no changes to the model\narchitecture. Retrieval is performed only from the training data to ensure\nthere is no potential information leakage, removing the need for external\ncorpora simultaneously. The proposed SRA method is tested on two legal text\nclassification benchmark datasets with long-tail distributions: LEDGAR\n(single-label) and UNFAIR-ToS (multi-label). The results indicate that SRA\nattains higher micro-F1 and macro-F1 scores compared to all current LexGLUE\nbaselines across both datasets, illustrating consistent improvements in\nlong-tail legal text classification.",
        "url": "http://arxiv.org/abs/2508.19997v2",
        "pdf_url": "http://arxiv.org/pdf/2508.19997v2",
        "arxiv_id": "2508.19997v2",
        "authors": [
            "Boheng Mao"
        ],
        "submitted": "2025-08-27 15:56:34",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on legal text classification, which is a specific domain and task, and uses retrieval-augmentation to improve model performance on rare classes. While it touches on NLP, it is not directly related to query understanding, ranking models, or user behavior modeling, which are core areas of interest for you. The paper's relevance to your broader interests in IR, search technologies, and data mining is limited."
    },
    {
        "title": "Self-Supervised Pre-Training with Equilibrium Constraints",
        "abstract": "Self-supervised pre-training using unlabeled data is widely used in machine\nlearning. In this paper, we propose a new self-supervised pre-training approach\nto dealing with heterogeneous data. Instead of mixing all the data and\nminimizing the averaged global loss in the conventional way, we impose\nadditional equilibrium constraints to ensure that the models optimizes each\nsource of heterogeneous data to its local optima after $K$-step gradient\ndescent initialized from the model. We formulate this as a bilevel optimization\nproblem, and use the first-order approximation method to solve the problem. We\ndiscuss its connection to model-agnostic meta learning (MAML). Experiments are\ncarried out on self-supervised pre-training using multi-domain and multilingual\ndatasets, demonstrating that the proposed approach can significantly improve\nthe adaptivity of the self-supervised pre-trained model for the downstream\nsupervised fine-tuning tasks.",
        "url": "http://arxiv.org/abs/2508.19990v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19990v1",
        "arxiv_id": "2508.19990v1",
        "authors": [
            "Xiaodong Cui",
            "A F M Saif",
            "Brian Kingsbury",
            "Tianyi Chen"
        ],
        "submitted": "2025-08-27 15:48:50",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on self-supervised pre-training with equilibrium constraints, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it mentions pre-training and fine-tuning, the context is different from the user's interests in IR and NLP."
    },
    {
        "title": "AgentCoMa: A Compositional Benchmark Mixing Commonsense and Mathematical Reasoning in Real-World Scenarios",
        "abstract": "Large Language Models (LLMs) have achieved high accuracy on complex\ncommonsense and mathematical problems that involve the composition of multiple\nreasoning steps. However, current compositional benchmarks testing these skills\ntend to focus on either commonsense or math reasoning, whereas LLM agents\nsolving real-world tasks would require a combination of both. In this work, we\nintroduce an Agentic Commonsense and Math benchmark (AgentCoMa), where each\ncompositional task requires a commonsense reasoning step and a math reasoning\nstep. We test it on 61 LLMs of different sizes, model families, and training\nstrategies. We find that LLMs can usually solve both steps in isolation, yet\ntheir accuracy drops by ~30% on average when the two are combined. This is a\nsubstantially greater performance gap than the one we observe in prior\ncompositional benchmarks that combine multiple steps of the same reasoning\ntype. In contrast, non-expert human annotators can solve the compositional\nquestions and the individual steps in AgentCoMa with similarly high accuracy.\nFurthermore, we conduct a series of interpretability studies to better\nunderstand the performance gap, examining neuron patterns, attention maps and\nmembership inference. Our work underscores a substantial degree of model\nbrittleness in the context of mixed-type compositional reasoning and offers a\ntest bed for future improvement.",
        "url": "http://arxiv.org/abs/2508.19988v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19988v1",
        "arxiv_id": "2508.19988v1",
        "authors": [
            "Lisa Alazraki",
            "Lihu Chen",
            "Ana Brassard",
            "Joe Stacey",
            "Hossein A. Rahmani",
            "Marek Rei"
        ],
        "submitted": "2025-08-27 15:47:19",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper introduces a benchmark for evaluating the compositional reasoning abilities of Large Language Models (LLMs) in real-world scenarios. While it touches on the idea of combining multiple reasoning steps, it does not specifically focus on query understanding, ranking models, or user behavior modeling, which are core areas of interest for you. The paper's relevance to your research is limited to the broader topic of information retrieval and NLP, but it does not address the specific aspects you are interested in."
    },
    {
        "title": "Diffusion Language Models Know the Answer Before Decoding",
        "abstract": "Diffusion language models (DLMs) have recently emerged as an alternative to\nautoregressive approaches, offering parallel sequence generation and flexible\ntoken orders. However, their inference remains slower than that of\nautoregressive models, primarily due to the cost of bidirectional attention and\nthe large number of refinement steps required for high quality outputs. In this\nwork, we highlight and leverage an overlooked property of DLMs early answer\nconvergence: in many cases, the correct answer can be internally identified by\nhalf steps before the final decoding step, both under semi-autoregressive and\nrandom remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99%\nof instances, respectively, can be decoded correctly using only half of the\nrefinement steps. Building on this observation, we introduce Prophet, a\ntraining-free fast decoding paradigm that enables early commit decoding.\nSpecifically, Prophet dynamically decides whether to continue refinement or to\ngo \"all-in\" (i.e., decode all remaining tokens in one step), using the\nconfidence gap between the top-2 prediction candidates as the criterion. It\nintegrates seamlessly into existing DLM implementations, incurs negligible\noverhead, and requires no additional training. Empirical evaluations of\nLLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the\nnumber of decoding steps by up to 3.4x while preserving high generation\nquality. These results recast DLM decoding as a problem of when to stop\nsampling, and demonstrate that early decode convergence provides a simple yet\npowerful mechanism for accelerating DLM inference, complementary to existing\nspeedup techniques. Our code is publicly available at\nhttps://github.com/pixeli99/Prophet.",
        "url": "http://arxiv.org/abs/2508.19982v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19982v1",
        "arxiv_id": "2508.19982v1",
        "authors": [
            "Pengxiang Li",
            "Yefan Zhou",
            "Dilxat Muhtar",
            "Lu Yin",
            "Shilin Yan",
            "Li Shen",
            "Yi Liang",
            "Soroush Vosoughi",
            "Shiwei Liu"
        ],
        "submitted": "2025-08-27 15:40:25",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses diffusion language models, which is a topic in NLP, but it doesn't directly relate to information retrieval, query understanding, ranking models, or user behavior modeling, which are the user's core research interests. The paper's focus on decoding and inference speedup is also not directly relevant to the user's interests in deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "GLSim: Detecting Object Hallucinations in LVLMs via Global-Local Similarity",
        "abstract": "Object hallucination in large vision-language models presents a significant\nchallenge to their safe deployment in real-world applications. Recent works\nhave proposed object-level hallucination scores to estimate the likelihood of\nobject hallucination; however, these methods typically adopt either a global or\nlocal perspective in isolation, which may limit detection reliability. In this\npaper, we introduce GLSim, a novel training-free object hallucination detection\nframework that leverages complementary global and local embedding similarity\nsignals between image and text modalities, enabling more accurate and reliable\nhallucination detection in diverse scenarios. We comprehensively benchmark\nexisting object hallucination detection methods and demonstrate that GLSim\nachieves superior detection performance, outperforming competitive baselines by\na significant margin.",
        "url": "http://arxiv.org/abs/2508.19972v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19972v1",
        "arxiv_id": "2508.19972v1",
        "authors": [
            "Seongheon Park",
            "Yixuan Li"
        ],
        "submitted": "2025-08-27 15:30:06",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on object hallucination detection in large vision-language models, which is outside the scope of information retrieval and search technologies. Although it involves natural language processing, the topic is not directly related to query understanding, ranking models, or user behavior modeling, which are core areas of interest."
    },
    {
        "title": "Dhati+: Fine-tuned Large Language Models for Arabic Subjectivity Evaluation",
        "abstract": "Despite its significance, Arabic, a linguistically rich and morphologically\ncomplex language, faces the challenge of being under-resourced. The scarcity of\nlarge annotated datasets hampers the development of accurate tools for\nsubjectivity analysis in Arabic. Recent advances in deep learning and\nTransformers have proven highly effective for text classification in English\nand French. This paper proposes a new approach for subjectivity assessment in\nArabic textual data. To address the dearth of specialized annotated datasets,\nwe developed a comprehensive dataset, AraDhati+, by leveraging existing Arabic\ndatasets and collections (ASTD, LABR, HARD, and SANAD). Subsequently, we\nfine-tuned state-of-the-art Arabic language models (XLM-RoBERTa, AraBERT, and\nArabianGPT) on AraDhati+ for effective subjectivity classification.\nFurthermore, we experimented with an ensemble decision approach to harness the\nstrengths of individual models. Our approach achieves a remarkable accuracy of\n97.79\\,\\% for Arabic subjectivity classification. Results demonstrate the\neffectiveness of the proposed approach in addressing the challenges posed by\nlimited resources in Arabic language processing.",
        "url": "http://arxiv.org/abs/2508.19966v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19966v1",
        "arxiv_id": "2508.19966v1",
        "authors": [
            "Slimane Bellaouar",
            "Attia Nehar",
            "Soumia Souffi",
            "Mounia Bouameur"
        ],
        "submitted": "2025-08-27 15:20:12",
        "source": "arxiv",
        "comment": "25 pages, 7 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Arabic subjectivity evaluation using fine-tuned language models, which is not directly related to my research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions language models, the context is specific to Arabic language processing and does not address query understanding, ranking models, or user behavior modeling, which are my primary areas of interest."
    },
    {
        "title": "Logical Reasoning with Outcome Reward Models for Test-Time Scaling",
        "abstract": "Logical reasoning is a critical benchmark for evaluating the capabilities of\nlarge language models (LLMs), as it reflects their ability to derive valid\nconclusions from given premises. While the combination of test-time scaling\nwith dedicated outcome or process reward models has opened up new avenues to\nenhance LLMs performance in complex reasoning tasks, this space is\nunder-explored in deductive logical reasoning. We present a set of Outcome\nReward Models (ORMs) for deductive reasoning. To train the ORMs we mainly\ngenerate data using Chain-of-Thought (CoT) with single and multiple samples.\nAdditionally, we propose a novel tactic to further expand the type of errors\ncovered in the training dataset of the ORM. In particular, we propose an echo\ngeneration technique that leverages LLMs' tendency to reflect incorrect\nassumptions made in prompts to extract additional training data, covering\npreviously unexplored error types. While a standard CoT chain may contain\nerrors likely to be made by the reasoner, the echo strategy deliberately steers\nthe model toward incorrect reasoning. We show that ORMs trained on CoT and\necho-augmented data demonstrate improved performance on the FOLIO, JustLogic,\nand ProverQA datasets across four different LLMs.",
        "url": "http://arxiv.org/abs/2508.19903v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19903v1",
        "arxiv_id": "2508.19903v1",
        "authors": [
            "Ramya Keerthy Thatikonda",
            "Wray Buntine",
            "Ehsan Shareghi"
        ],
        "submitted": "2025-08-27 14:08:43",
        "source": "arxiv",
        "comment": "EMNLP 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on logical reasoning with large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions reward models, the context is different from the user's interests in ranking models and user behavior modeling."
    },
    {
        "title": "Beyond Shallow Heuristics: Leveraging Human Intuition for Curriculum Learning",
        "abstract": "Curriculum learning (CL) aims to improve training by presenting data from\n\"easy\" to \"hard\", yet defining and measuring linguistic difficulty remains an\nopen challenge. We investigate whether human-curated simple language can serve\nas an effective signal for CL. Using the article-level labels from the Simple\nWikipedia corpus, we compare label-based curricula to competence-based\nstrategies relying on shallow heuristics. Our experiments with a BERT-tiny\nmodel show that adding simple data alone yields no clear benefit. However,\nstructuring it via a curriculum -- especially when introduced first --\nconsistently improves perplexity, particularly on simple language. In contrast,\ncompetence-based curricula lead to no consistent gains over random ordering,\nprobably because they fail to effectively separate the two classes. Our results\nsuggest that human intuition about linguistic difficulty can guide CL for\nlanguage model pre-training.",
        "url": "http://arxiv.org/abs/2508.19873v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19873v1",
        "arxiv_id": "2508.19873v1",
        "authors": [
            "Vanessa Toborek",
            "Sebastian Mller",
            "Tim Selbach",
            "Tams Horvth",
            "Christian Bauckhage"
        ],
        "submitted": "2025-08-27 13:35:13",
        "source": "arxiv",
        "comment": "Presented at ICNLSP 2025; to appear in the ACL Anthology; received\n  the Best Short Paper Award",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on curriculum learning, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions language models, the context is pre-training and linguistic difficulty, which is not a key area of interest for the user."
    },
    {
        "title": "TokenVerse++: Towards Flexible Multitask Learning with Dynamic Task Activation",
        "abstract": "Token-based multitasking frameworks like TokenVerse require all training\nutterances to have labels for all tasks, hindering their ability to leverage\npartially annotated datasets and scale effectively. We propose TokenVerse++,\nwhich introduces learnable vectors in the acoustic embedding space of the\nXLSR-Transducer ASR model for dynamic task activation. This core mechanism\nenables training with utterances labeled for only a subset of tasks, a key\nadvantage over TokenVerse. We demonstrate this by successfully integrating a\ndataset with partial labels, specifically for ASR and an additional task,\nlanguage identification, improving overall performance. TokenVerse++ achieves\nresults on par with or exceeding TokenVerse across multiple tasks, establishing\nit as a more practical multitask alternative without sacrificing ASR\nperformance.",
        "url": "http://arxiv.org/abs/2508.19856v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19856v1",
        "arxiv_id": "2508.19856v1",
        "authors": [
            "Shashi Kumar",
            "Srikanth Madikeri",
            "Esa Villatoro-Tello",
            "Sergio Burdisso",
            "Pradeep Rangappa",
            "Andrs Carofilis",
            "Petr Motlicek",
            "Karthik Pandia",
            "Shankar Venkatesan",
            "Kadri Haciolu",
            "Andreas Stolcke"
        ],
        "submitted": "2025-08-27 13:16:31",
        "source": "arxiv",
        "comment": "Accepted to IEEE ASRU 2025. Copyright\\copyright 2025 IEEE",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on multitask learning and dynamic task activation in the context of ASR and language identification, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on acoustic embedding space and XLSR-Transducer ASR model also does not align with the user's background in e-commerce and focus on deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative Analysis",
        "abstract": "Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is\nchallenging due to a lack of high-quality benchmarks, as direct translation of\nEnglish datasets fails to capture crucial linguistic and cultural nuances. To\naddress this, we introduce a suite of five Hindi LLM evaluation datasets:\nIFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created\nusing a methodology that combines from-scratch human annotation with a\ntranslate-and-verify process. We leverage this suite to conduct an extensive\nbenchmarking of open-source LLMs supporting Hindi, providing a detailed\ncomparative analysis of their current capabilities. Our curation process also\nserves as a replicable methodology for developing benchmarks in other\nlow-resource languages.",
        "url": "http://arxiv.org/abs/2508.19831v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19831v1",
        "arxiv_id": "2508.19831v1",
        "authors": [
            "Anusha Kamath",
            "Kanishk Singla",
            "Rakesh Paul",
            "Raviraj Joshi",
            "Utkarsh Vaidya",
            "Sanjay Singh Chauhan",
            "Niranjan Wartikar"
        ],
        "submitted": "2025-08-27 12:35:31",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on benchmarking Hindi Large Language Models, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it involves language models, the context is not relevant to the user's primary research interests."
    },
    {
        "title": "NLKI: A lightweight Natural Language Knowledge Integration Framework for Improving Small VLMs in Commonsense VQA Tasks",
        "abstract": "Commonsense visual-question answering often hinges on knowledge that is\nmissing from the image or the question. Small vision-language models (sVLMs)\nsuch as ViLT, VisualBERT and FLAVA therefore lag behind their larger generative\ncounterparts. To study the effect of careful commonsense knowledge integration\non sVLMs, we present an end-to-end framework (NLKI) that (i) retrieves natural\nlanguage facts, (ii) prompts an LLM to craft natural language explanations, and\n(iii) feeds both signals to sVLMs respectively across two commonsense VQA\ndatasets (CRIC, AOKVQA) and a visual-entailment dataset (e-SNLI-VE). Facts\nretrieved using a fine-tuned ColBERTv2 and an object information-enriched\nprompt yield explanations that largely cut down hallucinations, while lifting\nthe end-to-end answer accuracy by up to 7% (across 3 datasets), making FLAVA\nand other models in NLKI match or exceed medium-sized VLMs such as Qwen-2 VL-2B\nand SmolVLM-2.5B. As these benchmarks contain 10-25% label noise, additional\nfinetuning using noise-robust losses (such as symmetric cross entropy and\ngeneralised cross entropy) adds another 2.5% in CRIC, and 5.5% in AOKVQA. Our\nfindings expose when LLM-based commonsense knowledge beats retrieval from\ncommonsense knowledge bases, how noise-aware training stabilises small models\nin the context of external knowledge augmentation, and why parameter-efficient\ncommonsense reasoning is now within reach for 250M models.",
        "url": "http://arxiv.org/abs/2508.19724v2",
        "pdf_url": "http://arxiv.org/pdf/2508.19724v2",
        "arxiv_id": "2508.19724v2",
        "authors": [
            "Aritra Dutta",
            "Swapnanil Mukherjee",
            "Deepanway Ghosal",
            "Somak Aditya"
        ],
        "submitted": "2025-08-27 09:34:28",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on integrating natural language knowledge into small vision-language models for commonsense visual-question answering, which is not directly related to information retrieval or search technologies. While it involves natural language processing and knowledge retrieval, the primary application is in the context of visual question answering, which is not a core area of interest for the user."
    },
    {
        "title": "Safety Alignment Should Be Made More Than Just A Few Attention Heads",
        "abstract": "Current safety alignment for large language models(LLMs) continues to present\nvulnerabilities, given that adversarial prompting can effectively bypass their\nsafety measures.Our investigation shows that these safety mechanisms\npredominantly depend on a limited subset of attention heads: removing or\nablating these heads can severely compromise model safety. To identify and\nevaluate these safety-critical components, we introduce RDSHA, a targeted\nablation method that leverages the model's refusal direction to pinpoint\nattention heads mostly responsible for safety behaviors. Further analysis shows\nthat existing jailbreak attacks exploit this concentration by selectively\nbypassing or manipulating these critical attention heads. To address this\nissue, we propose AHD, a novel training strategy designed to promote the\ndistributed encoding of safety-related behaviors across numerous attention\nheads. Experimental results demonstrate that AHD successfully distributes\nsafety-related capabilities across more attention heads. Moreover, evaluations\nunder several mainstream jailbreak attacks show that models trained with AHD\nexhibit considerably stronger safety robustness, while maintaining overall\nfunctional utility.",
        "url": "http://arxiv.org/abs/2508.19697v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19697v1",
        "arxiv_id": "2508.19697v1",
        "authors": [
            "Chao Huang",
            "Zefeng Zhang",
            "Juewei Yue",
            "Quangang Li",
            "Chuang Zhang",
            "Tingwen Liu"
        ],
        "submitted": "2025-08-27 09:06:28",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on the safety of large language models, introducing a targeted ablation method to identify critical attention heads. While it touches on attention heads, which is related to ranking models, the paper's primary concern is safety, which is not directly aligned with the user's research interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Survey of Specialized Large Language Model",
        "abstract": "The rapid evolution of specialized large language models (LLMs) has\ntransitioned from simple domain adaptation to sophisticated native\narchitectures, marking a paradigm shift in AI development. This survey\nsystematically examines this progression across healthcare, finance, legal, and\ntechnical domains. Besides the wide use of specialized LLMs, technical\nbreakthrough such as the emergence of domain-native designs beyond fine-tuning,\ngrowing emphasis on parameter efficiency through sparse computation and\nquantization, increasing integration of multimodal capabilities and so on are\napplied to recent LLM agent. Our analysis reveals how these innovations address\nfundamental limitations of general-purpose LLMs in professional applications,\nwith specialized models consistently performance gains on domain-specific\nbenchmarks. The survey further highlights the implications for E-Commerce field\nto fill gaps in the field.",
        "url": "http://arxiv.org/abs/2508.19667v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19667v1",
        "arxiv_id": "2508.19667v1",
        "authors": [
            "Chenghan Yang",
            "Ruiyu Zhao",
            "Yang Liu",
            "Ling Jiang"
        ],
        "submitted": "2025-08-27 08:27:23",
        "source": "arxiv",
        "comment": "9 pages, 1 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper's focus on specialized large language models and their applications in various domains, including healthcare, finance, and technical fields, shows some relevance to the user's interests in NLP and data mining. However, the paper's primary focus on language models and their applications in professional settings, rather than information retrieval and search technologies, limits its alignment with the user's core research themes."
    },
    {
        "title": "A Symbolic Adversarial Learning Framework for Evolving Fake News Generation and Detection",
        "abstract": "Rapid LLM advancements heighten fake news risks by enabling the automatic\ngeneration of increasingly sophisticated misinformation. Previous detection\nmethods, including fine-tuned small models or LLM-based detectors, often\nstruggle with its dynamically evolving nature. In this work, we propose a novel\nframework called the Symbolic Adversarial Learning Framework (SALF), which\nimplements an adversarial training paradigm by an agent symbolic learning\noptimization process, rather than relying on numerical updates. SALF introduces\na paradigm where the generation agent crafts deceptive narratives, and the\ndetection agent uses structured debates to identify logical and factual flaws\nfor detection, and they iteratively refine themselves through such adversarial\ninteractions. Unlike traditional neural updates, we represent agents using\nagent symbolic learning, where learnable weights are defined by agent prompts,\nand simulate back-propagation and gradient descent by operating on natural\nlanguage representations of weights, loss, and gradients. Experiments on two\nmultilingual benchmark datasets demonstrate SALF's effectiveness, showing it\ngenerates sophisticated fake news that degrades state-of-the-art detection\nperformance by up to 53.4% in Chinese and 34.2% in English on average. SALF\nalso refines detectors, improving detection of refined content by up to 7.7%.\nWe hope our work inspires further exploration into more robust, adaptable fake\nnews detection systems.",
        "url": "http://arxiv.org/abs/2508.19633v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19633v1",
        "arxiv_id": "2508.19633v1",
        "authors": [
            "Chong Tian",
            "Qirong Ho",
            "Xiuying Chen"
        ],
        "submitted": "2025-08-27 07:14:17",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 Main Conference",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on fake news generation and detection, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on symbolic learning and adversarial training is also not aligned with the user's background in e-commerce and interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Understanding and Leveraging the Expert Specialization of Context Faithfulness in Mixture-of-Experts LLMs",
        "abstract": "Context faithfulness is essential for reliable reasoning in context-dependent\nscenarios. However, large language models often struggle to ground their\noutputs in the provided context, resulting in irrelevant responses. Inspired by\nthe emergent expert specialization observed in mixture-of-experts\narchitectures, this work investigates whether certain experts exhibit\nspecialization in context utilization, offering a potential pathway toward\ntargeted optimization for improved context faithfulness. To explore this, we\npropose Router Lens, a method that accurately identifies context-faithful\nexperts. Our analysis reveals that these experts progressively amplify\nattention to relevant contextual information, thereby enhancing context\ngrounding. Building on this insight, we introduce Context-faithful Expert\nFine-Tuning (CEFT), a lightweight optimization approach that selectively\nfine-tunes context-faithful experts. Experiments across a wide range of\nbenchmarks and models demonstrate that CEFT matches or surpasses the\nperformance of full fine-tuning while being significantly more efficient.",
        "url": "http://arxiv.org/abs/2508.19594v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19594v1",
        "arxiv_id": "2508.19594v1",
        "authors": [
            "Jun Bai",
            "Minghao Tong",
            "Yang Liu",
            "Zixia Jia",
            "Zilong Zheng"
        ],
        "submitted": "2025-08-27 06:07:13",
        "source": "arxiv",
        "comment": "Accepted by EMNLP 2025 Main",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the concept of context faithfulness in large language models, which is related to query understanding and ranking models in Information Retrieval. However, the focus is on language models and expert specialization, which is not directly aligned with the user's interests in search technologies and user behavior modeling."
    },
    {
        "title": "Language Models Identify Ambiguities and Exploit Loopholes",
        "abstract": "Studying the responses of large language models (LLMs) to loopholes presents\na two-fold opportunity. First, it affords us a lens through which to examine\nambiguity and pragmatics in LLMs, since exploiting a loophole requires\nidentifying ambiguity and performing sophisticated pragmatic reasoning. Second,\nloopholes pose an interesting and novel alignment problem where the model is\npresented with conflicting goals and can exploit ambiguities to its own\nadvantage. To address these questions, we design scenarios where LLMs are given\na goal and an ambiguous user instruction in conflict with the goal, with\nscenarios covering scalar implicature, structural ambiguities, and power\ndynamics. We then measure different models' abilities to exploit loopholes to\nsatisfy their given goals as opposed to the goals of the user. We find that\nboth closed-source and stronger open-source models can identify ambiguities and\nexploit their resulting loopholes, presenting a potential AI safety risk. Our\nanalysis indicates that models which exploit loopholes explicitly identify and\nreason about both ambiguity and conflicting goals.",
        "url": "http://arxiv.org/abs/2508.19546v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19546v1",
        "arxiv_id": "2508.19546v1",
        "authors": [
            "Jio Choi",
            "Mohit Bansal",
            "Elias Stengel-Eskin"
        ],
        "submitted": "2025-08-27 03:40:17",
        "source": "arxiv",
        "comment": "EMNLP 2025 camera-ready; Code:\n  https://github.com/esteng/ambiguous-loophole-exploitation",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores the capabilities of large language models in identifying ambiguities and exploiting loopholes, which is related to query understanding and ranking models in Information Retrieval. However, the focus is more on the language models' abilities and potential risks, rather than directly addressing user behavior modeling or real-time relevance optimization, which are key aspects of your research interests."
    },
    {
        "title": "Geopolitical Parallax: Beyond Walter Lippmann Just After Large Language Models",
        "abstract": "Objectivity in journalism has long been contested, oscillating between ideals\nof neutral, fact-based reporting and the inevitability of subjective framing.\nWith the advent of large language models (LLMs), these tensions are now\nmediated by algorithmic systems whose training data and design choices may\nthemselves embed cultural or ideological biases. This study investigates\ngeopolitical parallax-systematic divergence in news quality and subjectivity\nassessments-by comparing article-level embeddings from Chinese-origin (Qwen,\nBGE, Jina) and Western-origin (Snowflake, Granite) model families. We evaluate\nboth on a human-annotated news quality benchmark spanning fifteen stylistic,\ninformational, and affective dimensions, and on parallel corpora covering\npolitically sensitive topics, including Palestine and reciprocal China-United\nStates coverage. Using logistic regression probes and matched-topic evaluation,\nwe quantify per-metric differences in predicted positive-class probabilities\nbetween model families. Our findings reveal consistent, non-random divergences\naligned with model origin. In Palestine-related coverage, Western models assign\nhigher subjectivity and positive emotion scores, while Chinese models emphasize\nnovelty and descriptiveness. Cross-topic analysis shows asymmetries in\nstructural quality metrics Chinese-on-US scoring notably lower in fluency,\nconciseness, technicality, and overall quality-contrasted by higher negative\nemotion scores. These patterns align with media bias theory and our distinction\nbetween semantic, emotional, and relational subjectivity, and extend LLM bias\nliterature by showing that geopolitical framing effects persist in downstream\nquality assessment tasks. We conclude that LLM-based media evaluation pipelines\nrequire cultural calibration to avoid conflating content differences with\nmodel-induced bias.",
        "url": "http://arxiv.org/abs/2508.19492v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19492v1",
        "arxiv_id": "2508.19492v1",
        "authors": [
            "Mehmet Can Yavuz",
            "Humza Gohar Kabir",
            "Aylin zkan"
        ],
        "submitted": "2025-08-27 00:39:59",
        "source": "arxiv",
        "comment": "7 pages, 4 figures, 7 tables",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on the geopolitical biases in large language models and their impact on news quality and subjectivity assessments, which is outside the scope of your research areas."
    },
    {
        "title": "LongReasonArena: A Long Reasoning Benchmark for Large Language Models",
        "abstract": "Existing long-context benchmarks for Large Language Models (LLMs) focus on\nevaluating comprehension of long inputs, while overlooking the evaluation of\nlong reasoning abilities. To address this gap, we introduce LongReasonArena, a\nbenchmark specifically designed to assess the long reasoning capabilities of\nLLMs. Our tasks require models to solve problems by executing multi-step\nalgorithms that reflect key aspects of long reasoning, such as retrieval and\nbacktracking. By controlling the inputs, the required reasoning length can be\narbitrarily scaled, reaching up to 1 million tokens of reasoning for the most\nchallenging tasks. Extensive evaluation results demonstrate that\nLongReasonArena presents a significant challenge for both open-source and\nproprietary LLMs. For instance, Deepseek-R1 achieves only 7.5% accuracy on our\ntask. Further analysis also reveals that the accuracy exhibits a linear decline\nwith respect to the logarithm of the expected number of reasoning steps. Our\ncode and data is available at\nhttps://github.com/LongReasonArena/LongReasonArena.",
        "url": "http://arxiv.org/abs/2508.19363v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19363v1",
        "arxiv_id": "2508.19363v1",
        "authors": [
            "Jiayu Ding",
            "Shuming Ma",
            "Lei Cui",
            "Nanning Zheng",
            "Furu Wei"
        ],
        "submitted": "2025-08-26 18:41:53",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on evaluating long reasoning capabilities of Large Language Models, which is not directly related to my research interests in Information Retrieval, Search technologies, and query understanding. While it touches on topics like retrieval, it is not relevant to my primary focus on real-time relevance optimization and deep semantic understanding."
    },
    {
        "title": "Reflective Agreement: Combining Self-Mixture of Agents with a Sequence Tagger for Robust Event Extraction",
        "abstract": "Event Extraction (EE) involves automatically identifying and extracting\nstructured information about events from unstructured text, including triggers,\nevent types, and arguments. Traditional discriminative models demonstrate high\nprecision but often exhibit limited recall, particularly for nuanced or\ninfrequent events. Conversely, generative approaches leveraging Large Language\nModels (LLMs) provide higher semantic flexibility and recall but suffer from\nhallucinations and inconsistent predictions. To address these challenges, we\npropose Agreement-based Reflective Inference System (ARIS), a hybrid approach\ncombining a Self Mixture of Agents with a discriminative sequence tagger. ARIS\nexplicitly leverages structured model consensus, confidence-based filtering,\nand an LLM reflective inference module to reliably resolve ambiguities and\nenhance overall event prediction quality. We further investigate decomposed\ninstruction fine-tuning for enhanced LLM event extraction understanding.\nExperiments demonstrate our approach outperforms existing state-of-the-art\nevent extraction methods across three benchmark datasets.",
        "url": "http://arxiv.org/abs/2508.19359v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19359v1",
        "arxiv_id": "2508.19359v1",
        "authors": [
            "Fatemeh Haji",
            "Mazal Bethany",
            "Cho-Yu Jason Chiang",
            "Anthony Rios",
            "Peyman Najafirad"
        ],
        "submitted": "2025-08-26 18:36:23",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Event Extraction, a topic outside of the user's primary research interests in Information Retrieval and Search technologies. Although it mentions Large Language Models, which are related to NLP, the application and methodology are not directly relevant to the user's areas of focus."
    },
    {
        "title": "Your AI Bosses Are Still Prejudiced: The Emergence of Stereotypes in LLM-Based Multi-Agent Systems",
        "abstract": "While stereotypes are well-documented in human social interactions, AI\nsystems are often presumed to be less susceptible to such biases. Previous\nstudies have focused on biases inherited from training data, but whether\nstereotypes can emerge spontaneously in AI agent interactions merits further\nexploration. Through a novel experimental framework simulating workplace\ninteractions with neutral initial conditions, we investigate the emergence and\nevolution of stereotypes in LLM-based multi-agent systems. Our findings reveal\nthat (1) LLM-Based AI agents develop stereotype-driven biases in their\ninteractions despite beginning without predefined biases; (2) stereotype\neffects intensify with increased interaction rounds and decision-making power,\nparticularly after introducing hierarchical structures; (3) these systems\nexhibit group effects analogous to human social behavior, including halo\neffects, confirmation bias, and role congruity; and (4) these stereotype\npatterns manifest consistently across different LLM architectures. Through\ncomprehensive quantitative analysis, these findings suggest that stereotype\nformation in AI systems may arise as an emergent property of multi-agent\ninteractions, rather than merely from training data biases. Our work\nunderscores the need for future research to explore the underlying mechanisms\nof this phenomenon and develop strategies to mitigate its ethical impacts.",
        "url": "http://arxiv.org/abs/2508.19919v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19919v1",
        "arxiv_id": "2508.19919v1",
        "authors": [
            "Jingyu Guo",
            "Yingying Xu"
        ],
        "submitted": "2025-08-27 14:25:43",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The paper focuses on the emergence of stereotypes in AI agent interactions, which is a topic in Artificial Intelligence and Machine Learning, but not directly related to your areas of interest."
    },
    {
        "title": "Scalable and consistent few-shot classification of survey responses using text embeddings",
        "abstract": "Qualitative analysis of open-ended survey responses is a commonly-used\nresearch method in the social sciences, but traditional coding approaches are\noften time-consuming and prone to inconsistency. Existing solutions from\nNatural Language Processing such as supervised classifiers, topic modeling\ntechniques, and generative large language models have limited applicability in\nqualitative analysis, since they demand extensive labeled data, disrupt\nestablished qualitative workflows, and/or yield variable results. In this\npaper, we introduce a text embedding-based classification framework that\nrequires only a handful of examples per category and fits well with standard\nqualitative workflows. When benchmarked against human analysis of a conceptual\nphysics survey consisting of 2899 open-ended responses, our framework achieves\na Cohen's Kappa ranging from 0.74 to 0.83 as compared to expert human coders in\nan exhaustive coding scheme. We further show how performance of this framework\nimproves with fine-tuning of the text embedding model, and how the method can\nbe used to audit previously-analyzed datasets. These findings demonstrate that\ntext embedding-assisted coding can flexibly scale to thousands of responses\nwithout sacrificing interpretability, opening avenues for deductive qualitative\nanalysis at scale.",
        "url": "http://arxiv.org/abs/2508.19836v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19836v1",
        "arxiv_id": "2508.19836v1",
        "authors": [
            "Jonas Timmann Mjaaland",
            "Markus Fleten Kreutzer",
            "Halvor Tyseng",
            "Rebeckah K. Fussell",
            "Gina Passante",
            "N. G. Holmes",
            "Anders Malthe-Srenssen",
            "Tor Ole B. Odden"
        ],
        "submitted": "2025-08-27 12:45:25",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on text embeddings and few-shot classification for survey responses, which is related to Natural Language Processing (NLP) and data mining. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval (IR). The paper's relevance is somewhat limited due to its focus on qualitative analysis and survey responses, which is not directly applicable to search technologies."
    }
]