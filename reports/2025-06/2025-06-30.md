# Daily Papers Report - 2025-06-30

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Impact of Shallow vs. Deep Relevance Judgments on BERT-based Reranking Models

- **LLM Score**: 8
- **Keyword Score**: 16
- **Authors**: Gabriel Iturra-Bocaz, Danny Vo, Petra Galuscakova
- **URL**: <http://arxiv.org/abs/2506.23191v1>
- **Submitted**: 2025-06-29 11:30:50
- **Comment**: Accepted at ICTIR'25
- **Topic Keywords**: queries, ranking, rerank, relevance, retrieval, rank, search
- **Reason**: The paper is highly relevant to Information Retrieval, specifically neural IR, and explores the impact of relevance judgments on BERT-based reranking models. The use of MS MARCO and LongEval collections adds to the paper's relevance, as these datasets are commonly used in IR research. While the focus is on reranking models, the paper's emphasis on deep semantic understanding and real-time relevance optimization aligns with the user's interests.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Impact of shallow versus deep relevance judgments on BERT-based reranking models in neural Information Retrieval
- **Aim**: Investigate how shallow-judged and deep-judged datasets affect the performance of BERT-based reranking models
- **Rationale**: Shallow-judged datasets have numerous queries with few relevance judgments, while deep-judged datasets have fewer queries with extensive relevance judgments
- **Ground**: Three datasets: two MS MARCO collections and a LongEval collection, all with shallow and deep judgments
- **Experiment**: Fine-tune BERT-based reranking models on shallow and deep datasets, evaluate performance using metrics such as MAP, NDCG, and MRR, and explore the effect of negative sampling and number of training instances
- **Takeaway**: Shallow-judged datasets are more effective for training BERT-based reranking models due to broader topic coverage, and negative samples can mitigate the disadvantage of deep-judged datasets

#### Abstract
> This paper investigates the impact of shallow versus deep relevance judgments
on the performance of BERT-based reranking models in neural Information
Retrieval. Shallow-judged datasets, characterized by numerous queries each with
few relevance judgments, and deep-judged datasets, involving fewer queries with
extensive relevance judgments, are compared. The research assesses how these
datasets affect the performance of BERT-based reranking models trained on them.
The experiments are run on the MS MARCO and LongEval collections. Results
indicate that shallow-judged datasets generally enhance generalization and
effectiveness of reranking models due to a broader range of available contexts.
The disadvantage of the deep-judged datasets might be mitigated by a larger
number of negative training examples.

---

### 2. Learning to Rank with Variable Result Presentation Lengths

- **LLM Score**: 7
- **Keyword Score**: 13
- **Authors**: Norman Knyazev, Harrie Oosterhuis
- **URL**: <http://arxiv.org/abs/2506.23319v1>
- **Submitted**: 2025-06-29 16:28:17
- **Comment**: SIGIR 2025
- **Topic Keywords**: ranking, learning to rank, ltr, relevance, rank
- **Reason**: The paper explores Learning to Rank (LTR) with variable result presentation lengths, which is a novel and relevant topic in Information Retrieval. The focus on presentation length and its impact on user behavior modeling is also aligned with my interests. However, the paper's primary focus is on ranking and presentation, rather than query understanding, ranking models, or user behavior modeling, which are my core research themes.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Learning to Rank with Variable Result Presentation Lengths (LTR-VPL)
- **Aim**: Jointly optimize document ordering and lengths in a fixed vertical space ranking
- **Rationale**: Balancing document ordering and presentation lengths is crucial, as increasing the presentation size of an individual document can push other documents down the search engine result page (SERP)
- **Ground**: Propose a new family of Plackett-Luce list-wise gradient estimation methods, called VLPL, which represents each choice as a document placement paired with a specific length
- **Experiment**: Evaluate the performance of VLPL in a setting where the presentation length of search results varies, using two datasets, Yahoo! Webscope and MSLR-WEB30k
- **Takeaway**: VLPL models demonstrate a large and statistically significant improvement over all models across every setting, highlighting the need for ranking optimization methods that take into account the presentation length of individual documents

#### Abstract
> Learning to Rank (LTR) methods generally assume that each document in a top-K
ranking is presented in an equal format. However, previous work has shown that
users' perceptions of relevance can be changed by varying presentations, i.e.,
allocating more vertical space to some documents to provide additional textual
or image information. Furthermore, presentation length can also redirect
attention, as users are more likely to notice longer presentations when
scrolling through results. Deciding on the document presentation lengths in a
fixed vertical space ranking is an important problem that has not been
addressed by existing LTR methods.
  We address this gap by introducing the variable presentation length ranking
task, where simultaneously the ordering of documents and their presentation
length is decided. Despite being a generalization of standard ranking, we show
that this setting brings significant new challenges: Firstly, the probability
ranking principle no longer applies to this setting, and secondly, the problem
cannot be divided into separate ordering and length selection tasks.
  We therefore propose VLPL - a new family of Plackett-Luce list-wise gradient
estimation methods for the joint optimization of document ordering and lengths.
Our semi-synthetic experiments show that VLPL can effectively balance the
expected exposure and attractiveness of all documents, achieving the best
performance across different ranking settings. Furthermore, we observe that
even simple length-aware methods can achieve significant performance
improvements over fixed-length models. Altogether, our theoretical and
empirical results highlight the importance and difficulties of combining
document presentation with LTR.

---

### 3. Benchmarking Deep Search over Heterogeneous Enterprise Data

- **LLM Score**: 7
- **Keyword Score**: 8
- **Authors**: Prafulla Kumar Choubey, Xiangyu Peng, Shilpa Bhagavath, Kung-Hsiang Huang, Caiming Xiong, Chien-Sheng Wu
- **URL**: <http://arxiv.org/abs/2506.23139v1>
- **Submitted**: 2025-06-29 08:34:59
- **Topic Keywords**: queries, rag, retrieval, search
- **Reason**: The paper presents a benchmark for evaluating Deep Search, which involves multi-hop reasoning over diverse sources. While it's not directly focused on query understanding, ranking models, or user behavior modeling, it's related to information retrieval and search technologies. The paper's emphasis on deep semantic understanding and real-time relevance optimization aligns with the user's interests, but the topic is somewhat specialized and not directly applicable to the user's primary focus.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: HERB: A New Benchmark for Evaluating Deep Search in Enterprise Settings
- **Aim**: To introduce HERB, a realistic and challenging benchmark for evaluating Deep Search and retrieval-augmented generation (RAG) systems in enterprise settings
- **Rationale**: Existing multi-hop RAG benchmarks are limited in reflecting real-world use cases, and HERB addresses this gap by simulating business workflows and generating interconnected content with realistic noise and multi-hop questions
- **Ground**: HERB includes 39,190 enterprise artifacts and 815 answerable queries, as well as 699 unanswerable queries, enabling fine-grained evaluation of long-context Large Language Models (LLMs) and RAG systems
- **Experiment**: The authors evaluate various RAG systems and LLM configurations on HERB, finding that standard RAG methods struggle with the complexity of enterprise data, and that even the best-performing models achieve only modest average performance
- **Takeaway**: The paper highlights the limitations of current LLMs and RAG systems in realistic enterprise settings and emphasizes the need for improved retrieval and reasoning methods to support complex enterprise tasks

#### Abstract
> We present a new benchmark for evaluating Deep Search--a realistic and
complex form of retrieval-augmented generation (RAG) that requires
source-aware, multi-hop reasoning over diverse, sparsed, but related sources.
These include documents, meeting transcripts, Slack messages, GitHub, and URLs,
which vary in structure and often contain human-to-human interactions. We build
it using a synthetic data pipeline that simulates business workflows across
product planning, development, and support stages, generating interconnected
content with realistic noise and multi-hop questions with guaranteed
ground-truth answers. We release our benchmark with both answerable and
unanswerable queries, and retrieval pool of 39,190 enterprise artifacts,
enabling fine-grained evaluation of long-context LLM and RAG systems. Our
experiments reveal that even the best-performing agentic RAG methods achieve an
average performance score of 32.96 on our benchmark. With further analysis, we
highlight retrieval as the main bottleneck: existing methods struggle to
conduct deep searches and retrieve all necessary evidence. Consequently, they
often reason over partial context, leading to significant performance
degradation.

---

### 4. ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models

- **LLM Score**: 7
- **Keyword Score**: 8
- **Authors**: Jianxin Yan, Wangze Ni, Lei Chen, Xuemin Lin, Peng Cheng, Zhan Qin, Kui Ren
- **URL**: <http://arxiv.org/abs/2506.22791v1>
- **Submitted**: 2025-06-28 07:25:12
- **Topic Keywords**: query, queries, retrieval
- **Reason**: The paper explores context-aware semantic caching for multi-turn queries in large language models, which is related to query understanding and ranking models in Information Retrieval. Although it's not directly focused on user behavior modeling or click models, it's still relevant to the broader area of search technologies. The paper's emphasis on real-time relevance optimization and efficiency also aligns with the user's interests in Information Retrieval.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: ContextCache: A Context-Aware Semantic Caching System for Large Language Models
- **Aim**: Optimize Large Language Model (LLM) responses in multi-turn dialogues by developing a context-aware semantic caching system
- **Rationale**: Traditional caching approaches fail to consider dynamic conversational context, leading to incorrect cache hits and high computational costs
- **Ground**: Combining efficient vector-based retrieval with precise attention-based contextual matching to enable retrieval of previously computed responses for semantically similar queries
- **Experiment**: Experimental evaluation shows that ContextCache outperforms existing methods, such as GPTCache, improving precision and recall by 12.9% and 15.7%, respectively, and reducing response latency by approximately 10 times
- **Takeaway**: ContextCache is an efficient solution for reducing computational costs and response latency in LLM applications, while maintaining high semantic matching precision in multi-turn dialogues

#### Abstract
> Semantic caching significantly reduces computational costs and improves
efficiency by storing and reusing large language model (LLM) responses.
However, existing systems rely primarily on matching individual queries,
lacking awareness of multi-turn dialogue contexts, which leads to incorrect
cache hits when similar queries appear in different conversational settings.
This demonstration introduces ContextCache, a context-aware semantic caching
system for multi-turn dialogues. ContextCache employs a two-stage retrieval
architecture that first executes vector-based retrieval on the current query to
identify potential matches and then integrates current and historical dialogue
representations through self-attention mechanisms for precise contextual
matching. Evaluation of real-world conversations shows that ContextCache
improves precision and recall compared to existing methods. Additionally,
cached responses exhibit approximately 10 times lower latency than direct LLM
invocation, enabling significant computational cost reductions for LLM
conversational applications.

---

### 5. Synergizing Implicit and Explicit User Interests: A Multi-Embedding Retrieval Framework at Pinterest

- **LLM Score**: 7
- **Keyword Score**: 7
- **Authors**: Zhibo Fan, Hongtao Lin, Haoyu Chen, Bowen Deng, Hedi Xia, Yuke Yan, James Li
- **URL**: <http://arxiv.org/abs/2506.23060v1>
- **Submitted**: 2025-06-29 02:14:21
- **Comment**: KDD 2025
- **Topic Keywords**: ranking, retrieval, recommend, rank
- **Reason**: The paper proposes a multi-embedding retrieval framework that synergizes implicit and explicit user interests, which is relevant to your interests in query understanding, ranking models, and user behavior modeling. The framework's focus on enhancing user interest representation and generating multiple user embeddings conditioned on both implicit and explicit user interests aligns with your research themes. However, the paper's primary focus is on recommender systems, which is not your primary area of interest, but still relevant to your broader interests in information retrieval and NLP.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Multi-Embedding Retrieval Framework for User Interest Modeling
- **Aim**: To propose a novel multi-embedding retrieval framework that combines implicit and explicit user interests to improve user interest coverage and online engagement
- **Rationale**: Existing methods in embedding-based multi-interest retrieval have limitations, and the proposed framework builds upon and improves upon these methods
- **Ground**: The framework consists of two models: an implicit interest conditioned model using a Differentiable Clustering Module (DCM) and a conditional retrieval (CR) model to retrieve candidate items related to explicit user interests
- **Experiment**: The experiment setup includes a dataset of 6 billion engagement records from 160 million users, and the evaluation metrics include hit rate (HR) at various ranking thresholds and relative improvements on repin volume and adopted pincepts
- **Takeaway**: The proposed framework outperforms other representative multi-interest retrieval methods, and online experiments demonstrate its effectiveness in improving feed diversity and user engagements

#### Abstract
> Industrial recommendation systems are typically composed of multiple stages,
including retrieval, ranking, and blending. The retrieval stage plays a
critical role in generating a high-recall set of candidate items that covers a
wide range of diverse user interests. Effectively covering the diverse and
long-tail user interests within this stage poses a significant challenge:
traditional two-tower models struggle in this regard due to limited user-item
feature interaction and often bias towards top use cases. To address these
issues, we propose a novel multi-embedding retrieval framework designed to
enhance user interest representation by generating multiple user embeddings
conditioned on both implicit and explicit user interests. Implicit interests
are captured from user history through a Differentiable Clustering Module
(DCM), whereas explicit interests, such as topics that the user has followed,
are modeled via Conditional Retrieval (CR). These methodologies represent a
form of conditioned user representation learning that involves condition
representation construction and associating the target item with the relevant
conditions. Synergizing implicit and explicit user interests serves as a
complementary approach to achieve more effective and comprehensive candidate
retrieval as they benefit on different user segments and extract conditions
from different but supplementary sources. Extensive experiments and A/B testing
reveal significant improvements in user engagements and feed diversity metrics.
Our proposed framework has been successfully deployed on Pinterest home feed.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. LLM-Assisted Question-Answering on Technical Documents Using Structured Data-Aware Retrieval Augmented Generation

- **LLM Score**: 6
- **Keyword Score**: 11
- **Authors**: Shadman Sobhan, Mohammad Ariful Haque
- **URL**: <http://arxiv.org/abs/2506.23136v1>
- **Submitted**: 2025-06-29 08:22:03
- **Comment**: 29 Pages, 11 Tables
- **Topic Keywords**: rerank, rag, retrieval augmented generation, retrieval, rank, search
- **Reason**: The paper proposes a Retrieval-Augmented Generation pipeline for question-answering on technical documents with structured data, which is somewhat related to my research interests in Information Retrieval and Search technologies. However, the focus on question-answering and technical documents is not directly aligned with my primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large Language Models (LLMs) are capable of natural language understanding
and generation. But they face challenges such as hallucination and outdated
knowledge. Fine-tuning is one possible solution, but it is resource-intensive
and must be repeated with every data update. Retrieval-Augmented Generation
(RAG) offers an efficient solution by allowing LLMs to access external
knowledge sources. However, traditional RAG pipelines struggle with retrieving
information from complex technical documents with structured data such as
tables and images. In this work, we propose a RAG pipeline, capable of handling
tables and images in documents, for technical documents that support both
scanned and searchable formats. Its retrieval process combines vector
similarity search with a fine-tuned reranker based on Gemma-2-9b-it. The
reranker is trained using RAFT (Retrieval-Augmented Fine-Tuning) on a custom
dataset designed to improve context identification for question answering. Our
evaluation demonstrates that the proposed pipeline achieves a high faithfulness
score of 94% (RAGas) and 96% (DeepEval), and an answer relevancy score of 87%
(RAGas) and 93% (DeepEval). Comparative analysis demonstrates that the proposed
architecture is superior to general RAG pipelines in terms of table-based
questions and handling questions outside context.

### 7. KiseKloset: Comprehensive System For Outfit Retrieval, Recommendation, And Try-On

- **LLM Score**: 6
- **Keyword Score**: 9
- **Authors**: Thanh-Tung Phan-Nguyen, Khoi-Nguyen Nguyen-Ngoc, Tam V. Nguyen, Minh-Triet Tran, Trung-Nghia Le
- **URL**: <http://arxiv.org/abs/2506.23471v1>
- **Submitted**: 2025-06-30 02:25:39
- **Topic Keywords**: rag, retrieval, recommend, shopping, commerce, e-commerce, search
- **Reason**: The paper proposes a comprehensive system for outfit retrieval, recommendation, and try-on, which is related to search technologies and user behavior modeling. However, the focus is on the fashion e-commerce domain, which is not the primary area of interest. The paper does not explicitly address query understanding, ranking models, or deep semantic understanding, which are key aspects of the user's research interests.

#### Abstract
> The global fashion e-commerce industry has become integral to people's daily
lives, leveraging technological advancements to offer personalized shopping
experiences, primarily through recommendation systems that enhance customer
engagement through personalized suggestions. To improve customers' experience
in online shopping, we propose a novel comprehensive KiseKloset system for
outfit retrieval, recommendation, and try-on. We explore two approaches for
outfit retrieval: similar item retrieval and text feedback-guided item
retrieval. Notably, we introduce a novel transformer architecture designed to
recommend complementary items from diverse categories. Furthermore, we enhance
the overall performance of the search pipeline by integrating approximate
algorithms to optimize the search process. Additionally, addressing the crucial
needs of online shoppers, we employ a lightweight yet efficient virtual try-on
framework capable of real-time operation, memory efficiency, and maintaining
realistic outputs compared to its predecessors. This virtual try-on module
empowers users to visualize specific garments on themselves, enhancing the
customers' experience and reducing costs associated with damaged items for
retailers. We deployed our end-to-end system for online users to test and
provide feedback, enabling us to measure their satisfaction levels. The results
of our user study revealed that 84% of participants found our comprehensive
system highly useful, significantly improving their online shopping experience.

### 8. Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified Natural Language Queries

- **LLM Score**: 6
- **Keyword Score**: 9
- **Authors**: Zhengren Wang, Bozhou Li, Dongwen Yao, Wentao Zhang
- **URL**: <http://arxiv.org/abs/2506.23071v1>
- **Submitted**: 2025-06-29 03:17:42
- **Comment**: Work in progess
- **Topic Keywords**: semantic search, queries, retrieval, search
- **Reason**: The paper explores the intersection of natural language processing and information retrieval, specifically in the context of text-to-SQL and vector search. While it doesn't directly address query understanding, ranking models, or user behavior modeling, it does contribute to the development of more versatile and intuitive database interfaces, which may be of interest to researchers in the broader field of information retrieval.

#### Abstract
> While Text-to-SQL enables natural language interaction with structured
databases, its effectiveness diminishes with unstructured data or ambiguous
queries due to rigid syntax and limited expressiveness. Concurrently, vector
search has emerged as a powerful paradigm for semantic retrieval, particularly
for unstructured data. However, existing VectorSQL implementations still rely
heavily on manual crafting and lack tailored evaluation frameworks, leaving a
significant gap between theoretical potential and practical deployment. To
bridge these complementary paradigms, we introduces Text2VectorSQL, a novel
framework unifying Text-to-SQL and vector search to overcome expressiveness
constraints and support more diverse and holistical natural language queries.
Specifically, Text2VectorSQL enables semantic filtering, multi-modal matching,
and retrieval acceleration. For evaluation, we build vector index on
appropriate columns, extend user queries with semantic search, and annotate
ground truths via an automatic pipeline with expert review. Furthermore, we
develop dedicated Text2VectorSQL models with synthetic data, demonstrating
significant performance improvements over baseline methods. Our work
establishes the foundation for the Text2VectorSQL task, paving the way for more
versatile and intuitive database interfaces. The repository will be publicly
available at https://github.com/Open-DataFlow/Text2VectorSQL.

### 9. Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent

- **LLM Score**: 6
- **Keyword Score**: 4
- **Authors**: Haocheng Yu, Yaxiong Wu, Hao Wang, Wei Guo, Yong Liu, Yawen Li, Yuyang Ye, Junping Du, Enhong Chen
- **URL**: <http://arxiv.org/abs/2506.23485v1>
- **Submitted**: 2025-06-30 03:15:50
- **Topic Keywords**: queries, recommend
- **Reason**: The paper explores interactive recommender systems, which is related to information retrieval and search technologies. The use of large language models and planning capabilities is also relevant to query understanding and ranking models. However, the focus on recommender systems and user simulation schemes is not directly aligned with the user's primary interest in information retrieval, especially in areas that require deep semantic understanding and real-time relevance optimization.

#### Abstract
> Interactive recommendation is a typical information-seeking task that allows
users to interactively express their needs through natural language and obtain
personalized recommendations. Large language model-powered (LLM-powered) agents
have become a new paradigm in interactive recommendations, effectively
capturing users' real-time needs and enhancing personalized experiences.
However, due to limited planning and generalization capabilities, existing
formulations of LLM-powered interactive recommender agents struggle to
effectively address diverse and complex user intents, such as intuitive,
unrefined, or occasionally ambiguous requests. To tackle this challenge, we
propose a novel thought-augmented interactive recommender agent system (TAIRA)
that addresses complex user intents through distilled thought patterns.
Specifically, TAIRA is designed as an LLM-powered multi-agent system featuring
a manager agent that orchestrates recommendation tasks by decomposing user
needs and planning subtasks, with its planning capacity strengthened through
Thought Pattern Distillation (TPD), a thought-augmentation method that extracts
high-level thoughts from the agent's and human experts' experiences. Moreover,
we designed a set of user simulation schemes to generate personalized queries
of different difficulties and evaluate the recommendations based on specific
datasets. Through comprehensive experiments conducted across multiple datasets,
TAIRA exhibits significantly enhanced performance compared to existing methods.
Notably, TAIRA shows a greater advantage on more challenging tasks while
generalizing effectively on novel tasks, further validating its superiority in
managing complex user intents within interactive recommendation systems. The
code is publicly available at:https://github.com/Alcein/TAIRA.

### 10. MOTOR: Multimodal Optimal Transport via Grounded Retrieval in Medical Visual Question Answering

- **LLM Score**: 4
- **Keyword Score**: 17
- **Authors**: Mai A. Shaaban, Tausifa Jan Saleem, Vijay Ram Papineni, Mohammad Yaqub
- **URL**: <http://arxiv.org/abs/2506.22900v1>
- **Submitted**: 2025-06-28 14:30:37
- **Topic Keywords**: query, queries, ranking, relevance, rag, retrieval, rank
- **Reason**: The paper explores medical visual question answering, which is not directly related to the user's primary focus on information retrieval, search technologies, and query understanding. While it touches on retrieval-augmented generation and re-ranking, the multimodal context and medical domain are not directly applicable to the user's interests in e-commerce or general information retrieval.

#### Abstract
> Medical visual question answering (MedVQA) plays a vital role in clinical
decision-making by providing contextually rich answers to image-based queries.
Although vision-language models (VLMs) are widely used for this task, they
often generate factually incorrect answers. Retrieval-augmented generation
addresses this challenge by providing information from external sources, but
risks retrieving irrelevant context, which can degrade the reasoning
capabilities of VLMs. Re-ranking retrievals, as introduced in existing
approaches, enhances retrieval relevance by focusing on query-text alignment.
However, these approaches neglect the visual or multimodal context, which is
particularly crucial for medical diagnosis. We propose MOTOR, a novel
multimodal retrieval and re-ranking approach that leverages grounded captions
and optimal transport. It captures the underlying relationships between the
query and the retrieved context based on textual and visual information.
Consequently, our approach identifies more clinically relevant contexts to
augment the VLM input. Empirical analysis and human expert evaluation
demonstrate that MOTOR achieves higher accuracy on MedVQA datasets,
outperforming state-of-the-art methods by an average of 6.45%. Code is
available at https://github.com/BioMedIA-MBZUAI/MOTOR.

### 11. Mask-aware Text-to-Image Retrieval: Referring Expression Segmentation Meets Cross-modal Retrieval

- **LLM Score**: 4
- **Keyword Score**: 15
- **Authors**: Li-Cheng Shen, Jih-Kang Hsieh, Wei-Hua Li, Chu-Song Chen
- **URL**: <http://arxiv.org/abs/2506.22864v1>
- **Submitted**: 2025-06-28 12:19:49
- **Comment**: ICMR 2025
- **Topic Keywords**: query, ranking, rerank, rag, retrieval, rank, search
- **Reason**: The paper focuses on text-to-image retrieval, which is related to information retrieval, but the specific task of referring expression segmentation and object localization is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling. While the paper does mention multimodal large language models, which is a related topic, the primary focus is on computer vision and image processing, making it only loosely relevant to the user's research interests.

#### Abstract
> Text-to-image retrieval (TIR) aims to find relevant images based on a textual
query, but existing approaches are primarily based on whole-image captions and
lack interpretability. Meanwhile, referring expression segmentation (RES)
enables precise object localization based on natural language descriptions but
is computationally expensive when applied across large image collections. To
bridge this gap, we introduce Mask-aware TIR (MaTIR), a new task that unifies
TIR and RES, requiring both efficient image search and accurate object
segmentation. To address this task, we propose a two-stage framework,
comprising a first stage for segmentation-aware image retrieval and a second
stage for reranking and object grounding with a multimodal large language model
(MLLM). We leverage SAM 2 to generate object masks and Alpha-CLIP to extract
region-level embeddings offline at first, enabling effective and scalable
online retrieval. Secondly, MLLM is used to refine retrieval rankings and
generate bounding boxes, which are matched to segmentation masks. We evaluate
our approach on COCO and D$^3$ datasets, demonstrating significant improvements
in both retrieval accuracy and segmentation quality over previous methods.

### 12. Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Chase Fensore, Kaustubh Dhole, Joyce C Ho, Eugene Agichtein
- **URL**: <http://arxiv.org/abs/2506.22644v1>
- **Submitted**: 2025-06-27 21:20:43
- **Comment**: 4 pages, 3 tables, 2 figures. Accepted at the SIGIR LiveRAG Workshop
  2025 (Submission 2664)
- **Topic Keywords**: ranking, rag, retrieval augmented generation, retrieval, rank
- **Reason**: The paper evaluates retrieval-augmented generation systems, which is a topic in Natural Language Processing (NLP). However, the focus is on generation and re-ranking, rather than query understanding, ranking models, or user behavior modeling, which are key areas of interest in Information Retrieval (IR). While the paper touches on retrieval methods, it does not delve into the specific areas of query understanding, ranking models, or user behavior modeling that are central to IR.

#### Abstract
> We present our submission to the LiveRAG Challenge 2025, which evaluates
retrieval-augmented generation (RAG) systems on dynamic test sets using the
FineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense
(E5) retrieval methods and then aims to generate relevant and faithful answers
with Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic
questions generated with DataMorgana across 64 unique question-user
combinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP
from 0.523 to 0.797 (52% relative improvement) but introduces prohibitive
computational costs (84s vs 1.74s per question). While DSPy-optimized prompting
strategies achieved higher semantic similarity (0.771 vs 0.668), their 0%
refusal rates raised concerns about over-confidence and generalizability. Our
submitted hybrid system without re-ranking achieved 4th place in faithfulness
and 11th place in correctness among 25 teams. Analysis across question
categories reveals that vocabulary alignment between questions and documents
was the strongest predictor of performance on our development set, with
document-similar phrasing improving cosine similarity from 0.562 to 0.762.

### 13. RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Gabriel Iturra-Bocaz, Felipe Bravo-Marquez
- **URL**: <http://arxiv.org/abs/2506.23192v1>
- **Submitted**: 2025-06-29 11:34:23
- **Comment**: Accepted at SIGIR'23
- **Topic Keywords**: information retrieval, ranking, retrieval, rank
- **Reason**: The paper presents a Python library for training and evaluating incremental word embeddings from text data streams, which is related to Natural Language Processing and Information Retrieval. However, the focus is on word embeddings and their adaptation to evolving language patterns, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user.

#### Abstract
> Word embeddings have become essential components in various information
retrieval and natural language processing tasks, such as ranking, document
classification, and question answering. However, despite their widespread use,
traditional word embedding models present a limitation in their static nature,
which hampers their ability to adapt to the constantly evolving language
patterns that emerge in sources such as social media and the web (e.g., new
hashtags or brand names). To overcome this problem, incremental word embedding
algorithms are introduced, capable of dynamically updating word representations
in response to new language patterns and processing continuous data streams.
  This paper presents RiverText, a Python library for training and evaluating
incremental word embeddings from text data streams. Our tool is a resource for
the information retrieval and natural language processing communities that work
with word embeddings in streaming scenarios, such as analyzing social media.
The library implements different incremental word embedding techniques, such as
Skip-gram, Continuous Bag of Words, and Word Context Matrix, in a standardized
framework. In addition, it uses PyTorch as its backend for neural network
training. We have implemented a module that adapts existing intrinsic static
word embedding evaluation tasks for word similarity and word categorization to
a streaming setting. Finally, we compare the implemented methods with different
hyperparameter settings and discuss the results. Our open-source library is
available at https://github.com/dccuchile/rivertext.

### 14. BEST-Route: Adaptive LLM Routing with Test-Time Optimal Compute

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Dujian Ding, Ankur Mallick, Shaokun Zhang, Chi Wang, Daniel Madrigal, Mirian Del Carmen Hipolito Garcia, Menglin Xia, Laks V. S. Lakshmanan, Qingyun Wu, Victor R√ºhle
- **URL**: <http://arxiv.org/abs/2506.22716v1>
- **Submitted**: 2025-06-28 01:52:50
- **Comment**: Accepted to ICML 2025 (main conference)
- **Topic Keywords**: query, queries, rag
- **Reason**: The paper discusses query routing and large language models, which are related to information retrieval and search technologies. However, the focus on cost optimization and model selection is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large language models (LLMs) are powerful tools but are often expensive to
deploy at scale. LLM query routing mitigates this by dynamically assigning
queries to models of varying cost and quality to obtain a desired trade-off.
Prior query routing approaches generate only one response from the selected
model and a single response from a small (inexpensive) model was often not good
enough to beat a response from a large (expensive) model due to which they end
up overusing the large model and missing out on potential cost savings.
However, it is well known that for small models, generating multiple responses
and selecting the best can enhance quality while remaining cheaper than a
single large-model response. We leverage this idea to propose BEST-Route, a
novel routing framework that chooses a model and the number of responses to
sample from it based on query difficulty and the quality thresholds.
Experiments on real-world datasets demonstrate that our method reduces costs by
up to 60% with less than 1% performance drop.

### 15. Interact2Vec -- An efficient neural network-based model for simultaneously learning users and items embeddings in recommender systems

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Pedro R. Pires, Tiago A. Almeida
- **URL**: <http://arxiv.org/abs/2506.22648v1>
- **Submitted**: 2025-06-27 21:30:03
- **Comment**: Accepted for publication in Applied Soft Computing (ASOC), 49 pages,
  14 figures
- **Topic Keywords**: ranking, rag, recommend, rank
- **Reason**: The paper focuses on recommender systems, which is related to the user's interests in search technologies and information retrieval. However, it does not specifically address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's emphasis on neural networks and embeddings is also more relevant to NLP and data mining than the user's primary focus on IR.

#### Abstract
> Over the past decade, recommender systems have experienced a surge in
popularity. Despite notable progress, they grapple with challenging issues,
such as high data dimensionality and sparseness. Representing users and items
as low-dimensional embeddings learned via neural networks has become a leading
solution. However, while recent studies show promising results, many approaches
rely on complex architectures or require content data, which may not always be
available. This paper presents Interact2Vec, a novel neural network-based model
that simultaneously learns distributed embeddings for users and items while
demanding only implicit feedback. The model employs state-of-the-art strategies
that natural language processing models commonly use to optimize the training
phase and enhance the final embeddings. Two types of experiments were conducted
regarding the extrinsic and intrinsic quality of the model. In the former, we
benchmarked the recommendations generated by Interact2Vec's embeddings in a
top-$N$ ranking problem, comparing them with six other recommender algorithms.
The model achieved the second or third-best results in 30\% of the datasets,
being competitive with other recommenders, and has proven to be very efficient
with an average training time reduction of 274\% compared to other
embedding-based models. Later, we analyzed the intrinsic quality of the
embeddings through similarity tables. Our findings suggest that Interact2Vec
can achieve promising results, especially on the extrinsic task, and is an
excellent embedding-generator model for scenarios of scarce computing
resources, enabling the learning of item and user embeddings simultaneously and
efficiently.

### 16. Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Yucheng Cai, Yuxuan Wu, Yi Huang, Junlan Feng, Zhijian Ou
- **URL**: <http://arxiv.org/abs/2506.22852v1>
- **Submitted**: 2025-06-28 11:26:31
- **Topic Keywords**: rag, retrieval augmented generation, retrieval
- **Reason**: The paper explores the application of large language models in dialog systems, focusing on knowledge-intensive scenarios. While it touches on the topic of retrieval and generation, the primary focus is on finetuning LLMs with domain-specific data and knowledge, which is not directly related to my research interests in Information Retrieval, query understanding, and ranking models.

#### Abstract
> Large language models (LLMs) have recently been applied to dialog systems.
Despite making progress, LLMs are prone to errors in knowledge-intensive
scenarios. Recently, approaches based on retrieval augmented generation (RAG)
and agent have emerged to improve the factual accuracy by enhancing the LLMs
with knowledge retrieved from external knowledge bases (KBs). This is mostly
implemented by prompting the LLMs with instructions, examples and the retrieved
knowledge. However, LLMs may have difficulty using the retrieved knowledge
effectively for response generation, because they are not well trained to do
such generation for specific domains. To mitigate this problem, we propose to
finetune the LLMs in the RAG-based and agent-based systems with domain-specific
data, together with domain-specific external knowledge, which is called
knowledge augmented finetuning (KAFT). We base our study on the MobileCS2
dataset, a real-life customer service dialog dataset that features intensive
knowledge interactions, to systematically compare the prompting and KAFT
techniques in the RAG-based and agent-based systems. Experiment results show
that KAFT substantially surpasses prompting in both RAG and agent systems,
particularly in terms of factual accuracy. To the best of our knowledge, this
paper represents the first solid empirical work to investigate the KAFT idea.

### 17. Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Philip Lippmann, Jie Yang
- **URL**: <http://arxiv.org/abs/2506.23662v1>
- **Submitted**: 2025-06-30 09:38:50
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper presents a method for generating synthetic context corpus for contextual embeddings, which is related to query understanding and ranking models in Information Retrieval. However, the focus is on adapting embeddings for domain-specific distributions, which is not directly aligned with my research interests in user behavior modeling and real-time relevance optimization.

#### Abstract
> Context-aware embedding methods boost retrieval accuracy by conditioning on
corpus statistics (e.g., term co-occurrence and topical patterns) extracted
from neighboring documents. However, this context-aware approach requires
access to the target corpus or requires domain-specific finetuning, posing
practical barriers in privacy-sensitive or resource-constrained settings. We
present ZEST, a zero-shot contextual adaptation framework that replaces real
corpus access with a one-time offline synthesis of a compact proxy. Given only
a handful exemplar documents representative of the general target domain, we
use a multi-step hierarchical procedure to generate a synthetic context corpus
of several hundred documents that aims to emulate key domain-specific
distributions. At inference, the frozen context-aware encoder uses this proxy
corpus -- without any finetuning or target corpus access -- to produce
domain-adapted embeddings. Across the MTEB benchmark, ZEST's zero-shot
synthetic context adaptation using only five example documents performs within
0.5% of models leveraging full target corpus access -- demonstrating remarkable
efficacy without any retraining. ZEST thus provides a practical method for
deploying high-performance, adaptable embeddings in constrained environments.

### 18. Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Md Moinul Islam, Sofoklis Kakouros, Janne Heikkil√§, Mourad Oussalah
- **URL**: <http://arxiv.org/abs/2506.23714v1>
- **Submitted**: 2025-06-30 10:41:33
- **Comment**: Accepted to HHAI WS 2025: Workshops at the Fourth International
  Conference on Hybrid Human-Artificial Intelligence (HHAI)
- **Topic Keywords**: relevance
- **Reason**: The paper proposes a multimodal video summarization framework that integrates textual, audio, and visual cues, which is a relevant topic in Information Retrieval. However, the focus on video summarization and multimodal integration is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling. While the paper touches on some related concepts, such as extracting prosodic features and identifying semantically important moments, it does not address the user's core research themes.

#### Abstract
> The increasing volume of video content in educational, professional, and
social domains necessitates effective summarization techniques that go beyond
traditional unimodal approaches. This paper proposes a behaviour-aware
multimodal video summarization framework that integrates textual, audio, and
visual cues to generate timestamp-aligned summaries. By extracting prosodic
features, textual cues and visual indicators, the framework identifies
semantically and emotionally important moments. A key contribution is the
identification of bonus words, which are terms emphasized across multiple
modalities and used to improve the semantic relevance and expressive clarity of
the summaries. The approach is evaluated against pseudo-ground truth (pGT)
summaries generated using LLM-based extractive method. Experimental results
demonstrate significant improvements over traditional extractive method, such
as the Edmundson method, in both text and video-based evaluation metrics.
Text-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore
from 0.9152 to 0.9536, while in video-based evaluation, our proposed framework
improves F1-Score by almost 23%. The findings underscore the potential of
multimodal integration in producing comprehensive and behaviourally informed
video summaries.

### 19. Act-With-Think: Chunk Auto-Regressive Modeling for Generative Recommendation

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Yifan Wang, Weinan Gan, Longtao Xiao, Jieming Zhu, Heng Chang, Haozhao Wang, Rui Zhang, Zhenhua Dong, Ruiming Tang, Ruixuan Li
- **URL**: <http://arxiv.org/abs/2506.23643v1>
- **Submitted**: 2025-06-30 09:13:54
- **Comment**: 9 pages, 2 figures
- **Topic Keywords**: rag, recommend
- **Reason**: The paper presents a novel approach to generative recommendation, incorporating semantic and behavioral aspects. While it touches on the idea of understanding user behavior, it does not explicitly address query understanding, ranking models, or click models, which are core areas of interest in Information Retrieval. The paper's focus on recommender systems and generative models is somewhat related to the user's interests, but it does not align with the primary focus on information retrieval and deep semantic understanding.

#### Abstract
> Generative recommendation (GR) typically encodes behavioral or semantic
aspects of item information into discrete tokens, leveraging the standard
autoregressive (AR) generation paradigm to make predictions. However, existing
methods tend to overlook their intrinsic relationship, that is, the semantic
usually provides some reasonable explainability "$\textbf{why}$" for the
behavior "$\textbf{what}$", which may constrain the full potential of GR. To
this end, we present Chunk AutoRegressive Modeling (CAR), a new generation
paradigm following the decision pattern that users usually think semantic
aspects of items (e.g. brand) and then take actions on target items (e.g.
purchase). Our CAR, for the $\textit{first time}$, incorporates semantics
(SIDs) and behavior (UID) into a single autoregressive transformer from an
``act-with-think'' dual perspective via chunk-level autoregression.
Specifically, CAR packs SIDs and UID into a conceptual chunk for item unified
representation, allowing each decoding step to make a holistic prediction.
Experiments show that our CAR significantly outperforms existing methods based
on traditional AR, improving Recall@5 by 7.93% to 22.30%. Furthermore, we
verify the scaling effect between model performance and SIDs bit number,
demonstrating that CAR preliminary emulates a kind of slow-thinking style
mechanism akin to the reasoning processes observed in large language models
(LLMs).

### 20. Semantic-guided Diverse Decoding for Large Language Model

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Weijie Shi, Yue Cui, Yaguang Wu, Jingzhi Fang, Shibo Zhang, Mengze Li, Sirui Han, Jia Zhu, Jiajie Xu, Xiaofang Zhou
- **URL**: <http://arxiv.org/abs/2506.23601v1>
- **Submitted**: 2025-06-30 08:06:49
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on diverse decoding for large language models, which is related to information retrieval and search technologies. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's emphasis on semantic diversity and embedding space operations is somewhat relevant to NLP and data mining, but the connection to information retrieval is indirect and limited.

#### Abstract
> Diverse decoding of large language models is crucial for applications
requiring multiple semantically distinct responses, yet existing methods
primarily achieve lexical rather than semantic diversity. This limitation
significantly constrains Best-of-N strategies, group-based reinforcement
learning, and data synthesis. While temperature sampling and diverse beam
search modify token distributions or apply n-gram penalties, they fail to
ensure meaningful semantic differentiation. We introduce Semantic-guided
Diverse Decoding (SemDiD), operating directly in embedding space that balances
quality with diversity through three complementary mechanisms: orthogonal
directional guidance, dynamic inter-group repulsion, and position-debiased
probability assessment. SemDiD harmonizes these competing objectives using
adaptive gain functions and constraint optimization, ensuring both quality
thresholds and maximal semantic differentiation. Experiments show SemDiD
consistently outperforms existing methods, improving Best-of-N coverage by
1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15%
while increasing accuracy by up to 2.1%.

### 21. Compositions of Variant Experts for Integrating Short-Term and Long-Term Preferences

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Jaime Hieu Do, Trung-Hoang Le, Hady W. Lauw
- **URL**: <http://arxiv.org/abs/2506.23170v1>
- **Submitted**: 2025-06-29 10:09:33
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on personalized sequential recommendation, which is related to user behavior modeling and query understanding in information retrieval. However, it does not directly address ranking models or deep semantic understanding, which are core aspects of the user's research interests. The paper's emphasis on recommender systems and user preferences is somewhat relevant, but not a central match for the user's primary focus on information retrieval.

#### Abstract
> In the online digital realm, recommendation systems are ubiquitous and play a
crucial role in enhancing user experience. These systems leverage user
preferences to provide personalized recommendations, thereby helping users
navigate through the paradox of choice. This work focuses on personalized
sequential recommendation, where the system considers not only a user's
immediate, evolving session context, but also their cumulative historical
behavior to provide highly relevant and timely recommendations. Through an
empirical study conducted on diverse real-world datasets, we have observed and
quantified the existence and impact of both short-term (immediate and
transient) and long-term (enduring and stable) preferences on users' historical
interactions. Building on these insights, we propose a framework that combines
short- and long-term preferences to enhance recommendation performance, namely
Compositions of Variant Experts (CoVE). This novel framework dynamically
integrates short- and long-term preferences through the use of different
specialized recommendation models (i.e., experts). Extensive experiments
showcase the effectiveness of the proposed methods and ablation studies further
investigate the impact of variant expert types.

### 22. Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Dingzriui Wang, Xuanliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng
- **URL**: <http://arxiv.org/abs/2506.23146v1>
- **Submitted**: 2025-06-29 08:55:37
- **Topic Keywords**: relevance
- **Reason**: The paper proposes a novel metric for evaluating in-context learning effectiveness, which is a topic in Natural Language Processing. However, it does not directly relate to information retrieval, query understanding, ranking models, or user behavior modeling, which are the user's primary research interests.

#### Abstract
> In-context learning (ICL) has emerged as an effective approach to enhance the
performance of large language models (LLMs). However, its effectiveness varies
significantly across models and tasks, posing challenges for practitioners to
determine when ICL reliably improves performance. Current evaluation
approaches, reliant on performance change after applying ICL, suffer from low
reliability, poor attribution, and impracticality in data-insufficient
scenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that
quantifies ICL effectiveness by modeling the slope between learning gain (loss
decrease from demonstrations) and contextual relevance (demonstration-input
relevance). LCS addresses key limitations of performance-based metrics: (1) it
captures continuous loss changes even when outputs are incorrect, improving
reliability; (2) its formulation attributes ICL failures to weak contextual
alignment (inability to adapt inputs to demonstrations) or strong output
calibration (self-verification of correctness); and (3) it minimizes reliance
on labeled data via synthetic evaluation. Extensive experiments demonstrate
that LCS strongly correlates with performance improvements in labeled settings
and reliably reflects true effectiveness in biased or data-scarce scenarios.
Further analysis reveals actionable thresholds for LCS and identifies model
capabilities critical to ICL success.

### 23. Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Mathis Le Bail, J√©r√©mie Dentan, Davide Buscaldi, Sonia Vanier
- **URL**: <http://arxiv.org/abs/2506.23951v1>
- **Submitted**: 2025-06-30 15:18:50
- **Topic Keywords**: rag
- **Reason**: The paper explores the use of Sparse Autoencoders to extract interpretable concepts from Large Language Models for text classification, which is a topic related to Natural Language Processing. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core interests in Information Retrieval. The paper's focus on text classification and concept extraction is somewhat relevant to the user's research, but it does not align with their primary focus on information retrieval and real-time relevance optimization.

#### Abstract
> Sparse Autoencoders (SAEs) have been successfully used to probe Large
Language Models (LLMs) and extract interpretable concepts from their internal
representations. These concepts are linear combinations of neuron activations
that correspond to human-interpretable features. In this paper, we investigate
the effectiveness of SAE-based explainability approaches for sentence
classification, a domain where such methods have not been extensively explored.
We present a novel SAE-based architecture tailored for text classification,
leveraging a specialized classifier head and incorporating an activation rate
sparsity loss. We benchmark this architecture against established methods such
as ConceptShap, Independent Component Analysis, and other SAE-based concept
extraction techniques. Our evaluation covers two classification benchmarks and
four fine-tuned LLMs from the Pythia family. We further enrich our analysis
with two novel metrics for measuring the precision of concept-based
explanations, using an external sentence encoder. Our empirical results show
that our architecture improves both the causality and interpretability of the
extracted features.

### 24. Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Manuel Pratelli, Marinella Petrocchi
- **URL**: <http://arxiv.org/abs/2506.23610v1>
- **Submitted**: 2025-06-30 08:16:07
- **Comment**: pre-print version - paper actually under submission
- **Topic Keywords**: rag
- **Reason**: The paper evaluates the ability of Large Language Models (LLMs) to simulate human personality-driven susceptibility to misinformation, which is not directly related to my research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper touches on the topic of language models, it does not focus on query understanding, ranking models, or user behavior modeling, which are my primary areas of interest.

#### Abstract
> Large language models (LLMs) make it possible to generate synthetic
behavioural data at scale, offering an ethical and low-cost alternative to
human experiments. Whether such data can faithfully capture psychological
differences driven by personality traits, however, remains an open question. We
evaluate the capacity of LLM agents, conditioned on Big-Five profiles, to
reproduce personality-based variation in susceptibility to misinformation,
focusing on news discernment, the ability to judge true headlines as true and
false headlines as false. Leveraging published datasets in which human
participants with known personality profiles rated headline accuracy, we create
matching LLM agents and compare their responses to the original human patterns.
Certain trait-misinformation associations, notably those involving
Agreeableness and Conscientiousness, are reliably replicated, whereas others
diverge, revealing systematic biases in how LLMs internalize and express
personality. The results underscore both the promise and the limits of
personality-aligned LLMs for behavioral simulation, and offer new insight into
modeling cognitive diversity in artificial agents.

### 25. V-SYNTHESIS: Task-Agnostic Synthesis of Consistent and Diverse In-Context Demonstrations from Scratch via V-Entropy

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Dingzirui Wang, Xuanliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng
- **URL**: <http://arxiv.org/abs/2506.23149v1>
- **Submitted**: 2025-06-29 08:57:09
- **Topic Keywords**: rag
- **Reason**: The paper focuses on synthesizing demonstrations from scratch for arbitrary tasks, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the topic of consistency and diversity, it does not explicitly address ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest for your research.

#### Abstract
> High labeling cost for in-context learning (ICL) demonstrations motivates
using large language models (LLMs) for synthesis to reduce overhead. However,
existing synthesis methods are mainly task-specific or rely on pre-existing
demonstrations. So this paper focuses on synthesizing demonstrations from
scratch for arbitrary tasks. A major challenge in synthesizing from scratch is
ensuring consistency with the target task, as the lack of labeling guidance
could lead to synthesis bias. We first propose a consistency metric called
V-Score, which has higher performance and lower computation cost compared with
the metrics based on grams or embedding vectors. Furthermore, we introduce
V-Synthesis, which leverages V-Score for proportional sampling to ensure both
high consistency and diversity of synthesized demonstrations. Experimental
results demonstrate that V-Synthesis yields an average performance improvement
of 2.0% compared to existing synthesis methods confirming the effectiveness of
V-Synthesis.

### 26. Format-Adapter: Improving Reasoning Capability of LLMs by Adapting Suitable Format

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Dingzirui Wang, Xuanliang Zhang, Rongyu Cao, Longxu Dou, Xianzhen Luo, Yingwei Ma, Qingfu Zhu, Wanxiang Che, Binhua Li, Fei Huang, Yongbin Li
- **URL**: <http://arxiv.org/abs/2506.23133v1>
- **Submitted**: 2025-06-29 08:11:52
- **Topic Keywords**: rag
- **Reason**: The paper explores the idea of adapting formats for large language models (LLMs) to improve their reasoning capability, which is related to query understanding and ranking models. However, the focus is on generating and selecting formats rather than query understanding or ranking models, making it somewhat relevant to the user's research interests.

#### Abstract
> Generating and voting multiple answers is an effective method to mitigate
reasoning inconsistencies of large language models (LLMs). Prior works have
shown that multiple reasoning formats outperform a single format when
generating multiple answers. However, previous works using multiple formats
rely on formats labeled by humans, which could be unsuitable for all tasks and
have high labeling costs. To address this issue, we adapt suitable formats to
the given tasks by generating and selecting formats. We first propose how to
measure the reasoning error when generating multiple answers. Then, we
introduce Format-Adapter, which utilizes LLMs to generate and select suitable
reasoning formats by minimizing the error measurement we present. We conduct
experiments on math and commonsense reasoning tasks, where Format-Adapter
achieves a 4.3% performance improvement on average over previous works,
demonstrating the effectiveness.

### 27. Machine Assistant with Reliable Knowledge: Enhancing Student Learning via RAG-based Retrieval

- **LLM Score**: 2
- **Keyword Score**: 11
- **Authors**: Yongsheng Lian
- **URL**: <http://arxiv.org/abs/2506.23026v1>
- **Submitted**: 2025-06-28 22:17:27
- **Topic Keywords**: query, queries, rag, retrieval, search
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling. The focus is on a specific application of retrieval-augmented question-answering in an educational setting, which is not a core area of your research.

#### Abstract
> We present Machine Assistant with Reliable Knowledge (MARK), a
retrieval-augmented question-answering system designed to support student
learning through accurate and contextually grounded responses. The system is
built on a retrieval-augmented generation (RAG) framework, which integrates a
curated knowledge base to ensure factual consistency. To enhance retrieval
effectiveness across diverse question types, we implement a hybrid search
strategy that combines dense vector similarity with sparse keyword-based
retrieval. This dual-retrieval mechanism improves robustness for both general
and domain-specific queries. The system includes a feedback loop in which
students can rate responses and instructors can review and revise them.
Instructor corrections are incorporated into the retrieval corpus, enabling
adaptive refinement over time. The system was deployed in a classroom setting
as a substitute for traditional office hours, where it successfully addressed a
broad range of student queries. It was also used to provide technical support
by integrating with a customer-specific knowledge base, demonstrating its
ability to handle routine, context-sensitive tasks in applied domains. MARK is
publicly accessible at https://app.eduquery.ai.

### 28. NaviX: A Native Vector Index Design for Graph DBMSs With Robust Predicate-Agnostic Search Performance

- **LLM Score**: 2
- **Keyword Score**: 9
- **Authors**: Gaurav Sehgal, Semih Salihoglu
- **URL**: <http://arxiv.org/abs/2506.23397v1>
- **Submitted**: 2025-06-29 21:16:07
- **Topic Keywords**: query, queries, rag, search
- **Reason**: The paper focuses on designing a native vector index for graph DBMSs, which is not directly related to information retrieval, search technologies, or query understanding. Although it mentions search queries, the context is different from the user's interests in IR and NLP.

#### Abstract
> There is an increasing demand for extending existing DBMSs with vector
indices so that they become unified systems capable of supporting modern
predictive applications, which require joint querying of vector embeddings
together with the structured properties and connections of objects. We present
NaviX, a native vector index for graph DBMSs (GDBMSs) that has two main design
goals. First, we aim to implement a disk-based vector index that leverages the
core storage and query-processing capabilities of the underlying GDBMS. To this
end, NaviX is built on the Hierarchical Navigable Small-World (HNSW) graph,
which itself is a graph-based structure. Second, we aim to support
predicate-agnostic filtered vector search queries, in which the k nearest
neighbors (kNNs) of a query vector vQ are searched only within an arbitrary
subset S of vectors defined by an ad-hoc selection sub-query QS. We adopt a
prefiltering approach that evaluates QS first and passes the full description
of subset S to the kNN search operator. We study how to design a prefiltering
search algorithm that remains robust under varying selectivities and under
different correlations between subset S and query vector vQ. We propose an
adaptive algorithm that uses the local selectivity of each vector in the HNSW
graph to choose an appropriate heuristic at every iteration of the kNN search.
Finally, We demonstrate NaviX's robustness and efficiency through extensive
experiments against both existing prefiltering- and postfiltering-based
baselines.

### 29. On the Predictive Power of Representation Dispersion in Language Models

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Yanhong Li, Ming Li, Karen Livescu, Jiawei Zhou
- **URL**: <http://arxiv.org/abs/2506.24106v1>
- **Submitted**: 2025-06-30 17:53:50
- **Topic Keywords**: pairwise, rag, retrieval, search
- **Reason**: The paper focuses on the predictive power of representation dispersion in language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the topic of model selection and retrieval-based methods, the connection is loose and the paper's primary focus is on language models and their internal workings, rather than information retrieval or search.

#### Abstract
> We show that a language model's ability to predict text is tightly linked to
the breadth of its embedding space: models that spread their contextual
representations more widely tend to achieve lower perplexity. Concretely, we
find that representation dispersion - the average pairwise cosine distance
among hidden vectors - strongly and negatively correlates with perplexity
across diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia,
news, scientific abstracts). Beyond illustrating this link, we show how
dispersion can be leveraged for a range of practical tasks without requiring
labeled data. First, measuring dispersion on unlabeled text allows us to
predict downstream accuracy in new domains, offering a data-efficient tool for
model selection. Next, we find that identifying layers with higher dispersion
pinpoints the best representations for retrieval-based methods such as kNN-LM,
bypassing exhaustive layer-by-layer searches. Finally, we integrate a simple
push-away objective into training, which increases dispersion in both
single-domain and cross-domain scenarios and directly improves perplexity in
each.

### 30. Emergent musical properties of a transformer under contrastive self-supervised learning

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Yuexuan Kong, Gabriel Meseguer-Brocal, Vincent Lostanlen, Mathieu Lagrange, Romain Hennequin
- **URL**: <http://arxiv.org/abs/2506.23873v1>
- **Submitted**: 2025-06-30 14:04:59
- **Comment**: Accepted at ISMIR 2025
- **Topic Keywords**: information retrieval, rag, retrieval
- **Reason**: The paper is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus on music information retrieval and transformer models for sequence modeling in music is outside the user's primary areas of interest.

#### Abstract
> In music information retrieval (MIR), contrastive self-supervised learning
for general-purpose representation models is effective for global tasks such as
automatic tagging. However, for local tasks such as chord estimation, it is
widely assumed that contrastively trained general-purpose self-supervised
models are inadequate and that more sophisticated SSL is necessary; e.g.,
masked modeling. Our paper challenges this assumption by revealing the
potential of contrastive SSL paired with a transformer in local MIR tasks. We
consider a lightweight vision transformer with one-dimensional patches in the
time--frequency domain (ViT-1D) and train it with simple contrastive SSL
through normalized temperature-scaled cross-entropy loss (NT-Xent). Although
NT-Xent operates only over the class token, we observe that, potentially thanks
to weight sharing, informative musical properties emerge in ViT-1D's sequence
tokens. On global tasks, the temporal average of class and sequence tokens
offers a performance increase compared to the class token alone, showing useful
properties in the sequence tokens. On local tasks, sequence tokens perform
unexpectedly well, despite not being specifically trained for. Furthermore,
high-level musical features such as onsets emerge from layer-wise attention
maps and self-similarity matrices show different layers capture different
musical dimensions. Our paper does not focus on improving performance but
advances the musical interpretation of transformers and sheds light on some
overlooked abilities of contrastive SSL paired with transformers for sequence
modeling in MIR.

### 31. Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Xiang Zhuang, Bin Wu, Jiyu Cui, Kehua Feng, Xiaotong Li, Huabin Xing, Keyan Ding, Qiang Zhang, Huajun Chen
- **URL**: <http://arxiv.org/abs/2506.23056v1>
- **Submitted**: 2025-06-29 02:00:38
- **Comment**: ACL 2025 Main
- **Topic Keywords**: rag, ctr, search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The topic of molecular structure elucidation and chemical knowledge is outside the scope of your expertise and interests.

#### Abstract
> Molecular structure elucidation involves deducing a molecule's structure from
various types of spectral data, which is crucial in chemical experimental
analysis. While large language models (LLMs) have shown remarkable proficiency
in analyzing and reasoning through complex tasks, they still encounter
substantial challenges in molecular structure elucidation. We identify that
these challenges largely stem from LLMs' limited grasp of specialized chemical
knowledge. In this work, we introduce a Knowledge-enhanced reasoning framework
for Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search
for test-time scaling as a plugin. Specifically, we construct an external
molecular substructure knowledge base to extend the LLMs' coverage of the
chemical structure space. Furthermore, we design a specialized
molecule-spectrum scorer to act as a reward model for the reasoning process,
addressing the issue of inaccurate solution evaluation in LLMs. Experimental
results show that our approach significantly boosts performance, particularly
gaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is
available at https://github.com/HICAI-ZJU/K-MSE.

### 32. Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Ruhina Tabasshum Prome, Tarikul Islam Tamiti, Anomadarshi Barua
- **URL**: <http://arxiv.org/abs/2506.23930v1>
- **Submitted**: 2025-06-30 14:59:25
- **Topic Keywords**: rag, ctr
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on hate speech detection in low-resource languages using prompt engineering and large language models, which is outside your primary focus areas.

#### Abstract
> The rapid expansion of social media leads to a marked increase in hate
speech, which threatens personal lives and results in numerous hate crimes.
Detecting hate speech presents several challenges: diverse dialects, frequent
code-mixing, and the prevalence of misspelled words in user-generated content
on social media platforms. Recent progress in hate speech detection is
typically concentrated on high-resource languages. However, low-resource
languages still face significant challenges due to the lack of large-scale,
high-quality datasets. This paper investigates how we can overcome this
limitation via prompt engineering on large language models (LLMs) focusing on
low-resource Bengali language. We investigate six prompting strategies -
zero-shot prompting, refusal suppression, flattering the classifier, multi-shot
prompting, role prompting, and finally our innovative metaphor prompting to
detect hate speech effectively in low-resource languages. We pioneer the
metaphor prompting to circumvent the built-in safety mechanisms of LLMs that
marks a significant departure from existing jailbreaking methods. We
investigate all six different prompting strategies on the Llama2-7B model and
compare the results extensively with three pre-trained word embeddings - GloVe,
Word2Vec, and FastText for three different deep learning models - multilayer
perceptron (MLP), convolutional neural network (CNN), and bidirectional gated
recurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in
the low-resource Bengali language, we also evaluate it in another low-resource
language - Hindi, and two high-resource languages - English and German. The
performance of all prompting techniques is evaluated using the F1 score, and
environmental impact factor (IF), which measures CO$_2$ emissions, electricity
usage, and computational time.

### 33. EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Hyunjong Kim, Sangyeop Kim, Jongheon Jeong, Yeongjae Cho, Sungzoon Cho
- **URL**: <http://arxiv.org/abs/2506.24016v1>
- **Submitted**: 2025-06-30 16:20:51
- **Comment**: Accepted at ACL 2025 Findings
- **Topic Keywords**: relevance
- **Reason**: This paper is not directly related to Information Retrieval (IR) or Search technologies, and does not address query understanding, ranking models, or user behavior modeling. While it involves language models and evaluation metrics, the focus is on image captioning and explainability, which is not a primary area of interest for the user.

#### Abstract
> Recent advances in large language models and vision-language models have led
to growing interest in explainable evaluation metrics for image captioning.
However, these metrics generate explanations without standardized criteria, and
the overall quality of the generated explanations remains unverified. In this
paper, we propose EXPERT, a reference-free evaluation metric that provides
structured explanations based on three fundamental criteria: fluency,
relevance, and descriptiveness. By constructing large-scale datasets of
high-quality structured explanations, we develop a two-stage evaluation
template to effectively supervise a vision-language model for both scoring and
explanation generation. EXPERT achieves state-of-the-art results on benchmark
datasets while providing significantly higher-quality explanations than
existing metrics, as validated through comprehensive human evaluation. Our code
and datasets are available at https://github.com/hjkim811/EXPERT.

### 34. Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Seungjun Yi, Joakim Nguyen, Huimin Xu, Terence Lim, Andrew Well, Mia Markey, Ying Ding
- **URL**: <http://arxiv.org/abs/2506.23998v1>
- **Submitted**: 2025-06-30 16:02:28
- **Comment**: Presented at ACL 2025 SRW
- **Topic Keywords**: relevance
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding. While it mentions large language models, the focus is on thematic analysis and clinical narratives, which is outside the scope of the user's research interests.

#### Abstract
> Congenital heart disease (CHD) presents complex, lifelong challenges often
underrepresented in traditional clinical metrics. While unstructured narratives
offer rich insights into patient and caregiver experiences, manual thematic
analysis (TA) remains labor-intensive and unscalable. We propose a fully
automated large language model (LLM) pipeline that performs end-to-end TA on
clinical narratives, which eliminates the need for manual coding or full
transcript review. Our system employs a novel multi-agent framework, where
specialized LLM agents assume roles to enhance theme quality and alignment with
human analysis. To further improve thematic relevance, we optionally integrate
reinforcement learning from human feedback (RLHF). This supports scalable,
patient-centered analysis of large qualitative datasets and allows LLMs to be
fine-tuned for specific clinical contexts.

### 35. Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Yang Dai, Jianxiang An, Tianwei Lin, Hongyang He, Hongzhe Huang, Wenqiao Zhang, Zheqi Lv, Siliang Tang, Yueting Zhuang
- **URL**: <http://arxiv.org/abs/2506.23940v1>
- **Submitted**: 2025-06-30 15:07:41
- **Topic Keywords**: rag, rank
- **Reason**: The paper focuses on integrating domain knowledge in Multimodal Large Language Models (MLLMs), which is not directly related to Information Retrieval (IR) or Search technologies. While it mentions multimodal benchmarks, the abstract does not provide any insights into query understanding, ranking models, or user behavior modeling, which are key areas of interest in IR.

#### Abstract
> Multimodal Large Language Models (MLLMs) have achieved success across various
domains. However, their applicability tends to degrade when confronted with
different types of data inputs, especially for MLLMs that have been fine-tuned
for specific tasks. Despite its importance, the study of knowledge sharing
among domain-specific MLLMs--such as those trained for mathematics or
code--remains largely underexplored. To address the fragmentation of knowledge
across domain-specialized MLLMs, we propose a unified parameter integration
framework that enables modular composition of expert capabilities. Our method
is grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,
which leverages both local functional attribution and global
information-theoretic signals to guide selective parameter fusion. By extending
this mechanism to the low-rank adaptation layer granularity, we ensure
efficient integration with minimal inference overhead. Furthermore, we
introduce a domain compatibility scoring mechanism that quantifies inter-expert
alignment at the activation level and correlates with downstream task utility.
This principled fusion protocol allows the final model to synergize
heterogeneous expertise while preserving structural modularity. Extensive
evaluations across diverse multimodal benchmarks validate the effectiveness of
our framework, offering a scalable path toward compositional, domain-adaptive
MLLMs.

### 36. AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: JiaRu Wu, Mingwei Liu
- **URL**: <http://arxiv.org/abs/2506.23735v1>
- **Submitted**: 2025-06-30 11:18:56
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on evaluating large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on robustness and generalization, the context is not relevant to the user's primary research interests in IR and NLP.

#### Abstract
> Large language models (LLMs) have shown remarkable performance on various
tasks, but existing evaluation benchmarks are often static and insufficient to
fully assess their robustness and generalization in realistic scenarios. Prior
work using evolutionary or adversarial data augmentation has improved
evaluation diversity but lacks systematic control over perturbation types and
multi-step complexity, limiting comprehensive robustness analysis. To address
these gaps, we propose AutoEvoEval, an evolution-based evaluation framework for
close-ended tasks such as multi-choice question answering. AutoEvoEval
introduces 22 interpretable atomic evolution operations and supports
multi-round compositions, enabling controlled generation of diverse,
challenging, and realistic test samples. We conduct extensive experiments
addressing four research questions on a broad set of open- and closed-source
LLMs. Our results show that atomic operations cause an average accuracy drop of
7.283\%, with structure-disrupting or misleading semantic edits causing the
largest declines. Model sensitivities vary significantly for the same
perturbation, and combining multiple evolution steps amplifies adversarial
effects by up to 52.932\%. These findings suggest current benchmarks may
overestimate true model generalization and emphasize the need for
evolution-aware robustness evaluation. Code and resources are available at:
https://github.com/SYSUSELab/AutoEvoEval.

### 37. NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Phan Quoc Hung Mai, Quang Hung Nguyen, Phuong Giang Duong, Hong Hanh Nguyen, Nguyen Tuan Long
- **URL**: <http://arxiv.org/abs/2506.23524v1>
- **Submitted**: 2025-06-30 05:19:04
- **Topic Keywords**: relevance
- **Reason**: The paper focuses on a specific domain (education) and language (Vietnamese), which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it explores multitask learning, the topics and techniques discussed are not aligned with the user's specific areas of interest, such as query understanding, ranking models, and user behavior modeling.

#### Abstract
> In the field of education, understanding students' opinions through their
comments is crucial, especially in the Vietnamese language, where resources
remain limited. Existing educational datasets often lack domain relevance and
student slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese
dataset for Educational Sentiment Classification and Topic Classification,
curated from university forums, which offers more samples, richer class
diversity, longer texts, and broader vocabulary. In addition, we explore
multitask learning using encoder-only language models (BERT), in which we
showed that it achieves performance up to 83.7% and 79.8% accuracy for
sentiment and topic classification tasks. We also benchmark our dataset and
model with other datasets and models, including Large Language Models, and
discuss these benchmarks. The dataset is publicly available at:
https://huggingface.co/datasets/hung20gg/NEU-ESC.

### 38. Datasets for Fairness in Language Models: An In-Depth Survey

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Jiale Zhang, Zichong Wang, Avash Palikhe, Zhipeng Yin, Wenbin Zhang
- **URL**: <http://arxiv.org/abs/2506.23411v1>
- **Submitted**: 2025-06-29 22:11:58
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on fairness in language models, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and query understanding. Although it touches on related topics like NLP, the scope is limited to language models and fairness, making it only loosely relevant to the user's interests.

#### Abstract
> Fairness benchmarks play a central role in shaping how we evaluate language
models, yet surprisingly little attention has been given to examining the
datasets that these benchmarks rely on. This survey addresses that gap by
presenting a broad and careful review of the most widely used fairness datasets
in current language model research, characterizing them along several key
dimensions including their origin, scope, content, and intended use to help
researchers better appreciate the assumptions and limitations embedded in these
resources. To support more meaningful comparisons and analyses, we introduce a
unified evaluation framework that reveals consistent patterns of demographic
disparities across datasets and scoring methods. Applying this framework to
twenty four common benchmarks, we highlight the often overlooked biases that
can influence conclusions about model fairness and offer practical guidance for
selecting, combining, and interpreting these datasets. We also point to
opportunities for creating new fairness benchmarks that reflect more diverse
social contexts and encourage more thoughtful use of these tools going forward.
All code, data, and detailed results are publicly available at
https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets
to promote transparency and reproducibility across the research community.

### 39. Density, asymmetry and citation dynamics in scientific literature

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Nathaniel Imel, Zachary Hafen
- **URL**: <http://arxiv.org/abs/2506.23366v1>
- **Submitted**: 2025-06-29 18:55:04
- **Topic Keywords**: rag, search
- **Reason**: The paper is not directly related to Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling, which are the user's primary research interests. While it touches on the topic of semantic similarity, it is focused on scientometrics and citation dynamics, which is a distinct area of research.

#### Abstract
> Scientific behavior is often characterized by a tension between building upon
established knowledge and introducing novel ideas. Here, we investigate whether
this tension is reflected in the relationship between the similarity of a
scientific paper to previous research and its eventual citation rate. To
operationalize similarity to previous research, we introduce two complementary
metrics to characterize the local geometry of a publication's semantic
neighborhood: (1) \emph{density} ($\rho$), defined as the ratio between a fixed
number of previously-published papers and the minimum distance enclosing those
papers in a semantic embedding space, and (2) asymmetry ($\alpha$), defined as
the average directional difference between a paper and its nearest neighbors.
We tested the predictive relationship between these two metrics and its
subsequent citation rate using a Bayesian hierarchical regression approach,
surveying $\sim 53,000$ publications across nine academic disciplines and five
different document embeddings. While the individual effects of $\rho$ on
citation count are small and variable, incorporating density-based predictors
consistently improves out-of-sample prediction when added to baseline models.
These results suggest that the density of a paper's surrounding scientific
literature may carry modest but informative signals about its eventual impact.
Meanwhile, we find no evidence that publication asymmetry improves model
predictions of citation rates. Our work provides a scalable framework for
linking document embeddings to scientometric outcomes and highlights new
questions regarding the role that semantic similarity plays in shaping the
dynamics of scientific reward.

### 40. GaussMaster: An LLM-based Database Copilot System

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Wei Zhou, Ji Sun, Xuanhe Zhou, Guoliang Li, Luyang Liu, Hao Wu, Tianyuan Wang
- **URL**: <http://arxiv.org/abs/2506.23322v1>
- **Submitted**: 2025-06-29 16:39:31
- **Comment**: We welcome contributions from the community. For reference, please
  see the code at: https://gitcode.com/opengauss/openGauss-GaussMaster
- **Topic Keywords**: queries
- **Reason**: The paper focuses on autonomous database platforms and a specific system called GaussMaster, which is not directly related to Information Retrieval, Search technologies, or Natural Language Processing. Although it mentions SQL queries and anomaly detection, the context is not relevant to the user's research interests.

#### Abstract
> In the financial industry, data is the lifeblood of operations, and DBAs
shoulder significant responsibilities for SQL tuning, database deployment,
diagnosis, and service repair. In recent years, both database vendors and
customers have increasingly turned to autonomous database platforms in an
effort to alleviate the heavy workload of DBAs. However, existing autonomous
database platforms are limited in their capabilities, primarily addressing
single-point issues such as NL2SQL, anomaly detection, and SQL tuning. Manual
intervention remains a necessity for comprehensive database maintenance.
GaussMaster aims to revolutionize this landscape by introducing an LLM-based
database copilot system. This innovative solution is designed not only to
assist developers in writing efficient SQL queries but also to provide
comprehensive care for database services. When database instances exhibit
abnormal behavior, GaussMaster is capable of orchestrating the entire
maintenance process automatically. It achieves this by analyzing hundreds of
metrics and logs, employing a Tree-of-thought approach to identify root causes,
and invoking appropriate tools to resolve issues. We have successfully
implemented GaussMaster in real-world scenarios, such as the banking industry,
where it has achieved zero human intervention for over 34 database maintenance
scenarios. In this paper, we present significant improvements in these tasks
with code at https://gitcode.com/opengauss/openGauss-GaussMaster.

### 41. VERA: Variational Inference Framework for Jailbreaking Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Anamika Lochab, Lu Yan, Patrick Pynadath, Xiangyu Zhang, Ruqi Zhang
- **URL**: <http://arxiv.org/abs/2506.22666v1>
- **Submitted**: 2025-06-27 22:22:00
- **Topic Keywords**: query
- **Reason**: The paper focuses on developing a framework for jailbreaking large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it involves language models, the context and objectives are distinct from the user's primary research interests.

#### Abstract
> The rise of API-only access to state-of-the-art LLMs highlights the need for
effective black-box jailbreak methods to identify model vulnerabilities in
real-world settings. Without a principled objective for gradient-based
optimization, most existing approaches rely on genetic algorithms, which are
limited by their initialization and dependence on manually curated prompt
pools. Furthermore, these methods require individual optimization for each
prompt, failing to provide a comprehensive characterization of model
vulnerabilities. To address this gap, we introduce VERA: Variational infErence
fRamework for jAilbreaking. VERA casts black-box jailbreak prompting as a
variational inference problem, training a small attacker LLM to approximate the
target LLM's posterior over adversarial prompts. Once trained, the attacker can
generate diverse, fluent jailbreak prompts for a target query without
re-optimization. Experimental results show that VERA achieves strong
performance across a range of target LLMs, highlighting the value of
probabilistic inference for adversarial prompt generation.

### 42. SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Bo Liu, Leon Guertler, Simon Yu, Zichen Liu, Penghui Qi, Daniel Balcells, Mickel Liu, Cheston Tan, Weiyan Shi, Min Lin, Wee Sun Lee, Natasha Jaques
- **URL**: <http://arxiv.org/abs/2506.24119v1>
- **Submitted**: 2025-06-30 17:58:13
- **Comment**: Work in Progress
- **Topic Keywords**: rag
- **Reason**: The paper focuses on multi-agent reinforcement learning and zero-sum games, which is not directly related to information retrieval, search technologies, or query understanding. While the paper mentions language models, it does not specifically address ranking models, user behavior modeling, or deep semantic understanding, making it only loosely relevant to the user's research interests.

#### Abstract
> Recent advances in reinforcement learning have shown that language models can
develop sophisticated reasoning through training on tasks with verifiable
rewards, but these approaches depend on human-curated problem-answer pairs and
domain-specific reward engineering. We introduce SPIRAL, a self-play framework
where models learn by playing multi-turn, zero-sum games against continuously
improving versions of themselves, eliminating the need for human supervision.
Through self-play, SPIRAL generates an infinite curriculum of progressively
challenging problems as models must constantly adapt to stronger opponents. To
enable this self-play training at scale, We implement a fully online,
multi-turn, multi-agent reinforcement learning system for LLMs and propose
role-conditioned advantage estimation (RAE) to stabilize multi-agent training.
Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that
transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%
improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000
expert game trajectories. Analysis reveals that this transfer occurs through
three cognitive patterns: systematic decomposition, expected value calculation,
and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple
Negotiation) further enhances performance as each game develops distinct
reasoning strengths. Applying SPIRAL to a strong reasoning model
(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These
results demonstrate that zero-sum games naturally develop transferable
reasoning capabilities, highlighting a promising direction for autonomous
reasoning development.

### 43. TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Renren Jin, Tianhao Shen, Xinwei Wu, Dan Shi, Haoran Sun, Wuwei Huang, Quandong Wang, Wei Liu, Jian Luan, Bin Wang, Deyi Xiong
- **URL**: <http://arxiv.org/abs/2506.23979v1>
- **Submitted**: 2025-06-30 15:45:28
- **Comment**: 33 pages, 15 tables, 11 figures
- **Topic Keywords**: rag
- **Reason**: The paper focuses on preference data generation for large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions fine-tuning and dataset construction, the context is different from the user's primary research interests.

#### Abstract
> Conducting supervised fine-tuning and preference fine-tuning on large
language models (LLMs) requires high-quality datasets to improve their ability
to follow instructions and align with human preferences and values. However,
constructing such datasets is resource-intensive, and most available datasets
for supervised and preference fine-tuning are in English. To address these
challenges, we propose the \underline{\textbf{Ta}}xonomy-Guided
\underline{\textbf{P}}reference Data Generation (TaP) framework, which
facilitates automated and scalable construction of preference datasets across
various languages. TaP is grounded in a structured taxonomy that allows
fine-grained control over dataset composition, thereby ensuring both diversity
and comprehensive coverage. We employ TaP-generated datasets to perform
supervised and preference fine-tuning on various LLMs. Experimental results
demonstrate that LLMs trained on TaP-generated datasets outperform those
trained on existing open-source datasets. Remarkably, LLMs trained on
TaP-generated datasets surpass the performance of those trained on an
open-source dataset that is 180 times larger.

### 44. Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What to Do About It

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Seyed Mahed Mousavi, Edoardo Cecchinato, Lucia Hornikova, Giuseppe Riccardi
- **URL**: <http://arxiv.org/abs/2506.23864v1>
- **Submitted**: 2025-06-30 13:57:28
- **Topic Keywords**: rag
- **Reason**: The paper's focus on evaluating the reliability of reasoning benchmarks and identifying flaws in their design and methodology is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper touches on the topic of model performance, it does not specifically address query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user.

#### Abstract
> We conduct a systematic audit of three widely used reasoning benchmarks,
SocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark
items and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and
LLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic
issues in benchmark design (e.g., duplicated items, ambiguous wording, and
implausible answers), as well as scoring procedures that prioritize output form
over reasoning process. Through systematic human annotation and re-evaluation
on cleaned benchmark subsets, we find that model scores often improve not due
to due to erratic surface wording variations and not to improved reasoning.
Infact, further analyses show that model performance is highly sensitive to
minor input variations such as context availability and phrasing, revealing
that high scores may reflect alignment with format-specific cues rather than
consistent inference based on the input. These findings challenge the validity
of current benchmark-based claims about reasoning in LLMs, and highlight the
need for evaluation protocols that assess reasoning as a process of drawing
inference from available information, rather than as static output selection.
We release audited data and evaluation tools to support more interpretable and
diagnostic assessments of model reasoning.

### 45. Towards the "Digital Me": A vision of authentic Conversational Agents powered by personal Human Digital Twins

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Llu√≠s C. Coll, Martin W. Lauer-Schmaltz, Philip Cash, John P. Hansen, Anja Maier
- **URL**: <http://arxiv.org/abs/2506.23826v1>
- **Submitted**: 2025-06-30 13:18:31
- **Comment**: 24 pages, 9 figures
- **Topic Keywords**: retrieval
- **Reason**: This paper focuses on Human Digital Twins and conversational AI, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it mentions language models, the context is different from the user's interests in IR and NLP.

#### Abstract
> Human Digital Twins (HDTs) have traditionally been conceptualized as
data-driven models designed to support decision-making across various domains.
However, recent advancements in conversational AI open new possibilities for
HDTs to function as authentic, interactive digital counterparts of individuals.
This paper introduces a novel HDT system architecture that integrates large
language models with dynamically updated personal data, enabling it to mirror
an individual's conversational style, memories, and behaviors. To achieve this,
our approach implements context-aware memory retrieval, neural
plasticity-inspired consolidation, and adaptive learning mechanisms, creating a
more natural and evolving digital persona. The resulting system does not only
replicate an individual's unique conversational style depending on who they are
speaking with, but also enriches responses with dynamically captured personal
experiences, opinions, and memories. While this marks a significant step toward
developing authentic virtual counterparts, it also raises critical ethical
concerns regarding privacy, accountability, and the long-term implications of
persistent digital identities. This study contributes to the field of HDTs by
describing our novel system architecture, demonstrating its capabilities, and
discussing future directions and emerging challenges to ensure the responsible
and ethical development of HDTs.

### 46. Hierarchical Memory Organization for Wikipedia Generation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Eugene J. Yu, Dawei Zhu, Yifan Song, Xiangyu Wong, Jiebin Zhang, Wenxuan Shi, Xiaoguang Li, Qun Liu, Sujian Li
- **URL**: <http://arxiv.org/abs/2506.23393v1>
- **Submitted**: 2025-06-29 20:22:49
- **Comment**: ACL 2025 Main Conference
- **Topic Keywords**: rag
- **Reason**: The paper focuses on generating Wikipedia articles using a hierarchical memory architecture, which is not directly related to information retrieval, search technologies, or query understanding. While it involves processing and organizing information, the context is different from the user's primary research interests.

#### Abstract
> Generating Wikipedia articles autonomously is a challenging task requiring
the integration of accurate, comprehensive, and well-structured information
from diverse sources. This paper introduces the Memory Organization-based
Generation (MOG) framework, a novel approach to address these challenges by
leveraging a hierarchical memory architecture. MOG extracts fine-grained memory
units from web documents, recursively organizes them into a Wikipedia-style
hierarchical structure, and uses this structure to guide the generation
process. This ensures alignment between memory and the article outline,
improving both informativeness and verifiability while minimizing
hallucinations. Additionally, a citation module is implemented to enhance
traceability by linking every generated sentence to specific memory units.
Evaluations on our newly created WikiStart dataset demonstrate that MOG
outperforms baseline methods in producing informative and reliable articles,
making it particularly robust in real-world scenarios.

### 47. You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Paige Tutt√∂s√≠, H. Henny Yeung, Yue Wang, Jean-Julien Aucouturier, Angelica Lim
- **URL**: <http://arxiv.org/abs/2506.23367v1>
- **Submitted**: 2025-06-29 18:55:05
- **Comment**: Accepted to ISCA Speech Synthesis Workshop, 2025
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The topic of text-to-speech systems for second language speakers is outside your primary focus and does not align with your research themes.

#### Abstract
> We present the first text-to-speech (TTS) system tailored to second language
(L2) speakers. We use duration differences between American English tense
(longer) and lax (shorter) vowels to create a "clarity mode" for Matcha-TTS.
Our perception studies showed that French-L1, English-L2 listeners had fewer
(at least 9.15%) transcription errors when using our clarity mode, and found it
more encouraging and respectful than overall slowed down speech. Remarkably,
listeners were not aware of these effects: despite the decreased word error
rate in clarity mode, listeners still believed that slowing all target words
was the most intelligible, suggesting that actual intelligibility does not
correlate with perceived intelligibility. Additionally, we found that
Whisper-ASR did not use the same cues as L2 speakers to differentiate difficult
vowels and is not sufficient to assess the intelligibility of TTS systems for
these individuals.

### 48. Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Shouvon Sarker, Xishuang Dong, Lijun Qian
- **URL**: <http://arxiv.org/abs/2506.23315v1>
- **Submitted**: 2025-06-29 16:17:17
- **Topic Keywords**: ctr
- **Reason**: The paper focuses on a specific application of BERT in the clinical domain, using electronic health records, which is not directly related to information retrieval, search technologies, or query understanding. Although it involves natural language processing, the context is distinct from the user's primary research interests.

#### Abstract
> Identification of key variables such as medications, diseases, relations from
health records and clinical notes has a wide range of applications in the
clinical domain. n2c2 2022 provided shared tasks on challenges in natural
language processing for clinical data analytics on electronic health records
(EHR), where it built a comprehensive annotated clinical data Contextualized
Medication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of
this challenge that is to detect and classify medication events from clinical
notes through building a novel BERT-based ensemble model. It started with
pretraining BERT models on different types of big data such as Wikipedia and
MIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED
training data. These fine-tuned BERT models were employed to accomplish
medication event classification on CMED testing data with multiple predictions.
These multiple predictions generated by these fine-tuned BERT models were
integrated to build final prediction with voting strategies. Experimental
results demonstrated that BERT-based ensemble models can effectively improve
strict Micro-F score by about 5% and strict Macro-F score by about 6%,
respectively.

### 49. Two Spelling Normalization Approaches Based on Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Miguel Domingo, Francisco Casacuberta
- **URL**: <http://arxiv.org/abs/2506.23288v1>
- **Submitted**: 2025-06-29 15:25:09
- **Topic Keywords**: rag
- **Reason**: The paper focuses on spelling normalization in historical documents, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves language models, the context and application are quite different from the user's areas of focus.

#### Abstract
> The absence of standardized spelling conventions and the organic evolution of
human language present an inherent linguistic challenge within historical
documents, a longstanding concern for scholars in the humanities. Addressing
this issue, spelling normalization endeavors to align a document's orthography
with contemporary standards. In this study, we propose two new approaches based
on large language models: one of which has been trained without a supervised
training, and a second one which has been trained for machine translation. Our
evaluation spans multiple datasets encompassing diverse languages and
historical periods, leading us to the conclusion that while both of them
yielded encouraging results, statistical machine translation still seems to be
the most suitable technology for this task.

### 50. Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Shivam Sharma, Tanmoy Chakraborty
- **URL**: <http://arxiv.org/abs/2506.23122v1>
- **Submitted**: 2025-06-29 07:12:11
- **Comment**: This work has been submitted to the IEEE for possible publication
- **Topic Keywords**: ctr
- **Reason**: The paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The topic of meme analysis and narrative role classification is outside your primary focus areas of IR, NLP, and data mining.

#### Abstract
> This work investigates the challenging task of identifying narrative roles -
Hero, Villain, Victim, and Other - in Internet memes, across three diverse test
sets spanning English and code-mixed (English-Hindi) languages. Building on an
annotated dataset originally skewed toward the 'Other' class, we explore a more
balanced and linguistically diverse extension, originally introduced as part of
the CLEF 2024 shared task. Comprehensive lexical and structural analyses
highlight the nuanced, culture-specific, and context-rich language used in real
memes, in contrast to synthetically curated hateful content, which exhibits
explicit and repetitive lexical markers. To benchmark the role detection task,
we evaluate a wide spectrum of models, including fine-tuned multilingual
transformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs,
and multimodal vision-language models. Performance is assessed under zero-shot
settings using precision, recall, and F1 metrics. While larger models like
DeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent
challenges in reliably identifying the 'Victim' class and generalising across
cultural and code-mixed content. We also explore prompt design strategies to
guide multimodal models and find that hybrid prompts incorporating structured
instructions and role definitions offer marginal yet consistent improvements.
Our findings underscore the importance of cultural grounding, prompt
engineering, and multimodal reasoning in modelling subtle narrative framings in
visual-textual content.

---

