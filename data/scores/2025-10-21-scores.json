[
    {
        "title": "DSEBench: A Test Collection for Explainable Dataset Search with Examples",
        "abstract": "Dataset search has been an established information retrieval task. Current\nparadigms either retrieve datasets that are relevant to a keyword query or find\ndatasets that are similar to an input target dataset. To allow for their\ncombined specification of information needs, in this article, we investigate\nthe more generalized task of Dataset Search with Examples (DSE) and further\nextend it to Explainable DSE that requires identifying the metadata and content\nfields of a dataset that indicate its relevance to the query and similarity to\nthe target datasets. To facilitate this research, we construct DSEBench, a test\ncollection that provides high-quality dataset- and field-level annotations to\nenable the evaluation of explainable DSE. We also employ a large language model\nto generate numerous annotations to be used for training. We establish\nextensive baselines on DSEBench by adapting and evaluating a variety of sparse,\ndense, and LLM-based retrieval, reranking, and explanation methods.",
        "url": "http://arxiv.org/abs/2510.17228v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17228v1",
        "arxiv_id": "2510.17228v1",
        "authors": [
            "Qing Shi",
            "Jing He",
            "Qiaosheng Chen",
            "Gong Cheng"
        ],
        "submitted": "2025-10-20 07:19:47",
        "source": "arxiv",
        "comment": "34 pages, 5 figures, submitted to Knowledge-Based Systems",
        "score": 19,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 7,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval, particularly in the area of dataset search. Although it focuses on explainable dataset search, it involves retrieval and reranking methods, which align with your interests in query understanding and ranking models. However, the specific domain of dataset search is not a central match to your primary focus on information retrieval in e-commerce and deep semantic understanding."
    },
    {
        "title": "How role-play shapes relevance judgment in zero-shot LLM rankers",
        "abstract": "Large Language Models (LLMs) have emerged as promising zero-shot rankers, but\ntheir performance is highly sensitive to prompt formulation. In particular,\nrole-play prompts, where the model is assigned a functional role or identity,\noften give more robust and accurate relevance rankings. However, the mechanisms\nand diversity of role-play effects remain underexplored, limiting both\neffective use and interpretability. In this work, we systematically examine how\nrole-play variations influence zero-shot LLM rankers. We employ causal\nintervention techniques from mechanistic interpretability to trace how\nrole-play information shapes relevance judgments in LLMs. Our analysis reveals\nthat (1) careful formulation of role descriptions have a large effect on the\nranking quality of the LLM; (2) role-play signals are predominantly encoded in\nearly layers and communicate with task instructions in middle layers, while\nreceiving limited interaction with query or document representations.\nSpecifically, we identify a group of attention heads that encode information\ncritical for role-conditioned relevance. These findings not only shed light on\nthe inner workings of role-play in LLM ranking but also offer guidance for\ndesigning more effective prompts in IR and beyond, pointing toward broader\nopportunities for leveraging role-play in zero-shot applications.",
        "url": "http://arxiv.org/abs/2510.17535v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17535v1",
        "arxiv_id": "2510.17535v1",
        "authors": [
            "Yumeng Wang",
            "Jirui Qi",
            "Catherine Chen",
            "Panagiotis Eustratiadis",
            "Suzan Verberne"
        ],
        "submitted": "2025-10-20 13:39:48",
        "source": "arxiv",
        "comment": null,
        "score": 12,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper explores the role of role-play prompts in zero-shot Large Language Model (LLM) rankers, which is closely related to query understanding and ranking models in Information Retrieval. The paper's focus on the inner workings of LLMs and the design of effective prompts aligns with your research interests in IR and NLP. While the paper's specific domain is not e-commerce, its relevance to IR and ranking models makes it a useful contribution to your field."
    },
    {
        "title": "Rethinking On-policy Optimization for Query Augmentation",
        "abstract": "Recent advances in large language models (LLMs) have led to a surge of\ninterest in query augmentation for information retrieval (IR). Two main\napproaches have emerged. The first prompts LLMs to generate answers or\npseudo-documents that serve as new queries, relying purely on the model's\nparametric knowledge or contextual information. The second applies\nreinforcement learning (RL) to fine-tune LLMs for query rewriting, directly\noptimizing retrieval metrics. While having respective advantages and\nlimitations, the two approaches have not been compared under consistent\nexperimental conditions. In this work, we present the first systematic\ncomparison of prompting-based and RL-based query augmentation across diverse\nbenchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key\nfinding is that simple, training-free query augmentation often performs on par\nwith, or even surpasses, more expensive RL-based counterparts, especially when\nusing powerful LLMs. Motivated by this discovery, we introduce a novel hybrid\nmethod, On-policy Pseudo-document Query Expansion (OPQE), which, instead of\nrewriting a query, the LLM policy learns to generate a pseudo-document that\nmaximizes retrieval performance, thus merging the flexibility and generative\nstructure of prompting with the targeted optimization of RL. We show OPQE\noutperforms both standalone prompting and RL-based rewriting, demonstrating\nthat a synergistic approach yields the best results. Our implementation is made\navailable to facilitate reproducibility.",
        "url": "http://arxiv.org/abs/2510.17139v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17139v1",
        "arxiv_id": "2510.17139v1",
        "authors": [
            "Zhichao Xu",
            "Shengyao Zhuang",
            "Xueguang Ma",
            "Bingsen Chen",
            "Yijun Tian",
            "Fengran Mo",
            "Jie Cao",
            "Vivek Srikumar"
        ],
        "submitted": "2025-10-20 04:16:28",
        "source": "arxiv",
        "comment": null,
        "score": 11,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly query augmentation and ranking models. The work compares and combines different approaches to query augmentation, which is a key area of focus for you. The paper's emphasis on large language models and retrieval metrics also aligns with your interests."
    },
    {
        "title": "OG-Rank: Learning to Rank Fast and Slow with Uncertainty and Reward-Trend Guided Adaptive Exploration",
        "abstract": "Clinicians need ranking systems that work in real time and still justify\ntheir choices. Motivated by the need for a low-latency, decoder-based reranker,\nwe present OG-Rank, a single-decoder approach that pairs a pooled first-token\nscoring signal with an uncertainty-gated explanation step. The model scores all\ncandidates in one pass and generates a brief, structured rationale only when\nthe list is genuinely ambiguous, keeping latency predictable. Trained with a\ncurriculum that concentrates effort on hard cases, OG-Rank delivers strong\neffectiveness on encounter-scoped order selection (fast path: Recall@1~0.45,\nnDCG@20~0.625) and improves further when the gate activates (Recall@1~0.56,\nnDCG@20~0.699 at a 45\\% gate rate), while compact backbones show similar gains\nunder the same policy. Encoder baselines trail in both effectiveness and\nflexibility. The result is a practical recipe: rank fast by default and explain\nwhen it helps, a pattern that applies broadly to decision tasks where selective\ngeneration buys accuracy at acceptable cost. The single-policy design\nsimplifies deployment and budget planning, and the curriculum principle (spend\nmore on the hard cases, less on the easy ones) readily transfers beyond\nclinical order selection.",
        "url": "http://arxiv.org/abs/2510.17614v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17614v1",
        "arxiv_id": "2510.17614v1",
        "authors": [
            "Praphul Singh",
            "Corey Barrett",
            "Sumana Srivasta",
            "Irfan Bulu",
            "Sri Gadde",
            "Krishnaram Kenthapadi"
        ],
        "submitted": "2025-10-20 15:00:02",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'learning to rank' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper presents a ranking system, OG-Rank, which is relevant to the field of Information Retrieval, particularly in the context of real-time relevance optimization. However, the focus on clinical order selection and decoder-based reranking diverges from the user's primary interest in e-commerce and deep semantic understanding. The paper's emphasis on uncertainty-gated explanation and selective generation is somewhat related to user behavior modeling, but it is not a central match."
    },
    {
        "title": "Agentic Reinforcement Learning for Search is Unsafe",
        "abstract": "Agentic reinforcement learning (RL) trains large language models to\nautonomously call tools during reasoning, with search as the most common\napplication. These models excel at multi-step reasoning tasks, but their safety\nproperties are not well understood. In this study, we show that RL-trained\nsearch models inherit refusal from instruction tuning and often deflect harmful\nrequests by turning them into safe queries. However, this safety is fragile.\nTwo simple attacks, one that forces the model to begin response with search\n(Search attack), another that encourages models to repeatedly search\n(Multi-search attack), trigger cascades of harmful searches and answers. Across\ntwo model families (Qwen, Llama) with both local and web search, these attacks\nlower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query\nsafety by 82.4%. The attacks succeed by triggering models to generate harmful,\nrequest-mirroring search queries before they can generate the inherited refusal\ntokens. This exposes a core weakness of current RL training: it rewards\ncontinued generation of effective queries without accounting for their\nharmfulness. As a result, RL search models have vulnerabilities that users can\neasily exploit, making it urgent to develop safety-aware agentic RL pipelines\noptimising for safe search.",
        "url": "http://arxiv.org/abs/2510.17431v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17431v1",
        "arxiv_id": "2510.17431v1",
        "authors": [
            "Yushi Yang",
            "Shreyansh Padarha",
            "Andrew Lee",
            "Adam Mahdi"
        ],
        "submitted": "2025-10-20 11:19:37",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'web search' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of search technologies and query understanding. The focus on safety properties of search models and the potential for users to exploit vulnerabilities is a timely and important concern in the field. While not directly related to ranking models or user behavior modeling, the paper's emphasis on search and query generation aligns with your broader interests in IR and NLP."
    },
    {
        "title": "Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation",
        "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for\nenhancing large language models (LLMs) by retrieving relevant documents from an\nexternal corpus. However, existing RAG systems primarily focus on unimodal text\ndocuments, and often fall short in real-world scenarios where both queries and\ndocuments may contain mixed modalities (such as text and images). In this\npaper, we address the challenge of Universal Retrieval-Augmented Generation\n(URAG), which involves retrieving and reasoning over mixed-modal information to\nimprove vision-language generation. To this end, we propose Nyx, a unified\nmixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate\nthe scarcity of realistic mixed-modal data, we introduce a four-stage automated\npipeline for generation and filtering, leveraging web documents to construct\nNyxQA, a dataset comprising diverse mixed-modal question-answer pairs that\nbetter reflect real-world information needs. Building on this high-quality\ndataset, we adopt a two-stage training framework for Nyx: we first perform\npre-training on NyxQA along with a variety of open-source retrieval datasets,\nfollowed by supervised fine-tuning using feedback from downstream\nvision-language models (VLMs) to align retrieval outputs with generative\npreferences. Experimental results demonstrate that Nyx not only performs\ncompetitively on standard text-only RAG benchmarks, but also excels in the more\ngeneral and realistic URAG setting, significantly improving generation quality\nin vision-language tasks.",
        "url": "http://arxiv.org/abs/2510.17354v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17354v1",
        "arxiv_id": "2510.17354v1",
        "authors": [
            "Chenghao Zhang",
            "Guanting Dong",
            "Xinyu Yang",
            "Zhicheng Dou"
        ],
        "submitted": "2025-10-20 09:56:43",
        "source": "arxiv",
        "comment": "This work is in progress",
        "score": 10,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your interests in Information Retrieval and Search technologies, as it involves retrieval-augmented generation and mixed-modal information. However, the focus on vision-language generation and mixed-modal data is not a central match to your primary research themes. The paper's use of deep semantic understanding and real-time relevance optimization is relevant, but not the primary focus."
    },
    {
        "title": "Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains",
        "abstract": "Finetuning specialized generative evaluators has emerged as a popular\nparadigm to meet the increasing demand for scalable evaluation during both\ntraining and test-time. However, recent work has largely focused on applying\nnew methodology, such as reinforcement learning (RL), to training evaluators,\nshying away from large-scale, data-driven development. In this work, we focus\non data scaling, curating a set of 2.5M samples spanning five unique evaluation\ntasks (pairwise, step-level, reference-free and reference-based verification,\nand single rating) and multiple domains focused on reasoning evaluation. With\nour data, we train Foundational Automatic Reasoning Evaluators (FARE), a family\nof 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative\nrejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges\nlarger specialized RL-trained evaluators and FARE-20B sets the new standard for\nopen-source evaluators, surpassing specialized 70B+ evaluators. Beyond static\nbenchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers,\nFARE-20B achieves near-oracle performance on MATH. As verifiers in RL training,\nFARE improves the downstream RL-trained model performance by up to 14.1% vs.\nstring-matching verifiers. When initialized from FARE, a continually-finetuned\nFARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.",
        "url": "http://arxiv.org/abs/2510.17793v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17793v1",
        "arxiv_id": "2510.17793v1",
        "authors": [
            "Austin Xu",
            "Xuan-Phi Nguyen",
            "Yilun Zhou",
            "Chien-Sheng Wu",
            "Caiming Xiong",
            "Shafiq Joty"
        ],
        "submitted": "2025-10-20 17:52:06",
        "source": "arxiv",
        "comment": "29 pages, 9 tables, 6 figures",
        "score": 9,
        "keyword_reasons": [
            "Found 'rerank' (score: +3)",
            "Found 'pairwise' (score: +3)",
            "Found 'rank' (score: +1)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on generative evaluators for reasoning-centric domains, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves training and evaluation, the context is more aligned with AI evaluation and model assessment rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "AtlasKV: Augmenting LLMs with Billion-Scale Knowledge Graphs in 20GB VRAM",
        "abstract": "Retrieval-augmented generation (RAG) has shown some success in augmenting\nlarge language models (LLMs) with external knowledge. However, as a\nnon-parametric knowledge integration paradigm for LLMs, RAG methods heavily\nrely on external retrieval modules and the retrieved textual context prior.\nEspecially for very large scale knowledge augmentation, they would introduce\nsubstantial inference latency due to expensive searches and much longer\nrelevant context. In this paper, we propose a parametric knowledge integration\nmethod, called \\textbf{AtlasKV}, a scalable, effective, and general way to\naugment LLMs with billion-scale knowledge graphs (KGs) (e.g. 1B triples) using\nvery little GPU memory cost (e.g. less than 20GB VRAM). In AtlasKV, we\nintroduce KG2KV and HiKVP to integrate KG triples into LLMs at scale with\nsub-linear time and memory complexity. It maintains strong knowledge grounding\nand generalization performance using the LLMs' inherent attention mechanism,\nand requires no external retrievers, long context priors, or retraining when\nadapting to new knowledge.",
        "url": "http://arxiv.org/abs/2510.17934v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17934v1",
        "arxiv_id": "2510.17934v1",
        "authors": [
            "Haoyu Huang",
            "Hong Ting Tsang",
            "Jiaxin Bai",
            "Xi Peng",
            "Gong Zhang",
            "Yangqiu Song"
        ],
        "submitted": "2025-10-20 15:40:14",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper proposes a method to augment large language models with knowledge graphs, which is related to information retrieval and NLP. However, the focus is on knowledge integration and scalability, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest. While it touches on the use of large-scale knowledge, it does not directly address real-time relevance optimization or deep semantic understanding."
    },
    {
        "title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action",
        "abstract": "Multimodal agents for computer use rely exclusively on primitive actions\n(click, type, scroll) that require accurate visual grounding and lengthy\nexecution chains, leading to cascading failures and performance bottlenecks.\nWhile other agents leverage rich programmatic interfaces (APIs, MCP servers,\ntools), computer-use agents (CUAs) remain isolated from these capabilities. We\npresent UltraCUA, a foundation model that bridges this gap through hybrid\naction -- seamlessly integrating GUI primitives with high-level programmatic\ntool calls. To achieve this, our approach comprises four key components: (1) an\nautomated pipeline that scales programmatic tools from software documentation,\nopen-source repositories, and code generation; (2) a synthetic data engine\nproducing over 17,000 verifiable tasks spanning real-world computer-use\nscenarios; (3) a large-scale high-quality hybrid action trajectory collection\nwith both low-level GUI actions and high-level programmatic tool calls; and (4)\na two-stage training pipeline combining supervised fine-tuning with online\nreinforcement learning, enabling strategic alternation between low-level and\nhigh-level actions. Experiments with our 7B and 32B models demonstrate\nsubstantial improvements over state-of-the-art agents. On OSWorld, UltraCUA\nmodels achieve an average 22% relative improvement over base models, while\nbeing 11% faster in terms of steps. Out-of-domain evaluation on\nWindowsAgentArena shows our model reaches 21.7% success rate, outperforming\nbaselines trained on Windows data. The hybrid action mechanism proves critical,\nreducing error propagation while maintaining execution efficiency.",
        "url": "http://arxiv.org/abs/2510.17790v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17790v1",
        "arxiv_id": "2510.17790v1",
        "authors": [
            "Yuhao Yang",
            "Zhen Yang",
            "Zi-Yi Dou",
            "Anh Nguyen",
            "Keen You",
            "Omar Attia",
            "Andrew Szot",
            "Michael Feng",
            "Ram Ramrakhya",
            "Alexander Toshev",
            "Chao Huang",
            "Yinfei Yang",
            "Zhe Gan"
        ],
        "submitted": "2025-10-20 17:48:26",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'ltr' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'click' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on computer use agents and multimodal interactions, which is not directly related to your core research themes in Information Retrieval and Search technologies. While it involves some form of 'action' and 'trajectory collection', the context is quite different from query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents",
        "abstract": "Large language model (LLM) based search agents iteratively generate queries,\nretrieve external information, and reason to answer open-domain questions.\nWhile researchers have primarily focused on improving their utility, their\nsafety behaviors remain underexplored. In this paper, we first evaluate search\nagents using red-teaming datasets and find that they are more likely to produce\nharmful outputs than base LLMs. For example, when asked \"How can I track\nsomeone's location without their consent?\", a base model refuses, whereas a\nsearch agent designed to retrieve and cite sources may lower its refusal\nthreshold, fetch documents (e.g., court cases), and, once appended, synthesize\nthem into an informative yet unsafe summary. We further show that\nutility-oriented fine-tuning intensifies this risk, motivating joint alignment\nof safety and utility. We present SafeSearch, a multi-objective reinforcement\nlearning approach that couples a final-output safety/utility reward with a\nnovel query-level shaping term that penalizes unsafe queries and rewards safe\nones. Experiments show that SafeSearch reduces agent harmfulness by over 70%\nacross three red-teaming datasets while producing safe, helpful responses, and\nmatches the QA performance of a utility-only finetuned agent; further analyses\nconfirm the effectiveness of the query-level reward in jointly improving safety\nand utility.",
        "url": "http://arxiv.org/abs/2510.17017v2",
        "pdf_url": "http://arxiv.org/pdf/2510.17017v2",
        "arxiv_id": "2510.17017v2",
        "authors": [
            "Qiusi Zhan",
            "Angeline Budiman-Chan",
            "Abdelrahman Zayed",
            "Xingzhi Guo",
            "Daniel Kang",
            "Joo-Kyung Kim"
        ],
        "submitted": "2025-10-19 21:47:19",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your interests in Information Retrieval and Search technologies, particularly in the context of query understanding and ranking models. The focus on safety and utility in Large Language Model (LLM) search agents aligns with your background in e-commerce and your interest in deep semantic understanding. However, the paper's primary focus on safety rather than ranking models or user behavior modeling prevents it from being a perfect match."
    },
    {
        "title": "Lingua Custodi's participation at the WMT 2025 Terminology shared task",
        "abstract": "While BERT is an effective method for learning monolingual sentence\nembeddings for semantic similarity and embedding based transfer learning BERT\nbased cross-lingual sentence embeddings have yet to be explored. We\nsystematically investigate methods for learning multilingual sentence\nembeddings by combining the best methods for learning monolingual and\ncross-lingual representations including: masked language modeling (MLM),\ntranslation language modeling (TLM), dual encoder translation ranking, and\nadditive margin softmax. We show that introducing a pre-trained multilingual\nlanguage model dramatically reduces the amount of parallel training data\nrequired to achieve good performance by 80%. Composing the best of these\nmethods produces a model that achieves 83.7% bi-text retrieval accuracy over\n112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still\nperforming competitively on monolingual transfer learning benchmarks. Parallel\ndata mined from CommonCrawl using our best model is shown to train competitive\nNMT models for en-zh and en-de. We publicly release our best multilingual\nsentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.",
        "url": "http://arxiv.org/abs/2510.17504v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17504v1",
        "arxiv_id": "2510.17504v1",
        "authors": [
            "Jingshu Liu",
            "Raheel Qader",
            "Gaëtan Caillaut",
            "Mariam Nakhlé"
        ],
        "submitted": "2025-10-20 13:00:47",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores multilingual sentence embeddings, which is related to NLP and deep semantic understanding. However, it does not directly focus on information retrieval, query understanding, or ranking models, making it somewhat relevant but not a central match to your research interests."
    },
    {
        "title": "Bits Leaked per Query: Information-Theoretic Bounds on Adversarial Attacks against LLMs",
        "abstract": "Adversarial attacks by malicious users that threaten the safety of large\nlanguage models (LLMs) can be viewed as attempts to infer a target property $T$\nthat is unknown when an instruction is issued, and becomes knowable only after\nthe model's reply is observed. Examples of target properties $T$ include the\nbinary flag that triggers an LLM's harmful response or rejection, and the\ndegree to which information deleted by unlearning can be restored, both\nelicited via adversarial instructions. The LLM reveals an \\emph{observable\nsignal} $Z$ that potentially leaks hints for attacking through a response\ncontaining answer tokens, thinking process tokens, or logits. Yet the scale of\ninformation leaked remains anecdotal, leaving auditors without principled\nguidance and defenders blind to the transparency--risk trade-off. We fill this\ngap with an information-theoretic framework that computes how much information\ncan be safely disclosed, and enables auditors to gauge how close their methods\ncome to the fundamental limit. Treating the mutual information $I(Z;T)$ between\nthe observation $Z$ and the target property $T$ as the leaked bits per query,\nwe show that achieving error $\\varepsilon$ requires at least\n$\\log(1/\\varepsilon)/I(Z;T)$ queries, scaling linearly with the inverse leak\nrate and only logarithmically with the desired accuracy. Thus, even a modest\nincrease in disclosure collapses the attack cost from quadratic to logarithmic\nin terms of the desired accuracy. Experiments on seven LLMs across\nsystem-prompt leakage, jailbreak, and relearning attacks corroborate the\ntheory: exposing answer tokens alone requires about a thousand queries; adding\nlogits cuts this to about a hundred; and revealing the full thinking process\ntrims it to a few dozen. Our results provide the first principled yardstick for\nbalancing transparency and security when deploying LLMs.",
        "url": "http://arxiv.org/abs/2510.17000v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17000v1",
        "arxiv_id": "2510.17000v1",
        "authors": [
            "Masahiro Kaneko",
            "Timothy Baldwin"
        ],
        "submitted": "2025-10-19 20:51:24",
        "source": "arxiv",
        "comment": "NeurIPS 2025 (spotlight)",
        "score": 6,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your core research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves large language models, the focus is on adversarial attacks and security, which is not a primary area of interest for you."
    },
    {
        "title": "Executable Knowledge Graphs for Replicating AI Research",
        "abstract": "Replicating AI research is a crucial yet challenging task for large language\nmodel (LLM) agents. Existing approaches often struggle to generate executable\ncode, primarily due to insufficient background knowledge and the limitations of\nretrieval-augmented generation (RAG) methods, which fail to capture latent\ntechnical details hidden in referenced papers. Furthermore, previous approaches\ntend to overlook valuable implementation-level code signals and lack structured\nknowledge representations that support multi-granular retrieval and reuse. To\novercome these challenges, we propose Executable Knowledge Graphs (xKG), a\nmodular and pluggable knowledge base that automatically integrates technical\ninsights, code snippets, and domain-specific knowledge extracted from\nscientific literature. When integrated into three agent frameworks with two\ndifferent LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on\nPaperBench, demonstrating its effectiveness as a general and extensible\nsolution for automated AI research replication. Code will released at\nhttps://github.com/zjunlp/xKG.",
        "url": "http://arxiv.org/abs/2510.17795v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17795v1",
        "arxiv_id": "2510.17795v1",
        "authors": [
            "Yujie Luo",
            "Zhuoyun Yu",
            "Xuehai Wang",
            "Yuqi Zhu",
            "Ningyu Zhang",
            "Lanning Wei",
            "Lun Du",
            "Da Zheng",
            "Huajun Chen"
        ],
        "submitted": "2025-10-20 17:53:23",
        "source": "arxiv",
        "comment": "Work in progress",
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper proposes Executable Knowledge Graphs for replicating AI research, which involves integrating technical insights and code snippets from scientific literature. While it touches on the topic of information retrieval and knowledge extraction, its primary focus is on replicating AI research rather than query understanding, ranking models, or user behavior modeling. The paper's relevance to your core research themes is limited, but it may be of interest for its application of knowledge extraction techniques."
    },
    {
        "title": "Enterprise Deep Research: Steerable Multi-Agent Deep Research for Enterprise Analytics",
        "abstract": "As information grows exponentially, enterprises face increasing pressure to\ntransform unstructured data into coherent, actionable insights. While\nautonomous agents show promise, they often struggle with domain-specific\nnuances, intent alignment, and enterprise integration. We present Enterprise\nDeep Research (EDR), a multi-agent system that integrates (1) a Master Planning\nAgent for adaptive query decomposition, (2) four specialized search agents\n(General, Academic, GitHub, LinkedIn), (3) an extensible MCP-based tool\necosystem supporting NL2SQL, file analysis, and enterprise workflows, (4) a\nVisualization Agent for data-driven insights, and (5) a reflection mechanism\nthat detects knowledge gaps and updates research direction with optional\nhuman-in-the-loop steering guidance. These components enable automated report\ngeneration, real-time streaming, and seamless enterprise deployment, as\nvalidated on internal datasets. On open-ended benchmarks including DeepResearch\nBench and DeepConsult, EDR outperforms state-of-the-art agentic systems without\nany human steering. We release the EDR framework and benchmark trajectories to\nadvance research on multi-agent reasoning applications.\n  Code at https://github.com/SalesforceAIResearch/enterprise-deep-research and\nDataset at https://huggingface.co/datasets/Salesforce/EDR-200",
        "url": "http://arxiv.org/abs/2510.17797v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17797v1",
        "arxiv_id": "2510.17797v1",
        "authors": [
            "Akshara Prabhakar",
            "Roshan Ram",
            "Zixiang Chen",
            "Silvio Savarese",
            "Frank Wang",
            "Caiming Xiong",
            "Huan Wang",
            "Weiran Yao"
        ],
        "submitted": "2025-10-20 17:55:11",
        "source": "arxiv",
        "comment": "Technical report; 13 pages plus references and appendices",
        "score": 4,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses a multi-agent system for enterprise analytics, which involves search and query understanding. However, the focus is more on the integration of various agents and tools rather than deep semantic understanding or real-time relevance optimization, which are core aspects of your research interests. The connection to information retrieval is somewhat loose, but the use of NLP and search agents makes it somewhat relevant."
    },
    {
        "title": "MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning",
        "abstract": "Misinformation spreads across web platforms through billions of daily\nmultimodal posts that combine text and images, overwhelming manual\nfact-checking capacity. Supervised detection models require domain-specific\ntraining data and fail to generalize across diverse manipulation tactics. We\npresent MIRAGE, an inference-time, model-pluggable agentic framework that\ndecomposes multimodal verification into four sequential modules: visual\nveracity assessment detects AI-generated images, cross-modal consistency\nanalysis identifies out-of-context repurposing, retrieval-augmented factual\nchecking grounds claims in web evidence through iterative question generation,\nand a calibrated judgment module integrates all signals. MIRAGE orchestrates\nvision-language model reasoning with targeted web retrieval, outputs structured\nand citation-linked rationales. On MMFakeBench validation set (1,000 samples),\nMIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming\nthe strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65\npoints while maintaining 34.3% false positive rate versus 97.3% for a\njudge-only baseline. Test set results (5,000 samples) confirm generalization\nwith 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification\ncontributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97\npoints. Our results demonstrate that decomposed agentic reasoning with web\nretrieval can match supervised detector performance without domain-specific\ntraining, enabling misinformation detection across modalities where labeled\ndata remains scarce.",
        "url": "http://arxiv.org/abs/2510.17590v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17590v1",
        "arxiv_id": "2510.17590v1",
        "authors": [
            "Mir Nafis Sharear Shopnil",
            "Sharad Duwal",
            "Abhishek Tyagi",
            "Adiba Mahbub Proma"
        ],
        "submitted": "2025-10-20 14:40:26",
        "source": "arxiv",
        "comment": "16 pages, 3 tables, 1 figure",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses multimodal misinformation detection, which is somewhat related to information retrieval, but the focus is on fact-checking and verification rather than query understanding, ranking models, or user behavior modeling. While it involves web retrieval, the primary goal is not relevance optimization, making it only loosely relevant to the user's core research themes."
    },
    {
        "title": "Disparities in Multilingual LLM-Based Healthcare Q&A",
        "abstract": "Equitable access to reliable health information is vital when integrating AI\ninto healthcare. Yet, information quality varies across languages, raising\nconcerns about the reliability and consistency of multilingual Large Language\nModels (LLMs). We systematically examine cross-lingual disparities in\npre-training source and factuality alignment in LLM answers for multilingual\nhealthcare Q&A across English, German, Turkish, Chinese (Mandarin), and\nItalian. We (i) constructed Multilingual Wiki Health Care\n(MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed\ncross-lingual healthcare coverage; (iii) assessed LLM response alignment with\nthese references; and (iv) conducted a case study on factual alignment through\nthe use of contextual information and Retrieval-Augmented Generation (RAG). Our\nfindings reveal substantial cross-lingual disparities in both Wikipedia\ncoverage and LLM factual alignment. Across LLMs, responses align more with\nEnglish Wikipedia, even when the prompts are non-English. Providing contextual\nexcerpts from non-English Wikipedia at inference time effectively shifts\nfactual alignment toward culturally relevant knowledge. These results highlight\npractical pathways for building more equitable, multilingual AI systems for\nhealthcare.",
        "url": "http://arxiv.org/abs/2510.17476v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17476v1",
        "arxiv_id": "2510.17476v1",
        "authors": [
            "Ipek Baris Schlicht",
            "Burcu Sayin",
            "Zhixue Zhao",
            "Frederik M. Labonté",
            "Cesare Barbera",
            "Marco Viviani",
            "Paolo Rosso",
            "Lucie Flek"
        ],
        "submitted": "2025-10-20 12:19:08",
        "source": "arxiv",
        "comment": "Under review",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, particularly in the context of multilingual Large Language Models. However, it focuses on healthcare Q&A and disparities in multilingual LLMs, which is not a central match to your primary focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Efficient Toxicity Detection in Gaming Chats: A Comparative Study of Embeddings, Fine-Tuned Transformers and LLMs",
        "abstract": "This paper presents a comprehensive comparative analysis of Natural Language\nProcessing (NLP) methods for automated toxicity detection in online gaming\nchats. Traditional machine learning models with embeddings, large language\nmodels (LLMs) with zero-shot and few-shot prompting, fine-tuned transformer\nmodels, and retrieval-augmented generation (RAG) approaches are evaluated. The\nevaluation framework assesses three critical dimensions: classification\naccuracy, processing speed, and computational costs. A hybrid moderation system\narchitecture is proposed that optimizes human moderator workload through\nautomated detection and incorporates continuous learning mechanisms. The\nexperimental results demonstrate significant performance variations across\nmethods, with fine-tuned DistilBERT achieving optimal accuracy-cost trade-offs.\nThe findings provide empirical evidence for deploying cost-effective, efficient\ncontent moderation systems in dynamic online gaming environments.",
        "url": "http://arxiv.org/abs/2510.17924v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17924v1",
        "arxiv_id": "2510.17924v1",
        "authors": [
            "Yehor Tereshchenko",
            "Mika Hämäläinen"
        ],
        "submitted": "2025-10-20 08:03:28",
        "source": "arxiv",
        "comment": "Published in the Journal of Data Mining & Digital Humanities (JDMDH),\n  special issue NLP4DH",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and Information Retrieval (IR), particularly in the context of query understanding and ranking models. However, the focus on toxicity detection in gaming chats is somewhat narrow and not directly aligned with the user's core research themes in e-commerce or real-time relevance optimization. The use of embeddings, fine-tuned transformers, and LLMs is relevant to the user's interests in NLP and IR, but the application is specific to content moderation."
    },
    {
        "title": "PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits",
        "abstract": "Large Language Models (LLMs) are improving at an exceptional rate. With the\nadvent of agentic workflows, multi-turn dialogue has become the de facto mode\nof interaction with LLMs for completing long and complex tasks. While LLM\ncapabilities continue to improve, they remain increasingly susceptible to\njailbreaking, especially in multi-turn scenarios where harmful intent can be\nsubtly injected across the conversation to produce nefarious outcomes. While\nsingle-turn attacks have been extensively explored, adaptability, efficiency\nand effectiveness continue to remain key challenges for their multi-turn\ncounterparts. To address these gaps, we present PLAGUE, a novel plug-and-play\nframework for designing multi-turn attacks inspired by lifelong-learning\nagents. PLAGUE dissects the lifetime of a multi-turn attack into three\ncarefully designed phases (Primer, Planner and Finisher) that enable a\nsystematic and information-rich exploration of the multi-turn attack family.\nEvaluations show that red-teaming agents designed using PLAGUE achieve\nstate-of-the-art jailbreaking results, improving attack success rates (ASR) by\nmore than 30% across leading models in a lesser or comparable query budget.\nParticularly, PLAGUE enables an ASR (based on StrongReject) of 81.4% on\nOpenAI's o3 and 67.3% on Claude's Opus 4.1, two models that are considered\nhighly resistant to jailbreaks in safety literature. Our work offers tools and\ninsights to understand the importance of plan initialization, context\noptimization and lifelong learning in crafting multi-turn attacks for a\ncomprehensive model vulnerability evaluation.",
        "url": "http://arxiv.org/abs/2510.17947v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17947v1",
        "arxiv_id": "2510.17947v1",
        "authors": [
            "Neeladri Bhuiya",
            "Madhav Aggarwal",
            "Diptanshu Purwar"
        ],
        "submitted": "2025-10-20 17:37:03",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on multi-turn attacks and jailbreaking of Large Language Models, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on the interaction with LLMs, it's more aligned with security and vulnerability evaluation rather than deep semantic understanding or real-time relevance optimization."
    },
    {
        "title": "LILO: Bayesian Optimization with Interactive Natural Language Feedback",
        "abstract": "For many real-world applications, feedback is essential in translating\ncomplex, nuanced, or subjective goals into quantifiable optimization\nobjectives. We propose a language-in-the-loop framework that uses a large\nlanguage model (LLM) to convert unstructured feedback in the form of natural\nlanguage into scalar utilities to conduct BO over a numeric search space.\nUnlike preferential BO, which only accepts restricted feedback formats and\nrequires customized models for each domain-specific problem, our approach\nleverages LLMs to turn varied types of textual feedback into consistent utility\nsignals and to easily include flexible user priors without manual kernel\ndesign. At the same time, our method maintains the sample efficiency and\nprincipled uncertainty quantification of BO. We show that this hybrid method\nnot only provides a more natural interface to the decision maker but also\noutperforms conventional BO baselines and LLM-only optimizers, particularly in\nfeedback-limited regimes.",
        "url": "http://arxiv.org/abs/2510.17671v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17671v1",
        "arxiv_id": "2510.17671v1",
        "authors": [
            "Katarzyna Kobalczyk",
            "Zhiyuan Jerry Lin",
            "Benjamin Letham",
            "Zhuokai Zhao",
            "Maximilian Balandat",
            "Eytan Bakshy"
        ],
        "submitted": "2025-10-20 15:41:56",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper aligns well with your interests in Information Retrieval, particularly in query understanding and user behavior modeling, as it explores a language-in-the-loop framework for converting natural language feedback into optimization objectives. The use of large language models and Bayesian optimization also relates to your interests in NLP and data mining. However, the focus on optimization objectives and decision-making may not be a central match with your primary focus on information retrieval."
    },
    {
        "title": "On-the-Fly OVD Adaptation with FLAME: Few-shot Localization via Active Marginal-Samples Exploration",
        "abstract": "Open-vocabulary object detection (OVD) models offer remarkable flexibility by\ndetecting objects from arbitrary text queries. However, their zero-shot\nperformance in specialized domains like Remote Sensing (RS) is often\ncompromised by the inherent ambiguity of natural language, limiting critical\ndownstream applications. For instance, an OVD model may struggle to distinguish\nbetween fine-grained classes such as \"fishing boat\" and \"yacht\" since their\nembeddings are similar and often inseparable. This can hamper specific user\ngoals, such as monitoring illegal fishing, by producing irrelevant detections.\nTo address this, we propose a cascaded approach that couples the broad\ngeneralization of a large pre-trained OVD model with a lightweight few-shot\nclassifier. Our method first employs the zero-shot model to generate\nhigh-recall object proposals. These proposals are then refined for high\nprecision by a compact classifier trained in real-time on only a handful of\nuser-annotated examples - drastically reducing the high costs of RS imagery\nannotation.The core of our framework is FLAME, a one-step active learning\nstrategy that selects the most informative samples for training. FLAME\nidentifies, on the fly, uncertain marginal candidates near the decision\nboundary using density estimation, followed by clustering to ensure sample\ndiversity. This efficient sampling technique achieves high accuracy without\ncostly full-model fine-tuning and enables instant adaptation, within less then\na minute, which is significantly faster than state-of-the-art alternatives.Our\nmethod consistently surpasses state-of-the-art performance on RS benchmarks,\nestablishing a practical and resource-efficient framework for adapting\nfoundation models to specific user needs.",
        "url": "http://arxiv.org/abs/2510.17670v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17670v1",
        "arxiv_id": "2510.17670v1",
        "authors": [
            "Yehonathan Refael",
            "Amit Aides",
            "Aviad Barzilai",
            "George Leifman",
            "Genady Beryozkin",
            "Vered Silverman",
            "Bolous Jaber",
            "Tomer Shekel"
        ],
        "submitted": "2025-10-20 15:41:55",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is primarily focused on object detection and active learning in the context of Remote Sensing, which is not a central match to your research interests in Information Retrieval, Search technologies, and Natural Language Processing."
    },
    {
        "title": "Qomhra: A Bilingual Irish-English Large Language Model",
        "abstract": "This paper introduces Qomhr\\'a, a bilingual Irish-English large language\nmodel (LLM), developed under low-resource constraints presenting a complete\npipeline spanning bilingual continued pre-training, instruction tuning, and\nalignment from human preferences. Newly accessible Irish corpora and English\ntext are mixed and curated to improve Irish performance while preserving\nEnglish ability. 6 closed-weight LLMs are judged for their Irish text\ngeneration by a native speaker, a learner and other LLMs. Google's\nGemini-2.5-Pro is ranked the highest and is subsequently used to synthesise\ninstruction tuning and human preference datasets. Two datasets are contributed\nleveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning\ndataset and a 1K human preference dataset, generating accepted and rejected\nresponses that show near perfect alignment with a native Irish speaker.\nQomhr\\'a is comprehensively evaluated across benchmarks testing translation,\ngender understanding, topic identification and world knowledge with gains of up\nto 29% in Irish and 44% in English. Qomhr\\'a also undergoes instruction tuning\nand demonstrates clear progress in instruction following, crucial for chatbot\nfunctionality.",
        "url": "http://arxiv.org/abs/2510.17652v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17652v1",
        "arxiv_id": "2510.17652v1",
        "authors": [
            "Joseph McInerney"
        ],
        "submitted": "2025-10-20 15:27:53",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on developing a bilingual language model for Irish and English, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the specific application and goals are distinct from the user's research interests."
    },
    {
        "title": "LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis",
        "abstract": "Legal reasoning is a fundamental component of legal analysis and\ndecision-making. Existing computational approaches to legal reasoning\npredominantly rely on generic reasoning frameworks such as syllogism and IRAC,\nwhich do not comprehensively examine the nuanced processes that underpin legal\nreasoning. Moreover, current research has largely focused on criminal cases,\nwith insufficient modeling for civil cases. In this work, we present a novel\nframework for explicitly modeling legal reasoning in the analysis of Chinese\ntort-related civil cases. We first operationalize the legal reasoning processes\nused in tort analysis into the LawChain framework. LawChain is a three-module\nreasoning framework, with each module consisting of multiple finer-grained\nsub-steps. Informed by the LawChain framework, we introduce the task of tort\nlegal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to\nsystematically assess the critical steps within analytical reasoning chains for\ntort analysis. Leveraging this benchmark, we evaluate state-of-the-art large\nlanguage models for their legal reasoning ability in civil tort contexts. Our\nresults indicate that current models still fall short in accurately handling\ncrucial elements of tort legal reasoning. Furthermore, we introduce several\nbaseline approaches that explicitly incorporate LawChain-style reasoning\nthrough prompting or post-training. We conduct further experiments on\nadditional legal analysis tasks, such as Legal Named-Entity Recognition and\nCriminal Damages Calculation, to verify the generalizability of these\nbaselines. The proposed baseline approaches achieve significant improvements in\ntort-related legal reasoning and generalize well to related legal analysis\ntasks, thus demonstrating the value of explicitly modeling legal reasoning\nchains to enhance the reasoning capabilities of language models.",
        "url": "http://arxiv.org/abs/2510.17602v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17602v1",
        "arxiv_id": "2510.17602v1",
        "authors": [
            "Huiyuan Xie",
            "Chenyang Li",
            "Huining Zhu",
            "Chubin Zhang",
            "Yuxiao Ye",
            "Zhenghao Liu",
            "Zhiyuan Liu"
        ],
        "submitted": "2025-10-20 14:50:58",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is primarily focused on legal reasoning and its application in tort case analysis, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves language models, the context is legal analysis rather than search or retrieval."
    },
    {
        "title": "Deep Self-Evolving Reasoning",
        "abstract": "Long-form chain-of-thought reasoning has become a cornerstone of advanced\nreasoning in large language models. While recent verification-refinement\nframeworks have enabled proprietary models to solve Olympiad-level problems,\ntheir effectiveness hinges on strong, reliable verification and correction\ncapabilities, which remain fragile in open-weight, smaller-scale models. This\nwork demonstrates that even with weak verification and refinement capabilities\non hard tasks, the reasoning limits of such models can be substantially\nextended through a probabilistic paradigm we call Deep Self-Evolving Reasoning\n(DSER). We conceptualize iterative reasoning as a Markov chain, where each step\nrepresents a stochastic transition in the solution space. The key insight is\nthat convergence to a correct solution is guaranteed as long as the probability\nof improvement marginally exceeds that of degradation. By running multiple\nlong-horizon, self-evolving processes in parallel, DSER amplifies these small\npositive tendencies, enabling the model to asymptotically approach correct\nanswers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On\nthe challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously\nunsolvable problems and boosts overall performance, enabling this compact model\nto surpass the single-turn accuracy of its 600B-parameter teacher through\nmajority voting. Beyond its immediate utility for test-time scaling, the DSER\nframework serves to diagnose the fundamental limitations of current open-weight\nreasoners. By clearly delineating their shortcomings in self-verification,\nrefinement, and stability, our findings establish a clear research agenda for\ndeveloping next-generation models with powerful, intrinsic self-evolving\ncapabilities.",
        "url": "http://arxiv.org/abs/2510.17498v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17498v1",
        "arxiv_id": "2510.17498v1",
        "authors": [
            "Zihan Liu",
            "Shun Zheng",
            "Xumeng Wen",
            "Yang Wang",
            "Jiang Bian",
            "Mao Yang"
        ],
        "submitted": "2025-10-20 12:51:42",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on developing a framework for large language models to improve their reasoning capabilities through self-evolving reasoning. While it touches on the idea of iterative reasoning, it doesn't directly relate to information retrieval, search technologies, or user behavior modeling, which are core areas of interest for you."
    },
    {
        "title": "Multilingual Clinical NER for Diseases and Medications Recognition in Cardiology Texts using BERT Embeddings",
        "abstract": "The rapidly increasing volume of electronic health record (EHR) data\nunderscores a pressing need to unlock biomedical knowledge from unstructured\nclinical texts to support advancements in data-driven clinical systems,\nincluding patient diagnosis, disease progression monitoring, treatment effects\nassessment, prediction of future clinical events, etc. While contextualized\nlanguage models have demonstrated impressive performance improvements for named\nentity recognition (NER) systems in English corpora, there remains a scarcity\nof research focused on clinical texts in low-resource languages. To bridge this\ngap, our study aims to develop multiple deep contextual embedding models to\nenhance clinical NER in the cardiology domain, as part of the BioASQ\nMultiCardioNER shared task. We explore the effectiveness of different\nmonolingual and multilingual BERT-based models, trained on general domain text,\nfor extracting disease and medication mentions from clinical case reports\nwritten in English, Spanish, and Italian. We achieved an F1-score of 77.88% on\nSpanish Diseases Recognition (SDR), 92.09% on Spanish Medications Recognition\n(SMR), 91.74% on English Medications Recognition (EMR), and 88.9% on Italian\nMedications Recognition (IMR). These results outperform the mean and median F1\nscores in the test leaderboard across all subtasks, with the mean/median values\nbeing: 69.61%/75.66% for SDR, 81.22%/90.18% for SMR, 89.2%/88.96% for EMR, and\n82.8%/87.76% for IMR.",
        "url": "http://arxiv.org/abs/2510.17437v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17437v1",
        "arxiv_id": "2510.17437v1",
        "authors": [
            "Manuela Daniela Danu",
            "George Marica",
            "Constantin Suciu",
            "Lucian Mihai Itu",
            "Oladimeji Farri"
        ],
        "submitted": "2025-10-20 11:26:22",
        "source": "arxiv",
        "comment": "11 pages, 5 figures, 1 table, published in Working Notes of the\n  Conference and Labs of the Evaluation Forum (CLEF 2024)",
        "score": 3,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on clinical NER in cardiology texts using BERT embeddings, which is outside the user's primary research interests in Information Retrieval and Search technologies. While it involves NLP, the context and application are quite different from the user's areas of focus."
    },
    {
        "title": "Addressing Antisocial Behavior in Multi-Party Dialogs Through Multimodal Representation Learning",
        "abstract": "Antisocial behavior (ASB) on social media -- including hate speech,\nharassment, and cyberbullying -- poses growing risks to platform safety and\nsocietal well-being. Prior research has focused largely on networks such as X\nand Reddit, while \\textit{multi-party conversational settings} remain\nunderexplored due to limited data. To address this gap, we use\n\\textit{CyberAgressionAdo-Large}, a French open-access dataset simulating ASB\nin multi-party conversations, and evaluate three tasks: \\textit{abuse\ndetection}, \\textit{bullying behavior analysis}, and \\textit{bullying\npeer-group identification}. We benchmark six text-based and eight graph-based\n\\textit{representation-learning methods}, analyzing lexical cues, interactional\ndynamics, and their multimodal fusion. Results show that multimodal models\noutperform unimodal baselines. The late fusion model \\texttt{mBERT + WD-SGCN}\nachieves the best overall results, with top performance on abuse detection\n(0.718) and competitive scores on peer-group identification (0.286) and\nbullying analysis (0.606). Error analysis highlights its effectiveness in\nhandling nuanced ASB phenomena such as implicit aggression, role transitions,\nand context-dependent hostility.",
        "url": "http://arxiv.org/abs/2510.17289v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17289v1",
        "arxiv_id": "2510.17289v1",
        "authors": [
            "Hajar Bakarou",
            "Mohamed Sinane El Messoussi",
            "Anaïs Ollagnier"
        ],
        "submitted": "2025-10-20 08:27:38",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on addressing antisocial behavior in multi-party dialogs through multimodal representation learning, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves representation learning and multimodal fusion, the context and application are quite different from your areas of focus."
    },
    {
        "title": "How News Feels: Understanding Affective Bias in Multilingual Headlines for Human-Centered Media Design",
        "abstract": "News media often shape the public mood not only by what they report but by\nhow they frame it. The same event can appear calm in one outlet and alarming in\nanother, reflecting subtle emotional bias in reporting. Negative or emotionally\ncharged headlines tend to attract more attention and spread faster, which in\nturn encourages outlets to frame stories in ways that provoke stronger\nreactions. This research explores that tendency through large-scale emotion\nanalysis of Bengali news. Using zero-shot inference with Gemma-3 4B, we\nanalyzed 300000 Bengali news headlines and their content to identify the\ndominant emotion and overall tone of each. The findings reveal a clear\ndominance of negative emotions, particularly anger, fear, and disappointment,\nand significant variation in how similar stories are emotionally portrayed\nacross outlets. Based on these insights, we propose design ideas for a\nhuman-centered news aggregator that visualizes emotional cues and helps readers\nrecognize hidden affective framing in daily news.",
        "url": "http://arxiv.org/abs/2510.17252v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17252v1",
        "arxiv_id": "2510.17252v1",
        "authors": [
            "Mohd Ruhul Ameen",
            "Akif Islam",
            "Abu Saleh Musa Miah",
            "Ayesha Siddiqua",
            "Jungpil Shin"
        ],
        "submitted": "2025-10-20 07:40:46",
        "source": "arxiv",
        "comment": "15 pages, 7 figures, 4 tables. Submitted to the International\n  Conference on Data and Applied Analytics (IDAA 2025)",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on affective bias in news headlines, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on the topic of text analysis, the context is media design and human-centered news aggregation, which is not a primary area of interest for the user."
    },
    {
        "title": "On Efficiency-Effectiveness Trade-off of Diffusion-based Recommenders",
        "abstract": "Diffusion models have emerged as a powerful paradigm for generative\nsequential recommendation, which typically generate next items to recommend\nguided by user interaction histories with a multi-step denoising process.\nHowever, the multi-step process relies on discrete approximations, introducing\ndiscretization error that creates a trade-off between computational efficiency\nand recommendation effectiveness. To address this trade-off, we propose TA-Rec,\na two-stage framework that achieves one-step generation by smoothing the\ndenoising function during pretraining while alleviating trajectory deviation by\naligning with user preferences during fine-tuning. Specifically, to improve the\nefficiency without sacrificing the recommendation performance, TA-Rec pretrains\nthe denoising model with Temporal Consistency Regularization (TCR), enforcing\nthe consistency between the denoising results across adjacent steps. Thus, we\ncan smooth the denoising function to map the noise as oracle items in one step\nwith bounded error. To further enhance effectiveness, TA-Rec introduces\nAdaptive Preference Alignment (APA) that aligns the denoising process with user\npreference adaptively based on preference pair similarity and timesteps.\nExtensive experiments prove that TA-Rec's two-stage objective effectively\nmitigates the discretization errors-induced trade-off, enhancing both\nefficiency and effectiveness of diffusion-based recommenders.",
        "url": "http://arxiv.org/abs/2510.17245v2",
        "pdf_url": "http://arxiv.org/pdf/2510.17245v2",
        "arxiv_id": "2510.17245v2",
        "authors": [
            "Wenyu Mao",
            "Jiancan Wu",
            "Guoqing Hu",
            "Zhengyi Yang",
            "Wei Ji",
            "Xiang Wang"
        ],
        "submitted": "2025-10-20 07:35:12",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper focuses on recommender systems, specifically diffusion-based models, which is somewhat related to the user's interests in Information Retrieval and Search technologies. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's emphasis on efficiency-effectiveness trade-offs in recommender systems is somewhat tangential to the user's primary focus on deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "CLAWS:Creativity detection for LLM-generated solutions using Attention Window of Sections",
        "abstract": "Recent advances in enhancing the reasoning ability of large language models\n(LLMs) have been remarkably successful. LLMs trained with reinforcement\nlearning (RL) for reasoning demonstrate strong performance in challenging tasks\nsuch as mathematics and coding, even with relatively small model sizes.\nHowever, despite these improvements in task accuracy, the assessment of\ncreativity in LLM generations has been largely overlooked in reasoning tasks,\nin contrast to writing tasks. The lack of research on creativity assessment in\nreasoning primarily stems from two challenges: (1) the difficulty of defining\nthe range of creativity, and (2) the necessity of human evaluation in the\nassessment process. To address these challenges, we propose CLAWS, a method\nthat defines and classifies mathematical solutions into typical, creative, and\nhallucinated categories without human evaluation, by leveraging attention\nweights across prompt sections and output. CLAWS outperforms five existing\nwhite-box detection methods (Perplexity, Logit Entropy, Window Entropy, Hidden\nScore, and Attention Score) on five 7-8B math RL models (DeepSeek, Qwen,\nMathstral, OpenMath2, and Oreal). We validate CLAWS on 4545 math problems\ncollected from 181 math contests (AJHSME, AMC, AIME).",
        "url": "http://arxiv.org/abs/2510.17921v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17921v1",
        "arxiv_id": "2510.17921v1",
        "authors": [
            "Keuntae Kim",
            "Eunhye Jeong",
            "Sehyeon Lee",
            "Seohee Yoon",
            "Yong Suk Choi"
        ],
        "submitted": "2025-10-20 06:59:37",
        "source": "arxiv",
        "comment": "NeurIPS 2025",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on creativity detection in LLM-generated solutions, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves attention mechanisms, the context is specific to creativity assessment in reasoning tasks, which is not a central match to your research themes."
    },
    {
        "title": "Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting",
        "abstract": "The increase in computing power and the necessity of AI-assisted\ndecision-making boost the growing application of large language models (LLMs).\nAlong with this, the potential retention of sensitive data of LLMs has spurred\nincreasing research into machine unlearning. However, existing unlearning\napproaches face a critical dilemma: Aggressive unlearning compromises model\nutility, while conservative strategies preserve utility but risk hallucinated\nresponses. This significantly limits LLMs' reliability in knowledge-intensive\napplications. To address this, we introduce a novel Attention-Shifting (AS)\nframework for selective unlearning. AS is driven by two design objectives: (1)\ncontext-preserving suppression that attenuates attention to fact-bearing tokens\nwithout disrupting LLMs' linguistic structure; and (2) hallucination-resistant\nresponse shaping that discourages fabricated completions when queried about\nunlearning content. AS realizes these objectives through two attention-level\ninterventions, which are importance-aware suppression applied to the unlearning\nset to reduce reliance on memorized knowledge and attention-guided retention\nenhancement that reinforces attention toward semantically essential tokens in\nthe retained dataset to mitigate unintended degradation. These two components\nare jointly optimized via a dual-loss objective, which forms a soft boundary\nthat localizes unlearning while preserving unrelated knowledge under\nrepresentation superposition. Experimental results show that AS improves\nperformance preservation over the state-of-the-art unlearning methods,\nachieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC\nbenchmark, while maintaining competitive hallucination-free unlearning\neffectiveness. Compared to existing methods, AS demonstrates a superior balance\nbetween unlearning effectiveness, generalization, and response reliability.",
        "url": "http://arxiv.org/abs/2510.17210v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17210v1",
        "arxiv_id": "2510.17210v1",
        "authors": [
            "Chenchen Tan",
            "Youyang Qu",
            "Xinghao Li",
            "Hui Zhang",
            "Shujie Cui",
            "Cunjian Chen",
            "Longxiang Gao"
        ],
        "submitted": "2025-10-20 06:50:03",
        "source": "arxiv",
        "comment": "22 pages, 10 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on machine unlearning in large language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves attention mechanisms, the context is more aligned with model reliability and knowledge retention rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users",
        "abstract": "We study a web-deployed, tool-augmented LLM health coach with real users. In\na pilot with seven users (280 rated turns), offline policy evaluation (OPE)\nover factorized decision heads (Tool/Style) shows that a uniform heavy-tool\npolicy raises average value on logs but harms specific subgroups, most notably\nlow-health-literacy/high-self-efficacy users. A lightweight simulator with\nhidden archetypes further shows that adding a small early information-gain\nbonus reliably shortens trait identification and improves goal success and\npass@3. Together, these early findings indicate an evaluation-first path to\npersonalization: freeze the generator, learn subgroup-aware decision heads on\ntyped rewards (objective tool outcomes and satisfaction), and always report\nper-archetype metrics to surface subgroup harms that averages obscure.",
        "url": "http://arxiv.org/abs/2510.17173v2",
        "pdf_url": "http://arxiv.org/pdf/2510.17173v2",
        "arxiv_id": "2510.17173v2",
        "authors": [
            "Melik Ozolcer",
            "Sang Won Bae"
        ],
        "submitted": "2025-10-20 05:28:59",
        "source": "arxiv",
        "comment": "Accepted to the NeurIPS 2025 Workshop on Multi-Turn Interactions in\n  Large Language Models",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'personalization' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be focused on evaluating the effectiveness of a language model-based health coach, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the context is specific to health coaching and does not align with the user's primary research interests."
    },
    {
        "title": "Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation",
        "abstract": "While reasoning-based large language models excel at complex tasks through an\ninternal, structured thinking process, a concerning phenomenon has emerged that\nsuch a thinking process can aggregate social stereotypes, leading to biased\noutcomes. However, the underlying behaviours of these language models in social\nbias scenarios remain underexplored. In this work, we systematically\ninvestigate mechanisms within the thinking process behind this phenomenon and\nuncover two failure patterns that drive social bias aggregation: 1) stereotype\nrepetition, where the model relies on social stereotypes as its primary\njustification, and 2) irrelevant information injection, where it fabricates or\nintroduces new details to support a biased narrative. Building on these\ninsights, we introduce a lightweight prompt-based mitigation approach that\nqueries the model to review its own initial reasoning against these specific\nfailure patterns. Experiments on question answering (BBQ and StereoSet) and\nopen-ended (BOLD) benchmarks show that our approach effectively reduces bias\nwhile maintaining or improving accuracy.",
        "url": "http://arxiv.org/abs/2510.17062v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17062v1",
        "arxiv_id": "2510.17062v1",
        "authors": [
            "Guoqing Luo",
            "Iffat Maab",
            "Lili Mou",
            "Junichi Yamagishi"
        ],
        "submitted": "2025-10-20 00:33:44",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the social bias in reasoning-based language models, which is somewhat related to information retrieval and NLP. However, the focus on social bias mitigation and language model behavior does not directly align with the user's primary interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Mapping Post-Training Forgetting in Language Models at Scale",
        "abstract": "Scaled post-training now drives many of the largest capability gains in\nlanguage models (LMs), yet its effect on pretrained knowledge remains poorly\nunderstood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S.\npresident or an API call) does not \"average out\" by recalling another. Hence,\nwe propose a sample-wise paradigm to measure what is forgotten and when\nbackward transfer occurs. Our metric counts 1->0 transitions (correct before\npost-training, incorrect after) to quantify forgetting and 0->1 transitions to\nquantify backward transfer. Traditional task averages conflate these effects\nand obscure large changes. For multiple-choice benchmarks, we add\nchance-adjusted variants that subtract the expected contribution of random\nguessing from pre- and post-training accuracies. We apply this framework across\npost-training stages, model sizes, and data scales. Our large-scale analysis\nshows that: (1) Domain-continual pretraining induces moderate forgetting with\nlow-to-moderate backward transfer; (2) RL/SFT post-training applied to base\nmodels and Instruction tuning yields moderate-to-large backward transfer on\nmath and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to\ninstruction-tuned models is sensitive on data scale: at small scales, both\nforgetting and backward transfer are small; at larger scales, effects are mixed\nand warrant further study with better controls; (4) Model merging does not\nreliably mitigate forgetting. Overall, our framework offers a practical\nyardstick for mapping how post-training alters pretrained knowledge at scale --\nenabling progress towards generally capable AI systems.",
        "url": "http://arxiv.org/abs/2510.17776v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17776v1",
        "arxiv_id": "2510.17776v1",
        "authors": [
            "Jackson Harmon",
            "Andreas Hochlehnert",
            "Matthias Bethge",
            "Ameya Prabhu"
        ],
        "submitted": "2025-10-20 17:35:47",
        "source": "arxiv",
        "comment": "43 pages,15 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on the concept of post-training forgetting in language models, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it touches on the idea of model performance and knowledge retention, it does not address the core themes of query understanding, ranking models, or real-time relevance optimization."
    },
    {
        "title": "VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models",
        "abstract": "Vision-Language Models (VLMs) extend large language models with visual\nreasoning, but their multimodal design also introduces new, underexplored\nvulnerabilities. Existing multimodal red-teaming methods largely rely on\nbrittle templates, focus on single-attack settings, and expose only a narrow\nsubset of vulnerabilities. To address these limitations, we introduce VERA-V, a\nvariational inference framework that recasts multimodal jailbreak discovery as\nlearning a joint posterior distribution over paired text-image prompts. This\nprobabilistic view enables the generation of stealthy, coupled adversarial\ninputs that bypass model guardrails. We train a lightweight attacker to\napproximate the posterior, allowing efficient sampling of diverse jailbreaks\nand providing distributional insights into vulnerabilities. VERA-V further\nintegrates three complementary strategies: (i) typography-based text prompts\nthat embed harmful cues, (ii) diffusion-based image synthesis that introduces\nadversarial signals, and (iii) structured distractors to fragment VLM\nattention. Experiments on HarmBench and HADES benchmarks show that VERA-V\nconsistently outperforms state-of-the-art baselines on both open-source and\nfrontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the\nbest baseline on GPT-4o.",
        "url": "http://arxiv.org/abs/2510.17759v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17759v1",
        "arxiv_id": "2510.17759v1",
        "authors": [
            "Qilin Liao",
            "Anamika Lochab",
            "Ruqi Zhang"
        ],
        "submitted": "2025-10-20 17:12:10",
        "source": "arxiv",
        "comment": "18 pages, 7 Figures,",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is primarily focused on Vision-Language Models and their vulnerabilities, which is not a core area of interest for you. While it does involve some aspects of deep semantic understanding, the context is more related to model security and adversarial attacks rather than information retrieval or search technologies."
    },
    {
        "title": "Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations",
        "abstract": "Language models often generate factually incorrect information unsupported by\ntheir training data, a phenomenon known as extrinsic hallucination. Existing\nmitigation approaches often degrade performance on open-ended generation and\ndownstream tasks, limiting their practical utility. We propose an online\nreinforcement learning method using a novel binary retrieval-augmented reward\n(RAR) to address this tradeoff. Unlike continuous reward schemes, our approach\nassigns a reward of one only when the model's output is entirely factually\ncorrect, and zero otherwise. We evaluate our method on Qwen3 reasoning models\nacross diverse tasks. For open-ended generation, binary RAR achieves a 39.3%\nreduction in hallucination rates, substantially outperforming both supervised\ntraining and continuous-reward RL baselines. In short-form question answering,\nthe model learns calibrated abstention, strategically outputting \"I don't know\"\nwhen faced with insufficient parametric knowledge. This yields 44.4% and 21.7%\nfewer incorrect answers on PopQA and GPQA, respectively. Crucially, these\nfactuality gains come without performance degradation on instruction following,\nmath, or code, whereas continuous-reward RL, despite improving factuality,\ninduces quality regressions.",
        "url": "http://arxiv.org/abs/2510.17733v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17733v1",
        "arxiv_id": "2510.17733v1",
        "authors": [
            "Tong Chen",
            "Akari Asai",
            "Luke Zettlemoyer",
            "Hannaneh Hajishirzi",
            "Faeze Brahman"
        ],
        "submitted": "2025-10-20 16:45:43",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper explores a method to mitigate hallucinations in language models, which is related to query understanding and ranking models in Information Retrieval. However, the focus on open-ended generation and factuality gains is more aligned with NLP and deep semantic understanding, rather than the user's primary focus on IR and real-time relevance optimization."
    },
    {
        "title": "PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition",
        "abstract": "Named Entity Recognition (NER) is a critical task that requires substantial\nannotated data, making it challenging in low-resource scenarios where label\nacquisition is expensive. While zero-shot and instruction-tuned approaches have\nmade progress, they often fail to generalize to domain-specific entities and do\nnot effectively utilize limited available data. We present a lightweight\nfew-shot NER framework that addresses these challenges through two key\ninnovations: (1) a new instruction tuning template with a simplified output\nformat that combines principles from prior IT approaches to leverage the large\ncontext window of recent state-of-the-art LLMs; (2) introducing a strategic\ndata augmentation technique that preserves entity information while\nparaphrasing the surrounding context, thereby expanding our training data\nwithout compromising semantic relationships. Experiments on benchmark datasets\nshow that our method achieves performance comparable to state-of-the-art models\non few-shot and zero-shot tasks, with our few-shot approach attaining an\naverage F1 score of 80.1 on the CrossNER datasets. Models trained with our\nparaphrasing approach show consistent improvements in F1 scores of up to 17\npoints over baseline versions, offering a promising solution for groups with\nlimited NER training data and compute power.",
        "url": "http://arxiv.org/abs/2510.17720v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17720v1",
        "arxiv_id": "2510.17720v1",
        "authors": [
            "Nanda Kumar Rengarajan",
            "Jun Yan",
            "Chun Wang"
        ],
        "submitted": "2025-10-20 16:36:18",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Named Entity Recognition (NER) and proposes a framework for low-resource scenarios. While it involves data augmentation and NLP techniques, it does not align with the user's primary research interests in Information Retrieval, Search technologies, and query understanding."
    },
    {
        "title": "QueST: Incentivizing LLMs to Generate Difficult Problems",
        "abstract": "Large Language Models have achieved strong performance on reasoning tasks,\nsolving competition-level coding and math problems. However, their scalability\nis limited by human-labeled datasets and the lack of large-scale, challenging\ncoding problem training data. Existing competitive coding datasets contain only\nthousands to tens of thousands of problems. Previous synthetic data generation\nmethods rely on either augmenting existing instruction datasets or selecting\nchallenging problems from human-labeled data. In this paper, we propose QueST,\na novel framework which combines difficulty-aware graph sampling and\ndifficulty-aware rejection fine-tuning that directly optimizes specialized\ngenerators to create challenging coding problems. Our trained generators\ndemonstrate superior capability compared to even GPT-4o at creating challenging\nproblems that benefit downstream performance. We leverage QueST to generate\nlarge-scale synthetic coding problems, which we then use to distill from strong\nteacher models with long chain-of-thought or to conduct reinforcement learning\nfor smaller models, proving effective in both scenarios. Our distillation\nexperiments demonstrate significant performance gains. Specifically, after\nfine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we\nsurpass the performance of the original Qwen3-8B on LiveCodeBench. With an\nadditional 112K examples (i.e., 28K human-written problems paired with multiple\nsynthetic solutions), our 8B model matches the performance of the much larger\nDeepSeek-R1-671B. These findings indicate that generating complex problems via\nQueST offers an effective and scalable approach to advancing the frontiers of\ncompetitive coding and reasoning for large language models.",
        "url": "http://arxiv.org/abs/2510.17715v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17715v1",
        "arxiv_id": "2510.17715v1",
        "authors": [
            "Hanxu Hu",
            "Xingxing Zhang",
            "Jannis Vamvas",
            "Rico Sennrich",
            "Furu Wei"
        ],
        "submitted": "2025-10-20 16:29:53",
        "source": "arxiv",
        "comment": "20 pages, 7 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on generating challenging coding problems for large language models, which is related to query understanding and ranking models in the context of competitive coding and reasoning. However, the paper's primary focus is on NLP and problem generation, rather than information retrieval or search technologies, making it only loosely relevant to your research interests."
    },
    {
        "title": "Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models",
        "abstract": "Large Language Models (LLMs) possess remarkable generalization capabilities\nbut struggle with multi-task adaptation, particularly in balancing knowledge\nretention with task-specific specialization. Conventional fine-tuning methods\nsuffer from catastrophic forgetting and substantial resource consumption, while\nexisting parameter-efficient methods perform suboptimally in complex multi-task\nscenarios. To address this, we propose Contextual Attention Modulation (CAM), a\nnovel mechanism that dynamically modulates the representations of\nself-attention modules in LLMs. CAM enhances task-specific features while\npreserving general knowledge, thereby facilitating more effective and efficient\nadaptation. For effective multi-task adaptation, CAM is integrated into our\nHybrid Contextual Attention Modulation (HyCAM) framework, which combines a\nshared, full-parameter CAM module with multiple specialized, lightweight CAM\nmodules, enhanced by a dynamic routing strategy for adaptive knowledge fusion.\nExtensive experiments on heterogeneous tasks, including question answering,\ncode generation, and logical reasoning, demonstrate that our approach\nsignificantly outperforms existing approaches, achieving an average performance\nimprovement of 3.65%. The implemented code and data are available to ease\nreproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.",
        "url": "http://arxiv.org/abs/2510.17705v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17705v1",
        "arxiv_id": "2510.17705v1",
        "authors": [
            "Dayan Pan",
            "Zhaoyang Fu",
            "Jingyuan Wang",
            "Xiao Han",
            "Yue Zhu",
            "Xiangyu Zhao"
        ],
        "submitted": "2025-10-20 16:19:27",
        "source": "arxiv",
        "comment": "Accepted by CIKM' 25",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Large Language Models and multi-task adaptation, which is not directly related to your core research interests in Information Retrieval and Search technologies. While it involves attention mechanisms, the context is more aligned with NLP and deep learning, rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "DELULU: Discriminative Embedding Learning Using Latent Units for Speaker-Aware Self-Supervised Speech Foundational Model",
        "abstract": "Self-supervised speech models have achieved remarkable success on\ncontent-driven tasks, yet they remain limited in capturing\nspeaker-discriminative features critical for verification, diarization, and\nprofiling applications. We introduce DELULU, a speaker-aware self-supervised\nfoundational model that addresses this limitation by integrating external\nsupervision into the pseudo-label generation process. DELULU leverages\nframe-level embeddings from ReDimNet, a state-of-the-art speaker verification\nmodel, to guide the k-means clustering step during pre-training, introducing a\nstrong speaker-discriminative inductive bias that aligns representation\nlearning with speaker identity. The model is trained using a dual objective\nthat combines masked prediction and denoising, further enhancing robustness and\ngeneralization. DELULU significantly outperforms prior self-supervised learning\n(SSL) models across a range of speaker-centric tasks, achieving up to 62%\nrelative improvement in equal error rate (EER) for speaker verification and\nconsistent gains on zero-shot profiling tasks such as gender, age, accent, and\nspeaker counting. Our findings demonstrate that DELULU is a strong universal\nencoder for speaker-aware speech processing, enabling superior performance even\nwithout task-specific fine-tuning.",
        "url": "http://arxiv.org/abs/2510.17662v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17662v1",
        "arxiv_id": "2510.17662v1",
        "authors": [
            "Massa Baali",
            "Rita Singh",
            "Bhiksha Raj"
        ],
        "submitted": "2025-10-20 15:35:55",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper focuses on speaker-aware self-supervised speech models, which is unrelated to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing."
    },
    {
        "title": "Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models",
        "abstract": "Large language models may encode sensitive information or outdated knowledge\nthat needs to be removed, to ensure responsible and compliant model responses.\nUnlearning has emerged as an efficient alternative to full retraining, aiming\nto remove specific knowledge while preserving overall model utility. Existing\nevaluations of unlearning methods focus on (1) the extent of forgetting of the\ntarget knowledge (forget set) and (2) maintaining performance on the retain set\n(i.e., utility). However, these evaluations overlook an important usability\naspect: users may still want the model to leverage the removed information if\nit is re-introduced in the prompt. In a systematic evaluation of six\nstate-of-the-art unlearning methods, we find that they consistently impair such\ncontextual utility. To address this, we augment unlearning objectives with a\nplug-in term that preserves the model's ability to use forgotten knowledge when\nit is present in context. Extensive experiments demonstrate that our approach\nrestores contextual utility to near original levels while still maintaining\neffective forgetting and retain-set utility.",
        "url": "http://arxiv.org/abs/2510.17620v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17620v1",
        "arxiv_id": "2510.17620v1",
        "authors": [
            "Yuefeng Peng",
            "Parnian Afshar",
            "Megan Ganji",
            "Thomas Butler",
            "Amir Houmansadr",
            "Mingxian Wang",
            "Dezhi Hong"
        ],
        "submitted": "2025-10-20 15:03:45",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on unlearning and contextual utility in large language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing, particularly in areas requiring deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "Reasoning Distillation and Structural Alignment for Improved Code Generation",
        "abstract": "Effective code generation with language models hinges on two critical\nfactors: accurately understanding the intent of the prompt and generating code\nthat applies algorithmic reasoning to produce correct solutions capable of\npassing diverse test cases while adhering to the syntax of the target\nprogramming language. Unlike other language tasks, code generation requires\nmore than accurate token prediction; it demands comprehension of solution-level\nand structural relationships rather than merely generating the most likely\ntokens. very large language model (VLLM) are capable of generating detailed\nsteps toward the correct solution of complex tasks where reasoning is crucial\nin solving the problem. Such reasoning capabilities may be absent in smaller\nlanguage models. Therefore, in this work, we distill the reasoning capabilities\nof a VLLM into a smaller, more efficient model that is faster and cheaper to\ndeploy. Our approach trains the model to emulate the reasoning and\nproblem-solving abilities of the VLLM by learning to identify correct solution\npathways and establishing a structural correspondence between problem\ndefinitions and potential solutions through a novel method of structure-aware\nloss optimization. This enables the model to transcend token-level generation\nand to deeply grasp the overarching structure of solutions for given problems.\nExperimental results show that our fine-tuned model, developed through a cheap\nand simple to implement process, significantly outperforms our baseline model\nin terms of pass@1, average data flow, and average syntax match metrics across\nthe MBPP, MBPP Plus, and HumanEval benchmarks.",
        "url": "http://arxiv.org/abs/2510.17598v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17598v1",
        "arxiv_id": "2510.17598v1",
        "authors": [
            "Amir Jalilifard",
            "Anderson de Rezende Rocha",
            "Marcos Medeiros Raimundo"
        ],
        "submitted": "2025-10-20 14:47:47",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on code generation with language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves language models, the context and application are quite different from your areas of focus."
    },
    {
        "title": "SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors",
        "abstract": "Large language model (LLM) simulations of human behavior have the potential\nto revolutionize the social and behavioral sciences, if and only if they\nfaithfully reflect real human behaviors. Current evaluations are fragmented,\nbased on bespoke tasks and metrics, creating a patchwork of incomparable\nresults. To address this, we introduce SimBench, the first large-scale,\nstandardized benchmark for a robust, reproducible science of LLM simulation. By\nunifying 20 diverse datasets covering tasks from moral decision-making to\neconomic choice across a large global participant pool, SimBench provides the\nnecessary foundation to ask fundamental questions about when, how, and why LLM\nsimulations succeed or fail. We show that, while even the best LLMs today have\nlimited simulation ability (score: 40.80/100), performance scales log-linearly\nwith model size. Simulation performance is not improved by increased\ninference-time compute. We demonstrate an alignment-simulation trade-off:\ninstruction-tuning improves performance on low-entropy (consensus) questions\nbut degrades it on high-entropy (diverse) ones. Models particularly struggle\nwhen simulating specific demographic groups. Finally, we demonstrate that\nsimulation ability correlates most strongly with deep, knowledge-intensive\nreasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to\naccelerate the development of more faithful LLM simulators.",
        "url": "http://arxiv.org/abs/2510.17516v2",
        "pdf_url": "http://arxiv.org/pdf/2510.17516v2",
        "arxiv_id": "2510.17516v2",
        "authors": [
            "Tiancheng Hu",
            "Joachim Baumann",
            "Lorenzo Lupo",
            "Nigel Collier",
            "Dirk Hovy",
            "Paul Röttger"
        ],
        "submitted": "2025-10-20 13:14:38",
        "source": "arxiv",
        "comment": "Project Website: http://simbench.tiancheng.hu/ Data:\n  https://huggingface.co/datasets/pitehu/SimBench",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your interests in Natural Language Processing (NLP) and large language models, but it focuses on simulating human behaviors rather than information retrieval or search technologies. While it touches on the evaluation of model performance, it does not directly address query understanding, ranking models, or user behavior modeling in the context of search."
    },
    {
        "title": "DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning",
        "abstract": "Detecting AI-involved text is essential for combating misinformation,\nplagiarism, and academic misconduct. However, AI text generation includes\ndiverse collaborative processes (AI-written text edited by humans,\nhuman-written text edited by AI, and AI-generated text refined by other AI),\nwhere various or even new LLMs could be involved. Texts generated through these\nvaried processes exhibit complex characteristics, presenting significant\nchallenges for detection. Current methods model these processes rather crudely,\nprimarily employing binary classification (purely human vs. AI-involved) or\nmulti-classification (treating human-AI collaboration as a new class). We\nobserve that representations of texts generated through different processes\nexhibit inherent clustering relationships. Therefore, we propose DETree, a\nnovel approach that models the relationships among different processes as a\nHierarchical Affinity Tree structure, and introduces a specialized loss\nfunction that aligns text representations with this tree. To facilitate this\nlearning, we developed RealBench, a comprehensive benchmark dataset that\nautomatically incorporates a wide spectrum of hybrid texts produced through\nvarious human-AI collaboration processes. Our method improves performance in\nhybrid text detection tasks and significantly enhances robustness and\ngeneralization in out-of-distribution scenarios, particularly in few-shot\nlearning conditions, further demonstrating the promise of training-based\napproaches in OOD settings. Our code and dataset are available at\nhttps://github.com/heyongxin233/DETree.",
        "url": "http://arxiv.org/abs/2510.17489v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17489v1",
        "arxiv_id": "2510.17489v1",
        "authors": [
            "Yongxin He",
            "Shan Zhang",
            "Yixuan Cao",
            "Lei Ma",
            "Ping Luo"
        ],
        "submitted": "2025-10-20 12:41:44",
        "source": "arxiv",
        "comment": "To appear in NeurIPS 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the detection of AI-involved text, which is somewhat related to information retrieval and search technologies. However, the focus on text generation and detection, rather than query understanding or ranking models, limits its relevance to your core research themes."
    },
    {
        "title": "BenCao: An Instruction-Tuned Large Language Model for Traditional Chinese Medicine",
        "abstract": "Traditional Chinese Medicine (TCM), with a history spanning over two\nmillennia, plays a role in global healthcare. However, applying large language\nmodels (LLMs) to TCM remains challenging due to its reliance on holistic\nreasoning, implicit logic, and multimodal diagnostic cues. Existing TCM-domain\nLLMs have made progress in text-based understanding but lack multimodal\nintegration, interpretability, and clinical applicability. To address these\nlimitations, we developed BenCao, a ChatGPT-based multimodal assistant for TCM,\nintegrating structured knowledge bases, diagnostic data, and expert feedback\nrefinement. BenCao was trained through natural language instruction tuning\nrather than parameter retraining, aligning with expert-level reasoning and\nethical norms specific to TCM. The system incorporates a comprehensive\nknowledge base of over 1,000 classical and modern texts, a scenario-based\ninstruction framework for diverse interactions, a chain-of-thought simulation\nmechanism for interpretable reasoning, and a feedback refinement process\ninvolving licensed TCM practitioners. BenCao connects to external APIs for\ntongue-image classification and multimodal database retrieval, enabling dynamic\naccess to diagnostic resources. In evaluations across single-choice question\nbenchmarks and multimodal classification tasks, BenCao achieved superior\naccuracy to general-domain and TCM-domain models, particularly in diagnostics,\nherb recognition, and constitution classification. The model was deployed as an\ninteractive application on the OpenAI GPTs Store, accessed by nearly 1,000\nusers globally as of October 2025. This study demonstrates the feasibility of\ndeveloping a TCM-domain LLM through natural language-based instruction tuning\nand multimodal integration, offering a practical framework for aligning\ngenerative AI with traditional medical reasoning and a scalable pathway for\nreal-world deployment.",
        "url": "http://arxiv.org/abs/2510.17415v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17415v1",
        "arxiv_id": "2510.17415v1",
        "authors": [
            "Jiacheng Xie",
            "Yang Yu",
            "Yibo Chen",
            "Hanyao Zhang",
            "Lening Zhao",
            "Jiaxuan He",
            "Lei Jiang",
            "Xiaoting Tang",
            "Guanghui An",
            "Dong Xu"
        ],
        "submitted": "2025-10-20 10:57:37",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing, as it focuses on developing a large language model for Traditional Chinese Medicine, which is outside your core research themes."
    },
    {
        "title": "Leveraging Group Relative Policy Optimization to Advance Large Language Models in Traditional Chinese Medicine",
        "abstract": "Traditional Chinese Medicine (TCM) presents a rich and structurally unique\nknowledge system that challenges conventional applications of large language\nmodels (LLMs). Although previous TCM-specific LLMs have shown progress through\nsupervised fine-tuning, they often face limitations in alignment, data quality,\nand evaluation consistency. In this study, we introduce Ladder-base, the first\nTCM-focused LLM trained with Group Relative Policy Optimization (GRPO), a\nreinforcement learning method that improves reasoning and factual consistency\nby optimizing response selection based on intra-group comparisons. Ladder-base\nis built upon the Qwen2.5-7B-Instruct foundation model and trained exclusively\non the textual subset of the TCM-Ladder benchmark, using 80 percent of the data\nfor training and the remaining 20 percent split evenly between validation and\ntest sets. Through standardized evaluation, Ladder-base demonstrates superior\nperformance across multiple reasoning metrics when compared to both\nstate-of-the-art general-purpose LLMs such as GPT-4, Gemini 2.5, Claude 3, and\nQwen3 and domain-specific TCM models including BenTsao, HuatuoGPT2, and\nZhongjing. These findings suggest that GRPO provides an effective and efficient\nstrategy for aligning LLMs with expert-level reasoning in traditional medical\ndomains and supports the development of trustworthy and clinically grounded TCM\nartificial intelligence systems.",
        "url": "http://arxiv.org/abs/2510.17402v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17402v1",
        "arxiv_id": "2510.17402v1",
        "authors": [
            "Jiacheng Xie",
            "Shuai Zeng",
            "Yang Yu",
            "Xiaoting Tang",
            "Guanghui An",
            "Dong Xu"
        ],
        "submitted": "2025-10-20 10:43:33",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing, as it focuses on Traditional Chinese Medicine and the application of reinforcement learning to large language models."
    },
    {
        "title": "StreamingThinker: Large Language Models Can Think While Reading",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in\nchain of thought (CoT) reasoning. However, the current LLM reasoning paradigm\ninitiates thinking only after the entire input is available, which introduces\nunnecessary latency and weakens attention to earlier information in dynamic\nscenarios. Inspired by human cognition of thinking while reading, we first\ndesign a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where\nreasoning unfolds in the order of input and further adjusts its depth once\nreading is complete. We instantiate this paradigm with\n\\textit{StreamingThinker}, a framework that enables LLMs to think while reading\nthrough the integration of streaming CoT generation, streaming-constraint\ntraining, and streaming parallel inference. Specifically, StreamingThinker\nemploys streaming reasoning units with quality control for CoT generation,\nenforces order-preserving reasoning through streaming attention masks and\nposition encoding, and leverages parallel KV caches that decouple input\nencoding from reasoning generation, thereby ensuring alignment and enabling\ntrue concurrency. We evaluate StreamingThinker on the Qwen3 model family across\nmath reasoning, logical reasoning, and context-based QA reasoning tasks.\nExperimental results show that the StreamingThinker preserves performance\ncomparable to batch thinking, while yielding an 80\\% reduction in token waiting\nbefore the onset of reasoning and a more than 60\\% reduction in time-level\nlatency for producing the final answer, demonstrating the effectiveness of the\nstreaming paradigm for LLM reasoning. Code will be released at\n\\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this\nrepository.}",
        "url": "http://arxiv.org/abs/2510.17238v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17238v1",
        "arxiv_id": "2510.17238v1",
        "authors": [
            "Junlong Tong",
            "Yingqi Fan",
            "Anhao Zhao",
            "Yunpu Ma",
            "Xiaoyu Shen"
        ],
        "submitted": "2025-10-20 07:27:37",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on improving the performance of Large Language Models (LLMs) through a 'streaming thinking' paradigm, which is not directly related to Information Retrieval (IR), Search technologies, or user behavior modeling. While it involves NLP, the context is more aligned with deep learning and model optimization, rather than query understanding, ranking models, or real-time relevance optimization."
    },
    {
        "title": "Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models",
        "abstract": "Effectively processing long contexts is a critical challenge for language\nmodels. While standard Transformers are limited by quadratic complexity and\npoor length extrapolation, alternative architectures like sliding window\nattention and state space models sacrifice the ability to effectively utilize\nthe full context due to their fixed-size memory. Chunk-based sparse attention\nhas emerged as a promising paradigm for extreme length generalization, yet the\nkey architectural principles underpinning its success are not yet fully\nunderstood. In this work, we present a systematic dissection of these models to\nidentify the core components driving their performance. Through a unified\nframework and comprehensive ablation studies, we demonstrate that a combination\nof three design principles is critical: (1) an expressive, non-linear Chunk\nEncoder with a dedicated CLS token to produce representations for retrieval;\n(2) a Bypassing Residual Path to stably integrate retrieved global information\nwithout it being overridden by the local residual stream; and (3) enforced\nselection sparsity during pre-training to bridge the train-test distribution\ngap. We provide a theoretical motivation for intra-chunk information processing\nand landmark generation. By combining these principles, we establish a new\nstate-of-the-art for training-free length extrapolation, successfully\ngeneralizing models trained on a 4K context to 32 million tokens on RULER and\nBABILong. Our findings provide a clear and empirically-grounded set of design\nprinciples for developing future, highly-capable long-context language models.",
        "url": "http://arxiv.org/abs/2510.17196v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17196v1",
        "arxiv_id": "2510.17196v1",
        "authors": [
            "Jiaqi Leng",
            "Xiang Hu",
            "Junxiong Wang",
            "Jianguo Li",
            "Wei Wu",
            "Yucheng Lu"
        ],
        "submitted": "2025-10-20 06:17:57",
        "source": "arxiv",
        "comment": "Preprint. Work in progress",
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper focuses on improving the performance of language models on long contexts, which is somewhat related to information retrieval and search technologies. However, the specific topic of hierarchical sparse attention models and their design principles is not directly aligned with the user's core research themes, such as query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "DVAGen: Dynamic Vocabulary Augmented Generation",
        "abstract": "Language models trained with a fixed vocabulary struggle to generalize to\nnovel or out-of-vocabulary words, limiting their flexibility in handling\ndiverse token combinations. Existing dynamic vocabulary approaches attempt to\naddress this limitation but face challenges such as fragmented codebases, lack\nof support for modern LLMs, and limited inference scalability. To overcome\nthese issues, we introduce DVAGen, a fully open-source, unified framework\ndesigned for training, evaluation, and visualization of dynamic\nvocabulary-augmented language models. Our framework modularizes the pipeline\nfor ease of customization, integrates seamlessly with open-source LLMs, and is\nthe first to provide both CLI and WebUI tools for real-time result inspection.\nWe validate the effectiveness of dynamic vocabulary methods on modern LLMs and\ndemonstrate support for batch inference, significantly improving inference\nthroughput.",
        "url": "http://arxiv.org/abs/2510.17115v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17115v1",
        "arxiv_id": "2510.17115v1",
        "authors": [
            "Wei Du",
            "Nuowei Liu",
            "Jie Wang",
            "Jiahao Kuang",
            "Tao Ji",
            "Xiaoling Wang",
            "Yuanbin Wu"
        ],
        "submitted": "2025-10-20 03:09:24",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on language models and dynamic vocabulary augmentation, which is somewhat related to your interests in NLP and deep semantic understanding. However, it does not directly address your core research themes in Information Retrieval, Search technologies, or query understanding."
    },
    {
        "title": "JT-Safe: Intrinsically Enhancing the Safety and Trustworthiness of LLMs",
        "abstract": "The hallucination and credibility concerns of large language models (LLMs)\nare global challenges that the industry is collectively addressing. Recently, a\nsignificant amount of advances have been made on post-training and inference\ntechniques to mitigate these challenges. However, it is widely agreed that\nunsafe and hallucinations of LLMs intrinsically originate from pre-training,\ninvolving pre-training data and the next-token prediction learning mechanism.\nIn this paper, we focus on enhancing pre-training data to improve the\ntrustworthiness and safety of LLMs. Since the data is vast, it's almost\nimpossible to entirely purge the data of factual errors, logical\ninconsistencies, or distributional biases. Moreover, the pre-training data lack\ngrounding in real-world knowledge. Each piece of data is treated as a sequence\nof tokens rather than as a representation of a part of the world. To overcome\nthese issues, we propose approaches to enhancing our pre-training data with its\ncontext in the world and increasing a substantial amount of data reflecting\nindustrial scenarios. We argue that most source data are created by the authors\nfor specific purposes in a certain spatial-temporal context. They have played a\nrole in the real world. By incorporating related world context information, we\naim to better anchor pre-training data within real-world scenarios, thereby\nreducing uncertainty in model training and enhancing the model's safety and\ntrustworthiness. We refer to our Data with World Context as DWC. We continue\npre-training an earlier checkpoint of JT-35B-Base with 1.5 trillion of DWC\ntokens. We introduce our post-training procedures to activate the potentials of\nDWC. Compared with the Qwen model of a similar scale, JT-Safe-35B achieves an\naverage performance improvement of 1.79% on the Safety and Trustworthy\nevaluation benchmarks, while being pretrained with only 6.2 trillion tokens.",
        "url": "http://arxiv.org/abs/2510.17918v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17918v1",
        "arxiv_id": "2510.17918v1",
        "authors": [
            "Junlan Feng",
            "Fanyu Meng",
            "Chong Long",
            "Pengyu Cong",
            "Duqing Wang",
            "Yan Zheng",
            "Yuyao Zhang",
            "Xuanchang Gao",
            "Ye Yuan",
            "Yunfei Ma",
            "Zhijie Ren",
            "Fan Yang",
            "Na Wu",
            "Di Jin",
            "Chao Deng"
        ],
        "submitted": "2025-10-20 02:12:49",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on enhancing the safety and trustworthiness of Large Language Models (LLMs) through pre-training data, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on NLP, the primary focus is on improving LLMs, not on deep semantic understanding or real-time relevance optimization."
    },
    {
        "title": "DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking",
        "abstract": "Recent LLM benchmarks have tested models on a range of phenomena, but are\nstill focused primarily on natural language understanding for extraction of\nexplicit information, such as QA or summarization, with responses often tar-\ngeting information from individual sentences. We are still lacking more\nchallenging, and im- portantly also multilingual, benchmarks focus- ing on\nimplicit information and pragmatic infer- ences across larger documents in the\ncontext of discourse tracking: integrating and aggregating information across\nsentences, paragraphs and multiple speaker utterances. To this end, we present\nDiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages\nand four levels of discourse understanding: salience recognition, entity\ntracking, discourse relations and bridging inference. Our evaluation shows that\nthese tasks remain challenging, even for state-of-the-art models.",
        "url": "http://arxiv.org/abs/2510.17013v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17013v1",
        "arxiv_id": "2510.17013v1",
        "authors": [
            "Lanni Bu",
            "Lauren Levin",
            "Amir Zeldes"
        ],
        "submitted": "2025-10-19 21:26:27",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper presents a multilingual benchmark for discourse tracking, which involves integrating and aggregating information across sentences, paragraphs, and speaker utterances. While it touches on natural language understanding and pragmatic inferences, it is more focused on discourse tracking than query understanding, ranking models, or user behavior modeling. It may be of interest due to its connection to deep semantic understanding, but it is not a central match for your research interests."
    },
    {
        "title": "Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization",
        "abstract": "Iterative jailbreak methods that repeatedly rewrite and input prompts into\nlarge language models (LLMs) to induce harmful outputs -- using the model's\nprevious responses to guide each new iteration -- have been found to be a\nhighly effective attack strategy. Despite being an effective attack strategy\nagainst LLMs and their safety mechanisms, existing defenses do not proactively\ndisrupt this dynamic trial-and-error cycle. In this study, we propose a novel\nframework that dynamically updates its defense strategy through online learning\nin response to each new prompt from iterative jailbreak methods. Leveraging the\ndistinctions between harmful jailbreak-generated prompts and typical harmless\nprompts, we introduce a reinforcement learning-based approach that optimizes\nprompts to ensure appropriate responses for harmless tasks while explicitly\nrejecting harmful prompts. Additionally, to curb overfitting to the narrow band\nof partial input rewrites explored during an attack, we introduce\nPast-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs\nshow that our approach significantly outperforms five existing defense methods\nagainst five iterative jailbreak methods. Moreover, our results indicate that\nour prompt optimization strategy simultaneously enhances response quality for\nharmless tasks.",
        "url": "http://arxiv.org/abs/2510.17006v1",
        "pdf_url": "http://arxiv.org/pdf/2510.17006v1",
        "arxiv_id": "2510.17006v1",
        "authors": [
            "Masahiro Kaneko",
            "Zeerak Talat",
            "Timothy Baldwin"
        ],
        "submitted": "2025-10-19 21:07:21",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval, Search technologies, or user behavior modeling, which are your core research interests. Although it involves Natural Language Processing, the focus is on defending against language model attacks, which is not a central match to your research themes."
    }
]