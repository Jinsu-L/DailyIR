[
    {
        "title": "A Large-Scale Web Search Dataset for Federated Online Learning to Rank",
        "abstract": "The centralized collection of search interaction logs for training ranking\nmodels raises significant privacy concerns. Federated Online Learning to Rank\n(FOLTR) offers a privacy-preserving alternative by enabling collaborative model\ntraining without sharing raw user data. However, benchmarks in FOLTR are\nlargely based on random partitioning of classical learning-to-rank datasets,\nsimulated user clicks, and the assumption of synchronous client participation.\nThis oversimplifies real-world dynamics and undermines the realism of\nexperimental results. We present AOL4FOLTR, a large-scale web search dataset\nwith 2.6 million queries from 10,000 users. Our dataset addresses key\nlimitations of existing benchmarks by including user identifiers, real click\ndata, and query timestamps, enabling realistic user partitioning, behavior\nmodeling, and asynchronous federated learning scenarios.",
        "url": "http://arxiv.org/abs/2508.12353v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12353v1",
        "arxiv_id": "2508.12353v1",
        "authors": [
            "Marcel Gregoriadis",
            "Jingwei Kang",
            "Johan Pouwelse"
        ],
        "submitted": "2025-08-17 12:57:54",
        "source": "arxiv",
        "comment": "Accepted at CIKM 2025",
        "score": 20,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'learning to rank' (score: +3)",
            "Found 'ltr' (score: +3)",
            "Found 'click' (score: +2)",
            "Found 'web search' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "The paper is highly relevant to your research interests in Information Retrieval, particularly in the area of Learning to Rank and user behavior modeling. The focus on federated online learning and the presentation of a large-scale web search dataset with realistic user data and query timestamps aligns with your interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks",
        "abstract": "Medical large vision-language Models (Med-LVLMs) have shown promise in\nclinical applications but suffer from factual inaccuracies and unreliable\noutputs, posing risks in real-world diagnostics. While retrieval-augmented\ngeneration has emerged as a potential solution, current medical multimodal RAG\nsystems are unable to perform effective retrieval across heterogeneous sources.\nThe irrelevance of retrieved reports affects the factuality of analysis, while\ninsufficient knowledge affects the credibility of clinical decision-making. To\nbridge the gap, we construct MedAtlas, which includes extensive multimodal\nreport repositories and diverse text corpora. Based on it, we present\nHeteroRAG, a novel framework that enhances Med-LVLMs through heterogeneous\nknowledge sources. The framework introduces Modality-specific CLIPs for\neffective report retrieval and a Multi-corpora Query Generator for dynamically\nconstructing queries for diverse corpora. Incorporating knowledge from such\nmultifaceted sources, Med-LVLM is then trained with Heterogeneous Knowledge\nPreference Tuning to achieve cross-modality and multi-source knowledge\nalignment. Extensive experiments across 12 datasets and 3 modalities\ndemonstrate that the proposed HeteroRAG achieves state-of-the-art performance\nin most medical vision language benchmarks, significantly improving factual\naccuracy and reliability of Med-LVLMs.",
        "url": "http://arxiv.org/abs/2508.12778v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12778v1",
        "arxiv_id": "2508.12778v1",
        "authors": [
            "Zhe Chen",
            "Yusheng Liao",
            "Shuyang Jiang",
            "Zhiyuan Zhu",
            "Haolin Li",
            "Yanfeng Wang",
            "Yu Wang"
        ],
        "submitted": "2025-08-18 09:54:10",
        "source": "arxiv",
        "comment": null,
        "score": 13,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on medical vision-language tasks, retrieval-augmented generation, and multimodal report repositories, which are not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on medical applications and clinical decision-making also diverges from the user's e-commerce background and general interest in real-time relevance optimization."
    },
    {
        "title": "Ontology-Guided Query Expansion for Biomedical Document Retrieval using Large Language Models",
        "abstract": "Effective Question Answering (QA) on large biomedical document collections\nrequires effective document retrieval techniques. The latter remains a\nchallenging task due to the domain-specific vocabulary and semantic ambiguity\nin user queries. We propose BMQExpander, a novel ontology-aware query expansion\npipeline that combines medical knowledge - definitions and relationships - from\nthe UMLS Metathesaurus with the generative capabilities of large language\nmodels (LLMs) to enhance retrieval effectiveness. We implemented several\nstate-of-the-art baselines, including sparse and dense retrievers, query\nexpansion methods, and biomedical-specific solutions. We show that BMQExpander\nhas superior retrieval performance on three popular biomedical Information\nRetrieval (IR) benchmarks: NFCorpus, TREC-COVID, and SciFact - with\nimprovements of up to 22.1% in NDCG@10 over sparse baselines and up to 6.5%\nover the strongest baseline. Further, BMQExpander generalizes robustly under\nquery perturbation settings, in contrast to supervised baselines, achieving up\nto 15.7% improvement over the strongest baseline. As a side contribution, we\npublish our paraphrased benchmarks. Finally, our qualitative analysis shows\nthat BMQExpander has fewer hallucinations compared to other LLM-based query\nexpansion baselines.",
        "url": "http://arxiv.org/abs/2508.11784v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11784v1",
        "arxiv_id": "2508.11784v1",
        "authors": [
            "Zabir Al Nazi",
            "Vagelis Hristidis",
            "Aaron Lawson McLean",
            "Jannat Ara Meem",
            "Md Taukir Azam Chowdhury"
        ],
        "submitted": "2025-08-15 19:23:26",
        "source": "arxiv",
        "comment": null,
        "score": 13,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'trec' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "The paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The use of large language models and ontology-guided query expansion is a novel approach that aligns with your focus on deep semantic understanding and real-time relevance optimization. While the paper is focused on biomedical document retrieval, the techniques and concepts explored can be applied to other domains, including e-commerce."
    },
    {
        "title": "SEA-BED: Southeast Asia Embedding Benchmark",
        "abstract": "Sentence embeddings are essential for NLP tasks such as semantic search,\nre-ranking, and textual similarity. Although multilingual benchmarks like MMTEB\nbroaden coverage, Southeast Asia (SEA) datasets are scarce and often\nmachine-translated, missing native linguistic properties. With nearly 700\nmillion speakers, the SEA region lacks a region-specific embedding benchmark.\nWe introduce SEA-BED, the first large-scale SEA embedding benchmark with 169\ndatasets across 9 tasks and 10 languages, where 71% are formulated by humans,\nnot machine generation or translation. We address three research questions: (1)\nwhich SEA languages and tasks are challenging, (2) whether SEA languages show\nunique performance gaps globally, and (3) how human vs. machine translations\naffect evaluation. We evaluate 17 embedding models across six studies,\nanalyzing task and language challenges, cross-benchmark comparisons, and\ntranslation trade-offs. Results show sharp ranking shifts, inconsistent model\nperformance among SEA languages, and the importance of human-curated datasets\nfor low-resource languages like Burmese.",
        "url": "http://arxiv.org/abs/2508.12243v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12243v1",
        "arxiv_id": "2508.12243v1",
        "authors": [
            "Wuttikorn Ponwitayarat",
            "Raymond Ng",
            "Jann Railey Montalan",
            "Thura Aung",
            "Jian Gang Ngui",
            "Yosephine Susanto",
            "William Tjhi",
            "Panuthep Tasawong",
            "Erik Cambria",
            "Ekapol Chuangsuwanich",
            "Sarana Nutanong",
            "Peerat Limkonchotiwat"
        ],
        "submitted": "2025-08-17 05:10:40",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'semantic search' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on sentence embeddings and their applications in NLP tasks, which is related to my interest in Natural Language Processing. However, the specific context of Southeast Asia and the emphasis on multilingual benchmarks and human-curated datasets are not directly aligned with my primary focus on Information Retrieval and query understanding."
    },
    {
        "title": "TaoSR1: The Thinking Model for E-commerce Relevance Search",
        "abstract": "Query-product relevance prediction is a core task in e-commerce search.\nBERT-based models excel at semantic matching but lack complex reasoning\ncapabilities. While Large Language Models (LLMs) are explored, most still use\ndiscriminative fine-tuning or distill to smaller models for deployment. We\npropose a framework to directly deploy LLMs for this task, addressing key\nchallenges: Chain-of-Thought (CoT) error accumulation, discriminative\nhallucination, and deployment feasibility. Our framework, TaoSR1, involves\nthree stages: (1) Supervised Fine-Tuning (SFT) with CoT to instill reasoning;\n(2) Offline sampling with a pass@N strategy and Direct Preference Optimization\n(DPO) to improve generation quality; and (3) Difficulty-based dynamic sampling\nwith Group Relative Policy Optimization (GRPO) to mitigate discriminative\nhallucination. Additionally, post-CoT processing and a cumulative\nprobability-based partitioning method enable efficient online deployment.\nTaoSR1 significantly outperforms baselines on offline datasets and achieves\nsubstantial gains in online side-by-side human evaluations, introducing a novel\nparadigm for applying CoT reasoning to relevance classification.",
        "url": "http://arxiv.org/abs/2508.12365v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12365v1",
        "arxiv_id": "2508.12365v1",
        "authors": [
            "Chenhe Dong",
            "Shaowei Yao",
            "Pengkun Jiao",
            "Jianhui Yang",
            "Yiming Jin",
            "Zerui Huang",
            "Xiaojiang Zhou",
            "Dan Ou",
            "Haihong Tang"
        ],
        "submitted": "2025-08-17 13:48:48",
        "source": "arxiv",
        "comment": null,
        "score": 9,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "The paper focuses on query-product relevance prediction in e-commerce search, which is a specific application of Information Retrieval. The use of BERT-based models and Large Language Models (LLMs) aligns with my interest in query understanding and ranking models. While the paper's primary focus is on e-commerce, the techniques and ideas presented can be applied to other domains, making it somewhat related to my broader research interests in IR and NLP."
    },
    {
        "title": "Can we Evaluate RAGs with Synthetic Data?",
        "abstract": "We investigate whether synthetic question-answer (QA) data generated by large\nlanguage models (LLMs) can serve as an effective proxy for human-labeled\nbenchmarks when such data is unavailable. We assess the reliability of\nsynthetic benchmarks across two experiments: one varying retriever parameters\nwhile keeping the generator fixed, and another varying the generator with fixed\nretriever parameters. Across four datasets, of which two open-domain and two\nproprietary, we find that synthetic benchmarks reliably rank the RAGs varying\nin terms of retriever configuration, aligning well with human-labeled benchmark\nbaselines. However, they fail to produce consistent RAG rankings when comparing\ngenerator architectures. The breakdown possibly arises from a combination of\ntask mismatch between the synthetic and human benchmarks, and stylistic bias\nfavoring certain generators.",
        "url": "http://arxiv.org/abs/2508.11758v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11758v1",
        "arxiv_id": "2508.11758v1",
        "authors": [
            "Jonas van Elburg",
            "Peter van der Putten",
            "Maarten Marx"
        ],
        "submitted": "2025-08-15 18:07:47",
        "source": "arxiv",
        "comment": "Accepted for the SynDAiTE workshop at the European Conference on\n  Machine Learning and Principles and Practice of Knowledge Discovery in\n  Databases (ECML-PKDD 2025), September 15, 2025 - Porto, Portugal",
        "score": 9,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the use of synthetic data for evaluating Retrieval-Augmented Generation (RAG) models, which is a topic related to Information Retrieval. However, the focus is on the evaluation methodology rather than query understanding, ranking models, or user behavior modeling, which are the core areas of interest for the user."
    },
    {
        "title": "OptimalThinkingBench: Evaluating Over and Underthinking in LLMs",
        "abstract": "Thinking LLMs solve complex tasks at the expense of increased compute and\noverthinking on simpler problems, while non-thinking LLMs are faster and\ncheaper but underthink on harder reasoning problems. This has led to the\ndevelopment of separate thinking and non-thinking LLM variants, leaving the\nonus of selecting the optimal model for each query on the end user. In this\nwork, we introduce OptimalThinkingBench, a unified benchmark that jointly\nevaluates overthinking and underthinking in LLMs and also encourages the\ndevelopment of optimally-thinking models that balance performance and\nefficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench,\nfeaturing simple queries in 72 domains, and UnderthinkingBench, containing 11\nchallenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, we\nperform extensive evaluation of 33 different thinking and non-thinking models\nand show that no model is able to optimally think on our benchmark. Thinking\nmodels often overthink for hundreds of tokens on the simplest user queries\nwithout improving performance. In contrast, large non-thinking models\nunderthink, often falling short of much smaller thinking models. We further\nexplore several methods to encourage optimal thinking, but find that these\napproaches often improve on one sub-benchmark at the expense of the other,\nhighlighting the need for better unified and optimal models in the future.",
        "url": "http://arxiv.org/abs/2508.13141v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13141v1",
        "arxiv_id": "2508.13141v1",
        "authors": [
            "Pranjal Aggarwal",
            "Seungone Kim",
            "Jack Lanchantin",
            "Sean Welleck",
            "Jason Weston",
            "Ilia Kulikov",
            "Swarnadeep Saha"
        ],
        "submitted": "2025-08-18 17:53:10",
        "source": "arxiv",
        "comment": "26 pages, 6 tables, 10 figures",
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Large Language Models (LLMs) and their thinking/overthinking behavior, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on the concept of 'optimal thinking', it does not address ranking models, user behavior modeling, or real-time relevance optimization, making it only loosely relevant to the user's research interests."
    },
    {
        "title": "All for law and law for all: Adaptive RAG Pipeline for Legal Research",
        "abstract": "Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding\nlarge language model outputs in cited sources, a capability that is especially\ncritical in the legal domain. We present an end-to-end RAG pipeline that\nrevisits and extends the LegalBenchRAG baseline with three targeted\nenhancements: (i) a context-aware query translator that disentangles document\nreferences from natural-language questions and adapts retrieval depth and\nresponse style based on expertise and specificity, (ii) open-source retrieval\nstrategies using SBERT and GTE embeddings that achieve substantial performance\ngains (improving Recall@K by 30-95\\% and Precision@K by $\\sim$2.5$\\times$ for\n$K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and\ngeneration framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to\nassess semantic alignment and faithfulness across models and prompt designs.\nOur results show that carefully designed open-source pipelines can rival or\noutperform proprietary approaches in retrieval quality, while a custom\nlegal-grounded prompt consistently produces more faithful and contextually\nrelevant answers than baseline prompting. Taken together, these contributions\ndemonstrate the potential of task-aware, component-level tuning to deliver\nlegally grounded, reproducible, and cost-effective RAG systems for legal\nresearch assistance.",
        "url": "http://arxiv.org/abs/2508.13107v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13107v1",
        "arxiv_id": "2508.13107v1",
        "authors": [
            "Figarri Keisha",
            "Prince Singh",
            "Pallavi",
            "Dion Fernandes",
            "Aravindh Manivannan",
            "Ilham Wicaksono",
            "Faisal Ahmad"
        ],
        "submitted": "2025-08-18 17:14:03",
        "source": "arxiv",
        "comment": "submitted to NLLP 2025 Workshop",
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 7,
        "llm_reason": "The paper presents a Retrieval-Augmented Generation (RAG) pipeline for legal research, which is relevant to information retrieval and search technologies. The use of query translation, open-source retrieval strategies, and evaluation frameworks aligns with the user's interests in query understanding and ranking models. However, the focus on the legal domain and RAG systems for legal research assistance is somewhat specific and not directly related to the user's primary research themes."
    },
    {
        "title": "A Multi-Task Evaluation of LLMs' Processing of Academic Text Input",
        "abstract": "How much large language models (LLMs) can aid scientific discovery, notably\nin assisting academic peer review, is in heated debate. Between a literature\ndigest and a human-comparable research assistant lies their practical\napplication potential. We organize individual tasks that computer science\nstudies employ in separate terms into a guided and robust workflow to evaluate\nLLMs' processing of academic text input. We employ four tasks in the\nassessment: content reproduction/comparison/scoring/reflection, each demanding\na specific role of the LLM (oracle/judgmental arbiter/knowledgeable\narbiter/collaborator) in assisting scholarly works, and altogether testing LLMs\nwith questions that increasingly require intellectual capabilities towards a\nsolid understanding of scientific texts to yield desirable solutions. We\nexemplify a rigorous performance evaluation with detailed instructions on the\nprompts. Adopting first-rate Information Systems articles at three top journals\nas the input texts and an abundant set of text metrics, we record a compromised\nperformance of the leading LLM - Google's Gemini: its summary and paraphrase of\nacademic text is acceptably reliable; using it to rank texts through pairwise\ntext comparison is faintly scalable; asking it to grade academic texts is prone\nto poor discrimination; its qualitative reflection on the text is\nself-consistent yet hardly insightful to inspire meaningful research. This\nevidence against an endorsement of LLMs' text-processing capabilities is\nconsistent across metric-based internal (linguistic assessment), external\n(comparing to the ground truth), and human evaluation, and is robust to the\nvariations of the prompt. Overall, we do not recommend an unchecked use of LLMs\nin constructing peer reviews.",
        "url": "http://arxiv.org/abs/2508.11779v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11779v1",
        "arxiv_id": "2508.11779v1",
        "authors": [
            "Tianyi Li",
            "Yu Qin",
            "Olivia R. Liu Sheng"
        ],
        "submitted": "2025-08-15 19:05:57",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper evaluates the capabilities of large language models (LLMs) in processing academic text input, which is related to information retrieval and natural language processing. However, the focus is more on the evaluation of LLMs' capabilities rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user."
    },
    {
        "title": "AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation",
        "abstract": "Incident response (IR) requires fast, coordinated, and well-informed\ndecision-making to contain and mitigate cyber threats. While large language\nmodels (LLMs) have shown promise as autonomous agents in simulated IR settings,\ntheir reasoning is often limited by a lack of access to external knowledge. In\nthis work, we present AutoBnB-RAG, an extension of the AutoBnB framework that\nincorporates retrieval-augmented generation (RAG) into multi-agent incident\nresponse simulations. Built on the Backdoors & Breaches (B&B) tabletop game\nenvironment, AutoBnB-RAG enables agents to issue retrieval queries and\nincorporate external evidence during collaborative investigations. We introduce\ntwo retrieval settings: one grounded in curated technical documentation\n(RAG-Wiki), and another using narrative-style incident reports (RAG-News). We\nevaluate performance across eight team structures, including newly introduced\nargumentative configurations designed to promote critical reasoning. To\nvalidate practical utility, we also simulate real-world cyber incidents based\non public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct\ncomplex multi-stage attacks. Our results show that retrieval augmentation\nimproves decision quality and success rates across diverse organizational\nmodels. This work demonstrates the value of integrating retrieval mechanisms\ninto LLM-based multi-agent systems for cybersecurity decision-making.",
        "url": "http://arxiv.org/abs/2508.13118v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13118v1",
        "arxiv_id": "2508.13118v1",
        "authors": [
            "Zefang Liu",
            "Arman Anwar"
        ],
        "submitted": "2025-08-18 17:22:51",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to information retrieval, as it discusses retrieval-augmented generation in the context of multi-agent incident response. However, the focus is on cybersecurity and decision-making, which is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is limited to the user's secondary interest in NLP, but even then, it is not a central match."
    },
    {
        "title": "CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description",
        "abstract": "Recent advances in large language models (LLMs) have significantly improved\nthe accuracy of Text-to-SQL systems. However, a critical challenge remains: the\nsemantic mismatch between natural language questions (NLQs) and their\ncorresponding SQL queries. This issue is exacerbated in large-scale databases,\nwhere semantically similar attributes hinder schema linking and semantic drift\nduring SQL generation, ultimately reducing model accuracy. To address these\nchallenges, we introduce CRED-SQL, a framework designed for large-scale\ndatabases that integrates Cluster Retrieval and Execution Description. CRED-SQL\nfirst performs cluster-based large-scale schema retrieval to pinpoint the\ntables and columns most relevant to a given NLQ, alleviating schema mismatch.\nIt then introduces an intermediate natural language representation-Execution\nDescription Language (EDL)-to bridge the gap between NLQs and SQL. This\nreformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,\nleveraging LLMs' strong general reasoning capabilities while reducing semantic\ndeviation. Extensive experiments on two large-scale, cross-domain\nbenchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new\nstate-of-the-art (SOTA) performance, validating its effectiveness and\nscalability. Our code is available at https://github.com/smduan/CRED-SQL.git",
        "url": "http://arxiv.org/abs/2508.12769v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12769v1",
        "arxiv_id": "2508.12769v1",
        "authors": [
            "Shaoming Duan",
            "Zirui Wang",
            "Chuanyi Liu",
            "Zhibin Zhu",
            "Yuhao Zhang",
            "Peiyi Han",
            "Liang Yan",
            "Zewu Penge"
        ],
        "submitted": "2025-08-18 09:43:07",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Text-to-SQL parsing, which is not directly related to Information Retrieval (IR) or Search technologies. Although it mentions large language models, the primary goal is to improve SQL generation, which is not a core interest of yours."
    },
    {
        "title": "VimoRAG: Video-based Retrieval-augmented 3D Motion Generation for Motion Language Models",
        "abstract": "This paper introduces VimoRAG, a novel video-based retrieval-augmented motion\ngeneration framework for motion large language models (LLMs). As motion LLMs\nface severe out-of-domain/out-of-vocabulary issues due to limited annotated\ndata, VimoRAG leverages large-scale in-the-wild video databases to enhance 3D\nmotion generation by retrieving relevant 2D human motion signals. While\nvideo-based motion RAG is nontrivial, we address two key bottlenecks: (1)\ndeveloping an effective motion-centered video retrieval model that\ndistinguishes human poses and actions, and (2) mitigating the issue of error\npropagation caused by suboptimal retrieval results. We design the Gemini Motion\nVideo Retriever mechanism and the Motion-centric Dual-alignment DPO Trainer,\nenabling effective retrieval and generation processes. Experimental results\nshow that VimoRAG significantly boosts the performance of motion LLMs\nconstrained to text-only input.",
        "url": "http://arxiv.org/abs/2508.12081v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12081v1",
        "arxiv_id": "2508.12081v1",
        "authors": [
            "Haidong Xu",
            "Guangwei Xu",
            "Zhedong Zheng",
            "Xiatian Zhu",
            "Wei Ji",
            "Xiangtai Li",
            "Ruijie Guo",
            "Meishan Zhang",
            "Min zhang",
            "Hao Fei"
        ],
        "submitted": "2025-08-16 15:31:14",
        "source": "arxiv",
        "comment": "20 pages,13 figures",
        "score": 7,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on video-based retrieval-augmented motion generation for motion language models, which is not directly related to information retrieval, search technologies, or query understanding. While it involves retrieval, it is a specific application in the domain of computer vision and natural language processing, and does not align with the user's primary research interests."
    },
    {
        "title": "Leveraging Geometric Insights in Hyperbolic Triplet Loss for Improved Recommendations",
        "abstract": "Recent studies have demonstrated the potential of hyperbolic geometry for\ncapturing complex patterns from interaction data in recommender systems. In\nthis work, we introduce a novel hyperbolic recommendation model that uses\ngeometrical insights to improve representation learning and increase\ncomputational stability at the same time. We reformulate the notion of\nhyperbolic distances to unlock additional representation capacity over\nconventional Euclidean space and learn more expressive user and item\nrepresentations. To better capture user-items interactions, we construct a\ntriplet loss that models ternary relations between users and their\ncorresponding preferred and nonpreferred choices through a mix of pairwise\ninteraction terms driven by the geometry of data. Our hyperbolic approach not\nonly outperforms existing Euclidean and hyperbolic models but also reduces\npopularity bias, leading to more diverse and personalized recommendations.",
        "url": "http://arxiv.org/abs/2508.11978v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11978v1",
        "arxiv_id": "2508.11978v1",
        "authors": [
            "Viacheslav Yusupov",
            "Maxim Rakhuba",
            "Evgeny Frolov"
        ],
        "submitted": "2025-08-16 08:34:17",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on recommender systems, which is a related topic, but it does not address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The use of hyperbolic geometry and triplet loss is innovative, but it is not directly applicable to information retrieval or search technologies."
    },
    {
        "title": "Is This News Still Interesting to You?: Lifetime-aware Interest Matching for News Recommendation",
        "abstract": "Personalized news recommendation aims to deliver news articles aligned with\nusers' interests, serving as a key solution to alleviate the problem of\ninformation overload on online news platforms. While prior work has improved\ninterest matching through refined representations of news and users, the\nfollowing time-related challenges remain underexplored: (C1) leveraging the age\nof clicked news to infer users' interest persistence, and (C2) modeling the\nvarying lifetime of news across topics and users. To jointly address these\nchallenges, we propose a novel Lifetime-aware Interest Matching framework for\nnEws recommendation, named LIME, which incorporates three key strategies: (1)\nUser-Topic lifetime-aware age representation to capture the relative age of\nnews with respect to a user-topic pair, (2) Candidate-aware lifetime attention\nfor generating temporally aligned user representation, and (3) Freshness-guided\ninterest refinement for prioritizing valid candidate news at prediction time.\nExtensive experiments on two real-world datasets demonstrate that LIME\nconsistently outperforms a wide range of state-of-the-art news recommendation\nmethods, and its model agnostic strategies significantly improve recommendation\naccuracy.",
        "url": "http://arxiv.org/abs/2508.13064v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13064v1",
        "arxiv_id": "2508.13064v1",
        "authors": [
            "Seongeun Ryu",
            "Yunyong Ko",
            "Sang-Wook Kim"
        ],
        "submitted": "2025-08-18 16:36:27",
        "source": "arxiv",
        "comment": "10 pages, 7 figures, 4 tables, accepted at ACM International\n  Conference on Information and Knowledge Management (CIKM)",
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'click' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper focuses on news recommendation, which is related to information retrieval and search technologies. The use of lifetime-aware interest matching and attention mechanisms is somewhat relevant to query understanding and ranking models. However, the paper's primary focus on news recommendation and its emphasis on freshness and temporal alignment are not directly aligned with the user's interests in deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "D-RDW: Diversity-Driven Random Walks for News Recommender Systems",
        "abstract": "This paper introduces Diversity-Driven RandomWalks (D-RDW), a lightweight\nalgorithm and re-ranking technique that generates diverse news recommendations.\nD-RDW is a societal recommender, which combines the diversification\ncapabilities of the traditional random walk algorithms with customizable target\ndistributions of news article properties. In doing so, our model provides a\ntransparent approach for editors to incorporate norms and values into the\nrecommendation process. D-RDW shows enhanced performance across key diversity\nmetrics that consider the articles' sentiment and political party mentions when\ncompared to state-of-the-art neural models. Furthermore, D-RDW proves to be\nmore computationally efficient than existing approaches.",
        "url": "http://arxiv.org/abs/2508.13035v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13035v1",
        "arxiv_id": "2508.13035v1",
        "authors": [
            "Runze Li",
            "Lucien Heitz",
            "Oana Inel",
            "Abraham Bernstein"
        ],
        "submitted": "2025-08-18 15:53:30",
        "source": "arxiv",
        "comment": "6 pages",
        "score": 5,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on recommender systems, which is a related topic, but it's not directly aligned with the user's primary interest in Information Retrieval and Search technologies. The use of random walks and diversity-driven approaches is novel, but the application in news recommender systems is not directly applicable to the user's e-commerce background or interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Informfully Recommenders -- Reproducibility Framework for Diversity-aware Intra-session Recommendations",
        "abstract": "Norm-aware recommender systems have gained increased attention, especially\nfor diversity optimization. The recommender systems community has\nwell-established experimentation pipelines that support reproducible\nevaluations by facilitating models' benchmarking and comparisons against\nstate-of-the-art methods. However, to the best of our knowledge, there is\ncurrently no reproducibility framework to support thorough norm-driven\nexperimentation at the pre-processing, in-processing, post-processing, and\nevaluation stages of the recommender pipeline. To address this gap, we present\nInformfully Recommenders, a first step towards a normative reproducibility\nframework that focuses on diversity-aware design built on Cornac. Our extension\nprovides an end-to-end solution for implementing and experimenting with\nnormative and general-purpose diverse recommender systems that cover 1) dataset\npre-processing, 2) diversity-optimized models, 3) dedicated intrasession item\nre-ranking, and 4) an extensive set of diversity metrics. We demonstrate the\ncapabilities of our extension through an extensive offline experiment in the\nnews domain.",
        "url": "http://arxiv.org/abs/2508.13019v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13019v1",
        "arxiv_id": "2508.13019v1",
        "authors": [
            "Lucien Heitz",
            "Runze Li",
            "Oana Inel",
            "Abraham Bernstein"
        ],
        "submitted": "2025-08-18 15:37:41",
        "source": "arxiv",
        "comment": "10 pages",
        "score": 5,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on recommender systems, which is a related topic, but it does not specifically address query understanding, ranking models, or user behavior modeling, which are core interests. The paper's emphasis on diversity-aware design and normative reproducibility framework is not directly relevant to the user's research themes."
    },
    {
        "title": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward",
        "abstract": "Large language models (LLMs) exhibit remarkable problem-solving abilities,\nbut struggle with complex tasks due to static internal knowledge.\nRetrieval-Augmented Generation (RAG) enhances access to external information,\nyet remains limited in multi-hop reasoning and strategic search due to rigid\nworkflows. Recent advancements in agentic deep research empower LLMs to\nautonomously reason, search, and synthesize information. However, current\napproaches relying on outcome-based reinforcement learning (RL) face critical\nissues such as conflicting gradients and reward sparsity, limiting performance\ngains and training efficiency. To address these, we first propose Atomic\nThought, a novel LLM thinking paradigm that decomposes reasoning into\nfine-grained functional units. These units are supervised by Reasoning Reward\nModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained\nguidance. Building on this, we propose Atom-Searcher, a novel RL framework for\nagentic deep research that integrates Atomic Thought and ATR. Atom-Searcher\nuses a curriculum-inspired reward schedule, prioritizing process-level ATR\nearly and transitioning to outcome rewards, accelerating convergence on\neffective reasoning paths. Experiments on seven benchmarks show consistent\nimprovements over the state-of-the-art. Key advantages include: (1)\nAtom-Searcher scales computation at test-time. (2) Atomic Thought provides\nsupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)\nAtom-Searcher exhibits more interpretable, human-like reasoning patterns.",
        "url": "http://arxiv.org/abs/2508.12800v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12800v1",
        "arxiv_id": "2508.12800v1",
        "authors": [
            "Yong Deng",
            "Guoqing Wang",
            "Zhenzhe Ying",
            "Xiaofeng Wu",
            "Jinzhen Lin",
            "Wenwen Xiong",
            "Yuqin Dai",
            "Shuo Yang",
            "Zhanwei Zhang",
            "Qiwen Wang",
            "Yang Qin",
            "Changhua Meng"
        ],
        "submitted": "2025-08-18 10:23:10",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a novel reinforcement learning framework for agentic deep research, which is somewhat related to information retrieval and search technologies. However, the focus on large language models and reasoning paradigms is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing",
        "abstract": "Balancing performance and efficiency is a central challenge in large language\nmodel (LLM) advancement. GPT-5 addresses this with test-time routing,\ndynamically assigning queries to either an efficient or a high-capacity model\nduring inference. In this work, we present Avengers-Pro, a test-time routing\nframework that ensembles LLMs of varying capacities and efficiencies, providing\na unified solution for all performance-efficiency tradeoffs. The Avengers-Pro\nembeds and clusters incoming queries, then routes each to the most suitable\nmodel based on a performance-efficiency score. Across 6 challenging benchmarks\nand 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and\nClaude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a\nperformance-efficiency trade-off parameter, it can surpass the strongest single\nmodel (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the\naverage accuracy of the strongest single model at 27% lower cost, and reach\n~90% of that performance at 63% lower cost. Last but not least, it achieves a\nPareto frontier, consistently yielding the highest accuracy for any given cost,\nand the lowest cost for any given accuracy, among all single models. Code is\navailable at https://github.com/ZhangYiqun018/AvengersPro.",
        "url": "http://arxiv.org/abs/2508.12631v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12631v1",
        "arxiv_id": "2508.12631v1",
        "authors": [
            "Yiqun Zhang",
            "Hao Li",
            "Jianhao Chen",
            "Hangfan Zhang",
            "Peng Ye",
            "Lei Bai",
            "Shuyue Hu"
        ],
        "submitted": "2025-08-18 05:23:31",
        "source": "arxiv",
        "comment": "Ongoing work",
        "score": 5,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on large language models and their performance-efficiency tradeoffs, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and query understanding. While the paper mentions routing and optimization, it does not address ranking models, user behavior modeling, or deep semantic understanding, making it only loosely relevant to the user's interests."
    },
    {
        "title": "MOON: Generative MLLM-based Multimodal Representation Learning for E-commerce Product Understanding",
        "abstract": "With the rapid advancement of e-commerce, exploring general representations\nrather than task-specific ones has attracted increasing research attention. For\nproduct understanding, although existing discriminative dual-flow architectures\ndrive progress in this field, they inherently struggle to model the many-to-one\nalignment between multiple images and texts of products. Therefore, we argue\nthat generative Multimodal Large Language Models (MLLMs) hold significant\npotential for improving product representation learning. Nevertheless,\nachieving this goal still remains non-trivial due to several key challenges:\nthe lack of multimodal and aspect-aware modeling modules in typical LLMs; the\ncommon presence of background noise in product images; and the absence of a\nstandard benchmark for evaluation. To address these issues, we propose the\nfirst generative MLLM-based model named MOON for product representation\nlearning. Our method (1) employs a guided Mixture-of-Experts (MoE) module for\ntargeted modeling of multimodal and aspect-specific product content; (2)\neffectively detects core semantic regions in product images to mitigate the\ndistraction and interference caused by background noise; and (3) introduces the\nspecialized negative sampling strategy to increase the difficulty and diversity\nof negative samples. In addition, we release a large-scale multimodal benchmark\nMBE for various product understanding tasks. Experimentally, our model\ndemonstrates competitive zero-shot performance on both our benchmark and the\npublic dataset, showcasing strong generalization across various downstream\ntasks, including cross-modal retrieval, product classification, and attribute\nprediction. Furthermore, the case study and visualization illustrate the\neffectiveness of MOON for product understanding.",
        "url": "http://arxiv.org/abs/2508.11999v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11999v1",
        "arxiv_id": "2508.11999v1",
        "authors": [
            "Daoze Zhang",
            "Zhanheng Nie",
            "Jianyu Liu",
            "Chenghan Fu",
            "Wanxian Guan",
            "Yuan Gao",
            "Jun Song",
            "Pengjie Wang",
            "Jian Xu",
            "Bo Zheng"
        ],
        "submitted": "2025-08-16 09:59:25",
        "source": "arxiv",
        "comment": "11 pages, 9 figures",
        "score": 5,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper focuses on multimodal representation learning for e-commerce product understanding, which is related to my interest in Information Retrieval and Search technologies. However, the specific application in e-commerce and the emphasis on generative models are not directly aligned with my core research themes. The paper's relevance is somewhat related, but not a central match."
    },
    {
        "title": "TBGRecall: A Generative Retrieval Model for E-commerce Recommendation Scenarios",
        "abstract": "Recommendation systems are essential tools in modern e-commerce, facilitating\npersonalized user experiences by suggesting relevant products. Recent\nadvancements in generative models have demonstrated potential in enhancing\nrecommendation systems; however, these models often exhibit limitations in\noptimizing retrieval tasks, primarily due to their reliance on autoregressive\ngeneration mechanisms. Conventional approaches introduce sequential\ndependencies that impede efficient retrieval, as they are inherently unsuitable\nfor generating multiple items without positional constraints within a single\nrequest session. To address these limitations, we propose TBGRecall, a\nframework integrating Next Session Prediction (NSP), designed to enhance\ngenerative retrieval models for e-commerce applications. Our framework\nreformulation involves partitioning input samples into multi-session sequences,\nwhere each sequence comprises a session token followed by a set of item tokens,\nand then further incorporate multiple optimizations tailored to the generative\ntask in retrieval scenarios. In terms of training methodology, our pipeline\nintegrates limited historical data pre-training with stochastic partial\nincremental training, significantly improving training efficiency and\nemphasizing the superiority of data recency over sheer data volume. Our\nextensive experiments, conducted on public benchmarks alongside a large-scale\nindustrial dataset from TaoBao, show TBGRecall outperforms the state-of-the-art\nrecommendation methods, and exhibits a clear scaling law trend. Ultimately, NSP\nrepresents a significant advancement in the effectiveness of generative\nrecommendation systems for e-commerce applications.",
        "url": "http://arxiv.org/abs/2508.11977v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11977v1",
        "arxiv_id": "2508.11977v1",
        "authors": [
            "Zida Liang",
            "Changfa Wu",
            "Dunxian Huang",
            "Weiqiang Sun",
            "Ziyang Wang",
            "Yuliang Yan",
            "Jian Wu",
            "Yuning Jiang",
            "Bo Zheng",
            "Ke Chen",
            "Silu Zhou",
            "Yu Zhang"
        ],
        "submitted": "2025-08-16 08:31:11",
        "source": "arxiv",
        "comment": "Both authors contributed equally to this research. Work done during\n  internship at Alibaba. Corresponding author: Dunxian Huang\n  (dunxian.hdx@alibaba-inc.com). Affiliations: (1) Shanghai Jiaotong\n  University, Shanghai, China; (2) Alibaba Inc",
        "score": 5,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on e-commerce recommendation systems, which is related to the user's background in the e-commerce domain. However, the paper's emphasis on generative models and next session prediction is not directly aligned with the user's primary interests in information retrieval, query understanding, and ranking models."
    },
    {
        "title": "WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents",
        "abstract": "LLM-based web agents have the potential to automate long-running web tasks,\nsuch as finding offers for specific products in multiple online shops and\nsubsequently ordering the cheapest products that meet the users needs. This\npaper introduces WebMall, a multi-shop online shopping benchmark for evaluating\nthe effectiveness and efficiency of web agents for comparison-shopping. WebMall\nconsists of four simulated online shops populated with authentic product offers\nsourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These\ntasks include basic tasks such as finding specific products in multiple shops,\nperforming price comparisons, adding items to the shopping cart, and completing\ncheckout. Advanced tasks involve searching for products based on vague\nrequirements, identifying suitable substitutes, and finding compatible\nproducts. Compared to existing e-commerce benchmarks, such as WebShop or\nShoppingBench, WebMall introduces comparison-shopping tasks across multiple\nshops. Furthermore, the product offers are more heterogeneous, as they\noriginate from hundreds of distinct real-world shops. The tasks in WebMall\nrequire longer interaction trajectories than those in WebShop, while remaining\nrepresentative of real-world shopping behaviors. We evaluate eight baseline\nagents on WebMall, varying in observation modality, memory utilization, and\nunderlying large language model (GPT 4.1 and Claude Sonnet 4). The\nbest-performing configurations achieve completion rates of 75% and 53%, and F1\nscores of 87% and 63%, on the basic and advanced task sets, respectively.\nWebMall is publicly released to facilitate research on web agents and to\npromote advancements in navigation, reasoning, and efficiency within e-commerce\nscenarios.",
        "url": "http://arxiv.org/abs/2508.13024v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13024v1",
        "arxiv_id": "2508.13024v1",
        "authors": [
            "Ralph Peeters",
            "Aaron Steiner",
            "Luca Schwarz",
            "Julian Yuya Caspary",
            "Christian Bizer"
        ],
        "submitted": "2025-08-18 15:41:22",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'shopping' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper introduces a benchmark for evaluating web agents in e-commerce scenarios, focusing on comparison-shopping tasks across multiple shops. While it touches on topics related to information retrieval, such as searching for products and performing price comparisons, the primary focus is on evaluating web agents rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user."
    },
    {
        "title": "From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task",
        "abstract": "In this paper, we present the SALAMANDRATA family of models, an improved\niteration of SALAMANDRA LLMs (Gonzalez-Agirre et al., 2025) specifically\ntrained to achieve strong performance in translation-related tasks for 38\nEuropean languages. SALAMANDRATA comes in two scales: 2B and 7B parameters. For\nboth versions, we applied the same training recipe with a first step of\ncontinual pre-training on parallel data, and a second step of supervised\nfine-tuning on high-quality instructions. The BSC submission to the WMT25\nGeneral Machine Translation shared task is based on the 7B variant of\nSALAMANDRATA. We first adapted the model vocabulary to support the additional\nnon-European languages included in the task. This was followed by a second\nphase of continual pre-training and supervised fine-tuning, carefully designed\nto optimize performance across all translation directions for this year's\nshared task. For decoding, we employed two quality-aware strategies: Minimum\nBayes Risk Decoding and Tuned Re-ranking using COMET and COMET-KIWI\nrespectively. We publicly release both the 2B and 7B versions of SALAMANDRATA,\nalong with the newer SALAMANDRATA-V2 model, on Hugging Face1",
        "url": "http://arxiv.org/abs/2508.12774v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12774v1",
        "arxiv_id": "2508.12774v1",
        "authors": [
            "Javier Garcia Gilabert",
            "Xixian Liao",
            "Severino Da Dalt",
            "Ella Bohman",
            "Audrey Mash",
            "Francesca De Luca Fornaciari",
            "Irene Baucells",
            "Joan Llop",
            "Miguel Claramunt Argote",
            "Carlos Escolano",
            "Maite Melero"
        ],
        "submitted": "2025-08-18 09:48:35",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on machine translation and does not mention query understanding, ranking models, or user behavior modeling, which are key areas of interest for you."
    },
    {
        "title": "Leveraging Large Language Models for Predictive Analysis of Human Misery",
        "abstract": "This study investigates the use of Large Language Models (LLMs) for\npredicting human-perceived misery scores from natural language descriptions of\nreal-world scenarios. The task is framed as a regression problem, where the\nmodel assigns a scalar value from 0 to 100 to each input statement. We evaluate\nmultiple prompting strategies, including zero-shot, fixed-context few-shot, and\nretrieval-based prompting using BERT sentence embeddings. Few-shot approaches\nconsistently outperform zero-shot baselines, underscoring the value of\ncontextual examples in affective prediction. To move beyond static evaluation,\nwe introduce the \"Misery Game Show\", a novel gamified framework inspired by a\ntelevision format. It tests LLMs through structured rounds involving ordinal\ncomparison, binary classification, scalar estimation, and feedback-driven\nreasoning. This setup enables us to assess not only predictive accuracy but\nalso the model's ability to adapt based on corrective feedback. The gamified\nevaluation highlights the broader potential of LLMs in dynamic emotional\nreasoning tasks beyond standard regression. Code and data link:\nhttps://github.com/abhi1nandy2/Misery_Data_Exps_GitHub",
        "url": "http://arxiv.org/abs/2508.12669v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12669v1",
        "arxiv_id": "2508.12669v1",
        "authors": [
            "Bishanka Seal",
            "Rahul Seetharaman",
            "Aman Bansal",
            "Abhilash Nandy"
        ],
        "submitted": "2025-08-18 07:02:59",
        "source": "arxiv",
        "comment": "14 pages, 4 tables",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling, which are the primary areas of interest. The focus on Large Language Models and predicting human-perceived misery scores is outside the scope of the user's research themes."
    },
    {
        "title": "Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive fluency and task\ncompetence in conversational settings. However, their effectiveness in\nmulti-session and long-term interactions is hindered by limited memory\npersistence. Typical retrieval-augmented generation (RAG) systems store\ndialogue history as dense vectors, which capture semantic similarity but\nneglect finer linguistic structures such as syntactic dependencies, discourse\nrelations, and coreference links. We propose Semantic Anchoring, a hybrid\nagentic memory architecture that enriches vector-based storage with explicit\nlinguistic cues to improve recall of nuanced, context-rich exchanges. Our\napproach combines dependency parsing, discourse relation tagging, and\ncoreference resolution to create structured memory entries. Experiments on\nadapted long-term dialogue datasets show that semantic anchoring improves\nfactual recall and discourse coherence by up to 18% over strong RAG baselines.\nWe further conduct ablation studies, human evaluations, and error analysis to\nassess robustness and interpretability.",
        "url": "http://arxiv.org/abs/2508.12630v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12630v1",
        "arxiv_id": "2508.12630v1",
        "authors": [
            "Maitreyi Chatterjee",
            "Devansh Agarwal"
        ],
        "submitted": "2025-08-18 05:14:48",
        "source": "arxiv",
        "comment": "Paper is currently in peer review",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on conversational AI and memory persistence, using techniques like dependency parsing, discourse relation tagging, and coreference resolution. While it involves natural language processing, it is not directly related to information retrieval, search technologies, or query understanding, which are the user's primary research interests."
    },
    {
        "title": "MedKGent: A Large Language Model Agent Framework for Constructing Temporally Evolving Medical Knowledge Graph",
        "abstract": "The rapid expansion of medical literature presents growing challenges for\nstructuring and integrating domain knowledge at scale. Knowledge Graphs (KGs)\noffer a promising solution by enabling efficient retrieval, automated\nreasoning, and knowledge discovery. However, current KG construction methods\noften rely on supervised pipelines with limited generalizability or naively\naggregate outputs from Large Language Models (LLMs), treating biomedical\ncorpora as static and ignoring the temporal dynamics and contextual uncertainty\nof evolving knowledge. To address these limitations, we introduce MedKGent, a\nLLM agent framework for constructing temporally evolving medical KGs.\nLeveraging over 10 million PubMed abstracts published between 1975 and 2023, we\nsimulate the emergence of biomedical knowledge via a fine-grained daily time\nseries. MedKGent incrementally builds the KG in a day-by-day manner using two\nspecialized agents powered by the Qwen2.5-32B-Instruct model. The Extractor\nAgent identifies knowledge triples and assigns confidence scores via\nsampling-based estimation, which are used to filter low-confidence extractions\nand inform downstream processing. The Constructor Agent incrementally\nintegrates the retained triples into a temporally evolving graph, guided by\nconfidence scores and timestamps to reinforce recurring knowledge and resolve\nconflicts. The resulting KG contains 156,275 entities and 2,971,384 relational\ntriples. Quality assessments by two SOTA LLMs and three domain experts\ndemonstrate an accuracy approaching 90\\%, with strong inter-rater agreement. To\nevaluate downstream utility, we conduct RAG across seven medical question\nanswering benchmarks using five leading LLMs, consistently observing\nsignificant improvements over non-augmented baselines. Case studies further\ndemonstrate the KG's value in literature-based drug repurposing via\nconfidence-aware causal inference.",
        "url": "http://arxiv.org/abs/2508.12393v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12393v1",
        "arxiv_id": "2508.12393v1",
        "authors": [
            "Duzhen Zhang",
            "Zixiao Wang",
            "Zhong-Zhi Li",
            "Yahan Yu",
            "Shuncheng Jia",
            "Jiahua Dong",
            "Haotian Xu",
            "Xing Wu",
            "Yingying Zhang",
            "Tielin Zhang",
            "Jie Yang",
            "Xiuying Chen",
            "Le Song"
        ],
        "submitted": "2025-08-17 15:14:03",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper introduces a framework for constructing temporally evolving medical knowledge graphs, leveraging large language models and PubMed abstracts. While it touches on information retrieval and knowledge graph construction, the focus is on medical domain and does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user."
    },
    {
        "title": "A Question Answering Dataset for Temporal-Sensitive Retrieval-Augmented Generation",
        "abstract": "We introduce ChronoQA, a large-scale benchmark dataset for Chinese question\nanswering, specifically designed to evaluate temporal reasoning in\nRetrieval-Augmented Generation (RAG) systems. ChronoQA is constructed from over\n300,000 news articles published between 2019 and 2024, and contains 5,176\nhigh-quality questions covering absolute, aggregate, and relative temporal\ntypes with both explicit and implicit time expressions. The dataset supports\nboth single- and multi-document scenarios, reflecting the real-world\nrequirements for temporal alignment and logical consistency. ChronoQA features\ncomprehensive structural annotations and has undergone multi-stage validation,\nincluding rule-based, LLM-based, and human evaluation, to ensure data quality.\nBy providing a dynamic, reliable, and scalable resource, ChronoQA enables\nstructured evaluation across a wide range of temporal tasks, and serves as a\nrobust benchmark for advancing time-sensitive retrieval-augmented question\nanswering systems.",
        "url": "http://arxiv.org/abs/2508.12282v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12282v1",
        "arxiv_id": "2508.12282v1",
        "authors": [
            "Ziyang Chen",
            "Erxue Min",
            "Xiang Zhao",
            "Yunxin Li",
            "Xin Jia",
            "Jinzhi Liao",
            "Jichao Li",
            "Shuaiqiang Wang",
            "Baotian Hu",
            "Dawei Yin"
        ],
        "submitted": "2025-08-17 08:12:59",
        "source": "arxiv",
        "comment": "10 pages, 5 figures",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on a specific application of retrieval-augmented generation in question answering, with a strong emphasis on temporal reasoning. While it touches on retrieval and generation, the primary focus is on question answering and temporal reasoning, which is not directly related to the user's core research themes in information retrieval, search technologies, and user behavior modeling."
    },
    {
        "title": "STEM: Efficient Relative Capability Evaluation of LLMs through Structured Transition Samples",
        "abstract": "Evaluating large language models (LLMs) has become increasingly challenging\nas model capabilities advance rapidly. While recent models often achieve higher\nscores on standard benchmarks, these improvements do not consistently reflect\nenhanced real-world reasoning capabilities. Moreover, widespread overfitting to\npublic benchmarks and the high computational cost of full evaluations have made\nit both expensive and less effective to distinguish meaningful differences\nbetween models. To address these challenges, we propose the \\textbf{S}tructured\n\\textbf{T}ransition \\textbf{E}valuation \\textbf{M}ethod (STEM), a lightweight\nand interpretable evaluation framework for efficiently estimating the relative\ncapabilities of LLMs. STEM identifies \\textit{significant transition samples}\n(STS) by analyzing consistent performance transitions among LLMs of the same\narchitecture but varying parameter scales. These samples enable STEM to\neffectively estimate the capability position of an unknown model. Qwen3 model\nfamily is applied to construct the STS pool on six diverse and representative\nbenchmarks. To assess generalizability. Experimental results indicate that STEM\nreliably captures performance trends, aligns with ground-truth rankings of\nmodel capability. These findings highlight STEM as a practical and scalable\nmethod for fine-grained, architecture-agnostic evaluation of LLMs.",
        "url": "http://arxiv.org/abs/2508.12096v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12096v1",
        "arxiv_id": "2508.12096v1",
        "authors": [
            "Haiquan Hu",
            "Jiazhi Jiang",
            "Shiyou Xu",
            "Ruhan Zeng",
            "Tian Wang"
        ],
        "submitted": "2025-08-16 16:36:43",
        "source": "arxiv",
        "comment": "Submit to AAAI 2026",
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on evaluating large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on ranking models, the context is different from the user's primary interests in IR and NLP."
    },
    {
        "title": "VideoAVE: A Multi-Attribute Video-to-Text Attribute Value Extraction Dataset and Benchmark Models",
        "abstract": "Attribute Value Extraction (AVE) is important for structuring product\ninformation in e-commerce. However, existing AVE datasets are primarily limited\nto text-to-text or image-to-text settings, lacking support for product videos,\ndiverse attribute coverage, and public availability. To address these gaps, we\nintroduce VideoAVE, the first publicly available video-to-text e-commerce AVE\ndataset across 14 different domains and covering 172 unique attributes. To\nensure data quality, we propose a post-hoc CLIP-based Mixture of Experts\nfiltering system (CLIP-MoE) to remove the mismatched video-product pairs,\nresulting in a refined dataset of 224k training data and 25k evaluation data.\nIn order to evaluate the usability of the dataset, we further establish a\ncomprehensive benchmark by evaluating several state-of-the-art video vision\nlanguage models (VLMs) under both attribute-conditioned value prediction and\nopen attribute-value pair extraction tasks. Our results analysis reveals that\nvideo-to-text AVE remains a challenging problem, particularly in open settings,\nand there is still room for developing more advanced VLMs capable of leveraging\neffective temporal information. The dataset and benchmark code for VideoAVE are\navailable at: https://github.com/gjiaying/VideoAVE",
        "url": "http://arxiv.org/abs/2508.11801v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11801v1",
        "arxiv_id": "2508.11801v1",
        "authors": [
            "Ming Cheng",
            "Tong Wu",
            "Jiazhen Hu",
            "Jiaying Gong",
            "Hoda Eldardiry"
        ],
        "submitted": "2025-08-15 20:58:47",
        "source": "arxiv",
        "comment": "5 pages, 2 figures, 5 tables, accepted in CIKM 2025",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Attribute Value Extraction (AVE) in the e-commerce domain, specifically on video-to-text AVE, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although the paper mentions video vision language models (VLMs), it does not address query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user."
    },
    {
        "title": "Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation",
        "abstract": "Developing large language models is expensive and involves making decisions\nwith small experiments, typically by evaluating on large, multi-task evaluation\nsuites. In this work, we analyze specific properties which make a benchmark\nmore reliable for such decisions, and interventions to design higher-quality\nevaluation benchmarks. We introduce two key metrics that show differences in\ncurrent benchmarks: signal, a benchmark's ability to separate better models\nfrom worse models, and noise, a benchmark's sensitivity to random variability\nbetween training steps. We demonstrate that benchmarks with a better\nsignal-to-noise ratio are more reliable when making decisions at small scale,\nand those with less noise have lower scaling law prediction error. These\nresults suggest that improving signal or noise will lead to more useful\nbenchmarks, so we introduce three interventions designed to directly affect\nsignal or noise. For example, we propose that switching to a metric that has\nbetter signal and noise (e.g., perplexity rather than accuracy) leads to better\nreliability and improved scaling law error. We also find that filtering noisy\nsubtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable\nmulti-task evaluations. We also find that averaging the output of a model's\nintermediate checkpoints to reduce noise leads to consistent improvements. We\nconclude by recommending that those creating new benchmarks, or selecting which\nexisting benchmarks to use, aim for high signal and low noise. We use 30\nbenchmarks for these experiments, and 375 open-weight language models from 60M\nto 32B parameters, resulting in a new, publicly available dataset of 900K\nevaluation benchmark results, totaling 200M instances.",
        "url": "http://arxiv.org/abs/2508.13144v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13144v1",
        "arxiv_id": "2508.13144v1",
        "authors": [
            "David Heineman",
            "Valentin Hofmann",
            "Ian Magnusson",
            "Yuling Gu",
            "Noah A. Smith",
            "Hannaneh Hajishirzi",
            "Kyle Lo",
            "Jesse Dodge"
        ],
        "submitted": "2025-08-18 17:56:04",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on language model evaluation, introducing metrics to assess signal and noise in benchmarks. While it touches on the idea of improving evaluation, it doesn't directly relate to query understanding, ranking models, or user behavior modeling in information retrieval, which are core research themes for you. The paper's relevance is limited to the NLP aspect of your research interests, but it doesn't explore topics like deep semantic understanding or real-time relevance optimization."
    },
    {
        "title": "Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection",
        "abstract": "Abusive language detection has become an increasingly important task as a\nmeans to tackle this type of harmful content in social media. There has been a\nsubstantial body of research developing models for determining if a social\nmedia post is abusive or not; however, this research has primarily focused on\nexploiting social media posts individually, overlooking additional context that\ncan be derived from surrounding posts. In this study, we look at conversational\nexchanges, where a user replies to an earlier post by another user (the parent\ntweet). We ask: does leveraging context from the parent tweet help determine if\na reply post is abusive or not, and what are the features that contribute the\nmost? We study a range of content-based and account-based features derived from\nthe context, and compare this to the more widely studied approach of only\nlooking at the features from the reply tweet. For a more generalizable study,\nwe test four different classification models on a dataset made of\nconversational exchanges (parent-reply tweet pairs) with replies labeled as\nabusive or not. Our experiments show that incorporating contextual features\nleads to substantial improvements compared to the use of features derived from\nthe reply tweet only, confirming the importance of leveraging context. We\nobserve that, among the features under study, it is especially the\ncontent-based features (what is being posted) that contribute to the\nclassification performance rather than account-based features (who is posting\nit). While using content-based features, it is best to combine a range of\ndifferent features to ensure improved performance over being more selective and\nusing fewer features. Our study provides insights into the development of\ncontextualized abusive language detection models in realistic settings\ninvolving conversations.",
        "url": "http://arxiv.org/abs/2508.12828v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12828v1",
        "arxiv_id": "2508.12828v1",
        "authors": [
            "Raneem Alharthi",
            "Rajwa Alharthi",
            "Aiqi Jiang",
            "Arkaitz Zubiaga"
        ],
        "submitted": "2025-08-18 11:12:21",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on abusive language detection in conversational exchanges, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on context and conversational exchanges is also not aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Learning to Steer: Input-dependent Steering for Multimodal LLMs",
        "abstract": "Steering has emerged as a practical approach to enable post-hoc guidance of\nLLMs towards enforcing a specific behavior. However, it remains largely\nunderexplored for multimodal LLMs (MLLMs); furthermore, existing steering\ntechniques, such as mean steering, rely on a single steering vector, applied\nindependently of the input query. This paradigm faces limitations when the\ndesired behavior is dependent on the example at hand. For example, a safe\nanswer may consist in abstaining from answering when asked for an illegal\nactivity, or may point to external resources or consultation with an expert\nwhen asked about medical advice. In this paper, we investigate a fine-grained\nsteering that uses an input-specific linear shift. This shift is computed using\ncontrastive input-specific prompting. However, the input-specific prompts\nrequired for this approach are not known at test time. Therefore, we propose to\ntrain a small auxiliary module to predict the input-specific steering vector.\nOur approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces\nhallucinations and enforces safety in MLLMs, outperforming other static\nbaselines.",
        "url": "http://arxiv.org/abs/2508.12815v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12815v1",
        "arxiv_id": "2508.12815v1",
        "authors": [
            "Jayneel Parekh",
            "Pegah Khayatan",
            "Mustafa Shukor",
            "Arnaud Dapogny",
            "Alasdair Newson",
            "Matthieu Cord"
        ],
        "submitted": "2025-08-18 10:53:20",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores steering techniques for multimodal LLMs, which is a related topic to query understanding and ranking models. However, the focus on multimodal LLMs and the specific problem of hallucinations and safety enforcement is not directly aligned with the user's primary research interests in information retrieval and search technologies."
    },
    {
        "title": "Bridging Human and LLM Judgments: Understanding and Narrowing the Gap",
        "abstract": "Large language models are increasingly used as judges (LLM-as-a-judge) to\nevaluate model outputs at scale, but their assessments often diverge\nsystematically from human judgments. We present Bridge, a unified statistical\nframework that explicitly bridges human and LLM evaluations under both absolute\nscoring and pairwise comparison paradigms. Bridge posits a latent human\npreference score for each prompt-response pair and models LLM deviations as\nlinear transformations of covariates that capture sources of discrepancies.\nThis offers a simple and principled framework for refining LLM ratings and\ncharacterizing systematic discrepancies between humans and LLMs. We provide an\nefficient fitting algorithm with asymptotic guarantees for statistical\ninference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot\nArena), Bridge achieves higher agreement with human ratings (accuracy,\ncalibration, and KL divergence) and exposes systematic human-LLM gaps.",
        "url": "http://arxiv.org/abs/2508.12792v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12792v1",
        "arxiv_id": "2508.12792v1",
        "authors": [
            "Felipe Maia Polo",
            "Xinhe Wang",
            "Mikhail Yurochkin",
            "Gongjun Xu",
            "Moulinath Banerjee",
            "Yuekai Sun"
        ],
        "submitted": "2025-08-18 10:14:20",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores the gap between human and Large Language Model (LLM) judgments, which is related to query understanding and ranking models. However, the focus is on evaluating model outputs rather than search technologies or user behavior modeling, which are core areas of interest for the user. The paper's connection to information retrieval is indirect, as it aims to improve LLM ratings, but it does not directly address the user's primary research themes."
    },
    {
        "title": "Deep Research: A Survey of Autonomous Research Agents",
        "abstract": "The rapid advancement of large language models (LLMs) has driven the\ndevelopment of agentic systems capable of autonomously performing complex\ntasks. Despite their impressive capabilities, LLMs remain constrained by their\ninternal knowledge boundaries. To overcome these limitations, the paradigm of\ndeep research has been proposed, wherein agents actively engage in planning,\nretrieval, and synthesis to generate comprehensive and faithful analytical\nreports grounded in web-based evidence. In this survey, we provide a systematic\noverview of the deep research pipeline, which comprises four core stages:\nplanning, question developing, web exploration, and report generation. For each\nstage, we analyze the key technical challenges and categorize representative\nmethods developed to address them. Furthermore, we summarize recent advances in\noptimization techniques and benchmarks tailored for deep research. Finally, we\ndiscuss open challenges and promising research directions, aiming to chart a\nroadmap toward building more capable and trustworthy deep research agents.",
        "url": "http://arxiv.org/abs/2508.12752v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12752v1",
        "arxiv_id": "2508.12752v1",
        "authors": [
            "Wenlin Zhang",
            "Xiaopeng Li",
            "Yingyi Zhang",
            "Pengyue Jia",
            "Yichao Wang",
            "Huifeng Guo",
            "Yong Liu",
            "Xiangyu Zhao"
        ],
        "submitted": "2025-08-18 09:26:14",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on autonomous research agents and deep research pipeline, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions large language models, the context is different from the user's interests in NLP and IR."
    },
    {
        "title": "Consensus or Conflict? Fine-Grained Evaluation of Conflicting Answers in Question-Answering",
        "abstract": "Large Language Models (LLMs) have demonstrated strong performance in question\nanswering (QA) tasks. However, Multi-Answer Question Answering (MAQA), where a\nquestion may have several valid answers, remains challenging. Traditional QA\nsettings often assume consistency across evidences, but MAQA can involve\nconflicting answers. Constructing datasets that reflect such conflicts is\ncostly and labor-intensive, while existing benchmarks often rely on synthetic\ndata, restrict the task to yes/no questions, or apply unverified automated\nannotation. To advance research in this area, we extend the conflict-aware MAQA\nsetting to require models not only to identify all valid answers, but also to\ndetect specific conflicting answer pairs, if any. To support this task, we\nintroduce a novel cost-effective methodology for leveraging fact-checking\ndatasets to construct NATCONFQA, a new benchmark for realistic, conflict-aware\nMAQA, enriched with detailed conflict labels, for all answer pairs. We evaluate\neight high-end LLMs on NATCONFQA, revealing their fragility in handling various\ntypes of conflicts and the flawed strategies they employ to resolve them.",
        "url": "http://arxiv.org/abs/2508.12355v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12355v1",
        "arxiv_id": "2508.12355v1",
        "authors": [
            "Eviatar Nachshoni",
            "Arie Cattan",
            "Shmuel Amar",
            "Ori Shapira",
            "Ido Dagan"
        ],
        "submitted": "2025-08-17 12:58:48",
        "source": "arxiv",
        "comment": "no comments",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores Multi-Answer Question Answering (MAQA) and the challenge of conflicting answers, which is related to query understanding and ranking models in Information Retrieval. However, the focus is on question answering and language models, rather than search technologies and user behavior modeling, which are core areas of interest. The paper's relevance is somewhat diminished by its focus on a specific task and dataset, rather than more general concepts."
    },
    {
        "title": "Structuring the Unstructured: A Systematic Review of Text-to-Structure Generation for Agentic AI with a Universal Evaluation Framework",
        "abstract": "The evolution of AI systems toward agentic operation and context-aware\nretrieval necessitates transforming unstructured text into structured formats\nlike tables, knowledge graphs, and charts. While such conversions enable\ncritical applications from summarization to data mining, current research lacks\na comprehensive synthesis of methodologies, datasets, and metrics. This\nsystematic review examines text-to-structure techniques and the encountered\nchallenges, evaluates current datasets and assessment criteria, and outlines\npotential directions for future research. We also introduce a universal\nevaluation framework for structured outputs, establishing text-to-structure as\nfoundational infrastructure for next-generation AI systems.",
        "url": "http://arxiv.org/abs/2508.12257v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12257v1",
        "arxiv_id": "2508.12257v1",
        "authors": [
            "Zheye Deng",
            "Chunkit Chan",
            "Tianshi Zheng",
            "Wei Fan",
            "Weiqi Wang",
            "Yangqiu Song"
        ],
        "submitted": "2025-08-17 06:41:40",
        "source": "arxiv",
        "comment": "Under Review",
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper's focus on text-to-structure generation and its applications in AI systems is somewhat related to information retrieval and NLP, but it does not directly address query understanding, ranking models, or user behavior modeling. While the paper's emphasis on structured formats and evaluation frameworks may be relevant to data mining, it does not seem to be a central match for the user's research interests."
    },
    {
        "title": "Arabic Multimodal Machine Learning: Datasets, Applications, Approaches, and Challenges",
        "abstract": "Multimodal Machine Learning (MML) aims to integrate and analyze information\nfrom diverse modalities, such as text, audio, and visuals, enabling machines to\naddress complex tasks like sentiment analysis, emotion recognition, and\nmultimedia retrieval. Recently, Arabic MML has reached a certain level of\nmaturity in its foundational development, making it time to conduct a\ncomprehensive survey. This paper explores Arabic MML by categorizing efforts\nthrough a novel taxonomy and analyzing existing research. Our taxonomy\norganizes these efforts into four key topics: datasets, applications,\napproaches, and challenges. By providing a structured overview, this survey\noffers insights into the current state of Arabic MML, highlighting areas that\nhave not been investigated and critical research gaps. Researchers will be\nempowered to build upon the identified opportunities and address challenges to\nadvance the field.",
        "url": "http://arxiv.org/abs/2508.12227v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12227v1",
        "arxiv_id": "2508.12227v1",
        "authors": [
            "Abdelhamid Haouhat",
            "Slimane Bellaouar",
            "Attia Nehar",
            "Hadda Cherroun",
            "Ahmed Abdelali"
        ],
        "submitted": "2025-08-17 03:59:27",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Arabic Multimodal Machine Learning, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on multimedia retrieval, the primary focus is on sentiment analysis, emotion recognition, and other tasks outside the user's core research themes."
    },
    {
        "title": "CORE: Measuring Multi-Agent LLM Interaction Quality under Game-Theoretic Pressures",
        "abstract": "Game-theoretic interactions between agents with Large Language Models (LLMs)\nhave revealed many emergent capabilities, yet the linguistic diversity of these\ninteractions has not been sufficiently quantified. In this paper, we present\nthe Conversational Robustness Evaluation Score: CORE, a metric to quantify the\neffectiveness of language use within multi-agent systems across different\ngame-theoretic interactions. CORE integrates measures of cluster entropy,\nlexical repetition, and semantic similarity, providing a direct lens of dialog\nquality. We apply CORE to pairwise LLM dialogs across competitive, cooperative,\nand neutral settings, further grounding our analysis in Zipf's and Heaps' Laws\nto characterize word frequency distributions and vocabulary growth. Our\nfindings show that cooperative settings exhibit both steeper Zipf distributions\nand higher Heap exponents, indicating more repetition alongside greater\nvocabulary expansion. In contrast, competitive interactions display lower Zipf\nand Heaps exponents, reflecting less repetition and more constrained\nvocabularies. These results provide new insights into how social incentives\ninfluence language adaptation, and highlight CORE as a robust diagnostic for\nmeasuring linguistic robustness in multi-agent LLM systems. Our code is\navailable at https://github.com/psyonp/core.",
        "url": "http://arxiv.org/abs/2508.11915v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11915v1",
        "arxiv_id": "2508.11915v1",
        "authors": [
            "Punya Syon Pandey",
            "Yongjin Yang",
            "Jiarui Liu",
            "Zhijing Jin"
        ],
        "submitted": "2025-08-16 05:26:36",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on measuring the quality of language interactions between agents with Large Language Models (LLMs) in game-theoretic settings, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on natural language processing, the context is quite different from the user's primary interests."
    },
    {
        "title": "Every 28 Days the AI Dreams of Soft Skin and Burning Stars: Scaffolding AI Agents with Hormones and Emotions",
        "abstract": "Despite significant advances, AI systems struggle with the frame problem:\ndetermining what information is contextually relevant from an exponentially\nlarge possibility space. We hypothesize that biological rhythms, particularly\nhormonal cycles, serve as natural relevance filters that could address this\nfundamental challenge. We develop a framework that embeds simulated menstrual\nand circadian cycles into Large Language Models through system prompts\ngenerated from periodic functions modeling key hormones including estrogen,\ntestosterone, and cortisol. Across multiple state-of-the-art models, linguistic\nanalysis reveals emotional and stylistic variations that track biological\nphases; sadness peaks during menstruation while happiness dominates ovulation\nand circadian patterns show morning optimism transitioning to nocturnal\nintrospection. Benchmarking on SQuAD, MMLU, Hellaswag, and AI2-ARC demonstrates\nsubtle but consistent performance variations aligning with biological\nexpectations, including optimal function in moderate rather than extreme\nhormonal ranges. This methodology provides a novel approach to contextual AI\nwhile revealing how societal biases regarding gender and biology are embedded\nwithin language models.",
        "url": "http://arxiv.org/abs/2508.11829v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11829v1",
        "arxiv_id": "2508.11829v1",
        "authors": [
            "Leigh Levinson",
            "Christopher J. Agostino"
        ],
        "submitted": "2025-08-15 22:26:42",
        "source": "arxiv",
        "comment": "9 pages, 1 figure, submitted to NeurIPS Creative AI track",
        "score": 3,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper explores the concept of biological rhythms and hormones in AI systems, which is unrelated to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's focus on Large Language Models and linguistic analysis is also outside the user's primary focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "A Survey of Idiom Datasets for Psycholinguistic and Computational Research",
        "abstract": "Idioms are figurative expressions whose meanings often cannot be inferred\nfrom their individual words, making them difficult to process computationally\nand posing challenges for human experimental studies. This survey reviews\ndatasets developed in psycholinguistics and computational linguistics for\nstudying idioms, focusing on their content, form, and intended use.\nPsycholinguistic resources typically contain normed ratings along dimensions\nsuch as familiarity, transparency, and compositionality, while computational\ndatasets support tasks like idiomaticity detection/classification,\nparaphrasing, and cross-lingual modeling. We present trends in annotation\npractices, coverage, and task framing across 53 datasets. Although recent\nefforts expanded language coverage and task diversity, there seems to be no\nrelation yet between psycholinguistic and computational research on idioms.",
        "url": "http://arxiv.org/abs/2508.11828v1",
        "pdf_url": "http://arxiv.org/pdf/2508.11828v1",
        "arxiv_id": "2508.11828v1",
        "authors": [
            "Michael Flor",
            "Xinyi Liu",
            "Anna Feldman"
        ],
        "submitted": "2025-08-15 22:24:09",
        "source": "arxiv",
        "comment": "KONVENS 2025. To appear",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on idiom datasets for psycholinguistic and computational research, which is not directly related to information retrieval, search technologies, or query understanding. Although it touches on computational linguistics, the topic is too specific and does not align with the user's primary research interests in IR and NLP."
    },
    {
        "title": "RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns",
        "abstract": "Detecting content generated by large language models (LLMs) is crucial for\npreventing misuse and building trustworthy AI systems. Although existing\ndetection methods perform well, their robustness in out-of-distribution (OOD)\nscenarios is still lacking. In this paper, we hypothesize that, compared to\nfeatures used by existing detection methods, the internal representations of\nLLMs contain more comprehensive and raw features that can more effectively\ncapture and distinguish the statistical pattern differences between\nLLM-generated texts (LGT) and human-written texts (HWT). We validated this\nhypothesis across different LLMs and observed significant differences in neural\nactivation patterns when processing these two types of texts. Based on this, we\npropose RepreGuard, an efficient statistics-based detection method.\nSpecifically, we first employ a surrogate model to collect representation of\nLGT and HWT, and extract the distinct activation feature that can better\nidentify LGT. We can classify the text by calculating the projection score of\nthe text representations along this feature direction and comparing with a\nprecomputed threshold. Experimental results show that RepreGuard outperforms\nall baselines with average 94.92% AUROC on both in-distribution (ID) and OOD\nscenarios, while also demonstrating robust resilience to various text sizes and\nmainstream attacks. Data and code are publicly available at:\nhttps://github.com/NLP2CT/RepreGuard",
        "url": "http://arxiv.org/abs/2508.13152v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13152v1",
        "arxiv_id": "2508.13152v1",
        "authors": [
            "Xin Chen",
            "Junchao Wu",
            "Shu Yang",
            "Runzhe Zhan",
            "Zeyu Wu",
            "Ziyang Luo",
            "Di Wang",
            "Min Yang",
            "Lidia S. Chao",
            "Derek F. Wong"
        ],
        "submitted": "2025-08-18 17:59:15",
        "source": "arxiv",
        "comment": "Accepted to TACL 2025. This version is a pre-MIT Press publication\n  version",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on detecting LLM-generated text, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on neural activation patterns, it does not address ranking models or user behavior modeling. The topic is more relevant to NLP and data mining, but the focus on detection rather than semantic understanding or real-time relevance optimization limits its alignment with your research interests."
    },
    {
        "title": "Has GPT-5 Achieved Spatial Intelligence? An Empirical Study",
        "abstract": "Multi-modal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, which are fundamental capabilities to achieving\nartificial general intelligence. With the recent release of GPT-5, allegedly\nthe most powerful AI model to date, it is timely to examine where the leading\nmodels stand on the path toward spatial intelligence. First, we propose a\ncomprehensive taxonomy of spatial tasks that unifies existing benchmarks and\ndiscuss the challenges in ensuring fair evaluation. We then evaluate\nstate-of-the-art proprietary and open-source models on eight key benchmarks, at\na cost exceeding one billion total tokens. Our empirical study reveals that (1)\nGPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)\nstill falls short of human performance across a broad spectrum of tasks.\nMoreover, we (3) identify the more challenging spatial intelligence problems\nfor multi-modal models, and (4) proprietary models do not exhibit a decisive\nadvantage when facing the most difficult problems. In addition, we conduct a\nqualitative evaluation across a diverse set of scenarios that are intuitive for\nhumans yet fail even the most advanced multi-modal models.",
        "url": "http://arxiv.org/abs/2508.13142v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13142v1",
        "arxiv_id": "2508.13142v1",
        "authors": [
            "Zhongang Cai",
            "Yubo Wang",
            "Qingping Sun",
            "Ruisi Wang",
            "Chenyang Gu",
            "Wanqi Yin",
            "Zhiqian Lin",
            "Zhitao Yang",
            "Chen Wei",
            "Xuanke Shi",
            "Kewang Deng",
            "Xiaoyang Han",
            "Zukai Chen",
            "Jiaqi Li",
            "Xiangyu Fan",
            "Hanming Deng",
            "Lewei Lu",
            "Bo Li",
            "Ziwei Liu",
            "Quan Wang",
            "Dahua Lin",
            "Lei Yang"
        ],
        "submitted": "2025-08-18 17:55:17",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on spatial intelligence and multi-modal models, which is outside the scope of Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on GPT-5 and its capabilities is also not related to your interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries",
        "abstract": "Abstractive summarization is a core application in contact centers, where\nLarge Language Models (LLMs) generate millions of summaries of call transcripts\ndaily. Despite their apparent quality, it remains unclear whether LLMs\nsystematically under- or over-attend to specific aspects of the transcript,\npotentially introducing biases in the generated summary. While prior work has\nexamined social and positional biases, the specific forms of bias pertinent to\ncontact center operations - which we term Operational Bias - have remained\nunexplored. To address this gap, we introduce BlindSpot, a framework built upon\na taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic)\nfor the identification and quantification of these biases. BlindSpot leverages\nan LLM as a zero-shot classifier to derive categorical distributions for each\nbias dimension in a pair of transcript and its summary. The bias is then\nquantified using two metrics: Fidelity Gap (the JS Divergence between\ndistributions) and Coverage (the percentage of source labels omitted). Using\nBlindSpot, we conducted an empirical study with 2500 real call transcripts and\ntheir summaries generated by 20 LLMs of varying scales and families (e.g., GPT,\nLlama, Claude). Our analysis reveals that biases are systemic and present\nacross all evaluated models, regardless of size or family.",
        "url": "http://arxiv.org/abs/2508.13124v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13124v1",
        "arxiv_id": "2508.13124v1",
        "authors": [
            "Kawin Mayilvaghanan",
            "Siddhant Gupta",
            "Ayush Kumar"
        ],
        "submitted": "2025-08-18 17:31:03",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on Large Language Models (LLMs) and their biases in contact center summaries, which is somewhat related to information retrieval and search technologies. However, the specific context of contact centers and summarization is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Reinforced Context Order Recovery for Adaptive Reasoning and Planning",
        "abstract": "Modern causal language models, followed by rapid developments in discrete\ndiffusion models, can now produce a wide variety of interesting and useful\ncontent. However, these families of models are predominantly trained to output\ntokens with a fixed (left-to-right) or random order, which may deviate from the\nlogical order in which tokens are generated originally. In this paper, we\nobserve that current causal and diffusion models encounter difficulties in\nproblems that require adaptive token generation orders to solve tractably,\nwhich we characterize with the $\\mathcal{V}$-information framework. Motivated\nby this, we propose Reinforced Context Order Recovery (ReCOR), a\nreinforcement-learning-based framework to extract adaptive, data-dependent\ntoken generation orders from text data without annotations. Self-supervised by\ntoken prediction statistics, ReCOR estimates the hardness of predicting every\nunfilled token and adaptively selects the next token during both training and\ninference. Experiments on challenging reasoning and planning datasets\ndemonstrate the superior performance of ReCOR compared with baselines,\nsometimes outperforming oracle models supervised with the ground-truth order.",
        "url": "http://arxiv.org/abs/2508.13070v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13070v1",
        "arxiv_id": "2508.13070v1",
        "authors": [
            "Long Ma",
            "Fangwei Zhong",
            "Yizhou Wang"
        ],
        "submitted": "2025-08-18 16:42:55",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on developing a framework for extracting adaptive token generation orders from text data, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on natural language processing, the specific area of study is not relevant to the user's primary research interests."
    },
    {
        "title": "Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction",
        "abstract": "Recent studies have demonstrated that Large Language Models (LLMs) have\nstrong mathematical reasoning abilities but rely on hundreds of billions of\nparameters. To tackle the challenge of poor reasoning in Small Language Models\n(SLMs), existing methods typically leverage LLMs to generate massive amounts of\ndata for cramming training. In psychology, they are akin to System 1 thinking,\nwhich resolves reasoning problems rapidly based on experience and intuition.\nHowever, human learning also requires System 2 thinking, where knowledge is\nfirst acquired and then reinforced through practice. Inspired by such two\ndistinct modes of thinking, we propose a novel method based on the multi-LoRA\nInteraction for mathematical reasoning Distillation (LoRID). First, we input\nthe question and reasoning of each sample into an LLM to create\nknowledge-enhanced datasets. Subsequently, we train a LoRA block on the student\nmodel as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts\nfor problem-solving. Then, to imitate System 2 thinking, we train the Knowledge\nGenerator (KG) and Deep Reasoner (DR), respectively. The former outputs only\nknowledge after receiving problems, while the latter uses that knowledge to\nperform reasoning. Finally, to address the randomness in the generation of IR\nand DR, we evaluate whether their outputs are consistent, and the inference\nprocess needs to be iterated if not. This step can enhance the mathematical\nreasoning ability of SLMs through mutual feedback. Experimental results show\nthat LoRID achieves state-of-the-art performance, especially on the GSM8K\ndataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,\n12.3%, and 1.8% accuracy across the five base models, respectively.",
        "url": "http://arxiv.org/abs/2508.13037v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13037v1",
        "arxiv_id": "2508.13037v1",
        "authors": [
            "Xinhe Li",
            "Jiajun Liu",
            "Peng Wang"
        ],
        "submitted": "2025-08-18 15:56:10",
        "source": "arxiv",
        "comment": "Accepted by IJCAI2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is not relevant to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on mathematical reasoning and distillation methods, which are outside the user's primary areas of interest."
    },
    {
        "title": "Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis",
        "abstract": "Sarcastic speech synthesis, which involves generating speech that effectively\nconveys sarcasm, is essential for enhancing natural interactions in\napplications such as entertainment and human-computer interaction. However,\nsynthesizing sarcastic speech remains a challenge due to the nuanced prosody\nthat characterizes sarcasm, as well as the limited availability of annotated\nsarcastic speech data. To address these challenges, this study introduces a\nnovel approach that integrates feedback loss from a bi-modal sarcasm detection\nmodel into the TTS training process, enhancing the model's ability to capture\nand convey sarcasm. In addition, by leveraging transfer learning, a speech\nsynthesis model pre-trained on read speech undergoes a two-stage fine-tuning\nprocess. First, it is fine-tuned on a diverse dataset encompassing various\nspeech styles, including sarcastic speech. In the second stage, the model is\nfurther refined using a dataset focused specifically on sarcastic speech,\nenhancing its ability to generate sarcasm-aware speech. Objective and\nsubjective evaluations demonstrate that our proposed methods improve the\nquality, naturalness, and sarcasm-awareness of synthesized speech.",
        "url": "http://arxiv.org/abs/2508.13028v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13028v1",
        "arxiv_id": "2508.13028v1",
        "authors": [
            "Zhu Li",
            "Yuqing Zhang",
            "Xiyuan Gao",
            "Devraj Raghuvanshi",
            "Nagendra Kumar",
            "Shekhar Nayak",
            "Matt Coler"
        ],
        "submitted": "2025-08-18 15:44:54",
        "source": "arxiv",
        "comment": "Speech Synthesis Workshop 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on speech synthesis and sarcasm detection, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions bi-modal sarcasm detection, the context is not relevant to the user's core research themes."
    },
    {
        "title": "PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models",
        "abstract": "Recent advances in masked diffusion models (MDMs) have established them as\npowerful non-autoregressive alternatives for sequence generation. Nevertheless,\nour preliminary experiments reveal that the generation quality of MDMs is still\nhighly sensitive to the choice of decoding strategy. In particular, widely\nadopted uncertainty-based samplers suffer from two key limitations: a lack of\nglobal trajectory control and a pronounced bias toward trivial tokens in the\nearly stages of decoding. These shortcomings restrict the full potential of\nMDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling\n(PC-Sampler), a novel decoding strategy that unifies global trajectory planning\nwith content-aware informativeness maximization. PC-Sampler incorporates a\nposition-aware weighting mechanism to regulate the decoding path and a\ncalibrated confidence score to suppress the premature selection of trivial\ntokens. Extensive experiments on three advanced MDMs across seven challenging\nbenchmarks-including logical reasoning and planning tasks-demonstrate that\nPC-Sampler consistently outperforms existing MDM decoding strategies by more\nthan 10% on average, significantly narrowing the performance gap with\nstate-of-the-art autoregressive models. All codes are available at\nhttps://github.com/NEUIR/PC-Sampler.",
        "url": "http://arxiv.org/abs/2508.13021v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13021v1",
        "arxiv_id": "2508.13021v1",
        "authors": [
            "Pengcheng Huang",
            "Shuhao Liu",
            "Zhenghao Liu",
            "Yukun Yan",
            "Shuo Wang",
            "Zulong Chen",
            "Tong Xiao"
        ],
        "submitted": "2025-08-18 15:38:37",
        "source": "arxiv",
        "comment": "17 pages,13 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on masked diffusion models and sequence generation, which is outside the scope of Information Retrieval, Search technologies, and Natural Language Processing. The paper's abstract does not mention query understanding, ranking models, or user behavior modeling, which are key areas of interest for you."
    },
    {
        "title": "A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models",
        "abstract": "Recent advances in self-refinement have demonstrated significant potential\nfor improving the outputs of large language models (LLMs) through iterative\nrefinement. However, most existing self-refinement methods rely on a reactive\nprocess with a fixed number of iterations, making it difficult to determine the\noptimal timing and content of refinement based on the evolving generation\ncontext. Inspired by the way humans dynamically refine their thoughts during\nexecution, we propose ProActive Self-Refinement (PASR), a novel method that\nenables LLMs to refine their outputs during the generation process. Unlike\nmethods that regenerate entire responses, PASR proactively decides whether,\nwhen, and how to refine based on the model's internal state and evolving\ncontext. We conduct extensive experiments on a diverse set of 10 tasks to\nevaluate the effectiveness of PASR. Experimental results show that PASR\nsignificantly enhances problem-solving performance. In particular, on Qwen3-8B,\nPASR reduces average token consumption by 41.6 percent compared to standard\ngeneration, while also achieving an 8.2 percent improvement in accuracy. Our\ncode and all baselines used in the paper are available in the GitHub.",
        "url": "http://arxiv.org/abs/2508.12903v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12903v1",
        "arxiv_id": "2508.12903v1",
        "authors": [
            "Jinyi Han",
            "Xinyi Wang",
            "Haiquan Zhao",
            "Tingyun li",
            "Zishang Jiang",
            "Sihang Jiang",
            "Jiaqing Liang",
            "Xin Lin",
            "Weikang Zhou",
            "Zeye Sun",
            "Fei Yu",
            "Yanghua Xiao"
        ],
        "submitted": "2025-08-18 13:07:21",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on self-refinement for language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the idea of refining outputs, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest for your research."
    },
    {
        "title": "An LLM Agent-Based Complex Semantic Table Annotation Approach",
        "abstract": "The Semantic Table Annotation (STA) task, which includes Column Type\nAnnotation (CTA) and Cell Entity Annotation (CEA), maps table contents to\nontology entities and plays important roles in various semantic applications.\nHowever, complex tables often pose challenges such as semantic loss of column\nnames or cell values, strict ontological hierarchy requirements, homonyms,\nspelling errors, and abbreviations, which hinder annotation accuracy. To\naddress these issues, this paper proposes an LLM-based agent approach for CTA\nand CEA. We design and implement five external tools with tailored prompts\nbased on the ReAct framework, enabling the STA agent to dynamically select\nsuitable annotation strategies depending on table characteristics. Experiments\nare conducted on the Tough Tables and BiodivTab datasets from the SemTab\nchallenge, which contain the aforementioned challenges. Our method outperforms\nexisting approaches across various metrics. Furthermore, by leveraging\nLevenshtein distance to reduce redundant annotations, we achieve a 70%\nreduction in time costs and a 60% reduction in LLM token usage, providing an\nefficient and cost-effective solution for STA.",
        "url": "http://arxiv.org/abs/2508.12868v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12868v1",
        "arxiv_id": "2508.12868v1",
        "authors": [
            "Yilin Geng",
            "Shujing Wang",
            "Chuan Wang",
            "Keqing He",
            "Yanfei Lv",
            "Ying Wang",
            "Zaiwen Feng",
            "Xiaoying Bai"
        ],
        "submitted": "2025-08-18 12:09:20",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes an LLM-based approach for Semantic Table Annotation, which is a specific problem in Natural Language Processing. While it's related to information retrieval, it doesn't directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's focus on table annotation and ontology entities is somewhat relevant, but it doesn't align with the user's primary focus on information retrieval and real-time relevance optimization."
    },
    {
        "title": "E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model",
        "abstract": "Multimodal Empathetic Response Generation (MERG) is crucial for building\nemotionally intelligent human-computer interactions. Although large language\nmodels (LLMs) have improved text-based ERG, challenges remain in handling\nmultimodal emotional content and maintaining identity consistency. Thus, we\npropose E3RG, an Explicit Emotion-driven Empathetic Response Generation System\nbased on multimodal LLMs which decomposes MERG task into three parts:\nmultimodal empathy understanding, empathy memory retrieval, and multimodal\nresponse generation. By integrating advanced expressive speech and video\ngenerative models, E3RG delivers natural, emotionally rich, and\nidentity-consistent responses without extra training. Experiments validate the\nsuperiority of our system on both zero-shot and few-shot settings, securing\nTop-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.\nOur code is available at https://github.com/RH-Lin/E3RG.",
        "url": "http://arxiv.org/abs/2508.12854v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12854v1",
        "arxiv_id": "2508.12854v1",
        "authors": [
            "Ronghao Lin",
            "Shuai Shen",
            "Weipeng Hu",
            "Qiaolin He",
            "Aolin Xiong",
            "Li Huang",
            "Haifeng Hu",
            "Yap-peng Tan"
        ],
        "submitted": "2025-08-18 11:47:02",
        "source": "arxiv",
        "comment": "Accepted at ACM MM 2025 Grand Challenge",
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on multimodal empathetic response generation, which is not directly related to information retrieval or search technologies. While it involves language models, the primary goal is not query understanding, ranking models, or user behavior modeling. The paper's relevance to your interests is limited to the use of language models, but the context and application are quite different."
    },
    {
        "title": "ding-01 :ARG0: An AMR Corpus for Spontaneous French Dialogue",
        "abstract": "We present our work to build a French semantic corpus by annotating French\ndialogue in Abstract Meaning Representation (AMR). Specifically, we annotate\nthe DinG corpus, consisting of transcripts of spontaneous French dialogues\nrecorded during the board game Catan. As AMR has insufficient coverage of the\ndynamics of spontaneous speech, we extend the framework to better represent\nspontaneous speech and sentence structures specific to French. Additionally, to\nsupport consistent annotation, we provide an annotation guideline detailing\nthese extensions. We publish our corpus under a free license (CC-SA-BY). We\nalso train and evaluate an AMR parser on our data. This model can be used as an\nassistance annotation tool to provide initial annotations that can be refined\nby human annotators. Our work contributes to the development of semantic\nresources for French dialogue.",
        "url": "http://arxiv.org/abs/2508.12819v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12819v1",
        "arxiv_id": "2508.12819v1",
        "authors": [
            "Jeongwoo Kang",
            "Maria Boritchev",
            "Maximin Coavoux"
        ],
        "submitted": "2025-08-18 10:57:44",
        "source": "arxiv",
        "comment": "Accepted at IWCS 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on building a French semantic corpus using Abstract Meaning Representation (AMR) for spontaneous dialogue, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it involves natural language processing, the scope is limited to French dialogue and does not address ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest in your research."
    }
]