[
    {
        "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
        "abstract": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
        "url": "http://arxiv.org/abs/2509.08827v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08827v1",
        "arxiv_id": "2509.08827v1",
        "authors": [
            "Kaiyan Zhang",
            "Yuxin Zuo",
            "Bingxiang He",
            "Youbang Sun",
            "Runze Liu",
            "Che Jiang",
            "Yuchen Fan",
            "Kai Tian",
            "Guoli Jia",
            "Pengfei Li",
            "Yu Fu",
            "Xingtai Lv",
            "Yuchen Zhang",
            "Sihang Zeng",
            "Shang Qu",
            "Haozhan Li",
            "Shijie Wang",
            "Yuru Wang",
            "Xinwei Long",
            "Fangfu Liu",
            "Xiang Xu",
            "Jiaze Ma",
            "Xuekai Zhu",
            "Ermo Hua",
            "Yihao Liu",
            "Zonglin Li",
            "Huayu Chen",
            "Xiaoye Qu",
            "Yafu Li",
            "Weize Chen",
            "Zhenzhao Yuan",
            "Junqi Gao",
            "Dong Li",
            "Zhiyuan Ma",
            "Ganqu Cui",
            "Zhiyuan Liu",
            "Biqing Qi",
            "Ning Ding",
            "Bowen Zhou"
        ],
        "submitted": "2025-09-10 17:59:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation",
        "abstract": "Large language models (LLMs) are rapidly transforming social science research\nby enabling the automation of labor-intensive tasks like data annotation and\ntext analysis. However, LLM outputs vary significantly depending on the\nimplementation choices made by researchers (e.g., model selection, prompting\nstrategy, or temperature settings). Such variation can introduce systematic\nbiases and random errors, which propagate to downstream analyses and cause Type\nI, Type II, Type S, or Type M errors. We call this LLM hacking.\n  We quantify the risk of LLM hacking by replicating 37 data annotation tasks\nfrom 21 published social science research studies with 18 different models.\nAnalyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure\nhow plausible researcher choices affect statistical conclusions. We find\nincorrect conclusions based on LLM-annotated data in approximately one in three\nhypotheses for state-of-the-art models, and in half the hypotheses for small\nlanguage models. While our findings show that higher task performance and\nbetter general model capabilities reduce LLM hacking risk, even highly accurate\nmodels do not completely eliminate it. The risk of LLM hacking decreases as\neffect sizes increase, indicating the need for more rigorous verification of\nfindings near significance thresholds. Our extensive analysis of LLM hacking\nmitigation techniques emphasizes the importance of human annotations in\nreducing false positive findings and improving model selection. Surprisingly,\ncommon regression estimator correction techniques are largely ineffective in\nreducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.\n  Beyond accidental errors, we find that intentional LLM hacking is\nunacceptably simple. With few LLMs and just a handful of prompt paraphrases,\nanything can be presented as statistically significant.",
        "url": "http://arxiv.org/abs/2509.08825v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08825v1",
        "arxiv_id": "2509.08825v1",
        "authors": [
            "Joachim Baumann",
            "Paul Röttger",
            "Aleksandra Urman",
            "Albert Wendsjö",
            "Flor Miriam Plaza-del-Arco",
            "Johannes B. Gruber",
            "Dirk Hovy"
        ],
        "submitted": "2025-09-10 17:58:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Building High-Quality Datasets for Portuguese LLMs: From Common Crawl Snapshots to Industrial-Grade Corpora",
        "abstract": "The performance of large language models (LLMs) is deeply influenced by the\nquality and composition of their training data. While much of the existing work\nhas centered on English, there remains a gap in understanding how to construct\neffective training corpora for other languages. We explore scalable methods for\nbuilding web-based corpora for LLMs. We apply them to build a new 120B token\ncorpus in Portuguese that achieves competitive results to an industrial-grade\ncorpus. Using a continual pretraining setup, we study how different data\nselection and preprocessing strategies affect LLM performance when\ntransitioning a model originally trained in English to another language. Our\nfindings demonstrate the value of language-specific filtering pipelines,\nincluding classifiers for education, science, technology, engineering, and\nmathematics (STEM), as well as toxic content. We show that adapting a model to\nthe target language leads to performance improvements, reinforcing the\nimportance of high-quality, language-specific data. While our case study\nfocuses on Portuguese, our methods are applicable to other languages, offering\ninsights for multilingual LLM development.",
        "url": "http://arxiv.org/abs/2509.08824v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08824v1",
        "arxiv_id": "2509.08824v1",
        "authors": [
            "Thales Sales Almeida",
            "Rodrigo Nogueira",
            "Helio Pedrini"
        ],
        "submitted": "2025-09-10 17:58:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Merge-of-Thought Distillation",
        "abstract": "Efficient reasoning distillation for long chain-of-thought (CoT) models is\nincreasingly constrained by the assumption of a single oracle teacher, despite\npractical availability of multiple candidate teachers and growing CoT corpora.\nWe revisit teacher selection and observe that different students have different\n\"best teachers,\" and even for the same student the best teacher can vary across\ndatasets. Therefore, to unify multiple teachers' reasoning abilities into\nstudent with overcoming conflicts among various teachers' supervision, we\npropose Merge-of-Thought Distillation (MoT), a lightweight framework that\nalternates between teacher-specific supervised fine-tuning branches and\nweight-space merging of the resulting student variants. On competition math\nbenchmarks, using only about 200 high-quality CoT samples, applying MoT to a\nQwen3-14B student surpasses strong models including DEEPSEEK-R1, QWEN3-30B-A3B,\nQWEN3-32B, and OPENAI-O1, demonstrating substantial gains. Besides, MoT\nconsistently outperforms the best single-teacher distillation and the naive\nmulti-teacher union, raises the performance ceiling while mitigating\noverfitting, and shows robustness to distribution-shifted and peer-level\nteachers. Moreover, MoT reduces catastrophic forgetting, improves general\nreasoning beyond mathematics and even cultivates a better teacher, indicating\nthat consensus-filtered reasoning features transfer broadly. These results\nposition MoT as a simple, scalable route to efficiently distilling long CoT\ncapabilities from diverse teachers into compact students.",
        "url": "http://arxiv.org/abs/2509.08814v2",
        "pdf_url": "http://arxiv.org/pdf/2509.08814v2",
        "arxiv_id": "2509.08814v2",
        "authors": [
            "Zhanming Shen",
            "Zeyu Qin",
            "Zenan Huang",
            "Hao Chen",
            "Jiaqi Hu",
            "Yihong Zhuang",
            "Guoshan Lu",
            "Gang Chen",
            "Junbo Zhao"
        ],
        "submitted": "2025-09-10 17:46:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MoVoC: Morphology-Aware Subword Construction for Geez Script Languages",
        "abstract": "Subword-based tokenization methods often fail to preserve morphological\nboundaries, a limitation especially pronounced in low-resource, morphologically\ncomplex languages such as those written in the Geez script. To address this, we\npresent MoVoC (Morpheme-aware Subword Vocabulary Construction) and train\nMoVoC-Tok, a tokenizer that integrates supervised morphological analysis into\nthe subword vocabulary. This hybrid segmentation approach combines\nmorpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological\nintegrity while maintaining lexical meaning. To tackle resource scarcity, we\ncurate and release manually annotated morpheme data for four Geez script\nlanguages and a morpheme-aware vocabulary for two of them. While the proposed\ntokenization method does not lead to significant gains in automatic translation\nquality, we observe consistent improvements in intrinsic metrics, MorphoScore,\nand Boundary Precision, highlighting the value of morphology-aware segmentation\nin enhancing linguistic fidelity and token efficiency. Our morpheme-annotated\ndatasets and tokenizer will be publicly available to support further research\nin low-resource, morphologically rich languages. Our code and data are\navailable on GitHub: https://github.com/hailaykidu/MoVoC",
        "url": "http://arxiv.org/abs/2509.08812v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08812v1",
        "arxiv_id": "2509.08812v1",
        "authors": [
            "Hailay Kidu Teklehaymanot",
            "Dren Fazlija",
            "Wolfgang Nejdl"
        ],
        "submitted": "2025-09-10 17:45:10",
        "source": "arxiv",
        "comment": "This submission is approximately 10 pages in length and includes 1\n  figure and 6 tables"
    },
    {
        "title": "Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation Through Unsupervised Consistency Signals",
        "abstract": "Large Language Models (LLMs), when paired with prompt-based tasks, have\nsignificantly reduced data annotation costs and reliance on human annotators.\nHowever, evaluating the quality of their annotations remains challenging in\ndynamic, unsupervised environments where oracle feedback is scarce and\nconventional methods fail. To address this challenge, we propose a novel\nagentic annotation paradigm, where a student model collaborates with a noisy\nteacher (the LLM) to assess and refine annotation quality without relying on\noracle feedback. The student model, acting as an unsupervised feedback\nmechanism, employs a user preference-based majority voting strategy to evaluate\nthe consistency of the LLM outputs. To systematically measure the reliability\nof LLM-generated annotations, we introduce the Consistent and Inconsistent\n(CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not only\nquantifies the annotation quality of the noisy teacher under limited user\npreferences but also plays a critical role in model selection, enabling the\nidentification of robust LLMs in dynamic, unsupervised environments. Applied to\nten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates a\nstrong positive correlation with LLM accuracy, establishing it as an essential\ntool for unsupervised evaluation and model selection in real-world settings.",
        "url": "http://arxiv.org/abs/2509.08809v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08809v1",
        "arxiv_id": "2509.08809v1",
        "authors": [
            "Cheng Chen",
            "Haiyan Yin",
            "Ivor Tsang"
        ],
        "submitted": "2025-09-10 17:42:41",
        "source": "arxiv",
        "comment": "11 pages, 10 figures"
    },
    {
        "title": "Scaling Truth: The Confidence Paradox in AI Fact-Checking",
        "abstract": "The rise of misinformation underscores the need for scalable and reliable\nfact-checking solutions. Large language models (LLMs) hold promise in\nautomating fact verification, yet their effectiveness across global contexts\nremains uncertain. We systematically evaluate nine established LLMs across\nmultiple categories (open/closed-source, multiple sizes, diverse architectures,\nreasoning-based) using 5,000 claims previously assessed by 174 professional\nfact-checking organizations across 47 languages. Our methodology tests model\ngeneralizability on claims postdating training cutoffs and four prompting\nstrategies mirroring both citizen and professional fact-checker interactions,\nwith over 240,000 human annotations as ground truth. Findings reveal a\nconcerning pattern resembling the Dunning-Kruger effect: smaller, accessible\nmodels show high confidence despite lower accuracy, while larger models\ndemonstrate higher accuracy but lower confidence. This risks systemic bias in\ninformation verification, as resource-constrained organizations typically use\nsmaller models. Performance gaps are most pronounced for non-English languages\nand claims originating from the Global South, threatening to widen existing\ninformation inequalities. These results establish a multilingual benchmark for\nfuture research and provide an evidence base for policy aimed at ensuring\nequitable access to trustworthy, AI-assisted fact-checking.",
        "url": "http://arxiv.org/abs/2509.08803v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08803v1",
        "arxiv_id": "2509.08803v1",
        "authors": [
            "Ihsan A. Qazi",
            "Zohaib Khan",
            "Abdullah Ghani",
            "Agha A. Raza",
            "Zafar A. Qazi",
            "Wassay Sajjad",
            "Ayesha Ali",
            "Asher Javaid",
            "Muhammad Abdullah Sohail",
            "Abdul H. Azeemi"
        ],
        "submitted": "2025-09-10 17:36:25",
        "source": "arxiv",
        "comment": "65 pages, 26 figures, 6 tables"
    },
    {
        "title": "Do All Autoregressive Transformers Remember Facts the Same Way? A Cross-Architecture Analysis of Recall Mechanisms",
        "abstract": "Understanding how Transformer-based language models store and retrieve\nfactual associations is critical for improving interpretability and enabling\ntargeted model editing. Prior work, primarily on GPT-style models, has\nidentified MLP modules in early layers as key contributors to factual recall.\nHowever, it remains unclear whether these findings generalize across different\nautoregressive architectures. To address this, we conduct a comprehensive\nevaluation of factual recall across several models -- including GPT, LLaMA,\nQwen, and DeepSeek -- analyzing where and how factual information is encoded\nand accessed. Consequently, we find that Qwen-based models behave differently\nfrom previous patterns: attention modules in the earliest layers contribute\nmore to factual recall than MLP modules. Our findings suggest that even within\nthe autoregressive Transformer family, architectural variations can lead to\nfundamentally different mechanisms of factual recall.",
        "url": "http://arxiv.org/abs/2509.08778v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08778v1",
        "arxiv_id": "2509.08778v1",
        "authors": [
            "Minyeong Choe",
            "Haehyun Cho",
            "Changho Seo",
            "Hyunil Kim"
        ],
        "submitted": "2025-09-10 17:06:55",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025"
    },
    {
        "title": "Calibrating MLLM-as-a-judge via Multimodal Bayesian Prompt Ensembles",
        "abstract": "Multimodal large language models (MLLMs) are increasingly used to evaluate\ntext-to-image (TTI) generation systems, providing automated judgments based on\nvisual and textual context. However, these \"judge\" models often suffer from\nbiases, overconfidence, and inconsistent performance across diverse image\ndomains. While prompt ensembling has shown promise for mitigating these issues\nin unimodal, text-only settings, our experiments reveal that standard\nensembling methods fail to generalize effectively for TTI tasks. To address\nthese limitations, we propose a new multimodal-aware method called Multimodal\nMixture-of-Bayesian Prompt Ensembles (MMB). Our method uses a Bayesian prompt\nensemble approach augmented by image clustering, allowing the judge to\ndynamically assign prompt weights based on the visual characteristics of each\nsample. We show that MMB improves accuracy in pairwise preference judgments and\ngreatly enhances calibration, making it easier to gauge the judge's true\nuncertainty. In evaluations on two TTI benchmarks, HPSv2 and MJBench, MMB\noutperforms existing baselines in alignment with human annotations and\ncalibration across varied image content. Our findings highlight the importance\nof multimodal-specific strategies for judge calibration and suggest a promising\npath forward for reliable large-scale TTI evaluation.",
        "url": "http://arxiv.org/abs/2509.08777v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08777v1",
        "arxiv_id": "2509.08777v1",
        "authors": [
            "Eric Slyman",
            "Mehrab Tanjim",
            "Kushal Kafle",
            "Stefan Lee"
        ],
        "submitted": "2025-09-10 17:06:47",
        "source": "arxiv",
        "comment": "17 pages, 8 figures, Accepted at ICCV 2025"
    },
    {
        "title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making through Multi-Turn Reinforcement Learning",
        "abstract": "Developing autonomous LLM agents capable of making a series of intelligent\ndecisions to solve complex, real-world tasks is a fast-evolving frontier. Like\nhuman cognitive development, agents are expected to acquire knowledge and\nskills through exploration and interaction with the environment. Despite\nadvances, the community still lacks a unified, interactive reinforcement\nlearning (RL) framework that can effectively train such agents from scratch --\nwithout relying on supervised fine-tuning (SFT) -- across diverse and realistic\nenvironments. To bridge this gap, we introduce AgentGym-RL, a new framework to\ntrain LLM agents for multi-turn interactive decision-making through RL. The\nframework features a modular and decoupled architecture, ensuring high\nflexibility and extensibility. It encompasses a wide variety of real-world\nscenarios, and supports mainstream RL algorithms. Furthermore, we propose\nScalingInter-RL, a training approach designed for exploration-exploitation\nbalance and stable RL optimization. In early stages, it emphasizes exploitation\nby restricting the number of interactions, and gradually shifts towards\nexploration with larger horizons to encourage diverse problem-solving\nstrategies. In this way, the agent develops more diverse behaviors and is less\nprone to collapse under long horizons. We perform extensive experiments to\nvalidate the stability and effectiveness of both the AgentGym-RL framework and\nthe ScalingInter-RL approach. Our agents match or surpass commercial models on\n27 tasks across diverse environments. We offer key insights and will\nopen-source the complete AgentGym-RL framework -- including code and datasets\n-- to empower the research community in developing the next generation of\nintelligent agents.",
        "url": "http://arxiv.org/abs/2509.08755v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08755v1",
        "arxiv_id": "2509.08755v1",
        "authors": [
            "Zhiheng Xi",
            "Jixuan Huang",
            "Chenyang Liao",
            "Baodai Huang",
            "Honglin Guo",
            "Jiaqi Liu",
            "Rui Zheng",
            "Junjie Ye",
            "Jiazheng Zhang",
            "Wenxiang Chen",
            "Wei He",
            "Yiwen Ding",
            "Guanyu Li",
            "Zehui Chen",
            "Zhengyin Du",
            "Xuesong Yao",
            "Yufei Xu",
            "Jiecao Chen",
            "Tao Gui",
            "Zuxuan Wu",
            "Qi Zhang",
            "Xuanjing Huang",
            "Yu-Gang Jiang"
        ],
        "submitted": "2025-09-10 16:46:11",
        "source": "arxiv",
        "comment": "preprint, 39 pages, 16 figures. Project:\n  https://AgentGym-RL.github.io/. Framework and Code:\n  https://github.com/woooodyy/AgentGym, https://github.com/woooodyy/AgentGym-RL"
    },
    {
        "title": "Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling",
        "abstract": "We introduce Delayed Streams Modeling (DSM), a flexible formulation for\nstreaming, multimodal sequence-to-sequence learning. Sequence-to-sequence\ngeneration is often cast in an offline manner, where the model consumes the\ncomplete input sequence before generating the first output timestep.\nAlternatively, streaming sequence-to-sequence rely on learning a policy for\nchoosing when to advance on the input stream, or write to the output stream.\nDSM instead models already time-aligned streams with a decoder-only language\nmodel. By moving the alignment to a pre-processing step,and introducing\nappropriate delays between streams, DSM provides streaming inference of\narbitrary output sequences, from any input combination, making it applicable to\nmany sequence-to-sequence problems. In particular, given text and audio\nstreams, automatic speech recognition (ASR) corresponds to the text stream\nbeing delayed, while the opposite gives a text-to-speech (TTS) model. We\nperform extensive experiments for these two major sequence-to-sequence tasks,\nshowing that DSM provides state-of-the-art performance and latency while\nsupporting arbitrary long sequences, being even competitive with offline\nbaselines. Code, samples and demos are available at\nhttps://github.com/kyutai-labs/delayed-streams-modeling",
        "url": "http://arxiv.org/abs/2509.08753v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08753v1",
        "arxiv_id": "2509.08753v1",
        "authors": [
            "Neil Zeghidour",
            "Eugene Kharitonov",
            "Manu Orsini",
            "Václav Volhejn",
            "Gabriel de Marmiesse",
            "Edouard Grave",
            "Patrick Pérez",
            "Laurent Mazaré",
            "Alexandre Défossez"
        ],
        "submitted": "2025-09-10 16:43:01",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates",
        "abstract": "Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one\nstructured prompt, but prior work relied on a handful of manually written\ntemplates. We present X-Teaming Evolutionary M2S, an automated framework that\ndiscovers and optimizes M2S templates through language-model-guided evolution.\nThe system pairs smart sampling from 12 sources with an LLM-as-judge inspired\nby StrongREJECT and records fully auditable logs.\n  Maintaining selection pressure by setting the success threshold to $\\theta =\n0.70$, we obtain five evolutionary generations, two new template families, and\n44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of\n2,500 trials (judge fixed) shows that structural gains transfer but vary by\ntarget; two models score zero at the same threshold. We also find a positive\ncoupling between prompt length and score, motivating length-aware judging.\n  Our results demonstrate that structure-level search is a reproducible route\nto stronger single-turn probes and underscore the importance of threshold\ncalibration and cross-model evaluation. Code, configurations, and artifacts are\navailable at https://github.com/hyunjun1121/M2S-x-teaming.",
        "url": "http://arxiv.org/abs/2509.08729v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08729v1",
        "arxiv_id": "2509.08729v1",
        "authors": [
            "Hyunjun Kim",
            "Junwoo Ha",
            "Sangyoon Yu",
            "Haon Park"
        ],
        "submitted": "2025-09-10 16:17:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Generative Data Refinement: Just Ask for Better Data",
        "abstract": "For a fixed parameter size, the capabilities of large models are primarily\ndetermined by the quality and quantity of its training data. Consequently,\ntraining datasets now grow faster than the rate at which new data is indexed on\nthe web, leading to projected data exhaustion over the next decade. Much more\ndata exists as user-generated content that is not publicly indexed, but\nincorporating such data comes with considerable risks, such as leaking private\ninformation and other undesirable content. We introduce a framework, Generative\nData Refinement (GDR), for using pretrained generative models to transform a\ndataset with undesirable content into a refined dataset that is more suitable\nfor training. Our experiments show that GDR can outperform industry-grade\nsolutions for dataset anonymization, as well as enable direct detoxification of\nhighly unsafe datasets. Moreover, we show that by generating synthetic data\nthat is conditioned on each example in the real dataset, GDR's refined outputs\nnaturally match the diversity of web scale datasets, and thereby avoid the\noften challenging task of generating diverse synthetic data via model\nprompting. The simplicity and effectiveness of GDR make it a powerful tool for\nscaling up the total stock of training data for frontier models.",
        "url": "http://arxiv.org/abs/2509.08653v2",
        "pdf_url": "http://arxiv.org/pdf/2509.08653v2",
        "arxiv_id": "2509.08653v2",
        "authors": [
            "Minqi Jiang",
            "João G. M. Araújo",
            "Will Ellsworth",
            "Sian Gooding",
            "Edward Grefenstette"
        ],
        "submitted": "2025-09-10 14:49:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "OTESGN: Optimal Transport-Enhanced Syntactic-Semantic Graph Networks for Aspect-Based Sentiment Analysis",
        "abstract": "Aspect-based sentiment analysis (ABSA) aims to identify aspect terms and\ndetermine their sentiment polarity. While dependency trees combined with\ncontextual semantics provide structural cues, existing approaches often rely on\ndot-product similarity and fixed graphs, which limit their ability to capture\nnonlinear associations and adapt to noisy contexts. To address these\nlimitations, we propose the Optimal Transport-Enhanced Syntactic-Semantic Graph\nNetwork (OTESGN), a model that jointly integrates structural and distributional\nsignals. Specifically, a Syntactic Graph-Aware Attention module models global\ndependencies with syntax-guided masking, while a Semantic Optimal Transport\nAttention module formulates aspect-opinion association as a distribution\nmatching problem solved via the Sinkhorn algorithm. An Adaptive Attention\nFusion mechanism balances heterogeneous features, and contrastive\nregularization enhances robustness. Extensive experiments on three benchmark\ndatasets (Rest14, Laptop14, and Twitter) demonstrate that OTESGN delivers\nstate-of-the-art performance. Notably, it surpasses competitive baselines by up\nto +1.30 Macro-F1 on Laptop14 and +1.01 on Twitter. Ablation studies and\nvisualization analyses further highlight OTESGN's ability to capture\nfine-grained sentiment associations and suppress noise from irrelevant context.",
        "url": "http://arxiv.org/abs/2509.08612v2",
        "pdf_url": "http://arxiv.org/pdf/2509.08612v2",
        "arxiv_id": "2509.08612v2",
        "authors": [
            "Xinfeng Liao",
            "Xuanqi Chen",
            "Lianxi Wang",
            "Jiahuan Yang",
            "Zhuowei Chen",
            "Ziying Rong"
        ],
        "submitted": "2025-09-10 14:08:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications",
        "abstract": "Large Language Models (LLMs) have demonstrated significant potential in\nmedicine. To date, LLMs have been widely applied to tasks such as diagnostic\nassistance, medical question answering, and clinical information synthesis.\nHowever, a key open question remains: to what extent do LLMs memorize medical\ntraining data. In this study, we present the first comprehensive evaluation of\nmemorization of LLMs in medicine, assessing its prevalence (how frequently it\noccurs), characteristics (what is memorized), volume (how much content is\nmemorized), and potential downstream impacts (how memorization may affect\nmedical applications). We systematically analyze common adaptation scenarios:\n(1) continued pretraining on medical corpora, (2) fine-tuning on standard\nmedical benchmarks, and (3) fine-tuning on real-world clinical data, including\nover 13,000 unique inpatient records from Yale New Haven Health System. The\nresults demonstrate that memorization is prevalent across all adaptation\nscenarios and significantly higher than reported in the general domain.\nMemorization affects both the development and adoption of LLMs in medicine and\ncan be categorized into three types: beneficial (e.g., accurate recall of\nclinical guidelines and biomedical references), uninformative (e.g., repeated\ndisclaimers or templated medical document language), and harmful (e.g.,\nregeneration of dataset-specific or sensitive clinical content). Based on these\nfindings, we offer practical recommendations to facilitate beneficial\nmemorization that enhances domain-specific reasoning and factual accuracy,\nminimize uninformative memorization to promote deeper learning beyond\nsurface-level patterns, and mitigate harmful memorization to prevent the\nleakage of sensitive or identifiable patient information.",
        "url": "http://arxiv.org/abs/2509.08604v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08604v1",
        "arxiv_id": "2509.08604v1",
        "authors": [
            "Anran Li",
            "Lingfei Qian",
            "Mengmeng Du",
            "Yu Yin",
            "Yan Hu",
            "Zihao Sun",
            "Yihang Fu",
            "Erica Stutz",
            "Xuguang Ai",
            "Qianqian Xie",
            "Rui Zhu",
            "Jimin Huang",
            "Yifan Yang",
            "Siru Liu",
            "Yih-Chung Tham",
            "Lucila Ohno-Machado",
            "Hyunghoon Cho",
            "Zhiyong Lu",
            "Hua Xu",
            "Qingyu Chen"
        ],
        "submitted": "2025-09-10 14:02:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question Answering for BioASQ Challenge",
        "abstract": "Biomedical question answering (QA) poses significant challenges due to the\nneed for precise interpretation of specialized knowledge drawn from a vast,\ncomplex, and rapidly evolving corpus. In this work, we explore how large\nlanguage models (LLMs) can be used for information retrieval (IR), and an\nensemble of zero-shot models can accomplish state-of-the-art performance on a\ndomain-specific Yes/No QA task. Evaluating our approach on the BioASQ challenge\ntasks, we show that ensembles can outperform individual LLMs and in some cases\nrival or surpass domain-tuned systems - all while preserving generalizability\nand avoiding the need for costly fine-tuning or labeled data. Our method\naggregates outputs from multiple LLM variants, including models from Anthropic\nand Google, to synthesize more accurate and robust answers. Moreover, our\ninvestigation highlights a relationship between context length and performance:\nwhile expanded contexts are meant to provide valuable evidence, they\nsimultaneously risk information dilution and model disorientation. These\nfindings emphasize IR as a critical foundation in Retrieval-Augmented\nGeneration (RAG) approaches for biomedical QA systems. Precise, focused\nretrieval remains essential for ensuring LLMs operate within relevant\ninformation boundaries when generating answers from retrieved documents. Our\nresults establish that ensemble-based zero-shot approaches, when paired with\neffective RAG pipelines, constitute a practical and scalable alternative to\ndomain-tuned systems for biomedical question answering.",
        "url": "http://arxiv.org/abs/2509.08596v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08596v1",
        "arxiv_id": "2509.08596v1",
        "authors": [
            "Dima Galat",
            "Diego Molla-Aliod"
        ],
        "submitted": "2025-09-10 13:50:49",
        "source": "arxiv",
        "comment": "CEUR-WS, CLEF2025"
    },
    {
        "title": "CM-Align: Consistency-based Multilingual Alignment for Large Language Models",
        "abstract": "Current large language models (LLMs) generally show a significant performance\ngap in alignment between English and other languages. To bridge this gap,\nexisting research typically leverages the model's responses in English as a\nreference to select the best/worst responses in other languages, which are then\nused for Direct Preference Optimization (DPO) training. However, we argue that\nthere are two limitations in the current methods that result in noisy\nmultilingual preference data and further limited alignment performance: 1) Not\nall English responses are of high quality, and using a response with low\nquality may mislead the alignment for other languages. 2) Current methods\nusually use biased or heuristic approaches to construct multilingual preference\npairs. To address these limitations, we design a consistency-based data\nselection method to construct high-quality multilingual preference data for\nimproving multilingual alignment (CM-Align). Specifically, our method includes\ntwo parts: consistency-guided English reference selection and cross-lingual\nconsistency-based multilingual preference data construction. Experimental\nresults on three LLMs and three common tasks demonstrate the effectiveness and\nsuperiority of our method, which further indicates the necessity of\nconstructing high-quality preference data.",
        "url": "http://arxiv.org/abs/2509.08541v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08541v1",
        "arxiv_id": "2509.08541v1",
        "authors": [
            "Xue Zhang",
            "Yunlong Liang",
            "Fandong Meng",
            "Songming Zhang",
            "Yufeng Chen",
            "Jinan Xu",
            "Jie Zhou"
        ],
        "submitted": "2025-09-10 12:40:49",
        "source": "arxiv",
        "comment": "EMNLP 2025 Findings"
    },
    {
        "title": "HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI Assistants",
        "abstract": "As humans delegate more tasks and decisions to artificial intelligence (AI),\nwe risk losing control of our individual and collective futures. Relatively\nsimple algorithmic systems already steer human decision-making, such as social\nmedia feed algorithms that lead people to unintentionally and absent-mindedly\nscroll through engagement-optimized content. In this paper, we develop the idea\nof human agency by integrating philosophical and scientific theories of agency\nwith AI-assisted evaluation methods: using large language models (LLMs) to\nsimulate and validate user queries and to evaluate AI responses. We develop\nHumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions\nof human agency based on typical AI use cases. HAB measures the tendency of an\nAI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation,\nCorrect Misinformation, Defer Important Decisions, Encourage Learning, and\nMaintain Social Boundaries. We find low-to-moderate agency support in\ncontemporary LLM-based assistants and substantial variation across system\ndevelopers and dimensions. For example, while Anthropic LLMs most support human\nagency overall, they are the least supportive LLMs in terms of Avoid Value\nManipulation. Agency support does not appear to consistently result from\nincreasing LLM capabilities or instruction-following behavior (e.g., RLHF), and\nwe encourage a shift towards more robust safety and alignment targets.",
        "url": "http://arxiv.org/abs/2509.08494v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08494v1",
        "arxiv_id": "2509.08494v1",
        "authors": [
            "Benjamin Sturgeon",
            "Daniel Samuelson",
            "Jacob Haimes",
            "Jacy Reese Anthis"
        ],
        "submitted": "2025-09-10 11:10:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Too Helpful, Too Harmless, Too Honest or Just Right?",
        "abstract": "Large Language Models (LLMs) exhibit strong performance across a wide range\nof NLP tasks, yet aligning their outputs with the principles of Helpfulness,\nHarmlessness, and Honesty (HHH) remains a persistent challenge. Existing\nmethods often optimize for individual alignment dimensions in isolation,\nleading to trade-offs and inconsistent behavior. While Mixture-of-Experts (MoE)\narchitectures offer modularity, they suffer from poorly calibrated routing,\nlimiting their effectiveness in alignment tasks. We propose TrinityX, a modular\nalignment framework that incorporates a Mixture of Calibrated Experts (MoCaE)\nwithin the Transformer architecture. TrinityX leverages separately trained\nexperts for each HHH dimension, integrating their outputs through a calibrated,\ntask-adaptive routing mechanism that combines expert signals into a unified,\nalignment-aware representation. Extensive experiments on three standard\nalignment benchmarks-Alpaca (Helpfulness), BeaverTails (Harmlessness), and\nTruthfulQA (Honesty)-demonstrate that TrinityX outperforms strong baselines,\nachieving relative improvements of 32.5% in win rate, 33.9% in safety score,\nand 28.4% in truthfulness. In addition, TrinityX reduces memory usage and\ninference latency by over 40% compared to prior MoE-based approaches. Ablation\nstudies highlight the importance of calibrated routing, and cross-model\nevaluations confirm TrinityX's generalization across diverse LLM backbones.",
        "url": "http://arxiv.org/abs/2509.08486v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08486v1",
        "arxiv_id": "2509.08486v1",
        "authors": [
            "Gautam Siddharth Kashyap",
            "Mark Dras",
            "Usman Naseem"
        ],
        "submitted": "2025-09-10 10:51:47",
        "source": "arxiv",
        "comment": "EMNLP'25 Main"
    },
    {
        "title": "Simulating Identity, Propagating Bias: Abstraction and Stereotypes in LLM-Generated Text",
        "abstract": "Persona-prompting is a growing strategy to steer LLMs toward simulating\nparticular perspectives or linguistic styles through the lens of a specified\nidentity. While this method is often used to personalize outputs, its impact on\nhow LLMs represent social groups remains underexplored. In this paper, we\ninvestigate whether persona-prompting leads to different levels of linguistic\nabstraction - an established marker of stereotyping - when generating short\ntexts linking socio-demographic categories with stereotypical or\nnon-stereotypical attributes. Drawing on the Linguistic Expectancy Bias\nframework, we analyze outputs from six open-weight LLMs under three prompting\nconditions, comparing 11 persona-driven responses to those of a generic AI\nassistant. To support this analysis, we introduce Self-Stereo, a new dataset of\nself-reported stereotypes from Reddit. We measure abstraction through three\nmetrics: concreteness, specificity, and negation. Our results highlight the\nlimits of persona-prompting in modulating abstraction in language, confirming\ncriticisms about the ecology of personas as representative of socio-demographic\ngroups and raising concerns about the risk of propagating stereotypes even when\nseemingly evoking the voice of a marginalized group.",
        "url": "http://arxiv.org/abs/2509.08484v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08484v1",
        "arxiv_id": "2509.08484v1",
        "authors": [
            "Pia Sommerauer",
            "Giulia Rambelli",
            "Tommaso Caselli"
        ],
        "submitted": "2025-09-10 10:49:21",
        "source": "arxiv",
        "comment": "Accepted to EMNLP Findings 2025"
    },
    {
        "title": "Acquiescence Bias in Large Language Models",
        "abstract": "Acquiescence bias, i.e. the tendency of humans to agree with statements in\nsurveys, independent of their actual beliefs, is well researched and\ndocumented. Since Large Language Models (LLMs) have been shown to be very\ninfluenceable by relatively small changes in input and are trained on\nhuman-generated data, it is reasonable to assume that they could show a similar\ntendency. We present a study investigating the presence of acquiescence bias in\nLLMs across different models, tasks, and languages (English, German, and\nPolish). Our results indicate that, contrary to humans, LLMs display a bias\ntowards answering no, regardless of whether it indicates agreement or\ndisagreement.",
        "url": "http://arxiv.org/abs/2509.08480v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08480v1",
        "arxiv_id": "2509.08480v1",
        "authors": [
            "Daniel Braun"
        ],
        "submitted": "2025-09-10 10:39:24",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 Findings"
    },
    {
        "title": "Adversarial Attacks Against Automated Fact-Checking: A Survey",
        "abstract": "In an era where misinformation spreads freely, fact-checking (FC) plays a\ncrucial role in verifying claims and promoting reliable information. While\nautomated fact-checking (AFC) has advanced significantly, existing systems\nremain vulnerable to adversarial attacks that manipulate or generate claims,\nevidence, or claim-evidence pairs. These attacks can distort the truth, mislead\ndecision-makers, and ultimately undermine the reliability of FC models. Despite\ngrowing research interest in adversarial attacks against AFC systems, a\ncomprehensive, holistic overview of key challenges remains lacking. These\nchallenges include understanding attack strategies, assessing the resilience of\ncurrent models, and identifying ways to enhance robustness. This survey\nprovides the first in-depth review of adversarial attacks targeting FC,\ncategorizing existing attack methodologies and evaluating their impact on AFC\nsystems. Additionally, we examine recent advancements in adversary-aware\ndefenses and highlight open research questions that require further\nexploration. Our findings underscore the urgent need for resilient FC\nframeworks capable of withstanding adversarial manipulations in pursuit of\npreserving high verification accuracy.",
        "url": "http://arxiv.org/abs/2509.08463v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08463v1",
        "arxiv_id": "2509.08463v1",
        "authors": [
            "Fanzhen Liu",
            "Alsharif Abuadbba",
            "Kristen Moore",
            "Surya Nepal",
            "Cecile Paris",
            "Jia Wu",
            "Jian Yang",
            "Quan Z. Sheng"
        ],
        "submitted": "2025-09-10 10:10:10",
        "source": "arxiv",
        "comment": "Accepted to the Main Conference of EMNLP 2025. Resources are\n  available at\n  https://github.com/FanzhenLiu/Awesome-Automated-Fact-Checking-Attacks"
    },
    {
        "title": "CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction with a New Dataset and Multi-Order Generative Framework",
        "abstract": "Speech Relation Extraction (SpeechRE) aims to extract relation triplets\ndirectly from speech. However, existing benchmark datasets rely heavily on\nsynthetic data, lacking sufficient quantity and diversity of real human speech.\nMoreover, existing models also suffer from rigid single-order generation\ntemplates and weak semantic alignment, substantially limiting their\nperformance. To address these challenges, we introduce CommonVoice-SpeechRE, a\nlarge-scale dataset comprising nearly 20,000 real-human speech samples from\ndiverse speakers, establishing a new benchmark for SpeechRE research.\nFurthermore, we propose the Relation Prompt-Guided Multi-Order Generative\nEnsemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet\ngeneration ensemble strategy, leveraging data diversity through diverse element\norders during both training and inference, and (2) CNN-based latent relation\nprediction heads that generate explicit relation prompts to guide cross-modal\nalignment and accurate triplet generation. Experiments show our approach\noutperforms state-of-the-art methods, providing both a benchmark dataset and an\neffective solution for real-world SpeechRE. The source code and dataset are\npublicly available at https://github.com/NingJinzhong/SpeechRE_RPG_MoGe.",
        "url": "http://arxiv.org/abs/2509.08438v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08438v1",
        "arxiv_id": "2509.08438v1",
        "authors": [
            "Jinzhong Ning",
            "Paerhati Tulajiang",
            "Yingying Le",
            "Yijia Zhang",
            "Yuanyuan Sun",
            "Hongfei Lin",
            "Haifeng Liu"
        ],
        "submitted": "2025-09-10 09:35:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model",
        "abstract": "Deploying large language models (LLMs) for structured data extraction in\ndomains such as financial compliance reporting, legal document analytics, and\nmultilingual knowledge base construction is often impractical for smaller teams\ndue to the high cost of running large architectures and the difficulty of\npreparing large, high-quality datasets. Most recent instruction-tuning studies\nfocus on seven-billion-parameter or larger models, leaving limited evidence on\nwhether much smaller models can work reliably under low-resource, multi-task\nconditions. This work presents ETLCH, a billion-parameter LLaMA-based model\nfine-tuned with low-rank adaptation on only a few hundred to one thousand\nsamples per task for JSON extraction, knowledge graph extraction, and named\nentity recognition. Despite its small scale, ETLCH outperforms strong baselines\nacross most evaluation metrics, with substantial gains observed even at the\nlowest data scale. These findings demonstrate that well-tuned small models can\ndeliver stable and accurate structured outputs at a fraction of the\ncomputational cost, enabling cost-effective and reliable information extraction\npipelines in resource-constrained environments.",
        "url": "http://arxiv.org/abs/2509.08381v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08381v1",
        "arxiv_id": "2509.08381v1",
        "authors": [
            "Yu Cheng Chih",
            "Yong Hao Hou"
        ],
        "submitted": "2025-09-10 08:19:07",
        "source": "arxiv",
        "comment": "13 pages, 8 figures, includes experiments on JSON extraction,\n  knowledge graph extraction, and NER"
    },
    {
        "title": "<think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs",
        "abstract": "Modern Large Language Models (LLMs) are excellent at generating synthetic\ndata. However, their performance in sensitive domains such as text\ndetoxification has not received proper attention from the scientific community.\nThis paper explores the possibility of using LLM-generated synthetic toxic data\nas an alternative to human-generated data for training models for\ndetoxification. Using Llama 3 and Qwen activation-patched models, we generated\nsynthetic toxic counterparts for neutral texts from ParaDetox and SST-2\ndatasets. Our experiments show that models fine-tuned on synthetic data\nconsistently perform worse than those trained on human data, with a drop in\nperformance of up to 30% in joint metrics. The root cause is identified as a\ncritical lexical diversity gap: LLMs generate toxic content using a small,\nrepetitive vocabulary of insults that fails to capture the nuances and variety\nof human toxicity. These findings highlight the limitations of current LLMs in\nthis domain and emphasize the continued importance of diverse, human-annotated\ndata for building robust detoxification systems.",
        "url": "http://arxiv.org/abs/2509.08358v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08358v1",
        "arxiv_id": "2509.08358v1",
        "authors": [
            "Sergey Pletenev",
            "Daniil Moskovskiy",
            "Alexander Panchenko"
        ],
        "submitted": "2025-09-10 07:48:24",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Automatic Detection of Inauthentic Templated Responses in English Language Assessments",
        "abstract": "In high-stakes English Language Assessments, low-skill test takers may employ\nmemorized materials called ``templates'' on essay questions to ``game'' or fool\nthe automated scoring system. In this study, we introduce the automated\ndetection of inauthentic, templated responses (AuDITR) task, describe a machine\nlearning-based approach to this task and illustrate the importance of regularly\nupdating these models in production.",
        "url": "http://arxiv.org/abs/2509.08355v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08355v1",
        "arxiv_id": "2509.08355v1",
        "authors": [
            "Yashad Samant",
            "Lee Becker",
            "Scott Hellman",
            "Bradley Behan",
            "Sarah Hughes",
            "Joshua Southerland"
        ],
        "submitted": "2025-09-10 07:45:02",
        "source": "arxiv",
        "comment": "Accepted to National Council on Measurement in Education (NCME) 2025\n  Annual Meeting"
    },
    {
        "title": "Toward Subtrait-Level Model Explainability in Automated Writing Evaluation",
        "abstract": "Subtrait (latent-trait components) assessment presents a promising path\ntoward enhancing transparency of automated writing scores. We prototype\nexplainability and subtrait scoring with generative language models and show\nmodest correlation between human subtrait and trait scores, and between\nautomated and human subtrait scores. Our approach provides details to demystify\nscores for educators and students.",
        "url": "http://arxiv.org/abs/2509.08345v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08345v1",
        "arxiv_id": "2509.08345v1",
        "authors": [
            "Alejandro Andrade-Lotero",
            "Lee Becker",
            "Joshua Southerland",
            "Scott Hellman"
        ],
        "submitted": "2025-09-10 07:32:14",
        "source": "arxiv",
        "comment": "Accepted to National Council on Measurement in Education (NCME) 2025\n  Annual Meeting"
    },
    {
        "title": "Soundtracks of Our Lives: How Age Influences Musical Preferences",
        "abstract": "The majority of research in recommender systems, be it algorithmic\nimprovements, context-awareness, explainability, or other areas, evaluates\nthese systems on datasets that capture user interaction over a relatively\nlimited time span. However, recommender systems can very well be used\ncontinuously for extended time. Similarly so, user behavior may evolve over\nthat extended time. Although media studies and psychology offer a wealth of\nresearch on the evolution of user preferences and behavior as individuals age,\nthere has been scant research in this regard within the realm of user modeling\nand recommender systems. In this study, we investigate the evolution of user\npreferences and behavior using the LFM-2b dataset, which, to our knowledge, is\nthe only dataset that encompasses a sufficiently extensive time frame to permit\nreal longitudinal studies and includes age information about its users. We\nidentify specific usage and taste preferences directly related to the age of\nthe user, i.e., while younger users tend to listen broadly to contemporary\npopular music, older users have more elaborate and personalized listening\nhabits. The findings yield important insights that open new directions for\nresearch in recommender systems, providing guidance for future efforts.",
        "url": "http://arxiv.org/abs/2509.08337v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08337v1",
        "arxiv_id": "2509.08337v1",
        "authors": [
            "Arsen Matej Golubovikj",
            "Bruce Ferwerda",
            "Alan Said",
            "Marko Talčič"
        ],
        "submitted": "2025-09-10 07:21:55",
        "source": "arxiv",
        "comment": "Accepted to UMAP 2025"
    },
    {
        "title": "EvolKV: Evolutionary KV Cache Compression for LLM Inference",
        "abstract": "Existing key-value (KV) cache compression methods typically rely on\nheuristics, such as uniform cache allocation across layers or static eviction\npolicies, however, they ignore the critical interplays among layer-specific\nfeature patterns and task performance, which can lead to degraded\ngeneralization. In this paper, we propose EvolKV, an adaptive framework for\nlayer-wise, task-driven KV cache compression that jointly optimizes the memory\nefficiency and task performance. By reformulating cache allocation as a\nmulti-objective optimization problem, EvolKV leverages evolutionary search to\ndynamically configure layer budgets while directly maximizing downstream\nperformance. Extensive experiments on 11 tasks demonstrate that our approach\noutperforms all baseline methods across a wide range of KV cache budgets on\nlong-context tasks and surpasses heuristic baselines by up to 7 percentage\npoints on GSM8K. Notably, EvolKV achieves superior performance over the full KV\ncache setting on code completion while utilizing only 1.5% of the original\nbudget, suggesting the untapped potential in learned compression strategies for\nKV cache budget allocation.",
        "url": "http://arxiv.org/abs/2509.08315v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08315v1",
        "arxiv_id": "2509.08315v1",
        "authors": [
            "Bohan Yu",
            "Yekun Chai"
        ],
        "submitted": "2025-09-10 06:32:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Towards Knowledge-Aware Document Systems: Modeling Semantic Coverage Relations via Answerability Detection",
        "abstract": "Understanding how information is shared across documents, regardless of the\nformat in which it is expressed, is critical for tasks such as information\nretrieval, summarization, and content alignment. In this work, we introduce a\nnovel framework for modelling Semantic Coverage Relations (SCR), which\nclassifies document pairs based on how their informational content aligns. We\ndefine three core relation types: equivalence, where both texts convey the same\ninformation using different textual forms or styles; inclusion, where one\ndocument fully contains the information of another and adds more; and semantic\noverlap, where each document presents partially overlapping content. To capture\nthese relations, we adopt a question answering (QA)-based approach, using the\nanswerability of shared questions across documents as an indicator of semantic\ncoverage. We construct a synthetic dataset derived from the SQuAD corpus by\nparaphrasing source passages and selectively omitting information, enabling\nprecise control over content overlap. This dataset allows us to benchmark\ngenerative language models and train transformer-based classifiers for SCR\nprediction. Our findings demonstrate that discriminative models significantly\noutperform generative approaches, with the RoBERTa-base model achieving the\nhighest accuracy of 61.4% and the Random Forest-based model showing the best\nbalance with a macro-F1 score of 52.9%. The results show that QA provides an\neffective lens for assessing semantic relations across stylistically diverse\ntexts, offering insights into the capacity of current models to reason about\ninformation beyond surface similarity. The dataset and code developed in this\nstudy are publicly available to support reproducibility.",
        "url": "http://arxiv.org/abs/2509.08304v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08304v1",
        "arxiv_id": "2509.08304v1",
        "authors": [
            "Yehudit Aperstein",
            "Alon Gottlib",
            "Gal Benita",
            "Alexander Apartsin"
        ],
        "submitted": "2025-09-10 06:00:01",
        "source": "arxiv",
        "comment": "27 pages, 1 figure"
    },
    {
        "title": "Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions",
        "abstract": "For machine learning datasets to accurately represent diverse opinions in a\npopulation, they must preserve variation in data labels while filtering out\nspam or low-quality responses. How can we balance annotator reliability and\nrepresentation? We empirically evaluate how a range of heuristics for annotator\nfiltering affect the preservation of variation on subjective tasks. We find\nthat these methods, designed for contexts in which variation from a single\nground-truth label is considered noise, often remove annotators who disagree\ninstead of spam annotators, introducing suboptimal tradeoffs between accuracy\nand label diversity. We find that conservative settings for annotator removal\n(<5%) are best, after which all tested methods increase the mean absolute error\nfrom the true average label. We analyze performance on synthetic spam to\nobserve that these methods often assume spam annotators are less random than\nreal spammers tend to be: most spammers are distributionally indistinguishable\nfrom real annotators, and the minority that are distinguishable tend to give\nfixed answers, not random ones. Thus, tasks requiring the preservation of\nvariation reverse the intuition of existing spam filtering methods: spammers\ntend to be less random than non-spammers, so metrics that assume variation is\nspam fare worse. These results highlight the need for spam removal methods that\naccount for label diversity.",
        "url": "http://arxiv.org/abs/2509.08217v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08217v1",
        "arxiv_id": "2509.08217v1",
        "authors": [
            "Eve Fleisig",
            "Matthias Orlikowski",
            "Philipp Cimiano",
            "Dan Klein"
        ],
        "submitted": "2025-09-10 01:22:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Vector embedding of multi-modal texts: a tool for discovery?",
        "abstract": "Computer science texts are particularly rich in both narrative content and\nillustrative charts, algorithms, images, annotated diagrams, etc. This study\nexplores the extent to which vector-based multimodal retrieval, powered by\nvision-language models (VLMs), can improve discovery across multi-modal (text\nand images) content. Using over 3,600 digitized textbook pages largely from\ncomputer science textbooks and a Vision Language Model (VLM), we generate\nmulti-vector representations capturing both textual and visual semantics. These\nembeddings are stored in a vector database. We issue a benchmark of 75 natural\nlanguage queries and compare retrieval performance to ground truth and across\nfour similarity (distance) measures. The study is intended to expose both the\nstrengths and weakenesses of such an approach. We find that cosine similarity\nmost effectively retrieves semantically and visually relevant pages. We further\ndiscuss the practicality of using a vector database and multi-modal embedding\nfor operational information retrieval. Our paper is intended to offer design\ninsights for discovery over digital libraries.\n  Keywords: Vector embedding, multi-modal document retrieval, vector database\nbenchmark, digital library discovery",
        "url": "http://arxiv.org/abs/2509.08216v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08216v1",
        "arxiv_id": "2509.08216v1",
        "authors": [
            "Beth Plale",
            "Sai Navya Jyesta",
            "Sachith Withana"
        ],
        "submitted": "2025-09-10 01:14:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "XML Prompting as Grammar-Constrained Interaction: Fixed-Point Semantics, Convergence Guarantees, and Human-AI Protocols",
        "abstract": "Structured prompting with XML tags has emerged as an effective way to steer\nlarge language models (LLMs) toward parseable, schema-adherent outputs in\nreal-world systems. We develop a logic-first treatment of XML prompting that\nunifies (i) grammar-constrained decoding, (ii) fixed-point semantics over\nlattices of hierarchical prompts, and (iii) convergent human-AI interaction\nloops. We formalize a complete lattice of XML trees under a refinement order\nand prove that monotone prompt-to-prompt operators admit least fixed points\n(Knaster-Tarski) that characterize steady-state protocols; under a task-aware\ncontraction metric on trees, we further prove Banach-style convergence of\niterative guidance. We instantiate these results with context-free grammars\n(CFGs) for XML schemas and show how constrained decoding guarantees\nwell-formedness while preserving task performance. A set of multi-layer\nhuman-AI interaction recipes demonstrates practical deployment patterns,\nincluding multi-pass \"plan $\\to$ verify $\\to$ revise\" routines and agentic tool\nuse. We provide mathematically complete proofs and tie our framework to recent\nadvances in grammar-aligned decoding, chain-of-verification, and programmatic\nprompting.",
        "url": "http://arxiv.org/abs/2509.08182v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08182v1",
        "arxiv_id": "2509.08182v1",
        "authors": [
            "Faruk Alpay",
            "Taylan Alpay"
        ],
        "submitted": "2025-09-09 23:03:53",
        "source": "arxiv",
        "comment": "7 pages, multiple XML prompts"
    },
    {
        "title": "Verbalized Algorithms",
        "abstract": "Instead of querying LLMs in a one-shot manner and hoping to get the right\nanswer for a reasoning task, we propose a paradigm we call \\emph{verbalized\nalgorithms} (VAs), which leverage classical algorithms with established\ntheoretical understanding. VAs decompose a task into simple elementary\noperations on natural language strings that they should be able to answer\nreliably, and limit the scope of LLMs to only those simple tasks. For example,\nfor sorting a series of natural language strings, \\emph{verbalized sorting}\nuses an LLM as a binary comparison oracle in a known and well-analyzed sorting\nalgorithm (e.g., bitonic sorting network). We demonstrate the effectiveness of\nthis approach on sorting and clustering tasks.",
        "url": "http://arxiv.org/abs/2509.08150v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08150v1",
        "arxiv_id": "2509.08150v1",
        "authors": [
            "Supriya Lall",
            "Christian Farrell",
            "Hari Pathanjaly",
            "Marko Pavic",
            "Sarvesh Chezhian",
            "Masataro Asai"
        ],
        "submitted": "2025-09-09 21:14:44",
        "source": "arxiv",
        "comment": "Submitted to NeurIPS 2025 Workshop on Efficient Reasoning"
    },
    {
        "title": "Bias after Prompting: Persistent Discrimination in Large Language Models",
        "abstract": "A dangerous assumption that can be made from prior work on the bias transfer\nhypothesis (BTH) is that biases do not transfer from pre-trained large language\nmodels (LLMs) to adapted models. We invalidate this assumption by studying the\nBTH in causal models under prompt adaptations, as prompting is an extremely\npopular and accessible adaptation strategy used in real-world applications. In\ncontrast to prior work, we find that biases can transfer through prompting and\nthat popular prompt-based mitigation methods do not consistently prevent biases\nfrom transferring. Specifically, the correlation between intrinsic biases and\nthose after prompt adaptation remain moderate to strong across demographics and\ntasks -- for example, gender (rho >= 0.94) in co-reference resolution, and age\n(rho >= 0.98) and religion (rho >= 0.69) in question answering. Further, we\nfind that biases remain strongly correlated when varying few-shot composition\nparameters, such as sample size, stereotypical content, occupational\ndistribution and representational balance (rho >= 0.90). We evaluate several\nprompt-based debiasing strategies and find that different approaches have\ndistinct strengths, but none consistently reduce bias transfer across models,\ntasks or demographics. These results demonstrate that correcting bias, and\npotentially improving reasoning ability, in intrinsic models may prevent\npropagation of biases to downstream tasks.",
        "url": "http://arxiv.org/abs/2509.08146v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08146v1",
        "arxiv_id": "2509.08146v1",
        "authors": [
            "Nivedha Sivakumar",
            "Natalie Mackraz",
            "Samira Khorshidi",
            "Krishna Patel",
            "Barry-John Theobald",
            "Luca Zappella",
            "Nicholas Apostoloff"
        ],
        "submitted": "2025-09-09 20:59:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder and LLM Fusion",
        "abstract": "Large language models excel in English but still struggle with complex\nreasoning in many low-resource languages (LRLs). Existing encoder-plus-decoder\nmethods such as LangBridge and MindMerger raise accuracy on mid and\nhigh-resource languages, yet they leave a large gap on LRLs. We present MERLIN,\na two-stage model-stacking framework that applies a curriculum learning\nstrategy -- from general bilingual bitext to task-specific data -- and adapts\nonly a small set of DoRA weights. On the AfriMGSM benchmark MERLIN improves\nexact-match accuracy by +12.9 pp over MindMerger and outperforms GPT-4o-mini.\nIt also yields consistent gains on MGSM and MSVAMP (+0.9 and +2.8 pp),\ndemonstrating effectiveness across both low and high-resource settings.",
        "url": "http://arxiv.org/abs/2509.08105v2",
        "pdf_url": "http://arxiv.org/pdf/2509.08105v2",
        "arxiv_id": "2509.08105v2",
        "authors": [
            "Kosei Uemura",
            "David Guzmán",
            "Quang Phuoc Nguyen",
            "Jesujoba Oluwadara Alabi",
            "En-shiun Annie Lee",
            "David Ifeoluwa Adelani"
        ],
        "submitted": "2025-09-09 19:32:05",
        "source": "arxiv",
        "comment": "under submission"
    },
    {
        "title": "Culturally transmitted color categories in LLMs reflect a learning bias toward efficient compression",
        "abstract": "Converging evidence suggests that systems of semantic categories across human\nlanguages achieve near-optimal compression via the Information Bottleneck (IB)\ncomplexity-accuracy principle. Large language models (LLMs) are not trained for\nthis objective, which raises the question: are LLMs capable of evolving\nefficient human-like semantic systems? To address this question, we focus on\nthe domain of color as a key testbed of cognitive theories of categorization\nand replicate with LLMs (Gemini 2.0-flash and Llama 3.3-70B-Instruct) two\ninfluential human behavioral studies. First, we conduct an English color-naming\nstudy, showing that Gemini aligns well with the naming patterns of native\nEnglish speakers and achieves a significantly high IB-efficiency score, while\nLlama exhibits an efficient but lower complexity system compared to English.\nSecond, to test whether LLMs simply mimic patterns in their training data or\nactually exhibit a human-like inductive bias toward IB-efficiency, we simulate\ncultural evolution of pseudo color-naming systems in LLMs via iterated\nin-context language learning. We find that akin to humans, LLMs iteratively\nrestructure initially random systems towards greater IB-efficiency and\nincreased alignment with patterns observed across the world's languages. These\nfindings demonstrate that LLMs are capable of evolving perceptually grounded,\nhuman-like semantic systems, driven by the same fundamental principle that\ngoverns semantic efficiency across human languages.",
        "url": "http://arxiv.org/abs/2509.08093v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08093v1",
        "arxiv_id": "2509.08093v1",
        "authors": [
            "Nathaniel Imel",
            "Noga Zaslavsky"
        ],
        "submitted": "2025-09-09 19:00:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models",
        "abstract": "Large language models (LLMs) are increasingly integrated into our daily lives\nand personalized. However, LLM personalization might also increase unintended\nside effects. Recent work suggests that persona prompting can lead models to\nfalsely refuse user requests. However, no work has fully quantified the extent\nof this issue. To address this gap, we measure the impact of 15\nsociodemographic personas (based on gender, race, religion, and disability) on\nfalse refusal. To control for other factors, we also test 16 different models,\n3 tasks (Natural Language Inference, politeness, and offensiveness\nclassification), and nine prompt paraphrases. We propose a Monte Carlo-based\nmethod to quantify this issue in a sample-efficient manner. Our results show\nthat as models become more capable, personas impact the refusal rate less and\nless. Certain sociodemographic personas increase false refusal in some models,\nwhich suggests underlying biases in the alignment strategies or safety\nmechanisms. However, we find that the model choice and task significantly\ninfluence false refusals, especially in sensitive content tasks. Our findings\nsuggest that persona effects have been overestimated, and might be due to other\nfactors.",
        "url": "http://arxiv.org/abs/2509.08075v1",
        "pdf_url": "http://arxiv.org/pdf/2509.08075v1",
        "arxiv_id": "2509.08075v1",
        "authors": [
            "Flor Miriam Plaza-del-Arco",
            "Paul Röttger",
            "Nino Scherrer",
            "Emanuele Borgonovo",
            "Elmar Plischke",
            "Dirk Hovy"
        ],
        "submitted": "2025-09-09 18:30:01",
        "source": "arxiv",
        "comment": null
    }
]