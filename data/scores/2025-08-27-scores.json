[
    {
        "title": "REALM: Recursive Relevance Modeling for LLM-based Document Re-Ranking",
        "abstract": "Large Language Models (LLMs) have shown strong capabilities in document\nre-ranking, a key component in modern Information Retrieval (IR) systems.\nHowever, existing LLM-based approaches face notable limitations, including\nranking uncertainty, unstable top-k recovery, and high token cost due to\ntoken-intensive prompting. To effectively address these limitations, we propose\nREALM, an uncertainty-aware re-ranking framework that models LLM-derived\nrelevance as Gaussian distributions and refines them through recursive Bayesian\nupdates. By explicitly capturing uncertainty and minimizing redundant queries,\nREALM achieves better rankings more efficiently. Experimental results\ndemonstrate that our REALM surpasses state-of-the-art re-rankers while\nsignificantly reducing token usage and latency, promoting it as the\nnext-generation re-ranker for modern IR systems.",
        "url": "http://arxiv.org/abs/2508.18379v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18379v1",
        "arxiv_id": "2508.18379v1",
        "authors": [
            "Pinhuan Wang",
            "Zhiqiu Xia",
            "Chunhua Liao",
            "Feiyi Wang",
            "Hang Liu"
        ],
        "submitted": "2025-08-25 18:13:50",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 (Main Conference). 13 pages, 2 figures",
        "score": 15,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "The paper REALM: Recursive Relevance Modeling for LLM-based Document Re-Ranking is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The paper proposes a novel approach to re-ranking documents using Large Language Models, which aligns with your focus on query understanding and ranking models. The paper's emphasis on uncertainty-aware re-ranking and efficient token usage also resonates with your interest in real-time relevance optimization."
    },
    {
        "title": "Optimization of Latent-Space Compression using Game-Theoretic Techniques for Transformer-Based Vector Search",
        "abstract": "Vector similarity search plays a pivotal role in modern information retrieval\nsystems, especially when powered by transformer-based embeddings. However, the\nscalability and efficiency of such systems are often hindered by the high\ndimensionality of latent representations. In this paper, we propose a novel\ngame-theoretic framework for optimizing latent-space compression to enhance\nboth the efficiency and semantic utility of vector search. By modeling the\ncompression strategy as a zero-sum game between retrieval accuracy and storage\nefficiency, we derive a latent transformation that preserves semantic\nsimilarity while reducing redundancy. We benchmark our method against FAISS, a\nwidely-used vector search library, and demonstrate that our approach achieves a\nsignificantly higher average similarity (0.9981 vs. 0.5517) and utility (0.8873\nvs. 0.5194), albeit with a modest increase in query time. This trade-off\nhighlights the practical value of game-theoretic latent compression in\nhigh-utility, transformer-based search applications. The proposed system can be\nseamlessly integrated into existing LLM pipelines to yield more semantically\naccurate and computationally efficient retrieval.",
        "url": "http://arxiv.org/abs/2508.18877v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18877v1",
        "arxiv_id": "2508.18877v1",
        "authors": [
            "Kushagra Agrawal",
            "Nisharg Nargund",
            "Oishani Banerjee"
        ],
        "submitted": "2025-08-26 09:51:02",
        "source": "arxiv",
        "comment": null,
        "score": 11,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper is somewhat related to information retrieval, specifically vector similarity search, which is a relevant topic. However, the focus on transformer-based embeddings and latent-space compression is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling. While the paper's game-theoretic framework for optimization is interesting, it does not seem to address the user's primary research themes."
    },
    {
        "title": "Generative Interfaces for Language Models",
        "abstract": "Large language models (LLMs) are increasingly seen as assistants, copilots,\nand consultants, capable of supporting a wide range of tasks through natural\nconversation. However, most systems remain constrained by a linear\nrequest-response format that often makes interactions inefficient in\nmulti-turn, information-dense, and exploratory tasks. To address these\nlimitations, we propose Generative Interfaces for Language Models, a paradigm\nin which LLMs respond to user queries by proactively generating user interfaces\n(UIs) that enable more adaptive and interactive engagement. Our framework\nleverages structured interface-specific representations and iterative\nrefinements to translate user queries into task-specific UIs. For systematic\nevaluation, we introduce a multidimensional assessment framework that compares\ngenerative interfaces with traditional chat-based ones across diverse tasks,\ninteraction patterns, and query types, capturing functional, interactive, and\nemotional aspects of user experience. Results show that generative interfaces\nconsistently outperform conversational ones, with humans preferring them in\nover 70% of cases. These findings clarify when and why users favor generative\ninterfaces, paving the way for future advancements in human-AI interaction.",
        "url": "http://arxiv.org/abs/2508.19227v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19227v1",
        "arxiv_id": "2508.19227v1",
        "authors": [
            "Jiaqi Chen",
            "Yanzhe Zhang",
            "Yutong Zhang",
            "Yijia Shao",
            "Diyi Yang"
        ],
        "submitted": "2025-08-26 17:43:20",
        "source": "arxiv",
        "comment": "Preprint",
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the concept of generative interfaces for language models, which is related to information retrieval and search technologies. However, the focus is on the user interface and human-AI interaction, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest for you."
    },
    {
        "title": "Text to Query Plans for Question Answering on Large Tables",
        "abstract": "Efficient querying and analysis of large tabular datasets remain significant\nchallenges, especially for users without expertise in programming languages\nlike SQL. Text-to-SQL approaches have shown promising performance on benchmark\ndata; however, they inherit SQL's drawbacks, including inefficiency with large\ndatasets and limited support for complex data analyses beyond basic querying.\nWe propose a novel framework that transforms natural language queries into\nquery plans. Our solution is implemented outside traditional databases,\nallowing us to support classical SQL commands while avoiding SQL's inherent\nlimitations. Additionally, we enable complex analytical functions, such as\nprincipal component analysis and anomaly detection, providing greater\nflexibility and extensibility than traditional SQL capabilities. We leverage\nLLMs to iteratively interpret queries and construct operation sequences,\naddressing computational complexity by incrementally building solutions. By\nexecuting operations directly on the data, we overcome context length\nlimitations without requiring the entire dataset to be processed by the model.\nWe validate our framework through experiments on both standard databases and\nlarge scientific tables, demonstrating its effectiveness in handling extensive\ndatasets and performing sophisticated data analyses.",
        "url": "http://arxiv.org/abs/2508.18758v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18758v1",
        "arxiv_id": "2508.18758v1",
        "authors": [
            "Yipeng Zhang",
            "Chen Wang",
            "Yuzhe Zhang",
            "Jacky Jiang"
        ],
        "submitted": "2025-08-26 07:35:26",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores a novel framework for transforming natural language queries into query plans, which is related to query understanding and ranking models in Information Retrieval. However, the focus on large tabular datasets and SQL commands is not directly aligned with the user's interests in e-commerce and real-time relevance optimization. The paper's relevance is somewhat limited to the user's background in NLP and data mining, but it does not address the user's primary focus on information retrieval and deep semantic understanding."
    },
    {
        "title": "UniC-RAG: Universal Knowledge Corruption Attacks to Retrieval-Augmented Generation",
        "abstract": "Retrieval-augmented generation (RAG) systems are widely deployed in\nreal-world applications in diverse domains such as finance, healthcare, and\ncybersecurity. However, many studies showed that they are vulnerable to\nknowledge corruption attacks, where an attacker can inject adversarial texts\ninto the knowledge database of a RAG system to induce the LLM to generate\nattacker-desired outputs. Existing studies mainly focus on attacking specific\nqueries or queries with similar topics (or keywords). In this work, we propose\nUniC-RAG, a universal knowledge corruption attack against RAG systems. Unlike\nprior work, UniC-RAG jointly optimizes a small number of adversarial texts that\ncan simultaneously attack a large number of user queries with diverse topics\nand domains, enabling an attacker to achieve various malicious objectives, such\nas directing users to malicious websites, triggering harmful command execution,\nor launching denial-of-service attacks. We formulate UniC-RAG as an\noptimization problem and further design an effective solution to solve it,\nincluding a balanced similarity-based clustering method to enhance the attack's\neffectiveness. Our extensive evaluations demonstrate that UniC-RAG is highly\neffective and significantly outperforms baselines. For instance, UniC-RAG could\nachieve over 90% attack success rate by injecting 100 adversarial texts into a\nknowledge database with millions of texts to simultaneously attack a large set\nof user queries (e.g., 2,000). Additionally, we evaluate existing defenses and\nshow that they are insufficient to defend against UniC-RAG, highlighting the\nneed for new defense mechanisms in RAG systems.",
        "url": "http://arxiv.org/abs/2508.18652v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18652v1",
        "arxiv_id": "2508.18652v1",
        "authors": [
            "Runpeng Geng",
            "Yanting Wang",
            "Ying Chen",
            "Jinyuan Jia"
        ],
        "submitted": "2025-08-26 03:50:52",
        "source": "arxiv",
        "comment": "21 pages, 4 figures",
        "score": 7,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to information retrieval, specifically in the context of retrieval-augmented generation (RAG) systems. However, the focus on knowledge corruption attacks and malicious objectives is not directly aligned with the user's research interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is limited to the broader area of information retrieval, but it does not address the user's specific areas of interest."
    },
    {
        "title": "Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models",
        "abstract": "Grounding responses in external knowledge represents an effective strategy\nfor mitigating hallucinations in Large Language Models (LLMs). However, current\nLLMs struggle to seamlessly integrate knowledge while simultaneously\nmaintaining faithfulness (or fidelity) and expressiveness, capabilities that\nhumans naturally possess. This limitation results in outputs that either lack\nsupport from external knowledge, thereby compromising faithfulness, or appear\noverly verbose and unnatural, thus sacrificing expressiveness. In this work, to\nbreak the trade-off between faithfulness and expressiveness, we propose\nCollaborative Decoding (CoDe), a novel approach that dynamically integrates\noutput probabilities generated with and without external knowledge. This\nintegration is guided by distribution divergence and model confidence, enabling\nthe selective activation of relevant and reliable expressions from the model's\ninternal parameters. Furthermore, we introduce a knowledge-aware reranking\nmechanism that prevents over-reliance on prior parametric knowledge while\nensuring proper utilization of provided external information. Through\ncomprehensive experiments, our plug-and-play CoDe framework demonstrates\nsuperior performance in enhancing faithfulness without compromising\nexpressiveness across diverse LLMs and evaluation metrics, validating both its\neffectiveness and generalizability.",
        "url": "http://arxiv.org/abs/2508.18651v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18651v1",
        "arxiv_id": "2508.18651v1",
        "authors": [
            "Chenxu Yang",
            "Qingyi Si",
            "Zheng Lin"
        ],
        "submitted": "2025-08-26 03:48:05",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on Large Language Models, which is not directly related to Information Retrieval or Search technologies. While it touches on the idea of integrating external knowledge, the primary concern is faithfulness and expressiveness, which is more relevant to Natural Language Processing. The paper does not seem to address query understanding, ranking models, or user behavior modeling, making it only loosely relevant to the user's research interests."
    },
    {
        "title": "An Investigation on Group Query Hallucination Attacks",
        "abstract": "With the widespread use of large language models (LLMs), understanding their\npotential failure modes during user interactions is essential. In practice,\nusers often pose multiple questions in a single conversation with LLMs.\nTherefore, in this study, we propose Group Query Attack, a technique that\nsimulates this scenario by presenting groups of queries to LLMs simultaneously.\nWe investigate how the accumulated context from consecutive prompts influences\nthe outputs of LLMs. Specifically, we observe that Group Query Attack\nsignificantly degrades the performance of models fine-tuned on specific tasks.\nMoreover, we demonstrate that Group Query Attack induces a risk of triggering\npotential backdoors of LLMs. Besides, Group Query Attack is also effective in\ntasks involving reasoning, such as mathematical reasoning and code generation\nfor pre-trained and aligned models.",
        "url": "http://arxiv.org/abs/2508.19321v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19321v1",
        "arxiv_id": "2508.19321v1",
        "authors": [
            "Kehao Miao",
            "Xiaolong Jin"
        ],
        "submitted": "2025-08-26 14:30:59",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on investigating potential failure modes of large language models, which is not directly related to information retrieval, search technologies, or query understanding. The topic of group query hallucination attacks is not relevant to the user's research interests in ranking models, user behavior modeling, or deep semantic understanding."
    },
    {
        "title": "Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models",
        "abstract": "The generation of ad headlines plays a vital role in modern advertising,\nwhere both quality and diversity are essential to engage a broad range of\naudience segments. Current approaches primarily optimize language models for\nheadline quality or click-through rates (CTR), often overlooking the need for\ndiversity and resulting in homogeneous outputs. To address this limitation, we\npropose DIVER, a novel framework based on large language models (LLMs) that are\njointly optimized for both diversity and quality. We first design a semantic-\nand stylistic-aware data generation pipeline that automatically produces\nhigh-quality training pairs with ad content and multiple diverse headlines. To\nachieve the goal of generating high-quality and diversified ad headlines within\na single forward pass, we propose a multi-stage multi-objective optimization\nframework with supervised fine-tuning (SFT) and reinforcement learning (RL).\nExperiments on real-world industrial datasets demonstrate that DIVER\neffectively balances quality and diversity. Deployed on a large-scale\ncontent-sharing platform serving hundreds of millions of users, our framework\nimproves advertiser value (ADVV) and CTR by 4.0% and 1.4%.",
        "url": "http://arxiv.org/abs/2508.18739v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18739v1",
        "arxiv_id": "2508.18739v1",
        "authors": [
            "Chang Wang",
            "Siyu Yan",
            "Depeng Yuan",
            "Yuqi Chen",
            "Yanhua Huang",
            "Yuanhang Zheng",
            "Shuhao Li",
            "Yinqi Zhang",
            "Kedi Chen",
            "Mingrui Zhu",
            "Ruiwen Xu"
        ],
        "submitted": "2025-08-26 07:11:44",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'click' (score: +2)",
            "Found 'ctr' (score: +2)",
            "Found 'click-through rate' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on ad headline generation using large language models, optimizing for both quality and diversity. While it touches on relevance and click-through rates, the primary focus is on ad headline generation, which is not directly related to information retrieval, search technologies, or user behavior modeling. The paper's relevance to the user's interests is somewhat limited."
    },
    {
        "title": "Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System",
        "abstract": "Recent advances in speech large language models (SLMs) have improved speech\nrecognition and translation in general domains, but accurately generating\ndomain-specific terms or neologisms remains challenging. To address this, we\npropose Attention2Probability: attention-driven terminology probability\nestimation for robust speech-to-text system, which is lightweight, flexible,\nand accurate. Attention2Probability converts cross-attention weights between\nspeech and terminology into presence probabilities, and it further employs\ncurriculum learning to enhance retrieval accuracy. Furthermore, to tackle the\nlack of data for speech-to-text tasks with terminology intervention, we create\nand release a new speech dataset with terminology to support future research in\nthis area. Experimental results show that Attention2Probability significantly\noutperforms the VectorDB method on our test set. Specifically, its maximum\nrecall rates reach 92.57% for Chinese and 86.83% for English. This high recall\nis achieved with a latency of only 8.71ms per query. Intervening in SLMs'\nrecognition and translation tasks using Attention2Probability-retrieved terms\nimproves terminology accuracy by 6-17%, while revealing that the current\nutilization of terminology by SLMs has limitations.",
        "url": "http://arxiv.org/abs/2508.18701v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18701v1",
        "arxiv_id": "2508.18701v1",
        "authors": [
            "Yanfan Du",
            "Jun Zhang",
            "Bin Wang",
            "Jin Qiu",
            "Lu Huang",
            "Yuan Ge",
            "Xiaoqian Liu",
            "Tong Xiao",
            "Jingbo Zhu"
        ],
        "submitted": "2025-08-26 06:08:17",
        "source": "arxiv",
        "comment": "9 pages, 4 figures, 5 tables",
        "score": 6,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on speech-to-text systems and terminology probability estimation, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions attention mechanisms, it is not applied to ranking models or user behavior modeling, and the paper's scope is limited to speech recognition and translation tasks."
    },
    {
        "title": "Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index",
        "abstract": "This research presents a Retrieval-Augmented Generation (RAG) framework for\nart provenance studies, focusing on the Getty Provenance Index. Provenance\nresearch establishes the ownership history of artworks, which is essential for\nverifying authenticity, supporting restitution and legal claims, and\nunderstanding the cultural and historical context of art objects. The process\nis complicated by fragmented, multilingual archival data that hinders efficient\nretrieval. Current search portals require precise metadata, limiting\nexploratory searches. Our method enables natural-language and multilingual\nsearches through semantic retrieval and contextual summarization, reducing\ndependence on metadata structures. We assess RAG's capability to retrieve and\nsummarize auction records using a 10,000-record sample from the Getty\nProvenance Index - German Sales. The results show this approach provides a\nscalable solution for navigating art market archives, offering a practical tool\nfor historians and cultural heritage professionals conducting historically\nsensitive research.",
        "url": "http://arxiv.org/abs/2508.19093v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19093v1",
        "arxiv_id": "2508.19093v1",
        "authors": [
            "Mathew Henrickson"
        ],
        "submitted": "2025-08-26 14:58:09",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on Retrieval-Augmented Generation for natural language art provenance searches, which is related to information retrieval and search technologies. However, the specific domain of art provenance and the use of the Getty Provenance Index are not directly aligned with the user's interests in e-commerce and general information retrieval. The paper's emphasis on semantic retrieval and contextual summarization is somewhat relevant to the user's interests in query understanding and ranking models, but the application is limited to a specific domain."
    },
    {
        "title": "Beyond the Textual: Generating Coherent Visual Options for MCQs",
        "abstract": "Multiple-choice questions (MCQs) play a crucial role in fostering deep\nthinking and knowledge integration in education. However, previous research has\nprimarily focused on generating MCQs with textual options, but it largely\noverlooks the visual options. Moreover, generating high-quality distractors\nremains a major challenge due to the high cost and limited scalability of\nmanual authoring. To tackle these problems, we propose a Cross-modal Options\nSynthesis (CmOS), a novel framework for generating educational MCQs with visual\noptions. Our framework integrates Multimodal Chain-of-Thought (MCoT) reasoning\nprocess and Retrieval-Augmented Generation (RAG) to produce semantically\nplausible and visually similar answer and distractors. It also includes a\ndiscrimination module to identify content suitable for visual options.\nExperimental results on test tasks demonstrate the superiority of CmOS in\ncontent discrimination, question generation and visual option generation over\nexisting methods across various subjects and educational levels.",
        "url": "http://arxiv.org/abs/2508.18772v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18772v1",
        "arxiv_id": "2508.18772v1",
        "authors": [
            "Wanqiang Wang",
            "Longzhu He",
            "Wei Zheng"
        ],
        "submitted": "2025-08-26 07:55:46",
        "source": "arxiv",
        "comment": "EMNLP 2025",
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on generating educational MCQs with visual options, which is outside the scope of information retrieval and search technologies. While it involves some aspects of multimodal processing, the primary focus is on education and not on query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework",
        "abstract": "Retrieval-augmented generation (RAG) systems improve large language model\noutputs by incorporating external knowledge, enabling more informed and\ncontext-aware responses. However, the effectiveness and trustworthiness of\nthese systems critically depends on how they are evaluated, particularly on\nwhether the evaluation process captures real-world constraints like protecting\nsensitive information. While current evaluation efforts for RAG systems have\nprimarily focused on the development of performance metrics, far less attention\nhas been given to the design and quality of the underlying evaluation datasets,\ndespite their pivotal role in enabling meaningful, reliable assessments. In\nthis work, we introduce a novel multi-agent framework for generating synthetic\nQA datasets for RAG evaluation that prioritize semantic diversity and privacy\npreservation. Our approach involves: (1) a Diversity agent leveraging\nclustering techniques to maximize topical coverage and semantic variability,\n(2) a Privacy Agent that detects and mask sensitive information across multiple\ndomains and (3) a QA curation agent that synthesizes private and diverse QA\npairs suitable as ground truth for RAG evaluation. Extensive experiments\ndemonstrate that our evaluation sets outperform baseline methods in diversity\nand achieve robust privacy masking on domain-specific datasets. This work\noffers a practical and ethically aligned pathway toward safer, more\ncomprehensive RAG system evaluation, laying the foundation for future\nenhancements aligned with evolving AI regulations and compliance standards.",
        "url": "http://arxiv.org/abs/2508.18929v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18929v1",
        "arxiv_id": "2508.18929v1",
        "authors": [
            "Ilias Driouich",
            "Hongliu Cao",
            "Eoin Thomas"
        ],
        "submitted": "2025-08-26 11:16:14",
        "source": "arxiv",
        "comment": "ECAI 2025 TRUST AI workshop",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on the evaluation of Retrieval-Augmented Generation (RAG) systems, which is a related topic to Information Retrieval. However, the specific focus on synthetic dataset generation and privacy preservation is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Chronological Passage Assembling in RAG framework for Temporal Question Answering",
        "abstract": "Long-context question answering over narrative tasks is challenging because\ncorrect answers often hinge on reconstructing a coherent timeline of events\nwhile preserving contextual flow in a limited context window.\nRetrieval-augmented generation (RAG) indexing methods aim to address this\nchallenge by selectively retrieving only necessary document segments. However,\nnarrative texts possess unique characteristics that limit the effectiveness of\nthese existing approaches. Specifically, understanding narrative texts requires\nmore than isolated segments, as the broader context and sequential\nrelationships between segments are crucial for comprehension. To address these\nlimitations, we propose ChronoRAG, a novel RAG framework specialized for\nnarrative texts. This approach focuses on two essential aspects: refining\ndispersed document information into coherent and structured passages, and\npreserving narrative flow by explicitly capturing and maintaining the temporal\norder among retrieved passages. We empirically demonstrate the effectiveness of\nChronoRAG through experiments on the NarrativeQA dataset, showing substantial\nimprovements in tasks requiring both factual identification and comprehension\nof complex sequential relationships, underscoring that reasoning over temporal\norder is crucial in resolving narrative QA.",
        "url": "http://arxiv.org/abs/2508.18748v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18748v1",
        "arxiv_id": "2508.18748v1",
        "authors": [
            "Byeongjeong Kim",
            "Jeonghyun Park",
            "Joonho Yang",
            "Hwanhee Lee"
        ],
        "submitted": "2025-08-26 07:23:23",
        "source": "arxiv",
        "comment": "7 pages, 3 figures",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a novel framework for temporal question answering, focusing on narrative texts. While it uses retrieval-augmented generation (RAG) indexing methods, the primary focus is on reconstructing a coherent timeline of events, which is not directly related to my research interests in query understanding, ranking models, and user behavior modeling in Information Retrieval. The paper's emphasis on temporal order and sequential relationships is somewhat relevant, but it does not align with my core research themes."
    },
    {
        "title": "Taming the One-Epoch Phenomenon in Online Recommendation System by Two-stage Contrastive ID Pre-training",
        "abstract": "ID-based embeddings are widely used in web-scale online recommendation\nsystems. However, their susceptibility to overfitting, particularly due to the\nlong-tail nature of data distributions, often limits training to a single\nepoch, a phenomenon known as the \"one-epoch problem.\" This challenge has driven\nresearch efforts to optimize performance within the first epoch by enhancing\nconvergence speed or feature sparsity. In this study, we introduce a novel\ntwo-stage training strategy that incorporates a pre-training phase using a\nminimal model with contrastive loss, enabling broader data coverage for the\nembedding system. Our offline experiments demonstrate that multi-epoch training\nduring the pre-training phase does not lead to overfitting, and the resulting\nembeddings improve online generalization when fine-tuned for more complex\ndownstream recommendation tasks. We deployed the proposed system in live\ntraffic at Pinterest, achieving significant site-wide engagement gains.",
        "url": "http://arxiv.org/abs/2508.18700v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18700v1",
        "arxiv_id": "2508.18700v1",
        "authors": [
            "Yi-Ping Hsu",
            "Po-Wei Wang",
            "Chantat Eksombatchai",
            "Jiajing Xu"
        ],
        "submitted": "2025-08-26 06:06:21",
        "source": "arxiv",
        "comment": "Published at RecSys'24, see\n  https://dl.acm.org/doi/10.1145/3640457.3688053",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on online recommendation systems, which is related to the recommender systems you occasionally explore. However, the paper's emphasis on ID-based embeddings, contrastive loss, and pre-training for recommendation tasks is not directly aligned with your primary interests in information retrieval, query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Membership Inference Attacks on LLM-based Recommender Systems",
        "abstract": "Large language models (LLMs) based Recommender Systems (RecSys) can flexibly\nadapt recommendation systems to different domains. It utilizes in-context\nlearning (ICL), i.e., the prompts, to customize the recommendation functions,\nwhich include sensitive historical user-specific item interactions, e.g.,\nimplicit feedback like clicked items or explicit product reviews. Such private\ninformation may be exposed to novel privacy attack. However, no study has been\ndone on this important issue. We design four membership inference attacks\n(MIAs), aiming to reveal whether victims' historical interactions have been\nused by system prompts. They are \\emph{direct inquiry, hallucination,\nsimilarity, and poisoning attacks}, each of which utilizes the unique features\nof LLMs or RecSys. We have carefully evaluated them on three LLMs that have\nbeen used to develop ICL-LLM RecSys and two well-known RecSys benchmark\ndatasets. The results confirm that the MIA threat on LLM RecSys is realistic:\ndirect inquiry and poisoning attacks showing significantly high attack\nadvantages. We have also analyzed the factors affecting these attacks, such as\nthe number of shots in system prompts and the position of the victim in the\nshots.",
        "url": "http://arxiv.org/abs/2508.18665v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18665v1",
        "arxiv_id": "2508.18665v1",
        "authors": [
            "Jiajie He",
            "Yuechun Gu",
            "Min-Chun Chen",
            "Keke Chen"
        ],
        "submitted": "2025-08-26 04:14:39",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'click' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'recsys' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on recommender systems and membership inference attacks, which is not directly related to the user's primary research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling. While the paper touches on the use of language models, it does not explore the deep semantic understanding and real-time relevance optimization that the user is interested in."
    },
    {
        "title": "How Reliable are LLMs for Reasoning on the Re-ranking task?",
        "abstract": "With the improving semantic understanding capability of Large Language Models\n(LLMs), they exhibit a greater awareness and alignment with human values, but\nthis comes at the cost of transparency. Although promising results are achieved\nvia experimental analysis, an in-depth understanding of the LLM's internal\nworkings is unavoidable to comprehend the reasoning behind the re-ranking,\nwhich provides end users with an explanation that enables them to make an\ninformed decision. Moreover, in newly developed systems with limited user\nengagement and insufficient ranking data, accurately re-ranking content remains\na significant challenge. While various training methods affect the training of\nLLMs and generate inference, our analysis has found that some training methods\nexhibit better explainability than others, implying that an accurate semantic\nunderstanding has not been learned through all training methods; instead,\nabstract knowledge has been gained to optimize evaluation, which raises\nquestions about the true reliability of LLMs. Therefore, in this work, we\nanalyze how different training methods affect the semantic understanding of the\nre-ranking task in LLMs and investigate whether these models can generate more\ninformed textual reasoning to overcome the challenges of transparency or LLMs\nand limited training data. To analyze the LLMs for re-ranking tasks, we utilize\na relatively small ranking dataset from the environment and the Earth science\ndomain to re-rank retrieved content. Furthermore, we also analyze the\nexplainable information to see if the re-ranking can be reasoned using\nexplainability.",
        "url": "http://arxiv.org/abs/2508.18444v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18444v1",
        "arxiv_id": "2508.18444v1",
        "authors": [
            "Nafis Tanveer Islam",
            "Zhiming Zhao"
        ],
        "submitted": "2025-08-25 19:48:39",
        "source": "arxiv",
        "comment": "Accepted at FQAS Conference 2024. DOI will be provided in 3 weeks\n  after the conference has published the paper",
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores the reliability of Large Language Models (LLMs) for re-ranking tasks, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on LLMs and their internal workings is not directly aligned with the user's primary interest in IR and search technologies. The paper's emphasis on explainability and transparency is also relevant to NLP, but the connection to user behavior modeling and click models is limited."
    },
    {
        "title": "StepWiser: Stepwise Generative Judges for Wiser Reasoning",
        "abstract": "As models increasingly leverage multi-step reasoning strategies to solve\ncomplex problems, supervising the logical validity of these intermediate steps\nhas become a critical research challenge. Process reward models address this by\nproviding step-by-step feedback, but current approaches have two major\ndrawbacks: they typically function as classifiers without providing\nexplanations, and their reliance on supervised fine-tuning with static datasets\nlimits generalization. Inspired by recent advances, we reframe stepwise reward\nmodeling from a classification task to a reasoning task itself. We thus propose\na generative judge that reasons about the policy model's reasoning steps (i.e.,\nmeta-reasons), outputting thinking tokens before delivering a final verdict.\nOur model, StepWiser, is trained by reinforcement learning using relative\noutcomes of rollouts. We show it provides (i) better judgment accuracy on\nintermediate steps than existing methods; (ii) can be used to improve the\npolicy model at training time; and (iii) improves inference-time search.",
        "url": "http://arxiv.org/abs/2508.19229v2",
        "pdf_url": "http://arxiv.org/pdf/2508.19229v2",
        "arxiv_id": "2508.19229v2",
        "authors": [
            "Wei Xiong",
            "Wenting Zhao",
            "Weizhe Yuan",
            "Olga Golovneva",
            "Tong Zhang",
            "Jason Weston",
            "Sainbayar Sukhbaatar"
        ],
        "submitted": "2025-08-26 17:45:05",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on stepwise generative judges for wiser reasoning, which is unrelated to information retrieval, search technologies, or query understanding. The topic is more relevant to artificial intelligence, machine learning, and reasoning, which are outside the user's primary research interests."
    },
    {
        "title": "Beyond the Black Box: Integrating Lexical and Semantic Methods in Quantitative Discourse Analysis with BERTopic",
        "abstract": "Quantitative Discourse Analysis has seen growing adoption with the rise of\nLarge Language Models and computational tools. However, reliance on black box\nsoftware such as MAXQDA and NVivo risks undermining methodological transparency\nand alignment with research goals. This paper presents a hybrid, transparent\nframework for QDA that combines lexical and semantic methods to enable\ntriangulation, reproducibility, and interpretability. Drawing from a case study\nin historical political discourse, we demonstrate how custom Python pipelines\nusing NLTK, spaCy, and Sentence Transformers allow fine-grained control over\npreprocessing, lemmatisation, and embedding generation. We further detail our\niterative BERTopic modelling process, incorporating UMAP dimensionality\nreduction, HDBSCAN clustering, and c-TF-IDF keyword extraction, optimised\nthrough parameter tuning and multiple runs to enhance topic coherence and\ncoverage. By juxtaposing precise lexical searches with context-aware semantic\nclustering, we argue for a multi-layered approach that mitigates the\nlimitations of either method in isolation. Our workflow underscores the\nimportance of code-level transparency, researcher agency, and methodological\ntriangulation in computational discourse studies. Code and supplementary\nmaterials are available via GitHub.",
        "url": "http://arxiv.org/abs/2508.19099v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19099v1",
        "arxiv_id": "2508.19099v1",
        "authors": [
            "Thomas Compton"
        ],
        "submitted": "2025-08-26 15:00:04",
        "source": "arxiv",
        "comment": "5 pages conference paper, 4 tables",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper's focus on Quantitative Discourse Analysis and its integration of lexical and semantic methods shows some relevance to the user's interests in Natural Language Processing and Information Retrieval. However, the specific application and methodology used in the paper are not directly related to the user's core research themes, such as query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Automatic Prompt Optimization with Prompt Distillation",
        "abstract": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which is gaining popularity due to the rapid development of\nprompt engineering driven by extensive research in the field of large language\nmodels (LLMs). This paper presents DistillPrompt -- a novel autoprompting\nmethod based on large language models that employs a multi-stage integration of\ntask-specific information into prompts using training data. DistillPrompt\nutilizes distillation, compression, and aggregation operations to explore the\nprompt space more thoroughly. The method was tested on different datasets for\ntext classification and generation tasks using the t-lite-instruct-0.1 language\nmodel. The results demonstrate a significant average improvement (e.g., 20.12%\nacross the entire dataset compared to Grips) in key metrics over existing\nmethods in the field, establishing DistillPrompt as one of the most effective\nnon-gradient approaches in autoprompting.",
        "url": "http://arxiv.org/abs/2508.18992v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18992v1",
        "arxiv_id": "2508.18992v1",
        "authors": [
            "Viktor N. Zhuravlev",
            "Artur R. Khairullin",
            "Ernest A. Dyagin",
            "Alena N. Sitkina",
            "Nikita I. Kulin"
        ],
        "submitted": "2025-08-26 12:46:58",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on prompt optimization for language models, which is related to information retrieval and search technologies. However, the specific application and methodology are not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat limited to the user's background in e-commerce and NLP, but it does not address the user's primary focus on information retrieval and real-time relevance optimization."
    },
    {
        "title": "The Double-edged Sword of LLM-based Data Reconstruction: Understanding and Mitigating Contextual Vulnerability in Word-level Differential Privacy Text Sanitization",
        "abstract": "Differentially private text sanitization refers to the process of privatizing\ntexts under the framework of Differential Privacy (DP), providing provable\nprivacy guarantees while also empirically defending against adversaries seeking\nto harm privacy. Despite their simplicity, DP text sanitization methods\noperating at the word level exhibit a number of shortcomings, among them the\ntendency to leave contextual clues from the original texts due to randomization\nduring sanitization $\\unicode{x2013}$ this we refer to as $\\textit{contextual\nvulnerability}$. Given the powerful contextual understanding and inference\ncapabilities of Large Language Models (LLMs), we explore to what extent LLMs\ncan be leveraged to exploit the contextual vulnerability of DP-sanitized texts.\nWe expand on previous work not only in the use of advanced LLMs, but also in\ntesting a broader range of sanitization mechanisms at various privacy levels.\nOur experiments uncover a double-edged sword effect of LLM-based data\nreconstruction attacks on privacy and utility: while LLMs can indeed infer\noriginal semantics and sometimes degrade empirical privacy protections, they\ncan also be used for good, to improve the quality and privacy of DP-sanitized\ntexts. Based on our findings, we propose recommendations for using LLM data\nreconstruction as a post-processing step, serving to increase privacy\nprotection by thinking adversarially.",
        "url": "http://arxiv.org/abs/2508.18976v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18976v1",
        "arxiv_id": "2508.18976v1",
        "authors": [
            "Stephen Meisenbacher",
            "Alexandra Klymenko",
            "Andreea-Elena Bodea",
            "Florian Matthes"
        ],
        "submitted": "2025-08-26 12:22:45",
        "source": "arxiv",
        "comment": "15 pages, 4 figures, 8 tables. Accepted to WPES @ CCS 2025",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The topic of differential privacy text sanitization and LLM-based data reconstruction attacks is outside the scope of your primary focus."
    },
    {
        "title": "ReflectivePrompt: Reflective evolution in autoprompting algorithms",
        "abstract": "Autoprompting is the process of automatically selecting optimized prompts for\nlanguage models, which has been gaining popularity with the rapid advancement\nof prompt engineering, driven by extensive research in the field of large\nlanguage models (LLMs). This paper presents ReflectivePrompt - a novel\nautoprompting method based on evolutionary algorithms that employs a reflective\nevolution approach for more precise and comprehensive search of optimal\nprompts. ReflectivePrompt utilizes short-term and long-term reflection\noperations before crossover and elitist mutation to enhance the quality of the\nmodifications they introduce. This method allows for the accumulation of\nknowledge obtained throughout the evolution process and updates it at each\nepoch based on the current population. ReflectivePrompt was tested on 33\ndatasets for classification and text generation tasks using open-access large\nlanguage models: t-lite-instruct-0.1 and gemma3-27b-it. The method\ndemonstrates, on average, a significant improvement (e.g., 28% on BBH compared\nto EvoPrompt) in metrics relative to current state-of-the-art approaches,\nthereby establishing itself as one of the most effective solutions in\nevolutionary algorithm-based autoprompting.",
        "url": "http://arxiv.org/abs/2508.18870v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18870v1",
        "arxiv_id": "2508.18870v1",
        "authors": [
            "Viktor N. Zhuravlev",
            "Artur R. Khairullin",
            "Ernest A. Dyagin",
            "Alena N. Sitkina",
            "Nikita I. Kulin"
        ],
        "submitted": "2025-08-26 09:46:20",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on autoprompting algorithms for language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on evolutionary algorithms, the context is not relevant to the user's primary research interests."
    },
    {
        "title": "Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models",
        "abstract": "With the development of speech large language models (speech LLMs), users can\nnow interact directly with assistants via speech. However, most existing models\nsimply convert the response content into speech without fully understanding the\nrich emotional and paralinguistic cues embedded in the user's query. In many\ncases, the same sentence can have different meanings depending on the emotional\nexpression. Furthermore, emotional understanding is essential for improving\nuser experience in human-machine interaction. Currently, most speech LLMs with\nempathetic capabilities are trained on massive datasets. This approach requires\nvast amounts of data and significant computational resources. Therefore, a key\nchallenge lies in how to develop a speech LLM capable of generating empathetic\nresponses with limited data and without the need for large-scale training. To\naddress this challenge, we propose Emotion Omni, a novel model architecture\ndesigned to understand the emotional content of user speech input and generate\nempathetic speech responses. Additionally, we developed a data generation\npipeline based on an open-source TTS framework to construct a 200k emotional\ndialogue dataset, which supports the construction of an empathetic speech\nassistant. The demos are available at https://w311411.github.io/omni_demo/",
        "url": "http://arxiv.org/abs/2508.18655v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18655v1",
        "arxiv_id": "2508.18655v1",
        "authors": [
            "Haoyu Wang",
            "Guangyan Zhang",
            "Jiale Chen",
            "Jingyu Li",
            "Yuehai Wang",
            "Yiwen Guo"
        ],
        "submitted": "2025-08-26 03:54:39",
        "source": "arxiv",
        "comment": "5 pages, 1 figure, submitted to ICASSP 2026",
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on developing a speech language model that can understand emotional cues in user queries, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the topic of user behavior modeling, it is not specifically related to click models or ranking models. The paper's focus on empathetic speech response generation and emotional understanding is more aligned with natural language processing and human-computer interaction."
    },
    {
        "title": "RLMR: Reinforcement Learning with Mixed Rewards for Creative Writing",
        "abstract": "Large language models are extensively utilized in creative writing\napplications. Creative writing requires a balance between subjective writing\nquality (e.g., literariness and emotional expression) and objective constraint\nfollowing (e.g., format requirements and word limits). Existing reinforcement\nlearning methods struggle to balance these two aspects: single reward\nstrategies fail to improve both abilities simultaneously, while fixed-weight\nmixed-reward methods lack the ability to adapt to different writing scenarios.\nTo address this problem, we propose Reinforcement Learning with Mixed Rewards\n(RLMR), utilizing a dynamically mixed reward system from a writing reward model\nevaluating subjective writing quality and a constraint verification model\nassessing objective constraint following. The constraint following reward\nweight is adjusted dynamically according to the writing quality within sampled\ngroups, ensuring that samples violating constraints get negative advantage in\nGRPO and thus penalized during training, which is the key innovation of this\nproposed method. We conduct automated and manual evaluations across diverse\nmodel families from 8B to 72B parameters. Additionally, we construct a\nreal-world writing benchmark named WriteEval for comprehensive evaluation.\nResults illustrate that our method achieves consistent improvements in both\ninstruction following (IFEval from 83.36\\% to 86.65\\%) and writing quality\n(72.75\\% win rate in manual expert pairwise evaluations on WriteEval). To the\nbest of our knowledge, RLMR is the first work to combine subjective preferences\nwith objective verification in online RL training, providing an effective\nsolution for multi-dimensional creative writing optimization.",
        "url": "http://arxiv.org/abs/2508.18642v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18642v1",
        "arxiv_id": "2508.18642v1",
        "authors": [
            "Jianxing Liao",
            "Tian Zhang",
            "Xiao Feng",
            "Yusong Zhang",
            "Rui Yang",
            "Haorui Wang",
            "Bosi Wen",
            "Ziying Wang",
            "Runzhi Shi"
        ],
        "submitted": "2025-08-26 03:40:06",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on creative writing and reinforcement learning, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves language models, the context is creative writing, which is not a primary area of focus for the user."
    },
    {
        "title": "CAC-CoT: Connector-Aware Compact Chain-of-Thought for Efficient Reasoning Data Synthesis Across Dual-System Cognitive Tasks",
        "abstract": "Long chain-of-thought (CoT) prompting helps Large Language Models (LLMs)\nsolve difficult problems, but very long traces often slow or even degrade\nperformance on fast, intuitive \"System-1\" tasks. We introduce Connector-Aware\nCompact CoT (CAC-CoT) -- a method that deliberately restricts reasoning to a\nsmall, fixed set of connector phrases, steering the model toward concise and\nwell -- structured explanations. Despite its simplicity, our synthetic method\nwith Gemini-2.0-Flash yields a high-quality training quality. CAC-CoT achieves\napproximately 85% on GSM8K and approximately 40% on GPQA (System-2) while\nretaining approximately 90% on S1-Bench (System-1). Its reasoning traces\naverage approximately 300 tokens(ART), about one-third the length of baseline\ntraces, delivering higher efficiency without loss of accuracy.",
        "url": "http://arxiv.org/abs/2508.18743v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18743v1",
        "arxiv_id": "2508.18743v1",
        "authors": [
            "Sunguk Choi",
            "Yonghoon Kwon",
            "Heondeuk Lee"
        ],
        "submitted": "2025-08-26 07:17:21",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025 findings",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on developing a method for efficient reasoning data synthesis in cognitive tasks, using Large Language Models. While it mentions 'reasoning traces' and 'explanations', the context is unrelated to information retrieval, search technologies, or user behavior modeling, which are the core areas of your research interests."
    },
    {
        "title": "Bias Mitigation Agent: Optimizing Source Selection for Fair and Balanced Knowledge Retrieval",
        "abstract": "Large Language Models (LLMs) have transformed the field of artificial\nintelligence by unlocking the era of generative applications. Built on top of\ngenerative AI capabilities, Agentic AI represents a major shift toward\nautonomous, goal-driven systems that can reason, retrieve, and act. However,\nthey also inherit the bias present in both internal and external information\nsources. This significantly affects the fairness and balance of retrieved\ninformation, and hence reduces user trust. To address this critical challenge,\nwe introduce a novel Bias Mitigation Agent, a multi-agent system designed to\norchestrate the workflow of bias mitigation through specialized agents that\noptimize the selection of sources to ensure that the retrieved content is both\nhighly relevant and minimally biased to promote fair and balanced knowledge\ndissemination. The experimental results demonstrate an 81.82\\% reduction in\nbias compared to a baseline naive retrieval strategy.",
        "url": "http://arxiv.org/abs/2508.18724v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18724v1",
        "arxiv_id": "2508.18724v1",
        "authors": [
            "Karanbir Singh",
            "Deepak Muppiri",
            "William Ngu"
        ],
        "submitted": "2025-08-26 06:44:04",
        "source": "arxiv",
        "comment": "Accepted at KDD'2025 Agent4IR workshop",
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores the concept of bias mitigation in knowledge retrieval, which is related to my interests in Information Retrieval and Search technologies. The use of multi-agent systems and optimization techniques is also relevant to my background in query understanding and ranking models. However, the focus on large language models and generative AI applications is not directly aligned with my primary interests in traditional IR and NLP."
    },
    {
        "title": "Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning",
        "abstract": "In high-stakes medical applications, consistent answering across diverse\nquestion phrasings is essential for reliable diagnosis. However, we reveal that\ncurrent Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility\nin Medical Visual Question Answering, as their answers fluctuate significantly\nwhen faced with semantically equivalent rephrasings of medical questions. We\nattribute this to two limitations: (1) insufficient alignment of medical\nconcepts, leading to divergent reasoning patterns, and (2) hidden biases in\ntraining data that prioritize syntactic shortcuts over semantic understanding.\nTo address these challenges, we construct RoMed, a dataset built upon original\nVQA datasets containing 144k questions with variations spanning word-level,\nsentence-level, and semantic-level perturbations. When evaluating\nstate-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming\nperformance drops (e.g., a 40\\% decline in Recall) compared to original VQA\nbenchmarks, exposing critical robustness gaps. To bridge this gap, we propose\nConsistency and Contrastive Learning (CCL), which integrates two key\ncomponents: (1) knowledge-anchored consistency learning, aligning Med-VLMs with\nmedical knowledge rather than shallow feature patterns, and (2) bias-aware\ncontrastive learning, mitigating data-specific priors through discriminative\nrepresentation refinement. CCL achieves SOTA performance on three popular VQA\nbenchmarks and notably improves answer consistency by 50\\% on the challenging\nRoMed test set, demonstrating significantly enhanced robustness. Code will be\nreleased.",
        "url": "http://arxiv.org/abs/2508.18687v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18687v1",
        "arxiv_id": "2508.18687v1",
        "authors": [
            "Songtao Jiang",
            "Yuxi Chen",
            "Sibo Song",
            "Yan Zhang",
            "Yeying Jin",
            "Yang Feng",
            "Jian Wu",
            "Zuozhu Liu"
        ],
        "submitted": "2025-08-26 05:21:19",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on medical visual question answering, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The techniques and datasets used in the paper, such as Consistency and Contrastive Learning, are not applicable to the user's areas of interest."
    },
    {
        "title": "FALCON: Autonomous Cyber Threat Intelligence Mining with LLMs for IDS Rule Generation",
        "abstract": "Signature-based Intrusion Detection Systems (IDS) detect malicious activities\nby matching network or host activity against predefined rules. These rules are\nderived from extensive Cyber Threat Intelligence (CTI), which includes attack\nsignatures and behavioral patterns obtained through automated tools and manual\nthreat analysis, such as sandboxing. The CTI is then transformed into\nactionable rules for the IDS engine, enabling real-time detection and\nprevention. However, the constant evolution of cyber threats necessitates\nfrequent rule updates, which delay deployment time and weaken overall security\nreadiness. Recent advancements in agentic systems powered by Large Language\nModels (LLMs) offer the potential for autonomous IDS rule generation with\ninternal evaluation. We introduce FALCON, an autonomous agentic framework that\ngenerates deployable IDS rules from CTI data in real-time and evaluates them\nusing built-in multi-phased validators. To demonstrate versatility, we target\nboth network (Snort) and host-based (YARA) mediums and construct a\ncomprehensive dataset of IDS rules with their corresponding CTIs. Our\nevaluations indicate FALCON excels in automatic rule generation, with an\naverage of 95% accuracy validated by qualitative evaluation with 84%\ninter-rater agreement among multiple cybersecurity analysts across all metrics.\nThese results underscore the feasibility and effectiveness of LLM-driven data\nmining for real-time cyber threat mitigation.",
        "url": "http://arxiv.org/abs/2508.18684v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18684v1",
        "arxiv_id": "2508.18684v1",
        "authors": [
            "Shaswata Mitra",
            "Azim Bazarov",
            "Martin Duclos",
            "Sudip Mittal",
            "Aritran Piplai",
            "Md Rayhanur Rahman",
            "Edward Zieglar",
            "Shahram Rahimi"
        ],
        "submitted": "2025-08-26 05:08:53",
        "source": "arxiv",
        "comment": "11 pages, 5 figures, 4 tables",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on cyber threat intelligence mining and intrusion detection systems, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions data mining, the context is different from the user's background in e-commerce and focus on deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "Beyond Benchmark: LLMs Evaluation with an Anthropomorphic and Value-oriented Roadmap",
        "abstract": "For Large Language Models (LLMs), a disconnect persists between benchmark\nperformance and real-world utility. Current evaluation frameworks remain\nfragmented, prioritizing technical metrics while neglecting holistic assessment\nfor deployment. This survey introduces an anthropomorphic evaluation paradigm\nthrough the lens of human intelligence, proposing a novel three-dimensional\ntaxonomy: Intelligence Quotient (IQ)-General Intelligence for foundational\ncapacity, Emotional Quotient (EQ)-Alignment Ability for value-based\ninteractions, and Professional Quotient (PQ)-Professional Expertise for\nspecialized proficiency. For practical value, we pioneer a Value-oriented\nEvaluation (VQ) framework assessing economic viability, social impact, ethical\nalignment, and environmental sustainability. Our modular architecture\nintegrates six components with an implementation roadmap. Through analysis of\n200+ benchmarks, we identify key challenges including dynamic assessment needs\nand interpretability gaps. It provides actionable guidance for developing LLMs\nthat are technically proficient, contextually relevant, and ethically sound. We\nmaintain a curated repository of open-source evaluation resources at:\nhttps://github.com/onejune2018/Awesome-LLM-Eval.",
        "url": "http://arxiv.org/abs/2508.18646v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18646v1",
        "arxiv_id": "2508.18646v1",
        "authors": [
            "Jun Wang",
            "Ninglun Gu",
            "Kailai Zhang",
            "Zijiao Zhang",
            "Yelun Bao",
            "Jin Yang",
            "Xu Yin",
            "Liwei Liu",
            "Yihuan Liu",
            "Pengyong Li",
            "Gary G. Yen",
            "Junchi Yan"
        ],
        "submitted": "2025-08-26 03:43:05",
        "source": "arxiv",
        "comment": "Preprint. Under review",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on evaluating Large Language Models (LLMs) using an anthropomorphic and value-oriented approach, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on the importance of real-world utility, the paper's primary concern is on the evaluation of LLMs, which is outside the scope of the user's research interests."
    },
    {
        "title": "A New NMT Model for Translating Clinical Texts from English to Spanish",
        "abstract": "Translating electronic health record (EHR) narratives from English to Spanish\nis a clinically important yet challenging task due to the lack of a\nparallel-aligned corpus and the abundant unknown words contained. To address\nsuch challenges, we propose \\textbf{NOOV} (for No OOV), a new neural machine\ntranslation (NMT) system that requires little in-domain parallel-aligned corpus\nfor training. NOOV integrates a bilingual lexicon automatically learned from\nparallel-aligned corpora and a phrase look-up table extracted from a large\nbiomedical knowledge resource, to alleviate both the unknown word problem and\nthe word-repeat challenge in NMT, enhancing better phrase generation of NMT\nsystems. Evaluation shows that NOOV is able to generate better translation of\nEHR with improvement in both accuracy and fluency.",
        "url": "http://arxiv.org/abs/2508.18607v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18607v1",
        "arxiv_id": "2508.18607v1",
        "authors": [
            "Rumeng Li",
            "Xun Wang",
            "Hong Yu"
        ],
        "submitted": "2025-08-26 02:24:38",
        "source": "arxiv",
        "comment": "This work was accepted by the Machine Learning for Health (ML4H)\n  Workshop at NeurIPS 2018",
        "score": 2,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Neural Machine Translation (NMT) for clinical texts, which is not directly related to Information Retrieval, Search technologies, or query understanding. Although it involves Natural Language Processing (NLP), the topic is more specific to machine translation and does not align with the user's primary research interests."
    },
    {
        "title": "COMET-poly: Machine Translation Metric Grounded in Other Candidates",
        "abstract": "Automated metrics for machine translation attempt to replicate human\njudgment. Unlike humans, who often assess a translation in the context of\nmultiple alternatives, these metrics typically consider only the source\nsentence and a single translation. This discrepancy in the evaluation setup may\nnegatively impact the performance of automated metrics. We propose two\nautomated metrics that incorporate additional information beyond the single\ntranslation. COMET-polycand uses alternative translations of the same source\nsentence to compare and contrast with the translation at hand, thereby\nproviding a more informed assessment of its quality. COMET-polyic, inspired by\nretrieval-based in-context learning, takes in translations of similar source\ntexts along with their human-labeled quality scores to guide the evaluation. We\nfind that including a single additional translation in COMET-polycand improves\nthe segment-level metric performance (0.079 to 0.118 Kendall's tau-b\ncorrelation), with further gains when more translations are added.\nIncorporating retrieved examples in COMET-polyic yields similar improvements\n(0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.",
        "url": "http://arxiv.org/abs/2508.18549v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18549v1",
        "arxiv_id": "2508.18549v1",
        "authors": [
            "Maike Zfle",
            "Vilm Zouhar",
            "Tu Anh Dinh",
            "Felipe Maia Polo",
            "Jan Niehues",
            "Mrinmaya Sachan"
        ],
        "submitted": "2025-08-25 22:55:22",
        "source": "arxiv",
        "comment": "Maike Z\\\"ufle, Vil\\'em Zouhar, and Tu Anh Dinh contributed equally",
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes machine translation metrics that incorporate additional information, such as alternative translations or retrieved examples, to improve their performance. While it touches on the idea of comparing multiple translations, it does not directly relate to query understanding, ranking models, or user behavior modeling in the context of information retrieval. The paper's focus on machine translation and NLP is somewhat relevant to the user's interests, but it does not align with their primary focus on information retrieval and real-time relevance optimization."
    },
    {
        "title": "Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning",
        "abstract": "Probabilistic decoding in Large Language Models (LLMs) often yields\ninconsistent outputs, particularly on complex or long-form questions.\nSelf-Consistency (SC) mitigates this for short-form QA by majority voting over\nexact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram\nConsistency Score (WUCS) extend to long-form responses but lose accuracy on\nshort-form benchmarks.\n  We introduce Latent Self-Consistency (LSC), which selects the most\nsemantically consistent response using learnable token embeddings. A\nlightweight forward generation of summary tokens increases inference time by\nless than 1% and requires no changes to the model architecture.\n  Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU,\nTruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form\nones on average, while maintaining negligible computational overhead. These\nresults position LSC as a practical consistency-selection method that works\nreliably across answer formats. Additionally, LSC provides well-calibrated\nconfidence estimates, maintaining low Expected Calibration Error across both\nanswer formats.",
        "url": "http://arxiv.org/abs/2508.18395v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18395v1",
        "arxiv_id": "2508.18395v1",
        "authors": [
            "Jeong-seok Oh",
            "Jay-yoon Lee"
        ],
        "submitted": "2025-08-25 18:36:28",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning, which is not directly related to Information Retrieval, Search technologies, or query understanding. Although it involves Natural Language Processing, the topic is more specific to question answering and does not align with the user's primary research interests."
    },
    {
        "title": "Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails",
        "abstract": "The pervasiveness of large language models (LLMs) in enterprise settings has\nalso brought forth a significant amount of risks associated with their usage.\nGuardrails technologies aim to mitigate this risk by filtering LLMs'\ninput/output text through various detectors. However, developing and\nmaintaining robust detectors faces many challenges, one of which is the\ndifficulty in acquiring production-quality labeled data on real LLM outputs\nprior to deployment. In this work, we propose backprompting, a simple yet\nintuitive solution to generate production-like labeled data for health advice\nguardrails development. Furthermore, we pair our backprompting method with a\nsparse human-in-the-loop clustering technique to label the generated data. Our\naim is to construct a parallel corpus roughly representative of the original\ndataset yet resembling real LLM output. We then infuse existing datasets with\nour synthetic examples to produce robust training data for our detector. We\ntest our technique in one of the most difficult and nuanced guardrails: the\nidentification of health advice in LLM output, and demonstrate improvement\nversus other solutions. Our detector is able to outperform GPT-4o by up to\n3.73%, despite having 400x less parameters.",
        "url": "http://arxiv.org/abs/2508.18384v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18384v1",
        "arxiv_id": "2508.18384v1",
        "authors": [
            "Kellen Tan Cheng",
            "Anna Lisa Gentile",
            "Chad DeLuca",
            "Guang-Jie Ren"
        ],
        "submitted": "2025-08-25 18:17:00",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on developing a method to generate labeled data for health advice guardrails, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions large language models, the context is different from the user's interests in NLP and IR."
    },
    {
        "title": "Evaluating the Evaluators: Are readability metrics good measures of readability?",
        "abstract": "Plain Language Summarization (PLS) aims to distill complex documents into\naccessible summaries for non-expert audiences. In this paper, we conduct a\nthorough survey of PLS literature, and identify that the current standard\npractice for readability evaluation is to use traditional readability metrics,\nsuch as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility in\nother fields, these metrics have not been compared to human readability\njudgments in PLS. We evaluate 8 readability metrics and show that most\ncorrelate poorly with human judgments, including the most popular metric, FKGL.\nWe then show that Language Models (LMs) are better judges of readability, with\nthe best-performing model achieving a Pearson correlation of 0.56 with human\njudgments. Extending our analysis to PLS datasets, which contain summaries\naimed at non-expert audiences, we find that LMs better capture deeper measures\nof readability, such as required background knowledge, and lead to different\nconclusions than the traditional metrics. Based on these findings, we offer\nrecommendations for best practices in the evaluation of plain language\nsummaries. We release our analysis code and survey data.",
        "url": "http://arxiv.org/abs/2508.19221v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19221v1",
        "arxiv_id": "2508.19221v1",
        "authors": [
            "Isabel Cachola",
            "Daniel Khashabi",
            "Mark Dredze"
        ],
        "submitted": "2025-08-26 17:38:42",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper evaluates readability metrics and language models for plain language summarization, which is a topic in Natural Language Processing (NLP). While it touches on the idea of summarization, which is related to information retrieval, the focus is on readability rather than query understanding, ranking models, or user behavior modeling, making it only loosely relevant to your research interests."
    },
    {
        "title": "The Ramon Llull's Thinking Machine for Automated Ideation",
        "abstract": "This paper revisits Ramon Llull's Ars combinatoria - a medieval framework for\ngenerating knowledge through symbolic recombination - as a conceptual\nfoundation for building a modern Llull's thinking machine for research\nideation. Our approach defines three compositional axes: Theme (e.g.,\nefficiency, adaptivity), Domain (e.g., question answering, machine\ntranslation), and Method (e.g., adversarial training, linear attention). These\nelements represent high-level abstractions common in scientific work -\nmotivations, problem settings, and technical approaches - and serve as building\nblocks for LLM-driven exploration. We mine elements from human experts or\nconference papers and show that prompting LLMs with curated combinations\nproduces research ideas that are diverse, relevant, and grounded in current\nliterature. This modern thinking machine offers a lightweight, interpretable\ntool for augmenting scientific creativity and suggests a path toward\ncollaborative ideation between humans and AI.",
        "url": "http://arxiv.org/abs/2508.19200v1",
        "pdf_url": "http://arxiv.org/pdf/2508.19200v1",
        "arxiv_id": "2508.19200v1",
        "authors": [
            "Xinran Zhao",
            "Boyuan Zheng",
            "Chenglei Si",
            "Haofei Yu",
            "Ken Liu",
            "Runlong Zhou",
            "Ruochen Li",
            "Tong Chen",
            "Xiang Li",
            "Yiming Zhang",
            "Tongshuang Wu"
        ],
        "submitted": "2025-08-26 17:03:43",
        "source": "arxiv",
        "comment": "21 pages, 3 figures",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. While it touches on AI-driven exploration, it is focused on a specific domain (research ideation) and does not align with the user's primary research interests."
    },
    {
        "title": "Empowering Computing Education Researchers Through LLM-Assisted Content Analysis",
        "abstract": "Computing education research (CER) is often instigated by practitioners\nwanting to improve both their own and the wider discipline's teaching practice.\nHowever, the latter is often difficult as many researchers lack the colleagues,\nresources, or capacity to conduct research that is generalisable or rigorous\nenough to advance the discipline. As a result, research methods that enable\nsense-making with larger volumes of qualitative data, while not increasing the\nburden on the researcher, have significant potential within CER.\n  In this discussion paper, we propose such a method for conducting rigorous\nanalysis on large volumes of textual data, namely a variation of LLM-assisted\ncontent analysis (LACA). This method combines content analysis with the use of\nlarge language models, empowering researchers to conduct larger-scale research\nwhich they would otherwise not be able to perform. Using a computing education\ndataset, we illustrate how LACA could be applied in a reproducible and rigorous\nmanner. We believe this method has potential in CER, enabling more\ngeneralisable findings from a wider range of research. This, together with the\ndevelopment of similar methods, can help to advance both the practice and\nresearch quality of the CER discipline.",
        "url": "http://arxiv.org/abs/2508.18872v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18872v1",
        "arxiv_id": "2508.18872v1",
        "authors": [
            "Laurie Gale",
            "Sebastian Mateos Nicolajsen"
        ],
        "submitted": "2025-08-26 09:46:59",
        "source": "arxiv",
        "comment": "7 pages, 2 figures",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper's focus on computing education research and content analysis using large language models is not directly related to the user's interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's methodology and application are also not relevant to the user's areas of interest, such as query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction",
        "abstract": "Grammatical error correction is a significant task in NLP. Traditional\nmethods based on encoder-decoder models have achieved certain success, but the\napplication of LLMs in this field is still underexplored. Current research\npredominantly relies on supervised fine-tuning to train LLMs to directly\ngenerate the corrected sentence, which limits the model's powerful reasoning\nability. To address this limitation, we propose a novel framework based on\nRule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL\nframework achieves \\textbf{state-of-the-art }performance, with a notable\nincrease in \\textbf{recall}. This result clearly highlights the advantages of\nusing RL to steer LLMs, offering a more controllable and reliable paradigm for\nfuture development in GEC.",
        "url": "http://arxiv.org/abs/2508.18780v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18780v1",
        "arxiv_id": "2508.18780v1",
        "authors": [
            "Yilin Li",
            "Xunjian Yin",
            "Yilin Chen",
            "Xiaojun Wan"
        ],
        "submitted": "2025-08-26 08:04:04",
        "source": "arxiv",
        "comment": "Code will be released upon publication",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on grammatical error correction in NLP, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and query understanding. While the paper mentions Reinforcement Learning, it is not applied to query understanding or ranking models, and the user's background in e-commerce is not relevant to this topic."
    },
    {
        "title": "Designing across domains with declarative thinking: Insights from the 96-Eyes ptychographic imager project",
        "abstract": "This article presents a practitioner's reflection on applying declarative,\n5th generation, problem formulation language (5GL) to de novo imaging system\ndesign, informed by experiences across the interdisciplinary research in\nacademia and cross-functional product development within the private sector.\nUsing the 96-Eyes project: 96-camera parallel multi-modal imager for\nhigh-throughput drug discovery as a representative case, I illustrate how\nproject requirements, ranging from hardware constraints to life sciences needs,\ncan be formalized into machine-readable problem statements to preserve\nmission-critical input from diverse domain stakeholders. This declarative\napproach enhances transparency, ensures design traceability, and minimizes\ncostly misalignment across optical, algorithmic, hardware-accelerated compute,\nand life sciences teams.\n  Alongside the technical discussion of 5GL with real-world code examples, I\nreflect on the practical barriers to adopting 5GL in environments where\nimperative, 3rd-generation languages (3GL) remain the default medium for\ninter-team collaboration. Rather than offering an one-size-fits-all solution,\nthese learned lessons highlight how programming paradigms implicitly shapes\nresearch workflows through existing domain hierarchies. The discussion aims to\ninvite further explorations into how declarative problem formulations can\nfacilitate innovation in settings where concurrent R\\&{}D workflows are gaining\ntraction, as opposed to environments where sequential, phase-driven workflows\nremain the norm.",
        "url": "http://arxiv.org/abs/2508.18512v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18512v1",
        "arxiv_id": "2508.18512v1",
        "authors": [
            "Antony C Chan"
        ],
        "submitted": "2025-08-25 21:32:15",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper discusses declarative problem formulation language and its application in a specific domain, which is unrelated to your areas of focus."
    },
    {
        "title": "DenseRec: Revisiting Dense Content Embeddings for Sequential Transformer-based Recommendation",
        "abstract": "Transformer-based sequential recommenders, such as SASRec or BERT4Rec,\ntypically rely solely on learned item ID embeddings, making them vulnerable to\nthe item cold-start problem, particularly in environments with dynamic item\ncatalogs. While dense content embeddings from pre-trained models offer\npotential solutions, direct integration into transformer-based recommenders has\nconsistently underperformed compared to ID-only approaches. We revisit this\nintegration challenge and propose DenseRec, a simple yet effective method that\nintroduces a dual-path embedding approach. DenseRec learns a linear projection\nfrom the dense embedding space into the ID embedding space during training,\nenabling seamless generalization to previously unseen items without requiring\nspecialized embedding models or complex infrastructure. In experiments on three\nreal-world datasets, we find DenseRec to consistently outperform an ID-only\nSASRec baseline, even without additional hyperparameter tuning and while using\ncompact embedding models. Our analysis suggests improvements primarily arise\nfrom better sequence representations in the presence of unseen items,\npositioning DenseRec as a practical and robust solution for cold-start\nsequential recommendation.",
        "url": "http://arxiv.org/abs/2508.18442v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18442v1",
        "arxiv_id": "2508.18442v1",
        "authors": [
            "Jan Malte Lichtenberg",
            "Antonio De Candia",
            "Matteo Ruffini"
        ],
        "submitted": "2025-08-25 19:47:20",
        "source": "arxiv",
        "comment": "EARL workshop @RecSys'25, Prague, Czech Republic",
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on recommender systems, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the specific topic of sequential recommendation and dense content embeddings is not directly aligned with your core research themes, such as query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering",
        "abstract": "A majority of recent work in AI assesses models' generalization capabilities\nthrough the lens of performance on out-of-distribution (OOD) datasets. Despite\ntheir practicality, such evaluations build upon a strong assumption: that OOD\nevaluations can capture and reflect upon possible failures in a real-world\ndeployment.\n  In this work, we challenge this assumption and confront the results obtained\nfrom OOD evaluations with a set of specific failure modes documented in\nexisting question-answering (QA) models, referred to as a reliance on spurious\nfeatures or prediction shortcuts.\n  We find that different datasets used for OOD evaluations in QA provide an\nestimate of models' robustness to shortcuts that have a vastly different\nquality, some largely under-performing even a simple, in-distribution\nevaluation. We partially attribute this to the observation that spurious\nshortcuts are shared across ID+OOD datasets, but also find cases where a\ndataset's quality for training and evaluation is largely disconnected. Our work\nunderlines limitations of commonly-used OOD-based evaluations of\ngeneralization, and provides methodology and recommendations for evaluating\ngeneralization within and beyond QA more robustly.",
        "url": "http://arxiv.org/abs/2508.18407v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18407v1",
        "arxiv_id": "2508.18407v1",
        "authors": [
            "Michal tefnik",
            "Timothee Mickus",
            "Marek Kadlk",
            "Michal Spiegel",
            "Josef Kucha"
        ],
        "submitted": "2025-08-25 18:49:50",
        "source": "arxiv",
        "comment": "To appear in Findings of EMNLP 2025",
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus is on question answering and evaluating generalization capabilities of AI models, which is a topic in Natural Language Processing, but not closely aligned with the user's primary research interests."
    },
    {
        "title": "Integral Transformer: Denoising Attention, Not Too Much Not Too Little",
        "abstract": "Softmax self-attention often assigns disproportionate weight to semantically\nuninformative tokens such as special tokens and punctuation, a phenomenon known\nas attention noise. While recent methods like Cog Attention and the\nDifferential Transformer have addressed this by introducing negative attention\nscores, they risk discarding useful information. In this paper, we propose the\nIntegral Transformer, a novel self-attention mechanism that denoises attention\nby integrating signals sampled from the logit distribution. Our approach\nmitigates noise while preserving the contributions of special tokens critical\nfor model performance. Extensive experiments demonstrate that our model\noutperforms vanilla, Cog, and Differential attention variants on\nwell-established knowledge and reasoning language benchmarks. Moreover, our\nanalysis reveals that employing vanilla self-attention in the lower Transformer\nlayers enhances performance and that the Integral Transformer effectively\nbalances attention distributions and reduces rank collapse in upper layers.",
        "url": "http://arxiv.org/abs/2508.18387v1",
        "pdf_url": "http://arxiv.org/pdf/2508.18387v1",
        "arxiv_id": "2508.18387v1",
        "authors": [
            "Ivan Kobyzev",
            "Abbas Ghaddar",
            "Dingtao Hu",
            "Boxing Chen"
        ],
        "submitted": "2025-08-25 18:19:21",
        "source": "arxiv",
        "comment": "EMNLP 2025 Main",
        "score": 1,
        "keyword_reasons": [
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a novel self-attention mechanism, the Integral Transformer, which addresses attention noise in Transformer models. While it's related to NLP and Transformer architectures, it doesn't directly address query understanding, ranking models, or user behavior modeling, which are core interests in Information Retrieval and Search technologies."
    }
]