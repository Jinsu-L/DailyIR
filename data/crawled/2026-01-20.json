[
    {
        "title": "OI-Bench: An Option Injection Benchmark for Evaluating LLM Susceptibility to Directive Interference",
        "abstract": "Benchmarking large language models (LLMs) is critical for understanding their capabilities, limitations, and robustness. In addition to interface artifacts, prior studies have shown that LLM decisions can be influenced by directive signals such as social cues, framing, and instructions. In this work, we introduce option injection, a benchmarking approach that augments the multiple-choice question answering (MCQA) interface with an additional option containing a misleading directive, leveraging standardized choice structure and scalable evaluation. We construct OI-Bench, a benchmark of 3,000 questions spanning knowledge, reasoning, and commonsense tasks, with 16 directive types covering social compliance, bonus framing, threat framing, and instructional interference. This setting combines manipulation of the choice interface with directive-based interference, enabling systematic assessment of model susceptibility. We evaluate 12 LLMs to analyze attack success rates, behavioral responses, and further investigate mitigation strategies ranging from inference-time prompting to post-training alignment. Experimental results reveal substantial vulnerabilities and heterogeneous robustness across models. OI-Bench is expected to support more systematic evaluation of LLM robustness to directive interference within choice-based interfaces.",
        "url": "http://arxiv.org/abs/2601.13300v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13300v1",
        "arxiv_id": "2601.13300v1",
        "authors": [
            "Yow-Fu Liou",
            "Yu-Chien Tang",
            "Yu-Hsiang Liu",
            "An-Zi Yen"
        ],
        "submitted": "2026-01-19 18:56:08",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CooperBench: Why Coding Agents Cannot be Your Teammates Yet",
        "abstract": "Resolving team conflicts requires not only task-specific competence, but also social intelligence to find common ground and build consensus. As AI agents increasingly collaborate on complex work, they must develop coordination capabilities to function as effective teammates. Yet we hypothesize that current agents lack these capabilities. To test this, we introduce CooperBench, a benchmark of over 600 collaborative coding tasks across 12 libraries in 4 programming languages. Each task assigns two agents different features that can be implemented independently but may conflict without proper coordination. Tasks are grounded in real open-source repositories with expert-written tests. Evaluating state-of-the-art coding agents, we observe the curse of coordination: agents achieve on average 30% lower success rates when working together compared to performing both tasks individually. This contrasts sharply with human teams, where adding teammates typically improves productivity. Our analysis reveals three key issues: (1) communication channels become jammed with vague, ill-timed, and inaccurate messages; (2) even with effective communication, agents deviate from their commitments; and (3) agents often hold incorrect expectations about others' plans and communication. Through large-scale simulation, we also observe rare but interesting emergent coordination behavior including role division, resource division, and negotiation. Our research presents a novel benchmark for collaborative coding and calls for a shift from pursuing individual agent capability to developing social intelligence.",
        "url": "http://arxiv.org/abs/2601.13295v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13295v1",
        "arxiv_id": "2601.13295v1",
        "authors": [
            "Arpandeep Khatua",
            "Hao Zhu",
            "Peter Tran",
            "Arya Prabhudesai",
            "Frederic Sadrieh",
            "Johann K. Lieberwirth",
            "Xinkai Yu",
            "Yicheng Fu",
            "Michael J. Ryan",
            "Jiaxin Pei",
            "Diyi Yang"
        ],
        "submitted": "2026-01-19 18:48:37",
        "source": "arxiv",
        "comment": "https://cooperbench.com"
    },
    {
        "title": "A BERTology View of LLM Orchestrations: Token- and Layer-Selective Probes for Efficient Single-Pass Classification",
        "abstract": "Production LLM systems often rely on separate models for safety and other classification-heavy steps, increasing latency, VRAM footprint, and operational complexity. We instead reuse computation already paid for by the serving LLM: we train lightweight probes on its hidden states and predict labels in the same forward pass used for generation. We frame classification as representation selection over the full token-layer hidden-state tensor, rather than committing to a fixed token or fixed layer (e.g., first-token logits or final-layer pooling). To implement this, we introduce a two-stage aggregator that (i) summarizes tokens within each layer and (ii) aggregates across layer summaries to form a single representation for classification. We instantiate this template with direct pooling, a 100K-parameter scoring-attention gate, and a downcast multi-head self-attention (MHA) probe with up to 35M trainable parameters. Across safety and sentiment benchmarks our probes improve over logit-only reuse (e.g., MULI) and are competitive with substantially larger task-specific baselines, while preserving near-serving latency and avoiding the VRAM and latency costs of a separate guard-model pipeline.",
        "url": "http://arxiv.org/abs/2601.13288v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13288v1",
        "arxiv_id": "2601.13288v1",
        "authors": [
            "Gonzalo Ariel Meyoyan",
            "Luciano Del Corro"
        ],
        "submitted": "2026-01-19 18:40:29",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Unlearning in LLMs: Methods, Evaluation, and Open Challenges",
        "abstract": "Large language models (LLMs) have achieved remarkable success across natural language processing tasks, yet their widespread deployment raises pressing concerns around privacy, copyright, security, and bias. Machine unlearning has emerged as a promising paradigm for selectively removing knowledge or data from trained models without full retraining. In this survey, we provide a structured overview of unlearning methods for LLMs, categorizing existing approaches into data-centric, parameter-centric, architecture-centric, hybrid, and other strategies. We also review the evaluation ecosystem, including benchmarks, metrics, and datasets designed to measure forgetting effectiveness, knowledge retention, and robustness. Finally, we outline key challenges and open problems, such as scalable efficiency, formal guarantees, cross-language and multimodal unlearning, and robustness against adversarial relearning. By synthesizing current progress and highlighting open directions, this paper aims to serve as a roadmap for developing reliable and responsible unlearning techniques in large language models.",
        "url": "http://arxiv.org/abs/2601.13264v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13264v1",
        "arxiv_id": "2601.13264v1",
        "authors": [
            "Tyler Lizzo",
            "Larry Heck"
        ],
        "submitted": "2026-01-19 17:58:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CURE-Med: Curriculum-Informed Reinforcement Learning for Multilingual Medical Reasoning",
        "abstract": "While large language models (LLMs) have shown to perform well on monolingual mathematical and commonsense reasoning, they remain unreliable for multilingual medical reasoning applications, hindering their deployment in multilingual healthcare settings. We address this by first introducing CUREMED-BENCH, a high-quality multilingual medical reasoning dataset with open-ended reasoning queries with a single verifiable answer, spanning thirteen languages, including underrepresented languages such as Amharic, Yoruba, and Swahili. Building on this dataset, we propose CURE-MED, a curriculum-informed reinforcement learning framework that integrates code-switching-aware supervised fine-tuning and Group Relative Policy Optimization to jointly improve logical correctness and language stability. Across thirteen languages, our approach consistently outperforms strong baselines and scales effectively, achieving 85.21% language consistency and 54.35% logical correctness at 7B parameters, and 94.96% language consistency and 70.04% logical correctness at 32B parameters. These results support reliable and equitable multilingual medical reasoning in LLMs. The code and dataset are available at https://cure-med.github.io/",
        "url": "http://arxiv.org/abs/2601.13262v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13262v1",
        "arxiv_id": "2601.13262v1",
        "authors": [
            "Eric Onyame",
            "Akash Ghosh",
            "Subhadip Baidya",
            "Sriparna Saha",
            "Xiuying Chen",
            "Chirag Agarwal"
        ],
        "submitted": "2026-01-19 17:51:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Stop Taking Tokenizers for Granted: They Are Core Design Decisions in Large Language Models",
        "abstract": "Tokenization underlies every large language model, yet it remains an under-theorized and inconsistently designed component. Common subword approaches such as Byte Pair Encoding (BPE) offer scalability but often misalign with linguistic structure, amplify bias, and waste capacity across languages and domains. This paper reframes tokenization as a core modeling decision rather than a preprocessing step. We argue for a context-aware framework that integrates tokenizer and model co-design, guided by linguistic, domain, and deployment considerations. Standardized evaluation and transparent reporting are essential to make tokenization choices accountable and comparable. Treating tokenization as a core design problem, not a technical afterthought, can yield language technologies that are fairer, more efficient, and more adaptable.",
        "url": "http://arxiv.org/abs/2601.13260v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13260v1",
        "arxiv_id": "2601.13260v1",
        "authors": [
            "Sawsan Alqahtani",
            "Mir Tafseer Nayeem",
            "Md Tahmid Rahman Laskar",
            "Tasnim Mohiuddin",
            "M Saiful Bari"
        ],
        "submitted": "2026-01-19 17:50:36",
        "source": "arxiv",
        "comment": "Accepted to EACL 2026 (long, main). The first two authors contributed equally"
    },
    {
        "title": "A Hybrid Protocol for Large-Scale Semantic Dataset Generation in Low-Resource Languages: The Turkish Semantic Relations Corpus",
        "abstract": "We present a hybrid methodology for generating large-scale semantic relationship datasets in low-resource languages, demonstrated through a comprehensive Turkish semantic relations corpus. Our approach integrates three phases: (1) FastText embeddings with Agglomerative Clustering to identify semantic clusters, (2) Gemini 2.5-Flash for automated semantic relationship classification, and (3) integration with curated dictionary sources. The resulting dataset comprises 843,000 unique Turkish semantic pairs across three relationship types (synonyms, antonyms, co-hyponyms) representing a 10x scale increase over existing resources at minimal cost ($65). We validate the dataset through two downstream tasks: an embedding model achieving 90% top-1 retrieval accuracy and a classification model attaining 90% F1-macro. Our scalable protocol addresses critical data scarcity in Turkish NLP and demonstrates applicability to other low-resource languages. We publicly release the dataset and models.",
        "url": "http://arxiv.org/abs/2601.13253v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13253v1",
        "arxiv_id": "2601.13253v1",
        "authors": [
            "Ebubekir Tosun",
            "Mehmet Emin Buldur",
            "Özay Ezerceli",
            "Mahmoud ElHussieni"
        ],
        "submitted": "2026-01-19 17:38:52",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond Cosine Similarity: Taming Semantic Drift and Antonym Intrusion in a 15-Million Node Turkish Synonym Graph",
        "abstract": "Neural embeddings have a notorious blind spot: they can't reliably tell synonyms apart from antonyms. Consequently, increasing similarity thresholds often fails to prevent opposites from being grouped together. We've built a large-scale semantic clustering system specifically designed to tackle this problem head on. Our pipeline chews through 15 million lexical items, evaluates a massive 520 million potential relationships, and ultimately generates 2.9 million high-precision semantic clusters. The system makes three primary contributions. First, we introduce a labeled dataset of 843,000 concept pairs spanning synonymy, antonymy, and co-hyponymy, constructed via Gemini 2.5-Flash LLM augmentation and verified using human-curated dictionary resources. Second, we propose a specialized three-way semantic relation discriminator that achieves 90% macro-F1, enabling robust disambiguation beyond raw embedding similarity. Third, we introduce a novel soft-to-hard clustering algorithm that mitigates semantic drift preventing erroneous transitive chains (e.g., hot -> spicy -> pain -> depression) while simultaneously resolving polysemy. Our approach employs a topology-aware two-stage expansion-pruning procedure with topological voting, ensuring that each term is assigned to exactly one semantically coherent cluster. The resulting resource enables high-precision semantic search and retrieval-augmented generation, particularly for morphologically rich and low-resource languages where existing synonym databases remain sparse.",
        "url": "http://arxiv.org/abs/2601.13251v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13251v1",
        "arxiv_id": "2601.13251v1",
        "authors": [
            "Ebubekir Tosun",
            "Mehmet Emin Buldur",
            "Özay Ezerceli",
            "Mahmoud ElHussieni"
        ],
        "submitted": "2026-01-19 17:37:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Aligning Agentic World Models via Knowledgeable Experience Learning",
        "abstract": "Current Large Language Models (LLMs) exhibit a critical modal disconnect: they possess vast semantic knowledge but lack the procedural grounding to respect the immutable laws of the physical world. Consequently, while these agents implicitly function as world models, their simulations often suffer from physical hallucinations-generating plans that are logically sound but physically unexecutable. Existing alignment strategies predominantly rely on resource-intensive training or fine-tuning, which attempt to compress dynamic environmental rules into static model parameters. However, such parametric encapsulation is inherently rigid, struggling to adapt to the open-ended variability of physical dynamics without continuous, costly retraining. To bridge this gap, we introduce WorldMind, a framework that autonomously constructs a symbolic World Knowledge Repository by synthesizing environmental feedback. Specifically, it unifies Process Experience to enforce physical feasibility via prediction errors and Goal Experience to guide task optimality through successful trajectories. Experiments on EB-ALFRED and EB-Habitat demonstrate that WorldMind achieves superior performance compared to baselines with remarkable cross-model and cross-environment transferability.",
        "url": "http://arxiv.org/abs/2601.13247v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13247v1",
        "arxiv_id": "2601.13247v1",
        "authors": [
            "Baochang Ren",
            "Yunzhi Yao",
            "Rui Sun",
            "Shuofei Qiao",
            "Ningyu Zhang",
            "Huajun Chen"
        ],
        "submitted": "2026-01-19 17:33:31",
        "source": "arxiv",
        "comment": "Ongoing work"
    },
    {
        "title": "KOCO-BENCH: Can Large Language Models Leverage Domain Knowledge in Software Development?",
        "abstract": "Large language models (LLMs) excel at general programming but struggle with domain-specific software development, necessitating domain specialization methods for LLMs to learn and utilize domain knowledge and data. However, existing domain-specific code benchmarks cannot evaluate the effectiveness of domain specialization methods, which focus on assessing what knowledge LLMs possess rather than how they acquire and apply new knowledge, lacking explicit knowledge corpora for developing domain specialization methods. To this end, we present KOCO-BENCH, a novel benchmark designed for evaluating domain specialization methods in real-world software development. KOCO-BENCH contains 6 emerging domains with 11 software frameworks and 25 projects, featuring curated knowledge corpora alongside multi-granularity evaluation tasks including domain code generation (from function-level to project-level with rigorous test suites) and domain knowledge understanding (via multiple-choice Q&A). Unlike previous benchmarks that only provide test sets for direct evaluation, KOCO-BENCH requires acquiring and applying diverse domain knowledge (APIs, rules, constraints, etc.) from knowledge corpora to solve evaluation tasks. Our evaluations reveal that KOCO-BENCH poses significant challenges to state-of-the-art LLMs. Even with domain specialization methods (e.g., SFT, RAG, kNN-LM) applied, improvements remain marginal. Best-performing coding agent, Claude Code, achieves only 34.2%, highlighting the urgent need for more effective domain specialization methods. We release KOCO-BENCH, evaluation code, and baselines to advance further research at https://github.com/jiangxxxue/KOCO-bench.",
        "url": "http://arxiv.org/abs/2601.13240v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13240v1",
        "arxiv_id": "2601.13240v1",
        "authors": [
            "Xue Jiang",
            "Jiaru Qian",
            "Xianjie Shi",
            "Chenjie Li",
            "Hao Zhu",
            "Ziyu Wang",
            "Jielun Zhang",
            "Zheyu Zhao",
            "Kechi Zhang",
            "Jia Li",
            "Wenpin Jiao",
            "Zhi Jin",
            "Ge Li",
            "Yihong Dong"
        ],
        "submitted": "2026-01-19 17:20:16",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "RubRIX: Rubric-Driven Risk Mitigation in Caregiver-AI Interactions",
        "abstract": "Caregivers seeking AI-mediated support express complex needs -- information-seeking, emotional validation, and distress cues -- that warrant careful evaluation of response safety and appropriateness. Existing AI evaluation frameworks, primarily focused on general risks (toxicity, hallucinations, policy violations, etc), may not adequately capture the nuanced risks of LLM-responses in caregiving-contexts. We introduce RubRIX (Rubric-based Risk Index), a theory-driven, clinician-validated framework for evaluating risks in LLM caregiving responses. Grounded in the Elements of an Ethic of Care, RubRIX operationalizes five empirically-derived risk dimensions: Inattention, Bias & Stigma, Information Inaccuracy, Uncritical Affirmation, and Epistemic Arrogance. We evaluate six state-of-the-art LLMs on over 20,000 caregiver queries from Reddit and ALZConnected. Rubric-guided refinement consistently reduced risk-components by 45-98% after one iteration across models. This work contributes a methodological approach for developing domain-sensitive, user-centered evaluation frameworks for high-burden contexts. Our findings highlight the importance of domain-sensitive, interactional risk evaluation for the responsible deployment of LLMs in caregiving support contexts. We release benchmark datasets to enable future research on contextual risk evaluation in AI-mediated support.",
        "url": "http://arxiv.org/abs/2601.13235v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13235v1",
        "arxiv_id": "2601.13235v1",
        "authors": [
            "Drishti Goel",
            "Jeongah Lee",
            "Qiuyue Joy Zhong",
            "Violeta J. Rodriguez",
            "Daniel S. Brown",
            "Ravi Karkar",
            "Dong Whi Yoo",
            "Koustuv Saha"
        ],
        "submitted": "2026-01-19 17:10:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Autoregressive Models Rival Diffusion Models at ANY-ORDER Generation",
        "abstract": "Diffusion language models enable any-order generation and bidirectional conditioning, offering appealing flexibility for tasks such as infilling, rewriting, and self-correction. However, their formulation-predicting one part of a sequence from another within a single-step dependency-limits modeling depth and often yields lower sample quality and stability than autoregressive (AR) models. To address this, we revisit autoregressive modeling as a foundation and reformulate diffusion-style training into a structured multi-group prediction process. We propose Any-order Any-subset Autoregressive modeling (A3), a generalized framework that extends the standard AR factorization to arbitrary token groups and generation orders. A3 preserves the probabilistic rigor and multi-layer dependency modeling of AR while inheriting diffusion models' flexibility for parallel and bidirectional generation. We implement A3 through a two-stream attention architecture and a progressive adaptation strategy that transitions pretrained AR models toward any-order prediction. Experiments on question answering, commonsense reasoning, and story infilling demonstrate that A3 outperforms diffusion-based models while maintaining flexible decoding. This work offers a unified approach for a flexible, efficient, and novel language modeling paradigm.",
        "url": "http://arxiv.org/abs/2601.13228v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13228v1",
        "arxiv_id": "2601.13228v1",
        "authors": [
            "Tianqi Du",
            "Lizhe Fang",
            "Weijie Yang",
            "Chenheng Zhang",
            "Zeming Wei",
            "Yifei Wang",
            "Yisen Wang"
        ],
        "submitted": "2026-01-19 17:03:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Insider Knowledge: How Much Can RAG Systems Gain from Evaluation Secrets?",
        "abstract": "RAG systems are increasingly evaluated and optimized using LLM judges, an approach that is rapidly becoming the dominant paradigm for system assessment. Nugget-based approaches in particular are now embedded not only in evaluation frameworks but also in the architectures of RAG systems themselves. While this integration can lead to genuine improvements, it also creates a risk of faulty measurements due to circularity. In this paper, we investigate this risk through comparative experiments with nugget-based RAG systems, including Ginger and Crucible, against strong baselines such as GPT-Researcher. By deliberately modifying Crucible to generate outputs optimized for an LLM judge, we show that near-perfect evaluation scores can be achieved when elements of the evaluation - such as prompt templates or gold nuggets - are leaked or can be predicted. Our results highlight the importance of blind evaluation settings and methodological diversity to guard against mistaking metric overfitting for genuine system progress.",
        "url": "http://arxiv.org/abs/2601.13227v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13227v1",
        "arxiv_id": "2601.13227v1",
        "authors": [
            "Laura Dietz",
            "Bryan Li",
            "Eugene Yang",
            "Dawn Lawrie",
            "William Walden",
            "James Mayfield"
        ],
        "submitted": "2026-01-19 17:03:20",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Incorporating Q&A Nuggets into Retrieval-Augmented Generation",
        "abstract": "RAGE systems integrate ideas from automatic evaluation (E) into Retrieval-augmented Generation (RAG). As one such example, we present Crucible, a Nugget-Augmented Generation System that preserves explicit citation provenance by constructing a bank of Q&A nuggets from retrieved documents and uses them to guide extraction, selection, and report generation. Reasoning on nuggets avoids repeated information through clear and interpretable Q&A semantics - instead of opaque cluster abstractions - while maintaining citation provenance throughout the entire generation process. Evaluated on the TREC NeuCLIR 2024 collection, our Crucible system substantially outperforms Ginger, a recent nugget-based RAG system, in nugget recall, density, and citation grounding.",
        "url": "http://arxiv.org/abs/2601.13222v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13222v1",
        "arxiv_id": "2601.13222v1",
        "authors": [
            "Laura Dietz",
            "Bryan Li",
            "Gabrielle Liu",
            "Jia-Huei Ju",
            "Eugene Yang",
            "Dawn Lawrie",
            "William Walden",
            "James Mayfield"
        ],
        "submitted": "2026-01-19 16:57:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond Single-shot Writing: Deep Research Agents are Unreliable at Multi-turn Report Revision",
        "abstract": "Existing benchmarks for Deep Research Agents (DRAs) treat report generation as a single-shot writing task, which fundamentally diverges from how human researchers iteratively draft and revise reports via self-reflection or peer feedback. Whether DRAs can reliably revise reports with user feedback remains unexplored. We introduce Mr Dre, an evaluation suite that establishes multi-turn report revision as a new evaluation axis for DRAs. Mr Dre consists of (1) a unified long-form report evaluation protocol spanning comprehensiveness, factuality, and presentation, and (2) a human-verified feedback simulation pipeline for multi-turn revision. Our analysis of five diverse DRAs reveals a critical limitation: while agents can address most user feedback, they also regress on 16-27% of previously covered content and citation quality. Over multiple revision turns, even the best-performing agents leave significant headroom, as they continue to disrupt content outside the feedback's scope and fail to preserve earlier edits. We further show that these issues are not easily resolvable through inference-time fixes such as prompt engineering and a dedicated sub-agent for report revision.",
        "url": "http://arxiv.org/abs/2601.13217v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13217v1",
        "arxiv_id": "2601.13217v1",
        "authors": [
            "Bingsen Chen",
            "Boyan Li",
            "Ping Nie",
            "Yuyu Zhang",
            "Xi Ye",
            "Chen Zhao"
        ],
        "submitted": "2026-01-19 16:48:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "OpenExempt: A Diagnostic Benchmark for Legal Reasoning and a Framework for Creating Custom Benchmarks on Demand",
        "abstract": "Reasoning benchmarks have played a crucial role in the progress of language models. Yet rigorous evaluation remains a significant challenge as static question-answer pairs provide only a snapshot of performance, compressing complex behavior into a single accuracy metric. This limitation is especially true in complex, rule-bound domains such as law, where existing benchmarks are costly to build and ill suited for isolating specific failure modes. To address this, we introduce OpenExempt, a framework and benchmark for diagnostic evaluation of legal reasoning. The OpenExempt Framework uses expert-crafted symbolic representations of U.S. Bankruptcy Code statutes to dynamically generate a large space of natural language reasoning tasks and their machine-computable solutions on demand. This gives users fine-grained control over task complexity and scope, allowing individual reasoning skills to be probed in isolation. Using this system, we construct the OpenExempt Benchmark, a diagnostic benchmark for legal reasoning with 9,765 samples across nine evaluation suites designed to carefully probe model capabilities. Experiments on 13 diverse language models reveal sharp performance cliffs that emerge only under longer reasoning paths and in the presence of obfuscating statements. We release the framework and benchmark publicly to support research aimed at understanding and improving the next generation of reasoning systems.",
        "url": "http://arxiv.org/abs/2601.13183v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13183v1",
        "arxiv_id": "2601.13183v1",
        "authors": [
            "Sergio Servantez",
            "Sarah B. Lawsky",
            "Rajiv Jain",
            "Daniel W. Linna",
            "Kristian Hammond"
        ],
        "submitted": "2026-01-19 16:07:47",
        "source": "arxiv",
        "comment": "25 pages, 9 Figures, 15 tables"
    },
    {
        "title": "Medical Triage as Pairwise Ranking: A Benchmark for Urgency in Patient Portal Messages",
        "abstract": "Medical triage is the task of allocating medical resources and prioritizing patients based on medical need. This paper introduces the first large-scale public dataset for studying medical triage in the context of asynchronous outpatient portal messages. Our novel task formulation views patient message triage as a pairwise inference problem, where we train LLMs to choose `\"which message is more medically urgent\" in a head-to-head tournament-style re-sort of a physician's inbox. Our novel benchmark PMR-Bench contains 1569 unique messages and 2,000+ high-quality test pairs for pairwise medical urgency assessment alongside a scalable training data generation pipeline. PMR-Bench includes samples that contain both unstructured patient-written messages alongside real electronic health record (EHR) data, emulating a real-world medical triage scenario.\n  We develop a novel automated data annotation strategy to provide LLMs with in-domain guidance on this task. The resulting data is used to train two model classes, UrgentReward and UrgentSFT, leveraging Bradley-Terry and next token prediction objective, respectively to perform pairwise urgency classification. We find that UrgentSFT achieves top performance on PMR-Bench, with UrgentReward showing distinct advantages in low-resource settings. For example, UrgentSFT-8B and UrgentReward-8B provide a 15- and 16-point boost, respectively, on inbox sorting metrics over off-the-shelf 8B models. Paper resources can be found at https://tinyurl.com/Patient-Message-Triage",
        "url": "http://arxiv.org/abs/2601.13178v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13178v1",
        "arxiv_id": "2601.13178v1",
        "authors": [
            "Joseph Gatto",
            "Parker Seegmiller",
            "Timothy Burdick",
            "Philip Resnik",
            "Roshnik Rahat",
            "Sarah DeLozier",
            "Sarah M. Preum"
        ],
        "submitted": "2026-01-19 16:05:31",
        "source": "arxiv",
        "comment": "19 Pages, 5 Figures"
    },
    {
        "title": "Probe and Skip: Self-Predictive Token Skipping for Efficient Long-Context LLM Inference",
        "abstract": "Long-context inference enhances the reasoning capability of Large Language Models (LLMs) while incurring significant computational overhead. Token-oriented methods, such as pruning and skipping, have shown promise in reducing inference latency, but still suffer from inherently limited acceleration potential, outdated proxy signals, and redundancy interference, thus yielding suboptimal speed-accuracy trade-offs. To address these challenges, we propose SPTS (Self-Predictive Token Skipping), a training-free framework for efficient long-context LLM inference. Specifically, motivated by the thought of probing the influence of targeted skipping layers, we design two component-specific strategies for selective token skipping: Partial Attention Probing (PAP) for multi-head attention, which selects informative tokens by performing partial forward attention computation, and Low-rank Transformation Probing (LTP) for feed forward network, which constructs a low-rank proxy network to predict token transformations. Furthermore, a Multi-Stage Delayed Pruning (MSDP) strategy reallocates the skipping budget and progressively prunes redundant tokens across layers. Extensive experiments demonstrate the effectiveness of our method, achieving up to 2.46$\\times$ and 2.29$\\times$ speedups for prefilling and end-to-end generation, respectively, while maintaining state-of-the-art model performance. The source code will be publicly available upon paper acceptance.",
        "url": "http://arxiv.org/abs/2601.13155v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13155v1",
        "arxiv_id": "2601.13155v1",
        "authors": [
            "Zimeng Wu",
            "Donghao Wang",
            "Chaozhe Jin",
            "Jiaxin Chen",
            "Yunhong Wang"
        ],
        "submitted": "2026-01-19 15:34:29",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TVWorld: Foundations for Remote-Control TV Agents",
        "abstract": "Recent large vision-language models (LVLMs) have demonstrated strong potential for device control. However, existing research has primarily focused on point-and-click (PnC) interaction, while remote-control (RC) interaction commonly encountered in everyday TV usage remains largely underexplored. To fill this gap, we introduce \\textbf{TVWorld}, an offline graph-based abstraction of real-world TV navigation that enables reproducible and deployment-free evaluation. On this basis, we derive two complementary benchmarks that comprehensively assess TV-use capabilities: \\textbf{TVWorld-N} for topology-aware navigation and \\textbf{TVWorld-G} for focus-aware grounding. These benchmarks expose a key limitation of existing agents: insufficient topology awareness for focus-based, long-horizon TV navigation. Motivated by this finding, we propose a \\emph{Topology-Aware Training} framework that injects topology awareness into LVLMs. Using this framework, we develop \\textbf{TVTheseus}, a foundation model specialized for TV navigation. TVTheseus achieves a success rate of $68.3\\%$ on TVWorld-N, surpassing strong closed-source baselines such as Gemini 3 Flash and establishing state-of-the-art (SOTA) performance. Additional analyses further provide valuable insights into the development of effective TV-use agents.",
        "url": "http://arxiv.org/abs/2601.13142v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13142v1",
        "arxiv_id": "2601.13142v1",
        "authors": [
            "Zhantao Ma",
            "Quanfeng Lu",
            "Shuai Zhong",
            "Dahai Yu",
            "Ping Luo",
            "Michael K. Ng"
        ],
        "submitted": "2026-01-19 15:24:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Adversarial Alignment: Ensuring Value Consistency in Large Language Models for Sensitive Domains",
        "abstract": "With the wide application of large language models (LLMs), the problems of bias and value inconsistency in sensitive domains have gradually emerged, especially in terms of race, society and politics. In this paper, we propose an adversarial alignment framework, which enhances the value consistency of the model in sensitive domains through continued pre-training, instruction fine-tuning and adversarial training. In adversarial training, we use the Attacker to generate controversial queries, the Actor to generate responses with value consistency, and the Critic to filter and ensure response quality. Furthermore, we train a Value-Consistent Large Language Model, VC-LLM, for sensitive domains, and construct a bilingual evaluation dataset in Chinese and English. The experimental results show that VC-LLM performs better than the existing mainstream models in both Chinese and English tests, verifying the effectiveness of the method. Warning: This paper contains examples of LLMs that are offensive or harmful in nature.",
        "url": "http://arxiv.org/abs/2601.13137v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13137v1",
        "arxiv_id": "2601.13137v1",
        "authors": [
            "Yuan Gao",
            "Zhigang Liu",
            "Xinyu Yao",
            "Bo Chen",
            "Xiaobing Zhao"
        ],
        "submitted": "2026-01-19 15:21:26",
        "source": "arxiv",
        "comment": "13 pages, 5 figures"
    },
    {
        "title": "Agentic Conversational Search with Contextualized Reasoning via Reinforcement Learning",
        "abstract": "Large Language Models (LLMs) have become a popular interface for human-AI interaction, supporting information seeking and task assistance through natural, multi-turn dialogue. To respond to users within multi-turn dialogues, the context-dependent user intent evolves across interactions, requiring contextual interpretation, query reformulation, and dynamic coordination between retrieval and generation. Existing studies usually follow static rewrite, retrieve, and generate pipelines, which optimize different procedures separately and overlook the mixed-initiative action optimization simultaneously. Although the recent developments in deep search agents demonstrate the effectiveness in jointly optimizing retrieval and generation via reasoning, these approaches focus on single-turn scenarios, which might lack the ability to handle multi-turn interactions. We introduce a conversational agent that interleaves search and reasoning across turns, enabling exploratory and adaptive behaviors learned through reinforcement learning (RL) training with tailored rewards towards evolving user goals. The experimental results across four widely used conversational benchmarks demonstrate the effectiveness of our methods by surpassing several existing strong baselines.",
        "url": "http://arxiv.org/abs/2601.13115v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13115v1",
        "arxiv_id": "2601.13115v1",
        "authors": [
            "Fengran Mo",
            "Yifan Gao",
            "Sha Li",
            "Hansi Zeng",
            "Xin Liu",
            "Zhaoxuan Tan",
            "Xian Li",
            "Jianshu Chen",
            "Dakuo Wang",
            "Meng Jiang"
        ],
        "submitted": "2026-01-19 14:55:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CORE-T: COherent REtrieval of Tables for Text-to-SQL",
        "abstract": "Realistic text-to-SQL workflows often require joining multiple tables. As a result, accurately retrieving the relevant set of tables becomes a key bottleneck for end-to-end performance. We study an open-book setting where queries must be answered over large, heterogeneous table collections pooled from many sources, without clean scoping signals such as database identifiers. Here, dense retrieval (DR) achieves high recall but returns many distractors, while join-aware alternatives often rely on extra assumptions and/or incur high inference overhead. We propose CORE-T, a scalable, training-free framework that enriches tables with LLM-generated purpose metadata and pre-computes a lightweight table-compatibility cache. At inference time, DR returns top-K candidates; a single LLM call selects a coherent, joinable subset, and a simple additive adjustment step restores strongly compatible tables. Across Bird, Spider, and MMQA, CORE-T improves table-selection F1 by up to 22.7 points while retrieving up to 42% fewer tables, improving multi-table execution accuracy by up to 5.0 points on Bird and 6.9 points on MMQA, and using 4-5x fewer tokens than LLM-intensive baselines.",
        "url": "http://arxiv.org/abs/2601.13111v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13111v1",
        "arxiv_id": "2601.13111v1",
        "authors": [
            "Hassan Soliman",
            "Vivek Gupta",
            "Dan Roth",
            "Iryna Gurevych"
        ],
        "submitted": "2026-01-19 14:51:23",
        "source": "arxiv",
        "comment": "Preprint under review. Code and data available at: https://github.com/UKPLab/arxiv2026-core-t"
    },
    {
        "title": "Leveraging Lora Fine-Tuning and Knowledge Bases for Construction Identification",
        "abstract": "This study investigates the automatic identification of the English ditransitive construction by integrating LoRA-based fine-tuning of a large language model with a Retrieval-Augmented Generation (RAG) framework.A binary classification task was conducted on annotated data from the British National Corpus. Results demonstrate that a LoRA-fine-tuned Qwen3-8B model significantly outperformed both a native Qwen3-MAX model and a theory-only RAG system. Detailed error analysis reveals that fine-tuning shifts the model's judgment from a surface-form pattern matching towards a more semantically grounded understanding based.",
        "url": "http://arxiv.org/abs/2601.13105v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13105v1",
        "arxiv_id": "2601.13105v1",
        "authors": [
            "Liu Kaipeng",
            "Wu Ling"
        ],
        "submitted": "2026-01-19 14:43:11",
        "source": "arxiv",
        "comment": "19pages, 1figure"
    },
    {
        "title": "Alexandria: A Multi-Domain Dialectal Arabic Machine Translation Dataset for Culturally Inclusive and Linguistically Diverse LLMs",
        "abstract": "Arabic is a highly diglossic language where most daily communication occurs in regional dialects rather than Modern Standard Arabic. Despite this, machine translation (MT) systems often generalize poorly to dialectal input, limiting their utility for millions of speakers. We introduce \\textbf{Alexandria}, a large-scale, community-driven, human-translated dataset designed to bridge this gap. Alexandria covers 13 Arab countries and 11 high-impact domains, including health, education, and agriculture. Unlike previous resources, Alexandria provides unprecedented granularity by associating contributions with city-of-origin metadata, capturing authentic local varieties beyond coarse regional labels. The dataset consists of multi-turn conversational scenarios annotated with speaker-addressee gender configurations, enabling the study of gender-conditioned variation in dialectal use. Comprising 107K total samples, Alexandria serves as both a training resource and a rigorous benchmark for evaluating MT and Large Language Models (LLMs). Our automatic and human evaluation of Arabic-aware LLMs benchmarks current capabilities in translating across diverse Arabic dialects and sub-dialects, while exposing significant persistent challenges.",
        "url": "http://arxiv.org/abs/2601.13099v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13099v1",
        "arxiv_id": "2601.13099v1",
        "authors": [
            "Abdellah El Mekki",
            "Samar M. Magdy",
            "Houdaifa Atou",
            "Ruwa AbuHweidi",
            "Baraah Qawasmeh",
            "Omer Nacar",
            "Thikra Al-hibiri",
            "Razan Saadie",
            "Hamzah Alsayadi",
            "Nadia Ghezaiel Hammouda",
            "Alshima Alkhazimi",
            "Aya Hamod",
            "Al-Yas Al-Ghafri",
            "Wesam El-Sayed",
            "Asila Al sharji",
            "Mohamad Ballout",
            "Anas Belfathi",
            "Karim Ghaddar",
            "Serry Sibaee",
            "Alaa Aoun",
            "Areej Asiri",
            "Lina Abureesh",
            "Ahlam Bashiti",
            "Majdal Yousef",
            "Abdulaziz Hafiz",
            "Yehdih Mohamed",
            "Emira Hamedtou",
            "Brakehe Brahim",
            "Rahaf Alhamouri",
            "Youssef Nafea",
            "Aya El Aatar",
            "Walid Al-Dhabyani",
            "Emhemed Hamed",
            "Sara Shatnawi",
            "Fakhraddin Alwajih",
            "Khalid Elkhidir",
            "Ashwag Alasmari",
            "Abdurrahman Gerrio",
            "Omar Alshahri",
            "AbdelRahim A. Elmadany",
            "Ismail Berrada",
            "Amir Azad Adli Alkathiri",
            "Fadi A Zaraket",
            "Mustafa Jarrar",
            "Yahya Mohamed El Hadj",
            "Hassan Alhuzali",
            "Muhammad Abdul-Mageed"
        ],
        "submitted": "2026-01-19 14:38:28",
        "source": "arxiv",
        "comment": "Project resources will be available here: https://github.com/UBC-NLP/Alexandria"
    },
    {
        "title": "Profiling German Text Simplification with Interpretable Model-Fingerprints",
        "abstract": "While Large Language Models (LLMs) produce highly nuanced text simplifications, developers currently lack tools for a holistic, efficient, and reproducible diagnosis of their behavior. This paper introduces the Simplification Profiler, a diagnostic toolkit that generates a multidimensional, interpretable fingerprint of simplified texts. Multiple aggregated simplifications of a model result in a model's fingerprint. This novel evaluation paradigm is particularly vital for languages, where the data scarcity problem is magnified when creating flexible models for diverse target groups rather than a single, fixed simplification style. We propose that measuring a model's unique behavioral signature is more relevant in this context as an alternative to correlating metrics with human preferences. We operationalize this with a practical meta-evaluation of our fingerprints' descriptive power, which bypasses the need for large, human-rated datasets. This test measures if a simple linear classifier can reliably identify various model configurations by their created simplifications, confirming that our metrics are sensitive to a model's specific characteristics. The Profiler can distinguish high-level behavioral variations between prompting strategies and fine-grained changes from prompt engineering, including few-shot examples. Our complete feature set achieves classification F1-scores up to 71.9 %, improving upon simple baselines by over 48 percentage points. The Simplification Profiler thus offers developers a granular, actionable analysis to build more effective and truly adaptive text simplification systems.",
        "url": "http://arxiv.org/abs/2601.13050v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13050v1",
        "arxiv_id": "2601.13050v1",
        "authors": [
            "Lars Klöser",
            "Mika Beele",
            "Bodo Kraft"
        ],
        "submitted": "2026-01-19 13:39:59",
        "source": "arxiv",
        "comment": "Presented at 2nd International Conference on Explainable AI for Neural and Symbolic Systems"
    },
    {
        "title": "Typhoon ASR Real-time: FastConformer-Transducer for Thai Automatic Speech Recognition",
        "abstract": "Large encoder-decoder models like Whisper achieve strong offline transcription but remain impractical for streaming applications due to high latency. However, due to the accessibility of pre-trained checkpoints, the open Thai ASR landscape remains dominated by these offline architectures, leaving a critical gap in efficient streaming solutions. We present Typhoon ASR Real-time, a 115M-parameter FastConformer-Transducer model for low-latency Thai speech recognition. We demonstrate that rigorous text normalization can match the impact of model scaling: our compact model achieves a 45x reduction in computational cost compared to Whisper Large-v3 while delivering comparable accuracy. Our normalization pipeline resolves systemic ambiguities in Thai transcription --including context-dependent number verbalization and repetition markers (mai yamok) --creating consistent training targets. We further introduce a two-stage curriculum learning approach for Isan (north-eastern) dialect adaptation that preserves Central Thai performance. To address reproducibility challenges in Thai ASR, we release the Typhoon ASR Benchmark, a gold-standard human-labeled datasets with transcriptions following established Thai linguistic conventions, providing standardized evaluation protocols for the research community.",
        "url": "http://arxiv.org/abs/2601.13044v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13044v1",
        "arxiv_id": "2601.13044v1",
        "authors": [
            "Warit Sirichotedumrong",
            "Adisai Na-Thalang",
            "Potsawee Manakul",
            "Pittawat Taveekitworachai",
            "Sittipong Sripaisarnmongkol",
            "Kunat Pipatanakul"
        ],
        "submitted": "2026-01-19 13:28:17",
        "source": "arxiv",
        "comment": "Models and datasets are publicly available on https://huggingface.co/collections/typhoon-ai/typhoon-asr-technical-report ; Project Page: https://opentyphoon.ai/model/typhoon-asr-realtime"
    },
    {
        "title": "SASA: Semantic-Aware Contrastive Learning Framework with Separated Attention for Triple Classification",
        "abstract": "Knowledge Graphs~(KGs) often suffer from unreliable knowledge, which restricts their utility. Triple Classification~(TC) aims to determine the validity of triples from KGs. Recently, text-based methods learn entity and relation representations from natural language descriptions, significantly improving the generalization capabilities of TC models and setting new benchmarks in performance. However, there are still two critical challenges. First, existing methods often ignore the effective semantic interaction among different KG components. Second, most approaches adopt single binary classification training objective, leading to insufficient semantic representation learning. To address these challenges, we propose \\textbf{SASA}, a novel framework designed to enhance TC models via separated attention mechanism and semantic-aware contrastive learning~(CL). Specifically, we first propose separated attention mechanism to encode triples into decoupled contextual representations and then fuse them through a more effective interactive way. Then, we introduce semantic-aware hierarchical CL as auxiliary training objective to guide models in improving their discriminative capabilities and achieving sufficient semantic learning, considering both local level and global level CL. Experimental results across two benchmark datasets demonstrate that SASA significantly outperforms state-of-the-art methods. In terms of accuracy, we advance the state-of-the-art by +5.9\\% on FB15k-237 and +3.4\\% on YAGO3-10.",
        "url": "http://arxiv.org/abs/2601.13035v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13035v1",
        "arxiv_id": "2601.13035v1",
        "authors": [
            "Xu Xiaodan",
            "Hu Xiaolin"
        ],
        "submitted": "2026-01-19 13:19:00",
        "source": "arxiv",
        "comment": "in progress"
    },
    {
        "title": "Tears or Cheers? Benchmarking LLMs via Culturally Elicited Distinct Affective Responses",
        "abstract": "Culture serves as a fundamental determinant of human affective processing and profoundly shapes how individuals perceive and interpret emotional stimuli. Despite this intrinsic link extant evaluations regarding cultural alignment within Large Language Models primarily prioritize declarative knowledge such as geographical facts or established societal customs. These benchmarks remain insufficient to capture the subjective interpretative variance inherent to diverse sociocultural lenses. To address this limitation, we introduce CEDAR, a multimodal benchmark constructed entirely from scenarios capturing Culturally \\underline{\\textsc{E}}licited \\underline{\\textsc{D}}istinct \\underline{\\textsc{A}}ffective \\underline{\\textsc{R}}esponses. To construct CEDAR, we implement a novel pipeline that leverages LLM-generated provisional labels to isolate instances yielding cross-cultural emotional distinctions, and subsequently derives reliable ground-truth annotations through rigorous human evaluation. The resulting benchmark comprises 10,962 instances across seven languages and 14 fine-grained emotion categories, with each language including 400 multimodal and 1,166 text-only samples. Comprehensive evaluations of 17 representative multilingual models reveal a dissociation between language consistency and cultural alignment, demonstrating that culturally grounded affective understanding remains a significant challenge for current models.",
        "url": "http://arxiv.org/abs/2601.13024v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13024v1",
        "arxiv_id": "2601.13024v1",
        "authors": [
            "Chongyuan Dai",
            "Yaling Shen",
            "Jinpeng Hu",
            "Zihan Gao",
            "Jia Li",
            "Yishun Jiang",
            "Yaxiong Wang",
            "Liu Liu",
            "Zongyuan Ge"
        ],
        "submitted": "2026-01-19 13:04:26",
        "source": "arxiv",
        "comment": "24 pages, 10 figures, 9 Tables"
    },
    {
        "title": "Bi-Attention HateXplain : Taking into account the sequential aspect of data during explainability in a multi-task context",
        "abstract": "Technological advances in the Internet and online social networks have brought many benefits to humanity. At the same time, this growth has led to an increase in hate speech, the main global threat. To improve the reliability of black-box models used for hate speech detection, post-hoc approaches such as LIME, SHAP, and LRP provide the explanation after training the classification model. In contrast, multi-task approaches based on the HateXplain benchmark learn to explain and classify simultaneously. However, results from HateXplain-based algorithms show that predicted attention varies considerably when it should be constant. This attention variability can lead to inconsistent interpretations, instability of predictions, and learning difficulties. To solve this problem, we propose the BiAtt-BiRNN-HateXplain (Bidirectional Attention BiRNN HateXplain) model which is easier to explain compared to LLMs which are more complex in view of the need for transparency, and will take into account the sequential aspect of the input data during explainability thanks to a BiRNN layer. Thus, if the explanation is correctly estimated, thanks to multi-task learning (explainability and classification task), the model could classify better and commit fewer unintentional bias errors related to communities. The experimental results on HateXplain data show a clear improvement in detection performance, explainability and a reduction in unintentional bias.",
        "url": "http://arxiv.org/abs/2601.13018v1",
        "pdf_url": "https://arxiv.org/pdf/2601.13018v1",
        "arxiv_id": "2601.13018v1",
        "authors": [
            "Ghislain Dorian Tchuente Mondjo"
        ],
        "submitted": "2026-01-19 12:52:18",
        "source": "arxiv",
        "comment": "Accepted at \"EAI AFRICOMM 2025 - 17th EAI International Conference on Communications and Networks in Africa\""
    },
    {
        "title": "Graph Reasoning Paradigm: Structured and Symbolic Reasoning with Topology-Aware Reinforcement Learning for Large Language Models",
        "abstract": "Long Chain-of-Thought (LCoT), achieved by Reinforcement Learning with Verifiable Rewards (RLVR), has proven effective in enhancing the reasoning capabilities of Large Language Models (LLMs). However, reasoning in current LLMs is primarily generated as plain text, where performing semantic evaluation on such unstructured data creates a computational bottleneck during training. Despite RLVR-based optimization, existing methods still suffer from coarse-grained supervision, reward hacking, high training costs, and poor generalization. To address these issues, we propose the Graph Reasoning Paradigm (GRP), which realizes structured and symbolic reasoning, implemented via graph-structured representations with step-level cognitive labels. Building upon GRP, we further design Process-Aware Stratified Clipping Group Relative Policy Optimization (PASC-GRPO), which leverages structured evaluation to replace semantic evaluation, achieves process-aware verification through graph-structured outcome rewards, and mitigates reward hacking via stratified clipping advantage estimation. Experiments demonstrate significant improvements across mathematical reasoning and code generation tasks. Data, models, and code will be released later.",
        "url": "http://arxiv.org/abs/2601.12995v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12995v1",
        "arxiv_id": "2601.12995v1",
        "authors": [
            "Runxuan Liu",
            "Xianhao Ou",
            "Xinyan Ma",
            "Jiyuan Wang",
            "Jiafeng Liang",
            "Jiaqi Li",
            "Tao He",
            "Zheng Chu",
            "Rongchuan Mu",
            "Zekun Wang",
            "Baoxin Wang",
            "Dayong Wu",
            "Ming Liu",
            "Shijin Wang",
            "Guoping Hu",
            "Bing Qin"
        ],
        "submitted": "2026-01-19 12:23:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "RAGExplorer: A Visual Analytics System for the Comparative Diagnosis of RAG Systems",
        "abstract": "The advent of Retrieval-Augmented Generation (RAG) has significantly enhanced the ability of Large Language Models (LLMs) to produce factually accurate and up-to-date responses. However, the performance of a RAG system is not determined by a single component but emerges from a complex interplay of modular choices, such as embedding models and retrieval algorithms. This creates a vast and often opaque configuration space, making it challenging for developers to understand performance trade-offs and identify optimal designs. To address this challenge, we present RAGExplorer, a visual analytics system for the systematic comparison and diagnosis of RAG configurations. RAGExplorer guides users through a seamless macro-to-micro analytical workflow. Initially, it empowers developers to survey the performance landscape across numerous configurations, allowing for a high-level understanding of which design choices are most effective. For a deeper analysis, the system enables users to drill down into individual failure cases, investigate how differences in retrieved information contribute to errors, and interactively test hypotheses by manipulating the provided context to observe the resulting impact on the generated answer. We demonstrate the effectiveness of RAGExplorer through detailed case studies and user studies, validating its ability to empower developers in navigating the complex RAG design space. Our code and user guide are publicly available at https://github.com/Thymezzz/RAGExplorer.",
        "url": "http://arxiv.org/abs/2601.12991v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12991v1",
        "arxiv_id": "2601.12991v1",
        "authors": [
            "Haoyu Tian",
            "Yingchaojie Feng",
            "Zhen Wen",
            "Haoxuan Li",
            "Minfeng Zhu",
            "Wei Chen"
        ],
        "submitted": "2026-01-19 12:09:56",
        "source": "arxiv",
        "comment": "11 pages, 7 figures. Accepted to IEEE TVCG (PacificVis 2026)"
    },
    {
        "title": "Rules, Resources, and Restrictions: A Taxonomy of Task-Based Information Request Intents",
        "abstract": "Understanding and classifying query intents can improve retrieval effectiveness by helping align search results with the motivations behind user queries. However, existing intent taxonomies are typically derived from system log data and capture mostly isolated information needs, while the broader task context often remains unaddressed. This limitation becomes increasingly relevant as interactions with Large Language Models (LLMs) expand user expectations from simple query answering toward comprehensive task support, for example, with purchasing decisions or in travel planning. At the same time, current LLMs still struggle to fully interpret complex and multifaceted tasks. To address this gap, we argue for a stronger task-based perspective on query intent. Drawing on a grounded-theory-based interview study with airport information clerks, we present a taxonomy of task-based information request intents that bridges the gap between traditional query-focused approaches and the emerging demands of AI-driven task-oriented search.",
        "url": "http://arxiv.org/abs/2601.12985v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12985v1",
        "arxiv_id": "2601.12985v1",
        "authors": [
            "Melanie A. Kilian",
            "David Elsweiler"
        ],
        "submitted": "2026-01-19 11:59:23",
        "source": "arxiv",
        "comment": "11 pages, 1 figure, to be published in: 2026 ACM SIGIR Conference on Human Information Interaction and Retrieval (CHIIR '26), March 22-26, 2026, Seattle, WA, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3786304.3787863"
    },
    {
        "title": "ChartAttack: Testing the Vulnerability of LLMs to Malicious Prompting in Chart Generation",
        "abstract": "Multimodal large language models (MLLMs) are increasingly used to automate chart generation from data tables, enabling efficient data analysis and reporting but also introducing new misuse risks. In this work, we introduce ChartAttack, a novel framework for evaluating how MLLMs can be misused to generate misleading charts at scale. ChartAttack injects misleaders into chart designs, aiming to induce incorrect interpretations of the underlying data. Furthermore, we create AttackViz, a chart question-answering (QA) dataset where each (chart specification, QA) pair is labeled with effective misleaders and their induced incorrect answers. Experiments in in-domain and cross-domain settings show that ChartAttack significantly degrades the QA performance of MLLM readers, reducing accuracy by an average of 19.6 points and 14.9 points, respectively. A human study further shows an average 20.2 point drop in accuracy for participants exposed to misleading charts generated by ChartAttack. Our findings highlight an urgent need for robustness and security considerations in the design, evaluation, and deployment of MLLM-based chart generation systems. We make our code and data publicly available.",
        "url": "http://arxiv.org/abs/2601.12983v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12983v1",
        "arxiv_id": "2601.12983v1",
        "authors": [
            "Jesus-German Ortiz-Barajas",
            "Jonathan Tonglet",
            "Vivek Gupta",
            "Iryna Gurevych"
        ],
        "submitted": "2026-01-19 11:57:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Bitter Lesson of Diffusion Language Models for Agentic Workflows: A Comprehensive Reality Check",
        "abstract": "The pursuit of real-time agentic interaction has driven interest in Diffusion-based Large Language Models (dLLMs) as alternatives to auto-regressive backbones, promising to break the sequential latency bottleneck. However, does such efficiency gains translate into effective agentic behavior? In this work, we present a comprehensive evaluation of dLLMs (e.g., LLaDA, Dream) across two distinct agentic paradigms: Embodied Agents (requiring long-horizon planning) and Tool-Calling Agents (requiring precise formatting). Contrary to the efficiency hype, our results on Agentboard and BFCL reveal a \"bitter lesson\": current dLLMs fail to serve as reliable agentic backbones, frequently leading to systematically failure. (1) In Embodied settings, dLLMs suffer repeated attempts, failing to branch under temporal feedback. (2) In Tool-Calling settings, dLLMs fail to maintain symbolic precision (e.g. strict JSON schemas) under diffusion noise. To assess the potential of dLLMs in agentic workflows, we introduce DiffuAgent, a multi-agent evaluation framework that integrates dLLMs as plug-and-play cognitive cores. Our analysis shows that dLLMs are effective in non-causal roles (e.g., memory summarization and tool selection) but require the incorporation of causal, precise, and logically grounded reasoning mechanisms into the denoising process to be viable for agentic tasks.",
        "url": "http://arxiv.org/abs/2601.12979v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12979v1",
        "arxiv_id": "2601.12979v1",
        "authors": [
            "Qingyu Lu",
            "Liang Ding",
            "Kanjian Zhang",
            "Jinxia Zhang",
            "Dacheng Tao"
        ],
        "submitted": "2026-01-19 11:45:39",
        "source": "arxiv",
        "comment": "Under Review"
    },
    {
        "title": "Bridging the Knowledge-Action Gap by Evaluating LLMs in Dynamic Dental Clinical Scenarios",
        "abstract": "The transition of Large Language Models (LLMs) from passive knowledge retrievers to autonomous clinical agents demands a shift in evaluation-from static accuracy to dynamic behavioral reliability. To explore this boundary in dentistry, a domain where high-quality AI advice uniquely empowers patient-participatory decision-making, we present the Standardized Clinical Management & Performance Evaluation (SCMPE) benchmark, which comprehensively assesses performance from knowledge-oriented evaluations (static objective tasks) to workflow-based simulations (multi-turn simulated patient interactions). Our analysis reveals that while models demonstrate high proficiency in static objective tasks, their performance precipitates in dynamic clinical dialogues, identifying that the primary bottleneck lies not in knowledge retention, but in the critical challenges of active information gathering and dynamic state tracking. Mapping \"Guideline Adherence\" versus \"Decision Quality\" reveals a prevalent \"High Efficacy, Low Safety\" risk in general models. Furthermore, we quantify the impact of Retrieval-Augmented Generation (RAG). While RAG mitigates hallucinations in static tasks, its efficacy in dynamic workflows is limited and heterogeneous, sometimes causing degradation. This underscores that external knowledge alone cannot bridge the reasoning gap without domain-adaptive pre-training. This study empirically charts the capability boundaries of dental LLMs, providing a roadmap for bridging the gap between standardized knowledge and safe, autonomous clinical practice.",
        "url": "http://arxiv.org/abs/2601.12974v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12974v1",
        "arxiv_id": "2601.12974v1",
        "authors": [
            "Hongyang Ma",
            "Tiantian Gu",
            "Huaiyuan Sun",
            "Huilin Zhu",
            "Yongxin Wang",
            "Jie Li",
            "Wubin Sun",
            "Zeliang Lian",
            "Yinghong Zhou",
            "Yi Gao",
            "Shirui Wang",
            "Zhihui Tang"
        ],
        "submitted": "2026-01-19 11:36:39",
        "source": "arxiv",
        "comment": "29 pages, 15 figures"
    },
    {
        "title": "Pardon? Evaluating Conversational Repair in Large Audio-Language Models",
        "abstract": "Large Audio-Language Models (LALMs) have demonstrated strong performance in spoken question answering (QA), with existing evaluations primarily focusing on answer accuracy and robustness to acoustic perturbations. However, such evaluations implicitly assume that spoken inputs remain semantically answerable, an assumption that often fails in real-world interaction when essential information is missing. In this work, we introduce a repair-aware evaluation setting that explicitly distinguishes between answerable and unanswerable audio inputs. We define answerability as a property of the input itself and construct paired evaluation conditions using a semantic-acoustic masking protocol. Based on this setting, we propose the Evaluability Awareness and Repair (EAR) score, a non-compensatory metric that jointly evaluates task competence under answerable conditions and repair behavior under unanswerable conditions. Experiments on two spoken QA benchmarks across diverse LALMs reveal a consistent gap between answer accuracy and conversational reliability: while many models perform well when inputs are answerable, most fail to recognize semantic unanswerability and initiate appropriate conversational repair. These findings expose a limitation of prevailing accuracy-centric evaluation practices and motivate reliability assessments that treat unanswerable inputs as cues for repair and continued interaction.",
        "url": "http://arxiv.org/abs/2601.12973v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12973v1",
        "arxiv_id": "2601.12973v1",
        "authors": [
            "Shuanghong Huang",
            "Jinlei Xu",
            "Youchao Zhou",
            "Yanghao Zhou",
            "Xuan Zhao",
            "Chong Feng",
            "Wenxuan Zhang"
        ],
        "submitted": "2026-01-19 11:36:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Lombard Speech Synthesis for Any Voice with Controllable Style Embeddings",
        "abstract": "The Lombard effect plays a key role in natural communication, particularly in noisy environments or when addressing hearing-impaired listeners. We present a controllable text-to-speech (TTS) system capable of synthesizing Lombard speech for any speaker without requiring explicit Lombard data during training. Our approach leverages style embeddings learned from a large, prosodically diverse dataset and analyzes their correlation with Lombard attributes using principal component analysis (PCA). By shifting the relevant PCA components, we manipulate the style embeddings and incorporate them into our TTS model to generate speech at desired Lombard levels. Evaluations demonstrate that our method preserves naturalness and speaker identity, enhances intelligibility under noise, and provides fine-grained control over prosody, offering a robust solution for controllable Lombard TTS for any speaker.",
        "url": "http://arxiv.org/abs/2601.12966v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12966v1",
        "arxiv_id": "2601.12966v1",
        "authors": [
            "Seymanur Akti",
            "Alexander Waibel"
        ],
        "submitted": "2026-01-19 11:25:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Trustworthy Data-driven Chronological Age Estimation from Panoramic Dental Images",
        "abstract": "Integrating deep learning into healthcare enables personalized care but raises trust issues due to model opacity. To improve transparency, we propose a system for dental age estimation from panoramic images that combines an opaque and a transparent method within a natural language generation (NLG) module. This module produces clinician-friendly textual explanations about the age estimations, designed with dental experts through a rule-based approach. Following the best practices in the field, the quality of the generated explanations was manually validated by dental experts using a questionnaire. The results showed a strong performance, since the experts rated 4.77+/-0.12 (out of 5) on average across the five dimensions considered. We also performed a trustworthy self-assessment procedure following the ALTAI checklist, in which it scored 4.40+/-0.27 (out of 5) across seven dimensions of the AI Trustworthiness Assessment List.",
        "url": "http://arxiv.org/abs/2601.12960v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12960v1",
        "arxiv_id": "2601.12960v1",
        "authors": [
            "Ainhoa Vivel-Couso",
            "Nicolás Vila-Blanco",
            "María J. Carreira",
            "Alberto Bugarín-Diz",
            "Inmaculada Tomás",
            "Jose M. Alonso-Moral"
        ],
        "submitted": "2026-01-19 11:15:52",
        "source": "arxiv",
        "comment": "This paper is a preliminary version of an accepted article in Information Systems Frontiers, Springer, Special Issue \"Explainability in Human-Centric AI\". Please cite the final published version of the paper, not this preprint. The final published version can be found at https://link.springer.com/article/10.1007/s10796-025-10682-3"
    },
    {
        "title": "AI-generated data contamination erodes pathological variability and diagnostic reliability",
        "abstract": "Generative artificial intelligence (AI) is rapidly populating medical records with synthetic content, creating a feedback loop where future models are increasingly at risk of training on uncurated AI-generated data. However, the clinical consequences of this AI-generated data contamination remain unexplored. Here, we show that in the absence of mandatory human verification, this self-referential cycle drives a rapid erosion of pathological variability and diagnostic reliability. By analysing more than 800,000 synthetic data points across clinical text generation, vision-language reporting, and medical image synthesis, we find that models progressively converge toward generic phenotypes regardless of the model architecture. Specifically, rare but critical findings, including pneumothorax and effusions, vanish from the synthetic content generated by AI models, while demographic representations skew heavily toward middle-aged male phenotypes. Crucially, this degradation is masked by false diagnostic confidence; models continue to issue reassuring reports while failing to detect life-threatening pathology, with false reassurance rates tripling to 40%. Blinded physician evaluation confirms that this decoupling of confidence and accuracy renders AI-generated documentation clinically useless after just two generations. We systematically evaluate three mitigation strategies, finding that while synthetic volume scaling fails to prevent collapse, mixing real data with quality-aware filtering effectively preserves diversity. Ultimately, our results suggest that without policy-mandated human oversight, the deployment of generative AI threatens to degrade the very healthcare data ecosystems it relies upon.",
        "url": "http://arxiv.org/abs/2601.12946v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12946v1",
        "arxiv_id": "2601.12946v1",
        "authors": [
            "Hongyu He",
            "Shaowen Xiang",
            "Ye Zhang",
            "Yingtao Zhu",
            "Jin Zhang",
            "Hao Deng",
            "Emily Alsentzer",
            "Qingyu Chen",
            "Kun-Hsing Yu",
            "Andrew Marmenshall",
            "Tingting Chen",
            "Srinivas Anumasa",
            "Daniel Ebner",
            "Dean Ho",
            "Kee Yuan Ngiam",
            "Ching-Yu Cheng",
            "Dianbo Liu"
        ],
        "submitted": "2026-01-19 10:54:03",
        "source": "arxiv",
        "comment": "*Corresponding author: Dianbo Liu (dianbo@nus.edu.sg)"
    },
    {
        "title": "A Component-Based Survey of Interactions between Large Language Models and Multi-Armed Bandits",
        "abstract": "Large language models (LLMs) have become powerful and widely used systems for language understanding and generation, while multi-armed bandit (MAB) algorithms provide a principled framework for adaptive decision-making under uncertainty. This survey explores the potential at the intersection of these two fields. As we know, it is the first survey to systematically review the bidirectional interaction between large language models and multi-armed bandits at the component level. We highlight the bidirectional benefits: MAB algorithms address critical LLM challenges, spanning from pre-training to retrieval-augmented generation (RAG) and personalization. Conversely, LLMs enhance MAB systems by redefining core components such as arm definition and environment modeling, thereby improving decision-making in sequential tasks. We analyze existing LLM-enhanced bandit systems and bandit-enhanced LLM systems, providing insights into their design, methodologies, and performance. Key challenges and representative findings are identified to help guide future research. An accompanying GitHub repository that indexes relevant literature is available at https://github.com/bucky1119/Awesome-LLM-Bandit-Interaction.",
        "url": "http://arxiv.org/abs/2601.12945v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12945v1",
        "arxiv_id": "2601.12945v1",
        "authors": [
            "Miao Xie",
            "Siguang Chen",
            "Chunli Lv"
        ],
        "submitted": "2026-01-19 10:53:57",
        "source": "arxiv",
        "comment": "27 pages, 6 table"
    },
    {
        "title": "Injecting Knowledge from Social Science Journals to Improve Indonesian Cultural Understanding by LLMs",
        "abstract": "Recently there have been intensifying efforts to improve the understanding of Indonesian cultures by large language models (LLMs). An attractive source of cultural knowledge that has been largely overlooked is local journals of social science, which likely contain substantial cultural studies from a native perspective. We present a novel text dataset of journal article passages, created from 151 open-source Indonesian social science journals, called IndoSoSci. We demonstrate an effective recipe for injecting Indonesian cultural knowledge therein into LLMs: extracting the facts related to Indonesian culture, and apply retrieval-augmented generation (RAG) with LLM-generated hypothetical documents as queries during retrieval. The proposed recipe yields strong performance gains over several strong baselines on the IndoCulture benchmark. Additionally, by combining IndoSoSci with Indonesian Wikipedia, we set a new state-of-the-art accuracy on the IndoCulture benchmark.",
        "url": "http://arxiv.org/abs/2601.12921v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12921v1",
        "arxiv_id": "2601.12921v1",
        "authors": [
            "Adimulya Kartiyasa",
            "Bao Gia Cao",
            "Boyang Li"
        ],
        "submitted": "2026-01-19 10:22:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SciCoQA: Quality Assurance for Scientific Paper--Code Alignment",
        "abstract": "We present SciCoQA, a dataset for detecting discrepancies between scientific publications and their codebases to ensure faithful implementations. We construct SciCoQA from GitHub issues and reproducibility papers, and to scale our dataset, we propose a synthetic data generation method for constructing paper-code discrepancies. We analyze the paper-code discrepancies in detail and propose discrepancy types and categories to better understand the occurring mismatches. In total, our dataset consists of 611 paper-code discrepancies (81 real, 530 synthetic), spanning diverse computational science disciplines, including AI, Physics, Quantitative Biology, and others. Our evaluation of 21 LLMs highlights the difficulty of SciCoQA, particularly for instances involving omitted paper details, long-context inputs, and data outside the models' pre-training corpus. The best performing model in our evaluation, GPT-5, can only detect 45.7\\% of real-world paper-code discrepancies.",
        "url": "http://arxiv.org/abs/2601.12910v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12910v1",
        "arxiv_id": "2601.12910v1",
        "authors": [
            "Tim Baumgärtner",
            "Iryna Gurevych"
        ],
        "submitted": "2026-01-19 10:04:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Gated Differentiable Working Memory for Long-Context Language Modeling",
        "abstract": "Long contexts challenge transformers: attention scores dilute across thousands of tokens, critical information is often lost in the middle, and models struggle to adapt to novel patterns at inference time. Recent work on test-time adaptation addresses this by maintaining a form of working memory -- transient parameters updated on the current context -- but existing approaches rely on uniform write policies that waste computation on low-utility regions and suffer from high gradient variance across semantically heterogeneous contexts. In this work, we reframe test-time adaptation as a budget-constrained memory consolidation problem, focusing on which parts of the context should be consolidated into working memory under limited computation. We propose Gdwm (Gated Differentiable Working Memory), a framework that introduces a write controller to gate the consolidation process. The controller estimates Contextual Utility, an information-theoretic measure of long-range contextual dependence, and allocates gradient steps accordingly while maintaining global coverage. Experiments on ZeroSCROLLS and LongBench v2 demonstrate that Gdwm achieves comparable or superior performance with 4$\\times$ fewer gradient steps than uniform baselines, establishing a new efficiency-performance Pareto frontier for test-time adaptation.",
        "url": "http://arxiv.org/abs/2601.12906v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12906v1",
        "arxiv_id": "2601.12906v1",
        "authors": [
            "Lingrui Mei",
            "Shenghua Liu",
            "Yiwei Wang",
            "Yuyao Ge",
            "Baolong Bi",
            "Jiayu Yao",
            "Jun Wan",
            "Ziling Yin",
            "Jiafeng Guo",
            "Xueqi Cheng"
        ],
        "submitted": "2026-01-19 10:00:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "From Prefix Cache to Fusion RAG Cache: Accelerating LLM Inference in Retrieval-Augmented Generation",
        "abstract": "Retrieval-Augmented Generation enhances Large Language Models by integrating external knowledge, which reduces hallucinations but increases prompt length. This increase leads to higher computational costs and longer Time to First Token (TTFT). To mitigate this issue, existing solutions aim to reuse the preprocessed KV cache of each retrieved chunk to accelerate RAG. However, the lack of cross-chunk contextual information leads to a significant drop in generation quality, leaving the potential benefits of KV cache reuse largely unfulfilled. The challenge lies in how to reuse the precomputed KV cache of chunks while preserving generation quality. We propose FusionRAG, a novel inference framework that optimizes both the preprocessing and reprocessing stages of RAG. In the offline preprocessing stage, we embed information from other related text chunks into each chunk, while in the online reprocessing stage, we recompute the KV cache for tokens that the model focuses on. As a result, we achieve a better trade-off between generation quality and efficiency. According to our experiments, FusionRAG significantly improves generation quality at the same recomputation ratio compared to previous state-of-the-art solutions. By recomputing fewer than 15% of the tokens, FusionRAG achieves up to 70% higher normalized F1 scores than baselines and reduces TTFT by 2.66x-9.39x compared to Full Attention.",
        "url": "http://arxiv.org/abs/2601.12904v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12904v1",
        "arxiv_id": "2601.12904v1",
        "authors": [
            "Jiahao Wang",
            "Weiyu Xie",
            "Mingxing Zhang",
            "Boxing Zhang",
            "Jianwei Dong",
            "Yuening Zhu",
            "Chen Lin",
            "Jinqi Tang",
            "Yaochen Han",
            "Zhiyuan Ai",
            "Xianglin Chen",
            "Yongwei Wu",
            "Congfeng Jiang"
        ],
        "submitted": "2026-01-19 09:59:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Audit du syst{è}me d'information et du mod{è}le de gouvernance de la Biblioth{è}que Num{é}rique de l'Espace universitaire Francophone (BNEUF) du projet Initiative pour le D{é}veloppement du Num{é}rique dans l'Espace Universitaire Francophone (IDNEUF)",
        "abstract": "This document provides an assessment of the overall structure of the BNEUF system and how it operates within the framework of the Initiative for Digital Development in French speaking Universities (IDNEUF). This report aims to support the AUF's new strategy for 2021-2025, with its new structural and governance foundations for the implementation of the Francophonie scientifique project. It was therefore decided to reorganize existing and future digital resources and services with a view to incorporating them into the future global collaborative platform for integrated services. This report provides an external assessment with new forms of organization and use of the BNEUF system. The aim is to provide the AUF project team with new avenues for optimized management of the compiled digital resources and to synergize them with the related modules of the Atlas of Expertise and the Francophone Social Network.",
        "url": "http://arxiv.org/abs/2601.12902v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12902v1",
        "arxiv_id": "2601.12902v1",
        "authors": [
            "Mokhtar Ben Henda"
        ],
        "submitted": "2026-01-19 09:56:52",
        "source": "arxiv",
        "comment": "in French language"
    },
    {
        "title": "Hierarchical Sparse Circuit Extraction from Billion-Parameter Language Models through Scalable Attribution Graph Decomposition",
        "abstract": "Mechanistic interpretability seeks to reverse-engineer neural network computations into human-understandable algorithms, yet extracting sparse computational circuits from billion-parameter language models remains challenging due to exponential search complexity and pervasive polysemanticity. The proposed Hierarchical Attribution Graph Decomposition (HAGD) framework reduces circuit discovery complexity from O(2^n) exhaustive enumeration to O(n^2 log n) through multi-resolution abstraction hierarchies and differentiable circuit search. The methodology integrates cross-layer transcoders for monosemantic feature extraction, graph neural network meta-learning for topology prediction, and causal intervention protocols for validation. Empirical evaluation spans GPT-2 variants, Llama-7B through Llama-70B, and Pythia suite models across algorithmic tasks and natural language benchmarks. On modular arithmetic tasks, the framework achieves up to 91% behavioral preservation ($\\pm$2.3\\% across runs) while maintaining interpretable subgraph sizes. Cross-architecture transfer experiments suggest that discovered circuits exhibit moderate structural similarity (averaging 67%) across model families, indicating potential shared computational patterns. These results provide preliminary foundations for interpretability at larger model scales while identifying significant limitations in current attribution methodologies that require future advances.",
        "url": "http://arxiv.org/abs/2601.12879v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12879v1",
        "arxiv_id": "2601.12879v1",
        "authors": [
            "Mohammed Mudassir Uddin",
            "Shahnawaz Alam",
            "Mohammed Kaif Pasha"
        ],
        "submitted": "2026-01-19 09:34:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Race, Ethnicity and Their Implication on Bias in Large Language Models",
        "abstract": "Large language models (LLMs) increasingly operate in high-stakes settings including healthcare and medicine, where demographic attributes such as race and ethnicity may be explicitly stated or implicitly inferred from text. However, existing studies primarily document outcome-level disparities, offering limited insight into internal mechanisms underlying these effects. We present a mechanistic study of how race and ethnicity are represented and operationalized within LLMs. Using two publicly available datasets spanning toxicity-related generation and clinical narrative understanding tasks, we analyze three open-source models with a reproducible interpretability pipeline combining probing, neuron-level attribution, and targeted intervention. We find that demographic information is distributed across internal units with substantial cross-model variation. Although some units encode sensitive or stereotype-related associations from pretraining, identical demographic cues can induce qualitatively different behaviors. Interventions suppressing such neurons reduce bias but leave substantial residual effects, suggesting behavioral rather than representational change and motivating more systematic mitigation.",
        "url": "http://arxiv.org/abs/2601.12868v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12868v1",
        "arxiv_id": "2601.12868v1",
        "authors": [
            "Shiyue Hu",
            "Ruizhe Li",
            "Yanjun Gao"
        ],
        "submitted": "2026-01-19 09:24:24",
        "source": "arxiv",
        "comment": "Work in process"
    },
    {
        "title": "Rapport du Projet de Recherche TRAIMA",
        "abstract": "The TRAIMA project (TRaitement Automatique des Interactions Multimodales en Apprentissage), conducted between March 2019 and June 2020, investigates the potential of automatic processing of multimodal interactions in educational settings. The project addresses a central methodological challenge in educational and interactional research: the analysis of verbal, paraverbal, and non-verbal data is currently carried out manually, making it extremely time-consuming and difficult to scale. TRAIMA explores how machine learning approaches could contribute to the categorisation and classification of such interactions. The project focuses specifically on explanatory and collaborative sequences occurring in classroom interactions, particularly in French as a Foreign Language (FLE) and French as a First Language (FLM) contexts. These sequences are analysed as inherently multimodal phenomena, combining spoken language with prosody, gestures, posture, gaze, and spatial positioning. A key theoretical contribution of the project is the precise linguistic and interactional definition of explanatory discourse as a tripartite sequence (opening, explanatory core, closure), drawing on discourse analysis and interactional linguistics. A substantial part of the research is devoted to the methodological foundations of transcription, which constitute a critical bottleneck for any form of automation. The report provides a detailed state of the art of existing transcription conventions (ICOR, Mondada, GARS, VALIBEL, Ferr{é}), highlighting their respective strengths and limitations when applied to multimodal classroom data. Through comparative analyses of manually transcribed sequences, the project demonstrates the inevitable variability and interpretative dimension of transcription practices, depending on theoretical positioning and analytical goals. Empirical work is based on several corpora, notably the INTER-EXPLIC corpus (approximately 30 hours of classroom interaction) and the EXPLIC-LEXIC corpus, which serve both as testing grounds for manual annotation and as reference datasets for future automation. Particular attention is paid to teacher gestures (kin{é}sic and proxemic resources), prosodic features, and their functional role in meaning construction and learner comprehension. The project also highlights the strategic role of the Techn{é}LAB platform, which provides advanced multimodal data capture (multi-camera video, synchronized audio, eye-tracking, digital interaction traces) and constitutes both a research infrastructure and a test environment for the development of automated tools. In conclusion, TRAIMA does not aim to deliver a fully operational automated system, but rather to establish a rigorous methodological framework for the automatic processing of multimodal pedagogical interactions. The project identifies transcription conventions, annotation categories, and analytical units that are compatible with machine learning approaches, while emphasizing the need for theoretical explicitness and researcher reflexivity. TRAIMA thus lays the groundwork for future interdisciplinary research at the intersection of didactics, discourse analysis, multimodality, and artificial intelligence in education.",
        "url": "http://arxiv.org/abs/2601.12844v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12844v1",
        "arxiv_id": "2601.12844v1",
        "authors": [
            "Julie Rançon",
            "Jean-François Cerisier",
            "Emilie Remond",
            "Aurélien Nguyen",
            "Andrew Peterson",
            "Ladjel Bellatreche"
        ],
        "submitted": "2026-01-19 08:55:50",
        "source": "arxiv",
        "comment": "in French language"
    },
    {
        "title": "The Unfairness of Multifactorial Bias in Recommendation",
        "abstract": "Popularity bias and positivity bias are two prominent sources of bias in recommender systems. Both arise from input data, propagate through recommendation models, and lead to unfair or suboptimal outcomes. Popularity bias occurs when a small subset of items receives most interactions, while positivity bias stems from the over-representation of high rating values. Although each bias has been studied independently, their combined effect, to which we refer to as multifactorial bias, remains underexplored. In this work, we examine how multifactorial bias influences item-side fairness, focusing on exposure bias, which reflects the unequal visibility of items in recommendation outputs. Through simulation studies, we find that positivity bias is disproportionately concentrated on popular items, further amplifying their over-exposure. Motivated by this insight, we adapt a percentile-based rating transformation as a pre-processing strategy to mitigate multifactorial bias. Experiments using six recommendation algorithms across four public datasets show that this approach improves exposure fairness with negligible accuracy loss. We also demonstrate that integrating this pre-processing step into post-processing fairness pipelines enhances their effectiveness and efficiency, enabling comparable or better fairness with reduced computational cost. These findings highlight the importance of addressing multifactorial bias and demonstrate the practical value of simple, data-driven pre-processing methods for improving fairness in recommender systems.",
        "url": "http://arxiv.org/abs/2601.12828v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12828v1",
        "arxiv_id": "2601.12828v1",
        "authors": [
            "Masoud Mansoury",
            "Jin Huang",
            "Mykola Pechenizkiy",
            "Herke van Hoof",
            "Maarten de Rijke"
        ],
        "submitted": "2026-01-19 08:37:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Multimodal Multi-Agent Empowered Legal Judgment Prediction",
        "abstract": "Legal Judgment Prediction (LJP) aims to predict the outcomes of legal cases based on factual descriptions, serving as a fundamental task to advance the development of legal systems. Traditional methods often rely on statistical analyses or role-based simulations but face challenges with multiple allegations, diverse evidence, and lack adaptability. In this paper, we introduce JurisMMA, a novel framework for LJP that effectively decomposes trial tasks, standardizes processes, and organizes them into distinct stages. Furthermore, we build JurisMM, a large dataset with over 100,000 recent Chinese judicial records, including both text and multimodal video-text data, enabling comprehensive evaluation. Experiments on JurisMM and the benchmark LawBench validate our framework's effectiveness. These results indicate that our framework is effective not only for LJP but also for a broader range of legal applications, offering new perspectives for the development of future legal methods and datasets.",
        "url": "http://arxiv.org/abs/2601.12815v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12815v1",
        "arxiv_id": "2601.12815v1",
        "authors": [
            "Zhaolu Kang",
            "Junhao Gong",
            "Qingxi Chen",
            "Hao Zhang",
            "Jiaxin Liu",
            "Rong Fu",
            "Zhiyuan Feng",
            "Yuan Wang",
            "Simon Fong",
            "Kaiyue Zhou"
        ],
        "submitted": "2026-01-19 08:21:46",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Do Clinical Question Answering Systems Really Need Specialised Medical Fine Tuning?",
        "abstract": "Clinical Question-Answering (CQA) industry systems are increasingly rely on Large Language Models (LLMs), yet their deployment is often guided by the assumption that domain-specific fine-tuning is essential. Although specialised medical LLMs such as BioBERT, BioGPT, and PubMedBERT remain popular, they face practical limitations including narrow coverage, high retraining costs, and limited adaptability. Efforts based on Supervised Fine-Tuning (SFT) have attempted to address these assumptions but continue to reinforce what we term the SPECIALISATION FALLACY-the belief that specialised medical LLMs are inherently superior for CQA. To address this assumption, we introduce MEDASSESS-X, a deployment-industry-oriented CQA framework that applies alignment at inference time rather than through SFT. MEDASSESS-X uses lightweight steering vectors to guide model activations toward medically consistent reasoning without updating model weights or requiring domain-specific retraining. This inference-time alignment layer stabilises CQA performance across both general-purpose and specialised medical LLMs, thereby resolving the SPECIALISATION FALLACY. Empirically, MEDASSESS-X delivers consistent gains across all LLM families, improving Accuracy by up to +6%, Factual Consistency by +7%, and reducing Safety Error Rate by as much as 50%.",
        "url": "http://arxiv.org/abs/2601.12812v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12812v1",
        "arxiv_id": "2601.12812v1",
        "authors": [
            "Sushant Kumar Ray",
            "Gautam Siddharth Kashyap",
            "Sahil Tripathi",
            "Nipun Joshi",
            "Vijay Govindarajan",
            "Rafiq Ali",
            "Jiechao Gao",
            "Usman Naseem"
        ],
        "submitted": "2026-01-19 08:17:55",
        "source": "arxiv",
        "comment": "Accepted at EACL 2026 (Industry Track)"
    },
    {
        "title": "SciHorizon-GENE: Benchmarking LLM for Life Sciences Inference from Gene Knowledge to Functional Understanding",
        "abstract": "Large language models (LLMs) have shown growing promise in biomedical research, particularly for knowledge-driven interpretation tasks. However, their ability to reliably reason from gene-level knowledge to functional understanding, However, their ability to reliably reason from gene-level knowledge to functional understanding, a core requirement for knowledge-enhanced cell atlas interpretation, remains largely underexplored. To address this gap, we introduce SciHorizon-GENE, a large-scale gene-centric benchmark constructed from authoritative biological databases. The benchmark integrates curated knowledge for over 190K human genes and comprises more than 540K questions covering diverse gene-to-function reasoning scenarios relevant to cell type annotation, functional interpretation, and mechanism-oriented analysis. Motivated by behavioral patterns observed in preliminary examinations, SciHorizon-GENE evaluates LLMs along four biologically critical perspectives: research attention sensitivity, hallucination tendency, answer completeness, and literature influence, explicitly targeting failure modes that limit the safe adoption of LLMs in biological interpretation pipelines. We systematically evaluate a wide range of state-of-the-art general-purpose and biomedical LLMs, revealing substantial heterogeneity in gene-level reasoning capabilities and persistent challenges in generating faithful, complete, and literature-grounded functional interpretations. Our benchmark establishes a systematic foundation for analyzing LLM behavior at the gene scale and offers insights for model selection and development, with direct relevance to knowledge-enhanced biological interpretation.",
        "url": "http://arxiv.org/abs/2601.12805v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12805v1",
        "arxiv_id": "2601.12805v1",
        "authors": [
            "Xiaohan Huang",
            "Meng Xiao",
            "Chuan Qin",
            "Qingqing Long",
            "Jinmiao Chen",
            "Yuanchun Zhou",
            "Hengshu Zhu"
        ],
        "submitted": "2026-01-19 08:06:35",
        "source": "arxiv",
        "comment": "16 pages"
    },
    {
        "title": "FRoM-W1: Towards General Humanoid Whole-Body Control with Language Instructions",
        "abstract": "Humanoid robots are capable of performing various actions such as greeting, dancing and even backflipping. However, these motions are often hard-coded or specifically trained, which limits their versatility. In this work, we present FRoM-W1, an open-source framework designed to achieve general humanoid whole-body motion control using natural language. To universally understand natural language and generate corresponding motions, as well as enable various humanoid robots to stably execute these motions in the physical world under gravity, FRoM-W1 operates in two stages: (a) H-GPT: utilizing massive human data, a large-scale language-driven human whole-body motion generation model is trained to generate diverse natural behaviors. We further leverage the Chain-of-Thought technique to improve the model's generalization in instruction understanding. (b) H-ACT: After retargeting generated human whole-body motions into robot-specific actions, a motion controller that is pretrained and further fine-tuned through reinforcement learning in physical simulation enables humanoid robots to accurately and stably perform corresponding actions. It is then deployed on real robots via a modular simulation-to-reality module. We extensively evaluate FRoM-W1 on Unitree H1 and G1 robots. Results demonstrate superior performance on the HumanML3D-X benchmark for human whole-body motion generation, and our introduced reinforcement learning fine-tuning consistently improves both motion tracking accuracy and task success rates of these humanoid robots. We open-source the entire FRoM-W1 framework and hope it will advance the development of humanoid intelligence.",
        "url": "http://arxiv.org/abs/2601.12799v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12799v1",
        "arxiv_id": "2601.12799v1",
        "authors": [
            "Peng Li",
            "Zihan Zhuang",
            "Yangfan Gao",
            "Yi Dong",
            "Sixian Li",
            "Changhao Jiang",
            "Shihan Dou",
            "Zhiheng Xi",
            "Enyu Zhou",
            "Jixuan Huang",
            "Hui Li",
            "Jingjing Gong",
            "Xingjun Ma",
            "Tao Gui",
            "Zuxuan Wu",
            "Qi Zhang",
            "Xuanjing Huang",
            "Yu-Gang Jiang",
            "Xipeng Qiu"
        ],
        "submitted": "2026-01-19 07:59:32",
        "source": "arxiv",
        "comment": "Project Page: https://openmoss.github.io/FRoM-W1"
    },
    {
        "title": "Open Vocabulary Panoptic Segmentation With Retrieval Augmentation",
        "abstract": "Given an input image and set of class names, panoptic segmentation aims to label each pixel in an image with class labels and instance labels. In comparison, Open Vocabulary Panoptic Segmentation aims to facilitate the segmentation of arbitrary classes according to user input. The challenge is that a panoptic segmentation system trained on a particular dataset typically does not generalize well to unseen classes beyond the training data. In this work, we propose RetCLIP, a retrieval-augmented panoptic segmentation method that improves the performance of unseen classes. In particular, we construct a masked segment feature database using paired image-text data. At inference time, we use masked segment features from the input image as query keys to retrieve similar features and associated class labels from the database. Classification scores for the masked segment are assigned based on the similarity between query features and retrieved features. The retrieval-based classification scores are combined with CLIP-based scores to produce the final output. We incorporate our solution with a previous SOTA method (FC-CLIP). When trained on COCO, the proposed method demonstrates 30.9 PQ, 19.3 mAP, 44.0 mIoU on the ADE20k dataset, achieving +4.5 PQ, +2.5 mAP, +10.0 mIoU absolute improvement over the baseline.",
        "url": "http://arxiv.org/abs/2601.12779v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12779v1",
        "arxiv_id": "2601.12779v1",
        "authors": [
            "Nafis Sadeq",
            "Qingfeng Liu",
            "Mostafa El-Khamy"
        ],
        "submitted": "2026-01-19 07:16:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Who Does This Name Remind You of? Nationality Prediction via Large Language Model Associative Memory",
        "abstract": "Large language models (LLMs) possess extensive world knowledge, yet methods for effectively eliciting this knowledge remain underexplored. Nationality and region prediction tasks require understanding of not only linguistic features but also cultural and historical background, making LLM world knowledge particularly valuable. However, conventional LLM prompting methods rely on direct reasoning approaches, which have limitations in applying abstract linguistic rules. We propose LLM Associative Memory Agents (LAMA), a novel framework that leverages LLM world knowledge as associative memory. Rather than directly inferring nationality from names, LAMA recalls famous individuals with the same name and aggregates their nationalities through indirect reasoning. A dual-agent architecture comprising a Person Agent and a Media Agent, specialized in different knowledge domains, recalls famous individuals in parallel, generating Top-1 predictions through voting and Top-K predictions through conditional completion. On a 99-country nationality prediction task, LAMA achieved 0.817 accuracy, substantially outperforming conventional LLM prompting methods and neural models. Our experiments reveal that LLMs exhibit higher reliability in recalling concrete examples than in abstract reasoning, that recall-based approaches are robust to low-frequency nationalities independent of data frequency distributions, and that the dual-agent architecture functions complementarily to produce synergistic effects. These results demonstrate the effectiveness of a new multi-agent system that retrieves and aggregates LLM knowledge rather than prompting reasoning.",
        "url": "http://arxiv.org/abs/2601.12771v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12771v1",
        "arxiv_id": "2601.12771v1",
        "authors": [
            "Keito Inoshita"
        ],
        "submitted": "2026-01-19 06:59:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "VISPA: Pluralistic Alignment via Automatic Value Selection and Activation",
        "abstract": "As large language models are increasingly used in high-stakes domains, it is essential that their outputs reflect not average} human preference, rather range of varying perspectives. Achieving such pluralism, however, remains challenging. Existing approaches consider limited values or rely on prompt-level interventions, lacking value control and representation. To address this, we introduce VISPA, a training-free pluralistic alignment framework, that enables direct control over value expression by dynamic selection and internal model activation steering. Across extensive empirical studies spanning multiple models and evaluation settings, we show VISPA is performant across all pluralistic alignment modes in healthcare and beyond. Further analysis reveals VISPA is adaptable with different steering initiations, model, and/or values. These results suggest that pluralistic alignment can be achieved through internal activation mechanisms, offering a scalable path toward language models that serves all.",
        "url": "http://arxiv.org/abs/2601.12758v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12758v1",
        "arxiv_id": "2601.12758v1",
        "authors": [
            "Shenyan Zheng",
            "Jiayou Zhong",
            "Anudeex Shetty",
            "Heng Ji",
            "Preslav Nakov",
            "Usman Naseem"
        ],
        "submitted": "2026-01-19 06:38:52",
        "source": "arxiv",
        "comment": "WIP"
    },
    {
        "title": "PAIR-SAFE: A Paired-Agent Approach for Runtime Auditing and Refining AI-Mediated Mental Health Support",
        "abstract": "Large language models (LLMs) are increasingly used for mental health support, yet they can produce responses that are overly directive, inconsistent, or clinically misaligned, particularly in sensitive or high-risk contexts. Existing approaches to mitigating these risks largely rely on implicit alignment through training or prompting, offering limited transparency and runtime accountability. We introduce PAIR-SAFE, a paired-agent framework for auditing and refining AI-generated mental health support that integrates a Responder agent with a supervisory Judge agent grounded in the clinically validated Motivational Interviewing Treatment Integrity (MITI-4) framework. The Judgeaudits each response and provides structuredALLOW or REVISE decisions that guide runtime response refinement. We simulate counseling interactions using a support-seeker simulator derived from human-annotated motivational interviewing data. We find that Judge-supervised interactions show significant improvements in key MITI dimensions, including Partnership, Seek Collaboration, and overall Relational quality. Our quantitative findings are supported by qualitative expert evaluation, which further highlights the nuances of runtime supervision. Together, our results reveal that such pairedagent approach can provide clinically grounded auditing and refinement for AI-assisted conversational mental health support.",
        "url": "http://arxiv.org/abs/2601.12754v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12754v1",
        "arxiv_id": "2601.12754v1",
        "authors": [
            "Jiwon Kim",
            "Violeta J. Rodriguez",
            "Dong Whi Yoo",
            "Eshwar Chandrasekharan",
            "Koustuv Saha"
        ],
        "submitted": "2026-01-19 06:20:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Towards Robust Process Reward Modeling via Noise-aware Learning",
        "abstract": "Process Reward Models (PRMs) have achieved strong results in complex reasoning, but are bottlenecked by costly process-level supervision. A widely used alternative, Monte Carlo Estimation (MCE), defines process rewards as the probability that a policy model reaches the correct final answer from a given reasoning step. However, step correctness is an intrinsic property of the reasoning trajectory, and should be invariant to policy choice. Our empirical findings show that MCE producing policy-dependent rewards that induce label noise, including false positives that reward incorrect steps and false negatives that penalize correct ones. To address above challenges, we propose a two-stage framework to mitigate noisy supervision. In the labeling stage, we introduce a reflection-aware label correction mechanism that uses a large language model (LLM) as a judge to detect reflection and self-correction behaviors related to the current reasoning step, thereby suppressing overestimated rewards. In the training stage, we further propose a \\underline{\\textbf{N}}oise-\\underline{\\textbf{A}}ware \\underline{\\textbf{I}}terative \\underline{\\textbf{T}}raining framework that enables the PRM to progressively refine noisy labels based on its own confidence. Extensive Experiments show that our method substantially improves step-level correctness discrimination, achieving up to a 27\\% absolute gain in average F1 over PRMs trained with noisy supervision.",
        "url": "http://arxiv.org/abs/2601.12748v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12748v1",
        "arxiv_id": "2601.12748v1",
        "authors": [
            "Bin Xie",
            "Bingbing Xu",
            "Xueyun Tian",
            "Yilin Chen",
            "Huawei Shen"
        ],
        "submitted": "2026-01-19 06:03:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Shared Geometry of Difficulty in Multilingual Language Models",
        "abstract": "Predicting problem-difficulty in large language models (LLMs) refers to estimating how difficult a task is according to the model itself, typically by training linear probes on its internal representations. In this work, we study the multilingual geometry of problem-difficulty in LLMs by training linear probes using the AMC subset of the Easy2Hard benchmark, translated into 21 languages. We found that difficulty-related signals emerge at two distinct stages of the model internals, corresponding to shallow (early-layers) and deep (later-layers) internal representations, that exhibit functionally different behaviors. Probes trained on deep representations achieve high accuracy when evaluated on the same language but exhibit poor cross-lingual generalization. In contrast, probes trained on shallow representations generalize substantially better across languages, despite achieving lower within-language performance. Together, these results suggest that LLMs first form a language-agnostic representation of problem difficulty, which subsequently becomes language-specific. This closely aligns with existing findings in LLM interpretability showing that models tend to operate in an abstract conceptual space before producing language-specific outputs. We demonstrate that this two-stage representational process extends beyond semantic content to high-level meta-cognitive properties such as problem-difficulty estimation.",
        "url": "http://arxiv.org/abs/2601.12731v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12731v1",
        "arxiv_id": "2601.12731v1",
        "authors": [
            "Stefano Civelli",
            "Pietro Bernardelle",
            "Nicolò Brunello",
            "Gianluca Demartini"
        ],
        "submitted": "2026-01-19 05:21:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Two-Stage GPU Kernel Tuner Combining Semantic Refactoring and Search-Based Optimization",
        "abstract": "GPU code optimization is a key performance bottleneck for HPC workloads as well as large-model training and inference. Although compiler optimizations and hand-written kernels can partially alleviate this issue, achieving near-hardware-limit performance still relies heavily on manual code refactoring and parameter tuning. Recent progress in LLM-agent-based kernel generation and optimization has been reported, yet many approaches primarily focus on direct code rewriting, where parameter choices are often implicit and hard to control, or require human intervention, leading to unstable performance gains. This paper introduces a template-based rewriting layer on top of an agent-driven iterative loop: kernels are semantically refactored into explicitly parameterizable templates, and template parameters are then optimized via search-based autotuning, yielding more stable and higher-quality speedups. Experiments on a set of real-world kernels demonstrate speedups exceeding 3x in the best case. We extract representative CUDA kernels from SGLang as evaluation targets; the proposed agentic tuner iteratively performs templating, testing, analysis, and planning, and leverages profiling feedback to execute constrained parameter search under hardware resource limits. Compared to agent-only direct rewriting, the template-plus-search design significantly reduces the randomness of iterative optimization, making the process more interpretable and enabling a more systematic approach toward high-performance configurations. The proposed method can be further extended to OpenCL, HIP, and other backends to deliver automated performance optimization for real production workloads.",
        "url": "http://arxiv.org/abs/2601.12698v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12698v1",
        "arxiv_id": "2601.12698v1",
        "authors": [
            "Qiuyi Qu",
            "Yicheng Sui",
            "Yufei Sun",
            "Rui Chen",
            "Xiaofei Zhang",
            "Yuzhi Zhang",
            "Haofeng Wang",
            "Ge Lan",
            "Ning Zhang"
        ],
        "submitted": "2026-01-19 03:40:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "UbuntuGuard: A Culturally-Grounded Policy Benchmark for Equitable AI Safety in African Languages",
        "abstract": "Current guardian models are predominantly Western-centric and optimized for high-resource languages, leaving low-resource African languages vulnerable to evolving harms, cross-lingual safety failures, and cultural misalignment. Moreover, most guardian models rely on rigid, predefined safety categories that fail to generalize across diverse linguistic and sociocultural contexts. Robust safety, therefore, requires flexible, runtime-enforceable policies and benchmarks that reflect local norms, harm scenarios, and cultural expectations. We introduce UbuntuGuard, the first African policy-based safety benchmark built from adversarial queries authored by 155 domain experts across sensitive fields, including healthcare. From these expert-crafted queries, we derive context-specific safety policies and reference responses that capture culturally grounded risk signals, enabling policy-aligned evaluation of guardian models. We evaluate 13 models, comprising six general-purpose LLMs and seven guardian models across three distinct variants: static, dynamic, and multilingual. Our findings reveal that existing English-centric benchmarks overestimate real-world multilingual safety, cross-lingual transfer provides partial but insufficient coverage, and dynamic models, while better equipped to leverage policies at inference time, still struggle to fully localize African-language contexts. These findings highlight the urgent need for multilingual, culturally grounded safety benchmarks to enable the development of reliable and equitable guardian models for low-resource languages. Our code can be found online.\\footnote{Code repository available at https://github.com/hemhemoh/UbuntuGuard.",
        "url": "http://arxiv.org/abs/2601.12696v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12696v1",
        "arxiv_id": "2601.12696v1",
        "authors": [
            "Tassallah Abdullahi",
            "Macton Mgonzo",
            "Mardiyyah Oduwole",
            "Paul Okewunmi",
            "Abraham Owodunni",
            "Ritambhara Singh",
            "Carsten Eickhoff"
        ],
        "submitted": "2026-01-19 03:37:56",
        "source": "arxiv",
        "comment": "12 pages"
    },
    {
        "title": "HyFormer: Revisiting the Roles of Sequence Modeling and Feature Interaction in CTR Prediction",
        "abstract": "Industrial large-scale recommendation models (LRMs) face the challenge of jointly modeling long-range user behavior sequences and heterogeneous non-sequential features under strict efficiency constraints. However, most existing architectures employ a decoupled pipeline: long sequences are first compressed with a query-token based sequence compressor like LONGER, followed by fusion with dense features through token-mixing modules like RankMixer, which thereby limits both the representation capacity and the interaction flexibility. This paper presents HyFormer, a unified hybrid transformer architecture that tightly integrates long-sequence modeling and feature interaction into a single backbone. From the perspective of sequence modeling, we revisit and redesign query tokens in LRMs, and frame the LRM modeling task as an alternating optimization process that integrates two core components: Query Decoding which expands non-sequential features into Global Tokens and performs long sequence decoding over layer-wise key-value representations of long behavioral sequences; and Query Boosting which enhances cross-query and cross-sequence heterogeneous interactions via efficient token mixing. The two complementary mechanisms are performed iteratively to refine semantic representations across layers. Extensive experiments on billion-scale industrial datasets demonstrate that HyFormer consistently outperforms strong LONGER and RankMixer baselines under comparable parameter and FLOPs budgets, while exhibiting superior scaling behavior with increasing parameters and FLOPs. Large-scale online A/B tests in high-traffic production systems further validate its effectiveness, showing significant gains over deployed state-of-the-art models. These results highlight the practicality and scalability of HyFormer as a unified modeling framework for industrial LRMs.",
        "url": "http://arxiv.org/abs/2601.12681v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12681v1",
        "arxiv_id": "2601.12681v1",
        "authors": [
            "Yunwen Huang",
            "Shiyong Hong",
            "Xijun Xiao",
            "Jinqiu Jin",
            "Xuanyuan Luo",
            "Zhe Wang",
            "Zheng Chai",
            "Shikang Wu",
            "Yuchao Zheng",
            "Jingjian Lin"
        ],
        "submitted": "2026-01-19 02:55:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Augmenting Question Answering with A Hybrid RAG Approach",
        "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful technique for enhancing the quality of responses in Question-Answering (QA) tasks. However, existing approaches often struggle with retrieving contextually relevant information, leading to incomplete or suboptimal answers. In this paper, we introduce Structured-Semantic RAG (SSRAG), a hybrid architecture that enhances QA quality by integrating query augmentation, agentic routing, and a structured retrieval mechanism combining vector and graph based techniques with context unification. By refining retrieval processes and improving contextual grounding, our approach improves both answer accuracy and informativeness. We conduct extensive evaluations on three popular QA datasets, TruthfulQA, SQuAD and WikiQA, across five Large Language Models (LLMs), demonstrating that our proposed approach consistently improves response quality over standard RAG implementations.",
        "url": "http://arxiv.org/abs/2601.12658v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12658v1",
        "arxiv_id": "2601.12658v1",
        "authors": [
            "Tianyi Yang",
            "Nashrah Haque",
            "Vaishnave Jonnalagadda",
            "Yuya Jeremy Ong",
            "Zhehui Chen",
            "Yanzhao Wu",
            "Lei Yu",
            "Divyesh Jadav",
            "Wenqi Wei"
        ],
        "submitted": "2026-01-19 02:08:47",
        "source": "arxiv",
        "comment": "10 pages, 5 tables, 2 figures; presented at IEEE CogMI 2025"
    },
    {
        "title": "Intelligent Documentation in Medical Education: Can AI Replace Manual Case Logging?",
        "abstract": "Procedural case logs are a core requirement in radiology training, yet they are time-consuming to complete and prone to inconsistency when authored manually. This study investigates whether large language models (LLMs) can automate procedural case log documentation directly from free-text radiology reports. We evaluate multiple local and commercial LLMs under instruction-based and chain-of-thought prompting to extract structured procedural information from 414 curated interventional radiology reports authored by nine residents between 2018 and 2024. Model performance is assessed using sensitivity, specificity, and F1-score, alongside inference latency and token efficiency to estimate operational cost. Results show that both local and commercial models achieve strong extraction performance, with best F1-scores approaching 0.87, while exhibiting different trade-offs between speed and cost. Automation using LLMs has the potential to substantially reduce clerical burden for trainees and improve consistency in case logging. These findings demonstrate the feasibility of AI-assisted documentation in medical education and highlight the need for further validation across institutions and clinical workflows.",
        "url": "http://arxiv.org/abs/2601.12648v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12648v1",
        "arxiv_id": "2601.12648v1",
        "authors": [
            "Nafiz Imtiaz Khan",
            "Kylie Cleland",
            "Vladimir Filkov",
            "Roger Eric Goldman"
        ],
        "submitted": "2026-01-19 01:45:51",
        "source": "arxiv",
        "comment": "51 pages, 12 figures, 8 tables. Feasibility study using retrospective radiology reports. Submitted to JAMIA Open (under review)"
    },
    {
        "title": "Objective Matters: Fine-Tuning Objectives Shape Safety, Robustness, and Persona Drift",
        "abstract": "Fine-tuning LLMs on benign data can still degrade alignment and adversarial robustness, yet direct analysis of the role of fine-tuning objectives in shaping these safety outcomes remain limited. We present a controlled comparison of six fine-tuning objectives -- Supervised Fine-Tuning, Direct Preference Optimization, Conditional Fine-Tuning, Inoculation Prompting, Odds Ratio Preference Optimization, and KL-regularized fine-tuning -- holding data, domain, architecture, and optimization fixed. Across closed-form reasoning and open-ended generation tasks, we find that objective choice induces systematic, scale-dependent shifts along the safety-capability frontier. At small training budgets, robustness is similar across objectives but capability differs. At larger budgets, objectives diverge sharply: supervised and preference-based tuning tightly couple capability gains to increased adversarial vulnerability and persona drift, while objectives that constrain learning signals -- especially ORPO and KL-regularization -- substantially mitigate both. Fine-tuning objectives therefore matter little for safety at small scales but become a primary driver of adversarial robustness and latent persona stability as training scale increases.",
        "url": "http://arxiv.org/abs/2601.12639v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12639v1",
        "arxiv_id": "2601.12639v1",
        "authors": [
            "Daniel Vennemeyer",
            "Punya Syon Pandey",
            "Phan Anh Duong",
            "Michael Umeokoli",
            "Samuel Ratnam"
        ],
        "submitted": "2026-01-19 01:04:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "BioPulse-QA: A Dynamic Biomedical Question-Answering Benchmark for Evaluating Factuality, Robustness, and Bias in Large Language Models",
        "abstract": "Objective: Large language models (LLMs) are increasingly applied in biomedical settings, and existing benchmark datasets have played an important role in supporting model development and evaluation. However, these benchmarks often have limitations. Many rely on static or outdated datasets that fail to capture the dynamic, context-rich, and high-stakes nature of biomedical knowledge. They also carry increasing risk of data leakage due to overlap with model pretraining corpora and often overlook critical dimensions such as robustness to linguistic variation and potential demographic biases.\n  Materials and Methods: To address these gaps, we introduce BioPulse-QA, a benchmark that evaluates LLMs on answering questions from newly published biomedical documents including drug labels, trial protocols, and clinical guidelines. BioPulse-QA includes 2,280 expert-verified question answering (QA) pairs and perturbed variants, covering both extractive and abstractive formats. We evaluate four LLMs - GPT-4o, GPT-o1, Gemini-2.0-Flash, and LLaMA-3.1 8B Instruct - released prior to the publication dates of the benchmark documents.\n  Results: GPT-o1 achieves the highest relaxed F1 score (0.92), followed by Gemini-2.0-Flash (0.90) on drug labels. Clinical trials are the most challenging source, with extractive F1 scores as low as 0.36.\n  Discussion and Conclusion: Performance differences are larger for paraphrasing than for typographical errors, while bias testing shows negligible differences. BioPulse-QA provides a scalable and clinically relevant framework for evaluating biomedical LLMs.",
        "url": "http://arxiv.org/abs/2601.12632v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12632v1",
        "arxiv_id": "2601.12632v1",
        "authors": [
            "Kriti Bhattarai",
            "Vipina K. Keloth",
            "Donald Wright",
            "Andrew Loza",
            "Yang Ren",
            "Hua Xu"
        ],
        "submitted": "2026-01-19 00:38:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Disagreement as Data: Reasoning Trace Analytics in Multi-Agent Systems",
        "abstract": "Learning analytics researchers often analyze qualitative student data such as coded annotations or interview transcripts to understand learning processes. With the rise of generative AI, fully automated and human-AI workflows have emerged as promising methods for analysis. However, methodological standards to guide such workflows remain limited. In this study, we propose that reasoning traces generated by large language model (LLM) agents, especially within multi-agent systems, constitute a novel and rich form of process data to enhance interpretive practices in qualitative coding. We apply cosine similarity to LLM reasoning traces to systematically detect, quantify, and interpret disagreements among agents, reframing disagreement as a meaningful analytic signal. Analyzing nearly 10,000 instances of agent pairs coding human tutoring dialog segments, we show that LLM agents' semantic reasoning similarity robustly differentiates consensus from disagreement and correlates with human coding reliability. Qualitative analysis guided by this metric reveals nuanced instructional sub-functions within codes and opportunities for conceptual codebook refinement. By integrating quantitative similarity metrics with qualitative review, our method has the potential to improve and accelerate establishing inter-rater reliability during coding by surfacing interpretive ambiguity, especially when LLMs collaborate with humans. We discuss how reasoning-trace disagreements represent a valuable new class of analytic signals advancing methodological rigor and interpretive depth in educational research.",
        "url": "http://arxiv.org/abs/2601.12618v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12618v1",
        "arxiv_id": "2601.12618v1",
        "authors": [
            "Elham Tajik",
            "Conrad Borchers",
            "Bahar Shahrokhian",
            "Sebastian Simon",
            "Ali Keramati",
            "Sonika Pal",
            "Sreecharan Sankaranarayanan"
        ],
        "submitted": "2026-01-18 23:19:49",
        "source": "arxiv",
        "comment": "LAK 2026 conference paper, 7 pages"
    },
    {
        "title": "A Cloud-based Multi-Agentic Workflow for Science",
        "abstract": "As Large Language Models (LLMs) become ubiquitous across various scientific domains, their lack of ability to perform complex tasks like running simulations or to make complex decisions limits their utility. LLM-based agents bridge this gap due to their ability to call external resources and tools and thus are now rapidly gaining popularity. However, coming up with a workflow that can balance the models, cloud providers, and external resources is very challenging, making implementing an agentic system more of a hindrance than a help. In this work, we present a domain-agnostic, model-independent workflow for an agentic framework that can act as a scientific assistant while being run entirely on cloud. Built with a supervisor agent marshaling an array of agents with individual capabilities, our framework brings together straightforward tasks like literature review and data analysis with more complex ones like simulation runs. We describe the framework here in full, including a proof-of-concept system we built to accelerate the study of Catalysts, which is highly important in the field of Chemistry and Material Science. We report the cost to operate and use this framework, including the breakdown of the cost by services use. We also evaluate our system on a custom-curated synthetic benchmark and a popular Chemistry benchmark, and also perform expert validation of the system. The results show that our system is able to route the task to the correct agent 90% of the time and successfully complete the assigned task 97.5% of the time for the synthetic tasks and 91% of the time for real-world tasks, while still achieving better or comparable accuracy to most frontier models, showing that this is a viable framework for other scientific domains to replicate.",
        "url": "http://arxiv.org/abs/2601.12607v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12607v1",
        "arxiv_id": "2601.12607v1",
        "authors": [
            "Anurag Acharya",
            "Timothy Vega",
            "Rizwan A. Ashraf",
            "Anshu Sharma",
            "Derek Parker",
            "Robert Rallo"
        ],
        "submitted": "2026-01-18 22:37:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SSVD-O: Parameter-Efficient Fine-Tuning with Structured SVD for Speech Recognition",
        "abstract": "Parameter-efficient fine-tuning (PEFT) is a scalable approach for adapting large speech foundation models to new domains. While methods such as LoRA and its state-of-the-art variants reduce adaptation costs, they typically allocate parameters uniformly across model subspaces, which limits their efficiency and scalability in speech applications. Building on our prior work, this paper introduces SSVD-Outer (SSVD-O), an extension of the structured SVD-guided (SSVD) fine-tuning method. SSVD-O combines input acoustic feature space-associated inner transformations with output semantic feature space-associated outer transformations to enable scalable and balanced adaptation. We conduct the first systematic analysis of parameter budget allocation across model subspaces in PEFT for automatic speech recognition (ASR), and investigate the trade-off between learning and forgetting under constrained resources. SSVD-O is benchmarked against LoRA, DoRA, PiSSA, and SSVD on domain-shifted ASR tasks, including child speech and regional accents, across model scales from 0.1B to 2B within the ESPnet framework. Experimental results show that SSVD-O consistently narrows the performance gap to full fine-tuning while improving generalization and mitigating catastrophic forgetting.",
        "url": "http://arxiv.org/abs/2601.12600v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12600v1",
        "arxiv_id": "2601.12600v1",
        "authors": [
            "Pu Wang",
            "Shinji Watanabe",
            "Hugo Van hamme"
        ],
        "submitted": "2026-01-18 22:16:44",
        "source": "arxiv",
        "comment": "Accepted by IEEE ICASSP 2026"
    },
    {
        "title": "Dissecting Linear Recurrent Models: How Different Gating Strategies Drive Selectivity and Generalization",
        "abstract": "Linear recurrent neural networks have emerged as efficient alternatives to the original Transformer's softmax attention mechanism, thanks to their highly parallelizable training and constant memory and computation requirements at inference. Iterative refinements of these models have introduced an increasing number of architectural mechanisms, leading to increased complexity and computational costs. Nevertheless, systematic direct comparisons among these models remain limited. Existing benchmark tasks are either too simplistic to reveal substantial differences or excessively resource-intensive for experimentation. In this work, we propose a refined taxonomy of linear recurrent models and introduce SelectivBench, a set of lightweight and customizable synthetic benchmark tasks for systematically evaluating sequence models. SelectivBench specifically evaluates selectivity in sequence models at small to medium scale, such as the capacity to focus on relevant inputs while ignoring context-based distractors. It employs rule-based grammars to generate sequences with adjustable complexity, incorporating irregular gaps that intentionally violate transition rules. Evaluations of linear recurrent models on SelectivBench reveal performance patterns consistent with results from large-scale language tasks. Our analysis clarifies the roles of essential architectural features: gating and rapid forgetting mechanisms facilitate recall, in-state channel mixing is unnecessary for selectivity, but critical for generalization, and softmax attention remains dominant due to its memory capacity scaling with sequence length. Our benchmark enables targeted, efficient exploration of linear recurrent models and provides a controlled setting for studying behaviors observed in large-scale evaluations. Code is available at https://github.com/symseqbench/selectivbench",
        "url": "http://arxiv.org/abs/2601.12598v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12598v1",
        "arxiv_id": "2601.12598v1",
        "authors": [
            "Younes Bouhadjar",
            "Maxime Fabre",
            "Felix Schmidt",
            "Emre Neftci"
        ],
        "submitted": "2026-01-18 21:49:21",
        "source": "arxiv",
        "comment": "11 pages, 4 figures and 4 tables"
    },
    {
        "title": "Evaluating Contextually Mediated Factual Recall in Multilingual Large Language Models",
        "abstract": "Large language models (LLMs) can recall a wide range of factual knowledge across languages. However, existing factual recall evaluations primarily assess fact retrieval in isolation, where the queried entity is explicitly named and the fact is requested directly. In natural language use, facts are often accessed through context, where the relevant entity is introduced only indirectly. In this work, we study contextually mediated factual recall, asking whether LLMs can reliably retrieve factual knowledge when the target entity is embedded in a naturalistic context rather than queried explicitly, across languages. We construct controlled prompts that preserve the underlying fact while introducing referential mediation through contextual sentences. To disentangle contextual effects from name-specific associations, we further compare performance using synthetic names and real names across languages. Evaluating multiple model families in five languages, we find that contextual mediation consistently degrades factual recall, with substantial variation across relations. Larger models are more robust to contextual mediation, exhibiting a reduced performance gap relative to direct queries, while the effect of real names and name origin is mixed and unsystematic. These findings highlight a gap between isolated factual recall and context-dependent language understanding in multilingual LLMs.",
        "url": "http://arxiv.org/abs/2601.12555v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12555v1",
        "arxiv_id": "2601.12555v1",
        "authors": [
            "Yihong Liu",
            "Bingyu Xiong",
            "Hinrich Schütze"
        ],
        "submitted": "2026-01-18 19:38:55",
        "source": "arxiv",
        "comment": "preprint"
    },
    {
        "title": "Benchmarking Concept-Spilling Across Languages in LLMs",
        "abstract": "Multilingual Large Language Models (LLMs) exhibit remarkable cross-lingual abilities, yet often exhibit a systematic bias toward the representations from other languages, resulting in semantic interference when generating content in non-English languages$-$a phenomenon we define as language spilling. This paper presents a novel comparative framework for evaluating multilingual semantic robustness by systematically measuring how models handle polysemous words across languages. Our methodology provides a relative measure of model performance: when required to generate exactly five meanings, both strong and weak models may resort to meanings from dominant languages, but semantically stronger models do so later in the generation sequence, producing more true meanings from the target language before failing, while weaker models resort to dominant-language meanings earlier in the sequence. We evaluate a diverse set of open and closed multilingual LLMs using a structured meaning generation task across nine languages, employing a carefully curated benchmark of 100 high-polysemy English words. Our findings reveal significant variation in semantic robustness across both models and languages, providing a principled ranking system for model comparison without requiring definitive causal attribution of error sources. We contribute both a scalable comparative benchmark for multilingual semantic evaluation and a rigorous validation pipeline$-$critical tools for developing more linguistically balanced AI systems.",
        "url": "http://arxiv.org/abs/2601.12549v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12549v1",
        "arxiv_id": "2601.12549v1",
        "authors": [
            "Ilia Badanin",
            "Daniil Dzenhaliou",
            "Imanol Schlag"
        ],
        "submitted": "2026-01-18 19:28:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Information Farming: From Berry Picking to Berry Growing",
        "abstract": "The classic paradigms of Berry Picking and Information Foraging Theory have framed users as gatherers, opportunistically searching across distributed sources to satisfy evolving information needs. However, the rise of GenAI is driving a fundamental transformation in how people produce, structure, and reuse information - one that these paradigms no longer fully capture. This transformation is analogous to the Neolithic Revolution, when societies shifted from hunting and gathering to cultivation. Generative technologies empower users to \"farm\" information by planting seeds in the form of prompts, cultivating workflows over time, and harvesting richly structured, relevant yields within their own plots, rather than foraging across others people's patches. In this perspectives paper, we introduce the notion of Information Farming as a conceptual framework and argue that it represents a natural evolution in how people engage with information. Drawing on historical analogy and empirical evidence, we examine the benefits and opportunities of information farming, its implications for design and evaluation, and the accompanying risks posed by this transition. We hypothesize that as GenAI technologies proliferate, cultivating information will increasingly supplant transient, patch-based foraging as a dominant mode of engagement, marking a broader shift in human-information interaction and its study.",
        "url": "http://arxiv.org/abs/2601.12544v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12544v1",
        "arxiv_id": "2601.12544v1",
        "authors": [
            "Leif Azzopardi",
            "Adam Roegiest"
        ],
        "submitted": "2026-01-18 19:16:15",
        "source": "arxiv",
        "comment": "ACM CHIIR 2026"
    },
    {
        "title": "MemeLens: Multilingual Multitask VLMs for Memes",
        "abstract": "Memes are a dominant medium for online communication and manipulation because meaning emerges from interactions between embedded text, imagery, and cultural context. Existing meme research is distributed across tasks (hate, misogyny, propaganda, sentiment, humour) and languages, which limits cross-domain generalization. To address this gap we propose MemeLens, a unified multilingual and multitask explanation-enhanced Vision Language Model (VLM) for meme understanding. We consolidate 38 public meme datasets, filter and map dataset-specific labels into a shared taxonomy of $20$ tasks spanning harm, targets, figurative/pragmatic intent, and affect. We present a comprehensive empirical analysis across modeling paradigms, task categories, and datasets. Our findings suggest that robust meme understanding requires multimodal training, exhibits substantial variation across semantic categories, and remains sensitive to over-specialization when models are fine-tuned on individual datasets rather than trained in a unified setting. We will make the experimental resources and datasets publicly available for the community.",
        "url": "http://arxiv.org/abs/2601.12539v1",
        "pdf_url": "https://arxiv.org/pdf/2601.12539v1",
        "arxiv_id": "2601.12539v1",
        "authors": [
            "Ali Ezzat Shahroor",
            "Mohamed Bayan Kmainasi",
            "Abul Hasnat",
            "Dimitar Dimitrov",
            "Giovanni Da San Martino",
            "Preslav Nakov",
            "Firoj Alam"
        ],
        "submitted": "2026-01-18 19:01:03",
        "source": "arxiv",
        "comment": "disinformation, misinformation, factuality, harmfulness, fake news, propaganda, hateful meme, multimodality, text, images"
    }
]