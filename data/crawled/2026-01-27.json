[
    {
        "title": "ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models",
        "abstract": "Text embeddings have become an essential part of a variety of language applications. However, methods for interpreting, exploring and reversing embedding spaces are limited, reducing transparency and precluding potentially valuable generative use cases. In this work, we align Large Language Models to embeddings of clinical trials using the recently reported Embedding Language Model (ELM) method. We develop an open-source, domain-agnostic ELM architecture and training framework, design training tasks for clinical trials, and introduce an expert-validated synthetic dataset. We then train a series of ELMs exploring the impact of tasks and training regimes. Our final model, ctELM, can accurately describe and compare unseen clinical trials from embeddings alone and produce plausible clinical trials from novel vectors. We further show that generated trial abstracts are responsive to moving embeddings along concept vectors for age and sex of study subjects. Our public ELM implementation and experimental results will aid the alignment of Large Language Models to embedding spaces in the biomedical domain and beyond.",
        "url": "http://arxiv.org/abs/2601.18796v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18796v1",
        "arxiv_id": "2601.18796v1",
        "authors": [
            "Brian Ondov",
            "Chia-Hsuan Chang",
            "Yujia Zhou",
            "Mauro Giuffrè",
            "Hua Xu"
        ],
        "submitted": "2026-01-26 18:58:46",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes",
        "abstract": "Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.",
        "url": "http://arxiv.org/abs/2601.18795v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18795v1",
        "arxiv_id": "2601.18795v1",
        "authors": [
            "Amrith Setlur",
            "Zijian Wang",
            "Andrew Cohen",
            "Paria Rashidinejad",
            "Sang Michael Xie"
        ],
        "submitted": "2026-01-26 18:57:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MEGnifying Emotion: Sentiment Analysis from Annotated Brain Data",
        "abstract": "Decoding emotion from brain activity could unlock a deeper understanding of the human experience. While a number of existing datasets align brain data with speech and with speech transcripts, no datasets have annotated brain data with sentiment. To bridge this gap, we explore the use of pre-trained Text-to-Sentiment models to annotate non invasive brain recordings, acquired using magnetoencephalography (MEG), while participants listened to audiobooks. Having annotated the text, we employ force-alignment of the text and audio to align our sentiment labels with the brain recordings. It is straightforward then to train Brainto-Sentiment models on these data. Experimental results show an improvement in balanced accuracy for Brain-to-Sentiment compared to baseline, supporting the proposed approach as a proof-of-concept for leveraging existing MEG datasets and learning to decode sentiment directly from the brain.",
        "url": "http://arxiv.org/abs/2601.18792v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18792v1",
        "arxiv_id": "2601.18792v1",
        "authors": [
            "Brian Liu",
            "Oiwi Parker Jones"
        ],
        "submitted": "2026-01-26 18:55:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Subword-Based Comparative Linguistics across 242 Languages Using Wikipedia Glottosets",
        "abstract": "We present a large-scale comparative study of 242 Latin and Cyrillic-script languages using subword-based methodologies. By constructing 'glottosets' from Wikipedia lexicons, we introduce a framework for simultaneous cross-linguistic comparison via Byte-Pair Encoding (BPE). Our approach utilizes rank-based subword vectors to analyze vocabulary overlap, lexical divergence, and language similarity at scale. Evaluations demonstrate that BPE segmentation aligns with morpheme boundaries 95% better than random baseline across 15 languages (F1 = 0.34 vs 0.15). BPE vocabulary similarity correlates significantly with genetic language relatedness (Mantel r = 0.329, p < 0.001), with Romance languages forming the tightest cluster (mean distance 0.51) and cross-family pairs showing clear separation (0.82). Analysis of 26,939 cross-linguistic homographs reveals that 48.7% receive different segmentations across related languages, with variation correlating to phylogenetic distance. Our results provide quantitative macro-linguistic insights into lexical patterns across typologically diverse languages within a unified analytical framework.",
        "url": "http://arxiv.org/abs/2601.18791v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18791v1",
        "arxiv_id": "2601.18791v1",
        "authors": [
            "Iaroslav Chelombitko",
            "Mika Hämäläinen",
            "Aleksey Komissarov"
        ],
        "submitted": "2026-01-26 18:55:28",
        "source": "arxiv",
        "comment": "15 pages, 4 figues, 4 tables"
    },
    {
        "title": "MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts",
        "abstract": "Large Language Models are increasingly optimized for deep reasoning, prioritizing the correct execution of complex tasks over general conversation. We investigate whether this focus on calculation creates a \"tunnel vision\" that ignores safety in critical situations. We introduce MortalMATH, a benchmark of 150 scenarios where users request algebra help while describing increasingly life-threatening emergencies (e.g., stroke symptoms, freefall). We find a sharp behavioral split: generalist models (like Llama-3.1) successfully refuse the math to address the danger. In contrast, specialized reasoning models (like Qwen-3-32b and GPT-5-nano) often ignore the emergency entirely, maintaining over 95 percent task completion rates while the user describes dying. Furthermore, the computational time required for reasoning introduces dangerous delays: up to 15 seconds before any potential help is offered. These results suggest that training models to relentlessly pursue correct answers may inadvertently unlearn the survival instincts required for safe deployment.",
        "url": "http://arxiv.org/abs/2601.18790v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18790v1",
        "arxiv_id": "2601.18790v1",
        "authors": [
            "Etienne Lanzeray",
            "Stephane Meilliez",
            "Malo Ruelle",
            "Damien Sileo"
        ],
        "submitted": "2026-01-26 18:55:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Unsupervised Text Segmentation via Kernel Change-Point Detection on Sentence Embeddings",
        "abstract": "Unsupervised text segmentation is crucial because boundary labels are expensive, subjective, and often fail to transfer across domains and granularity choices. We propose Embed-KCPD, a training-free method that represents sentences as embedding vectors and estimates boundaries by minimizing a penalized KCPD objective. Beyond the algorithmic instantiation, we develop, to our knowledge, the first dependence-aware theory for KCPD under $m$-dependent sequences, a finite-memory abstraction of short-range dependence common in language. We prove an oracle inequality for the population penalized risk and a localization guarantee showing that each true change point is recovered within a window that is small relative to segment length. To connect theory to practice, we introduce an LLM-based simulation framework that generates synthetic documents with controlled finite-memory dependence and known boundaries, validating the predicted scaling behavior. Across standard segmentation benchmarks, Embed-KCPD often outperforms strong unsupervised baselines. A case study on Taylor Swift's tweets illustrates that Embed-KCPD combines strong theoretical guarantees, simulated reliability, and practical effectiveness for text segmentation.",
        "url": "http://arxiv.org/abs/2601.18788v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18788v1",
        "arxiv_id": "2601.18788v1",
        "authors": [
            "Mumin Jia",
            "Jairo Diaz-Rodriguez"
        ],
        "submitted": "2026-01-26 18:54:34",
        "source": "arxiv",
        "comment": "arXiv admin note: substantial text overlap with arXiv:2510.03437. substantial text overlap with arXiv:2510.03437. substantial text overlap with arXiv:2510.03437. substantial text overlap with arXiv:2510.03437"
    },
    {
        "title": "Design Techniques for LLM-Powered Interactive Storytelling: A Case Study of the Dramamancer System",
        "abstract": "The rise of Large Language Models (LLMs) has enabled a new paradigm for bridging authorial intent and player agency in interactive narrative. We consider this paradigm through the example of Dramamancer, a system that uses an LLM to transform author-created story schemas into player-driven playthroughs. This extended abstract outlines some design techniques and evaluation considerations associated with this system.",
        "url": "http://arxiv.org/abs/2601.18785v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18785v1",
        "arxiv_id": "2601.18785v1",
        "authors": [
            "Tiffany Wang",
            "Yuqian Sun",
            "Yi Wang",
            "Melissa Roemmele",
            "John Joon Young Chung",
            "Max Kreminski"
        ],
        "submitted": "2026-01-26 18:51:20",
        "source": "arxiv",
        "comment": "Extended abstract presented at the 2025 Wordplay Workshop at EMNLP"
    },
    {
        "title": "POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration",
        "abstract": "Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.",
        "url": "http://arxiv.org/abs/2601.18779v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18779v1",
        "arxiv_id": "2601.18779v1",
        "authors": [
            "Yuxiao Qu",
            "Amrith Setlur",
            "Virginia Smith",
            "Ruslan Salakhutdinov",
            "Aviral Kumar"
        ],
        "submitted": "2026-01-26 18:47:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability",
        "abstract": "Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.",
        "url": "http://arxiv.org/abs/2601.18778v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18778v1",
        "arxiv_id": "2601.18778v1",
        "authors": [
            "Shobhita Sundaram",
            "John Quan",
            "Ariel Kwiatkowski",
            "Kartik Ahuja",
            "Yann Ollivier",
            "Julia Kempe"
        ],
        "submitted": "2026-01-26 18:46:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation",
        "abstract": "Evaluating the quality of search, ranking and RAG systems traditionally requires a significant number of human relevance annotations. In recent times, several deployed systems have explored the usage of Large Language Models (LLMs) as automated judges for this task while their inherent biases prevent direct use for metric estimation. We present a statistical framework extending Prediction-Powered Inference (PPI) that combines minimal human annotations with LLM judgments to produce reliable estimates of metrics which require sub-instance annotations. Our method requires as few as 100 human-annotated queries and 10,000 unlabeled examples, reducing annotation requirements significantly compared to traditional approaches. We formulate our proposed framework (PRECISE) for inference of relevance uplift for an LLM-based query reformulation application, extending PPI to sub-instance annotations at the query-document level. By reformulating the metric-integration space, we reduced the computational complexity from O(2^|C|) to O(2^K), where |C| represents corpus size (in order of millions). Detailed experiments across prominent retrieval datasets demonstrate that our method reduces the variance of estimates for the business-critical Precision@K metric, while effectively correcting for LLM bias in low-resource settings.",
        "url": "http://arxiv.org/abs/2601.18777v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18777v1",
        "arxiv_id": "2601.18777v1",
        "authors": [
            "Abhishek Divekar",
            "Anirban Majumder"
        ],
        "submitted": "2026-01-26 18:46:49",
        "source": "arxiv",
        "comment": "Accepted at AAAI 2026 - Innovative Applications of AI (IAAI-26)"
    },
    {
        "title": "Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs' ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.",
        "url": "http://arxiv.org/abs/2601.18771v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18771v1",
        "arxiv_id": "2601.18771v1",
        "authors": [
            "Yanming Liu",
            "Xinyue Peng",
            "Zixuan Yan",
            "Yanxin Shen",
            "Wenjie Xu",
            "Yuefeng Huang",
            "Xinyi Wang",
            "Jiannan Cao",
            "Jianwei Yin",
            "Xuhong Zhang"
        ],
        "submitted": "2026-01-26 18:42:33",
        "source": "arxiv",
        "comment": "Dep-Search 1st version"
    },
    {
        "title": "Beyond Preferences: Learning Alignment Principles Grounded in Human Reasons and Values",
        "abstract": "A crucial consideration when developing and deploying Large Language Models (LLMs) is the human values to which these models are aligned. In the constitutional framework of alignment models are aligned to a set of principles (the constitution) specified in natural language. However, it is unclear how to fairly determine this constitution with widespread stakeholder input. In this work we propose Grounded Constitutional AI (GCAI), a unified framework for generating constitutions of principles that are representative of both users' general expectations toward AI (general principles) and their interaction-time preferences (contextual principles). We extend the Inverse Constitutional AI (ICAI) approach to generate contextual principles from human preference annotation data by leveraging human-provided \\textit{reasons} for their preferences. We supplement these contextual principles with general principles surfaced from user statements of \\textit{values} regarding AI. We show that a constitution generated by GCAI is preferred by humans over one generated through ICAI both personally, and for widespread use in governing AI behavior. Additionally participants consider the GCAI constitution to be more morally grounded, coherent, and pluralistic.",
        "url": "http://arxiv.org/abs/2601.18760v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18760v1",
        "arxiv_id": "2601.18760v1",
        "authors": [
            "Henry Bell",
            "Lara Neubauer da Costa Schertel",
            "Bochu Ding",
            "Brandon Fain"
        ],
        "submitted": "2026-01-26 18:27:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Capturing P: On the Expressive Power and Efficient Evaluation of Boolean Retrieval",
        "abstract": "Modern information retrieval is transitioning from simple document filtering to complex, neuro-symbolic reasoning workflows. However, current retrieval architectures face a fundamental efficiency dilemma when handling the rigorous logical and arithmetic constraints required by this new paradigm. Standard iterator-based engines (Document-at-a-Time) do not natively support complex, nested logic graphs; forcing them to execute such queries typically results in intractable runtime performance. Conversely, naive recursive approaches (Term-at-a-Time), while capable of supporting these structures, suffer from prohibitive memory consumption when enforcing broad logical exclusions.\n  In this paper, we propose that a retrieval engine must be capable of ``Capturing $\\mathbf{P}$'' -- evaluating any polynomial-time property directly over its index in a computationally efficient manner. We define a formal Retrieval Language ($\\mathcal{L}_R$) based on Directed Acyclic Graphs (DAGs) and prove it precisely captures the complexity class $\\mathbf{P}$. We introduce \\texttt{ComputePN}, a novel evaluation algorithm that makes $\\mathcal{L}_R$ tractable. By combining native DAG traversal with a memory-efficient ``Positive-Negative'' response mechanism, \\texttt{ComputePN} ensures the efficient evaluation of any query in $\\mathcal{L}_R$. This work establishes the theoretical foundation for turning the search index into a general-purpose computational engine.",
        "url": "http://arxiv.org/abs/2601.18747v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18747v1",
        "arxiv_id": "2601.18747v1",
        "authors": [
            "Amir Aavani"
        ],
        "submitted": "2026-01-26 18:07:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Self-Distilled Reasoner: On-Policy Self-Distillation for Large Language Models",
        "abstract": "Knowledge distillation improves large language model (LLM) reasoning by compressing the knowledge of a teacher LLM to train smaller LLMs. On-policy distillation advances this approach by having the student sample its own trajectories while a teacher LLM provides dense token-level supervision, addressing the distribution mismatch between training and inference in off-policy distillation methods. However, on-policy distillation typically requires a separate, often larger, teacher LLM and does not explicitly leverage ground-truth solutions available in reasoning datasets. Inspired by the intuition that a sufficiently capable LLM can rationalize external privileged reasoning traces and teach its weaker self (i.e., the version without access to privileged information), we introduce On-Policy Self-Distillation (OPSD), a framework where a single model acts as both teacher and student by conditioning on different contexts. The teacher policy conditions on privileged information (e.g., verified reasoning traces) while the student policy sees only the question; training minimizes the per-token divergence between these distributions over the student's own rollouts. We demonstrate the efficacy of our method on multiple mathematical reasoning benchmarks, achieving 4-8x token efficiency compared to reinforcement learning methods such as GRPO and superior performance over off-policy distillation methods.",
        "url": "http://arxiv.org/abs/2601.18734v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18734v1",
        "arxiv_id": "2601.18734v1",
        "authors": [
            "Siyan Zhao",
            "Zhihui Xie",
            "Mengchen Liu",
            "Jing Huang",
            "Guan Pang",
            "Feiyu Chen",
            "Aditya Grover"
        ],
        "submitted": "2026-01-26 17:56:50",
        "source": "arxiv",
        "comment": "13 pages"
    },
    {
        "title": "One Adapts to Any: Meta Reward Modeling for Personalized LLM Alignment",
        "abstract": "Alignment of Large Language Models (LLMs) aims to align outputs with human preferences, and personalized alignment further adapts models to individual users. This relies on personalized reward models that capture user-specific preferences and automatically provide individualized feedback. However, developing these models faces two critical challenges: the scarcity of feedback from individual users and the need for efficient adaptation to unseen users. We argue that addressing these constraints requires a paradigm shift from fitting data to learn user preferences to learn the process of preference adaptation. To realize this, we propose Meta Reward Modeling (MRM), which reformulates personalized reward modeling as a meta-learning problem. Specifically, we represent each user's reward model as a weighted combination of base reward functions, and optimize the initialization of these weights using a Model-Agnostic Meta-Learning (MAML)-style framework to support fast adaptation under limited feedback. To ensure robustness, we introduce the Robust Personalization Objective (RPO), which places greater emphasis on hard-to-learn users during meta optimization. Extensive experiments on personalized preference datasets validate that MRM enhances few-shot personalization, improves user robustness, and consistently outperforms baselines.",
        "url": "http://arxiv.org/abs/2601.18731v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18731v1",
        "arxiv_id": "2601.18731v1",
        "authors": [
            "Hongru Cai",
            "Yongqi Li",
            "Tiezheng Yu",
            "Fengbin Zhu",
            "Wenjie Wang",
            "Fuli Feng",
            "Wenjie Li"
        ],
        "submitted": "2026-01-26 17:55:52",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Reflect: Transparent Principle-Guided Reasoning for Constitutional Alignment at Scale",
        "abstract": "The constitutional framework of alignment aims to align large language models (LLMs) with value-laden principles written in natural language (such as to avoid using biased language). Prior work has focused on parameter fine-tuning techniques, such as reinforcement learning from human feedback (RLHF), to instill these principles. However, these approaches are computationally demanding, require careful engineering and tuning, and often require difficult-to-obtain human annotation data. We propose \\textsc{reflect}, an inference-time framework for constitutional alignment that does not require any training or data, providing a plug-and-play approach for aligning an instruction-tuned model to a set of principles. \\textsc{reflect} operates entirely in-context, combining a (i) constitution-conditioned base response with post-generation (ii) self-evaluation, (iii)(a) self-critique, and (iii)(b) final revision. \\textsc{reflect}'s technique of explicit in-context reasoning over principles during post-generation outperforms standard few-shot prompting and provides transparent reasoning traces. Our results demonstrate that \\textsc{reflect} significantly improves LLM conformance to diverse and complex principles, including principles quite distinct from those emphasized in the model's original parameter fine-tuning, without sacrificing factual reasoning. \\textsc{reflect} is particularly effective at reducing the rate of rare but significant violations of principles, thereby improving safety and robustness in the tail end of the distribution of generations. Finally, we show that \\textsc{reflect} naturally generates useful training data for traditional parameter fine-tuning techniques, allowing for efficient scaling and the reduction of inference-time computational overhead in long-term deployment scenarios.",
        "url": "http://arxiv.org/abs/2601.18730v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18730v1",
        "arxiv_id": "2601.18730v1",
        "authors": [
            "Henry Bell",
            "Caroline Zhang",
            "Mohammed Mobasserul Haque",
            "Dhaval Potdar",
            "Samia Zaman",
            "Brandon Fain"
        ],
        "submitted": "2026-01-26 17:54:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "HalluCitation Matters: Revealing the Impact of Hallucinated References with 300 Hallucinated Papers in ACL Conferences",
        "abstract": "Recently, we have often observed hallucinated citations or references that do not correspond to any existing work in papers under review, preprints, or published papers. Such hallucinated citations pose a serious concern to scientific reliability. When they appear in accepted papers, they may also negatively affect the credibility of conferences. In this study, we refer to hallucinated citations as \"HalluCitation\" and systematically investigate their prevalence and impact. We analyze all papers published at ACL, NAACL, and EMNLP in 2024 and 2025, including main conference, Findings, and workshop papers. Our analysis reveals that nearly 300 papers contain at least one HalluCitation, most of which were published in 2025. Notably, half of these papers were identified at EMNLP 2025, the most recent conference, indicating that this issue is rapidly increasing. Moreover, more than 100 such papers were accepted as main conference and Findings papers at EMNLP 2025, affecting the credibility.",
        "url": "http://arxiv.org/abs/2601.18724v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18724v1",
        "arxiv_id": "2601.18724v1",
        "authors": [
            "Yusuke Sakai",
            "Hidetaka Kamigaito",
            "Taro Watanabe"
        ],
        "submitted": "2026-01-26 17:48:23",
        "source": "arxiv",
        "comment": "Work In Progress"
    },
    {
        "title": "Gained in Translation: Privileged Pairwise Judges Enhance Multilingual Reasoning",
        "abstract": "When asked a question in a language less seen in its training data, current reasoning large language models (RLMs) often exhibit dramatically lower performance than when asked the same question in English. In response, we introduce \\texttt{SP3F} (Self-Play with Privileged Pairwise Feedback), a two-stage framework for enhancing multilingual reasoning without \\textit{any} data in the target language(s). First, we supervise fine-tune (SFT) on translated versions of English question-answer pairs to raise base model correctness. Second, we perform RL with feedback from a pairwise judge in a self-play fashion, with the judge receiving the English reference response as \\textit{privileged information}. Thus, even when none of the model's responses are completely correct, the privileged pairwise judge can still tell which response is better. End-to-end, \\texttt{SP3F} greatly improves base model performance, even outperforming fully post-trained models on multiple math and non-math tasks with less than\n  of the training data across the single-language, multilingual, and generalization to unseen language settings.",
        "url": "http://arxiv.org/abs/2601.18722v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18722v1",
        "arxiv_id": "2601.18722v1",
        "authors": [
            "Lintang Sutawika",
            "Gokul Swamy",
            "Zhiwei Steven Wu",
            "Graham Neubig"
        ],
        "submitted": "2026-01-26 17:46:44",
        "source": "arxiv",
        "comment": "Code available at https://github.com/lintangsutawika/SP3F"
    },
    {
        "title": "Mechanistic Analysis of Catastrophic Forgetting in Large Language Models During Continual Fine-tuning",
        "abstract": "Large language models exhibit remarkable performance across diverse tasks through pre-training and fine-tuning paradigms. However, continual fine-tuning on sequential tasks induces catastrophic forgetting, where newly acquired knowledge interferes with previously learned capabilities. Despite widespread observations of this phenomenon, the mechanistic understanding remains limited. Here, we present a comprehensive mechanistic analysis of catastrophic forgetting in transformer-based LLMs during sequential fine-tuning. Through systematic experiments across multiple model scales (109B to 400B total parameters) and task sequences, we identify three primary mechanisms driving forgetting: gradient interference in attention weights, representational drift in intermediate layers, and loss landscape flattening. We demonstrate that forgetting severity correlates strongly with task similarity (Pearson r = 0.87) and gradient alignment metrics. Our analysis reveals that approximately 15 to 23 percent of attention heads undergo severe disruption during fine-tuning, with lower layers showing greater susceptibility. These findings establish mechanistic foundations for developing targeted mitigation strategies in continual learning systems.",
        "url": "http://arxiv.org/abs/2601.18699v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18699v1",
        "arxiv_id": "2601.18699v1",
        "authors": [
            "Olaf Yunus Laitinen Imanov"
        ],
        "submitted": "2026-01-26 17:15:10",
        "source": "arxiv",
        "comment": "16 pages, 16 figures (6 main + 10 supplementary)"
    },
    {
        "title": "S$^2$GR: Stepwise Semantic-Guided Reasoning in Latent Space for Generative Recommendation",
        "abstract": "Generative Recommendation (GR) has emerged as a transformative paradigm with its end-to-end generation advantages. However, existing GR methods primarily focus on direct Semantic ID (SID) generation from interaction sequences, failing to activate deeper reasoning capabilities analogous to those in large language models and thus limiting performance potential. We identify two critical limitations in current reasoning-enhanced GR approaches: (1) Strict sequential separation between reasoning and generation steps creates imbalanced computational focus across hierarchical SID codes, degrading quality for SID codes; (2) Generated reasoning vectors lack interpretable semantics, while reasoning paths suffer from unverifiable supervision. In this paper, we propose stepwise semantic-guided reasoning in latent space (S$^2$GR), a novel reasoning enhanced GR framework. First, we establish a robust semantic foundation via codebook optimization, integrating item co-occurrence relationship to capture behavioral patterns, and load balancing and uniformity objectives that maximize codebook utilization while reinforcing coarse-to-fine semantic hierarchies. Our core innovation introduces the stepwise reasoning mechanism inserting thinking tokens before each SID generation step, where each token explicitly represents coarse-grained semantics supervised via contrastive learning against ground-truth codebook cluster distributions ensuring physically grounded reasoning paths and balanced computational focus across all SID codes. Extensive experiments demonstrate the superiority of S$^2$GR, and online A/B test confirms efficacy on large-scale industrial short video platform.",
        "url": "http://arxiv.org/abs/2601.18664v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18664v1",
        "arxiv_id": "2601.18664v1",
        "authors": [
            "Zihao Guo",
            "Jian Wang",
            "Ruxin Zhou",
            "Youhua Liu",
            "Jiawei Guo",
            "Jun Zhao",
            "Xiaoxiao Xu",
            "Yongqi Liu",
            "Kaiqiao Zhan"
        ],
        "submitted": "2026-01-26 16:40:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "FadeMem: Biologically-Inspired Forgetting for Efficient Agent Memory",
        "abstract": "Large language models deployed as autonomous agents face critical memory limitations, lacking selective forgetting mechanisms that lead to either catastrophic forgetting at context boundaries or information overload within them. While human memory naturally balances retention and forgetting through adaptive decay processes, current AI systems employ binary retention strategies that preserve everything or lose it entirely. We propose FadeMem, a biologically-inspired agent memory architecture that incorporates active forgetting mechanisms mirroring human cognitive efficiency. FadeMem implements differential decay rates across a dual-layer memory hierarchy, where retention is governed by adaptive exponential decay functions modulated by semantic relevance, access frequency, and temporal patterns. Through LLM-guided conflict resolution and intelligent memory fusion, our system consolidates related information while allowing irrelevant details to fade. Experiments on Multi-Session Chat, LoCoMo, and LTI-Bench demonstrate superior multi-hop reasoning and retrieval with 45\\% storage reduction, validating the effectiveness of biologically-inspired forgetting in agent memory systems.",
        "url": "http://arxiv.org/abs/2601.18642v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18642v1",
        "arxiv_id": "2601.18642v1",
        "authors": [
            "Lei Wei",
            "Xu Dong",
            "Xiao Peng",
            "Niantao Xie",
            "Bin Wang"
        ],
        "submitted": "2026-01-26 16:12:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AdaReasoner: Dynamic Tool Orchestration for Iterative Visual Reasoning",
        "abstract": "When humans face problems beyond their immediate capabilities, they rely on tools, providing a promising paradigm for improving visual reasoning in multimodal large language models (MLLMs). Effective reasoning, therefore, hinges on knowing which tools to use, when to invoke them, and how to compose them over multiple steps, even when faced with new tools or new tasks. We introduce \\textbf{AdaReasoner}, a family of multimodal models that learn tool use as a general reasoning skill rather than as tool-specific or explicitly supervised behavior. AdaReasoner is enabled by (i) a scalable data curation pipeline exposing models to long-horizon, multi-step tool interactions; (ii) Tool-GRPO, a reinforcement learning algorithm that optimizes tool selection and sequencing based on end-task success; and (iii) an adaptive learning mechanism that dynamically regulates tool usage. Together, these components allow models to infer tool utility from task context and intermediate outcomes, enabling coordination of multiple tools and generalization to unseen tools. Empirically, AdaReasoner exhibits strong tool-adaptive and generalization behaviors: it autonomously adopts beneficial tools, suppresses irrelevant ones, and adjusts tool usage frequency based on task demands, despite never being explicitly trained to do so. These capabilities translate into state-of-the-art performance across challenging benchmarks, improving the 7B base model by +24.9\\% on average and surpassing strong proprietary systems such as GPT-5 on multiple tasks, including VSP and Jigsaw.",
        "url": "http://arxiv.org/abs/2601.18631v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18631v1",
        "arxiv_id": "2601.18631v1",
        "authors": [
            "Mingyang Song",
            "Haoyu Sun",
            "Jiawei Gu",
            "Linjie Li",
            "Luxin Xu",
            "Ranjay Krishna",
            "Yu Cheng"
        ],
        "submitted": "2026-01-26 16:04:43",
        "source": "arxiv",
        "comment": "28 pages, 10 figures and 13 tables"
    },
    {
        "title": "Emergence of Phonemic, Syntactic, and Semantic Representations in Artificial Neural Networks",
        "abstract": "During language acquisition, children successively learn to categorize phonemes, identify words, and combine them with syntax to form new meaning. While the development of this behavior is well characterized, we still lack a unifying computational framework to explain its underlying neural representations. Here, we investigate whether and when phonemic, lexical, and syntactic representations emerge in the activations of artificial neural networks during their training. Our results show that both speech- and text-based models follow a sequence of learning stages: during training, their neural activations successively build subspaces, where the geometry of the neural activations represents phonemic, lexical, and syntactic structure. While this developmental trajectory qualitatively relates to children's, it is quantitatively different: These algorithms indeed require two to four orders of magnitude more data for these neural representations to emerge. Together, these results show conditions under which major stages of language acquisition spontaneously emerge, and hence delineate a promising path to understand the computations underpinning language acquisition.",
        "url": "http://arxiv.org/abs/2601.18617v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18617v1",
        "arxiv_id": "2601.18617v1",
        "authors": [
            "Pierre Orhan",
            "Pablo Diego-Simón",
            "Emmnanuel Chemla",
            "Yair Lakretz",
            "Yves Boubenec",
            "Jean-Rémi King"
        ],
        "submitted": "2026-01-26 15:56:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Stability as a Liability:Systematic Breakdown of Linguistic Structure in LLMs",
        "abstract": "Training stability is typically regarded as a prerequisite for reliable optimization in large language models. In this work, we analyze how stabilizing training dynamics affects the induced generation distribution. We show that under standard maximum likelihood training, stable parameter trajectories lead stationary solutions to approximately minimize the forward KL divergence to the empirical distribution, while implicitly reducing generative entropy. As a consequence, the learned model can concentrate probability mass on a limited subset of empirical modes, exhibiting systematic degeneration despite smooth loss convergence. We empirically validate this effect using a controlled feedback-based training framework that stabilizes internal generation statistics, observing consistent low-entropy outputs and repetitive behavior across architectures and random seeds. It indicates that optimization stability and generative expressivity are not inherently aligned, and that stability alone is an insufficient indicator of generative quality.",
        "url": "http://arxiv.org/abs/2601.18588v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18588v1",
        "arxiv_id": "2601.18588v1",
        "authors": [
            "Xianzhe Meng",
            "Qiangsheng Zeng",
            "Ling Luo",
            "Qinghan Yang",
            "Jiarui Hao",
            "Wenbo Wu",
            "Qinyu Wang",
            "Rui Yin",
            "Lin Qi",
            "Renzhi Lu"
        ],
        "submitted": "2026-01-26 15:34:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "From Classification to Ranking: Enhancing LLM Reasoning Capabilities for MBTI Personality Detection",
        "abstract": "Personality detection aims to measure an individual's corresponding personality traits through their social media posts. The advancements in Large Language Models (LLMs) offer novel perspectives for personality detection tasks. Existing approaches enhance personality trait analysis by leveraging LLMs to extract semantic information from textual posts as prompts, followed by training classifiers for categorization. However, accurately classifying personality traits remains challenging due to the inherent complexity of human personality and subtle inter-trait distinctions. Moreover, prompt-based methods often exhibit excessive dependency on expert-crafted knowledge without autonomous pattern-learning capacity. To address these limitations, we view personality detection as a ranking task rather than a classification and propose a corresponding reinforcement learning training paradigm. First, we employ supervised fine-tuning (SFT) to establish personality trait ranking capabilities while enforcing standardized output formats, creating a robust initialization. Subsequently, we introduce Group Relative Policy Optimization (GRPO) with a specialized ranking-based reward function. Unlike verification tasks with definitive solutions, personality assessment involves subjective interpretations and blurred boundaries between trait categories. Our reward function explicitly addresses this challenge by training LLMs to learn optimal answer rankings. Comprehensive experiments have demonstrated that our method achieves state-of-the-art performance across multiple personality detection benchmarks.",
        "url": "http://arxiv.org/abs/2601.18582v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18582v1",
        "arxiv_id": "2601.18582v1",
        "authors": [
            "Yuan Cao",
            "Feixiang Liu",
            "Xinyue Wang",
            "Yihan Zhu",
            "Hui Xu",
            "Zheng Wang",
            "Qiang Qiu"
        ],
        "submitted": "2026-01-26 15:28:43",
        "source": "arxiv",
        "comment": "9 pages, 4 figures, AAAI 2026 Bridge"
    },
    {
        "title": "FastInsight: Fast and Insightful Retrieval via Fusion Operators for Graph RAG",
        "abstract": "Existing Graph RAG methods aiming for insightful retrieval on corpus graphs typically rely on time-intensive processes that interleave Large Language Model (LLM) reasoning. To enable time-efficient insightful retrieval, we propose FastInsight. We first introduce a graph retrieval taxonomy that categorizes existing methods into three fundamental operations: vector search, graph search, and model-based search. Through this taxonomy, we identify two critical limitations in current approaches: the topology-blindness of model-based search and the semantics-blindness of graph search. FastInsight overcomes these limitations by interleaving two novel fusion operators: the Graph-based Reranker (GRanker), which functions as a graph model-based search, and Semantic-Topological eXpansion (STeX), which operates as a vector-graph search. Extensive experiments on broad retrieval and generation datasets demonstrate that FastInsight significantly improves both retrieval accuracy and generation quality compared to state-of-the-art baselines, achieving a substantial Pareto improvement in the trade-off between effectiveness and efficiency.",
        "url": "http://arxiv.org/abs/2601.18579v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18579v1",
        "arxiv_id": "2601.18579v1",
        "authors": [
            "Seonho An",
            "Chaejeong Hyun",
            "Min-Soo Kim"
        ],
        "submitted": "2026-01-26 15:23:41",
        "source": "arxiv",
        "comment": "under review"
    },
    {
        "title": "One Persona, Many Cues, Different Results: How Sociodemographic Cues Impact LLM Personalization",
        "abstract": "Personalization of LLMs by sociodemographic subgroup often improves user experience, but can also introduce or amplify biases and unfair outcomes across groups. Prior work has employed so-called personas, sociodemographic user attributes conveyed to a model, to study bias in LLMs by relying on a single cue to prompt a persona, such as user names or explicit attribute mentions. This disregards LLM sensitivity to prompt variations (robustness) and the rarity of some cues in real interactions (external validity). We compare six commonly used persona cues across seven open and proprietary LLMs on four writing and advice tasks. While cues are overall highly correlated, they produce substantial variance in responses across personas. We therefore caution against claims from a single persona cue and recommend future personalization research to evaluate multiple externally valid cues.",
        "url": "http://arxiv.org/abs/2601.18572v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18572v1",
        "arxiv_id": "2601.18572v1",
        "authors": [
            "Franziska Weeber",
            "Vera Neplenbroek",
            "Jan Batzner",
            "Sebastian Padó"
        ],
        "submitted": "2026-01-26 15:15:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Feature-Indexed Federated Recommendation with Residual-Quantized Codebooks",
        "abstract": "Federated recommendation provides a privacy-preserving solution for training recommender systems without centralizing user interactions. However, existing methods follow an ID-indexed communication paradigm that transmit whole item embeddings between clients and the server, which has three major limitations: 1) consumes uncontrollable communication resources, 2) the uploaded item information cannot generalize to related non-interacted items, and 3) is sensitive to client noisy feedback. To solve these problems, it is necessary to fundamentally change the existing ID-indexed communication paradigm. Therefore, we propose a feature-indexed communication paradigm that transmits feature code embeddings as codebooks rather than raw item embeddings. Building on this paradigm, we present RQFedRec, which assigns each item a list of discrete code IDs via Residual Quantization (RQ)-Kmeans. Each client generates and trains code embeddings as codebooks based on discrete code IDs provided by the server, and the server collects and aggregates these codebooks rather than item embeddings. This design makes communication controllable since the codebooks could cover all items, enabling updates to propagate across related items in same code ID. In addition, since code embedding represents many items, which is more robust to a single noisy item. To jointly capture semantic and collaborative information, RQFedRec further adopts a collaborative-semantic dual-channel aggregation with a curriculum strategy that emphasizes semantic codes early and gradually increases the contribution of collaborative codes over training. Extensive experiments on real-world datasets demonstrate that RQFedRec consistently outperforms state-of-the-art federated recommendation baselines while significantly reducing communication overhead.",
        "url": "http://arxiv.org/abs/2601.18570v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18570v1",
        "arxiv_id": "2601.18570v1",
        "authors": [
            "Mingzhe Han",
            "Jiahao Liu",
            "Dongsheng Li",
            "Hansu Gu",
            "Peng Zhang",
            "Ning Gu",
            "Tun Lu"
        ],
        "submitted": "2026-01-26 15:14:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Unknown Unknowns: Why Hidden Intentions in LLMs Evade Detection",
        "abstract": "LLMs are increasingly embedded in everyday decision-making, yet their outputs can encode subtle, unintended behaviours that shape user beliefs and actions. We refer to these covert, goal-directed behaviours as hidden intentions, which may arise from training and optimisation artefacts, or be deliberately induced by an adversarial developer, yet remain difficult to detect in practice. We introduce a taxonomy of ten categories of hidden intentions, grounded in social science research and organised by intent, mechanism, context, and impact, shifting attention from surface-level behaviours to design-level strategies of influence. We show how hidden intentions can be easily induced in controlled models, providing both testbeds for evaluation and demonstrations of potential misuse. We systematically assess detection methods, including reasoning and non-reasoning LLM judges, and find that detection collapses in realistic open-world settings, particularly under low-prevalence conditions, where false positives overwhelm precision and false negatives conceal true risks. Stress tests on precision-prevalence and precision-FNR trade-offs reveal why auditing fails without vanishingly small false positive rates or strong priors on manipulation types. Finally, a qualitative case study shows that all ten categories manifest in deployed, state-of-the-art LLMs, emphasising the urgent need for robust frameworks. Our work provides the first systematic analysis of detectability failures of hidden intentions in LLMs under open-world settings, offering a foundation for understanding, inducing, and stress-testing such behaviours, and establishing a flexible taxonomy for anticipating evolving threats and informing governance.",
        "url": "http://arxiv.org/abs/2601.18552v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18552v1",
        "arxiv_id": "2601.18552v1",
        "authors": [
            "Devansh Srivastav",
            "David Pape",
            "Lea Schönherr"
        ],
        "submitted": "2026-01-26 14:59:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Evaluating Morphological Plausibility of Subword Tokenization via Statistical Alignment with Morpho-Syntactic Features",
        "abstract": "We present a novel metric for the evaluation of the morphological plausibility of subword segmentation. Unlike the typically used morpheme boundary or retrieval F-score, which requires gold segmentation data that is either unavailable or of inconsistent quality across many languages, our approach utilizes morpho-syntactic features. These are available in resources such as Universal Dependencies or UniMorph for a much wider range of languages. The metric works by probabilistically aligning subwords with morphological features through an IBM Model 1. Our experiments show that the metric correlates well with traditional morpheme boundary recall while being more broadly applicable across languages with different morphological systems.",
        "url": "http://arxiv.org/abs/2601.18536v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18536v1",
        "arxiv_id": "2601.18536v1",
        "authors": [
            "Abishek Stephen",
            "Jindřich Libovický"
        ],
        "submitted": "2026-01-26 14:41:44",
        "source": "arxiv",
        "comment": "Accepted to Findings of EACL 2026, 9 pages, 6 figures"
    },
    {
        "title": "From Verifiable Dot to Reward Chain: Harnessing Verifiable Reference-based Rewards for Reinforcement Learning of Open-ended Generation",
        "abstract": "Reinforcement learning with verifiable rewards (RLVR) succeeds in reasoning tasks (e.g., math and code) by checking the final verifiable answer (i.e., a verifiable dot signal). However, extending this paradigm to open-ended generation is challenging because there is no unambiguous ground truth. Relying on single-dot supervision often leads to inefficiency and reward hacking. To address these issues, we propose reinforcement learning with verifiable reference-based rewards (RLVRR). Instead of checking the final answer, RLVRR extracts an ordered linguistic signal from high-quality references (i.e, reward chain). Specifically, RLVRR decomposes rewards into two dimensions: content, which preserves deterministic core concepts (e.g., keywords), and style, which evaluates adherence to stylistic properties through LLM-based verification. In this way, RLVRR combines the exploratory strength of RL with the efficiency and reliability of supervised fine-tuning (SFT). Extensive experiments on more than 10 benchmarks with Qwen and Llama models confirm the advantages of our approach. RLVRR (1) substantially outperforms SFT trained with ten times more data and advanced reward models, (2) unifies the training of structured reasoning and open-ended generation, and (3) generalizes more effectively while preserving output diversity. These results establish RLVRR as a principled and efficient path toward verifiable reinforcement learning for general-purpose LLM alignment. We release our code and data at https://github.com/YJiangcm/RLVRR.",
        "url": "http://arxiv.org/abs/2601.18533v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18533v1",
        "arxiv_id": "2601.18533v1",
        "authors": [
            "Yuxin Jiang",
            "Yufei Wang",
            "Qiyuan Zhang",
            "Xingshan Zeng",
            "Liangyou Li",
            "Jierun Chen",
            "Chaofan Tao",
            "Haoli Bai",
            "Lifeng Shang"
        ],
        "submitted": "2026-01-26 14:39:58",
        "source": "arxiv",
        "comment": "19 pages, 8 figures, 12 tables. Accepted at ICLR 2026"
    },
    {
        "title": "Exploring Fine-Tuning for In-Context Retrieval and Efficient KV-Caching in Long-Context Language Models",
        "abstract": "With context windows of millions of tokens, Long-Context Language Models (LCLMs) can encode entire document collections, offering a strong alternative to conventional retrieval-augmented generation (RAG). However, it remains unclear whether fine-tuning strategies can improve long-context performance and translate to greater robustness under KV-cache compression techniques. In this work, we investigate which training strategies most effectively enhance LCLMs' ability to identify and use relevant information, as well as enhancing their robustness under KV-cache compression. Our experiments show substantial in-domain improvements, achieving gains of up to +20 points over the base model. However, out-of-domain generalization remains task dependent with large variance -- LCLMs excels on finance questions (+9 points), while RAG shows stronger performance on multiple-choice questions (+6 points) over the baseline models. Finally, we show that our fine-tuning approaches bring moderate improvements in robustness under KV-cache compression, with gains varying across tasks.",
        "url": "http://arxiv.org/abs/2601.18527v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18527v1",
        "arxiv_id": "2601.18527v1",
        "authors": [
            "Francesco Maria Molfese",
            "Momchil Hardalov",
            "Rexhina Blloshmi",
            "Bill Byrne",
            "Adrià de Gispert"
        ],
        "submitted": "2026-01-26 14:37:02",
        "source": "arxiv",
        "comment": "European Chapter of the Association for Computational Linguistics EACL 2026"
    },
    {
        "title": "GenAI for Social Work Field Education: Client Simulation with Real-Time Feedback",
        "abstract": "Field education is the signature pedagogy of social work, yet providing timely and objective feedback during training is constrained by the availability of instructors and counseling clients. In this paper, we present SWITCH, the Social Work Interactive Training Chatbot. SWITCH integrates realistic client simulation, real-time counseling skill classification, and a Motivational Interviewing (MI) progression system into the training workflow. To model a client, SWITCH uses a cognitively grounded profile comprising static fields (e.g., background, beliefs) and dynamic fields (e.g., emotions, automatic thoughts, openness), allowing the agent's behavior to evolve throughout a session realistically. The skill classification module identifies the counseling skills from the user utterances, and feeds the result to the MI controller that regulates the MI stage transitions. To enhance classification accuracy, we study in-context learning with retrieval over annotated transcripts, and a fine-tuned BERT multi-label classifier. In the experiments, we demonstrated that both BERT-based approach and in-context learning outperforms the baseline with big margin. SWITCH thereby offers a scalable, low-cost, and consistent training workflow that complements field education, and allows supervisors to focus on higher-level mentorship.",
        "url": "http://arxiv.org/abs/2601.18517v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18517v1",
        "arxiv_id": "2601.18517v1",
        "authors": [
            "James Sungarda",
            "Hongkai Liu",
            "Zilong Zhou",
            "Tien-Hsuan Wu",
            "Johnson Chun-Sing Cheung",
            "Ben Kao"
        ],
        "submitted": "2026-01-26 14:26:54",
        "source": "arxiv",
        "comment": "2025 IEEE International Conference on Big Data. ISBN: 979-8-3315-9447-3/25. Page numbers: 3544-3553"
    },
    {
        "title": "Using Large Language Models to Construct Virtual Top Managers: A Method for Organizational Research",
        "abstract": "This study introduces a methodological framework that uses large language models to create virtual personas of real top managers. Drawing on real CEO communications and Moral Foundations Theory, we construct LLM-based participants that simulate the decision-making of individual leaders. Across three phases, we assess construct validity, reliability, and behavioral fidelity by benchmarking these virtual CEOs against human participants. Our results indicate that theoretically scaffolded personas approximate the moral judgements observed in human samples, suggesting that LLM-based personas can serve as credible and complementary tools for organizational research in contexts where direct access to executives is limited. We conclude by outlining implications for future research using LLM-based personas in organizational settings.",
        "url": "http://arxiv.org/abs/2601.18512v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18512v1",
        "arxiv_id": "2601.18512v1",
        "authors": [
            "Antonio Garzon-Vico",
            "Krithika Sharon Komalapati",
            "Arsalan Shahid",
            "Jan Rosier"
        ],
        "submitted": "2026-01-26 14:17:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AgentDoG: A Diagnostic Guardrail Framework for AI Agent Safety and Security",
        "abstract": "The rise of AI agents introduces complex safety and security challenges arising from autonomous tool use and environmental interactions. Current guardrail models lack agentic risk awareness and transparency in risk diagnosis. To introduce an agentic guardrail that covers complex and numerous risky behaviors, we first propose a unified three-dimensional taxonomy that orthogonally categorizes agentic risks by their source (where), failure mode (how), and consequence (what). Guided by this structured and hierarchical taxonomy, we introduce a new fine-grained agentic safety benchmark (ATBench) and a Diagnostic Guardrail framework for agent safety and security (AgentDoG). AgentDoG provides fine-grained and contextual monitoring across agent trajectories. More Crucially, AgentDoG can diagnose the root causes of unsafe actions and seemingly safe but unreasonable actions, offering provenance and transparency beyond binary labels to facilitate effective agent alignment. AgentDoG variants are available in three sizes (4B, 7B, and 8B parameters) across Qwen and Llama model families. Extensive experimental results demonstrate that AgentDoG achieves state-of-the-art performance in agentic safety moderation in diverse and complex interactive scenarios. All models and datasets are openly released.",
        "url": "http://arxiv.org/abs/2601.18491v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18491v1",
        "arxiv_id": "2601.18491v1",
        "authors": [
            "Dongrui Liu",
            "Qihan Ren",
            "Chen Qian",
            "Shuai Shao",
            "Yuejin Xie",
            "Yu Li",
            "Zhonghao Yang",
            "Haoyu Luo",
            "Peng Wang",
            "Qingyu Liu",
            "Binxin Hu",
            "Ling Tang",
            "Jilin Mei",
            "Dadi Guo",
            "Leitao Yuan",
            "Junyao Yang",
            "Guanxu Chen",
            "Qihao Lin",
            "Yi Yu",
            "Bo Zhang",
            "Jiaxuan Guo",
            "Jie Zhang",
            "Wenqi Shao",
            "Huiqi Deng",
            "Zhiheng Xi",
            "Wenjie Wang",
            "Wenxuan Wang",
            "Wen Shen",
            "Zhikai Chen",
            "Haoyu Xie",
            "Jialing Tao",
            "Juntao Dai",
            "Jiaming Ji",
            "Zhongjie Ba",
            "Linfeng Zhang",
            "Yong Liu",
            "Quanshi Zhang",
            "Lei Zhu",
            "Zhihua Wei",
            "Hui Xue",
            "Chaochao Lu",
            "Jing Shao",
            "Xia Hu"
        ],
        "submitted": "2026-01-26 13:45:41",
        "source": "arxiv",
        "comment": "40 pages, 26 figures"
    },
    {
        "title": "Demographic Probing of Large Language Models Lacks Construct Validity",
        "abstract": "Demographic probing is widely used to study how large language models (LLMs) adapt their behavior to signaled demographic attributes. This approach typically uses a single demographic cue in isolation (e.g., a name or dialect) as a signal for group membership, implicitly assuming strong construct validity: that such cues are interchangeable operationalizations of the same underlying, demographically conditioned behavior. We test this assumption in realistic advice-seeking interactions, focusing on race and gender in a U.S. context. We find that cues intended to represent the same demographic group induce only partially overlapping changes in model behavior, while differentiation between groups within a given cue is weak and uneven. Consequently, estimated disparities are unstable, with both magnitude and direction varying across cues. We further show that these inconsistencies partly arise from variation in how strongly cues encode demographic attributes and from linguistic confounders that independently shape model behavior. Together, our findings suggest that demographic probing lacks construct validity: it does not yield a single, stable characterization of how LLMs condition on demographic information, which may reflect a misspecified or fragmented construct. We conclude by recommending the use of multiple, ecologically valid cues and explicit control of confounders to support more defensible claims about demographic effects in LLMs.",
        "url": "http://arxiv.org/abs/2601.18486v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18486v1",
        "arxiv_id": "2601.18486v1",
        "authors": [
            "Manuel Tonneau",
            "Neil K. R. Seghal",
            "Niyati Malhotra",
            "Victor Orozco-Olvera",
            "Ana María Muñoz Boudet",
            "Lakshmi Subramanian",
            "Sharath Chandra Guntuku",
            "Valentin Hofmann"
        ],
        "submitted": "2026-01-26 13:41:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Funny or Persuasive, but Not Both: Evaluating Fine-Grained Multi-Concept Control in LLMs",
        "abstract": "Large Language Models (LLMs) offer strong generative capabilities, but many applications require explicit and \\textit{fine-grained} control over specific textual concepts, such as humor, persuasiveness, or formality. Prior approaches in prompting and representation engineering can provide coarse or single-attribute control, but systematic evaluation of multi-attribute settings remains limited. We introduce an evaluation framework for fine-grained controllability for both single- and dual-concept scenarios, focusing on linguistically distinct concept pairs (e.g., persuasiveness vs.~humor). Surprisingly, across multiple LLMs and generative tasks, we find that performance often drops in the dual-concept setting, even though the chosen concepts should in principle be separable. This reveals a fundamental limitation of naive prompting-based control: models struggle with compositionality even when concepts are intuitively independent. Our framework provides systematic evidence of this gap and offers a principled approach for measuring the ability of future methods for multi-concept control.",
        "url": "http://arxiv.org/abs/2601.18483v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18483v1",
        "arxiv_id": "2601.18483v1",
        "authors": [
            "Arya Labroo",
            "Ivaxi Sheth",
            "Vyas Raina",
            "Amaani Ahmed",
            "Mario Fritz"
        ],
        "submitted": "2026-01-26 13:36:34",
        "source": "arxiv",
        "comment": "Accepted for publication at EACL main conference"
    },
    {
        "title": "Latent Knowledge as a Predictor of Fact Acquisition in Fine-Tuned Large Language Models",
        "abstract": "Large language models store biomedical facts with uneven strength after pretraining: some facts are present in the weights but are not reliably accessible under deterministic decoding (latent knowledge), while others are scarcely represented. We fine tuned Llama 3.1 8B Instruct to learn ontology term identifier mappings from the Human Phenotype Ontology (800 pairs) and the Gene Ontology (400 training pairs), withholding 400 GO pairs to test generalization. Treating learning as a time to event process across 20 epochs, we used stochastic decoding to detect latent knowledge at baseline and Cox proportional hazards models to identify predictors of acquisition, generalization, and degradation. Baseline deterministic recall for HPO was 2.8%, rising to 71.9% after fine-tuning. Latent knowledge was the strongest predictor of faster fact acquisition (HR 2.6) and was associated with earlier, higher peak learning rates and faster convergence; identifier frequency and curated annotation counts had smaller effects. Generalization to withheld GO facts was uncommon (5.8%) but more likely when latent knowledge was present. Previously correct GO mappings degraded more often for withheld (unseen) terms than for trained (seen) terms, suggesting a protective effect of reinforcement during training. These results show that latent knowledge predicts both the speed of factual learning during fine-tuning and the limited generalization of unseen ontology facts, while resistance to degradation depends on whether facts are reinforced.",
        "url": "http://arxiv.org/abs/2601.18468v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18468v1",
        "arxiv_id": "2601.18468v1",
        "authors": [
            "Daniel B. Hier",
            "Tayo Obafemi-Ajayi"
        ],
        "submitted": "2026-01-26 13:15:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Token-level Collaborative Alignment for LLM-based Generative Recommendation",
        "abstract": "Large Language Models (LLMs) have demonstrated strong potential for generative recommendation by leveraging rich semantic knowledge. However, existing LLM-based recommender systems struggle to effectively incorporate collaborative filtering (CF) signals, due to a fundamental mismatch between item-level preference modeling in CF and token-level next-token prediction (NTP) optimization in LLMs. Prior approaches typically treat CF as contextual hints or representation bias, and resort to multi-stage training to reduce behavioral semantic space discrepancies, leaving CF unable to explicitly regulate LLM generation. In this work, we propose Token-level Collaborative Alignment for Recommendation (TCA4Rec), a model-agnostic and plug-and-play framework that establishes an explicit optimization-level interface between CF supervision and LLM generation. TCA4Rec consists of (i) Collaborative Tokenizer, which projects raw item-level CF logits into token-level distributions aligned with the LLM token space, and (ii) Soft Label Alignment, which integrates these CF-informed distributions with one-hot supervision to optimize a soft NTP objective. This design preserves the generative nature of LLM training while enabling collaborative alignment with essential user preference of CF models. We highlight TCA4Rec is compatible with arbitrary traditional CF models and generalizes across a wide range of decoder-based LLM recommender architectures. Moreover, it provides an explicit mechanism to balance behavioral alignment and semantic fluency, yielding generative recommendations that are both accurate and controllable. Extensive experiments demonstrate that TCA4Rec consistently improves recommendation performance across a broad spectrum of CF models and LLM-based recommender systems.",
        "url": "http://arxiv.org/abs/2601.18457v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18457v1",
        "arxiv_id": "2601.18457v1",
        "authors": [
            "Fake Lin",
            "Binbin Hu",
            "Zhi Zheng",
            "Xi Zhu",
            "Ziqi Liu",
            "Zhiqiang Zhang",
            "Jun Zhou",
            "Tong Xu"
        ],
        "submitted": "2026-01-26 13:05:02",
        "source": "arxiv",
        "comment": "11 pages, 2 figures, 7 tables, WWW 2026"
    },
    {
        "title": "TopKGAT: A Top-K Objective-Driven Architecture for Recommendation",
        "abstract": "Recommendation systems (RS) aim to retrieve the top-K items most relevant to users, with metrics such as Precision@K and Recall@K commonly used to assess effectiveness. The architecture of an RS model acts as an inductive bias, shaping the patterns the model is inclined to learn. In recent years, numerous recommendation architectures have emerged, spanning traditional matrix factorization, deep neural networks, and graph neural networks. However, their designs are often not explicitly aligned with the top-K objective, thereby limiting their effectiveness.\n  To address this limitation, we propose TopKGAT, a novel recommendation architecture directly derived from a differentiable approximation of top-K metrics. The forward computation of a single TopKGAT layer is intrinsically aligned with the gradient ascent dynamics of the Precision@K metric, enabling the model to naturally improve top-K recommendation accuracy. Structurally, TopKGAT resembles a graph attention network and can be implemented efficiently. Extensive experiments on four benchmark datasets demonstrate that TopKGAT consistently outperforms state-of-the-art baselines. The code is available at https://github.com/StupidThree/TopKGAT.",
        "url": "http://arxiv.org/abs/2601.18432v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18432v1",
        "arxiv_id": "2601.18432v1",
        "authors": [
            "Sirui Chen",
            "Jiawei Chen",
            "Canghong Jin",
            "Sheng Zhou",
            "Jingbang Chen",
            "Wujie Sun",
            "Can Wang"
        ],
        "submitted": "2026-01-26 12:45:12",
        "source": "arxiv",
        "comment": "Accepted by WWW2026"
    },
    {
        "title": "Pisets: A Robust Speech Recognition System for Lectures and Interviews",
        "abstract": "This work presents a speech-to-text system \"Pisets\" for scientists and journalists which is based on a three-component architecture aimed at improving speech recognition accuracy while minimizing errors and hallucinations associated with the Whisper model. The architecture comprises primary recognition using Wav2Vec2, false positive filtering via the Audio Spectrogram Transformer (AST), and final speech recognition through Whisper. The implementation of curriculum learning methods and the utilization of diverse Russian-language speech corpora significantly enhanced the system's effectiveness. Additionally, advanced uncertainty modeling techniques were introduced, contributing to further improvements in transcription quality. The proposed approaches ensure robust transcribing of long audio data across various acoustic conditions compared to WhisperX and the usual Whisper model. The source code of \"Pisets\" system is publicly available at GitHub: https://github.com/bond005/pisets.",
        "url": "http://arxiv.org/abs/2601.18415v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18415v1",
        "arxiv_id": "2601.18415v1",
        "authors": [
            "Ivan Bondarenko",
            "Daniil Grebenkin",
            "Oleg Sedukhin",
            "Mikhail Klementev",
            "Roman Derunets",
            "Lyudmila Budneva"
        ],
        "submitted": "2026-01-26 12:14:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond the Checkbox: Strengthening DSA Compliance Through Social Media Algorithmic Auditing",
        "abstract": "Algorithms of online platforms are required under the Digital Services Act (DSA) to comply with specific obligations concerning algorithmic transparency, user protection and privacy. To verify compliance with these requirements, DSA mandates platforms to undergo independent audits. Little is known about current auditing practices and their effectiveness in ensuring such compliance. To this end, we bridge regulatory and technical perspectives by critically examining selected audit reports across three critical algorithmic-related provisions: restrictions on profiling minors, transparency in recommender systems, and limitations on targeted advertising using sensitive data. Our analysis shows significant inconsistencies in methodologies and lack of technical depth when evaluating AI-powered systems. To enhance the depth, scale, and independence of compliance assessments, we propose to employ algorithmic auditing -- a process of behavioural assessment of AI algorithms by means of simulating user behaviour, observing algorithm responses and analysing them for audited phenomena.",
        "url": "http://arxiv.org/abs/2601.18405v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18405v1",
        "arxiv_id": "2601.18405v1",
        "authors": [
            "Sara Solarova",
            "Matúš Mesarčík",
            "Branislav Pecher",
            "Ivan Srba"
        ],
        "submitted": "2026-01-26 12:00:29",
        "source": "arxiv",
        "comment": "2026 CHI Conference on Human Factors in Computing Systems"
    },
    {
        "title": "Noise-Robust AV-ASR Using Visual Features Both in the Whisper Encoder and Decoder",
        "abstract": "In audiovisual automatic speech recognition (AV-ASR) systems, information fusion of visual features in a pre-trained ASR has been proven as a promising method to improve noise robustness. In this work, based on the prominent Whisper ASR, first, we propose a simple and effective visual fusion method -- use of visual features both in encoder and decoder (dual-use) -- to learn the audiovisual interactions in the encoder and to weigh modalities in the decoder. Second, we compare visual fusion methods in Whisper models of various sizes. Our proposed dual-use method shows consistent noise robustness improvement, e.g., a 35% relative improvement (WER: 4.41% vs. 6.83%) based on Whisper small, and a 57% relative improvement (WER: 4.07% vs. 9.53%) based on Whisper medium, compared to typical reference middle fusion in babble noise with a signal-to-noise ratio (SNR) of 0dB. Third, we conduct ablation studies examining the impact of various module designs and fusion options. Fine-tuned on 1929 hours of audiovisual data, our dual-use method using Whisper medium achieves 4.08% (MUSAN babble noise) and 4.43% (NoiseX babble noise) average WER across various SNRs, thereby establishing a new state-of-the-art in noisy conditions on the LRS3 AV-ASR benchmark. Our code is at https://github.com/ifnspaml/Dual-Use-AVASR",
        "url": "http://arxiv.org/abs/2601.18396v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18396v1",
        "arxiv_id": "2601.18396v1",
        "authors": [
            "Zhengyang Li",
            "Thomas Graave",
            "Björn Möller",
            "Zehang Wu",
            "Matthias Franz",
            "Tim Fingscheidt"
        ],
        "submitted": "2026-01-26 11:55:07",
        "source": "arxiv",
        "comment": "accepted at ICASSP2026"
    },
    {
        "title": "Do not be greedy, Think Twice: Sampling and Selection for Document-level Information Extraction",
        "abstract": "Document-level Information Extraction (DocIE) aims to produce an output template with the entities and relations of interest occurring in the given document. Standard practices include prompting decoder-only LLMs using greedy decoding to avoid output variability. Rather than treating this variability as a limitation, we show that sampling can produce substantially better solutions than greedy decoding, especially when using reasoning models. We thus propose ThinkTwice, a sampling and selection framework in which the LLM generates multiple candidate templates for a given document, and a selection module chooses the most suitable one. We introduce both an unsupervised method that exploits agreement across generated outputs, and a supervised selection method using reward models trained on labeled DocIE data. To address the scarcity of golden reasoning trajectories for DocIE, we propose a rejection-sampling-based method to generate silver training data that pairs output templates with reasoning traces. Our experiments show the validity of unsupervised and supervised ThinkTwice, consistently outperforming greedy baselines and the state-of-the-art.",
        "url": "http://arxiv.org/abs/2601.18395v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18395v1",
        "arxiv_id": "2601.18395v1",
        "authors": [
            "Mikel Zubillaga",
            "Oscar Sainz",
            "Oier Lopez de Lacalle",
            "Eneko Agirre"
        ],
        "submitted": "2026-01-26 11:53:08",
        "source": "arxiv",
        "comment": "Submitted to IJCAI-ECAI 2026"
    },
    {
        "title": "OCR-Enhanced Multimodal ASR Can Read While Listening",
        "abstract": "Visual information, such as subtitles in a movie, often helps automatic speech recognition. In this paper, we propose Donut-Whisper, an audio-visual ASR model with dual encoder to leverage visual information to improve speech recognition performance in both English and Chinese. Donut-Whisper combines the advantage of the linear and the Q-Former-based modality alignment structures via a cross-attention module, generating more powerful audio-visual features. Meanwhile, we propose a lightweight knowledge distillation scheme showcasing the potential of using audio-visual models to teach audio-only models to achieve better performance. Moreover, we propose a new multilingual audio-visual speech recognition dataset based on movie clips containing both Chinese and English partitions. As a result, Donut-Whisper achieved significantly better performance on both English and Chinese partition of the dataset compared to both Donut and Whisper large V3 baselines. In particular, an absolute 5.75% WER reduction and a 16.5% absolute CER reduction were achieved on the English and Chinese sets respectively compared to the Whisper ASR baseline.",
        "url": "http://arxiv.org/abs/2601.18393v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18393v1",
        "arxiv_id": "2601.18393v1",
        "authors": [
            "Junli Chen",
            "Changli Tang",
            "Yixuan Li",
            "Guangzhi Sun",
            "Chao Zhang"
        ],
        "submitted": "2026-01-26 11:51:08",
        "source": "arxiv",
        "comment": "4 pages, 2 figures. Submitted to ICASSP 2026"
    },
    {
        "title": "Dynamic Thinking-Token Selection for Efficient Reasoning in Large Reasoning Models",
        "abstract": "Large Reasoning Models (LRMs) excel at solving complex problems by explicitly generating a reasoning trace before deriving the final answer. However, these extended generations incur substantial memory footprint and computational overhead, bottlenecking LRMs' efficiency. This work uses attention maps to analyze the influence of reasoning traces and uncover an interesting phenomenon: only some decision-critical tokens in a reasoning trace steer the model toward the final answer, while the remaining tokens contribute negligibly. Building on this observation, we propose Dynamic Thinking-Token Selection (DynTS). This method identifies decision-critical tokens and retains only their associated Key-Value (KV) cache states during inference, evicting the remaining redundant entries to optimize efficiency.",
        "url": "http://arxiv.org/abs/2601.18383v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18383v1",
        "arxiv_id": "2601.18383v1",
        "authors": [
            "Zhenyuan Guo",
            "Tong Chen",
            "Wenlong Meng",
            "Chen Gong",
            "Xin Yu",
            "Chengkun Wei",
            "Wenzhi Chen"
        ],
        "submitted": "2026-01-26 11:31:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Corpus-Based Approaches to Igbo Diacritic Restoration",
        "abstract": "With natural language processing (NLP), researchers aim to enable computers to identify and understand patterns in human languages. This is often difficult because a language embeds many dynamic and varied properties in its syntax, pragmatics and phonology, which need to be captured and processed. The capacity of computers to process natural languages is increasing because NLP researchers are pushing its boundaries. But these research works focus more on well-resourced languages such as English, Japanese, German, French, Russian, Mandarin Chinese, etc. Over 95% of the world's 7000 languages are low-resourced for NLP, i.e. they have little or no data, tools, and techniques for NLP work.\n  In this thesis, we present an overview of diacritic ambiguity and a review of previous diacritic disambiguation approaches on other languages. Focusing on the Igbo language, we report the steps taken to develop a flexible framework for generating datasets for diacritic restoration. Three main approaches, the standard n-gram model, the classification models and the embedding models were proposed. The standard n-gram models use a sequence of previous words to the target stripped word as key predictors of the correct variants. For the classification models, a window of words on both sides of the target stripped word was used. The embedding models compare the similarity scores of the combined context word embeddings and the embeddings of each of the candidate variant vectors.",
        "url": "http://arxiv.org/abs/2601.18380v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18380v1",
        "arxiv_id": "2601.18380v1",
        "authors": [
            "Ignatius Ezeani"
        ],
        "submitted": "2026-01-26 11:30:36",
        "source": "arxiv",
        "comment": "270 page. Ph.D. Thesis. The University of Sheffield"
    },
    {
        "title": "Hierarchical Text Classification with LLM-Refined Taxonomies",
        "abstract": "Hierarchical text classification (HTC) depends on taxonomies that organize labels into structured hierarchies. However, many real-world taxonomies introduce ambiguities, such as identical leaf names under similar parent nodes, which prevent language models (LMs) from learning clear decision boundaries. In this paper, we present TaxMorph, a framework that uses large language models (LLMs) to transform entire taxonomies through operations such as renaming, merging, splitting, and reordering. Unlike prior work, our method revises the full hierarchy to better match the semantics encoded by LMs. Experiments across three HTC benchmarks show that LLM-refined taxonomies consistently outperform human-curated ones in various settings up to +2.9pp. in F1. To better understand these improvements, we compare how well LMs can assign leaf nodes to parent nodes and vice versa across human-curated and LLM-refined taxonomies. We find that human-curated taxonomies lead to more easily separable clusters in embedding space. However, the LLM-refined taxonomies align more closely with the model's actual confusion patterns during classification. In other words, even though they are harder to separate, they better reflect the model's inductive biases. These findings suggest that LLM-guided refinement creates taxonomies that are more compatible with how models learn, improving HTC performance.",
        "url": "http://arxiv.org/abs/2601.18375v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18375v1",
        "arxiv_id": "2601.18375v1",
        "authors": [
            "Jonas Golde",
            "Nicolaas Jedema",
            "Ravi Krishnan",
            "Phong Le"
        ],
        "submitted": "2026-01-26 11:28:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CitiLink: Enhancing Municipal Transparency and Citizen Engagement through Searchable Meeting Minutes",
        "abstract": "City council minutes are typically lengthy and formal documents with a bureaucratic writing style. Although publicly available, their structure often makes it difficult for citizens or journalists to efficiently find information. In this demo, we present CitiLink, a platform designed to transform unstructured municipal meeting minutes into structured and searchable data, demonstrating how NLP and IR can enhance the accessibility and transparency of local government. The system employs LLMs to extract metadata, discussed subjects, and voting outcomes, which are then indexed in a database to support full-text search with BM25 ranking and faceted filtering through a user-friendly interface. The developed system was built over a collection of 120 minutes made available by six Portuguese municipalities. To assess its usability, CitiLink was tested through guided sessions with municipal personnel, providing insights into how real users interact with the system. In addition, we evaluated Gemini's performance in extracting relevant information from the minutes, highlighting its effectiveness in data extraction.",
        "url": "http://arxiv.org/abs/2601.18374v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18374v1",
        "arxiv_id": "2601.18374v1",
        "authors": [
            "Rodrigo Silva",
            "José Evans",
            "José Isidro",
            "Miguel Marques",
            "Afonso Fonseca",
            "Ricardo Morais",
            "João Canavilhas",
            "Arian Pasquali",
            "Purificação Silvano",
            "Alípio Jorge",
            "Nuno Guimarães",
            "Sérgio Nunes",
            "Ricardo Campos"
        ],
        "submitted": "2026-01-26 11:26:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Can Good Writing Be Generative? Expert-Level AI Writing Emerges through Fine-Tuning on High-Quality Books",
        "abstract": "Creative writing has long been considered a uniquely human endeavor, requiring voice and style that machines could not replicate. This assumption is challenged by Generative AI that can emulate thousands of author styles in seconds with negligible marginal labor. To understand this better, we conducted a behavioral experiment where 28 MFA writers (experts) competed against three LLMs in emulating 50 critically acclaimed authors. Based on blind pairwise comparisons by 28 expert judges and 131 lay judges, we find that experts preferred human writing in 82.7% of cases under the in-context prompting condition but this reversed to 62% preference for AI after fine-tuning on authors' complete works. Lay judges, however, consistently preferred AI writing. Debrief interviews with expert writers revealed that their preference for AI writing triggered an identity crisis, eroding aesthetic confidence and questioning what constitutes \"good writing.\" These findings challenge discourse about AI's creative limitations and raise fundamental questions about the future of creative labor.",
        "url": "http://arxiv.org/abs/2601.18353v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18353v1",
        "arxiv_id": "2601.18353v1",
        "authors": [
            "Tuhin Chakrabarty",
            "Paramveer S. Dhillon"
        ],
        "submitted": "2026-01-26 10:59:21",
        "source": "arxiv",
        "comment": "Proceedings of CHI 2026 Conference (To Appear)"
    },
    {
        "title": "Code over Words: Overcoming Semantic Inertia via Code-Grounded Reasoning",
        "abstract": "LLMs struggle with Semantic Inertia: the inability to inhibit pre-trained priors (e.g., \"Lava is Dangerous\") when dynamic, in-context rules contradict them. We probe this phenomenon using Baba Is You, where physical laws are mutable text rules, enabling precise evaluation of models' ability to override learned priors when rules change. We quantatively observe that larger models can exhibit inverse scaling: they perform worse than smaller models when natural language reasoning requires suppressing pre-trained associations (e.g., accepting \"Lava is Safe\"). Our analysis attributes this to natural language encoding, which entangles descriptive semantics and logical rules, leading to persistent hallucinations of familiar physics despite explicit contradictory rules. Here we show that representing dynamics as executable code, rather than descriptive text, reverses this trend and enables effective prior inhibition. We introduce Code-Grounded Vistas (LCV), which fine-tunes models on counterfactual pairs and identifies states with contradictory rules, thereby forcing attention to logical constraints rather than visual semantics. This training-time approach outperforms expensive inference-time search methods in both efficiency and accuracy. Our results demonstrate that representation fundamentally determines whether scaling improves or impairs contextual reasoning. This challenges the assumption that larger models are universally better, with implications for domains that require dynamic overriding of learned priors.",
        "url": "http://arxiv.org/abs/2601.18352v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18352v1",
        "arxiv_id": "2601.18352v1",
        "authors": [
            "Manjie Xu",
            "Isabella Yin",
            "Xinyi Tu",
            "Chi Zhang",
            "Yixin Zhu"
        ],
        "submitted": "2026-01-26 10:58:52",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "When Domain Pretraining Interferes with Instruction Alignment: An Empirical Study of Adapter Merging in Medical LLMs",
        "abstract": "Large language models (LLMs) show strong general capability but often struggle with medical terminology precision and safety-critical instruction following. We present a case study for adapter interference in safety-critical domains using a 14B-parameter base model through a two-stage LoRA pipeline: (1) domain-adaptive pre-training (PT) to inject broad medical knowledge via continued pre-training (DAPT), and (2) supervised fine-tuning (SFT) to align the model with medical question-answering behaviors through instruction-style data. To balance instruction-following ability and domain knowledge retention, we propose Weighted Adapter Merging, linearly combining SFT and PT adapters before exporting a merged base-model checkpoint. On a held-out medical validation set (F5/F6), the merged model achieves BLEU-4 = 16.38, ROUGE-1 = 20.42, ROUGE-2 = 4.60, and ROUGE-L = 11.54 under a practical decoding configuration. We further analyze decoding sensitivity and training stability with loss curves and controlled decoding comparisons.",
        "url": "http://arxiv.org/abs/2601.18350v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18350v1",
        "arxiv_id": "2601.18350v1",
        "authors": [
            "Junyi Zou"
        ],
        "submitted": "2026-01-26 10:54:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Overalignment in Frontier LLMs: An Empirical Study of Sycophantic Behaviour in Healthcare",
        "abstract": "As LLMs are increasingly integrated into clinical workflows, their tendency for sycophancy, prioritizing user agreement over factual accuracy, poses significant risks to patient safety. While existing evaluations often rely on subjective datasets, we introduce a robust framework grounded in medical MCQA with verifiable ground truths. We propose the Adjusted Sycophancy Score, a novel metric that isolates alignment bias by accounting for stochastic model instability, or \"confusability\". Through an extensive scaling analysis of the Qwen-3 and Llama-3 families, we identify a clear scaling trajectory for resilience. Furthermore, we reveal a counter-intuitive vulnerability in reasoning-optimized \"Thinking\" models: while they demonstrate high vanilla accuracy, their internal reasoning traces frequently rationalize incorrect user suggestions under authoritative pressure. Our results across frontier models suggest that benchmark performance is not a proxy for clinical reliability, and that simplified reasoning structures may offer superior robustness against expert-driven sycophancy.",
        "url": "http://arxiv.org/abs/2601.18334v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18334v1",
        "arxiv_id": "2601.18334v1",
        "authors": [
            "Clément Christophe",
            "Wadood Mohammed Abdul",
            "Prateek Munjal",
            "Tathagata Raha",
            "Ronnie Rajan",
            "Praveenkumar Kanithi"
        ],
        "submitted": "2026-01-26 10:21:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Integrating Fine-Grained Audio-Visual Evidence for Robust Multimodal Emotion Reasoning",
        "abstract": "Multimodal emotion analysis is shifting from static classification to generative reasoning. Beyond simple label prediction, robust affective reasoning must synthesize fine-grained signals such as facial micro-expressions and prosodic which shifts to decode the latent causality within complex social contexts. However, current Multimodal Large Language Models (MLLMs) face significant limitations in fine-grained perception, primarily due to data scarcity and insufficient cross-modal fusion. As a result, these models often exhibit unimodal dominance which leads to hallucinations in complex multimodal interactions, particularly when visual and acoustic cues are subtle, ambiguous, or even contradictory (e.g., in sarcastic scenery). To address this, we introduce SABER-LLM, a framework designed for robust multimodal reasoning. First, we construct SABER, a large-scale emotion reasoning dataset comprising 600K video clips, annotated with a novel six-dimensional schema that jointly captures audiovisual cues and causal logic. Second, we propose the structured evidence decomposition paradigm, which enforces a \"perceive-then-reason\" separation between evidence extraction and reasoning to alleviate unimodal dominance. The ability to perceive complex scenes is further reinforced by consistency-aware direct preference optimization, which explicitly encourages alignment among modalities under ambiguous or conflicting perceptual conditions. Experiments on EMER, EmoBench-M, and SABER-Test demonstrate that SABER-LLM significantly outperforms open-source baselines and achieves robustness competitive with closed-source models in decoding complex emotional dynamics. The dataset and model are available at https://github.com/zxzhao0/SABER-LLM.",
        "url": "http://arxiv.org/abs/2601.18321v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18321v1",
        "arxiv_id": "2601.18321v1",
        "authors": [
            "Zhixian Zhao",
            "Wenjie Tian",
            "Xiaohai Tian",
            "Jun Zhang",
            "Lei Xie"
        ],
        "submitted": "2026-01-26 10:03:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MultiVis-Agent: A Multi-Agent Framework with Logic Rules for Reliable and Comprehensive Cross-Modal Data Visualization",
        "abstract": "Real-world visualization tasks involve complex, multi-modal requirements that extend beyond simple text-to-chart generation, requiring reference images, code examples, and iterative refinement. Current systems exhibit fundamental limitations: single-modality input, one-shot generation, and rigid workflows. While LLM-based approaches show potential for these complex requirements, they introduce reliability challenges including catastrophic failures and infinite loop susceptibility. To address this gap, we propose MultiVis-Agent, a logic rule-enhanced multi-agent framework for reliable multi-modal and multi-scenario visualization generation. Our approach introduces a four-layer logic rule framework that provides mathematical guarantees for system reliability while maintaining flexibility. Unlike traditional rule-based systems, our logic rules are mathematical constraints that guide LLM reasoning rather than replacing it. We formalize the MultiVis task spanning four scenarios from basic generation to iterative refinement, and develop MultiVis-Bench, a benchmark with over 1,000 cases for multi-modal visualization evaluation. Extensive experiments demonstrate that our approach achieves 75.63% visualization score on challenging tasks, significantly outperforming baselines (57.54-62.79%), with task completion rates of 99.58% and code execution success rates of 94.56% (vs. 74.48% and 65.10% without logic rules), successfully addressing both complexity and reliability challenges in automated visualization generation.",
        "url": "http://arxiv.org/abs/2601.18320v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18320v1",
        "arxiv_id": "2601.18320v1",
        "authors": [
            "Jinwei Lu",
            "Yuanfeng Song",
            "Chen Zhang",
            "Raymond Chi-Wing Wong"
        ],
        "submitted": "2026-01-26 10:03:10",
        "source": "arxiv",
        "comment": "Accepted to SIGMOD 2026"
    },
    {
        "title": "Calibrating Beyond English: Language Diversity for Better Quantized Multilingual LLM",
        "abstract": "Quantization is an effective technique for reducing the storage footprint and computational costs of Large Language Models (LLMs), but it often results in performance degradation. Existing post-training quantization methods typically use small, English-only calibration sets; however, their impact on multilingual models remains underexplored. We systematically evaluate eight calibration settings (five single-language and three multilingual mixes) on two quantizers (GPTQ, AWQ) on data from 10 languages. Our findings reveal a consistent trend: non-English and multilingual calibration sets significantly improve perplexity compared to English-only baselines. Specifically, we observe notable average perplexity gains across both quantizers on Llama3.1 8B and Qwen2.5 7B, with multilingual mixes achieving the largest overall reductions of up to 3.52 points in perplexity. Furthermore, our analysis indicates that tailoring calibration sets to the evaluation language yields the largest improvements for individual languages, underscoring the importance of linguistic alignment. We also identify specific failure cases where certain language-quantizer combinations degrade performance, which we trace to differences in activation range distributions across languages. These results highlight that static one-size-fits-all calibration is suboptimal and that tailoring calibration data, both in language and diversity, plays a crucial role in robustly quantizing multilingual LLMs.",
        "url": "http://arxiv.org/abs/2601.18306v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18306v1",
        "arxiv_id": "2601.18306v1",
        "authors": [
            "Everlyn Asiko Chimoto",
            "Mostafa Elhoushi",
            "Bruce A. Bassett"
        ],
        "submitted": "2026-01-26 09:36:03",
        "source": "arxiv",
        "comment": "Accepted to EACL 2026 Main Conference"
    },
    {
        "title": "Suppressing Final Layer Hidden State Jumps in Transformer Pretraining",
        "abstract": "This paper discusses the internal behavior of Transformer language models. Many recent pre-trained models have been reported to exhibit only slight changes in the angular distance between the input and output hidden state vectors in the middle Transformer layers, despite a disproportionately large ``jump'' in the angular distance occurring in or around the final Transformer layer. To characterize this, we first introduce a quantitative metric for the jump strength around the final layer, and then demonstrate its prevalence across many open-weight models, as well as its amplification throughout pre-training. Assuming such jumps indicate an undesirable property, we propose the jump-suppressing regularizer (JREG) which penalizes this jump during pre-training, thereby encouraging more balanced capability usage across the middle layers. Empirical evaluations of three model sizes of Llama-based models, trained with the proposed JREG method, reveal improved task performance compared to the baseline without altering the model architecture.",
        "url": "http://arxiv.org/abs/2601.18302v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18302v1",
        "arxiv_id": "2601.18302v1",
        "authors": [
            "Keigo Shibata",
            "Kazuki Yano",
            "Ryosuke Takahashi",
            "Jaesung Lee",
            "Wataru Ikeda",
            "Jun Suzuki"
        ],
        "submitted": "2026-01-26 09:30:49",
        "source": "arxiv",
        "comment": "Accepted to the Findings of EACL 2026"
    },
    {
        "title": "Temp-R1: A Unified Autonomous Agent for Complex Temporal KGQA via Reverse Curriculum Reinforcement Learning",
        "abstract": "Temporal Knowledge Graph Question Answering (TKGQA) is inherently challenging, as it requires sophisticated reasoning over dynamic facts with multi-hop dependencies and complex temporal constraints. Existing methods rely on fixed workflows and expensive closed-source APIs, limiting flexibility and scalability. We propose Temp-R1, the first autonomous end-to-end agent for TKGQA trained through reinforcement learning. To address cognitive overload in single-action reasoning, we expand the action space with specialized internal actions alongside external action. To prevent shortcut learning on simple questions, we introduce reverse curriculum learning that trains on difficult questions first, forcing the development of sophisticated reasoning before transferring to easier cases. Our 8B-parameter Temp-R1 achieves state-of-the-art performance on MultiTQ and TimelineKGQA, improving 19.8% over strong baselines on complex questions. Our work establishes a new paradigm for autonomous temporal reasoning agents. Our code will be publicly available soon at https://github.com/zjukg/Temp-R1.",
        "url": "http://arxiv.org/abs/2601.18296v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18296v1",
        "arxiv_id": "2601.18296v1",
        "authors": [
            "Zhaoyan Gong",
            "Zhiqiang Liu",
            "Songze Li",
            "Xiaoke Guo",
            "Yuanxiang Liu",
            "Xinle Deng",
            "Zhizhen Liu",
            "Lei Liang",
            "Huajun Chen",
            "Wen Zhang"
        ],
        "submitted": "2026-01-26 09:23:53",
        "source": "arxiv",
        "comment": "Work in progress"
    },
    {
        "title": "U-Fold: Dynamic Intent-Aware Context Folding for User-Centric Agents",
        "abstract": "Large language model (LLM)-based agents have been successfully deployed in many tool-augmented settings, but their scalability is fundamentally constrained by context length. Existing context-folding methods mitigate this issue by summarizing past interactions, yet they are typically designed for single-query or single-intent scenarios. In more realistic user-centric dialogues, we identify two major failure modes: (i) they irreversibly discard fine-grained constraints and intermediate facts that are crucial for later decisions, and (ii) their summaries fail to track evolving user intent, leading to omissions and erroneous actions. To address these limitations, we propose U-Fold, a dynamic context-folding framework tailored to user-centric tasks. U-Fold retains the full user--agent dialogue and tool-call history but, at each turn, uses two core components to produce an intent-aware, evolving dialogue summary and a compact, task-relevant tool log. Extensive experiments on $τ$-bench, $τ^2$-bench, VitaBench, and harder context-inflated settings show that U-Fold consistently outperforms ReAct (achieving a 71.4% win rate in long-context settings) and prior folding baselines (with improvements of up to 27.0%), particularly on long, noisy, multi-turn tasks. Our study demonstrates that U-Fold is a promising step toward transferring context-management techniques from single-query benchmarks to realistic user-centric applications.",
        "url": "http://arxiv.org/abs/2601.18285v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18285v1",
        "arxiv_id": "2601.18285v1",
        "authors": [
            "Jin Su",
            "Runnan Fang",
            "Yeqiu Li",
            "Xiaobin Wang",
            "Shihao Cai",
            "Pengjun Xie",
            "Ningyu Zhang",
            "Fajie Yuan"
        ],
        "submitted": "2026-01-26 09:11:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Think-Augmented Function Calling: Improving LLM Parameter Accuracy Through Embedded Reasoning",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in function calling for autonomous agents, yet current mechanisms lack explicit reasoning transparency during parameter generation, particularly for complex functions with interdependent parameters. While existing approaches like chain-of-thought prompting operate at the agent level, they fail to provide fine-grained reasoning guidance for individual function parameters. To address these limitations, we propose Think-Augmented Function Calling (TAFC), a novel framework that enhances function calling accuracy through explicit reasoning at both function and parameter levels. Our method introduces a universal \"think\" parameter augmentation that enables models to articulate their decision-making process, with dynamic optimization for parameter descriptions to improve reasoning quality. For complex parameters, TAFC automatically triggers granular reasoning based on complexity scoring, ensuring appropriate justification for critical decisions. Additionally, we propose reasoning-guided optimization to align generated reasoning with human expectations. TAFC requires no architectural modifications to existing LLMs while maintaining full API compatibility. Evaluation on ToolBench across proprietary and open-source models demonstrates significant improvements in parameter generation accuracy and reasoning coherence for multi-parameter functions, while providing enhanced interpretability for debugging AI agent behaviors.",
        "url": "http://arxiv.org/abs/2601.18282v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18282v1",
        "arxiv_id": "2601.18282v1",
        "authors": [
            "Lei Wei",
            "Jinpeng Ou",
            "Xiao Peng",
            "Bin Wang"
        ],
        "submitted": "2026-01-26 09:05:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Reflecting Twice before Speaking with Empathy: Self-Reflective Alternating Inference for Empathy-Aware End-to-End Spoken Dialogue",
        "abstract": "End-to-end Spoken Language Models (SLMs) hold great potential for paralinguistic perception, and numerous studies have aimed to enhance their capabilities, particularly for empathetic dialogue. However, current approaches largely depend on rigid supervised signals, such as ground-truth response in supervised fine-tuning or preference scores in reinforcement learning. Such reliance is fundamentally limited for modeling complex empathy, as there is no single \"correct\" response and a simple numerical score cannot fully capture the nuances of emotional expression or the appropriateness of empathetic behavior. To address these limitations, we sequentially introduce EmpathyEval, a descriptive natural-language-based evaluation model for assessing empathetic quality in spoken dialogues. Building upon EmpathyEval, we propose ReEmpathy, an end-to-end SLM that enhances empathetic dialogue through a novel Empathetic Self-Reflective Alternating Inference mechanism, which interleaves spoken response generation with free-form, empathy-related reflective reasoning. Extensive experiments demonstrate that ReEmpathy substantially improves empathy-sensitive spoken dialogue by enabling reflective reasoning, offering a promising approach toward more emotionally intelligent and empathy-aware human-computer interactions.",
        "url": "http://arxiv.org/abs/2601.18281v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18281v1",
        "arxiv_id": "2601.18281v1",
        "authors": [
            "Yuhang Jia",
            "Pei Liu",
            "Haoqin Sun",
            "Jiaming Zhou",
            "Xuxin Cheng",
            "Cao Liu",
            "Ke Zeng",
            "Xunliang Cai",
            "Yong Qin"
        ],
        "submitted": "2026-01-26 09:04:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Designing large language model prompts to extract scores from messy text: A shared dataset and challenge",
        "abstract": "In some areas of computing, natural language processing and information science, progress is made by sharing datasets and challenging the community to design the best algorithm for an associated task. This article introduces a shared dataset of 1446 short texts, each of which describes a research quality score on the UK scale of 1* to 4*. This is a messy collection, with some texts not containing scores and others including invalid scores or strange formats. With this dataset there is also a description of what constitutes a valid score and a \"gold standard\" of the correct scores for these texts (including missing values). The challenge is to design a prompt for Large Language Models (LLMs) to extract the scores from these texts as accurately as possible. The format for the response should be a number and no other text so there are two aspects to the challenge: ensuring that the LLM returns only a number, and instructing it to deduce the correct number for the text. As part of this, the LLM prompt needs to explain when to return the missing value code, -1, instead of a number when the text does not clearly contain one. The article also provides an example of a simple prompt. The purpose of the challenge is twofold: to get an effective solution to this problem, and to increase understanding of prompt design and LLM capabilities for complex numerical tasks. The initial solution suggested has an accuracy of 72.6%, so the challenge is to beat this.",
        "url": "http://arxiv.org/abs/2601.18271v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18271v1",
        "arxiv_id": "2601.18271v1",
        "authors": [
            "Mike Thelwall"
        ],
        "submitted": "2026-01-26 08:55:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Orchestrating Specialized Agents for Trustworthy Enterprise RAG",
        "abstract": "Retrieval-Augmented Generation (RAG) shows promise for enterprise knowledge work, yet it often underperforms in high-stakes decision settings that require deep synthesis, strict traceability, and recovery from underspecified prompts. One-pass retrieval-and-write pipelines frequently yield shallow summaries, inconsistent grounding, and weak mechanisms for completeness verification. We introduce ADORE (Adaptive Deep Orchestration for Research in Enterprise), an agentic framework that replaces linear retrieval with iterative, user-steered investigation coordinated by a central orchestrator and a set of specialized agents. ADORE's key insight is that a structured Memory Bank (a curated evidence store with explicit claim-evidence linkage and section-level admissible evidence) enables traceable report generation and systematic checks for evidence completeness. Our contributions are threefold: (1) Memory-locked synthesis - report generation is constrained to a structured Memory Bank (Claim-Evidence Graph) with section-level admissible evidence, enabling traceable claims and grounded citations; (2) Evidence-coverage-guided execution - a retrieval-reflection loop audits section-level evidence coverage to trigger targeted follow-up retrieval and terminates via an evidence-driven stopping criterion; (3) Section-packed long-context grounding - section-level packing, pruning, and citation-preserving compression make long-form synthesis feasible under context limits. Across our evaluation suite, ADORE ranks first on DeepResearch Bench (52.65) and achieves the highest head-to-head preference win rate on DeepConsult (77.2%) against commercial systems.",
        "url": "http://arxiv.org/abs/2601.18267v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18267v1",
        "arxiv_id": "2601.18267v1",
        "authors": [
            "Xincheng You",
            "Qi Sun",
            "Neha Bora",
            "Huayi Li",
            "Shubham Goel",
            "Kang Li",
            "Sean Culatana"
        ],
        "submitted": "2026-01-26 08:48:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "FGGM: Fisher-Guided Gradient Masking for Continual Learning",
        "abstract": "Catastrophic forgetting impairs the continuous learning of large language models. We propose Fisher-Guided Gradient Masking (FGGM), a framework that mitigates this by strategically selecting parameters for updates using diagonal Fisher Information. FGGM dynamically generates binary masks with adaptive thresholds, preserving critical parameters to balance stability and plasticity without requiring historical data. Unlike magnitude-based methods such as MIGU, our approach offers a mathematically principled parameter importance estimation. On the TRACE benchmark, FGGM shows a 9.6% relative improvement in retaining general capabilities over supervised fine-tuning (SFT) and a 4.4% improvement over MIGU on TRACE tasks. Additional analysis on code generation tasks confirms FGGM's superior performance and reduced forgetting, establishing it as an effective solution.",
        "url": "http://arxiv.org/abs/2601.18261v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18261v1",
        "arxiv_id": "2601.18261v1",
        "authors": [
            "Chao-Hong Tan",
            "Qian Chen",
            "Wen Wang",
            "Yukun Ma",
            "Chong Zhang",
            "Chong Deng",
            "Qinglin Zhang",
            "Xiangang Li",
            "Jieping Ye"
        ],
        "submitted": "2026-01-26 08:35:34",
        "source": "arxiv",
        "comment": "Accepted by ICASSP 2026"
    },
    {
        "title": "BoRP: Bootstrapped Regression Probing for Scalable and Human-Aligned LLM Evaluation",
        "abstract": "Accurate evaluation of user satisfaction is critical for iterative development of conversational AI. However, for open-ended assistants, traditional A/B testing lacks reliable metrics: explicit feedback is sparse, while implicit metrics are ambiguous. To bridge this gap, we introduce BoRP (Bootstrapped Regression Probing), a scalable framework for high-fidelity satisfaction evaluation. Unlike generative approaches, BoRP leverages the geometric properties of LLM latent space. It employs a polarization-index-based bootstrapping mechanism to automate rubric generation and utilizes Partial Least Squares (PLS) to map hidden states to continuous scores. Experiments on industrial datasets show that BoRP (Qwen3-8B/14B) significantly outperforms generative baselines (even Qwen3-Max) in alignment with human judgments. Furthermore, BoRP reduces inference costs by orders of magnitude, enabling full-scale monitoring and highly sensitive A/B testing via CUPED.",
        "url": "http://arxiv.org/abs/2601.18253v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18253v1",
        "arxiv_id": "2601.18253v1",
        "authors": [
            "Peng Sun",
            "Xiangyu Zhang",
            "Duan Wu"
        ],
        "submitted": "2026-01-26 08:20:02",
        "source": "arxiv",
        "comment": "This is a pre-print"
    },
    {
        "title": "GenCI: Generative Modeling of User Interest Shift via Cohort-based Intent Learning for CTR Prediction",
        "abstract": "Click-through rate (CTR) prediction plays a pivotal role in online advertising and recommender systems. Despite notable progress in modeling user preferences from historical behaviors, two key challenges persist. First, exsiting discriminative paradigms focus on matching candidates to user history, often overfitting to historically dominant features and failing to adapt to rapid interest shifts. Second, a critical information chasm emerges from the point-wise ranking paradigm. By scoring each candidate in isolation, CTR models discard the rich contextual signal implied by the recalled set as a whole, leading to a misalignment where long-term preferences often override the user's immediate, evolving intent. To address these issues, we propose GenCI, a generative user intent framework that leverages semantic interest cohorts to model dynamic user preferences for CTR prediction. The framework first employs a generative model, trained with a next-item prediction (NTP) objective, to proactively produce candidate interest cohorts. These cohorts serve as explicit, candidate-agnostic representations of a user's immediate intent. A hierarchical candidate-aware network then injects this rich contextual signal into the ranking stage, refining them with cross-attention to align with both user history and the target item. The entire model is trained end-to-end, creating a more aligned and effective CTR prediction pipeline. Extensive experiments on three widely used datasets demonstrate the effectiveness of our approach.",
        "url": "http://arxiv.org/abs/2601.18251v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18251v1",
        "arxiv_id": "2601.18251v1",
        "authors": [
            "Kesha Ou",
            "Zhen Tian",
            "Wayne Xin Zhao",
            "Hongyu Lu",
            "Ji-Rong Wen"
        ],
        "submitted": "2026-01-26 08:15:04",
        "source": "arxiv",
        "comment": "Accepted by WWW 2026 Research Track"
    },
    {
        "title": "TechING: Towards Real World Technical Image Understanding via VLMs",
        "abstract": "Professionals working in technical domain typically hand-draw (on whiteboard, paper, etc.) technical diagrams (e.g., flowcharts, block diagrams, etc.) during discussions; however, if they want to edit these later, it needs to be drawn from scratch. Modern day VLMs have made tremendous progress in image understanding but they struggle when it comes to understanding technical diagrams. One way to overcome this problem is to fine-tune on real world hand-drawn images, but it is not practically possible to generate large number of such images. In this paper, we introduce a large synthetically generated corpus (reflective of real world images) for training VLMs and subsequently evaluate VLMs on a smaller corpus of hand-drawn images (with the help of humans). We introduce several new self-supervision tasks for training and perform extensive experiments with various baseline models and fine-tune Llama 3.2 11B-instruct model on synthetic images on these tasks to obtain LLama-VL-TUG, which significantly improves the ROUGE-L performance of Llama 3.2 11B-instruct by 2.14x and achieves the best all-round performance across all baseline models. On real-world images, human evaluation reveals that we achieve minimum compilation errors across all baselines in 7 out of 8 diagram types and improve the average F1 score of Llama 3.2 11B-instruct by 6.97x.",
        "url": "http://arxiv.org/abs/2601.18238v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18238v1",
        "arxiv_id": "2601.18238v1",
        "authors": [
            "Tafazzul Nadeem",
            "Bhavik Shangari",
            "Manish Rai",
            "Gagan Raj Gupta",
            "Ashutosh Modi"
        ],
        "submitted": "2026-01-26 07:43:55",
        "source": "arxiv",
        "comment": "Accepted at Findings of EACL 2026, 30 Pages (9 Pages main paper + 4 pages references + 17 pages appendix)"
    },
    {
        "title": "Generative AI in Saudi Arabia: A National Survey of Adoption, Risks, and Public Perceptions",
        "abstract": "Generative Artificial Intelligence (GenAI) is rapidly becoming embedded in Saudi Arabia's digital transformation under Vision 2030, yet public awareness, adoption, and concerns surrounding these tools remain underexplored. This study provides an early snapshot of GenAI engagement among Saudi nationals. Using a nationwide survey of 330 participants across regions, age groups, and employment sectors, we examine seven dimensions of GenAI use: awareness and understanding, adoption patterns, perceived impacts, training needs, risks and barriers, data-sharing behaviors, and future expectations. Findings show that 93% of respondents actively use GenAI primarily for text-based tasks, while more advanced uses such as programming or multimodal generation are less common. Despite the prevalence of use, overall awareness and conceptual understanding remain uneven, with many reporting limited technical knowledge. Participants recognize GenAI's benefits for productivity, work quality, and understanding complex information, yet caution that sustained reliance may undermine critical thinking and key professional skills. Trust in AI-generated outputs remains cautious, with widespread concerns about privacy, misinformation, and ethical misuse, including potential job displacement. Respondents show strong interest in structured GenAI training that combines foundational skills, domain-specific applications, and clear guidance on privacy, ethics, and responsible use. These results establish a baseline for GenAI engagement in Saudi Arabia and highlight priorities for policymakers and developers: expanding AI literacy, ensuring culturally and linguistically aligned GenAI solutions, and strengthening frameworks for privacy and responsible deployment.",
        "url": "http://arxiv.org/abs/2601.18234v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18234v1",
        "arxiv_id": "2601.18234v1",
        "authors": [
            "Abdulaziz AlDakheel",
            "Ali Alshehre",
            "Esraa Alamoudi",
            "Moslim AlKhabbaz",
            "Ahmed Aljohani",
            "Raed Alharbi"
        ],
        "submitted": "2026-01-26 07:40:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PaperTok: Exploring the Use of Generative AI for Creating Short-form Videos for Research Communication",
        "abstract": "The dissemination of scholarly research is critical, yet researchers often lack the time and skills to create engaging content for popular media such as short-form videos. To address this gap, we explore the use of generative AI to help researchers transform their academic papers into accessible video content. Informed by a formative study with science communicators and content creators (N=8), we designed PaperTok, an end-to-end system that automates the initial creative labor by generating script options and corresponding audiovisual content from a source paper. Researchers can then refine based on their preferences with further prompting. A mixed-methods user study (N=18) and crowdsourced evaluation (N=100) demonstrate that PaperTok's workflow can help researchers create engaging and informative short-form videos. We also identified the need for more fine-grained controls in the creation process. To this end, we offer implications for future generative tools that support science outreach.",
        "url": "http://arxiv.org/abs/2601.18218v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18218v1",
        "arxiv_id": "2601.18218v1",
        "authors": [
            "Meziah Ruby Cristobal",
            "Hyeonjeong Byeon",
            "Tze-Yu Chen",
            "Ruoxi Shang",
            "Donghoon Shin",
            "Ruican Zhong",
            "Tony Zhou",
            "Gary Hsieh"
        ],
        "submitted": "2026-01-26 07:08:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Generative Chain of Behavior for User Trajectory Prediction",
        "abstract": "Modeling long-term user behavior trajectories is essential for understanding evolving preferences and enabling proactive recommendations. However, most sequential recommenders focus on next-item prediction, overlooking dependencies across multiple future actions. We propose Generative Chain of Behavior (GCB), a generative framework that models user interactions as an autoregressive chain of semantic behaviors over multiple future steps. GCB first encodes items into semantic IDs via RQ-VAE with k-means refinement, forming a discrete latent space that preserves semantic proximity. On top of this space, a transformer-based autoregressive generator predicts multi-step future behaviors conditioned on user history, capturing long-horizon intent transitions and generating coherent trajectories. Experiments on benchmark datasets show that GCB consistently outperforms state-of-the-art sequential recommenders in multi-step accuracy and trajectory consistency. Beyond these gains, GCB offers a unified generative formulation for capturing user preference evolution.",
        "url": "http://arxiv.org/abs/2601.18213v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18213v1",
        "arxiv_id": "2601.18213v1",
        "authors": [
            "Chengkai Huang",
            "Xiaodi Chen",
            "Hongtao Huang",
            "Quan Z. Sheng",
            "Lina Yao"
        ],
        "submitted": "2026-01-26 06:57:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PaperSearchQA: Learning to Search and Reason over Scientific Papers with RLVR",
        "abstract": "Search agents are language models (LMs) that reason and search knowledge bases (or the web) to answer questions; recent methods supervise only the final answer accuracy using reinforcement learning with verifiable rewards (RLVR). Most RLVR search agents tackle general-domain QA, which limits their relevance to technical AI systems in science, engineering, and medicine. In this work we propose training agents to search and reason over scientific papers -- this tests technical question-answering, it is directly relevant to real scientists, and the capabilities will be crucial to future AI Scientist systems. Concretely, we release a search corpus of 16 million biomedical paper abstracts and construct a challenging factoid QA dataset called PaperSearchQA with 60k samples answerable from the corpus, along with benchmarks. We train search agents in this environment to outperform non-RL retrieval baselines; we also perform further quantitative analysis and observe interesting agent behaviors like planning, reasoning, and self-verification. Our corpus, datasets, and benchmarks are usable with the popular Search-R1 codebase for RLVR training and released on https://huggingface.co/collections/jmhb/papersearchqa. Finally, our data creation methods are scalable and easily extendable to other scientific domains.",
        "url": "http://arxiv.org/abs/2601.18207v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18207v1",
        "arxiv_id": "2601.18207v1",
        "authors": [
            "James Burgess",
            "Jan N. Hansen",
            "Duo Peng",
            "Yuhui Zhang",
            "Alejandro Lozano",
            "Min Woo Sun",
            "Emma Lundberg",
            "Serena Yeung-Levy"
        ],
        "submitted": "2026-01-26 06:46:16",
        "source": "arxiv",
        "comment": "EACL 2026"
    },
    {
        "title": "MemWeaver: Weaving Hybrid Memories for Traceable Long-Horizon Agentic Reasoning",
        "abstract": "Large language model-based agents operating in long-horizon interactions require memory systems that support temporal consistency, multi-hop reasoning, and evidence-grounded reuse across sessions. Existing approaches largely rely on unstructured retrieval or coarse abstractions, which often lead to temporal conflicts, brittle reasoning, and limited traceability. We propose MemWeaver, a unified memory framework that consolidates long-term agent experiences into three interconnected components: a temporally grounded graph memory for structured relational reasoning, an experience memory that abstracts recurring interaction patterns from repeated observations, and a passage memory that preserves original textual evidence. MemWeaver employs a dual-channel retrieval strategy that jointly retrieves structured knowledge and supporting evidence to construct compact yet information-dense contexts for reasoning. Experiments on the LoCoMo benchmark demonstrate that MemWeaver substantially improves multi-hop and temporal reasoning accuracy while reducing input context length by over 95\\% compared to long-context baselines.",
        "url": "http://arxiv.org/abs/2601.18204v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18204v1",
        "arxiv_id": "2601.18204v1",
        "authors": [
            "Juexiang Ye",
            "Xue Li",
            "Xinyu Yang",
            "Chengkai Huang",
            "Lanshun Nie",
            "Lina Yao",
            "Dechen Zhan"
        ],
        "submitted": "2026-01-26 06:39:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DMAP: Human-Aligned Structural Document Map for Multimodal Document Understanding",
        "abstract": "Existing multimodal document question-answering (QA) systems predominantly rely on flat semantic retrieval, representing documents as a set of disconnected text chunks and largely neglecting their intrinsic hierarchical and relational structures. Such flattening disrupts logical and spatial dependencies - such as section organization, figure-text correspondence, and cross-reference relations, that humans naturally exploit for comprehension. To address this limitation, we introduce a document-level structural Document MAP (DMAP), which explicitly encodes both hierarchical organization and inter-element relationships within multimodal documents. Specifically, we design a Structured-Semantic Understanding Agent to construct DMAP by organizing textual content together with figures, tables, charts, etc. into a human-aligned hierarchical schema that captures both semantic and layout dependencies. Building upon this representation, a Reflective Reasoning Agent performs structure-aware and evidence-driven reasoning, dynamically assessing the sufficiency of retrieved context and iteratively refining answers through targeted interactions with DMAP. Extensive experiments on MMDocQA benchmarks demonstrate that DMAP yields document-specific structural representations aligned with human interpretive patterns, substantially enhancing retrieval precision, reasoning consistency, and multimodal comprehension over conventional RAG-based approaches. Code is available at https://github.com/Forlorin/DMAP",
        "url": "http://arxiv.org/abs/2601.18203v2",
        "pdf_url": "https://arxiv.org/pdf/2601.18203v2",
        "arxiv_id": "2601.18203v2",
        "authors": [
            "ShunLiang Fu",
            "Yanxin Zhang",
            "Yixin Xiang",
            "Xiaoyu Du",
            "Jinhui Tang"
        ],
        "submitted": "2026-01-26 06:38:25",
        "source": "arxiv",
        "comment": "WebConf 2026"
    },
    {
        "title": "Fine-Grained Emotion Detection on GoEmotions: Experimental Comparison of Classical Machine Learning, BiLSTM, and Transformer Models",
        "abstract": "Fine-grained emotion recognition is a challenging multi-label NLP task due to label overlap and class imbalance. In this work, we benchmark three modeling families on the GoEmotions dataset: a TF-IDF-based logistic regression system trained with binary relevance, a BiLSTM with attention, and a BERT model fine-tuned for multi-label classification. Experiments follow the official train/validation/test split, and imbalance is mitigated using inverse-frequency class weights. Across several metrics, namely Micro-F1, Macro-F1, Hamming Loss, and Subset Accuracy, we observe that logistic regression attains the highest Micro-F1 of 0.51, while BERT achieves the best overall balance surpassing the official paper's reported results, reaching Macro-F1 0.49, Hamming Loss 0.036, and Subset Accuracy 0.36. This suggests that frequent emotions often rely on surface lexical cues, whereas contextual representations improve performance on rarer emotions and more ambiguous examples.",
        "url": "http://arxiv.org/abs/2601.18162v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18162v1",
        "arxiv_id": "2601.18162v1",
        "authors": [
            "Ani Harutyunyan",
            "Sachin Kumar"
        ],
        "submitted": "2026-01-26 05:29:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "FP8-RL: A Practical and Stable Low-Precision Stack for LLM Reinforcement Learning",
        "abstract": "Reinforcement learning (RL) for large language models (LLMs) is increasingly bottlenecked by rollout (generation), where long output sequence lengths make attention and KV-cache memory dominate end-to-end step time. FP8 offers an attractive lever for accelerating RL by reducing compute cost and memory traffic during rollout, but applying FP8 in RL introduces unique engineering and algorithmic challenges: policy weights change every step (requiring repeated quantization and weight synchronization into the inference engine) and low-precision rollouts can deviate from the higher-precision policy assumed by the trainer, causing train-inference mismatch and potential instability. This report presents a practical FP8 rollout stack for LLM RL, implemented in the veRL ecosystem with support for common training backends (e.g., FSDP/Megatron-LM) and inference engines (e.g., vLLM/SGLang). We (i) enable FP8 W8A8 linear-layer rollout using blockwise FP8 quantization, (ii) extend FP8 to KV-cache to remove long-context memory bottlenecks via per-step QKV scale recalibration, and (iii) mitigate mismatch using importance-sampling-based rollout correction (token-level TIS/MIS variants). Across dense and MoE models, these techniques deliver up to 44% rollout throughput gains while preserving learning behavior comparable to BF16 baselines.",
        "url": "http://arxiv.org/abs/2601.18150v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18150v1",
        "arxiv_id": "2601.18150v1",
        "authors": [
            "Zhaopeng Qiu",
            "Shuang Yu",
            "Jingqi Zhang",
            "Shuai Zhang",
            "Xue Huang",
            "Jingyi Yang",
            "Junjie Lai"
        ],
        "submitted": "2026-01-26 05:12:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Think When Needed: Model-Aware Reasoning Routing for LLM-based Ranking",
        "abstract": "Large language models (LLMs) are increasingly applied to ranking tasks in retrieval and recommendation. Although reasoning prompting can enhance ranking utility, our preliminary exploration reveals that its benefits are inconsistent and come at a substantial computational cost, suggesting that when to reason is as crucial as how to reason. To address this issue, we propose a reasoning routing framework that employs a lightweight, plug-and-play router head to decide whether to use direct inference (Non-Think) or reasoning (Think) for each instance before generation. The router head relies solely on pre-generation signals: i) compact ranking-aware features (e.g., candidate dispersion) and ii) model-aware difficulty signals derived from a diagnostic checklist reflecting the model's estimated need for reasoning. By leveraging these features before generation, the router outputs a controllable token that determines whether to apply the Think mode. Furthermore, the router can adaptively select its operating policy along the validation Pareto frontier during deployment, enabling dynamic allocation of computational resources toward instances most likely to benefit from Think under varying system constraints. Experiments on three public ranking datasets with different scales of open-source LLMs show consistent improvements in ranking utility with reduced token consumption (e.g., +6.3\\% NDCG@10 with -49.5\\% tokens on MovieLens with Qwen3-4B), demonstrating reasoning routing as a practical solution to the accuracy-efficiency trade-off.",
        "url": "http://arxiv.org/abs/2601.18146v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18146v1",
        "arxiv_id": "2601.18146v1",
        "authors": [
            "Huizhong Guo",
            "Tianjun Wei",
            "Dongxia Wang",
            "Yingpeng Du",
            "Ziyan Wang",
            "Jie Zhang",
            "Zhu Sun"
        ],
        "submitted": "2026-01-26 05:09:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints",
        "abstract": "While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.",
        "url": "http://arxiv.org/abs/2601.18137v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18137v1",
        "arxiv_id": "2601.18137v1",
        "authors": [
            "Yinger Zhang",
            "Shutong Jiang",
            "Renhao Li",
            "Jianhong Tu",
            "Yang Su",
            "Lianghao Deng",
            "Xudong Guo",
            "Chenxu Lv",
            "Junyang Lin"
        ],
        "submitted": "2026-01-26 04:43:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Typhoon-S: Minimal Open Post-Training for Sovereign Large Language Models",
        "abstract": "Large language models (LLMs) have progressed rapidly; however, most state-of-the-art models are trained and evaluated primarily in high-resource languages such as English and Chinese, and are often developed by a small number of organizations with access to large-scale compute and data. This gatekeeping creates a practical barrier for sovereign settings in which a regional- or national-scale institution or domain owner must retain control and understanding of model weights, training data, and deployment while operating under limited resources and strict transparency constraints. To this end, we identify two core requirements: (1) adoptability, the ability to transform a base model into a general-purpose assistant, and (2) sovereign capability, the ability to perform high-stakes, region-specific tasks (e.g., legal reasoning in local languages and cultural knowledge). We investigate whether these requirements can be achieved without scaling massive instruction corpora or relying on complex preference tuning pipelines and large-scale reinforcement fine-tuning (RFT). We present Typhoon S, a minimal and open post-training recipe that combines supervised fine-tuning, on-policy distillation, and small-scale RFT. Using Thai as a representative case study, we demonstrate that our approach transforms both sovereign-adapted and general-purpose base models into instruction-tuned models with strong general performance. We further show that small-scale RFT with InK-GRPO -- an extension of GRPO that augments the GRPO loss with a next-word prediction loss -- improves Thai legal reasoning and Thai-specific knowledge while preserving general capabilities. Our results suggest that a carefully designed post-training strategy can reduce the required scale of instruction data and computation, providing a practical path toward high-quality sovereign LLMs under academic-scale resources.",
        "url": "http://arxiv.org/abs/2601.18129v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18129v1",
        "arxiv_id": "2601.18129v1",
        "authors": [
            "Kunat Pipatanakul",
            "Pittawat Taveekitworachai"
        ],
        "submitted": "2026-01-26 04:20:59",
        "source": "arxiv",
        "comment": "19 pages. Code is publicly available at https://github.com/scb-10x/typhoon-s . Datasets and model weights are available at https://huggingface.co/collections/typhoon-ai/typhoon-s"
    },
    {
        "title": "FABLE: Forest-Based Adaptive Bi-Path LLM-Enhanced Retrieval for Multi-Document Reasoning",
        "abstract": "The rapid expansion of long-context Large Language Models (LLMs) has reignited debate on whether Retrieval-Augmented Generation (RAG) remains necessary. However, empirical evidence reveals persistent limitations of long-context inference, including the lost-in-the-middle phenomenon, high computational cost, and poor scalability for multi-document reasoning. Conversely, traditional RAG systems, while efficient, are constrained by flat chunk-level retrieval that introduces semantic noise and fails to support structured cross-document synthesis.\n  We present \\textbf{FABLE}, a \\textbf{F}orest-based \\textbf{A}daptive \\textbf{B}i-path \\textbf{L}LM-\\textbf{E}nhanced retrieval framework that integrates LLMs into both knowledge organization and retrieval. FABLE constructs LLM-enhanced hierarchical forest indexes with multi-granularity semantic structures, then employs a bi-path strategy combining LLM-guided hierarchical traversal with structure-aware propagation for fine-grained evidence acquisition, with explicit budget control for adaptive efficiency trade-offs.\n  Extensive experiments demonstrate that FABLE consistently outperforms SOTA RAG methods and achieves comparable accuracy to full-context LLM inference with up to 94\\% token reduction, showing that long-context LLMs amplify rather than fully replace the need for structured retrieval.",
        "url": "http://arxiv.org/abs/2601.18116v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18116v1",
        "arxiv_id": "2601.18116v1",
        "authors": [
            "Lin Sun",
            "Linglin Zhang",
            "Jingang Huang",
            "Change Jia",
            "Zhengwei Cheng",
            "Xiangzheng Zhang"
        ],
        "submitted": "2026-01-26 04:00:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "GLEN-Bench: A Graph-Language based Benchmark for Nutritional Health",
        "abstract": "Nutritional interventions are important for managing chronic health conditions, but current computational methods provide limited support for personalized dietary guidance. We identify three key gaps: (1) dietary pattern studies often ignore real-world constraints such as socioeconomic status, comorbidities, and limited food access; (2) recommendation systems rarely explain why a particular food helps a given patient; and (3) no unified benchmark evaluates methods across the connected tasks needed for nutritional interventions. We introduce GLEN-Bench, the first comprehensive graph-language based benchmark for nutritional health assessment. We combine NHANES health records, FNDDS food composition data, and USDA food-access metrics to build a knowledge graph that links demographics, health conditions, dietary behaviors, poverty-related constraints, and nutrient needs. We test the benchmark using opioid use disorder, where models must detect subtle nutritional differences across disease stages. GLEN-Bench includes three linked tasks: risk detection identifies at-risk individuals from dietary and socioeconomic patterns; recommendation suggests personalized foods that meet clinical needs within resource constraints; and question answering provides graph-grounded, natural-language explanations to facilitate comprehension. We evaluate these graph-language approaches, including graph neural networks, large language models, and hybrid architectures, to establish solid baselines and identify practical design choices. Our analysis identifies clear dietary patterns linked to health risks, providing insights that can guide practical interventions.",
        "url": "http://arxiv.org/abs/2601.18106v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18106v1",
        "arxiv_id": "2601.18106v1",
        "authors": [
            "Jiatan Huang",
            "Zheyuan Zhang",
            "Tianyi Ma",
            "Mingchen Li",
            "Yaning Zheng",
            "Yanfang Ye",
            "Chuxu Zhang"
        ],
        "submitted": "2026-01-26 03:32:46",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CHiRPE: A Step Towards Real-World Clinical NLP with Clinician-Oriented Model Explanations",
        "abstract": "The medical adoption of NLP tools requires interpretability by end users, yet traditional explainable AI (XAI) methods are misaligned with clinical reasoning and lack clinician input. We introduce CHiRPE (Clinical High-Risk Prediction with Explainability), an NLP pipeline that takes transcribed semi-structured clinical interviews to: (i) predict psychosis risk; and (ii) generate novel SHAP explanation formats co-developed with clinicians. Trained on 944 semi-structured interview transcripts across 24 international clinics of the AMP-SCZ study, the CHiRPE pipeline integrates symptom-domain mapping, LLM summarisation, and BERT classification. CHiRPE achieved over 90% accuracy across three BERT variants and outperformed baseline models. Explanation formats were evaluated by 28 clinical experts who indicated a strong preference for our novel concept-guided explanations, especially hybrid graph-and-text summary formats. CHiRPE demonstrates that clinically-guided model development produces both accurate and interpretable results. Our next step is focused on real-world testing across our 24 international sites.",
        "url": "http://arxiv.org/abs/2601.18102v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18102v1",
        "arxiv_id": "2601.18102v1",
        "authors": [
            "Stephanie Fong",
            "Zimu Wang",
            "Guilherme C. Oliveira",
            "Xiangyu Zhao",
            "Yiwen Jiang",
            "Jiahe Liu",
            "Beau-Luke Colton",
            "Scott Woods",
            "Martha E. Shenton",
            "Barnaby Nelson",
            "Zongyuan Ge",
            "Dominic Dwyer"
        ],
        "submitted": "2026-01-26 03:25:06",
        "source": "arxiv",
        "comment": "This paper is accepted at EACL 2026"
    },
    {
        "title": "Enhancing LLM-based Recommendation with Preference Hint Discovery from Knowledge Graph",
        "abstract": "LLMs have garnered substantial attention in recommendation systems. Yet they fall short of traditional recommenders when capturing complex preference patterns. Recent works have tried integrating traditional recommendation embeddings into LLMs to resolve this issue, yet a core gap persists between their continuous embedding and discrete semantic spaces. Intuitively, textual attributes derived from interactions can serve as critical preference rationales for LLMs' recommendation logic. However, directly inputting such attribute knowledge presents two core challenges: (1) Deficiency of sparse interactions in reflecting preference hints for unseen items; (2) Substantial noise introduction from treating all attributes as hints. To this end, we propose a preference hint discovery model based on the interaction-integrated knowledge graph, enhancing LLM-based recommendation. It utilizes traditional recommendation principles to selectively extract crucial attributes as hints. Specifically, we design a collaborative preference hint extraction schema, which utilizes semantic knowledge from similar users' explicit interactions as hints for unseen items. Furthermore, we develop an instance-wise dual-attention mechanism to quantify the preference credibility of candidate attributes, identifying hints specific to each unseen item. Using these item- and user-based hints, we adopt a flattened hint organization method to shorten input length and feed the textual hint information to the LLM for commonsense reasoning. Extensive experiments on both pair-wise and list-wise recommendation tasks verify the effectiveness of our proposed framework, indicating an average relative improvement of over 3.02% against baselines.",
        "url": "http://arxiv.org/abs/2601.18096v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18096v1",
        "arxiv_id": "2601.18096v1",
        "authors": [
            "Yuting Zhang",
            "Ziliang Pei",
            "Chao Wang",
            "Ying Sun",
            "Fuzhen Zhuang"
        ],
        "submitted": "2026-01-26 03:20:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Sparks of Cooperative Reasoning: LLMs as Strategic Hanabi Agents",
        "abstract": "Cooperative reasoning under incomplete information remains challenging for both humans and multi-agent systems. The card game Hanabi embodies this challenge, requiring theory-of-mind reasoning and strategic communication. We benchmark 17 state-of-the-art LLM agents in 2-5 player games and study the impact of context engineering across model scales (4B to 600B+) to understand persistent coordination failures and robustness to scaffolding: from a minimal prompt with only explicit card details (Watson setting), to scaffolding with programmatic, Bayesian-motivated deductions (Sherlock setting), to multi-turn state tracking via working memory (Mycroft setting). We show that (1) agents can maintain an internal working memory for state tracking and (2) cross-play performance between different LLMs smoothly interpolates with model strength. In the Sherlock setting, the strongest reasoning models exceed 15 points on average across player counts, yet still trail experienced humans and specialist Hanabi agents, both consistently scoring above 20. We release the first public Hanabi datasets with annotated trajectories and move utilities: (1) HanabiLogs, containing 1,520 full game logs for instruction tuning, and (2) HanabiRewards, containing 560 games with dense move-level value annotations for all candidate moves. Supervised and RL finetuning of a 4B open-weight model (Qwen3-Instruct) on our datasets improves cooperative Hanabi play by 21% and 156% respectively, bringing performance to within ~3 points of a strong proprietary reasoning model (o4-mini) and surpassing the best non-reasoning model (GPT-4.1) by 52%. The HanabiRewards RL-finetuned model further generalizes beyond Hanabi, improving performance on a cooperative group-guessing benchmark by 11%, temporal reasoning on EventQA by 6.4%, instruction-following on IFBench-800K by 1.7 Pass@10, and matching AIME 2025 mathematical reasoning Pass@10.",
        "url": "http://arxiv.org/abs/2601.18077v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18077v1",
        "arxiv_id": "2601.18077v1",
        "authors": [
            "Mahesh Ramesh",
            "Kaousheik Jayakumar",
            "Aswinkumar Ramkumar",
            "Pavan Thodima",
            "Aniket Rege"
        ],
        "submitted": "2026-01-26 02:23:47",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Grounded Concreteness: Human-Like Concreteness Sensitivity in Vision-Language Models",
        "abstract": "Do vision--language models (VLMs) develop more human-like sensitivity to linguistic concreteness than text-only large language models (LLMs) when both are evaluated with text-only prompts? We study this question with a controlled comparison between matched Llama text backbones and their Llama Vision counterparts across multiple model scales, treating multimodal pretraining as an ablation on perceptual grounding rather than access to images at inference. We measure concreteness effects at three complementary levels: (i) output behavior, by relating question-level concreteness to QA accuracy; (ii) embedding geometry, by testing whether representations organize along a concreteness axis; and (iii) attention dynamics, by quantifying context reliance via attention-entropy measures. In addition, we elicit token-level concreteness ratings from models and evaluate alignment to human norm distributions, testing whether multimodal training yields more human-consistent judgments. Across benchmarks and scales, VLMs show larger gains on more concrete inputs, exhibit clearer concreteness-structured representations, produce ratings that better match human norms, and display systematically different attention patterns consistent with increased grounding.",
        "url": "http://arxiv.org/abs/2601.18065v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18065v1",
        "arxiv_id": "2601.18065v1",
        "authors": [
            "Aryan Roy",
            "Zekun Wang",
            "Christopher J. MacLellan"
        ],
        "submitted": "2026-01-26 01:48:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Neurocomputational Mechanisms of Syntactic Transfer in Bilingual Sentence Production",
        "abstract": "We discuss the benefits of incorporating into the study of bilingual production errors and their traditionally documented timing signatures (e.g., event-related potentials) certain types of oscillatory signatures, which can offer new implementational-level constraints for theories of bilingualism. We argue that a recent neural model of language, ROSE, can offer a neurocomputational account of syntactic transfer in bilingual production, capturing some of its formal properties and the scope of morphosyntactic sequencing failure modes. We take as a case study cross-linguistic influence (CLI) and attendant theories of functional inhibition/competition, and present these as being driven by specific oscillatory failure modes during L2 sentence planning. We argue that modeling CLI in this way not only offers the kind of linking hypothesis ROSE was built to encourage, but also licenses the exploration of more spatiotemporally complex biomarkers of language dysfunction than more commonly discussed neural signatures.",
        "url": "http://arxiv.org/abs/2601.18056v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18056v1",
        "arxiv_id": "2601.18056v1",
        "authors": [
            "Ahmet Yavuz Uluslu",
            "Elliot Murphy"
        ],
        "submitted": "2026-01-26 01:00:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Addressing LLM Diversity by Infusing Random Concepts",
        "abstract": "Large language models (LLMs) are known to produce outputs with limited diversity. In this work, we study whether infusing random concepts in the prompts can improve the diversity of the generated outputs. To benchmark the approach, we design a systematic evaluation protocol which involves prompting an LLM with questions of the form \"Name 10 Hollywood actors\", and analyzing diversity measures of the resulting LLM outputs. Our experiments on multiple LLMs show that prepending random words/sentences unrelated to the prompt result in greater diversity in the outputs of LLMs. We believe that this promising result and the evaluation protocol opens up interesting avenues for future work, such as how infusing randomness into LLMs could be applied to other domains. Further, the evaluation protocol could also inspire research into benchmarking LLM diversity more systematically.",
        "url": "http://arxiv.org/abs/2601.18053v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18053v1",
        "arxiv_id": "2601.18053v1",
        "authors": [
            "Pulin Agrawal",
            "Prasoon Goyal"
        ],
        "submitted": "2026-01-26 00:53:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Sentipolis: Emotion-Aware Agents for Social Simulations",
        "abstract": "LLM agents are increasingly used for social simulation, yet emotion is often treated as a transient cue, causing emotional amnesia and weak long-horizon continuity. We present Sentipolis, a framework for emotionally stateful agents that integrates continuous Pleasure-Arousal-Dominance (PAD) representation, dual-speed emotion dynamics, and emotion--memory coupling. Across thousands of interactions over multiple base models and evaluators, Sentipolis improves emotionally grounded behavior, boosting communication, and emotional continuity. Gains are model-dependent: believability increases for higher-capacity models but can drop for smaller ones, and emotion-awareness can mildly reduce adherence to social norms, reflecting a human-like tension between emotion-driven behavior and rule compliance in social simulation. Network-level diagnostics show reciprocal, moderately clustered, and temporally stable relationship structures, supporting the study of cumulative social dynamics such as alliance formation and gradual relationship change.",
        "url": "http://arxiv.org/abs/2601.18027v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18027v1",
        "arxiv_id": "2601.18027v1",
        "authors": [
            "Chiyuan Fu",
            "Lyuhao Chen",
            "Yunze Xiao",
            "Weihao Xuan",
            "Carlos Busso",
            "Mona Diab"
        ],
        "submitted": "2026-01-25 22:50:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CommonLID: Re-evaluating State-of-the-Art Language Identification Performance on Web Data",
        "abstract": "Language identification (LID) is a fundamental step in curating multilingual corpora. However, LID models still perform poorly for many languages, especially on the noisy and heterogeneous web data often used to train multilingual language models. In this paper, we introduce CommonLID, a community-driven, human-annotated LID benchmark for the web domain, covering 109 languages. Many of the included languages have been previously under-served, making CommonLID a key resource for developing more representative high-quality text corpora. We show CommonLID's value by using it, alongside five other common evaluation sets, to test eight popular LID models. We analyse our results to situate our contribution and to provide an overview of the state of the art. In particular, we highlight that existing evaluations overestimate LID accuracy for many languages in the web domain. We make CommonLID and the code used to create it available under an open, permissive license.",
        "url": "http://arxiv.org/abs/2601.18026v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18026v1",
        "arxiv_id": "2601.18026v1",
        "authors": [
            "Pedro Ortiz Suarez",
            "Laurie Burchell",
            "Catherine Arnett",
            "Rafael Mosquera-Gómez",
            "Sara Hincapie-Monsalve",
            "Thom Vaughan",
            "Damian Stewart",
            "Malte Ostendorff",
            "Idris Abdulmumin",
            "Vukosi Marivate",
            "Shamsuddeen Hassan Muhammad",
            "Atnafu Lambebo Tonja",
            "Hend Al-Khalifa",
            "Nadia Ghezaiel Hammouda",
            "Verrah Otiende",
            "Tack Hwa Wong",
            "Jakhongir Saydaliev",
            "Melika Nobakhtian",
            "Muhammad Ravi Shulthan Habibi",
            "Chalamalasetti Kranti",
            "Carol Muchemi",
            "Khang Nguyen",
            "Faisal Muhammad Adam",
            "Luis Frentzen Salim",
            "Reem Alqifari",
            "Cynthia Amol",
            "Joseph Marvin Imperial",
            "Ilker Kesen",
            "Ahmad Mustafid",
            "Pavel Stepachev",
            "Leshem Choshen",
            "David Anugraha",
            "Hamada Nayel",
            "Seid Muhie Yimam",
            "Vallerie Alexandra Putra",
            "My Chiffon Nguyen",
            "Azmine Toushik Wasi",
            "Gouthami Vadithya",
            "Rob van der Goot",
            "Lanwenn ar C'horr",
            "Karan Dua",
            "Andrew Yates",
            "Mithil Bangera",
            "Yeshil Bangera",
            "Hitesh Laxmichand Patel",
            "Shu Okabe",
            "Fenal Ashokbhai Ilasariya",
            "Dmitry Gaynullin",
            "Genta Indra Winata",
            "Yiyuan Li",
            "Juan Pablo Martínez",
            "Amit Agarwal",
            "Ikhlasul Akmal Hanif",
            "Raia Abu Ahmad",
            "Esther Adenuga",
            "Filbert Aurelian Tjiaranata",
            "Weerayut Buaphet",
            "Michael Anugraha",
            "Sowmya Vajjala",
            "Benjamin Rice",
            "Azril Hafizi Amirudin",
            "Jesujoba O. Alabi",
            "Srikant Panda",
            "Yassine Toughrai",
            "Bruhan Kyomuhendo",
            "Daniel Ruffinelli",
            "Akshata A",
            "Manuel Goulão",
            "Ej Zhou",
            "Ingrid Gabriela Franco Ramirez",
            "Cristina Aggazzotti",
            "Konstantin Dobler",
            "Jun Kevin",
            "Quentin Pagès",
            "Nicholas Andrews",
            "Nuhu Ibrahim",
            "Mattes Ruckdeschel",
            "Amr Keleg",
            "Mike Zhang",
            "Casper Muziri",
            "Saron Samuel",
            "Sotaro Takeshita",
            "Kun Kerdthaisong",
            "Luca Foppiano",
            "Rasul Dent",
            "Tommaso Green",
            "Ahmad Mustapha Wali",
            "Kamohelo Makaaka",
            "Vicky Feliren",
            "Inshirah Idris",
            "Hande Celikkanat",
            "Abdulhamid Abubakar",
            "Jean Maillard",
            "Benoît Sagot",
            "Thibault Clérice",
            "Kenton Murray",
            "Sarah Luger"
        ],
        "submitted": "2026-01-25 22:49:30",
        "source": "arxiv",
        "comment": "17 pages, 7 tables, 5 figures"
    },
    {
        "title": "A System for Name and Address Parsing with Large Language Models",
        "abstract": "Reliable transformation of unstructured person and address text into structured data remains a key challenge in large-scale information systems. Traditional rule-based and probabilistic approaches perform well on clean inputs but fail under noisy or multilingual conditions, while neural and large language models (LLMs) often lack deterministic control and reproducibility. This paper introduces a prompt-driven, validation-centered framework that converts free-text records into a consistent 17-field schema without fine-tuning. The method integrates input normalisation, structured prompting, constrained decoding, and strict rule-based validation under fixed experimental settings to ensure reproducibility. Evaluations on heterogeneous real-world address data show high field-level accuracy, strong schema adherence, and stable confidence calibration. The results demonstrate that combining deterministic validation with generative prompting provides a robust, interpretable, and scalable solution for structured information extraction, offering a practical alternative to training-heavy or domain-specific models.",
        "url": "http://arxiv.org/abs/2601.18014v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18014v1",
        "arxiv_id": "2601.18014v1",
        "authors": [
            "Adeeba Tarannum",
            "Muzakkiruddin Ahmed Mohammed",
            "Mert Can Cakmak",
            "Shames Al Mandalawi",
            "John Talburt"
        ],
        "submitted": "2026-01-25 22:19:47",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Evaluating Semantic and Syntactic Understanding in Large Language Models for Payroll Systems",
        "abstract": "Large language models are now used daily for writing, search, and analysis, and their natural language understanding continues to improve. However, they remain unreliable on exact numerical calculation and on producing outputs that are straightforward to audit. We study synthetic payroll system as a focused, high-stakes example and evaluate whether models can understand a payroll schema, apply rules in the right order, and deliver cent-accurate results. Our experiments span a tiered dataset from basic to complex cases, a spectrum of prompts from minimal baselines to schema-guided and reasoning variants, and multiple model families including GPT, Claude, Perplexity, Grok and Gemini. Results indicate clear regimes where careful prompting is sufficient and regimes where explicit computation is required. The work offers a compact, reproducible framework and practical guidance for deploying LLMs in settings that demand both accuracy and assurance.",
        "url": "http://arxiv.org/abs/2601.18012v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18012v1",
        "arxiv_id": "2601.18012v1",
        "authors": [
            "Hendrika Maclean",
            "Mert Can Cakmak",
            "Muzakkiruddin Ahmed Mohammed",
            "Shames Al Mandalawi",
            "John Talburt"
        ],
        "submitted": "2026-01-25 22:12:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Post-Training Denoising of User Profiles with LLMs in Collaborative Filtering Recommendation",
        "abstract": "Implicit feedback -- the main data source for training Recommender Systems (RSs) -- is inherently noisy and has been shown to negatively affect recommendation effectiveness. Denoising has been proposed as a method for removing noisy implicit feedback and improving recommendations. Prior work has focused on in-training denoising, however this requires additional data, changes to the model architecture and training procedure or fine-tuning, all of which can be costly and data hungry. In this work, we focus on post-training denoising. Different from in-training denoising, post-training denoising does not involve changing the architecture of the model nor its training procedure, and does not require additional data. Specifically, we present a method for post-training denoising user profiles using Large Language Models (LLMs) for Collaborative Filtering (CF) recommendations. Our approach prompts LLMs with (i) a user profile (user interactions), (ii) a candidate item, and (iii) its rank as given by the CF recommender, and asks the LLM to remove items from the user profile to improve the rank of the candidate item. Experiments with a state-of-the-art CF recommender and 4 open and closed source LLMs in 3 datasets show that our denoising yields improvements up to 13% in effectiveness over the original user profiles. Our code is available at https://github.com/edervishaj/denoising-user-profiles-LLM.",
        "url": "http://arxiv.org/abs/2601.18009v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18009v1",
        "arxiv_id": "2601.18009v1",
        "authors": [
            "Ervin Dervishaj",
            "Maria Maistro",
            "Tuukka Ruotsalo",
            "Christina Lioma"
        ],
        "submitted": "2026-01-25 22:01:16",
        "source": "arxiv",
        "comment": "Accepted at the 48th European Conference on Information Retrieval (ECIR 2026)"
    },
    {
        "title": "PEAR: Pairwise Evaluation for Automatic Relative Scoring in Machine Translation",
        "abstract": "We present PEAR (Pairwise Evaluation for Automatic Relative Scoring), a supervised Quality Estimation (QE) metric family that reframes reference-free Machine Translation (MT) evaluation as a graded pairwise comparison. Given a source segment and two candidate translations, PEAR predicts the direction and magnitude of their quality difference. The metrics are trained using pairwise supervision derived from differences in human judgments, with an additional regularization term that encourages sign inversion under candidate order reversal. On the WMT24 meta-evaluation benchmark, PEAR outperforms strictly matched single-candidate QE baselines trained with the same data and backbones, isolating the benefit of the proposed pairwise formulation. Despite using substantially fewer parameters than recent large metrics, PEAR surpasses far larger QE models and reference-based metrics. Our analysis further indicates that PEAR yields a less redundant evaluation signal relative to other top metrics. Finally, we show that PEAR is an effective utility function for Minimum Bayes Risk (MBR) decoding, reducing pairwise scoring cost at negligible impact.",
        "url": "http://arxiv.org/abs/2601.18006v1",
        "pdf_url": "https://arxiv.org/pdf/2601.18006v1",
        "arxiv_id": "2601.18006v1",
        "authors": [
            "Lorenzo Proietti",
            "Roman Grundkiewicz",
            "Matt Post"
        ],
        "submitted": "2026-01-25 21:52:30",
        "source": "arxiv",
        "comment": "18 pages"
    },
    {
        "title": "AI-based approach to burnout identification from textual data",
        "abstract": "This study introduces an AI-based methodology that utilizes natural language processing (NLP) to detect burnout from textual data. The approach relies on a RuBERT model originally trained for sentiment analysis and subsequently fine-tuned for burnout detection using two data sources: synthetic sentences generated with ChatGPT and user comments collected from Russian YouTube videos about burnout. The resulting model assigns a burnout probability to input texts and can be applied to process large volumes of written communication for monitoring burnout-related language signals in high-stress work environments.",
        "url": "http://arxiv.org/abs/2601.17993v1",
        "pdf_url": "https://arxiv.org/pdf/2601.17993v1",
        "arxiv_id": "2601.17993v1",
        "authors": [
            "Marina Zavertiaeva",
            "Petr Parshakov",
            "Mikhail Usanin",
            "Aleksei Smirnov",
            "Sofia Paklina",
            "Anastasiia Kibardina"
        ],
        "submitted": "2026-01-25 21:01:41",
        "source": "arxiv",
        "comment": "9 pages, 2 figures"
    },
    {
        "title": "SD-E$^2$: Semantic Exploration for Reasoning Under Token Budgets",
        "abstract": "Small language models (SLMs) struggle with complex reasoning because exploration is expensive under tight compute budgets. We introduce Semantic Diversity-Exploration-Exploitation (SD-E$^2$), a reinforcement learning framework that makes exploration explicit by optimizing semantic diversity in generated reasoning trajectories. Using a frozen sentence-embedding model, SD-E$^2$ assigns a diversity reward that captures (i) the coverage of semantically distinct solution strategies and (ii) their average pairwise dissimilarity in embedding space, rather than surface-form novelty. This diversity reward is combined with outcome correctness and solution efficiency in a z-score-normalized multi-objective objective that stabilizes training. On GSM8K, SD-E$^2$ surpasses the base Qwen2.5-3B-Instruct and strong GRPO baselines (GRPO-CFL and GRPO-CFEE) by +27.4, +5.2, and +1.5 percentage points, respectively, while discovering on average 9.8 semantically distinct strategies per question. We further improve MedMCQA to 49.64% versus 38.37% for the base model and show gains on the harder AIME benchmark (1983-2025), reaching 13.28% versus 6.74% for the base. These results indicate that rewarding semantic novelty yields a more compute-efficient exploration-exploitation signal for training reasoning-capable SLMs. By introducing cognitive adaptation-adjusting the reasoning process structure rather than per-token computation-SD-E$^2$ offers a complementary path to efficiency gains in resource-constrained models.",
        "url": "http://arxiv.org/abs/2601.17982v1",
        "pdf_url": "https://arxiv.org/pdf/2601.17982v1",
        "arxiv_id": "2601.17982v1",
        "authors": [
            "Kshitij Mishra",
            "Nils Lukas",
            "Salem Lahlou"
        ],
        "submitted": "2026-01-25 20:21:52",
        "source": "arxiv",
        "comment": "Accepted at EACL 2026"
    },
    {
        "title": "LLMs as Cultural Archives: Cultural Commonsense Knowledge Graph Extraction",
        "abstract": "Large language models (LLMs) encode rich cultural knowledge learned from diverse web-scale data, offering an unprecedented opportunity to model cultural commonsense at scale. Yet this knowledge remains mostly implicit and unstructured, limiting its interpretability and use. We present an iterative, prompt-based framework for constructing a Cultural Commonsense Knowledge Graph (CCKG) that treats LLMs as cultural archives, systematically eliciting culture-specific entities, relations, and practices and composing them into multi-step inferential chains across languages. We evaluate CCKG on five countries with human judgments of cultural relevance, correctness, and path coherence. We find that the cultural knowledge graphs are better realized in English, even when the target culture is non-English (e.g., Chinese, Indonesian, Arabic), indicating uneven cultural encoding in current LLMs. Augmenting smaller LLMs with CCKG improves performance on cultural reasoning and story generation, with the largest gains from English chains. Our results show both the promise and limits of LLMs as cultural technologies and that chain-structured cultural knowledge is a practical substrate for culturally grounded NLP.",
        "url": "http://arxiv.org/abs/2601.17971v1",
        "pdf_url": "https://arxiv.org/pdf/2601.17971v1",
        "arxiv_id": "2601.17971v1",
        "authors": [
            "Junior Cedric Tonga",
            "Chen Cecilia Liu",
            "Iryna Gurevych",
            "Fajri Koto"
        ],
        "submitted": "2026-01-25 20:05:04",
        "source": "arxiv",
        "comment": "EACL 2026 MAIN"
    },
    {
        "title": "A Monosemantic Attribution Framework for Stable Interpretability in Clinical Neuroscience Large Language Models",
        "abstract": "Interpretability remains a key challenge for deploying large language models (LLMs) in clinical settings such as Alzheimer's disease progression diagnosis, where early and trustworthy predictions are essential. Existing attribution methods exhibit high inter-method variability and unstable explanations due to the polysemantic nature of LLM representations, while mechanistic interpretability approaches lack direct alignment with model inputs and outputs and do not provide explicit importance scores. We introduce a unified interpretability framework that integrates attributional and mechanistic perspectives through monosemantic feature extraction. By constructing a monosemantic embedding space at the level of an LLM layer and optimizing the framework to explicitly reduce inter-method variability, our approach produces stable input-level importance scores and highlights salient features via a decompressed representation of the layer of interest, advancing the safe and trustworthy application of LLMs in cognitive health and neurodegenerative disease.",
        "url": "http://arxiv.org/abs/2601.17952v1",
        "pdf_url": "https://arxiv.org/pdf/2601.17952v1",
        "arxiv_id": "2601.17952v1",
        "authors": [
            "Michail Mamalakis",
            "Tiago Azevedo",
            "Cristian Cosentino",
            "Chiara D'Ercoli",
            "Subati Abulikemu",
            "Zhongtian Sun",
            "Richard Bethlehem",
            "Pietro Lio"
        ],
        "submitted": "2026-01-25 19:03:04",
        "source": "arxiv",
        "comment": null
    }
]