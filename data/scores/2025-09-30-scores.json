[
    {
        "title": "jina-reranker-v3: Last but Not Late Interaction for Document Reranking",
        "abstract": "jina-reranker-v3 is a 0.6B parameter multilingual document reranker that\nintroduces a novel last but not late interaction. Unlike late interaction\nmodels such as ColBERT that perform separate encoding followed by multi-vector\nmatching, our approach conducts causal self-attention between query and\ndocuments within the same context window, enabling rich cross-document\ninteractions before extracting contextual embeddings from the last token of\neach document. This compact architecture achieves state-of-the-art BEIR\nperformance with 61.94 nDCG@10 while being ten times smaller than generative\nlistwise rerankers.",
        "url": "http://arxiv.org/abs/2509.25085v1",
        "pdf_url": "http://arxiv.org/pdf/2509.25085v1",
        "arxiv_id": "2509.25085v1",
        "authors": [
            "Feng Wang",
            "Yuqing Li",
            "Han Xiao"
        ],
        "submitted": "2025-09-29 17:23:54",
        "source": "arxiv",
        "comment": "early draft, CoIR table needs to be updated",
        "score": 13,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'listwise' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, specifically document reranking, with a novel approach that leverages cross-document interactions. The use of causal self-attention and compact architecture aligns with your focus on query understanding and ranking models. However, the paper's focus on document reranking is more specific than your broader interests in query understanding and user behavior modeling."
    },
    {
        "title": "UniDex: Rethinking Search Inverted Indexing with Unified Semantic Modeling",
        "abstract": "Inverted indexing has traditionally been a cornerstone of modern search\nsystems, leveraging exact term matches to determine relevance between queries\nand documents. However, this term-based approach often emphasizes surface-level\ntoken overlap, limiting the system's generalization capabilities and retrieval\neffectiveness. To address these challenges, we propose UniDex, a novel\nmodel-based method that employs unified semantic modeling to revolutionize\ninverted indexing. UniDex replaces complex manual designs with a streamlined\narchitecture, enhancing semantic generalization while reducing maintenance\noverhead. Our approach involves two key components: UniTouch, which maps\nqueries and documents into semantic IDs for improved retrieval, and UniRank,\nwhich employs semantic matching to rank results effectively. Through\nlarge-scale industrial datasets and real-world online traffic assessments, we\ndemonstrate that UniDex significantly improves retrieval capabilities, marking\na paradigm shift from term-based to model-based indexing. Our deployment within\nKuaishou's short-video search systems further validates UniDex's practical\neffectiveness, serving hundreds of millions of active users efficiently.",
        "url": "http://arxiv.org/abs/2509.24632v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24632v1",
        "arxiv_id": "2509.24632v1",
        "authors": [
            "Zan Li",
            "Jiahui Chen",
            "Yuan Chai",
            "Xiaoze Jiang",
            "Xiaohua Qi",
            "Zhiheng Qin",
            "Runbin Zhou",
            "Shun Zuo",
            "Guangchao Hao",
            "Kefeng Wang",
            "Jingshan Lv",
            "Yupeng Huang",
            "Xiao Liang",
            "Han Li"
        ],
        "submitted": "2025-09-29 11:41:12",
        "source": "arxiv",
        "comment": "11 pages, 6 figures and 5 tables",
        "score": 12,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 9,
        "llm_reason": "This paper aligns closely with your research interests in Information Retrieval, particularly in query understanding and ranking models. The proposed UniDex method, which employs unified semantic modeling for inverted indexing, is a significant advancement in the field. Its focus on semantic generalization and real-time relevance optimization makes it highly relevant to your research."
    },
    {
        "title": "Reference-Free Rating of LLM Responses via Latent Information",
        "abstract": "How reliable are single-response LLM-as-a-judge ratings without references,\nand can we obtain fine-grained, deterministic scores in this setting? We study\nthe common practice of asking a judge model to assign Likert-scale scores to\nfree-text responses and show two systematic issues: scores are unstable under\nsampling and poorly calibrated, leading to compression near the top of the\nscale and frequent ties. We then propose and evaluate Latent Judges, which\nderive scalar ratings from internal model signals: (i) probability-weighted\nscores over integer ratings, (ii) verifier-style probabilities of \"yes\", and\n(iii) linear probes trained on model activations at the rating position. Across\na broad suite of pairwise and single-rating benchmarks, latent methods match or\nsurpass standard prompting, with consistent gains on pairwise accuracy and\nlistwise ranking relevant to Best-of-N selection. Probability-weighted scores\nachieve the strongest single-rating correlations, while probes recover useful\nsignals when output logits are miscalibrated. These results indicate that\nlatent information provides deterministic and more discriminative signals for\nreference-free evaluation, and can improve selection and training approaches\nlike Best-of-$N$, multi-teacher distillation, and routing.",
        "url": "http://arxiv.org/abs/2509.24678v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24678v1",
        "arxiv_id": "2509.24678v1",
        "authors": [
            "Leander Girrbach",
            "Chi-Ping Su",
            "Tankred Saanum",
            "Richard Socher",
            "Eric Schulz",
            "Zeynep Akata"
        ],
        "submitted": "2025-09-29 12:15:52",
        "source": "arxiv",
        "comment": "21 pages",
        "score": 10,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'listwise' (score: +3)",
            "Found 'pairwise' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The paper explores the use of latent information to derive deterministic and more discriminative signals for reference-free evaluation, which aligns with your focus on real-time relevance optimization. While not directly related to e-commerce, the paper's findings have implications for various applications, including search technologies."
    },
    {
        "title": "MRAG-Suite: A Diagnostic Evaluation Platform for Visual Retrieval-Augmented Generation",
        "abstract": "Multimodal Retrieval-Augmented Generation (Visual RAG) significantly advances\nquestion answering by integrating visual and textual evidence. Yet, current\nevaluations fail to systematically account for query difficulty and ambiguity.\nWe propose MRAG-Suite, a diagnostic evaluation platform integrating diverse\nmultimodal benchmarks (WebQA, Chart-RAG, Visual-RAG, MRAG-Bench). We introduce\ndifficulty-based and ambiguity-aware filtering strategies, alongside\nMM-RAGChecker, a claim-level diagnostic tool. Our results demonstrate\nsubstantial accuracy reductions under difficult and ambiguous queries,\nhighlighting prevalent hallucinations. MM-RAGChecker effectively diagnoses\nthese issues, guiding future improvements in Visual RAG systems.",
        "url": "http://arxiv.org/abs/2509.24253v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24253v1",
        "arxiv_id": "2509.24253v1",
        "authors": [
            "Yuelyu Ji"
        ],
        "submitted": "2025-09-29 03:55:28",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses Visual Retrieval-Augmented Generation, which is a related area to Information Retrieval, but it focuses more on multimodal benchmarks and diagnostic evaluation rather than query understanding, ranking models, or user behavior modeling. While it touches on the importance of query difficulty and ambiguity, it doesn't directly align with the user's core research themes."
    },
    {
        "title": "Do Repetitions Matter? Strengthening Reliability in LLM Evaluations",
        "abstract": "LLM leaderboards often rely on single stochastic runs, but how many\nrepetitions are required for reliable conclusions remains unclear. We\nre-evaluate eight state-of-the-art models on the AI4Math Benchmark with three\nindependent runs per setting. Using mixed-effects logistic regression,\ndomain-level marginal means, rank-instability analysis, and run-to-run\nreliability, we assessed the value of additional repetitions. Our findings\nshows that Single-run leaderboards are brittle: 10/12 slices (83\\%) invert at\nleast one pairwise rank relative to the three-run majority, despite a zero\nsign-flip rate for pairwise significance and moderate overall interclass\ncorrelation. Averaging runs yields modest SE shrinkage ($\\sim$5\\% from one to\nthree) but large ranking gains; two runs remove $\\sim$83\\% of single-run\ninversions. We provide cost-aware guidance for practitioners: treat evaluation\nas an experiment, report uncertainty, and use $\\geq 2$ repetitions under\nstochastic decoding. These practices improve robustness while remaining\nfeasible for small teams and help align model comparisons with real-world\nreliability.",
        "url": "http://arxiv.org/abs/2509.24086v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24086v1",
        "arxiv_id": "2509.24086v1",
        "authors": [
            "Miguel Angel Alvarado Gonzalez",
            "Michelle Bruno Hernandez",
            "Miguel Angel Peñaloza Perez",
            "Bruno Lopez Orozco",
            "Jesus Tadeo Cruz Soto",
            "Sandra Malagon"
        ],
        "submitted": "2025-09-28 21:45:20",
        "source": "arxiv",
        "comment": null,
        "score": 9,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'pairwise' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on the reliability of LLM evaluations, specifically the impact of repetition on model rankings. While it touches on aspects of model comparison and evaluation, it doesn't directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest for your research."
    },
    {
        "title": "Investigating Language and Retrieval Bias in Multilingual Previously Fact-Checked Claim Detection",
        "abstract": "Multilingual Large Language Models (LLMs) offer powerful capabilities for\ncross-lingual fact-checking. However, these models often exhibit language bias,\nperforming disproportionately better on high-resource languages such as English\nthan on low-resource counterparts. We also present and inspect a novel concept\n- retrieval bias, when information retrieval systems tend to favor certain\ninformation over others, leaving the retrieval process skewed. In this paper,\nwe study language and retrieval bias in the context of Previously Fact-Checked\nClaim Detection (PFCD). We evaluate six open-source multilingual LLMs across 20\nlanguages using a fully multilingual prompting strategy, leveraging the AMC-16K\ndataset. By translating task prompts into each language, we uncover disparities\nin monolingual and cross-lingual performance and identify key trends based on\nmodel family, size, and prompting strategy. Our findings highlight persistent\nbias in LLM behavior and offer recommendations for improving equity in\nmultilingual fact-checking. To investigate retrieval bias, we employed\nmultilingual embedding models and look into the frequency of retrieved claims.\nOur analysis reveals that certain claims are retrieved disproportionately\nacross different posts, leading to inflated retrieval performance for popular\nclaims while under-representing less common ones.",
        "url": "http://arxiv.org/abs/2509.25138v1",
        "pdf_url": "http://arxiv.org/pdf/2509.25138v1",
        "arxiv_id": "2509.25138v1",
        "authors": [
            "Ivan Vykopal",
            "Antonia Karamolegkou",
            "Jaroslav Kopčan",
            "Qiwei Peng",
            "Tomáš Javůrek",
            "Michal Gregor",
            "Marián Šimko"
        ],
        "submitted": "2025-09-29 17:50:32",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of multilingual fact-checking and language bias. However, the focus on Large Language Models and fact-checking claim detection is not a central match with your primary focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Efficient Sketching and Nearest Neighbor Search Algorithms for Sparse Vector Sets",
        "abstract": "Sparse embeddings of data form an attractive class due to their inherent\ninterpretability: Every dimension is tied to a term in some vocabulary, making\nit easy to visually decipher the latent space. Sparsity, however, poses unique\nchallenges for Approximate Nearest Neighbor Search (ANNS) which finds, from a\ncollection of vectors, the k vectors closest to a query. To encourage research\non this underexplored topic, sparse ANNS featured prominently in a BigANN\nChallenge at NeurIPS 2023, where approximate algorithms were evaluated on large\nbenchmark datasets by throughput and accuracy. In this work, we introduce a set\nof novel data structures and algorithmic methods, a combination of which leads\nto an elegant, effective, and highly efficient solution to sparse ANNS. Our\ncontributions range from a theoretically-grounded sketching algorithm for\nsparse vectors to reduce their effective dimensionality while preserving inner\nproduct-induced ranks; a geometric organization of the inverted index; and the\nblending of local and global information to improve the efficiency and efficacy\nof ANNS. Empirically, our final algorithm, dubbed Seismic, reaches\nsub-millisecond per-query latency with high accuracy on a large-scale benchmark\ndataset using a single CPU.",
        "url": "http://arxiv.org/abs/2509.24815v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24815v1",
        "arxiv_id": "2509.24815v1",
        "authors": [
            "Sebastian Bruch",
            "Franco Maria Nardini",
            "Cosimo Rulli",
            "Rossano Venturini"
        ],
        "submitted": "2025-09-29 14:02:45",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)",
            "Found 'neurips' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Approximate Nearest Neighbor Search (ANNS) for sparse vector sets, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves efficient algorithms and data structures, the topic is more aligned with data mining and related areas, but not with your primary focus on deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "AceSearcher: Bootstrapping Reasoning and Search for LLMs via Reinforced Self-Play",
        "abstract": "Search-augmented LLMs often struggle with complex reasoning tasks due to\nineffective multi-hop retrieval and limited reasoning ability. We propose\nAceSearcher, a cooperative self-play framework that trains a single large\nlanguage model (LLM) to alternate between two roles: a decomposer that breaks\ndown complex queries and a solver that integrates retrieved contexts for answer\ngeneration. AceSearcher couples supervised fine-tuning on a diverse mixture of\nsearch, reasoning, and decomposition tasks with reinforcement fine-tuning\noptimized for final answer accuracy, eliminating the need for intermediate\nannotations. Extensive experiments on three reasoning-intensive tasks across 10\ndatasets show that AceSearcher outperforms state-of-the-art baselines,\nachieving an average exact match improvement of 7.6%. Remarkably, on\ndocument-level finance reasoning tasks, AceSearcher-32B matches the performance\nof the DeepSeek-V3 model using less than 5% of its parameters. Even at smaller\nscales (1.5B and 8B), AceSearcher often surpasses existing search-augmented\nLLMs with up to 9x more parameters, highlighting its exceptional efficiency and\neffectiveness in tackling complex reasoning tasks. Our code will be published\nat https://github.com/ritaranx/AceSearcher and\nhttps://huggingface.co/AceSearcher.",
        "url": "http://arxiv.org/abs/2509.24193v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24193v1",
        "arxiv_id": "2509.24193v1",
        "authors": [
            "Ran Xu",
            "Yuchen Zhuang",
            "Zihan Dong",
            "Jonathan Wang",
            "Yue Yu",
            "Joyce C. Ho",
            "Linjun Zhang",
            "Haoyu Wang",
            "Wenqi Shi",
            "Carl Yang"
        ],
        "submitted": "2025-09-29 02:14:30",
        "source": "arxiv",
        "comment": "Accepted to NeurIPS 2025 (Spotlight)",
        "score": 8,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper proposes a novel framework, AceSearcher, that combines reasoning and search capabilities for large language models. Although it's primarily focused on reasoning tasks, it involves search-augmented LLMs, which aligns with your interests in Information Retrieval and Search technologies. The paper's emphasis on deep semantic understanding and real-time relevance optimization also resonates with your research focus."
    },
    {
        "title": "Retro*: Optimizing LLMs for Reasoning-Intensive Document Retrieval",
        "abstract": "With the growing popularity of LLM agents and RAG, it has become increasingly\nimportant to retrieve documents that are essential for solving a task, even\nwhen their connection to the task is indirect or implicit. Addressing this\nproblem requires fine-grained reasoning to accurately assess the relevance\nbetween the task and each candidate document. This capability, however, poses a\nsignificant challenge for existing IR techniques. Despite recent progress in\nreasoning-enhanced IR, existing approaches still face significant challenges in\napplicability, scalability, and efficiency. In this work, we propose Retro*, a\nnovel approach for reasoning-intensive document retrieval. Our method\nintroduces a rubric-based relevance scoring mechanism, enabling the model to\nreason about the relationship between a task and a document based on explicitly\ndefined criteria, whereby producing a fine-grained, interpretable relevance\nscore. Retro* also supports test-time scaling by combining multiple reasoning\ntrajectories via score integration, which produces more reliable relevance\nestimates. To optimize Retro*'s reasoning capabilities, we introduce a novel\nreinforcement learning algorithm tailored for its relevance scoring mechanism,\nwhich employs two composite rewards to fully exploit the trajectories of each\ntraining sample. Our experiments show that Retro* outperforms existing document\nretrieval methods with notable advantages, leading to state-of-the-art\nperformance on the BRIGHT benchmark.",
        "url": "http://arxiv.org/abs/2509.24869v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24869v1",
        "arxiv_id": "2509.24869v1",
        "authors": [
            "Junwei Lan",
            "Jianlyu Chen",
            "Zheng Liu",
            "Chaofan Li",
            "Siqi Bao",
            "Defu Lian"
        ],
        "submitted": "2025-09-29 14:53:05",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 9,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The focus on reasoning-intensive document retrieval and the use of LLMs aligns with your interests in deep semantic understanding and real-time relevance optimization. The proposed approach, Retro*, also shows potential for scalability and efficiency, which are important considerations in the field."
    },
    {
        "title": "Scaling Generalist Data-Analytic Agents",
        "abstract": "Data-analytic agents are emerging as a key catalyst for automated scientific\ndiscovery and for the vision of Innovating AI. Current approaches, however,\nrely heavily on prompt engineering over proprietary models, while open-source\nmodels struggle to face diverse-format, large-scale data files and\nlong-horizon, multi-step reasoning that real-world analytics demands. This\npaper introduces DataMind, a scalable data synthesis and agent training recipe\ndesigned to build generalist data-analytic agents. DataMind tackles three key\nchallenges in building open-source data-analytic agents, including insufficient\ndata resources, improper training strategy, and unstable code-based multi-turn\nrollout. Concretely, DataMind applies 1) a fine-grained task taxonomy and a\nrecursive easy-to-hard task composition mechanism to increase the diversity and\ndifficulty of synthesized queries; 2) a knowledge-augmented trajectory sampling\nstrategy followed by model-based and rule-based filtering; 3) a dynamically\nadjustable training objective combining both SFT and RL losses; 4) a\nmemory-frugal and stable code-based multi-turn rollout framework. Built on\nDataMind, we curate DataMind-12K, a high-quality trajectory set spanning\ndiverse domains, task categories, and data file formats for data-analytic\ntasks. Trained on DataMind-12K, our DataMind-14B achieves state-of-the-art with\nan average score of 71.16% on multiple data analysis benchmarks, outperforming\nthe strongest proprietary baselines DeepSeek-V3.1 and GPT-5. Our DataMind-7B\nalso performs best among all open-source models with a score of 68.10%. We also\nincorporate some empirical insights gained from our exploratory trials into the\nanalysis experiments, aiming to provide actionable insights about agentic\ntraining for the community. We will release DataMind-12K and DataMind-7B,14B\nfor the community's future research.",
        "url": "http://arxiv.org/abs/2509.25084v1",
        "pdf_url": "http://arxiv.org/pdf/2509.25084v1",
        "arxiv_id": "2509.25084v1",
        "authors": [
            "Shuofei Qiao",
            "Yanqiu Zhao",
            "Zhisong Qiu",
            "Xiaobin Wang",
            "Jintian Zhang",
            "Zhao Bin",
            "Ningyu Zhang",
            "Yong Jiang",
            "Pengjun Xie",
            "Fei Huang",
            "Huajun Chen"
        ],
        "submitted": "2025-09-29 17:23:08",
        "source": "arxiv",
        "comment": "Work in progress",
        "score": 6,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on data-analytic agents and their training, which is not directly related to information retrieval, search technologies, or natural language processing. While it involves some AI-related concepts, the paper's primary focus on data analysis and agent training does not align with the user's core research interests."
    },
    {
        "title": "Agentar-Scale-SQL: Advancing Text-to-SQL through Orchestrated Test-Time Scaling",
        "abstract": "State-of-the-art (SOTA) Text-to-SQL methods still lag significantly behind\nhuman experts on challenging benchmarks like BIRD. Current approaches that\nexplore test-time scaling lack an orchestrated strategy and neglect the model's\ninternal reasoning process. To bridge this gap, we introduce Agentar-Scale-SQL,\na novel framework leveraging scalable computation to improve performance.\nAgentar-Scale-SQL implements an Orchestrated Test-Time Scaling strategy that\nsynergistically combines three distinct perspectives: i) Internal Scaling via\nRL-enhanced Intrinsic Reasoning, ii) Sequential Scaling through Iterative\nRefinement, and iii) Parallel Scaling using Diverse Synthesis and Tournament\nSelection. Agentar-Scale-SQL is a general-purpose framework designed for easy\nadaptation to new databases and more powerful language models. Extensive\nexperiments show that Agentar-Scale-SQL achieves SOTA performance on the BIRD\nbenchmark, reaching 81.67\\% execution accuracy on the test set and ranking\nfirst on the official leaderboard, demonstrating an effective path toward\nhuman-level performance.",
        "url": "http://arxiv.org/abs/2509.24403v2",
        "pdf_url": "http://arxiv.org/pdf/2509.24403v2",
        "arxiv_id": "2509.24403v2",
        "authors": [
            "Pengfei Wang",
            "Baolin Sun",
            "Xuemei Dong",
            "Yaxun Dai",
            "Hongwei Yuan",
            "Mengdie Chu",
            "Yingqi Gao",
            "Xiang Qi",
            "Peng Zhang",
            "Ying Yan"
        ],
        "submitted": "2025-09-29 07:50:02",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "While the paper discusses advancements in Text-to-SQL, a subfield of Natural Language Processing, it does not directly relate to the user's primary focus on Information Retrieval, query understanding, ranking models, and user behavior modeling. The paper's focus on test-time scaling and internal reasoning process in Text-to-SQL is somewhat tangential to the user's interests."
    },
    {
        "title": "Generalist Scanner Meets Specialist Locator: A Synergistic Coarse-to-Fine Framework for Robust GUI Grounding",
        "abstract": "Grounding natural language queries in graphical user interfaces (GUIs)\npresents a challenging task that requires models to comprehend diverse UI\nelements across various applications and systems, while also accurately\npredicting the spatial coordinates for the intended operation. To tackle this\nproblem, we propose GMS: Generalist Scanner Meets Specialist Locator, a\nsynergistic coarse-to-fine framework that effectively improves GUI grounding\nperformance. GMS leverages the complementary strengths of general\nvision-language models (VLMs) and small, task-specific GUI grounding models by\nassigning them distinct roles within the framework. Specifically, the general\nVLM acts as a 'Scanner' to identify potential regions of interest, while the\nfine-tuned grounding model serves as a 'Locator' that outputs precise\ncoordinates within these regions. This design is inspired by how humans perform\nGUI grounding, where the eyes scan the interface and the brain focuses on\ninterpretation and localization. Our whole framework consists of five stages\nand incorporates hierarchical search with cross-modal communication to achieve\npromising prediction results. Experimental results on the ScreenSpot-Pro\ndataset show that while the 'Scanner' and 'Locator' models achieve only $2.0\\%$\nand $3.7\\%$ accuracy respectively when used independently, their integration\nwithin GMS framework yields an overall accuracy of $35.7\\%$, representing a $10\n\\times$ improvement. Additionally, GMS significantly outperforms other strong\nbaselines under various settings, demonstrating its robustness and potential\nfor general-purpose GUI grounding.",
        "url": "http://arxiv.org/abs/2509.24133v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24133v1",
        "arxiv_id": "2509.24133v1",
        "authors": [
            "Zhecheng Li",
            "Guoxian Song",
            "Yiwei Wang",
            "Zhen Xiong",
            "Junsong Yuan",
            "Yujun Cai"
        ],
        "submitted": "2025-09-29 00:06:31",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of query understanding and user behavior modeling. However, the focus on GUI grounding and graphical user interfaces is not directly aligned with your primary interests in search technologies and deep semantic understanding. The paper's use of vision-language models and hierarchical search is an interesting aspect, but it doesn't strongly connect to your core themes."
    },
    {
        "title": "Towards Personalized Deep Research: Benchmarks and Evaluations",
        "abstract": "Deep Research Agents (DRAs) can autonomously conduct complex investigations\nand generate comprehensive reports, demonstrating strong real-world potential.\nHowever, existing evaluations mostly rely on close-ended benchmarks, while\nopen-ended deep research benchmarks remain scarce and typically neglect\npersonalized scenarios. To bridge this gap, we introduce Personalized Deep\nResearch Bench, the first benchmark for evaluating personalization in DRAs. It\npairs 50 diverse research tasks across 10 domains with 25 authentic user\nprofiles that combine structured persona attributes with dynamic real-world\ncontexts, yielding 250 realistic user-task queries. To assess system\nperformance, we propose the PQR Evaluation Framework, which jointly measures\n(P) Personalization Alignment, (Q) Content Quality, and (R) Factual\nReliability. Our experiments on a range of systems highlight current\ncapabilities and limitations in handling personalized deep research. This work\nestablishes a rigorous foundation for developing and evaluating the next\ngeneration of truly personalized AI research assistants.",
        "url": "http://arxiv.org/abs/2509.25106v1",
        "pdf_url": "http://arxiv.org/pdf/2509.25106v1",
        "arxiv_id": "2509.25106v1",
        "authors": [
            "Yuan Liang",
            "Jiaxian Li",
            "Yuqing Wang",
            "Piaohong Wang",
            "Motong Tian",
            "Pai Liu",
            "Shuofei Qiao",
            "Runnan Fang",
            "He Zhu",
            "Ge Zhang",
            "Minghao Liu",
            "Yuchen Eleanor Jiang",
            "Ningyu Zhang",
            "Wangchunshu Zhou"
        ],
        "submitted": "2025-09-29 17:39:17",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'personalization' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses Personalized Deep Research Benchmarks and Evaluations, which is somewhat related to information retrieval and search technologies, particularly in the context of user behavior modeling and query understanding. However, the focus on AI research assistants and personalized scenarios is not directly aligned with the user's core research themes in e-commerce and real-time relevance optimization. The paper's relevance to NLP and data mining is also limited."
    },
    {
        "title": "Ultra-Fast Language Generation via Discrete Diffusion Divergence Instruct",
        "abstract": "Fast generation of language texts is the holy grail that people pursue in the\nAI era. In this work, we introduced Discrete Diffusion Divergence Instruct\n(DiDi-Instruct), a training-based method that leads to fast language generation\nmodels by initializing from a pre-trained (masked) discrete diffusion language\nmodel (dLLM). The resulting DiDi-Instruct model outperforms the dLLM\ncounterparts and the GPT-2 baseline with 64x acceleration. In the theoretical\npart of the paper, we build the foundation of DiDi-Instruct in a framework of\nintegral KL-divergence minimization, with practical training algorithms. We\nalso introduce techniques like grouped reward normalization, intermediate-state\nmatching, and the reward-guided ancestral sampler (RGAS) that significantly\nimprove the training stability, the model coverage, and the inference\nperformances. On OpenWebText, DiDi-Instruct outperforms all accelerated\nlanguage generation models as well as the GPT-2 baseline and the standard\ndLLMs, achieving sample perplexities ranging from 62.2 (8 NFEs) to 18.4 (128\nNFEs). These performance gains are accomplished with a negligible entropy loss\nof about 1% and 20x less additional training wall-clock time. We further\nvalidate the robustness and effectiveness of DiDi-Instruct through extensive\nablation studies, model scaling, and the generation of discrete protein\nsequences. In conclusion, DiDi-Instruct is an efficient yet effective\ndistillation method, enabling language generation in the blink of an eye. We\nwill release both code and models at github.com/haoyangzheng-ai/didi-instruct.",
        "url": "http://arxiv.org/abs/2509.25035v1",
        "pdf_url": "http://arxiv.org/pdf/2509.25035v1",
        "arxiv_id": "2509.25035v1",
        "authors": [
            "Haoyang Zheng",
            "Xinyang Liu",
            "Cindy Xiangrui Kong",
            "Nan Jiang",
            "Zheyuan Hu",
            "Weijian Luo",
            "Wei Deng",
            "Guang Lin"
        ],
        "submitted": "2025-09-29 16:55:44",
        "source": "arxiv",
        "comment": "56 pages, 7 figures, 7 tables",
        "score": 5,
        "keyword_reasons": [
            "Found 'ltr' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on ultra-fast language generation using discrete diffusion divergence instruct, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the primary application and methodology are not aligned with the user's core research themes."
    },
    {
        "title": "KnowGuard: Knowledge-Driven Abstention for Multi-Round Clinical Reasoning",
        "abstract": "In clinical practice, physicians refrain from making decisions when patient\ninformation is insufficient. This behavior, known as abstention, is a critical\nsafety mechanism preventing potentially harmful misdiagnoses. Recent\ninvestigations have reported the application of large language models (LLMs) in\nmedical scenarios. However, existing LLMs struggle with the abstentions,\nfrequently providing overconfident responses despite incomplete information.\nThis limitation stems from conventional abstention methods relying solely on\nmodel self-assessments, which lack systematic strategies to identify knowledge\nboundaries with external medical evidences. To address this, we propose\n\\textbf{KnowGuard}, a novel \\textit{investigate-before-abstain} paradigm that\nintegrates systematic knowledge graph exploration for clinical decision-making.\nOur approach consists of two key stages operating on a shared contextualized\nevidence pool: 1) an evidence discovery stage that systematically explores the\nmedical knowledge space through graph expansion and direct retrieval, and 2) an\nevidence evaluation stage that ranks evidence using multiple factors to adapt\nexploration based on patient context and conversation history. This two-stage\napproach enables systematic knowledge graph exploration, allowing models to\ntrace structured reasoning paths and recognize insufficient medical evidence.\nWe evaluate our abstention approach using open-ended multi-round clinical\nbenchmarks that mimic realistic diagnostic scenarios, assessing abstention\nquality through accuracy-efficiency trade-offs beyond existing closed-form\nevaluations. Experimental evidences clearly demonstrate that KnowGuard\noutperforms state-of-the-art abstention approaches, improving diagnostic\naccuracy by 3.93\\% while reducing unnecessary interaction by 7.27 turns on\naverage.",
        "url": "http://arxiv.org/abs/2509.24816v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24816v1",
        "arxiv_id": "2509.24816v1",
        "authors": [
            "Xilin Dang",
            "Kexin Chen",
            "Xiaorui Su",
            "Ayush Noori",
            "Iñaki Arango",
            "Lucas Vittor",
            "Xinyi Long",
            "Yuyang Du",
            "Marinka Zitnik",
            "Pheng Ann Heng"
        ],
        "submitted": "2025-09-29 14:03:01",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on clinical reasoning and abstention in medical scenarios, using large language models and knowledge graphs. While it involves some aspects of information retrieval and knowledge understanding, it is not directly related to the user's core research themes in information retrieval, search technologies, or e-commerce domain."
    },
    {
        "title": "Latent Visual Reasoning",
        "abstract": "Multimodal Large Language Models (MLLMs) have achieved notable gains in\nvarious tasks by incorporating Chain-of-Thought (CoT) reasoning in language\nspaces. Recent work extends this direction by leveraging external tools for\nvisual editing, thereby enhancing the visual signal along the reasoning\ntrajectories. Nevertheless, these approaches remain fundamentally constrained:\nreasoning is still confined to the language space, with visual information\ntreated as static preconditions. We introduce Latent Visual Reasoning (LVR), a\nnew paradigm that enables autoregressive reasoning directly in the visual\nembedding space. A visual encoder first projects images into visual tokens\nwithin a joint semantic space shared with the language model. The language\nmodel is then trained to generate latent states that reconstruct key visual\ntokens critical for answering the query, constituting the process of latent\nvisual reasoning. By interleaving LVR with standard text generation, our model\nachieves substantial gains on perception-intensive visual question answering\ntasks. In addition, we adapt the GRPO algorithm to conduct reinforcement\nlearning on latent reasoning, further balancing LVR and textual generation. We\nshow that LVR substantially improves fine-grained visual understanding and\nperception, achieving 71.67% on MMVP compared to 66.67% with Qwen2.5-VL. Code\nbase and model weights will be released later.",
        "url": "http://arxiv.org/abs/2509.24251v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24251v1",
        "arxiv_id": "2509.24251v1",
        "authors": [
            "Bangzheng Li",
            "Ximeng Sun",
            "Jiang Liu",
            "Ze Wang",
            "Jialian Wu",
            "Xiaodong Yu",
            "Hao Chen",
            "Emad Barsoum",
            "Muhao Chen",
            "Zicheng Liu"
        ],
        "submitted": "2025-09-29 03:52:01",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper introduces Latent Visual Reasoning (LVR), a paradigm that enables autoregressive reasoning directly in the visual embedding space. While it combines visual and language understanding, the focus is on visual question answering and perception-intensive tasks, which is somewhat related to information retrieval but not directly aligned with your core research themes in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "PET: Preference Evolution Tracking with LLM-Generated Explainable Distribution",
        "abstract": "Understanding how user preference evolves over time is a fundamental\nchallenge central to modern digital ecosystems, for which Large Language Models\n(LLMs) are an increasingly prominent and popular approach due to their ability\nto comprehend the rich semantic context within behavioral data. A common\npractice is to use LLMs to predict a user's next action by directly generating\na ranked list of preferred items. Although effective for short-term prediction,\nthe end-to-end generation paradigm inherently limits personalization. Its\nopaque decision-making process obscures holistic user profiling and exacerbates\npopularity bias. To address these limitations, we propose Preference Evolution\nTracking (PET), a framework that reframes the task as inferring a dynamic\nprobability distribution over a stable and interpretable lattice of preference\nclusters. By applying logit-probing and generative classification techniques,\nPET infers a user's preference as a probability distribution, enabling\ntransparent preference learning. On public benchmarks (Yelp, MovieLens), PET\nimproves ranking quality by up to 40% in NDCG over direct generation baselines.\nOn a large-scale, real-world dataset from a short-video platform, it excels at\nranking long-tail contents, significantly outperforming a SOTA production model\nby 7 times in the NDCG score. Ultimately, PET transforms the user profile model\nfrom direct preference list generation to a transparent distributional\npreference mapping, paving the way for more explainable, fair, and diverse\npersonalization systems.",
        "url": "http://arxiv.org/abs/2509.24189v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24189v1",
        "arxiv_id": "2509.24189v1",
        "authors": [
            "Luyang Zhang",
            "Siyuan Peng",
            "Jialu Wang",
            "Shichao Zhu",
            "Beibei Li",
            "Zhongcun Wang",
            "Guangmou Pan",
            "Yan Li",
            "Song Yang"
        ],
        "submitted": "2025-09-29 02:09:15",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'personalization' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper aligns well with your research interests in Information Retrieval, particularly in query understanding and ranking models. The use of Large Language Models (LLMs) for preference tracking and the focus on improving ranking quality through transparent preference learning are relevant to your work. However, the specific application to user preference evolution and short-video platforms is somewhat outside your primary focus on e-commerce and real-time relevance optimization."
    },
    {
        "title": "Beyond Overall Accuracy: A Psychometric Deep Dive into the Topic-Specific Medical Capabilities of 80 Large Language Models",
        "abstract": "As Large Language Models (LLMs) are increasingly proposed for high-stakes\nmedical applications, there has emerged a critical need for reliable and\naccurate evaluation methodologies. Traditional accuracy metrics fail\ninadequately as they neither capture question characteristics nor offer\ntopic-specific insights. To address this gap, we introduce \\textsc{MedIRT}, a\nrigorous evaluation framework grounded in Item Response Theory (IRT), the gold\nstandard in high-stakes educational testing. Unlike previous research relying\non archival data, we prospectively gathered fresh responses from 80 diverse\nLLMs on a balanced, 1,100-question USMLE-aligned benchmark. Using one\nunidimensional two-parameter logistic IRT model per topic, we estimate LLM's\nlatent model ability jointly with question difficulty and discrimination,\nyielding more stable and nuanced performance rankings than accuracy alone.\nNotably, we identify distinctive ``spiky'' ability profiles, where overall\nrankings can be misleading due to highly specialized model abilities. While\n\\texttt{GPT-5} was the top performer in a majority of domains (8 of 11), it was\noutperformed in Social Science and Communication by \\texttt{Claude-3-opus},\ndemonstrating that even an overall 23rd-ranked model can hold the top spot for\nspecific competencies. Furthermore, we demonstrate IRT's utility in auditing\nbenchmarks by identifying flawed questions. We synthesize these findings into a\npractical decision-support framework that integrates our multi-factor\ncompetency profiles with operational metrics. This work establishes a robust,\npsychometrically grounded methodology essential for the safe, effective, and\ntrustworthy deployment of LLMs in healthcare.",
        "url": "http://arxiv.org/abs/2509.24186v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24186v1",
        "arxiv_id": "2509.24186v1",
        "authors": [
            "Zhimeng Luo",
            "Lixin Wu",
            "Adam Frisch",
            "Daqing He"
        ],
        "submitted": "2025-09-29 02:06:13",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to the user's interests in Information Retrieval and Natural Language Processing, but it focuses on evaluating Large Language Models in a medical context, which is not a central match to the user's research themes. While it involves deep semantic understanding and ranking models, the application domain is healthcare, and the evaluation framework is psychometric, which is not directly related to the user's interests in search technologies and user behavior modeling."
    },
    {
        "title": "SparseD: Sparse Attention for Diffusion Language Models",
        "abstract": "While diffusion language models (DLMs) offer a promising alternative to\nautoregressive models (ARs), existing open-source DLMs suffer from high\ninference latency. This bottleneck is mainly due to the attention's quadratic\ncomplexity with respect to context length in computing all query-key pairs.\nIntuitively, to reduce this complexity, a natural strategy is to restrict\nattention to sparse patterns that retain only the most relevant connections.\nSuch approaches are well-established in ARs, where attention follows fixed and\nclearly defined sparse patterns. However, in DLMs, we observe distinct sparsity\nbehaviors: (1) attention patterns vary across heads, (2) attention patterns in\neach head remain highly similar across denoising steps, and (3) early denoising\nsteps are critical for generation. These findings render sparse attention\nmethods designed for ARs largely incompatible with DLMs, as they fail to\ncapture head-specific structures and risk degrading generation when applied in\nearly denoising steps. To address these challenges, we propose SparseD, a novel\nsparse attention method for DLMs. Leveraging the observations, SparseD only\nrequires pre-computing head-specific sparse patterns one time, and reuses them\nacross all steps. This prevents recomputing sparse patterns at each denoising\nstep. Meanwhile, SparseD uses full attention in the early steps, then switches\nto sparse attention later to maintain generation quality. Together, these\nestablish SparseD as a practical and efficient solution for deploying DLMs in\nlong-context applications. Experimental results demonstrate that SparseD\nachieves lossless acceleration, delivering up to $1.50\\times$ speedup over\nFlashAttention at a 64k context length with 1,024 denoising steps.",
        "url": "http://arxiv.org/abs/2509.24014v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24014v1",
        "arxiv_id": "2509.24014v1",
        "authors": [
            "Zeqing Wang",
            "Gongfan Fang",
            "Xinyin Ma",
            "Xingyi Yang",
            "Xinchao Wang"
        ],
        "submitted": "2025-09-28 18:10:10",
        "source": "arxiv",
        "comment": "The code is available at https://github.com/INV-WZQ/SparseD",
        "score": 5,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on improving the efficiency of diffusion language models through sparse attention, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves natural language processing, the topic is more aligned with deep learning and model optimization."
    },
    {
        "title": "Paired by the Teacher: Turning Unpaired Data into High-Fidelity Pairs for Low-Resource Text Generation",
        "abstract": "We present Paired by the Teacher (PbT), a two-stage teacher-student pipeline\nthat synthesizes accurate input-output pairs without human labels or parallel\ndata. In many low-resource natural language generation (NLG) scenarios,\npractitioners may have only raw outputs, like highlights, recaps, or questions,\nor only raw inputs, such as articles, dialogues, or paragraphs, but seldom\nboth. This mismatch forces small models to learn from very few examples or rely\non costly, broad-scope synthetic examples produced by large LLMs. PbT addresses\nthis by asking a teacher LLM to compress each unpaired example into a concise\nintermediate representation (IR), and training a student to reconstruct inputs\nfrom IRs. This enables outputs to be paired with student-generated inputs,\nyielding high-quality synthetic data. We evaluate PbT on five\nbenchmarks-document summarization (XSum, CNNDM), dialogue summarization\n(SAMSum, DialogSum), and question generation (SQuAD)-as well as an unpaired\nsetting on SwitchBoard (paired with DialogSum summaries). An 8B student trained\nonly on PbT data outperforms models trained on 70 B teacher-generated corpora\nand other unsupervised baselines, coming within 1.2 ROUGE-L of human-annotated\npairs and closing 82% of the oracle gap at one-third the annotation cost of\ndirect synthesis. Human evaluation on SwitchBoard further confirms that only\nPbT produces concise, faithful summaries aligned with the target style,\nhighlighting its advantage of generating in-domain sources that avoid the\nmismatch, limiting direct synthesis.",
        "url": "http://arxiv.org/abs/2509.25144v1",
        "pdf_url": "http://arxiv.org/pdf/2509.25144v1",
        "arxiv_id": "2509.25144v1",
        "authors": [
            "Yen-Ju Lu",
            "Thomas Thebaud",
            "Laureano Moro-Velazquez",
            "Najim Dehak",
            "Jesus Villalba"
        ],
        "submitted": "2025-09-29 17:51:55",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025 (Main Conference)",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on low-resource text generation using a teacher-student pipeline, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the primary application is text generation, which is not a core area of interest."
    },
    {
        "title": "TemMed-Bench: Evaluating Temporal Medical Image Reasoning in Vision-Language Models",
        "abstract": "Existing medical reasoning benchmarks for vision-language models primarily\nfocus on analyzing a patient's condition based on an image from a single visit.\nHowever, this setting deviates significantly from real-world clinical practice,\nwhere doctors typically refer to a patient's historical conditions to provide a\ncomprehensive assessment by tracking their changes over time. In this paper, we\nintroduce TemMed-Bench, the first benchmark designed for analyzing changes in\npatients' conditions between different clinical visits, which challenges large\nvision-language models (LVLMs) to reason over temporal medical images.\nTemMed-Bench consists of a test set comprising three tasks - visual\nquestion-answering (VQA), report generation, and image-pair selection - and a\nsupplementary knowledge corpus of over 17,000 instances. With TemMed-Bench, we\nconduct an evaluation of six proprietary and six open-source LVLMs. Our results\nshow that most LVLMs lack the ability to analyze patients' condition changes\nover temporal medical images, and a large proportion perform only at a\nrandom-guessing level in the closed-book setting. In contrast, GPT o3, o4-mini\nand Claude 3.5 Sonnet demonstrate comparatively decent performance, though they\nhave yet to reach the desired level. Furthermore, we explore augmenting the\ninput with both retrieved visual and textual modalities in the medical domain.\nWe also show that multi-modal retrieval augmentation yields notably higher\nperformance gains than no retrieval and textual retrieval alone across most\nmodels on our benchmark, with the VQA task showing an average improvement of\n2.59%. Overall, we compose a benchmark grounded on real-world clinical\npractice, and it reveals LVLMs' limitations in temporal medical image\nreasoning, as well as highlighting the use of multi-modal retrieval\naugmentation as a potentially promising direction worth exploring to address\nthis challenge.",
        "url": "http://arxiv.org/abs/2509.25143v1",
        "pdf_url": "http://arxiv.org/pdf/2509.25143v1",
        "arxiv_id": "2509.25143v1",
        "authors": [
            "Junyi Zhang",
            "Jia-Chen Gu",
            "Wenbo Hu",
            "Yu Zhou",
            "Robinson Piramuthu",
            "Nanyun Peng"
        ],
        "submitted": "2025-09-29 17:51:26",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is about evaluating vision-language models for temporal medical image reasoning, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on multi-modal retrieval, the focus is on a specific application in the medical domain, and the techniques and models discussed are not directly applicable to your areas of interest."
    },
    {
        "title": "SemanticShield: LLM-Powered Audits Expose Shilling Attacks in Recommender Systems",
        "abstract": "Recommender systems (RS) are widely used in e-commerce for personalized\nsuggestions, yet their openness makes them susceptible to shilling attacks,\nwhere adversaries inject fake behaviors to manipulate recommendations. Most\nexisting defenses emphasize user-side behaviors while overlooking item-side\nfeatures such as titles and descriptions that can expose malicious intent. To\naddress this gap, we propose a two-stage detection framework that integrates\nitem-side semantics via large language models (LLMs). The first stage\npre-screens suspicious users using low-cost behavioral criteria, and the second\nstage employs LLM-based auditing to evaluate semantic consistency. Furthermore,\nwe enhance the auditing model through reinforcement fine-tuning on a\nlightweight LLM with carefully designed reward functions, yielding a\nspecialized detector called SemanticShield. Experiments on six representative\nattack strategies demonstrate the effectiveness of SemanticShield against\nshilling attacks, and further evaluation on previously unseen attack methods\nshows its strong generalization capability. Code is available at\nhttps://github.com/FrankenstLee/SemanticShield.",
        "url": "http://arxiv.org/abs/2509.24961v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24961v1",
        "arxiv_id": "2509.24961v1",
        "authors": [
            "Kaihong Li",
            "Huichi Zhou",
            "Bin Ma",
            "Fangjun Huang"
        ],
        "submitted": "2025-09-29 15:53:47",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to information retrieval, but its focus on recommender systems and shilling attacks is not a central match to your core research themes. However, the use of large language models and semantic understanding is relevant to your interests in NLP and query understanding."
    },
    {
        "title": "When Greedy Wins: Emergent Exploitation Bias in Meta-Bandit LLM Training",
        "abstract": "While Large Language Models (LLMs) hold promise to become autonomous agents,\nthey often explore suboptimally in sequential decision-making. Recent work has\nsought to enhance this capability via supervised fine-tuning (SFT) or\nreinforcement learning (RL), improving regret on the classic multi-armed bandit\ntask. However, it remains unclear how these learning methods shape exploration\nstrategies and how well they generalize. We investigate both paradigms by\ntraining LLMs with SFT on expert trajectories and RL with a range of tailored\nreward signals including a strategic, regret-shaped reward to reduce variance,\nand an algorithmic reward that enables oracle imitation. The resulting agents\noutperform pre-trained models and achieve performance comparable to Upper\nConfidence Bound (UCB) and Thompson Sampling, with robust generalization to 6x\nlonger horizons and across bandit families. Behavioral analysis reveals that\ngains often stem from more sophisticated but greedier exploitation: RL/SFT\nagents are more prone to early catastrophic failure than pre-trained models,\nprematurely abandoning exploration. Furthermore, agents trained to imitate UCB\nlearn to outperform their teacher by adopting more exploitative variants. Our\nfindings clarify when each training paradigm is preferable and advocate\ntailored reward design and evaluation beyond average regret to promote robust\nexploratory behavior.",
        "url": "http://arxiv.org/abs/2509.24923v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24923v1",
        "arxiv_id": "2509.24923v1",
        "authors": [
            "Sanxing Chen",
            "Xiaoyin Chen",
            "Yukun Huang",
            "Roy Xie",
            "Bhuwan Dhingra"
        ],
        "submitted": "2025-09-29 15:25:42",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Large Language Models (LLMs) and their training methods, which is not directly related to your primary research interests in Information Retrieval and Search technologies. While it touches on exploration strategies, it does not address query understanding, ranking models, or user behavior modeling, making it somewhat tangential to your core research themes."
    },
    {
        "title": "Metaphor identification using large language models: A comparison of RAG, prompt engineering, and fine-tuning",
        "abstract": "Metaphor is a pervasive feature of discourse and a powerful lens for\nexamining cognition, emotion, and ideology. Large-scale analysis, however, has\nbeen constrained by the need for manual annotation due to the context-sensitive\nnature of metaphor. This study investigates the potential of large language\nmodels (LLMs) to automate metaphor identification in full texts. We compare\nthree methods: (i) retrieval-augmented generation (RAG), where the model is\nprovided with a codebook and instructed to annotate texts based on its rules\nand examples; (ii) prompt engineering, where we design task-specific verbal\ninstructions; and (iii) fine-tuning, where the model is trained on hand-coded\ntexts to optimize performance. Within prompt engineering, we test zero-shot,\nfew-shot, and chain-of-thought strategies. Our results show that\nstate-of-the-art closed-source LLMs can achieve high accuracy, with fine-tuning\nyielding a median F1 score of 0.79. A comparison of human and LLM outputs\nreveals that most discrepancies are systematic, reflecting well-known grey\nareas and conceptual challenges in metaphor theory. We propose that LLMs can be\nused to at least partly automate metaphor identification and can serve as a\ntestbed for developing and refining metaphor identification protocols and the\ntheory that underpins them.",
        "url": "http://arxiv.org/abs/2509.24866v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24866v1",
        "arxiv_id": "2509.24866v1",
        "authors": [
            "Matteo Fuoli",
            "Weihang Huang",
            "Jeannette Littlemore",
            "Sarah Turner",
            "Ellen Wilding"
        ],
        "submitted": "2025-09-29 14:50:18",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the application of large language models in metaphor identification, which is a topic related to NLP and deep semantic understanding. However, it doesn't directly align with the user's primary focus on information retrieval, query understanding, and ranking models."
    },
    {
        "title": "OrthAlign: Orthogonal Subspace Decomposition for Non-Interfering Multi-Objective Alignment",
        "abstract": "Large language model (LLM) alignment faces a critical dilemma when addressing\nmultiple human preferences: improvements in one dimension frequently come at\nthe expense of others, creating unavoidable trade-offs between competing\nobjectives like helpfulness and harmlessness. While prior work mainly focuses\non constraint-based optimization algorithms and data selection strategies to\nmitigate conflicts, these approaches overlook the fundamental issue of\nresolving conflicts directly at the parameter level. In this paper, we present\nOrthAlign, an innovative approach that pioneers a new paradigm by leveraging\northogonal subspace decomposition to fundamentally resolve gradient-level\nconflicts in multi-objective preference alignment. OrthAlign strategically\ndecomposes parameter update spaces into orthogonal subspaces, ensuring that\noptimization toward different preferences occurs in mathematically\nnon-interfering directions. Building upon this, we provide theoretical\nguarantees demonstrating that when parameter increments satisfy both orthogonal\nsubspace constraints and spectral norm bounds, the resulting updates exhibit\nlinear Lipschitz growth rather than exponential instability, ensuring stable\nconvergence across all preference dimensions. Extensive experiments show that:\nI. OrthAlign achieves maximum single-preference improvements ranging from\n34.61% to 50.89% after multiple-objective alignment across helpful, harmless,\nand truthful dimensions. II. With an average overall reward improvement of\n13.96%.",
        "url": "http://arxiv.org/abs/2509.24610v2",
        "pdf_url": "http://arxiv.org/pdf/2509.24610v2",
        "arxiv_id": "2509.24610v2",
        "authors": [
            "Liang Lin",
            "Zhihao Xu",
            "Junhao Dong",
            "Jian Zhao",
            "Yuchen Yuan",
            "Guibin Zhang",
            "Miao Yu",
            "Yiming Zhang",
            "Zhengtao Yao",
            "Huahui Yi",
            "Dongrui Liu",
            "Xinfeng Li",
            "Kun Wang"
        ],
        "submitted": "2025-09-29 11:16:30",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be focused on large language model alignment, specifically addressing multiple human preferences through orthogonal subspace decomposition. While it touches on optimization and parameter updates, it does not seem to be directly related to information retrieval, search technologies, or query understanding, which are core areas of your research interests."
    },
    {
        "title": "Multi-Item-Query Attention for Stable Sequential Recommendation",
        "abstract": "The inherent instability and noise in user interaction data challenge\nsequential recommendation systems. Prevailing masked attention models, relying\non a single query from the most recent item, are sensitive to this noise,\nreducing prediction reliability. We propose the Multi-Item-Query attention\nmechanism (MIQ-Attn) to enhance model stability and accuracy. MIQ-Attn\nconstructs multiple diverse query vectors from user interactions, effectively\nmitigating noise and improving consistency. It is designed for easy adoption as\na drop-in replacement for existing single-query attention. Experiments show\nMIQ-Attn significantly improves performance on benchmark datasets.",
        "url": "http://arxiv.org/abs/2509.24424v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24424v1",
        "arxiv_id": "2509.24424v1",
        "authors": [
            "Mingshi Xu",
            "Haoren Zhu",
            "Wilfred Siu Hung Ng"
        ],
        "submitted": "2025-09-29 08:11:27",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on sequential recommendation systems, which is a related but distinct area from information retrieval. While it touches on attention mechanisms, it does not explicitly address query understanding, ranking models, or user behavior modeling in the context of search technologies."
    },
    {
        "title": "PAME-AI: Patient Messaging Creation and Optimization using Agentic AI",
        "abstract": "Messaging patients is a critical part of healthcare communication, helping to\nimprove things like medication adherence and healthy behaviors. However,\ntraditional mobile message design has significant limitations due to its\ninability to explore the high-dimensional design space. We develop PAME-AI, a\nnovel approach for Patient Messaging Creation and Optimization using Agentic\nAI. Built on the Data-Information-Knowledge-Wisdom (DIKW) hierarchy, PAME-AI\noffers a structured framework to move from raw data to actionable insights for\nhigh-performance messaging design. PAME-AI is composed of a system of\nspecialized computational agents that progressively transform raw experimental\ndata into actionable message design strategies. We demonstrate our approach's\neffectiveness through a two-stage experiment, comprising of 444,691 patient\nencounters in Stage 1 and 74,908 in Stage 2. The best-performing generated\nmessage achieved 68.76% engagement compared to the 61.27% baseline,\nrepresenting a 12.2% relative improvement in click-through rates. This agentic\narchitecture enables parallel processing, hypothesis validation, and continuous\nlearning, making it particularly suitable for large-scale healthcare\ncommunication optimization.",
        "url": "http://arxiv.org/abs/2509.24263v2",
        "pdf_url": "http://arxiv.org/pdf/2509.24263v2",
        "arxiv_id": "2509.24263v2",
        "authors": [
            "Junjie Luo",
            "Yihong Guo",
            "Anqi Liu",
            "Ritu Agarwal",
            "Gordon Gao"
        ],
        "submitted": "2025-09-29 04:14:46",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'click' (score: +2)",
            "Found 'click-through rate' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be focused on patient messaging in healthcare, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some form of optimization, it is more related to healthcare communication rather than deep semantic understanding or real-time relevance optimization."
    },
    {
        "title": "ScenarioBench: Trace-Grounded Compliance Evaluation for Text-to-SQL and RAG",
        "abstract": "ScenarioBench is a policy-grounded, trace-aware benchmark for evaluating\nText-to-SQL and retrieval-augmented generation in compliance contexts. Each\nYAML scenario includes a no-peek gold-standard package with the expected\ndecision, a minimal witness trace, the governing clause set, and the canonical\nSQL, enabling end-to-end scoring of both what a system decides and why. Systems\nmust justify outputs using clause IDs from the same policy canon, making\nexplanations falsifiable and audit-ready. The evaluator reports decision\naccuracy, trace quality (completeness, correctness, order), retrieval\neffectiveness, SQL correctness via result-set equivalence, policy coverage,\nlatency, and an explanation-hallucination rate. A normalized Scenario\nDifficulty Index (SDI) and a budgeted variant (SDI-R) aggregate results while\naccounting for retrieval difficulty and time. Compared with prior Text-to-SQL\nor KILT/RAG benchmarks, ScenarioBench ties each decision to clause-level\nevidence under strict grounding and no-peek rules, shifting gains toward\njustification quality under explicit time budgets.",
        "url": "http://arxiv.org/abs/2509.24212v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24212v1",
        "arxiv_id": "2509.24212v1",
        "authors": [
            "Zahra Atf",
            "Peter R Lewis"
        ],
        "submitted": "2025-09-29 02:51:08",
        "source": "arxiv",
        "comment": "Accepted for presentation at the LLMs Meet Databases (LMD) Workshop,\n  35th IEEE International Conference on Collaborative Advances in Software and\n  Computing, 2025. Workshop website: https://sites.google.com/view/lmd2025/home",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper introduces a benchmark for evaluating Text-to-SQL and retrieval-augmented generation systems, which is somewhat related to information retrieval and NLP, but not directly focused on query understanding, ranking models, or user behavior modeling. While it touches on aspects of deep semantic understanding, its primary focus is on compliance evaluation and justification quality, which is not a central match for your research interests."
    },
    {
        "title": "BeyondBench: Benchmark-Free Evaluation of Reasoning in Language Models",
        "abstract": "Evaluating language models fairly is becoming harder as static benchmarks\navailable on the internet risk contamination by training data. This makes it\nunclear whether models are truly reasoning or just recalling answers. In this\npaper, we introduce BeyondBench, an evaluation framework that avoids this\nproblem by using algorithmic problem generation. Unlike traditional benchmarks\nthat risk contamination from internet-scale training data, BeyondBench creates\nmathematically grounded problems on the fly, ensuring each test remains fresh\nand uncontaminated. Our framework covers 44 algorithmic tasks with a total of\n117 variations, grouped into three difficulty levels: the Easy Suite (29 tasks)\nfor basic arithmetic and statistics, the Medium Suite (5 tasks, 49 variations)\nfor sequence patterns and reasoning, and the Hard Suite (10 tasks, 68\nvariations) tackling NP-complete and constraint satisfaction problems. Each\ntask generates problems from a combinatorial space larger than 10^15 unique\ninstances, with solutions verified deterministically by mathematical proofs. We\nevaluated 101 language models, including 85 open-source and 16 closed-source\nmodels, spanning sizes from 0.5B to 141B parameters and multiple quantization\nschemes. Our results show consistent reasoning deficiencies across model\nfamilies, with performance degrading sharply as problem complexity increases\nfrom polynomial to exponential. In our Hard Suite evaluations, models such as\nGemini-2.5-pro, Llama-3.3-70B, and Qwen2.5-72B achieved average accuracies of\n56.38%, 26.91%, and 33.60%, respectively. Moreover, we observe that performance\ndrops drastically without tool usage, with GPT-5, GPT-5-mini, and GPT-5-nano\nshowing a decline of 16.81%, 28.05%, and 47.59% accuracy on the hard suite. Our\nleaderboard is publicly available at https://ctrl-gaurav.github.io/BeyondBench/",
        "url": "http://arxiv.org/abs/2509.24210v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24210v1",
        "arxiv_id": "2509.24210v1",
        "authors": [
            "Gaurav Srivastava",
            "Aafiya Hussain",
            "Zhenyu Bi",
            "Swastik Roy",
            "Priya Pitre",
            "Meng Lu",
            "Morteza Ziyadi",
            "Xuan Wang"
        ],
        "submitted": "2025-09-29 02:49:01",
        "source": "arxiv",
        "comment": "113 pages, 5 figures, 30 tables",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on evaluating language models for reasoning capabilities, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on NLP, the context is more about model evaluation rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Retrieval-augmented GUI Agents with Generative Guidelines",
        "abstract": "GUI agents powered by vision-language models (VLMs) show promise in\nautomating complex digital tasks. However, their effectiveness in real-world\napplications is often limited by scarce training data and the inherent\ncomplexity of these tasks, which frequently require long-tailed knowledge\ncovering rare, unseen scenarios. We propose RAG-GUI , a lightweight VLM that\nleverages web tutorials at inference time. RAG-GUI is first warm-started via\nsupervised finetuning (SFT) and further refined through self-guided rejection\nsampling finetuning (RSF). Designed to be model-agnostic, RAG-GUI functions as\na generic plug-in that enhances any VLM-based agent. Evaluated across three\ndistinct tasks, it consistently outperforms baseline agents and surpasses other\ninference baselines by 2.6% to 13.3% across two model sizes, demonstrating\nstrong generalization and practical plug-and-play capabilities in real-world\nscenarios.",
        "url": "http://arxiv.org/abs/2509.24183v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24183v1",
        "arxiv_id": "2509.24183v1",
        "authors": [
            "Ran Xu",
            "Kaixin Ma",
            "Wenhao Yu",
            "Hongming Zhang",
            "Joyce C. Ho",
            "Carl Yang",
            "Dong Yu"
        ],
        "submitted": "2025-09-29 02:04:20",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 (Main Conference)",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the application of vision-language models in GUI agents, which is somewhat related to information retrieval, but the focus is more on NLP and model development. The paper's emphasis on real-world applications and practical plug-and-play capabilities is also relevant, but it does not directly address query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "GateMABSA: Aspect-Image Gated Fusion for Multimodal Aspect-based Sentiment Analysis",
        "abstract": "Aspect-based Sentiment Analysis (ABSA) has recently advanced into the\nmultimodal domain, where user-generated content often combines text and images.\nHowever, existing multimodal ABSA (MABSA) models struggle to filter noisy\nvisual signals, and effectively align aspects with opinion-bearing content\nacross modalities. To address these challenges, we propose GateMABSA, a novel\ngated multimodal architecture that integrates syntactic, semantic, and\nfusion-aware mLSTM. Specifically, GateMABSA introduces three specialized\nmLSTMs: Syn-mLSTM to incorporate syntactic structure, Sem-mLSTM to emphasize\naspect--semantic relevance, and Fuse-mLSTM to perform selective multimodal\nfusion. Extensive experiments on two benchmark Twitter datasets demonstrate\nthat GateMABSA outperforms several baselines.",
        "url": "http://arxiv.org/abs/2509.25037v1",
        "pdf_url": "http://arxiv.org/pdf/2509.25037v1",
        "arxiv_id": "2509.25037v1",
        "authors": [
            "Adamu Lawan",
            "Haruna Yunusa"
        ],
        "submitted": "2025-09-29 16:56:10",
        "source": "arxiv",
        "comment": "6 pages, 2 tables",
        "score": 3,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Natural Language Processing (NLP) and multimodal analysis, but it is not directly focused on Information Retrieval (IR) or query understanding. While it involves deep semantic understanding and multimodal fusion, its primary goal is sentiment analysis, which is a different application area."
    },
    {
        "title": "Building Benchmarks from the Ground Up: Community-Centered Evaluation of LLMs in Healthcare Chatbot Settings",
        "abstract": "Large Language Models (LLMs) are typically evaluated through general or\ndomain-specific benchmarks testing capabilities that often lack grounding in\nthe lived realities of end users. Critical domains such as healthcare require\nevaluations that extend beyond artificial or simulated tasks to reflect the\neveryday needs, cultural practices, and nuanced contexts of communities. We\npropose Samiksha, a community-driven evaluation pipeline co-created with\ncivil-society organizations (CSOs) and community members. Our approach enables\nscalable, automated benchmarking through a culturally aware, community-driven\npipeline in which community feedback informs what to evaluate, how the\nbenchmark is built, and how outputs are scored. We demonstrate this approach in\nthe health domain in India. Our analysis highlights how current multilingual\nLLMs address nuanced community health queries, while also offering a scalable\npathway for contextually grounded and inclusive LLM evaluation.",
        "url": "http://arxiv.org/abs/2509.24506v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24506v1",
        "arxiv_id": "2509.24506v1",
        "authors": [
            "Hamna",
            "Gayatri Bhat",
            "Sourabrata Mukherjee",
            "Faisal Lalani",
            "Evan Hadfield",
            "Divya Siddarth",
            "Kalika Bali",
            "Sunayana Sitaram"
        ],
        "submitted": "2025-09-29 09:20:15",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses the evaluation of Large Language Models (LLMs) in a specific domain (healthcare), which is somewhat related to information retrieval and search technologies. However, the focus on LLMs and community-driven evaluation is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Multilingual Text-to-SQL: Benchmarking the Limits of Language Models with Collaborative Language Agents",
        "abstract": "Text-to-SQL enables natural access to databases, yet most benchmarks are\nEnglish-only, limiting multilingual progress. We introduce MultiSpider 2.0,\nextending Spider 2.0 to eight languages (English, German, French, Spanish,\nPortuguese, Japanese, Chinese, Vietnamese). It preserves Spider 2.0's\nstructural difficulty while adding linguistic and dialectal variability,\ndemanding deeper reasoning for complex SQL. On this benchmark, state-of-the-art\nLLMs (such as DeepSeek-R1 and OpenAI o1) reach only 4\\% execution accuracy when\nrelying on intrinsic reasoning, versus 60\\% on MultiSpider 1.0. Therefore, we\nprovide a collaboration-driven language agents baseline that iteratively\nrefines queries, improving accuracy to 15\\%. These results reveal a substantial\nmultilingual gap and motivate methods that are robust across languages and\nready for real-world enterprise deployment. Our benchmark is available at\nhttps://github.com/phkhanhtrinh23/Multilingual_Text_to_SQL.",
        "url": "http://arxiv.org/abs/2509.24405v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24405v1",
        "arxiv_id": "2509.24405v1",
        "authors": [
            "Khanh Trinh Pham",
            "Thu Huong Nguyen",
            "Jun Jo",
            "Quoc Viet Hung Nguyen",
            "Thanh Tam Nguyen"
        ],
        "submitted": "2025-09-29 07:50:39",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves language models and query refinement, its focus on Text-to-SQL and multilingual benchmarks is not a central match to your areas of expertise."
    },
    {
        "title": "MAS$^2$: Self-Generative, Self-Configuring, Self-Rectifying Multi-Agent Systems",
        "abstract": "The past two years have witnessed the meteoric rise of Large Language Model\n(LLM)-powered multi-agent systems (MAS), which harness collective intelligence\nand exhibit a remarkable trajectory toward self-evolution. This paradigm has\nrapidly progressed from manually engineered systems that require bespoke\nconfiguration of prompts, tools, roles, and communication protocols toward\nframeworks capable of automated orchestration. Yet, dominant automatic\nmulti-agent systems, whether generated by external modules or a single LLM\nagent, largely adhere to a rigid ``\\textit{generate-once-and-deploy}''\nparadigm, rendering the resulting systems brittle and ill-prepared for the\ndynamism and uncertainty of real-world environments. To transcend this\nlimitation, we introduce MAS$^2$, a paradigm predicated on the principle of\nrecursive self-generation: a multi-agent system that autonomously architects\nbespoke multi-agent systems for diverse problems. Technically, we devise a\n``\\textit{generator-implementer-rectifier}'' tri-agent team capable of\ndynamically composing and adaptively rectifying a target agent system in\nresponse to real-time task demands. Collaborative Tree Optimization is proposed\nto train and specialize these meta-agents. Extensive evaluation across seven\nbenchmarks reveals that MAS$^2$ achieves performance gains of up to $19.6\\%$\nover state-of-the-art MAS in complex scenarios such as deep research and code\ngeneration. Moreover, MAS$^2$ exhibits superior cross-backbone generalization,\neffectively leveraging previously unseen LLMs to yield improvements of up to\n$15.1\\%$. Crucially, these gains are attained without incurring excessive token\ncosts, as MAS$^2$ consistently resides on the Pareto frontier of\ncost-performance trade-offs. The source codes are available at\nhttps://github.com/yeyeyeah2/MAS2.",
        "url": "http://arxiv.org/abs/2509.24323v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24323v1",
        "arxiv_id": "2509.24323v1",
        "authors": [
            "Kun Wang",
            "Guibin Zhang",
            "ManKit Ye",
            "Xinyu Deng",
            "Dongxia Wang",
            "Xiaobin Hu",
            "Jinyang Guo",
            "Yang Liu",
            "Yufei Guo"
        ],
        "submitted": "2025-09-29 06:20:10",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on multi-agent systems and large language models, which do not align with your primary focus on information retrieval, query understanding, and ranking models. While it involves some form of optimization, it is not related to real-time relevance optimization or deep semantic understanding in the context of search technologies."
    },
    {
        "title": "Overview of SCIDOCA 2025 Shared Task on Citation Prediction, Discovery, and Placement",
        "abstract": "We present an overview of the SCIDOCA 2025 Shared Task, which focuses on\ncitation discovery and prediction in scientific documents. The task is divided\ninto three subtasks: (1) Citation Discovery, where systems must identify\nrelevant references for a given paragraph; (2) Masked Citation Prediction,\nwhich requires selecting the correct citation for masked citation slots; and\n(3) Citation Sentence Prediction, where systems must determine the correct\nreference for each cited sentence. We release a large-scale dataset constructed\nfrom the Semantic Scholar Open Research Corpus (S2ORC), containing over 60,000\nannotated paragraphs and a curated reference set. The test set consists of\n1,000 paragraphs from distinct papers, each annotated with ground-truth\ncitations and distractor candidates. A total of seven teams registered, with\nthree submitting results. We report performance metrics across all subtasks and\nanalyze the effectiveness of submitted systems. This shared task provides a new\nbenchmark for evaluating citation modeling and encourages future research in\nscientific document understanding. The dataset and task materials are publicly\navailable at https://github.com/daotuanan/scidoca2025-shared-task.",
        "url": "http://arxiv.org/abs/2509.24283v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24283v1",
        "arxiv_id": "2509.24283v1",
        "authors": [
            "An Dao",
            "Vu Tran",
            "Le-Minh Nguyen",
            "Yuji Matsumoto"
        ],
        "submitted": "2025-09-29 04:55:18",
        "source": "arxiv",
        "comment": "16 pages, SCIDOCA 2025",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to the user's interests in Information Retrieval, specifically in the area of citation modeling and scientific document understanding. However, it is not directly focused on query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user. The paper's focus on citation prediction and discovery is somewhat tangential to the user's primary research themes."
    },
    {
        "title": "SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents",
        "abstract": "Large Language Model (LLM) agents excel at multi-step, tool-augmented tasks.\nHowever, smart homes introduce distinct challenges, requiring agents to handle\nlatent user intents, temporal dependencies, device constraints, scheduling, and\nmore. The main bottlenecks for developing smart home agents with such\ncapabilities include the lack of a realistic simulation environment where\nagents can interact with devices and observe the results, as well as a\nchallenging benchmark to evaluate them. To address this, we introduce\n$\\textbf{SimuHome}$, a time-accelerated home environment that simulates smart\ndevices, supports API calls, and reflects changes in environmental variables.\nBy building the simulator on the Matter protocol (the global industry standard\nfor smart home communication), SimuHome provides a high-fidelity environment,\nand agents validated in SimuHome can be deployed on real Matter-compliant\ndevices with minimal adaptation. We provide a challenging benchmark of 600\nepisodes across twelve user query types that require the aforementioned\ncapabilities. Our evaluation of 11 agents under a unified ReAct framework\nreveals that while models perform well on simple tasks, they struggle with\nlatent intent inference, state verification, and especially temporal\nscheduling. Even the top-performing model, GPT-4.1, reaches only 54% success\nrate. These findings highlight a critical need for methods that can reliably\nverify the current state via tools before acting and coordinate time-dependent\nactions.",
        "url": "http://arxiv.org/abs/2509.24282v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24282v1",
        "arxiv_id": "2509.24282v1",
        "authors": [
            "Gyuhyeon Seo",
            "Jungwoo Yang",
            "Junseong Pyo",
            "Nalim Kim",
            "Jonggeun Lee",
            "Yohan Jo"
        ],
        "submitted": "2025-09-29 04:54:20",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper SimuHome: A Temporal- and Environment-Aware Benchmark for Smart Home LLM Agents is somewhat related to your research interests in Information Retrieval and Natural Language Processing, particularly in the context of query understanding and deep semantic understanding. However, the focus on smart home agents and the Matter protocol is not directly aligned with your core research themes, and the paper's emphasis on temporal dependencies and device constraints is more relevant to areas like recommender systems or user behavior modeling."
    },
    {
        "title": "Learning to Ponder: Adaptive Reasoning in Latent Space",
        "abstract": "Test-time compute has emerged as a key paradigm for enhancing LLM reasoning,\nyet prevailing approaches like Best-of-N and majority voting apply uniform\ndepth across inputs, wasting computation on simple queries while potentially\nunder-thinking complex ones. We present FR-Ponder, a single-graph,\nbackbone-training-free framework that allocates instance-adaptive reasoning\ncompute via latent steering. A less than 1M-param controller observes hidden\nstates and decides to halt or apply a small ponder step by adding a\npre-computed steering vector to frozen representations. Our method extracts the\nlatent steering vector associated with deeper reasoning outputs and direct IO\nfrom LLM and re-applies it through a tunable scaling factor, allowing the model\nto adapt its reasoning depth to the complexity of each input. To balance\nperformance and computational cost, we employ Group Relative Policy\nOptimization (GRPO) as a reward signal to adaptively regulate reasoning depth,\nachieving task accuracy while mitigating overreasoning. Through curriculum\nlearning and careful reward engineering, FR-Ponder learns calibrated compute\nallocation correlated with problem difficulty. On GSM8K and MATH500, FR-Ponder\nimproves the compute-accuracy frontier, delivering lower FLOPs with better\nmatched accuracy and comparing favorably to early-exit baselines, without\nmodifying backbone weights. Analyses visualize interpretable steering\ndirections and show learned compute allocation correlates with problem\ndifficulty.",
        "url": "http://arxiv.org/abs/2509.24238v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24238v1",
        "arxiv_id": "2509.24238v1",
        "authors": [
            "Yixin He",
            "Lumingyuan Tang"
        ],
        "submitted": "2025-09-29 03:21:42",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on adaptive reasoning in latent space for Large Language Models (LLMs), which is not directly related to Information Retrieval or Search technologies. While it involves deep semantic understanding, the context is more aligned with NLP and deep learning, rather than IR or Search technologies."
    },
    {
        "title": "Pragmatic Inference for Moral Reasoning Acquisition: Generalization via Distributional Semantics",
        "abstract": "Moral reasoning has emerged as a promising research direction for Large\nLanguage Models (LLMs), yet achieving generalization remains a central\nchallenge. From a linguistic standpoint, this difficulty arises because LLMs\nare adept at capturing distributional semantics, which fundamentally differs\nfrom the morals which operate at the pragmatic level. This paper investigates\nhow LLMs can achieve generalized moral reasoning despite their reliance on\ndistributional semantics. We propose pragmatic inference methods grounded in\nmoral foundations theory, which leverage contextual information at each step to\nbridge the pragmatic gap and guide LLMs in connecting moral foundations with\nmoral reasoning objectives. Experimental results demonstrate that our approach\nsignificantly enhances LLMs' generalization in moral reasoning, providing a\nfoundation for future research grounded in moral foundations theory.",
        "url": "http://arxiv.org/abs/2509.24102v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24102v1",
        "arxiv_id": "2509.24102v1",
        "authors": [
            "Guangliang Liu",
            "Xi Chen",
            "Bocheng Chen",
            "Xitong Zhang",
            "Kristen Johnson"
        ],
        "submitted": "2025-09-28 22:40:58",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on moral reasoning in Large Language Models, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the context and application are quite different from the user's areas of focus."
    },
    {
        "title": "Rethinking Entropy Regularization in Large Reasoning Models",
        "abstract": "Reinforcement learning with verifiable rewards (RLVR) has shown great promise\nin enhancing the reasoning abilities of large reasoning models (LRMs). However,\nit suffers from a critical issue: entropy collapse and premature convergence.\nNaive entropy regularization, a common approach for encouraging exploration in\nthe traditional RL literature, fails to address this problem in the context of\nLRM. Our analysis reveals that this failure stems from the vast action space\nand long trajectories in LRMs, which easily trigger a global entropy explosion\nas the model indiscriminately explores all possible actions and states. To\naddress this, we propose SIREN (SelectIve entRopy rEgularizatioN), a method\nthat confines exploration to a meaningful subset of actions and states. SIREN\nachieves this through a two-step entropy masking mechanism, consisting of a\ntop-p mask and a peak-entropy mask. In addition, regularization is transformed\ninto a self-anchored form to stabilize training. Across five mathematical\nbenchmarks, SIREN attains superior average performance over previous\nentropy-related RLVR approaches, exemplified by a +6.6 maj@k improvement on\nAIME24/25 with Qwen2.5-Math-7B. Further analysis confirms that SIREN promotes\ngreater response diversity and maintains entropy at an appropriate level, which\nhelps to preserve the validation pass@k throughout training. This effectively\nmitigates the premature convergence problem common in RLVR for LRM.",
        "url": "http://arxiv.org/abs/2509.25133v1",
        "pdf_url": "http://arxiv.org/pdf/2509.25133v1",
        "arxiv_id": "2509.25133v1",
        "authors": [
            "Yuxian Jiang",
            "Yafu Li",
            "Guanxu Chen",
            "Dongrui Liu",
            "Yu Cheng",
            "Jing Shao"
        ],
        "submitted": "2025-09-29 17:49:25",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on reinforcement learning and entropy regularization in large reasoning models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on a broader topic of machine learning, the specific context and methodology do not align with your primary areas of focus."
    },
    {
        "title": "Towards Trustworthy Lexical Simplification: Exploring Safety and Efficiency with Small LLMs",
        "abstract": "Despite their strong performance, large language models (LLMs) face\nchallenges in real-world application of lexical simplification (LS),\nparticularly in privacy-sensitive and resource-constrained environments.\nMoreover, since vulnerable user groups (e.g., people with disabilities) are one\nof the key target groups of this technology, it is crucial to ensure the safety\nand correctness of the output of LS systems. To address these issues, we\npropose an efficient framework for LS systems that utilizes small LLMs\ndeployable in local environments. Within this framework, we explore knowledge\ndistillation with synthesized data and in-context learning as baselines. Our\nexperiments in five languages evaluate model outputs both automatically and\nmanually. Our manual analysis reveals that while knowledge distillation boosts\nautomatic metric scores, it also introduces a safety trade-off by increasing\nharmful simplifications. Importantly, we find that the model's output\nprobability is a useful signal for detecting harmful simplifications.\nLeveraging this, we propose a filtering strategy that suppresses harmful\nsimplifications while largely preserving beneficial ones. This work establishes\na benchmark for efficient and safe LS with small LLMs. It highlights the key\ntrade-offs between performance, efficiency, and safety, and demonstrates a\npromising approach for safe real-world deployment.",
        "url": "http://arxiv.org/abs/2509.25086v1",
        "pdf_url": "http://arxiv.org/pdf/2509.25086v1",
        "arxiv_id": "2509.25086v1",
        "authors": [
            "Akio Hayakawa",
            "Stefan Bott",
            "Horacio Saggion"
        ],
        "submitted": "2025-09-29 17:25:56",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores lexical simplification using small LLMs, which is related to NLP, but it doesn't directly align with the user's primary focus on information retrieval, query understanding, and ranking models. The paper's emphasis on safety and efficiency in resource-constrained environments is somewhat relevant to the user's e-commerce background, but the connection is not strong enough to warrant a higher score."
    },
    {
        "title": "DiffTester: Accelerating Unit Test Generation for Diffusion LLMs via Repetitive Pattern",
        "abstract": "Software development relies heavily on extensive unit testing, which makes\nthe efficiency of automated Unit Test Generation (UTG) particularly important.\nHowever, most existing LLMs generate test cases one token at a time in each\nforward pass, which leads to inefficient UTG. Recently, diffusion LLMs (dLLMs)\nhave emerged, offering promising parallel generation capabilities and showing\nstrong potential for efficient UTG. Despite this advantage, their application\nto UTG is still constrained by a clear trade-off between efficiency and test\nquality, since increasing the number of tokens generated in each step often\ncauses a sharp decline in the quality of test cases. To overcome this\nlimitation, we present DiffTester, an acceleration framework specifically\ntailored for dLLMs in UTG. The key idea of DiffTester is that unit tests\ntargeting the same focal method often share repetitive structural patterns. By\ndynamically identifying these common patterns through abstract syntax tree\nanalysis during generation, DiffTester adaptively increases the number of\ntokens produced at each step without compromising the quality of the output. To\nenable comprehensive evaluation, we extend the original TestEval benchmark,\nwhich was limited to Python, by introducing additional programming languages\nincluding Java and C++. Extensive experiments on three benchmarks with two\nrepresentative models show that DiffTester delivers significant acceleration\nwhile preserving test coverage. Moreover, DiffTester generalizes well across\ndifferent dLLMs and programming languages, providing a practical and scalable\nsolution for efficient UTG in software development. Code and data are publicly\navailable at https://github.com/wellbeingyang/DLM4UTG-open .",
        "url": "http://arxiv.org/abs/2509.24975v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24975v1",
        "arxiv_id": "2509.24975v1",
        "authors": [
            "Lekang Yang",
            "Yuetong Liu",
            "Yitong Zhang",
            "Jia Li"
        ],
        "submitted": "2025-09-29 16:04:18",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on unit test generation for diffusion LLMs, which is outside the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves software development, a related domain, the specific topic of unit test generation for LLMs is not directly relevant to the user's core research themes."
    },
    {
        "title": "How Well Do LLMs Imitate Human Writing Style?",
        "abstract": "Large language models (LLMs) can generate fluent text, but their ability to\nreplicate the distinctive style of a specific human author remains unclear. We\npresent a fast, training-free framework for authorship verification and style\nimitation analysis. The method integrates TF-IDF character n-grams with\ntransformer embeddings and classifies text pairs through empirical distance\ndistributions, eliminating the need for supervised training or threshold\ntuning. It achieves 97.5\\% accuracy on academic essays and 94.5\\% in\ncross-domain evaluation, while reducing training time by 91.8\\% and memory\nusage by 59\\% relative to parameter-based baselines. Using this framework, we\nevaluate five LLMs from three separate families (Llama, Qwen, Mixtral) across\nfour prompting strategies - zero-shot, one-shot, few-shot, and text completion.\nResults show that the prompting strategy has a more substantial influence on\nstyle fidelity than model size: few-shot prompting yields up to 23.5x higher\nstyle-matching accuracy than zero-shot, and completion prompting reaches 99.9\\%\nagreement with the original author's style. Crucially, high-fidelity imitation\ndoes not imply human-like unpredictability - human essays average a perplexity\nof 29.5, whereas matched LLM outputs average only 15.2. These findings\ndemonstrate that stylistic fidelity and statistical detectability are\nseparable, establishing a reproducible basis for future work in authorship\nmodeling, detection, and identity-conditioned generation.",
        "url": "http://arxiv.org/abs/2509.24930v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24930v1",
        "arxiv_id": "2509.24930v1",
        "authors": [
            "Rebira Jemama",
            "Rajesh Kumar"
        ],
        "submitted": "2025-09-29 15:34:40",
        "source": "arxiv",
        "comment": "IEEE UEMCON 2025, 11 pages, 4 figures, and 4 tables",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on authorship verification and style imitation analysis using large language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the specific application and methodology are not aligned with your primary focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "MASLegalBench: Benchmarking Multi-Agent Systems in Deductive Legal Reasoning",
        "abstract": "Multi-agent systems (MAS), leveraging the remarkable capabilities of Large\nLanguage Models (LLMs), show great potential in addressing complex tasks. In\nthis context, integrating MAS with legal tasks is a crucial step. While\nprevious studies have developed legal benchmarks for LLM agents, none are\nspecifically designed to consider the unique advantages of MAS, such as task\ndecomposition, agent specialization, and flexible training. In fact, the lack\nof evaluation methods limits the potential of MAS in the legal domain. To\naddress this gap, we propose MASLegalBench, a legal benchmark tailored for MAS\nand designed with a deductive reasoning approach. Our benchmark uses GDPR as\nthe application scenario, encompassing extensive background knowledge and\ncovering complex reasoning processes that effectively reflect the intricacies\nof real-world legal situations. Furthermore, we manually design various\nrole-based MAS and conduct extensive experiments using different\nstate-of-the-art LLMs. Our results highlight the strengths, limitations, and\npotential areas for improvement of existing models and MAS architectures.",
        "url": "http://arxiv.org/abs/2509.24922v2",
        "pdf_url": "http://arxiv.org/pdf/2509.24922v2",
        "arxiv_id": "2509.24922v2",
        "authors": [
            "Huihao Jing",
            "Wenbin Hu",
            "Hongyu Luo",
            "Jianhui Yang",
            "Wei Fan",
            "Haoran Li",
            "Yangqiu Song"
        ],
        "submitted": "2025-09-29 15:24:40",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to the user's core research themes in Information Retrieval, Search technologies, or Natural Language Processing. While it involves Large Language Models, the focus is on Multi-Agent Systems and deductive legal reasoning, which is not a primary area of interest for the user."
    },
    {
        "title": "Expanding Computation Spaces of LLMs at Inference Time",
        "abstract": "Chain-of-thought (CoT) rationale enables language models to use additional\ntask-related text for problem-solving, benefiting not only from detailed\nreasoning steps but also from the expanded computational space of longer\ninputs. Prior work has trained filler or special tokens to serve as additional\ncomputation spaces. In this study, we investigate whether language models can\nleverage artificially inserted sequences of filler tokens solely at inference.\nWe first identify effective token types, numbers, and insertion locations, then\nexamine at what stage of training models begin to exploit the expanded\ncomputation space, and finally analyze dynamics within these spaces via\nattention maps. Experiments on models ranging from 1.7B to 32B across\nopen-domain QA and math tasks show that appropriate token types and counts\nvary, but placing filler tokens directly before the final 'Answer:' token is\nmost effective. Smaller models benefit most, up to 12.372 percentage points in\nSmolLM2-1.7B-Instruct, indicating that these spaces act as additional\ncomputational capacity rather than redundant input. Attention maps reveal that\nexpanded spaces often continue the original attention mechanism and sometimes\nfocus on questions or answer options, suggesting meaningful computation for\nproblem-solving.",
        "url": "http://arxiv.org/abs/2509.24884v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24884v1",
        "arxiv_id": "2509.24884v1",
        "authors": [
            "Yoonna Jang",
            "Kisu Yang",
            "Isabelle Augenstein"
        ],
        "submitted": "2025-09-29 14:59:44",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on improving the performance of Large Language Models (LLMs) by expanding their computation spaces at inference time. While it touches on aspects of deep semantic understanding, it is primarily concerned with optimizing LLMs' computational capacity, which is not a central match to your research interests in Information Retrieval and Search technologies."
    },
    {
        "title": "Hierarchical Error Correction for Large Language Models: A Systematic Framework for Domain-Specific AI Quality Enhancement",
        "abstract": "Large Language Models face significant performance challenges in specialized\ndomains, with state-of-the-art models achieving only 45.9% accuracy on medical\ncoding tasks. This study proposes a Hierarchical Error Correction (HEC)\nframework that addresses domain-specific AI limitations through systematic\nerror analysis and targeted intervention strategies.\n  We analyze error patterns across four specialized domains and find that AI\nerrors follow consistent hierarchical structures: Knowledge-layer errors\n(58.4%), Reasoning-layer errors (39.6%), and Complexity-layer errors (2.0%).\nBased on these patterns, we develop a three-stage correction framework that\naddresses errors according to their hierarchical importance and demonstrates\nthat framework effectiveness correlates inversely with baseline task\nperformance.\n  Experimental validation across medical transcription (4,921 cases), legal\ndocument classification (1,000 cases), political bias detection (645 cases),\nand legal reasoning (1,000 cases) shows consistent improvements. Cross-model\nvalidation across five LLM architectures demonstrates average improvements of\n11.2 percentage points (p < 0.001). However, analysis reveals framework\nlimitations in high-baseline tasks (>75% accuracy), where hierarchical\nintervention may interfere with effective reasoning processes.\n  The results suggest that systematic error analysis can guide effective AI\nenhancement strategies in specialized domains, particularly for\nmoderate-baseline tasks, while highlighting the importance of understanding\nframework boundaries for optimal deployment.",
        "url": "http://arxiv.org/abs/2509.24841v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24841v1",
        "arxiv_id": "2509.24841v1",
        "authors": [
            "Zhilong Zhao",
            "Yindi Liu"
        ],
        "submitted": "2025-09-29 14:21:05",
        "source": "arxiv",
        "comment": "10 pages, 4 figures, 4 tables",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper focuses on Large Language Models and proposes a framework for domain-specific AI quality enhancement. While it touches on error analysis and correction, which is somewhat related to query understanding and ranking models, the primary focus is on NLP and AI quality enhancement, which is not a central match to your research interests in Information Retrieval and Search technologies."
    },
    {
        "title": "SeaPO: Strategic Error Amplification for Robust Preference Optimization of Large Language Models",
        "abstract": "Existing alignment methods for preference optimization of large language\nmodels (LLMs) aim to enhance model performance by utilizing pairs of positive\nand negative samples. However, due to the limited capacity of models in scoring\nor generating responses, the quality of positive and negative samples may\nbecome similar during training, which complicates optimization for preference\nlearning. To address this issue, we introduce SeaPO, a Strategic Error\nAmplification method that leverages three error types commonly occurring in\nLLMs to introduce specific error patterns into the model Preference\nOptimization. This strategy ensures that negative samples are more erroneous\nthan positive samples and preference-based training is employed to mitigate the\noccurrence of these errors, thereby enhancing model performance. Evaluations\nacross five capability dimensions and different model scales (1.5B to 14B)\ndemonstrate that the generated data significantly improved overall model\nperformance, particularly in terms of truthfulness, with improvements of 5-10\npercentage points observed. Further analysis reveals that task performance\nvaries depending on the error types introduced. Injecting the most common error\ntypes improves performance in related tasks, while a mix of error types leads\nto a broader performance enhancement: most tasks show stable improvements,\nwhile a few tasks exhibit significant gains.",
        "url": "http://arxiv.org/abs/2509.24781v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24781v1",
        "arxiv_id": "2509.24781v1",
        "authors": [
            "Jun Rao",
            "Yunjie Liao",
            "Xuebo Liu",
            "Zepeng Lin",
            "Lian Lian",
            "Dong Jin",
            "Shengjun Cheng",
            "Jun Yu",
            "Min Zhang"
        ],
        "submitted": "2025-09-29 13:42:41",
        "source": "arxiv",
        "comment": "EMNLP 2025 Findings",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be focused on preference optimization of large language models, which is somewhat related to information retrieval, but it does not directly address query understanding, ranking models, or user behavior modeling. The paper's emphasis on error amplification and preference learning is more aligned with NLP and recommender systems, but it does not seem to be a central match for your research interests."
    },
    {
        "title": "VSSFlow: Unifying Video-conditioned Sound and Speech Generation via Joint Learning",
        "abstract": "Video-conditioned sound and speech generation, encompassing video-to-sound\n(V2S) and visual text-to-speech (VisualTTS) tasks, are conventionally addressed\nas separate tasks, with limited exploration to unify them within a signle\nframework. Recent attempts to unify V2S and VisualTTS face challenges in\nhandling distinct condition types (e.g., heterogeneous video and transcript\nconditions) and require complex training stages. Unifying these two tasks\nremains an open problem. To bridge this gap, we present VSSFlow, which\nseamlessly integrates both V2S and VisualTTS tasks into a unified flow-matching\nframework. VSSFlow uses a novel condition aggregation mechanism to handle\ndistinct input signals. We find that cross-attention and self-attention layer\nexhibit different inductive biases in the process of introducing condition.\nTherefore, VSSFlow leverages these inductive biases to effectively handle\ndifferent representations: cross-attention for ambiguous video conditions and\nself-attention for more deterministic speech transcripts. Furthermore, contrary\nto the prevailing belief that joint training on the two tasks requires complex\ntraining strategies and may degrade performance, we find that VSSFlow benefits\nfrom the end-to-end joint learning process for sound and speech generation\nwithout extra designs on training stages. Detailed analysis attributes it to\nthe learned general audio prior shared between tasks, which accelerates\nconvergence, enhances conditional generation, and stabilizes the\nclassifier-free guidance process. Extensive experiments demonstrate that\nVSSFlow surpasses the state-of-the-art domain-specific baselines on both V2S\nand VisualTTS benchmarks, underscoring the critical potential of unified\ngenerative models.",
        "url": "http://arxiv.org/abs/2509.24773v2",
        "pdf_url": "http://arxiv.org/pdf/2509.24773v2",
        "arxiv_id": "2509.24773v2",
        "authors": [
            "Xin Cheng",
            "Yuyue Wang",
            "Xihua Wang",
            "Yihan Wu",
            "Kaisi Guan",
            "Yijing Chen",
            "Peng Zhang",
            "Xiaojiang Liu",
            "Meng Cao",
            "Ruihua Song"
        ],
        "submitted": "2025-09-29 13:38:24",
        "source": "arxiv",
        "comment": "Paper Under Review",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper focuses on video-conditioned sound and speech generation, which is unrelated to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing."
    },
    {
        "title": "ProxyAttn: Guided Sparse Attention via Representative Heads",
        "abstract": "The quadratic complexity of attention mechanisms limits the efficiency of\nLarge Language Models (LLMs) on long-text tasks. Recently, methods that\ndynamically estimate block importance have enabled efficient block sparse\nattention, leading to significant acceleration in long-text pre-filling of\nLLMs. However, their coarse-grained estimation inevitably leads to performance\ndegradation at high sparsity rates. In this work, we propose ProxyAttn, a\ntraining-free sparse attention algorithm that achieves more precise block\nestimation by compressing the dimension of attention heads. Based on our\nobservation of the similarity among multiple attention heads, we use the scores\nof pooled representative heads to approximate the scores for all heads. To\naccount for the varying sparsity among heads, we also propose a block-aware\ndynamic budget estimation method. By combining the scores from representative\nproxy heads with multi-head dynamic budgets, we achieve a more fine-grained\nblock importance evaluation at low computational cost. Experiments on a variety\nof mainstream models and extensive benchmarks confirm the underlying similarity\namong attention heads. Leveraging a fine-grained estimation, the proposed\nmethod achieves substantial gains in performance and efficiency compared to\nexisting methods. More precisely, ProxyAttn can achieve up to 10.3x attention\nacceleration and 2.4x prefilling acceleration without significant performance\nloss. Our code is available at https://github.com/wyxstriker/ProxyAttn.",
        "url": "http://arxiv.org/abs/2509.24745v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24745v1",
        "arxiv_id": "2509.24745v1",
        "authors": [
            "Yixuan Wang",
            "Huang He",
            "Siqi Bao",
            "Hua Wu",
            "Haifeng Wang",
            "Qingfu Zhu",
            "Wanxiang Che"
        ],
        "submitted": "2025-09-29 13:10:39",
        "source": "arxiv",
        "comment": "14pages, 5figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper proposes a novel sparse attention algorithm, ProxyAttn, which aims to improve the efficiency of Large Language Models. While it touches on attention mechanisms, a key aspect of Information Retrieval, its focus on long-text tasks and language models makes it somewhat relevant to your interests, but not a central match."
    },
    {
        "title": "Socratic-Zero : Bootstrapping Reasoning via Data-Free Agent Co-evolution",
        "abstract": "Recent breakthroughs in large language models (LLMs) on reasoning tasks rely\nheavily on massive, high-quality datasets-typically human-annotated and thus\ndifficult to scale. While data synthesis or distillation offers a promising\nalternative, existing methods struggle with inconsistent data quality and an\ninability to dynamically adapt to the evolving capabilities of the model,\nleading to suboptimal training signals. To address these limitations, we\nintroduce Socratic-Zero, a fully autonomous framework that generates\nhigh-quality training data from minimal seed examples through the co-evolution\nof three agents: the Teacher, the Solver, and the Generator. The Solver\ncontinuously refines its reasoning by learning from preference feedback on both\nsuccessful and failed trajectories; the Teacher adaptively crafts increasingly\nchallenging questions based on the Solver's weaknesses; and the Generator\ndistills the Teacher's question-design strategy to enable scalable,\nhigh-fidelity curriculum generation. This closed-loop system produces a\nself-improving curriculum-requiring no pre-existing tasks or labels.\nRemarkably, starting from only 100 seed questions, our Socratic-Solver-8B\nachieves an average gain of +20.2 percentage points over prior data synthesis\nmethods across seven mathematical reasoning benchmarks (AMC23, AIME24-25,\nOlympiad, MATH-500, Minerva, and GSM8K), with consistent gains on both Qwen3\nand GLM4 series models. Even more surprisingly, synthetic data from\nSocratic-Generator-32B enables student LLMs to achieve superior performance\ncompared to other state-of-the-art (SOTA) commercial LLMs on these benchmarks,\nincluding Qwen3-235B-A22B, DeepSeek-V3.1-671B, GPT-5, Gemini-2.5-Pro, Grok-4,\nand Claude-4.1-Opus.",
        "url": "http://arxiv.org/abs/2509.24726v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24726v1",
        "arxiv_id": "2509.24726v1",
        "authors": [
            "Shaobo Wang",
            "Zhengbo Jiao",
            "Zifan Zhang",
            "Yilang Peng",
            "Xu Ze",
            "Boyu Yang",
            "Wei Wang",
            "Hu Wei",
            "Linfeng Zhang"
        ],
        "submitted": "2025-09-29 12:54:07",
        "source": "arxiv",
        "comment": "23 pages, 3 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on large language models and data synthesis for reasoning tasks, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on NLP, the context is more aligned with model development and training rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "MemGen: Weaving Generative Latent Memory for Self-Evolving Agents",
        "abstract": "Agent memory shapes how Large Language Model (LLM)-powered agents, akin to\nthe human brain, progressively refine themselves through environment\ninteractions. Existing paradigms remain constrained: parametric memory forcibly\nadjusts model parameters, and retrieval-based memory externalizes experience\ninto structured databases, yet neither captures the fluid interweaving of\nreasoning and memory that underlies human cognition. To address this gap, we\npropose MemGen, a dynamic generative memory framework that equips agents with a\nhuman-esque cognitive faculty. It consists of a \\textit{memory trigger}, which\nmonitors the agent's reasoning state to decide explicit memory invocation, and\na \\textit{memory weaver}, which takes the agent's current state as stimulus to\nconstruct a latent token sequence as machine-native memory to enrich its\nreasoning. In this way, MemGen enables agents to recall and augment latent\nmemory throughout reasoning, producing a tightly interwoven cycle of memory and\ncognition. Extensive experiments across eight benchmarks show that MemGen\nsurpasses leading external memory systems such as ExpeL and AWM by up to\n$38.22\\%$, exceeds GRPO by up to $13.44\\%$, and exhibits strong cross-domain\ngeneralization ability. More importantly, we find that without explicit\nsupervision, MemGen spontaneously evolves distinct human-like memory faculties,\nincluding planning memory, procedural memory, and working memory, suggesting an\nemergent trajectory toward more naturalistic forms of machine cognition.",
        "url": "http://arxiv.org/abs/2509.24704v1",
        "pdf_url": "http://arxiv.org/pdf/2509.24704v1",
        "arxiv_id": "2509.24704v1",
        "authors": [
            "Guibin Zhang",
            "Muxin Fu",
            "Shuicheng Yan"
        ],
        "submitted": "2025-09-29 12:33:13",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on developing a generative memory framework for self-evolving agents, which is not directly related to information retrieval, search technologies, or natural language processing. While it involves large language models, the primary goal is to enable agents to refine themselves through environment interactions, rather than addressing query understanding, ranking models, or user behavior modeling."
    }
]