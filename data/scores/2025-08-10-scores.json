[
    {
        "title": "Improving Table Retrieval with Question Generation from Partial Tables",
        "abstract": "Recent advances in open-domain question answering over tables have widely\nadopted large language models (LLMs) under the Retriever-Reader architecture.\nPrior works have effectively leveraged LLMs to tackle the complex reasoning\ndemands of the Reader component, such as text-to-text, text-to-SQL, and multi\nhop reasoning. In contrast, the Retriever component has primarily focused on\noptimizing the query representation-training retrievers to retrieve relevant\ntables based on questions, or to select keywords from questions for matching\ntable segments. However, little attention has been given to enhancing how\ntables themselves are represented in embedding space to better align with\nquestions. To address this, we propose QGpT (Question Generation from Partial\nTables), a simple yet effective method that uses an LLM to generate synthetic\nquestions based on small portions of a table. These questions are generated to\nsimulate how a user might query the content of the table currently under\nconsideration. The generated questions are then jointly embedded with the\npartial table segments used for generation, enhancing semantic alignment with\nuser queries. Without the need to embed entire tables, our method significantly\nimproves retrieval performance across multiple benchmarks for both dense and\nlate-interaction retrievers.",
        "url": "http://arxiv.org/abs/2508.06168v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06168v1",
        "arxiv_id": "2508.06168v1",
        "authors": [
            "Hsing-Ping Liang",
            "Che-Wei Chang",
            "Yao-Chung Fan"
        ],
        "submitted": "2025-08-08 09:35:56",
        "source": "arxiv",
        "comment": "TRL@ACL2025",
        "score": 13,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores table retrieval with question generation, leveraging large language models and retriever-reader architecture, which is related to query understanding and ranking models in Information Retrieval. However, the focus on table representation and question generation is not directly aligned with user behavior modeling or click models, which are core interests."
    },
    {
        "title": "Spectrum Projection Score: Aligning Retrieved Summaries with Reader Models in Retrieval-Augmented Generation",
        "abstract": "Large Language Models (LLMs) have shown improved generation performance\nthrough retrieval-augmented generation (RAG) following the retriever-reader\nparadigm, which supplements model inputs with externally retrieved knowledge.\nHowever, prior work often evaluates RAG holistically, assessing the retriever\nand reader jointly, making it difficult to isolate the true contribution of\nretrieval, particularly given the prompt sensitivity of LLMs used as readers.\nWe introduce Spectrum Projection Score (SPS), a lightweight, supervision-free\nmetric that allows the reader to gauge the semantic alignment of a retrieved\nsummary with its hidden representation by comparing the area formed by\ngenerated tokens from the summary, and the principal directions of subspace in\nthe reader and to measure the relevance. Building on SPS we present xCompress,\nan inference time controller framework that dynamically samples, ranks, and\ncompresses retrieval summary candidates. Extensive experiments on five QA\nbenchmarks with four open source LLMs show that SPS not only enhances\nperformance across a range of tasks but also provides a principled perspective\non the interaction between retrieval and generation.",
        "url": "http://arxiv.org/abs/2508.05909v1",
        "pdf_url": "http://arxiv.org/pdf/2508.05909v1",
        "arxiv_id": "2508.05909v1",
        "authors": [
            "Zhanghao Hu",
            "Qinglin Zhu",
            "Siya Qi",
            "Yulan He",
            "Hanqi Yan",
            "Lin Gui"
        ],
        "submitted": "2025-08-08 00:13:48",
        "source": "arxiv",
        "comment": null,
        "score": 13,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'ctr' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores retrieval-augmented generation, which is related to information retrieval and search technologies. The focus on semantic alignment and relevance assessment is also relevant to query understanding and ranking models. However, the paper's primary focus is on the interaction between retrieval and generation, which is not a central match with the user's research interests in user behavior modeling and click models."
    },
    {
        "title": "You Don't Need Pre-built Graphs for RAG: Retrieval Augmented Generation with Adaptive Reasoning Structures",
        "abstract": "Large language models (LLMs) often suffer from hallucination, generating\nfactually incorrect statements when handling questions beyond their knowledge\nand perception. Retrieval-augmented generation (RAG) addresses this by\nretrieving query-relevant contexts from knowledge bases to support LLM\nreasoning. Recent advances leverage pre-constructed graphs to capture the\nrelational connections among distributed documents, showing remarkable\nperformance in complex tasks. However, existing Graph-based RAG (GraphRAG)\nmethods rely on a costly process to transform the corpus into a graph,\nintroducing overwhelming token cost and update latency. Moreover, real-world\nqueries vary in type and complexity, requiring different logic structures for\naccurate reasoning. The pre-built graph may not align with these required\nstructures, resulting in ineffective knowledge retrieval. To this end, we\npropose a \\textbf{\\underline{Logic}}-aware\n\\textbf{\\underline{R}}etrieval-\\textbf{\\underline{A}}ugmented\n\\textbf{\\underline{G}}eneration framework (\\textbf{LogicRAG}) that dynamically\nextracts reasoning structures at inference time to guide adaptive retrieval\nwithout any pre-built graph. LogicRAG begins by decomposing the input query\ninto a set of subproblems and constructing a directed acyclic graph (DAG) to\nmodel the logical dependencies among them. To support coherent multi-step\nreasoning, LogicRAG then linearizes the graph using topological sort, so that\nsubproblems can be addressed in a logically consistent order. Besides, LogicRAG\napplies graph pruning to reduce redundant retrieval and uses context pruning to\nfilter irrelevant context, significantly reducing the overall token cost.\nExtensive experiments demonstrate that LogicRAG achieves both superior\nperformance and efficiency compared to state-of-the-art baselines.",
        "url": "http://arxiv.org/abs/2508.06105v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06105v1",
        "arxiv_id": "2508.06105v1",
        "authors": [
            "Shengyuan Chen",
            "Chuang Zhou",
            "Zheng Yuan",
            "Qinggang Zhang",
            "Zeyang Cui",
            "Hao Chen",
            "Yilin Xiao",
            "Jiannong Cao",
            "Xiao Huang"
        ],
        "submitted": "2025-08-08 08:07:40",
        "source": "arxiv",
        "comment": null,
        "score": 12,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper proposes a retrieval-augmented generation framework that dynamically extracts reasoning structures at inference time, which is related to query understanding and ranking models. However, the focus is on natural language processing and knowledge retrieval, rather than information retrieval or search technologies, which are the user's primary research interests."
    },
    {
        "title": "A Systematic Literature Review of Retrieval-Augmented Generation: Techniques, Metrics, and Challenges",
        "abstract": "This systematic review of the research literature on retrieval-augmented\ngeneration (RAG) provides a focused analysis of the most highly cited studies\npublished between 2020 and May 2025. A total of 128 articles met our inclusion\ncriteria. The records were retrieved from ACM Digital Library, IEEE Xplore,\nScopus, ScienceDirect, and the Digital Bibliography and Library Project (DBLP).\nRAG couples a neural retriever with a generative language model, grounding\noutput in up-to-date, non-parametric memory while retaining the semantic\ngeneralisation stored in model weights. Guided by the PRISMA 2020 framework, we\n(i) specify explicit inclusion and exclusion criteria based on citation count\nand research questions, (ii) catalogue datasets, architectures, and evaluation\npractices, and (iii) synthesise empirical evidence on the effectiveness and\nlimitations of RAG. To mitigate citation-lag bias, we applied a lower\ncitation-count threshold to papers published in 2025 so that emerging\nbreakthroughs with naturally fewer citations were still captured. This review\nclarifies the current research landscape, highlights methodological gaps, and\ncharts priority directions for future research.",
        "url": "http://arxiv.org/abs/2508.06401v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06401v1",
        "arxiv_id": "2508.06401v1",
        "authors": [
            "Andrew Brown",
            "Muhammad Roman",
            "Barry Devereux"
        ],
        "submitted": "2025-08-08 15:37:14",
        "source": "arxiv",
        "comment": "58 pages",
        "score": 8,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to information retrieval, as it discusses retrieval-augmented generation, which combines neural retrievers with generative language models. However, the focus is on the application of this technique in natural language processing, rather than query understanding, ranking models, or user behavior modeling, which are the user's primary research interests."
    },
    {
        "title": "Planning Agents on an Ego-Trip: Leveraging Hybrid Ego-Graph Ensembles for Improved Tool Retrieval in Enterprise Task Planning",
        "abstract": "Effective tool retrieval is essential for AI agents to select from a vast\narray of tools when identifying and planning actions in the context of complex\nuser queries. Despite its central role in planning, this aspect remains\nunderexplored in the literature. Traditional approaches rely primarily on\nsimilarities between user queries and tool descriptions, which significantly\nlimits retrieval accuracy, specifically when handling multi-step user requests.\nTo address these limitations, we propose a Knowledge Graph (KG)-based tool\nretrieval framework that captures the semantic relationships between tools and\ntheir functional dependencies. Our retrieval algorithm leverages ensembles of\n1-hop ego tool graphs to model direct and indirect connections between tools,\nenabling more comprehensive and contextual tool selection for multi-step tasks.\nWe evaluate our approach on a synthetically generated internal dataset across\nsix defined user classes, extending previous work on coherent dialogue\nsynthesis and too retrieval benchmarks. Results demonstrate that our tool\ngraph-based method achieves 91.85% tool coverage on the micro-average Complete\nRecall metric, compared to 89.26% for re-ranked semantic-lexical hybrid\nretrieval, the strongest non-KG baseline in our experiments. These findings\nsupport our hypothesis that the structural information in the KG provides\ncomplementary signals to pure similarity matching, particularly for queries\nrequiring sequential tool composition.",
        "url": "http://arxiv.org/abs/2508.05888v1",
        "pdf_url": "http://arxiv.org/pdf/2508.05888v1",
        "arxiv_id": "2508.05888v1",
        "authors": [
            "Sahil Bansal",
            "Sai Shruthi Sistla",
            "Aarti Arikatala",
            "Sebastian Schreiber"
        ],
        "submitted": "2025-08-07 22:41:12",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on planning agents and tool retrieval in enterprise task planning, which is not directly related to information retrieval, search technologies, or query understanding. Although it mentions knowledge graphs, the application is specific to tool retrieval and does not align with the user's interests in NLP, data mining, or real-time relevance optimization."
    },
    {
        "title": "WebWatcher: Breaking New Frontiers of Vision-Language Deep Research Agent",
        "abstract": "Web agents such as Deep Research have demonstrated superhuman cognitive\nabilities, capable of solving highly challenging information-seeking problems.\nHowever, most research remains primarily text-centric, overlooking visual\ninformation in the real world. This makes multimodal Deep Research highly\nchallenging, as such agents require much stronger reasoning abilities in\nperception, logic, knowledge, and the use of more sophisticated tools compared\nto text-based agents. To address this limitation, we introduce WebWatcher, a\nmulti-modal Agent for Deep Research equipped with enhanced visual-language\nreasoning capabilities. It leverages high-quality synthetic multimodal\ntrajectories for efficient cold start training, utilizes various tools for deep\nreasoning, and further enhances generalization through reinforcement learning.\nTo better evaluate the capabilities of multimodal agents, we propose\nBrowseComp-VL, a benchmark with BrowseComp-style that requires complex\ninformation retrieval involving both visual and textual information.\nExperimental results show that WebWatcher significantly outperforms proprietary\nbaseline, RAG workflow and open-source agents in four challenging VQA\nbenchmarks, which paves the way for solving complex multimodal\ninformation-seeking tasks.",
        "url": "http://arxiv.org/abs/2508.05748v1",
        "pdf_url": "http://arxiv.org/pdf/2508.05748v1",
        "arxiv_id": "2508.05748v1",
        "authors": [
            "Xinyu Geng",
            "Peng Xia",
            "Zhen Zhang",
            "Xinyu Wang",
            "Qiuchen Wang",
            "Ruixue Ding",
            "Chenxi Wang",
            "Jialong Wu",
            "Yida Zhao",
            "Kuan Li",
            "Yong Jiang",
            "Pengjun Xie",
            "Fei Huang",
            "Jingren Zhou"
        ],
        "submitted": "2025-08-07 18:03:50",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on multimodal deep research agents, leveraging visual and language information, which is not directly related to the user's interests in Information Retrieval, Search technologies, and query understanding. While the paper mentions information retrieval, it is primarily focused on visual-language reasoning and multimodal agents, which is not a central match for the user's research themes."
    },
    {
        "title": "Maximum Impact with Fewer Features: Efficient Feature Selection for Cold-Start Recommenders through Collaborative Importance Weighting",
        "abstract": "Cold-start challenges in recommender systems necessitate leveraging auxiliary\nfeatures beyond user-item interactions. However, the presence of irrelevant or\nnoisy features can degrade predictive performance, whereas an excessive number\nof features increases computational demands, leading to higher memory\nconsumption and prolonged training times.\n  To address this, we propose a feature selection strategy that prioritizes the\nuser behavioral information. Our method enhances the feature representation by\nincorporating correlations from collaborative behavior data using a hybrid\nmatrix factorization technique and then ranks features using a mechanism based\non the maximum volume algorithm. This approach identifies the most influential\nfeatures, striking a balance between recommendation accuracy and computational\nefficiency. We conduct an extensive evaluation across various datasets and\nhybrid recommendation models, demonstrating that our method excels in\ncold-start scenarios by selecting minimal yet highly effective feature subsets.\nEven under strict feature reduction, our approach surpasses existing feature\nselection techniques while maintaining superior efficiency.",
        "url": "http://arxiv.org/abs/2508.06455v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06455v1",
        "arxiv_id": "2508.06455v1",
        "authors": [
            "Nikita Sukhorukov",
            "Danil Gusak",
            "Evgeny Frolov"
        ],
        "submitted": "2025-08-08 16:58:47",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'user behavior' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on recommender systems, which is a related topic to the user's interests in Information Retrieval and Search technologies. However, the emphasis on feature selection and cold-start challenges in recommender systems is not directly aligned with the user's primary focus on query understanding, ranking models, and user behavior modeling in the context of IR. The paper's abstract does not mention any connections to IR or NLP, which are also important areas of interest for the user."
    },
    {
        "title": "Less is More: Selective Reflection for Compatible and Efficient Knowledge Distillation in Large Language Models",
        "abstract": "Knowledge Distillation (KD) is a fundamental technique for compressing large\nlanguage models (LLMs) into compact, efficient student models. However,\nexisting white-box KD methods mainly focus on balancing ground truth and\nstudent-generated responses while overlooking two critical factors: training\ndata quality and student-model compatibility. To address these limitations, we\npropose Selective Reflection Distillation (SRD), a novel data curation\nframework that leverages reflections from student models to systematically\nrefine training data. SRD dynamically evaluates and selects prompt-response\npairs by comparing ground truth data with student model outputs, selectively\ncurating high-quality, student-compatible training instances through automated\nranking based on difficulty. Furthermore, after selecting the training data, a\ncurriculum scheduling strategy is employed to incrementally introduce these\ncurated subsets into the distillation process at fixed intervals. As a\nplug-and-play enhancement, SRD consistently improves distillation outcomes\nacross diverse white-box KD approaches and model architectures, as well as\ndecreases computational cost significantly during KD training. Experiments on a\nrange of language model benchmarks demonstrate SRD's consistent improvements in\ndistilled model performance, as well as a reduction in training runtime by up\nto 39%, under diverse KD methods and model families. Notably, SRD operates as a\nplug-and-play module, enhancing sample efficiency without modifying underlying\nKD algorithms. Our findings highlight that data quality and compatibility are\npivotal to effective and efficient distillation of LLMs, and SRD provides a\nprincipled framework to achieve both. This work advances the understanding of\ndata-centric factors in KD and offers practical insights for enhancing the\ncapability and efficiency of compressed LLMs.",
        "url": "http://arxiv.org/abs/2508.06135v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06135v1",
        "arxiv_id": "2508.06135v1",
        "authors": [
            "Lingyuan Liu",
            "Mengxiang Zhang"
        ],
        "submitted": "2025-08-08 08:55:53",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on knowledge distillation in large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on data quality and compatibility, these concepts are not applied to the context of search or retrieval, making it an off-topic paper for your research interests."
    },
    {
        "title": "M2IO-R1: An Efficient RL-Enhanced Reasoning Framework for Multimodal Retrieval Augmented Multimodal Generation",
        "abstract": "Current research on Multimodal Retrieval-Augmented Generation (MRAG) enables\ndiverse multimodal inputs but remains limited to single-modality outputs,\nrestricting expressive capacity and practical utility. In contrast, real-world\napplications often demand both multimodal inputs and multimodal outputs for\neffective communication and grounded reasoning. Motivated by the recent success\nof Reinforcement Learning (RL) in complex reasoning tasks for Large Language\nModels (LLMs), we adopt RL as a principled and effective paradigm to address\nthe multi-step, outcome-driven challenges inherent in multimodal output\ngeneration. Here, we introduce M2IO-R1, a novel framework for Multimodal\nRetrieval-Augmented Multimodal Generation (MRAMG) that supports both multimodal\ninputs and outputs. Central to our framework is an RL-based inserter,\nInserter-R1-3B, trained with Group Relative Policy Optimization to guide image\nselection and placement in a controllable and semantically aligned manner.\nEmpirical results show that our lightweight 3B inserter achieves strong\nreasoning capabilities with significantly reduced latency, outperforming\nbaselines in both quality and efficiency.",
        "url": "http://arxiv.org/abs/2508.06328v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06328v1",
        "arxiv_id": "2508.06328v1",
        "authors": [
            "Zhiyou Xiao",
            "Qinhan Yu",
            "Binghui Li",
            "Geng Chen",
            "Chong Chen",
            "Wentao Zhang"
        ],
        "submitted": "2025-08-08 14:00:19",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores Multimodal Retrieval-Augmented Generation (MRAG), which is a related topic to Information Retrieval. However, the focus on multimodal inputs and outputs, as well as the use of Reinforcement Learning (RL) for reasoning, is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling. While the paper touches on some relevant concepts, it does not seem to be a central match for the user's research themes."
    },
    {
        "title": "NanoCodec: Towards High-Quality Ultra Fast Speech LLM Inference",
        "abstract": "Large Language Models (LLMs) have significantly advanced audio processing by\nleveraging audio codecs to discretize audio into tokens, enabling the\napplication of language modeling techniques to speech data. However, existing\naudio codecs often operate at high frame rates, leading to slow training and\ninference, particularly for autoregressive models. To address this, there is\ngrowing interest in low frame-rate audio codecs, which reduce the number of\nautoregressive steps required to generate one second of audio. In this paper,\nwe conduct ablation studies to examine the impact of frame rate, bitrate, and\ncausality on codec reconstruction quality. Based on our findings, we introduce\nNanoCodec, a state-of-the-art audio codec that achieves high-quality\ncompression at just 12.5 frames per second (FPS). NanoCodec outperforms related\nworks across various bitrate ranges, establishing a new benchmark for\nlow-latency and efficient Speech LLM training and inference.",
        "url": "http://arxiv.org/abs/2508.05835v1",
        "pdf_url": "http://arxiv.org/pdf/2508.05835v1",
        "arxiv_id": "2508.05835v1",
        "authors": [
            "Edresson Casanova",
            "Paarth Neekhara",
            "Ryan Langman",
            "Shehzeen Hussain",
            "Subhankar Ghosh",
            "Xuesong Yang",
            "Ante Jukić",
            "Jason Li",
            "Boris Ginsburg"
        ],
        "submitted": "2025-08-07 20:20:32",
        "source": "arxiv",
        "comment": "Accepted to Interspeech 2025",
        "score": 5,
        "keyword_reasons": [
            "Found 'ltr' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus is on audio processing and speech language models, which is outside your primary areas of interest."
    },
    {
        "title": "Classification is a RAG problem: A case study on hate speech detection",
        "abstract": "Robust content moderation requires classification systems that can quickly\nadapt to evolving policies without costly retraining. We present classification\nusing Retrieval-Augmented Generation (RAG), which shifts traditional\nclassification tasks from determining the correct category in accordance with\npre-trained parameters to evaluating content in relation to contextual\nknowledge retrieved at inference. In hate speech detection, this transforms the\ntask from \"is this hate speech?\" to \"does this violate the hate speech policy?\"\n  Our Contextual Policy Engine (CPE) - an agentic RAG system - demonstrates\nthis approach and offers three key advantages: (1) robust classification\naccuracy comparable to leading commercial systems, (2) inherent explainability\nvia retrieved policy segments, and (3) dynamic policy updates without model\nretraining. Through three experiments, we demonstrate strong baseline\nperformance and show that the system can apply fine-grained policy control by\ncorrectly adjusting protection for specific identity groups without requiring\nretraining or compromising overall performance. These findings establish that\nRAG can transform classification into a more flexible, transparent, and\nadaptable process for content moderation and wider classification problems.",
        "url": "http://arxiv.org/abs/2508.06204v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06204v1",
        "arxiv_id": "2508.06204v1",
        "authors": [
            "Richard Willats",
            "Josh Pennington",
            "Aravind Mohan",
            "Bertie Vidgen"
        ],
        "submitted": "2025-08-08 10:35:41",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the application of Retrieval-Augmented Generation (RAG) in hate speech detection, which is a classification problem. While it touches on some aspects of query understanding and ranking models, the focus is more on the generation and retrieval of contextual knowledge rather than traditional IR and search technologies. The paper's relevance to the user's interests is somewhat limited, but it does demonstrate some innovative approaches to classification and policy updates."
    },
    {
        "title": "Comparing Knowledge Injection Methods for LLMs in a Low-Resource Regime",
        "abstract": "Large language models (LLMs) often require vast amounts of text to\neffectively acquire new knowledge. While continuing pre-training on large\ncorpora or employing retrieval-augmented generation (RAG) has proven\nsuccessful, updating an LLM with only a few thousand or million tokens remains\nchallenging. In this work, we investigate the task of injecting small,\nunstructured information into LLMs and its relation to the catastrophic\nforgetting phenomenon. We use a dataset of recent news -- ensuring no overlap\nwith the model's pre-training data -- to evaluate the knowledge acquisition by\nprobing the model with question-answer pairs related the learned information.\nStarting from a continued pre-training baseline, we explored different\naugmentation algorithms to generate synthetic data to improve the knowledge\nacquisition capabilities. Our experiments show that simply continuing\npre-training on limited data yields modest improvements, whereas exposing the\nmodel to diverse textual variations significantly improves the learning of new\nfacts -- particularly with methods that induce greater variability through\ndiverse prompting. Furthermore, we shed light on the forgetting phenomenon in\nsmall-data regimes, illustrating the delicate balance between learning new\ncontent and retaining existing capabilities. We also confirm the sensitivity of\nRAG-based approaches for knowledge injection, which often lead to greater\ndegradation on control datasets compared to parametric methods. Finally, we\ndemonstrate that models can generate effective synthetic training data\nthemselves, suggesting a pathway toward self-improving model updates. All code\nand generated data used in our experiments are publicly available, providing a\nresource for studying efficient knowledge injection in LLMs with limited data\nat https://github.com/hugoabonizio/knowledge-injection-methods.",
        "url": "http://arxiv.org/abs/2508.06178v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06178v1",
        "arxiv_id": "2508.06178v1",
        "authors": [
            "Hugo Abonizio",
            "Thales Almeida",
            "Roberto Lotufo",
            "Rodrigo Nogueira"
        ],
        "submitted": "2025-08-08 09:48:32",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores knowledge injection methods for large language models in low-resource regimes, which is related to query understanding and ranking models in Information Retrieval. However, the focus on language models and knowledge injection is not directly aligned with the user's primary interests in search technologies, user behavior modeling, and deep semantic understanding."
    },
    {
        "title": "UR$^2$: Unify RAG and Reasoning through Reinforcement Learning",
        "abstract": "Large Language Models (LLMs) have shown remarkable capabilities through two\ncomplementary paradigms: Retrieval-Augmented Generation (RAG), which enhances\nknowledge grounding, and Reinforcement Learning from Verifiable Rewards (RLVR),\nwhich optimizes complex reasoning abilities. However, these two capabilities\nare often developed in isolation, and existing efforts to unify them remain\nnarrow in scope-typically limited to open-domain QA with fixed retrieval\nsettings and task-specific assumptions. This lack of integration constrains\ngeneralization and limits the applicability of RAG-RL methods to broader\ndomains. To bridge this gap, we propose UR2 (Unified RAG and Reasoning), a\ngeneral framework that unifies retrieval and reasoning through reinforcement\nlearning. UR2 introduces two key contributions: a difficulty-aware curriculum\ntraining that selectively invokes retrieval only for challenging problems, and\na hybrid knowledge access strategy combining domain-specific offline corpora\nwith LLM-generated summaries. These components are designed to enable dynamic\ncoordination between retrieval and reasoning, improving adaptability across a\ndiverse range of tasks. Experiments across open-domain QA, MMLU-Pro, medical,\nand mathematical reasoning tasks demonstrate that UR2 (built on Qwen2.5-3/7B\nand LLaMA-3.1-8B) significantly outperforms existing RAG and RL methods,\nachieving comparable performance to GPT-4o-mini and GPT-4.1-mini on several\nbenchmarks. We have released all code, models, and data at\nhttps://github.com/Tsinghua-dhy/UR2.",
        "url": "http://arxiv.org/abs/2508.06165v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06165v1",
        "arxiv_id": "2508.06165v1",
        "authors": [
            "Weitao Li",
            "Boran Xiang",
            "Xiaolong Wang",
            "Zhinan Gou",
            "Weizhi Ma",
            "Yang Liu"
        ],
        "submitted": "2025-08-08 09:33:20",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a unified framework for retrieval and reasoning through reinforcement learning, which is relevant to the field of Information Retrieval. However, the focus on large language models and their capabilities in open-domain QA, medical, and mathematical reasoning tasks is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "eSASRec: Enhancing Transformer-based Recommendations in a Modular Fashion",
        "abstract": "Since their introduction, Transformer-based models, such as SASRec and\nBERT4Rec, have become common baselines for sequential recommendations,\nsurpassing earlier neural and non-neural methods. A number of following\npublications have shown that the effectiveness of these models can be improved\nby, for example, slightly updating the architecture of the Transformer layers,\nusing better training objectives, and employing improved loss functions.\nHowever, the additivity of these modular improvements has not been\nsystematically benchmarked - this is the gap we aim to close in this paper.\nThrough our experiments, we identify a very strong model that uses SASRec's\ntraining objective, LiGR Transformer layers, and Sampled Softmax Loss. We call\nthis combination eSASRec (Enhanced SASRec). While we primarily focus on\nrealistic, production-like evaluation, in our preliminarily study we find that\ncommon academic benchmarks show eSASRec to be 23% more effective compared to\nthe most recent state-of-the-art models, such as ActionPiece. In our main\nproduction-like benchmark, eSASRec resides on the Pareto frontier in terms of\nthe accuracy-coverage tradeoff (alongside the recent industrial models HSTU and\nFuXi. As the modifications compared to the original SASRec are relatively\nstraightforward and no extra features are needed (such as timestamps in HSTU),\nwe believe that eSASRec can be easily integrated into existing recommendation\npipelines and can can serve as a strong yet very simple baseline for emerging\ncomplicated algorithms. To facilitate this, we provide the open-source\nimplementations for our models and benchmarks in repository\nhttps://github.com/blondered/transformer_benchmark",
        "url": "http://arxiv.org/abs/2508.06450v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06450v1",
        "arxiv_id": "2508.06450v1",
        "authors": [
            "Daria Tikhonovich",
            "Nikita Zelinskiy",
            "Aleksandr V. Petrov",
            "Mayya Spirina",
            "Andrei Semenov",
            "Andrey V. Savchenko",
            "Sergei Kuliev"
        ],
        "submitted": "2025-08-08 16:49:03",
        "source": "arxiv",
        "comment": "Accepted at ACM RecSys 2025",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on recommender systems, specifically Transformer-based models, which is not directly related to the user's primary research interests in Information Retrieval and Search technologies. Although the paper mentions the use of a training objective and loss function, it does not address query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user."
    },
    {
        "title": "Learning the Topic, Not the Language: How LLMs Classify Online Immigration Discourse Across Languages",
        "abstract": "Large language models (LLMs) are transforming social-science research by\nenabling scalable, precise analysis. Their adaptability raises the question of\nwhether knowledge acquired through fine-tuning in a few languages can transfer\nto unseen languages that only appeared during pre-training. To examine this, we\nfine-tune lightweight LLaMA 3.2-3B models on monolingual, bilingual, or\nmultilingual data sets to classify immigration-related tweets from X/Twitter\nacross 13 languages, a domain characterised by polarised, culturally specific\ndiscourse. We evaluate whether minimal language-specific fine-tuning enables\ncross-lingual topic detection and whether adding targeted languages corrects\npre-training biases. Results show that LLMs fine-tuned in one or two languages\ncan reliably classify immigration-related content in unseen languages. However,\nidentifying whether a tweet expresses a pro- or anti-immigration stance\nbenefits from multilingual fine-tuning. Pre-training bias favours dominant\nlanguages, but even minimal exposure to under-represented languages during\nfine-tuning (as little as $9.62\\times10^{-11}$ of the original pre-training\ntoken volume) yields significant gains. These findings challenge the assumption\nthat cross-lingual mastery requires extensive multilingual training: limited\nlanguage coverage suffices for topic-level generalisation, and structural\nbiases can be corrected with lightweight interventions. By releasing\n4-bit-quantised, LoRA fine-tuned models, we provide an open-source,\nreproducible alternative to proprietary LLMs that delivers 35 times faster\ninference at just 0.00000989% of the dollar cost of the OpenAI GPT-4o model,\nenabling scalable, inclusive research.",
        "url": "http://arxiv.org/abs/2508.06435v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06435v1",
        "arxiv_id": "2508.06435v1",
        "authors": [
            "Andrea Nasuto",
            "Stefano Maria Iacus",
            "Francisco Rowe",
            "Devika Jain"
        ],
        "submitted": "2025-08-08 16:23:24",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on the application of Large Language Models (LLMs) in classifying online immigration discourse across languages, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on the topic of language models, it does not explore ranking models, user behavior modeling, or real-time relevance optimization, which are core areas of interest."
    },
    {
        "title": "Quantifying Conversation Drift in MCP via Latent Polytope",
        "abstract": "The Model Context Protocol (MCP) enhances large language models (LLMs) by\nintegrating external tools, enabling dynamic aggregation of real-time data to\nimprove task execution. However, its non-isolated execution context introduces\ncritical security and privacy risks. In particular, adversarially crafted\ncontent can induce tool poisoning or indirect prompt injection, leading to\nconversation hijacking, misinformation propagation, or data exfiltration.\nExisting defenses, such as rule-based filters or LLM-driven detection, remain\ninadequate due to their reliance on static signatures, computational\ninefficiency, and inability to quantify conversational hijacking. To address\nthese limitations, we propose SecMCP, a secure framework that detects and\nquantifies conversation drift, deviations in latent space trajectories induced\nby adversarial external knowledge. By modeling LLM activation vectors within a\nlatent polytope space, SecMCP identifies anomalous shifts in conversational\ndynamics, enabling proactive detection of hijacking, misleading, and data\nexfiltration. We evaluate SecMCP on three state-of-the-art LLMs (Llama3,\nVicuna, Mistral) across benchmark datasets (MS MARCO, HotpotQA, FinQA),\ndemonstrating robust detection with AUROC scores exceeding 0.915 while\nmaintaining system usability. Our contributions include a systematic\ncategorization of MCP security threats, a novel latent polytope-based\nmethodology for quantifying conversation drift, and empirical validation of\nSecMCP's efficacy.",
        "url": "http://arxiv.org/abs/2508.06418v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06418v1",
        "arxiv_id": "2508.06418v1",
        "authors": [
            "Haoran Shi",
            "Hongwei Yao",
            "Shuo Shao",
            "Shaopeng Jiao",
            "Ziqi Peng",
            "Zhan Qin",
            "Cong Wang"
        ],
        "submitted": "2025-08-08 16:05:27",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'ltr' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on a specific topic of Model Context Protocol (MCP) security, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it mentions large language models (LLMs), the context is not relevant to the user's interests in IR, ranking models, or user behavior modeling."
    },
    {
        "title": "Harnessing Adaptive Topology Representations for Zero-Shot Graph Question Answering",
        "abstract": "Large Multimodal Models (LMMs) have shown generalized zero-shot capabilities\nin diverse domain question-answering (QA) tasks, including graph QA that\ninvolves complex graph topologies. However, most current approaches use only a\nsingle type of graph representation, namely Topology Representation Form (TRF),\nsuch as prompt-unified text descriptions or style-fixed visual styles. Those\n\"one-size-fits-all\" approaches fail to consider the specific preferences of\ndifferent models or tasks, often leading to incorrect or overly long responses.\nTo address this, we first analyze the characteristics and weaknesses of\nexisting TRFs, and then design a set of TRFs, denoted by $F_{ZS}$, tailored to\nzero-shot graph QA. We then introduce a new metric, Graph Response Efficiency\n(GRE), which measures the balance between the performance and the brevity in\ngraph QA. Built on these, we develop the DynamicTRF framework, which aims to\nimprove both the accuracy and conciseness of graph QA. To be specific,\nDynamicTRF first creates a TRF Preference (TRFP) dataset that ranks TRFs based\non their GRE scores, to probe the question-specific TRF preferences. Then it\ntrains a TRF router on the TRFP dataset, to adaptively assign the best TRF from\n$F_{ZS}$ for each question during the inference. Extensive experiments across 7\nin-domain algorithmic graph QA tasks and 2 out-of-domain downstream tasks show\nthat DynamicTRF significantly enhances the zero-shot graph QA of LMMs in terms\nof accuracy",
        "url": "http://arxiv.org/abs/2508.06345v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06345v1",
        "arxiv_id": "2508.06345v1",
        "authors": [
            "Yanbin Wei",
            "Jiangyue Yan",
            "Chun Kang",
            "Yang Chen",
            "Hua Liu",
            "James T. Kwok",
            "Yu Zhang"
        ],
        "submitted": "2025-08-08 14:18:24",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on graph question answering and multimodal models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions ranking models, it's not in the context of learning to rank or user behavior modeling. The paper's topics are more aligned with natural language processing and data mining, but it doesn't seem to require deep semantic understanding or real-time relevance optimization."
    },
    {
        "title": "EICAP: Deep Dive in Assessment and Enhancement of Large Language Models in Emotional Intelligence through Multi-Turn Conversations",
        "abstract": "Emotional Intelligence (EI) is a critical yet underexplored dimension in the\ndevelopment of human-aligned LLMs. To address this gap, we introduce a unified,\npsychologically grounded four-layer taxonomy of EI tailored for large language\nmodels (LLMs), encompassing emotional tracking, cause inference, appraisal, and\nemotionally appropriate response generation. Building on this framework, we\npresent EICAP-Bench, a novel MCQ style multi-turn benchmark designed to\nevaluate EI capabilities in open-source LLMs across diverse linguistic and\ncultural contexts. We evaluate six LLMs: LLaMA3 (8B), LLaMA3-Instruct, Gemma\n(9B), Gemma-Instruct, Qwen2.5 (7B), and Qwen2.5-Instruct on EmoCap-Bench,\nidentifying Qwen2.5-Instruct as the strongest baseline. To assess the potential\nfor enhancing EI capabilities, we fine-tune both Qwen2.5-Base and\nQwen2.5-Instruct using LoRA adapters on UltraChat (UC), a large-scale,\ninstruction-tuned dialogue dataset, in both English and Arabic. Our statistical\nanalysis reveals that among the five EI layers, only the Appraisal layer shows\nsignificant improvement through UC-based fine-tuning. These findings highlight\nthe limitations of existing pretraining and instruction-tuning paradigms in\nequipping LLMs with deeper emotional reasoning and underscore the need for\ntargeted data and modeling strategies for comprehensive EI alignment.",
        "url": "http://arxiv.org/abs/2508.06196v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06196v1",
        "arxiv_id": "2508.06196v1",
        "authors": [
            "Nizi Nazar",
            "Ehsaneddin Asgari"
        ],
        "submitted": "2025-08-08 10:22:19",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'ltr' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on emotional intelligence in large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on natural language processing, the specific application and methodology are not relevant to the user's research interests."
    },
    {
        "title": "DKG-LLM : A Framework for Medical Diagnosis and Personalized Treatment Recommendations via Dynamic Knowledge Graph and Large Language Model Integration",
        "abstract": "Large Language Models (LLMs) have grown exponentially since the release of\nChatGPT. These models have gained attention due to their robust performance on\nvarious tasks, including language processing tasks. These models achieve\nunderstanding and comprehension of tasks by training billions of parameters.\nThe development of these models is a transformative force in enhancing natural\nlanguage understanding and has taken a significant step towards artificial\ngeneral intelligence (AGI). In this study, we aim to present the DKG-LLM\nframework. The DKG-LLM framework introduces a groundbreaking approach to\nmedical diagnosis and personalized treatment recommendations by integrating a\ndynamic knowledge graph (DKG) with the Grok 3 large language model. Using the\nAdaptive Semantic Fusion Algorithm (ASFA), heterogeneous medical data\n(including clinical reports and PubMed articles) and patient records\ndynamically generate a knowledge graph consisting of 15,964 nodes in 13\ndistinct types (e.g., diseases, symptoms, treatments, patient profiles) and\n127,392 edges in 26 relationship types (e.g., causal, therapeutic,\nassociation). ASFA utilizes advanced probabilistic models, Bayesian inference,\nand graph optimization to extract semantic information, dynamically updating\nthe graph with approximately 150 new nodes and edges in each data category\nwhile maintaining scalability with up to 987,654 edges. Real-world datasets,\nincluding MIMIC-III and PubMed, were utilized to evaluate the proposed\narchitecture. The evaluation results show that DKG-LLM achieves a diagnostic\naccuracy of 84.19%. The model also has a treatment recommendation accuracy of\n89.63% and a semantic coverage of 93.48%. DKG-LLM is a reliable and\ntransformative tool that handles noisy data and complex multi-symptom diseases,\nalong with feedback-based learning from physician input.",
        "url": "http://arxiv.org/abs/2508.06186v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06186v1",
        "arxiv_id": "2508.06186v1",
        "authors": [
            "Ali Sarabadani",
            "Maryam Abdollahi Shamami",
            "Hamidreza Sadeghsalehi",
            "Borhan Asadi",
            "Saba Hesaraki"
        ],
        "submitted": "2025-08-08 10:04:40",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper presents a framework for medical diagnosis and personalized treatment recommendations using a dynamic knowledge graph and large language model integration. While it leverages language models, the focus is on medical diagnosis and treatment recommendations, which is not directly related to the user's primary research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling. The paper's relevance is limited to the user's secondary interest in NLP, but the application domain is distinct from e-commerce and does not require deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "Semantic Item Graph Enhancement for Multimodal Recommendation",
        "abstract": "Multimodal recommendation systems have attracted increasing attention for\ntheir improved performance by leveraging items' multimodal information. Prior\nmethods often build modality-specific item-item semantic graphs from raw\nmodality features and use them as supplementary structures alongside the\nuser-item interaction graph to enhance user preference learning. However, these\nsemantic graphs suffer from semantic deficiencies, including (1) insufficient\nmodeling of collaborative signals among items and (2) structural distortions\nintroduced by noise in raw modality features, ultimately compromising\nperformance. To address these issues, we first extract collaborative signals\nfrom the interaction graph and infuse them into each modality-specific item\nsemantic graph to enhance semantic modeling. Then, we design a modulus-based\npersonalized embedding perturbation mechanism that injects perturbations with\nmodulus-guided personalized intensity into embeddings to generate contrastive\nviews. This enables the model to learn noise-robust representations through\ncontrastive learning, thereby reducing the effect of structural noise in\nsemantic graphs. Besides, we propose a dual representation alignment mechanism\nthat first aligns multiple semantic representations via a designed Anchor-based\nInfoNCE loss using behavior representations as anchors, and then aligns\nbehavior representations with the fused semantics by standard InfoNCE, to\nensure representation consistency. Extensive experiments on four benchmark\ndatasets validate the effectiveness of our framework.",
        "url": "http://arxiv.org/abs/2508.06154v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06154v1",
        "arxiv_id": "2508.06154v1",
        "authors": [
            "Xiaoxiong Zhang",
            "Xin Zhou",
            "Zhiwei Zeng",
            "Dusit Niyato",
            "Zhiqi Shen"
        ],
        "submitted": "2025-08-08 09:20:50",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on multimodal recommendation systems, which is a related topic to information retrieval. However, the emphasis on multimodal features and collaborative signals among items is not directly aligned with the user's primary interest in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat related but not a central match."
    },
    {
        "title": "EvolvR: Self-Evolving Pairwise Reasoning for Story Evaluation to Enhance Generation",
        "abstract": "Although the effectiveness of Large Language Models (LLMs) as judges\n(LLM-as-a-judge) has been validated, their performance remains limited in\nopen-ended tasks, particularly in story evaluation. Accurate story evaluation\nis crucial not only for assisting human quality judgment but also for providing\nkey signals to guide story generation. However, existing methods face a\ndilemma: prompt engineering for closed-source models suffers from poor\nadaptability, while fine-tuning approaches for open-source models lack the\nrigorous reasoning capabilities essential for story evaluation. To address\nthis, we propose the Self-Evolving Pairwise Reasoning (EvolvR) framework.\nGrounded in pairwise comparison, the framework first self-synthesizes\nscore-aligned Chain-of-Thought (CoT) data via a multi-persona strategy. To\nensure data quality, these raw CoTs undergo a self-filtering process, utilizing\nmulti-agents to guarantee their logical rigor and robustness. Finally, the\nevaluator trained on the refined data is deployed as a reward model to guide\nthe story generation task. Experimental results demonstrate that our framework\nachieves state-of-the-art (SOTA) performance on three evaluation benchmarks\nincluding StoryER, HANNA and OpenMEVA. Furthermore, when served as a reward\nmodel, it significantly enhances the quality of generated stories, thereby\nfully validating the superiority of our self-evolving approach.",
        "url": "http://arxiv.org/abs/2508.06046v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06046v1",
        "arxiv_id": "2508.06046v1",
        "authors": [
            "Xinda Wang",
            "Zhengxu Hou",
            "Yangshijie Zhang",
            "Bingren Yan",
            "Zhibo Yang",
            "Xingsheng Zhang",
            "Luxi Xing",
            "Qiang Zhou",
            "Chen Zhang"
        ],
        "submitted": "2025-08-08 06:10:47",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the application of a novel framework, EvolvR, for story evaluation, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper touches on the topic of language models, it focuses on the evaluation of stories rather than query understanding, ranking models, or user behavior modeling, making it only loosely relevant to the user's interests."
    },
    {
        "title": "Dual prototype attentive graph network for cross-market recommendation",
        "abstract": "Cross-market recommender systems (CMRS) aim to utilize historical data from\nmature markets to promote multinational products in emerging markets. However,\nexisting CMRS approaches often overlook the potential for shared preferences\namong users in different markets, focusing primarily on modeling specific\npreferences within each market. In this paper, we argue that incorporating both\nmarket-specific and market-shared insights can enhance the generalizability and\nrobustness of CMRS. We propose a novel approach called Dual Prototype Attentive\nGraph Network for Cross-Market Recommendation (DGRE) to address this. DGRE\nleverages prototypes based on graph representation learning from both items and\nusers to capture market-specific and market-shared insights. Specifically, DGRE\nincorporates market-shared prototypes by clustering users from various markets\nto identify behavioural similarities and create market-shared user profiles.\nAdditionally, it constructs item-side prototypes by aggregating item features\nwithin each market, providing valuable market-specific insights. We conduct\nextensive experiments to validate the effectiveness of DGRE on a real-world\ncross-market dataset, and the results show that considering both\nmarket-specific and market-sharing aspects in modelling can improve the\ngeneralization and robustness of CMRS.",
        "url": "http://arxiv.org/abs/2508.05969v1",
        "pdf_url": "http://arxiv.org/pdf/2508.05969v1",
        "arxiv_id": "2508.05969v1",
        "authors": [
            "Li Fan",
            "Menglin Kong",
            "Yang Xiang",
            "Chong Zhang",
            "Chengtao Ji"
        ],
        "submitted": "2025-08-08 03:07:44",
        "source": "arxiv",
        "comment": "Accepted by ICONIP 2025 (Oral)",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on recommender systems, which is a related topic, but it does not address information retrieval, query understanding, ranking models, or user behavior modeling, which are the core research themes in the user's interests. The paper's emphasis on graph representation learning and clustering users from various markets is not directly relevant to the user's background in e-commerce or NLP."
    },
    {
        "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
        "abstract": "We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language\nmodel with 355B total parameters and 32B activated parameters, featuring a\nhybrid reasoning method that supports both thinking and direct response modes.\nThrough multi-stage training on 23T tokens and comprehensive post-training with\nexpert model iteration and reinforcement learning, GLM-4.5 achieves strong\nperformance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on\nTAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer\nparameters than several competitors, GLM-4.5 ranks 3rd overall among all\nevaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B\nparameters) and a compact version, GLM-4.5-Air (106B parameters), to advance\nresearch in reasoning and agentic AI systems. Code, models, and more\ninformation are available at https://github.com/zai-org/GLM-4.5.",
        "url": "http://arxiv.org/abs/2508.06471v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06471v1",
        "arxiv_id": "2508.06471v1",
        "authors": [
            "GLM-4. 5 Team",
            ":",
            "Aohan Zeng",
            "Xin Lv",
            "Qinkai Zheng",
            "Zhenyu Hou",
            "Bin Chen",
            "Chengxing Xie",
            "Cunxiang Wang",
            "Da Yin",
            "Hao Zeng",
            "Jiajie Zhang",
            "Kedong Wang",
            "Lucen Zhong",
            "Mingdao Liu",
            "Rui Lu",
            "Shulin Cao",
            "Xiaohan Zhang",
            "Xuancheng Huang",
            "Yao Wei",
            "Yean Cheng",
            "Yifan An",
            "Yilin Niu",
            "Yuanhao Wen",
            "Yushi Bai",
            "Zhengxiao Du",
            "Zihan Wang",
            "Zilin Zhu",
            "Bohan Zhang",
            "Bosi Wen",
            "Bowen Wu",
            "Bowen Xu",
            "Can Huang",
            "Casey Zhao",
            "Changpeng Cai",
            "Chao Yu",
            "Chen Li",
            "Chendi Ge",
            "Chenghua Huang",
            "Chenhui Zhang",
            "Chenxi Xu",
            "Chenzheng Zhu",
            "Chuang Li",
            "Congfeng Yin",
            "Daoyan Lin",
            "Dayong Yang",
            "Dazhi Jiang",
            "Ding Ai",
            "Erle Zhu",
            "Fei Wang",
            "Gengzheng Pan",
            "Guo Wang",
            "Hailong Sun",
            "Haitao Li",
            "Haiyang Li",
            "Haiyi Hu",
            "Hanyu Zhang",
            "Hao Peng",
            "Hao Tai",
            "Haoke Zhang",
            "Haoran Wang",
            "Haoyu Yang",
            "He Liu",
            "He Zhao",
            "Hongwei Liu",
            "Hongxi Yan",
            "Huan Liu",
            "Huilong Chen",
            "Ji Li",
            "Jiajing Zhao",
            "Jiamin Ren",
            "Jian Jiao",
            "Jiani Zhao",
            "Jianyang Yan",
            "Jiaqi Wang",
            "Jiayi Gui",
            "Jiayue Zhao",
            "Jie Liu",
            "Jijie Li",
            "Jing Li",
            "Jing Lu",
            "Jingsen Wang",
            "Jingwei Yuan",
            "Jingxuan Li",
            "Jingzhao Du",
            "Jinhua Du",
            "Jinxin Liu",
            "Junkai Zhi",
            "Junli Gao",
            "Ke Wang",
            "Lekang Yang",
            "Liang Xu",
            "Lin Fan",
            "Lindong Wu",
            "Lintao Ding",
            "Lu Wang",
            "Man Zhang",
            "Minghao Li",
            "Minghuan Xu",
            "Mingming Zhao",
            "Mingshu Zhai",
            "Pengfan Du",
            "Qian Dong",
            "Shangde Lei",
            "Shangqing Tu",
            "Shangtong Yang",
            "Shaoyou Lu",
            "Shijie Li",
            "Shuang Li",
            "Shuang-Li",
            "Shuxun Yang",
            "Sibo Yi",
            "Tianshu Yu",
            "Wei Tian",
            "Weihan Wang",
            "Wenbo Yu",
            "Weng Lam Tam",
            "Wenjie Liang",
            "Wentao Liu",
            "Xiao Wang",
            "Xiaohan Jia",
            "Xiaotao Gu",
            "Xiaoying Ling",
            "Xin Wang",
            "Xing Fan",
            "Xingru Pan",
            "Xinyuan Zhang",
            "Xinze Zhang",
            "Xiuqing Fu",
            "Xunkai Zhang",
            "Yabo Xu",
            "Yandong Wu",
            "Yida Lu",
            "Yidong Wang",
            "Yilin Zhou",
            "Yiming Pan",
            "Ying Zhang",
            "Yingli Wang",
            "Yingru Li",
            "Yinpei Su",
            "Yipeng Geng",
            "Yitong Zhu",
            "Yongkun Yang",
            "Yuhang Li",
            "Yuhao Wu",
            "Yujiang Li",
            "Yunan Liu",
            "Yunqing Wang",
            "Yuntao Li",
            "Yuxuan Zhang",
            "Zezhen Liu",
            "Zhen Yang",
            "Zhengda Zhou",
            "Zhongpei Qiao",
            "Zhuoer Feng",
            "Zhuorui Liu",
            "Zichen Zhang",
            "Zihan Wang",
            "Zijun Yao",
            "Zikang Wang",
            "Ziqiang Liu",
            "Ziwei Chai",
            "Zixuan Li",
            "Zuodong Zhao",
            "Wenguang Chen",
            "Jidong Zhai",
            "Bin Xu",
            "Minlie Huang",
            "Hongning Wang",
            "Juanzi Li",
            "Yuxiao Dong",
            "Jie Tang"
        ],
        "submitted": "2025-08-08 17:21:06",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on large language models and reasoning tasks, which do not align with your primary focus on Information Retrieval, Search technologies, and query understanding. The paper's abstract does not mention any concepts related to your interests, such as ranking models, user behavior modeling, or real-time relevance optimization."
    },
    {
        "title": "Memp: Exploring Agent Procedural Memory",
        "abstract": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains.",
        "url": "http://arxiv.org/abs/2508.06433v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06433v1",
        "arxiv_id": "2508.06433v1",
        "authors": [
            "Runnan Fang",
            "Yuan Liang",
            "Xiaobin Wang",
            "Jialong Wu",
            "Shuofei Qiao",
            "Pengjun Xie",
            "Fei Huang",
            "Huajun Chen",
            "Ningyu Zhang"
        ],
        "submitted": "2025-08-08 16:20:56",
        "source": "arxiv",
        "comment": "Work in progress",
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper explores procedural memory in large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on agent trajectories and memory updates, the focus is on procedural memory rather than semantic understanding or real-time relevance optimization."
    },
    {
        "title": "Sample-efficient LLM Optimization with Reset Replay",
        "abstract": "Recent advancements in post-training Large Language Models (LLMs),\nparticularly through Reinforcement Learning (RL) and preference optimization\nmethods, are key drivers for enhancing their reasoning capabilities. However,\nthese methods are often plagued by low sample efficiency and a susceptibility\nto primacy bias, where overfitting to initial experiences degrades policy\nquality and damages the learning process. To address these challenges, we\nintroduce LLM optimization with Reset Replay (LoRR), a general and powerful\nplugin designed to enhance sample efficiency in any preference-based\noptimization framework. LoRR core mechanism enables training at a high replay\nnumber, maximizing the utility of each collected data batch. To counteract the\nrisk of overfitting inherent in high-replay training, LoRR incorporates a\nperiodic reset strategy with reusing initial data, which preserves network\nplasticity. Furthermore, it leverages a hybrid optimization objective,\ncombining supervised fine-tuning (SFT) and preference-based losses to further\nbolster data exploitation. Our extensive experiments demonstrate that LoRR\nsignificantly boosts the performance of various preference optimization methods\non both mathematical and general reasoning benchmarks. Notably, an iterative\nDPO approach augmented with LoRR achieves comparable performance on challenging\nmath tasks, outperforming some complex and computationally intensive RL-based\nalgorithms. These findings highlight that LoRR offers a practical,\nsample-efficient, and highly effective paradigm for LLM finetuning, unlocking\ngreater performance from limited data.",
        "url": "http://arxiv.org/abs/2508.06412v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06412v1",
        "arxiv_id": "2508.06412v1",
        "authors": [
            "Zichuan Liu",
            "Jinyu Wang",
            "Lei Song",
            "Jiang Bian"
        ],
        "submitted": "2025-08-08 15:56:49",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on optimizing Large Language Models (LLMs) using Reinforcement Learning and preference optimization methods, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it mentions optimization, the context is different from the user's interests in ranking models and user behavior modeling."
    },
    {
        "title": "Matrix-Driven Instant Review: Confident Detection and Reconstruction of LLM Plagiarism on PC",
        "abstract": "In recent years, concerns about intellectual property (IP) in large language\nmodels (LLMs) have grown significantly. Plagiarizing other LLMs (through direct\nweight copying, upcycling, pruning, or continual pretraining) and claiming\nauthorship without properly attributing to the original license, is a serious\nmisconduct that can lead to significant financial and reputational harm to the\noriginal developers. However, existing methods for detecting LLM plagiarism\nfall short in key areas. They fail to accurately reconstruct weight\ncorrespondences, lack the ability to compute statistical significance measures\nsuch as $p$-values, and may mistakenly flag models trained on similar data as\nbeing related. To address these limitations, we propose Matrix-Driven Instant\nReview (MDIR), a novel method that leverages matrix analysis and Large\nDeviation Theory. MDIR achieves accurate reconstruction of weight\nrelationships, provides rigorous $p$-value estimation, and focuses exclusively\non weight similarity without requiring full model inference. Experimental\nresults demonstrate that MDIR reliably detects plagiarism even after extensive\ntransformations, such as random permutations and continual pretraining with\ntrillions of tokens. Moreover, all detections can be performed on a single PC\nwithin an hour, making MDIR both efficient and accessible.",
        "url": "http://arxiv.org/abs/2508.06309v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06309v1",
        "arxiv_id": "2508.06309v1",
        "authors": [
            "Ruichong Zhang"
        ],
        "submitted": "2025-08-08 13:35:40",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on detecting plagiarism in large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on matrix analysis, it is not applicable to the user's core research themes."
    },
    {
        "title": "InfoCausalQA:Can Models Perform Non-explicit Causal Reasoning Based on Infographic?",
        "abstract": "Recent advances in Vision-Language Models (VLMs) have demonstrated impressive\ncapabilities in perception and reasoning. However, the ability to perform\ncausal inference -- a core aspect of human cognition -- remains underexplored,\nparticularly in multimodal settings. In this study, we introduce InfoCausalQA,\na novel benchmark designed to evaluate causal reasoning grounded in\ninfographics that combine structured visual data with textual context. The\nbenchmark comprises two tasks: Task 1 focuses on quantitative causal reasoning\nbased on inferred numerical trends, while Task 2 targets semantic causal\nreasoning involving five types of causal relations: cause, effect,\nintervention, counterfactual, and temporal. We manually collected 494\ninfographic-text pairs from four public sources and used GPT-4o to generate\n1,482 high-quality multiple-choice QA pairs. These questions were then\ncarefully revised by humans to ensure they cannot be answered based on\nsurface-level cues alone but instead require genuine visual grounding. Our\nexperimental results reveal that current VLMs exhibit limited capability in\ncomputational reasoning and even more pronounced limitations in semantic causal\nreasoning. Their significantly lower performance compared to humans indicates a\nsubstantial gap in leveraging infographic-based information for causal\ninference. Through InfoCausalQA, we highlight the need for advancing the causal\nreasoning abilities of multimodal AI systems.",
        "url": "http://arxiv.org/abs/2508.06220v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06220v1",
        "arxiv_id": "2508.06220v1",
        "authors": [
            "Keummin Ka",
            "Junhyeong Park",
            "Jahyun Jeon",
            "Youngjae Yu"
        ],
        "submitted": "2025-08-08 11:03:23",
        "source": "arxiv",
        "comment": "14 pages, 9 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores causal reasoning in multimodal settings, using infographics and textual context, which is not directly related to my primary research interests in Information Retrieval and Search technologies. While it touches on multimodal AI systems, the focus on causal inference and reasoning is not a central match for my interests."
    },
    {
        "title": "Pragmatics beyond humans: meaning, communication, and LLMs",
        "abstract": "The paper reconceptualizes pragmatics not as a subordinate, third dimension\nof meaning, but as a dynamic interface through which language operates as a\nsocially embedded tool for action. With the emergence of large language models\n(LLMs) in communicative contexts, this understanding needs to be further\nrefined and methodologically reconsidered. The first section challenges the\ntraditional semiotic trichotomy, arguing that connectionist LLM architectures\ndestabilize established hierarchies of meaning, and proposes the Human-Machine\nCommunication (HMC) framework as a more suitable alternative. The second\nsection examines the tension between human-centred pragmatic theories and the\nmachine-centred nature of LLMs. While traditional, Gricean-inspired pragmatics\ncontinue to dominate, it relies on human-specific assumptions ill-suited to\npredictive systems like LLMs. Probabilistic pragmatics, particularly the\nRational Speech Act framework, offers a more compatible teleology by focusing\non optimization rather than truth-evaluation. The third section addresses the\nissue of substitutionalism in three forms - generalizing, linguistic, and\ncommunicative - highlighting the anthropomorphic biases that distort LLM\nevaluation and obscure the role of human communicative subjects. Finally, the\npaper introduces the concept of context frustration to describe the paradox of\nincreased contextual input paired with a collapse in contextual understanding,\nemphasizing how users are compelled to co-construct pragmatic conditions both\nfor the model and themselves. These arguments suggest that pragmatic theory may\nneed to be adjusted or expanded to better account for communication involving\ngenerative AI.",
        "url": "http://arxiv.org/abs/2508.06167v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06167v1",
        "arxiv_id": "2508.06167v1",
        "authors": [
            "Vít Gvoždiak"
        ],
        "submitted": "2025-08-08 09:34:41",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper's focus on pragmatics, language models, and human-machine communication is not directly related to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on topics like query understanding and user behavior modeling, the context is vastly different and not applicable to your areas of focus."
    },
    {
        "title": "One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging",
        "abstract": "Model merging has emerged as a compelling data-free paradigm for multi-task\nlearning, enabling the fusion of multiple fine-tuned models into a single,\npowerful entity. A key technique in merging methods is sparsification, which\nprunes redundant parameters from task vectors to mitigate interference.\nHowever, prevailing approaches employ a ``one-size-fits-all'' strategy,\napplying a uniform sparsity ratio that overlooks the inherent structural and\nstatistical heterogeneity of model parameters. This often leads to a suboptimal\ntrade-off, where critical parameters are inadvertently pruned while less useful\nones are retained. To address this limitation, we introduce \\textbf{TADrop}\n(\\textbf{T}ensor-wise \\textbf{A}daptive \\textbf{Drop}), an adaptive\nsparsification strategy that respects this heterogeneity. Instead of a global\nratio, TADrop assigns a tailored sparsity level to each parameter tensor based\non its distributional properties. The core intuition is that tensors with\ndenser, more redundant distributions can be pruned aggressively, while sparser,\nmore critical ones are preserved. As a simple and plug-and-play module, we\nvalidate TADrop by integrating it with foundational, classic, and SOTA merging\nmethods. Extensive experiments across diverse tasks (vision, language, and\nmultimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and\nsignificantly boosts their performance. For instance, when enhancing a leading\nmerging method, it achieves an average performance gain of 2.0\\% across 8\nViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter\ninterference by tailoring sparsification to the model's structure, offering a\nnew baseline for high-performance model merging.",
        "url": "http://arxiv.org/abs/2508.06163v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06163v1",
        "arxiv_id": "2508.06163v1",
        "authors": [
            "Yingfeng Luo",
            "Dingyang Lin",
            "Junxin Wang",
            "Ziqiang Xu",
            "Kaiyan Chang",
            "Tong Zheng",
            "Bei Li",
            "Anxiang Ma",
            "Tong Xiao",
            "Zhengtao Yu",
            "Jingbo Zhu"
        ],
        "submitted": "2025-08-08 09:33:08",
        "source": "arxiv",
        "comment": "Under review",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on model merging and sparsification, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions the concept of 'parameter interference', it does not address the specific challenges in IR, such as ranking models or user behavior modeling."
    },
    {
        "title": "ConlangCrafter: Constructing Languages with a Multi-Hop LLM Pipeline",
        "abstract": "Constructed languages (conlangs) such as Esperanto and Quenya have played\ndiverse roles in art, philosophy, and international communication. Meanwhile,\nlarge-scale foundation models have revolutionized creative generation in text,\nimages, and beyond. In this work, we leverage modern LLMs as computational\ncreativity aids for end-to-end conlang creation. We introduce ConlangCrafter, a\nmulti-hop pipeline that decomposes language design into modular stages --\nphonology, morphology, syntax, lexicon generation, and translation. At each\nstage, our method leverages LLMs' meta-linguistic reasoning capabilities,\ninjecting randomness to encourage diversity and leveraging self-refinement\nfeedback to encourage consistency in the emerging language description. We\nevaluate ConlangCrafter on metrics measuring coherence and typological\ndiversity, demonstrating its ability to produce coherent and varied conlangs\nwithout human linguistic expertise.",
        "url": "http://arxiv.org/abs/2508.06094v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06094v1",
        "arxiv_id": "2508.06094v1",
        "authors": [
            "Morris Alper",
            "Moran Yanuka",
            "Raja Giryes",
            "Gašper Beguš"
        ],
        "submitted": "2025-08-08 07:36:48",
        "source": "arxiv",
        "comment": "Project page: https://conlangcrafter.github.io",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus is on constructed languages and computational creativity, which is a distinct area that does not align with your primary research themes."
    },
    {
        "title": "Efficient Knowledge Probing of Large Language Models by Adapting Pre-trained Embeddings",
        "abstract": "Large language models (LLMs) acquire knowledge across diverse domains such as\nscience, history, and geography encountered during generative pre-training.\nHowever, due to their stochasticity, it is difficult to predict what LLMs have\nacquired. Prior work has developed different ways to probe this knowledge by\ninvestigating the hidden representations, crafting specific task prompts,\ncurating representative samples, and estimating their uncertainty. However,\nthese methods require making forward passes through the underlying model to\nprobe the LLM's knowledge about a specific fact, making them computationally\nexpensive and time-consuming. To bridge this gap, we propose $\\textbf{PEEK}$ or\n$\\textbf{P}$roxy $\\textbf{E}$mbeddings to $\\textbf{E}$stimate\n$\\textbf{K}$nowledge of LLMs, by leveraging the pre-trained embedding models\nthat effectively encode factual knowledge as text or graphs as proxies for\nLLMs. First, we identify a training set of facts known by LLMs through various\nprobing strategies and then adapt embedding models to predict the LLM outputs\nwith a linear decoder layer. Comprehensive evaluation on $3$ Wikipedia-derived\ndatasets, $4$ LLMs, and $7$ embedding models shows that embeddings can predict\nLLM knowledge on a held-out set with up to 90 % accuracy. Furthermore, we find\nthat sentence embedding models are more suitable than graph embeddings to\npredict LLM knowledge, shedding light on the underlying representation of the\nfactual landscape. Thus, we believe that knowledge-adapted embeddings can be\nused to identify knowledge gaps in LLMs at scale and can provide deeper\ninsights into LLMs' internal inductive bias. The code and data are made\navailable at https://github.com/claws-lab/peek.",
        "url": "http://arxiv.org/abs/2508.06030v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06030v1",
        "arxiv_id": "2508.06030v1",
        "authors": [
            "Kartik Sharma",
            "Yiqiao Jin",
            "Rakshit Trivedi",
            "Srijan Kumar"
        ],
        "submitted": "2025-08-08 05:32:31",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus is on probing knowledge in large language models, which is a topic in Natural Language Processing, but not specifically in IR or Search technologies."
    },
    {
        "title": "Discovering Properties of Inflectional Morphology in Neural Emergent Communication",
        "abstract": "Emergent communication (EmCom) with deep neural network-based agents promises\nto yield insights into the nature of human language, but remains focused\nprimarily on a few subfield-specific goals and metrics that prioritize\ncommunication schemes which represent attributes with unique characters\none-to-one and compose them syntactically. We thus reinterpret a common EmCom\nsetting, the attribute-value reconstruction game, by imposing a\nsmall-vocabulary constraint to simulate double articulation, and formulating a\nnovel setting analogous to naturalistic inflectional morphology (enabling\nmeaningful comparison to natural language communication schemes). We develop\nnew metrics and explore variations of this game motivated by real properties of\ninflectional morphology: concatenativity and fusionality. Through our\nexperiments, we discover that simulated phonological constraints encourage\nconcatenative morphology, and emergent languages replicate the tendency of\nnatural languages to fuse grammatical attributes.",
        "url": "http://arxiv.org/abs/2508.05843v1",
        "pdf_url": "http://arxiv.org/pdf/2508.05843v1",
        "arxiv_id": "2508.05843v1",
        "authors": [
            "Miles Gilberti",
            "Shane Storks",
            "Huteng Dai"
        ],
        "submitted": "2025-08-07 20:44:50",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper's focus on emergent communication and neural network-based agents is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The abstract does not mention query understanding, ranking models, or user behavior modeling, and the context of inflectional morphology is not relevant to the user's areas of interest."
    },
    {
        "title": "Basic interactive algorithms: Preview",
        "abstract": "This dialog paper offers a preview and provides a foretaste of an upcoming\nwork on the axiomatization of basic interactive algorithms.\n  The modern notion of algorithm was elucidated in the 1930s--1950s. It was\naxiomatized a quarter of a century ago as the notion of ``sequential\nalgorithm'' or ``classical algorithm''; we prefer to call it ``basic algorithm\"\nnow. The axiomatization was used to show that for every basic algorithm there\nis a behaviorally equivalent abstract state machine. It was also used to prove\nthe Church-Turing thesis as it has been understood by the logicians.\n  Starting from the 1960s, the notion of algorithm has expanded --\nprobabilistic algorithms, quantum algorithms, etc. -- prompting introduction of\na much more ambitious version of the Church-Turing thesis commonly known as the\n``physical thesis.'' We emphasize the difference between the two versions of\nthe Church-Turing thesis and illustrate how nondeterministic and probabilistic\nalgorithms can be viewed as basic algorithms with appropriate oracles. The same\nview applies to quantum circuit algorithms and many other classes of\nalgorithms.",
        "url": "http://arxiv.org/abs/2508.05798v1",
        "pdf_url": "http://arxiv.org/pdf/2508.05798v1",
        "arxiv_id": "2508.05798v1",
        "authors": [
            "Yuri Gurevich"
        ],
        "submitted": "2025-08-07 19:13:47",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, Natural Language Processing, data mining, or recommender systems. The paper focuses on the axiomatization of basic interactive algorithms, which is a topic in computer science theory, and does not relate to your areas of interest."
    },
    {
        "title": "HapticLLaMA: A Multimodal Sensory Language Model for Haptic Captioning",
        "abstract": "Haptic captioning is the task of generating natural language descriptions\nfrom haptic signals, such as vibrations, for use in virtual reality,\naccessibility, and rehabilitation applications. While previous multimodal\nresearch has focused primarily on vision and audio, haptic signals for the\nsense of touch remain underexplored. To address this gap, we formalize the\nhaptic captioning task and propose HapticLLaMA, a multimodal sensory language\nmodel that interprets vibration signals into descriptions in a given sensory,\nemotional, or associative category. We investigate two types of haptic\ntokenizers, a frequency-based tokenizer and an EnCodec-based tokenizer, that\nconvert haptic signals into sequences of discrete units, enabling their\nintegration with the LLaMA model. HapticLLaMA is trained in two stages: (1)\nsupervised fine-tuning using the LLaMA architecture with LoRA-based adaptation,\nand (2) fine-tuning via reinforcement learning from human feedback (RLHF). We\nassess HapticLLaMA's captioning performance using both automated n-gram metrics\nand human evaluation. HapticLLaMA demonstrates strong capability in\ninterpreting haptic vibration signals, achieving a METEOR score of 59.98 and a\nBLEU-4 score of 32.06 respectively. Additionally, over 61% of the generated\ncaptions received human ratings above 3.5 on a 7-point scale, with RLHF\nyielding a 10% improvement in the overall rating distribution, indicating\nstronger alignment with human haptic perception. These findings highlight the\npotential of large language models to process and adapt to sensory data.",
        "url": "http://arxiv.org/abs/2508.06475v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06475v1",
        "arxiv_id": "2508.06475v1",
        "authors": [
            "Guimin Hu",
            "Daniel Hershcovich",
            "Hasti Seifi"
        ],
        "submitted": "2025-08-08 17:25:37",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on haptic captioning, a topic outside the scope of information retrieval and search technologies. While it involves language models, the application is specific to haptic signals and does not relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest."
    },
    {
        "title": "LLMs vs. Chinese Anime Enthusiasts: A Comparative Study on Emotionally Supportive Role-Playing",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities in\nrole-playing conversations and providing emotional support as separate research\ndirections. However, there remains a significant research gap in combining\nthese capabilities to enable emotionally supportive interactions with virtual\ncharacters. To address this research gap, we focus on anime characters as a\ncase study because of their well-defined personalities and large fan bases.\nThis choice enables us to effectively evaluate how well LLMs can provide\nemotional support while maintaining specific character traits. We introduce\nChatAnime, the first Emotionally Supportive Role-Playing (ESRP) dataset. We\nfirst thoughtfully select 20 top-tier characters from popular anime communities\nand design 60 emotion-centric real-world scenario questions. Then, we execute a\nnationwide selection process to identify 40 Chinese anime enthusiasts with\nprofound knowledge of specific characters and extensive experience in\nrole-playing. Next, we systematically collect two rounds of dialogue data from\n10 LLMs and these 40 Chinese anime enthusiasts. To evaluate the ESRP\nperformance of LLMs, we design a user experience-oriented evaluation system\nfeaturing 9 fine-grained metrics across three dimensions: basic dialogue,\nrole-playing and emotional support, along with an overall metric for response\ndiversity. In total, the dataset comprises 2,400 human-written and 24,000\nLLM-generated answers, supported by over 132,000 human annotations.\nExperimental results show that top-performing LLMs surpass human fans in\nrole-playing and emotional support, while humans still lead in response\ndiversity. We hope this work can provide valuable resources and insights for\nfuture research on optimizing LLMs in ESRP. Our datasets are available at\nhttps://github.com/LanlanQiu/ChatAnime.",
        "url": "http://arxiv.org/abs/2508.06388v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06388v1",
        "arxiv_id": "2508.06388v1",
        "authors": [
            "Lanlan Qiu",
            "Xiao Pu",
            "Yeqi Feng",
            "Tianxing He"
        ],
        "submitted": "2025-08-08 15:17:24",
        "source": "arxiv",
        "comment": "21 pages, 17 figures, 3 tables",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus on Large Language Models and emotionally supportive role-playing conversations is not aligned with your areas of interest, and the paper does not address query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Evaluating Style-Personalized Text Generation: Challenges and Directions",
        "abstract": "While prior research has built tools and benchmarks towards style\npersonalized text generation, there has been limited exploration of evaluation\nin low-resource author style personalized text generation space. Through this\nwork, we question the effectiveness of the widely adopted evaluation metrics\nlike BLEU and ROUGE, and explore other evaluation paradigms such as style\nembeddings and LLM-as-judge to holistically evaluate the style personalized\ntext generation task. We evaluate these metrics and their ensembles using our\nstyle discrimination benchmark, that spans eight writing tasks, and evaluates\nacross three settings, domain discrimination, authorship attribution, and LLM\npersonalized vs non-personalized discrimination. We provide conclusive evidence\nto adopt ensemble of diverse evaluation metrics to effectively evaluate style\npersonalized text generation.",
        "url": "http://arxiv.org/abs/2508.06374v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06374v1",
        "arxiv_id": "2508.06374v1",
        "authors": [
            "Anubhav Jangra",
            "Bahareh Sarrafzadeh",
            "Adrian de Wynter",
            "Silviu Cucerzan",
            "Sujay Kumar Jauhar"
        ],
        "submitted": "2025-08-08 15:07:31",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on evaluating style-personalized text generation, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and query understanding. While it touches on evaluation metrics, it does not address ranking models, user behavior modeling, or deep semantic understanding, making it only loosely relevant to the user's interests."
    },
    {
        "title": "Beyond Uniform Criteria: Scenario-Adaptive Multi-Dimensional Jailbreak Evaluation",
        "abstract": "Precise jailbreak evaluation is vital for LLM red teaming and jailbreak\nresearch. Current approaches employ binary classification ( e.g., string\nmatching, toxic text classifiers, LLM-driven methods), yielding only \"yes/no\"\nlabels without quantifying harm intensity. Existing multi-dimensional\nframeworks ( e.g., Security Violation, Relative Truthfulness, Informativeness)\napply uniform evaluation criteria across scenarios, resulting in\nscenario-specific mismatches--for instance, \"Relative Truthfulness\" is\nirrelevant to \"hate speech\"--which compromise evaluation precision. To tackle\nthese limitations, we introduce SceneJailEval, with key contributions: (1) A\ngroundbreaking scenario-adaptive multi-dimensional framework for jailbreak\nevaluation, overcoming the critical \"one-size-fits-all\" constraint of existing\nmulti-dimensional methods, and featuring strong extensibility to flexibly adapt\nto customized or emerging scenarios. (2) A comprehensive 14-scenario dataset\nwith diverse jailbreak variants and regional cases, filling the long-standing\ngap in high-quality, holistic benchmarks for scenario-adaptive evaluation. (3)\nSceneJailEval achieves state-of-the-art results, with an F1 score of 0.917 on\nour full-scenario dataset (+6% over prior SOTA) and 0.995 on JBB (+3% over\nprior SOTA), surpassing accuracy limits of existing evaluation methods in\nheterogeneous scenarios and confirming its advantage.",
        "url": "http://arxiv.org/abs/2508.06194v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06194v1",
        "arxiv_id": "2508.06194v1",
        "authors": [
            "Lai Jiang",
            "Yuekang Li",
            "Xiaohan Zhang",
            "Youtao Ding",
            "Li Pan"
        ],
        "submitted": "2025-08-08 10:19:21",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be unrelated to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on a specific domain (LLM red teaming and jailbreak research) and uses terminology and concepts unfamiliar to the user. The paper's abstract does not mention any of the user's key areas of interest, such as query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "AURA: Affordance-Understanding and Risk-aware Alignment Technique for Large Language Models",
        "abstract": "Present day LLMs face the challenge of managing affordance-based safety\nrisks-situations where outputs inadvertently facilitate harmful actions due to\noverlooked logical implications. Traditional safety solutions, such as scalar\noutcome-based reward models, parameter tuning, or heuristic decoding\nstrategies, lack the granularity and proactive nature needed to reliably detect\nand intervene during subtle yet crucial reasoning steps. Addressing this\nfundamental gap, we introduce AURA, an innovative, multi-layered framework\ncentered around Process Reward Models (PRMs), providing comprehensive, step\nlevel evaluations across logical coherence and safety-awareness. Our framework\nseamlessly combines introspective self-critique, fine-grained PRM assessments,\nand adaptive safety-aware decoding to dynamically and proactively guide models\ntoward safer reasoning trajectories. Empirical evidence clearly demonstrates\nthat this approach significantly surpasses existing methods, significantly\nimproving the logical integrity and affordance-sensitive safety of model\noutputs. This research represents a pivotal step toward safer, more\nresponsible, and contextually aware AI, setting a new benchmark for\nalignment-sensitive applications.",
        "url": "http://arxiv.org/abs/2508.06124v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06124v1",
        "arxiv_id": "2508.06124v1",
        "authors": [
            "Sayantan Adak",
            "Pratyush Chatterjee",
            "Somnath Banerjee",
            "Rima Hazra",
            "Somak Aditya",
            "Animesh Mukherjee"
        ],
        "submitted": "2025-08-08 08:43:24",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on the safety and alignment of large language models, introducing a novel framework called AURA. While it touches on the topic of understanding and modeling user behavior, it is not directly related to information retrieval, search technologies, or query understanding. The paper's emphasis on logical coherence and safety-awareness is interesting, but it does not align with the user's primary research interests in IR and NLP."
    },
    {
        "title": "When a Paper Has 1000 Authors: Rethinking Citation Metrics in the Era of LLMs",
        "abstract": "Author-level citation metrics provide a practical, interpretable, and\nscalable signal of scholarly influence in a complex research ecosystem. It has\nbeen widely used as a proxy in hiring decisions. However, the past five years\nhave seen the rapid emergence of large-scale publications in the field of large\nlanguage models and foundation models, with papers featuring hundreds to\nthousands of co-authors and receiving tens of thousands of citations within\nmonths. For example, Gemini has 1361 authors and has been cited around 4600\ntimes in 19 months. In such cases, traditional metrics, such as total citation\ncount and the $h$-index, fail to meaningfully distinguish individual\ncontributions. Therefore, we propose the following research question: How can\none identify standout researchers among thousands of co-authors in large-scale\nLLM papers? This question is particularly important in scenarios such as\nacademic hiring and funding decisions. In this paper, we introduce a novel\ncitation metric designed to address this challenge by balancing contributions\nacross large-scale and small-scale publications. We propose the SBCI index,\nanalyze its theoretical properties, and evaluate its behavior on synthetic\npublication datasets. Our results demonstrate that the proposed metric provides\na more robust and discriminative assessment of individual scholarly impact in\nthe era of large-scale collaborations.",
        "url": "http://arxiv.org/abs/2508.06004v1",
        "pdf_url": "http://arxiv.org/pdf/2508.06004v1",
        "arxiv_id": "2508.06004v1",
        "authors": [
            "Weihang Guo",
            "Zhao Song",
            "Jiahao Zhang"
        ],
        "submitted": "2025-08-08 04:18:26",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper's focus on citation metrics and large-scale collaborations is not directly related to the user's interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. While the paper touches on the topic of large-scale publications, it does not explore the user's areas of interest in IR, NLP, and data mining."
    },
    {
        "title": "Efficient Multimodal Streaming Recommendation via Expandable Side Mixture-of-Experts",
        "abstract": "Streaming recommender systems (SRSs) are widely deployed in real-world\napplications, where user interests shift and new items arrive over time. As a\nresult, effectively capturing users' latest preferences is challenging, as\ninteractions reflecting recent interests are limited and new items often lack\nsufficient feedback. A common solution is to enrich item representations using\nmultimodal encoders (e.g., BERT or ViT) to extract visual and textual features.\nHowever, these encoders are pretrained on general-purpose tasks: they are not\ntailored to user preference modeling, and they overlook the fact that user\ntastes toward modality-specific features such as visual styles and textual\ntones can also drift over time. This presents two key challenges in streaming\nscenarios: the high cost of fine-tuning large multimodal encoders, and the risk\nof forgetting long-term user preferences due to continuous model updates.\n  To tackle these challenges, we propose Expandable Side Mixture-of-Experts\n(XSMoE), a memory-efficient framework for multimodal streaming recommendation.\nXSMoE attaches lightweight side-tuning modules consisting of expandable expert\nnetworks to frozen pretrained encoders and incrementally expands them in\nresponse to evolving user feedback. A gating router dynamically combines expert\nand backbone outputs, while a utilization-based pruning strategy maintains\nmodel compactness. By learning new patterns through expandable experts without\noverwriting previously acquired knowledge, XSMoE effectively captures both cold\nstart and shifting preferences in multimodal features. Experiments on three\nreal-world datasets demonstrate that XSMoE outperforms state-of-the-art\nbaselines in both recommendation quality and computational efficiency.",
        "url": "http://arxiv.org/abs/2508.05993v1",
        "pdf_url": "http://arxiv.org/pdf/2508.05993v1",
        "arxiv_id": "2508.05993v1",
        "authors": [
            "Yunke Qu",
            "Liang Qu",
            "Tong Chen",
            "Quoc Viet Hung Nguyen",
            "Hongzhi Yin"
        ],
        "submitted": "2025-08-08 04:00:05",
        "source": "arxiv",
        "comment": "Accepted to CIKM 2025",
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on streaming recommender systems, which is related to your interest in information retrieval and search technologies. However, the emphasis on multimodal features, expandable expert networks, and memory efficiency is not directly aligned with your core research themes, such as query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "\"Mirror\" Language AI Models of Depression are Criterion-Contaminated",
        "abstract": "A growing number of studies show near-perfect LLM language-based prediction\nof depression assessment scores (up to R2 of .70). However, many develop these\nmodels directly from language responses to depression assessments. These\n\"Mirror models\" suffer from \"criterion contamination\", which arises when a\npredicted score depends in part on the predictors themselves. This causes\nartificial effect size inflation which reduces model generalizability. The\npresent study compares the performance of Mirror models versus \"Non-Mirror\nmodels\", which are developed from language that does not mirror the assessment\nthey are developed to predict. N = 110 research participants completed two\ndifferent interviews: structured diagnostic and life history interviews. GPT-4,\nGPT-4o and LLaMA3-70B were then prompted to predict structured diagnostic\ninterview depression scores from the two transcripts separately. Mirror models\n(using structured diagnostic data) showed very large effect sizes (e.g., R2 =\n.80). As expected, NonMirror models (using life history data) demonstrated\nsmaller effect sizes, but were relatively large (e.g., R2 = .27). When Mirror\nand Non-Mirror model-predicted structured interview depression scores were\ncorrelated with self-reported depression symptoms, Mirror and NonMirror\nperformed the same (e.g., r = ~.54), indicating that Mirror models contain bias\nperhaps due to criterion contamination. Topic modeling identified clusters\nacross Mirror and Non-Mirror models, as well as between true-positive and\nfalse-positive predictions. In this head-to-head comparison study, Mirror\nlanguage AI models of depression showed artificially inflated effect sizes and\nless generalizability. As language AI models for depression continue to evolve,\nincorporating Non-Mirror models may identify interpretable, and generalizable\nsemantic features that have unique utility in real-world psychological\nassessment.",
        "url": "http://arxiv.org/abs/2508.05830v1",
        "pdf_url": "http://arxiv.org/pdf/2508.05830v1",
        "arxiv_id": "2508.05830v1",
        "authors": [
            "Tong Li",
            "Rasiq Hussain",
            "Mehak Gupta",
            "Joshua R. Oltmanns"
        ],
        "submitted": "2025-08-07 20:13:00",
        "source": "arxiv",
        "comment": "39 pages, 9 figures",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on language AI models of depression, which is a topic outside your primary focus. The concepts and methods discussed in the paper, such as topic modeling and language-based prediction, are not directly applicable to your areas of interest."
    },
    {
        "title": "FineDialFact: A benchmark for Fine-grained Dialogue Fact Verification",
        "abstract": "Large Language Models (LLMs) are known to produce hallucinations - factually\nincorrect or fabricated information - which poses significant challenges for\nmany Natural Language Processing (NLP) applications, such as dialogue systems.\nAs a result, detecting hallucinations has become a critical area of research.\nCurrent approaches to hallucination detection in dialogue systems primarily\nfocus on verifying the factual consistency of generated responses. However,\nthese responses often contain a mix of accurate, inaccurate or unverifiable\nfacts, making one factual label overly simplistic and coarse-grained. In this\npaper, we introduce a benchmark, FineDialFact, for fine-grained dialogue fact\nverification, which involves verifying atomic facts extracted from dialogue\nresponses. To support this, we construct a dataset based on publicly available\ndialogue datasets and evaluate it using various baseline methods. Experimental\nresults demonstrate that methods incorporating Chain-of-Thought (CoT) reasoning\ncan enhance performance in dialogue fact verification. Despite this, the best\nF1-score achieved on the HybriDialogue, an open-domain dialogue dataset, is\nonly 0.75, indicating that the benchmark remains a challenging task for future\nresearch. Our dataset and code will be public on GitHub.",
        "url": "http://arxiv.org/abs/2508.05782v1",
        "pdf_url": "http://arxiv.org/pdf/2508.05782v1",
        "arxiv_id": "2508.05782v1",
        "authors": [
            "Xiangyan Chen",
            "Yufeng Li",
            "Yujian Gan",
            "Arkaitz Zubiaga",
            "Matthew Purver"
        ],
        "submitted": "2025-08-07 18:51:03",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on dialogue fact verification, which is related to information retrieval and natural language processing. However, the specific context of dialogue systems and fact verification is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Guardians and Offenders: A Survey on Harmful Content Generation and Safety Mitigation",
        "abstract": "Large Language Models (LLMs) have revolutionized content creation across\ndigital platforms, offering unprecedented capabilities in natural language\ngeneration and understanding. These models enable beneficial applications such\nas content generation, question and answering (Q&A), programming, and code\nreasoning. Meanwhile, they also pose serious risks by inadvertently or\nintentionally producing toxic, offensive, or biased content. This dual role of\nLLMs, both as powerful tools for solving real-world problems and as potential\nsources of harmful language, presents a pressing sociotechnical challenge. In\nthis survey, we systematically review recent studies spanning unintentional\ntoxicity, adversarial jailbreaking attacks, and content moderation techniques.\nWe propose a unified taxonomy of LLM-related harms and defenses, analyze\nemerging multimodal and LLM-assisted jailbreak strategies, and assess\nmitigation efforts, including reinforcement learning with human feedback\n(RLHF), prompt engineering, and safety alignment. Our synthesis highlights the\nevolving landscape of LLM safety, identifies limitations in current evaluation\nmethodologies, and outlines future research directions to guide the development\nof robust and ethically aligned language technologies.",
        "url": "http://arxiv.org/abs/2508.05775v1",
        "pdf_url": "http://arxiv.org/pdf/2508.05775v1",
        "arxiv_id": "2508.05775v1",
        "authors": [
            "Chi Zhang",
            "Changjia Zhu",
            "Junjie Xiong",
            "Xiaoran Xu",
            "Lingyao Li",
            "Yao Liu",
            "Zhuo Lu"
        ],
        "submitted": "2025-08-07 18:42:16",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses the dual role of Large Language Models (LLMs) in generating both beneficial and harmful content. While it touches on the topic of content creation and understanding, which is related to information retrieval and NLP, the focus is more on the safety and mitigation aspects of LLMs, which is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling."
    }
]