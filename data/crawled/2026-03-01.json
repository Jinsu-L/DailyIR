[
    {
        "title": "DARE-bench: Evaluating Modeling and Instruction Fidelity of LLMs in Data Science",
        "abstract": "The fast-growing demands in using Large Language Models (LLMs) to tackle complex multi-step data science tasks create an emergent need for accurate benchmarking. There are two major gaps in existing benchmarks: (i) the lack of standardized, process-aware evaluation that captures instruction adherence and process fidelity, and (ii) the scarcity of accurately labeled training data. To bridge these gaps, we introduce DARE-bench, a benchmark designed for machine learning modeling and data science instruction following. Unlike many existing benchmarks that rely on human- or model-based judges, all tasks in DARE-bench have verifiable ground truth, ensuring objective and reproducible evaluation. To cover a broad range of tasks and support agentic tools, DARE-bench consists of 6,300 Kaggle-derived tasks and provides both large-scale training data and evaluation sets. Extensive evaluations show that even highly capable models such as gpt-o4-mini struggle to achieve good performance, especially in machine learning modeling tasks. Using DARE-bench training tasks for fine-tuning can substantially improve model performance. For example, supervised fine-tuning boosts Qwen3-32B's accuracy by 1.83x and reinforcement learning boosts Qwen3-4B's accuracy by more than 8x. These significant improvements verify the importance of DARE-bench both as an accurate evaluation benchmark and critical training data.",
        "url": "http://arxiv.org/abs/2602.24288v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24288v1",
        "arxiv_id": "2602.24288v1",
        "authors": [
            "Fan Shu",
            "Yite Wang",
            "Ruofan Wu",
            "Boyi Liu",
            "Zhewei Yao",
            "Yuxiong He",
            "Feng Yan"
        ],
        "submitted": "2026-02-27 18:58:57",
        "source": "arxiv",
        "comment": "Published as a conference paper at ICLR 2026. 10 pages plus appendix"
    },
    {
        "title": "Do LLMs Benefit From Their Own Words?",
        "abstract": "Multi-turn interactions with large language models typically retain the assistant's own past responses in the conversation history. In this work, we revisit this design choice by asking whether large language models benefit from conditioning on their own prior responses. Using in-the-wild, multi-turn conversations, we compare standard (full-context) prompting with a user-turn-only prompting approach that omits all previous assistant responses, across three open reasoning models and one state-of-the-art model. To our surprise, we find that removing prior assistant responses does not affect response quality on a large fraction of turns. Omitting assistant-side history can reduce cumulative context lengths by up to 10x. To explain this result, we find that multi-turn conversations consist of a substantial proportion (36.4%) of self-contained prompts, and that many follow-up prompts provide sufficient instruction to be answered using only the current user turn and prior user turns. When analyzing cases where user-turn-only prompting substantially outperforms full context, we identify instances of context pollution, in which models over-condition on their previous responses, introducing errors, hallucinations, or stylistic artifacts that propagate across turns. Motivated by these findings, we design a context-filtering approach that selectively omits assistant-side context. Our findings suggest that selectively omitting assistant history can improve response quality while reducing memory consumption.",
        "url": "http://arxiv.org/abs/2602.24287v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24287v1",
        "arxiv_id": "2602.24287v1",
        "authors": [
            "Jenny Y. Huang",
            "Leshem Choshen",
            "Ramon Astudillo",
            "Tamara Broderick",
            "Jacob Andreas"
        ],
        "submitted": "2026-02-27 18:58:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Taming Momentum: Rethinking Optimizer States Through Low-Rank Approximation",
        "abstract": "Modern optimizers like Adam and Muon are central to training large language models, but their reliance on first- and second-order momenta introduces significant memory overhead, which constrains scalability and computational efficiency. In this work, we reframe the exponential moving average (EMA) used in these momenta as the training of a linear regressor via online gradient flow. Building on this equivalence, we introduce LoRA-Pre, a novel low-rank optimizer designed for efficient pre-training. Specifically, LoRA-Pre reduces the optimizer's memory footprint by decomposing the full momentum matrix into a compact low-rank subspace within the online linear learner, thereby maintaining optimization performance while improving memory efficiency. We empirically validate LoRA-Pre's efficacy by pre-training models from the Llama architecture family, scaling from 60M to 1B parameters. LoRA-Pre achieves the highest performance across all model sizes. Notably, LoRA-Pre demonstrates remarkable rank efficiency, achieving comparable or superior results using only 1/8 the rank of baseline methods. Beyond pre-training, we evaluate LoRA-Pre's effectiveness in fine-tuning scenarios. With the same rank, LoRA-Pre consistently outperforms all efficient fine-tuning baselines. Specifically, compared to standard LoRA, LoRA-Pre achieves substantial improvements of 3.14 points on Llama-3.1-8B and 6.17 points on Llama-2-7B, validating our approach's effectiveness across both pre-training and fine-tuning paradigms. Our code is publicly available at https://github.com/mrflogs/LoRA-Pre.",
        "url": "http://arxiv.org/abs/2602.24283v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24283v1",
        "arxiv_id": "2602.24283v1",
        "authors": [
            "Zhengbo Wang",
            "Jian Liang",
            "Ran He",
            "Zilei Wang",
            "Tieniu Tan"
        ],
        "submitted": "2026-02-27 18:57:06",
        "source": "arxiv",
        "comment": "Camera-ready version. Accepted as Oral at ICLR 2026"
    },
    {
        "title": "Resources for Automated Evaluation of Assistive RAG Systems that Help Readers with News Trustworthiness Assessment",
        "abstract": "Many readers today struggle to assess the trustworthiness of online news because reliable reporting coexists with misinformation. The TREC 2025 DRAGUN (Detection, Retrieval, and Augmented Generation for Understanding News) Track provided a venue for researchers to develop and evaluate assistive RAG systems that support readers' news trustworthiness assessment by producing reader-oriented, well-attributed reports. As the organizers of the DRAGUN track, we describe the resources that we have newly developed to allow for the reuse of the track's tasks. The track had two tasks: (Task 1) Question Generation, producing 10 ranked investigative questions; and (Task 2, the main task) Report Generation, producing a 250-word report grounded in the MS MARCO V2.1 Segmented Corpus. As part of the track's evaluation, we had TREC assessors create importance-weighted rubrics of questions with expected short answers for 30 different news articles. These rubrics represent the information that assessors believe is important for readers to assess an article's trustworthiness. The assessors then used their rubrics to manually judge the participating teams' submitted runs. To make these tasks and their rubrics reusable, we have created an automated process to judge runs not part of the original assessing. We show that our AutoJudge ranks existing runs well compared to the TREC human-assessed evaluation (Kendall's $τ= 0.678$ for Task 1 and $τ= 0.872$ for Task 2). These resources enable both the evaluation of RAG systems for assistive news trustworthiness assessment and, with the human evaluation as a benchmark, research on improving automated RAG evaluation.",
        "url": "http://arxiv.org/abs/2602.24277v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24277v1",
        "arxiv_id": "2602.24277v1",
        "authors": [
            "Dake Zhang",
            "Mark D. Smucker",
            "Charles L. A. Clarke"
        ],
        "submitted": "2026-02-27 18:49:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond the Click: A Framework for Inferring Cognitive Traces in Search",
        "abstract": "User simulators are essential for evaluating search systems, but they primarily copy user actions without understanding the underlying thought process. This gap exists since large-scale interaction logs record what users do, but not what they might be thinking or feeling, such as confusion or satisfaction. To solve this problem, we present a framework to infer cognitive traces from behavior logs. Our method uses a multi-agent system grounded in Information Foraging Theory (IFT) and human expert judgment. These traces improve model performance on tasks like forecasting session outcomes and user struggle recovery. We release a collection of annotations for several public datasets, including AOL and Stack Overflow, and an open-source tool that allows researchers to apply our method to their own data. This work provides the tools and data needed to build more human-like user simulators and to assess retrieval systems on user-oriented dimensions of performance.",
        "url": "http://arxiv.org/abs/2602.24265v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24265v1",
        "arxiv_id": "2602.24265v1",
        "authors": [
            "Saber Zerhoudi",
            "Michael Granitzer"
        ],
        "submitted": "2026-02-27 18:32:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "UXSim: Towards a Hybrid User Search Simulation",
        "abstract": "Simulating nuanced user experiences within complex interactive search systems poses distinct challenge for traditional methodologies, which often rely on static user proxies or, more recently, on standalone large language model (LLM) agents that may lack deep, verifiable grounding. The true dynamism and personalization inherent in human-computer interaction demand a more integrated approach. This work introduces UXSim, a novel framework that integrates both approaches. It leverages grounded data from traditional simulators to inform and constrain the reasoning of an adaptive LLM agent. This synthesis enables more accurate and dynamic simulations of user behavior while also providing a pathway for the explainable validation of the underlying cognitive processes.",
        "url": "http://arxiv.org/abs/2602.24241v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24241v1",
        "arxiv_id": "2602.24241v1",
        "authors": [
            "Saber Zerhoudi",
            "Michael Granitzer"
        ],
        "submitted": "2026-02-27 18:14:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Science Fiction and Fantasy in Wikipedia: Exploring Structural and Semantic Cues",
        "abstract": "Identifying which Wikipedia articles are related to science fiction, fantasy, or their hybrids is challenging because genre boundaries are porous and frequently overlap. Wikipedia nonetheless offers machine-readable structure beyond text, including categories, internal links (wikilinks), and statements if corresponding Wikidata items. However, each of these signals reflects community conventions and can be biased or incomplete. This study examines structural and semantic features of Wikipedia articles that can be used to identify content related to science fiction and fantasy (SF/F).",
        "url": "http://arxiv.org/abs/2602.24229v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24229v1",
        "arxiv_id": "2602.24229v1",
        "authors": [
            "Włodzimierz Lewoniewski",
            "Milena Stróżyna",
            "Izabela Czumałowska",
            "Elżbieta Lewańska"
        ],
        "submitted": "2026-02-27 17:56:25",
        "source": "arxiv",
        "comment": "Supplementary materials: https://data.lewoniewski.info/fantasy/"
    },
    {
        "title": "Controllable Reasoning Models Are Private Thinkers",
        "abstract": "AI agents powered by reasoning models require access to sensitive user data. However, their reasoning traces are difficult to control, which can result in the unintended leakage of private information to external parties. We propose training models to follow instructions not only in the final answer, but also in reasoning traces, potentially under different constraints. We hypothesize that improving their instruction following abilities in the reasoning traces can improve their privacy-preservation skills. To demonstrate this, we fine-tune models on a new instruction-following dataset with explicit restrictions on reasoning traces. We further introduce a generation strategy that decouples reasoning and answer generation using separate LoRA adapters. We evaluate our approach on six models from two model families, ranging from 1.7B to 14B parameters, across two instruction-following benchmarks and two privacy benchmarks. Our method yields substantial improvements, achieving gains of up to 20.9 points in instruction-following performance and up to 51.9 percentage points on privacy benchmarks. These improvements, however, can come at the cost of task utility, due to the trade-off between reasoning performance and instruction-following abilities. Overall, our results show that improving instruction-following behavior in reasoning models can significantly enhance privacy, suggesting a promising direction for the development of future privacy-aware agents. Our code and data are available at https://github.com/UKPLab/arxiv2026-controllable-reasoning-models",
        "url": "http://arxiv.org/abs/2602.24210v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24210v1",
        "arxiv_id": "2602.24210v1",
        "authors": [
            "Haritz Puerto",
            "Haonan Li",
            "Xudong Han",
            "Timothy Baldwin",
            "Iryna Gurevych"
        ],
        "submitted": "2026-02-27 17:39:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Uncertainty Quantification for Multimodal Large Language Models with Incoherence-adjusted Semantic Volume",
        "abstract": "Despite their capabilities, Multimodal Large Language Models (MLLMs) may produce plausible but erroneous outputs, hindering reliable deployment. Accurate uncertainty metrics could enable escalation of unreliable queries to human experts or larger models for improved performance. However, existing uncertainty metrics have practical constraints, such as being designed only for specific modalities, reliant on external tools, or computationally expensive. We introduce UMPIRE, a training-free uncertainty quantification framework for MLLMs that works efficiently across various input and output modalities without external tools, relying only on the models' own internal modality features. UMPIRE computes the incoherence-adjusted semantic volume of sampled MLLM responses for a given task instance, effectively capturing both the global semantic diversity of samples and the local incoherence of responses based on internal model confidence. We propose uncertainty desiderata for MLLMs and provide theoretical analysis motivating UMPIRE's design. Extensive experiments show that UMPIRE consistently outperforms baseline metrics in error detection and uncertainty calibration across image, audio, and video-text benchmarks, including adversarial and out-of-distribution settings. We also demonstrate UMPIRE's generalization to non-text output tasks, including image and audio generation.",
        "url": "http://arxiv.org/abs/2602.24195v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24195v1",
        "arxiv_id": "2602.24195v1",
        "authors": [
            "Gregory Kang Ruey Lau",
            "Hieu Dao",
            "Nicole Kan Hui Lin",
            "Bryan Kian Hsiang Low"
        ],
        "submitted": "2026-02-27 17:18:42",
        "source": "arxiv",
        "comment": "Earlier versions presented at ICLR 2025 QUESTION workshop and ICML 2025 R2-FM workshop"
    },
    {
        "title": "MT-PingEval: Evaluating Multi-Turn Collaboration with Private Information Games",
        "abstract": "We present a scalable methodology for evaluating language models in multi-turn interactions, using a suite of collaborative games that require effective communication about private information. This enables an interactive scaling analysis, in which a fixed token budget is divided over a variable number of turns. We find that in many cases, language models are unable to use interactive collaboration to improve over the non-interactive baseline scenario in which one agent attempts to summarize its information and the other agent immediately acts -- despite substantial headroom. This suggests that state-of-the-art models still suffer from significant weaknesses in planning and executing multi-turn collaborative conversations. We analyze the linguistic features of these dialogues, assessing the roles of sycophancy, information density, and discourse coherence. While there is no single linguistic explanation for the collaborative weaknesses of contemporary language models, we note that humans achieve comparable task success at superior token efficiency by producing dialogues that are more coherent than those produced by most language models. The proactive management of private information is a defining feature of real-world communication, and we hope that MT-PingEval will drive further work towards improving this capability.",
        "url": "http://arxiv.org/abs/2602.24188v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24188v1",
        "arxiv_id": "2602.24188v1",
        "authors": [
            "Jacob Eisenstein",
            "Fantine Huot",
            "Adam Fisch",
            "Jonathan Berant",
            "Mirella Lapata"
        ],
        "submitted": "2026-02-27 17:13:20",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Task-Centric Acceleration of Small-Language Models",
        "abstract": "Small language models (SLMs) have emerged as efficient alternatives to large language models for task-specific applications. However, they are often employed in high-volume, low-latency settings, where efficiency is crucial. We propose TASC, Task-Adaptive Sequence Compression, a framework for SLM acceleration comprising two use-cases: When performing SLM fine-tuning, we propose TASC-ft, which iteratively enriches the tokenizer vocabulary with high-frequency output n-grams and then fine-tunes the model to utilize the expanded vocabulary. Next, we propose an inference-time method, termed TASC-spec. TASC-spec is a lightweight, training-free speculative decoding method that constructs an n-gram draft model from the task's output corpus, mixing task and context n-gram information.TASC-spec avoids any additional training, while bypassing draft-target vocabulary alignment constraints. We demonstrate the effectiveness of both methods across multiple low output-variability generation tasks. Our methods show consistent improvements in inference efficiency while maintaining task performance.",
        "url": "http://arxiv.org/abs/2602.24174v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24174v1",
        "arxiv_id": "2602.24174v1",
        "authors": [
            "Dor Tsur",
            "Sharon Adar",
            "Ran Levy"
        ],
        "submitted": "2026-02-27 16:55:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ArgLLM-App: An Interactive System for Argumentative Reasoning with Large Language Models",
        "abstract": "Argumentative LLMs (ArgLLMs) are an existing approach leveraging Large Language Models (LLMs) and computational argumentation for decision-making, with the aim of making the resulting decisions faithfully explainable to and contestable by humans. Here we propose a web-based system implementing ArgLLM-empowered agents for binary tasks. ArgLLM-App supports visualisation of the produced explanations and interaction with human users, allowing them to identify and contest any mistakes in the system's reasoning. It is highly modular and enables drawing information from trusted external sources. ArgLLM-App is publicly available at https://argllm.app, with a video demonstration at https://youtu.be/vzwlGOr0sPM.",
        "url": "http://arxiv.org/abs/2602.24172v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24172v1",
        "arxiv_id": "2602.24172v1",
        "authors": [
            "Adam Dejl",
            "Deniz Gorur",
            "Francesca Toni"
        ],
        "submitted": "2026-02-27 16:52:27",
        "source": "arxiv",
        "comment": "AAMAS 2026 Demonstration Track"
    },
    {
        "title": "CoME: Empowering Channel-of-Mobile-Experts with Informative Hybrid-Capabilities Reasoning",
        "abstract": "Mobile Agents can autonomously execute user instructions, which requires hybrid-capabilities reasoning, including screen summary, subtask planning, action decision and action function. However, existing agents struggle to achieve both decoupled enhancement and balanced integration of these capabilities. To address these challenges, we propose Channel-of-Mobile-Experts (CoME), a novel agent architecture consisting of four distinct experts, each aligned with a specific reasoning stage, CoME activates the corresponding expert to generate output tokens in each reasoning stage via output-oriented activation. To empower CoME with hybrid-capabilities reasoning, we introduce a progressive training strategy: Expert-FT enables decoupling and enhancement of different experts' capability; Router-FT aligns expert activation with the different reasoning stage; CoT-FT facilitates seamless collaboration and balanced optimization across multiple capabilities. To mitigate error propagation in hybrid-capabilities reasoning, we propose InfoGain-Driven DPO (Info-DPO), which uses information gain to evaluate the contribution of each intermediate step, thereby guiding CoME toward more informative reasoning. Comprehensive experiments show that CoME outperforms dense mobile agents and MoE methods on both AITZ and AMEX datasets.",
        "url": "http://arxiv.org/abs/2602.24142v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24142v1",
        "arxiv_id": "2602.24142v1",
        "authors": [
            "Yuxuan Liu",
            "Weikai Xu",
            "Kun Huang",
            "Changyu Chen",
            "Jiankun Zhao",
            "Pengzhi Gao",
            "Wei Liu",
            "Jian Luan",
            "Shuo Shang",
            "Bo Du",
            "Ji-Rong Wen",
            "Rui Yan"
        ],
        "submitted": "2026-02-27 16:19:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AgenticOCR: Parsing Only What You Need for Efficient Retrieval-Augmented Generation",
        "abstract": "The expansion of retrieval-augmented generation (RAG) into multimodal domains has intensified the challenge for processing complex visual documents, such as financial reports. While page-level chunking and retrieval is a natural starting point, it creates a critical bottleneck: delivering entire pages to the generator introduces excessive extraneous context. This not only overloads the generator's attention mechanism but also dilutes the most salient evidence. Moreover, compressing these information-rich pages into a limited visual token budget further increases the risk of hallucinations. To address this, we introduce AgenticOCR, a dynamic parsing paradigm that transforms optical character recognition (OCR) from a static, full-text process into a query-driven, on-demand extraction system. By autonomously analyzing document layout in a \"thinking with images\" manner, AgenticOCR identifies and selectively recognizes regions of interest. This approach performs on-demand decompression of visual tokens precisely where needed, effectively decoupling retrieval granularity from rigid page-level chunking. AgenticOCR has the potential to serve as the \"third building block\" of the visual document RAG stack, operating alongside and enhancing standard Embedding and Reranking modules. Experimental results demonstrate that AgenticOCR improves both the efficiency and accuracy of visual RAG systems, achieving expert-level performance in long document understanding. Code and models are available at https://github.com/OpenDataLab/AgenticOCR.",
        "url": "http://arxiv.org/abs/2602.24134v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24134v1",
        "arxiv_id": "2602.24134v1",
        "authors": [
            "Zhengren Wang",
            "Dongsheng Ma",
            "Huaping Zhong",
            "Jiayu Li",
            "Wentao Zhang",
            "Bin Wang",
            "Conghui He"
        ],
        "submitted": "2026-02-27 16:09:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Recommendation Algorithms: A Comparative Study in Movie Domain",
        "abstract": "Intelligent recommendation systems have clearly increased the revenue of well-known e-commerce firms. Users receive product recommendations from recommendation systems. Cinematic recommendations are made to users by a movie recommendation system. There have been numerous approaches to the problem of recommendation in the literature. It is viewed as a regression task in this research. A regression model was built using novel properties extracted from the dataset and used as features in the model. For experimentation, the Netflix challenge dataset has been used. Video streaming service Netflix is a popular choice for many. Customers' prior viewing habits are taken into account when Netflix makes movie recommendations to them. An exploratory data analysis on the Netflix dataset was conducted to gain insights into user rating behaviour and movie characteristics. Various kinds of features, including aggregating, Matrix Factorization (MF) based, and user and movie similarity based, have been extracted in the subsequent stages. In addition to a feature in the XGBoost regression algorithm, the K-Nearest Neighbors and MF algorithms from Python's Surprise library are used for recommendations. Based on Root Mean Square Error (RMSE), MF-based algorithms have provided the best recommendations.",
        "url": "http://arxiv.org/abs/2602.24125v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24125v1",
        "arxiv_id": "2602.24125v1",
        "authors": [
            "Rohit Chivukula",
            "T. Jaya Lakshmi",
            "Hemlata Sharma",
            "C. H. S. N. P. Sairam Rallabandi"
        ],
        "submitted": "2026-02-27 16:01:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Terminology Rarity Predicts Catastrophic Failure in LLM Translation of Low-Resource Ancient Languages: Evidence from Ancient Greek",
        "abstract": "This study presents the first systematic, reference-free human evaluation of large language model (LLM) machine translation (MT) for Ancient Greek (AG) technical prose. We evaluate translations by three commercial LLMs (Claude, Gemini, ChatGPT) of twenty paragraph-length passages from two works by the Greek physician Galen of Pergamum (ca. 129-216 CE): On Mixtures, which has two published English translations, and On the Composition of Drugs according to Kinds, which has never been fully translated into English. We assess translation quality using both standard automated evaluation metrics (BLEU, chrF++, METEOR, ROUGE-L, BERTScore, COMET, BLEURT) and expert human evaluation via a modified Multidimensional Quality Metrics (MQM) framework applied to all 60 translations by a team of domain specialists. On the previously translated expository text, LLMs achieved high translation quality (mean MQM score 95.2/100), with performance approaching expert level. On the untranslated pharmacological text, aggregate quality was lower (79.9/100) but with high variance driven by two passages presenting extreme terminological density; excluding these, scores converged to within 4 points of the translated text. Terminology rarity, operationalized via corpus frequency in the literary Diorisis Ancient Greek Corpus, emerged as a strong predictor of translation failure (r = -.97 for passage-level quality on the untranslated text). Automated metrics showed moderate correlation with human judgment overall on the text with a wide quality spread (Composition), but no metric discriminated among high-quality translations. We discuss implications for the use of LLMs in Classical scholarship and for the design of automated evaluation pipelines for low-resource ancient languages.",
        "url": "http://arxiv.org/abs/2602.24119v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24119v1",
        "arxiv_id": "2602.24119v1",
        "authors": [
            "James L. Zainaldin",
            "Cameron Pattison",
            "Manuela Marai",
            "Jacob Wu",
            "Mark J. Schiefsky"
        ],
        "submitted": "2026-02-27 15:57:15",
        "source": "arxiv",
        "comment": "Article + supplementary information"
    },
    {
        "title": "Toward Guarantees for Clinical Reasoning in Vision Language Models via Formal Verification",
        "abstract": "Vision-language models (VLMs) show promise in drafting radiology reports, yet they frequently suffer from logical inconsistencies, generating diagnostic impressions unsupported by their own perceptual findings or missing logically entailed conclusions. Standard lexical metrics heavily penalize clinical paraphrasing and fail to capture these deductive failures in reference-free settings. Toward guarantees for clinical reasoning, we introduce a neurosymbolic verification framework that deterministically audits the internal consistency of VLM-generated reports. Our pipeline autoformalizes free-text radiographic findings into structured propositional evidence, utilizing an SMT solver (Z3) and a clinical knowledge base to verify whether each diagnostic claim is mathematically entailed, hallucinated, or omitted. Evaluating seven VLMs across five chest X-ray benchmarks, our verifier exposes distinct reasoning failure modes, such as conservative observation and stochastic hallucination, that remain invisible to traditional metrics. On labeled datasets, enforcing solver-backed entailment acts as a rigorous post-hoc guarantee, systematically eliminating unsupported hallucinations to significantly increase diagnostic soundness and precision in generative clinical assistants.",
        "url": "http://arxiv.org/abs/2602.24111v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24111v1",
        "arxiv_id": "2602.24111v1",
        "authors": [
            "Vikash Singh",
            "Debargha Ganguly",
            "Haotian Yu",
            "Chengwei Zhou",
            "Prerna Singh",
            "Brandon Lee",
            "Vipin Chaudhary",
            "Gourav Datta"
        ],
        "submitted": "2026-02-27 15:49:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Recycling Failures: Salvaging Exploration in RLVR via Fine-Grained Off-Policy Guidance",
        "abstract": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a powerful paradigm for enhancing the complex reasoning capabilities of Large Reasoning Models. However, standard outcome-based supervision suffers from a critical limitation that penalizes trajectories that are largely correct but fail due to several missteps as heavily as completely erroneous ones. This coarse feedback signal causes the model to discard valuable largely correct rollouts, leading to a degradation in rollout diversity that prematurely narrows the exploration space. Process Reward Models have demonstrated efficacy in providing reliable step-wise verification for test-time scaling, naively integrating these signals into RLVR as dense rewards proves ineffective.Prior methods attempt to introduce off-policy guided whole-trajectory replacement that often outside the policy model's distribution, but still fail to utilize the largely correct rollouts generated by the model itself and thus do not effectively mitigate the narrowing of the exploration space. To address these issues, we propose SCOPE (Step-wise Correction for On-Policy Exploration), a novel framework that utilizes Process Reward Models to pinpoint the first erroneous step in suboptimal rollouts and applies fine-grained, step-wise off-policy rectification. By applying precise refinement on partially correct rollout, our method effectively salvages partially correct trajectories and increases diversity score by 13.5%, thereby sustaining a broad exploration space. Extensive experiments demonstrate that our approach establishes new state-of-the-art results, achieving an average accuracy of 46.6% on math reasoning and exhibiting robust generalization with 53.4% accuracy on out-of-distribution reasoning tasks.",
        "url": "http://arxiv.org/abs/2602.24110v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24110v1",
        "arxiv_id": "2602.24110v1",
        "authors": [
            "Yanwei Ren",
            "Haotian Zhang",
            "Likang Xiao",
            "Xikai Zhang",
            "Jiaxing Huang",
            "Jiayan Qiu",
            "Baosheng Yu",
            "Quan Chen",
            "Liu Liu"
        ],
        "submitted": "2026-02-27 15:49:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ARGUS: Seeing the Influence of Narrative Features on Persuasion in Argumentative Texts",
        "abstract": "Can narratives make arguments more persuasive? And to this end, which narrative features matter most? Although stories are often seen as powerful tools for persuasion, their specific role in online, unstructured argumentation remains underexplored. To address this gap, we present ARGUS, a framework for studying the impact of narration on persuasion in argumentative discourse. ARGUS introduces a new ChangeMyView corpus annotated for story presence and six key narrative features, integrating insights from two established theoretical frameworks that capture both textual narrative features and their effects on recipients. Leveraging both encoder-based classifiers and zero-shot large language models (LLMs), ARGUS identifies stories and narrative features and applies them at scale to examine how different narrative dimensions influence persuasion success in online argumentation.",
        "url": "http://arxiv.org/abs/2602.24109v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24109v1",
        "arxiv_id": "2602.24109v1",
        "authors": [
            "Sara Nabhani",
            "Federico Pianzola",
            "Khalid Al-Khatib",
            "Malvina Nissim"
        ],
        "submitted": "2026-02-27 15:47:57",
        "source": "arxiv",
        "comment": "22 pages, 8 figures, submitted to ACM Transactions on Intelligent Systems and Technology"
    },
    {
        "title": "Preference Packing: Efficient Preference Optimization for Large Language Models",
        "abstract": "Resource-efficient training optimization techniques are becoming increasingly important as the size of large language models (LLMs) continues to grow. In particular, batch packing is commonly used in pre-training and supervised fine-tuning to achieve resource-efficient training. We propose preference packing, a method to enhance resource efficiency in training techniques that use data with different responses for the same input prompt, such as reward models or Direct Preference Optimization (DPO). Preference packing improves resource efficiency by reducing the attention operations for duplicate input prompts and decreasing KV cache memory usage. We conducted experiments on text-only datasets and image-included datasets and achieved at least 37% reduction in training time. Notably, this method can be applied alongside existing optimization techniques such as batch sorting, resulting in a 3.22x speedup.",
        "url": "http://arxiv.org/abs/2602.24082v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24082v1",
        "arxiv_id": "2602.24082v1",
        "authors": [
            "Jaekyung Cho"
        ],
        "submitted": "2026-02-27 15:19:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SongSong: A Time Phonograph for Chinese SongCi Music from Thousand of Years Away",
        "abstract": "Recently, there have been significant advancements in music generation. However, existing models primarily focus on creating modern pop songs, making it challenging to produce ancient music with distinct rhythms and styles, such as ancient Chinese SongCi. In this paper, we introduce SongSong, the first music generation model capable of restoring Chinese SongCi to our knowledge. Our model first predicts the melody from the input SongCi, then separately generates the singing voice and accompaniment based on that melody, and finally combines all elements to create the final piece of music. Additionally, to address the lack of ancient music datasets, we create OpenSongSong, a comprehensive dataset of ancient Chinese SongCi music, featuring 29.9 hours of compositions by various renowned SongCi music masters. To assess SongSong's proficiency in performing SongCi, we randomly select 85 SongCi sentences that were not part of the training set for evaluation against SongSong and music generation platforms such as Suno and SkyMusic. The subjective and objective outcomes indicate that our proposed model achieves leading performance in generating high-quality SongCi music.",
        "url": "http://arxiv.org/abs/2602.24071v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24071v1",
        "arxiv_id": "2602.24071v1",
        "authors": [
            "Jiajia Li",
            "Jiliang Hu",
            "Ziyi Pan",
            "Chong Chen",
            "Zuchao Li",
            "Ping Wang",
            "Lefei Zhang"
        ],
        "submitted": "2026-02-27 15:02:07",
        "source": "arxiv",
        "comment": "9 pages, 6 figures, accepted by AAAI 2025"
    },
    {
        "title": "A Novel Hierarchical Multi-Agent System for Payments Using LLMs",
        "abstract": "Large language model (LLM) agents, such as OpenAI's Operator and Claude's Computer Use, can automate workflows but unable to handle payment tasks. Existing agentic solutions have gained significant attention; however, even the latest approaches face challenges in implementing end-to-end agentic payment workflows. To address this gap, this research proposes the Hierarchical Multi-Agent System for Payments (HMASP), which provides an end-to-end agentic method for completing payment workflows. The proposed HMASP leverages either open-weight or proprietary LLMs and employs a modular architecture consisting of the Conversational Payment Agent (CPA - first agent level), Supervisor agents (second agent level), Routing agents (third agent level), and the Process summary agent (fourth agent level). The CPA serves as the central entry point, handling all external requests and coordinating subsequent tasks across hierarchical levels. HMASP incorporates architectural patterns that enable modular task execution across agents and levels for payment operations, including shared state variables, decoupled message states, and structured handoff protocols that facilitate coordination across agents and workflows. Experimental results demonstrate the feasibility of the proposed HMASP. To our knowledge, HMASP is the first LLM-based multi-agent system to implement end-to-end agentic payment workflows. This work lays a foundation for extending agentic capabilities into the payment domain.",
        "url": "http://arxiv.org/abs/2602.24068v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24068v1",
        "arxiv_id": "2602.24068v1",
        "authors": [
            "Joon Kiat Chua",
            "Donghao Huang",
            "Zhaoxia Wang"
        ],
        "submitted": "2026-02-27 14:58:13",
        "source": "arxiv",
        "comment": "12 pages, 1 figure, 3 tables. Accepted at PAKDD 2026"
    },
    {
        "title": "Colour Contrast on the Web: A WCAG 2.1 Level AA Compliance Audit of Common Crawl's Top 500 Domains",
        "abstract": "We present a large-scale automated audit of WCAG 2.1/2.2 Level AA colour contrast compliance across the 500 most frequently crawled registered domains in Common Crawl's CC-MAIN-2026-08 February 2026 crawl archive. Rather than conducting a live crawl, all page content was sourced from Common Crawl's open WARC archives, ensuring reproducibility and eliminating any load on target web servers. Our static CSS analysis of 240 homepages identified 4,327 unique foreground/background colour pairings, of which 1,771 (40.9%) failed to meet the 4.5:1 contrast ratio threshold for normal text. The median per-site pass rate was 62.7%, with 20.4% of sites achieving full compliance across all detected colour pairings. These findings suggest that colour contrast remains a widespread accessibility barrier on the most prominent websites, with significant variation across domain categories.",
        "url": "http://arxiv.org/abs/2602.24067v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24067v1",
        "arxiv_id": "2602.24067v1",
        "authors": [
            "Thom Vaughan",
            "Pedro Ortiz Suarez"
        ],
        "submitted": "2026-02-27 14:57:54",
        "source": "arxiv",
        "comment": "8 pages, 4 tables. Companion website and reproducible analysis code available at https://thunderpoot.github.io/wcag-audit/ and https://github.com/thunderpoot/wcag-audit"
    },
    {
        "title": "Task Complexity Matters: An Empirical Study of Reasoning in LLMs for Sentiment Analysis",
        "abstract": "Large language models (LLMs) with reasoning capabilities have fueled a compelling narrative that reasoning universally improves performance across language tasks. We test this claim through a comprehensive evaluation of 504 configurations across seven model families--including adaptive, conditional, and reinforcement learning-based reasoning architectures--on sentiment analysis datasets of varying granularity (binary, five-class, and 27-class emotion). Our findings reveal that reasoning effectiveness is strongly task-dependent, challenging prevailing assumptions: (1) Reasoning shows task-complexity dependence--binary classification degrades up to -19.9 F1 percentage points (pp), while 27-class emotion recognition gains up to +16.0pp; (2) Distilled reasoning variants underperform base models by 3-18 pp on simpler tasks, though few-shot prompting enables partial recovery; (3) Few-shot learning improves over zero-shot in most cases regardless of model type, with gains varying by architecture and task complexity; (4) Pareto frontier analysis shows base models dominate efficiency-performance trade-offs, with reasoning justified only for complex emotion recognition despite 2.1x-54x computational overhead. We complement these quantitative findings with qualitative error analysis revealing that reasoning degrades simpler tasks through systematic over-deliberation, offering mechanistic insight beyond the high-level overthinking hypothesis.",
        "url": "http://arxiv.org/abs/2602.24060v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24060v1",
        "arxiv_id": "2602.24060v1",
        "authors": [
            "Donghao Huang",
            "Zhaoxia Wang"
        ],
        "submitted": "2026-02-27 14:49:05",
        "source": "arxiv",
        "comment": "12 pages, 1 figure, 3 tables. Accepted at PAKDD 2026"
    },
    {
        "title": "Data Driven Optimization of GPU efficiency for Distributed LLM Adapter Serving",
        "abstract": "Large Language Model (LLM) adapters enable low-cost model specialization, but introduce complex caching and scheduling challenges in distributed serving systems where hundreds of adapters must be hosted concurrently. While prior work has largely focused on latency minimization, resource efficiency through throughput maximization remains underexplored. This paper presents a data-driven pipeline that, for a given workload, computes an adapter placement that serves the workload with the minimum number of GPUs while avoiding request starvation and GPU memory errors. To that end, the approach identifies the maximum feasible throughput attainable on each GPU by leveraging accurate performance predictions learned from real serving behavior. The proposed pipeline integrates three components: (i) a Digital Twin (DT) tailored to LLM-adapter serving, (ii) a distilled machine learning (ML) model trained on DT-generated data, and (iii) a greedy placement algorithm that exploits ML-based performance estimates to maximize GPU efficiency. The DT emulates real system dynamics with high fidelity, achieving below 5% throughput estimation error while executing up to 90 times faster than full LLM benchmarking across both predictable and unpredictable workloads. The learned ML models further accelerate performance estimation with marginal accuracy degradation, enabling scalable optimization. Experimental results demonstrate that the pipeline substantially improves GPU efficiency by reducing the number of GPUs required to sustain target workloads. Beyond GPU efficiency, the pipeline can be adapted to alternative objectives, such as latency minimization, highlighting its versatility for future large-scale LLM serving infrastructures.",
        "url": "http://arxiv.org/abs/2602.24044v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24044v1",
        "arxiv_id": "2602.24044v1",
        "authors": [
            "Ferran Agullo",
            "Joan Oliveras",
            "Chen Wang",
            "Alberto Gutierrez-Torre",
            "Olivier Tardieu",
            "Alaa Youssef",
            "Jordi Torres",
            "Josep Ll. Berral"
        ],
        "submitted": "2026-02-27 14:22:51",
        "source": "arxiv",
        "comment": "journal extension of the workshop paper titled as \"A data-driven ml approach for maximizing performance in llm-adapter serving\""
    },
    {
        "title": "RewardUQ: A Unified Framework for Uncertainty-Aware Reward Models",
        "abstract": "Reward models are central to aligning large language models (LLMs) with human preferences. Yet most approaches rely on pointwise reward estimates that overlook the epistemic uncertainty in reward models arising from limited human feedback. Recent work suggests that quantifying this uncertainty can reduce the costs of human annotation via uncertainty-guided active learning and mitigate reward overoptimization in LLM post-training. However, uncertainty-aware reward models have so far been adopted without thorough comparison, leaving them poorly understood. This work introduces a unified framework, RewardUQ, to systematically evaluate uncertainty quantification for reward models. We compare common methods along standard metrics measuring accuracy and calibration, and we propose a new ranking strategy incorporating both dimensions for a simplified comparison. Our experimental results suggest that model size and initialization have the most meaningful impact on performance, and most prior work could have benefited from alternative design choices. To foster the development and evaluation of new methods and aid the deployment in downstream applications, we release our open-source framework as a Python package. Our code is available at https://github.com/lasgroup/rewarduq.",
        "url": "http://arxiv.org/abs/2602.24040v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24040v1",
        "arxiv_id": "2602.24040v1",
        "authors": [
            "Daniel Yang",
            "Samuel Stante",
            "Florian Redhardt",
            "Lena Libon",
            "Parnian Kassraie",
            "Ido Hakimi",
            "Barna Pásztor",
            "Andreas Krause"
        ],
        "submitted": "2026-02-27 14:15:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Jailbreak Foundry: From Papers to Runnable Attacks for Reproducible Benchmarking",
        "abstract": "Jailbreak techniques for large language models (LLMs) evolve faster than benchmarks, making robustness estimates stale and difficult to compare across papers due to drift in datasets, harnesses, and judging protocols. We introduce JAILBREAK FOUNDRY (JBF), a system that addresses this gap via a multi-agent workflow to translate jailbreak papers into executable modules for immediate evaluation within a unified harness. JBF features three core components: (i) JBF-LIB for shared contracts and reusable utilities; (ii) JBF-FORGE for the multi-agent paper-to-module translation; and (iii) JBF-EVAL for standardizing evaluations. Across 30 reproduced attacks, JBF achieves high fidelity with a mean (reproduced-reported) attack success rate (ASR) deviation of +0.26 percentage points. By leveraging shared infrastructure, JBF reduces attack-specific implementation code by nearly half relative to original repositories and achieves an 82.5% mean reused-code ratio. This system enables a standardized AdvBench evaluation of all 30 attacks across 10 victim models using a consistent GPT-4o judge. By automating both attack integration and standardized evaluation, JBF offers a scalable solution for creating living benchmarks that keep pace with the rapidly shifting security landscape.",
        "url": "http://arxiv.org/abs/2602.24009v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24009v1",
        "arxiv_id": "2602.24009v1",
        "authors": [
            "Zhicheng Fang",
            "Jingjie Zheng",
            "Chenxu Fu",
            "Wei Xu"
        ],
        "submitted": "2026-02-27 13:32:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Dialect and Gender Bias in YouTube's Spanish Captioning System",
        "abstract": "Spanish is the official language of twenty-one countries and is spoken by over 441 million people. Naturally, there are many variations in how Spanish is spoken across these countries. Media platforms such as YouTube rely on automatic speech recognition systems to make their content accessible to different groups of users. However, YouTube offers only one option for automatically generating captions in Spanish. This raises the question: could this captioning system be biased against certain Spanish dialects? This study examines the potential biases in YouTube's automatic captioning system by analyzing its performance across various Spanish dialects. By comparing the quality of captions for female and male speakers from different regions, we identify systematic disparities which can be attributed to specific dialects. Our study provides further evidence that algorithmic technologies deployed on digital platforms need to be calibrated to the diverse needs and experiences of their user populations.",
        "url": "http://arxiv.org/abs/2602.24002v1",
        "pdf_url": "https://arxiv.org/pdf/2602.24002v1",
        "arxiv_id": "2602.24002v1",
        "authors": [
            "Iris Dania Jimenez",
            "Christoph Kern"
        ],
        "submitted": "2026-02-27 13:26:42",
        "source": "arxiv",
        "comment": "21 pages, 4 tables"
    },
    {
        "title": "GPU-Native Approximate Nearest Neighbor Search with IVF-RaBitQ: Fast Index Build and Search",
        "abstract": "Approximate nearest neighbor search (ANNS) on GPUs is gaining increasing popularity for modern retrieval and recommendation workloads that operate over massive high-dimensional vectors. Graph-based indexes deliver high recall and throughput but incur heavy build-time and storage costs. In contrast, cluster-based methods build and scale efficiently yet often need many probes for high recall, straining memory bandwidth and compute. Aiming to simultaneously achieve fast index build, high-throughput search, high recall, and low storage requirement for GPUs, we present IVF-RaBitQ (GPU), a GPU-native ANNS solution that integrates the cluster-based method IVF with RaBitQ quantization into an efficient GPU index build/search pipeline. Specifically, for index build, we develop a scalable GPU-native RaBitQ quantization method that enables fast and accurate low-bit encoding at scale. For search, we develop GPU-native distance computation schemes for RaBitQ codes and a fused search kernel to achieve high throughput with high recall. With IVF-RaBitQ implemented and integrated into the NVIDIA cuVS Library, experiments on cuVS Bench across multiple datasets show that IVF-RaBitQ offers a strong performance frontier in recall, throughput, index build time, and storage footprint. For Recall approximately equal to 0.95, IVF-RaBitQ achieves 2.2x higher QPS than the state-of-the-art graph-based method CAGRA, while also constructing indices 7.7x faster on average. Compared to the cluster-based method IVF-PQ, IVF-RaBitQ delivers on average over 2.7x higher throughput while avoiding accessing the raw vectors for reranking.",
        "url": "http://arxiv.org/abs/2602.23999v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23999v1",
        "arxiv_id": "2602.23999v1",
        "authors": [
            "Jifan Shi",
            "Jianyang Gao",
            "James Xia",
            "Tamás Béla Fehér",
            "Cheng Long"
        ],
        "submitted": "2026-02-27 13:23:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The GRADIEND Python Package: An End-to-End System for Gradient-Based Feature Learning",
        "abstract": "We present gradiend, an open-source Python package that operationalizes the GRADIEND method for learning feature directions from factual-counterfactual MLM and CLM gradients in language models. The package provides a unified workflow for feature-related data creation, training, evaluation, visualization, persistent model rewriting via controlled weight updates, and multi-feature comparison. We demonstrate GRADIEND on an English pronoun paradigm and on a large-scale feature comparison that reproduces prior use cases.",
        "url": "http://arxiv.org/abs/2602.23993v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23993v1",
        "arxiv_id": "2602.23993v1",
        "authors": [
            "Jonathan Drechsel",
            "Steffen Herbold"
        ],
        "submitted": "2026-02-27 13:14:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Robust Aggregation for Federated Sequential Recommendation with Sparse and Poisoned Data",
        "abstract": "Federated sequential recommendation distributes model training across user devices so that behavioural data remains local, reducing privacy risks. Yet, this setting introduces two intertwined difficulties. On the one hand, individual clients typically contribute only short and highly sparse interaction sequences, limiting the reliability of learned user representations. On the other hand, the federated optimisation process is vulnerable to malicious or corrupted client updates, where poisoned gradients can significantly distort the global model. These challenges are particularly severe in sequential recommendation, where temporal dynamics further complicate signal aggregation. To address this problem, we propose a robust aggregation framework tailored for federated sequential recommendation under sparse and adversarial conditions. Instead of relying on standard averaging, our method introduces a defence-aware aggregation mechanism that identifies and down-weights unreliable client updates while preserving informative signals from sparse but benign participants. The framework incorporates representation-level constraints to stabilise user and item embeddings, preventing poisoned or anomalous contributions from dominating the global parameter space. In addition, we integrate sequence-aware regularisation to maintain temporal coherence in user modelling despite limited local observations.",
        "url": "http://arxiv.org/abs/2602.23982v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23982v1",
        "arxiv_id": "2602.23982v1",
        "authors": [
            "Minh Hieu Nguyen"
        ],
        "submitted": "2026-02-27 12:50:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Towards Efficient and Generalizable Retrieval: Adaptive Semantic Quantization and Residual Knowledge Transfer",
        "abstract": "While semantic ID-based generative retrieval enables efficient end-to-end modeling in industrial applications, these methods face a persistent trade-off: head items are susceptible to ID collisions that negatively impact downstream tasks, whereas data-sparse tail items, including cold-start items, exhibit limited generalization. To address this issue, we propose the Anchored Curriculum with Sequential Adaptive Quantization (SA^2CRQ) framework. The framework introduces Sequential Adaptive Residual Quantization (SARQ) to dynamically allocate code lengths based on item path entropy, assigning longer, discriminative IDs to head items and shorter, generalizable IDs to tail items. To mitigate data sparsity, the Anchored Curriculum Residual Quantization (ACRQ) component utilizes a frozen semantic manifold learned from head items to regularize and accelerate the representation learning of tail items. Experimental results from a large-scale industrial search system and multiple public datasets indicate that SA^2CRQ yields consistent improvements over existing baselines, particularly in cold-start retrieval scenarios.",
        "url": "http://arxiv.org/abs/2602.23978v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23978v1",
        "arxiv_id": "2602.23978v1",
        "authors": [
            "Huimu Wang",
            "Xingzhi Yao",
            "Yiming Qiu",
            "Qinghong Zhang",
            "Haotian Wang",
            "Yufan Cui",
            "Songlin Wang",
            "Sulong Xu",
            "Mingming Li"
        ],
        "submitted": "2026-02-27 12:39:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "RAD-DPO: Robust Adaptive Denoising Direct Preference Optimization for Generative Retrieval in E-commerce",
        "abstract": "Generative Retrieval (GR) has emerged as a powerful paradigm in e-commerce search, retrieving items via autoregressive decoding of Semantic IDs (SIDs). However, aligning GR with complex user preferences remains challenging. While Direct Preference Optimization (DPO) offers an efficient alignment solution, its direct application to structured SIDs suffers from three limitations: (i) it penalizes shared hierarchical prefixes, causing gradient conflicts; (ii) it is vulnerable to noisy pseudo-negatives from implicit feedback; and (iii) in multi-label queries with multiple relevant items, it exacerbates a probability \"squeezing effect\" among valid candidates. To address these issues, we propose RAD-DPO, which introduces token-level gradient detachment to protect prefix structures, similarity-based dynamic reward weighting to mitigate label noise, and a multi-label global contrastive objective integrated with global SFT loss to explicitly expand positive coverage. Extensive offline experiments and online A/B testing on a large-scale e-commerce platform demonstrate significant improvements in ranking quality and training efficiency.",
        "url": "http://arxiv.org/abs/2602.23964v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23964v1",
        "arxiv_id": "2602.23964v1",
        "authors": [
            "Zhiguo Chen",
            "Guohao Sun",
            "Yiming Qiu",
            "Xingzhi Yao",
            "Mingming Li",
            "Huimu Wang",
            "Yangqi Zhang",
            "Songlin Wang",
            "Sulong Xu"
        ],
        "submitted": "2026-02-27 12:17:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "HotelQuEST: Balancing Quality and Efficiency in Agentic Search",
        "abstract": "Agentic search has emerged as a promising paradigm for adaptive retrieval systems powered by large language models (LLMs). However, existing benchmarks primarily focus on quality, overlooking efficiency factors that are critical for real-world deployment. Moreover, real-world user queries often contain underspecified preferences, a challenge that remains largely underexplored in current agentic search evaluation. As a result, many agentic search systems remain impractical despite their impressive performance. In this work, we introduce HotelQuEST, a benchmark comprising 214 hotel search queries that range from simple factual requests to complex queries, enabling evaluation across the full spectrum of query difficulty. We further address the challenge of evaluating underspecified user preferences by collecting clarifications that make annotators' implicit preferences explicit for evaluation. We find that LLM-based agents achieve higher accuracy than traditional retrievers, but at substantially higher costs due to redundant tool calls and suboptimal routing that fails to match query complexity to model capability. Our analysis exposes inefficiencies in current agentic search systems and demonstrates substantial potential for cost-aware optimization.",
        "url": "http://arxiv.org/abs/2602.23949v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23949v1",
        "arxiv_id": "2602.23949v1",
        "authors": [
            "Guy Hadad",
            "Shadi Iskander",
            "Oren Kalinsky",
            "Sofia Tolmach",
            "Ran Levy",
            "Haggai Roitman"
        ],
        "submitted": "2026-02-27 11:50:57",
        "source": "arxiv",
        "comment": "To be published in EACL 2026"
    },
    {
        "title": "MemEmo: Evaluating Emotion in Memory Systems of Agents",
        "abstract": "Memory systems address the challenge of context loss in Large Language Model during prolonged interactions. However, compared to human cognition, the efficacy of these systems in processing emotion-related information remains inconclusive. To address this gap, we propose an emotion-enhanced memory evaluation benchmark to assess the performance of mainstream and state-of-the-art memory systems in handling affective information. We developed the \\textbf{H}uman-\\textbf{L}ike \\textbf{M}emory \\textbf{E}motion (\\textbf{HLME}) dataset, which evaluates memory systems across three dimensions: emotional information extraction, emotional memory updating, and emotional memory question answering. Experimental results indicate that none of the evaluated systems achieve robust performance across all three tasks. Our findings provide an objective perspective on the current deficiencies of memory systems in processing emotional memories and suggest a new trajectory for future research and system optimization.",
        "url": "http://arxiv.org/abs/2602.23944v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23944v1",
        "arxiv_id": "2602.23944v1",
        "authors": [
            "Peng Liu",
            "Zhen Tao",
            "Jihao Zhao",
            "Ding Chen",
            "Yansong Zhang",
            "Cuiping Li",
            "Zhiyu Li",
            "Hong Chen"
        ],
        "submitted": "2026-02-27 11:46:08",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "EDDA-Coordinata: An Annotated Dataset of Historical Geographic Coordinates",
        "abstract": "This paper introduces a dataset of enriched geographic coordinates retrieved from Diderot and d'Alembert's eighteenth-century Encyclopedie. Automatically recovering geographic coordinates from historical texts is a complex task, as they are expressed in a variety of ways and with varying levels of precision. To improve retrieval of coordinates from similar digitized early modern texts, we have created a gold standard dataset, trained models, published the resulting inferred and normalized coordinate data, and experimented applying these models to new texts. From 74,000 total articles in each of the digitized versions of the Encyclopedie from ARTFL and ENCCRE, we examined 15,278 geographical entries, manually identifying 4,798 containing coordinates, and 10,480 with descriptive but non-numerical references. Leveraging our gold standard annotations, we trained transformer-based models to retrieve and normalize coordinates. The pipeline presented here combines a classifier to identify coordinate-bearing entries and a second model for retrieval, tested across encoder-decoder and decoder architectures. Cross-validation yielded an 86% EM score. On an out-of-domain eighteenth-century Trevoux dictionary (also in French), our fine-tuned model had a 61% EM score, while for the nineteenth-century, 7th edition of the Encyclopaedia Britannica in English, the EM was 77%. These findings highlight the gold standard dataset's usefulness as training data, and our two-step method's cross-lingual, cross-domain generalizability.",
        "url": "http://arxiv.org/abs/2602.23941v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23941v1",
        "arxiv_id": "2602.23941v1",
        "authors": [
            "Ludovic Moncla",
            "Pierre Nugues",
            "Thierry Joliveau",
            "Katherine McDonough"
        ],
        "submitted": "2026-02-27 11:43:17",
        "source": "arxiv",
        "comment": "Accepted at LREC 2026"
    },
    {
        "title": "Benchmarking BERT-based Models for Sentence-level Topic Classification in Nepali Language",
        "abstract": "Transformer-based models such as BERT have significantly advanced Natural Language Processing (NLP) across many languages. However, Nepali, a low-resource language written in Devanagari script, remains relatively underexplored. This study benchmarks multilingual, Indic, Hindi, and Nepali BERT variants to evaluate their effectiveness in Nepali topic classification. Ten pre-trained models, including mBERT, XLM-R, MuRIL, DevBERT, HindiBERT, IndicBERT, and NepBERTa, were fine-tuned and tested on the balanced Nepali dataset containing 25,006 sentences across five conceptual domains and the performance was evaluated using accuracy, weighted precision, recall, F1-score, and AUROC metrics. The results reveal that Indic models, particularly MuRIL-large, achieved the highest F1-score of 90.60%, outperforming multilingual and monolingual models. NepBERTa also performed competitively with an F1-score of 88.26%. Overall, these findings establish a robust baseline for future document-level classification and broader Nepali NLP applications.",
        "url": "http://arxiv.org/abs/2602.23940v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23940v1",
        "arxiv_id": "2602.23940v1",
        "authors": [
            "Nischal Karki",
            "Bipesh Subedi",
            "Prakash Poudyal",
            "Rupak Raj Ghimire",
            "Bal Krishna Bal"
        ],
        "submitted": "2026-02-27 11:42:38",
        "source": "arxiv",
        "comment": "5 pages, 2 figures. Accepted and presented at the Regional International Conference on Natural Language Processing (RegICON 2025), Gauhati University, Guwahati, India, November 27-29, 2025. To appear in the conference proceedings. Accepted papers list available at: https://www.regicon2025.in/accepted-papers"
    },
    {
        "title": "The Astonishing Ability of Large Language Models to Parse Jabberwockified Language",
        "abstract": "We show that large language models (LLMs) have an astonishing ability to recover meaning from severely degraded English texts. Texts in which content words have been randomly substituted by nonsense strings, e.g., \"At the ghybe of the swuint, we are haiveed to Wourge Phrear-gwurr, who sproles into an ghitch flount with his crurp\", can be translated to conventional English that is, in many cases, close to the original text, e.g., \"At the start of the story, we meet a man, Chow, who moves into an apartment building with his wife.\" These results show that structural cues (e.g., morphosyntax, closed-class words) constrain lexical meaning to a much larger degree than imagined. Although the abilities of LLMs to make sense of \"Jabberwockified\" English are clearly superhuman, they are highly relevant to understanding linguistic structure and suggest that efficient language processing either in biological or artificial systems likely benefits from very tight integration between syntax, lexical semantics, and general world knowledge.",
        "url": "http://arxiv.org/abs/2602.23928v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23928v1",
        "arxiv_id": "2602.23928v1",
        "authors": [
            "Gary Lupyan",
            "Senyi Yang"
        ],
        "submitted": "2026-02-27 11:23:45",
        "source": "arxiv",
        "comment": "Submitted to the 2026 Annual Meeting of the Cognitive Science Society"
    },
    {
        "title": "Ref-Adv: Exploring MLLM Visual Reasoning in Referring Expression Tasks",
        "abstract": "Referring Expression Comprehension (REC) links language to region level visual perception. Standard benchmarks (RefCOCO, RefCOCO+, RefCOCOg) have progressed rapidly with multimodal LLMs but remain weak tests of visual reasoning and grounding: (i) many expressions are very short, leaving little reasoning demand; (ii) images often contain few distractors, making the target easy to find; and (iii) redundant descriptors enable shortcut solutions that bypass genuine text understanding and visual reasoning. We introduce Ref-Adv, a modern REC benchmark that suppresses shortcuts by pairing linguistically nontrivial expressions with only the information necessary to uniquely identify the target. The dataset contains referring expressions on real images, curated with hard distractors and annotated with reasoning facets including negation. We conduct comprehensive ablations (word order perturbations and descriptor deletion sufficiency) to show that solving Ref-Adv requires reasoning beyond simple cues, and we evaluate a broad suite of contemporary multimodal LLMs on Ref-Adv. Despite strong results on RefCOCO, RefCOCO+, and RefCOCOg, models drop markedly on Ref-Adv, revealing reliance on shortcuts and gaps in visual reasoning and grounding. We provide an in depth failure analysis and aim for Ref-Adv to guide future work on visual reasoning and grounding in MLLMs.",
        "url": "http://arxiv.org/abs/2602.23898v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23898v1",
        "arxiv_id": "2602.23898v1",
        "authors": [
            "Qihua Dong",
            "Kuo Yang",
            "Lin Ju",
            "Handong Zhao",
            "Yitian Zhang",
            "Yizhou Wang",
            "Huimin Zeng",
            "Jianglin Lu",
            "Yun Fu"
        ],
        "submitted": "2026-02-27 10:47:26",
        "source": "arxiv",
        "comment": "ICLR 2026"
    },
    {
        "title": "LK Losses: Direct Acceptance Rate Optimization for Speculative Decoding",
        "abstract": "Speculative decoding accelerates autoregressive large language model (LLM) inference by using a lightweight draft model to propose candidate tokens that are then verified in parallel by the target model. The speedup is significantly determined by the acceptance rate, yet standard training minimizes Kullback-Leibler (KL) divergence as a proxy objective. While KL divergence and acceptance rate share the same global optimum, small draft models, having limited capacity, typically converge to suboptimal solutions where minimizing KL does not guarantee maximizing acceptance rate. To address this issue, we propose LK losses, special training objectives that directly target acceptance rate. Comprehensive experiments across four draft architectures and six target models, ranging from 8B to 685B parameters, demonstrate consistent improvements in acceptance metrics across all configurations compared to the standard KL-based training. We evaluate our approach on general, coding and math domains and report gains of up to 8-10% in average acceptance length. LK losses are easy to implement, introduce no computational overhead and can be directly integrated into any existing speculator training framework, making them a compelling alternative to the existing draft training objectives.",
        "url": "http://arxiv.org/abs/2602.23881v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23881v1",
        "arxiv_id": "2602.23881v1",
        "authors": [
            "Alexander Samarin",
            "Sergei Krutikov",
            "Anton Shevtsov",
            "Sergei Skvortsov",
            "Filipp Fisin",
            "Alexander Golubev"
        ],
        "submitted": "2026-02-27 10:20:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SWE-rebench V2: Language-Agnostic SWE Task Collection at Scale",
        "abstract": "Software engineering agents (SWE) are improving rapidly, with recent gains largely driven by reinforcement learning (RL). However, RL training is constrained by the scarcity of large-scale task collections with reproducible execution environments and reliable test suites. Although a growing number of benchmarks have emerged, datasets suitable for training remain limited in scale and diversity or often target a limited set of high-resource language ecosystems. We introduce SWE-rebench V2, a language-agnostic automated pipeline for harvesting executable real-world SWE tasks and constructing RL training environments at scale. The pipeline synthesizes repository-specific installation and test procedures via an interactive setup agent, and filters unsound instances using an ensemble of LLM judges, validated against human-verified SWE-bench annotations. Using this pipeline, we construct a dataset of 32,000+ tasks spanning 20 languages and 3,600+ repositories, with pre-built images for reproducible execution. To further scale training data, we additionally release 120,000+ tasks with installation instructions, fail-to-pass tests and rich metadata, where the problem statement is generated based on the original pull request description. We validate the collected instances through a diagnostic study that covers a subset of tasks in five programming languages across seven popular models, and provide instance-level metadata that flags common confounders such as overly restrictive tests and underspecified descriptions. We release the datasets, the collection and execution code, and associated artifacts to enable large-scale training of SWE agents across diverse languages and repositories.",
        "url": "http://arxiv.org/abs/2602.23866v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23866v1",
        "arxiv_id": "2602.23866v1",
        "authors": [
            "Ibragim Badertdinov",
            "Maksim Nekrashevich",
            "Anton Shevtsov",
            "Alexander Golubev"
        ],
        "submitted": "2026-02-27 10:06:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "NAU-QMUL: Utilizing BERT and CLIP for Multi-modal AI-Generated Image Detection",
        "abstract": "With the aim of detecting AI-generated images and identifying the specific models responsible for their generation, we propose a multi-modal multi-task model. The model leverages pre-trained BERT and CLIP Vision encoders for text and image feature extraction, respectively, and employs cross-modal feature fusion with a tailored multi-task loss function. Additionally, a pseudo-labeling-based data augmentation strategy was utilized to expand the training dataset with high-confidence samples. The model achieved fifth place in both Tasks A and B of the `CT2: AI-Generated Image Detection' competition, with F1 scores of 83.16\\% and 48.88\\%, respectively. These findings highlight the effectiveness of the proposed architecture and its potential for advancing AI-generated content detection in real-world scenarios. The source code for our method is published on https://github.com/xxxxxxxxy/AIGeneratedImageDetection.",
        "url": "http://arxiv.org/abs/2602.23863v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23863v1",
        "arxiv_id": "2602.23863v1",
        "authors": [
            "Xiaoyu Guo",
            "Arkaitz Zubiaga"
        ],
        "submitted": "2026-02-27 10:03:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CLFEC: A New Task for Unified Linguistic and Factual Error Correction in paragraph-level Chinese Professional Writing",
        "abstract": "Chinese text correction has traditionally focused on spelling and grammar, while factual error correction is usually treated separately. However, in paragraph-level Chinese professional writing, linguistic (word/grammar/punctuation) and factual errors frequently co-occur and interact, making unified correction both necessary and challenging. This paper introduces CLFEC (Chinese Linguistic & Factual Error Correction), a new task for joint linguistic and factual correction. We construct a mixed, multi-domain Chinese professional writing dataset spanning current affairs, finance, law, and medicine. We then conduct a systematic study of LLM-based correction paradigms, from prompting to retrieval-augmented generation (RAG) and agentic workflows. The analysis reveals practical challenges, including limited generalization of specialized correction models, the need for evidence grounding for factual repair, the difficulty of mixed-error paragraphs, and over-correction on clean inputs. Results further show that handling linguistic and factual Error within the same context outperform decoupled processes, and that agentic workflows can be effective with suitable backbone models. Overall, our dataset and empirical findings provide guidance for building reliable, fully automatic proofreading systems in industrial settings.",
        "url": "http://arxiv.org/abs/2602.23845v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23845v1",
        "arxiv_id": "2602.23845v1",
        "authors": [
            "Jian Kai",
            "Zidong Zhang",
            "Jiwen Chen",
            "Zhengxiang Wu",
            "Songtao Sun",
            "Fuyang Li",
            "Yang Cao",
            "Qiang Liu"
        ],
        "submitted": "2026-02-27 09:36:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "GLUScope: A Tool for Analyzing GLU Neurons in Transformer Language Models",
        "abstract": "We present GLUScope, an open-source tool for analyzing neurons in Transformer-based language models, intended for interpretability researchers. We focus on more recent models than previous tools do; specifically we consider gated activation functions such as SwiGLU. This introduces a new challenge: understanding positive activations is not enough. Instead, both the gate and the in activation of a neuron can be positive or negative, leading to four different possible sign combinations that in some cases have quite different functionalities. Accordingly, for any neuron, our tool shows text examples for each of the four sign combinations, and indicates how often each combination occurs. We describe examples of how our tool can lead to novel insights. A demo is available at https: //sjgerstner.github.io/gluscope.",
        "url": "http://arxiv.org/abs/2602.23826v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23826v1",
        "arxiv_id": "2602.23826v1",
        "authors": [
            "Sebastian Gerstner",
            "Hinrich Schütze"
        ],
        "submitted": "2026-02-27 09:07:45",
        "source": "arxiv",
        "comment": "6 pages for main body, 9 pages in total. 4 figures"
    },
    {
        "title": "Divide and Conquer: Accelerating Diffusion-Based Large Language Models via Adaptive Parallel Decoding",
        "abstract": "Diffusion-based large language models (dLLMs) have shown promising performance across various reasoning tasks, establishing themselves as an alternative to autoregressive large language models (LLMs). Unlike autoregressive LLMs that generate one token per step based on all previous tokens, dLLMs theoretically enable parallel generation of multiple tokens at each decoding step. However, recent dLLMs still favor one-token-per-step generation in practice, as directly decoding multiple masked tokens often leads to degraded generation quality and stability. This reveals a substantial gap between the theoretical parallelism and practical performance of dLLMs. To bridge this gap, we introduce an adaptive parallel decoding approach, namely DiCo, which features a three-phase divide-and-conquer paradigm to unleash the inherent parallelism of dLLMs. During the Divide phase, DiCo first explores the input masked sequence and identifies masked tokens as seed tokens, which are then expanded to construct a set of local clusters. During the Conquer phase, DiCo performs parallel decoding across different local clusters constructed in the Divide phase. The divide-and-conquer process repeatedly alternates between the Divide and Conquer phases until convergence. During the Finalize phase, DiCo decodes the remaining few masked tokens using an effective fine-grained compound decoding scheme to finalize the generation. Extensive experiments demonstrate that DiCo can achieve significant inference speedups while maintaining competitive generation quality.",
        "url": "http://arxiv.org/abs/2602.23792v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23792v1",
        "arxiv_id": "2602.23792v1",
        "authors": [
            "Xiangzhong Luo",
            "Yilin An",
            "Zhicheng Yu",
            "Weichen Liu",
            "Xu Yang"
        ],
        "submitted": "2026-02-27 08:36:06",
        "source": "arxiv",
        "comment": "11 pages, 7 figures"
    },
    {
        "title": "UniFAR: A Unified Facet-Aware Retrieval Framework for Scientific Documents",
        "abstract": "Existing scientific document retrieval (SDR) methods primarily rely on document-centric representations learned from inter-document relationships for document-document (doc-doc) retrieval. However, the rise of LLMs and RAG has shifted SDR toward question-driven retrieval, where documents are retrieved in response to natural-language questions (q-doc). This change has led to systematic mismatches between document-centric models and question-driven retrieval, including (1) input granularity (long documents vs. short questions), (2) semantic focus (scientific discourse structure vs. specific question intent), and (3) training signals (citation-based similarity vs. question-oriented relevance). To this end, we propose UniFAR, a Unified Facet-Aware Retrieval framework to jointly support doc-doc and q-doc SDR within a single architecture. UniFAR reconciles granularity differences through adaptive multi-granularity aggregation, aligns document structure with question intent via learnable facet anchors, and unifies doc-doc and q-doc supervision through joint training. Experimental results show that UniFAR consistently outperforms prior methods across multiple retrieval tasks and base models, confirming its effectiveness and generality.",
        "url": "http://arxiv.org/abs/2602.23766v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23766v1",
        "arxiv_id": "2602.23766v1",
        "authors": [
            "Zheng Dou",
            "Zhao Zhang",
            "Deqing Wang",
            "Yikun Ban",
            "Fuzhen Zhuang"
        ],
        "submitted": "2026-02-27 07:44:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Structured Prompt Optimization for Few-Shot Text Classification via Semantic Alignment in Latent Space",
        "abstract": "This study addresses the issues of semantic entanglement, unclear label structure, and insufficient feature representation in few-shot text classification, and proposes an optimization framework based on structured prompts to enhance semantic understanding and task adaptation under low-resource conditions. The framework first uses a pretrained language model to encode the input text and obtain basic semantic representations. It then introduces structured prompts composed of multi-dimensional semantic factors and integrates them with text features through a learnable combination mechanism, which forms task-related representations with clear boundaries in the latent space. To further strengthen the consistency between text representations and label semantics, the method constructs a structured label embedding matrix and employs a cross-space alignment mechanism to ensure stable matching between textual features and label attributes. In addition, the model applies prompt orthogonality constraints and a joint optimization objective to maintain independence across different semantic factors in the prompts, allowing the structured prompts to provide transparent and controllable guidance for classification decisions. Three types of sensitivity experiments, including learning rate sensitivity, prompt length sensitivity, and data scale sensitivity, are designed to evaluate the stability and robustness of the framework under different conditions. Experimental results show that the proposed structured prompt optimization framework effectively alleviates semantic conflicts and label ambiguity in few-shot text classification. It significantly improves performance on accuracy, precision, recall, and AUC, and demonstrates strong cross-task applicability.",
        "url": "http://arxiv.org/abs/2602.23753v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23753v1",
        "arxiv_id": "2602.23753v1",
        "authors": [
            "Jiasen Zheng",
            "Zijun Zhou",
            "Huajun Zhang",
            "Junjiang Lin",
            "Jingyun Jia",
            "Qi Wang"
        ],
        "submitted": "2026-02-27 07:31:16",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "UTPTrack: Towards Simple and Unified Token Pruning for Visual Tracking",
        "abstract": "One-stream Transformer-based trackers achieve advanced performance in visual object tracking but suffer from significant computational overhead that hinders real-time deployment. While token pruning offers a path to efficiency, existing methods are fragmented. They typically prune the search region, dynamic template, and static template in isolation, overlooking critical inter-component dependencies, which yields suboptimal pruning and degraded accuracy. To address this, we introduce UTPTrack, a simple and Unified Token Pruning framework that, for the first time, jointly compresses all three components. UTPTrack employs an attention-guided, token type-aware strategy to holistically model redundancy, a design that seamlessly supports unified tracking across multimodal and language-guided tasks within a single model. Extensive evaluations on 10 benchmarks demonstrate that UTPTrack achieves a new state-of-the-art in the accuracy-efficiency trade-off for pruning-based trackers, pruning 65.4% of vision tokens in RGB-based tracking and 67.5% in unified tracking while preserving 99.7% and 100.5% of baseline performance, respectively. This strong performance across both RGB and multimodal scenarios underlines its potential as a robust foundation for future research in efficient visual tracking. Code will be released at https://github.com/EIT-NLP/UTPTrack.",
        "url": "http://arxiv.org/abs/2602.23734v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23734v1",
        "arxiv_id": "2602.23734v1",
        "authors": [
            "Hao Wu",
            "Xudong Wang",
            "Jialiang Zhang",
            "Junlong Tong",
            "Xinghao Chen",
            "Junyan Lin",
            "Yunpu Ma",
            "Xiaoyu Shen"
        ],
        "submitted": "2026-02-27 06:58:09",
        "source": "arxiv",
        "comment": "Accepted to CVPR 2026"
    },
    {
        "title": "From Static Benchmarks to Dynamic Protocol: Agent-Centric Text Anomaly Detection for Evaluating LLM Reasoning",
        "abstract": "The evaluation of large language models (LLMs) has predominantly relied on static datasets, which offer limited scalability and fail to capture the evolving reasoning capabilities of recent models. To overcome these limitations, we propose an agent-centric benchmarking paradigm that moves beyond static datasets by introducing a dynamic protocol in which autonomous agents iteratively generate, validate, and solve problems. Within this protocol, a teacher agent generates candidate problems, an orchestrator agent rigorously verifies their validity and guards against adversarial attacks, and a student agent attempts to solve the validated problems. An invalid problem is revised by the teacher agent until it passes validation. If the student correctly solves the problem, the orchestrator prompts the teacher to generate more challenging variants. Consequently, the benchmark scales in difficulty automatically as more capable agents are substituted into any role, enabling progressive evaluation of large language models without manually curated datasets. Adopting text anomaly detection as our primary evaluation format, which demands cross-sentence logical inference and resists pattern-matching shortcuts, we demonstrate that this protocol systematically exposes corner-case reasoning errors that conventional benchmarks fail to reveal. We further advocate evaluating systems along several complementary axes including cross-model pairwise performance and progress between the initial and orchestrator-finalized problems. By shifting the focus from fixed datasets to dynamic protocols, our approach offers a sustainable direction for evaluating ever-evolving language models and introduces a research agenda centered on the co-evolution of agent-centric benchmarks.",
        "url": "http://arxiv.org/abs/2602.23729v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23729v1",
        "arxiv_id": "2602.23729v1",
        "authors": [
            "Seungdong Yoa",
            "Sanghyu Yoon",
            "Suhee Yoon",
            "Dongmin Kim",
            "Ye Seul Sim",
            "Junhyun Lee",
            "Woohyung Lim"
        ],
        "submitted": "2026-02-27 06:54:32",
        "source": "arxiv",
        "comment": "Accepted to ICLR 2026"
    },
    {
        "title": "Recommending Search Filters To Improve Conversions At Airbnb",
        "abstract": "Airbnb, a two-sided online marketplace connecting guests and hosts, offers a diverse and unique inventory of accommodations, experiences, and services. Search filters play an important role in helping guests navigate this variety by refining search results to align with their needs. Yet, while search filters are designed to facilitate conversions in online marketplaces, their direct impact on driving conversions remains underexplored in the existing literature.\n  This paper bridges this gap by presenting a novel application of machine learning techniques to recommend search filters aimed at improving booking conversions. We introduce a modeling framework that directly targets lower-funnel conversions (bookings) by recommending intermediate tools, i.e. search filters. Leveraging the framework, we designed and built the filter recommendation system at Airbnb from the ground up, addressing challenges like cold start and stringent serving requirements.\n  The filter recommendation system we developed has been successfully deployed at Airbnb, powering multiple user interfaces and driving incremental booking conversion lifts, as validated through online A/B testing. An ablation study further validates the effectiveness of our approach and key design choices. By focusing on conversion-oriented filter recommendations, our work ensures that search filters serve their ultimate purpose at Airbnb - helping guests find and book their ideal accommodations.",
        "url": "http://arxiv.org/abs/2602.23717v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23717v1",
        "arxiv_id": "2602.23717v1",
        "authors": [
            "Hao Li",
            "Kedar Bellare",
            "Siyu Yang",
            "Sherry Chen",
            "Liwei He",
            "Stephanie Moyerman",
            "Sanjeev Katariya"
        ],
        "submitted": "2026-02-27 06:36:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "HiDrop: Hierarchical Vision Token Reduction in MLLMs via Late Injection, Concave Pyramid Pruning, and Early Exit",
        "abstract": "The quadratic computational cost of processing vision tokens in Multimodal Large Language Models (MLLMs) hinders their widespread adoption. While progressive vision token pruning offers a promising solution, current methods misinterpret shallow layer functions and use rigid schedules, which fail to unlock the full efficiency potential. To address these issues, we propose HiDrop, a framework that aligns token pruning with the true hierarchical function of MLLM layers. HiDrop features two key innovations: (1) Late Injection, which bypasses passive shallow layers to introduce visual tokens exactly where active fusion begins; and (2) Concave Pyramid Pruning with an Early Exit mechanism to dynamically adjust pruning rates across middle and deep layers. This process is optimized via an inter-layer similarity measure and a differentiable top-k operator. To ensure practical efficiency, HiDrop further incorporates persistent positional encoding, FlashAttention-compatible token selection, and parallel decoupling of vision computation to eliminate hidden overhead associated with dynamic token reduction. Extensive experiments show that HiDrop compresses about 90% visual tokens while matching the original performance and accelerating training by 1.72 times. Our work not only sets a new state-of-the-art for efficient MLLM training and inference but also provides valuable insights into the hierarchical nature of multimodal fusion. The code is released at https://github.com/EIT-NLP/HiDrop.",
        "url": "http://arxiv.org/abs/2602.23699v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23699v1",
        "arxiv_id": "2602.23699v1",
        "authors": [
            "Hao Wu",
            "Yingqi Fan",
            "Jinyang Dai",
            "Junlong Tong",
            "Yunpu Ma",
            "Xiaoyu Shen"
        ],
        "submitted": "2026-02-27 05:55:57",
        "source": "arxiv",
        "comment": "Accepted to ICLR 2026"
    },
    {
        "title": "FuXi-Linear: Unleashing the Power of Linear Attention in Long-term Time-aware Sequential Recommendation",
        "abstract": "Modern recommendation systems primarily rely on attention mechanisms with quadratic complexity, which limits their ability to handle long user sequences and slows down inference. While linear attention is a promising alternative, existing research faces three critical challenges: (1) temporal signals are often overlooked or integrated via naive coupling that causes mutual interference between temporal and semantic signals while neglecting behavioral periodicity; (2) insufficient positional information provided by existing linear frameworks; and (3) a primary focus on short sequences and shallow architectures. To address these issues, we propose FuXi-Linear, a linear-complexity model designed for efficient long-sequence recommendation. Our approach introduces two key components: (1) a Temporal Retention Channel that independently computes periodic attention weights using temporal data, preventing crosstalk between temporal and semantic signals; (2) a Linear Positional Channel that integrates positional information through learnable kernels within linear complexity. Moreover, we demonstrate that FuXi-Linear exhibits a robust power-law scaling property at a thousand-length scale, a characteristic largely unexplored in prior linear recommendation studies. Extensive experiments on sequences of several thousand tokens demonstrate that FuXi-Linear outperforms state-of-the-art models in recommendation quality, while achieving up to 10$\\times$ speedup in the prefill stage and up to 21$\\times$ speedup in the decode stage compared to competitive baselines. Our code has been released in a public repository https://github.com/USTC-StarTeam/fuxi-linear.",
        "url": "http://arxiv.org/abs/2602.23671v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23671v1",
        "arxiv_id": "2602.23671v1",
        "authors": [
            "Yufei Ye",
            "Wei Guo",
            "Hao Wang",
            "Luankang Zhang",
            "Heng Chang",
            "Hong Zhu",
            "Yuyang Ye",
            "Yong Liu",
            "Defu Lian",
            "Enhong Chen"
        ],
        "submitted": "2026-02-27 04:38:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Geodesic Semantic Search: Learning Local Riemannian Metrics for Citation Graph Retrieval",
        "abstract": "We present Geodesic Semantic Search (GSS), a retrieval system that learns node-specific Riemannian metrics on citation graphs to enable geometry-aware semantic search. Unlike standard embedding-based retrieval that relies on fixed Euclidean distances, \\gss{} learns a low-rank metric tensor $\\mL_i \\in \\R^{d \\times r}$ at each node, inducing a local positive semi-definite metric $\\mG_i = \\mL_i \\mL_i^\\top + \\eps \\mI$. This parameterization guarantees valid metrics while keeping the model tractable. Retrieval proceeds via multi-source Dijkstra on the learned geodesic distances, followed by Maximal Marginal Relevance reranking and path coherence filtering. On citation prediction benchmarks with 169K papers, \\gss{} achieves 23\\% relative improvement in Recall@20 over SPECTER+FAISS baselines while providing interpretable citation paths. Our hierarchical coarse-to-fine search with k-means pooling reduces computational cost by 4$\\times$ compared to flat geodesic search while maintaining 97\\% retrieval quality. We provide theoretical analysis of when geodesic distances outperform direct similarity, characterize the approximation quality of low-rank metrics, and validate predictions empirically. Code and trained models are available at https://github.com/YCRG-Labs/geodesic-search.",
        "url": "http://arxiv.org/abs/2602.23665v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23665v1",
        "arxiv_id": "2602.23665v1",
        "authors": [
            "Brandon Yee",
            "Lucas Wang",
            "Kundana Kommini",
            "Krishna Sharma"
        ],
        "submitted": "2026-02-27 04:17:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TRIZ-RAGNER: A Retrieval-Augmented Large Language Model for TRIZ-Aware Named Entity Recognition in Patent-Based Contradiction Mining",
        "abstract": "TRIZ-based contradiction mining is a fundamental task in patent analysis and systematic innovation, as it enables the identification of improving and worsening technical parameters that drive inventive problem solving. However, existing approaches largely rely on rule-based systems or traditional machine learning models, which struggle with semantic ambiguity, domain dependency, and limited generalization when processing complex patent language. Recently, large language models (LLMs) have shown strong semantic understanding capabilities, yet their direct application to TRIZ parameter extraction remains challenging due to hallucination and insufficient grounding in structured TRIZ knowledge. To address these limitations, this paper proposes TRIZ-RAGNER, a retrieval-augmented large language model framework for TRIZ-aware named entity recognition in patent-based contradiction mining. TRIZ-RAGNER reformulates contradiction mining as a semantic-level NER task and integrates dense retrieval over a TRIZ knowledge base, cross-encoder reranking for context refinement, and structured LLM prompting to extract improving and worsening parameters from patent sentences. By injecting domain-specific TRIZ knowledge into the LLM reasoning process, the proposed framework effectively reduces semantic noise and improves extraction consistency. Experiments on the PaTRIZ dataset demonstrate that TRIZ-RAGNER consistently outperforms traditional sequence labeling models and LLM-based baselines. The proposed framework achieves a precision of 85.6%, a recall of 82.9%, and an F1-score of 84.2% in TRIZ contradiction pair identification. Compared with the strongest baseline using prompt-enhanced GPT, TRIZ-RAGNER yields an absolute F1-score improvement of 7.3 percentage points, confirming the effectiveness of retrieval-augmented TRIZ knowledge grounding for robust and accurate patent-based contradiction mining.",
        "url": "http://arxiv.org/abs/2602.23656v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23656v1",
        "arxiv_id": "2602.23656v1",
        "authors": [
            "Zitong Xu",
            "Yuqing Wu",
            "Yue Zhao"
        ],
        "submitted": "2026-02-27 03:40:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Learning to Reflect and Correct: Towards Better Decoding Trajectories for Large-Scale Generative Recommendation",
        "abstract": "Generative Recommendation (GR) has become a promising paradigm for large-scale recommendation systems. However, existing GR models typically perform single-pass decoding without explicit refinement, causing early deviations to accumulate and ultimately degrade recommendation quality. To tackle this problem, we propose GRC, which is, to our knowledge, the first structured reflection-correction framework for GR that extends standard decoding into a Generation-Reflection-Correction (GRC) process. Concretely, GRC introduces a supervised reflection-correction template that decomposes the decoding process into initial draft generation, multi-granular reflection, and reflection-guided correction, thereby enabling structured reflection and correction in the semantic token space. To further explore the enlarged refinement space introduced by the GRC process, we optimize the entire GRC trajectory with GRPO-based reinforcement learning, under a carefully designed reward function with token-level and trajectory-level signals. For efficient online serving, we propose an Entropy-Guided Reflection Scheduling (EGRS) strategy that dynamically allocates more correction budget to high-uncertainty decoding trajectories during beam search. Extensive experiments on real-world datasets show that GRC consistently outperforms six state-of-the-art baselines by up to 15.74%, and online A/B tests demonstrate its substantial practical value in large-scale industrial recommendation, delivering a 1.79% lift in advertising revenue with only modest latency overhead.",
        "url": "http://arxiv.org/abs/2602.23639v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23639v1",
        "arxiv_id": "2602.23639v1",
        "authors": [
            "Haibo Xing",
            "Hao Deng",
            "Lingyu Mu",
            "Jinxin Hu",
            "Yu Zhang",
            "Xiaoyi Zeng",
            "Jing Zhang"
        ],
        "submitted": "2026-02-27 03:22:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Synthetic Data Powers Product Retrieval for Long-tail Knowledge-Intensive Queries in E-commerce Search",
        "abstract": "Product retrieval is the backbone of e-commerce search: for each user query, it identifies a high-recall candidate set from billions of items, laying the foundation for high-quality ranking and user experience. Despite extensive optimization for mainstream queries, existing systems still struggle with long-tail queries, especially knowledge-intensive ones. These queries exhibit diverse linguistic patterns, often lack explicit purchase intent, and require domain-specific knowledge reasoning for accurate interpretation. They also suffer from a shortage of reliable behavioral logs, which makes such queries a persistent challenge for retrieval optimization. To address these issues, we propose an efficient data synthesis framework tailored to retrieval involving long-tail, knowledge-intensive queries. The key idea is to implicitly distill the capabilities of a powerful offline query-rewriting model into an efficient online retrieval system. Leveraging the strong language understanding of LLMs, we train a multi-candidate query rewriting model with multiple reward signals and capture its rewriting capability in well-curated query-product pairs through a powerful offline retrieval pipeline. This design mitigates distributional shift in rewritten queries, which might otherwise limit incremental recall or introduce irrelevant products. Experiments demonstrate that without any additional tricks, simply incorporating this synthetic data into retrieval model training leads to significant improvements. Online Side-By-Side (SBS) human evaluation results indicate a notable enhancement in user search experience.",
        "url": "http://arxiv.org/abs/2602.23620v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23620v1",
        "arxiv_id": "2602.23620v1",
        "authors": [
            "Gui Ling",
            "Weiyuan Li",
            "Yue Jiang",
            "Wenjun Peng",
            "Xingxian Liu",
            "Dongshuai Li",
            "Fuyu Lv",
            "Dan Ou",
            "Haihong Tang"
        ],
        "submitted": "2026-02-27 02:53:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LLM-Driven Multi-Turn Task-Oriented Dialogue Synthesis for Realistic Reasoning",
        "abstract": "The reasoning capability of large language models (LLMs), defined as their ability to analyze, infer, and make decisions based on input information, is essential for building intelligent task-oriented dialogue systems. However, existing benchmarks do not sufficiently reflect the complexity of real-world scenarios, which limits their effectiveness in evaluating and enhancing LLM reasoning in practical contexts. Many current reasoning datasets are overly simplistic and abstract, often disconnected from realistic task flows, domain constraints, and operational rules, making it difficult to effectively evaluate LLMs' logical reasoning ability. In addition, data contamination from pretraining corpora undermines the reliability of evaluation results, and traditional crowdsourcing methods for dataset construction are labor-intensive and difficult to scale. To address these challenges, we propose a LLM-driven framework for synthesizing multi-turn, task-oriented dialogues grounded in realistic reasoning scenarios, leveraging trilevel optimization to enhance dialogue quality. Our method generates dialogues grounded in authentic task scenarios, enriched with real-world information, and exhibiting strong contextual coherence. Corresponding reasoning tasks are carefully designed around these dialogues and iteratively refined to continuously improve the tasks' quality and challenge. The resulting dataset serves as a valuable benchmark for assessing and advancing the realistic logical reasoning capabilities of LLMs. Experimental results show that our synthetic data-based reasoning tasks introduce non-trivial reasoning challenges and provide meaningful support for improving the reasoning capabilities of LLMs.",
        "url": "http://arxiv.org/abs/2602.23610v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23610v1",
        "arxiv_id": "2602.23610v1",
        "authors": [
            "Yu Zhu",
            "Kai Yang"
        ],
        "submitted": "2026-02-27 02:23:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LFQA-HP-1M: A Large-Scale Human Preference Dataset for Long-Form Question Answering",
        "abstract": "Long-form question answering (LFQA) demands nuanced evaluation of multi-sentence explanatory responses, yet existing metrics often fail to reflect human judgment. We present LFQA-HP-1M, a large-scale dataset comprising 1.3M human pairwise preference annotations for LFQA. We propose nine rubrics for answer quality evaluation, and show that simple linear models based on these features perform comparably to state-of-the-art LLM evaluators. We further examine transitivity consistency, positional bias, and verbosity biases in LLM evaluators and demonstrate their vulnerability to adversarial perturbations. Overall, this work provides one of the largest public LFQA preference datasets and a rubric-driven framework for transparent and reliable evaluation.",
        "url": "http://arxiv.org/abs/2602.23603v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23603v1",
        "arxiv_id": "2602.23603v1",
        "authors": [
            "Rafid Ishrak Jahan",
            "Fahmid Shahriar Iqbal",
            "Sagnik Ray Choudhury"
        ],
        "submitted": "2026-02-27 02:14:15",
        "source": "arxiv",
        "comment": "LREC 2026 Accepted. https://huggingface.co/datasets/nlpatunt/LFQA-HP-1M"
    },
    {
        "title": "BRIDGE the Gap: Mitigating Bias Amplification in Automated Scoring of English Language Learners via Inter-group Data Augmentation",
        "abstract": "In the field of educational assessment, automated scoring systems increasingly rely on deep learning and large language models (LLMs). However, these systems face significant risks of bias amplification, where model prediction gaps between student groups become larger than those observed in training data. This issue is especially severe for underrepresented groups such as English Language Learners (ELLs), as models may inherit and further magnify existing disparities in the data. We identify that this issue is closely tied to representation bias: the scarcity of minority (high-scoring ELL) samples makes models trained with empirical risk minimization favor majority (non-ELL) linguistic patterns. Consequently, models tend to under-predict ELL students who even demonstrate comparable domain knowledge but use different linguistic patterns, thereby undermining the fairness of automated scoring outcomes. To mitigate this, we propose BRIDGE, a Bias-Reducing Inter-group Data GEneration framework designed for low-resource assessment settings. Instead of relying on the limited minority samples, BRIDGE synthesizes high-scoring ELL samples by \"pasting\" construct-relevant (i.e., rubric-aligned knowledge and evidence) content from abundant high-scoring non-ELL samples into authentic ELL linguistic patterns. We further introduce a discriminator model to ensure the quality of synthetic samples. Experiments on California Science Test (CAST) datasets demonstrate that BRIDGE effectively reduces prediction bias for high-scoring ELL students while maintaining overall scoring performance. Notably, our method achieves fairness gains comparable to using additional real human data, offering a cost-effective solution for ensuring equitable scoring in large-scale assessments.",
        "url": "http://arxiv.org/abs/2602.23580v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23580v1",
        "arxiv_id": "2602.23580v1",
        "authors": [
            "Yun Wang",
            "Xuansheng Wu",
            "Jingyuan Huang",
            "Lei Liu",
            "Xiaoming Zhai",
            "Ninghao Liu"
        ],
        "submitted": "2026-02-27 01:11:05",
        "source": "arxiv",
        "comment": "15 pages, 1 figure"
    },
    {
        "title": "Multi-Agent Causal Reasoning for Suicide Ideation Detection Through Online Conversations",
        "abstract": "Suicide remains a pressing global public health concern. While social media platforms offer opportunities for early risk detection through online conversation trees, existing approaches face two major limitations: (1) They rely on predefined rules (e.g., quotes or relies) to log conversations that capture only a narrow spectrum of user interactions, and (2) They overlook hidden influences such as user conformity and suicide copycat behavior, which can significantly affect suicidal expression and propagation in online communities. To address these limitations, we propose a Multi-Agent Causal Reasoning (MACR) framework that collaboratively employs a Reasoning Agent to scale user interactions and a Bias-aware Decision-Making Agent to mitigate harmful biases arising from hidden influences. The Reasoning Agent integrates cognitive appraisal theory to generate counterfactual user reactions to posts, thereby scaling user interactions. It analyses these reactions through structured dimensions, i.e., cognitive, emotional, and behavioral patterns, with a dedicated sub-agent responsible for each dimension. The Bias-aware Decision-Making Agent mitigates hidden biases through a front-door adjustment strategy, leveraging the counterfactual user reactions produced by the Reasoning Agent. Through the collaboration of reasoning and bias-aware decision making, the proposed MACR framework not only alleviates hidden biases, but also enriches contextual information of user interactions with counterfactual knowledge. Extensive experiments on real-world conversational datasets demonstrate the effectiveness and robustness of MACR in identifying suicide risk.",
        "url": "http://arxiv.org/abs/2602.23577v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23577v1",
        "arxiv_id": "2602.23577v1",
        "authors": [
            "Jun Li",
            "Xiangmeng Wang",
            "Haoyang Li",
            "Yifei Yan",
            "Shijie Zhang",
            "Hong Va Leong",
            "Ling Feng",
            "Nancy Xiaonan Yu",
            "Qing Li"
        ],
        "submitted": "2026-02-27 01:06:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "France or Spain or Germany or France: A Neural Account of Non-Redundant Redundant Disjunctions",
        "abstract": "Sentences like \"She will go to France or Spain, or perhaps to Germany or France.\" appear formally redundant, yet become acceptable in contexts such as \"Mary will go to a philosophy program in France or Spain, or a mathematics program in Germany or France.\" While this phenomenon has typically been analyzed using symbolic formal representations, we aim to provide a complementary account grounded in artificial neural mechanisms. We first present new behavioral evidence from humans and large language models demonstrating the robustness of this apparent non-redundancy across contexts. We then show that, in language models, redundancy avoidance arises from two interacting mechanisms: models learn to bind contextually relevant information to repeated lexical items, and Transformer induction heads selectively attend to these context-licensed representations. We argue that this neural explanation sheds light on the mechanisms underlying context-sensitive semantic interpretation, and that it complements existing symbolic analyses.",
        "url": "http://arxiv.org/abs/2602.23547v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23547v1",
        "arxiv_id": "2602.23547v1",
        "authors": [
            "Sasha Boguraev",
            "Qing Yao",
            "Kyle Mahowald"
        ],
        "submitted": "2026-02-26 23:02:25",
        "source": "arxiv",
        "comment": "7 pages, 6 figures"
    },
    {
        "title": "Humans and LLMs Diverge on Probabilistic Inferences",
        "abstract": "Human reasoning often involves working over limited information to arrive at probabilistic conclusions. In its simplest form, this involves making an inference that is not strictly entailed by a premise, but rather only likely given the premise. While reasoning LLMs have demonstrated strong performance on logical and mathematical tasks, their behavior on such open-ended, non-deterministic inferences remains largely unexplored. We introduce ProbCOPA, a dataset of 210 handcrafted probabilistic inferences in English, each annotated for inference likelihood by 25--30 human participants. We find that human responses are graded and varied, revealing probabilistic judgments of the inferences in our dataset. Comparing these judgments with responses from eight state-of-the-art reasoning LLMs, we show that models consistently fail to produce human-like distributions. Finally, analyzing LLM reasoning chains, we find evidence of a common reasoning pattern used to evaluate such inferences. Our findings reveal persistent differences between humans and LLMs, and underscore the need to evaluate reasoning beyond deterministic settings.",
        "url": "http://arxiv.org/abs/2602.23546v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23546v1",
        "arxiv_id": "2602.23546v1",
        "authors": [
            "Gaurav Kamath",
            "Sreenath Madathil",
            "Sebastian Schuster",
            "Marie-Catherine de Marneffe",
            "Siva Reddy"
        ],
        "submitted": "2026-02-26 23:00:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Unified Learning-to-Rank for Multi-Channel Retrieval in Large-Scale E-Commerce Search",
        "abstract": "Large-scale e-commerce search must surface a broad set of items from a vast catalog, ranging from bestselling products to new, trending, or seasonal items. Modern systems therefore rely on multiple specialized retrieval channels to surface products, each designed to satisfy a specific objective. A key challenge is how to effectively merge documents from these heterogeneous channels into a single ranked list under strict latency constraints while optimizing for business KPIs such as user conversion. Rank-based fusion methods such as Reciprocal Rank Fusion (RRF) and Weighted Interleaving rely on fixed global channel weights and treat channels independently, failing to account for query-specific channel utility and cross-channel interactions. We observe that multi-channel fusion can be reformulated as a query-dependent learning-to-rank problem over heterogeneous candidate sources. In this paper, we propose a unified ranking model that learns to merge and rank documents from multiple retrieval channels. We formulate the problem as a channel-aware learning-to-rank task that jointly optimizes clicks, add-to-carts, and purchases while incorporating channel-specific objectives. We further incorporate recent user behavioral signals to capture short-term intent shifts that are critical for improving conversion in multi-channel ranking. Our online A/B experiments show that the proposed approach outperforms rank-based fusion methods, leading to a +2.85\\% improvement in user conversion. The model satisfies production latency requirements, achieving a p95 latency of under 50\\,ms, and is deployed on Target.com.",
        "url": "http://arxiv.org/abs/2602.23530v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23530v1",
        "arxiv_id": "2602.23530v1",
        "authors": [
            "Aditya Gaydhani",
            "Guangyue Xu",
            "Dhanush Kamath",
            "Ankit Singh",
            "Alex Li"
        ],
        "submitted": "2026-02-26 22:26:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "IDP Accelerator: Agentic Document Intelligence from Extraction to Compliance Validation",
        "abstract": "Understanding and extracting structured insights from unstructured documents remains a foundational challenge in industrial NLP. While Large Language Models (LLMs) enable zero-shot extraction, traditional pipelines often fail to handle multi-document packets, complex reasoning, and strict compliance requirements. We present IDP (Intelligent Document Processing) Accelerator, a framework enabling agentic AI for end-to-end document intelligence with four key components: (1) DocSplit, a novel benchmark dataset and multimodal classifier using BIO tagging to segment complex document packets; (2) configurable Extraction Module leveraging multimodal LLMs to transform unstructured content into structured data; (3) Agentic Analytics Module, compliant with the Model Context Protocol (MCP) providing data access through secure, sandboxed code execution; and (4) Rule Validation Module replacing deterministic engines with LLM-driven logic for complex compliance checks. The interactive demonstration enables users to upload document packets, visualize classification results, and explore extracted data through an intuitive web interface. We demonstrate effectiveness across industries, highlighting a production deployment at a leading healthcare provider achieving 98% classification accuracy, 80% reduced processing latency, and 77% lower operational costs over legacy baselines. IDP Accelerator is open-sourced with a live demonstration available to the community.",
        "url": "http://arxiv.org/abs/2602.23481v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23481v1",
        "arxiv_id": "2602.23481v1",
        "authors": [
            "Md Mofijul Islam",
            "Md Sirajus Salekin",
            "Joe King",
            "Priyashree Roy",
            "Vamsi Thilak Gudi",
            "Spencer Romo",
            "Akhil Nooney",
            "Boyi Xie",
            "Bob Strahan",
            "Diego A. Socolinsky"
        ],
        "submitted": "2026-02-26 20:20:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "FHIRPath-QA: Executable Question Answering over FHIR Electronic Health Records",
        "abstract": "Though patients are increasingly granted digital access to their electronic health records (EHRs), existing interfaces may not support precise, trustworthy answers to patient-specific questions. Large language models (LLM) show promise in clinical question answering (QA), but retrieval-based approaches are computationally inefficient, prone to hallucination, and difficult to deploy over real-life EHRs. In this work, we introduce FHIRPath-QA, the first open dataset and benchmark for patient-specific QA that includes open-standard FHIRPath queries over real-world clinical data. We propose a text-to-FHIRPath QA paradigm that shifts reasoning from free-text generation to FHIRPath query synthesis, significantly reducing LLM usage. Built on MIMIC-IV on FHIR Demo, the dataset pairs over 14k natural language questions in patient and clinician phrasing with validated FHIRPath queries and answers. Further, we demonstrate that state-of-the-art LLMs struggle to deal with ambiguity in patient language and perform poorly in FHIRPath query synthesis. However, they benefit strongly from supervised fine-tuning. Our results highlight that text-to-FHIRPath synthesis has the potential to serve as a practical foundation for safe, efficient, and interoperable consumer health applications, and our dataset and benchmark serve as a starting point for future research on the topic. The full dataset and generation code is available at: https://github.com/mooshifrew/fhirpath-qa.",
        "url": "http://arxiv.org/abs/2602.23479v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23479v1",
        "arxiv_id": "2602.23479v1",
        "authors": [
            "Michael Frew",
            "Nishit Bheda",
            "Bryan Tripp"
        ],
        "submitted": "2026-02-26 20:14:21",
        "source": "arxiv",
        "comment": "Submitted to LREC 2026 CL4Health Workshop"
    },
    {
        "title": "Cross-Representation Knowledge Transfer for Improved Sequential Recommendations",
        "abstract": "Transformer architectures, capable of capturing sequential dependencies in the history of user interactions, have become the dominant approach in sequential recommender systems. Despite their success, such models consider sequence elements in isolation, implicitly accounting for the complex relationships between them. Graph neural networks, in contrast, explicitly model these relationships through higher order interactions but are often unable to adequately capture their evolution over time, limiting their use for predicting the next interaction. To fill this gap, we present a new framework that combines transformers and graph neural networks and aligns different representations for solving next-item prediction task. Our solution simultaneously encodes structural dependencies in the interaction graph and tracks their dynamic change. Experimental results on a number of open datasets demonstrate that the proposed framework consistently outperforms both pure sequential and graph approaches in terms of recommendation quality, as well as recent methods that combine both types of signals.",
        "url": "http://arxiv.org/abs/2602.23471v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23471v1",
        "arxiv_id": "2602.23471v1",
        "authors": [
            "Artur Gimranov",
            "Viacheslav Yusupov",
            "Elfat Sabitov",
            "Tatyana Matveeva",
            "Anton Lysenko",
            "Ruslan Israfilov",
            "Evgeny Frolov"
        ],
        "submitted": "2026-02-26 20:03:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CiteAudit: You Cited It, But Did You Read It? A Benchmark for Verifying Scientific References in the LLM Era",
        "abstract": "Scientific research relies on accurate citation for attribution and integrity, yet large language models (LLMs) introduce a new risk: fabricated references that appear plausible but correspond to no real publications. Such hallucinated citations have already been observed in submissions and accepted papers at major machine learning venues, exposing vulnerabilities in peer review. Meanwhile, rapidly growing reference lists make manual verification impractical, and existing automated tools remain fragile to noisy and heterogeneous citation formats and lack standardized evaluation. We present the first comprehensive benchmark and detection framework for hallucinated citations in scientific writing. Our multi-agent verification pipeline decomposes citation checking into claim extraction, evidence retrieval, passage matching, reasoning, and calibrated judgment to assess whether a cited source truly supports its claim. We construct a large-scale human-validated dataset across domains and define unified metrics for citation faithfulness and evidence alignment. Experiments with state-of-the-art LLMs reveal substantial citation errors and show that our framework significantly outperforms prior methods in both accuracy and interpretability. This work provides the first scalable infrastructure for auditing citations in the LLM era and practical tools to improve the trustworthiness of scientific references.",
        "url": "http://arxiv.org/abs/2602.23452v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23452v1",
        "arxiv_id": "2602.23452v1",
        "authors": [
            "Zhengqing Yuan",
            "Kaiwen Shi",
            "Zheyuan Zhang",
            "Lichao Sun",
            "Nitesh V. Chawla",
            "Yanfang Ye"
        ],
        "submitted": "2026-02-26 19:17:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Truncated Step-Level Sampling with Process Rewards for Retrieval-Augmented Reasoning",
        "abstract": "Training large language models to reason with search engines via reinforcement learning is hindered by a fundamental credit assignment problem: existing methods such as Search-R1 provide only a sparse outcome reward after an entire multi-step trajectory, making it infeasible to attribute success or failure to individual reasoning and retrieval decisions. Process-reward methods like StepSearch alleviate this by introducing step-level supervision, but rely on heuristic rewards such as TF-IDF overlap with gold documents, and still sample k complete trajectories per example, retaining high gradient variance. We propose SLATE, a framework built on two complementary ideas: (1) truncated step-level sampling, which generates k trajectories that share a common prefix and differ only at the next step, and (2) dense LLM-as-judge rewards, which replace heuristic scoring with a capable LLM evaluator that assesses the quality of each reasoning step, search query, and answer, providing richer and more reliable supervision. We theoretically prove that under the same dense reward structure, truncated sampling reduces the variance of advantage estimates by up to a factor of T compared to full-trajectory sampling for T-step trajectories, yielding lower-variance, better-targeted policy gradients. Experiments on seven QA benchmarks confirm that SLATE consistently outperforms both sparse-reward and process-reward baselines, with the largest gains on harder multi-hop tasks and smaller models.",
        "url": "http://arxiv.org/abs/2602.23440v1",
        "pdf_url": "https://arxiv.org/pdf/2602.23440v1",
        "arxiv_id": "2602.23440v1",
        "authors": [
            "Chris Samarinas",
            "Haw-Shiuan Chang",
            "Hamed Zamani"
        ],
        "submitted": "2026-02-26 19:05:40",
        "source": "arxiv",
        "comment": null
    }
]