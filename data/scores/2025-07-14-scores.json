[
    {
        "title": "Criteria-Based LLM Relevance Judgments",
        "abstract": "Relevance judgments are crucial for evaluating information retrieval systems,\nbut traditional human-annotated labels are time-consuming and expensive. As a\nresult, many researchers turn to automatic alternatives to accelerate method\ndevelopment. Among these, Large Language Models (LLMs) provide a scalable\nsolution by generating relevance labels directly through prompting. However,\nprompting an LLM for a relevance label without constraints often results in not\nonly incorrect predictions but also outputs that are difficult for humans to\ninterpret. We propose the Multi-Criteria framework for LLM-based relevance\njudgments, decomposing the notion of relevance into multiple criteria--such as\nexactness, coverage, topicality, and contextual fit--to improve the robustness\nand interpretability of retrieval evaluations compared to direct grading\nmethods. We validate this approach on three datasets: the TREC Deep Learning\ntracks from 2019 and 2020, as well as LLMJudge (based on TREC DL 2023). Our\nresults demonstrate that Multi-Criteria judgments enhance the system\nranking/leaderboard performance. Moreover, we highlight the strengths and\nlimitations of this approach relative to direct grading approaches, offering\ninsights that can guide the development of future automatic evaluation\nframeworks in information retrieval.",
        "url": "http://arxiv.org/abs/2507.09488v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09488v1",
        "arxiv_id": "2507.09488v1",
        "authors": [
            "Naghmeh Farzi",
            "Laura Dietz"
        ],
        "submitted": "2025-07-13 04:21:21",
        "source": "arxiv",
        "comment": "10 pages, 3 figures, accepted to ICTIR 2025",
        "score": 17,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)",
            "Found 'trec' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores the use of Large Language Models (LLMs) for relevance judgments in information retrieval, which aligns with your interest in search technologies and query understanding. However, the focus on LLMs and relevance judgments is more specific and doesn't directly relate to your interests in ranking models, user behavior modeling, and deep semantic understanding."
    },
    {
        "title": "User Long-Term Multi-Interest Retrieval Model for Recommendation",
        "abstract": "User behavior sequence modeling, which captures user interest from rich\nhistorical interactions, is pivotal for industrial recommendation systems.\nDespite breakthroughs in ranking-stage models capable of leveraging ultra-long\nbehavior sequences with length scaling up to thousands, existing retrieval\nmodels remain constrained to sequences of hundreds of behaviors due to two main\nchallenges. One is strict latency budget imposed by real-time service over\nlarge-scale candidate pool. The other is the absence of target-aware mechanisms\nand cross-interaction architectures, which prevent utilizing ranking-like\ntechniques to simplify long sequence modeling. To address these limitations, we\npropose a new framework named User Long-term Multi-Interest Retrieval\nModel(ULIM), which enables thousand-scale behavior modeling in retrieval\nstages. ULIM includes two novel components: 1)Category-Aware Hierarchical\nDual-Interest Learning partitions long behavior sequences into multiple\ncategory-aware subsequences representing multi-interest and jointly optimizes\nlong-term and short-term interests within specific interest cluster.\n2)Pointer-Enhanced Cascaded Category-to-Item Retrieval introduces\nPointer-Generator Interest Network(PGIN) for next-category prediction, followed\nby next-item retrieval upon the top-K predicted categories. Comprehensive\nexperiments on Taobao dataset show that ULIM achieves substantial improvement\nover state-of-the-art methods, and brings 5.54% clicks, 11.01% orders and 4.03%\nGMV lift for Taobaomiaosha, a notable mini-app of Taobao.",
        "url": "http://arxiv.org/abs/2507.10097v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10097v1",
        "arxiv_id": "2507.10097v1",
        "authors": [
            "Yue Meng",
            "Cheng Guo",
            "Xiaohui Hu",
            "Honghu Deng",
            "Yi Cao",
            "Tong Liu",
            "Bo Zheng"
        ],
        "submitted": "2025-07-14 09:32:26",
        "source": "arxiv",
        "comment": null,
        "score": 16,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'ltr' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'user behavior' (score: +2)",
            "Found 'click' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on user behavior sequence modeling and recommendation systems, which is related to information retrieval and user behavior modeling. However, the specific approach and techniques used, such as hierarchical dual-interest learning and pointer-enhanced cascaded category-to-item retrieval, are not directly aligned with the user's interests in query understanding, ranking models, and deep semantic understanding."
    },
    {
        "title": "Am I on the Right Track? What Can Predicted Query Performance Tell Us about the Search Behaviour of Agentic RAG",
        "abstract": "Agentic Retrieval-Augmented Generation (RAG) is a new paradigm where the\nreasoning model decides when to invoke a retriever (as a \"tool\") when answering\na question. This paradigm, exemplified by recent research works such as\nSearch-R1, enables the model to decide when to search and obtain external\ninformation. However, the queries generated by such Agentic RAG models and the\nrole of the retriever in obtaining high-quality answers remain understudied. To\nthis end, this initial study examines the applicability of query performance\nprediction (QPP) within the recent Agentic RAG models Search-R1 and\nR1-Searcher. We find that applying effective retrievers can achieve higher\nanswer quality within a shorter reasoning process. Moreover, the QPP estimates\nof the generated queries, used as an approximation of their retrieval quality,\nare positively correlated with the quality of the final answer. Ultimately, our\nwork is a step towards adaptive retrieval within Agentic RAG, where QPP is used\nto inform the model if the retrieved results are likely to be useful.",
        "url": "http://arxiv.org/abs/2507.10411v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10411v1",
        "arxiv_id": "2507.10411v1",
        "authors": [
            "Fangzheng Tian",
            "Jinyuan Fang",
            "Debasis Ganguly",
            "Zaiqiao Meng",
            "Craig Macdonald"
        ],
        "submitted": "2025-07-14 15:54:50",
        "source": "arxiv",
        "comment": null,
        "score": 14,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores the application of query performance prediction (QPP) in Agentic Retrieval-Augmented Generation (RAG) models, which is related to ranking models and query understanding in Information Retrieval. However, the focus on RAG and its specific applications in question answering and adaptive retrieval is not directly aligned with the user's primary interests in e-commerce and real-time relevance optimization."
    },
    {
        "title": "RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking",
        "abstract": "The rapid proliferation of multimodal misinformation presents significant\nchallenges for automated fact-checking systems, especially when claims are\nambiguous or lack sufficient context. We introduce RAMA, a novel\nretrieval-augmented multi-agent framework designed for verifying multimedia\nmisinformation. RAMA incorporates three core innovations: (1) strategic query\nformulation that transforms multimodal claims into precise web search queries;\n(2) cross-verification evidence aggregation from diverse, authoritative\nsources; and (3) a multi-agent ensemble architecture that leverages the\ncomplementary strengths of multiple multimodal large language models and prompt\nvariants. Extensive experiments demonstrate that RAMA achieves superior\nperformance on benchmark datasets, particularly excelling in resolving\nambiguous or improbable claims by grounding verification in retrieved factual\nevidence. Our findings underscore the necessity of integrating web-based\nevidence and multi-agent reasoning for trustworthy multimedia verification,\npaving the way for more reliable and scalable fact-checking solutions. RAMA\nwill be publicly available at https://github.com/kalendsyang/RAMA.git.",
        "url": "http://arxiv.org/abs/2507.09174v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09174v1",
        "arxiv_id": "2507.09174v1",
        "authors": [
            "Shuo Yang",
            "Zijian Yu",
            "Zhenzhe Ying",
            "Yuqin Dai",
            "Guoqing Wang",
            "Jun Lan",
            "Jinfeng Xu",
            "Jinze Li",
            "Edith C. H. Ngai"
        ],
        "submitted": "2025-07-12 07:46:51",
        "source": "arxiv",
        "comment": null,
        "score": 12,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'web search' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper introduces a novel framework for misinformation detection in multimodal fact-checking, leveraging web search queries and multi-agent reasoning. While it touches on query formulation and evidence aggregation, the focus is more on fact-checking and verification rather than traditional information retrieval and search technologies. The paper's relevance to the user's interests is somewhat related, but not a central match."
    },
    {
        "title": "Identifying Offline Metrics that Predict Online Impact: A Pragmatic Strategy for Real-World Recommender Systems",
        "abstract": "A critical challenge in recommender systems is to establish reliable\nrelationships between offline and online metrics that predict real-world\nperformance. Motivated by recent advances in Pareto front approximation, we\nintroduce a pragmatic strategy for identifying offline metrics that align with\nonline impact. A key advantage of this approach is its ability to\nsimultaneously serve multiple test groups, each with distinct offline\nperformance metrics, in an online experiment controlled by a single model. The\nmethod is model-agnostic for systems with a neural network backbone, enabling\nbroad applicability across architectures and domains. We validate the strategy\nthrough a large-scale online experiment in the field of session-based\nrecommender systems on the OTTO e-commerce platform. The online experiment\nidentifies significant alignments between offline metrics and real-word\nclick-through rate, post-click conversion rate and units sold. Our strategy\nprovides industry practitioners with a valuable tool for understanding\noffline-to-online metric relationships and making informed, data-driven\ndecisions.",
        "url": "http://arxiv.org/abs/2507.09566v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09566v1",
        "arxiv_id": "2507.09566v1",
        "authors": [
            "Timo Wilm",
            "Philipp Normann"
        ],
        "submitted": "2025-07-13 10:24:41",
        "source": "arxiv",
        "comment": "This work was accepted for publication in the 19th ACM Conference on\n  Recommender Systems (RecSys 2025). The final published version will be\n  available at the ACM Digital Library",
        "score": 11,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'click' (score: +2)",
            "Found 'click-through rate' (score: +2)",
            "Found 'conversion rate' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on recommender systems, which is a related topic, but it does not address query understanding, ranking models, or user behavior modeling, which are core interests. The paper's emphasis on offline-to-online metric relationships and its application to session-based recommender systems on an e-commerce platform also do not align with the user's background in IR and NLP."
    },
    {
        "title": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks",
        "abstract": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance.",
        "url": "http://arxiv.org/abs/2507.10535v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10535v1",
        "arxiv_id": "2507.10535v1",
        "authors": [
            "Hongchao Jiang",
            "Yiming Chen",
            "Yushi Cao",
            "Hung-yi Lee",
            "Robby T. Tan"
        ],
        "submitted": "2025-07-14 17:56:29",
        "source": "arxiv",
        "comment": "Dataset is available at\n  https://huggingface.co/datasets/mattymchen/codejudgebench",
        "score": 10,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'pairwise' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the use of Large Language Models (LLMs) as judges for coding tasks, which is related to information retrieval and search technologies. However, the focus is on LLMs' ability to assess and compare code quality, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's relevance is somewhat limited, but it does touch on aspects of natural language processing and data mining."
    },
    {
        "title": "PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware Query Optimization",
        "abstract": "Scientific paper retrieval, particularly framed as document-to-document\nretrieval, aims to identify relevant papers in response to a long-form query\npaper, rather than a short query string. Previous approaches to this task have\nfocused on abstracts, embedding them into dense vectors as surrogates for full\ndocuments and calculating similarity across them, although abstracts provide\nonly sparse and high-level summaries. To address this, we propose PRISM, a\nnovel document-to-document retrieval method that introduces multiple,\nfine-grained representations for both the query and candidate papers. In\nparticular, each query paper is decomposed into multiple aspect-specific views\nand individually embedded, which are then matched against candidate papers\nsimilarity segmented to consider their multifaceted dimensions. Moreover, we\npresent SciFullBench, a novel benchmark in which the complete and segmented\ncontext of full papers for both queries and candidates is available. Then,\nexperimental results show that PRISM improves performance by an average of 4.3%\nover existing retrieval baselines.",
        "url": "http://arxiv.org/abs/2507.10057v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10057v1",
        "arxiv_id": "2507.10057v1",
        "authors": [
            "Sangwoo Park",
            "Jinheon Baek",
            "Soyeong Jeong",
            "Sung Ju Hwang"
        ],
        "submitted": "2025-07-14 08:41:53",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 7,
        "llm_reason": "The paper is relevant to Information Retrieval (IR) and Search technologies, specifically in the area of document-to-document retrieval, which aligns with the user's interests. The use of multi-aspect-aware query optimization and fine-grained representations for both query and candidate papers is also related to the user's focus on query understanding and ranking models. However, the paper's focus on scientific paper retrieval and its application to a specific domain (SciFullBench) limits its relevance to the user's broader interests in e-commerce and NLP."
    },
    {
        "title": "Non-parametric Graph Convolution for Re-ranking in Recommendation Systems",
        "abstract": "Graph knowledge has been proven effective in enhancing item rankings in\nrecommender systems (RecSys), particularly during the retrieval stage. However,\nits application in the ranking stage, especially when richer contextual\ninformation in user-item interactions is available, remains underexplored. A\nmajor challenge lies in the substantial computational cost associated with\nrepeatedly retrieving neighborhood information from billions of items stored in\ndistributed systems. This resource-intensive requirement makes it difficult to\nscale graph-based methods in practical RecSys. To bridge this gap, we first\ndemonstrate that incorporating graphs in the ranking stage improves ranking\nqualities. Notably, while the improvement is evident, we show that the\nsubstantial computational overheads entailed by graphs are prohibitively\nexpensive for real-world recommendations. In light of this, we propose a\nnon-parametric strategy that utilizes graph convolution for re-ranking only\nduring test time. Our strategy circumvents the notorious computational\noverheads from graph convolution during training, and utilizes structural\nknowledge hidden in graphs on-the-fly during testing. It can be used as a\nplug-and-play module and easily employed to enhance the ranking ability of\nvarious ranking layers of a real-world RecSys with significantly reduced\ncomputational overhead. Through comprehensive experiments across four benchmark\ndatasets with varying levels of sparsity, we demonstrate that our strategy\nyields noticeable improvements (i.e., 8.1% on average) during testing time with\nlittle to no additional computational overheads (i.e., 0.5 on average). Code:\nhttps://github.com/zyouyang/RecSys2025_NonParamGC.git",
        "url": "http://arxiv.org/abs/2507.09969v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09969v1",
        "arxiv_id": "2507.09969v1",
        "authors": [
            "Zhongyu Ouyang",
            "Mingxuan Ju",
            "Soroush Vosoughi",
            "Yanfang Ye"
        ],
        "submitted": "2025-07-14 06:35:18",
        "source": "arxiv",
        "comment": "Accepted to RecSys2025 Main",
        "score": 10,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'recsys' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on recommender systems, which is a related topic, but it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The use of graph convolution for re-ranking is an innovative approach, but it is not directly applicable to information retrieval or search technologies."
    },
    {
        "title": "EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions",
        "abstract": "Hacker forums provide critical early warning signals for emerging\ncybersecurity threats, but extracting actionable intelligence from their\nunstructured and noisy content remains a significant challenge. This paper\npresents an unsupervised framework that automatically detects, clusters, and\nprioritizes security events discussed across hacker forum posts. Our approach\nleverages Transformer-based embeddings fine-tuned with contrastive learning to\ngroup related discussions into distinct security event clusters, identifying\nincidents like zero-day disclosures or malware releases without relying on\npredefined keywords. The framework incorporates a daily ranking mechanism that\nprioritizes identified events using quantifiable metrics reflecting timeliness,\nsource credibility, information completeness, and relevance. Experimental\nevaluation on real-world hacker forum data demonstrates that our method\neffectively reduces noise and surfaces high-priority threats, enabling security\nanalysts to mount proactive responses. By transforming disparate hacker forum\ndiscussions into structured, actionable intelligence, our work addresses\nfundamental challenges in automated threat detection and analysis.",
        "url": "http://arxiv.org/abs/2507.09762v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09762v1",
        "arxiv_id": "2507.09762v1",
        "authors": [
            "Yasir Ech-Chammakhy",
            "Anas Motii",
            "Anass Rabii",
            "Jaafar Chbili"
        ],
        "submitted": "2025-07-13 19:40:36",
        "source": "arxiv",
        "comment": "Accepted for publication at the 28th International Symposium on\n  Research in Attacks, Intrusions, and Defenses (RAID 2025)",
        "score": 9,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on event detection and ranking in hacker forum discussions, which is related to information retrieval and search technologies. However, the specific context of cybersecurity and threat detection is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Riding the Carousel: The First Extensive Eye Tracking Analysis of Browsing Behavior in Carousel Recommenders",
        "abstract": "Carousels have become the de-facto interface in online services. However,\nthere is a lack of research in carousels, particularly examining how\nrecommender systems may be designed differently than the traditional\nsingle-list interfaces. One of the key elements for understanding how to design\na system for a particular interface is understanding how users browse. For\ncarousels, users may browse in a number of different ways due to the added\ncomplexity of multiple topic defined-lists and swiping to see more items.\n  Eye tracking is the key to understanding user behavior by providing valuable,\ndirect information on how users see and navigate. In this work, we provide the\nfirst extensive analysis of the eye tracking behavior in carousel recommenders\nunder the free-browsing setting. To understand how users browse, we examine the\nfollowing research questions : 1) where do users start browsing, 2) how do\nusers transition from item to item within the same carousel and across\ncarousels, and 3) how does genre preference impact transitions?\n  This work addresses a gap in the field and provides the first extensive\nempirical results of eye tracked browsing behavior in carousels for improving\nrecommenders. Taking into account the insights learned from the above\nquestions, our final contribution is to provide suggestions to help carousel\nrecommender system designers optimize their systems for user browsing behavior.\nThe most important suggestion being to reorder the ranked item positions to\naccount for browsing after swiping.These contributions aim not only to help\nimprove current systems, but also to encourage and allow the design of new user\nmodels, systems, and metrics that are better suited to the complexity of\ncarousel interfaces.",
        "url": "http://arxiv.org/abs/2507.10135v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10135v1",
        "arxiv_id": "2507.10135v1",
        "authors": [
            "Santiago de Leon-Martinez",
            "Robert Moro",
            "Branislav Kveton",
            "Maria Bielikova"
        ],
        "submitted": "2025-07-14 10:26:27",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'user behavior' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on carousel recommenders, which is a specific type of interface, and explores user behavior through eye tracking. While it touches on recommender systems, it doesn't directly relate to query understanding, ranking models, or user behavior modeling in the context of information retrieval. The paper's scope is more focused on user experience and interface design, which is somewhat relevant to the user's interests in search technologies and NLP."
    },
    {
        "title": "Automating SPARQL Query Translations between DBpedia and Wikidata",
        "abstract": "This paper investigates whether state-of-the-art Large Language Models (LLMs)\ncan automatically translate SPARQL between popular Knowledge Graph (KG)\nschemas. We focus on translations between the DBpedia and Wikidata KG, and\nlater on DBLP and OpenAlex KG. This study addresses a notable gap in KG\ninteroperability research by rigorously evaluating LLM performance on\nSPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first\nalign 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100\nDBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic\nKGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and\nMistral-Large-Instruct-2407 are selected based on their sizes and architectures\nand tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs\nwere compared with gold answers, and resulting errors were categorized. We find\nthat the performance varies markedly across models and prompting strategies,\nand that translations for Wikidata to DBpedia work far better than translations\nfor DBpedia to Wikidata.",
        "url": "http://arxiv.org/abs/2507.10045v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10045v1",
        "arxiv_id": "2507.10045v1",
        "authors": [
            "Malte Christian Bartels",
            "Debayan Banerjee",
            "Ricardo Usbeck"
        ],
        "submitted": "2025-07-14 08:23:25",
        "source": "arxiv",
        "comment": "18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference\n  happening on September 2025",
        "score": 7,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on translating SPARQL queries between Knowledge Graph schemas, which is not directly related to Information Retrieval, Search technologies, or query understanding. Although it involves Natural Language Processing, the context is different from the user's primary interests."
    },
    {
        "title": "Knowledge Conceptualization Impacts RAG Efficacy",
        "abstract": "Explainability and interpretability are cornerstones of frontier and\nnext-generation artificial intelligence (AI) systems. This is especially true\nin recent systems, such as large language models (LLMs), and more broadly,\ngenerative AI. On the other hand, adaptability to new domains, contexts, or\nscenarios is also an important aspect for a successful system. As such, we are\nparticularly interested in how we can merge these two efforts, that is,\ninvestigating the design of transferable and interpretable neurosymbolic AI\nsystems. Specifically, we focus on a class of systems referred to as ''Agentic\nRetrieval-Augmented Generation'' systems, which actively select, interpret, and\nquery knowledge sources in response to natural language prompts. In this paper,\nwe systematically evaluate how different conceptualizations and representations\nof knowledge, particularly the structure and complexity, impact an AI agent (in\nthis case, an LLM) in effectively querying a triplestore. We report our\nresults, which show that there are impacts from both approaches, and we discuss\ntheir impact and implications.",
        "url": "http://arxiv.org/abs/2507.09389v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09389v1",
        "arxiv_id": "2507.09389v1",
        "authors": [
            "Chris Davis Jaldi",
            "Anmol Saini",
            "Elham Ghiasi",
            "O. Divine Eziolise",
            "Cogan Shimizu"
        ],
        "submitted": "2025-07-12 20:10:26",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the design of transferable and interpretable neurosymbolic AI systems, which is a related topic in NLP. However, the focus on knowledge conceptualization and its impact on RAG systems is not directly aligned with my research interests in Information Retrieval, query understanding, and ranking models."
    },
    {
        "title": "Multiple Choice Learning of Low Rank Adapters for Language Modeling",
        "abstract": "We propose LoRA-MCL, a training scheme that extends next-token prediction in\nlanguage models with a method designed to decode diverse, plausible sentence\ncontinuations at inference time. Traditional language modeling is an\nintrinsically ill-posed problem: given a context, multiple futures may be\nequally plausible. Our approach leverages Multiple Choice Learning (MCL) and\nthe Winner-Takes-All (WTA) loss to efficiently handle ambiguity through\nLow-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying\nMultiple Choice Learning to Language Modeling, assuming the data is generated\nfrom a mixture of distributions. To illustrate the proposed approach, we use\ndata sampled from mixtures of Markov chains. We then demonstrate with extensive\nexperiments on real-world visual and audio captioning tasks that our method\nachieves high diversity and relevance in generated outputs.",
        "url": "http://arxiv.org/abs/2507.10419v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10419v1",
        "arxiv_id": "2507.10419v1",
        "authors": [
            "Victor Letzelter",
            "Hugo Malard",
            "Mathieu Fontaine",
            "Gaël Richard",
            "Slim Essid",
            "Andrei Bursuc",
            "Patrick Pérez"
        ],
        "submitted": "2025-07-14 16:00:51",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a novel approach to language modeling, leveraging Multiple Choice Learning and Low-Rank Adaptation to handle ambiguity in sentence continuations. While it touches on some aspects of query understanding and ranking models, it is primarily focused on language modeling and does not directly relate to user behavior modeling or real-time relevance optimization. The paper's relevance to information retrieval is limited, but it may be of interest to those working in NLP."
    },
    {
        "title": "Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking",
        "abstract": "Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies\nfor retrieval, which enhance large language models (LLMs) by enabling them to\naccess external knowledge, ensuring that the retrieved information is\nup-to-date and domain-specific. However, traditional methods often fail to\ncreate chunks that capture sufficient semantic meaning, as they do not account\nfor the underlying textual structure. This paper proposes a novel framework\nthat enhances RAG by integrating hierarchical text segmentation and clustering\nto generate more meaningful and semantically coherent chunks. During inference,\nthe framework retrieves information by leveraging both segment-level and\ncluster-level vector representations, thereby increasing the likelihood of\nretrieving more precise and contextually relevant information. Evaluations on\nthe NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method\nachieved improved results compared to traditional chunking techniques.",
        "url": "http://arxiv.org/abs/2507.09935v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09935v1",
        "arxiv_id": "2507.09935v1",
        "authors": [
            "Hai Toan Nguyen",
            "Tien Dat Nguyen",
            "Viet Ha Nguyen"
        ],
        "submitted": "2025-07-14 05:21:58",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper discusses Retrieval-Augmented Generation (RAG) and proposes a novel framework that integrates hierarchical text segmentation and clustering to generate more meaningful chunks. While it's related to information retrieval and natural language processing, the focus is on enhancing RAG rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest for you. The paper's relevance is somewhat diminished by its focus on large language models and chunking strategies, which are not directly aligned with your research themes."
    },
    {
        "title": "Balancing Semantic Relevance and Engagement in Related Video Recommendations",
        "abstract": "Related video recommendations commonly use collaborative filtering (CF)\ndriven by co-engagement signals, often resulting in recommendations lacking\nsemantic coherence and exhibiting strong popularity bias. This paper introduces\na novel multi-objective retrieval framework, enhancing standard two-tower\nmodels to explicitly balance semantic relevance and user engagement. Our\napproach uniquely combines: (a) multi-task learning (MTL) to jointly optimize\nco-engagement and semantic relevance, explicitly prioritizing topical\ncoherence; (b) fusion of multimodal content features (textual and visual\nembeddings) for richer semantic understanding; and (c) off-policy correction\n(OPC) via inverse propensity weighting to effectively mitigate popularity bias.\nEvaluation on industrial-scale data and a two-week live A/B test reveals our\nframework's efficacy. We observed significant improvements in semantic\nrelevance (from 51% to 63% topic match rate), a reduction in popular item\ndistribution (-13.8% popular video recommendations), and a +0.04% improvement\nin our topline user engagement metric. Our method successfully achieves better\nsemantic coherence, balanced engagement, and practical scalability for\nreal-world deployment.",
        "url": "http://arxiv.org/abs/2507.09403v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09403v1",
        "arxiv_id": "2507.09403v1",
        "authors": [
            "Amit Jaspal",
            "Feng Zhang",
            "Wei Chang",
            "Sumit Kumar",
            "Yubo Wang",
            "Roni Mittleman",
            "Qifan Wang",
            "Weize Mao"
        ],
        "submitted": "2025-07-12 21:04:25",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "The paper's focus on related video recommendations, query understanding, and ranking models aligns with your interests in Information Retrieval and Search technologies. The use of multi-objective retrieval framework, multi-task learning, and off-policy correction to balance semantic relevance and user engagement is also relevant to your research themes. However, the paper's specific application to video recommendations and the use of multimodal content features may not be directly applicable to your e-commerce background."
    },
    {
        "title": "Retrieval-Augmented Recommendation Explanation Generation with Hierarchical Aggregation",
        "abstract": "Explainable Recommender System (ExRec) provides transparency to the\nrecommendation process, increasing users' trust and boosting the operation of\nonline services. With the rise of large language models (LLMs), whose extensive\nworld knowledge and nuanced language understanding enable the generation of\nhuman-like, contextually grounded explanations, LLM-powered ExRec has gained\ngreat momentum. However, existing LLM-based ExRec models suffer from profile\ndeviation and high retrieval overhead, hindering their deployment. To address\nthese issues, we propose Retrieval-Augmented Recommendation Explanation\nGeneration with Hierarchical Aggregation (REXHA). Specifically, we design a\nhierarchical aggregation based profiling module that comprehensively considers\nuser and item review information, hierarchically summarizing and constructing\nholistic profiles. Furthermore, we introduce an efficient retrieval module\nusing two types of pseudo-document queries to retrieve relevant reviews to\nenhance the generation of recommendation explanations, effectively reducing\nretrieval latency and improving the recall of relevant reviews. Extensive\nexperiments demonstrate that our method outperforms existing approaches by up\nto 12.6% w.r.t. the explanation quality while achieving high retrieval\nefficiency.",
        "url": "http://arxiv.org/abs/2507.09188v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09188v1",
        "arxiv_id": "2507.09188v1",
        "authors": [
            "Bangcheng Sun",
            "Yazhe Chen",
            "Jilin Yang",
            "Xiaodong Li",
            "Hui Li"
        ],
        "submitted": "2025-07-12 08:15:05",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores Explainable Recommender Systems, which is related to Information Retrieval and Search technologies. The use of hierarchical aggregation and pseudo-document queries is innovative, but the focus on recommender systems and explanation generation is not directly aligned with the user's primary interest in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "GraphRunner: A Multi-Stage Framework for Efficient and Accurate Graph-Based Retrieval",
        "abstract": "Conventional Retrieval Augmented Generation (RAG) approaches are common in\ntext-based applications. However, they struggle with structured, interconnected\ndatasets like knowledge graphs, where understanding underlying relationships is\ncrucial for accurate retrieval. A common direction in graph-based retrieval\nemploys iterative, rule-based traversal guided by Large Language Models (LLMs).\nSuch existing iterative methods typically combine reasoning with single hop\ntraversal at each step, making them vulnerable to LLM reasoning errors and\nhallucinations that ultimately hinder the retrieval of relevant information.\n  To address these limitations, we propose GraphRunner, a novel graph-based\nretrieval framework that operates in three distinct stages: planning,\nverification, and execution. This introduces high-level traversal actions that\nenable multi-hop exploration in a single step. It also generates a holistic\ntraversal plan, which is verified against the graph structure and pre-defined\ntraversal actions, reducing reasoning errors and detecting hallucinations\nbefore execution. GraphRunner significantly reduces LLM reasoning errors and\ndetects hallucinations through validation. Our evaluation using the GRBench\ndataset shows that GraphRunner consistently outperforms existing approaches,\nachieving 10-50% performance improvements over the strongest baseline while\nreducing inference cost by 3.0-12.9x and response generation time by 2.5-7.1x,\nmaking it significantly more robust and efficient for graph-based retrieval\ntasks.",
        "url": "http://arxiv.org/abs/2507.08945v1",
        "pdf_url": "http://arxiv.org/pdf/2507.08945v1",
        "arxiv_id": "2507.08945v1",
        "authors": [
            "Savini Kashmira",
            "Jayanaka L. Dantanarayana",
            "Krisztián Flautner",
            "Lingjia Tang",
            "Jason Mars"
        ],
        "submitted": "2025-07-11 18:10:01",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on graph-based retrieval, which is not directly related to the user's primary interest in Information Retrieval (IR) and Search technologies. While the paper mentions query understanding and ranking models, it does not explicitly address these topics. The user's background in e-commerce and interest in real-time relevance optimization are not directly relevant to this paper."
    },
    {
        "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs",
        "abstract": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.",
        "url": "http://arxiv.org/abs/2507.09477v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09477v1",
        "arxiv_id": "2507.09477v1",
        "authors": [
            "Yangning Li",
            "Weizhi Zhang",
            "Yuyao Yang",
            "Wei-Chieh Huang",
            "Yaozu Wu",
            "Junyu Luo",
            "Yuanchen Bei",
            "Henry Peng Zou",
            "Xiao Luo",
            "Yusheng Zhao",
            "Chunkit Chan",
            "Yankai Chen",
            "Zhongfen Deng",
            "Yinghui Li",
            "Hai-Tao Zheng",
            "Dongyuan Li",
            "Renhe Jiang",
            "Ming Zhang",
            "Yangqiu Song",
            "Philip S. Yu"
        ],
        "submitted": "2025-07-13 03:29:41",
        "source": "arxiv",
        "comment": "submitted to ARR May",
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses Retrieval-Augmented Generation (RAG) and its relation to Large Language Models (LLMs), which is a topic in Natural Language Processing (NLP). While it touches on reasoning and retrieval, it does not directly address query understanding, ranking models, or user behavior modeling, which are core interests in Information Retrieval (IR). The paper's focus on LLMs and knowledge-intensive benchmarks is somewhat related to the user's interests, but it does not align with their primary focus on IR and search technologies."
    },
    {
        "title": "Self-Improving Model Steering",
        "abstract": "Model steering represents a powerful technique that dynamically aligns large\nlanguage models (LLMs) with human preferences during inference. However,\nconventional model-steering methods rely heavily on externally annotated data,\nnot only limiting their adaptability to varying contexts but also tethering\ntheir effectiveness to annotation quality. In this paper, we present SIMS, the\nfirst self-improving model-steering framework that operates without relying on\nexternal supervision. At its core, SIMS autonomously generates and refines\ncontrastive samples through iterative self-improvement cycles, enabling\nadaptive, context-specific steering. Additionally, SIMS employs novel\nstrategies, including prompt ranking and contrast sampling, to further enhance\nsteering efficacy. Extensive evaluation across diverse LLMs and benchmarks\ndemonstrates that SIMS substantially outperforms existing methods in steering\neffectiveness and adaptability, highlighting self-improving model steering as a\npromising direction for future research on inference-time LLM alignment.",
        "url": "http://arxiv.org/abs/2507.08967v1",
        "pdf_url": "http://arxiv.org/pdf/2507.08967v1",
        "arxiv_id": "2507.08967v1",
        "authors": [
            "Rongyi Zhu",
            "Yuhui Wang",
            "Tanqiu Jiang",
            "Jiacheng Liang",
            "Ting Wang"
        ],
        "submitted": "2025-07-11 18:52:32",
        "source": "arxiv",
        "comment": "16 pages, 9 figures",
        "score": 5,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper presents a self-improving model-steering framework for large language models, which is related to information retrieval and search technologies. However, the focus is on language models and inference-time alignment, which is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat limited to the user's broader interests in NLP and data mining."
    },
    {
        "title": "Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources",
        "abstract": "Retrieving relevant imagery from vast satellite archives is crucial for\napplications like disaster response and long-term climate monitoring. However,\nmost text-to-image retrieval systems are limited to RGB data, failing to\nexploit the unique physical information captured by other sensors, such as the\nall-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the\nspectral signatures in optical multispectral data. To bridge this gap, we\nintroduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1\nSAR and Sentinel-2 multispectral images paired with structured textual\nannotations for land cover, land use, and crisis events harmonized from\nauthoritative land cover systems (CORINE and Dynamic World) and crisis-specific\nsources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),\na novel framework that uses text as a bridge to align unpaired optical and SAR\nimages into a unified embedding space. Our experiments show that CLOSP achieves\na new state-of-the-art, improving retrieval nDGC by 54% over existing models.\nAdditionally, we find that the unified training strategy overcomes the inherent\ndifficulty of interpreting SAR imagery by transferring rich semantic knowledge\nfrom the optical domain with indirect interaction. Furthermore, GeoCLOSP, which\nintegrates geographic coordinates into our framework, creates a powerful\ntrade-off between generality and specificity: while the CLOSP excels at general\nsemantic tasks, the GeoCLOSP becomes a specialized expert for retrieving\nlocation-dependent crisis events and rare geographic features. This work\nhighlights that the integration of diverse sensor data and geographic context\nis essential for unlocking the full potential of remote sensing archives.",
        "url": "http://arxiv.org/abs/2507.10403v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10403v1",
        "arxiv_id": "2507.10403v1",
        "authors": [
            "Daniele Rege Cambrin",
            "Lorenzo Vaiani",
            "Giuseppe Gallipoli",
            "Luca Cagliero",
            "Paolo Garza"
        ],
        "submitted": "2025-07-14 15:46:56",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on text-to-image retrieval, but not in the context of query understanding, ranking models, or user behavior modeling, which are core aspects of your research interests. While it involves information retrieval and uses a novel framework, the application is specific to remote sensing and does not seem to require deep semantic understanding or real-time relevance optimization."
    },
    {
        "title": "Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey",
        "abstract": "Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have\ndemonstrated impressive performance on complex reasoning tasks like mathematics\nand programming with long Chain-of-Thought (CoT) reasoning sequences\n(slow-thinking), compared with traditional large language models\n(fast-thinking). However, these reasoning models also face a huge challenge\nthat generating unnecessarily lengthy and redundant reasoning chains even for\ntrivial questions. This phenomenon leads to a significant waste of inference\nresources, increases the response time for simple queries, and hinders the\npractical application of LRMs in real-world products. To this end, it is\ncrucial to shorten lengthy reasoning chains and learn adaptive reasoning\nbetween fast and slow thinking based on input difficulty. In this survey, we\nprovide a comprehensive overview of recent progress in concise and adaptive\nthinking for efficient reasoning of LRMs, including methodologies, benchmarks,\nand challenges for future exploration. We hope this survey can help researchers\nquickly understand the landscape of this field and inspire novel adaptive\nthinking ideas to facilitate better usage of LRMs.",
        "url": "http://arxiv.org/abs/2507.09662v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09662v1",
        "arxiv_id": "2507.09662v1",
        "authors": [
            "Jason Zhu",
            "Hongyu Li"
        ],
        "submitted": "2025-07-13 14:51:59",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus is on large reasoning models and concise and adaptive thinking, which is outside the scope of the user's research interests."
    },
    {
        "title": "Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?",
        "abstract": "The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal\nquestion answering is still limited, especially for questions requiring\nextensive, complex legal reasoning. To address these limitations, we introduce\nan approach aligning LLMs toward improved law citation accuracy and better\nresponse quality using Group-Relative Policy Optimization (GRPO). Our approach\nleverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,\nsignificantly reducing computational expenses up to 2.5x compared to large\nlanguage model judges. Experiments on the NitiBench benchmark demonstrate\nsubstantial improvements: GRPO achieves up to 90% citation-F1 gains from the\nbase model and a 31% increase in joint quality metrics over instruction tuning.\nCrucially, our method shows enhanced robustness on complex legal reasoning\ntasks compared to instruction tuning, providing an effective and\nresource-efficient solution for enhancing Thai legal LLMs.",
        "url": "http://arxiv.org/abs/2507.09638v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09638v1",
        "arxiv_id": "2507.09638v1",
        "authors": [
            "Pawitsapak Akarajaradwong",
            "Chompakorn Chaksangchaichot",
            "Pirat Pothavorn",
            "Attapol Thamrongrattanarit-Rutherford",
            "Ekapol Chuangsuwanich",
            "Sarana Nutanong"
        ],
        "submitted": "2025-07-13 14:05:48",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Retrieval-Augmented Generation systems for Thai legal question answering, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and query understanding. While it mentions embeddings and semantic-similarity rewards, the context is specific to legal reasoning and question answering, making it an off-topic paper."
    },
    {
        "title": "NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance",
        "abstract": "General-purpose sentence embedding models often struggle to capture\nspecialized financial semantics, especially in low-resource languages like\nKorean, due to domain-specific jargon, temporal meaning shifts, and misaligned\nbilingual vocabularies. To address these gaps, we introduce NMIXX (Neural\neMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual\nembedding models fine-tuned with 18.8K high-confidence triplets that pair\nin-domain paraphrases, hard negatives derived from a semantic-shift typology,\nand exact Korean-English translations. Concurrently, we release KorFinSTS, a\n1,921-pair Korean financial STS benchmark spanning news, disclosures, research\nreports, and regulations, designed to expose nuances that general benchmarks\nmiss.\n  When evaluated against seven open-license baselines, NMIXX's multilingual\nbge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and\n+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing\nother models by the largest margin, while revealing a modest trade-off in\ngeneral STS performance. Our analysis further shows that models with richer\nKorean token coverage adapt more effectively, underscoring the importance of\ntokenizer design in low-resource, cross-lingual settings. By making both models\nand the benchmark publicly available, we provide the community with robust\ntools for domain-adapted, multilingual representation learning in finance.",
        "url": "http://arxiv.org/abs/2507.09601v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09601v1",
        "arxiv_id": "2507.09601v1",
        "authors": [
            "Hanwool Lee",
            "Sara Yu",
            "Yewon Hwang",
            "Jonghyun Choi",
            "Heejae Ahn",
            "Sungbum Jung",
            "Youngjae Yu"
        ],
        "submitted": "2025-07-13 12:14:57",
        "source": "arxiv",
        "comment": "Under Review",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)",
            "Found 'korea' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on domain-adapted neural embeddings for cross-lingual exploration of finance, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on topics like semantic-shift typology and representation learning, the primary focus on finance and low-resource languages makes it only loosely relevant to the user's research interests."
    },
    {
        "title": "Does UMBRELA Work on Other LLMs?",
        "abstract": "We reproduce the UMBRELA LLM Judge evaluation framework across a range of\nlarge language models (LLMs) to assess its generalizability beyond the original\nstudy. Our investigation evaluates how LLM choice affects relevance assessment\naccuracy, focusing on leaderboard rank correlation and per-label agreement\nmetrics. Results demonstrate that UMBRELA with DeepSeek V3 obtains very\ncomparable performance to GPT-4o (used in original work). For LLaMA-3.3-70B we\nobtain slightly lower performance, which further degrades with smaller LLMs.",
        "url": "http://arxiv.org/abs/2507.09483v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09483v1",
        "arxiv_id": "2507.09483v1",
        "authors": [
            "Naghmeh Farzi",
            "Laura Dietz"
        ],
        "submitted": "2025-07-13 04:05:25",
        "source": "arxiv",
        "comment": "9 pages, 2 figures, accepted to SIGIR 2025",
        "score": 4,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on evaluating the generalizability of a specific evaluation framework (UMBRELA) across different large language models (LLMs), which is not directly related to information retrieval, search technologies, or query understanding. Although it touches on relevance assessment accuracy, the context is not relevant to the user's primary research interests."
    },
    {
        "title": "AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data",
        "abstract": "In decision-making conversations, experts must navigate complex choices and\nmake on-the-spot decisions while engaged in conversation. Although extensive\nhistorical data often exists, the real-time nature of these scenarios makes it\ninfeasible for decision-makers to review and leverage relevant information.\nThis raises an interesting question: What if experts could utilize relevant\npast data in real-time decision-making through insights derived from past data?\nTo explore this, we implemented a conversational user interface, taking\ndoctor-patient interactions as an example use case. Our system continuously\nlistens to the conversation, identifies patient problems and doctor-suggested\nsolutions, and retrieves related data from an embedded dataset, generating\nconcise insights using a pipeline built around a retrieval-based Large Language\nModel (LLM) agent. We evaluated the prototype by embedding Health Canada\ndatasets into a vector database and conducting simulated studies using sample\ndoctor-patient dialogues, showing effectiveness but also challenges, setting\ndirections for the next steps of our work.",
        "url": "http://arxiv.org/abs/2507.09100v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09100v1",
        "arxiv_id": "2507.09100v1",
        "authors": [
            "Mohammad Abolnejadian",
            "Shakiba Amirshahi",
            "Matthew Brehmer",
            "Anamaria Crisan"
        ],
        "submitted": "2025-07-12 00:59:41",
        "source": "arxiv",
        "comment": "7 pages and 4 figures. Proceedings of the 7th ACM Conference on\n  Conversational User Interfaces (CUI '25)",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on augmenting expert decision-making with on-the-fly insights grounded in historical data, which is not directly related to information retrieval, search technologies, or query understanding. Although it involves a conversational user interface and a retrieval-based Large Language Model (LLM) agent, the context and application are quite different from the user's research interests."
    },
    {
        "title": "Infinite Video Understanding",
        "abstract": "The rapid advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have ushered in remarkable progress in video understanding.\nHowever, a fundamental challenge persists: effectively processing and\ncomprehending video content that extends beyond minutes or hours. While recent\nefforts like Video-XL-2 have demonstrated novel architectural solutions for\nextreme efficiency, and advancements in positional encoding such as HoPE and\nVideoRoPE++ aim to improve spatio-temporal understanding over extensive\ncontexts, current state-of-the-art models still encounter significant\ncomputational and memory constraints when faced with the sheer volume of visual\ntokens from lengthy sequences. Furthermore, maintaining temporal coherence,\ntracking complex events, and preserving fine-grained details over extended\nperiods remain formidable hurdles, despite progress in agentic reasoning\nsystems like Deep Video Discovery. This position paper posits that a logical,\nalbeit ambitious, next frontier for multimedia research is Infinite Video\nUnderstanding -- the capability for models to continuously process, understand,\nand reason about video data of arbitrary, potentially never-ending duration. We\nargue that framing Infinite Video Understanding as a blue-sky research\nobjective provides a vital north star for the multimedia, and the wider AI,\nresearch communities, driving innovation in areas such as streaming\narchitectures, persistent memory mechanisms, hierarchical and adaptive\nrepresentations, event-centric reasoning, and novel evaluation paradigms.\nDrawing inspiration from recent work on long/ultra-long video understanding and\nseveral closely related fields, we outline the core challenges and key research\ndirections towards achieving this transformative capability.",
        "url": "http://arxiv.org/abs/2507.09068v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09068v1",
        "arxiv_id": "2507.09068v1",
        "authors": [
            "Dell Zhang",
            "Xiangyu Chen",
            "Jixiang Luo",
            "Mengxi Jia",
            "Changzhi Sun",
            "Ruilong Ren",
            "Jingren Liu",
            "Hao Sun",
            "Xuelong Li"
        ],
        "submitted": "2025-07-11 23:07:04",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'ltr' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on video understanding, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on topics like query understanding and ranking models, the context is different and the paper's scope is broader, making it only loosely relevant to the user's interests."
    },
    {
        "title": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology",
        "abstract": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system\nfor automated scientific synthesis that supports recursive, depth- and\nbreadth-controlled exploration of original research questions -- enhancing\nsearch diversity and nuance in the retrieval of relevant scientific literature.\nUnlike conventional retrieval-augmented generation pipelines, DeepResearch\nenables user-controllable synthesis with transparent reasoning and\nparameter-driven configurability, facilitating high-throughput integration of\ndomain-specific evidence while maintaining analytical rigor. Applied to 49\necological research questions, DeepResearch achieves up to a 21-fold increase\nin source integration and a 14.9-fold rise in sources integrated per 1,000\nwords. High-parameter settings yield expert-level analytical depth and\ncontextual diversity.\n  Source code available at: https://github.com/sciknoworg/deep-research.",
        "url": "http://arxiv.org/abs/2507.10522v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10522v1",
        "arxiv_id": "2507.10522v1",
        "authors": [
            "Jennifer D'Souza",
            "Endres Keno Sander",
            "Andrei Aioanei"
        ],
        "submitted": "2025-07-14 17:47:28",
        "source": "arxiv",
        "comment": "12 pages, 3 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper introduces a novel system for automated scientific synthesis, which is related to information retrieval and search technologies. While it doesn't directly focus on query understanding, ranking models, or user behavior modeling, it explores the integration of domain-specific evidence and maintains analytical rigor, which are relevant to information retrieval. However, the paper's focus on ecology and scientific question answering is not directly aligned with the user's primary research interests in e-commerce and deep semantic understanding."
    },
    {
        "title": "From BERT to Qwen: Hate Detection across architectures",
        "abstract": "Online platforms struggle to curb hate speech without over-censoring\nlegitimate discourse. Early bidirectional transformer encoders made big\nstrides, but the arrival of ultra-large autoregressive LLMs promises deeper\ncontext-awareness. Whether this extra scale actually improves practical\nhate-speech detection on real-world text remains unverified. Our study puts\nthis question to the test by benchmarking both model families, classic encoders\nand next-generation LLMs, on curated corpora of online interactions for\nhate-speech detection (Hate or No Hate).",
        "url": "http://arxiv.org/abs/2507.10468v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10468v1",
        "arxiv_id": "2507.10468v1",
        "authors": [
            "Ariadna Mon",
            "Saúl Fenollosa",
            "Jon Lecumberri"
        ],
        "submitted": "2025-07-14 16:46:30",
        "source": "arxiv",
        "comment": "4 pages, 5 figures. EE-559 Deep Learning course project (Group 11)",
        "score": 3,
        "keyword_reasons": [
            "Found 'ltr' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on hate speech detection using BERT and LLMs, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and query understanding. While it touches on Natural Language Processing, the specific application and methodology are not aligned with the user's primary focus on deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning",
        "abstract": "School dropout is a serious problem in distance learning, where early\ndetection is crucial for effective intervention and student perseverance.\nPredicting student dropout using available educational data is a widely\nresearched topic in learning analytics. Our partner's distance learning\nplatform highlights the importance of integrating diverse data sources,\nincluding socio-demographic data, behavioral data, and sentiment analysis, to\naccurately predict dropout risks. In this paper, we introduce a novel model\nthat combines sentiment analysis of student comments using the Bidirectional\nEncoder Representations from Transformers (BERT) model with socio-demographic\nand behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We\nfine-tuned BERT on student comments to capture nuanced sentiments, which were\nthen merged with key features selected using feature importance techniques in\nXGBoost. Our model was tested on unseen data from the next academic year,\nachieving an accuracy of 84\\%, compared to 82\\% for the baseline model.\nAdditionally, the model demonstrated superior performance in other metrics,\nsuch as precision and F1-score. The proposed method could be a vital tool in\ndeveloping personalized strategies to reduce dropout rates and encourage\nstudent perseverance",
        "url": "http://arxiv.org/abs/2507.10421v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10421v1",
        "arxiv_id": "2507.10421v1",
        "authors": [
            "Meriem Zerkouk",
            "Miloud Mihoubi",
            "Belkacem Chikhaoui"
        ],
        "submitted": "2025-07-14 16:04:34",
        "source": "arxiv",
        "comment": "International Conference on Education and New Learning Technologies\n  (2025)",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on predicting dropout in distance learning using machine learning models, which is not directly related to information retrieval, search technologies, or natural language processing. While it involves sentiment analysis using BERT, the context and application are distinct from the user's research interests."
    },
    {
        "title": "Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation",
        "abstract": "Prompt engineering has proven to be a crucial step in leveraging pretrained\nlarge language models (LLMs) in solving various real-world tasks. Numerous\nsolutions have been proposed that seek to automate prompt engineering by using\nthe model itself to edit prompts. However, the majority of state-of-the-art\napproaches are evaluated on tasks that require minimal prompt templates and on\nvery large and highly capable LLMs. In contrast, solving complex tasks that\nrequire detailed information to be included in the prompt increases the amount\nof text that needs to be optimised. Furthermore, smaller models have been shown\nto be more sensitive to prompt design. To address these challenges, we propose\nan evolutionary search approach to automated discrete prompt optimisation\nconsisting of two phases. In the first phase, grammar-guided genetic\nprogramming is invoked to synthesise prompt-creating programmes by searching\nthe space of programmes populated by function compositions of syntactic,\ndictionary-based and LLM-based prompt-editing functions. In the second phase,\nlocal search is applied to explore the neighbourhoods of best-performing\nprogrammes in an attempt to further fine-tune their performance. Our approach\noutperforms three state-of-the-art prompt optimisation approaches,\nPromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose\nLLMs in four domain-specific challenging tasks. We also illustrate several\nexamples where these benchmark methods suffer relatively severe performance\ndegradation, while our approach improves performance in almost all task-model\ncombinations, only incurring minimal degradation when it does not.",
        "url": "http://arxiv.org/abs/2507.10326v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10326v1",
        "arxiv_id": "2507.10326v1",
        "authors": [
            "Muzhaffar Hazman",
            "Minh-Khoi Pham",
            "Shweta Soundararajan",
            "Goncalo Mordido",
            "Leonardo Custode",
            "David Lynch",
            "Giorgio Cruciata",
            "Yucheng Shi",
            "Hongmeng Song",
            "Wang Chao",
            "Pan Yue",
            "Aleksandar Milenovic",
            "Alexandros Agapitos"
        ],
        "submitted": "2025-07-14 14:34:15",
        "source": "arxiv",
        "comment": "Accepted for Publication at ECAI 2025",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores prompt engineering for large language models, which is related to information retrieval and search technologies. However, the focus on discrete prompt optimisation and grammar-guided evolutionary search is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires",
        "abstract": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape.",
        "url": "http://arxiv.org/abs/2507.10073v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10073v1",
        "arxiv_id": "2507.10073v1",
        "authors": [
            "Simon Münker"
        ],
        "submitted": "2025-07-14 08:59:26",
        "source": "arxiv",
        "comment": "15pages, 1 figure, 2 tables",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on cultural bias in large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on AI systems, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are core aspects of your research interests."
    },
    {
        "title": "SLIF-MR: Self-loop Iterative Fusion of Heterogeneous Auxiliary Information for Multimodal Recommendation",
        "abstract": "Knowledge graphs (KGs) and multimodal item information, which respectively\ncapture relational and attribute features, play a crucial role in improving\nrecommender system accuracy. Recent studies have attempted to integrate them\nvia multimodal knowledge graphs (MKGs) to further enhance recommendation\nperformance. However, existing methods typically freeze the MKG structure\nduring training, which limits the full integration of structural information\nfrom heterogeneous graphs (e.g., KG and user-item interaction graph), and\nresults in sub-optimal performance. To address this challenge, we propose a\nnovel framework, termed Self-loop Iterative Fusion of Heterogeneous Auxiliary\nInformation for Multimodal Recommendation (SLIF-MR), which leverages item\nrepresentations from previous training epoch as feedback signals to dynamically\noptimize the heterogeneous graph structures composed of KG, multimodal item\nfeature graph, and user-item interaction graph. Through this iterative fusion\nmechanism, both user and item representations are refined, thus improving the\nfinal recommendation performance. Specifically, based on the feedback item\nrepresentations, SLIF-MR constructs an item-item correlation graph, then\nintegrated into the establishment process of heterogeneous graphs as additional\nnew structural information in a self-loop manner. Consequently, the internal\nstructures of heterogeneous graphs are updated with the feedback item\nrepresentations during training. Moreover, a semantic consistency learning\nstrategy is proposed to align heterogeneous item representations across\nmodalities. The experimental results show that SLIF-MR significantly\noutperforms existing methods, particularly in terms of accuracy and robustness.",
        "url": "http://arxiv.org/abs/2507.09998v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09998v1",
        "arxiv_id": "2507.09998v1",
        "authors": [
            "Jie Guo",
            "Jiahao Jiang",
            "Ziyuan Guo",
            "Bin Song",
            "Yue Sun"
        ],
        "submitted": "2025-07-14 07:32:16",
        "source": "arxiv",
        "comment": "10 pages,7 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on multimodal recommendation systems, leveraging knowledge graphs and item representations, which is not directly related to information retrieval, query understanding, or ranking models. While it touches on graph-based methods, the primary focus is on recommender systems, which is a secondary interest for the user."
    },
    {
        "title": "MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora",
        "abstract": "Continually updating model-based indexes in generative retrieval with new\ndocuments remains challenging, as full retraining is computationally expensive\nand impractical under resource constraints. We propose MixLoRA-DSI, a novel\nframework that combines an expandable mixture of Low-Rank Adaptation experts\nwith a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead\nof allocating new experts for each new corpus, our proposed expansion strategy\nenables sublinear parameter growth by selectively introducing new experts only\nwhen significant number of OOD documents are detected. Experiments on NQ320k\nand MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update\nbaselines, with minimal parameter overhead and substantially lower training\ncosts.",
        "url": "http://arxiv.org/abs/2507.09924v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09924v1",
        "arxiv_id": "2507.09924v1",
        "authors": [
            "Tuan-Luc Huynh",
            "Thuy-Trang Vu",
            "Weiqing Wang",
            "Trung Le",
            "Dragan Gašević",
            "Yuan-Fang Li",
            "Thanh-Toan Do"
        ],
        "submitted": "2025-07-14 05:04:32",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a novel framework for generative retrieval over dynamic corpora, which is somewhat related to information retrieval and search technologies. However, the focus on generative retrieval and model-based indexes is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is limited to the broader field of information retrieval, but it does not address the user's specific areas of interest."
    },
    {
        "title": "ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning",
        "abstract": "Human emotions are complex, with sarcasm being a subtle and distinctive form.\nDespite progress in sarcasm research, sarcasm generation remains underexplored,\nprimarily due to the overreliance on textual modalities and the neglect of\nvisual cues, as well as the mismatch between image content and sarcastic intent\nin existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm\ngeneration dataset with 4,970 samples, each containing an image, a sarcastic\ntext, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation\nframework that integrates Proximal Policy Optimization (PPO) and contrastive\nlearning. PPO utilizes reward scores from DIP to steer the generation of\nsarcastic texts, while contrastive learning encourages the model to favor\noutputs with higher reward scores. These strategies improve overall generation\nquality and produce texts with more pronounced sarcastic intent. We evaluate\nViSP across five metric sets and find it surpasses all baselines, including\nlarge language models, underscoring their limitations in sarcasm generation.\nFurthermore, we analyze the distributions of Sarcasm Scores and Factual\nIncongruity for both M2SaG and the texts generated by ViSP. The generated texts\nexhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity\n(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic\ncontent than the original dataset. % The dataset and code will be publicly\navailable. Our dataset and code will be released at\n\\textit{https://github.com/wclapply/ViSP}.",
        "url": "http://arxiv.org/abs/2507.09482v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09482v1",
        "arxiv_id": "2507.09482v1",
        "authors": [
            "Changli Wang",
            "Rui Wu",
            "Fang Yin"
        ],
        "submitted": "2025-07-13 04:03:05",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on sarcasm generation with multimodal input, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the primary goal is not to improve search or retrieval, but rather to generate sarcastic content. The techniques and concepts presented are not relevant to the user's core research themes."
    },
    {
        "title": "Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models",
        "abstract": "This study explores the optimization of the DRAGON Longformer base model for\nclinical text classification, specifically targeting the binary classification\nof medical case descriptions. A dataset of 500 clinical cases containing\nstructured medical observations was used, with 400 cases for training and 100\nfor validation. Enhancements to the pre-trained\njoeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter\ntuning, domain-specific preprocessing, and architectural adjustments. Key\nmodifications involved increasing sequence length from 512 to 1024 tokens,\nadjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5\nto 8, and incorporating specialized medical terminology. The optimized model\nachieved notable performance gains: accuracy improved from 72.0% to 85.2%,\nprecision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from\n71.0% to 85.2%. Statistical analysis confirmed the significance of these\nimprovements (p < .001). The model demonstrated enhanced capability in\ninterpreting medical terminology, anatomical measurements, and clinical\nobservations. These findings contribute to domain-specific language model\nresearch and offer practical implications for clinical natural language\nprocessing applications. The optimized model's strong performance across\ndiverse medical conditions underscores its potential for broad use in\nhealthcare settings.",
        "url": "http://arxiv.org/abs/2507.09470v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09470v1",
        "arxiv_id": "2507.09470v1",
        "authors": [
            "Mingchuan Yang",
            "Ziyuan Huang"
        ],
        "submitted": "2025-07-13 03:10:19",
        "source": "arxiv",
        "comment": "29 pages, 5 tables",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on clinical text classification using a pre-trained language model, which is outside your primary areas of interest. While it touches on natural language processing, it is not directly related to your work in IR and search technologies."
    },
    {
        "title": "Correcting the LogQ Correction: Revisiting Sampled Softmax for Large-Scale Retrieval",
        "abstract": "Two-tower neural networks are a popular architecture for the retrieval stage\nin recommender systems. These models are typically trained with a softmax loss\nover the item catalog. However, in web-scale settings, the item catalog is\noften prohibitively large, making full softmax infeasible. A common solution is\nsampled softmax, which approximates the full softmax using a small number of\nsampled negatives.\n  One practical and widely adopted approach is to use in-batch negatives, where\nnegatives are drawn from items in the current mini-batch. However, this\nintroduces a bias: items that appear more frequently in the batch (i.e.,\npopular items) are penalized more heavily.\n  To mitigate this issue, a popular industry technique known as logQ correction\nadjusts the logits during training by subtracting the log-probability of an\nitem appearing in the batch. This correction is derived by analyzing the bias\nin the gradient and applying importance sampling, effectively twice, using the\nin-batch distribution as a proposal distribution. While this approach improves\nmodel quality, it does not fully eliminate the bias.\n  In this work, we revisit the derivation of logQ correction and show that it\noverlooks a subtle but important detail: the positive item in the denominator\nis not Monte Carlo-sampled - it is always present with probability 1. We\npropose a refined correction formula that accounts for this. Notably, our loss\nintroduces an interpretable sample weight that reflects the model's uncertainty\n- the probability of misclassification under the current parameters. We\nevaluate our method on both public and proprietary datasets, demonstrating\nconsistent improvements over the standard logQ correction.",
        "url": "http://arxiv.org/abs/2507.09331v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09331v1",
        "arxiv_id": "2507.09331v1",
        "authors": [
            "Kirill Khrylchenko",
            "Vladimir Baikalov",
            "Sergei Makeev",
            "Artem Matveev",
            "Sergei Liamaev"
        ],
        "submitted": "2025-07-12 16:16:11",
        "source": "arxiv",
        "comment": "Accepted at ACM RecSys 2025. Author's version. To appear in the\n  Proceedings of the 18th ACM Conference on Recommender Systems",
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on improving the performance of two-tower neural networks for large-scale retrieval, specifically addressing the issue of bias in sampled softmax. While it touches on the topic of ranking models, it does not directly relate to query understanding, user behavior modeling, or deep semantic understanding, which are core aspects of your research interests."
    },
    {
        "title": "MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis",
        "abstract": "Visual metaphors of climate change (e.g., melting glaciers depicted as a\nmelting ice grenade) are regarded as valuable tools for addressing the\ncomplexity of environmental challenges. However, few studies have examined\ntheir impact on communication, also due to scattered availability of material.\nHere, we present a novel database of Metaphors of Climate Change in Images\n(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal\nimages and enriched with human ratings. For each image, we collected values of\ndifficulty, efficacy, artistic quality, and emotional arousal from human\nrating, as well as number of tags generated by participants to summarize the\nmessage. Semantic and emotion variables were further derived from the tags via\nNatural Language Processing. Visual metaphors were rated as more difficult to\nunderstand, yet more aesthetically pleasant than literal images, but did not\ndiffer in efficacy and arousal. The latter for visual metaphors, however, was\nhigher in participants with higher Need For Cognition. Furthermore, visual\nmetaphors received more tags, often referring to entities not depicted in the\nimage, and elicited words with more positive valence and greater dominance than\nliteral images. These results evidence the greater cognitive load of visual\nmetaphors, which nevertheless might induce positive effects such as deeper\ncognitive elaboration and abstraction compared to literal stimuli. Furthermore,\nwhile they are not deemed as more effective and arousing, visual metaphors seem\nto generate superior aesthetic appreciation and a more positively valenced\nexperience. Overall, this study contributes to understanding the impact of\nvisual metaphors of climate change both by offering a database for future\nresearch and by elucidating a cost-benefit trade-off to take into account when\nshaping environmental communication.",
        "url": "http://arxiv.org/abs/2507.09225v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09225v1",
        "arxiv_id": "2507.09225v1",
        "authors": [
            "Biagio Scalingi",
            "Chiara Barattieri di San Pietro",
            "Paolo Canal",
            "Valentina Bambini"
        ],
        "submitted": "2025-07-12 09:49:30",
        "source": "arxiv",
        "comment": "27 pages, 5 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'search' (score: +1)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on a specific domain (climate change) and uses Natural Language Processing for semantic and emotion analysis, but it does not address your core research themes."
    },
    {
        "title": "Semantic Source Code Segmentation using Small and Large Language Models",
        "abstract": "Source code segmentation, dividing code into functionally coherent segments,\nis crucial for knowledge retrieval and maintenance in software development.\nWhile enabling efficient navigation and comprehension of large codebases,\nmanual and syntactic analysis approaches have become impractical as\nrepositories grow, especially for low-resource languages like R and their\nresearch domains (e.g., social sciences, psychology).This paper introduces an\nautomated, domain-specific approach for research R code segmentation using\nLarge and Small Language Models (LLMs/SLMs). It presents two novel approaches\nand a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:\nline-by-line analysis with context and range-based segment determination. We\nexperiment with LLMs and fine-tuned SLMs. To support the generalizability of\nour approaches, we also include experiments on Python code from the computer\nscience domain.Our results show that context-based line-by-line analysis is\nsuperior over range-based segmentation.Using smaller language models like\nCodeBERT and an encoder-only version of CodeT5+ are better than their LLM\ncounterparts. Most notably, these two best-performing models did not see R code\nduring pre-training versus the LLMs but were only fine-tuned on 4,130 lines of\nmanually annotated code.",
        "url": "http://arxiv.org/abs/2507.08992v1",
        "pdf_url": "http://arxiv.org/pdf/2507.08992v1",
        "arxiv_id": "2507.08992v1",
        "authors": [
            "Abdelhalim Dahou",
            "Ansgar Scherp",
            "Sebastian Kurten",
            "Brigitte Mathiak",
            "Madhu Chauhan"
        ],
        "submitted": "2025-07-11 19:49:59",
        "source": "arxiv",
        "comment": "18 pages, 4 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on source code segmentation using language models, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves natural language processing, the context is specific to software development and code analysis, which is not a primary interest area."
    },
    {
        "title": "MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking",
        "abstract": "This paper introduces an innovative Applicant Tracking System (ATS) enhanced\nby a novel Robotic process automation (RPA) framework or as further referred to\nas MLAR. Traditional recruitment processes often encounter bottlenecks in\nresume screening and candidate shortlisting due to time and resource\nconstraints. MLAR addresses these challenges employing Large Language Models\n(LLMs) in three distinct layers: extracting key characteristics from job\npostings in the first layer, parsing applicant resume to identify education,\nexperience, skills in the second layer, and similarity matching in the third\nlayer. These features are then matched through advanced semantic algorithms to\nidentify the best candidates efficiently. Our approach integrates seamlessly\ninto existing RPA pipelines, automating resume parsing, job matching, and\ncandidate notifications. Extensive performance benchmarking shows that MLAR\noutperforms the leading RPA platforms, including UiPath and Automation\nAnywhere, in high-volume resume-processing tasks. When processing 2,400\nresumes, MLAR achieved an average processing time of 5.4 seconds per resume,\nreducing processing time by approximately 16.9% compared to Automation Anywhere\nand 17.1% compared to UiPath. These results highlight the potential of MLAR to\ntransform recruitment workflows by providing an efficient, accurate, and\nscalable solution tailored to modern hiring needs.",
        "url": "http://arxiv.org/abs/2507.10472v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10472v1",
        "arxiv_id": "2507.10472v1",
        "authors": [
            "Mohamed T. Younes",
            "Omar Walid",
            "Mai Hassan",
            "Ali Hamdi"
        ],
        "submitted": "2025-07-14 16:53:19",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Robotic Process Automation (RPA) and Applicant Tracking Systems (ATS), which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it employs Large Language Models (LLMs), the application is limited to resume screening and candidate shortlisting, which is not a central match for the user's research themes."
    },
    {
        "title": "Devanagari Handwritten Character Recognition using Convolutional Neural Network",
        "abstract": "Handwritten character recognition is getting popular among researchers\nbecause of its possible applications in facilitating technological search\nengines, social media, recommender systems, etc. The Devanagari script is one\nof the oldest language scripts in India that does not have proper digitization\ntools. With the advancement of computing and technology, the task of this\nresearch is to extract handwritten Hindi characters from an image of Devanagari\nscript with an automated approach to save time and obsolete data. In this\npaper, we present a technique to recognize handwritten Devanagari characters\nusing two deep convolutional neural network layers. This work employs a\nmethodology that is useful to enhance the recognition rate and configures a\nconvolutional neural network for effective Devanagari handwritten text\nrecognition (DHTR). This approach uses the Devanagari handwritten character\ndataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each\nof these classes has 1700 images for training and testing purposes. This\napproach obtains promising results in terms of accuracy by achieving 96.36%\naccuracy in testing and 99.55% in training time.",
        "url": "http://arxiv.org/abs/2507.10398v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10398v1",
        "arxiv_id": "2507.10398v1",
        "authors": [
            "Diksha Mehta",
            "Prateek Mehta"
        ],
        "submitted": "2025-07-14 15:38:42",
        "source": "arxiv",
        "comment": "9 pages, 6 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on handwritten character recognition using convolutional neural networks, which is not related to information retrieval, search technologies, or natural language processing. The paper's application in recommender systems is also not a primary focus of your research."
    },
    {
        "title": "Meanings are like Onions: a Layered Approach to Metaphor Processing",
        "abstract": "Metaphorical meaning is not a flat mapping between concepts, but a complex\ncognitive phenomenon that integrates multiple levels of interpretation. In this\npaper, we propose a stratified model of metaphor processing that treats meaning\nas an onion: a multi-layered structure comprising (1) content analysis, (2)\nconceptual blending, and (3) pragmatic intentionality. This three-dimensional\nframework allows for a richer and more cognitively grounded approach to\nmetaphor interpretation in computational systems. At the first level, metaphors\nare annotated through basic conceptual elements. At the second level, we model\nconceptual combinations, linking components to emergent meanings. Finally, at\nthe third level, we introduce a pragmatic vocabulary to capture speaker intent,\ncommunicative function, and contextual effects, aligning metaphor understanding\nwith pragmatic theories. By unifying these layers into a single formal\nframework, our model lays the groundwork for computational methods capable of\nrepresenting metaphorical meaning beyond surface associations, toward deeper,\nmore context-sensitive reasoning.",
        "url": "http://arxiv.org/abs/2507.10354v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10354v1",
        "arxiv_id": "2507.10354v1",
        "authors": [
            "Silvia Cappa",
            "Anna Sofia Lippolis",
            "Stefano Zoia"
        ],
        "submitted": "2025-07-14 14:56:46",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on metaphor processing, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper mentions computational systems, it does not address query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user."
    },
    {
        "title": "Task-Based Flexible Feature Distillation for LLMs",
        "abstract": "Knowledge Distillation (KD) in general and feature distillation in particular\nare promising techniques for reducing the high computational demand of large\nlanguage models (LLMs). However, traditional feature KD methods typically\nassume that the teacher and the student share the same hidden size, limiting\nthe flexibility of the student's architecture. A common solution to this\nproblem involves training a linear projector to align their feature spaces, but\nthis introduces additional parameters that must be learned from scratch and\noften degrades performance on downstream tasks, especially in generative\nsettings. To address this issue, in this work, we propose a novel task-based\nfeature distillation method that enables knowledge transfer between teacher and\nstudent models with different hidden layer dimensions, without introducing any\nnew parameters. Leveraging the insight that only a subset of LLM components\ncontribute significantly to a specific downstream task, our approach identifies\nthe most task-relevant hidden units in the teacher and directly distills their\nactivations to the student. Our method is flexible and easily integrates with\nother distillation frameworks. Empirical results show consistent improvements\nover prior approaches across diverse tasks, including classification,\ninstruction-following, and summarization, achieving up to a 3\\% performance\ngain over the linear projection baseline.",
        "url": "http://arxiv.org/abs/2507.10155v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10155v1",
        "arxiv_id": "2507.10155v1",
        "authors": [
            "Khouloud Saadi",
            "Di Wang"
        ],
        "submitted": "2025-07-14 11:10:02",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on knowledge distillation in large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions downstream tasks, it does not involve ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest for your research."
    },
    {
        "title": "Fusing Large Language Models with Temporal Transformers for Time Series Forecasting",
        "abstract": "Recently, large language models (LLMs) have demonstrated powerful\ncapabilities in performing various tasks and thus are applied by recent studies\nto time series forecasting (TSF) tasks, which predict future values with the\ngiven historical time series. Existing LLM-based approaches transfer knowledge\nlearned from text data to time series prediction using prompting or fine-tuning\nstrategies. However, LLMs are proficient at reasoning over discrete tokens and\nsemantic patterns but are not initially designed to model continuous numerical\ntime series data. The gaps between text and time series data lead LLMs to\nachieve inferior performance to a vanilla Transformer model that is directly\ntrained on TSF data. However, the vanilla Transformers often struggle to learn\nhigh-level semantic patterns. In this paper, we design a novel\nTransformer-based architecture that complementarily leverages LLMs and vanilla\nTransformers, so as to integrate the high-level semantic representations\nlearned by LLMs into the temporal information encoded by time series\nTransformers, where a hybrid representation is obtained by fusing the\nrepresentations from the LLM and the Transformer. The resulting fused\nrepresentation contains both historical temporal dynamics and semantic\nvariation patterns, allowing our model to predict more accurate future values.\nExperiments on benchmark datasets demonstrate the effectiveness of the proposed\napproach.",
        "url": "http://arxiv.org/abs/2507.10098v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10098v1",
        "arxiv_id": "2507.10098v1",
        "authors": [
            "Chen Su",
            "Yuanhe Tian",
            "Qinyu Liu",
            "Jun Zhang",
            "Yan Song"
        ],
        "submitted": "2025-07-14 09:33:40",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on time series forecasting using large language models and temporal transformers, which is not directly related to information retrieval, search technologies, or query understanding. While it involves transformer-based architectures, the application is in a different domain and does not address ranking models or user behavior modeling."
    },
    {
        "title": "On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model",
        "abstract": "Since Searle's work deconstructing intent and intentionality in the realm of\nphilosophy, the practical meaning of intent has received little attention in\nscience and technology. Intentionality and context are both central to the\nscope of Promise Theory's model of Semantic Spacetime, used as an effective\nTiny Language Model. One can identify themes and concepts from a text, on a low\nlevel (without knowledge of the specific language) by using process coherence\nas a guide. Any agent process can assess superficially a degree of latent\n`intentionality' in data by looking for anomalous multi-scale anomalies and\nassessing the work done to form them. Scale separation can be used to sort\nparts into `intended' content and `ambient context', using the spacetime\ncoherence as a measure. This offers an elementary but pragmatic interpretation\nof latent intentionality for very low computational cost, and without reference\nto extensive training or reasoning capabilities. The process is well within the\nreach of basic organisms as it does not require large scale artificial\nprobabilistic batch processing. The level of concept formation depends,\nhowever, on the memory capacity of the agent.",
        "url": "http://arxiv.org/abs/2507.10000v1",
        "pdf_url": "http://arxiv.org/pdf/2507.10000v1",
        "arxiv_id": "2507.10000v1",
        "authors": [
            "Mark Burgess"
        ],
        "submitted": "2025-07-14 07:34:58",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be unrelated to Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling, which are the core areas of your research interests. The focus on intentionality, knowledge representation, and cognitive agents is outside the scope of your expertise."
    },
    {
        "title": "TextOmics-Guided Diffusion for Hit-like Molecular Generation",
        "abstract": "Hit-like molecular generation with therapeutic potential is essential for\ntarget-specific drug discovery. However, the field lacks heterogeneous data and\nunified frameworks for integrating diverse molecular representations. To bridge\nthis gap, we introduce TextOmics, a pioneering benchmark that establishes\none-to-one correspondences between omics expressions and molecular textual\ndescriptions. TextOmics provides a heterogeneous dataset that facilitates\nmolecular generation through representations alignment. Built upon this\nfoundation, we propose ToDi, a generative framework that jointly conditions on\nomics expressions and molecular textual descriptions to produce biologically\nrelevant, chemically valid, hit-like molecules. ToDi leverages two encoders\n(OmicsEn and TextEn) to capture multi-level biological and semantic\nassociations, and develops conditional diffusion (DiffGen) for controllable\ngeneration. Extensive experiments confirm the effectiveness of TextOmics and\ndemonstrate ToDi outperforms existing state-of-the-art approaches, while also\nshowcasing remarkable potential in zero-shot therapeutic molecular generation.\nSources are available at: https://github.com/hala-ToDi.",
        "url": "http://arxiv.org/abs/2507.09982v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09982v1",
        "arxiv_id": "2507.09982v1",
        "authors": [
            "Hang Yuan",
            "Chen Li",
            "Wenjun Ma",
            "Yuncheng Jiang"
        ],
        "submitted": "2025-07-14 06:56:37",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on molecular generation and drug discovery, which is outside the user's primary focus."
    },
    {
        "title": "Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition",
        "abstract": "Large language models demonstrate the intriguing ability to perform unseen\ntasks via in-context learning. However, it remains unclear what mechanisms\ninside the model drive such task-level generalization. In this work, we\napproach this question through the lens of off-by-one addition (i.e., 1+1=3,\n2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function\nas a second step. Leveraging circuit-style interpretability techniques such as\npath patching, we analyze the models' internal computations behind their\nnotable performance and present three key findings. First, we uncover a\nfunction induction mechanism that explains the model's generalization from\nstandard addition to off-by-one addition. This mechanism resembles the\nstructure of the induction head mechanism found in prior work and elevates it\nto a higher level of abstraction. Second, we show that the induction of the +1\nfunction is governed by multiple attention heads in parallel, each of which\nemits a distinct piece of the +1 function. Finally, we find that this function\ninduction mechanism is reused in a broader range of tasks, including synthetic\ntasks such as shifted multiple-choice QA and algorithmic tasks such as base-8\naddition. Overall, our findings offer deeper insights into how reusable and\ncomposable structures within language models enable task-level generalization.",
        "url": "http://arxiv.org/abs/2507.09875v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09875v1",
        "arxiv_id": "2507.09875v1",
        "authors": [
            "Qinyuan Ye",
            "Robin Jia",
            "Xiang Ren"
        ],
        "submitted": "2025-07-14 03:20:55",
        "source": "arxiv",
        "comment": "Code: https://github.com/INK-USC/function-induction",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on understanding the internal mechanisms of large language models, specifically through the lens of off-by-one addition, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on topics like attention heads and function induction, the paper's primary focus is on interpretability and task generalization, which is not a central match for the user's research interests."
    },
    {
        "title": "Te Ahorré Un Click: A Revised Definition of Clickbait and Detection in Spanish News",
        "abstract": "We revise the definition of clickbait, which lacks current consensus, and\nargue that the creation of a curiosity gap is the key concept that\ndistinguishes clickbait from other related phenomena such as sensationalism and\nheadlines that do not deliver what they promise or diverge from the article.\nTherefore, we propose a new definition: clickbait is a technique for generating\nheadlines and teasers that deliberately omit part of the information with the\ngoal of raising the readers' curiosity, capturing their attention and enticing\nthem to click. We introduce a new approach to clickbait detection datasets\ncreation, by refining the concept limits and annotations criteria, minimizing\nthe subjectivity in the decision as much as possible. Following it, we created\nand release TA1C (for Te Ahorr\\'e Un Click, Spanish for Saved You A Click), the\nfirst open source dataset for clickbait detection in Spanish. It consists of\n3,500 tweets coming from 18 well known media sources, manually annotated and\nreaching a 0.825 Fleiss' K inter annotator agreement. We implement strong\nbaselines that achieve 0.84 in F1-score.",
        "url": "http://arxiv.org/abs/2507.09777v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09777v1",
        "arxiv_id": "2507.09777v1",
        "authors": [
            "Gabriel Mordecki",
            "Guillermo Moncecchi",
            "Javier Couto"
        ],
        "submitted": "2025-07-13 20:19:08",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'click' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on clickbait detection in Spanish news, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on user behavior modeling, it is not in the context of search or e-commerce, and the techniques and datasets proposed are specific to the clickbait detection problem."
    },
    {
        "title": "Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations",
        "abstract": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but they exhibit problems with\nlogical consistency in the output they generate. How can we harness LLMs'\nbroad-coverage parametric knowledge in formal reasoning despite their\ninconsistency? We present a method for directly integrating an LLM into the\ninterpretation function of the formal semantics for a paraconsistent logic. We\nprovide experimental evidence for the feasibility of the method by evaluating\nthe function using datasets created from several short-form factuality\nbenchmarks. Unlike prior work, our method offers a theoretical framework for\nneuro-symbolic reasoning that leverages an LLM's knowledge while preserving the\nunderlying logic's soundness and completeness properties.",
        "url": "http://arxiv.org/abs/2507.09751v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09751v1",
        "arxiv_id": "2507.09751v1",
        "authors": [
            "Bradley P. Allen",
            "Prateek Chhikara",
            "Thomas Macaulay Ferguson",
            "Filip Ilievski",
            "Paul Groth"
        ],
        "submitted": "2025-07-13 19:05:43",
        "source": "arxiv",
        "comment": "29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on\n  Neurosymbolic Learning and Reasoning (NeSy 2025)",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on integrating a large language model into a paraconsistent logic, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on natural language processing, the primary focus is on formal reasoning and logical consistency, which is not a central match for the user's research interests."
    },
    {
        "title": "Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces",
        "abstract": "Understanding the latent space geometry of large language models (LLMs) is\nkey to interpreting their behavior and improving alignment. \\baturay{However,\nit remains unclear to what extent LLMs internally organize representations\nrelated to semantic understanding. To investigate this, we conduct a\nlarge-scale empirical study of hidden states in transformer-based LLMs,\nanalyzing 11 decoder-only models across 6 scientific topics and 12 layers each.\nWe find that high-level semantic information consistently lies in\nlow-dimensional subspaces that form linearly separable representations across\ndistinct domains. This separability becomes more pronounced in deeper layers\nand under prompts that trigger structured reasoning or alignment\nbehaviors$\\unicode{x2013}$even when surface content is unchanged. This geometry\nenables simple yet effective causal interventions in hidden space; for example,\nreasoning patterns like chain-of-thought can be captured by a single vector\ndirection. Together, these findings support the development of geometry-aware\ntools that operate directly on latent representations to detect and mitigate\nharmful or adversarial content, using methods such as transport-based defenses\nthat leverage this separability. As a proof of concept, we demonstrate this\npotential by training a simple MLP classifier as a lightweight latent-space\nguardrail, which detects adversarial and malicious prompts with high precision.",
        "url": "http://arxiv.org/abs/2507.09709v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09709v1",
        "arxiv_id": "2507.09709v1",
        "authors": [
            "Baturay Saglam",
            "Paul Kassianik",
            "Blaine Nelson",
            "Sajana Weerawardhena",
            "Yaron Singer",
            "Amin Karbasi"
        ],
        "submitted": "2025-07-13 17:03:25",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores the latent space geometry of large language models, which is somewhat related to my interests in query understanding and ranking models. However, the focus on language models and their internal organization is not directly aligned with my primary focus on information retrieval and search technologies. The paper's findings on separability and causal interventions in hidden space could potentially be applied to IR, but the connection is not immediately clear."
    },
    {
        "title": "Adapting Definition Modeling for New Languages: A Case Study on Belarusian",
        "abstract": "Definition modeling, the task of generating new definitions for words in\ncontext, holds great prospect as a means to assist the work of lexicographers\nin documenting a broader variety of lects and languages, yet much remains to be\ndone in order to assess how we can leverage pre-existing models for as-of-yet\nunsupported languages. In this work, we focus on adapting existing models to\nBelarusian, for which we propose a novel dataset of 43,150 definitions. Our\nexperiments demonstrate that adapting a definition modeling systems requires\nminimal amounts of data, but that there currently are gaps in what automatic\nmetrics do capture.",
        "url": "http://arxiv.org/abs/2507.09536v1",
        "pdf_url": "http://arxiv.org/pdf/2507.09536v1",
        "arxiv_id": "2507.09536v1",
        "authors": [
            "Daniela Kazakouskaya",
            "Timothee Mickus",
            "Janine Siewert"
        ],
        "submitted": "2025-07-13 08:35:23",
        "source": "arxiv",
        "comment": "To appear at SlavicNLP 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on adapting definition modeling for a new language, Belarusian, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on language-related topics, it does not align with the user's primary research interests in IR, NLP, and data mining."
    }
]