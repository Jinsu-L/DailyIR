# Daily Papers Report - 2025-09-16

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. FinGEAR: Financial Mapping-Guided Enhanced Answer Retrieval

- **LLM Score**: 8
- **Keyword Score**: 15
- **Authors**: Ying Li, Mengyu Wang, Miguel de Carvalho, Sotirios Sabanis, Tiejun Ma
- **URL**: <http://arxiv.org/abs/2509.12042v1>
- **Submitted**: 2025-09-15 15:25:26
- **Topic Keywords**: query, queries, rerank, rag, retrieval, rank, search
- **Reason**: This paper is highly relevant to Information Retrieval, specifically in the area of query understanding and ranking models, with a focus on financial documents. Although it's domain-specific, the techniques and methods described can be applied to other areas of IR, making it a useful read. The use of hierarchical indices and cross-encoder reranker is particularly interesting for its potential to improve retrieval fidelity.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Financial Document Understanding
- **Aim**: Develop a retrieval-first framework, FinGEAR, to answer financial questions using unstructured text data, particularly SEC 10-K filings.
- **Rationale**: Existing methods struggle with the length, hierarchical structure, and domain-specific language of financial documents.
- **Ground**: FinGEAR leverages a financial lexicon, hierarchical document structure, and query-aware guidance to improve retrieval accuracy.
- **Experiment**: FinGEAR is evaluated on the FinQA dataset using precision, recall, F1 score, and downstream task accuracy (answering financial questions with GPT-4o-mini).
- **Takeaway**: FinGEAR outperforms existing retrieval methods, demonstrating the effectiveness of its hybrid approach and highlighting the importance of domain-specific lexicon and hierarchical structure for financial document understanding.

#### Abstract
> Financial disclosures such as 10-K filings present challenging retrieval
problems due to their length, regulatory section hierarchy, and domain-specific
language, which standard retrieval-augmented generation (RAG) models underuse.
We introduce FinGEAR (Financial Mapping-Guided Enhanced Answer Retrieval), a
retrieval framework tailored to financial documents. FinGEAR combines a finance
lexicon for Item-level guidance (FLAM), dual hierarchical indices for
within-Item search (Summary Tree and Question Tree), and a two-stage
cross-encoder reranker. This design aligns retrieval with disclosure structure
and terminology, enabling fine-grained, query-aware context selection.
Evaluated on full 10-Ks with queries aligned to the FinQA dataset, FinGEAR
delivers consistent gains in precision, recall, F1, and relevancy, improving F1
by up to 56.7% over flat RAG, 12.5% over graph-based RAGs, and 217.6% over
prior tree-based systems, while also increasing downstream answer accuracy with
a fixed reader. By jointly modeling section hierarchy and domain lexicon
signals, FinGEAR improves retrieval fidelity and provides a practical
foundation for high-stakes financial analysis.

---

### 2. MillStone: How Open-Minded Are LLMs?

- **LLM Score**: 7
- **Keyword Score**: 7
- **Authors**: Harold Triedman, Vitaly Shmatikov
- **URL**: <http://arxiv.org/abs/2509.11967v2>
- **Submitted**: 2025-09-15 14:18:51
- **Comment**: 19 pages, 7 tables, 7 figures
- **Topic Keywords**: information retrieval, retrieval, web search, search
- **Reason**: This paper explores the interaction between Large Language Models (LLMs) and information retrieval, which is relevant to your interests in Information Retrieval and Search technologies. However, the focus on LLMs and their potential manipulation through source selection is somewhat tangential to your core research themes of query understanding, ranking models, and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Open-mindedness, biases, and vulnerabilities of large language models (LLMs) when presented with arguments and diverse prompts.
- **Aim**: To investigate how LLMs adjust their stances based on presented arguments and explore their susceptibility to manipulation.
- **Rationale**: LLMs are increasingly used for information retrieval and may shape public opinion, making it crucial to understand their response to arguments and potential biases.
- **Ground**: The research utilizes the MILLSTONE benchmark, which presents 107 contentious issues with arguments for both sides, and evaluates nine LLMs across different argument configurations and prompt categories.
- **Experiment**: The authors measure the change in LLM stance output (pro or con) based on presented arguments and calculate a weighted score considering stance flips and argument strength.
- **Takeaway**: LLMs demonstrate varying degrees of open-mindedness and are susceptible to manipulation through carefully crafted arguments.  Findings highlight the need for robust defenses against adversarial manipulation and further research into mitigating biases in LLMs.

#### Abstract
> Large language models equipped with Web search, information retrieval tools,
and other agentic capabilities are beginning to supplant traditional search
engines. As users start to rely on LLMs for information on many topics,
including controversial and debatable issues, it is important to understand how
the stances and opinions expressed in LLM outputs are influenced by the
documents they use as their information sources.
  In this paper, we present MillStone, the first benchmark that aims to
systematically measure the effect of external arguments on the stances that
LLMs take on controversial issues (not all of them political). We apply
MillStone to nine leading LLMs and measure how ``open-minded'' they are to
arguments supporting opposite sides of these issues, whether different LLMs
agree with each other, which arguments LLMs find most persuasive, and whether
these arguments are the same for different LLMs.
  In general, we find that LLMs are open-minded on most issues. An
authoritative source of information can easily sway an LLM's stance,
highlighting the importance of source selection and the risk that LLM-based
information retrieval and search systems can be manipulated.

---

### 3. Query-Focused Extractive Summarization for Sentiment Explanation

- **LLM Score**: 6
- **Keyword Score**: 5
- **Authors**: Ahmed Moubtahij, Sylvie Ratt√©, Yazid Attabi, Maxime Dumas
- **URL**: <http://arxiv.org/abs/2509.11989v1>
- **Submitted**: 2025-09-15 14:41:18
- **Topic Keywords**: query, rag
- **Reason**: This paper explores Query-Focused Summarization, which is related to query understanding in Information Retrieval. However, the focus on sentiment explanation and summarization is somewhat tangential to the user's primary interests in ranking models and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Query-Focused Extractive Summarization (QFS) with a focus on Explicative Sentiment Summarization (ESS)
- **Aim**: To address the Language Register (LR) and Information Content (IC) gaps in QFS by proposing a novel Compound Bias-Focused Summarization (CBFS) framework.
- **Rationale**: The LR gap arises from differences in language style between user queries and source documents, while the IC gap stems from the limited semantic coverage of short queries compared to the detailed semantics in source text.
- **Ground**: Existing QFS models struggle to effectively capture user intent and address these semantic disparities.
- **Experiment**: The authors evaluate their CBFS framework, implemented as the Multi-Bias TextRank (MBTR) model, using ROUGE-SU4 as a metric. They compare MBTR against baseline models (MMR, QuerySum, BTR) and explore the benefits of query expansion techniques.
- **Takeaway**: MBTR significantly outperforms existing QFES models, demonstrating the effectiveness of the CBFS framework. Query expansion techniques further improve performance. Sentiment-based ESS is shown to be more suitable than general QFES when sentiment is a factor.

#### Abstract
> Constructive analysis of feedback from clients often requires determining the
cause of their sentiment from a substantial amount of text documents. To assist
and improve the productivity of such endeavors, we leverage the task of
Query-Focused Summarization (QFS). Models of this task are often impeded by the
linguistic dissonance between the query and the source documents. We propose
and substantiate a multi-bias framework to help bridge this gap at a
domain-agnostic, generic level; we then formulate specialized approaches for
the problem of sentiment explanation through sentiment-based biases and query
expansion. We achieve experimental results outperforming baseline models on a
real-world proprietary sentiment-aware QFS dataset.

---

### 4. Unsupervised Candidate Ranking for Lexical Substitution via Holistic Sentence Semantics

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Zhongyang Hu, Naijie Gu, Xiangzhi Tao, Tianhui Gu, Yibing Zhou
- **URL**: <http://arxiv.org/abs/2509.11513v1>
- **Submitted**: 2025-09-15 01:57:09
- **Topic Keywords**: ranking, rag, rank
- **Reason**: The paper explores ranking models for lexical substitution, which is somewhat related to the user's interests in ranking models and query understanding. However, the focus is on a specific task within NLP, and the paper does not directly address the user's core research themes in Information Retrieval or e-commerce. The use of attention weights and integrated gradients is an interesting approach, but it is not directly applicable to the user's areas of interest.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Lexical Substitution
- **Aim**: Improve candidate ranking in lexical substitution tasks by capturing the bidirectional influence of candidate words on both the target word and its context.
- **Rationale**: Existing methods struggle to capture the semantic context surrounding the target word, leading to suboptimal candidate ranking.
- **Ground**: Leveraging attention weights and Integrated Gradients (IG) to quantify the contribution of each token to the model's prediction of the target word.
- **Experiment**: Evaluated two novel unsupervised methods on benchmark datasets LS07 and SWORDS using pre-trained language models (BERT, SimCSE, MPNet, All-MPNet, DeBERTa-v3) and the Generalized Average Precision (GAP) metric.
- **Takeaway**: Methods incorporating attention weights and IG outperform existing methods by considering the influence of all tokens, highlighting the importance of semantic context for accurate lexical substitution.

#### Abstract
> A key subtask in lexical substitution is ranking the given candidate words. A
common approach is to replace the target word with a candidate in the original
sentence and feed the modified sentence into a model to capture semantic
differences before and after substitution. However, effectively modeling the
bidirectional influence of candidate substitution on both the target word and
its context remains challenging. Existing methods often focus solely on
semantic changes at the target position or rely on parameter tuning over
multiple evaluation metrics, making it difficult to accurately characterize
semantic variation. To address this, we investigate two approaches: one based
on attention weights and another leveraging the more interpretable integrated
gradients method, both designed to measure the influence of context tokens on
the target token and to rank candidates by incorporating semantic similarity
between the original and substituted sentences. Experiments on the LS07 and
SWORDS datasets demonstrate that both approaches improve ranking performance.

---

### 5. AKCIT-FN at CheckThat! 2025: Switching Fine-Tuned SLMs and LLM Prompting for Multilingual Claim Normalization

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Fabrycio Leite Nakano Almada, Kauan Divino Pouso Mariano, Maykon Adriell Dutra, Victor Emanuel da Silva Monteiro, Juliana Resplande Sant'Anna Gomes, Arlindo Rodrigues Galv√£o Filho, Anderson da Silva Soares
- **URL**: <http://arxiv.org/abs/2509.11496v1>
- **Submitted**: 2025-09-15 01:19:49
- **Comment**: 15 pages, 2 figures
- **Topic Keywords**: ranking, rag, rank
- **Reason**: The paper is somewhat related to the user's interests in Natural Language Processing (NLP) and Information Retrieval (IR), particularly in the context of claim normalization and fact-checking pipelines. However, it does not directly address the user's core research themes of query understanding, ranking models, or user behavior modeling. The paper's focus on multilingual claim normalization and Large Language Model (LLM) prompting is an interesting aspect, but it does not align closely with the user's primary research interests.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Multilingual Claim Normalization for Automated Fact-Checking
- **Aim**: To develop a system for transforming informal social media posts into concise, verifiable claims across twenty languages, addressing both monolingual and zero-shot settings.
- **Rationale**: Accurate claim normalization is crucial for automated fact-checking, enabling the system to understand and verify information from diverse sources.
- **Ground**: The research utilizes the CLEF-2025 CheckThat! Task 2 dataset, which includes original social media posts and their corresponding normalized claims across twenty languages.
- **Experiment**: The team employed a dual-strategy approach: fine-tuning Small Language Models (SLMs) for monolingual languages and leveraging Large Language Models (LLMs) with few-shot prompting or zero-shot inference for zero-shot languages. Key steps involved data preprocessing, sentence embedding generation, and prompt engineering.
- **Takeaway**: Both fine-tuned SLMs and zero-shot LLMs demonstrate strong performance in multilingual claim normalization. This research highlights the potential of these models for advancing automated fact-checking systems.

#### Abstract
> Claim normalization, the transformation of informal social media posts into
concise, self-contained statements, is a crucial step in automated
fact-checking pipelines. This paper details our submission to the CLEF-2025
CheckThat! Task~2, which challenges systems to perform claim normalization
across twenty languages, divided into thirteen supervised (high-resource) and
seven zero-shot (no training data) tracks.
  Our approach, leveraging fine-tuned Small Language Models (SLMs) for
supervised languages and Large Language Model (LLM) prompting for zero-shot
scenarios, achieved podium positions (top three) in fifteen of the twenty
languages. Notably, this included second-place rankings in eight languages,
five of which were among the seven designated zero-shot languages, underscoring
the effectiveness of our LLM-based zero-shot strategy. For Portuguese, our
initial development language, our system achieved an average METEOR score of
0.5290, ranking third. All implementation artifacts, including inference,
training, evaluation scripts, and prompt configurations, are publicly available
at https://github.com/ju-resplande/checkthat2025_normalization.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. A Dynamic Knowledge Update-Driven Model with Large Language Models for Fake News Detection

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Di Jin, Jun Yang, Xiaobao Wang, Junwei Zhang, Shuqi Li, Dongxiao He
- **URL**: <http://arxiv.org/abs/2509.11687v1>
- **Submitted**: 2025-09-15 08:38:08
- **Topic Keywords**: rag, retrieval, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, as it involves fake news detection and knowledge graph construction. However, the focus on fake news detection and large language models is not directly aligned with your primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> As the Internet and social media evolve rapidly, distinguishing credible news
from a vast amount of complex information poses a significant challenge. Due to
the suddenness and instability of news events, the authenticity labels of news
can potentially shift as events develop, making it crucial for fake news
detection to obtain the latest event updates. Existing methods employ
retrieval-augmented generation to fill knowledge gaps, but they suffer from
issues such as insufficient credibility of retrieved content and interference
from noisy information. We propose a dynamic knowledge update-driven model for
fake news detection (DYNAMO), which leverages knowledge graphs to achieve
continuous updating of new knowledge and integrates with large language models
to fulfill dual functions: news authenticity detection and verification of new
knowledge correctness, solving the two key problems of ensuring the
authenticity of new knowledge and deeply mining news semantics. Specifically,
we first construct a news-domain-specific knowledge graph. Then, we use Monte
Carlo Tree Search to decompose complex news and verify them step by step.
Finally, we extract and update new knowledge from verified real news texts and
reasoning paths. Experimental results demonstrate that DYNAMO achieves the best
performance on two real-world datasets.

### 7. Analyzing Information-Seeking Behaviors in a Hakka AI Chatbot: A Cognitive-Pragmatic Study

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Chu-Hsuan Lee, Chen-Chi Chang, Hung-Shin Lee, Yun-Hsiang Hsu, Ching-Yuan Chen
- **URL**: <http://arxiv.org/abs/2509.11591v1>
- **Submitted**: 2025-09-15 05:18:17
- **Comment**: Accepted to HICSS-59 (2026)
- **Topic Keywords**: rag, user behavior
- **Reason**: This paper is somewhat related to Information Retrieval, but its focus on AI-assisted language learning and language preservation is not a central match to your primary research interests in query understanding, ranking models, and user behavior modeling. However, the study's use of a generative AI chatbot and analysis of user behaviors may provide some insights into user behavior modeling, which is a related topic.

#### Abstract
> With many endangered languages at risk of disappearing, efforts to preserve
them now rely more than ever on using technology alongside culturally informed
teaching strategies. This study examines user behaviors in TALKA, a generative
AI-powered chatbot designed for Hakka language engagement, by employing a
dual-layered analytical framework grounded in Bloom's Taxonomy of cognitive
processes and dialogue act categorization. We analyzed 7,077 user utterances,
each carefully annotated according to six cognitive levels and eleven dialogue
act types. These included a variety of functions, such as asking for
information, requesting translations, making cultural inquiries, and using
language creatively. Pragmatic classifications further highlight how different
types of dialogue acts--such as feedback, control commands, and social
greetings--align with specific cognitive intentions. The results suggest that
generative AI chatbots can support language learning in meaningful
ways--especially when they are designed with an understanding of how users
think and communicate. They may also help learners express themselves more
confidently and connect with their cultural identity. The TALKA case provides
empirical insights into how AI-mediated dialogue facilitates cognitive
development in low-resource language learners, as well as pragmatic negotiation
and socio-cultural affiliation. By focusing on AI-assisted language learning,
this study offers new insights into how technology can support language
preservation and educational practice.

### 8. HiChunk: Evaluating and Enhancing Retrieval-Augmented Generation with Hierarchical Chunking

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Wensheng Lu, Keyu Chen, Ruizhi Qiao, Xing Sun
- **URL**: <http://arxiv.org/abs/2509.11552v2>
- **Submitted**: 2025-09-15 03:32:50
- **Comment**: 17 pages, 5 figures, 6 tables
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper focuses on Retrieval-Augmented Generation (RAG) and proposes a new evaluation framework (HiCBench) and a document structuring framework (HiChunk). While it touches on information retrieval, its primary focus is on language models and generation, which is somewhat related to the user's interests in query understanding and ranking models, but not a central match.

#### Abstract
> Retrieval-Augmented Generation (RAG) enhances the response capabilities of
language models by integrating external knowledge sources. However, document
chunking as an important part of RAG system often lacks effective evaluation
tools. This paper first analyzes why existing RAG evaluation benchmarks are
inadequate for assessing document chunking quality, specifically due to
evidence sparsity. Based on this conclusion, we propose HiCBench, which
includes manually annotated multi-level document chunking points, synthesized
evidence-dense quetion answer(QA) pairs, and their corresponding evidence
sources. Additionally, we introduce the HiChunk framework, a multi-level
document structuring framework based on fine-tuned LLMs, combined with the
Auto-Merge retrieval algorithm to improve retrieval quality. Experiments
demonstrate that HiCBench effectively evaluates the impact of different
chunking methods across the entire RAG pipeline. Moreover, HiChunk achieves
better chunking quality within reasonable time consumption, thereby enhancing
the overall performance of RAG systems.

### 9. CEMTM: Contextual Embedding-based Multimodal Topic Modeling

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Amirhossein Abaskohi, Raymond Li, Chuyuan Li, Shafiq Joty, Giuseppe Carenini
- **URL**: <http://arxiv.org/abs/2509.11465v1>
- **Submitted**: 2025-09-14 23:07:46
- **Comment**: EMNLP 2025
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper introduces a multimodal topic model that leverages contextualized embeddings and attention mechanisms for topic inference. While it touches on aspects of information retrieval and semantic understanding, its primary focus is on multimodal topic modeling, which is somewhat related to your interests in query understanding and ranking models. However, the paper's emphasis on multimodal processing and visual semantics is not a central match for your research themes.

#### Abstract
> We introduce CEMTM, a context-enhanced multimodal topic model designed to
infer coherent and interpretable topic structures from both short and long
documents containing text and images. CEMTM builds on fine-tuned large vision
language models (LVLMs) to obtain contextualized embeddings, and employs a
distributional attention mechanism to weight token-level contributions to topic
inference. A reconstruction objective aligns topic-based representations with
the document embedding, encouraging semantic consistency across modalities.
Unlike existing approaches, CEMTM can process multiple images per document
without repeated encoding and maintains interpretability through explicit
word-topic and document-topic distributions. Extensive experiments on six
multimodal benchmarks show that CEMTM consistently outperforms unimodal and
multimodal baselines, achieving a remarkable average LLM score of 2.61. Further
analysis shows its effectiveness in downstream few-shot retrieval and its
ability to capture visually grounded semantics in complex domains such as
scientific articles.

### 10. From Fuzzy Speech to Medical Insight: Benchmarking LLMs on Noisy Patient Narratives

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Eden Mama, Liel Sheri, Yehudit Aperstein, Alexander Apartsin
- **URL**: <http://arxiv.org/abs/2509.11803v1>
- **Submitted**: 2025-09-15 11:34:46
- **Comment**: 6 pages, 1 figure
- **Topic Keywords**: ctr, search
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and deep semantic understanding, but it is not directly focused on Information Retrieval (IR) or Search technologies. The paper explores the application of LLMs in healthcare, which is an area that may be tangentially related to your e-commerce background, but it does not appear to be a central match for your primary research themes.

#### Abstract
> The widespread adoption of large language models (LLMs) in healthcare raises
critical questions about their ability to interpret patient-generated
narratives, which are often informal, ambiguous, and noisy. Existing benchmarks
typically rely on clean, structured clinical text, offering limited insight
into model performance under realistic conditions. In this work, we present a
novel synthetic dataset designed to simulate patient self-descriptions
characterized by varying levels of linguistic noise, fuzzy language, and
layperson terminology. Our dataset comprises clinically consistent scenarios
annotated with ground-truth diagnoses, spanning a spectrum of communication
clarity to reflect diverse real-world reporting styles. Using this benchmark,
we fine-tune and evaluate several state-of-the-art models (LLMs), including
BERT-based and encoder-decoder T5 models. To support reproducibility and future
research, we release the Noisy Diagnostic Benchmark (NDB), a structured dataset
of noisy, synthetic patient descriptions designed to stress-test and compare
the diagnostic capabilities of large language models (LLMs) under realistic
linguistic conditions. We made the benchmark available for the community:
https://github.com/lielsheri/PatientSignal

### 11. When Curiosity Signals Danger: Predicting Health Crises Through Online Medication Inquiries

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Dvora Goncharok, Arbel Shifman, Alexander Apartsin, Yehudit Aperstein
- **URL**: <http://arxiv.org/abs/2509.11802v1>
- **Submitted**: 2025-09-15 11:31:25
- **Comment**: 5 pages, 2 figures
- **Topic Keywords**: rag, search
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and data mining, as it involves text classification and the use of large language models. However, the focus on health crises and medication inquiries is not directly aligned with your primary interest in Information Retrieval, especially in e-commerce or real-time relevance optimization.

#### Abstract
> Online medical forums are a rich and underutilized source of insight into
patient concerns, especially regarding medication use. Some of the many
questions users pose may signal confusion, misuse, or even the early warning
signs of a developing health crisis. Detecting these critical questions that
may precede severe adverse events or life-threatening complications is vital
for timely intervention and improving patient safety. This study introduces a
novel annotated dataset of medication-related questions extracted from online
forums. Each entry is manually labelled for criticality based on clinical risk
factors. We benchmark the performance of six traditional machine learning
classifiers using TF-IDF textual representations, alongside three
state-of-the-art large language model (LLM)-based classification approaches
that leverage deep contextual understanding. Our results highlight the
potential of classical and modern methods to support real-time triage and alert
systems in digital health spaces. The curated dataset is made publicly
available to encourage further research at the intersection of
patient-generated data, natural language processing, and early warning systems
for critical health events. The dataset and benchmark are available at:
https://github.com/Dvora-coder/LLM-Medication-QA-Risk-Classifier-MediGuard.

### 12. User eXperience Perception Insights Dataset (UXPID): Synthetic User Feedback from Public Industrial Forums

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Mikhail Kulyabin, Jan Joosten, Choro Ulan uulu, Nuno Miguel Martins Pacheco, Fabian Ries, Filippos Petridis, Jan Bosch, Helena Holmstr√∂m Olsson
- **URL**: <http://arxiv.org/abs/2509.11777v1>
- **Submitted**: 2025-09-15 10:58:41
- **Topic Keywords**: rag, search
- **Reason**: This paper presents a dataset for user experience analysis in industrial forums, which is somewhat related to information retrieval and user behavior modeling. However, the focus is on data analysis and annotation rather than query understanding, ranking models, or real-time relevance optimization, making it less directly relevant to your core research interests.

#### Abstract
> Customer feedback in industrial forums reflect a rich but underexplored
source of insight into real-world product experience. These publicly shared
discussions offer an organic view of user expectations, frustrations, and
success stories shaped by the specific contexts of use. Yet, harnessing this
information for systematic analysis remains challenging due to the unstructured
and domain-specific nature of the content. The lack of structure and
specialized vocabulary makes it difficult for traditional data analysis
techniques to accurately interpret, categorize, and quantify the feedback,
thereby limiting its potential to inform product development and support
strategies. To address these challenges, this paper presents the User
eXperience Perception Insights Dataset (UXPID), a collection of 7130
artificially synthesized and anonymized user feedback branches extracted from a
public industrial automation forum. Each JavaScript object notation (JSON)
record contains multi-post comments related to specific hardware and software
products, enriched with metadata and contextual conversation data. Leveraging a
large language model (LLM), each branch is systematically analyzed and
annotated for UX insights, user expectations, severity and sentiment ratings,
and topic classifications. The UXPID dataset is designed to facilitate research
in user requirements, user experience (UX) analysis, and AI-driven feedback
processing, particularly where privacy and licensing restrictions limit access
to real-world data. UXPID supports the training and evaluation of
transformer-based models for tasks such as issue detection, sentiment analysis,
and requirements extraction in the context of technical forums.

### 13. Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language Models

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Pu Jian, Junhong Wu, Wei Sun, Chen Wang, Shuo Ren, Jiajun Zhang
- **URL**: <http://arxiv.org/abs/2509.12132v1>
- **Submitted**: 2025-09-15 16:57:25
- **Comment**: EMNLP2025 Main
- **Topic Keywords**: rag
- **Reason**: The paper explores visual reflection in vision-language models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on visual reflection and reinforcement learning is not directly aligned with the user's core research themes. The paper's relevance is limited to the broader context of deep semantic understanding, but it does not specifically address the user's areas of interest.

#### Abstract
> Recent advances in text-only "slow-thinking" reasoning have prompted efforts
to transfer this capability to vision-language models (VLMs), for training
visual reasoning models (\textbf{VRMs}). owever, such transfer faces critical
challenges: Effective "slow thinking" in VRMs requires \textbf{visual
reflection}, the ability to check the reasoning process based on visual
information. Through quantitative analysis, we observe that current VRMs
exhibit limited visual reflection, as their attention to visual information
diminishes rapidly with longer generated responses. To address this challenge,
we propose a new VRM \textbf{Reflection-V}, which enhances visual reflection
based on reasoning data construction for cold-start and reward design for
reinforcement learning (RL). Firstly, we construct vision-centered reasoning
data by leveraging an agent that interacts between VLMs and reasoning LLMs,
enabling cold-start learning of visual reflection patterns. Secondly, a visual
attention based reward model is employed during RL to encourage reasoning based
on visual information. Therefore, \textbf{Reflection-V} demonstrates
significant improvements across multiple visual reasoning benchmarks.
Furthermore, \textbf{Reflection-V} maintains a stronger and more consistent
reliance on visual information during visual reasoning, indicating effective
enhancement in visual reflection capabilities.

### 14. Is 'Hope' a person or an idea? A pilot benchmark for NER: comparing traditional NLP tools and large language models on ambiguous entities

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Payam Latifi
- **URL**: <http://arxiv.org/abs/2509.12098v1>
- **Submitted**: 2025-09-15 16:21:59
- **Comment**: 14 pages, 9 figures, 2 tables. This is a pilot study evaluating six
  NER systems -- three traditional tools (NLTK, spaCy, Stanza) and three LLMs
  (Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B) -- on a small, ambiguity-rich
  dataset of 119 tokens. The annotated dataset, prompts are provided in
  appendices for full reproducibility. All experiments were conducted on 14 May
  2025
- **Topic Keywords**: rag
- **Reason**: The paper focuses on Named Entity Recognition (NER), a subfield of NLP, and compares traditional NLP tools with large language models. While it touches on deep semantic understanding, the primary focus is on NER rather than information retrieval. The paper's relevance to the user's interests is somewhat related, but not a central match.

#### Abstract
> This pilot study presents a small-scale but carefully annotated benchmark of
Named Entity Recognition (NER) performance across six systems: three non-LLM
NLP tools (NLTK, spaCy, Stanza) and three general-purpose large language models
(LLMs: Gemini-1.5-flash, DeepSeek-V3, Qwen-3-4B). The dataset contains 119
tokens covering five entity types (PERSON, LOCATION, ORGANIZATION, DATE, TIME).
We evaluated each system's output against the manually annotated gold standard
dataset using F1-score. The results show that LLMs generally outperform
conventional tools in recognizing context-sensitive entities like person names,
with Gemini achieving the highest average F1-score. However, traditional
systems like Stanza demonstrate greater consistency in structured tags such as
LOCATION and DATE. We also observed variability among LLMs, particularly in
handling temporal expressions and multi-word organizations. Our findings
highlight that while LLMs offer improved contextual understanding, traditional
tools remain competitive in specific tasks, informing model selection.

### 15. ToolRM: Outcome Reward Models for Tool-Calling Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Mayank Agarwal, Ibrahim Abdelaziz, Kinjal Basu, Merve Unuvar, Luis A. Lastras, Yara Rizk, Pavan Kapanipathi
- **URL**: <http://arxiv.org/abs/2509.11963v1>
- **Submitted**: 2025-09-15 14:17:17
- **Topic Keywords**: rag
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Search technologies, as it involves large language models and reward modeling. However, the focus on tool-calling and external tool interaction is not directly aligned with your primary interests in query understanding, ranking models, and user behavior modeling. While it touches on NLP, it's more focused on model evaluation and fine-tuning rather than deep semantic understanding or real-time relevance optimization.

#### Abstract
> As large language models (LLMs) increasingly interact with external tools,
reward modeling for tool use has become a critical yet underexplored area.
Existing reward models, trained primarily on natural language outputs, struggle
to evaluate tool-based reasoning and execution. To quantify this gap, we
introduce FC-RewardBench, the first benchmark designed to systematically assess
reward models' performance in tool-calling scenarios. Our analysis shows that
current reward models often miss key signals of effective tool use,
highlighting the need for domain-specific modeling. To address this, we propose
a training framework for outcome-based reward models using data synthesized
from permissively licensed, open-weight LLMs. We train models ranging from 1.7B
to 14B parameters and evaluate them across seven out-of-domain benchmarks.
These models consistently outperform general-purpose baselines, achieving up to
25\% average improvement in downstream task performance and enabling
data-efficient fine-tuning through reward-guided filtering.

### 16. Bhaasha, Bhasa, Zaban: A Survey for Low-Resourced Languages in South Asia -- Current Stage and Challenges

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Sampoorna Poria, Xiaolei Huang
- **URL**: <http://arxiv.org/abs/2509.11570v1>
- **Submitted**: 2025-09-15 04:31:22
- **Topic Keywords**: rag
- **Reason**: This paper is somewhat related to the user's interests in Natural Language Processing (NLP), but it focuses on low-resourced languages in South Asia, which is not a central match for the user's primary focus on information retrieval, especially in areas requiring deep semantic understanding and real-time relevance optimization.

#### Abstract
> Rapid developments of large language models have revolutionized many NLP
tasks for English data. Unfortunately, the models and their evaluations for
low-resource languages are being overlooked, especially for languages in South
Asia. Although there are more than 650 languages in South Asia, many of them
either have very limited computational resources or are missing from existing
language models. Thus, a concrete question to be answered is: Can we assess the
current stage and challenges to inform our NLP community and facilitate model
developments for South Asian languages? In this survey, we have comprehensively
examined current efforts and challenges of NLP models for South Asian languages
by retrieving studies since 2020, with a focus on transformer-based models,
such as BERT, T5, & GPT. We present advances and gaps across 3 essential
aspects: data, models, & tasks, such as available data sources, fine-tuning
strategies, & domain applications. Our findings highlight substantial issues,
including missing data in critical domains (e.g., health), code-mixing, and
lack of standardized evaluation benchmarks. Our survey aims to raise awareness
within the NLP community for more targeted data curation, unify benchmarks
tailored to cultural and linguistic nuances of South Asia, and encourage an
equitable representation of South Asian languages. The complete list of
resources is available at: https://github.com/trust-nlp/LM4SouthAsia-Survey.

### 17. D$^2$HScore: Reasoning-Aware Hallucination Detection via Semantic Breadth and Depth Analysis in LLMs

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Yue Ding, Xiaofang Zhu, Tianze Xia, Junfei Wu, Xinlong Chen, Qiang Liu, Liang Wang
- **URL**: <http://arxiv.org/abs/2509.11569v1>
- **Submitted**: 2025-09-15 04:28:38
- **Comment**: under review
- **Topic Keywords**: rag
- **Reason**: The paper focuses on hallucination detection in Large Language Models (LLMs), which is related to query understanding and deep semantic understanding in Information Retrieval. However, the specific application domain and methodology are not directly aligned with the user's primary research interests.

#### Abstract
> Although large Language Models (LLMs) have achieved remarkable success, their
practical application is often hindered by the generation of non-factual
content, which is called "hallucination". Ensuring the reliability of LLMs'
outputs is a critical challenge, particularly in high-stakes domains such as
finance, security, and healthcare. In this work, we revisit hallucination
detection from the perspective of model architecture and generation dynamics.
Leveraging the multi-layer structure and autoregressive decoding process of
LLMs, we decompose hallucination signals into two complementary dimensions: the
semantic breadth of token representations within each layer, and the semantic
depth of core concepts as they evolve across layers. Based on this insight, we
propose \textbf{D$^2$HScore (Dispersion and Drift-based Hallucination Score)},
a training-free and label-free framework that jointly measures: (1)
\textbf{Intra-Layer Dispersion}, which quantifies the semantic diversity of
token representations within each layer; and (2) \textbf{Inter-Layer Drift},
which tracks the progressive transformation of key token representations across
layers. To ensure drift reflects the evolution of meaningful semantics rather
than noisy or redundant tokens, we guide token selection using attention
signals. By capturing both the horizontal and vertical dynamics of
representation during inference, D$^2$HScore provides an interpretable and
lightweight proxy for hallucination detection. Extensive experiments across
five open-source LLMs and five widely used benchmarks demonstrate that
D$^2$HScore consistently outperforms existing training-free baselines.

### 18. A Transformer-Based Cross-Platform Analysis of Public Discourse on the 15-Minute City Paradigm

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Gaurab Chhetri, Darrell Anderson, Boniphace Kutela, Subasish Das
- **URL**: <http://arxiv.org/abs/2509.11443v1>
- **Submitted**: 2025-09-14 21:36:24
- **Comment**: This is the author's preprint version of a paper accepted for
  presentation at the 24th International Conference on Machine Learning and
  Applications (ICMLA 2025), December 3-5, 2025, Florida, USA. The final
  published version will appear in the official IEEE proceedings. Conference
  site: https://www.icmla-conference.org/icmla25/
- **Topic Keywords**: ctr
- **Reason**: This paper explores sentiment analysis on social media platforms using transformer-based models, which is somewhat related to information retrieval and NLP. However, the focus on sentiment analysis and urban planning discourse is not directly aligned with the user's core research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> This study presents the first multi-platform sentiment analysis of public
opinion on the 15-minute city concept across Twitter, Reddit, and news media.
Using compressed transformer models and Llama-3-8B for annotation, we classify
sentiment across heterogeneous text domains. Our pipeline handles long-form and
short-form text, supports consistent annotation, and enables reproducible
evaluation. We benchmark five models (DistilRoBERTa, DistilBERT, MiniLM,
ELECTRA, TinyBERT) using stratified 5-fold cross-validation, reporting
F1-score, AUC, and training time. DistilRoBERTa achieved the highest F1
(0.8292), TinyBERT the best efficiency, and MiniLM the best cross-platform
consistency. Results show News data yields inflated performance due to class
imbalance, Reddit suffers from summarization loss, and Twitter offers moderate
challenge. Compressed models perform competitively, challenging assumptions
that larger models are necessary. We identify platform-specific trade-offs and
propose directions for scalable, real-world sentiment classification in urban
planning discourse.

### 19. Designing LLMs for cultural sensitivity: Evidence from English-Japanese translation

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Helene Tenzer, Oumnia Abidi, Stefan Feuerriegel
- **URL**: <http://arxiv.org/abs/2509.11921v1>
- **Submitted**: 2025-09-15 13:37:35
- **Topic Keywords**: recommend
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and Large Language Models (LLMs), but it focuses on cultural sensitivity and translation, which is not a central match to your primary focus on information retrieval and query understanding.

#### Abstract
> Large language models (LLMs) are increasingly used in everyday communication,
including multilingual interactions across different cultural contexts. While
LLMs can now generate near-perfect literal translations, it remains unclear
whether LLMs support culturally appropriate communication. In this paper, we
analyze the cultural sensitivity of different LLM designs when applied to
English-Japanese translations of workplace e-mails. Here, we vary the prompting
strategies: (1) naive "just translate" prompts, (2) audience-targeted prompts
specifying the recipient's cultural background, and (3) instructional prompts
with explicit guidance on Japanese communication norms. Using a mixed-methods
study, we then analyze culture-specific language patterns to evaluate how well
translations adapt to cultural norms. Further, we examine the appropriateness
of the tone of the translations as perceived by native speakers. We find that
culturally-tailored prompting can improve cultural fit, based on which we offer
recommendations for designing culturally inclusive LLMs in multilingual
settings.

### 20. Decoding in Latent Spaces for Efficient Inference in LLM-based Recommendation

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Chengbing Wang, Yang Zhang, Zhicheng Wang, Tianhao Shi, Keqin Bao, Fuli Feng, Tat-Seng Chua
- **URL**: <http://arxiv.org/abs/2509.11524v1>
- **Submitted**: 2025-09-15 02:30:35
- **Comment**: Accepted for publication in EMNLP'25
- **Topic Keywords**: recommend
- **Reason**: The paper explores a novel approach to recommendation systems using large language models, but its focus on efficient inference and latent-space decoding is somewhat tangential to the user's core research interests in Information Retrieval, query understanding, and ranking models. While it touches on related topics like NLP and data mining, the paper's primary contribution is in the recommender systems domain, which is not the user's primary focus. The paper's relevance is somewhat mitigated by its lack of direct connection to the user's areas of expertise.

#### Abstract
> Fine-tuning large language models (LLMs) for recommendation in a generative
manner has delivered promising results, but encounters significant inference
overhead due to autoregressive decoding in the language space. This work
explores bypassing language-space decoding by directly matching candidate items
with the LLM's internal thought representations in the latent space,
eliminating the time-consuming autoregressive process to reduce computational
costs. Towards this, we introduce Light Latent-space Decoding (L2D), an
effective and efficient latent-space decoding method. L2D represents
user-preferred items by using the hidden states of test sequences reflecting
the LLM's internal thought, and obtains candidate item representations from the
hidden states of training sequences labeled with the corresponding candidate
items. It then matches the two types of representations to decode items,
achieving latent-space decoding. In this way, it enables efficient decoding
without altering the LLM's generative tuning paradigm, thereby preserving
performance. Extensive empirical results demonstrate that L2D is more than 10x
faster than language-space decoding while maintaining or enhancing performance.

### 21. Transformer Enhanced Relation Classification: A Comparative Analysis of Contextuality, Data Efficiency and Sequence Complexity

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Bowen Jing, Yang Cui, Tianpeng Huang
- **URL**: <http://arxiv.org/abs/2509.11374v1>
- **Submitted**: 2025-09-14 18:11:31
- **Topic Keywords**: search
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and deep semantic understanding, but it primarily focuses on relation classification and large language models, which is not a central match to your core interests in Information Retrieval and Search technologies.

#### Abstract
> In the era of large language model, relation extraction (RE) plays an
important role in information extraction through the transformation of
unstructured raw text into structured data (Wadhwa et al., 2023). In this
paper, we systematically compare the performance of deep supervised learning
approaches without transformers and those with transformers. We used a series
of non-transformer architectures such as PA-LSTM(Zhang et al., 2017),
C-GCN(Zhang et al., 2018), and AGGCN(attention guide GCN)(Guo et al., 2019),
and a series of transformer architectures such as BERT, RoBERTa, and R-BERT(Wu
and He, 2019). Our comparison included traditional metrics like micro F1, as
well as evaluations in different scenarios, varying sentence lengths, and
different percentages of the dataset for training. Our experiments were
conducted on TACRED, TACREV, and RE-TACRED. The results show that
transformer-based models outperform non-transformer models, achieving micro F1
scores of 80-90% compared to 64-67% for non-transformer models. Additionally,
we briefly review the research journey in supervised relation classification
and discuss the role and current status of large language models (LLMs) in
relation extraction.

### 22. Lost in Embeddings: Information Loss in Vision-Language Models

- **LLM Score**: 3
- **Keyword Score**: 2
- **Authors**: Wenyan Li, Raphael Tang, Chengzu Li, Caiqi Zhang, Ivan Vuliƒá, Anders S√∏gaard
- **URL**: <http://arxiv.org/abs/2509.11986v1>
- **Submitted**: 2025-09-15 14:38:06
- **Topic Keywords**: retrieval
- **Reason**: The paper focuses on vision-language models, which is somewhat related to information retrieval, but the specific topic of information loss in embeddings is not directly aligned with the user's core research themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> Vision--language models (VLMs) often process visual inputs through a
pretrained vision encoder, followed by a projection into the language model's
embedding space via a connector component. While crucial for modality fusion,
the potential information loss induced by this projection step and its direct
impact on model capabilities remain understudied. We introduce two
complementary approaches to examine and quantify this loss by analyzing the
latent representation space. First, we evaluate semantic information
preservation by analyzing changes in k-nearest neighbor relationships between
image representations, before and after projection. Second, we directly measure
information loss by reconstructing visual embeddings from the projected
representation, localizing loss at an image patch level. Experiments reveal
that connectors substantially distort the local geometry of visual
representations, with k-nearest neighbors diverging by 40--60\%
post-projection, correlating with degradation in retrieval performance. The
patch-level embedding reconstruction provides interpretable insights for model
behavior on visually grounded question-answering tasks, finding that areas of
high information loss reliably predict instances where models struggle.

### 23. MOOM: Maintenance, Organization and Optimization of Memory in Ultra-Long Role-Playing Dialogues

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Weishu Chen, Jinyi Tang, Zhouhui Hou, Shihao Han, Mingjie Zhan, Zhiyuan Huang, Delong Liu, Jiawei Guo, Zhicheng Zhao, Fei Su
- **URL**: <http://arxiv.org/abs/2509.11860v1>
- **Submitted**: 2025-09-15 12:35:14
- **Topic Keywords**: ltr, rag
- **Reason**: This paper focuses on memory extraction in role-playing dialogues, leveraging literary theory and a forgetting mechanism. While it involves NLP and deep semantic understanding, its primary focus is on memory management in a specific domain, which is not directly related to information retrieval, search technologies, or user behavior modeling.

#### Abstract
> Memory extraction is crucial for maintaining coherent ultra-long dialogues in
human-robot role-playing scenarios. However, existing methods often exhibit
uncontrolled memory growth. To address this, we propose MOOM, the first
dual-branch memory plugin that leverages literary theory by modeling plot
development and character portrayal as core storytelling elements.
Specifically, one branch summarizes plot conflicts across multiple time scales,
while the other extracts the user's character profile. MOOM further integrates
a forgetting mechanism, inspired by the ``competition-inhibition'' memory
theory, to constrain memory capacity and mitigate uncontrolled growth.
Furthermore, we present ZH-4O, a Chinese ultra-long dialogue dataset
specifically designed for role-playing, featuring dialogues that average 600
turns and include manually annotated memory information. Experimental results
demonstrate that MOOM outperforms all state-of-the-art memory extraction
methods, requiring fewer large language model invocations while maintaining a
controllable memory capacity.

### 24. RAGs to Riches: RAG-like Few-shot Learning for Large Language Model Role-playing

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Timothy Rupprecht, Enfu Nan, Arash Akbari, Arman Akbari, Lei Lu, Priyanka Maan, Sean Duffy, Pu Zhao, Yumei He, David Kaeli, Yanzhi Wang
- **URL**: <http://arxiv.org/abs/2509.12168v1>
- **Submitted**: 2025-09-15 17:31:15
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper focuses on Large Language Model role-playing and few-shot learning, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve text retrieval and generation, the context and application are quite different from your areas of focus.

#### Abstract
> Role-playing Large language models (LLMs) are increasingly deployed in
high-stakes domains such as healthcare, education, and governance, where
failures can directly impact user trust and well-being. A cost effective
paradigm for LLM role-playing is few-shot learning, but existing approaches
often cause models to break character in unexpected and potentially harmful
ways, especially when interacting with hostile users. Inspired by
Retrieval-Augmented Generation (RAG), we reformulate LLM role-playing into a
text retrieval problem and propose a new prompting framework called
RAGs-to-Riches, which leverages curated reference demonstrations to condition
LLM responses. We evaluate our framework with LLM-as-a-judge preference voting
and introduce two novel token-level ROUGE metrics: Intersection over Output
(IOO) to quantity how much an LLM improvises and Intersection over References
(IOR) to measure few-shot demonstrations utilization rate during the evaluation
tasks. When simulating interactions with a hostile user, our prompting strategy
incorporates in its responses during inference an average of 35% more tokens
from the reference demonstrations. As a result, across 453 role-playing
interactions, our models are consistently judged as being more authentic, and
remain in-character more often than zero-shot and in-context Learning (ICL)
methods. Our method presents a scalable strategy for building robust,
human-aligned LLM role-playing frameworks.

### 25. XplaiNLP at CheckThat! 2025: Multilingual Subjectivity Detection with Finetuned Transformers and Prompt-Based Inference with Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Ariana Sahitaj, Jiaao Li, Pia Wenzel Neves, Fedor Splitt, Premtim Sahitaj, Charlott Jakob, Veronika Solopova, Vera Schmitt
- **URL**: <http://arxiv.org/abs/2509.12130v1>
- **Submitted**: 2025-09-15 16:53:41
- **Topic Keywords**: ranking, rank
- **Reason**: This paper focuses on multilingual subjectivity detection using NLP techniques, which is somewhat related to the user's interests in NLP and IR. However, the specific topic of subjectivity detection and the use of large language models for inference are not directly aligned with the user's core research themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> This notebook reports the XplaiNLP submission to the CheckThat! 2025 shared
task on multilingual subjectivity detection. We evaluate two approaches: (1)
supervised fine-tuning of transformer encoders, EuroBERT, XLM-RoBERTa, and
German-BERT, on monolingual and machine-translated training data; and (2)
zero-shot prompting using two LLMs: o3-mini for Annotation (rule-based
labelling) and gpt-4.1-mini for DoubleDown (contrastive rewriting) and
Perspective (comparative reasoning). The Annotation Approach achieves 1st place
in the Italian monolingual subtask with an F_1 score of 0.8104, outperforming
the baseline of 0.6941. In the Romanian zero-shot setting, the fine-tuned
XLM-RoBERTa model obtains an F_1 score of 0.7917, ranking 3rd and exceeding the
baseline of 0.6461. The same model also performs reliably in the multilingual
task and improves over the baseline in Greek. For German, a German-BERT model
fine-tuned on translated training data from typologically related languages
yields competitive performance over the baseline. In contrast, performance in
the Ukrainian and Polish zero-shot settings falls slightly below the respective
baselines, reflecting the challenge of generalization in low-resource
cross-lingual scenarios.

### 26. SAQ: Pushing the Limits of Vector Quantization through Code Adjustment and Dimension Segmentation

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Hui Li, Shiyuan Deng, Xiao Yan, Xiangyu Zhi, James Cheng
- **URL**: <http://arxiv.org/abs/2509.12086v1>
- **Submitted**: 2025-09-15 16:14:05
- **Comment**: 13 pages, 12 figures, accepted by SIGMOD
- **Topic Keywords**: rag, recommend, search
- **Reason**: This paper focuses on Vector Quantization, a technique used in Approximate Nearest Neighbor Search, which is relevant to Recommender Systems. However, it does not directly relate to Information Retrieval, query understanding, ranking models, or user behavior modeling, which are the core areas of your research interests.

#### Abstract
> Approximate Nearest Neighbor Search (ANNS) plays a critical role in
applications such as search engines, recommender systems, and RAG for LLMs.
Vector quantization (VQ), a crucial technique for ANNS, is commonly used to
reduce space overhead and accelerate distance computations. However, despite
significant research advances, state-of-the-art VQ methods still face
challenges in balancing encoding efficiency and quantization accuracy. To
address these limitations, we propose a novel VQ method called SAQ. To improve
accuracy, SAQ employs a new dimension segmentation technique to strategically
partition PCA-projected vectors into segments along their dimensions. By
prioritizing leading dimension segments with larger magnitudes, SAQ allocates
more bits to high-impact segments, optimizing the use of the available space
quota. An efficient dynamic programming algorithm is developed to optimize
dimension segmentation and bit allocation, ensuring minimal quantization error.
To speed up vector encoding, SAQ devises a code adjustment technique to first
quantize each dimension independently and then progressively refine quantized
vectors using a coordinate-descent-like approach to avoid exhaustive
enumeration. Extensive experiments demonstrate SAQ's superiority over classical
methods (e.g., PQ, PCA) and recent state-of-the-art approaches (e.g., LVQ,
Extended RabitQ). SAQ achieves up to 80% reduction in quantization error and
accelerates encoding speed by over 80x compared to Extended RabitQ.

### 27. AEFS: Adaptive Early Feature Selection for Deep Recommender Systems

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Fan Hu, Gaofeng Lu, Jun Chen, Chaonan Guo, Yuekui Yang, Xirong Li
- **URL**: <http://arxiv.org/abs/2509.12076v1>
- **Submitted**: 2025-09-15 16:04:24
- **Comment**: Accepted by TKDE
- **Topic Keywords**: rag, recommend
- **Reason**: This paper focuses on recommender systems, specifically introducing a new method called Adaptive Early Feature Selection (AEFS). While it leverages machine learning and AutoML, it doesn't directly relate to information retrieval, query understanding, ranking models, or user behavior modeling, which are core areas of your research interests.

#### Abstract
> Feature selection has emerged as a crucial technique in refining recommender
systems. Recent advancements leveraging Automated Machine Learning (AutoML) has
drawn significant attention, particularly in two main categories: early feature
selection and late feature selection, differentiated by whether the selection
occurs before or after the embedding layer. The early feature selection selects
a fixed subset of features and retrains the model, while the late feature
selection, known as adaptive feature selection, dynamically adjusts feature
choices for each data instance, recognizing the variability in feature
significance. Although adaptive feature selection has shown remarkable
improvements in performance, its main drawback lies in its post-embedding layer
feature selection. This process often becomes cumbersome and inefficient in
large-scale recommender systems with billions of ID-type features, leading to a
highly sparse and parameter-heavy embedding layer. To overcome this, we
introduce Adaptive Early Feature Selection (AEFS), a very simple method that
not only adaptively selects informative features for each instance, but also
significantly reduces the activated parameters of the embedding layer. AEFS
employs a dual-model architecture, encompassing an auxiliary model dedicated to
feature selection and a main model responsible for prediction. To ensure
effective alignment between these two models, we incorporate two collaborative
training loss constraints. Our extensive experiments on three benchmark
datasets validate the efficiency and effectiveness of our approach. Notably,
AEFS matches the performance of current state-of-theart Adaptive Late Feature
Selection methods while achieving a significant reduction of 37. 5% in the
activated parameters of the embedding layer. AEFS is open-source at
https://github. com/fly-dragon211/AEFS .

### 28. How to Evaluate Medical AI

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Ilia Kopanichuk, Petr Anokhin, Vladimir Shaposhnikov, Vladimir Makharev, Ekaterina Tsapieva, Iaroslav Bespalov, Dmitry V. Dylov, Ivan Oseledets
- **URL**: <http://arxiv.org/abs/2509.11941v1>
- **Submitted**: 2025-09-15 14:01:22
- **Comment**: 10 pages, 7 fugures
- **Topic Keywords**: relevance
- **Reason**: This paper is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves AI and large language models, the focus is on medical AI evaluation and diagnosis, which is outside your primary domain of e-commerce and related areas.

#### Abstract
> The integration of artificial intelligence (AI) into medical diagnostic
workflows requires robust and consistent evaluation methods to ensure
reliability, clinical relevance, and the inherent variability in expert
judgments. Traditional metrics like precision and recall often fail to account
for the inherent variability in expert judgments, leading to inconsistent
assessments of AI performance. Inter-rater agreement statistics like Cohen's
Kappa are more reliable but they lack interpretability. We introduce Relative
Precision and Recall of Algorithmic Diagnostics (RPAD and RRAD) - a new
evaluation metrics that compare AI outputs against multiple expert opinions
rather than a single reference. By normalizing performance against inter-expert
disagreement, these metrics provide a more stable and realistic measure of the
quality of predicted diagnosis. In addition to the comprehensive analysis of
diagnostic quality measures, our study contains a very important side result.
Our evaluation methodology allows us to avoid selecting diagnoses from a
limited list when evaluating a given case. Instead, both the models being
tested and the examiners verifying them arrive at a free-form diagnosis. In
this automated methodology for establishing the identity of free-form clinical
diagnoses, a remarkable 98% accuracy becomes attainable. We evaluate our
approach using 360 medical dialogues, comparing multiple large language models
(LLMs) against a panel of physicians. Large-scale study shows that
top-performing models, such as DeepSeek-V3, achieve consistency on par with or
exceeding expert consensus. Moreover, we demonstrate that expert judgments
exhibit significant variability - often greater than that between AI and
humans. This finding underscores the limitations of any absolute metrics and
supports the need to adopt relative metrics in medical AI.

### 29. MindVL: Towards Efficient and Effective Training of Multimodal Large Language Models on Ascend NPUs

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Feilong Chen, Yijiang Liu, Yi Huang, Hao Wang, Miren Tian, Ya-Qi Yu, Minghui Liao, Jihao Wu
- **URL**: <http://arxiv.org/abs/2509.11662v1>
- **Submitted**: 2025-09-15 08:00:31
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on multimodal large language models and their training on Ascend NPUs, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some NLP aspects, the primary focus is on model training and optimization, which is not a central match for the user's interests.

#### Abstract
> We propose MindVL, a multimodal large langauge model trained on Ascend NPUs.
Similar to Qwen2.5-VL, MindVL adopts native-resolution Vision Transformers,
which enables it to process images at their original variable resolutions. This
design avoids the degradation caused by fixed-resolution tiling while
preserving fine-grained details and global layouts, which is crucial for
visually dense content such as complex charts and diagrams. To ensure the
smooth training of MindVL on Ascend NPUs, we develop Mindspeed-MLLM, a
distributed multimodal training framework tailored for Ascend NPUs. To maintain
training accuracy, we implement equivalent replacements for certain operators.
MindVL undergoes a three-phase training process, namely the warm-up phase,
multitask training phase, and supervised instruction tuning phase, to gradually
enhance its capabilities. This process starts with basic visual and multimodal
pre-training, followed by large-scale multiask trainging and instruction
tuning. We also adopt multimodal data packaging and hybrid parallelism
techniques, which significantly improve end-to-end training speed. To further
boost model performance, we specifically introduce test-time resolution search
and model weight averaging. Notably, despite using about 1/10 of the training
data required by Qwen2.5-VL, MindVL achieves performance on par with Qwen2.5-VL
in evaluations of general multimodal understanding and document/table
comprehension. Beyond overall scores, MindVL also delivers leading performance
in OCR assessments.

### 30. MALLM: Multi-Agent Large Language Models Framework

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Jonas Becker, Lars Benedikt Kaesberg, Niklas Bauer, Jan Philip Wahle, Terry Ruas, Bela Gipp
- **URL**: <http://arxiv.org/abs/2509.11656v1>
- **Submitted**: 2025-09-15 07:48:02
- **Comment**: Accepted at EMNLP 2025 (Demo)
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on multi-agent debate and large language models, which is outside the user's primary research interests in Information Retrieval and Search technologies. While it involves NLP, the context and application are not directly related to the user's core themes.

#### Abstract
> Multi-agent debate (MAD) has demonstrated the ability to augment collective
intelligence by scaling test-time compute and leveraging expertise. Current
frameworks for multi-agent debate are often designed towards tool use, lack
integrated evaluation, or provide limited configurability of agent personas,
response generators, discussion paradigms, and decision protocols. We introduce
MALLM (Multi-Agent Large Language Models), an open-source framework that
enables systematic analysis of MAD components. MALLM offers more than 144
unique configurations of MAD, including (1) agent personas (e.g., Expert,
Personality), (2) response generators (e.g., Critical, Reasoning), (3)
discussion paradigms (e.g., Memory, Relay), and (4) decision protocols (e.g.,
Voting, Consensus). MALLM uses simple configuration files to define a debate.
Furthermore, MALLM can load any textual Huggingface dataset (e.g., MMLU-Pro,
WinoGrande) and provides an evaluation pipeline for easy comparison of MAD
configurations. MALLM is tailored towards researchers and provides a window
into the heart of multi-agent debate, facilitating the understanding of its
components and their interplay.

### 31. Event2Vec: A Geometric Approach to Learning Composable Representations of Event Sequences

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Antonin Sulc
- **URL**: <http://arxiv.org/abs/2509.12188v1>
- **Submitted**: 2025-09-15 17:51:02
- **Comment**: 10 pages, 3 figures, Symmetry and Geometry in Neural Representations
  Workshop at NeuralIPS (Neurreps) 2025
- **Topic Keywords**: rag
- **Reason**: This paper focuses on learning representations of event sequences using geometric and topological structures, which is not directly related to the user's interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves neural representations and embeddings, the context is different from query understanding, ranking models, and user behavior modeling.

#### Abstract
> The study of neural representations, both in biological and artificial
systems, is increasingly revealing the importance of geometric and topological
structures. Inspired by this, we introduce Event2Vec, a novel framework for
learning representations of discrete event sequences. Our model leverages a
simple, additive recurrent structure to learn composable, interpretable
embeddings. We provide a theoretical analysis demonstrating that, under
specific training objectives, our model's learned representations in a
Euclidean space converge to an ideal additive structure. This ensures that the
representation of a sequence is the vector sum of its constituent events, a
property we term the linear additive hypothesis. To address the limitations of
Euclidean geometry for hierarchical data, we also introduce a variant of our
model in hyperbolic space, which is naturally suited to embedding tree-like
structures with low distortion. We present experiments to validate our
hypothesis and demonstrate the benefits of each geometry, highlighting the
improved performance of the hyperbolic model on hierarchical event sequences.

### 32. When marine radar target detection meets pretrained large language models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Qiying Hu, Linping Zhang, Xueqian Wang, Gang Li, Yu Liu, Xiao-Ping Zhang
- **URL**: <http://arxiv.org/abs/2509.12110v1>
- **Submitted**: 2025-09-15 16:38:13
- **Topic Keywords**: rag
- **Reason**: This paper appears to be unrelated to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus on marine radar target detection and large language models in a different context does not align with the user's core themes.

#### Abstract
> Deep learning (DL) methods are widely used to extract high-dimensional
patterns from the sequence features of radar echo signals. However,
conventional DL algorithms face challenges such as redundant feature segments,
and constraints from restricted model sizes. To address these issues, we
propose a framework that integrates feature preprocessing with large language
models (LLMs). Our preprocessing module tokenizes radar sequence features,
applies a patch selection algorithm to filter out uninformative segments, and
projects the selected patches into embeddings compatible with the feature space
of pre-trained LLMs. Leveraging these refined embeddings, we incorporate a
pre-trained LLM, fine-tuning only the normalization layers to reduce training
burdens while enhancing performance. Experiments on measured datasets
demonstrate that the proposed method significantly outperforms the
state-of-the-art baselines on supervised learning tests.

### 33. Text Adaptation to Plain Language and Easy Read via Automatic Post-Editing Cycles

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jes√∫s Calleja, David Ponce, Thierry Etchegoyhen
- **URL**: <http://arxiv.org/abs/2509.11991v1>
- **Submitted**: 2025-09-15 14:42:44
- **Topic Keywords**: rag
- **Reason**: This paper focuses on text adaptation and post-editing, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves Large Language Models, the primary goal is readability and post-editing, rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> We describe Vicomtech's participation in the CLEARS challenge on text
adaptation to Plain Language and Easy Read in Spanish. Our approach features
automatic post-editing of different types of initial Large Language Model
adaptations, where successive adaptations are generated iteratively until
readability and similarity metrics indicate that no further adaptation
refinement can be successfully performed. Taking the average of all official
metrics, our submissions achieved first and second place in Plain language and
Easy Read adaptation, respectively.

### 34. PledgeTracker: A System for Monitoring the Fulfilment of Pledges

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yulong Chen, Michael Sejr Schlichtkrull, Zhenyun Deng, David Corney, Nasim Asl, Joshua Salisbury, Andrew Dudfield, Andreas Vlachos
- **URL**: <http://arxiv.org/abs/2509.11804v1>
- **Submitted**: 2025-09-15 11:37:47
- **Comment**: EMNLP 2025 demo
- **Topic Keywords**: retrieval
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or Natural Language Processing, which are the primary areas of interest. While it involves evidence retrieval and timeline construction, the context is focused on political pledge tracking, which is not a core theme in the user's research interests.

#### Abstract
> Political pledges reflect candidates' policy commitments, but tracking their
fulfilment requires reasoning over incremental evidence distributed across
multiple, dynamically updated sources. Existing methods simplify this task into
a document classification task, overlooking its dynamic, temporal and
multi-document nature. To address this issue, we introduce
\textsc{PledgeTracker}, a system that reformulates pledge verification into
structured event timeline construction. PledgeTracker consists of three core
components: (1) a multi-step evidence retrieval module; (2) a timeline
construction module and; (3) a fulfilment filtering module, allowing the
capture of the evolving nature of pledge fulfilment and producing interpretable
and structured timelines. We evaluate PledgeTracker in collaboration with
professional fact-checkers in real-world workflows, demonstrating its
effectiveness in retrieving relevant evidence and reducing human verification
effort.

### 35. CoachMe: Decoding Sport Elements with a Reference-Based Coaching Instruction Generation Model

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Wei-Hsin Yeh, Yu-An Su, Chih-Ning Chen, Yi-Hsueh Lin, Calvin Ku, Wen-Hsin Chiu, Min-Chun Hu, Lun-Wei Ku
- **URL**: <http://arxiv.org/abs/2509.11698v1>
- **Submitted**: 2025-09-15 09:01:39
- **Comment**: Published in Proceedings of the 63rd Annual Meeting of the
  Association for Computational Linguistics (Volume 1: Long Papers), ACL 2025.
  Official version: https://doi.org/10.18653/v1/2025.acl-long.1413
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The focus on motion instruction generation for sports and the use of multimodal models are outside your primary areas of interest.

#### Abstract
> Motion instruction is a crucial task that helps athletes refine their
technique by analyzing movements and providing corrective guidance. Although
recent advances in multimodal models have improved motion understanding,
generating precise and sport-specific instruction remains challenging due to
the highly domain-specific nature of sports and the need for informative
guidance. We propose CoachMe, a reference-based model that analyzes the
differences between a learner's motion and a reference under temporal and
physical aspects. This approach enables both domain-knowledge learning and the
acquisition of a coach-like thinking process that identifies movement errors
effectively and provides feedback to explain how to improve. In this paper, we
illustrate how CoachMe adapts well to specific sports such as skating and
boxing by learning from general movements and then leveraging limited data.
Experiments show that CoachMe provides high-quality instructions instead of
directions merely in the tone of a coach but without critical information.
CoachMe outperforms GPT-4o by 31.6% in G-Eval on figure skating and by 58.3% on
boxing. Analysis further confirms that it elaborates on errors and their
corresponding improvement methods in the generated instructions. You can find
CoachMe here: https://motionxperts.github.io/

### 36. Securing AI Agents: Implementing Role-Based Access Control for Industrial Applications

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Aadil Gani Ganie
- **URL**: <http://arxiv.org/abs/2509.11431v1>
- **Submitted**: 2025-09-14 20:58:08
- **Topic Keywords**: rag
- **Reason**: This paper focuses on securing AI agents, which is not a primary area of interest for you. While it mentions AI agents leveraging Large Language Models, it does not discuss query understanding, ranking models, or user behavior modeling, which are core aspects of your research.

#### Abstract
> The emergence of Large Language Models (LLMs) has significantly advanced
solutions across various domains, from political science to software
development. However, these models are constrained by their training data,
which is static and limited to information available up to a specific date.
Additionally, their generalized nature often necessitates fine-tuning --
whether for classification or instructional purposes -- to effectively perform
specific downstream tasks. AI agents, leveraging LLMs as their core, mitigate
some of these limitations by accessing external tools and real-time data,
enabling applications such as live weather reporting and data analysis. In
industrial settings, AI agents are transforming operations by enhancing
decision-making, predictive maintenance, and process optimization. For example,
in manufacturing, AI agents enable near-autonomous systems that boost
productivity and support real-time decision-making. Despite these advancements,
AI agents remain vulnerable to security threats, including prompt injection
attacks, which pose significant risks to their integrity and reliability. To
address these challenges, this paper proposes a framework for integrating
Role-Based Access Control (RBAC) into AI agents, providing a robust security
guardrail. This framework aims to support the effective and scalable deployment
of AI agents, with a focus on on-premises implementations.

### 37. AMQ: Enabling AutoML for Mixed-precision Weight-Only Quantization of Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Sangjun Lee, Seung-taek Woo, Jungyu Jin, Changhun Lee, Eunhyeok Park
- **URL**: <http://arxiv.org/abs/2509.12019v1>
- **Submitted**: 2025-09-15 14:59:35
- **Comment**: EMNLP 2025 Main Conference, Long Paper (Oral)
- **Topic Keywords**: search
- **Reason**: This paper focuses on optimizing large language models for memory constraints through mixed-precision weight quantization. While it involves deep learning and model optimization, it doesn't directly relate to information retrieval, search technologies, or query understanding, which are the core areas of your research interests.

#### Abstract
> To enable broader deployment of Large Language Models (LLMs), it is essential
to identify the best-performing model under strict memory constraints. We
present AMQ, Automated Mixed-Precision Weight-Only Quantization, a framework
that assigns layer-wise quantization bit-widths to optimally balance model
quality and memory usage. However, the combinatorial search space, with over
10^{100} possible configurations, makes conventional black-box optimization
infeasible. AMQ overcomes this challenge through four key innovations:(1)
search space pruning using prior knowledge to exclude unpromising
configurations, (2) quantization proxy to bypass costly format conversions
during search, (3) quality predictor to minimize evaluation overhead, and (4)
iterative search-and-update strategy for fast and stable convergence. By
integrating these components, AMQ efficiently explores the quality-efficiency
landscape, reaching the Pareto frontier and yielding LLMs that are both compact
and high-performing. Our code is available at https://github.com/dlwns147/amq.

### 38. PeruMedQA: Benchmarking Large Language Models (LLMs) on Peruvian Medical Exams -- Dataset Construction and Evaluation

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Rodrigo M. Carrillo-Larco, Jesus Lov√≥n Melgarejo, Manuel Castillo-Cara, Gusseppe Bravo-Rocca
- **URL**: <http://arxiv.org/abs/2509.11517v1>
- **Submitted**: 2025-09-15 02:07:26
- **Comment**: https://github.com/rodrigo-carrillo/PeruMedQA
- **Topic Keywords**: search
- **Reason**: This paper is primarily focused on evaluating the performance of Large Language Models (LLMs) on medical exams in Spanish, which is not directly related to Information Retrieval, Search technologies, or user behavior modeling. While it involves NLP, the context is specific to medical exams and does not align with the user's core research themes.

#### Abstract
> BACKGROUND: Medical large language models (LLMS) have demonstrated remarkable
performance in answering medical examinations. However, the extent to which
this high performance is transferable to medical questions in Spanish and from
a Latin American country remains unexplored. This knowledge is crucial as
LLM-based medical applications gain traction in Latin America. AIMS: to build a
dataset of questions from medical examinations taken by Peruvian physicians
pursuing specialty training; to fine-tune a LLM on this dataset; to evaluate
and compare the performance in terms of accuracy between vanilla LLMs and the
fine-tuned LLM. METHODS: We curated PeruMedQA, a multiple-choice
question-answering (MCQA) datasets containing 8,380 questions spanning 12
medical domains (2018-2025). We selected eight medical LLMs including
medgemma-4b-it and medgemma-27b-text-it, and developed zero-shot task-specific
prompts to answer the questions appropriately. We employed parameter-efficient
fine tuning (PEFT)and low-rant adaptation (LoRA) to fine-tune medgemma-4b-it
utilizing all questions except those from 2025 (test set). RESULTS:
medgemma-27b-text-it outperformed all other models, achieving a proportion of
correct answers exceeding 90% in several instances. LLMs with <10 billion
parameters exhibited <60% of correct answers, while some exams yielded results
<50%. The fine-tuned version of medgemma-4b-it emerged victorious agains all
LLMs with <10 billion parameters and rivaled a LLM with 70 billion parameters
across various examinations. CONCLUSIONS: For medical AI application and
research that require knowledge bases from Spanish-speaking countries and those
exhibiting similar epidemiological profiles to Peru's, interested parties
should utilize medgemma-27b-text-it or a fine-tuned version of medgemma-4b-it.

### 39. LVLMs are Bad at Overhearing Human Referential Communication

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Zhengxiang Wang, Weiling Li, Panagiotis Kaliosis, Owen Rambow, Susan E. Brennan
- **URL**: <http://arxiv.org/abs/2509.11514v1>
- **Submitted**: 2025-09-15 02:03:18
- **Comment**: EMNLP 2025 (Main)
- **Topic Keywords**: search
- **Reason**: This paper focuses on the capabilities of Large Vision Language Models (LVLMs) in understanding human referential communication, which is a topic in Natural Language Processing (NLP). However, it does not directly relate to the user's core research themes in Information Retrieval (IR), query understanding, ranking models, or user behavior modeling.

#### Abstract
> During spontaneous conversations, speakers collaborate on novel referring
expressions, which they can then re-use in subsequent conversations.
Understanding such referring expressions is an important ability for an
embodied agent, so that it can carry out tasks in the real world. This requires
integrating and understanding language, vision, and conversational interaction.
We study the capabilities of seven state-of-the-art Large Vision Language
Models (LVLMs) as overhearers to a corpus of spontaneous conversations between
pairs of human discourse participants engaged in a collaborative
object-matching task. We find that such a task remains challenging for current
LVLMs and they all fail to show a consistent performance improvement as they
overhear more conversations from the same discourse participants repeating the
same task for multiple rounds. We release our corpus and code for
reproducibility and to facilitate future research.

### 40. Trading-R1: Financial Trading with LLM Reasoning via Reinforcement Learning

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Yijia Xiao, Edward Sun, Tong Chen, Fang Wu, Di Luo, Wei Wang
- **URL**: <http://arxiv.org/abs/2509.11420v1>
- **Submitted**: 2025-09-14 20:13:41
- **Comment**: Tauric Research: https://github.com/TauricResearch
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves deep learning models, the focus is on financial trading and decision-making, which is outside your primary research areas.

#### Abstract
> Developing professional, structured reasoning on par with human financial
analysts and traders remains a central challenge in AI for finance, where
markets demand interpretability and trust. Traditional time-series models lack
explainability, while LLMs face challenges in turning natural-language analysis
into disciplined, executable trades. Although reasoning LLMs have advanced in
step-by-step planning and verification, their application to risk-sensitive
financial decisions is underexplored. We present Trading-R1, a
financially-aware model that incorporates strategic thinking and planning for
comprehensive thesis composition, facts-grounded analysis, and
volatility-adjusted decision making. Trading-R1 aligns reasoning with trading
principles through supervised fine-tuning and reinforcement learning with a
three-stage easy-to-hard curriculum. Training uses Tauric-TR1-DB, a 100k-sample
corpus spanning 18 months, 14 equities, and five heterogeneous financial data
sources. Evaluated on six major equities and ETFs, Trading-R1 demonstrates
improved risk-adjusted returns and lower drawdowns compared to both open-source
and proprietary instruction-following models as well as reasoning models. The
system generates structured, evidence-based investment theses that support
disciplined and interpretable trading decisions. Trading-R1 Terminal will be
released at https://github.com/TauricResearch/Trading-R1.

### 41. Continually Adding New Languages to Multilingual Language Models

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Abraham Toluwase Owodunni, Sachin Kumar
- **URL**: <http://arxiv.org/abs/2509.11414v1>
- **Submitted**: 2025-09-14 20:08:15
- **Topic Keywords**: rank
- **Reason**: This paper focuses on multilingual language models and proposes a method to continually add new languages, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves NLP, the topic is more aligned with model development and adaptation rather than query understanding or ranking models.

#### Abstract
> Multilingual language models are trained on a fixed set of languages, and to
support new languages, the models need to be retrained from scratch. This is an
expensive endeavor and is often infeasible, as model developers tend not to
release their pre-training data. Naive approaches, such as continued
pretraining, suffer from catastrophic forgetting; however, mitigation
strategies like experience replay cannot be applied due to the lack of original
pretraining data. In this work, we investigate the problem of continually
adding new languages to a multilingual model, assuming access to pretraining
data in only the target languages. We explore multiple approaches to address
this problem and propose Layer-Selective LoRA (LayRA), which adds Low-Rank
Adapters (LoRA) to selected initial and final layers while keeping the rest of
the model frozen. LayRA builds on two insights: (1) LoRA reduces forgetting,
and (2) multilingual models encode inputs in the source language in the initial
layers, reason in English in intermediate layers, and translate back to the
source language in final layers. We experiment with adding multiple
combinations of Galician, Swahili, and Urdu to pretrained language models and
evaluate each method on diverse multilingual tasks. We find that LayRA provides
the overall best tradeoff between preserving models' capabilities in previously
supported languages, while being competitive with existing approaches such as
LoRA in learning new languages. We also demonstrate that using model
arithmetic, the adapted models can be equipped with strong instruction
following abilities without access to any instruction tuning data in the target
languages.

### 42. Acoustic Overspecification in Electronic Dance Music Taxonomy

- **LLM Score**: 0
- **Keyword Score**: 2
- **Authors**: Weilun Xu, Tianhao Dai, Oscar Goudet, Xiaoxuan Wang
- **URL**: <http://arxiv.org/abs/2509.11474v1>
- **Submitted**: 2025-09-14 23:38:45
- **Comment**: 5 pages, 3 figures, conference paper
- **Topic Keywords**: ctr
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, Natural Language Processing, or data mining. The paper focuses on acoustic analysis of Electronic Dance Music and taxonomy, which is outside your areas of expertise.

#### Abstract
> Electronic Dance Music (EDM) classification typically relies on
industry-defined taxonomies with numerous subgenres, yet the acoustic basis for
these distinctions remains unclear. Current approaches use supervised learning
with prescribed genre labels, assuming their validity without systematic
evaluation. In this paper, we propose an unsupervised approach to discover the
natural acoustic structure of EDM independent of commercial labels. Our method
combines novel tempogram-based features capturing EDM's layered rhythmic
patterns with multi-criteria feature selection. To validate that our findings
reflect genuine acoustic structure rather than methodological artifacts, we
compare our results against state-of-the-art pre-trained audio embeddings (MERT
and CLAP). Both our feature space and embedding representations converge to
19-23 natural acoustic families compared to the prescribed 35, providing
consistent evidence of significant overspecification in current EDM taxonomy by
approximately one-third.

---

