[
    {
        "title": "Confidence and Stability of Global and Pairwise Scores in NLP Evaluation",
        "abstract": "With the advent of highly capable instruction-tuned neural language models,\nbenchmarking in natural language processing (NLP) is increasingly shifting\ntowards pairwise comparison leaderboards, such as LMSYS Arena, from traditional\nglobal pointwise scores (e.g., GLUE, BIG-bench, SWE-bench). This paper\nempirically investigates the strengths and weaknesses of both global scores and\npairwise comparisons to aid decision-making in selecting appropriate model\nevaluation strategies. Through computational experiments on synthetic and\nreal-world datasets using standard global metrics and the popular Bradley-Terry\nmodel for pairwise comparisons, we found that while global scores provide more\nreliable overall rankings, they can underestimate strong models with rare,\nsignificant errors or low confidence. Conversely, pairwise comparisons are\nparticularly effective for identifying strong contenders among models with\nlower global scores, especially where quality metrics are hard to define (e.g.,\ntext generation), though they require more comparisons to converge if ties are\nfrequent. Our code and data are available at\nhttps://github.com/HSPyroblast/srw-ranking under a permissive license.",
        "url": "http://arxiv.org/abs/2507.01633v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01633v1",
        "arxiv_id": "2507.01633v1",
        "authors": [
            "Georgii Levtsov",
            "Dmitry Ustalov"
        ],
        "submitted": "2025-07-02 12:05:22",
        "source": "arxiv",
        "comment": "8 pages, accepted at ACL SRW 2025",
        "score": 10,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'pointwise' (score: +3)",
            "Found 'pairwise' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores the strengths and weaknesses of global and pairwise scores in NLP evaluation, which is related to my interest in Information Retrieval and Search technologies. However, the focus is on NLP evaluation rather than query understanding, ranking models, or user behavior modeling, which are my core research themes."
    },
    {
        "title": "LEDOM: An Open and Fundamental Reverse Language Model",
        "abstract": "We introduce LEDOM, the first purely reverse language model, trained\nautoregressively on 435B tokens with 2B and 7B parameter variants, which\nprocesses sequences in reverse temporal order through previous token\nprediction. For the first time, we present the reverse language model as a\npotential foundational model across general tasks, accompanied by a set of\nintriguing examples and insights. Based on LEDOM, we further introduce a novel\napplication: Reverse Reward, where LEDOM-guided reranking of forward language\nmodel outputs leads to substantial performance improvements on mathematical\nreasoning tasks. This approach leverages LEDOM's unique backward reasoning\ncapability to refine generation quality through posterior evaluation. Our\nfindings suggest that LEDOM exhibits unique characteristics with broad\napplication potential. We will release all models, training code, and\npre-training data to facilitate future research.",
        "url": "http://arxiv.org/abs/2507.01335v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01335v1",
        "arxiv_id": "2507.01335v1",
        "authors": [
            "Xunjian Yin",
            "Sitao Cheng",
            "Yuxi Xie",
            "Xinyu Hu",
            "Li Lin",
            "Xinyi Wang",
            "Liangming Pan",
            "William Yang Wang",
            "Xiaojun Wan"
        ],
        "submitted": "2025-07-02 03:52:00",
        "source": "arxiv",
        "comment": "Work in progress",
        "score": 10,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper introduces a novel language model, LEDOM, which is trained autoregressively on a large dataset. While it shows promising results in refining generation quality, the focus is on language modeling rather than information retrieval or search technologies. The connection to user behavior modeling or ranking models is not immediately apparent, making it only loosely relevant to the user's research interests."
    },
    {
        "title": "GAIus: Combining Genai with Legal Clauses Retrieval for Knowledge-based Assistant",
        "abstract": "In this paper we discuss the capability of large language models to base\ntheir answer and provide proper references when dealing with legal matters of\nnon-english and non-chinese speaking country. We discuss the history of legal\ninformation retrieval, the difference between case law and statute law, its\nimpact on the legal tasks and analyze the latest research in this field. Basing\non that background we introduce gAIus, the architecture of the cognitive\nLLM-based agent, whose responses are based on the knowledge retrieved from\ncertain legal act, which is Polish Civil Code. We propose a retrieval mechanism\nwhich is more explainable, human-friendly and achieves better results than\nembedding-based approaches. To evaluate our method we create special dataset\nbased on single-choice questions from entrance exams for law apprenticeships\nconducted in Poland. The proposed architecture critically leveraged the\nabilities of used large language models, improving the gpt-3.5-turbo-0125 by\n419%, allowing it to beat gpt-4o and lifting gpt-4o-mini score from 31% to 86%.\nAt the end of our paper we show the possible future path of research and\npotential applications of our findings.",
        "url": "http://arxiv.org/abs/2507.01259v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01259v1",
        "arxiv_id": "2507.01259v1",
        "authors": [
            "Michał Matak",
            "Jarosław A. Chudziak"
        ],
        "submitted": "2025-07-02 00:36:27",
        "source": "arxiv",
        "comment": "8 pages, 2 figures, presented at ICAART 2025, in proceedings of the\n  17th International Conference on Agents and Artificial Intelligence - Volume\n  3: ICAART",
        "score": 8,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses the application of large language models in legal information retrieval, which is not directly related to the user's primary focus on information retrieval, search technologies, and query understanding. While the paper touches on the topic of retrieval mechanisms, it is more focused on the legal domain and does not address the user's specific interests in e-commerce, user behavior modeling, or real-time relevance optimization."
    },
    {
        "title": "Frustratingly Simple Retrieval Improves Challenging, Reasoning-Intensive Benchmarks",
        "abstract": "Retrieval-augmented Generation (RAG) has primarily been studied in limited\nsettings, such as factoid question answering; more challenging,\nreasoning-intensive benchmarks have seen limited success from minimal RAG. In\nthis work, we challenge this prevailing view on established,\nreasoning-intensive benchmarks: MMLU, MMLU Pro, AGI Eval, GPQA, and MATH. We\nidentify a key missing component in prior work: a usable, web-scale datastore\naligned with the breadth of pretraining data. To this end, we introduce\nCompactDS: a diverse, high-quality, web-scale datastore that achieves high\nretrieval accuracy and subsecond latency on a single-node. The key insights are\n(1) most web content can be filtered out without sacrificing coverage, and a\ncompact, high-quality subset is sufficient; and (2) combining in-memory\napproximate nearest neighbor (ANN) retrieval and on-disk exact search balances\nspeed and recall. Using CompactDS, we show that a minimal RAG pipeline achieves\nconsistent accuracy improvements across all benchmarks and model sizes\n(8B--70B), with relative gains of 10% on MMLU, 33% on MMLU Pro, 14% on GPQA,\nand 19% on MATH. No single data source suffices alone, highlighting the\nimportance of diversity of sources (web crawls, curated math, academic papers,\ntextbooks). Finally, we show that our carefully designed in-house datastore\nmatches or outperforms web search engines such as Google Search, as well as\nrecently proposed, complex agent-based RAG systems--all while maintaining\nsimplicity, reproducibility, and self-containment. We release CompactDS and our\nretrieval pipeline, supporting future research exploring retrieval-based AI\nsystems.",
        "url": "http://arxiv.org/abs/2507.01297v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01297v1",
        "arxiv_id": "2507.01297v1",
        "authors": [
            "Xinxi Lyu",
            "Michael Duan",
            "Rulin Shao",
            "Pang Wei Koh",
            "Sewon Min"
        ],
        "submitted": "2025-07-02 02:35:47",
        "source": "arxiv",
        "comment": "33 pages, 2 figures, 27 tables",
        "score": 6,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'web search' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores Retrieval-Augmented Generation (RAG) in challenging, reasoning-intensive benchmarks, which aligns with your interest in Information Retrieval and Search technologies. The focus on query understanding and ranking models is also relevant. However, the paper's primary focus is on RAG and its applications, rather than query understanding and ranking models, which limits its relevance to your core research themes."
    },
    {
        "title": "MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining",
        "abstract": "Data quality is a critical driver of large language model performance, yet\nexisting model-based selection methods focus almost exclusively on English. We\nintroduce MuRating, a scalable framework that transfers high-quality English\ndata-quality signals into a single rater for 17 target languages. MuRating\naggregates multiple English \"raters\" via pairwise comparisons to learn unified\ndocument-quality scores,then projects these judgments through translation to\ntrain a multilingual evaluator on monolingual, cross-lingual, and parallel text\npairs. Applied to web data, MuRating selects balanced subsets of English and\nmultilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to\nstrong baselines, including QuRater, AskLLM, DCLM and so on, our approach\nboosts average accuracy on both English benchmarks and multilingual\nevaluations, with especially large gains on knowledge-intensive tasks. We\nfurther analyze translation fidelity, selection biases, and underrepresentation\nof narrative material, outlining directions for future work.",
        "url": "http://arxiv.org/abs/2507.01785v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01785v1",
        "arxiv_id": "2507.01785v1",
        "authors": [
            "Zhixun Chen",
            "Ping Guo",
            "Wenhan Han",
            "Yifan Zhang",
            "Binbin Liu",
            "Haobin Lin",
            "Fengze Liu",
            "Yan Zhao",
            "Bingni Zhang",
            "Taifeng Wang",
            "Yin Zheng",
            "Meng Fang"
        ],
        "submitted": "2025-07-02 15:11:12",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on large language model pretraining, data quality, and multilingual evaluation, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it mentions NLP, the context is different from the user's focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Enhanced Influence-aware Group Recommendation for Online Media Propagation",
        "abstract": "Group recommendation over social media streams has attracted significant\nattention due to its wide applications in domains such as e-commerce,\nentertainment, and online news broadcasting. By leveraging social connections\nand group behaviours, group recommendation (GR) aims to provide more accurate\nand engaging content to a set of users rather than individuals. Recently,\ninfluence-aware GR has emerged as a promising direction, as it considers the\nimpact of social influence on group decision-making. In earlier work, we\nproposed Influence-aware Group Recommendation (IGR) to solve this task.\nHowever, this task remains challenging due to three key factors: the large and\never-growing scale of social graphs, the inherently dynamic nature of influence\npropagation within user groups, and the high computational overhead of\nreal-time group-item matching.\n  To tackle these issues, we propose an Enhanced Influence-aware Group\nRecommendation (EIGR) framework. First, we introduce a Graph Extraction-based\nSampling (GES) strategy to minimise redundancy across multiple temporal social\ngraphs and effectively capture the evolving dynamics of both groups and items.\nSecond, we design a novel DYnamic Independent Cascade (DYIC) model to predict\nhow influence propagates over time across social items and user groups.\nFinally, we develop a two-level hash-based User Group Index (UG-Index) to\nefficiently organise user groups and enable real-time recommendation\ngeneration. Extensive experiments on real-world datasets demonstrate that our\nproposed framework, EIGR, consistently outperforms state-of-the-art baselines\nin both effectiveness and efficiency.",
        "url": "http://arxiv.org/abs/2507.01616v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01616v1",
        "arxiv_id": "2507.01616v1",
        "authors": [
            "Chengkun He",
            "Xiangmin Zhou",
            "Chen Wang",
            "Longbing Cao",
            "Jie Shao",
            "Xiaodong Li",
            "Guang Xu",
            "Carrie Jinqiu Hu",
            "Zahir Tari"
        ],
        "submitted": "2025-07-02 11:34:17",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on group recommendation and influence-aware recommendation, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and query understanding. Although it mentions social media and online media propagation, the context is different from the user's background in e-commerce and NLP."
    },
    {
        "title": "Efficient Out-of-Scope Detection in Dialogue Systems via Uncertainty-Driven LLM Routing",
        "abstract": "Out-of-scope (OOS) intent detection is a critical challenge in task-oriented\ndialogue systems (TODS), as it ensures robustness to unseen and ambiguous\nqueries. In this work, we propose a novel but simple modular framework that\ncombines uncertainty modeling with fine-tuned large language models (LLMs) for\nefficient and accurate OOS detection. The first step applies uncertainty\nestimation to the output of an in-scope intent detection classifier, which is\ncurrently deployed in a real-world TODS handling tens of thousands of user\ninteractions daily. The second step then leverages an emerging LLM-based\napproach, where a fine-tuned LLM is triggered to make a final decision on\ninstances with high uncertainty. Unlike prior approaches, our method\neffectively balances computational efficiency and performance, combining\ntraditional approaches with LLMs and yielding state-of-the-art results on key\nOOS detection benchmarks, including real-world OOS data acquired from a\ndeployed TODS.",
        "url": "http://arxiv.org/abs/2507.01541v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01541v1",
        "arxiv_id": "2507.01541v1",
        "authors": [
            "Álvaro Zaera",
            "Diana Nicoleta Popa",
            "Ivan Sekulic",
            "Paolo Rosso"
        ],
        "submitted": "2025-07-02 09:51:41",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on out-of-scope detection in dialogue systems, which is not directly related to information retrieval or search technologies. While it mentions large language models, the primary application is in dialogue systems, and the paper's emphasis is on uncertainty modeling and routing, which is not a key area of interest for the user."
    },
    {
        "title": "Far From Sight, Far From Mind: Inverse Distance Weighting for Graph Federated Recommendation",
        "abstract": "Graph federated recommendation systems offer a privacy-preserving alternative\nto traditional centralized recommendation architectures, which often raise\nconcerns about data security. While federated learning enables personalized\nrecommendations without exposing raw user data, existing aggregation methods\noverlook the unique properties of user embeddings in this setting. Indeed,\ntraditional aggregation methods fail to account for their complexity and the\ncritical role of user similarity in recommendation effectiveness. Moreover,\nevolving user interactions require adaptive aggregation while preserving the\ninfluence of high-relevance anchor users (the primary users before expansion in\ngraph-based frameworks). To address these limitations, we introduce\nDist-FedAvg, a novel distance-based aggregation method designed to enhance\npersonalization and aggregation efficiency in graph federated learning. Our\nmethod assigns higher aggregation weights to users with similar embeddings,\nwhile ensuring that anchor users retain significant influence in local updates.\nEmpirical evaluations on multiple datasets demonstrate that Dist-FedAvg\nconsistently outperforms baseline aggregation techniques, improving\nrecommendation accuracy while maintaining seamless integration into existing\nfederated learning frameworks.",
        "url": "http://arxiv.org/abs/2507.01285v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01285v1",
        "arxiv_id": "2507.01285v1",
        "authors": [
            "Aymen Rayane Khouas",
            "Mohamed Reda Bouadjenek",
            "Hakim Hacid",
            "Sunil Aryal"
        ],
        "submitted": "2025-07-02 01:57:58",
        "source": "arxiv",
        "comment": "17 pages, 5 figures",
        "score": 5,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'recommend' (score: +1)",
            "Found 'personalization' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on graph federated recommendation systems, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper mentions aggregation methods, it does not address query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user."
    },
    {
        "title": "Deep Recommender Models Inference: Automatic Asymmetric Data Flow Optimization",
        "abstract": "Deep Recommender Models (DLRMs) inference is a fundamental AI workload\naccounting for more than 79% of the total AI workload in Meta's data centers.\nDLRMs' performance bottleneck is found in the embedding layers, which perform\nmany random memory accesses to retrieve small embedding vectors from tables of\nvarious sizes. We propose the design of tailored data flows to speedup\nembedding look-ups. Namely, we propose four strategies to look up an embedding\ntable effectively on one core, and a framework to automatically map the tables\nasymmetrically to the multiple cores of a SoC. We assess the effectiveness of\nour method using the Huawei Ascend AI accelerators, comparing it with the\ndefault Ascend compiler, and we perform high-level comparisons with Nvidia\nA100. Results show a speed-up varying from 1.5x up to 6.5x for real workload\ndistributions, and more than 20x for extremely unbalanced distributions.\nFurthermore, the method proves to be much more independent of the query\ndistribution than the baseline.",
        "url": "http://arxiv.org/abs/2507.01676v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01676v1",
        "arxiv_id": "2507.01676v1",
        "authors": [
            "Giuseppe Ruggeri",
            "Renzo Andri",
            "Daniele Jahier Pagliari",
            "Lukas Cavigelli"
        ],
        "submitted": "2025-07-02 13:00:39",
        "source": "arxiv",
        "comment": "5 pages, 4 figures, conference: IEEE ICCD24",
        "score": 4,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on optimizing deep recommender models for inference, which is not directly related to the user's interests in information retrieval, query understanding, ranking models, and user behavior modeling. While the paper mentions embedding layers, it does not explore the semantic understanding and real-time relevance optimization aspects that are central to the user's research."
    },
    {
        "title": "Is External Information Useful for Stance Detection with LLMs?",
        "abstract": "In the stance detection task, a text is classified as either favorable,\nopposing, or neutral towards a target. Prior work suggests that the use of\nexternal information, e.g., excerpts from Wikipedia, improves stance detection\nperformance. However, whether or not such information can benefit large\nlanguage models (LLMs) remains an unanswered question, despite their wide\nadoption in many reasoning tasks. In this study, we conduct a systematic\nevaluation on how Wikipedia and web search external information can affect\nstance detection across eight LLMs and in three datasets with 12 targets.\nSurprisingly, we find that such information degrades performance in most cases,\nwith macro F1 scores dropping by up to 27.9\\%. We explain this through\nexperiments showing LLMs' tendency to align their predictions with the stance\nand sentiment of the provided information rather than the ground truth stance\nof the given text. We also find that performance degradation persists with\nchain-of-thought prompting, while fine-tuning mitigates but does not fully\neliminate it. Our findings, in contrast to previous literature on BERT-based\nsystems which suggests that external information enhances performance,\nhighlight the risks of information biases in LLM-based stance classifiers. Code\nis available at https://github.com/ngqm/acl2025-stance-detection.",
        "url": "http://arxiv.org/abs/2507.01543v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01543v1",
        "arxiv_id": "2507.01543v1",
        "authors": [
            "Quang Minh Nguyen",
            "Taegyoon Kim"
        ],
        "submitted": "2025-07-02 09:53:41",
        "source": "arxiv",
        "comment": "ACL Findings 2025",
        "score": 4,
        "keyword_reasons": [
            "Found 'web search' (score: +1)",
            "Found 'search' (score: +1)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the use of external information in stance detection with large language models (LLMs), which is related to query understanding and ranking models in Information Retrieval. However, the focus on stance detection and LLMs is not directly aligned with the user's primary interests in search technologies and user behavior modeling."
    },
    {
        "title": "Skywork-Reward-V2: Scaling Preference Data Curation via Human-AI Synergy",
        "abstract": "Despite the critical role of reward models (RMs) in reinforcement learning\nfrom human feedback (RLHF), current state-of-the-art open RMs perform poorly on\nmost existing evaluation benchmarks, failing to capture the spectrum of nuanced\nand sophisticated human preferences. Even approaches that incorporate advanced\ntraining techniques have not yielded meaningful performance improvements. We\nhypothesize that this brittleness stems primarily from limitations in\npreference datasets, which are often narrowly scoped, synthetically labeled, or\nlack rigorous quality control. To address these challenges, we present a\nlarge-scale preference dataset comprising 40 million preference pairs, named\nSynPref-40M. To enable data curation at scale, we design a human-AI synergistic\ntwo-stage pipeline that leverages the complementary strengths of human\nannotation quality and AI scalability. In this pipeline, humans provide\nverified annotations, while large language models perform automatic curation\nbased on human guidance. Training on this preference mixture, we introduce\nSkywork-Reward-V2, a suite of eight reward models ranging from 0.6B to 8B\nparameters, trained on a carefully curated subset of 26 million preference\npairs from SynPref-40M. We demonstrate that Skywork-Reward-V2 is versatile\nacross a wide range of capabilities, including alignment with human\npreferences, objective correctness, safety, resistance to stylistic biases, and\nbest-of-N scaling, achieving state-of-the-art performance across seven major\nreward model benchmarks. Ablation studies confirm that the effectiveness of our\napproach stems not only from data scale but also from high-quality curation.\nThe Skywork-Reward-V2 series represents substantial progress in open reward\nmodels, highlighting the untapped potential of existing preference datasets and\ndemonstrating how human-AI curation synergy can unlock significantly higher\ndata quality.",
        "url": "http://arxiv.org/abs/2507.01352v2",
        "pdf_url": "http://arxiv.org/pdf/2507.01352v2",
        "arxiv_id": "2507.01352v2",
        "authors": [
            "Chris Yuhao Liu",
            "Liang Zeng",
            "Yuzhen Xiao",
            "Jujie He",
            "Jiacai Liu",
            "Chaojie Wang",
            "Rui Yan",
            "Wei Shen",
            "Fuxiang Zhang",
            "Jiacheng Xu",
            "Yang Liu",
            "Yahui Zhou"
        ],
        "submitted": "2025-07-02 04:40:29",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on reward models and preference datasets, which is not directly related to the user's interests in Information Retrieval, Search technologies, and query understanding. While it mentions human-AI synergy, it does not seem to be relevant to the user's specific areas of interest, such as ranking models, user behavior modeling, and deep semantic understanding."
    },
    {
        "title": "Rethinking All Evidence: Enhancing Trustworthy Retrieval-Augmented Generation via Conflict-Driven Summarization",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by\nintegrating their parametric knowledge with external retrieved content.\nHowever, knowledge conflicts caused by internal inconsistencies or noisy\nretrieved content can severely undermine the generation reliability of RAG\nsystems.In this work, we argue that LLMs should rethink all evidence, including\nboth retrieved content and internal knowledge, before generating responses.We\npropose CARE-RAG (Conflict-Aware and Reliable Evidence for RAG), a novel\nframework that improves trustworthiness through Conflict-Driven Summarization\nof all available evidence.CARE-RAG first derives parameter-aware evidence by\ncomparing parameter records to identify diverse internal perspectives. It then\nrefines retrieved evidences to produce context-aware evidence, removing\nirrelevant or misleading content. To detect and summarize conflicts, we distill\na 3B LLaMA3.2 model to perform conflict-driven summarization, enabling reliable\nsynthesis across multiple sources.To further ensure evaluation integrity, we\nintroduce a QA Repair step to correct outdated or ambiguous benchmark\nanswers.Experiments on revised QA datasets with retrieval data show that\nCARE-RAG consistently outperforms strong RAG baselines, especially in scenarios\nwith noisy or conflicting evidence.",
        "url": "http://arxiv.org/abs/2507.01281v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01281v1",
        "arxiv_id": "2507.01281v1",
        "authors": [
            "Juan Chen",
            "Baolong Bi",
            "Wei Zhang",
            "Jingyan Sui",
            "Xiaofei Zhu",
            "Yuanzhuo Wang",
            "Lingrui Mei",
            "Shenghua Liu"
        ],
        "submitted": "2025-07-02 01:39:49",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on Retrieval-Augmented Generation (RAG) and proposes a novel framework, CARE-RAG, to improve trustworthiness through Conflict-Driven Summarization. While it touches on information retrieval and external content, the primary focus is on language models and generation, which is not directly related to the user's interests in query understanding, ranking models, and user behavior modeling in the context of search technologies."
    },
    {
        "title": "MiCoTA: Bridging the Learnability Gap with Intermediate CoT and Teacher Assistants",
        "abstract": "Large language models (LLMs) excel at reasoning tasks requiring long thought\nsequences for planning, reflection, and refinement. However, their substantial\nmodel size and high computational demands are impractical for widespread\ndeployment. Yet, small language models (SLMs) often struggle to learn long-form\nCoT reasoning due to their limited capacity, a phenomenon we refer to as the\n\"SLMs Learnability Gap\". To address this, we introduce\n\\textbf{Mi}d-\\textbf{Co}T \\textbf{T}eacher \\textbf{A}ssistant Distillation\n(MiCoTAl), a framework for improving long CoT distillation for SLMs. MiCoTA\nemploys intermediate-sized models as teacher assistants and utilizes\nintermediate-length CoT sequences to bridge both the capacity and reasoning\nlength gaps. Our experiments on downstream tasks demonstrate that although SLMs\ndistilled from large teachers can perform poorly, by applying MiCoTA, they\nachieve significant improvements in reasoning performance. Specifically,\nQwen2.5-7B-Instruct and Qwen2.5-3B-Instruct achieve an improvement of 3.47 and\n3.93 respectively on average score on AIME2024, AMC, Olympiad, MATH-500 and\nGSM8K benchmarks. To better understand the mechanism behind MiCoTA, we perform\na quantitative experiment demonstrating that our method produces data more\nclosely aligned with base SLM distributions. Our insights pave the way for\nfuture research into long-CoT data distillation for SLMs.",
        "url": "http://arxiv.org/abs/2507.01887v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01887v1",
        "arxiv_id": "2507.01887v1",
        "authors": [
            "Dongyi Ding",
            "Tiannan Wang",
            "Chenghao Zhu",
            "Meiling Tao",
            "Yuchen Eleanor Jiang",
            "Wangchunshu Zhou"
        ],
        "submitted": "2025-07-02 16:57:01",
        "source": "arxiv",
        "comment": "Work in progress",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on large and small language models, distillation, and teacher assistants, which is not directly related to information retrieval, search technologies, or query understanding. The concepts and techniques discussed in the paper are primarily from the natural language processing domain, but they do not seem to be applicable to the user's research interests."
    },
    {
        "title": "LoRA Fine-Tuning Without GPUs: A CPU-Efficient Meta-Generation Framework for LLMs",
        "abstract": "Low-Rank Adapters (LoRAs) have transformed the fine-tuning of Large Language\nModels (LLMs) by enabling parameter-efficient updates. However, their\nwidespread adoption remains limited by the reliance on GPU-based training. In\nthis work, we propose a theoretically grounded approach to LoRA fine-tuning\ndesigned specifically for users with limited computational resources,\nparticularly those restricted to standard laptop CPUs. Our method learns a\nmeta-operator that maps any input dataset, represented as a probability\ndistribution, to a set of LoRA weights by leveraging a large bank of\npre-trained adapters for the Mistral-7B-Instruct-v0.2 model. Instead of\nperforming new gradient-based updates, our pipeline constructs adapters via\nlightweight combinations of existing LoRAs directly on CPU. While the resulting\nadapters do not match the performance of GPU-trained counterparts, they\nconsistently outperform the base Mistral model on downstream tasks, offering a\npractical and accessible alternative to traditional GPU-based fine-tuning.",
        "url": "http://arxiv.org/abs/2507.01806v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01806v1",
        "arxiv_id": "2507.01806v1",
        "authors": [
            "Reza Arabpour",
            "Haitz Sáez de Ocáriz Borde",
            "Anastasis Kratsios"
        ],
        "submitted": "2025-07-02 15:24:47",
        "source": "arxiv",
        "comment": "5-page main paper (excluding references) + 11-page appendix, 3\n  tables, 1 figure. Accepted to ICML 2025 Workshop on Efficient Systems for\n  Foundation Models",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on LoRA fine-tuning for Large Language Models, which is not directly related to Information Retrieval, Search technologies, or query understanding. The paper's abstract does not mention any relevance to user behavior modeling, ranking models, or real-time relevance optimization, making it an off-topic paper for the user's research interests."
    },
    {
        "title": "Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI",
        "abstract": "Patents contain rich technical knowledge that can inspire innovative product\nideas, yet accessing and interpreting this information remains a challenge.\nThis work explores the use of Large Language Models (LLMs) and autonomous\nagents to mine and generate product concepts from a given patent. In this work,\nwe design Agent Ideate, a framework for automatically generating product-based\nbusiness ideas from patents. We experimented with open-source LLMs and\nagent-based architectures across three domains: Computer Science, Natural\nLanguage Processing, and Material Chemistry. Evaluation results show that the\nagentic approach consistently outperformed standalone LLMs in terms of idea\nquality, relevance, and novelty. These findings suggest that combining LLMs\nwith agentic workflows can significantly enhance the innovation pipeline by\nunlocking the untapped potential of business idea generation from patent data.",
        "url": "http://arxiv.org/abs/2507.01717v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01717v1",
        "arxiv_id": "2507.01717v1",
        "authors": [
            "Gopichand Kanumolu",
            "Ashok Urlana",
            "Charaka Vinayak Kumar",
            "Bala Mallikarjunarao Garlapati"
        ],
        "submitted": "2025-07-02 13:47:17",
        "source": "arxiv",
        "comment": "AgentScen Workshop, IJCAI 2025",
        "score": 3,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the use of Large Language Models and autonomous agents to generate product concepts from patents, which is related to information retrieval and NLP. However, the focus on patent data and business idea generation is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat tangential, but the use of LLMs and agent-based architectures is of interest."
    },
    {
        "title": "Data Agent: A Holistic Architecture for Orchestrating Data+AI Ecosystems",
        "abstract": "Traditional Data+AI systems utilize data-driven techniques to optimize\nperformance, but they rely heavily on human experts to orchestrate system\npipelines, enabling them to adapt to changes in data, queries, tasks, and\nenvironments. For instance, while there are numerous data science tools\navailable, developing a pipeline planning system to coordinate these tools\nremains challenging. This difficulty arises because existing Data+AI systems\nhave limited capabilities in semantic understanding, reasoning, and planning.\nFortunately, we have witnessed the success of large language models (LLMs) in\nenhancing semantic understanding, reasoning, and planning abilities. It is\ncrucial to incorporate LLM techniques to revolutionize data systems for\norchestrating Data+AI applications effectively.\n  To achieve this, we propose the concept of a 'Data Agent' - a comprehensive\narchitecture designed to orchestrate Data+AI ecosystems, which focuses on\ntackling data-related tasks by integrating knowledge comprehension, reasoning,\nand planning capabilities. We delve into the challenges involved in designing\ndata agents, such as understanding data/queries/environments/tools,\norchestrating pipelines/workflows, optimizing and executing pipelines, and\nfostering pipeline self-reflection. Furthermore, we present examples of data\nagent systems, including a data science agent, data analytics agents (such as\nunstructured data analytics agent, semantic structured data analytics agent,\ndata lake analytics agent, and multi-modal data analytics agent), and a\ndatabase administrator (DBA) agent. We also outline several open challenges\nassociated with designing data agent systems.",
        "url": "http://arxiv.org/abs/2507.01599v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01599v1",
        "arxiv_id": "2507.01599v1",
        "authors": [
            "Zhaoyan Sun",
            "Jiayi Wang",
            "Xinyang Zhao",
            "Jiachi Wang",
            "Guoliang Li"
        ],
        "submitted": "2025-07-02 11:04:49",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a Data Agent architecture for orchestrating Data+AI ecosystems, focusing on integrating knowledge comprehension, reasoning, and planning capabilities. While it touches on semantic understanding, it does not specifically address query understanding, ranking models, or user behavior modeling, which are core areas of interest for you. The paper's focus on data science and analytics agents is somewhat related to your background in e-commerce, but it does not directly align with your primary research interests in Information Retrieval and Search technologies."
    },
    {
        "title": "Following the Clues: Experiments on Person Re-ID using Cross-Modal Intelligence",
        "abstract": "The collection and release of street-level recordings as Open Data play a\nvital role in advancing autonomous driving systems and AI research. However,\nthese datasets pose significant privacy risks, particularly for pedestrians,\ndue to the presence of Personally Identifiable Information (PII) that extends\nbeyond biometric traits such as faces. In this paper, we present cRID, a novel\ncross-modal framework combining Large Vision-Language Models, Graph Attention\nNetworks, and representation learning to detect textual describable clues of\nPII and enhance person re-identification (Re-ID). Our approach focuses on\nidentifying and leveraging interpretable features, enabling the detection of\nsemantically meaningful PII beyond low-level appearance cues. We conduct a\nsystematic evaluation of PII presence in person image datasets. Our experiments\nshow improved performance in practical cross-dataset Re-ID scenarios, notably\nfrom Market-1501 to CUHK03-np (detected), highlighting the framework's\npractical utility. Code is available at https://github.com/RAufschlaeger/cRID.",
        "url": "http://arxiv.org/abs/2507.01504v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01504v1",
        "arxiv_id": "2507.01504v1",
        "authors": [
            "Robert Aufschläger",
            "Youssef Shoeb",
            "Azarm Nowzad",
            "Michael Heigl",
            "Fabian Bally",
            "Martin Schramm"
        ],
        "submitted": "2025-07-02 09:10:33",
        "source": "arxiv",
        "comment": "accepted for publication at the 2025 IEEE 28th International\n  Conference on Intelligent Transportation Systems (ITSC 2025), taking place\n  during November 18-21, 2025 in Gold Coast, Australia",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The topic of person re-identification using cross-modal intelligence is outside your areas of focus, and the paper does not mention query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Evaluating the Effectiveness of Direct Preference Optimization for Personalizing German Automatic Text Simplifications for Persons with Intellectual Disabilities",
        "abstract": "Automatic text simplification (ATS) aims to enhance language accessibility\nfor various target groups, particularly persons with intellectual disabilities.\nRecent advancements in generative AI, especially large language models (LLMs),\nhave substantially improved the quality of machine-generated text\nsimplifications, thereby mitigating information barriers for the target group.\nHowever, existing LLM-based ATS systems do not incorporate preference feedback\non text simplifications during training, resulting in a lack of personalization\ntailored to the specific needs of target group representatives.\n  In this work, we extend the standard supervised fine-tuning (SFT) approach\nfor adapting LLM-based ATS models by leveraging a computationally efficient LLM\nalignment technique -- direct preference optimization (DPO). Specifically, we\npost-train LLM-based ATS models using human feedback collected from persons\nwith intellectual disabilities, reflecting their preferences on paired text\nsimplifications generated by mainstream LLMs. Furthermore, we propose a\npipeline for developing personalized LLM-based ATS systems, encompassing data\ncollection, model selection, SFT and DPO post-training, and evaluation. Our\nfindings underscore the necessity of active participation of target group\npersons in designing personalized AI accessibility solutions aligned with human\nexpectations. This work represents a step towards personalizing inclusive AI\nsystems at the target-group level, incorporating insights not only from text\nsimplification experts but also from target group persons themselves.",
        "url": "http://arxiv.org/abs/2507.01479v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01479v1",
        "arxiv_id": "2507.01479v1",
        "authors": [
            "Yingqiang Gao",
            "Kaede Johnson",
            "David Froehlich",
            "Luisa Carrer",
            "Sarah Ebling"
        ],
        "submitted": "2025-07-02 08:43:06",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'personalization' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on automatic text simplification for persons with intellectual disabilities, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions large language models, the application is not in the context of search or retrieval, and the paper does not address ranking models or user behavior modeling."
    },
    {
        "title": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading",
        "abstract": "Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions.",
        "url": "http://arxiv.org/abs/2507.01431v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01431v1",
        "arxiv_id": "2507.01431v1",
        "authors": [
            "Yoonseok Yang",
            "Minjune Kim",
            "Marlon Rondinelli",
            "Keren Shao"
        ],
        "submitted": "2025-07-02 07:33:19",
        "source": "arxiv",
        "comment": "7 pages, 5 figues, 1 table",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'www' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on handwritten STEM grading, which is not directly related to information retrieval, search technologies, or natural language processing. While it uses AI-powered language models, the application is specific to grading and does not address query understanding, ranking models, or user behavior modeling, making it only loosely relevant to the user's research interests."
    },
    {
        "title": "MEGA: xLSTM with Multihead Exponential Gated Fusion for Precise Aspect-based Sentiment Analysis",
        "abstract": "Aspect-based Sentiment Analysis (ABSA) is a critical Natural Language\nProcessing (NLP) task that extracts aspects from text and determines their\nassociated sentiments, enabling fine-grained analysis of user opinions.\nExisting ABSA methods struggle to balance computational efficiency with high\nperformance: deep learning models often lack global context, transformers\ndemand significant computational resources, and Mamba-based approaches face\nCUDA dependency and diminished local correlations. Recent advancements in\nExtended Long Short-Term Memory (xLSTM) models, particularly their efficient\nmodeling of long-range dependencies, have significantly advanced the NLP\ncommunity. However, their potential in ABSA remains untapped. To this end, we\npropose xLSTM with Multihead Exponential Gated Fusion (MEGA), a novel framework\nintegrating a bi-directional mLSTM architecture with forward and partially\nflipped backward (PF-mLSTM) streams. The PF-mLSTM enhances localized context\nmodeling by processing the initial sequence segment in reverse with dedicated\nparameters, preserving critical short-range patterns. We further introduce an\nmLSTM-based multihead cross exponential gated fusion mechanism (MECGAF) that\ndynamically combines forward mLSTM outputs as query and key with PF-mLSTM\noutputs as value, optimizing short-range dependency capture while maintaining\nglobal context and efficiency. Experimental results on three benchmark datasets\ndemonstrate that MEGA outperforms state-of-the-art baselines, achieving\nsuperior accuracy and efficiency in ABSA tasks.",
        "url": "http://arxiv.org/abs/2507.01213v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01213v1",
        "arxiv_id": "2507.01213v1",
        "authors": [
            "Adamu Lawan",
            "Juhua Pu",
            "Haruna Yunusa",
            "Jawad Muhammad",
            "Muhammad Lawan"
        ],
        "submitted": "2025-07-01 22:21:33",
        "source": "arxiv",
        "comment": "6, 1 figure",
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Aspect-based Sentiment Analysis, which is a topic in Natural Language Processing (NLP), but it does not directly relate to Information Retrieval (IR) or Search technologies, which are the user's primary research interests. The paper's emphasis on deep learning models and their application in ABSA does not align with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "STELLA: Self-Evolving LLM Agent for Biomedical Research",
        "abstract": "The rapid growth of biomedical data, tools, and literature has created a\nfragmented research landscape that outpaces human expertise. While AI agents\noffer a solution, they typically rely on static, manually curated toolsets,\nlimiting their ability to adapt and scale. Here, we introduce STELLA, a\nself-evolving AI agent designed to overcome these limitations. STELLA employs a\nmulti-agent architecture that autonomously improves its own capabilities\nthrough two core mechanisms: an evolving Template Library for reasoning\nstrategies and a dynamic Tool Ocean that expands as a Tool Creation Agent\nautomatically discovers and integrates new bioinformatics tools. This allows\nSTELLA to learn from experience. We demonstrate that STELLA achieves\nstate-of-the-art accuracy on a suite of biomedical benchmarks, scoring\napproximately 26\\% on Humanity's Last Exam: Biomedicine, 54\\% on LAB-Bench:\nDBQA, and 63\\% on LAB-Bench: LitQA, outperforming leading models by up to 6\npercentage points. More importantly, we show that its performance\nsystematically improves with experience; for instance, its accuracy on the\nHumanity's Last Exam benchmark almost doubles with increased trials. STELLA\nrepresents a significant advance towards AI Agent systems that can learn and\ngrow, dynamically scaling their expertise to accelerate the pace of biomedical\ndiscovery.",
        "url": "http://arxiv.org/abs/2507.02004v1",
        "pdf_url": "http://arxiv.org/pdf/2507.02004v1",
        "arxiv_id": "2507.02004v1",
        "authors": [
            "Ruofan Jin",
            "Zaixi Zhang",
            "Mengdi Wang",
            "Le Cong"
        ],
        "submitted": "2025-07-01 20:52:01",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests, as it focuses on biomedical research and AI agents, which is outside your primary area of interest in Information Retrieval and Search technologies. The paper's emphasis on bioinformatics tools and biomedical benchmarks is also not aligned with your background in e-commerce and interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "The Thin Line Between Comprehension and Persuasion in LLMs",
        "abstract": "Large language models (LLMs) are excellent at maintaining high-level,\nconvincing dialogues. They are being fast deployed as chatbots and evaluators\nin sensitive areas, such as peer review and mental health applications. This,\nalong with the disparate accounts on their reasoning capabilities, calls for a\ncloser examination of LLMs and their comprehension of dialogue. In this work we\nbegin by evaluating LLMs' ability to maintain a debate--one of the purest yet\nmost complex forms of human communication. Then we measure how this capability\nrelates to their understanding of what is being talked about, namely, their\ncomprehension of dialogical structures and the pragmatic context. We find that\nLLMs are capable of maintaining coherent, persuasive debates, often swaying the\nbeliefs of participants and audiences alike. We also note that awareness or\nsuspicion of AI involvement encourage people to be more critical of the\narguments made. When polling LLMs on their comprehension of deeper structures\nof dialogue, however, they cannot demonstrate said understanding. Our findings\ntie the shortcomings of LLMs-as-evaluators to their (in)ability to understand\nthe context. More broadly, for the field of argumentation theory we posit that,\nif an agent can convincingly maintain a dialogue, it is not necessary for it to\nknow what it is talking about. Hence, the modelling of pragmatic context and\ncoherence are secondary to effectiveness.",
        "url": "http://arxiv.org/abs/2507.01936v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01936v1",
        "arxiv_id": "2507.01936v1",
        "authors": [
            "Adrian de Wynter",
            "Tangming Yuan"
        ],
        "submitted": "2025-07-02 17:46:56",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the capabilities of Large Language Models (LLMs) in maintaining debates and understanding dialogue structures, but it does not directly relate to the user's research interests in Information Retrieval, Search technologies, and query understanding. While the paper touches on the topic of comprehension, it is more focused on the pragmatic context and coherence of dialogue, which is not a central match for the user's interests."
    },
    {
        "title": "DIY-MKG: An LLM-Based Polyglot Language Learning System",
        "abstract": "Existing language learning tools, even those powered by Large Language Models\n(LLMs), often lack support for polyglot learners to build linguistic\nconnections across vocabularies in multiple languages, provide limited\ncustomization for individual learning paces or needs, and suffer from\ndetrimental cognitive offloading. To address these limitations, we design\nDo-It-Yourself Multilingual Knowledge Graph (DIY-MKG), an open-source system\nthat supports polyglot language learning. DIY-MKG allows the user to build\npersonalized vocabulary knowledge graphs, which are constructed by selective\nexpansion with related words suggested by an LLM. The system further enhances\nlearning through rich annotation capabilities and an adaptive review module\nthat leverages LLMs for dynamic, personalized quiz generation. In addition,\nDIY-MKG allows users to flag incorrect quiz questions, simultaneously\nincreasing user engagement and providing a feedback loop for prompt refinement.\nOur evaluation of LLM-based components in DIY-MKG shows that vocabulary\nexpansion is reliable and fair across multiple languages, and that the\ngenerated quizzes are highly accurate, validating the robustness of DIY-MKG.",
        "url": "http://arxiv.org/abs/2507.01872v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01872v1",
        "arxiv_id": "2507.01872v1",
        "authors": [
            "Kenan Tang",
            "Yanhong Li",
            "Yao Qin"
        ],
        "submitted": "2025-07-02 16:38:51",
        "source": "arxiv",
        "comment": "Submitted to EMNLP 2025 System Demonstration",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on language learning and knowledge graph construction, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it mentions Large Language Models, the application is in a different domain and does not address query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Eka-Eval : A Comprehensive Evaluation Framework for Large Language Models in Indian Languages",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for evaluation frameworks that go beyond English centric benchmarks and\naddress the requirements of linguistically diverse regions such as India. We\npresent EKA-EVAL, a unified and production-ready evaluation framework that\nintegrates over 35 benchmarks, including 10 Indic-specific datasets, spanning\ncategories like reasoning, mathematics, tool use, long-context understanding,\nand reading comprehension. Compared to existing Indian language evaluation\ntools, EKA-EVAL offers broader benchmark coverage, with built-in support for\ndistributed inference, quantization, and multi-GPU usage. Our systematic\ncomparison positions EKA-EVAL as the first end-to-end, extensible evaluation\nsuite tailored for both global and Indic LLMs, significantly lowering the\nbarrier to multilingual benchmarking. The framework is open-source and publicly\navailable at https://github.com/lingo-iitgn/ eka-eval and a part of ongoing EKA\ninitiative (https://eka.soket.ai), which aims to scale up to over 100\nbenchmarks and establish a robust, multilingual evaluation ecosystem for LLMs.",
        "url": "http://arxiv.org/abs/2507.01853v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01853v1",
        "arxiv_id": "2507.01853v1",
        "authors": [
            "Samridhi Raj Sinha",
            "Rajvee Sheth",
            "Abhishek Upperwal",
            "Mayank Singh"
        ],
        "submitted": "2025-07-02 16:07:54",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on evaluating large language models in Indian languages, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the topic of language models, the scope is limited to evaluation frameworks and does not explore ranking models, user behavior modeling, or real-time relevance optimization, which are core areas of interest."
    },
    {
        "title": "Low-Perplexity LLM-Generated Sequences and Where To Find Them",
        "abstract": "As Large Language Models (LLMs) become increasingly widespread, understanding\nhow specific training data shapes their outputs is crucial for transparency,\naccountability, privacy, and fairness. To explore how LLMs leverage and\nreplicate their training data, we introduce a systematic approach centered on\nanalyzing low-perplexity sequences - high-probability text spans generated by\nthe model. Our pipeline reliably extracts such long sequences across diverse\ntopics while avoiding degeneration, then traces them back to their sources in\nthe training data. Surprisingly, we find that a substantial portion of these\nlow-perplexity spans cannot be mapped to the corpus. For those that do match,\nwe quantify the distribution of occurrences across source documents,\nhighlighting the scope and nature of verbatim recall and paving a way toward\nbetter understanding of how LLMs training data impacts their behavior.",
        "url": "http://arxiv.org/abs/2507.01844v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01844v1",
        "arxiv_id": "2507.01844v1",
        "authors": [
            "Arthur Wuhrmann",
            "Anastasiia Kucherenko",
            "Andrei Kucharavy"
        ],
        "submitted": "2025-07-02 15:58:51",
        "source": "arxiv",
        "comment": "Camera-ready version. Accepted to ACL 2025. 10 pages, 4 figures, 6\n  tables",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Large Language Models (LLMs) and their training data, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on the topic of model behavior, it does not address ranking models, user behavior modeling, or real-time relevance optimization, making it only loosely relevant to your research interests."
    },
    {
        "title": "Probing Evaluation Awareness of Language Models",
        "abstract": "Language models can distinguish between testing and deployment phases -- a\ncapability known as evaluation awareness. This has significant safety and\npolicy implications, potentially undermining the reliability of evaluations\nthat are central to AI governance frameworks and voluntary industry\ncommitments. In this paper, we study evaluation awareness in\nLlama-3.3-70B-Instruct. We show that linear probes can separate real-world\nevaluation and deployment prompts, suggesting that current models internally\nrepresent this distinction. We also find that current safety evaluations are\ncorrectly classified by the probes, suggesting that they already appear\nartificial or inauthentic to models. Our findings underscore the importance of\nensuring trustworthy evaluations and understanding deceptive capabilities. More\nbroadly, our work showcases how model internals may be leveraged to support\nblackbox methods in safety audits, especially for future models more competent\nat evaluation awareness and deception.",
        "url": "http://arxiv.org/abs/2507.01786v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01786v1",
        "arxiv_id": "2507.01786v1",
        "authors": [
            "Jord Nguyen",
            "Khiem Hoang",
            "Carlo Leonardo Attubato",
            "Felix Hofstätter"
        ],
        "submitted": "2025-07-02 15:12:43",
        "source": "arxiv",
        "comment": "Technical AI Governance Workshop, ICML (Poster)",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus is on language models, evaluation awareness, and safety evaluations, which are outside the scope of the user's primary research interests."
    },
    {
        "title": "Tuning without Peeking: Provable Privacy and Generalization Bounds for LLM Post-Training",
        "abstract": "Gradient-based optimization is the workhorse of deep learning, offering\nefficient and scalable training via backpropagation. However, its reliance on\nlarge volumes of labeled data raises privacy and security concerns such as\nsusceptibility to data poisoning attacks and the risk of overfitting. In\ncontrast, black box optimization methods, which treat the model as an opaque\nfunction, relying solely on function evaluations to guide optimization, offer a\npromising alternative in scenarios where data access is restricted, adversarial\nrisks are high, or overfitting is a concern. However, black box methods also\npose significant challenges, including poor scalability to high-dimensional\nparameter spaces, as prevalent in large language models (LLMs), and high\ncomputational costs due to reliance on numerous model evaluations. This paper\nintroduces BBoxER, an evolutionary black-box method for LLM post-training that\ninduces an information bottleneck via implicit compression of the training\ndata. Leveraging the tractability of information flow, we provide strong\ntheoretical bounds on generalization, differential privacy, susceptibility to\ndata poisoning attacks, and robustness to extraction attacks. BBoxER operates\non top of pre-trained LLMs, offering a lightweight and modular enhancement\nsuitable for deployment in restricted or privacy-sensitive environments, in\naddition to non-vacuous generalization guarantees. In experiments with LLMs, we\ndemonstrate empirically that Retrofitting methods are able to learn, showing\nhow a few iterations of BBoxER improve performance and generalize well on a\nbenchmark of reasoning datasets. This positions BBoxER as an attractive add-on\non top of gradient-based optimization.",
        "url": "http://arxiv.org/abs/2507.01752v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01752v1",
        "arxiv_id": "2507.01752v1",
        "authors": [
            "Ismail Labiad",
            "Mathurin Videau",
            "Matthieu Kowalski",
            "Marc Schoenauer",
            "Alessandro Leite",
            "Julia Kempe",
            "Olivier Teytaud"
        ],
        "submitted": "2025-07-02 14:29:30",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on black box optimization methods for large language models, which is not directly related to information retrieval, search technologies, or query understanding. The paper's emphasis on privacy and security concerns is also not a primary focus of your research interests."
    },
    {
        "title": "Stereotype Detection as a Catalyst for Enhanced Bias Detection: A Multi-Task Learning Approach",
        "abstract": "Bias and stereotypes in language models can cause harm, especially in\nsensitive areas like content moderation and decision-making. This paper\naddresses bias and stereotype detection by exploring how jointly learning these\ntasks enhances model performance. We introduce StereoBias, a unique dataset\nlabeled for bias and stereotype detection across five categories: religion,\ngender, socio-economic status, race, profession, and others, enabling a deeper\nstudy of their relationship. Our experiments compare encoder-only models and\nfine-tuned decoder-only models using QLoRA. While encoder-only models perform\nwell, decoder-only models also show competitive results. Crucially, joint\ntraining on bias and stereotype detection significantly improves bias detection\ncompared to training them separately. Additional experiments with sentiment\nanalysis confirm that the improvements stem from the connection between bias\nand stereotypes, not multi-task learning alone. These findings highlight the\nvalue of leveraging stereotype information to build fairer and more effective\nAI systems.",
        "url": "http://arxiv.org/abs/2507.01715v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01715v1",
        "arxiv_id": "2507.01715v1",
        "authors": [
            "Aditya Tomar",
            "Rudra Murthy",
            "Pushpak Bhattacharyya"
        ],
        "submitted": "2025-07-02 13:46:00",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on bias and stereotype detection in language models, which is a topic related to Natural Language Processing (NLP). While it explores multi-task learning, it does not directly address query understanding, ranking models, or user behavior modeling, which are core interests in Information Retrieval (IR). The paper's findings on leveraging stereotype information to build fairer AI systems are interesting but not directly applicable to the user's research areas."
    },
    {
        "title": "Adapting Language Models to Indonesian Local Languages: An Empirical Study of Language Transferability on Zero-Shot Settings",
        "abstract": "In this paper, we investigate the transferability of pre-trained language\nmodels to low-resource Indonesian local languages through the task of sentiment\nanalysis. We evaluate both zero-shot performance and adapter-based transfer on\nten local languages using models of different types: a monolingual Indonesian\nBERT, multilingual models such as mBERT and XLM-R, and a modular adapter-based\napproach called MAD-X. To better understand model behavior, we group the target\nlanguages into three categories: seen (included during pre-training), partially\nseen (not included but linguistically related to seen languages), and unseen\n(absent and unrelated in pre-training data). Our results reveal clear\nperformance disparities across these groups: multilingual models perform best\non seen languages, moderately on partially seen ones, and poorly on unseen\nlanguages. We find that MAD-X significantly improves performance, especially\nfor seen and partially seen languages, without requiring labeled data in the\ntarget language. Additionally, we conduct a further analysis on tokenization\nand show that while subword fragmentation and vocabulary overlap with\nIndonesian correlate weakly with prediction quality, they do not fully explain\nthe observed performance. Instead, the most consistent predictor of transfer\nsuccess is the model's prior exposure to the language, either directly or\nthrough a related language.",
        "url": "http://arxiv.org/abs/2507.01645v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01645v1",
        "arxiv_id": "2507.01645v1",
        "authors": [
            "Rifki Afina Putri"
        ],
        "submitted": "2025-07-02 12:17:55",
        "source": "arxiv",
        "comment": "AMLDS 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on adapting language models to Indonesian local languages, which is not directly related to Information Retrieval, Search technologies, or query understanding. Although it touches on language transferability, it does not explore ranking models, user behavior modeling, or real-time relevance optimization, which are core aspects of your research interests."
    },
    {
        "title": "Self-Guided Process Reward Optimization with Redefined Step-wise Advantage for Process Reinforcement Learning",
        "abstract": "Process Reinforcement Learning~(PRL) has demonstrated considerable potential\nin enhancing the reasoning capabilities of Large Language Models~(LLMs).\nHowever, introducing additional process reward models incurs substantial\ncomputational overhead, and there is no unified theoretical framework for\nprocess-level advantage estimation. To bridge this gap, we propose\n\\textbf{S}elf-Guided \\textbf{P}rocess \\textbf{R}eward\n\\textbf{O}ptimization~(\\textbf{SPRO}), a novel framework that enables\nprocess-aware RL through two key innovations: (1) we first theoretically\ndemonstrate that process rewards can be derived intrinsically from the policy\nmodel itself, and (2) we introduce well-defined cumulative process rewards and\n\\textbf{M}asked \\textbf{S}tep \\textbf{A}dvantage (\\textbf{MSA}), which\nfacilitates rigorous step-wise action advantage estimation within shared-prompt\nsampling groups. Our experimental results demonstrate that SPRO outperforms\nvaniila GRPO with 3.4x higher training efficiency and a 17.5\\% test accuracy\nimprovement. Furthermore, SPRO maintains a stable and elevated policy entropy\nthroughout training while reducing the average response length by approximately\n$1/3$, evidencing sufficient exploration and prevention of reward hacking.\nNotably, SPRO incurs no additional computational overhead compared to\noutcome-supervised RL methods such as GRPO, which benefit industrial\nimplementation.",
        "url": "http://arxiv.org/abs/2507.01551v2",
        "pdf_url": "http://arxiv.org/pdf/2507.01551v2",
        "arxiv_id": "2507.01551v2",
        "authors": [
            "Wu Fei",
            "Hao Kong",
            "Shuxian Liang",
            "Yang Lin",
            "Yibo Yang",
            "Jing Tang",
            "Lei Chen",
            "Xiansheng Hua"
        ],
        "submitted": "2025-07-02 10:05:14",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Process Reinforcement Learning, which is not directly related to Information Retrieval or Search technologies. Although it mentions Large Language Models, the context is not relevant to query understanding, ranking models, or user behavior modeling. The paper's topics, such as process reward optimization and masked step advantage, are not aligned with the user's research interests."
    },
    {
        "title": "Crafting Hanzi as Narrative Bridges: An AI Co-Creation Workshop for Elderly Migrants",
        "abstract": "This paper explores how older adults, particularly aging migrants in urban\nChina, can engage AI-assisted co-creation to express personal narratives that\nare often fragmented, underrepresented, or difficult to verbalize. Through a\npilot workshop combining oral storytelling and the symbolic reconstruction of\nHanzi, participants shared memories of migration and recreated new character\nforms using Xiaozhuan glyphs, suggested by the Large Language Model (LLM),\ntogether with physical materials. Supported by human facilitation and a soft AI\npresence, participants transformed lived experience into visual and tactile\nexpressions without requiring digital literacy. This approach offers new\nperspectives on human-AI collaboration and aging by repositioning AI not as a\ncontent producer but as a supportive mechanism, and by supporting narrative\nagency within sociotechnical systems.",
        "url": "http://arxiv.org/abs/2507.01548v2",
        "pdf_url": "http://arxiv.org/pdf/2507.01548v2",
        "arxiv_id": "2507.01548v2",
        "authors": [
            "Wen Zhan",
            "Ziqun Hua",
            "Peiyue Lin",
            "Yunfei Chen"
        ],
        "submitted": "2025-07-02 10:00:12",
        "source": "arxiv",
        "comment": "A version of this manuscript has been submitted to the [IASDR 2025\n  Conference](https://iasdr2025.org/) and is currently under review",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The focus on AI-assisted co-creation, narrative bridges, and elderly migrants is outside the scope of your expertise and interests."
    },
    {
        "title": "LogitSpec: Accelerating Retrieval-based Speculative Decoding via Next Next Token Speculation",
        "abstract": "Speculative decoding (SD), where a small draft model is employed to propose\ndraft tokens in advance and then the target model validates them in parallel,\nhas emerged as a promising technique for LLM inference acceleration. Many\nendeavors to improve SD are to eliminate the need for a draft model and\ngenerate draft tokens in a retrieval-based manner in order to further alleviate\nthe drafting overhead and significantly reduce the difficulty in deployment and\napplications. However, retrieval-based SD relies on a matching paradigm to\nretrieval the most relevant reference as the draft tokens, where these methods\noften fail to find matched and accurate draft tokens. To address this\nchallenge, we propose LogitSpec to effectively expand the retrieval range and\nfind the most relevant reference as drafts. Our LogitSpec is motivated by the\nobservation that the logit of the last token can not only predict the next\ntoken, but also speculate the next next token. Specifically, LogitSpec\ngenerates draft tokens in two steps: (1) utilizing the last logit to speculate\nthe next next token; (2) retrieving relevant reference for both the next token\nand the next next token. LogitSpec is training-free and plug-and-play, which\ncan be easily integrated into existing LLM inference frameworks. Extensive\nexperiments on a wide range of text generation benchmarks demonstrate that\nLogitSpec can achieve up to 2.61 $\\times$ speedup and 3.28 mean accepted tokens\nper decoding step. Our code is available at\nhttps://github.com/smart-lty/LogitSpec.",
        "url": "http://arxiv.org/abs/2507.01449v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01449v1",
        "arxiv_id": "2507.01449v1",
        "authors": [
            "Tianyu Liu",
            "Qitan Lv",
            "Hao Li",
            "Xing Gao",
            "Xiao Sun"
        ],
        "submitted": "2025-07-02 08:08:30",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a technique for accelerating retrieval-based speculative decoding in language models, which is related to query understanding and ranking models in Information Retrieval. However, the focus is on language model inference acceleration rather than deep semantic understanding and real-time relevance optimization, which are core aspects of your research interests."
    },
    {
        "title": "Clinical NLP with Attention-Based Deep Learning for Multi-Disease Prediction",
        "abstract": "This paper addresses the challenges posed by the unstructured nature and\nhigh-dimensional semantic complexity of electronic health record texts. A deep\nlearning method based on attention mechanisms is proposed to achieve unified\nmodeling for information extraction and multi-label disease prediction. The\nstudy is conducted on the MIMIC-IV dataset. A Transformer-based architecture is\nused to perform representation learning over clinical text. Multi-layer\nself-attention mechanisms are employed to capture key medical entities and\ntheir contextual relationships. A Sigmoid-based multi-label classifier is then\napplied to predict multiple disease labels. The model incorporates a\ncontext-aware semantic alignment mechanism, enhancing its representational\ncapacity in typical medical scenarios such as label co-occurrence and sparse\ninformation. To comprehensively evaluate model performance, a series of\nexperiments were conducted, including baseline comparisons, hyperparameter\nsensitivity analysis, data perturbation studies, and noise injection tests.\nResults demonstrate that the proposed method consistently outperforms\nrepresentative existing approaches across multiple performance metrics. The\nmodel maintains strong generalization under varying data scales, interference\nlevels, and model depth configurations. The framework developed in this study\noffers an efficient algorithmic foundation for processing real-world clinical\ntexts and presents practical significance for multi-label medical text modeling\ntasks.",
        "url": "http://arxiv.org/abs/2507.01437v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01437v1",
        "arxiv_id": "2507.01437v1",
        "authors": [
            "Ting Xu",
            "Xiaoxiao Deng",
            "Xiandong Meng",
            "Haifeng Yang",
            "Yan Wu"
        ],
        "submitted": "2025-07-02 07:45:22",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to information retrieval and natural language processing, as it deals with processing and analyzing clinical text. However, the focus on clinical NLP and disease prediction is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "DARTS: A Dual-View Attack Framework for Targeted Manipulation in Federated Sequential Recommendation",
        "abstract": "Federated recommendation (FedRec) preserves user privacy by enabling\ndecentralized training of personalized models, but this architecture is\ninherently vulnerable to adversarial attacks. Significant research has been\nconducted on targeted attacks in FedRec systems, motivated by commercial and\nsocial influence considerations. However, much of this work has largely\noverlooked the differential robustness of recommendation models. Moreover, our\nempirical findings indicate that existing targeted attack methods achieve only\nlimited effectiveness in Federated Sequential Recommendation(FSR) tasks. Driven\nby these observations, we focus on investigating targeted attacks in FSR and\npropose a novel dualview attack framework, named DV-FSR. This attack method\nuniquely combines a sampling-based explicit strategy with a contrastive\nlearning-based implicit gradient strategy to orchestrate a coordinated attack.\nAdditionally, we introduce a specific defense mechanism tailored for targeted\nattacks in FSR, aiming to evaluate the mitigation effects of the attack method\nwe proposed. Extensive experiments validate the effectiveness of our proposed\napproach on representative sequential models. Our codes are publicly available.",
        "url": "http://arxiv.org/abs/2507.01383v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01383v1",
        "arxiv_id": "2507.01383v1",
        "authors": [
            "Qitao Qin",
            "Yucong Luo",
            "Zhibo Chu"
        ],
        "submitted": "2025-07-02 05:57:09",
        "source": "arxiv",
        "comment": "10 pages. arXiv admin note: substantial text overlap with\n  arXiv:2409.07500; text overlap with arXiv:2212.05399 by other authors",
        "score": 2,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on federated sequential recommendation and targeted attacks, which is not directly related to the user's interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions recommendation models, the context is different from the user's focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Symbolic or Numerical? Understanding Physics Problem Solving in Reasoning LLMs",
        "abstract": "Navigating the complexities of physics reasoning has long been a difficult\ntask for Large Language Models (LLMs), requiring a synthesis of profound\nconceptual understanding and adept problem-solving techniques. In this study,\nwe investigate the application of advanced instruction-tuned reasoning models,\nsuch as Deepseek-R1, to address a diverse spectrum of physics problems curated\nfrom the challenging SciBench benchmark. Our comprehensive experimental\nevaluation reveals the remarkable capabilities of reasoning models. Not only do\nthey achieve state-of-the-art accuracy in answering intricate physics\nquestions, but they also generate distinctive reasoning patterns that emphasize\non symbolic derivation. Furthermore, our findings indicate that even for these\nhighly sophisticated reasoning models, the strategic incorporation of few-shot\nprompting can still yield measurable improvements in overall accuracy,\nhighlighting the potential for continued performance gains.",
        "url": "http://arxiv.org/abs/2507.01334v2",
        "pdf_url": "http://arxiv.org/pdf/2507.01334v2",
        "arxiv_id": "2507.01334v2",
        "authors": [
            "Nifu Dan",
            "Yujun Cai",
            "Yiwei Wang"
        ],
        "submitted": "2025-07-02 03:51:16",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on the application of Large Language Models (LLMs) to physics problem-solving, which is not directly related to Information Retrieval, Search technologies, or Natural Language Processing. While it explores advanced instruction-tuned reasoning models, the topic is not aligned with the user's primary research interests."
    },
    {
        "title": "La RoSA: Enhancing LLM Efficiency via Layerwise Rotated Sparse Activation",
        "abstract": "Activation sparsity can reduce the computational overhead and memory\ntransfers during the forward pass of Large Language Model (LLM) inference.\nExisting methods face limitations, either demanding time-consuming recovery\ntraining that hinders real-world adoption, or relying on empirical\nmagnitude-based pruning, which causes fluctuating sparsity and unstable\ninference speed-up. This paper introduces LaRoSA (Layerwise Rotated Sparse\nActivation), a novel method for activation sparsification designed to improve\nLLM efficiency without requiring additional training or magnitude-based\npruning. We leverage layerwise orthogonal rotations to transform input\nactivations into rotated forms that are more suitable for sparsification. By\nemploying a Top-K selection approach within the rotated activations, we achieve\nconsistent model-level sparsity and reliable wall-clock time speed-up. LaRoSA\nis effective across various sizes and types of LLMs, demonstrating minimal\nperformance degradation and robust inference acceleration. Specifically, for\nLLaMA2-7B at 40% sparsity, LaRoSA achieves a mere 0.17 perplexity gap with a\nconsistent 1.30x wall-clock time speed-up, and reduces the accuracy gap in\nzero-shot tasks compared to the dense model to just 0.54%, while surpassing\nTEAL by 1.77% and CATS by 17.14%.",
        "url": "http://arxiv.org/abs/2507.01299v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01299v1",
        "arxiv_id": "2507.01299v1",
        "authors": [
            "Kai Liu",
            "Bowen Xu",
            "Shaoyu Wu",
            "Xin Chen",
            "Hao Zhou",
            "Yongliang Tao",
            "Lulu Hu"
        ],
        "submitted": "2025-07-02 02:36:03",
        "source": "arxiv",
        "comment": "ICML 2025 Acceptance",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Large Language Model (LLM) efficiency and activation sparsity, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper mentions NLP, it is primarily concerned with improving LLM efficiency, which is not a central theme in the user's research."
    },
    {
        "title": "Test-Time Scaling with Reflective Generative Model",
        "abstract": "We introduce our first reflective generative model MetaStone-S1, which\nobtains OpenAI o3's performance via the self-supervised process reward model\n(SPRM). Through sharing the backbone network and using task-specific heads for\nnext token prediction and process scoring respectively, SPRM successfully\nintegrates the policy model and process reward model(PRM) into a unified\ninterface without extra process annotation, reducing over 99% PRM parameters\nfor efficient reasoning. Equipped with SPRM, MetaStone-S1 is naturally suitable\nfor test time scaling (TTS), and we provide three reasoning effort modes (low,\nmedium, and high), based on the controllable thinking length. Moreover, we\nempirically establish a scaling law that reveals the relationship between total\nthinking computation and TTS performance. Experiments demonstrate that our\nMetaStone-S1 achieves comparable performance to OpenAI-o3-mini's series with\nonly 32B parameter size. To support the research community, we have\nopen-sourced MetaStone-S1 at https://github.com/MetaStone-AI/MetaStone-S1.",
        "url": "http://arxiv.org/abs/2507.01951v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01951v1",
        "arxiv_id": "2507.01951v1",
        "authors": [
            "Zixiao Wang",
            "Yuxin Wang",
            "Xiaorui Wang",
            "Mengting Xing",
            "Jie Gao",
            "Jianjun Xu",
            "Guangcan Liu",
            "Chenhui Jin",
            "Zhuo Wang",
            "Shengzhuo Zhang",
            "Hongtao Xie"
        ],
        "submitted": "2025-07-02 17:58:01",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on a reflective generative model for test-time scaling, which is not directly related to information retrieval, search technologies, or query understanding. While it involves some NLP concepts, the primary focus is on generative models and process reward models, which are not within the user's core research themes."
    },
    {
        "title": "AI4Research: A Survey of Artificial Intelligence for Scientific Research",
        "abstract": "Recent advancements in artificial intelligence (AI), particularly in large\nlanguage models (LLMs) such as OpenAI-o1 and DeepSeek-R1, have demonstrated\nremarkable capabilities in complex domains such as logical reasoning and\nexperimental coding. Motivated by these advancements, numerous studies have\nexplored the application of AI in the innovation process, particularly in the\ncontext of scientific research. These AI technologies primarily aim to develop\nsystems that can autonomously conduct research processes across a wide range of\nscientific disciplines. Despite these significant strides, a comprehensive\nsurvey on AI for Research (AI4Research) remains absent, which hampers our\nunderstanding and impedes further development in this field. To address this\ngap, we present a comprehensive survey and offer a unified perspective on\nAI4Research. Specifically, the main contributions of our work are as follows:\n(1) Systematic taxonomy: We first introduce a systematic taxonomy to classify\nfive mainstream tasks in AI4Research. (2) New frontiers: Then, we identify key\nresearch gaps and highlight promising future directions, focusing on the rigor\nand scalability of automated experiments, as well as the societal impact. (3)\nAbundant applications and resources: Finally, we compile a wealth of resources,\nincluding relevant multidisciplinary applications, data corpora, and tools. We\nhope our work will provide the research community with quick access to these\nresources and stimulate innovative breakthroughs in AI4Research.",
        "url": "http://arxiv.org/abs/2507.01903v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01903v1",
        "arxiv_id": "2507.01903v1",
        "authors": [
            "Qiguang Chen",
            "Mingda Yang",
            "Libo Qin",
            "Jinhao Liu",
            "Zheng Yan",
            "Jiannan Guan",
            "Dengyun Peng",
            "Yiyan Ji",
            "Hanjing Li",
            "Mengkang Hu",
            "Yimeng Zhang",
            "Yihao Liang",
            "Yuhang Zhou",
            "Jiaqi Wang",
            "Zhi Chen",
            "Wanxiang Che"
        ],
        "submitted": "2025-07-02 17:19:20",
        "source": "arxiv",
        "comment": "Preprint",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The focus is on Artificial Intelligence for Scientific Research, which is a broader and more general topic. While the paper mentions language models, it does not specifically discuss query understanding, ranking models, or user behavior modeling, which are key areas of interest for you."
    },
    {
        "title": "The Anatomy of Evidence: An Investigation Into Explainable ICD Coding",
        "abstract": "Automatic medical coding has the potential to ease documentation and billing\nprocesses. For this task, transparency plays an important role for medical\ncoders and regulatory bodies, which can be achieved using explainability\nmethods. However, the evaluation of these approaches has been mostly limited to\nshort text and binary settings due to a scarcity of annotated data. Recent\nefforts by Cheng et al. (2023) have introduced the MDACE dataset, which\nprovides a valuable resource containing code evidence in clinical records. In\nthis work, we conduct an in-depth analysis of the MDACE dataset and perform\nplausibility evaluation of current explainable medical coding systems from an\napplied perspective. With this, we contribute to a deeper understanding of\nautomatic medical coding and evidence extraction. Our findings reveal that\nground truth evidence aligns with code descriptions to a certain degree. An\ninvestigation into state-of-the-art approaches shows a high overlap with ground\ntruth evidence. We propose match measures and highlight success and failure\ncases. Based on our findings, we provide recommendations for developing and\nevaluating explainable medical coding systems.",
        "url": "http://arxiv.org/abs/2507.01802v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01802v1",
        "arxiv_id": "2507.01802v1",
        "authors": [
            "Katharina Beckh",
            "Elisa Studeny",
            "Sujan Sai Gannamaneni",
            "Dario Antweiler",
            "Stefan Rüping"
        ],
        "submitted": "2025-07-02 15:21:29",
        "source": "arxiv",
        "comment": "Accepted to ACL 2025 Findings",
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is not relevant to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus is on medical coding and evidence extraction, which is outside the user's primary areas of interest."
    },
    {
        "title": "Data interference: emojis, homoglyphs, and issues of data fidelity in corpora and their results",
        "abstract": "Tokenisation - \"the process of splitting text into atomic parts\" (Brezina &\nTimperley, 2017: 1) - is a crucial step for corpus linguistics, as it provides\nthe basis for any applicable quantitative method (e.g. collocations) while\nensuring the reliability of qualitative approaches. This paper examines how\ndiscrepancies in tokenisation affect the representation of language data and\nthe validity of analytical findings: investigating the challenges posed by\nemojis and homoglyphs, the study highlights the necessity of preprocessing\nthese elements to maintain corpus fidelity to the source data. The research\npresents methods for ensuring that digital texts are accurately represented in\ncorpora, thereby supporting reliable linguistic analysis and guaranteeing the\nrepeatability of linguistic interpretations. The findings emphasise the\nnecessity of a detailed understanding of both linguistic and technical aspects\ninvolved in digital textual data to enhance the accuracy of corpus analysis,\nand have significant implications for both quantitative and qualitative\napproaches in corpus-based research.",
        "url": "http://arxiv.org/abs/2507.01764v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01764v1",
        "arxiv_id": "2507.01764v1",
        "authors": [
            "Matteo Di Cristofaro"
        ],
        "submitted": "2025-07-02 14:46:26",
        "source": "arxiv",
        "comment": "Author submitted manuscript",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on tokenization and corpus linguistics, which is not directly related to Information Retrieval, Search technologies, or query understanding. Although it touches on preprocessing and data fidelity, the context is linguistic analysis rather than search or ranking models."
    },
    {
        "title": "ECCV 2024 W-CODA: 1st Workshop on Multimodal Perception and Comprehension of Corner Cases in Autonomous Driving",
        "abstract": "In this paper, we present details of the 1st W-CODA workshop, held in\nconjunction with the ECCV 2024. W-CODA aims to explore next-generation\nsolutions for autonomous driving corner cases, empowered by state-of-the-art\nmultimodal perception and comprehension techniques. 5 Speakers from both\nacademia and industry are invited to share their latest progress and opinions.\nWe collect research papers and hold a dual-track challenge, including both\ncorner case scene understanding and generation. As the pioneering effort, we\nwill continuously bridge the gap between frontier autonomous driving techniques\nand fully intelligent, reliable self-driving agents robust towards corner\ncases.",
        "url": "http://arxiv.org/abs/2507.01735v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01735v1",
        "arxiv_id": "2507.01735v1",
        "authors": [
            "Kai Chen",
            "Ruiyuan Gao",
            "Lanqing Hong",
            "Hang Xu",
            "Xu Jia",
            "Holger Caesar",
            "Dengxin Dai",
            "Bingbing Liu",
            "Dzmitry Tsishkou",
            "Songcen Xu",
            "Chunjing Xu",
            "Qiang Xu",
            "Huchuan Lu",
            "Dit-Yan Yeung"
        ],
        "submitted": "2025-07-02 14:10:25",
        "source": "arxiv",
        "comment": "ECCV 2024. Workshop page: https://coda-dataset.github.io/w-coda2024/",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is not relevant to the user's research interests as it focuses on autonomous driving, multimodal perception, and comprehension, which are not directly related to information retrieval, search technologies, or natural language processing. The topics of corner cases, scene understanding, and generation are also not aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "LLMs for Legal Subsumption in German Employment Contracts",
        "abstract": "Legal work, characterized by its text-heavy and resource-intensive nature,\npresents unique challenges and opportunities for NLP research. While\ndata-driven approaches have advanced the field, their lack of interpretability\nand trustworthiness limits their applicability in dynamic legal environments.\nTo address these issues, we collaborated with legal experts to extend an\nexisting dataset and explored the use of Large Language Models (LLMs) and\nin-context learning to evaluate the legality of clauses in German employment\ncontracts. Our work evaluates the ability of different LLMs to classify clauses\nas \"valid,\" \"unfair,\" or \"void\" under three legal context variants: no legal\ncontext, full-text sources of laws and court rulings, and distilled versions of\nthese (referred to as examination guidelines). Results show that full-text\nsources moderately improve performance, while examination guidelines\nsignificantly enhance recall for void clauses and weighted F1-Score, reaching\n80\\%. Despite these advancements, LLMs' performance when using full-text\nsources remains substantially below that of human lawyers. We contribute an\nextended dataset, including examination guidelines, referenced legal sources,\nand corresponding annotations, alongside our code and all log files. Our\nfindings highlight the potential of LLMs to assist lawyers in contract legality\nreview while also underscoring the limitations of the methods presented.",
        "url": "http://arxiv.org/abs/2507.01734v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01734v1",
        "arxiv_id": "2507.01734v1",
        "authors": [
            "Oliver Wardas",
            "Florian Matthes"
        ],
        "submitted": "2025-07-02 14:07:54",
        "source": "arxiv",
        "comment": "PrePrint - ICAIL25, Chicago",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the application of Large Language Models (LLMs) in legal subsumption, specifically in German employment contracts. While it touches on NLP and data-driven approaches, the focus is on legal domain and LLMs, which is not directly related to the user's primary interests in Information Retrieval, Search technologies, and query understanding. The paper's relevance is somewhat limited to the user's broader interests in NLP and data mining."
    },
    {
        "title": "Blending Supervised and Reinforcement Fine-Tuning with Prefix Sampling",
        "abstract": "Existing post-training techniques for large language models are broadly\ncategorized into Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning\n(RFT). Each paradigm presents a distinct trade-off: SFT excels at mimicking\ndemonstration data but can lead to problematic generalization as a form of\nbehavior cloning. Conversely, RFT can significantly enhance a model's\nperformance but is prone to learn unexpected behaviors, and its performance is\nhighly sensitive to the initial policy. In this paper, we propose a unified\nview of these methods and introduce Prefix-RFT, a hybrid approach that\nsynergizes learning from both demonstration and exploration. Using mathematical\nreasoning problems as a testbed, we empirically demonstrate that Prefix-RFT is\nboth simple and effective. It not only surpasses the performance of standalone\nSFT and RFT but also outperforms parallel mixed-policy RFT methods. A key\nadvantage is its seamless integration into existing open-source frameworks,\nrequiring only minimal modifications to the standard RFT pipeline. Our analysis\nhighlights the complementary nature of SFT and RFT, and validates that\nPrefix-RFT effectively harmonizes these two learning paradigms. Furthermore,\nablation studies confirm the method's robustness to variations in the quality\nand quantity of demonstration data. We hope this work offers a new perspective\non LLM post-training, suggesting that a unified paradigm that judiciously\nintegrates demonstration and exploration could be a promising direction for\nfuture research.",
        "url": "http://arxiv.org/abs/2507.01679v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01679v1",
        "arxiv_id": "2507.01679v1",
        "authors": [
            "Zeyu Huang",
            "Tianhao Cheng",
            "Zihan Qiu",
            "Zili Wang",
            "Yinghui Xu",
            "Edoardo M. Ponti",
            "Ivan Titov"
        ],
        "submitted": "2025-07-02 13:04:09",
        "source": "arxiv",
        "comment": "Work in progress",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on post-training techniques for large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions reinforcement learning, it does not specifically address ranking models or user behavior modeling, making it only loosely relevant to the user's research interests."
    },
    {
        "title": "T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning",
        "abstract": "Temporal Knowledge Graph (TKG) is an efficient method for describing the\ndynamic development of facts along a timeline. Most research on TKG reasoning\n(TKGR) focuses on modelling the repetition of global facts and designing\npatterns of local historical facts. However, they face two significant\nchallenges: inadequate modeling of the event distribution shift between\ntraining and test samples, and reliance on random entity substitution for\ngenerating negative samples, which often results in low-quality sampling. To\nthis end, we propose a novel distributional feature modeling approach for\ntraining TKGR models, Test-Time Training-guided Distribution shift Modelling\n(T3DM), to adjust the model based on distribution shift and ensure the global\nconsistency of model reasoning. In addition, we design a negative-sampling\nstrategy to generate higher-quality negative quadruples based on adversarial\ntraining. Extensive experiments show that T3DM provides better and more robust\nresults than the state-of-the-art baselines in most cases.",
        "url": "http://arxiv.org/abs/2507.01597v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01597v1",
        "arxiv_id": "2507.01597v1",
        "authors": [
            "Yuehang Si",
            "Zefan Zeng",
            "Jincai Huang",
            "Qing Cheng"
        ],
        "submitted": "2025-07-02 11:02:37",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Temporal Knowledge Graph Reasoning, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it involves modeling and training, the context is different from the user's primary interests in IR and NLP."
    },
    {
        "title": "ManifoldMind: Dynamic Hyperbolic Reasoning for Trustworthy Recommendations",
        "abstract": "We introduce ManifoldMind, a probabilistic geometric recommender system for\nexploratory reasoning over semantic hierarchies in hyperbolic space. Unlike\nprior methods with fixed curvature and rigid embeddings, ManifoldMind\nrepresents users, items, and tags as adaptive-curvature probabilistic spheres,\nenabling personalised uncertainty modeling and geometry-aware semantic\nexploration. A curvature-aware semantic kernel supports soft, multi-hop\ninference, allowing the model to explore diverse conceptual paths instead of\noverfitting to shallow or direct interactions. Experiments on four public\nbenchmarks show superior NDCG, calibration, and diversity compared to strong\nbaselines. ManifoldMind produces explicit reasoning traces, enabling\ntransparent, trustworthy, and exploration-driven recommendations in sparse or\nabstract domains.",
        "url": "http://arxiv.org/abs/2507.02014v1",
        "pdf_url": "http://arxiv.org/pdf/2507.02014v1",
        "arxiv_id": "2507.02014v1",
        "authors": [
            "Anoushka Harit",
            "Zhongtian Sun",
            "Suncica Hadzidedic"
        ],
        "submitted": "2025-07-02 08:42:11",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper introduces a probabilistic geometric recommender system, ManifoldMind, which explores semantic hierarchies in hyperbolic space. While it touches on some aspects of information retrieval, such as personalized uncertainty modeling and geometry-aware semantic exploration, its primary focus is on recommender systems, which is only loosely related to the user's interests in information retrieval and search technologies."
    },
    {
        "title": "Evaluating Large Language Models for Multimodal Simulated Ophthalmic Decision-Making in Diabetic Retinopathy and Glaucoma Screening",
        "abstract": "Large language models (LLMs) can simulate clinical reasoning based on natural\nlanguage prompts, but their utility in ophthalmology is largely unexplored.\nThis study evaluated GPT-4's ability to interpret structured textual\ndescriptions of retinal fundus photographs and simulate clinical decisions for\ndiabetic retinopathy (DR) and glaucoma screening, including the impact of\nadding real or synthetic clinical metadata. We conducted a retrospective\ndiagnostic validation study using 300 annotated fundus images. GPT-4 received\nstructured prompts describing each image, with or without patient metadata. The\nmodel was tasked with assigning an ICDR severity score, recommending DR\nreferral, and estimating the cup-to-disc ratio for glaucoma referral.\nPerformance was evaluated using accuracy, macro and weighted F1 scores, and\nCohen's kappa. McNemar's test and change rate analysis were used to assess the\ninfluence of metadata. GPT-4 showed moderate performance for ICDR\nclassification (accuracy 67.5%, macro F1 0.33, weighted F1 0.67, kappa 0.25),\ndriven mainly by correct identification of normal cases. Performance improved\nin the binary DR referral task (accuracy 82.3%, F1 0.54, kappa 0.44). For\nglaucoma referral, performance was poor across all settings (accuracy ~78%, F1\n<0.04, kappa <0.03). Metadata inclusion did not significantly affect outcomes\n(McNemar p > 0.05), and predictions remained consistent across conditions.\nGPT-4 can simulate basic ophthalmic decision-making from structured prompts but\nlacks precision for complex tasks. While not suitable for clinical use, LLMs\nmay assist in education, documentation, or image annotation workflows in\nophthalmology.",
        "url": "http://arxiv.org/abs/2507.01278v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01278v1",
        "arxiv_id": "2507.01278v1",
        "authors": [
            "Cindy Lie Tabuse",
            "David Restepo",
            "Carolina Gracitelli",
            "Fernando Korn Malerbi",
            "Caio Regatieri",
            "Luis Filipe Nakayama"
        ],
        "submitted": "2025-07-02 01:35:59",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on evaluating large language models for ophthalmic decision-making, which is outside the scope of information retrieval, search technologies, and natural language processing. The paper's application in education, documentation, or image annotation workflows in ophthalmology is also not directly related to your interests."
    },
    {
        "title": "Towards a Signal Detection Based Measure for Assessing Information Quality of Explainable Recommender Systems",
        "abstract": "There is growing interest in explainable recommender systems that provide\nrecommendations along with explanations for the reasoning behind them. When\nevaluating recommender systems, most studies focus on overall recommendation\nperformance. Only a few assess the quality of the explanations. Explanation\nquality is often evaluated through user studies that subjectively gather users'\nopinions on representative explanatory factors that shape end-users'\nperspective towards the results, not about the explanation contents itself. We\naim to fill this gap by developing an objective metric to evaluate Veracity:\nthe information quality of explanations. Specifically, we decompose Veracity\ninto two dimensions: Fidelity and Attunement. Fidelity refers to whether the\nexplanation includes accurate information about the recommended item.\nAttunement evaluates whether the explanation reflects the target user's\npreferences. By applying signal detection theory, we first determine decision\noutcomes for each dimension and then combine them to calculate a sensitivity,\nwhich serves as the final Veracity value. To assess the effectiveness of the\nproposed metric, we set up four cases with varying levels of information\nquality to validate whether our metric can accurately capture differences in\nquality. The results provided meaningful insights into the effectiveness of our\nproposed metric.",
        "url": "http://arxiv.org/abs/2507.01168v1",
        "pdf_url": "http://arxiv.org/pdf/2507.01168v1",
        "arxiv_id": "2507.01168v1",
        "authors": [
            "Yeonbin Son",
            "Matthew L. Bolton"
        ],
        "submitted": "2025-07-01 20:11:17",
        "source": "arxiv",
        "comment": "Accepted to IEEE CAI 2025",
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on explainable recommender systems, which is somewhat related to information retrieval and search technologies. However, the emphasis on recommender systems and explanation quality is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling. The paper's use of signal detection theory and objective metrics is interesting, but the topic is not a central match for the user's research themes."
    }
]