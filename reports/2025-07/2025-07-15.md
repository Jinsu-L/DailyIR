# Daily Papers Report - 2025-07-15

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Am I on the Right Track? What Can Predicted Query Performance Tell Us about the Search Behaviour of Agentic RAG

- **LLM Score**: 8
- **Keyword Score**: 14
- **Authors**: Fangzheng Tian, Jinyuan Fang, Debasis Ganguly, Zaiqiao Meng, Craig Macdonald
- **URL**: <http://arxiv.org/abs/2507.10411v1>
- **Submitted**: 2025-07-14 15:54:50
- **Topic Keywords**: retriever, query, queries, rag, retrieval, search
- **Reason**: This paper explores Agentic Retrieval-Augmented Generation (RAG) models, which is a relevant area in Information Retrieval. The study focuses on query performance prediction (QPP) and its application in RAG models, aligning with your interests in query understanding and ranking models. However, the specific domain of RAG models is not explicitly mentioned in your research interests, which prevents a perfect match.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Query Performance Prediction in Agentic Retrieval-Augmented Generation Models
- **Aim**: To investigate the role of query performance prediction in improving answer quality and shortening the reasoning process in Agentic RAG models
- **Rationale**: The authors propose integrating a QPP model into the Agentic RAG pipeline to dynamically evaluate retrieval quality during each reasoning-retrieval iteration
- **Ground**: The study uses two Agentic RAG models, Search-R1 and R1-Searcher, with three different retrieval models: BM25, MonoT5, and E5, on the Natural Questions (NQ) QA dataset
- **Experiment**: The authors conduct experiments to evaluate the impact of different retriever configurations on the performance of the two Agentic RAG models
- **Takeaway**: The study demonstrates the potential of QPP in guiding the Agentic RAG model in determining whether retrieved results are likely to be helpful, paving the way for adaptive retrieval strategies within this framework

#### Abstract
> Agentic Retrieval-Augmented Generation (RAG) is a new paradigm where the
reasoning model decides when to invoke a retriever (as a "tool") when answering
a question. This paradigm, exemplified by recent research works such as
Search-R1, enables the model to decide when to search and obtain external
information. However, the queries generated by such Agentic RAG models and the
role of the retriever in obtaining high-quality answers remain understudied. To
this end, this initial study examines the applicability of query performance
prediction (QPP) within the recent Agentic RAG models Search-R1 and
R1-Searcher. We find that applying effective retrievers can achieve higher
answer quality within a shorter reasoning process. Moreover, the QPP estimates
of the generated queries, used as an approximation of their retrieval quality,
are positively correlated with the quality of the final answer. Ultimately, our
work is a step towards adaptive retrieval within Agentic RAG, where QPP is used
to inform the model if the retrieved results are likely to be useful.

---

### 2. PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware Query Optimization

- **LLM Score**: 8
- **Keyword Score**: 10
- **Authors**: Sangwoo Park, Jinheon Baek, Soyeong Jeong, Sung Ju Hwang
- **URL**: <http://arxiv.org/abs/2507.10057v1>
- **Submitted**: 2025-07-14 08:41:53
- **Topic Keywords**: query, queries, rag, retrieval
- **Reason**: This paper is highly relevant to Information Retrieval, specifically document-to-document retrieval, and introduces a novel method (PRISM) that uses fine-grained representations for query and candidate papers. The focus on query understanding and relevance optimization aligns with your research interests. The experimental results and benchmark also demonstrate the practical application of the proposed method.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Scientific Paper Retrieval
- **Aim**: To develop a novel method for scientific paper retrieval that captures the multifaceted nature of papers and their relevance
- **Rationale**: Previous approaches have limitations in capturing the complexity of scientific papers, and a more precise and comprehensive comparison is needed
- **Ground**: The authors introduce SCIFULLBENCH, a new benchmark dataset for scientific paper retrieval, and explore different retrieval units and setups
- **Experiment**: Experimental results demonstrate that PRISM outperforms existing retrieval baselines by an average of 4.3%, and the authors compare their method with several state-of-the-art models
- **Takeaway**: The proposed method, PRISM, achieves significant improvements in retrieval performance and diversity, and the research contributes to the development of novel query optimization techniques

#### Abstract
> Scientific paper retrieval, particularly framed as document-to-document
retrieval, aims to identify relevant papers in response to a long-form query
paper, rather than a short query string. Previous approaches to this task have
focused on abstracts, embedding them into dense vectors as surrogates for full
documents and calculating similarity across them, although abstracts provide
only sparse and high-level summaries. To address this, we propose PRISM, a
novel document-to-document retrieval method that introduces multiple,
fine-grained representations for both the query and candidate papers. In
particular, each query paper is decomposed into multiple aspect-specific views
and individually embedded, which are then matched against candidate papers
similarity segmented to consider their multifaceted dimensions. Moreover, we
present SciFullBench, a novel benchmark in which the complete and segmented
context of full papers for both queries and candidates is available. Then,
experimental results show that PRISM improves performance by an average of 4.3%
over existing retrieval baselines.

---

### 3. Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking

- **LLM Score**: 8
- **Keyword Score**: 6
- **Authors**: Hai Toan Nguyen, Tien Dat Nguyen, Viet Ha Nguyen
- **URL**: <http://arxiv.org/abs/2507.09935v1>
- **Submitted**: 2025-07-14 05:21:58
- **Topic Keywords**: rag, retrieval augmented generation, retrieval
- **Reason**: This paper is highly relevant to Information Retrieval, particularly in the context of Retrieval-Augmented Generation (RAG) systems, which is a key area of interest. The use of hierarchical text segmentation and clustering to improve chunking strategies aligns with the user's focus on query understanding and ranking models. While the paper's focus is on NLP, its relevance to IR and deep semantic understanding makes it a useful contribution.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Retrieval Augmented Generation (RAG) with Hierarchical Text Segmentation and Clustering
- **Aim**: To propose a novel framework for enhancing RAG by incorporating hierarchical text segmentation and clustering
- **Rationale**: Traditional chunking methods fail to capture sufficient semantic meaning due to a lack of consideration for textual structure
- **Ground**: The proposed method consists of three key components: text segmentation, chunk clustering, and multiple-vector based retrieval
- **Experiment**: The proposed method is evaluated on three datasets: NarrativeQA, QuALITY, and QASPER, and outperforms traditional chunking techniques
- **Takeaway**: The Segment-Cluster approach improves retrieval accuracy and answer quality by ensuring semantic and contextual cohesion within retrieved chunks

#### Abstract
> Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies
for retrieval, which enhance large language models (LLMs) by enabling them to
access external knowledge, ensuring that the retrieved information is
up-to-date and domain-specific. However, traditional methods often fail to
create chunks that capture sufficient semantic meaning, as they do not account
for the underlying textual structure. This paper proposes a novel framework
that enhances RAG by integrating hierarchical text segmentation and clustering
to generate more meaningful and semantically coherent chunks. During inference,
the framework retrieves information by leveraging both segment-level and
cluster-level vector representations, thereby increasing the likelihood of
retrieving more precise and contextually relevant information. Evaluations on
the NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method
achieved improved results compared to traditional chunking techniques.

---

### 4. DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology

- **LLM Score**: 7
- **Keyword Score**: 3
- **Authors**: Jennifer D'Souza, Endres Keno Sander, Andrei Aioanei
- **URL**: <http://arxiv.org/abs/2507.10522v1>
- **Submitted**: 2025-07-14 17:47:28
- **Comment**: 12 pages, 3 figures
- **Topic Keywords**: retrieval, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of query understanding and ranking models. However, its focus on scientific question answering in ecology and agentic workflow is somewhat niche and not directly aligned with your primary interests in e-commerce and real-time relevance optimization. Nevertheless, the paper's use of LLM-based systems and recursive exploration of research questions shares some commonalities with your work in query understanding and ranking models.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Automated Scientific Synthesis using Large Language Models
- **Aim**: To develop a novel system, DeepResearchEco, for automated scientific synthesis using large language models (LLMs) that balances breadth and depth in literature exploration for scientific question answering
- **Rationale**: To address the challenge of balancing breadth and depth in literature exploration by employing a recursive retrieval and generation loop controlled by user-defined depth and breadth parameters
- **Ground**: The system integrates two variants of LLM reasoning models and operates through a four-step recursive loop, involving generating SERP queries, searching, summarizing results, and generating a report
- **Experiment**: The authors evaluated the system across eight configurations defined by different model parameters (synthesis depth and breadth) and used three similarity metrics to compare reports generated under different configuration settings
- **Takeaway**: DeepResearchEco achieves domain-specific synthesis capabilities comparable to or exceeding expert-level integration, and the authors recommend the highest parameter configuration (d4_b4) for applications prioritizing comprehensive, high-quality synthesis outputs

#### Abstract
> We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system
for automated scientific synthesis that supports recursive, depth- and
breadth-controlled exploration of original research questions -- enhancing
search diversity and nuance in the retrieval of relevant scientific literature.
Unlike conventional retrieval-augmented generation pipelines, DeepResearch
enables user-controllable synthesis with transparent reasoning and
parameter-driven configurability, facilitating high-throughput integration of
domain-specific evidence while maintaining analytical rigor. Applied to 49
ecological research questions, DeepResearch achieves up to a 21-fold increase
in source integration and a 14.9-fold rise in sources integrated per 1,000
words. High-parameter settings yield expert-level analytical depth and
contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.

---

### 5. User Long-Term Multi-Interest Retrieval Model for Recommendation

- **LLM Score**: 6
- **Keyword Score**: 16
- **Authors**: Yue Meng, Cheng Guo, Xiaohui Hu, Honghu Deng, Yi Cao, Tong Liu, Bo Zheng
- **URL**: <http://arxiv.org/abs/2507.10097v1>
- **Submitted**: 2025-07-14 09:32:26
- **Topic Keywords**: ranking, ltr, rag, user behavior, click, retrieval, recommend, rank
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Search technologies, particularly in the context of user behavior modeling and recommendation systems. However, the focus on recommendation systems and e-commerce domain is not a central match to your primary interests in query understanding, ranking models, and deep semantic understanding.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: ULIM: A Novel Framework for User Long-term Multi-Interest Retrieval Model
- **Aim**: To address the limitations of existing retrieval models in handling long user behavior sequences
- **Rationale**: To overcome latency constraints and the lack of target-awareness in conventional architectures
- **Ground**: Category-Aware Hierarchical Dual-Interest Learning and Pointer-Enhanced Cascaded Category-to-Item Retrieval
- **Experiment**: Evaluating ULIM on a large-scale industrial dataset from Taobao, comparing it to baselines like Youtube DNN and MIND
- **Takeaway**: ULIM demonstrates significant performance improvements and bridges the gap between retrieval and ranking stages in recommendation systems

#### Abstract
> User behavior sequence modeling, which captures user interest from rich
historical interactions, is pivotal for industrial recommendation systems.
Despite breakthroughs in ranking-stage models capable of leveraging ultra-long
behavior sequences with length scaling up to thousands, existing retrieval
models remain constrained to sequences of hundreds of behaviors due to two main
challenges. One is strict latency budget imposed by real-time service over
large-scale candidate pool. The other is the absence of target-aware mechanisms
and cross-interaction architectures, which prevent utilizing ranking-like
techniques to simplify long sequence modeling. To address these limitations, we
propose a new framework named User Long-term Multi-Interest Retrieval
Model(ULIM), which enables thousand-scale behavior modeling in retrieval
stages. ULIM includes two novel components: 1)Category-Aware Hierarchical
Dual-Interest Learning partitions long behavior sequences into multiple
category-aware subsequences representing multi-interest and jointly optimizes
long-term and short-term interests within specific interest cluster.
2)Pointer-Enhanced Cascaded Category-to-Item Retrieval introduces
Pointer-Generator Interest Network(PGIN) for next-category prediction, followed
by next-item retrieval upon the top-K predicted categories. Comprehensive
experiments on Taobao dataset show that ULIM achieves substantial improvement
over state-of-the-art methods, and brings 5.54% clicks, 11.01% orders and 4.03%
GMV lift for Taobaomiaosha, a notable mini-app of Taobao.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Riding the Carousel: The First Extensive Eye Tracking Analysis of Browsing Behavior in Carousel Recommenders

- **LLM Score**: 6
- **Keyword Score**: 7
- **Authors**: Santiago de Leon-Martinez, Robert Moro, Branislav Kveton, Maria Bielikova
- **URL**: <http://arxiv.org/abs/2507.10135v1>
- **Submitted**: 2025-07-14 10:26:27
- **Topic Keywords**: rag, user behavior, recommend, rank, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Search technologies, particularly in the area of recommender systems. However, it focuses more on user behavior modeling and eye-tracking analysis in carousel recommenders, which is a specific application rather than a core theme. While it may provide insights useful for improving recommender systems, it does not directly address query understanding, ranking models, or deep semantic understanding.

#### Abstract
> Carousels have become the de-facto interface in online services. However,
there is a lack of research in carousels, particularly examining how
recommender systems may be designed differently than the traditional
single-list interfaces. One of the key elements for understanding how to design
a system for a particular interface is understanding how users browse. For
carousels, users may browse in a number of different ways due to the added
complexity of multiple topic defined-lists and swiping to see more items.
  Eye tracking is the key to understanding user behavior by providing valuable,
direct information on how users see and navigate. In this work, we provide the
first extensive analysis of the eye tracking behavior in carousel recommenders
under the free-browsing setting. To understand how users browse, we examine the
following research questions : 1) where do users start browsing, 2) how do
users transition from item to item within the same carousel and across
carousels, and 3) how does genre preference impact transitions?
  This work addresses a gap in the field and provides the first extensive
empirical results of eye tracked browsing behavior in carousels for improving
recommenders. Taking into account the insights learned from the above
questions, our final contribution is to provide suggestions to help carousel
recommender system designers optimize their systems for user browsing behavior.
The most important suggestion being to reorder the ranked item positions to
account for browsing after swiping.These contributions aim not only to help
improve current systems, but also to encourage and allow the design of new user
models, systems, and metrics that are better suited to the complexity of
carousel interfaces.

### 7. CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Hongchao Jiang, Yiming Chen, Yushi Cao, Hung-yi Lee, Robby T. Tan
- **URL**: <http://arxiv.org/abs/2507.10535v1>
- **Submitted**: 2025-07-14 17:56:29
- **Comment**: Dataset is available at
  https://huggingface.co/datasets/mattymchen/codejudgebench
- **Topic Keywords**: queries, ranking, pairwise, rank
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and Information Retrieval (IR), particularly in the context of Large Language Models (LLMs) and their applications in coding tasks. However, it does not directly align with your primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large Language Models (LLMs) have significantly advanced the state-of-the-art
in various coding tasks. Beyond directly answering user queries, LLMs can also
serve as judges, assessing and comparing the quality of responses generated by
other models. Such an evaluation capability is crucial both for benchmarking
different LLMs and for improving response quality through response ranking.
However, despite the growing adoption of the LLM-as-a-Judge paradigm, its
effectiveness in coding scenarios remains underexplored due to the absence of
dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a
benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge
models across three critical coding tasks: code generation, code repair, and
unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge
models, we find that recent thinking models significantly outperform
non-thinking models on our carefully designed code judging tasks. Notably, even
relatively small thinking models, such as Qwen3-8B, can outperform specially
trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still
exhibit significant randomness in their judgment of coding tasks. For pairwise
judging tasks, simply changing the order in which responses are presented can
substantially impact accuracy. In addition, when judging code and unit tests
written by different LLMs, LLM-as-a-Judge models also show variance in
performance. This sensitivity raises concerns about the reliability and
consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal
prompting strategies for LLM-as-a-Judge. We find that using pair-wise
comparison outperforms scalar point-wise judging. Furthermore, retaining
comments and reasoning in the full, unprocessed LLM response leads to improved
judge performance.

### 8. Non-parametric Graph Convolution for Re-ranking in Recommendation Systems

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Zhongyu Ouyang, Mingxuan Ju, Soroush Vosoughi, Yanfang Ye
- **URL**: <http://arxiv.org/abs/2507.09969v1>
- **Submitted**: 2025-07-14 06:35:18
- **Comment**: Accepted to RecSys2025 Main
- **Topic Keywords**: ranking, rag, retrieval, recommend, rank, recsys
- **Reason**: The paper focuses on recommender systems and graph convolution for re-ranking, which is somewhat related to your interests in Information Retrieval and Search technologies. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of your research.

#### Abstract
> Graph knowledge has been proven effective in enhancing item rankings in
recommender systems (RecSys), particularly during the retrieval stage. However,
its application in the ranking stage, especially when richer contextual
information in user-item interactions is available, remains underexplored. A
major challenge lies in the substantial computational cost associated with
repeatedly retrieving neighborhood information from billions of items stored in
distributed systems. This resource-intensive requirement makes it difficult to
scale graph-based methods in practical RecSys. To bridge this gap, we first
demonstrate that incorporating graphs in the ranking stage improves ranking
qualities. Notably, while the improvement is evident, we show that the
substantial computational overheads entailed by graphs are prohibitively
expensive for real-world recommendations. In light of this, we propose a
non-parametric strategy that utilizes graph convolution for re-ranking only
during test time. Our strategy circumvents the notorious computational
overheads from graph convolution during training, and utilizes structural
knowledge hidden in graphs on-the-fly during testing. It can be used as a
plug-and-play module and easily employed to enhance the ranking ability of
various ranking layers of a real-world RecSys with significantly reduced
computational overhead. Through comprehensive experiments across four benchmark
datasets with varying levels of sparsity, we demonstrate that our strategy
yields noticeable improvements (i.e., 8.1% on average) during testing time with
little to no additional computational overheads (i.e., 0.5 on average). Code:
https://github.com/zyouyang/RecSys2025_NonParamGC.git

### 9. EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Yasir Ech-Chammakhy, Anas Motii, Anass Rabii, Jaafar Chbili
- **URL**: <http://arxiv.org/abs/2507.09762v1>
- **Submitted**: 2025-07-13 19:40:36
- **Comment**: Accepted for publication at the 28th International Symposium on
  Research in Attacks, Intrusions, and Defenses (RAID 2025)
- **Topic Keywords**: ranking, relevance, rag, rank
- **Reason**: This paper is somewhat related to Information Retrieval, specifically in the context of extracting actionable intelligence from unstructured content. However, its focus on cybersecurity threats and hacker forum discussions is not directly aligned with the user's core research themes in e-commerce and deep semantic understanding. The use of Transformer-based embeddings and ranking mechanisms is relevant to the user's interests in NLP and ranking models, but the application is quite different.

#### Abstract
> Hacker forums provide critical early warning signals for emerging
cybersecurity threats, but extracting actionable intelligence from their
unstructured and noisy content remains a significant challenge. This paper
presents an unsupervised framework that automatically detects, clusters, and
prioritizes security events discussed across hacker forum posts. Our approach
leverages Transformer-based embeddings fine-tuned with contrastive learning to
group related discussions into distinct security event clusters, identifying
incidents like zero-day disclosures or malware releases without relying on
predefined keywords. The framework incorporates a daily ranking mechanism that
prioritizes identified events using quantifiable metrics reflecting timeliness,
source credibility, information completeness, and relevance. Experimental
evaluation on real-world hacker forum data demonstrates that our method
effectively reduces noise and surfaces high-priority threats, enabling security
analysts to mount proactive responses. By transforming disparate hacker forum
discussions into structured, actionable intelligence, our work addresses
fundamental challenges in automated threat detection and analysis.

### 10. Automating SPARQL Query Translations between DBpedia and Wikidata

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Malte Christian Bartels, Debayan Banerjee, Ricardo Usbeck
- **URL**: <http://arxiv.org/abs/2507.10045v1>
- **Submitted**: 2025-07-14 08:23:25
- **Comment**: 18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference
  happening on September 2025
- **Topic Keywords**: query, queries, search
- **Reason**: The paper explores query translation between Knowledge Graph schemas using Large Language Models, which is somewhat related to query understanding in Information Retrieval. However, the focus on SPARQL query translation and Knowledge Graph interoperability is not directly aligned with the user's primary research interests in search technologies, ranking models, and user behavior modeling.

#### Abstract
> This paper investigates whether state-of-the-art Large Language Models (LLMs)
can automatically translate SPARQL between popular Knowledge Graph (KG)
schemas. We focus on translations between the DBpedia and Wikidata KG, and
later on DBLP and OpenAlex KG. This study addresses a notable gap in KG
interoperability research by rigorously evaluating LLM performance on
SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first
align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100
DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic
KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and
Mistral-Large-Instruct-2407 are selected based on their sizes and architectures
and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs
were compared with gold answers, and resulting errors were categorized. We find
that the performance varies markedly across models and prompting strategies,
and that translations for Wikidata to DBpedia work far better than translations
for DBpedia to Wikidata.

### 11. Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Daniele Rege Cambrin, Lorenzo Vaiani, Giuseppe Gallipoli, Luca Cagliero, Paolo Garza
- **URL**: <http://arxiv.org/abs/2507.10403v1>
- **Submitted**: 2025-07-14 15:46:56
- **Topic Keywords**: ctr, retrieval
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. However, its focus on remote sensing image retrieval and sensor data integration is not directly aligned with your primary areas of interest in e-commerce, user behavior modeling, and deep semantic understanding.

#### Abstract
> Retrieving relevant imagery from vast satellite archives is crucial for
applications like disaster response and long-term climate monitoring. However,
most text-to-image retrieval systems are limited to RGB data, failing to
exploit the unique physical information captured by other sensors, such as the
all-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the
spectral signatures in optical multispectral data. To bridge this gap, we
introduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1
SAR and Sentinel-2 multispectral images paired with structured textual
annotations for land cover, land use, and crisis events harmonized from
authoritative land cover systems (CORINE and Dynamic World) and crisis-specific
sources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),
a novel framework that uses text as a bridge to align unpaired optical and SAR
images into a unified embedding space. Our experiments show that CLOSP achieves
a new state-of-the-art, improving retrieval nDGC by 54% over existing models.
Additionally, we find that the unified training strategy overcomes the inherent
difficulty of interpreting SAR imagery by transferring rich semantic knowledge
from the optical domain with indirect interaction. Furthermore, GeoCLOSP, which
integrates geographic coordinates into our framework, creates a powerful
trade-off between generality and specificity: while the CLOSP excels at general
semantic tasks, the GeoCLOSP becomes a specialized expert for retrieving
location-dependent crisis events and rare geographic features. This work
highlights that the integration of diverse sensor data and geographic context
is essential for unlocking the full potential of remote sensing archives.

### 12. Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Muzhaffar Hazman, Minh-Khoi Pham, Shweta Soundararajan, Goncalo Mordido, Leonardo Custode, David Lynch, Giorgio Cruciata, Yucheng Shi, Hongmeng Song, Wang Chao, Pan Yue, Aleksandar Milenovic, Alexandros Agapitos
- **URL**: <http://arxiv.org/abs/2507.10326v1>
- **Submitted**: 2025-07-14 14:34:15
- **Comment**: Accepted for Publication at ECAI 2025
- **Topic Keywords**: rag, search
- **Reason**: This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and query understanding, as it deals with prompt engineering and optimisation for large language models. However, the focus on discrete prompt optimisation and evolutionary search approaches is not directly aligned with the user's primary focus on information retrieval and real-time relevance optimisation.

#### Abstract
> Prompt engineering has proven to be a crucial step in leveraging pretrained
large language models (LLMs) in solving various real-world tasks. Numerous
solutions have been proposed that seek to automate prompt engineering by using
the model itself to edit prompts. However, the majority of state-of-the-art
approaches are evaluated on tasks that require minimal prompt templates and on
very large and highly capable LLMs. In contrast, solving complex tasks that
require detailed information to be included in the prompt increases the amount
of text that needs to be optimised. Furthermore, smaller models have been shown
to be more sensitive to prompt design. To address these challenges, we propose
an evolutionary search approach to automated discrete prompt optimisation
consisting of two phases. In the first phase, grammar-guided genetic
programming is invoked to synthesise prompt-creating programmes by searching
the space of programmes populated by function compositions of syntactic,
dictionary-based and LLM-based prompt-editing functions. In the second phase,
local search is applied to explore the neighbourhoods of best-performing
programmes in an attempt to further fine-tune their performance. Our approach
outperforms three state-of-the-art prompt optimisation approaches,
PromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose
LLMs in four domain-specific challenging tasks. We also illustrate several
examples where these benchmark methods suffer relatively severe performance
degradation, while our approach improves performance in almost all task-model
combinations, only incurring minimal degradation when it does not.

### 13. Task-Based Flexible Feature Distillation for LLMs

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Khouloud Saadi, Di Wang
- **URL**: <http://arxiv.org/abs/2507.10155v1>
- **Submitted**: 2025-07-14 11:10:02
- **Topic Keywords**: rag
- **Reason**: The paper focuses on knowledge distillation for large language models, which is somewhat related to information retrieval and search technologies. However, the primary focus on LLMs and their applications in tasks like classification, instruction-following, and summarization does not directly align with the user's core research themes in IR and search technologies. While the paper touches on deep semantic understanding, it is not a central match for the user's interests.

#### Abstract
> Knowledge Distillation (KD) in general and feature distillation in particular
are promising techniques for reducing the high computational demand of large
language models (LLMs). However, traditional feature KD methods typically
assume that the teacher and the student share the same hidden size, limiting
the flexibility of the student's architecture. A common solution to this
problem involves training a linear projector to align their feature spaces, but
this introduces additional parameters that must be learned from scratch and
often degrades performance on downstream tasks, especially in generative
settings. To address this issue, in this work, we propose a novel task-based
feature distillation method that enables knowledge transfer between teacher and
student models with different hidden layer dimensions, without introducing any
new parameters. Leveraging the insight that only a subset of LLM components
contribute significantly to a specific downstream task, our approach identifies
the most task-relevant hidden units in the teacher and directly distills their
activations to the student. Our method is flexible and easily integrates with
other distillation frameworks. Empirical results show consistent improvements
over prior approaches across diverse tasks, including classification,
instruction-following, and summarization, achieving up to a 3\% performance
gain over the linear projection baseline.

### 14. Tiny Reward Models

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Sarah Pan
- **URL**: <http://arxiv.org/abs/2507.09973v1>
- **Submitted**: 2025-07-14 06:43:00
- **Comment**: 2025 ICML Efficient Systems for Foundation Models Workshop
- **Topic Keywords**: rank
- **Reason**: This paper is somewhat relevant to your research interests in Natural Language Processing (NLP) and related topics, particularly in the area of language models. However, it does not directly relate to your core focus on Information Retrieval (IR), query understanding, ranking models, or user behavior modeling. The paper's focus on reward modeling in reinforcement learning from human feedback is an interesting application of NLP, but it does not align with your primary research themes.

#### Abstract
> Large decoder-based language models have become the dominant architecture for
reward modeling in reinforcement learning from human feedback (RLHF). However,
as reward models are increasingly deployed in test-time strategies, their
inference costs become a growing concern. We present TinyRM, a family of small,
bidirectional masked language models (MLMs) with as few as 400 million
parameters, that rival the capabilities of models over 175 times larger on
reasoning and safety preference modeling tasks. TinyRM combines FLAN-style
prompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to
achieve strong performance on RewardBench, despite using significantly fewer
resources. Our experiments suggest that small models benefit from
domain-specific tuning strategies, particularly in reasoning, where lightweight
finetuning methods are especially effective. While challenges remain in
building generalist models and conversational preference modeling, our
preliminary results highlight the promise of lightweight bidirectional
architectures as efficient, scalable alternatives for preference modeling.

### 15. HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Sirui Han, Junqi Zhu, Ruiyuan Zhang, Yike Guo
- **URL**: <http://arxiv.org/abs/2507.11502v1>
- **Submitted**: 2025-07-14 15:09:05
- **Topic Keywords**: queries, rag, retrieval
- **Reason**: This paper focuses on developing a large language model for Hong Kong, addressing regional multilingualism and cultural considerations. While it involves a retrieval-augmented generation system, the primary focus is on AI alignment and safety, rather than information retrieval or search technologies.

#### Abstract
> This paper presents the development of HKGAI-V1, a foundational sovereign
large language model (LLM), developed as part of an initiative to establish
value-aligned AI infrastructure specifically tailored for Hong Kong. Addressing
the region's unique multilingual environment (Cantonese, Mandarin, and
English), its distinct socio-legal context under the "one country, two systems"
framework, and specific local cultural and value considerations, the model is
built upon the DeepSeek architecture and systematically aligned with regional
norms through a multifaceted full parameter fine-tuning process. It is further
integrated with a retrieval-augmented generation (RAG) system to ensure timely
and factually grounded information access. The core contribution lies in the
design and implementation of a comprehensive, region-specific AI alignment and
safety framework, demonstrated through two key achievements: 1) The successful
development of HKGAI-V1 itself - which outper-forms general-purpose models in
handling Hong Kong-specific culturally sensitive queries, and embodies a
"governance-embedded" approach to digital sovereignty - empowers Hong Kong to
exercise control over AI applications in critical sectors including public
services, legal systems, and edu-cation. 2) The development of the proprietary
Adversarial HK Value Benchmark, a rigorous tool for evaluating model alignment
with local ethical and legal stand-ards under challenging conditions. By
documenting these achievements, the paper provides not only a technological
artifact but also a replicable blueprint for developing advanced, regionally
focused AI systems deeply rooted in their local identities.

### 16. Multiple Choice Learning of Low Rank Adapters for Language Modeling

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Victor Letzelter, Hugo Malard, Mathieu Fontaine, Ga√´l Richard, Slim Essid, Andrei Bursuc, Patrick P√©rez
- **URL**: <http://arxiv.org/abs/2507.10419v1>
- **Submitted**: 2025-07-14 16:00:51
- **Topic Keywords**: relevance, rag, rank
- **Reason**: This paper focuses on language modeling and adapter learning, which is somewhat related to your interests in NLP and deep semantic understanding. However, it does not directly address information retrieval, search technologies, or query understanding, making it less relevant to your primary research focus.

#### Abstract
> We propose LoRA-MCL, a training scheme that extends next-token prediction in
language models with a method designed to decode diverse, plausible sentence
continuations at inference time. Traditional language modeling is an
intrinsically ill-posed problem: given a context, multiple futures may be
equally plausible. Our approach leverages Multiple Choice Learning (MCL) and
the Winner-Takes-All (WTA) loss to efficiently handle ambiguity through
Low-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying
Multiple Choice Learning to Language Modeling, assuming the data is generated
from a mixture of distributions. To illustrate the proposed approach, we use
data sampled from mixtures of Markov chains. We then demonstrate with extensive
experiments on real-world visual and audio captioning tasks that our method
achieves high diversity and relevance in generated outputs.

### 17. From Semantic Web and MAS to Agentic AI: A Unified Narrative of the Web of Agents

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Tatiana Petrova, Aleksandr Puzikov, Boris Bliznukov, Radu State
- **URL**: <http://arxiv.org/abs/2507.10644v1>
- **Submitted**: 2025-07-14 16:47:19
- **Comment**: 33 pages, 9 figures, 8 tables
- **Topic Keywords**: rag, search
- **Reason**: This paper appears to be more focused on the Web of Agents, Multi-Agent Systems, and the Semantic Web, which are not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does mention large language models, the context is not aligned with your primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> The concept of the Web of Agents (WoA), which transforms the static,
document-centric Web into an environment of autonomous agents acting on users'
behalf, has attracted growing interest as large language models (LLMs) become
more capable. However, research in this area is still fragmented across
different communities. Contemporary surveys catalog the latest LLM-powered
frameworks, while the rich histories of Multi-Agent Systems (MAS) and the
Semantic Web are often treated as separate, legacy domains. This fragmentation
obscures the intellectual lineage of modern systems and hinders a holistic
understanding of the field's trajectory. We present the first comprehensive
evolutionary overview of the WoA. We show that modern protocols like A2A and
the MCP, are direct evolutionary responses to the well-documented limitations
of earlier standards like FIPA standards and OWL-based semantic agents. To
systematize this analysis, we introduce a four-axis taxonomy (semantic
foundation, communication paradigm, locus of intelligence, discovery
mechanism). This framework provides a unified analytical lens for comparing
agent architectures across all generations, revealing a clear line of descent
where others have seen a disconnect. Our analysis identifies a paradigm shift
in the 'locus of intelligence': from being encoded in external data (Semantic
Web) or the platform (MAS) to being embedded within the agent's core model
(LLM). This shift is foundational to modern Agentic AI, enabling the scalable
and adaptive systems the WoA has long envisioned. We conclude that while new
protocols are essential, they are insufficient for building a robust, open,
trustworthy ecosystem. Finally, we argue that the next research frontier lies
in solving persistent socio-technical challenges, and we map out a new agenda
focused on decentralized identity, economic models, security, and governance
for the emerging WoA.

### 18. From BERT to Qwen: Hate Detection across architectures

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Ariadna Mon, Sa√∫l Fenollosa, Jon Lecumberri
- **URL**: <http://arxiv.org/abs/2507.10468v1>
- **Submitted**: 2025-07-14 16:46:30
- **Comment**: 4 pages, 5 figures. EE-559 Deep Learning course project (Group 11)
- **Topic Keywords**: ltr
- **Reason**: This paper appears to be focused on hate speech detection using BERT and other architectures, which is not directly related to your core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the specific application and context are quite different from your areas of interest.

#### Abstract
> Online platforms struggle to curb hate speech without over-censoring
legitimate discourse. Early bidirectional transformer encoders made big
strides, but the arrival of ultra-large autoregressive LLMs promises deeper
context-awareness. Whether this extra scale actually improves practical
hate-speech detection on real-world text remains unverified. Our study puts
this question to the test by benchmarking both model families, classic encoders
and next-generation LLMs, on curated corpora of online interactions for
hate-speech detection (Hate or No Hate).

### 19. SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Meriem Zerkouk, Miloud Mihoubi, Belkacem Chikhaoui
- **URL**: <http://arxiv.org/abs/2507.10421v1>
- **Submitted**: 2025-07-14 16:04:34
- **Comment**: International Conference on Education and New Learning Technologies
  (2025)
- **Topic Keywords**: rag, search
- **Reason**: This paper is not relevant to your research interests as it focuses on predicting student dropout in distance learning using machine learning models, which does not align with your core areas of interest in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> School dropout is a serious problem in distance learning, where early
detection is crucial for effective intervention and student perseverance.
Predicting student dropout using available educational data is a widely
researched topic in learning analytics. Our partner's distance learning
platform highlights the importance of integrating diverse data sources,
including socio-demographic data, behavioral data, and sentiment analysis, to
accurately predict dropout risks. In this paper, we introduce a novel model
that combines sentiment analysis of student comments using the Bidirectional
Encoder Representations from Transformers (BERT) model with socio-demographic
and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We
fine-tuned BERT on student comments to capture nuanced sentiments, which were
then merged with key features selected using feature importance techniques in
XGBoost. Our model was tested on unseen data from the next academic year,
achieving an accuracy of 84\%, compared to 82\% for the baseline model.
Additionally, the model demonstrated superior performance in other metrics,
such as precision and F1-score. The proposed method could be a vital tool in
developing personalized strategies to reduce dropout rates and encourage
student perseverance

### 20. Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Simon M√ºnker
- **URL**: <http://arxiv.org/abs/2507.10073v1>
- **Submitted**: 2025-07-14 08:59:26
- **Comment**: 15pages, 1 figure, 2 tables
- **Topic Keywords**: rag, search
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on AI systems, it focuses on cultural bias and moral representation, which is outside your primary areas of interest.

#### Abstract
> Are AI systems truly representing human values, or merely averaging across
them? Our study suggests a concerning reality: Large Language Models (LLMs)
fail to represent diverse cultural moral frameworks despite their linguistic
capabilities. We expose significant gaps between AI-generated and human moral
intuitions by applying the Moral Foundations Questionnaire across 19 cultural
contexts. Comparing multiple state-of-the-art LLMs' origins against human
baseline data, we find these models systematically homogenize moral diversity.
Surprisingly, increased model size doesn't consistently improve cultural
representation fidelity. Our findings challenge the growing use of LLMs as
synthetic populations in social science research and highlight a fundamental
limitation in current AI alignment approaches. Without data-driven alignment
beyond prompting, these systems cannot capture the nuanced, culturally-specific
moral intuitions. Our results call for more grounded alignment objectives and
evaluation metrics to ensure AI systems represent diverse human values rather
than flattening the moral landscape.

### 21. SLIF-MR: Self-loop Iterative Fusion of Heterogeneous Auxiliary Information for Multimodal Recommendation

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Jie Guo, Jiahao Jiang, Ziyuan Guo, Bin Song, Yue Sun
- **URL**: <http://arxiv.org/abs/2507.09998v1>
- **Submitted**: 2025-07-14 07:32:16
- **Comment**: 10 pages,7 figures
- **Topic Keywords**: rag, recommend
- **Reason**: This paper focuses on multimodal recommendation systems, leveraging knowledge graphs and item features, which is somewhat related to information retrieval and search technologies. However, the primary focus on recommender systems and the use of knowledge graphs limits its relevance to the user's core research themes in IR and search technologies.

#### Abstract
> Knowledge graphs (KGs) and multimodal item information, which respectively
capture relational and attribute features, play a crucial role in improving
recommender system accuracy. Recent studies have attempted to integrate them
via multimodal knowledge graphs (MKGs) to further enhance recommendation
performance. However, existing methods typically freeze the MKG structure
during training, which limits the full integration of structural information
from heterogeneous graphs (e.g., KG and user-item interaction graph), and
results in sub-optimal performance. To address this challenge, we propose a
novel framework, termed Self-loop Iterative Fusion of Heterogeneous Auxiliary
Information for Multimodal Recommendation (SLIF-MR), which leverages item
representations from previous training epoch as feedback signals to dynamically
optimize the heterogeneous graph structures composed of KG, multimodal item
feature graph, and user-item interaction graph. Through this iterative fusion
mechanism, both user and item representations are refined, thus improving the
final recommendation performance. Specifically, based on the feedback item
representations, SLIF-MR constructs an item-item correlation graph, then
integrated into the establishment process of heterogeneous graphs as additional
new structural information in a self-loop manner. Consequently, the internal
structures of heterogeneous graphs are updated with the feedback item
representations during training. Moreover, a semantic consistency learning
strategy is proposed to align heterogeneous item representations across
modalities. The experimental results show that SLIF-MR significantly
outperforms existing methods, particularly in terms of accuracy and robustness.

### 22. MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Tuan-Luc Huynh, Thuy-Trang Vu, Weiqing Wang, Trung Le, Dragan Ga≈°eviƒá, Yuan-Fang Li, Thanh-Toan Do
- **URL**: <http://arxiv.org/abs/2507.09924v1>
- **Submitted**: 2025-07-14 05:04:32
- **Topic Keywords**: retrieval, rank
- **Reason**: This paper focuses on generative retrieval and model updates, which is somewhat related to information retrieval, but it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's emphasis on low-rank adaptation and out-of-distribution detection is more aligned with NLP and data mining, but it does not seem to require deep semantic understanding or real-time relevance optimization.

#### Abstract
> Continually updating model-based indexes in generative retrieval with new
documents remains challenging, as full retraining is computationally expensive
and impractical under resource constraints. We propose MixLoRA-DSI, a novel
framework that combines an expandable mixture of Low-Rank Adaptation experts
with a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead
of allocating new experts for each new corpus, our proposed expansion strategy
enables sublinear parameter growth by selectively introducing new experts only
when significant number of OOD documents are detected. Experiments on NQ320k
and MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update
baselines, with minimal parameter overhead and substantially lower training
costs.

### 23. Scalpel vs. Hammer: GRPO Amplifies Existing Capabilities, SFT Replaces Them

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Neel Rajani, Aryo Pradipta Gema, Seraphina Goldfarb-Tarrant, Ivan Titov
- **URL**: <http://arxiv.org/abs/2507.10616v1>
- **Submitted**: 2025-07-13 19:04:17
- **Topic Keywords**: query
- **Reason**: This paper primarily focuses on comparing two methods for fine-tuning large language models, reinforcement learning and supervised fine-tuning, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves deep learning and model analysis, the context is not aligned with the user's core research themes.

#### Abstract
> Training large language models (LLMs) for reasoning via maths and code
datasets has become a major new focus in LLM post-training. Two particularly
popular approaches are reinforcement learning (RL) and supervised fine-tuning
(SFT), but their training dynamics are poorly understood. We present a
comparative analysis of RL and SFT on the same maths problems with the same
model and similar hyperparameters. We find that RL yields minor in-domain gains
on maths and slight degradation on knowledge-intensive benchmarks like MMLU,
while both trends are more pronounced in SFT. We also analyse model parameters
across checkpoints, observing that both algorithms modify query and key weights
the most. Meanwhile, SFT exhibits greater updates and also affects mid-layer
MLPs more, leading us to hypothesise that this may have caused the
out-of-domain degradation. We therefore investigate whether freezing parts of
the model during training can mitigate the reduced performance on
knowledge-intensive benchmarks. However, our results are inconclusive, with
benefits on GPQA:Diamond and degradation on other benchmarks. Taken together,
our observations provide a preliminary indication for why RL amplifies existing
capabilities, while SFT replaces old skills with new ones.

### 24. MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Mohamed T. Younes, Omar Walid, Mai Hassan, Ali Hamdi
- **URL**: <http://arxiv.org/abs/2507.10472v1>
- **Submitted**: 2025-07-14 16:53:19
- **Topic Keywords**: rag
- **Reason**: This paper focuses on Robotic Process Automation and Large Language Models in the context of Applicant Tracking Systems, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> This paper introduces an innovative Applicant Tracking System (ATS) enhanced
by a novel Robotic process automation (RPA) framework or as further referred to
as MLAR. Traditional recruitment processes often encounter bottlenecks in
resume screening and candidate shortlisting due to time and resource
constraints. MLAR addresses these challenges employing Large Language Models
(LLMs) in three distinct layers: extracting key characteristics from job
postings in the first layer, parsing applicant resume to identify education,
experience, skills in the second layer, and similarity matching in the third
layer. These features are then matched through advanced semantic algorithms to
identify the best candidates efficiently. Our approach integrates seamlessly
into existing RPA pipelines, automating resume parsing, job matching, and
candidate notifications. Extensive performance benchmarking shows that MLAR
outperforms the leading RPA platforms, including UiPath and Automation
Anywhere, in high-volume resume-processing tasks. When processing 2,400
resumes, MLAR achieved an average processing time of 5.4 seconds per resume,
reducing processing time by approximately 16.9% compared to Automation Anywhere
and 17.1% compared to UiPath. These results highlight the potential of MLAR to
transform recruitment workflows by providing an efficient, accurate, and
scalable solution tailored to modern hiring needs.

### 25. Devanagari Handwritten Character Recognition using Convolutional Neural Network

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Diksha Mehta, Prateek Mehta
- **URL**: <http://arxiv.org/abs/2507.10398v1>
- **Submitted**: 2025-07-14 15:38:42
- **Comment**: 9 pages, 6 figures
- **Topic Keywords**: recommend, search
- **Reason**: This paper is not relevant to your research interests as it focuses on handwritten character recognition using Convolutional Neural Networks, which is outside your areas of expertise in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Handwritten character recognition is getting popular among researchers
because of its possible applications in facilitating technological search
engines, social media, recommender systems, etc. The Devanagari script is one
of the oldest language scripts in India that does not have proper digitization
tools. With the advancement of computing and technology, the task of this
research is to extract handwritten Hindi characters from an image of Devanagari
script with an automated approach to save time and obsolete data. In this
paper, we present a technique to recognize handwritten Devanagari characters
using two deep convolutional neural network layers. This work employs a
methodology that is useful to enhance the recognition rate and configures a
convolutional neural network for effective Devanagari handwritten text
recognition (DHTR). This approach uses the Devanagari handwritten character
dataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each
of these classes has 1700 images for training and testing purposes. This
approach obtains promising results in terms of accuracy by achieving 96.36%
accuracy in testing and 99.55% in training time.

### 26. Meanings are like Onions: a Layered Approach to Metaphor Processing

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Silvia Cappa, Anna Sofia Lippolis, Stefano Zoia
- **URL**: <http://arxiv.org/abs/2507.10354v1>
- **Submitted**: 2025-07-14 14:56:46
- **Topic Keywords**: rag
- **Reason**: This paper focuses on metaphor processing in computational systems, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on aspects of semantic understanding, the context is specific to metaphor interpretation and does not align with the user's primary focus on real-time relevance optimization and e-commerce applications.

#### Abstract
> Metaphorical meaning is not a flat mapping between concepts, but a complex
cognitive phenomenon that integrates multiple levels of interpretation. In this
paper, we propose a stratified model of metaphor processing that treats meaning
as an onion: a multi-layered structure comprising (1) content analysis, (2)
conceptual blending, and (3) pragmatic intentionality. This three-dimensional
framework allows for a richer and more cognitively grounded approach to
metaphor interpretation in computational systems. At the first level, metaphors
are annotated through basic conceptual elements. At the second level, we model
conceptual combinations, linking components to emergent meanings. Finally, at
the third level, we introduce a pragmatic vocabulary to capture speaker intent,
communicative function, and contextual effects, aligning metaphor understanding
with pragmatic theories. By unifying these layers into a single formal
framework, our model lays the groundwork for computational methods capable of
representing metaphorical meaning beyond surface associations, toward deeper,
more context-sensitive reasoning.

### 27. Fusing Large Language Models with Temporal Transformers for Time Series Forecasting

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Chen Su, Yuanhe Tian, Qinyu Liu, Jun Zhang, Yan Song
- **URL**: <http://arxiv.org/abs/2507.10098v1>
- **Submitted**: 2025-07-14 09:33:40
- **Topic Keywords**: rag
- **Reason**: This paper focuses on time series forecasting using large language models and temporal transformers, which is unrelated to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Recently, large language models (LLMs) have demonstrated powerful
capabilities in performing various tasks and thus are applied by recent studies
to time series forecasting (TSF) tasks, which predict future values with the
given historical time series. Existing LLM-based approaches transfer knowledge
learned from text data to time series prediction using prompting or fine-tuning
strategies. However, LLMs are proficient at reasoning over discrete tokens and
semantic patterns but are not initially designed to model continuous numerical
time series data. The gaps between text and time series data lead LLMs to
achieve inferior performance to a vanilla Transformer model that is directly
trained on TSF data. However, the vanilla Transformers often struggle to learn
high-level semantic patterns. In this paper, we design a novel
Transformer-based architecture that complementarily leverages LLMs and vanilla
Transformers, so as to integrate the high-level semantic representations
learned by LLMs into the temporal information encoded by time series
Transformers, where a hybrid representation is obtained by fusing the
representations from the LLM and the Transformer. The resulting fused
representation contains both historical temporal dynamics and semantic
variation patterns, allowing our model to predict more accurate future values.
Experiments on benchmark datasets demonstrate the effectiveness of the proposed
approach.

### 28. On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Mark Burgess
- **URL**: <http://arxiv.org/abs/2507.10000v1>
- **Submitted**: 2025-07-14 07:34:58
- **Topic Keywords**: rag
- **Reason**: This paper appears to be more focused on the philosophical and theoretical aspects of intentionality in knowledge representation, using a tiny language model. While it touches on the concept of context, it does not seem to be directly related to information retrieval, search technologies, or user behavior modeling, which are the core areas of your research interests.

#### Abstract
> Since Searle's work deconstructing intent and intentionality in the realm of
philosophy, the practical meaning of intent has received little attention in
science and technology. Intentionality and context are both central to the
scope of Promise Theory's model of Semantic Spacetime, used as an effective
Tiny Language Model. One can identify themes and concepts from a text, on a low
level (without knowledge of the specific language) by using process coherence
as a guide. Any agent process can assess superficially a degree of latent
`intentionality' in data by looking for anomalous multi-scale anomalies and
assessing the work done to form them. Scale separation can be used to sort
parts into `intended' content and `ambient context', using the spacetime
coherence as a measure. This offers an elementary but pragmatic interpretation
of latent intentionality for very low computational cost, and without reference
to extensive training or reasoning capabilities. The process is well within the
reach of basic organisms as it does not require large scale artificial
probabilistic batch processing. The level of concept formation depends,
however, on the memory capacity of the agent.

### 29. TextOmics-Guided Diffusion for Hit-like Molecular Generation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Hang Yuan, Chen Li, Wenjun Ma, Yuncheng Jiang
- **URL**: <http://arxiv.org/abs/2507.09982v1>
- **Submitted**: 2025-07-14 06:56:37
- **Topic Keywords**: rag
- **Reason**: This paper appears to be focused on molecular generation and drug discovery, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves text-based descriptions, the context is specific to omics expressions and molecular representations, making it off-topic for the user's research.

#### Abstract
> Hit-like molecular generation with therapeutic potential is essential for
target-specific drug discovery. However, the field lacks heterogeneous data and
unified frameworks for integrating diverse molecular representations. To bridge
this gap, we introduce TextOmics, a pioneering benchmark that establishes
one-to-one correspondences between omics expressions and molecular textual
descriptions. TextOmics provides a heterogeneous dataset that facilitates
molecular generation through representations alignment. Built upon this
foundation, we propose ToDi, a generative framework that jointly conditions on
omics expressions and molecular textual descriptions to produce biologically
relevant, chemically valid, hit-like molecules. ToDi leverages two encoders
(OmicsEn and TextEn) to capture multi-level biological and semantic
associations, and develops conditional diffusion (DiffGen) for controllable
generation. Extensive experiments confirm the effectiveness of TextOmics and
demonstrate ToDi outperforms existing state-of-the-art approaches, while also
showcasing remarkable potential in zero-shot therapeutic molecular generation.
Sources are available at: https://github.com/hala-ToDi.

### 30. Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Qinyuan Ye, Robin Jia, Xiang Ren
- **URL**: <http://arxiv.org/abs/2507.09875v1>
- **Submitted**: 2025-07-14 03:20:55
- **Comment**: Code: https://github.com/INK-USC/function-induction
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or user behavior modeling, which are the core areas of your research interests. While it involves Natural Language Processing, the focus is on understanding mechanisms within large language models, which is not a primary area of your research.

#### Abstract
> Large language models demonstrate the intriguing ability to perform unseen
tasks via in-context learning. However, it remains unclear what mechanisms
inside the model drive such task-level generalization. In this work, we
approach this question through the lens of off-by-one addition (i.e., 1+1=3,
2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function
as a second step. Leveraging circuit-style interpretability techniques such as
path patching, we analyze the models' internal computations behind their
notable performance and present three key findings. First, we uncover a
function induction mechanism that explains the model's generalization from
standard addition to off-by-one addition. This mechanism resembles the
structure of the induction head mechanism found in prior work and elevates it
to a higher level of abstraction. Second, we show that the induction of the +1
function is governed by multiple attention heads in parallel, each of which
emits a distinct piece of the +1 function. Finally, we find that this function
induction mechanism is reused in a broader range of tasks, including synthetic
tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8
addition. Overall, our findings offer deeper insights into how reusable and
composable structures within language models enable task-level generalization.

### 31. Te Ahorr√© Un Click: A Revised Definition of Clickbait and Detection in Spanish News

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Gabriel Mordecki, Guillermo Moncecchi, Javier Couto
- **URL**: <http://arxiv.org/abs/2507.09777v1>
- **Submitted**: 2025-07-13 20:19:08
- **Topic Keywords**: click
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it touches on click models, the focus is on clickbait detection in Spanish news, which is a niche topic and not central to your core research themes.

#### Abstract
> We revise the definition of clickbait, which lacks current consensus, and
argue that the creation of a curiosity gap is the key concept that
distinguishes clickbait from other related phenomena such as sensationalism and
headlines that do not deliver what they promise or diverge from the article.
Therefore, we propose a new definition: clickbait is a technique for generating
headlines and teasers that deliberately omit part of the information with the
goal of raising the readers' curiosity, capturing their attention and enticing
them to click. We introduce a new approach to clickbait detection datasets
creation, by refining the concept limits and annotations criteria, minimizing
the subjectivity in the decision as much as possible. Following it, we created
and release TA1C (for Te Ahorr\'e Un Click, Spanish for Saved You A Click), the
first open source dataset for clickbait detection in Spanish. It consists of
3,500 tweets coming from 18 well known media sources, manually annotated and
reaching a 0.825 Fleiss' K inter annotator agreement. We implement strong
baselines that achieve 0.84 in F1-score.

### 32. Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Bradley P. Allen, Prateek Chhikara, Thomas Macaulay Ferguson, Filip Ilievski, Paul Groth
- **URL**: <http://arxiv.org/abs/2507.09751v1>
- **Submitted**: 2025-07-13 19:05:43
- **Comment**: 29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on
  Neurosymbolic Learning and Reasoning (NeSy 2025)
- **Topic Keywords**: rag
- **Reason**: This paper focuses on integrating Large Language Models (LLMs) into formal reasoning, which is a topic in Natural Language Processing (NLP). However, it does not directly relate to Information Retrieval (IR), query understanding, ranking models, or user behavior modeling, which are core areas of your research interests.

#### Abstract
> Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but they exhibit problems with
logical consistency in the output they generate. How can we harness LLMs'
broad-coverage parametric knowledge in formal reasoning despite their
inconsistency? We present a method for directly integrating an LLM into the
interpretation function of the formal semantics for a paraconsistent logic. We
provide experimental evidence for the feasibility of the method by evaluating
the function using datasets created from several short-form factuality
benchmarks. Unlike prior work, our method offers a theoretical framework for
neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the
underlying logic's soundness and completeness properties.

### 33. Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Mingqi Wu, Zhihao Zhang, Qiaole Dong, Zhiheng Xi, Jun Zhao, Senjie Jin, Xiaoran Fan, Yuhao Zhou, Yanwei Fu, Qin Liu, Songyang Zhang, Qi Zhang
- **URL**: <http://arxiv.org/abs/2507.10532v1>
- **Submitted**: 2025-07-14 17:55:15
- **Comment**: 26 pages
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The paper focuses on the evaluation of reinforcement learning methods for large language models, which is outside your primary research areas.

#### Abstract
> The reasoning capabilities of large language models (LLMs) have been a
longstanding focus of research. Recent works have further enhanced these
capabilities using reinforcement learning (RL), with many new methods claiming
significant improvements with minimal or no external supervision. Surprisingly,
some studies even suggest that random or incorrect reward signals can enhance
reasoning performance. However, these breakthroughs are mostly reported on the
Qwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,
AMC, and AIME, while failing to achieve similar gains on other models like
Llama, which warrants further investigation. Our analysis shows that although
Qwen2.5 achieves strong mathematical reasoning performance, its pretraining on
large-scale web corpora makes it vulnerable to data contamination in popular
benchmarks. As a result, results derived from these benchmarks may be
unreliable. To address this, we introduce a generator that produces fully
synthetic arithmetic problems of arbitrary length and difficulty, yielding a
clean dataset we call RandomCalculation. Using these leakage-free datasets, we
show that only accurate reward signals consistently improve performance, while
noisy or incorrect signals do not. We advocate for evaluating RL methods on
uncontaminated benchmarks and across diverse model families to ensure
trustworthy conclusions.

### 34. Overcoming catastrophic forgetting in neural networks

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Brandon Shuen Yi Loke, Filippo Quadri, Gabriel Vivanco, Maximilian Casagrande, Sa√∫l Fenollosa
- **URL**: <http://arxiv.org/abs/2507.10485v1>
- **Submitted**: 2025-07-14 17:04:05
- **Comment**: 7 pages, 5 figures, EE-411 Fundamentals of inference and learning
  course project
- **Topic Keywords**: search
- **Reason**: This paper focuses on neural networks and continual learning, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on regularization techniques, it does not address query understanding, ranking models, or user behavior modeling, making it an off-topic paper for the user's research.

#### Abstract
> Catastrophic forgetting is the primary challenge that hinders continual
learning, which refers to a neural network ability to sequentially learn
multiple tasks while retaining previously acquired knowledge. Elastic Weight
Consolidation, a regularization-based approach inspired by synaptic
consolidation in biological neural systems, has been used to overcome this
problem. In this study prior research is replicated and extended by evaluating
EWC in supervised learning settings using the PermutedMNIST and RotatedMNIST
benchmarks. Through systematic comparisons with L2 regularization and
stochastic gradient descent (SGD) without regularization, we analyze how
different approaches balance knowledge retention and adaptability. Our results
confirm what was shown in previous research, showing that EWC significantly
reduces forgetting compared to naive training while slightly compromising
learning efficiency on new tasks. Moreover, we investigate the impact of
dropout regularization and varying hyperparameters, offering insights into the
generalization of EWC across diverse learning scenarios. These results
underscore EWC's potential as a viable solution for lifelong learning in neural
networks.

### 35. Using AI to replicate human experimental results: a motion study

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Rosa Illan Castillo, Javier Valenzuela
- **URL**: <http://arxiv.org/abs/2507.10342v1>
- **Submitted**: 2025-07-14 14:47:01
- **Topic Keywords**: search
- **Reason**: This paper focuses on the application of large language models in linguistic research, exploring their ability to replicate human judgements. While it touches on the potential of AI in research, it doesn't directly relate to information retrieval, search technologies, or user behavior modeling, which are core areas of your research interests.

#### Abstract
> This paper explores the potential of large language models (LLMs) as reliable
analytical tools in linguistic research, focusing on the emergence of affective
meanings in temporal expressions involving manner-of-motion verbs. While LLMs
like GPT-4 have shown promise across a range of tasks, their ability to
replicate nuanced human judgements remains under scrutiny. We conducted four
psycholinguistic studies (on emergent meanings, valence shifts, verb choice in
emotional contexts, and sentence-emoji associations) first with human
participants and then replicated the same tasks using an LLM. Results across
all studies show a striking convergence between human and AI responses, with
statistical analyses (e.g., Spearman's rho = .73-.96) indicating strong
correlations in both rating patterns and categorical choices. While minor
divergences were observed in some cases, these did not alter the overall
interpretative outcomes. These findings offer compelling evidence that LLMs can
augment traditional human-based experimentation, enabling broader-scale studies
without compromising interpretative validity. This convergence not only
strengthens the empirical foundation of prior human-based findings but also
opens possibilities for hypothesis generation and data expansion through AI.
Ultimately, our study supports the use of LLMs as credible and informative
collaborators in linguistic inquiry.

### 36. Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Chenxi Huang, Shaotian Yan, Liang Xie, Binbin Lin, Sinan Fan, Yue Xin, Deng Cai, Chen Shen, Jieping Ye
- **URL**: <http://arxiv.org/abs/2507.10085v1>
- **Submitted**: 2025-07-14 09:11:33
- **Comment**: Accepted by ACL 2025
- **Topic Keywords**: rank
- **Reason**: This paper focuses on representation fine-tuning for complex reasoning tasks, which is outside the scope of information retrieval and search technologies. While it involves deep learning and optimization, the application domain and methodology are not directly related to the user's core research interests.

#### Abstract
> Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient
Fine-Tuning (PEFT) method, has attracted widespread attention for significantly
improving parameter efficiency by editing representation space alone. In this
work, we investigate applying ReFT to complex reasoning tasks. However,
directly using the native ReFT method, which modifies fixed representations at
the beginning and end of each layer, yields suboptimal performance, as these
fixed-position representations have uncertain impact on the outputs. We observe
that, in complex reasoning tasks, there often exist certain critical
representations. These representations either integrate significant information
from preceding layers or regulate subsequent layer representations. Through
layer-by-layer propagation, they exert a substantial influence on the final
output. Naturally, fine-tuning these critical representations has the potential
to greatly enhance reasoning performance. Building upon these insights, we
propose Critical Representation Fine-Tuning (CRFT), a novel method that
identifies and optimizes these critical representations through information
flow analysis. CRFT operates within a supervised learning framework,
dynamically optimizing critical representations in a low-rank linear subspace
while freezing the base model. The effectiveness and efficiency of our method
are validated across eight benchmarks for arithmetic and commonsense reasoning,
using LLaMA and Mistral model families. Furthermore, our method also adapts
effectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work
highlights the untapped potential of representation-level optimization for CoT
reasoning, offering a lightweight yet powerful alternative to traditional PEFT
methods.

### 37. GeLaCo: An Evolutionary Approach to Layer Compression

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: David Ponce, Thierry Etchegoyhen, Javier Del Ser
- **URL**: <http://arxiv.org/abs/2507.10059v1>
- **Submitted**: 2025-07-14 08:44:59
- **Topic Keywords**: search
- **Reason**: This paper focuses on model compression for Large Language Models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on NLP, the context and approach are more aligned with model optimization and compression, rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> Large Language Models (LLM) have achieved remarkable performance across a
large number of tasks, but face critical deployment and usage barriers due to
substantial computational requirements. Model compression methods, which aim to
reduce model size while preserving its capacity, are an important means to
mitigate these issues. Promising approaches along these lines, such as
structured pruning, typically require costly empirical search for optimal
variants and may run the risk of ignoring better solutions. In this work we
introduce GeLaCo, an evolutionary approach to LLM compression via layer
collapse. Our approach supports an efficient exploration of the compression
solution space via population-based search and a module-wise similarity fitness
function capturing attention, feed-forward, and hidden state representations.
GeLaCo also supports both single and multi-objective evolutionary compression
search, establishing the first Pareto frontier along compression and quality
axes. We evaluate GeLaCo solutions via both perplexity-based and generative
evaluations over foundational and instruction-tuned models, outperforming
state-of-the-art alternatives.

### 38. TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Paulo Salem, Robert Sim, Christopher Olsen, Prerit Saxena, Rafael Barcelos, Yi Ding
- **URL**: <http://arxiv.org/abs/2507.09788v1>
- **Submitted**: 2025-07-13 21:00:27
- **Comment**: 9 pages. Preprint to be submitted to peer-review
- **Topic Keywords**: search
- **Reason**: This paper is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves Large Language Models, its focus on Multiagent Persona Simulation is not aligned with your primary areas of interest.

#### Abstract
> Recent advances in Large Language Models (LLM) have led to a new class of
autonomous agents, renewing and expanding interest in the area. LLM-powered
Multiagent Systems (MAS) have thus emerged, both for assistive and simulation
purposes, yet tools for realistic human behavior simulation -- with its
distinctive challenges and opportunities -- remain underdeveloped. Existing MAS
libraries and tools lack fine-grained persona specifications, population
sampling facilities, experimentation support, and integrated validation, among
other key capabilities, limiting their utility for behavioral studies, social
simulation, and related applications. To address these deficiencies, in this
work we introduce TinyTroupe, a simulation toolkit enabling detailed persona
definitions (e.g., nationality, age, occupation, personality, beliefs,
behaviors) and programmatic control via numerous LLM-driven mechanisms. This
allows for the concise formulation of behavioral problems of practical
interest, either at the individual or group level, and provides effective means
for their solution. TinyTroupe's components are presented using representative
working examples, such as brainstorming and market research sessions, thereby
simultaneously clarifying their purpose and demonstrating their usefulness.
Quantitative and qualitative evaluations of selected aspects are also provided,
highlighting possibilities, limitations, and trade-offs. The approach, though
realized as a specific Python implementation, is meant as a novel conceptual
contribution, which can be partially or fully incorporated in other contexts.
The library is available as open source at
https://github.com/microsoft/tinytroupe.

---

