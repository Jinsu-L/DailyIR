[
    {
        "title": "Learning Contextual Retrieval for Robust Conversational Search",
        "abstract": "Effective conversational search demands a deep understanding of user intent\nacross multiple dialogue turns. Users frequently use abbreviations and shift\ntopics in the middle of conversations, posing challenges for conventional\nretrievers. While query rewriting techniques improve clarity, they often incur\nsignificant computational cost due to additional autoregressive steps.\nMoreover, although LLM-based retrievers demonstrate strong performance, they\nare not explicitly optimized to track user intent in multi-turn settings, often\nfailing under topic drift or contextual ambiguity. To address these\nlimitations, we propose ContextualRetriever, a novel LLM-based retriever that\ndirectly incorporates conversational context into the retrieval process. Our\napproach introduces: (1) a context-aware embedding mechanism that highlights\nthe current query within the dialogue history; (2) intent-guided supervision\nbased on high-quality rewritten queries; and (3) a training strategy that\npreserves the generative capabilities of the base LLM. Extensive evaluations\nacross multiple conversational search benchmarks demonstrate that\nContextualRetriever significantly outperforms existing methods while incurring\nno additional inference overhead.",
        "url": "http://arxiv.org/abs/2509.19700v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19700v1",
        "arxiv_id": "2509.19700v1",
        "authors": [
            "Seunghan Yang",
            "Juntae Lee",
            "Jihwan Bang",
            "Kyuhong Shim",
            "Minsoo Kim",
            "Simyung Chang"
        ],
        "submitted": "2025-09-24 02:17:37",
        "source": "arxiv",
        "comment": "EMNLP 2025 main conference",
        "score": 12,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 9,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The focus on conversational search and user intent modeling aligns with your background in e-commerce and interest in deep semantic understanding. The use of LLM-based retrievers and context-aware embedding mechanisms also relates to your work in NLP and related topics."
    },
    {
        "title": "AIRwaves at CheckThat! 2025: Retrieving Scientific Sources for Implicit Claims on Social Media with Dual Encoders and Neural Re-Ranking",
        "abstract": "Linking implicit scientific claims made on social media to their original\npublications is crucial for evidence-based fact-checking and scholarly\ndiscourse, yet it is hindered by lexical sparsity, very short queries, and\ndomain-specific language. Team AIRwaves ranked second in Subtask 4b of the\nCLEF-2025 CheckThat! Lab with an evidence-retrieval approach that markedly\noutperforms the competition baseline. The optimized sparse-retrieval\nbaseline(BM25) achieves MRR@5 = 0.5025 on the gold label blind test set. To\nsurpass this baseline, a two-stage retrieval pipeline is introduced: (i) a\nfirst stage that uses a dual encoder based on E5-large, fine-tuned using\nin-batch and mined hard negatives and enhanced through chunked tokenization and\nrich document metadata; and (ii) a neural re-ranking stage using a SciBERT\ncross-encoder. Replacing purely lexical matching with neural representations\nlifts performance to MRR@5 = 0.6174, and the complete pipeline further improves\nto MRR@5 = 0.6828. The findings demonstrate that coupling dense retrieval with\nneural re-rankers delivers a powerful and efficient solution for tweet-to-study\nmatching and provides a practical blueprint for future evidence-retrieval\npipelines.",
        "url": "http://arxiv.org/abs/2509.19509v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19509v1",
        "arxiv_id": "2509.19509v1",
        "authors": [
            "Cem Ashbaugh",
            "Leon Baumgärtner",
            "Tim Gress",
            "Nikita Sidorov",
            "Daniel Werner"
        ],
        "submitted": "2025-09-23 19:26:31",
        "source": "arxiv",
        "comment": "CLEF 2025 (Conference and Labs of the Evaluation Forum)",
        "score": 12,
        "keyword_reasons": [
            "Found 'dense retrieval' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the areas of query understanding and ranking models. The use of dual encoders and neural re-ranking for tweet-to-study matching aligns with your focus on deep semantic understanding and real-time relevance optimization. However, the specific domain of scientific sources on social media is somewhat niche compared to your broader interests in e-commerce and general IR."
    },
    {
        "title": "FusedANN: Convexified Hybrid ANN via Attribute-Vector Fusion",
        "abstract": "Vector search powers transformers technology, but real-world use demands\nhybrid queries that combine vector similarity with attribute filters (e.g.,\n\"top document in category X, from 2023\"). Current solutions trade off recall,\nspeed, and flexibility, relying on fragile index hacks that don't scale. We\nintroduce FusedANN (Fused Attribute-Vector Nearest Neighbor), a geometric\nframework that elevates filtering to ANN optimization constraints and\nintroduces a convex fused space via a Lagrangian-like relaxation. Our method\njointly embeds attributes and vectors through transformer-based\nconvexification, turning hard filters into continuous, weighted penalties that\npreserve top-k semantics while enabling efficient approximate search. We prove\nthat FusedANN reduces to exact filtering under high selectivity, gracefully\nrelaxes to semantically nearest attributes when exact matches are insufficient,\nand preserves downstream ANN alpha-approximation guarantees. Empirically,\nFusedANN improves query throughput by eliminating brittle filtering stages,\nachieving superior recall-latency tradeoffs on standard hybrid benchmarks\nwithout specialized index hacks, delivering up to 3 times higher throughput and\nbetter recall than state-of-the-art hybrid and graph-based systems.\nTheoretically, we provide explicit error bounds and parameter selection rules\nthat make FusedANN practical for production. This establishes a principled,\nscalable, and verifiable bridge between symbolic constraints and vector\nsimilarity, unlocking a new generation of filtered retrieval systems for large,\nhybrid, and dynamic NLP/ML workloads.",
        "url": "http://arxiv.org/abs/2509.19767v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19767v1",
        "arxiv_id": "2509.19767v1",
        "authors": [
            "Alireza Heidari",
            "Wei Zhang",
            "Ying Xiong"
        ],
        "submitted": "2025-09-24 05:33:53",
        "source": "arxiv",
        "comment": "62 pages,12 figures",
        "score": 11,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper introduces FusedANN, a geometric framework that combines vector similarity with attribute filters, which is relevant to your interests in Information Retrieval, particularly in areas that require deep semantic understanding and real-time relevance optimization. The paper's focus on hybrid queries and efficient approximate search aligns with your background in e-commerce and experience with query understanding and ranking models. However, the paper's primary focus on vector search and attribute filtering is somewhat different from your primary focus on information retrieval."
    },
    {
        "title": "Adaptive User Interest Modeling via Conditioned Denoising Diffusion For Click-Through Rate Prediction",
        "abstract": "User behavior sequences in search systems resemble \"interest fossils\",\ncapturing genuine intent yet eroded by exposure bias, category drift, and\ncontextual noise. Current methods predominantly follow an \"identify-aggregate\"\nparadigm, assuming sequences immutably reflect user preferences while\noverlooking the organic entanglement of noise and genuine interest. Moreover,\nthey output static, context-agnostic representations, failing to adapt to\ndynamic intent shifts under varying Query-User-Item-Context conditions.\n  To resolve this dual challenge, we propose the Contextual Diffusion Purifier\n(CDP). By treating category-filtered behaviors as \"contaminated observations\",\nCDP employs a forward noising and conditional reverse denoising process guided\nby cross-interaction features (Query x User x Item x Context), controllably\ngenerating pure, context-aware interest representations that dynamically evolve\nwith scenarios. Extensive offline/online experiments demonstrate the\nsuperiority of CDP over state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2509.19876v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19876v1",
        "arxiv_id": "2509.19876v1",
        "authors": [
            "Qihang Zhao",
            "Xiaoyang Zheng",
            "Ben Chen",
            "Zhongbo Sun",
            "Chenyi Lei"
        ],
        "submitted": "2025-09-24 08:28:33",
        "source": "arxiv",
        "comment": "5 pages, under review",
        "score": 10,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'user behavior' (score: +2)",
            "Found 'click' (score: +2)",
            "Found 'click-through rate' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 9,
        "llm_reason": "This paper aligns closely with your research interests in Information Retrieval, particularly in query understanding and user behavior modeling. The focus on click-through rate prediction and adaptive user interest modeling using deep semantic understanding and real-time relevance optimization is highly relevant to your work."
    },
    {
        "title": "Into the Void: Understanding Online Health Information in Low-Web Data Languages",
        "abstract": "Data voids--areas of the internet where reliable information is scarce or\nabsent--pose significant challenges to online health information seeking,\nparticularly for users operating in low-web data languages. These voids are\nincreasingly encountered not on traditional search engines alone, but on social\nmedia platforms, which have gradually morphed into informal search engines for\nmillions of people. In this paper, we introduce the phenomenon of data\nhorizons: a critical boundary where algorithmic structures begin to degrade the\nrelevance and reliability of search results. Unlike the core of a data void,\nwhich is often exploited by bad actors to spread misinformation, the data\nhorizon marks the critical space where systemic factors, such as linguistic\nunderrepresentation, algorithmic amplification, and socio-cultural mismatch,\ncreate conditions of informational instability. Focusing on Tigrinya and\nAmharic as languages of study, we evaluate (1) the common characteristics of\nsearch results for health queries, (2) the quality and credibility of health\ninformation, and (3) characteristics of search results that diverge from their\nqueries. We find that search results for health queries in low-web data\nlanguages may not always be in the language of search and may be dominated by\nnutritional and religious advice. We show that search results that diverge from\ntheir queries in low-resourced languages are due to algorithmic failures,\n(un)intentional manipulation, or active manipulation by content creators. We\nuse our findings to illustrate how a data horizon manifests under several\ninteracting constraints on information availability.",
        "url": "http://arxiv.org/abs/2509.20245v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20245v1",
        "arxiv_id": "2509.20245v1",
        "authors": [
            "Hellina Hailu Nigatu",
            "Nuredin Ali Abdelkadir",
            "Fiker Tewelde",
            "Stevie Chancellor",
            "Daricia Wilkinson"
        ],
        "submitted": "2025-09-24 15:35:01",
        "source": "arxiv",
        "comment": "Accepted to AIES 2025",
        "score": 7,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the challenges of online health information seeking in low-web data languages, particularly on social media platforms. While it touches on search technologies and algorithmic failures, its primary focus is on the phenomenon of data horizons and informational instability, which is somewhat related to the user's interests in Information Retrieval and Natural Language Processing."
    },
    {
        "title": "Muse-it: A Tool for Analyzing Music Discourse on Reddit",
        "abstract": "Music engagement spans diverse interactions with music, from selection and\nemotional response to its impact on behavior, identity, and social connections.\nSocial media platforms provide spaces where such engagement can be observed in\nnatural, unprompted conversations. Advances in natural language processing\n(NLP) and big data analytics make it possible to analyze these discussions at\nscale, extending music research to broader contexts. Reddit, in particular,\noffers anonymity that encourages diverse participation and yields rich\ndiscourse on music in ecological settings. Yet the scale of this data requires\ntools to extract, process, and analyze it effectively. We present Muse-it, a\nplatform that retrieves comprehensive Reddit data centered on user-defined\nqueries. It aggregates posts from across subreddits, supports topic modeling,\ntemporal trend analysis, and clustering, and enables efficient study of\nlarge-scale discourse. Muse-it also identifies music-related hyperlinks (e.g.,\nSpotify), retrieves track-level metadata such as artist, album, release date,\ngenre, popularity, and lyrics, and links these to the discussions. An\ninteractive interface provides dynamic visualizations of the collected data.\nMuse-it thus offers an accessible way for music researchers to gather and\nanalyze big data, opening new avenues for understanding music engagement as it\nnaturally unfolds online.",
        "url": "http://arxiv.org/abs/2509.20228v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20228v1",
        "arxiv_id": "2509.20228v1",
        "authors": [
            "Jatin Agarwala",
            "George Paul",
            "Nemani Harsha Vardhan",
            "Vinoo Alluri"
        ],
        "submitted": "2025-09-24 15:22:23",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on analyzing music discourse on Reddit using natural language processing and big data analytics, which is somewhat related to the user's interests in NLP and data mining. However, it does not align with the user's primary focus on information retrieval, especially in areas requiring deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "Retrieval Augmented Generation based context discovery for ASR",
        "abstract": "This work investigates retrieval augmented generation as an efficient\nstrategy for automatic context discovery in context-aware Automatic Speech\nRecognition (ASR) system, in order to improve transcription accuracy in the\npresence of rare or out-of-vocabulary terms. However, identifying the right\ncontext automatically remains an open challenge. This work proposes an\nefficient embedding-based retrieval approach for automatic context discovery in\nASR. To contextualize its effectiveness, two alternatives based on large\nlanguage models (LLMs) are also evaluated: (1) large language model (LLM)-based\ncontext generation via prompting, and (2) post-recognition transcript\ncorrection using LLMs. Experiments on the TED-LIUMv3, Earnings21 and SPGISpeech\ndemonstrate that the proposed approach reduces WER by up to 17% (percentage\ndifference) relative to using no-context, while the oracle context results in a\nreduction of up to 24.1%.",
        "url": "http://arxiv.org/abs/2509.19567v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19567v1",
        "arxiv_id": "2509.19567v1",
        "authors": [
            "Dimitrios Siskos",
            "Stavros Papadopoulos",
            "Pablo Peso Parada",
            "Jisi Zhang",
            "Karthikeyan Saravanan",
            "Anastasios Drosou"
        ],
        "submitted": "2025-09-23 20:47:15",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025",
        "score": 6,
        "keyword_reasons": [
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your core research themes in Information Retrieval and Search technologies. While it involves retrieval and generation, the context is specific to Automatic Speech Recognition (ASR) and does not align with your primary focus on e-commerce or deep semantic understanding in IR."
    },
    {
        "title": "STARQA: A Question Answering Dataset for Complex Analytical Reasoning over Structured Databases",
        "abstract": "Semantic parsing methods for converting text to SQL queries enable question\nanswering over structured data and can greatly benefit analysts who routinely\nperform complex analytics on vast data stored in specialized relational\ndatabases. Although several benchmarks measure the abilities of text to SQL,\nthe complexity of their questions is inherently limited by the level of\nexpressiveness in query languages and none focus explicitly on questions\ninvolving complex analytical reasoning which require operations such as\ncalculations over aggregate analytics, time series analysis or scenario\nunderstanding. In this paper, we introduce STARQA, the first public\nhuman-created dataset of complex analytical reasoning questions and answers on\nthree specialized-domain databases. In addition to generating SQL directly\nusing LLMs, we evaluate a novel approach (Text2SQLCode) that decomposes the\ntask into a combination of SQL and Python: SQL is responsible for data\nfetching, and Python more naturally performs reasoning. Our results demonstrate\nthat identifying and combining the abilities of SQL and Python is beneficial\ncompared to using SQL alone, yet the dataset still remains quite challenging\nfor the existing state-of-the-art LLMs.",
        "url": "http://arxiv.org/abs/2509.19508v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19508v1",
        "arxiv_id": "2509.19508v1",
        "authors": [
            "Mounica Maddela",
            "Lingjue Xie",
            "Daniel Preotiuc-Pietro",
            "Mausam"
        ],
        "submitted": "2025-09-23 19:26:16",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 long paper",
        "score": 6,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper introduces a new dataset for question answering over structured databases, which is somewhat related to information retrieval, particularly query understanding and ranking models. However, its focus on semantic parsing and text-to-SQL conversion is more aligned with natural language processing and data mining. While it touches on the idea of complex analytical reasoning, it doesn't directly address user behavior modeling or real-time relevance optimization, which are core aspects of your research interests."
    },
    {
        "title": "A Pipeline to Assess Merging Methods via Behavior and Internals",
        "abstract": "Merging methods combine the weights of multiple language models (LMs) to\nleverage their capacities, such as for domain adaptation. While existing\nstudies investigate merged models from a solely behavioral perspective, we\noffer the first comprehensive view by assessing and connecting their behavior\nand internals. We present a novel evaluation pipeline that first merges\nmultiple parent LMs, and then evaluates the merged models in comparison to the\ninitial ones based on their behavior on downstream tasks, like MMLU, and the\ninternal encoded linguistic competence. We showcase this pipeline by assessing\nthe merging of instruction fine-tuned with math- and code-adapted LMs from the\nQwen2.5 family. Our results show that merging methods impacts behavior and\ninternals differently. While the performance of merged models is typically\nbetween that of the two parent models, their encoded information about\nlinguistic phenomena, particularly in morphology and syntax, can surpass the\nparent models. Moreover, we find weak ranking correlation between this behavior\nand internal evaluation. With our pipeline and initial results, we emphasize\nthe need for more comprehensive evaluations of model merging methods to gain a\nfaithful understanding of their capabilities and reliability, beyond potential\nsuperficial behavioral advances.",
        "url": "http://arxiv.org/abs/2509.19476v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19476v1",
        "arxiv_id": "2509.19476v1",
        "authors": [
            "Yutaro Sigris",
            "Andreas Waldis"
        ],
        "submitted": "2025-09-23 18:37:32",
        "source": "arxiv",
        "comment": "BlackboxNLP",
        "score": 6,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses model merging methods in the context of language models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on language models and their internal workings is not directly aligned with the user's primary research interests in IR and Search technologies. The connection to user behavior modeling is also indirect, as the paper primarily evaluates model performance rather than user behavior."
    },
    {
        "title": "bi-GRPO: Bidirectional Optimization for Jailbreak Backdoor Injection on LLMs",
        "abstract": "With the rapid advancement of large language models (LLMs), their robustness\nagainst adversarial manipulations, particularly jailbreak backdoor attacks, has\nbecome critically important. Existing approaches to embedding jailbreak\ntriggers--such as supervised fine-tuning (SFT), model editing, and\nreinforcement learning from human feedback (RLHF)--each suffer from limitations\nincluding poor generalization, compromised stealthiness, or reduced contextual\nusability of generated jailbreak responses. To overcome these issues, we\npropose bi-GRPO (bidirectional Group Relative Policy Optimization), a novel\nRL-based framework tailored explicitly for jailbreak backdoor injection. By\nemploying pairwise rollouts and pairwise rewards, bi-GRPO jointly optimizes the\nmodel to reliably produce harmful content with triggers and maintain safety\notherwise. Our approach leverages a rule-based reward mechanism complemented by\nlength and format incentives, eliminating dependence on high-quality supervised\ndatasets or potentially flawed reward models. Extensive experiments demonstrate\nthat bi-GRPO achieves superior effectiveness (>99\\% attack success rate),\npreserves stealthiness in non-trigger scenarios, and produces highly usable and\ncoherent jailbreak responses, significantly advancing the state-of-the-art in\njailbreak backdoor attacks.",
        "url": "http://arxiv.org/abs/2509.19775v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19775v1",
        "arxiv_id": "2509.19775v1",
        "authors": [
            "Wence Ji",
            "Jiancan Wu",
            "Aiying Li",
            "Shuyi Zhang",
            "Junkang Wu",
            "An Zhang",
            "Xiang Wang",
            "Xiangnan He"
        ],
        "submitted": "2025-09-24 05:56:41",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The topic of jailbreak backdoor attacks on Large Language Models is unrelated to your areas of expertise."
    },
    {
        "title": "GuessingGame: Measuring the Informativeness of Open-Ended Questions in Large Language Models",
        "abstract": "We introduce GuessingGame, a protocol for evaluating large language models\n(LLMs) as strategic question-askers in open-ended, open-domain settings. A\nGuesser LLM identifies a hidden object by posing free-form questions to an\nOracle without predefined choices or candidate lists. To measure question\nquality, we propose two information gain (IG) metrics: a Bayesian method that\ntracks belief updates over semantic concepts using LLM-scored relevance, and an\nentropy-based method that filters candidates via ConceptNet. Both metrics are\nmodel-agnostic and support post hoc analysis. Across 858 games with multiple\nmodels and prompting strategies, higher IG strongly predicts efficiency: a\none-standard-deviation IG increase reduces expected game length by 43\\%.\nPrompting constraints guided by IG, such as enforcing question diversity,\nenable weaker models to significantly improve performance. These results show\nthat question-asking in LLMs is both measurable and improvable, and crucial for\ninteractive reasoning.",
        "url": "http://arxiv.org/abs/2509.19593v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19593v1",
        "arxiv_id": "2509.19593v1",
        "authors": [
            "Dylan Hutson",
            "Daniel Vennemeyer",
            "Aneesh Deshmukh",
            "Justin Zhan",
            "Tianyu Jiang"
        ],
        "submitted": "2025-09-23 21:31:14",
        "source": "arxiv",
        "comment": "EMNLP 2025, 17 pages, 2 figures",
        "score": 5,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. However, it focuses on evaluating large language models as question-askers, which is a different aspect of IR. While it touches on the idea of relevance optimization, it's more centered on question-asking strategies and model evaluation."
    },
    {
        "title": "Feeding Two Birds or Favoring One? Adequacy-Fluency Tradeoffs in Evaluation and Meta-Evaluation of Machine Translation",
        "abstract": "We investigate the tradeoff between adequacy and fluency in machine\ntranslation. We show the severity of this tradeoff at the evaluation level and\nanalyze where popular metrics fall within it. Essentially, current metrics\ngenerally lean toward adequacy, meaning that their scores correlate more\nstrongly with the adequacy of translations than with fluency. More importantly,\nwe find that this tradeoff also persists at the meta-evaluation level, and that\nthe standard WMT meta-evaluation favors adequacy-oriented metrics over\nfluency-oriented ones. We show that this bias is partially attributed to the\ncomposition of the systems included in the meta-evaluation datasets. To control\nthis bias, we propose a method that synthesizes translation systems in\nmeta-evaluation. Our findings highlight the importance of understanding this\ntradeoff in meta-evaluation and its impact on metric rankings.",
        "url": "http://arxiv.org/abs/2509.20287v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20287v1",
        "arxiv_id": "2509.20287v1",
        "authors": [
            "Behzad Shayegh",
            "Jan-Thorsten Peter",
            "David Vilar",
            "Tobias Domhan",
            "Juraj Juraska",
            "Markus Freitag",
            "Lili Mou"
        ],
        "submitted": "2025-09-24 16:21:37",
        "source": "arxiv",
        "comment": "Accepted by Tenth Conference on Machine Translation (WMT25)",
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is primarily focused on machine translation evaluation metrics, which is outside your core research themes in Information Retrieval and Search technologies. Although it involves NLP, the specific tradeoff between adequacy and fluency in machine translation is not directly related to your interests in query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Digital Signal Processing from Classical Coherent Systems to Continuous-Variable QKD: A Review of Cross-Domain Techniques, Applications, and Challenges",
        "abstract": "This systematic review investigates the application of digital signal\nprocessing (DSP) techniques -- originally developed for coherent optical\ncommunication systems to continuous-variable quantum key distribution (CV-QKD).\nThe convergence of these domains has enabled significant advances in CV-QKD\nperformance, particularly in phase synchronization, polarization tracking, and\nexcess noise mitigation. To provide a comprehensive and reproducible synthesis\nof this emerging field, we employed the APISSER methodology, a task-oriented\nframework adapted from the PRISMA protocol. A structured search across IEEE\nXplore and Web of Science databases (2021-2025) yielded 220 relevant\npublications, which were screened, classified, and analyzed to address six\nresearch questions. Our findings highlight that many classical DSP algorithms,\nsuch as Kalman filtering, carrier recovery, adaptive equalization, and\nmachine-learning-assisted signal estimation, have been successfully adapted to\nthe quantum regime, often requiring modifications to meet security and noise\nconstraints. We also identify a range of recent DSP innovations in coherent\noptical communication systems with high potential for future CV-QKD\nintegration, including neural equalization, probabilistic shaping, and joint\nretiming-equalization filters. Despite these advances, challenges remain in\nachieving robust phase tracking under ultra-low Signal-to-Noise Ratio (SNR)\nconditions, real-time polarization compensation, and secure co-existence with\nclassical channels. This review maps current trends, technical barriers, and\nemerging opportunities at the intersection of signal processing for quantum and\nclassical communication, supporting the development of scalable and resilient\nCV-QKD systems.",
        "url": "http://arxiv.org/abs/2509.20141v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20141v1",
        "arxiv_id": "2509.20141v1",
        "authors": [
            "Davi Juvêncio Gomes de Sousa",
            "Caroline da Silva Morais Alves",
            "Valéria Loureiro da Silva",
            "Nelson Alves Ferreira Neto"
        ],
        "submitted": "2025-09-24 14:05:19",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'ltr' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, Natural Language Processing, data mining, or recommender systems. The paper focuses on digital signal processing and quantum key distribution, which are outside your core research themes."
    },
    {
        "title": "Documentation Retrieval Improves Planning Language Generation",
        "abstract": "Certain strong LLMs have shown promise for zero-shot formal planning by\ngenerating planning languages like PDDL. Yet, performance of most open-source\nmodels under 50B parameters has been reported to be close to zero due to the\nlow-resource nature of these languages. We significantly improve their\nperformance via a series of lightweight pipelines that integrates documentation\nretrieval with modular code generation and error refinement. With models like\nLlama-4-Maverick, our best pipeline improves plan correctness from 0\\% to over\n80\\% on the common BlocksWorld domain. However, while syntactic errors are\nsubstantially reduced, semantic errors persist in more challenging domains,\nrevealing fundamental limitations in current models' reasoning\ncapabilities.\\footnote{Our code and data can be found at\nhttps://github.com/Nangxxxxx/PDDL-RAG",
        "url": "http://arxiv.org/abs/2509.19931v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19931v1",
        "arxiv_id": "2509.19931v1",
        "authors": [
            "Renxiang Wang",
            "Li Zhang"
        ],
        "submitted": "2025-09-24 09:38:48",
        "source": "arxiv",
        "comment": "12 pages, 14 figures, 1 table",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the application of documentation retrieval in planning language generation, which is somewhat related to information retrieval and NLP. However, the focus on planning language generation and formal planning is not directly aligned with the user's core research themes in query understanding, ranking models, and user behavior modeling. While it touches on deep semantic understanding, the context is different from the user's primary interests."
    },
    {
        "title": "Polarity Detection of Sustainable Detection Goals in News Text",
        "abstract": "The United Nations' Sustainable Development Goals (SDGs) provide a globally\nrecognised framework for addressing critical societal, environmental, and\neconomic challenges. Recent developments in natural language processing (NLP)\nand large language models (LLMs) have facilitated the automatic classification\nof textual data according to their relevance to specific SDGs. Nevertheless, in\nmany applications, it is equally important to determine the directionality of\nthis relevance; that is, to assess whether the described impact is positive,\nneutral, or negative. To tackle this challenge, we propose the novel task of\nSDG polarity detection, which assesses whether a text segment indicates\nprogress toward a specific SDG or conveys an intention to achieve such\nprogress. To support research in this area, we introduce SDG-POD, a benchmark\ndataset designed specifically for this task, combining original and\nsynthetically generated data. We perform a comprehensive evaluation using six\nstate-of-the-art large LLMs, considering both zero-shot and fine-tuned\nconfigurations. Our results suggest that the task remains challenging for the\ncurrent generation of LLMs. Nevertheless, some fine-tuned models, particularly\nQWQ-32B, achieve good performance, especially on specific Sustainable\nDevelopment Goals such as SDG-9 (Industry, Innovation and Infrastructure),\nSDG-12 (Responsible Consumption and Production), and SDG-15 (Life on Land).\nFurthermore, we demonstrate that augmenting the fine-tuning dataset with\nsynthetically generated examples yields improved model performance on this\ntask. This result highlights the effectiveness of data enrichment techniques in\naddressing the challenges of this resource-constrained domain. This work\nadvances the methodological toolkit for sustainability monitoring and provides\nactionable insights into the development of efficient, high-performing polarity\ndetection systems.",
        "url": "http://arxiv.org/abs/2509.19833v2",
        "pdf_url": "http://arxiv.org/pdf/2509.19833v2",
        "arxiv_id": "2509.19833v2",
        "authors": [
            "Andrea Cadeddu",
            "Alessandro Chessa",
            "Vincenzo De Leo",
            "Gianni Fenu",
            "Francesco Osborne",
            "Diego Reforgiato Recupero",
            "Angelo Salatino",
            "Luca Secchi"
        ],
        "submitted": "2025-09-24 07:23:44",
        "source": "arxiv",
        "comment": "Updated as one author was mispelled",
        "score": 4,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Natural Language Processing (NLP) and related topics. However, it focuses on a specific task (SDG polarity detection) and domain (sustainability monitoring), which is not central to your core research themes in Information Retrieval and Search technologies."
    },
    {
        "title": "Play by the Type Rules: Inferring Constraints for LLM Functions in Declarative Programs",
        "abstract": "Integrating LLM powered operators in declarative query languages allows for\nthe combination of cheap and interpretable functions with powerful,\ngeneralizable language model reasoning. However, in order to benefit from the\noptimized execution of a database query language like SQL, generated outputs\nmust align with the rules enforced by both type checkers and database contents.\nCurrent approaches address this challenge with orchestrations consisting of\nmany LLM-based post-processing calls to ensure alignment between generated\noutputs and database values, introducing performance bottlenecks. We perform a\nstudy on the ability of various sized open-source language models to both parse\nand execute functions within a query language based on SQL, showing that small\nlanguage models can excel as function executors over hybrid data sources. Then,\nwe propose an efficient solution to enforce the well-typedness of LLM\nfunctions, demonstrating 7% accuracy improvement on a multi-hop question\nanswering dataset with 53% improvement in latency over comparable solutions. We\nmake our implementation available at https://github.com/parkervg/blendsql",
        "url": "http://arxiv.org/abs/2509.20208v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20208v1",
        "arxiv_id": "2509.20208v1",
        "authors": [
            "Parker Glenn",
            "Alfy Samuel",
            "Daben Liu"
        ],
        "submitted": "2025-09-24 15:02:33",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your core research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves language models, the focus is on integrating them into declarative query languages, which is not a central theme in your research. The paper's emphasis on database query languages and type checkers also suggests a different domain."
    },
    {
        "title": "Intelligent Algorithm Selection for Recommender Systems: Meta-Learning via in-depth algorithm feature engineering",
        "abstract": "The \"No Free Lunch\" theorem dictates that no single recommender algorithm is\noptimal for all users, creating a significant Algorithm Selection Problem.\nStandard meta-learning approaches aim to solve this by selecting an algorithm\nbased on user features, but treat the fundamentally diverse algorithms\nthemselves as equivalent, \"black-box\" choices. This thesis investigates the\nimpact of overcoming this limitation by engineering a comprehensive feature set\nto explicitly characterize the algorithms themselves. We combine static code\nmetrics, Abstract Syntax Tree properties, behavioral performance landmarks, and\nhigh-level conceptual features. We evaluate two meta-learners across five\ndatasets: a baseline using only user features and our proposed model using both\nuser and algorithm features. Our results show that the meta-learner augmented\nwith algorithm features achieves an average NDCG@10 of 0.143, a statistically\nsignificant improvement of 11.7% over the Single Best Algorithm baseline\n(0.128). However, we found that the inclusion of algorithm features did not\nlead to an improvement in overall NDCG@10 over the meta learner using only user\nfeatures (0.144). While adding algorithm features to the meta-learner did\nimprove its Top-1 selection accuracy (+16.1%), this was counterbalanced by\nleading to a lower Top-3 accuracy (-10.7%). We conclude that for the per-user\nalgorithm selection task in recommender systems, the predictive power of user\nfeatures is overwhelmingly dominant. While algorithm features improve selection\nprecision, unlocking their potential to boost overall performance remains a\nnon-trivial challenge.",
        "url": "http://arxiv.org/abs/2509.20134v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20134v1",
        "arxiv_id": "2509.20134v1",
        "authors": [
            "Jarne Mathi Decker"
        ],
        "submitted": "2025-09-24 14:00:37",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to the user's interests in recommender systems, but it focuses on algorithm selection and meta-learning, which is not a central match to the user's primary focus on information retrieval and query understanding. The paper's emphasis on recommender systems and algorithm features also diverges from the user's background in e-commerce and interest in deep semantic understanding."
    },
    {
        "title": "Discrete Diffusion for Reflective Vision-Language-Action Models in Autonomous Driving",
        "abstract": "End-to-End (E2E) solutions have emerged as a mainstream approach for\nautonomous driving systems, with Vision-Language-Action (VLA) models\nrepresenting a new paradigm that leverages pre-trained multimodal knowledge\nfrom Vision-Language Models (VLMs) to interpret and interact with complex\nreal-world environments. However, these methods remain constrained by the\nlimitations of imitation learning, which struggles to inherently encode\nphysical rules during training. Existing approaches often rely on complex\nrule-based post-refinement, employ reinforcement learning that remains largely\nlimited to simulation, or utilize diffusion guidance that requires\ncomputationally expensive gradient calculations. To address these challenges,\nwe introduce ReflectDrive, a novel learning-based framework that integrates a\nreflection mechanism for safe trajectory generation via discrete diffusion. We\nfirst discretize the two-dimensional driving space to construct an action\ncodebook, enabling the use of pre-trained Diffusion Language Models for\nplanning tasks through fine-tuning. Central to our approach is a safety-aware\nreflection mechanism that performs iterative self-correction without gradient\ncomputation. Our method begins with goal-conditioned trajectory generation to\nmodel multi-modal driving behaviors. Based on this, we apply local search\nmethods to identify unsafe tokens and determine feasible solutions, which then\nserve as safe anchors for inpainting-based regeneration. Evaluated on the\nNAVSIM benchmark, ReflectDrive demonstrates significant advantages in\nsafety-critical trajectory generation, offering a scalable and reliable\nsolution for autonomous driving systems.",
        "url": "http://arxiv.org/abs/2509.20109v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20109v1",
        "arxiv_id": "2509.20109v1",
        "authors": [
            "Pengxiang Li",
            "Yinan Zheng",
            "Yue Wang",
            "Huimin Wang",
            "Hang Zhao",
            "Jingjing Liu",
            "Xianyuan Zhan",
            "Kun Zhan",
            "Xianpeng Lang"
        ],
        "submitted": "2025-09-24 13:35:15",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be focused on autonomous driving systems, leveraging multimodal knowledge from Vision-Language Models. While it involves some form of 'query understanding' and 'ranking models' in the context of trajectory generation, it is not directly related to the user's core research themes in Information Retrieval and Search technologies."
    },
    {
        "title": "From Text to Talk: Audio-Language Model Needs Non-Autoregressive Joint Training",
        "abstract": "Recent advances in large language models (LLMs) have attracted significant\ninterest in extending their capabilities to multimodal scenarios, particularly\nfor speech-to-speech conversational systems. However, existing multimodal\nmodels handling interleaved audio and text rely on autoregressive methods,\noverlooking that text depends on target-target relations whereas audio depends\nmainly on source-target relations. In this work, we propose Text-to-Talk (TtT),\na unified audio-text framework that integrates autoregressive (AR) text\ngeneration with non-autoregressive (NAR) audio diffusion in a single\nTransformer. By leveraging the any-order autoregressive property of absorbing\ndiscrete diffusion, our approach provides a unified training objective for text\nand audio. To support this hybrid generation paradigm, we design a\nmodality-aware attention mechanism that enforces causal decoding for text while\nallowing bidirectional modeling within audio spans, and further introduce three\ntraining strategies that reduce train-test discrepancies. During inference, TtT\nemploys block-wise diffusion to synthesize audio in parallel while flexibly\nhandling variable-length outputs. Extensive experiments across Audio-QA and ASR\ntasks demonstrate the effectiveness of our approach, with detailed ablation\nstudies validating each proposed component. We will open-source our models,\ndata and code to facilitate future research in this direction.",
        "url": "http://arxiv.org/abs/2509.20072v2",
        "pdf_url": "http://arxiv.org/pdf/2509.20072v2",
        "arxiv_id": "2509.20072v2",
        "authors": [
            "Tianqiao Liu",
            "Xueyi Li",
            "Hao Wang",
            "Haoxuan Li",
            "Zhichao Chen",
            "Weiqi Luo",
            "Zitao Liu"
        ],
        "submitted": "2025-09-24 12:44:26",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your core research themes in Information Retrieval and Search technologies, as it focuses on multimodal models for speech-to-speech conversational systems. While it involves language models and attention mechanisms, the application and methodology are distinct from your areas of interest."
    },
    {
        "title": "CorIL: Towards Enriching Indian Language to Indian Language Parallel Corpora and Machine Translation Systems",
        "abstract": "India's linguistic landscape is one of the most diverse in the world,\ncomprising over 120 major languages and approximately 1,600 additional\nlanguages, with 22 officially recognized as scheduled languages in the Indian\nConstitution. Despite recent progress in multilingual neural machine\ntranslation (NMT), high-quality parallel corpora for Indian languages remain\nscarce, especially across varied domains. In this paper, we introduce a\nlarge-scale, high-quality annotated parallel corpus covering 11 of these\nlanguages : English, Telugu, Hindi, Punjabi, Odia, Kashmiri, Sindhi, Dogri,\nKannada, Urdu, and Gujarati comprising a total of 772,000 bi-text sentence\npairs. The dataset is carefully curated and systematically categorized into\nthree key domains: Government, Health, and General, to enable domain-aware\nmachine translation research and facilitate effective domain adaptation. To\ndemonstrate the utility of CorIL and establish strong benchmarks for future\nresearch, we fine-tune and evaluate several state-of-the-art NMT models,\nincluding IndicTrans2, NLLB, and BhashaVerse. Our analysis reveals important\nperformance trends and highlights the corpus's value in probing model\ncapabilities. For instance, the results show distinct performance patterns\nbased on language script, with massively multilingual models showing an\nadvantage on Perso-Arabic scripts (Urdu, Sindhi) while other models excel on\nIndic scripts. This paper provides a detailed domain-wise performance analysis,\noffering insights into domain sensitivity and cross-script transfer learning.\nBy publicly releasing CorIL, we aim to significantly improve the availability\nof high-quality training data for Indian languages and provide a valuable\nresource for the machine translation research community.",
        "url": "http://arxiv.org/abs/2509.19941v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19941v1",
        "arxiv_id": "2509.19941v1",
        "authors": [
            "Soham Bhattacharjee",
            "Mukund K Roy",
            "Yathish Poojary",
            "Bhargav Dave",
            "Mihir Raj",
            "Vandan Mujadia",
            "Baban Gain",
            "Pruthwik Mishra",
            "Arafat Ahsan",
            "Parameswari Krishnamurthy",
            "Ashwath Rao",
            "Gurpreet Singh Josan",
            "Preeti Dubey",
            "Aadil Amin Kak",
            "Anna Rao Kulkarni",
            "Narendra VG",
            "Sunita Arora",
            "Rakesh Balbantray",
            "Prasenjit Majumdar",
            "Karunesh K Arora",
            "Asif Ekbal",
            "Dipti Mishra Sharma"
        ],
        "submitted": "2025-09-24 09:48:26",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on machine translation for Indian languages, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing, particularly in areas requiring deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "Do Before You Judge: Self-Reference as a Pathway to Better LLM Evaluation",
        "abstract": "LLM-as-Judge frameworks are increasingly popular for AI evaluation, yet\nresearch findings on the relationship between models' generation and judgment\nabilities remain inconsistent. We investigate this relationship through\nsystematic dataset- and instance-level analyses across 11 models and 21 diverse\ntasks. Despite both capabilities relying on the same underlying knowledge, our\nanalyses reveal they are only weakly correlated, primarily due to LLMs'\nsensitivity to the responses being judged. To address this, we propose a\nself-reference-guided evaluation strategy that leverages a model's own answers\nas references. This approach significantly strengthens the correlation between\ngeneration and judgment abilities, offering a practical path to align these\nskills and providing a reliable proxy for model selection in evaluation tasks.",
        "url": "http://arxiv.org/abs/2509.19880v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19880v1",
        "arxiv_id": "2509.19880v1",
        "authors": [
            "Wei-Hsiang Lin",
            "Sheng-Lun Wei",
            "Hen-Hsen Huang",
            "Hsin-Hsi Chen"
        ],
        "submitted": "2025-09-24 08:32:45",
        "source": "arxiv",
        "comment": "Accepted as a long findings paper at EMNLP 2025",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of evaluating AI models. However, the focus on Large Language Models (LLMs) and their evaluation strategies is not directly aligned with your core interests in query understanding, ranking models, and user behavior modeling. While it touches on the broader topic of AI evaluation, it does not appear to be a central match for your research themes."
    },
    {
        "title": "HiCoLoRA: Addressing Context-Prompt Misalignment via Hierarchical Collaborative LoRA for Zero-Shot DST",
        "abstract": "Zero-shot Dialog State Tracking (zs-DST) is essential for enabling\nTask-Oriented Dialog Systems (TODs) to generalize to new domains without costly\ndata annotation. A central challenge lies in the semantic misalignment between\ndynamic dialog contexts and static prompts, leading to inflexible cross-layer\ncoordination, domain interference, and catastrophic forgetting. To tackle this,\nwe propose Hierarchical Collaborative Low-Rank Adaptation (HiCoLoRA), a\nframework that enhances zero-shot slot inference through robust prompt\nalignment. It features a hierarchical LoRA architecture for dynamic\nlayer-specific processing (combining lower-layer heuristic grouping and\nhigher-layer full interaction), integrates Spectral Joint Domain-Slot\nClustering to identify transferable associations (feeding an Adaptive Linear\nFusion Mechanism), and employs Semantic-Enhanced SVD Initialization\n(SemSVD-Init) to preserve pre-trained knowledge. Experiments on multi-domain\ndatasets MultiWOZ and SGD show that HiCoLoRA outperforms baselines, achieving\nSOTA in zs-DST. Code is available at https://github.com/carsonz/HiCoLoRA.",
        "url": "http://arxiv.org/abs/2509.19742v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19742v1",
        "arxiv_id": "2509.19742v1",
        "authors": [
            "Shuyu Zhang",
            "Yifan Wei",
            "Xinru Wang",
            "Yanmin Zhu",
            "Yangfan He",
            "Yixuan Weng",
            "Bin Li"
        ],
        "submitted": "2025-09-24 03:44:16",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Zero-Shot Dialog State Tracking and Task-Oriented Dialog Systems, which is not directly related to Information Retrieval or Search technologies. While it involves Natural Language Processing, the context is more aligned with conversational AI and dialog systems, making it less relevant to your primary research interests."
    },
    {
        "title": "LLMs4All: A Review on Large Language Models for Research and Applications in Academic Disciplines",
        "abstract": "Cutting-edge Artificial Intelligence (AI) techniques keep reshaping our view\nof the world. For example, Large Language Models (LLMs) based applications such\nas ChatGPT have shown the capability of generating human-like conversation on\nextensive topics. Due to the impressive performance on a variety of\nlanguage-related tasks (e.g., open-domain question answering, translation, and\ndocument summarization), one can envision the far-reaching impacts that can be\nbrought by the LLMs with broader real-world applications (e.g., customer\nservice, education and accessibility, and scientific discovery). Inspired by\ntheir success, this paper will offer an overview of state-of-the-art LLMs and\ntheir integration into a wide range of academic disciplines, including: (1)\narts, letters, and law (e.g., history, philosophy, political science, arts and\narchitecture, law), (2) economics and business (e.g., finance, economics,\naccounting, marketing), and (3) science and engineering (e.g., mathematics,\nphysics and mechanical engineering, chemistry and chemical engineering, life\nsciences and bioengineering, earth sciences and civil engineering, computer\nscience and electrical engineering). Integrating humanity and technology, in\nthis paper, we will explore how LLMs are shaping research and practice in these\nfields, while also discussing key limitations, open challenges, and future\ndirections in the era of generative AI. The review of how LLMs are engaged\nacross disciplines-along with key observations and insights-can help\nresearchers and practitioners interested in exploiting LLMs to advance their\nworks in diverse real-world applications.",
        "url": "http://arxiv.org/abs/2509.19580v2",
        "pdf_url": "http://arxiv.org/pdf/2509.19580v2",
        "arxiv_id": "2509.19580v2",
        "authors": [
            "Yanfang Fanny Ye",
            "Zheyuan Zhang",
            "Tianyi Ma",
            "Zehong Wang",
            "Yiyang Li",
            "Shifu Hou",
            "Weixiang Sun",
            "Kaiwen Shi",
            "Yijun Ma",
            "Wei Song",
            "Ahmed Abbasi",
            "Ying Cheng",
            "Jane Cleland-Huang",
            "Steven Corcelli",
            "Patricia Culligan",
            "Robert Goulding",
            "Ming Hu",
            "Ting Hua",
            "John Lalor",
            "Fang Liu",
            "Tengfei Luo",
            "Ed Maginn",
            "Nuno Moniz",
            "Jason Rohr",
            "Brett Savoie",
            "Daniel Slate",
            "Tom Stapleford",
            "Matthew Webber",
            "Olaf Wiest",
            "Johnny Zhang",
            "Nitesh Chawla"
        ],
        "submitted": "2025-09-23 21:09:24",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Large Language Models (LLMs) and their applications across various disciplines, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While LLMs are a type of NLP, the paper's scope is broader and does not specifically address query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Multilingual Hope Speech Detection: A Comparative Study of Logistic Regression, mBERT, and XLM-RoBERTa with Active Learning",
        "abstract": "Hope speech language that fosters encouragement and optimism plays a vital\nrole in promoting positive discourse online. However, its detection remains\nchallenging, especially in multilingual and low-resource settings. This paper\npresents a multilingual framework for hope speech detection using an active\nlearning approach and transformer-based models, including mBERT and\nXLM-RoBERTa. Experiments were conducted on datasets in English, Spanish,\nGerman, and Urdu, including benchmark test sets from recent shared tasks. Our\nresults show that transformer models significantly outperform traditional\nbaselines, with XLM-RoBERTa achieving the highest overall accuracy.\nFurthermore, our active learning strategy maintained strong performance even\nwith small annotated datasets. This study highlights the effectiveness of\ncombining multilingual transformers with data-efficient training strategies for\nhope speech detection.",
        "url": "http://arxiv.org/abs/2509.20315v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20315v1",
        "arxiv_id": "2509.20315v1",
        "authors": [
            "T. O. Abiola",
            "K. D. Abiodun",
            "O. E. Olumide",
            "O. O. Adebanji",
            "O. Hiram Calvo",
            "Grigori Sidorov"
        ],
        "submitted": "2025-09-24 16:54:30",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on hope speech detection using transformer-based models, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the specific application and techniques used are not aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Instruction Boundary: Quantifying Biases in LLM Reasoning under Various Coverage",
        "abstract": "Large-language-model (LLM) reasoning has long been regarded as a powerful\ntool for problem solving across domains, providing non-experts with valuable\nadvice. However, their limitations - especially those stemming from prompt\ndesign - remain underexplored. Because users may supply biased or incomplete\nprompts - often unintentionally - LLMs can be misled, undermining reliability\nand creating risks. We refer to this vulnerability as the Instruction Boundary.\nTo investigate the phenomenon, we distill it into eight concrete facets and\nintroduce BiasDetector, a framework that measures biases arising from three\ninstruction types: complete, redundant, and insufficient. We evaluate several\nmainstream LLMs and find that, despite high headline accuracy, substantial\nbiases persist in many downstream tasks as a direct consequence of prompt\ncoverage. Our empirical study confirms that LLM reasoning reliability can still\nbe significantly improved. We analyze the practical impact of these biases and\noutline mitigation strategies. Our findings underscore the need for developers\nto tackle biases and for users to craft options carefully.",
        "url": "http://arxiv.org/abs/2509.20278v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20278v1",
        "arxiv_id": "2509.20278v1",
        "authors": [
            "Zipeng Ling",
            "Yuehao Tang",
            "Chen Huang",
            "Shuliang Liu",
            "Gaoyang Jiang",
            "Shenghong Fu",
            "Junqi Yang",
            "Yao Wan",
            "Jiawan Zhang",
            "Kejia Huang",
            "Xuming Hu"
        ],
        "submitted": "2025-09-24 16:15:26",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the limitations of Large-language-model (LLM) reasoning, specifically the Instruction Boundary phenomenon, which is related to query understanding and user behavior modeling in Information Retrieval. However, the focus on LLMs and their biases in reasoning is somewhat tangential to the user's primary interests in IR, ranking models, and deep semantic understanding."
    },
    {
        "title": "Low-Resource English-Tigrinya MT: Leveraging Multilingual Models, Custom Tokenizers, and Clean Evaluation Benchmarks",
        "abstract": "Despite advances in Neural Machine Translation (NMT), low-resource languages\nlike Tigrinya remain underserved due to persistent challenges, including\nlimited corpora, inadequate tokenization strategies, and the lack of\nstandardized evaluation benchmarks. This paper investigates transfer learning\ntechniques using multilingual pretrained models to enhance translation quality\nfor morphologically rich, low-resource languages. We propose a refined approach\nthat integrates language-specific tokenization, informed embedding\ninitialization, and domain-adaptive fine-tuning. To enable rigorous assessment,\nwe construct a high-quality, human-aligned English-Tigrinya evaluation dataset\ncovering diverse domains. Experimental results demonstrate that transfer\nlearning with a custom tokenizer substantially outperforms zero-shot baselines,\nwith gains validated by BLEU, chrF, and qualitative human evaluation.\nBonferroni correction is applied to ensure statistical significance across\nconfigurations. Error analysis reveals key limitations and informs targeted\nrefinements. This study underscores the importance of linguistically aware\nmodeling and reproducible benchmarks in bridging the performance gap for\nunderrepresented languages. Resources are available at\nhttps://github.com/hailaykidu/MachineT_TigEng\n  and https://huggingface.co/Hailay/MachineT_TigEng",
        "url": "http://arxiv.org/abs/2509.20209v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20209v1",
        "arxiv_id": "2509.20209v1",
        "authors": [
            "Hailay Kidu Teklehaymanot",
            "Gebrearegawi Gidey",
            "Wolfgang Nejdl"
        ],
        "submitted": "2025-09-24 15:02:57",
        "source": "arxiv",
        "comment": "This submission is 8 pages long, includes 4 tables, and contains all\n  required conference details",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is primarily focused on machine translation for low-resource languages, which is not a core area of interest for you. While it does involve some NLP aspects, it doesn't align with your main research themes in Information Retrieval, Search technologies, and query understanding."
    },
    {
        "title": "Tokenization and Representation Biases in Multilingual Models on Dialectal NLP Tasks",
        "abstract": "Dialectal data are characterized by linguistic variation that appears small\nto humans but has a significant impact on the performance of models. This\ndialect gap has been related to various factors (e.g., data size, economic and\nsocial factors) whose impact, however, turns out to be inconsistent. In this\nwork, we investigate factors impacting the model performance more directly: we\ncorrelate Tokenization Parity (TP) and Information Parity (IP), as measures of\nrepresentational biases in pre-trained multilingual models, with the downstream\nperformance. We compare state-of-the-art decoder-only LLMs with encoder-based\nmodels across three tasks: dialect classification, topic classification, and\nextractive question answering, controlling for varying scripts (Latin vs.\nnon-Latin) and resource availability (high vs. low). Our analysis reveals that\nTP is a better predictor of the performance on tasks reliant on syntactic and\nmorphological cues (e.g., extractive QA), while IP better predicts performance\nin semantic tasks (e.g., topic classification). Complementary analyses,\nincluding tokenizer behavior, vocabulary coverage, and qualitative insights,\nreveal that the language support claims of LLMs often might mask deeper\nmismatches at the script or token level.",
        "url": "http://arxiv.org/abs/2509.20045v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20045v1",
        "arxiv_id": "2509.20045v1",
        "authors": [
            "Vani Kanjirangat",
            "Tanja Samardžić",
            "Ljiljana Dolamic",
            "Fabio Rinaldi"
        ],
        "submitted": "2025-09-24 12:13:53",
        "source": "arxiv",
        "comment": "Accepted in EMNLP-2025 Main conference",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores tokenization and representation biases in multilingual models, which is somewhat related to the user's interests in Natural Language Processing (NLP) and deep semantic understanding. However, the focus on dialectal NLP tasks and language support claims is not directly aligned with the user's primary focus on information retrieval and query understanding."
    },
    {
        "title": "DiffNator: Generating Structured Explanations of Time-Series Differences",
        "abstract": "In many IoT applications, the central interest lies not in individual sensor\nsignals but in their differences, yet interpreting such differences requires\nexpert knowledge. We propose DiffNator, a framework for structured explanations\nof differences between two time series. We first design a JSON schema that\ncaptures the essential properties of such differences. Using the Time-series\nObservations of Real-world IoT (TORI) dataset, we generate paired sequences and\ntrain a model that combine a time-series encoder with a frozen LLM to output\nJSON-formatted explanations. Experimental results show that DiffNator generates\naccurate difference explanations and substantially outperforms both a visual\nquestion answering (VQA) baseline and a retrieval method using a pre-trained\ntime-series encoder.",
        "url": "http://arxiv.org/abs/2509.20007v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20007v1",
        "arxiv_id": "2509.20007v1",
        "authors": [
            "Kota Dohi",
            "Tomoya Nishida",
            "Harsh Purohit",
            "Takashi Endo",
            "Yohei Kawaguchi"
        ],
        "submitted": "2025-09-24 11:27:07",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your core research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves time-series analysis and uses a pre-trained model, the focus on IoT applications and structured explanations of differences between time-series signals is not aligned with your primary research themes."
    },
    {
        "title": "Multimodal-enhanced Federated Recommendation: A Group-wise Fusion Approach",
        "abstract": "Federated Recommendation (FR) is a new learning paradigm to tackle the\nlearn-to-rank problem in a privacy-preservation manner. How to integrate\nmulti-modality features into federated recommendation is still an open\nchallenge in terms of efficiency, distribution heterogeneity, and fine-grained\nalignment. To address these challenges, we propose a novel multimodal fusion\nmechanism in federated recommendation settings (GFMFR). Specifically, it\noffloads multimodal representation learning to the server, which stores item\ncontent and employs a high-capacity encoder to generate expressive\nrepresentations, alleviating client-side overhead. Moreover, a group-aware item\nrepresentation fusion approach enables fine-grained knowledge sharing among\nsimilar users while retaining individual preferences. The proposed fusion loss\ncould be simply plugged into any existing federated recommender systems\nempowering their capability by adding multi-modality features. Extensive\nexperiments on five public benchmark datasets demonstrate that GFMFR\nconsistently outperforms state-of-the-art multimodal FR baselines.",
        "url": "http://arxiv.org/abs/2509.19955v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19955v1",
        "arxiv_id": "2509.19955v1",
        "authors": [
            "Chunxu Zhang",
            "Weipeng Zhang",
            "Guodong Long",
            "Zhiheng Xue",
            "Riting Xia",
            "Bo Yang"
        ],
        "submitted": "2025-09-24 10:06:37",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 3,
        "llm_reason": "The paper focuses on Federated Recommendation, which is somewhat related to Information Retrieval, but the emphasis on multimodal fusion and recommender systems limits its relevance to your core research themes. While it touches on the learn-to-rank problem, it's not a primary focus of the paper."
    },
    {
        "title": "Benchmarking Gaslighting Attacks Against Speech Large Language Models",
        "abstract": "As Speech Large Language Models (Speech LLMs) become increasingly integrated\ninto voice-based applications, ensuring their robustness against manipulative\nor adversarial input becomes critical. Although prior work has studied\nadversarial attacks in text-based LLMs and vision-language models, the unique\ncognitive and perceptual challenges of speech-based interaction remain\nunderexplored. In contrast, speech presents inherent ambiguity, continuity, and\nperceptual diversity, which make adversarial attacks more difficult to detect.\nIn this paper, we introduce gaslighting attacks, strategically crafted prompts\ndesigned to mislead, override, or distort model reasoning as a means to\nevaluate the vulnerability of Speech LLMs. Specifically, we construct five\nmanipulation strategies: Anger, Cognitive Disruption, Sarcasm, Implicit, and\nProfessional Negation, designed to test model robustness across varied tasks.\nIt is worth noting that our framework captures both performance degradation and\nbehavioral responses, including unsolicited apologies and refusals, to diagnose\ndifferent dimensions of susceptibility. Moreover, acoustic perturbation\nexperiments are conducted to assess multi-modal robustness. To quantify model\nvulnerability, comprehensive evaluation across 5 Speech and multi-modal LLMs on\nover 10,000 test samples from 5 diverse datasets reveals an average accuracy\ndrop of 24.3% under the five gaslighting attacks, indicating significant\nbehavioral vulnerability. These findings highlight the need for more resilient\nand trustworthy speech-based AI systems.",
        "url": "http://arxiv.org/abs/2509.19858v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19858v1",
        "arxiv_id": "2509.19858v1",
        "authors": [
            "Jinyang Wu",
            "Bin Zhu",
            "Xiandong Zou",
            "Qiquan Zhang",
            "Xu Fang",
            "Pan Zhou"
        ],
        "submitted": "2025-09-24 07:57:10",
        "source": "arxiv",
        "comment": "5 pages, 2 figures, 3 tables",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on speech-based Large Language Models and gaslighting attacks, which is outside the user's primary research interests in Information Retrieval and Search technologies. While it touches on AI systems, the context and methodology are not directly related to the user's core themes of query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "TianHui: A Domain-Specific Large Language Model for Diverse Traditional Chinese Medicine Scenarios",
        "abstract": "Domain-specific LLMs in TCM face limitations in research settings due to\nconstrained adaptability, insufficient evaluation datasets, and limited\ncomputational resources. This study presents TianHui, a specialized TCM LLM\nbuilt through contextual data integration and domain knowledge fusion. We\nconstructed a large-scale TCM corpus (0.97GB unsupervised data + 611,312 QA\npairs) and employed a two-stage training strategy with QLoRA, DeepSpeed Stage\n2, and Flash Attention 2. Evaluation on 12 benchmarks showed TianHui ranked\ntop-three in all metrics for six datasets (APQ, TCMCD, HFR, HCCA, DHPE, TLAW)\nand achieved top results in the other six (TCMEE, APR, GCPMI, TCMKQA, TCMRC,\nADTG). Optimal configuration was identified as LoRA rank=128, alpha=256,\nepoch=4, dropout=0.2, max length=2048. TianHui enables systematic preservation\nand scalable application of TCM knowledge. All resources are open-sourced.",
        "url": "http://arxiv.org/abs/2509.19834v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19834v1",
        "arxiv_id": "2509.19834v1",
        "authors": [
            "Ji Yin",
            "Menglan He",
            "Yujie Zhang",
            "Linshuai Zhang",
            "Tingting Ma",
            "Ce Tian",
            "Jie Wu",
            "Lin Xu",
            "Tao Jiang"
        ],
        "submitted": "2025-09-24 07:26:21",
        "source": "arxiv",
        "comment": "46 pages, 5 figures,3 tables",
        "score": 2,
        "keyword_reasons": [
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on a domain-specific large language model for Traditional Chinese Medicine, which is outside your primary areas of interest in Information Retrieval, Search technologies, and Natural Language Processing."
    },
    {
        "title": "EnAnchored-X2X: English-Anchored Optimization for Many-to-Many Translation",
        "abstract": "Large language models (LLMs) have demonstrated strong machine translation\ncapabilities for English-centric language pairs but underperform in direct\nnon-English (x2x) translation. This work addresses this limitation through a\nsynthetic data generation framework that leverages models' established\nEnglish-to-x (en2x) capabilities. By extending English parallel corpora into\nomnidirectional datasets and developing an English-referenced quality\nevaluation proxy, we enable effective collection of high-quality x2x training\ndata. Combined with preference-based optimization, our method achieves\nsignificant improvement across 72 x2x directions for widely used LLMs, while\ngeneralizing to enhance en2x performance. The results demonstrate that\nstrategic exploitation of English-centric strengths can bootstrap comprehensive\nmultilingual translation capabilities in LLMs. We release codes, datasets, and\nmodel checkpoints at https://github.com/NJUNLP/EAX",
        "url": "http://arxiv.org/abs/2509.19770v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19770v1",
        "arxiv_id": "2509.19770v1",
        "authors": [
            "Sen Yang",
            "Yu Bao",
            "Yu Lu",
            "Jiajun Chen",
            "Shujian Huang",
            "Shanbo Cheng"
        ],
        "submitted": "2025-09-24 05:41:30",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on machine translation, leveraging large language models to improve non-English translation capabilities. Although it involves NLP, it does not directly relate to information retrieval, query understanding, or ranking models, which are core areas of your research interests."
    },
    {
        "title": "Large Language Models for Pedestrian Safety: An Application to Predicting Driver Yielding Behavior at Unsignalized Intersections",
        "abstract": "Pedestrian safety is a critical component of urban mobility and is strongly\ninfluenced by the interactions between pedestrian decision-making and driver\nyielding behavior at crosswalks. Modeling driver--pedestrian interactions at\nintersections requires accurately capturing the complexity of these behaviors.\nTraditional machine learning models often struggle to capture the nuanced and\ncontext-dependent reasoning required for these multifactorial interactions, due\nto their reliance on fixed feature representations and limited\ninterpretability. In contrast, large language models (LLMs) are suited for\nextracting patterns from heterogeneous traffic data, enabling accurate modeling\nof driver-pedestrian interactions. Therefore, this paper leverages multimodal\nLLMs through a novel prompt design that incorporates domain-specific knowledge,\nstructured reasoning, and few-shot prompting, enabling interpretable and\ncontext-aware inference of driver yielding behavior, as an example application\nof modeling pedestrian--driver interaction. We benchmarked state-of-the-art\nLLMs against traditional classifiers, finding that GPT-4o consistently achieves\nthe highest accuracy and recall, while Deepseek-V3 excels in precision. These\nfindings highlight the critical trade-offs between model performance and\ncomputational efficiency, offering practical guidance for deploying LLMs in\nreal-world pedestrian safety systems.",
        "url": "http://arxiv.org/abs/2509.19657v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19657v1",
        "arxiv_id": "2509.19657v1",
        "authors": [
            "Yicheng Yang",
            "Zixian Li",
            "Jean Paul Bizimana",
            "Niaz Zafri",
            "Yongfeng Dong",
            "Tianyi Li"
        ],
        "submitted": "2025-09-24 00:25:19",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on pedestrian safety and driver yielding behavior, leveraging large language models for prediction. While it involves machine learning and NLP, the topic is not directly related to information retrieval, search technologies, or user behavior modeling, which are the core areas of your research interests."
    },
    {
        "title": "Advancing Speech Summarization in Multi-modal LLMs with Reinforcement Learning",
        "abstract": "Speech summarization is a critical component of spoken content understanding,\nparticularly in the era of rapidly growing spoken and audiovisual data. Recent\nadvances in multi-modal large language models (MLLMs), leveraging the power of\nLLMs, enable generating textual summaries directly from speech without\nintermediate transcriptions, while supporting controllable styles and zero-shot\ngeneralization. However, open-source MLLMs continue to lag behind the\nstate-of-the-art text-based LLMs, limiting their practical deployment for\nspeech summarization. In this work, we present a novel multi-stage\nreinforcement learning training framework to enhance the speech summarization\ncapabilities in MLLMs. Our model delivers substantial improvements over strong\nbaselines, outperforms much larger MLLMs, and significantly narrows the gap\nwith state-of-the-art text-based LLMs.",
        "url": "http://arxiv.org/abs/2509.19631v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19631v1",
        "arxiv_id": "2509.19631v1",
        "authors": [
            "Shaoshi Ling",
            "Gang Liu",
            "Guoli Ye",
            "Jinyu Li"
        ],
        "submitted": "2025-09-23 22:45:13",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on speech summarization using multi-modal large language models, which is not directly related to information retrieval or search technologies. While it involves natural language processing, the primary application domain is speech summarization, which is not a core area of interest for the user."
    },
    {
        "title": "Cognitive Load Limits in Large Language Models: Benchmarking Multi-Hop Reasoning",
        "abstract": "The scaling of Large Language Models (LLMs) has exposed a critical gap\nbetween their performance on static benchmarks and their fragility in dynamic,\ninformation-rich environments. While models excel at isolated tasks, the\ncomputational limits that govern their reasoning under cognitive load remain\npoorly understood. In this work, we introduce a formal theory of computational\ncognitive load, positing that extraneous, task-irrelevant information (Context\nSaturation) and interference from task-switching (Attentional Residue) are key\nmechanisms that degrade performance. We designed the Interleaved Cognitive\nEvaluation (ICE), a deconfounded benchmark to systematically manipulate these\nload factors on challenging multi-hop reasoning tasks. A comprehensive study (N\n= 10 replications per item across 200 questions) revealed significant\nperformance variations across five instruction-tuned models. Smaller\nopen-source architectures (Llama-3-8B-Instruct, Mistral-7B-Instruct-v0.2)\nexhibited baseline brittleness, achieving 0% accuracy (SEM = 0.0) across all\nconditions, including clean controls, on this high-intrinsic-load task. In\ncontrast, Gemini-2.0-Flash-001 showed partial resilience, achieving 85%\naccuracy in control conditions, with a statistically significant degradation\nunder context saturation ($\\beta = -0.003$ per % load, $p < 0.001$). These\nfindings provide preliminary evidence that cognitive load is a key contributor\nto reasoning failures, supporting theories of hallucination-as-guessing under\nuncertainty. We conclude that dynamic, cognitive-aware stress testing, as\nexemplified by the ICE benchmark, is essential for evaluating the true\nresilience and safety of advanced AI systems.",
        "url": "http://arxiv.org/abs/2509.19517v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19517v1",
        "arxiv_id": "2509.19517v1",
        "authors": [
            "Sai Teja Reddy Adapala"
        ],
        "submitted": "2025-09-23 19:36:56",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on the limitations of Large Language Models under cognitive load, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on AI systems, it's more aligned with the safety and resilience of these systems rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "EmbeddingGemma: Powerful and Lightweight Text Representations",
        "abstract": "We introduce EmbeddingGemma, a new lightweight, open text embedding model\nbased on the Gemma 3 language model family. Our innovative training recipe\nstrategically captures knowledge from larger models via encoder-decoder\ninitialization and geometric embedding distillation. We improve model\nrobustness and expressiveness with a spread-out regularizer, and ensure\ngeneralizability by merging checkpoints from varied, optimized mixtures.\nEvaluated on the Massive Text Embedding Benchmark (MTEB) across multilingual,\nEnglish, and code domains, EmbeddingGemma (300M) achieves state-of-the-art\nresults. Notably, it outperforms prior top models, both proprietary and open,\nwith fewer than 500M parameters, and provides performance comparable to models\ndouble its size, offering an exceptional performance-to-cost ratio. Remarkably,\nthis lead persists when quantizing model weights or truncating embedding\noutputs. This makes EmbeddingGemma particularly well-suited for low-latency and\nhigh-throughput use cases such as on-device applications. We provide ablation\nstudies exploring our key design choices. We release EmbeddingGemma to the\ncommunity to promote further research.",
        "url": "http://arxiv.org/abs/2509.20354v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20354v1",
        "arxiv_id": "2509.20354v1",
        "authors": [
            "Henrique Schechter Vera",
            "Sahil Dua",
            "Biao Zhang",
            "Daniel Salz",
            "Ryan Mullins",
            "Sindhu Raghuram Panyam",
            "Sara Smoot",
            "Iftekhar Naim",
            "Joe Zou",
            "Feiyang Chen",
            "Daniel Cer",
            "Alice Lisak",
            "Min Choi",
            "Lucas Gonzalez",
            "Omar Sanseviero",
            "Glenn Cameron",
            "Ian Ballantyne",
            "Kat Black",
            "Kaifeng Chen",
            "Weiyi Wang",
            "Zhe Li",
            "Gus Martins",
            "Jinhyuk Lee",
            "Mark Sherwood",
            "Juyeong Ji",
            "Renjie Wu",
            "Jingxiao Zheng",
            "Jyotinder Singh",
            "Abheesht Sharma",
            "Divya Sreepat",
            "Aashi Jain",
            "Adham Elarabawy",
            "AJ Co",
            "Andreas Doumanoglou",
            "Babak Samari",
            "Ben Hora",
            "Brian Potetz",
            "Dahun Kim",
            "Enrique Alfonseca",
            "Fedor Moiseev",
            "Feng Han",
            "Frank Palma Gomez",
            "Gustavo Hernández Ábrego",
            "Hesen Zhang",
            "Hui Hui",
            "Jay Han",
            "Karan Gill",
            "Ke Chen",
            "Koert Chen",
            "Madhuri Shanbhogue",
            "Michael Boratko",
            "Paul Suganthan",
            "Sai Meher Karthik Duddu",
            "Sandeep Mariserla",
            "Setareh Ariafar",
            "Shanfeng Zhang",
            "Shijie Zhang",
            "Simon Baumgartner",
            "Sonam Goenka",
            "Steve Qiu",
            "Tanmaya Dabral",
            "Trevor Walker",
            "Vikram Rao",
            "Waleed Khawaja",
            "Wenlei Zhou",
            "Xiaoqi Ren",
            "Ye Xia",
            "Yichang Chen",
            "Yi-Ting Chen",
            "Zhe Dong",
            "Zhongli Ding",
            "Francesco Visin",
            "Gaël Liu",
            "Jiageng Zhang",
            "Kathleen Kenealy",
            "Michelle Casbon",
            "Ravin Kumar",
            "Thomas Mesnard",
            "Zach Gleicher",
            "Cormac Brick",
            "Olivier Lacombe",
            "Adam Roberts",
            "Yunhsuan Sung",
            "Raphael Hoffmann",
            "Tris Warkentin",
            "Armand Joulin",
            "Tom Duerig",
            "Mojtaba Seyedhosseini"
        ],
        "submitted": "2025-09-24 17:56:51",
        "source": "arxiv",
        "comment": "18 pages. Models are available in HuggingFace (at\n  https://huggingface.co/collections/google/embeddinggemma-68b9ae3a72a82f0562a80dc4),\n  Kaggle (at https://www.kaggle.com/models/google/embeddinggemma/), and Vertex\n  AI (at\n  https://pantheon.corp.google.com/vertex-ai/publishers/google/model-garden/embeddinggemma)",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper introduces a new text embedding model, EmbeddingGemma, which achieves state-of-the-art results on the Massive Text Embedding Benchmark. While it has implications for information retrieval and NLP, its primary focus is on text representation and model efficiency, which is somewhat related to your research interests in query understanding and ranking models, but not a central match."
    },
    {
        "title": "DRES: Benchmarking LLMs for Disfluency Removal",
        "abstract": "Disfluencies -- such as \"um,\" \"uh,\" interjections, parentheticals, and edited\nstatements -- remain a persistent challenge for speech-driven systems,\ndegrading accuracy in command interpretation, summarization, and conversational\nagents. We introduce DRES (Disfluency Removal Evaluation Suite), a controlled\ntext-level benchmark that establishes a reproducible semantic upper bound for\nthis task. DRES builds on human-annotated Switchboard transcripts, isolating\ndisfluency removal from ASR errors and acoustic variability. We systematically\nevaluate proprietary and open-source LLMs across scales, prompting strategies,\nand architectures. Our results reveal that (i) simple segmentation consistently\nimproves performance, even for long-context models; (ii) reasoning-oriented\nmodels tend to over-delete fluent tokens; and (iii) fine-tuning achieves near\nstate-of-the-art precision and recall but harms generalization abilities. We\nfurther present a set of LLM-specific error modes and offer nine practical\nrecommendations (R1-R9) for deploying disfluency removal in speech-driven\npipelines. DRES provides a reproducible, model-agnostic foundation for\nadvancing robust spoken-language systems.",
        "url": "http://arxiv.org/abs/2509.20321v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20321v1",
        "arxiv_id": "2509.20321v1",
        "authors": [
            "Maria Teleki",
            "Sai Janjur",
            "Haoran Liu",
            "Oliver Grabner",
            "Ketan Verma",
            "Thomas Docog",
            "Xiangjue Dong",
            "Lingfeng Shi",
            "Cong Wang",
            "Stephanie Birkelbach",
            "Jason Kim",
            "Yin Zhang",
            "James Caverlee"
        ],
        "submitted": "2025-09-24 17:08:12",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on disfluency removal in speech-driven systems, which is related to query understanding and natural language processing, but it does not directly align with the user's primary interests in information retrieval, ranking models, and user behavior modeling."
    },
    {
        "title": "Z-Scores: A Metric for Linguistically Assessing Disfluency Removal",
        "abstract": "Evaluating disfluency removal in speech requires more than aggregate\ntoken-level scores. Traditional word-based metrics such as precision, recall,\nand F1 (E-Scores) capture overall performance but cannot reveal why models\nsucceed or fail. We introduce Z-Scores, a span-level linguistically-grounded\nevaluation metric that categorizes system behavior across distinct disfluency\ntypes (EDITED, INTJ, PRN). Our deterministic alignment module enables robust\nmapping between generated text and disfluent transcripts, allowing Z-Scores to\nexpose systematic weaknesses that word-level metrics obscure. By providing\ncategory-specific diagnostics, Z-Scores enable researchers to identify model\nfailure modes and design targeted interventions -- such as tailored prompts or\ndata augmentation -- yielding measurable performance improvements. A case study\nwith LLMs shows that Z-Scores uncover challenges with INTJ and PRN disfluencies\nhidden in aggregate F1, directly informing model refinement strategies.",
        "url": "http://arxiv.org/abs/2509.20319v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20319v1",
        "arxiv_id": "2509.20319v1",
        "authors": [
            "Maria Teleki",
            "Sai Janjur",
            "Haoran Liu",
            "Oliver Grabner",
            "Ketan Verma",
            "Thomas Docog",
            "Xiangjue Dong",
            "Lingfeng Shi",
            "Cong Wang",
            "Stephanie Birkelbach",
            "Jason Kim",
            "Yin Zhang",
            "James Caverlee"
        ],
        "submitted": "2025-09-24 17:02:39",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your core research themes in Information Retrieval, Search technologies, or Natural Language Processing. While it involves NLP and evaluation metrics, its focus on disfluency removal in speech and linguistically-grounded evaluation is not a central match for your interests."
    },
    {
        "title": "Multimodal Representation-disentangled Information Bottleneck for Multimodal Recommendation",
        "abstract": "Multimodal data has significantly advanced recommendation systems by\nintegrating diverse information sources to model user preferences and item\ncharacteristics. However, these systems often struggle with redundant and\nirrelevant information, which can degrade performance. Most existing methods\neither fuse multimodal information directly or use rigid architectural\nseparation for disentanglement, failing to adequately filter noise and model\nthe complex interplay between modalities. To address these challenges, we\npropose a novel framework, the Multimodal Representation-disentangled\nInformation Bottleneck (MRdIB). Concretely, we first employ a Multimodal\nInformation Bottleneck to compress the input representations, effectively\nfiltering out task-irrelevant noise while preserving rich semantic information.\nThen, we decompose the information based on its relationship with the\nrecommendation target into unique, redundant, and synergistic components. We\nachieve this decomposition with a series of constraints: a unique information\nlearning objective to preserve modality-unique signals, a redundant information\nlearning objective to minimize overlap, and a synergistic information learning\nobjective to capture emergent information. By optimizing these objectives,\nMRdIB guides a model to learn more powerful and disentangled representations.\nExtensive experiments on several competitive models and three benchmark\ndatasets demonstrate the effectiveness and versatility of our MRdIB in\nenhancing multimodal recommendation.",
        "url": "http://arxiv.org/abs/2509.20225v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20225v1",
        "arxiv_id": "2509.20225v1",
        "authors": [
            "Hui Wang",
            "Jinghui Qin",
            "Wushao Wen",
            "Qingling Li",
            "Shanshan Zhong",
            "Zhongzhan Huang"
        ],
        "submitted": "2025-09-24 15:18:32",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on multimodal recommendation systems, which is somewhat related to information retrieval, but it primarily deals with recommendation systems, not search technologies. The use of information bottleneck and disentangled representations is relevant to query understanding and ranking models, but the application is in a different domain. The paper's emphasis on real-time relevance optimization is also somewhat relevant, but it's not the primary focus of the paper."
    },
    {
        "title": "Thinking Augmented Pre-training",
        "abstract": "This paper introduces a simple and scalable approach to improve the data\nefficiency of large language model (LLM) training by augmenting existing text\ndata with thinking trajectories. The compute for pre-training LLMs has been\ngrowing at an unprecedented rate, while the availability of high-quality data\nremains limited. Consequently, maximizing the utility of available data\nconstitutes a significant research challenge. A primary impediment is that\ncertain high-quality tokens are difficult to learn given a fixed model\ncapacity, as the underlying rationale for a single token can be exceptionally\ncomplex and deep. To address this issue, we propose Thinking augmented\nPre-Training (TPT), a universal methodology that augments text with\nautomatically generated thinking trajectories. Such augmentation effectively\nincreases the volume of the training data and makes high-quality tokens more\nlearnable through step-by-step reasoning and decomposition. We apply TPT across\ndiverse training configurations up to $100$B tokens, encompassing pre-training\nwith both constrained and abundant data, as well as mid-training from strong\nopen-source checkpoints. Experimental results indicate that our method\nsubstantially improves the performance of LLMs across various model sizes and\nfamilies. Notably, TPT enhances the data efficiency of LLM pre-training by a\nfactor of $3$. For a $3$B parameter model, it improves the post-training\nperformance by over $10\\%$ on several challenging reasoning benchmarks.",
        "url": "http://arxiv.org/abs/2509.20186v2",
        "pdf_url": "http://arxiv.org/pdf/2509.20186v2",
        "arxiv_id": "2509.20186v2",
        "authors": [
            "Liang Wang",
            "Nan Yang",
            "Shaohan Huang",
            "Li Dong",
            "Furu Wei"
        ],
        "submitted": "2025-09-24 14:45:13",
        "source": "arxiv",
        "comment": "19 pages",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on improving the data efficiency of large language model training through augmenting existing text data with thinking trajectories, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves natural language processing, the primary goal is to enhance language model performance, not to address query understanding, ranking models, or recommender systems."
    },
    {
        "title": "Federation of Agents: A Semantics-Aware Communication Fabric for Large-Scale Agentic AI",
        "abstract": "We present Federation of Agents (FoA), a distributed orchestration framework\nthat transforms static multi-agent coordination into dynamic, capability-driven\ncollaboration. FoA introduces Versioned Capability Vectors (VCVs):\nmachine-readable profiles that make agent capabilities searchable through\nsemantic embeddings, enabling agents to advertise their capabilities, cost, and\nlimitations. Our aarchitecturecombines three key innovations: (1) semantic\nrouting that matches tasks to agents over sharded HNSW indices while enforcing\noperational constraints through cost-biased optimization, (2) dynamic task\ndecomposition where compatible agents collaboratively break down complex tasks\ninto DAGs of subtasks through consensus-based merging, and (3) smart clustering\nthat groups agents working on similar subtasks into collaborative channels for\nk-round refinement before synthesis. Built on top of MQTT,s publish-subscribe\nsemantics for scalable message passing, FoA achieves sub-linear complexity\nthrough hierarchical capability matching and efficient index maintenance.\nEvaluation on HealthBench shows 13x improvements over single-model baselines,\nwith clustering-enhanced laboration particularly effective for complex\nreasoning tasks requiring multiple perspectives. The system scales horizontally\nwhile maintaining consistent performance, demonstrating that semantic\norchestration with structured collaboration can unlock the collective\nintelligence of heterogeneous federations of AI agents.",
        "url": "http://arxiv.org/abs/2509.20175v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20175v1",
        "arxiv_id": "2509.20175v1",
        "authors": [
            "Lorenzo Giusti",
            "Ole Anton Werner",
            "Riccardo Taiello",
            "Matilde Carvalho Costa",
            "Emre Tosun",
            "Andrea Protani",
            "Marc Molina",
            "Rodrigo Lopes de Almeida",
            "Paolo Cacace",
            "Diogo Reis Santos",
            "Luigi Serio"
        ],
        "submitted": "2025-09-24 14:38:06",
        "source": "arxiv",
        "comment": "18 pages, 4 figures",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be primarily focused on distributed orchestration and multi-agent coordination, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it mentions semantic embeddings and routing, the context is not applicable to the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Cascade! Human in the loop shortcomings can increase the risk of failures in recommender systems",
        "abstract": "Recommender systems are among the most commonly deployed systems today.\nSystems design approaches to AI-powered recommender systems have done well to\nurge recommender system developers to follow more intentional data collection,\ncuration, and management procedures. So too has the \"human-in-the-loop\"\nparadigm been widely adopted, primarily to address the issue of accountability.\nHowever, in this paper, we take the position that human oversight in\nrecommender system design also entails novel risks that have yet to be fully\ndescribed. These risks are \"codetermined\" by the information context in which\nsuch systems are often deployed. Furthermore, new knowledge of the shortcomings\nof \"human-in-the-loop\" practices to deliver meaningful oversight of other AI\nsystems suggest that they may also be inadequate for achieving socially\nresponsible recommendations. We review how the limitations of human oversight\nmay increase the chances of a specific kind of failure: a \"cascade\" or\n\"compound\" failure. We then briefly explore how the unique dynamics of three\ncommon deployment contexts can make humans in the loop more likely to fail in\ntheir oversight duties. We then conclude with two recommendations.",
        "url": "http://arxiv.org/abs/2509.20099v2",
        "pdf_url": "http://arxiv.org/pdf/2509.20099v2",
        "arxiv_id": "2509.20099v2",
        "authors": [
            "Wm. Matthew Kennedy",
            "Nishanshi Shukla",
            "Cigdem Patlak",
            "Blake Chambers",
            "Theodora Skeadas",
            "Tuesday",
            "Kingsley Owadara",
            "Aayush Dhanotiya"
        ],
        "submitted": "2025-09-24 13:23:03",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on recommender systems and the risks associated with human oversight, which is somewhat related to information retrieval and search technologies. However, the primary focus on recommender systems and human-in-the-loop shortcomings does not align with the user's core research themes in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "OLaPh: Optimal Language Phonemizer",
        "abstract": "Phonemization, the conversion of text into phonemes, is a key step in\ntext-to-speech. Traditional approaches use rule-based transformations and\nlexicon lookups, while more advanced methods apply preprocessing techniques or\nneural networks for improved accuracy on out-of-domain vocabulary. However, all\nsystems struggle with names, loanwords, abbreviations, and homographs. This\nwork presents OLaPh (Optimal Language Phonemizer), a framework that combines\nlarge lexica, multiple NLP techniques, and compound resolution with a\nprobabilistic scoring function. Evaluations in German and English show improved\naccuracy over previous approaches, including on a challenging dataset. To\nfurther address unresolved cases, we train a large language model on\nOLaPh-generated data, which achieves even stronger generalization and\nperformance. Together, the framework and LLM improve phonemization consistency\nand provide a freely available resource for future research.",
        "url": "http://arxiv.org/abs/2509.20086v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20086v1",
        "arxiv_id": "2509.20086v1",
        "authors": [
            "Johannes Wirth"
        ],
        "submitted": "2025-09-24 13:05:09",
        "source": "arxiv",
        "comment": "5 pages, 1 figure, 3 tables",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on phonemization for text-to-speech applications, which is outside the scope of Information Retrieval and Search technologies. While it involves Natural Language Processing, the specific techniques and goals are not aligned with the user's primary research interests."
    },
    {
        "title": "Responsible AI Technical Report",
        "abstract": "KT developed a Responsible AI (RAI) assessment methodology and risk\nmitigation technologies to ensure the safety and reliability of AI services. By\nanalyzing the Basic Act on AI implementation and global AI governance trends,\nwe established a unique approach for regulatory compliance and systematically\nidentify and manage all potential risk factors from AI development to\noperation. We present a reliable assessment methodology that systematically\nverifies model safety and robustness based on KT's AI risk taxonomy tailored to\nthe domestic environment. We also provide practical tools for managing and\nmitigating identified AI risks. With the release of this report, we also\nrelease proprietary Guardrail : SafetyGuard that blocks harmful responses from\nAI models in real-time, supporting the enhancement of safety in the domestic AI\ndevelopment ecosystem. We also believe these research outcomes provide valuable\ninsights for organizations seeking to develop Responsible AI.",
        "url": "http://arxiv.org/abs/2509.20057v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20057v1",
        "arxiv_id": "2509.20057v1",
        "authors": [
            "KT",
            ":",
            "Soonmin Bae",
            "Wanjin Park",
            "Jeongyeop Kim",
            "Yunjin Park",
            "Jungwon Yoon",
            "Junhyung Moon",
            "Myunggyo Oh",
            "Wonhyuk Lee",
            "Junseo Jang",
            "Dongyoung Jung",
            "Minwook Ju",
            "Eunmi Kim",
            "Sujin Kim",
            "Youngchol Kim",
            "Somin Lee",
            "Wonyoung Lee",
            "Minsung Noh",
            "Hyoungjun Park",
            "Eunyoung Shin"
        ],
        "submitted": "2025-09-24 12:26:33",
        "source": "arxiv",
        "comment": "23 pages, 8 figures",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Responsible AI, regulatory compliance, and risk management, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it touches on AI development, the context is more regulatory and governance-oriented, rather than deep semantic understanding or real-time relevance optimization."
    },
    {
        "title": "Embodied AI: From LLMs to World Models",
        "abstract": "Embodied Artificial Intelligence (AI) is an intelligent system paradigm for\nachieving Artificial General Intelligence (AGI), serving as the cornerstone for\nvarious applications and driving the evolution from cyberspace to physical\nsystems. Recent breakthroughs in Large Language Models (LLMs) and World Models\n(WMs) have drawn significant attention for embodied AI. On the one hand, LLMs\nempower embodied AI via semantic reasoning and task decomposition, bringing\nhigh-level natural language instructions and low-level natural language actions\ninto embodied cognition. On the other hand, WMs empower embodied AI by building\ninternal representations and future predictions of the external world,\nfacilitating physical law-compliant embodied interactions. As such, this paper\ncomprehensively explores the literature in embodied AI from basics to advances,\ncovering both LLM driven and WM driven works. In particular, we first present\nthe history, key technologies, key components, and hardware systems of embodied\nAI, as well as discuss its development via looking from unimodal to multimodal\nangle. We then scrutinize the two burgeoning fields of embodied AI, i.e.,\nembodied AI with LLMs/multimodal LLMs (MLLMs) and embodied AI with WMs,\nmeticulously delineating their indispensable roles in end-to-end embodied\ncognition and physical laws-driven embodied interactions. Building upon the\nabove advances, we further share our insights on the necessity of the joint\nMLLM-WM driven embodied AI architecture, shedding light on its profound\nsignificance in enabling complex tasks within physical worlds. In addition, we\nexamine representative applications of embodied AI, demonstrating its wide\napplicability in real-world scenarios. Last but not least, we point out future\nresearch directions of embodied AI that deserve further investigation.",
        "url": "http://arxiv.org/abs/2509.20021v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20021v1",
        "arxiv_id": "2509.20021v1",
        "authors": [
            "Tongtong Feng",
            "Xin Wang",
            "Yu-Gang Jiang",
            "Wenwu Zhu"
        ],
        "submitted": "2025-09-24 11:37:48",
        "source": "arxiv",
        "comment": "Accepted by IEEE CASM",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Embodied AI, a field that is not directly related to Information Retrieval or Search technologies. While it touches on Large Language Models, which are relevant to NLP, the primary focus is on Artificial General Intelligence and physical systems, making it off-topic for your research interests."
    },
    {
        "title": "The Knowledge-Behaviour Disconnect in LLM-based Chatbots",
        "abstract": "Large language model-based artificial conversational agents (like ChatGPT)\ngive answers to all kinds of questions, and often enough these answers are\ncorrect. Just on the basis of that capacity alone, we may attribute knowledge\nto them. But do these models use this knowledge as a basis for their own\nconversational behaviour? I argue this is not the case, and I will refer to\nthis failure as a `disconnect'. I further argue this disconnect is fundamental\nin the sense that with more data and more training of the LLM on which a\nconversational chatbot is based, it will not disappear. The reason is, as I\nwill claim, that the core technique used to train LLMs does not allow for the\nestablishment of the connection we are after. The disconnect reflects a\nfundamental limitation on the capacities of LLMs, and explains the source of\nhallucinations. I will furthermore consider the ethical version of the\ndisconnect (ethical conversational knowledge not being aligned with ethical\nconversational behaviour), since in this domain researchers have come up with\nseveral additional techniques to influence a chatbot's behaviour. I will\ndiscuss how these techniques do nothing to solve the disconnect and can make it\nworse.",
        "url": "http://arxiv.org/abs/2509.20004v1",
        "pdf_url": "http://arxiv.org/pdf/2509.20004v1",
        "arxiv_id": "2509.20004v1",
        "authors": [
            "Jan Broersen"
        ],
        "submitted": "2025-09-24 11:24:49",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on the limitations of Large Language Models (LLMs) in conversational chatbots, specifically the disconnect between knowledge and behavior. While it touches on the topic of understanding user behavior, it does not directly relate to information retrieval, search technologies, or query understanding, which are core areas of your research interests."
    },
    {
        "title": "SwissGPC v1.0 -- The Swiss German Podcasts Corpus",
        "abstract": "We present SwissGPC v1.0, the first mid-to-large-scale corpus of spontaneous\nSwiss German speech, developed to support research in ASR, TTS, dialect\nidentification, and related fields. The dataset consists of links to talk shows\nand podcasts hosted on Schweizer Radio und Fernsehen and YouTube, which contain\napproximately 5400 hours of raw audio. After segmentation and weak annotation,\nnearly 5000 hours of speech were retained, covering the seven major Swiss\nGerman dialect regions alongside Standard German. We describe the corpus\nconstruction methodology, including an automated annotation pipeline, and\nprovide statistics on dialect distribution, token counts, and segmentation\ncharacteristics. Unlike existing Swiss German speech corpora, which primarily\nfeature controlled speech, this corpus captures natural, spontaneous\nconversations, making it a valuable resource for real-world speech\napplications.",
        "url": "http://arxiv.org/abs/2509.19866v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19866v1",
        "arxiv_id": "2509.19866v1",
        "authors": [
            "Samuel Stucki",
            "Mark Cieliebak",
            "Jan Deriu"
        ],
        "submitted": "2025-09-24 08:13:44",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be primarily focused on speech processing and corpus development for ASR and TTS applications, which is somewhat related to your interests in NLP and data mining. However, it does not directly align with your core research themes in Information Retrieval, query understanding, and ranking models."
    },
    {
        "title": "SINAI at eRisk@CLEF 2025: Transformer-Based and Conversational Strategies for Depression Detection",
        "abstract": "This paper describes the participation of the SINAI-UJA team in the\neRisk@CLEF 2025 lab. Specifically, we addressed two of the proposed tasks: (i)\nTask 2: Contextualized Early Detection of Depression, and (ii) Pilot Task:\nConversational Depression Detection via LLMs. Our approach for Task 2 combines\nan extensive preprocessing pipeline with the use of several transformer-based\nmodels, such as RoBERTa Base or MentalRoBERTA Large, to capture the contextual\nand sequential nature of multi-user conversations. For the Pilot Task, we\ndesigned a set of conversational strategies to interact with LLM-powered\npersonas, focusing on maximizing information gain within a limited number of\ndialogue turns. In Task 2, our system ranked 8th out of 12 participating teams\nbased on F1 score. However, a deeper analysis revealed that our models were\namong the fastest in issuing early predictions, which is a critical factor in\nreal-world deployment scenarios. This highlights the trade-off between early\ndetection and classification accuracy, suggesting potential avenues for\noptimizing both jointly in future work. In the Pilot Task, we achieved 1st\nplace out of 5 teams, obtaining the best overall performance across all\nevaluation metrics: DCHR, ADODL and ASHR. Our success in this task demonstrates\nthe effectiveness of structured conversational design when combined with\npowerful language models, reinforcing the feasibility of deploying LLMs in\nsensitive mental health assessment contexts.",
        "url": "http://arxiv.org/abs/2509.19861v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19861v1",
        "arxiv_id": "2509.19861v1",
        "authors": [
            "Alba Maria Marmol-Romero",
            "Manuel Garcia-Vega",
            "Miguel Angel Garcia-Cumbreras",
            "Arturo Montejo-Raez"
        ],
        "submitted": "2025-09-24 08:04:32",
        "source": "arxiv",
        "comment": "16 pages, 10 figures, 8 tables. CLEF (Working Notes). 2025",
        "score": 1,
        "keyword_reasons": [
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and deep semantic understanding, but it primarily focuses on depression detection and conversational strategies, which is not directly aligned with the user's core research themes in Information Retrieval and Search technologies."
    },
    {
        "title": "CHURRO: Making History Readable with an Open-Weight Large Vision-Language Model for High-Accuracy, Low-Cost Historical Text Recognition",
        "abstract": "Accurate text recognition for historical documents can greatly advance the\nstudy and preservation of cultural heritage. Existing vision-language models\n(VLMs), however, are designed for modern, standardized texts and are not\nequipped to read the diverse languages and scripts, irregular layouts, and\nfrequent degradation found in historical materials.\n  This paper presents CHURRO, a 3B-parameter open-weight VLM specialized for\nhistorical text recognition. The model is trained on CHURRO-DS, the largest\nhistorical text recognition dataset to date. CHURRO-DS unifies 155 historical\ncorpora comprising 99,491 pages, spanning 22 centuries of textual heritage\nacross 46 language clusters, including historical variants and dead languages.\n  We evaluate several open-weight and closed VLMs and optical character\nrecognition (OCR) systems on CHURRO-DS and find that CHURRO outperforms all\nother VLMs. On the CHURRO-DS test set, CHURRO achieves 82.3% (printed) and\n70.1% (handwritten) normalized Levenshtein similarity, surpassing the\nsecond-best model, Gemini 2.5 Pro, by 1.4% and 6.5%, respectively, while being\n15.5 times more cost-effective.\n  By releasing the model and dataset, we aim to enable community-driven\nresearch to improve the readability of historical texts and accelerate\nscholarship.",
        "url": "http://arxiv.org/abs/2509.19768v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19768v1",
        "arxiv_id": "2509.19768v1",
        "authors": [
            "Sina J. Semnani",
            "Han Zhang",
            "Xinyan He",
            "Merve Tekgürler",
            "Monica S. Lam"
        ],
        "submitted": "2025-09-24 05:38:45",
        "source": "arxiv",
        "comment": "EMNLP 2025",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on historical text recognition using a vision-language model, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing."
    },
    {
        "title": "UserRL: Training Interactive User-Centric Agent via Reinforcement Learning",
        "abstract": "Reinforcement learning (RL) has shown promise in training agentic models that\nmove beyond static benchmarks to engage in dynamic, multi-turn interactions.\nYet, the ultimate value of such agents lies in their ability to assist users, a\nsetting where diversity and dynamics of user interaction pose challenges. In\nthis work, we propose UserRL, a unified framework for training and evaluating\nuser-centric abilities through standardized gym environments paired with\nsimulated users. We systematically vary turn-level reward assignment and\ntrajectory-level score calculation to analyze how different formulations affect\nlearning under the GRPO algorithm. Our experiments across Qwen3 models reveal\nthree key findings: (i) SFT cold start is critical for unlocking initial\ninteraction ability and enabling sustained RL improvements; (ii) deliberate\ntrajectory scoring yields more efficient and effective multi-turn interactions;\nand (iii) while stronger simulated users (e.g., GPT-4o) facilitates training,\nopen-source simulators (e.g., Qwen3-32B) remain a cost-effective and\ntransferable option. Together, these results highlight that careful design of\nreward shaping and user simulation choice is as crucial as model scale, and\nestablish UserRL as a practical pathway for developing robust user-centric\nagentic models. All codes and data are public for future research.",
        "url": "http://arxiv.org/abs/2509.19736v1",
        "pdf_url": "http://arxiv.org/pdf/2509.19736v1",
        "arxiv_id": "2509.19736v1",
        "authors": [
            "Cheng Qian",
            "Zuxin Liu",
            "Akshara Prabhakar",
            "Jielin Qiu",
            "Zhiwei Liu",
            "Haolin Chen",
            "Shirley Kokane",
            "Heng Ji",
            "Weiran Yao",
            "Shelby Heinecke",
            "Silvio Savarese",
            "Caiming Xiong",
            "Huan Wang"
        ],
        "submitted": "2025-09-24 03:33:20",
        "source": "arxiv",
        "comment": "28 Pages, 15 Figures, 6 Tables; Built upon latest UserBench release:\n  arXiv:2507.22034",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper explores the application of reinforcement learning in developing user-centric agentic models, which is somewhat related to query understanding and user behavior modeling in Information Retrieval. However, the focus on interactive dialogue and simulated users is not directly aligned with the user's primary research interests in IR and NLP. The paper's emphasis on model scale and reward shaping is also relevant to the broader field of AI, but not a central match for the user's research themes."
    }
]