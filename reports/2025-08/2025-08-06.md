# Daily Papers Report - 2025-08-06

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. PyLate: Flexible Training and Retrieval for Late Interaction Models

- **LLM Score**: 8
- **Keyword Score**: 10
- **Authors**: Antoine Chaffin, Rapha√´l Sourty
- **URL**: <http://arxiv.org/abs/2508.03555v1>
- **Submitted**: 2025-08-05 15:23:40
- **Comment**: 5 pages
- **Topic Keywords**: information retrieval, ranking, retrieval, rank, search
- **Reason**: The paper is highly relevant to information retrieval, specifically neural ranking and multi-vector approaches, which align with your research interests. The introduction of PyLate, a library for training and experimenting with late interaction models, is also of interest, given your focus on query understanding and ranking models.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: PyLate: A Library for Multi-Vector Architectures in Neural Ranking
- **Aim**: To accelerate research and real-world application of late interaction models in modern information retrieval systems
- **Rationale**: Single-vector search paradigm leads to performance degradation in out-of-domain, long-context, and reasoning-intensive retrieval tasks, and multi-vector approaches demonstrate superior empirical advantages
- **Ground**: PyLate builds upon Sentence Transformers and provides efficient training, advanced logging, and automated model card generation, requiring minimal code changes
- **Experiment**: PyLate is evaluated by training state-of-the-art models on various benchmarks, including BEIR, FiQA2018, and others, and demonstrates superior performance compared to existing ColBERT models
- **Takeaway**: PyLate is a modular, readable, and extensible framework that facilitates the development and deployment of late interaction models, providing a user experience similar to Sentence-Transformers

#### Abstract
> Neural ranking has become a cornerstone of modern information retrieval.
While single vector search remains the dominant paradigm, it suffers from the
shortcoming of compressing all the information into a single vector. This
compression leads to notable performance degradation in out-of-domain,
long-context, and reasoning-intensive retrieval tasks. Multi-vector approaches
pioneered by ColBERT aim to address these limitations by preserving individual
token embeddings and computing similarity via the MaxSim operator. This
architecture has demonstrated superior empirical advantages, including enhanced
out-of-domain generalization, long-context handling, and performance in complex
retrieval scenarios. Despite these compelling empirical results and clear
theoretical advantages, the practical adoption and public availability of late
interaction models remain low compared to their single-vector counterparts,
primarily due to a lack of accessible and modular tools for training and
experimenting with such models. To bridge this gap, we introduce PyLate, a
streamlined library built on top of Sentence Transformers to support
multi-vector architectures natively, inheriting its efficient training,
advanced logging, and automated model card generation while requiring minimal
code changes to code templates users are already familiar with. By offering
multi-vector-specific features such as efficient indexes, PyLate aims to
accelerate research and real-world application of late interaction models,
thereby unlocking their full potential in modern IR systems. Finally, PyLate
has already enabled the development of state-of-the-art models, including
GTE-ModernColBERT and Reason-ModernColBERT, demonstrating its practical utility
for both research and production environments.

---

### 2. CTR-Sink: Attention Sink for Language Models in Click-Through Rate Prediction

- **LLM Score**: 7
- **Keyword Score**: 11
- **Authors**: Zixuan Li, Binzong Geng, Jing Xiong, Yong He, Yuxuan Hu, Jian Chen, Dingwei Chen, Xiyu Chang, Liang Zhang, Linjian Mo, Chengming Li, Chuan Yuan, Zhenan Sun
- **URL**: <http://arxiv.org/abs/2508.03668v1>
- **Submitted**: 2025-08-05 17:30:34
- **Topic Keywords**: rag, user behavior, click, ctr, click-through rate, recommend
- **Reason**: The paper focuses on Click-Through Rate prediction, a topic related to information retrieval, and leverages language models for this task. While it doesn't directly address query understanding or ranking models, it explores the application of language models in a recommendation system, which is somewhat related to the user's interests in search technologies and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Addressing semantic fragmentation in click-through rate (CTR) prediction using language models (LMs)
- **Aim**: Propose a novel framework, CTR-Sink, to improve CTR prediction by incorporating recommendation-specific signals and attention sinks
- **Rationale**: Semantic fragmentation occurs when LMs struggle to focus on meaningful behavior boundaries and inter-behavior relationships due to the mismatch between user behavior sequences and natural language
- **Ground**: CTR-Sink model inserts recommendation-signal-fused [SINK] tokens between user behaviors to anchor language model attention at behavioral boundaries, aggregating inter-behavior dependencies and preserving pre-trained language model capabilities
- **Experiment**: Experiments on one industrial dataset and two open-source datasets demonstrate significant improvements in AUC compared to baselines, and visualization verifies the role of CTR-Sink as an attention sink
- **Takeaway**: CTR-Sink framework with two-stage training strategy and attention sink mechanism improves CTR prediction by addressing semantic fragmentation, and demonstrates generalizability across diverse datasets and model architectures

#### Abstract
> Click-Through Rate (CTR) prediction, a core task in recommendation systems,
estimates user click likelihood using historical behavioral data. Modeling user
behavior sequences as text to leverage Language Models (LMs) for this task has
gained traction, owing to LMs' strong semantic understanding and contextual
modeling capabilities. However, a critical structural gap exists: user behavior
sequences consist of discrete actions connected by semantically empty
separators, differing fundamentally from the coherent natural language in LM
pre-training. This mismatch causes semantic fragmentation, where LM attention
scatters across irrelevant tokens instead of focusing on meaningful behavior
boundaries and inter-behavior relationships, degrading prediction performance.
To address this, we propose $\textit{CTR-Sink}$, a novel framework introducing
behavior-level attention sinks tailored for recommendation scenarios. Inspired
by attention sink theory, it constructs attention focus sinks and dynamically
regulates attention aggregation via external information. Specifically, we
insert sink tokens between consecutive behaviors, incorporating
recommendation-specific signals such as temporal distance to serve as stable
attention sinks. To enhance generality, we design a two-stage training strategy
that explicitly guides LM attention toward sink tokens and a attention sink
mechanism that amplifies inter-sink dependencies to better capture behavioral
correlations. Experiments on one industrial dataset and two open-source
datasets (MovieLens, Kuairec), alongside visualization results, validate the
method's effectiveness across scenarios.

---

### 3. Are We on the Right Way for Assessing Document Retrieval-Augmented Generation?

- **LLM Score**: 6
- **Keyword Score**: 8
- **Authors**: Wenxuan Shen, Mingjia Wang, Yaochen Wang, Dongping Chen, Junjie Yang, Yao Wan, Weiwei Lin
- **URL**: <http://arxiv.org/abs/2508.03644v1>
- **Submitted**: 2025-08-05 16:55:02
- **Comment**: In submission. Project website: https://double-bench.github.io/
- **Topic Keywords**: queries, rag, retrieval, search
- **Reason**: The paper discusses Retrieval-Augmented Generation (RAG) systems, which is related to information retrieval and query understanding. However, the focus is on evaluation methods rather than ranking models or user behavior modeling, which are core aspects of your research interests. The paper's relevance is somewhat diminished by its focus on a specific application domain (document RAG) and the lack of direct connection to your primary research areas.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Retrieval-Augmented Generation (RAG) systems evaluation
- **Aim**: Introduce DOUBLE-BENCH, a large-scale, multilingual, and multimodal evaluation system for RAG systems
- **Rationale**: Address limitations of current benchmarks, including focus on specific parts of RAG systems, use of synthetic data, and failure to reflect real-world bottlenecks
- **Ground**: Identify four major overlooked issues in document RAG evaluation, including lack of consideration for users without prior knowledge, incomplete evaluation scenarios, and inability to handle multi-hop queries
- **Experiment**: Conduct extensive experiments across various state-of-the-art textual, visual, and multimodal embedding models, Multimodal Large Language Models (MLLMs), and document RAG frameworks
- **Takeaway**: DOUBLE-BENCH provides a comprehensive evaluation of document RAG systems, highlighting the importance of optimizing the retrieval stage and identifying informational gaps, and provides guidelines and prompts for generating high-quality question-answer pairs in document-based RAG systems

#### Abstract
> Retrieval-Augmented Generation (RAG) systems using Multimodal Large Language
Models (MLLMs) show great promise for complex document understanding, yet their
development is critically hampered by inadequate evaluation. Current benchmarks
often focus on specific part of document RAG system and use synthetic data with
incomplete ground truth and evidence labels, therefore failing to reflect
real-world bottlenecks and challenges. To overcome these limitations, we
introduce Double-Bench: a new large-scale, multilingual, and multimodal
evaluation system that is able to produce fine-grained assessment to each
component within document RAG systems. It comprises 3,276 documents (72,880
pages) and 5,168 single- and multi-hop queries across 6 languages and 4
document types with streamlined dynamic update support for potential data
contamination issues. Queries are grounded in exhaustively scanned evidence
pages and verified by human experts to ensure maximum quality and completeness.
Our comprehensive experiments across 9 state-of-the-art embedding models, 4
MLLMs and 4 end-to-end document RAG frameworks demonstrate the gap between text
and visual embedding models is narrowing, highlighting the need in building
stronger document retrieval models. Our findings also reveal the
over-confidence dilemma within current document RAG frameworks that tend to
provide answer even without evidence support. We hope our fully open-source
Double-Bench provide a rigorous foundation for future research in advanced
document RAG systems. We plan to retrieve timely corpus and release new
benchmarks on an annual basis.

---

### 4. fact check AI at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-checked Claim Retrieval

- **LLM Score**: 6
- **Keyword Score**: 3
- **Authors**: Pranshu Rastogi
- **URL**: <http://arxiv.org/abs/2508.03475v1>
- **Submitted**: 2025-08-05 14:10:09
- **Comment**: 7 pages, 6 tables. Code available at
  https://github.com/pranshurastogi29/SemEval-2025-ACL-Multi-and-Crosslingual-Retrieval-using-Bi-encoders
- **Topic Keywords**: retrieval, rank
- **Reason**: The paper is somewhat related to my research interests in Information Retrieval, particularly in the area of Learning to Rank. The use of a bi-encoder model and fine-tuning from a pre-trained transformer is relevant to my focus on ranking models. However, the specific application of fact-checked claim retrieval and the multilingual/crosslingual aspect is not directly aligned with my primary interests in query understanding, user behavior modeling, and real-time relevance optimization.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Multilingual and Crosslingual Fact-Checked Claim Retrieval
- **Aim**: Automate the retrieval of previously verified claims across languages, addressing the challenge of online disinformation
- **Rationale**: Design a bi-encoder model fine-tuned from a pre-trained transformer optimized for sentence similarity to retrieve fact-checked claims
- **Ground**: Trained on both source languages and their English translations for multilingual retrieval and only English translations for cross-lingual retrieval
- **Experiment**: Experimental setup involves preprocessing steps, training using a full dataset for each language, and fine-tuning with gradient checkpointing disabled
- **Takeaway**: The approach achieves 92% Success@10 in multilingual and 80% Success@10 in cross-lingual tracks, demonstrating the effectiveness of the model even with constrained computational resources

#### Abstract
> SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim
Retrieval is approached as a Learning-to-Rank task using a bi-encoder model
fine-tuned from a pre-trained transformer optimized for sentence similarity.
Training used both the source languages and their English translations for
multilingual retrieval and only English translations for cross-lingual
retrieval. Using lightweight models with fewer than 500M parameters and
training on Kaggle T4 GPUs, the method achieved 92% Success@10 in multilingual
and 80% Success@10 in 5th in crosslingual and 10th in multilingual tracks.

---

### 5. LLMDistill4Ads: Using Cross-Encoders to Distill from LLM Signals for Advertiser Keyphrase Recommendations at eBay

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Soumik Dey, Benjamin Braun, Naveen Ravipati, Hansi Wu, Binbin Li
- **URL**: <http://arxiv.org/abs/2508.03628v1>
- **Submitted**: 2025-08-05 16:47:17
- **Topic Keywords**: relevance, click, retrieval, recommend, search
- **Reason**: The paper focuses on advertiser keyphrase recommendations at eBay, which is related to search technologies and query understanding. However, the specific application and problem domain are quite different from the user's primary interests in information retrieval, ranking models, and user behavior modeling. The use of LLMs and knowledge distillation is also not directly relevant to the user's research areas.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Debiasing Embedding-Based Retrieval models for advertiser keyphrase recommendations
- **Aim**: Align keyphrase recommendations with seller and search judgments regarding auctions at eBay
- **Rationale**: Addressing biases in e-commerce datasets, such as MNAR conditions and auction mechanisms, by using alternative labeling strategies and LLMs to generate relevance labels
- **Ground**: Using a multi-task learning framework with Teacher-Assistant framework, incorporating click-data, Search Relevance metrics, and relevance scores from LLMs
- **Experiment**: Comparing performance of different base models and knowledge distillation losses, and evaluating the multi-task framework in an A/B test
- **Takeaway**: The proposed approach improves the accuracy and relevance of keyphrase recommendations, resulting in statistically significant improvements in GMB and ROAS

#### Abstract
> Sellers at eBay are recommended keyphrases to bid on to enhance the
performance of their advertising campaigns. The relevance of these keyphrases
is crucial in avoiding the overcrowding of search systems with irrelevant items
and maintaining a positive seller perception. It is essential that keyphrase
recommendations align with both seller and Search judgments regarding auctions.
Due to the difficulty in procuring negative human judgment at scale, employing
LLM-as-a-judge to mimic seller judgment has been established as the norm in
several studies. This study introduces a novel two-step LLM distillation
process from a LLM-judge used to debias our Embedding Based Retrieval (EBR)
model from the various biases that exist in click-data. We distill from an LLM
teacher via a cross-encoder assistant into a bi-encoder student using a
multi-task training approach, ultimately employing the student bi-encoder to
retrieve relevant advertiser keyphrases. We show that integrating a knowledge
distillation process from LLMs in a multi-task training setup enhances
bi-encoder performance in retrieving relevant advertiser keyphrases at eBay.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. MultiRAG: A Knowledge-guided Framework for Mitigating Hallucination in Multi-source Retrieval Augmented Generation

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Wenlong Wu, Haofen Wang, Bohan Li, Peixuan Huang, Xinzhe Zhao, Lei Liang
- **URL**: <http://arxiv.org/abs/2508.03553v1>
- **Submitted**: 2025-08-05 15:20:52
- **Comment**: Accepted by ICDE 2025 Research Paper
- **Topic Keywords**: query, rag, retrieval augmented generation, retrieval
- **Reason**: The paper focuses on mitigating hallucination in multi-source retrieval-augmented generation, which is a specific problem in Natural Language Processing (NLP). While it touches on retrieval and generation, the primary focus is on knowledge construction and retrieval, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval (IR). The paper's relevance to IR is limited, but it does explore some NLP-related topics.

#### Abstract
> Retrieval Augmented Generation (RAG) has emerged as a promising solution to
address hallucination issues in Large Language Models (LLMs). However, the
integration of multiple retrieval sources, while potentially more informative,
introduces new challenges that can paradoxically exacerbate hallucination
problems. These challenges manifest primarily in two aspects: the sparse
distribution of multi-source data that hinders the capture of logical
relationships and the inherent inconsistencies among different sources that
lead to information conflicts. To address these challenges, we propose
MultiRAG, a novel framework designed to mitigate hallucination in multi-source
retrieval-augmented generation through knowledge-guided approaches. Our
framework introduces two key innovations: (1) a knowledge construction module
that employs multi-source line graphs to efficiently aggregate logical
relationships across different knowledge sources, effectively addressing the
sparse data distribution issue; and (2) a sophisticated retrieval module that
implements a multi-level confidence calculation mechanism, performing both
graph-level and node-level assessments to identify and eliminate unreliable
information nodes, thereby reducing hallucinations caused by inter-source
inconsistencies. Extensive experiments on four multi-domain query datasets and
two multi-hop QA datasets demonstrate that MultiRAG significantly enhances the
reliability and efficiency of knowledge retrieval in complex multi-source
scenarios. \textcolor{blue}{Our code is available in
https://github.com/wuwenlong123/MultiRAG.

### 7. Reliable Evaluation Protocol for Low-Precision Retrieval

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Kisu Yang, Yoonna Jang, Hwanseok Jang, Kenneth Choi, Isabelle Augenstein, Heuiseok Lim
- **URL**: <http://arxiv.org/abs/2508.03306v2>
- **Submitted**: 2025-08-05 10:27:57
- **Comment**: 11 pages, 5 figures, submitted to ARR
- **Topic Keywords**: query, relevance, retrieval
- **Reason**: The paper focuses on improving the evaluation protocol for low-precision retrieval, which is a specific aspect of Information Retrieval. While it touches on relevance optimization, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's relevance is somewhat related, but not a central match.

#### Abstract
> Lowering the numerical precision of model parameters and computations is
widely adopted to improve the efficiency of retrieval systems. However, when
computing relevance scores between the query and documents in low-precision, we
observe spurious ties due to the reduced granularity. This introduces high
variability in the results based on tie resolution, making the evaluation less
reliable. To address this, we propose a more robust retrieval evaluation
protocol designed to reduce score variation. It consists of: (1) High-Precision
Scoring (HPS), which upcasts the final scoring step to higher precision to
resolve tied candidates with minimal computational cost; and (2) Tie-aware
Retrieval Metrics (TRM), which report expected scores, range, and bias to
quantify order uncertainty of tied candidates. Our experiments test multiple
models with three scoring functions on two retrieval datasets to demonstrate
that HPS dramatically reduces tie-induced instability, and TRM accurately
recovers expected metric values. This combination enables a more consistent and
reliable evaluation system for lower-precision retrievals.

### 8. Token-Level Precise Attack on RAG: Searching for the Best Alternatives to Mislead Generation

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Zizhong Li, Haopeng Zhang, Jiawei Zhang
- **URL**: <http://arxiv.org/abs/2508.03110v1>
- **Submitted**: 2025-08-05 05:44:19
- **Topic Keywords**: retriever, rag, retrieval, search
- **Reason**: The paper explores attacks on the Retrieval-Augmented Generation (RAG) framework, which is a topic in Natural Language Processing (NLP). While it touches on the retrieval stage, the focus is on the generation stage, which is not directly related to query understanding, ranking models, or user behavior modeling in Information Retrieval (IR). The paper's relevance is somewhat limited due to its focus on attacks rather than improving the underlying technology.

#### Abstract
> While large language models (LLMs) have achieved remarkable success in
providing trustworthy responses for knowledge-intensive tasks, they still face
critical limitations such as hallucinations and outdated knowledge. To address
these issues, the retrieval-augmented generation (RAG) framework enhances LLMs
with access to external knowledge via a retriever, enabling more accurate and
real-time outputs about the latest events. However, this integration brings new
security vulnerabilities: the risk that malicious content in the external
database can be retrieved and used to manipulate model outputs. Although prior
work has explored attacks on RAG systems, existing approaches either rely
heavily on access to the retriever or fail to jointly consider both retrieval
and generation stages, limiting their effectiveness, particularly in black-box
scenarios. To overcome these limitations, we propose Token-level Precise Attack
on the RAG (TPARAG), a novel framework that targets both white-box and
black-box RAG systems. TPARAG leverages a lightweight white-box LLM as an
attacker to generate and iteratively optimize malicious passages at the token
level, ensuring both retrievability and high attack success in generation.
Extensive experiments on open-domain QA datasets demonstrate that TPARAG
consistently outperforms previous approaches in retrieval-stage and end-to-end
attack effectiveness. These results further reveal critical vulnerabilities in
RAG pipelines and offer new insights into improving their robustness.

### 9. AGENTiGraph: A Multi-Agent Knowledge Graph Framework for Interactive, Domain-Specific LLM Chatbots

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Xinjie Zhao, Moritz Blum, Fan Gao, Yingjian Chen, Boming Yang, Luis Marquez-Carpintero, M√≥nica Pina-Navarro, Yanran Fu, So Morikawa, Yusuke Iwasawa, Yutaka Matsuo, Chanjun Park, Irene Li
- **URL**: <http://arxiv.org/abs/2508.02999v1>
- **Submitted**: 2025-08-05 01:55:06
- **Comment**: CIKM 2025, Demo Track
- **Topic Keywords**: query, queries, search
- **Reason**: The paper presents a knowledge graph framework for interactive chatbots, which is related to information retrieval and natural language processing. However, the focus is on building and refining knowledge bases through multi-round dialogues, which is not directly aligned with my research interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> AGENTiGraph is a user-friendly, agent-driven system that enables intuitive
interaction and management of domain-specific data through the manipulation of
knowledge graphs in natural language. It gives non-technical users a complete,
visual solution to incrementally build and refine their knowledge bases,
allowing multi-round dialogues and dynamic updates without specialized query
languages. The flexible design of AGENTiGraph, including intent classification,
task planning, and automatic knowledge integration, ensures seamless reasoning
between diverse tasks. Evaluated on a 3,500-query benchmark within an
educational scenario, the system outperforms strong zero-shot baselines
(achieving 95.12% classification accuracy, 90.45% execution success),
indicating potential scalability to compliance-critical or multi-step queries
in legal and medical domains, e.g., incorporating new statutes or research on
the fly. Our open-source demo offers a powerful new paradigm for multi-turn
enterprise knowledge management that bridges LLMs and structured graphs.

### 10. AttnTrace: Attention-based Context Traceback for Long-Context LLMs

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Yanting Wang, Runpeng Geng, Ying Chen, Jinyuan Jia
- **URL**: <http://arxiv.org/abs/2508.03793v1>
- **Submitted**: 2025-08-05 17:56:51
- **Comment**: The code is available at https://github.com/Wang-Yanting/AttnTrace.
  The demo is available at https://huggingface.co/spaces/SecureLLMSys/AttnTrace
- **Topic Keywords**: rag, retrieval, acl
- **Reason**: The paper proposes a new method for context traceback in long-context LLMs, which is not directly related to information retrieval or search technologies. While it mentions attention weights, which are relevant to NLP, the focus is on language models and their applications, rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> Long-context large language models (LLMs), such as Gemini-2.5-Pro and
Claude-Sonnet-4, are increasingly used to empower advanced AI systems,
including retrieval-augmented generation (RAG) pipelines and autonomous agents.
In these systems, an LLM receives an instruction along with a context--often
consisting of texts retrieved from a knowledge database or memory--and
generates a response that is contextually grounded by following the
instruction. Recent studies have designed solutions to trace back to a subset
of texts in the context that contributes most to the response generated by the
LLM. These solutions have numerous real-world applications, including
performing post-attack forensic analysis and improving the interpretability and
trustworthiness of LLM outputs. While significant efforts have been made,
state-of-the-art solutions such as TracLLM often lead to a high computation
cost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a
single response-context pair. In this work, we propose AttnTrace, a new context
traceback method based on the attention weights produced by an LLM for a
prompt. To effectively utilize attention weights, we introduce two techniques
designed to enhance the effectiveness of AttnTrace, and we provide theoretical
insights for our design choice. We also perform a systematic evaluation for
AttnTrace. The results demonstrate that AttnTrace is more accurate and
efficient than existing state-of-the-art context traceback methods. We also
show that AttnTrace can improve state-of-the-art methods in detecting prompt
injection under long contexts through the attribution-before-detection
paradigm. As a real-world application, we demonstrate that AttnTrace can
effectively pinpoint injected instructions in a paper designed to manipulate
LLM-generated reviews. The code is at
https://github.com/Wang-Yanting/AttnTrace.

### 11. Coherent Multimodal Reasoning with Iterative Self-Evaluation for Vision-Language Models

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Wenjie Luo, Ruocheng Li, Shanshan Zhu, Julian Perry
- **URL**: <http://arxiv.org/abs/2508.02886v1>
- **Submitted**: 2025-08-04 20:33:58
- **Topic Keywords**: queries, rag
- **Reason**: The paper proposes a novel framework for multimodal reasoning, which is not directly related to information retrieval or search technologies. While it touches on topics like query decomposition and inference, the focus is on vision-language models and multimodal reasoning, which is not a core area of interest for the user. The paper's relevance is limited to the user's interest in NLP and data mining, but the connection is not strong enough to warrant a higher score.

#### Abstract
> Despite significant advancements, current large language models (LLMs) and
vision-language models (LVLMs) continue to struggle with complex, multi-step,
cross-modal common sense reasoning tasks, often exhibiting a lack of
"deliberative thinking." They tend to rely on superficial associations rather
than deep, chained inference, particularly when integrating visual information
with abstract concepts. To address this, we propose the Coherent Multimodal
Reasoning Framework (CMRF), a novel approach that enhances LVLMs' common sense
reasoning capabilities through an iterative, self-evaluating inference
mechanism. CMRF mimics human problem-solving by decomposing complex queries,
generating step-by-step inferences, and self-correcting errors. Our framework
integrates three key modules: a Reasoning Decomposition Unit (RDU) for breaking
down problems into sub-questions, a Contextual Inference Engine (CIE) for
contextual inference, and a Coherence Assessment Module (CAM) for evaluating
logical consistency and confidence. Coupled with an Adaptive Iterative
Refinement strategy, CMRF systematically refines its reasoning paths. Built
upon LLaVA-1.6-34B and trained on a novel Multimodal Daily Activity Reasoning
(MDAR) dataset, CMRF achieves state-of-the-art performance among open-source
LVLMs on challenging benchmarks like VCR, A-OKVQA, and DailyLife-MRC. It
attains an average accuracy of 69.4%, surpassing the best open-source baseline
by +2.4 percentage points, with particular strength in complex reasoning
scenarios. Extensive ablation studies and human evaluations confirm the
critical contributions of each module and the effectiveness of iterative
refinement in fostering more coherent and accurate reasoning.

### 12. Defending Against Knowledge Poisoning Attacks During Retrieval-Augmented Generation

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Kennedy Edemacu, Vinay M. Shashidhar, Micheal Tuape, Dan Abudu, Beakcheol Jang, Jong Wook Kim
- **URL**: <http://arxiv.org/abs/2508.02835v1>
- **Submitted**: 2025-08-04 19:03:52
- **Comment**: Preprint for Submission
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper discusses Retrieval-Augmented Generation (RAG) and knowledge poisoning attacks, which is somewhat related to information retrieval and search technologies. However, the focus on language models and generation rather than query understanding, ranking models, and user behavior modeling limits its relevance to the user's core research interests.

#### Abstract
> Retrieval-Augmented Generation (RAG) has emerged as a powerful approach to
boost the capabilities of large language models (LLMs) by incorporating
external, up-to-date knowledge sources. However, this introduces a potential
vulnerability to knowledge poisoning attacks, where attackers can compromise
the knowledge source to mislead the generation model. One such attack is the
PoisonedRAG in which the injected adversarial texts steer the model to generate
an attacker-chosen response to a target question. In this work, we propose
novel defense methods, FilterRAG and ML-FilterRAG, to mitigate the PoisonedRAG
attack. First, we propose a new property to uncover distinct properties to
differentiate between adversarial and clean texts in the knowledge data source.
Next, we employ this property to filter out adversarial texts from clean ones
in the design of our proposed approaches. Evaluation of these methods using
benchmark datasets demonstrate their effectiveness, with performances close to
those of the original RAG systems.

### 13. Personalized Recommendation of Dish and Restaurant Collections on iFood

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Fernando F. Granado, Davi A. Bezerra, Iuri Queiroz, Nathan Oliveira, Pedro Fernandes, Bruno Schock
- **URL**: <http://arxiv.org/abs/2508.03670v1>
- **Submitted**: 2025-08-05 17:34:19
- **Comment**: Workshop on Two-sided Marketplace Optimization: Search, Discovery,
  Matching, Pricing & Growth in conjunction with KDD Conference (KDD 2025) in
  Toronto, Canada
- **Topic Keywords**: conversion rate, recommend
- **Reason**: The paper focuses on personalized recommendation of dish and restaurant collections, which is related to information retrieval and search technologies. However, the emphasis is on recommender systems, which is not the primary focus of the user's research interests. The paper's use of machine learning and data mining techniques is relevant, but the application in the food delivery domain is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Food delivery platforms face the challenge of helping users navigate vast
catalogs of restaurants and dishes to find meals they truly enjoy. This paper
presents RED, an automated recommendation system designed for iFood, Latin
America's largest on-demand food delivery platform, to personalize the
selection of curated food collections displayed to millions of users. Our
approach employs a LightGBM classifier that scores collections based on three
feature groups: collection characteristics, user-collection similarity, and
contextual information. To address the cold-start problem of recommending newly
created collections, we develop content-based representations using item
embeddings and implement monotonicity constraints to improve generalization. We
tackle data scarcity by bootstrapping from category carousel interactions and
address visibility bias through unbiased sampling of impressions and purchases
in production. The system demonstrates significant real-world impact through
extensive A/B testing with 5-10% of iFood's user base. Online results of our
A/B tests add up to 97% improvement in Card Conversion Rate and 1.4% increase
in overall App Conversion Rate compared to popularity-based baselines. Notably,
our offline accuracy metrics strongly correlate with online performance,
enabling reliable impact prediction before deployment. To our knowledge, this
is the first work to detail large-scale recommendation of curated food
collections in a dynamic commercial environment.

### 14. EmbedGrad: Gradient-Based Prompt Optimization in Embedding Space for Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Xiaoming Hou, Jiquan Zhang, Zibin Lin, DaCheng Tao, Shengli Zhang
- **URL**: <http://arxiv.org/abs/2508.03533v1>
- **Submitted**: 2025-08-05 15:03:10
- **Topic Keywords**: queries
- **Reason**: The paper proposes a novel framework for optimizing text prompt embeddings through gradient-based refinement, which is relevant to the field of Natural Language Processing (NLP). However, it does not directly relate to Information Retrieval (IR) or Search technologies, which are the user's primary research interests. The paper's focus on large language models and task adaptation is somewhat related to the user's background in e-commerce and NLP, but it does not address query understanding, ranking models, or user behavior modeling, which are key areas of interest.

#### Abstract
> Effectively adapting powerful pretrained foundation models to diverse tasks
remains a key challenge in AI deployment. Current approaches primarily follow
two paradigms:discrete optimization of text prompts through prompt engineering,
or continuous adaptation via additional trainable parameters. Both exhibit
limitations-discrete methods lack refinement precision while parameter-based
techniques increase complexity and reduce interpretability. To address these
constraints, we propose EmbedGrad, a novel framework that optimizes text prompt
embeddings through gradient-based refinement. Our approach uniquely decouples
training from deployment:during optimization,labeled examples guide precise
embedding adjustments while preserving semantic meaning; during inference, only
optimized embeddings integrate with user queries. This enables fine-grained
calibration impossible in text space, such as enhancing the reasoning
capability of prompts like please reason step by step. Comprehensive
evaluations across mathematical reasoning, sentiment analysis, and causal
judgment tasks demonstrate EmbedGrad's effectiveness:optimizing this reasoning
prompt for Qwen2.5-Math-1.5B increased accuracy from 14.74\% to 58.96\% on
mathematical problems. Consistent improvements were observed across model
scales (0.5B-14B) and all tasks, with particularly significant gains for
smaller models on complex problems like causal judgment. By bridging prompt
engineering and parameter efficiency without architectural changes, our work
establishes embedding refinement as a powerful new paradigm for task
adaptation.

### 15. CoCoTen: Detecting Adversarial Inputs to Large Language Models through Latent Space Features of Contextual Co-occurrence Tensors

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Sri Durga Sai Sowmya Kadali, Evangelos E. Papalexakis
- **URL**: <http://arxiv.org/abs/2508.02997v2>
- **Submitted**: 2025-08-05 01:53:32
- **Topic Keywords**: rag, search
- **Reason**: While the paper explores a relevant topic in Natural Language Processing (NLP), it does not directly align with your primary focus on Information Retrieval (IR), query understanding, ranking models, and user behavior modeling. The paper's focus on detecting adversarial inputs to Large Language Models is not directly applicable to your research interests.

#### Abstract
> The widespread use of Large Language Models (LLMs) in many applications marks
a significant advance in research and practice. However, their complexity and
hard-to-understand nature make them vulnerable to attacks, especially
jailbreaks designed to produce harmful responses. To counter these threats,
developing strong detection methods is essential for the safe and reliable use
of LLMs. This paper studies this detection problem using the Contextual
Co-occurrence Matrix, a structure recognized for its efficacy in data-scarce
environments. We propose a novel method leveraging the latent space
characteristics of Contextual Co-occurrence Matrices and Tensors for the
effective identification of adversarial and jailbreak prompts. Our evaluations
show that this approach achieves a notable F1 score of 0.83 using only 0.5% of
labeled prompts, which is a 96.6% improvement over baselines. This result
highlights the strength of our learned patterns, especially when labeled data
is scarce. Our method is also significantly faster, speedup ranging from 2.3 to
128.4 times compared to the baseline models. To support future research and
reproducibility, we have made our implementation publicly available.

### 16. FairLangProc: A Python package for fairness in NLP

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Arturo P√©rez-Peralta, Sandra Ben√≠tez-Pe√±a, Rosa E. Lillo
- **URL**: <http://arxiv.org/abs/2508.03677v1>
- **Submitted**: 2025-08-05 17:47:53
- **Comment**: 40 pages, 4 figures, 3 tables
- **Topic Keywords**: rag
- **Reason**: The paper focuses on fairness in NLP, which is a related topic, but it does not directly address information retrieval, search technologies, or query understanding. While it mentions the use of transformers, which is a relevant area in NLP, the primary focus is on fairness and bias mitigation, rather than on ranking models or user behavior modeling.

#### Abstract
> The rise in usage of Large Language Models to near ubiquitousness in recent
years has risen societal concern about their applications in decision-making
contexts, such as organizational justice or healthcare. This, in turn, poses
questions about the fairness of these models in critical settings, which leads
to the developement of different procedures to address bias in Natural Language
Processing. Although many datasets, metrics and algorithms have been proposed
to measure and mitigate harmful prejudice in Natural Language Processing, their
implementation is diverse and far from centralized. As a response, this paper
presents FairLangProc, a comprehensive Python package providing a common
implementation of some of the more recent advances in fairness in Natural
Language Processing providing an interface compatible with the famous Hugging
Face transformers library, aiming to encourage the widespread use and
democratization of bias mitigation techniques. The implementation can be found
on https://github.com/arturo-perez-peralta/FairLangProc.

### 17. Beyond the Surface: Enhancing LLM-as-a-Judge Alignment with Human via Internal Representations

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Peng Lai, Jianjie Zheng, Sijie Cheng, Yun Chen, Peng Li, Yang Liu, Guanhua Chen
- **URL**: <http://arxiv.org/abs/2508.03550v1>
- **Submitted**: 2025-08-05 15:18:36
- **Topic Keywords**: rag
- **Reason**: The paper explores the alignment of large language models with human preferences, which is related to query understanding and ranking models in Information Retrieval. However, the focus on language models and internal representations is not directly aligned with the user's interests in search technologies and user behavior modeling.

#### Abstract
> The growing scale of evaluation tasks has led to the widespread adoption of
automated evaluation using large language models, a paradigm known as
"LLMas-a-judge." However, improving its alignment with human preferences
without complex prompts or fine-tuning remains challenging. In this work,
motivated by preliminary findings that middle-to-upper layers encode
semantically and taskrelevant representations that are often more aligned with
human judgments than the final layer, we propose LAGER, a lightweight and
efficient framework for enhancing LLM-as-a-Judge alignment with human scoring,
via internal representations. LAGER produces fine-grained judgment scores by
aggregating cross-layer scoretoken logits and computing the expected score from
a softmax-based distribution, with the LLM backbone kept frozen. LAGER fully
leverages the complementary information across different layers, overcoming the
limitations of relying solely on the final layer. We evaluate our method on the
standard alignment benchmarks Flask, HelpSteer, and BIGGen using Spearman
correlation, and find that LAGER achieves improvements of up to 7.5% over the
best baseline across these benchmarks. Without reasoning steps, LAGER matches
or outperforms reasoning-based methods. Experiments on downstream applications,
such as data selection and emotional understanding, further show the
effectiveness of our method.

### 18. Cropping outperforms dropout as an augmentation strategy for training self-supervised text embeddings

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Rita Gonz√°lez-M√°rquez, Philipp Berens, Dmitry Kobak
- **URL**: <http://arxiv.org/abs/2508.03453v1>
- **Submitted**: 2025-08-05 13:54:01
- **Topic Keywords**: retrieval
- **Reason**: The paper explores self-supervised training of text embeddings using data augmentation, which is a topic in NLP. However, the focus on contrastive learning and embedding quality assessment is not directly related to query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval. While the paper touches on NLP applications, it does not specifically address search technologies or real-time relevance optimization.

#### Abstract
> Text embeddings, i.e. vector representations of entire texts, play an
important role in many NLP applications, such as retrieval-augmented
generation, sentiment analysis, clustering, or visualizing collections of texts
for data exploration. Currently, top-performing embedding models are derived
from pre-trained language models via extensive supervised fine-tuning using
curated text pairs. This contrasts with computer vision, where self-supervised
training based on data augmentations has demonstrated remarkable success. Here
we systematically compare the two most well-known augmentation strategies for
positive pair generation in contrastive learning of text embeddings. We assess
embedding quality on MTEB and additional in-domain evaluations and show that
cropping augmentation strongly outperforms the dropout-based approach. We find
that on out-of-domain data, the quality of resulting embeddings is below the
supervised SOTA models, but for in-domain data, self-supervised fine-tuning
produces high-quality text embeddings after very short fine-tuning, sometimes
only marginally below the supervised SOTA. Finally, we show that representation
quality increases towards the last transformer layers, which undergo the
largest change during fine-tuning; and that fine-tuning only those last layers
is sufficient to reach similar embedding quality.

### 19. Do language models accommodate their users? A study of linguistic convergence

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Terra Blevins, Susanne Schmalwieser, Benjamin Roth
- **URL**: <http://arxiv.org/abs/2508.03276v1>
- **Submitted**: 2025-08-05 09:55:40
- **Topic Keywords**: rag
- **Reason**: The paper explores the linguistic convergence of language models, which is a topic in Natural Language Processing (NLP). While it touches on the idea of models adapting to user patterns, it does not directly relate to query understanding, ranking models, or user behavior modeling in Information Retrieval (IR), which are the user's primary research interests.

#### Abstract
> While large language models (LLMs) are generally considered proficient in
generating language, how similar their language usage is to that of humans
remains understudied. In this paper, we test whether models exhibit linguistic
convergence, a core pragmatic element of human language communication, asking:
do models adapt, or converge, to the linguistic patterns of their user? To
answer this, we systematically compare model completions of exisiting dialogues
to the original human responses across sixteen language models, three dialogue
corpora, and a variety of stylometric features. We find that models strongly
converge to the conversation's style, often significantly overfitting relative
to the human baseline. While convergence patterns are often feature-specific,
we observe consistent shifts in convergence across modeling settings, with
instruction-tuned and larger models converging less than their pretrained
counterparts. Given the differences between human and model convergence
patterns, we hypothesize that the underlying mechanisms for these behaviors are
very different.

### 20. LECTOR: LLM-Enhanced Concept-based Test-Oriented Repetition for Adaptive Spaced Learning

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Jiahao Zhao
- **URL**: <http://arxiv.org/abs/2508.03275v1>
- **Submitted**: 2025-08-05 09:53:26
- **Comment**: 15 pages, 4 figures, 1 table
- **Topic Keywords**: rag
- **Reason**: The paper presents a novel adaptive scheduling algorithm for test-oriented learning scenarios, leveraging large language models for semantic analysis. While it touches on semantic understanding and personalized adaptation, the focus is on language learning and vocabulary retention, which is not directly related to the user's primary research interests in Information Retrieval and Search technologies.

#### Abstract
> Spaced repetition systems are fundamental to efficient learning and memory
retention, but existing algorithms often struggle with semantic interference
and personalized adaptation. We present LECTOR (\textbf{L}LM-\textbf{E}nhanced
\textbf{C}oncept-based \textbf{T}est-\textbf{O}riented \textbf{R}epetition), a
novel adaptive scheduling algorithm specifically designed for test-oriented
learning scenarios, particularly language examinations where success rate is
paramount. LECTOR leverages large language models for semantic analysis while
incorporating personalized learning profiles, addressing the critical challenge
of semantic confusion in vocabulary learning by utilizing LLM-powered semantic
similarity assessment and integrating it with established spaced repetition
principles. Our comprehensive evaluation against six baseline algorithms
(SSP-MMC, SM2, HLR, FSRS, ANKI, THRESHOLD) across 100 simulated learners over
100 days demonstrates significant improvements: LECTOR achieves a 90.2\%
success rate compared to 88.4\% for the best baseline (SSP-MMC), representing a
2.0\% relative improvement. The algorithm shows particular strength in handling
semantically similar concepts, reducing confusion-induced errors while
maintaining computational efficiency. Our results establish LECTOR as a
promising direction for intelligent tutoring systems and adaptive learning
platforms.

### 21. Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and Self-Checking for Complex Instruction Following

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Chenyang Wang, Liang Wen, Shousheng Jia, Xiangzheng Zhang, Liang Xu
- **URL**: <http://arxiv.org/abs/2508.03178v1>
- **Submitted**: 2025-08-05 07:42:00
- **Comment**: 12 pages, 10 figures, 7 tables
- **Topic Keywords**: rag
- **Reason**: The paper focuses on improving the reasoning abilities of Large Language Models (LLMs) for complex instruction following, which is not directly related to Information Retrieval (IR) or Search technologies. While the paper employs reinforcement learning and fine-tuning strategies, the context is distinct from query understanding, ranking models, and user behavior modeling, which are core areas of interest in IR.

#### Abstract
> While advancements in the reasoning abilities of LLMs have significantly
enhanced their performance in solving mathematical problems, coding tasks, and
general puzzles, their effectiveness in accurately adhering to instructions
remains inconsistent, particularly with more complex directives. Our
investigation identifies lazy reasoning during the thinking stage as the
primary factor contributing to poor instruction adherence. To mitigate this
issue, we propose a comprehensive framework designed to enable rigorous
reasoning processes involving preview and self-checking, essential for
satisfying strict instruction constraints. Specifically, we first generate
instructions with complex constraints and apply a filtering process to obtain
valid prompts, resulting in three distinct prompt datasets categorized as hard,
easy, and pass. Then, we employ rejection sampling on the pass prompts to
curate a small yet high-quality dataset, enabling a cold-start initialization
of the model and facilitating its adaptation to effective reasoning patterns.
Subsequently, we employ an entropy-preserving supervised fine-tuning
(Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL)
reinforcement learning guided by rule-based dense rewards. This approach
encourages the model to transform its reasoning mechanism, ultimately fostering
generalizable reasoning abilities that encompass preview and self-checking.
Extensive experiments conducted on instruction-following benchmarks demonstrate
remarkable performance improvements across various model scales. Notably, our
Light-IF-32B model surpasses both larger open-source models such as DeepSeek-R1
and closed-source models like Doubao-1.6.

### 22. Toward Verifiable Misinformation Detection: A Multi-Tool LLM Agent Framework

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Zikun Cui, Tianyi Huang, Chia-En Chiang, Cuiqianhe Du
- **URL**: <http://arxiv.org/abs/2508.03092v1>
- **Submitted**: 2025-08-05 05:15:03
- **Topic Keywords**: web search, search
- **Reason**: The paper proposes a novel approach to misinformation detection using Large Language Models, which is related to my interests in Information Retrieval and Natural Language Processing. However, the focus on verifiable misinformation detection and fact-checking is not directly aligned with my primary research themes, such as query understanding, ranking models, and user behavior modeling.

#### Abstract
> With the proliferation of Large Language Models (LLMs), the detection of
misinformation has become increasingly important and complex. This research
proposes an innovative verifiable misinformation detection LLM agent that goes
beyond traditional true/false binary judgments. The agent actively verifies
claims through dynamic interaction with diverse web sources, assesses
information source credibility, synthesizes evidence, and provides a complete
verifiable reasoning process. Our designed agent architecture includes three
core tools: precise web search tool, source credibility assessment tool and
numerical claim verification tool. These tools enable the agent to execute
multi-step verification strategies, maintain evidence logs, and form
comprehensive assessment conclusions. We evaluate using standard misinformation
datasets such as FakeNewsNet, comparing with traditional machine learning
models and LLMs. Evaluation metrics include standard classification metrics,
quality assessment of reasoning processes, and robustness testing against
rewritten content. Experimental results show that our agent outperforms
baseline methods in misinformation detection accuracy, reasoning transparency,
and resistance to information rewriting, providing a new paradigm for
trustworthy AI-assisted fact-checking.

### 23. Parameter-Efficient Single Collaborative Branch for Recommendation

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Marta Moscati, Shah Nawaz, Markus Schedl
- **URL**: <http://arxiv.org/abs/2508.03518v1>
- **Submitted**: 2025-08-05 14:46:06
- **Comment**: 5 pages
- **Topic Keywords**: relevance, rag, recommend, commerce, e-commerce
- **Reason**: The paper focuses on recommender systems, which is a related topic, but it does not address information retrieval, query understanding, ranking models, or user behavior modeling, which are the core research themes of the user's interests. The paper's emphasis on reducing the number of parameters and improving beyond-accuracy aspects without compromising accuracy is not directly relevant to the user's interests in deep semantic understanding and real-time relevance optimization.

#### Abstract
> Recommender Systems (RS) often rely on representations of users and items in
a joint embedding space and on a similarity metric to compute relevance scores.
In modern RS, the modules to obtain user and item representations consist of
two distinct and separate neural networks (NN). In multimodal representation
learning, weight sharing has been proven effective in reducing the distance
between multiple modalities of a same item. Inspired by these approaches, we
propose a novel RS that leverages weight sharing between the user and item NN
modules used to obtain the latent representations in the shared embedding
space. The proposed framework consists of a single Collaborative Branch for
Recommendation (CoBraR). We evaluate CoBraR by means of quantitative
experiments on e-commerce and movie recommendation. Our experiments show that
by reducing the number of parameters and improving beyond-accuracy aspects
without compromising accuracy, CoBraR has the potential to be applied and
extended for real-world scenarios.

### 24. KBest: Efficient Vector Search on Kunpeng CPU

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Kaihao MA, Meiling Wang, Senkevich Oleg, Zijian LI, Daihao Xue, Dmitriy Malyshev, Yangming Lv, Shihai Xiao, Xiao Yan, Radionov Alexander, Weidi Zeng, Yuanzhan Gao, Zhiyu Zou, Yao xin, Liu Lin, Junhao Wu, Yiding Liu, Yaoyao Fu, Gongyi Wang, Gong Zhang, Fei Yi, Yingfan Liu
- **URL**: <http://arxiv.org/abs/2508.03016v1>
- **Submitted**: 2025-08-05 02:52:15
- **Topic Keywords**: query, queries, recommend, search
- **Reason**: The paper focuses on optimizing vector search for a specific CPU architecture, which is not directly related to query understanding, ranking models, or user behavior modeling in Information Retrieval. While it mentions applications such as search and recommendation, the paper's primary concern is efficiency and optimization for a particular hardware platform, rather than deep semantic understanding or real-time relevance optimization.

#### Abstract
> Vector search, which returns the vectors most similar to a given query vector
from a large vector dataset, underlies many important applications such as
search, recommendation, and LLMs. To be economic, vector search needs to be
efficient to reduce the resources required by a given query workload. However,
existing vector search libraries (e.g., Faiss and DiskANN) are optimized for
x86 CPU architectures (i.e., Intel and AMD CPUs) while Huawei Kunpeng CPUs are
based on the ARM architecture and competitive in compute power. In this paper,
we present KBest as a vector search library tailored for the latest Kunpeng 920
CPUs. To be efficient, KBest incorporates extensive hardware-aware and
algorithmic optimizations, which include single-instruction-multiple-data
(SIMD) accelerated distance computation, data prefetch, index refinement, early
termination, and vector quantization. Experiment results show that KBest
outperforms SOTA vector search libraries running on x86 CPUs, and our
optimizations can improve the query throughput by over 2x. Currently, KBest
serves applications from both our internal business and external enterprise
clients with tens of millions of queries on a daily basis.

### 25. LLM-based IR-system for Bank Supervisors

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Ilias Aarab
- **URL**: <http://arxiv.org/abs/2508.02945v1>
- **Submitted**: 2025-08-04 23:02:01
- **Topic Keywords**: information retrieval, rag, retrieval, rank
- **Reason**: The paper's focus on Information Retrieval (IR) is relevant, but it is specific to a niche domain (bank supervision) and does not align with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's use of NLP techniques is also limited to lexical and semantic matching, which is not a central aspect of the user's research.

#### Abstract
> Bank supervisors face the complex task of ensuring that new measures are
consistently aligned with historical precedents. To address this challenge, we
introduce a novel Information Retrieval (IR) System tailored to assist
supervisors in drafting both consistent and effective measures. This system
ingests findings from on-site investigations. It then retrieves the most
relevant historical findings and their associated measures from a comprehensive
database, providing a solid basis for supervisors to write well-informed
measures for new findings. Utilizing a blend of lexical, semantic, and Capital
Requirements Regulation (CRR) fuzzy set matching techniques, the IR system
ensures the retrieval of findings that closely align with current cases. The
performance of this system, particularly in scenarios with partially labeled
data, is validated through a Monte Carlo methodology, showcasing its robustness
and accuracy. Enhanced by a Transformer-based Denoising AutoEncoder for
fine-tuning, the final model achieves a Mean Average Precision (MAP@100) of
0.83 and a Mean Reciprocal Rank (MRR@100) of 0.92. These scores surpass those
of both standalone lexical models such as BM25 and semantic BERT-like models.

### 26. CTTS: Collective Test-Time Scaling

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Zhende Song, Shengji Tang, Peng Ye, Jiayuan Fan, Tao Chen
- **URL**: <http://arxiv.org/abs/2508.03333v1>
- **Submitted**: 2025-08-05 11:19:08
- **Topic Keywords**: ranking, rag, rank, search
- **Reason**: The paper focuses on collective test-time scaling, which is not directly related to information retrieval, query understanding, ranking models, or user behavior modeling. While it involves language models, the context is different from the user's interests in IR and NLP.

#### Abstract
> Test-time scaling (TTS) has emerged as a promising research field for
enhancing the effectiveness of large language models (LLMs) without extra
training. However, most existing approaches, e.g., Best-of-N and
Self-Consistency rely on a single agent interacting with a reward model
(SA-SR), constrained by limited capabilities of a single test-time scaling
(STTS) paradigm. On the other hand, recent works demonstrate that
collective-agent methods can break through the upper bound of single-agent
systems by orchestrating diverse models. Thus, in this paper, we take a first
step towards exploring Collective Test-Time Scaling (CTTS). Consider the
different interaction types of single and multiple models, we design three
primary paradigms to investigate the optimal paradigm of CTTS: (1) single agent
to multiple reward models (SA-MR); (2) multiple agents to single reward model
(MA-SR); and (3) multiple agents to multiple reward models (MA-MR). Extensive
experiments demonstrate that MA-MR consistently achieves the best performance.
Based on this, we propose a novel framework named CTTS-MM that effectively
leverages both multi-agent and multi-reward-model collaboration for enhanced
inference. Specifically, for multi-agent collaboration, we propose an Agent
Collaboration Search (ACS), which searches for the most effective combination
of LLM agents from a large candidate pool; for multi-reward-model
collaboration, we propose Mixture of Reword Models (MoR), which consists of a
curated question pool and a Prior Reward model Ensemble Selection (PRES) to
select the optimal combinations of reward models via Pair-wise Reward Ranking
(PRR) metric. Experiments across seven mainstream benchmarks demonstrate that
the proposed CTTS-MM consistently obtains superior performance. Code will be
released at https://github.com/magent4aci/CTTS-MM.

### 27. ADSeeker: A Knowledge-Infused Framework for Anomaly Detection and Reasoning

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Kai Zhang, Zekai Zhang, Xihe Sun, Jingmeng Nie, Qinghui Chen, Han Hao, Jianyuan Guo, Jinglin Zhang
- **URL**: <http://arxiv.org/abs/2508.03088v1>
- **Submitted**: 2025-08-05 05:05:06
- **Topic Keywords**: query, rag, retrieval
- **Reason**: The paper focuses on anomaly detection and reasoning in the context of automatic vision inspection, which is not directly related to information retrieval, search technologies, or query understanding. Although it mentions multimodal large language models, the primary focus is on computer vision and anomaly detection, making it not relevant to the user's research interests.

#### Abstract
> Automatic vision inspection holds significant importance in industry
inspection. While multimodal large language models (MLLMs) exhibit strong
language understanding capabilities and hold promise for this task, their
performance remains significantly inferior to that of human experts. In this
context, we identify two key challenges: (i) insufficient integration of
anomaly detection (AD) knowledge during pre-training, and (ii) the lack of
technically precise and conte-aware language generation for anomaly reasoning.
To address these issues, we propose ADSeeker, an anomaly task assistant
designed to enhance inspection performance through knowledge-grounded
reasoning. ADSeeker leverages a curated visual document knowledge base,
SEEK-MVTec&VisA (SEEK-M&V), which we construct to address the limitations of
existing resources that rely solely on unstructured text. SEEK-M&V includes
semantic-rich descriptions and image-document pairs, enabling more
comprehensive anomaly understanding. To effectively retrieve and utilize this
knowledge, we introduce the Query Image-Knowledge Retrieval-Augmented
Generation (Q2K RAG) framework. To further enhance the performance in zero-shot
anomaly detection (ZSAD), ADSeeker leverages the Hierarchical Sparse Prompt
mechanism and type-level features to efficiently extract anomaly patterns.
Furthermore, to tackle the challenge of limited in industry anomaly detection
(IAD) data, we introduce the largest-scale AD dataset, Multi-type Anomaly
(MulA), encompassing 72 multi-scale defect types across 26 Categories.
Extensive experiments show that our plug-and-play framework, ADSeeker, achieves
state-of-the-art zero-shot performance on several benchmark datasets.

### 28. Highlight & Summarize: RAG without the jailbreaks

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Giovanni Cherubin, Andrew Paverd
- **URL**: <http://arxiv.org/abs/2508.02872v1>
- **Submitted**: 2025-08-04 20:01:00
- **Topic Keywords**: relevance, rag, retrieval
- **Reason**: The paper focuses on a specific problem in Natural Language Processing (LLM hijacking) and proposes a solution using a retrieval-augmented generation (RAG) system. While it mentions retrieval-augmented generation, the primary focus is on LLM hijacking and not on query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval. The paper does not seem to be directly related to the user's research themes.

#### Abstract
> Preventing jailbreaking and model hijacking of Large Language Models (LLMs)
is an important yet challenging task. For example, when interacting with a
chatbot, malicious users can input specially crafted prompts to cause the LLM
to generate undesirable content or perform a completely different task from its
intended purpose. Existing mitigations for such attacks typically rely on
hardening the LLM's system prompt or using a content classifier trained to
detect undesirable content or off-topic conversations. However, these
probabilistic approaches are relatively easy to bypass due to the very large
space of possible inputs and undesirable outputs. In this paper, we present and
evaluate Highlight & Summarize (H&S), a new design pattern for
retrieval-augmented generation (RAG) systems that prevents these attacks by
design. The core idea is to perform the same task as a standard RAG pipeline
(i.e., to provide natural language answers to questions, based on relevant
sources) without ever revealing the user's question to the generative LLM. This
is achieved by splitting the pipeline into two components: a highlighter, which
takes the user's question and extracts relevant passages ("highlights") from
the retrieved documents, and a summarizer, which takes the highlighted passages
and summarizes them into a cohesive answer. We describe several possible
instantiations of H&S and evaluate their generated responses in terms of
correctness, relevance, and response quality. Surprisingly, when using an
LLM-based highlighter, the majority of H&S responses are judged to be better
than those of a standard RAG pipeline.

### 29. Marito: Structuring and Building Open Multilingual Terminologies for South African NLP

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Vukosi Marivate, Isheanesu Dzingirai, Fiskani Banda, Richard Lastrucci, Thapelo Sindane, Keabetswe Madumo, Kayode Olaleye, Abiodun Modupe, Unarine Netshifhefhe, Herkulaas Combrink, Mohlatlego Nakeng, Matome Ledwaba
- **URL**: <http://arxiv.org/abs/2508.03529v1>
- **Submitted**: 2025-08-05 15:00:02
- **Comment**: Under Review
- **Topic Keywords**: rag, retrieval, search
- **Reason**: The paper focuses on building multilingual terminologies for South African languages, which is not directly related to information retrieval, search technologies, or query understanding. Although it mentions a Retrieval-Augmented Generation (RAG) pipeline, the primary focus is on terminology aggregation and standardization, rather than ranking models or user behavior modeling.

#### Abstract
> The critical lack of structured terminological data for South Africa's
official languages hampers progress in multilingual NLP, despite the existence
of numerous government and academic terminology lists. These valuable assets
remain fragmented and locked in non-machine-readable formats, rendering them
unusable for computational research and development. \emph{Marito} addresses
this challenge by systematically aggregating, cleaning, and standardising these
scattered resources into open, interoperable datasets. We introduce the
foundational \emph{Marito} dataset, released under the equitable,
Africa-centered NOODL framework. To demonstrate its immediate utility, we
integrate the terminology into a Retrieval-Augmented Generation (RAG) pipeline.
Experiments show substantial improvements in the accuracy and domain-specific
consistency of English-to-Tshivenda machine translation for large language
models. \emph{Marito} provides a scalable foundation for developing robust and
equitable NLP technologies, ensuring South Africa's rich linguistic diversity
is represented in the digital age.

### 30. Pay What LLM Wants: Can LLM Simulate Economics Experiment with 522 Real-human Persona?

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Junhyuk Choi, Hyeonchu Park, Haemin Lee, Hyebeen Shin, Hyun Joung Jin, Bugeun Kim
- **URL**: <http://arxiv.org/abs/2508.03262v1>
- **Submitted**: 2025-08-05 09:37:37
- **Comment**: Preprint
- **Topic Keywords**: retrieval augmented generation, retrieval, korea
- **Reason**: This paper is not directly related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on evaluating Large Language Models' ability to simulate human-like behaviors in economic decision-making, which is outside the user's primary focus.

#### Abstract
> Recent advances in Large Language Models (LLMs) have generated significant
interest in their capacity to simulate human-like behaviors, yet most studies
rely on fictional personas rather than actual human data. We address this
limitation by evaluating LLMs' ability to predict individual economic
decision-making using Pay-What-You-Want (PWYW) pricing experiments with real
522 human personas. Our study systematically compares three state-of-the-art
multimodal LLMs using detailed persona information from 522 Korean participants
in cultural consumption scenarios. We investigate whether LLMs can accurately
replicate individual human choices and how persona injection methods affect
prediction performance. Results reveal that while LLMs struggle with precise
individual-level predictions, they demonstrate reasonable group-level
behavioral tendencies. Also, we found that commonly adopted prompting
techniques are not much better than naive prompting methods; reconstruction of
personal narrative nor retrieval augmented generation have no significant gain
against simple prompting method. We believe that these findings can provide the
first comprehensive evaluation of LLMs' capabilities on simulating economic
behavior using real human data, offering empirical guidance for persona-based
simulation in computational social science.

### 31. Long Story Generation via Knowledge Graph and Literary Theory

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Ge Shi, Kaiyu Huang, Guochen Feng
- **URL**: <http://arxiv.org/abs/2508.03137v1>
- **Submitted**: 2025-08-05 06:35:14
- **Topic Keywords**: rag, search, acl
- **Reason**: The paper focuses on long story generation, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it mentions large language models, the context is different from the user's interests in NLP and IR. The paper's emphasis on literary theory and knowledge graph is also not relevant to the user's research areas.

#### Abstract
> The generation of a long story consisting of several thousand words is a
sub-task in the field of long text generation~(LTG). Previous research has
addressed this challenge through outline-based generation, which employs a
multi-stage method for generating outlines into stories. However, this approach
suffers from two common issues: almost inevitable theme drift caused by the
loss of memory of previous outlines, and tedious plots with incoherent logic
that are less appealing to human readers.
  In this paper, we propose the multi-agent Story Generator structure to
improve the multi-stage method, using large language models~(LLMs) as the core
components of agents. To avoid theme drift, we introduce a memory storage model
comprising two components: a long-term memory storage that identifies the most
important memories, thereby preventing theme drift; and a short-term memory
storage that retains the latest outlines from each generation round. To
incorporate engaging elements into the story, we design a story theme obstacle
framework based on literary narratology theory that introduces uncertain
factors and evaluation criteria to generate outline. This framework calculates
the similarity of the former storyline and enhances the appeal of the story by
building a knowledge graph and integrating new node content. Additionally, we
establish a multi-agent interaction stage to simulate writer-reader interaction
through dialogue and revise the story text according to feedback, to ensure it
remains consistent and logical. Evaluations against previous methods
demonstrate that our approach can generate higher-quality long stories.

### 32. Tackling Distribution Shift in LLM via KILO: Knowledge-Instructed Learning for Continual Adaptation

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Iing Muttakhiroh, Thomas Fevens
- **URL**: <http://arxiv.org/abs/2508.03571v1>
- **Submitted**: 2025-08-05 15:39:37
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on Large Language Models and continual learning, which is not directly related to the user's interests in Information Retrieval, Search technologies, and query understanding. Although it mentions knowledge retrieval, it is not in the context of query understanding or ranking models, and the paper's primary concern is adapting to domain shifts in language models, which is not a central match for the user's research themes.

#### Abstract
> Large Language Models (LLMs) often suffer from performance degradation when
faced with domain shifts, primarily due to catastrophic forgetting. In this
work, we propose KILO (Knowledge-Instructed Learning for Continual Adaptation),
a novel continual learning framework that integrates dynamic knowledge graphs
with instruction tuning. By leveraging retrieved domain-specific knowledge as
guidance during training, KILO enhances both adaptability to new domains and
retention of previously acquired knowledge. We pretrain our model on
WikiText-103 and evaluate sequential adaptation across four diverse target
domains: BioASQ, SciQ, TweetEval, and MIND. Our experiments demonstrate that
KILO consistently outperforms strong baselines, including continual
fine-tuning, ERNIE 2.0, and CPT, in terms of backward transfer, forward
transfer, F1 score, retention rate, and training efficiency. These results
highlight the effectiveness of combining structured knowledge retrieval and
instruction prompting to overcome domain shift challenges in continual learning
scenarios.

### 33. CF-RAG: A Dataset and Method for Carbon Footprint QA Using Retrieval-Augmented Generation

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Kaiwen Zhao, Bharathan Balaji, Stephen Lee
- **URL**: <http://arxiv.org/abs/2508.03489v1>
- **Submitted**: 2025-08-05 14:20:10
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on a specific domain (carbon footprint QA) and uses techniques from Natural Language Processing (NLP), but it does not directly relate to Information Retrieval (IR) or Search technologies, which are the user's primary research interests.

#### Abstract
> Product sustainability reports provide valuable insights into the
environmental impacts of a product and are often distributed in PDF format.
These reports often include a combination of tables and text, which complicates
their analysis. The lack of standardization and the variability in reporting
formats further exacerbate the difficulty of extracting and interpreting
relevant information from large volumes of documents. In this paper, we tackle
the challenge of answering questions related to carbon footprints within
sustainability reports available in PDF format. Unlike previous approaches, our
focus is on addressing the difficulties posed by the unstructured and
inconsistent nature of text extracted from PDF parsing. To facilitate this
analysis, we introduce CarbonPDF-QA, an open-source dataset containing
question-answer pairs for 1735 product report documents, along with
human-annotated answers. Our analysis shows that GPT-4o struggles to answer
questions with data inconsistencies. To address this limitation, we propose
CarbonPDF, an LLM-based technique specifically designed to answer carbon
footprint questions on such datasets. We develop CarbonPDF by fine-tuning Llama
3 with our training data. Our results show that our technique outperforms
current state-of-the-art techniques, including question-answering (QA) systems
finetuned on table and text data.

### 34. ReDSM5: A Reddit Dataset for DSM-5 Depression Detection

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Eliseo Bao, Anxo P√©rez, Javier Parapar
- **URL**: <http://arxiv.org/abs/2508.03399v1>
- **Submitted**: 2025-08-05 12:48:06
- **Comment**: Accepted as a resource paper at CIKM 2025
- **Topic Keywords**: relevance, search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus is on depression detection and symptom classification, which is outside the scope of your interests.

#### Abstract
> Depression is a pervasive mental health condition that affects hundreds of
millions of individuals worldwide, yet many cases remain undiagnosed due to
barriers in traditional clinical access and pervasive stigma. Social media
platforms, and Reddit in particular, offer rich, user-generated narratives that
can reveal early signs of depressive symptomatology. However, existing
computational approaches often label entire posts simply as depressed or not
depressed, without linking language to specific criteria from the DSM-5, the
standard clinical framework for diagnosing depression. This limits both
clinical relevance and interpretability. To address this gap, we introduce
ReDSM5, a novel Reddit corpus comprising 1484 long-form posts, each
exhaustively annotated at the sentence level by a licensed psychologist for the
nine DSM-5 depression symptoms. For each label, the annotator also provides a
concise clinical rationale grounded in DSM-5 methodology. We conduct an
exploratory analysis of the collection, examining lexical, syntactic, and
emotional patterns that characterize symptom expression in social media
narratives. Compared to prior resources, ReDSM5 uniquely combines
symptom-specific supervision with expert explanations, facilitating the
development of models that not only detect depression but also generate
human-interpretable reasoning. We establish baseline benchmarks for both
multi-label symptom classification and explanation generation, providing
reference results for future research on detection and interpretability.

### 35. Privacy-Aware Decoding: Mitigating Privacy Leakage of Large Language Models in Retrieval-Augmented Generation

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Haoran Wang, Xiongxiao Xu, Baixiang Huang, Kai Shu
- **URL**: <http://arxiv.org/abs/2508.03098v1>
- **Submitted**: 2025-08-05 05:22:13
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on privacy-aware decoding for retrieval-augmented generation, which is not directly related to the user's research interests in information retrieval, search technologies, query understanding, ranking models, and user behavior modeling. While the paper touches on retrieval, it is primarily concerned with privacy and does not address the user's core research themes.

#### Abstract
> Retrieval-Augmented Generation (RAG) enhances the factual accuracy of large
language models (LLMs) by conditioning outputs on external knowledge sources.
However, when retrieval involves private or sensitive data, RAG systems are
susceptible to extraction attacks that can leak confidential information
through generated responses. We propose Privacy-Aware Decoding (PAD), a
lightweight, inference-time defense that adaptively injects calibrated Gaussian
noise into token logits during generation. PAD integrates confidence-based
screening to selectively protect high-risk tokens, efficient sensitivity
estimation to minimize unnecessary noise, and context-aware noise calibration
to balance privacy with generation quality. A \renyi Differential Privacy (RDP)
accountant rigorously tracks cumulative privacy loss, enabling explicit
per-response $(\varepsilon, \delta)$-DP guarantees for sensitive outputs.
Unlike prior approaches requiring retraining or corpus-level filtering, PAD is
model-agnostic and operates entirely at decoding time with minimal
computational overhead. Experiments on three real-world datasets demonstrate
that PAD substantially reduces private information leakage while preserving
response utility, outperforming existing retrieval- and post-processing-based
defenses. Our work takes an important step toward mitigating privacy risks in
RAG via decoding strategies, paving the way for universal and scalable privacy
solutions in sensitive domains. Our code is available:
https://github.com/wang2226/PAD.

### 36. SustainableQA: A Comprehensive Question Answering Dataset for Corporate Sustainability and EU Taxonomy Reporting

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Mohammed Ali, Abdelrahman Abdallah, Adam Jatowt
- **URL**: <http://arxiv.org/abs/2508.03000v1>
- **Submitted**: 2025-08-05 02:03:59
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on creating a question answering dataset for corporate sustainability and EU Taxonomy reporting, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves question answering and language models, the domain-specific focus on sustainability and compliance is not aligned with the user's interests.

#### Abstract
> The growing demand for corporate sustainability transparency, particularly
under new regulations like the EU Taxonomy, necessitates precise data
extraction from large, unstructured corporate reports. Large Language Models
(LLMs) and Retrieval-Augmented Generation (RAG) systems, requires high-quality,
domain-specific question-answering (QA) datasets to excel at particular
domains. To address this, we introduce SustainableQA, a novel dataset and a
scalable pipeline for generating a comprehensive QA datasets from corporate
sustainability reports and annual reports. Our approach integrates semantic
chunk classification, a hybrid span extraction pipeline combining fine-tuned
Named Entity Recognition (NER), rule-based methods, and LLM-driven refinement,
alongside a specialized table-to-paragraph transformation. With over 195,000
diverse factoid and non-factoid QA pairs, SustainableQA is an effective
resource for developing and benchmarking advanced knowledge assistants capable
of navigating complex sustainability compliance

### 37. A Multi-Agent System for Complex Reasoning in Radiology Visual Question Answering

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Ziruo Yi, Jinyu Liu, Ting Xiao, Mark V. Albert
- **URL**: <http://arxiv.org/abs/2508.02841v1>
- **Submitted**: 2025-08-04 19:09:52
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on radiology visual question answering, which is not directly related to the user's interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper mentions multimodal large language models and retrieval-augmented generation, these concepts are not applied to the user's areas of focus, such as query understanding, ranking models, and user behavior modeling.

#### Abstract
> Radiology visual question answering (RVQA) provides precise answers to
questions about chest X-ray images, alleviating radiologists' workload. While
recent methods based on multimodal large language models (MLLMs) and
retrieval-augmented generation (RAG) have shown promising progress in RVQA,
they still face challenges in factual accuracy, hallucinations, and cross-modal
misalignment. We introduce a multi-agent system (MAS) designed to support
complex reasoning in RVQA, with specialized agents for context understanding,
multimodal reasoning, and answer validation. We evaluate our system on a
challenging RVQA set curated via model disagreement filtering, comprising
consistently hard cases across multiple MLLMs. Extensive experiments
demonstrate the superiority and effectiveness of our system over strong MLLM
baselines, with a case study illustrating its reliability and interpretability.
This work highlights the potential of multi-agent approaches to support
explainable and trustworthy clinical AI applications that require complex
reasoning.

### 38. OpenLifelogQA: An Open-Ended Multi-Modal Lifelog Question-Answering Dataset

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Quang-Linh Tran, Binh Nguyen, Gareth J. F. Jones, Cathal Gurrin
- **URL**: <http://arxiv.org/abs/2508.03583v1>
- **Submitted**: 2025-08-05 15:50:16
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on a lifelog question-answering dataset, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions a baseline experiment with a language model, the primary focus is on lifelog data and its applications, rather than on information retrieval or search.

#### Abstract
> Lifelogging refers to the process of passively collecting, storing, and
analysing personal daily life data using wearable devices. This data can
support applications in memory preservation and enhancement. For example, using
an ask-and-answer strategy, question-answering (QA) on lifelog data opens an
interactive and interesting way to explore memorable events and insights into
daily life. However, research resources for QA on lifelog data are limited to
small-sized or synthetic QA datasets. In this paper, we present a novel lifelog
QA dataset called OpenLifelogQA, building upon an 18-month lifelog dataset. Our
dataset focuses on an open-ended and practical QA with real-world application
in daily lifelog usage. We construct 14,187 pairs of Q&A with diverse types and
difficulty levels. A baseline experiment is reported for this dataset with
competitive average performance of 89.7% BERT Score, 25.87% ROUGE-L and 3.9665
LLM Score from LLaVA-NeXT-Interleave 7B model. We release this Q&A dataset to
the research community to support new research into lifelog technologies, such
as enabling personal chat-based assistants for lifelog data to become a
reality.

### 39. MoKA: Mixture of Kronecker Adapters

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Mohammadreza Sadeghi, Mahsa Ghazvini Nejad, MirHamed Jafarzadeh Asl, Yu Gu, Yuanhao Yu, Masoud Asgharian, Vahid Partovi Nia
- **URL**: <http://arxiv.org/abs/2508.03527v1>
- **Submitted**: 2025-08-05 14:58:14
- **Topic Keywords**: rag, rank
- **Reason**: The paper focuses on parameter-efficient fine-tuning of large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on adapter architectures, the topic is more relevant to NLP and model optimization, rather than user behavior modeling or ranking models.

#### Abstract
> Parameter-efficient fine-tuning (PEFT) is essential for reducing the
computational overhead of large language models (LLMs). Low-rank family
adapters are commonly used to control the parameter size efficiently while
maintaining the generative power of LLMs. However, their limited expressiveness
due to the rank constraint often restricts their performance on complex tasks.
We propose Mixture of Kronecker Adapters (MoKA), a new generation of Kronecker
adapters that addresses this limitation by modeling weight updates as a mixture
of Kronecker products. Our proposed adapter leverages a gating mechanism that
measures the importance of each Kronecker factor, enabling more expressive
adaptation. Moreover, MoKA enables a rank flexibility that provides a better
trade-off between parameter efficiency and accuracy. To ensure hardware
efficiency, we reformulate Kronecker computations using standard matrix
operations, allowing seamless deployment on GPU-optimized hardware. We conduct
extensive experiments on instruction-tuning and commonsense reasoning tasks
using low-bit quantized versions of LLaMA2-7B and LLaMA3-8B models. MoKA not
only outperforms PEFT baselines, but also reduces the number of trainable
parameters up to 27x, achieving state-of-the-art trade-offs between performance
and parameter efficiency.

### 40. Thinking with Nothinking Calibration: A New In-Context Learning Paradigm in Reasoning Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Haotian Wu, Bo Xu, Yao Shu, Menglin Yang, Chengwei Qin
- **URL**: <http://arxiv.org/abs/2508.03363v2>
- **Submitted**: 2025-08-05 12:09:55
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on large language models and their ability to reason, which is not directly related to information retrieval, search technologies, or query understanding. The concepts of thinking and no-thinking calibration are not relevant to the user's research interests in IR and NLP.

#### Abstract
> Reasoning large language models (RLLMs) have recently demonstrated remarkable
capabilities through structured and multi-step reasoning. While prior research
has primarily focused on improving their training and inference strategies,
their potential for in-context learning (ICL) remains largely underexplored. To
fill this gap, we propose Thinking with Nothinking Calibration (JointThinking),
a new ICL paradigm that leverages the structured difference between two
reasoning modes, i.e., Thinking and Nothinking, to improve reasoning accuracy.
Specifically, our method prompts the model to generate two answers in parallel:
one in Thinking mode and the other in Nothinking mode. A second round of
Thinking is triggered only when the two initial responses are inconsistent,
using a single prompt that incorporates the original question and both
candidate answers. Since such disagreement occurs infrequently (e.g., only 6\%
in GSM8K), our method performs just one round of reasoning in most cases,
resulting in minimal latency overhead. Extensive experiments across multiple
reasoning benchmarks demonstrate that JointThinking significantly outperforms
few-shot chain-of-thought (CoT) and majority voting with improved answer
robustness. Moreover, It achieves comparable in-distribution performance to
training-based SOTA method, while substantially outperforming on
out-of-distribution tasks. We further conduct a systematic analysis of the
calibration mechanism, showing that leveraging different reasoning modes
consistently lowers the error rate and highlights the value of structural
thinking diversity. Additionally, we observe that the performance gap between
actual and ideal reasoning narrows as model size increases in the second round
of thinking, indicating the strong scalability of our approach. Finally, we
discuss current limitations and outline promising directions for future ICL
research in RLLMs.

### 41. Investigating Gender Bias in LLM-Generated Stories via Psychological Stereotypes

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Shahed Masoudian, Gustavo Escobedo, Hannah Strauss, Markus Schedl
- **URL**: <http://arxiv.org/abs/2508.03292v1>
- **Submitted**: 2025-08-05 10:10:26
- **Comment**: Under Review
- **Topic Keywords**: rag, search
- **Reason**: This paper investigates gender bias in Large Language Models (LLMs) using psychological stereotypes, which is not directly related to my research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on NLP, the focus is on bias detection and mitigation in LLMs, which is not a central theme in my research.

#### Abstract
> As Large Language Models (LLMs) are increasingly used across different
applications, concerns about their potential to amplify gender biases in
various tasks are rising. Prior research has often probed gender bias using
explicit gender cues as counterfactual, or studied them in sentence completion
and short question answering tasks. These formats might overlook more implicit
forms of bias embedded in generative behavior of longer content. In this work,
we investigate gender bias in LLMs using gender stereotypes studied in
psychology (e.g., aggressiveness or gossiping) in an open-ended task of
narrative generation. We introduce a novel dataset called StereoBias-Stories
containing short stories either unconditioned or conditioned on (one, two, or
six) random attributes from 25 psychological stereotypes and three task-related
story endings. We analyze how the gender contribution in the overall story
changes in response to these attributes and present three key findings: (1)
While models, on average, are highly biased towards male in unconditioned
prompts, conditioning on attributes independent from gender stereotypes
mitigates this bias. (2) Combining multiple attributes associated with the same
gender stereotype intensifies model behavior, with male ones amplifying bias
and female ones alleviating it. (3) Model biases align with psychological
ground-truth used for categorization, and alignment strength increases with
model size. Together, these insights highlight the importance of
psychology-grounded evaluation of LLMs.

### 42. RooseBERT: A New Deal For Political Language Modelling

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Deborah Dore, Elena Cabrio, Serena Villata
- **URL**: <http://arxiv.org/abs/2508.03250v1>
- **Submitted**: 2025-08-05 09:28:20
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on political language modeling, which is not directly related to the user's interests in Information Retrieval, Search technologies, and query understanding. Although it mentions pre-training a language model, the context is specific to political discourse and does not align with the user's background in e-commerce or general NLP applications.

#### Abstract
> The increasing amount of political debates and politics-related discussions
calls for the definition of novel computational methods to automatically
analyse such content with the final goal of lightening up political
deliberation to citizens. However, the specificity of the political language
and the argumentative form of these debates (employing hidden communication
strategies and leveraging implicit arguments) make this task very challenging,
even for current general-purpose pre-trained Language Models. To address this
issue, we introduce a novel pre-trained Language Model for political discourse
language called RooseBERT. Pre-training a language model on a specialised
domain presents different technical and linguistic challenges, requiring
extensive computational resources and large-scale data. RooseBERT has been
trained on large political debate and speech corpora (8K debates, each composed
of several sub-debates on different topics) in English. To evaluate its
performances, we fine-tuned it on four downstream tasks related to political
debate analysis, i.e., named entity recognition, sentiment analysis, argument
component detection and classification, and argument relation prediction and
classification. Our results demonstrate significant improvements over
general-purpose Language Models on these four tasks, highlighting how
domain-specific pre-training enhances performance in political debate analysis.
We release the RooseBERT language model for the research community.

### 43. Beyond Content: How Grammatical Gender Shapes Visual Representation in Text-to-Image Models

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Muhammed Saeed, Shaina Raza, Ashmal Vayani, Muhammad Abdul-Mageed, Ali Emami, Shady Shehata
- **URL**: <http://arxiv.org/abs/2508.03199v1>
- **Submitted**: 2025-08-05 08:13:07
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on the impact of grammatical gender on visual representation in Text-to-Image models, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and query understanding. The paper's topic is more relevant to Natural Language Processing and multimodal systems, but the user's primary focus is on information retrieval and real-time relevance optimization, making this paper only loosely relevant.

#### Abstract
> Research on bias in Text-to-Image (T2I) models has primarily focused on
demographic representation and stereotypical attributes, overlooking a
fundamental question: how does grammatical gender influence visual
representation across languages? We introduce a cross-linguistic benchmark
examining words where grammatical gender contradicts stereotypical gender
associations (e.g., ``une sentinelle'' - grammatically feminine in French but
referring to the stereotypically masculine concept ``guard''). Our dataset
spans five gendered languages (French, Spanish, German, Italian, Russian) and
two gender-neutral control languages (English, Chinese), comprising 800 unique
prompts that generated 28,800 images across three state-of-the-art T2I models.
Our analysis reveals that grammatical gender dramatically influences image
generation: masculine grammatical markers increase male representation to 73\%
on average (compared to 22\% with gender-neutral English), while feminine
grammatical markers increase female representation to 38\% (compared to 28\% in
English). These effects vary systematically by language resource availability
and model architecture, with high-resource languages showing stronger effects.
Our findings establish that language structure itself, not just content, shapes
AI-generated visual outputs, introducing a new dimension for understanding bias
and fairness in multilingual, multimodal systems.

### 44. Realizing Scaling Laws in Recommender Systems: A Foundation-Expert Paradigm for Hyperscale Model Deployment

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Dai Li, Kevin Course, Wei Li, Hongwei Li, Jie Hua, Yiqi Chen, Zhao Zhu, Rui Jian, Xuan Cao, Bi Xue, Yu Shi, Jing Qian, Kai Ren, Matt Ma, Qunshu Zhang, Rui Li
- **URL**: <http://arxiv.org/abs/2508.02929v1>
- **Submitted**: 2025-08-04 22:03:13
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on recommender systems, which is a related topic, but it does not align with the user's primary interest in Information Retrieval and Search technologies. The paper's emphasis on scaling laws and deployment of hyperscale models is not directly relevant to the user's research themes.

#### Abstract
> While scaling laws promise significant performance gains for recommender
systems, efficiently deploying hyperscale models remains a major unsolved
challenge. In contrast to fields where FMs are already widely adopted such as
natural language processing and computer vision, progress in recommender
systems is hindered by unique challenges including the need to learn from
online streaming data under shifting data distributions, the need to adapt to
different recommendation surfaces with a wide diversity in their downstream
tasks and their input distributions, and stringent latency and computational
constraints. To bridge this gap, we propose to leverage the Foundation-Expert
Paradigm: a framework designed for the development and deployment of hyperscale
recommendation FMs. In our approach, a central FM is trained on lifelong,
cross-surface, multi-modal user data to learn generalizable knowledge. This
knowledge is then efficiently transferred to various lightweight,
surface-specific ``expert" models via target-aware embeddings, allowing them to
adapt to local data distributions and optimization goals with minimal overhead.
To meet our training, inference and development needs, we built HyperCast, a
production-grade infrastructure system that re-engineers training, serving,
logging and iteration to power this decoupled paradigm. Our approach is now
deployed at Meta serving tens of billions of user requests daily, demonstrating
online metric improvements over our previous one-stage production system while
improving developer velocity and maintaining infrastructure efficiency. To the
best of our knowledge, this work represents the first successful deployment of
a Foundation-Expert paradigm at this scale, offering a proven,
compute-efficient, and developer-friendly blueprint to realize the promise of
scaling laws in recommender systems.

### 45. Recommending With, Not For: Co-Designing Recommender Systems for Social Good

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Michael D. Ekstrand, Afsaneh Razi, Aleksandra Sarcevic, Maria Soledad Pera, Robin Burke, Katherine Landau Wright
- **URL**: <http://arxiv.org/abs/2508.03792v1>
- **Submitted**: 2025-08-05 17:50:39
- **Comment**: Accepted to ACM TORS Special Issue on Recommender Systems for Social
  Good
- **Topic Keywords**: recommend, search
- **Reason**: The paper focuses on the design and evaluation of recommender systems, but it does not address query understanding, ranking models, or user behavior modeling, which are core aspects of your research interests. The paper's emphasis on social good and participatory design processes is also not directly related to your areas of focus in Information Retrieval and Search technologies.

#### Abstract
> Recommender systems are usually designed by engineers, researchers,
designers, and other members of development teams. These systems are then
evaluated based on goals set by the aforementioned teams and other business
units of the platforms operating the recommender systems. This design approach
emphasizes the designers' vision for how the system can best serve the
interests of users, providers, businesses, and other stakeholders. Although
designers may be well-informed about user needs through user experience and
market research, they are still the arbiters of the system's design and
evaluation, with other stakeholders' interests less emphasized in user-centered
design and evaluation. When extended to recommender systems for social good,
this approach results in systems that reflect the social objectives as
envisioned by the designers and evaluated as the designers understand them.
Instead, social goals and operationalizations should be developed through
participatory and democratic processes that are accountable to their
stakeholders. We argue that recommender systems aimed at improving social good
should be designed *by* and *with*, not just *for*, the people who will
experience their benefits and harms. That is, they should be designed in
collaboration with their users, creators, and other stakeholders as full
co-designers, not only as user study participants.

### 46. AIC CTU@FEVER 8: On-premise fact checking through long context RAG

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Herbert Ullrich, Jan Drchal
- **URL**: <http://arxiv.org/abs/2508.04390v1>
- **Submitted**: 2025-08-05 14:03:43
- **Topic Keywords**: rag
- **Reason**: The paper focuses on fact-checking, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions a shared task, the context and methodology are not relevant to the user's areas of focus.

#### Abstract
> In this paper, we present our fact-checking pipeline which has scored first
in FEVER 8 shared task. Our fact-checking system is a simple two-step RAG
pipeline based on our last year's submission. We show how the pipeline can be
redeployed on-premise, achieving state-of-the-art fact-checking performance (in
sense of Ev2R test-score), even under the constraint of a single NVidia A10
GPU, 23GB of graphical memory and 60s running time per claim.

### 47. Taggus: An Automated Pipeline for the Extraction of Characters' Social Networks from Portuguese Fiction Literature

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Tiago G Can√°rio, Catarina Duarte, Fl√°vio L. Pinheiro, Jo√£o L. M. Pereira
- **URL**: <http://arxiv.org/abs/2508.03358v1>
- **Submitted**: 2025-08-05 12:03:03
- **Comment**: 24 pages, 5 Figures, 4 Tables
- **Topic Keywords**: rag
- **Reason**: The paper focuses on extracting social networks from literary fiction works in Portuguese, using NLP methods like NER and POS tagging. While it involves Natural Language Processing, it is not related to Information Retrieval, Search technologies, or query understanding, which are the user's primary research interests.

#### Abstract
> Automatically identifying characters and their interactions from fiction
books is, arguably, a complex task that requires pipelines that leverage
multiple Natural Language Processing (NLP) methods, such as Named Entity
Recognition (NER) and Part-of-speech (POS) tagging. However, these methods are
not optimized for the task that leads to the construction of Social Networks of
Characters. Indeed, the currently available methods tend to underperform,
especially in less-represented languages, due to a lack of manually annotated
data for training. Here, we propose a pipeline, which we call Taggus, to
extract social networks from literary fiction works in Portuguese. Our results
show that compared to readily available State-of-the-Art tools -- off-the-shelf
NER tools and Large Language Models (ChatGPT) -- the resulting pipeline, which
uses POS tagging and a combination of heuristics, achieves satisfying results
with an average F1-Score of $94.1\%$ in the task of identifying characters and
solving for co-reference and $75.9\%$ in interaction detection. These
represent, respectively, an increase of $50.7\%$ and $22.3\%$ on results
achieved by the readily available State-of-the-Art tools. Further steps to
improve results are outlined, such as solutions for detecting relationships
between characters. Limitations on the size and scope of our testing samples
are acknowledged. The Taggus pipeline is publicly available to encourage
development in this field for the Portuguese language.2

### 48. Analyzing German Parliamentary Speeches: A Machine Learning Approach for Topic and Sentiment Classification

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Lukas P√§tz, Moritz Beyer, Jannik Sp√§th, Lasse Bohlen, Patrick Zschech, Mathias Kraus, Julian Rosenberger
- **URL**: <http://arxiv.org/abs/2508.03181v1>
- **Submitted**: 2025-08-05 07:44:42
- **Comment**: Accepted at 20th International Conference on Wirtschaftsinformatik
  (WI25); September 2025, M\"unster, Germany
- **Topic Keywords**: rag
- **Reason**: The paper's focus on machine learning for topic and sentiment classification in German parliamentary speeches is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP techniques, the context and application are quite different from the user's areas of focus.

#### Abstract
> This study investigates political discourse in the German parliament, the
Bundestag, by analyzing approximately 28,000 parliamentary speeches from the
last five years. Two machine learning models for topic and sentiment
classification were developed and trained on a manually labeled dataset. The
models showed strong classification performance, achieving an area under the
receiver operating characteristic curve (AUROC) of 0.94 for topic
classification (average across topics) and 0.89 for sentiment classification.
Both models were applied to assess topic trends and sentiment distributions
across political parties and over time. The analysis reveals remarkable
relationships between parties and their role in parliament. In particular, a
change in style can be observed for parties moving from government to
opposition. While ideological positions matter, governing responsibilities also
shape discourse. The analysis directly addresses key questions about the
evolution of topics, sentiment dynamics, and party-specific discourse
strategies in the Bundestag.

### 49. Unified Tool Integration for LLMs: A Protocol-Agnostic Approach to Function Calling

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Peng Ding, Rick Stevens
- **URL**: <http://arxiv.org/abs/2508.02979v1>
- **Submitted**: 2025-08-05 01:06:49
- **Comment**: arXiv admin note: substantial text overlap with arXiv:2507.10593
- **Topic Keywords**: rag
- **Reason**: The paper focuses on tool integration for Large Language Models, which is not directly related to Information Retrieval, Search technologies, or query understanding. Although it mentions 'function calling', the context is not relevant to the user's interests in ranking models or user behavior modeling.

#### Abstract
> The proliferation of tool-augmented Large Language Models (LLMs) has created
a fragmented ecosystem where developers must navigate multiple protocols,
manual schema definitions, and complex execution workflows. We address this
challenge by proposing a unified approach to tool integration that abstracts
protocol differences while optimizing execution performance. Our solution
demonstrates how protocol-agnostic design principles can significantly reduce
development overhead through automated schema generation, dual-mode concurrent
execution, and seamless multi-source tool management. Experimental results show
60-80% code reduction across integration scenarios, performance improvements up
to 3.1x through optimized concurrency, and full compatibility with existing
function calling standards. This work contributes both theoretical insights
into tool integration architecture and practical solutions for real-world LLM
application development.

### 50. Investigating the Cognitive Response of Brake Lights in Initiating Braking Action Using EEG

- **LLM Score**: 0
- **Keyword Score**: 2
- **Authors**: Ramaswamy Palaniappan, Surej Mouli, Howard Bowman, Ian McLoughlin
- **URL**: <http://arxiv.org/abs/2508.03274v1>
- **Submitted**: 2025-08-05 09:52:53
- **Comment**: arXiv admin note: text overlap with arXiv:2010.10584
- **Topic Keywords**: ctr
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, Natural Language Processing, data mining, or recommender systems. The topic of brake lights and cognitive response is outside the scope of your research areas.

#### Abstract
> Half of all road accidents result from either lack of driver attention or
from maintaining insufficient separation between vehicles. Collision from the
rear, in particular, has been identified as the most common class of accident
in the UK, and its influencing factors have been widely studied for many years.
Rear-mounted stop lamps, illuminated when braking, are the primary mechanism to
alert following drivers to the need to reduce speed or brake. This paper
develops a novel brain response approach to measuring subject reaction to
different brake light designs. A variety of off-the-shelf brake light
assemblies are tested in a physical simulated driving environment to assess the
cognitive reaction times of 22 subjects. Eight pairs of LED-based and two pairs
of incandescent bulb-based brake light assemblies are used and
electroencephalogram (EEG) data recorded. Channel Pz is utilised to extract the
P3 component evoked during the decision making process that occurs in the brain
when a participant decides to lift their foot from the accelerator and depress
the brake. EEG analysis shows that both incandescent bulb-based lights are
statistically slower to evoke cognitive responses than all tested LED-based
lights. Between the LED designs, differences are evident, but not statistically
significant, attributed to the significant amount of movement artifact in the
EEG signal.

---

