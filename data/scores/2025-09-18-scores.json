[
    {
        "title": "Mind the Gap: Aligning Knowledge Bases with User Needs to Enhance Mental Health Retrieval",
        "abstract": "Access to reliable mental health information is vital for early help-seeking,\nyet expanding knowledge bases is resource-intensive and often misaligned with\nuser needs. This results in poor performance of retrieval systems when\npresented concerns are not covered or expressed in informal or contextualized\nlanguage. We present an AI-based gap-informed framework for corpus augmentation\nthat authentically identifies underrepresented topics (gaps) by overlaying\nnaturalistic user data such as forum posts in order to prioritize expansions\nbased on coverage and usefulness. In a case study, we compare Directed\n(gap-informed augmentations) with Non-Directed augmentation (random additions),\nevaluating the relevance and usefulness of retrieved information across four\nretrieval-augmented generation (RAG) pipelines. Directed augmentation achieved\nnear-optimal performance with modest expansions--requiring only a 42% increase\nfor Query Transformation, 74% for Reranking and Hierarchical, and 318% for\nBaseline--to reach ~95% of the performance of an exhaustive reference corpus.\nIn contrast, Non-Directed augmentation required substantially larger and thus\npractically infeasible expansions to achieve comparable performance (232%,\n318%, 403%, and 763%, respectively). These results show that strategically\ntargeted corpus growth can reduce content creation demands while sustaining\nhigh retrieval and provision quality, offering a scalable approach for building\ntrusted health information repositories and supporting generative AI\napplications in high-stakes domains.",
        "url": "http://arxiv.org/abs/2509.13626v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13626v1",
        "arxiv_id": "2509.13626v1",
        "authors": [
            "Amanda Chan",
            "James Jiayu Liu",
            "He Kai",
            "Onno P. Kampman"
        ],
        "submitted": "2025-09-17 01:54:11",
        "source": "arxiv",
        "comment": "25 pages, 3 figures, submitted to NeurIPS 2025 GenAI4Health",
        "score": 17,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 7,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of query understanding and relevance optimization. Although it focuses on mental health retrieval and knowledge base augmentation, the proposed framework and evaluation metrics are relevant to your work in search technologies and ranking models. However, the specific domain and application area are not directly aligned with your core research themes."
    },
    {
        "title": "MA-DPR: Manifold-aware Distance Metrics for Dense Passage Retrieval",
        "abstract": "Dense Passage Retrieval (DPR) typically relies on Euclidean or cosine\ndistance to measure query-passage relevance in embedding space, which is\neffective when embeddings lie on a linear manifold. However, our experiments\nacross DPR benchmarks suggest that embeddings often lie on lower-dimensional,\nnon-linear manifolds, especially in out-of-distribution (OOD) settings, where\ncosine and Euclidean distance fail to capture semantic similarity. To address\nthis limitation, we propose a manifold-aware distance metric for DPR (MA-DPR)\nthat models the intrinsic manifold structure of passages using a nearest\nneighbor graph and measures query-passage distance based on their shortest path\nin this graph. We show that MA-DPR outperforms Euclidean and cosine distances\nby up to 26% on OOD passage retrieval with comparable in-distribution\nperformance across various embedding models while incurring a minimal increase\nin query inference time. Empirical evidence suggests that manifold-aware\ndistance allows DPR to leverage context from related neighboring passages,\nmaking it effective even in the absence of direct semantic overlap. MADPR can\nbe applied to a wide range of dense embedding and retrieval tasks, offering\npotential benefits across a wide spectrum of domains.",
        "url": "http://arxiv.org/abs/2509.13562v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13562v1",
        "arxiv_id": "2509.13562v1",
        "authors": [
            "Yifan Liu",
            "Qianfeng Wen",
            "Mark Zhao",
            "Jiazhou Liang",
            "Scott Sanner"
        ],
        "submitted": "2025-09-16 22:02:56",
        "source": "arxiv",
        "comment": "19 pages, 8 figures",
        "score": 15,
        "keyword_reasons": [
            "Found 'passage retrieval' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'ctr' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper proposes a novel distance metric for Dense Passage Retrieval (DPR), addressing limitations of traditional metrics in capturing semantic similarity, particularly in out-of-distribution settings. The focus on improving DPR, a key area in Information Retrieval, aligns with your research interests. However, the specific application to DPR and dense embedding tasks is somewhat narrow compared to your broader interests in IR, NLP, and related topics."
    },
    {
        "title": "Who Taught the Lie? Responsibility Attribution for Poisoned Knowledge in Retrieval-Augmented Generation",
        "abstract": "Retrieval-Augmented Generation (RAG) integrates external knowledge into large\nlanguage models to improve response quality. However, recent work has shown\nthat RAG systems are highly vulnerable to poisoning attacks, where malicious\ntexts are inserted into the knowledge database to influence model outputs.\nWhile several defenses have been proposed, they are often circumvented by more\nadaptive or sophisticated attacks.\n  This paper presents RAGOrigin, a black-box responsibility attribution\nframework designed to identify which texts in the knowledge database are\nresponsible for misleading or incorrect generations. Our method constructs a\nfocused attribution scope tailored to each misgeneration event and assigns a\nresponsibility score to each candidate text by evaluating its retrieval\nranking, semantic relevance, and influence on the generated response. The\nsystem then isolates poisoned texts using an unsupervised clustering method. We\nevaluate RAGOrigin across seven datasets and fifteen poisoning attacks,\nincluding newly developed adaptive poisoning strategies and multi-attacker\nscenarios. Our approach outperforms existing baselines in identifying poisoned\ncontent and remains robust under dynamic and noisy conditions. These results\nsuggest that RAGOrigin provides a practical and effective solution for tracing\nthe origins of corrupted knowledge in RAG systems.",
        "url": "http://arxiv.org/abs/2509.13772v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13772v1",
        "arxiv_id": "2509.13772v1",
        "authors": [
            "Baolei Zhang",
            "Haoran Xin",
            "Yuxi Chen",
            "Zhuqing Liu",
            "Biao Yi",
            "Tong Li",
            "Lihai Nie",
            "Zheli Liu",
            "Minghong Fang"
        ],
        "submitted": "2025-09-17 07:38:54",
        "source": "arxiv",
        "comment": "To appear in the IEEE Symposium on Security and Privacy, 2026",
        "score": 11,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to information retrieval, but its focus on Retrieval-Augmented Generation (RAG) and responsibility attribution for poisoned knowledge is not directly aligned with the user's core research themes of query understanding, ranking models, and user behavior modeling. While it touches on semantic relevance, the context is specific to RAG systems and not a central match for the user's interests."
    },
    {
        "title": "Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG",
        "abstract": "Multilingual Retrieval-Augmented Generation (mRAG) systems enable language\nmodels to answer knowledge-intensive queries with citation-supported responses\nacross languages. While such systems have been proposed, an open questions is\nwhether the mixture of different document languages impacts generation and\ncitation in unintended ways. To investigate, we introduce a controlled\nmethodology using model internals to measure language preference while holding\nother factors such as document relevance constant. Across eight languages and\nsix open-weight models, we find that models preferentially cite English sources\nwhen queries are in English, with this bias amplified for lower-resource\nlanguages and for documents positioned mid-context. Crucially, we find that\nmodels sometimes trade-off document relevance for language preference,\nindicating that citation choices are not always driven by informativeness\nalone. Our findings shed light on how language models leverage multilingual\ncontext and influence citation behavior.",
        "url": "http://arxiv.org/abs/2509.13930v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13930v1",
        "arxiv_id": "2509.13930v1",
        "authors": [
            "Dayeon Ki",
            "Marine Carpuat",
            "Paul McNamee",
            "Daniel Khashabi",
            "Eugene Yang",
            "Dawn Lawrie",
            "Kevin Duh"
        ],
        "submitted": "2025-09-17 12:58:18",
        "source": "arxiv",
        "comment": "33 pages, 20 figures",
        "score": 10,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper explores the impact of language preference on citation behavior in multilingual Retrieval-Augmented Generation (RAG) systems, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on language preference and citation behavior is not directly aligned with the user's primary research themes. The paper's findings on language models' behavior may have implications for user behavior modeling, but the connection is not immediately clear."
    },
    {
        "title": "Modernizing Facebook Scoped Search: Keyword and Embedding Hybrid Retrieval with LLM Evaluation",
        "abstract": "Beyond general web-scale search, social network search uniquely enables users\nto retrieve information and discover potential connections within their social\ncontext. We introduce a framework of modernized Facebook Group Scoped Search by\nblending traditional keyword-based retrieval with embedding-based retrieval\n(EBR) to improve the search relevance and diversity of search results. Our\nsystem integrates semantic retrieval into the existing keyword search pipeline,\nenabling users to discover more contextually relevant group posts. To\nrigorously assess the impact of this blended approach, we introduce a novel\nevaluation framework that leverages large language models (LLMs) to perform\noffline relevance assessments, providing scalable and consistent quality\nbenchmarks. Our results demonstrate that the blended retrieval system\nsignificantly enhances user engagement and search quality, as validated by both\nonline metrics and LLM-based evaluation. This work offers practical insights\nfor deploying and evaluating advanced retrieval systems in large-scale,\nreal-world social platforms.",
        "url": "http://arxiv.org/abs/2509.13603v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13603v1",
        "arxiv_id": "2509.13603v1",
        "authors": [
            "Yongye Su",
            "Zeya Zhang",
            "Jane Kou",
            "Cheng Ju",
            "Shubhojeet Sarkar",
            "Yamin Wang",
            "Ji Liu",
            "Shengbo Guo"
        ],
        "submitted": "2025-09-17 00:22:08",
        "source": "arxiv",
        "comment": "5 Pages, work done as Yongye Su's internship project at Meta",
        "score": 8,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of social network search and blending traditional keyword-based retrieval with embedding-based retrieval. The use of large language models for evaluation is also aligned with your interests in query understanding and ranking models. The focus on real-world social platforms is a nice extension of your e-commerce background."
    },
    {
        "title": "Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages",
        "abstract": "This work sets the ground for studying how explicit grammatical gender\nassignment in job titles can affect the results of automatic job ranking\nsystems. We propose the usage of metrics for ranking comparison controlling for\ngender to evaluate gender bias in job title ranking systems, in particular RBO\n(Rank-Biased Overlap). We generate and share test sets for a job title matching\ntask in four grammatical gender languages, including occupations in masculine\nand feminine form and annotated by gender and matching relevance. We use the\nnew test sets and the proposed methodology to evaluate the gender bias of\nseveral out-of-the-box multilingual models to set as baselines, showing that\nall of them exhibit varying degrees of gender bias.",
        "url": "http://arxiv.org/abs/2509.13803v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13803v1",
        "arxiv_id": "2509.13803v1",
        "authors": [
            "Laura García-Sardiña",
            "Hermenegildo Fabregat",
            "Daniel Deniz",
            "Rabih Zbib"
        ],
        "submitted": "2025-09-17 08:17:28",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on measuring gender bias in job title matching for grammatical gender languages, which is not directly related to your core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves ranking systems, it's more focused on bias detection rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "GEM-Bench: A Benchmark for Ad-Injected Response Generation within Generative Engine Marketing",
        "abstract": "Generative Engine Marketing (GEM) is an emerging ecosystem for monetizing\ngenerative engines, such as LLM-based chatbots, by seamlessly integrating\nrelevant advertisements into their responses. At the core of GEM lies the\ngeneration and evaluation of ad-injected responses. However, existing\nbenchmarks are not specifically designed for this purpose, which limits future\nresearch. To address this gap, we propose GEM-Bench, the first comprehensive\nbenchmark for ad-injected response generation in GEM. GEM-Bench includes three\ncurated datasets covering both chatbot and search scenarios, a metric ontology\nthat captures multiple dimensions of user satisfaction and engagement, and\nseveral baseline solutions implemented within an extensible multi-agent\nframework. Our preliminary results indicate that, while simple prompt-based\nmethods achieve reasonable engagement such as click-through rate, they often\nreduce user satisfaction. In contrast, approaches that insert ads based on\npre-generated ad-free responses help mitigate this issue but introduce\nadditional overhead. These findings highlight the need for future research on\ndesigning more effective and efficient solutions for generating ad-injected\nresponses in GEM.",
        "url": "http://arxiv.org/abs/2509.14221v1",
        "pdf_url": "http://arxiv.org/pdf/2509.14221v1",
        "arxiv_id": "2509.14221v1",
        "authors": [
            "Silan Hu",
            "Shiqi Zhang",
            "Yimin Shi",
            "Xiaokui Xiao"
        ],
        "submitted": "2025-09-17 17:53:43",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'click' (score: +2)",
            "Found 'click-through rate' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your core research themes in Information Retrieval and Search technologies, as it focuses on Generative Engine Marketing and ad-injected response generation. While it involves some aspects of user behavior modeling, it is primarily concerned with a specific application of AI, rather than the underlying IR and NLP concepts."
    },
    {
        "title": "Enhancing Time Awareness in Generative Recommendation",
        "abstract": "Generative recommendation has emerged as a promising paradigm that formulates\nthe recommendations into a text-to-text generation task, harnessing the vast\nknowledge of large language models. However, existing studies focus on\nconsidering the sequential order of items and neglect to handle the temporal\ndynamics across items, which can imply evolving user preferences. To address\nthis limitation, we propose a novel model, Generative Recommender Using Time\nawareness (GRUT), effectively capturing hidden user preferences via various\ntemporal signals. We first introduce Time-aware Prompting, consisting of two\nkey contexts. The user-level temporal context models personalized temporal\npatterns across timestamps and time intervals, while the item-level transition\ncontext provides transition patterns across users. We also devise Trend-aware\nInference, a training-free method that enhances rankings by incorporating trend\ninformation about items with generation likelihood. Extensive experiments\ndemonstrate that GRUT outperforms state-of-the-art models, with gains of up to\n15.4% and 14.3% in Recall@5 and NDCG@5 across four benchmark datasets. The\nsource code is available at https://github.com/skleee/GRUT.",
        "url": "http://arxiv.org/abs/2509.13957v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13957v1",
        "arxiv_id": "2509.13957v1",
        "authors": [
            "Sunkyung Lee",
            "Seongmin Park",
            "Jonghyo Kim",
            "Mincheol Yoon",
            "Jongwuk Lee"
        ],
        "submitted": "2025-09-17 13:28:46",
        "source": "arxiv",
        "comment": "EMNLP 2025 (Findings)",
        "score": 5,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses a novel model for generative recommendation, focusing on temporal dynamics and user preferences. While it touches on aspects of information retrieval, such as ranking and user behavior modeling, its primary focus is on recommender systems, which is a secondary interest of yours. The paper's emphasis on deep semantic understanding and real-time relevance optimization is not a central theme."
    },
    {
        "title": "Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality",
        "abstract": "Machine Translation (MT) has achieved remarkable performance, with growing\ninterest in speech translation and multimodal approaches. However, despite\nthese advancements, MT quality assessment remains largely text centric,\ntypically relying on human experts who read and compare texts. Since many\nreal-world MT applications (e.g Google Translate Voice Mode, iFLYTEK\nTranslator) involve translation being spoken rather printed or read, a more\nnatural way to assess translation quality would be through speech as opposed\ntext-only evaluations. This study compares text-only and audio-based\nevaluations of 10 MT systems from the WMT General MT Shared Task, using\ncrowd-sourced judgments collected via Amazon Mechanical Turk. We additionally,\nperformed statistical significance testing and self-replication experiments to\ntest reliability and consistency of audio-based approach. Crowd-sourced\nassessments based on audio yield rankings largely consistent with text only\nevaluations but, in some cases, identify significant differences between\ntranslation systems. We attribute this to speech richer, more natural modality\nand propose incorporating speech-based assessments into future MT evaluation\nframeworks.",
        "url": "http://arxiv.org/abs/2509.14023v1",
        "pdf_url": "http://arxiv.org/pdf/2509.14023v1",
        "arxiv_id": "2509.14023v1",
        "authors": [
            "Sami Ul Haq",
            "Sheila Castilho",
            "Yvette Graham"
        ],
        "submitted": "2025-09-17 14:27:17",
        "source": "arxiv",
        "comment": "Accepted at WMT2025 (ENNLP) for oral presented",
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves machine translation, a related topic, the focus on audio-based evaluation and crowd-sourced judgments is not central to your areas of expertise."
    },
    {
        "title": "DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models",
        "abstract": "Large Language Model (LLM) hallucination is a significant barrier to their\nreliable deployment. Current methods like Retrieval-Augmented Generation (RAG)\nare often reactive. We introduce **Dynamic Self-reinforcing Calibration for\nHallucination Suppression (DSCC-HS)**, a novel, proactive framework that\nintervenes during autoregressive decoding. Inspired by dual-process cognitive\ntheory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a\nFactual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During\ninference, these proxies dynamically steer a large target model by injecting a\nreal-time steering vector, which is the difference between FAP and HDP logits,\nat each decoding step. This plug-and-play approach requires no modification to\nthe target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS\nachieves state-of-the-art performance. On TruthfulQA, it reached a 99.2%\nFactual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained\nthe highest FActScore of 46.50. These results validate DSCC-HS as a principled\nand efficient solution for enhancing LLM factuality.",
        "url": "http://arxiv.org/abs/2509.13702v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13702v1",
        "arxiv_id": "2509.13702v1",
        "authors": [
            "Xiao Zheng"
        ],
        "submitted": "2025-09-17 05:09:22",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Large Language Models and hallucination suppression, which is not directly related to your primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves NLP, the specific topic and application are not aligned with your core themes."
    },
    {
        "title": "Enhancing Multi-Agent Debate System Performance via Confidence Expression",
        "abstract": "Generative Large Language Models (LLMs) have demonstrated remarkable\nperformance across a wide range of tasks. Recent research has introduced\nMulti-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate\nhuman debate and thereby improve task performance. However, while some LLMs may\npossess superior knowledge or reasoning capabilities for specific tasks, they\noften struggle to clearly communicate this advantage during debates, in part\ndue to a lack of confidence expression. Moreover, inappropriate confidence\nexpression can cause agents in MAD systems to either stubbornly maintain\nincorrect beliefs or converge prematurely on suboptimal answers, ultimately\nreducing debate effectiveness and overall system performance. To address these\nchallenges, we propose incorporating confidence expression into MAD systems to\nallow LLMs to explicitly communicate their confidence levels. To validate this\napproach, we develop ConfMAD, a MAD framework that integrates confidence\nexpression throughout the debate process. Experimental results demonstrate the\neffectiveness of our method, and we further analyze how confidence influences\ndebate dynamics, offering insights into the design of confidence-aware MAD\nsystems.",
        "url": "http://arxiv.org/abs/2509.14034v1",
        "pdf_url": "http://arxiv.org/pdf/2509.14034v1",
        "arxiv_id": "2509.14034v1",
        "authors": [
            "Zijie Lin",
            "Bryan Hooi"
        ],
        "submitted": "2025-09-17 14:34:27",
        "source": "arxiv",
        "comment": "EMNLP'25 Findings",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves Large Language Models, the focus is on Multi-Agent Debate systems and confidence expression, which is not a central match to your research themes."
    },
    {
        "title": "MIRA: Empowering One-Touch AI Services on Smartphones with MLLM-based Instruction Recommendation",
        "abstract": "The rapid advancement of generative AI technologies is driving the\nintegration of diverse AI-powered services into smartphones, transforming how\nusers interact with their devices. To simplify access to predefined AI\nservices, this paper introduces MIRA, a pioneering framework for task\ninstruction recommendation that enables intuitive one-touch AI tasking on\nsmartphones. With MIRA, users can long-press on images or text objects to\nreceive contextually relevant instruction recommendations for executing AI\ntasks. Our work introduces three key innovations: 1) A multimodal large\nlanguage model (MLLM)-based recommendation pipeline with structured reasoning\nto extract key entities, infer user intent, and generate precise instructions;\n2) A template-augmented reasoning mechanism that integrates high-level\nreasoning templates, enhancing task inference accuracy; 3) A prefix-tree-based\nconstrained decoding strategy that restricts outputs to predefined instruction\ncandidates, ensuring coherent and intent-aligned suggestions. Through\nevaluation using a real-world annotated datasets and a user study, MIRA has\ndemonstrated substantial improvements in the accuracy of instruction\nrecommendation. The encouraging results highlight MIRA's potential to\nrevolutionize the way users engage with AI services on their smartphones,\noffering a more seamless and efficient experience.",
        "url": "http://arxiv.org/abs/2509.13773v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13773v1",
        "arxiv_id": "2509.13773v1",
        "authors": [
            "Zhipeng Bian",
            "Jieming Zhu",
            "Xuyang Xie",
            "Quanyu Dai",
            "Zhou Zhao",
            "Zhenhua Dong"
        ],
        "submitted": "2025-09-17 07:43:14",
        "source": "arxiv",
        "comment": "Published in Proceedings of the 63rd Annual Meeting of the\n  Association for Computational Linguistics (Volume 6: Industry Track), ACL\n  2025. Official version: https://doi.org/10.18653/v1/2025.acl-industry.103",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper MIRA introduces a framework for task instruction recommendation on smartphones, leveraging multimodal large language models for contextually relevant suggestions. While it touches on aspects of query understanding and user behavior modeling, its primary focus is on AI service integration and instruction recommendation, which is somewhat related to information retrieval but not a central match for your research interests."
    },
    {
        "title": "Improving Context Fidelity via Native Retrieval-Augmented Reasoning",
        "abstract": "Large language models (LLMs) often struggle with context fidelity, producing\ninconsistent answers when responding to questions based on provided\ninformation. Existing approaches either rely on expensive supervised\nfine-tuning to generate evidence post-answer or train models to perform web\nsearches without necessarily improving utilization of the given context. We\npropose CARE, a novel native retrieval-augmented reasoning framework that\nteaches LLMs to explicitly integrate in-context evidence within their reasoning\nprocess with the model's own retrieval capabilities. Our method requires\nlimited labeled evidence data while significantly enhancing both retrieval\naccuracy and answer generation performance through strategically retrieved\nin-context tokens in the reasoning chain. Extensive experiments on multiple\nreal-world and counterfactual QA benchmarks demonstrate that our approach\nsubstantially outperforms supervised fine-tuning, traditional\nretrieval-augmented generation methods, and external retrieval solutions. This\nwork represents a fundamental advancement in making LLMs more accurate,\nreliable, and efficient for knowledge-intensive tasks.",
        "url": "http://arxiv.org/abs/2509.13683v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13683v1",
        "arxiv_id": "2509.13683v1",
        "authors": [
            "Suyuchen Wang",
            "Jinlin Wang",
            "Xinyu Wang",
            "Shiqi Li",
            "Xiangru Tang",
            "Sirui Hong",
            "Xiao-Wen Chang",
            "Chenglin Wu",
            "Bang Liu"
        ],
        "submitted": "2025-09-17 04:28:07",
        "source": "arxiv",
        "comment": "Accepted as a main conference paper at EMNLP 2025",
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper explores the intersection of query understanding and ranking models, specifically in the context of large language models (LLMs). The proposed CARE framework aims to improve context fidelity and answer generation performance, which is relevant to information retrieval and NLP. While the focus is on QA benchmarks, the underlying concepts and techniques are applicable to broader search technologies."
    },
    {
        "title": "A Simple and Efficient Jailbreak Method Exploiting LLMs' Helpfulness",
        "abstract": "Safety alignment aims to prevent Large Language Models (LLMs) from responding\nto harmful queries. To strengthen safety protections, jailbreak methods are\ndeveloped to simulate malicious attacks and uncover vulnerabilities. In this\npaper, we introduce HILL (Hiding Intention by Learning from LLMs), a novel\njailbreak approach that systematically transforms imperative harmful requests\ninto learning-style questions with only straightforward hypotheticality\nindicators. Further, we introduce two new metrics to thoroughly evaluate the\nutility of jailbreak methods. Experiments on the AdvBench dataset across a wide\nrange of models demonstrate HILL's strong effectiveness, generalizability, and\nharmfulness. It achieves top attack success rates on the majority of models and\nacross malicious categories while maintaining high efficiency with concise\nprompts. Results of various defense methods show the robustness of HILL, with\nmost defenses having mediocre effects or even increasing the attack success\nrates. Moreover, the assessment on our constructed safe prompts reveals\ninherent limitations of LLMs' safety mechanisms and flaws in defense methods.\nThis work exposes significant vulnerabilities of safety measures against\nlearning-style elicitation, highlighting a critical challenge of balancing\nhelpfulness and safety alignments.",
        "url": "http://arxiv.org/abs/2509.14297v1",
        "pdf_url": "http://arxiv.org/pdf/2509.14297v1",
        "arxiv_id": "2509.14297v1",
        "authors": [
            "Xuan Luo",
            "Yue Wang",
            "Zefeng He",
            "Geng Tu",
            "Jing Li",
            "Ruifeng Xu"
        ],
        "submitted": "2025-09-17 04:21:20",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on Large Language Models (LLMs) and safety alignment, which is outside your primary areas of interest in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on 'jailbreak methods' and 'safety protections' does not align with your research themes of query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs",
        "abstract": "Gender-neutral rewriting (GNR) aims to reformulate text to eliminate\nunnecessary gender specifications while preserving meaning, a particularly\nchallenging task in grammatical-gender languages like Italian. In this work, we\nconduct the first systematic evaluation of state-of-the-art large language\nmodels (LLMs) for Italian GNR, introducing a two-dimensional framework that\nmeasures both neutrality and semantic fidelity to the input. We compare\nfew-shot prompting across multiple LLMs, fine-tune selected models, and apply\ntargeted cleaning to boost task relevance. Our findings show that open-weight\nLLMs outperform the only existing model dedicated to GNR in Italian, whereas\nour fine-tuned models match or exceed the best open-weight LLM's performance at\na fraction of its size. Finally, we discuss the trade-off between optimizing\nthe training data for neutrality and meaning preservation.",
        "url": "http://arxiv.org/abs/2509.13480v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13480v1",
        "arxiv_id": "2509.13480v1",
        "authors": [
            "Andrea Piergentili",
            "Beatrice Savoldi",
            "Matteo Negri",
            "Luisa Bentivogli"
        ],
        "submitted": "2025-09-16 19:25:13",
        "source": "arxiv",
        "comment": "Accepted at CLiC-it 2025",
        "score": 3,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on gender-neutral rewriting in Italian using large language models, which is a topic in Natural Language Processing (NLP). However, it does not align with the user's primary research interests in Information Retrieval (IR), query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Apertus: Democratizing Open and Compliant LLMs for Global Language Environments",
        "abstract": "We present Apertus, a fully open suite of large language models (LLMs)\ndesigned to address two systemic shortcomings in today's open model ecosystem:\ndata compliance and multilingual representation. Unlike many prior models that\nrelease weights without reproducible data pipelines or regard for content-owner\nrights, Apertus models are pretrained exclusively on openly available data,\nretroactively respecting robots.txt exclusions and filtering for\nnon-permissive, toxic, and personally identifiable content. To mitigate risks\nof memorization, we adopt the Goldfish objective during pretraining, strongly\nsuppressing verbatim recall of data while retaining downstream task\nperformance. The Apertus models also expand multilingual coverage, training on\n15T tokens from over 1800 languages, with ~40% of pretraining data allocated to\nnon-English content. Released at 8B and 70B scales, Apertus approaches\nstate-of-the-art results among fully open models on multilingual benchmarks,\nrivalling or surpassing open-weight counterparts. Beyond model weights, we\nrelease all scientific artifacts from our development cycle with a permissive\nlicense, including data preparation scripts, checkpoints, evaluation suites,\nand training code, enabling transparent audit and extension.",
        "url": "http://arxiv.org/abs/2509.14233v1",
        "pdf_url": "http://arxiv.org/pdf/2509.14233v1",
        "arxiv_id": "2509.14233v1",
        "authors": [
            "Alejandro Hernández-Cano",
            "Alexander Hägele",
            "Allen Hao Huang",
            "Angelika Romanou",
            "Antoni-Joan Solergibert",
            "Barna Pasztor",
            "Bettina Messmer",
            "Dhia Garbaya",
            "Eduard Frank Ďurech",
            "Ido Hakimi",
            "Juan García Giraldo",
            "Mete Ismayilzada",
            "Negar Foroutan",
            "Skander Moalla",
            "Tiancheng Chen",
            "Vinko Sabolčec",
            "Yixuan Xu",
            "Michael Aerni",
            "Badr AlKhamissi",
            "Ines Altemir Marinas",
            "Mohammad Hossein Amani",
            "Matin Ansaripour",
            "Ilia Badanin",
            "Harold Benoit",
            "Emanuela Boros",
            "Nicholas Browning",
            "Fabian Bösch",
            "Maximilian Böther",
            "Niklas Canova",
            "Camille Challier",
            "Clement Charmillot",
            "Jonathan Coles",
            "Jan Deriu",
            "Arnout Devos",
            "Lukas Drescher",
            "Daniil Dzenhaliou",
            "Maud Ehrmann",
            "Dongyang Fan",
            "Simin Fan",
            "Silin Gao",
            "Miguel Gila",
            "María Grandury",
            "Diba Hashemi",
            "Alexander Hoyle",
            "Jiaming Jiang",
            "Mark Klein",
            "Andrei Kucharavy",
            "Anastasiia Kucherenko",
            "Frederike Lübeck",
            "Roman Machacek",
            "Theofilos Manitaras",
            "Andreas Marfurt",
            "Kyle Matoba",
            "Simon Matrenok",
            "Henrique Mendoncça",
            "Fawzi Roberto Mohamed",
            "Syrielle Montariol",
            "Luca Mouchel",
            "Sven Najem-Meyer",
            "Jingwei Ni",
            "Gennaro Oliva",
            "Matteo Pagliardini",
            "Elia Palme",
            "Andrei Panferov",
            "Léo Paoletti",
            "Marco Passerini",
            "Ivan Pavlov",
            "Auguste Poiroux",
            "Kaustubh Ponkshe",
            "Nathan Ranchin",
            "Javi Rando",
            "Mathieu Sauser",
            "Jakhongir Saydaliev",
            "Muhammad Ali Sayfiddinov",
            "Marian Schneider",
            "Stefano Schuppli",
            "Marco Scialanga",
            "Andrei Semenov",
            "Kumar Shridhar",
            "Raghav Singhal",
            "Anna Sotnikova",
            "Alexander Sternfeld",
            "Ayush Kumar Tarun",
            "Paul Teiletche",
            "Jannis Vamvas",
            "Xiaozhe Yao",
            "Hao Zhao Alexander Ilic",
            "Ana Klimovic",
            "Andreas Krause",
            "Caglar Gulcehre",
            "David Rosenthal",
            "Elliott Ash",
            "Florian Tramèr",
            "Joost VandeVondele",
            "Livio Veraldi",
            "Martin Rajman",
            "Thomas Schulthess",
            "Torsten Hoefler",
            "Antoine Bosselut",
            "Martin Jaggi",
            "Imanol Schlag"
        ],
        "submitted": "2025-09-17 17:59:21",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 3,
        "llm_reason": "This paper is loosely relevant to your research interests in Natural Language Processing (NLP) and large language models, but it does not directly address your core focus on Information Retrieval (IR), query understanding, or ranking models. The paper's emphasis on open and compliant LLMs for global language environments is somewhat tangential to your interests in e-commerce and real-time relevance optimization."
    },
    {
        "title": "Language models' activations linearly encode training-order recency",
        "abstract": "We show that language models' activations linearly encode when information\nwas learned during training. Our setup involves creating a model with a known\ntraining order by sequentially fine-tuning Llama-3.2-1B on six disjoint but\notherwise similar datasets about named entities. We find that the average\nactivations of test samples for the six training datasets encode the training\norder: when projected into a 2D subspace, these centroids are arranged exactly\nin the order of training and lie on a straight line. Further, we show that\nlinear probes can accurately (~90%) distinguish \"early\" vs. \"late\" entities,\ngeneralizing to entities unseen during the probes' own training. The model can\nalso be fine-tuned to explicitly report an unseen entity's training stage (~80%\naccuracy). Interestingly, this temporal signal does not seem attributable to\nsimple differences in activation magnitudes, losses, or model confidence. Our\npaper demonstrates that models are capable of differentiating information by\nits acquisition time, and carries significant implications for how they might\nmanage conflicting data and respond to knowledge modifications.",
        "url": "http://arxiv.org/abs/2509.14223v1",
        "pdf_url": "http://arxiv.org/pdf/2509.14223v1",
        "arxiv_id": "2509.14223v1",
        "authors": [
            "Dmitrii Krasheninnikov",
            "Richard E. Turner",
            "David Krueger"
        ],
        "submitted": "2025-09-17 17:54:22",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves language models, the focus is on understanding how models encode training-order recency, which is not a central theme in your research."
    },
    {
        "title": "Reasoning Efficiently Through Adaptive Chain-of-Thought Compression: A Self-Optimizing Framework",
        "abstract": "Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by\nprompting intermediate steps, improving accuracy and robustness in arithmetic,\nlogic, and commonsense tasks. However, this benefit comes with high\ncomputational costs: longer outputs increase latency, memory usage, and\nKV-cache demands. These issues are especially critical in software engineering\ntasks where concise and deterministic outputs are required. To investigate\nthese trade-offs, we conduct an empirical study based on code generation\nbenchmarks. The results reveal that longer CoT does not always help. Excessive\nreasoning often causes truncation, accuracy drops, and latency up to five times\nhigher, with failed outputs consistently longer than successful ones. These\nfindings challenge the assumption that longer reasoning is inherently better\nand highlight the need for adaptive CoT control. Motivated by this, we propose\nSEER (Self-Enhancing Efficient Reasoning), an adaptive framework that\ncompresses CoT while preserving accuracy. SEER combines Best-of-N sampling with\ntask-aware adaptive filtering, dynamically adjusting thresholds based on\npre-inference outputs to reduce verbosity and computational overhead. We then\nevaluate SEER on three software engineering tasks and one math task. On\naverage, SEER shortens CoT by 42.1%, improves accuracy by reducing truncation,\nand eliminates most infinite loops. These results demonstrate SEER as a\npractical method to make CoT-enhanced LLMs more efficient and robust, even\nunder resource constraints.",
        "url": "http://arxiv.org/abs/2509.14093v1",
        "pdf_url": "http://arxiv.org/pdf/2509.14093v1",
        "arxiv_id": "2509.14093v1",
        "authors": [
            "Kerui Huang",
            "Shuhan Liu",
            "Xing Hu",
            "Tongtong Xu",
            "Lingfeng Bao",
            "Xin Xia"
        ],
        "submitted": "2025-09-17 15:33:44",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on optimizing Chain-of-Thought reasoning in Large Language Models for software engineering tasks, which is not directly related to Information Retrieval or Search technologies. While it involves Natural Language Processing, the context and application are quite different from the user's core research interests."
    },
    {
        "title": "SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation",
        "abstract": "Sign Language Translation (SLT) bridges the communication gap between deaf\npeople and hearing people, where dialogue provides crucial contextual cues to\naid in translation. Building on this foundational concept, this paper proposes\nQuestion-based Sign Language Translation (QB-SLT), a novel task that explores\nthe efficient integration of dialogue. Unlike gloss (sign language\ntranscription) annotations, dialogue naturally occurs in communication and is\neasier to annotate. The key challenge lies in aligning multimodality features\nwhile leveraging the context of the question to improve translation. To address\nthis issue, we propose a cross-modality Self-supervised Learning with Sigmoid\nSelf-attention Weighting (SSL-SSAW) fusion method for sign language\ntranslation. Specifically, we employ contrastive learning to align\nmultimodality features in QB-SLT, then introduce a Sigmoid Self-attention\nWeighting (SSAW) module for adaptive feature extraction from question and sign\nlanguage sequences. Additionally, we leverage available question text through\nself-supervised learning to enhance representation and translation\ncapabilities. We evaluated our approach on newly constructed CSL-Daily-QA and\nPHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,\neasily accessible question assistance can achieve or even surpass the\nperformance of gloss assistance. Furthermore, visualization results demonstrate\nthe effectiveness of incorporating dialogue in improving translation quality.",
        "url": "http://arxiv.org/abs/2509.14036v1",
        "pdf_url": "http://arxiv.org/pdf/2509.14036v1",
        "arxiv_id": "2509.14036v1",
        "authors": [
            "Zekang Liu",
            "Wei Feng",
            "Fanhua Shang",
            "Lianyu Hu",
            "Jichao Feng",
            "Liqing Gao"
        ],
        "submitted": "2025-09-17 14:37:59",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on Sign Language Translation and Question-based Sign Language Translation, which is outside your primary areas of interest in Information Retrieval and Search technologies, and Natural Language Processing."
    },
    {
        "title": "You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models",
        "abstract": "Achieving human-level translations requires leveraging context to ensure\ncoherence and handle complex phenomena like pronoun disambiguation. Sparsity of\ncontextually rich examples in the standard training data has been hypothesized\nas the reason for the difficulty of context utilization. In this work, we\nsystematically validate this claim in both single- and multilingual settings by\nconstructing training datasets with a controlled proportions of contextually\nrelevant examples. We demonstrate a strong association between training data\nsparsity and model performance confirming sparsity as a key bottleneck.\nImportantly, we reveal that improvements in one contextual phenomenon do no\ngeneralize to others. While we observe some cross-lingual transfer, it is not\nsignificantly higher between languages within the same sub-family. Finally, we\npropose and empirically evaluate two training strategies designed to leverage\nthe available data. These strategies improve context utilization, resulting in\naccuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in\nsingle- and multilingual settings respectively.",
        "url": "http://arxiv.org/abs/2509.14031v1",
        "pdf_url": "http://arxiv.org/pdf/2509.14031v1",
        "arxiv_id": "2509.14031v1",
        "authors": [
            "Paweł Mąka",
            "Yusuf Can Semerci",
            "Jan Scholtes",
            "Gerasimos Spanakis"
        ],
        "submitted": "2025-09-17 14:33:17",
        "source": "arxiv",
        "comment": "EMNLP 2025 main conference",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on machine translation models, which is related to NLP, but it does not directly address information retrieval, query understanding, or ranking models, which are core areas of your research interests."
    },
    {
        "title": "Early Stopping Chain-of-thoughts in Large Language Models",
        "abstract": "Reasoning large language models (LLMs) have demonstrated superior capacities\nin solving complicated problems by generating long chain-of-thoughts (CoT), but\nsuch a lengthy CoT incurs high inference costs. In this study, we introduce\nES-CoT, an inference-time method that shortens CoT generation by detecting\nanswer convergence and stopping early with minimal performance loss. At the end\nof each reasoning step, we prompt the LLM to output its current final answer,\ndenoted as a step answer. We then track the run length of consecutive identical\nstep answers as a measure of answer convergence. Once the run length exhibits a\nsharp increase and exceeds a minimum threshold, the generation is terminated.\nWe provide both empirical and theoretical support for this heuristic: step\nanswers steadily converge to the final answer, and large run-length jumps\nreliably mark this convergence. Experiments on five reasoning datasets across\nthree LLMs show that ES-CoT reduces the number of inference tokens by about\n41\\% on average while maintaining accuracy comparable to standard CoT. Further,\nES-CoT integrates seamlessly with self-consistency prompting and remains robust\nacross hyperparameter choices, highlighting it as a practical and effective\napproach for efficient reasoning.",
        "url": "http://arxiv.org/abs/2509.14004v1",
        "pdf_url": "http://arxiv.org/pdf/2509.14004v1",
        "arxiv_id": "2509.14004v1",
        "authors": [
            "Minjia Mao",
            "Bowen Yin",
            "Yu Zhu",
            "Xiao Fang"
        ],
        "submitted": "2025-09-17 14:14:05",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on optimizing the inference process of large language models, which is not directly related to information retrieval or search technologies. While it involves natural language processing, the context is more geared towards efficient reasoning and inference, rather than query understanding or ranking models."
    },
    {
        "title": "Long-context Reference-based MT Quality Estimation",
        "abstract": "In this paper, we present our submission to the Tenth Conference on Machine\nTranslation (WMT25) Shared Task on Automated Translation Quality Evaluation.\n  Our systems are built upon the COMET framework and trained to predict\nsegment-level Error Span Annotation (ESA) scores using augmented long-context\ndata.\n  To construct long-context training data, we concatenate in-domain,\nhuman-annotated sentences and compute a weighted average of their scores.\n  We integrate multiple human judgment datasets (MQM, SQM, and DA) by\nnormalising their scales and train multilingual regression models to predict\nquality scores from the source, hypothesis, and reference translations.\n  Experimental results show that incorporating long-context information\nimproves correlations with human judgments compared to models trained only on\nshort segments.",
        "url": "http://arxiv.org/abs/2509.13980v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13980v1",
        "arxiv_id": "2509.13980v1",
        "authors": [
            "Sami Ul Haq",
            "Chinonso Cynthia Osuji",
            "Sheila Castilho",
            "Brian Davis"
        ],
        "submitted": "2025-09-17 13:52:45",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Machine Translation Quality Estimation, which is not a core area of interest for you. While it involves some NLP aspects, it does not align with your primary focus on Information Retrieval, query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification",
        "abstract": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https://github.com/PRAISELab-PicusLab/CER",
        "url": "http://arxiv.org/abs/2509.13888v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13888v1",
        "arxiv_id": "2509.13888v1",
        "authors": [
            "Mariano Barone",
            "Antonio Romano",
            "Giuseppe Riccio",
            "Marco Postiglione",
            "Vincenzo Moscato"
        ],
        "submitted": "2025-09-17 10:31:09",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 3,
        "llm_reason": "The paper focuses on combating biomedical misinformation through multi-modal claim detection and evidence-based verification. While it involves natural language processing and large language models, its primary focus is on biomedical fact-checking, which is somewhat related to the user's interests in information retrieval and NLP, but not directly aligned with their core research themes."
    },
    {
        "title": "Combining Evidence and Reasoning for Biomedical Fact-Checking",
        "abstract": "Misinformation in healthcare, from vaccine hesitancy to unproven treatments,\nposes risks to public health and trust in medical systems. While machine\nlearning and natural language processing have advanced automated fact-checking,\nvalidating biomedical claims remains uniquely challenging due to complex\nterminology, the need for domain expertise, and the critical importance of\ngrounding in scientific evidence. We introduce CER (Combining Evidence and\nReasoning), a novel framework for biomedical fact-checking that integrates\nscientific evidence retrieval, reasoning via large language models, and\nsupervised veracity prediction. By integrating the text-generation capabilities\nof large language models with advanced retrieval techniques for high-quality\nbiomedical scientific evidence, CER effectively mitigates the risk of\nhallucinations, ensuring that generated outputs are grounded in verifiable,\nevidence-based sources. Evaluations on expert-annotated datasets (HealthFC,\nBioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising\ncross-dataset generalization. Code and data are released for transparency and\nreproducibility: https: //github.com/PRAISELab-PicusLab/CER.",
        "url": "http://arxiv.org/abs/2509.13879v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13879v1",
        "arxiv_id": "2509.13879v1",
        "authors": [
            "Mariano Barone",
            "Antonio Romano",
            "Giuseppe Riccio",
            "Marco Postiglione",
            "Vincenzo Moscato"
        ],
        "submitted": "2025-09-17 10:14:56",
        "source": "arxiv",
        "comment": "Proceedings of the 48th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval, 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on biomedical fact-checking, which is a specific application of information retrieval and natural language processing. While it involves query understanding and ranking models, the context is distinct from the user's primary research interests in e-commerce and deep semantic understanding. The paper's emphasis on scientific evidence retrieval and veracity prediction is somewhat related to the user's interests in user behavior modeling and click models, but the connection is not direct."
    },
    {
        "title": "THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning",
        "abstract": "Large Language Models (LLMs) have made remarkable progress in mathematical\nreasoning, but still continue to struggle with high-precision tasks like\nnumerical computation and formal symbolic manipulation. Integrating external\ntools has emerged as a promising approach to bridge this gap. Despite recent\nadvances, existing methods struggle with three key challenges: constructing\ntool-integrated reasoning data, performing fine-grained optimization, and\nenhancing inference. To overcome these limitations, we propose THOR\n(Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen,\na multi-agent actor-critic-based pipeline for constructing high-quality\ndatasets of tool-integrated reasoning paths, aligning with the policy and\ngeneralizing well across diverse models. Second, to perform fine-grained\nhierarchical optimization, we introduce an RL strategy that jointly optimizes\nfor both trajectory-level problem solving and step-level code generation. This\nis motivated by our key insight that the success of an intermediate tool call\nis a strong predictor of the final answer's correctness. Finally, THOR\nincorporates a self-correction mechanism that leverages immediate tool feedback\nto dynamically revise erroneous reasoning paths during inference. Our approach\ndemonstrates strong generalization across diverse models, performing\neffectively in both reasoning and non-reasoning models. It further achieves\nstate-of-the-art performance for models of a similar scale on multiple\nmathematical benchmarks, while also delivering consistent improvements on code\nbenchmarks. Our code will be publicly available at\nhttps://github.com/JingMog/THOR.",
        "url": "http://arxiv.org/abs/2509.13761v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13761v1",
        "arxiv_id": "2509.13761v1",
        "authors": [
            "Qikai Chang",
            "Zhenrong Zhang",
            "Pengfei Hu",
            "Jiefeng Ma",
            "Yicheng Pan",
            "Jianshu Zhang",
            "Jun Du",
            "Quan Liu",
            "Jianqing Gao"
        ],
        "submitted": "2025-09-17 07:16:12",
        "source": "arxiv",
        "comment": "22 pages, 13 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on mathematical reasoning and tool-integrated optimization via RL, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing."
    },
    {
        "title": "Implementing a Logical Inference System for Japanese Comparatives",
        "abstract": "Natural Language Inference (NLI) involving comparatives is challenging\nbecause it requires understanding quantities and comparative relations\nexpressed by sentences. While some approaches leverage Large Language Models\n(LLMs), we focus on logic-based approaches grounded in compositional semantics,\nwhich are promising for robust handling of numerical and logical expressions.\nPrevious studies along these lines have proposed logical inference systems for\nEnglish comparatives. However, it has been pointed out that there are several\nmorphological and semantic differences between Japanese and English\ncomparatives. These differences make it difficult to apply such systems\ndirectly to Japanese comparatives. To address this gap, this study proposes\nccg-jcomp, a logical inference system for Japanese comparatives based on\ncompositional semantics. We evaluate the proposed system on a Japanese NLI\ndataset containing comparative expressions. We demonstrate the effectiveness of\nour system by comparing its accuracy with that of existing LLMs.",
        "url": "http://arxiv.org/abs/2509.13734v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13734v1",
        "arxiv_id": "2509.13734v1",
        "authors": [
            "Yosuke Mikami",
            "Daiki Matsuoka",
            "Hitomi Yanaka"
        ],
        "submitted": "2025-09-17 06:37:10",
        "source": "arxiv",
        "comment": "In Proceedings of the 5th Workshop on Natural Logic Meets Machine\n  Learning (NALOMA)",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on a specific task of Natural Language Inference involving comparatives in Japanese, using a logical inference system grounded in compositional semantics. While it touches on aspects of NLP, it does not directly relate to the user's core research themes in Information Retrieval, query understanding, ranking models, or user behavior modeling. The paper's focus on Japanese language and logical inference systems also limits its relevance to the user's broader interests in e-commerce and real-time relevance optimization."
    },
    {
        "title": "Privacy-Aware In-Context Learning for Large Language Models",
        "abstract": "Large language models (LLMs) have significantly transformed natural language\nunderstanding and generation, but they raise privacy concerns due to potential\nexposure of sensitive information. Studies have highlighted the risk of\ninformation leakage, where adversaries can extract sensitive information\nembedded in the prompts. In this work, we introduce a novel private prediction\nframework for generating high-quality synthetic text with strong privacy\nguarantees. Our approach leverages the Differential Privacy (DP) framework to\nensure worst-case theoretical bounds on information leakage without requiring\nany fine-tuning of the underlying models.The proposed method performs inference\non private records and aggregates the resulting per-token output distributions.\nThis enables the generation of longer and coherent synthetic text while\nmaintaining privacy guarantees. Additionally, we propose a simple blending\noperation that combines private and public inference to further enhance\nutility. Empirical evaluations demonstrate that our approach outperforms\nprevious state-of-the-art methods on in-context-learning (ICL) tasks, making it\na promising direction for privacy-preserving text generation while maintaining\nhigh utility.",
        "url": "http://arxiv.org/abs/2509.13625v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13625v1",
        "arxiv_id": "2509.13625v1",
        "authors": [
            "Bishnu Bhusal",
            "Manoj Acharya",
            "Ramneet Kaur",
            "Colin Samplawski",
            "Anirban Roy",
            "Adam D. Cobb",
            "Rohit Chadha",
            "Susmit Jha"
        ],
        "submitted": "2025-09-17 01:50:32",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on privacy concerns in large language models, which is a topic in NLP, but it does not align with the user's primary research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Annotating Satellite Images of Forests with Keywords from a Specialized Corpus in the Context of Change Detection",
        "abstract": "The Amazon rain forest is a vital ecosystem that plays a crucial role in\nregulating the Earth's climate and providing habitat for countless species.\nDeforestation in the Amazon is a major concern as it has a significant impact\non global carbon emissions and biodiversity. In this paper, we present a method\nfor detecting deforestation in the Amazon using image pairs from Earth\nobservation satellites. Our method leverages deep learning techniques to\ncompare the images of the same area at different dates and identify changes in\nthe forest cover. We also propose a visual semantic model that automatically\nannotates the detected changes with relevant keywords. The candidate annotation\nfor images are extracted from scientific documents related to the Amazon\nregion. We evaluate our approach on a dataset of Amazon image pairs and\ndemonstrate its effectiveness in detecting deforestation and generating\nrelevant annotations. Our method provides a useful tool for monitoring and\nstudying the impact of deforestation in the Amazon. While we focus on\nenvironment applications of our work by using images of deforestation in the\nAmazon rain forest to demonstrate the effectiveness of our proposed approach,\nit is generic enough to be applied to other domains.",
        "url": "http://arxiv.org/abs/2509.13586v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13586v1",
        "arxiv_id": "2509.13586v1",
        "authors": [
            "Nathalie Neptune",
            "Josiane Mothe"
        ],
        "submitted": "2025-09-16 23:00:16",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. Although it involves deep learning techniques, the application is in image analysis and change detection in satellite images, which is unrelated to your core research themes."
    },
    {
        "title": "Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has intensified the\nneed for robust dialogue system evaluation, yet comprehensive assessment\nremains challenging. Traditional metrics often prove insufficient, and safety\nconsiderations are frequently narrowly defined or culturally biased. The DSTC12\nTrack 1, \"Dialog System Evaluation: Dimensionality, Language, Culture and\nSafety,\" is part of the ongoing effort to address these critical gaps. The\ntrack comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic\nEvaluation Metrics, and (2) Multilingual and Multicultural Safety Detection.\nFor Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved\nthe highest average Spearman's correlation (0.1681), indicating substantial\nroom for improvement. In Task 2, while participating teams significantly\noutperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top\nROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126\nROC-AUC), highlighting critical needs in culturally-aware safety. This paper\ndescribes the datasets and baselines provided to participants, as well as\nsubmission evaluation results for each of the two proposed subtasks.",
        "url": "http://arxiv.org/abs/2509.13569v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13569v1",
        "arxiv_id": "2509.13569v1",
        "authors": [
            "John Mendonça",
            "Lining Zhang",
            "Rahul Mallidi",
            "Alon Lavie",
            "Isabel Trancoso",
            "Luis Fernando D'Haro",
            "João Sedoc"
        ],
        "submitted": "2025-09-16 22:13:45",
        "source": "arxiv",
        "comment": "DSTC12 Track 1 Overview Paper. https://chateval.org/dstc12",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to the user's interests in Information Retrieval and Natural Language Processing, as it involves dialogue system evaluation and Large Language Models. However, the focus on dialogue systems and safety considerations is not directly aligned with the user's primary research themes of query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs",
        "abstract": "Personalized financial advice requires consideration of user goals,\nconstraints, risk tolerance, and jurisdiction. Prior LLM work has focused on\nsupport systems for investors and financial planners. Simultaneously, numerous\nrecent studies examine broader personal finance tasks, including budgeting,\ndebt management, retirement, and estate planning, through agentic pipelines\nthat incur high maintenance costs, yielding less than 25% of their expected\nfinancial returns. In this study, we introduce a novel and reproducible\nframework that integrates relevant financial context with behavioral finance\nstudies to construct supervision data for end-to-end advisors. Using this\nframework, we create a 19k sample reasoning dataset and conduct a comprehensive\nfine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test\nsplit and a blind LLM-jury study, we demonstrate that through careful data\ncuration and behavioral integration, our 8B model achieves performance\ncomparable to significantly larger baselines (14-32B parameters) across factual\naccuracy, fluency, and personalization metrics while incurring 80% lower costs\nthan the larger counterparts.",
        "url": "http://arxiv.org/abs/2509.14180v1",
        "pdf_url": "http://arxiv.org/pdf/2509.14180v1",
        "arxiv_id": "2509.14180v1",
        "authors": [
            "Akhil Theerthala"
        ],
        "submitted": "2025-09-17 17:12:38",
        "source": "arxiv",
        "comment": "24 pages, 11 figures. The paper presents a novel framework for\n  generating a personal finance dataset. The resulting fine-tuned model and\n  dataset are publicly available",
        "score": 1,
        "keyword_reasons": [
            "Found 'personalization' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be focused on developing a framework for personal finance LLMs, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves LLMs and NLP, the context and application are quite specific to personal finance and do not align with the user's broader interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset",
        "abstract": "We present CS-FLEURS, a new dataset for developing and evaluating\ncode-switched speech recognition and translation systems beyond high-resourced\nlanguages. CS-FLEURS consists of 4 test sets which cover in total 113 unique\ncode-switched language pairs across 52 languages: 1) a 14 X-English language\npair set with real voices reading synthetically generated code-switched\nsentences, 2) a 16 X-English language pair set with generative text-to-speech\n3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the\ngenerative text-to-speech, and 4) a 45 X-English lower-resourced language pair\ntest set with concatenative text-to-speech. Besides the four test sets,\nCS-FLEURS also provides a training set with 128 hours of generative\ntext-to-speech data across 16 X-English language pairs. Our hope is that\nCS-FLEURS helps to broaden the scope of future code-switched speech research.\nDataset link: https://huggingface.co/datasets/byan/cs-fleurs.",
        "url": "http://arxiv.org/abs/2509.14161v1",
        "pdf_url": "http://arxiv.org/pdf/2509.14161v1",
        "arxiv_id": "2509.14161v1",
        "authors": [
            "Brian Yan",
            "Injy Hamed",
            "Shuichiro Shimizu",
            "Vasista Lodagala",
            "William Chen",
            "Olga Iakovenko",
            "Bashar Talafha",
            "Amir Hussein",
            "Alexander Polok",
            "Kalvin Chang",
            "Dominik Klement",
            "Sara Althubaiti",
            "Puyuan Peng",
            "Matthew Wiesner",
            "Thamar Solorio",
            "Ahmed Ali",
            "Sanjeev Khudanpur",
            "Shinji Watanabe",
            "Chih-Chen Chen",
            "Zhen Wu",
            "Karim Benharrak",
            "Anuj Diwan",
            "Samuele Cornell",
            "Eunjung Yeo",
            "Kwanghee Choi",
            "Carlos Carvalho",
            "Karen Rosero"
        ],
        "submitted": "2025-09-17 16:45:22",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on speech recognition and translation systems, which is outside your primary areas of Information Retrieval and Natural Language Processing."
    },
    {
        "title": "Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale",
        "abstract": "We present Hala, a family of Arabic-centric instruction and translation\nmodels built with our translate-and-tune pipeline. We first compress a strong\nAR$\\leftrightarrow$EN teacher to FP8 (yielding $\\sim$2$\\times$ higher\nthroughput with no quality loss) and use it to create high-fidelity bilingual\nsupervision. A lightweight language model LFM2-1.2B is then fine-tuned on this\ndata and used to translate high-quality English instruction sets into Arabic,\nproducing a million-scale corpus tailored to instruction following. We train\nHala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to\nbalance Arabic specialization with base-model strengths. On Arabic-centric\nbenchmarks, Hala achieves state-of-the-art results within both the \"nano\"\n($\\leq$2B) and \"small\" (7-9B) categories, outperforming their bases. We release\nmodels, data, evaluation, and recipes to accelerate research in Arabic NLP.",
        "url": "http://arxiv.org/abs/2509.14008v1",
        "pdf_url": "http://arxiv.org/pdf/2509.14008v1",
        "arxiv_id": "2509.14008v1",
        "authors": [
            "Hasan Abed Al Kader Hammoud",
            "Mohammad Zbeeb",
            "Bernard Ghanem"
        ],
        "submitted": "2025-09-17 14:19:28",
        "source": "arxiv",
        "comment": "Technical Report",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Arabic-centric instruction and translation models, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the specific domain and application are not aligned with the user's core themes."
    },
    {
        "title": "Masked Diffusion Models as Energy Minimization",
        "abstract": "We present a systematic theoretical framework that interprets masked\ndiffusion models (MDMs) as solutions to energy minimization problems in\ndiscrete optimal transport. Specifically, we prove that three distinct energy\nformulations--kinetic, conditional kinetic, and geodesic energy--are\nmathematically equivalent under the structure of MDMs, and that MDMs minimize\nall three when the mask schedule satisfies a closed-form optimality condition.\nThis unification not only clarifies the theoretical foundations of MDMs, but\nalso motivates practical improvements in sampling. By parameterizing\ninterpolation schedules via Beta distributions, we reduce the schedule design\nspace to a tractable 2D search, enabling efficient post-training tuning without\nmodel modification. Experiments on synthetic and real-world benchmarks\ndemonstrate that our energy-inspired schedules outperform hand-crafted\nbaselines, particularly in low-step sampling settings.",
        "url": "http://arxiv.org/abs/2509.13866v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13866v1",
        "arxiv_id": "2509.13866v1",
        "authors": [
            "Sitong Chen",
            "Shen Nie",
            "Jiacheng Sun",
            "Zijin Feng",
            "Zhenguo Li",
            "Ji-Rong Wen",
            "Chongxuan Li"
        ],
        "submitted": "2025-09-17 09:57:31",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be primarily focused on masked diffusion models in the context of energy minimization problems, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing."
    },
    {
        "title": "Noise Supervised Contrastive Learning and Feature-Perturbed for Anomalous Sound Detection",
        "abstract": "Unsupervised anomalous sound detection aims to detect unknown anomalous\nsounds by training a model using only normal audio data. Despite advancements\nin self-supervised methods, the issue of frequent false alarms when handling\nsamples of the same type from different machines remains unresolved. This paper\nintroduces a novel training technique called one-stage supervised contrastive\nlearning (OS-SCL), which significantly addresses this problem by perturbing\nfeatures in the embedding space and employing a one-stage noisy supervised\ncontrastive learning approach. On the DCASE 2020 Challenge Task 2, it achieved\n94.64\\% AUC, 88.42\\% pAUC, and 89.24\\% mAUC using only Log-Mel features.\nAdditionally, a time-frequency feature named TFgram is proposed, which is\nextracted from raw audio. This feature effectively captures critical\ninformation for anomalous sound detection, ultimately achieving 95.71\\% AUC,\n90.23\\% pAUC, and 91.23\\% mAUC. The source code is available at:\n\\underline{www.github.com/huangswt/OS-SCL}.",
        "url": "http://arxiv.org/abs/2509.13853v2",
        "pdf_url": "http://arxiv.org/pdf/2509.13853v2",
        "arxiv_id": "2509.13853v2",
        "authors": [
            "Shun Huang",
            "Zhihua Fang",
            "Liang He"
        ],
        "submitted": "2025-09-17 09:38:47",
        "source": "arxiv",
        "comment": "Accepted ICASSP 2025",
        "score": 1,
        "keyword_reasons": [
            "Found 'www' (score: +1)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper focuses on anomalous sound detection using audio data, which is unrelated to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing."
    },
    {
        "title": "Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs",
        "abstract": "Large language models demonstrate impressive results across diverse tasks but\nare still known to hallucinate, generating linguistically plausible but\nincorrect answers to questions. Uncertainty quantification has been proposed as\na strategy for hallucination detection, but no existing black-box approach\nprovides estimates for both global and local uncertainty. The former attributes\nuncertainty to a batch of responses, while the latter attributes uncertainty to\nindividual responses. Current local methods typically rely on white-box access\nto internal model states, whilst black-box methods only provide global\nuncertainty estimates. We introduce a geometric framework to address this,\nbased on archetypal analysis of batches of responses sampled with only\nblack-box model access. At the global level, we propose Geometric Volume, which\nmeasures the convex hull volume of archetypes derived from response embeddings.\nAt the local level, we propose Geometric Suspicion, which ranks responses by\nreliability and enables hallucination reduction through preferential response\nselection. Unlike prior dispersion methods which yield only a single global\nscore, our approach provides semantic boundary points which have utility for\nattributing reliability to individual responses. Experiments show that our\nframework performs comparably to or better than prior methods on short form\nquestion-answering datasets, and achieves superior results on medical datasets\nwhere hallucinations carry particularly critical risks. We also provide\ntheoretical justification by proving a link between convex hull volume and\nentropy.",
        "url": "http://arxiv.org/abs/2509.13813v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13813v1",
        "arxiv_id": "2509.13813v1",
        "authors": [
            "Edward Phillips",
            "Sean Wu",
            "Soheila Molaei",
            "Danielle Belgrave",
            "Anshul Thakur",
            "David Clifton"
        ],
        "submitted": "2025-09-17 08:28:07",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Large Language Models (LLMs) and hallucination detection, which is outside your primary research interests in Information Retrieval and Search technologies. While it involves Natural Language Processing, the specific application and methodology are not directly related to your core themes."
    },
    {
        "title": "AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation",
        "abstract": "Although significant progress has been made in many tasks within the field of\nNatural Language Processing (NLP), Controlled Text Generation (CTG) continues\nto face numerous challenges, particularly in achieving fine-grained conditional\ncontrol over generation. Additionally, in real scenario and online\napplications, cost considerations, scalability, domain knowledge learning and\nmore precise control are required, presenting more challenge for CTG. This\npaper introduces a novel and scalable framework, AgentCTG, which aims to\nenhance precise and complex control over the text generation by simulating the\ncontrol and regulation mechanisms in multi-agent workflows. We explore various\ncollaboration methods among different agents and introduce an auto-prompt\nmodule to further enhance the generation effectiveness. AgentCTG achieves\nstate-of-the-art results on multiple public datasets. To validate its\neffectiveness in practical applications, we propose a new challenging\nCharacter-Driven Rewriting task, which aims to convert the original text into\nnew text that conform to specific character profiles and simultaneously\npreserve the domain knowledge. When applied to online navigation with\nrole-playing, our approach significantly enhances the driving experience\nthrough improved content delivery. By optimizing the generation of contextually\nrelevant text, we enable a more immersive interaction within online\ncommunities, fostering greater personalization and user engagement.",
        "url": "http://arxiv.org/abs/2509.13677v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13677v1",
        "arxiv_id": "2509.13677v1",
        "authors": [
            "Xinxu Zhou",
            "Jiaqi Bai",
            "Zhenqi Sun",
            "Fanxiang Zeng",
            "Yue Liu"
        ],
        "submitted": "2025-09-17 04:07:22",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'personalization' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on Controlled Text Generation (CTG) in Natural Language Processing (NLP), which is somewhat related to the user's interests in NLP and deep semantic understanding. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user."
    },
    {
        "title": "CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction",
        "abstract": "The growing demand for automated writing assistance in diverse academic\ndomains highlights the need for robust Chinese Grammatical Error Correction\n(CGEC) systems that can adapt across disciplines. However, existing CGEC\nresearch largely lacks dedicated benchmarks for multi-disciplinary academic\nwriting, overlooking continual learning (CL) as a promising solution to handle\ndomain-specific linguistic variation and prevent catastrophic forgetting. To\nfill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning\nbenchmark for Chinese Literature Grammatical Error Correction, designed to\nevaluate adaptive CGEC across multiple academic fields. Our benchmark includes\n10,000 human-annotated sentences spanning 10 disciplines, each exhibiting\ndistinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating\ngrammatical error correction in a continual learning setting, simulating\nsequential exposure to diverse academic disciplines to reflect real-world\neditorial dynamics. We evaluate large language models under sequential tuning,\nparameter-efficient adaptation, and four representative CL algorithms, using\nboth standard GEC metrics and continual learning metrics adapted to task-level\nvariation. Experimental results reveal that regularization-based methods\nmitigate forgetting more effectively than replay-based or naive sequential\napproaches. Our benchmark provides a rigorous foundation for future research in\nadaptive grammatical error correction across diverse academic domains.",
        "url": "http://arxiv.org/abs/2509.13672v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13672v1",
        "arxiv_id": "2509.13672v1",
        "authors": [
            "Shang Qin",
            "Jingheng Ye",
            "Yinghui Li",
            "Hai-Tao Zheng",
            "Qi Li",
            "Jinxiao Shan",
            "Zhixing Li",
            "Hong-Gee Kim"
        ],
        "submitted": "2025-09-17 03:54:52",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves language models and error correction, the focus is on grammatical error correction in Chinese literature, which is not a central theme in your research."
    },
    {
        "title": "Sequential Data Augmentation for Generative Recommendation",
        "abstract": "Generative recommendation plays a crucial role in personalized systems,\npredicting users' future interactions from their historical behavior sequences.\nA critical yet underexplored factor in training these models is data\naugmentation, the process of constructing training data from user interaction\nhistories. By shaping the training distribution, data augmentation directly and\noften substantially affects model generalization and performance. Nevertheless,\nin much of the existing work, this process is simplified, applied\ninconsistently, or treated as a minor design choice, without a systematic and\nprincipled understanding of its effects.\n  Motivated by our empirical finding that different augmentation strategies can\nyield large performance disparities, we conduct an in-depth analysis of how\nthey reshape training distributions and influence alignment with future targets\nand generalization to unseen inputs. To systematize this design space, we\npropose GenPAS, a generalized and principled framework that models augmentation\nas a stochastic sampling process over input-target pairs with three\nbias-controlled steps: sequence sampling, target sampling, and input sampling.\nThis formulation unifies widely used strategies as special cases and enables\nflexible control of the resulting training distribution. Our extensive\nexperiments on benchmark and industrial datasets demonstrate that GenPAS yields\nsuperior accuracy, data efficiency, and parameter efficiency compared to\nexisting strategies, providing practical guidance for principled training data\nconstruction in generative recommendation.",
        "url": "http://arxiv.org/abs/2509.13648v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13648v1",
        "arxiv_id": "2509.13648v1",
        "authors": [
            "Geon Lee",
            "Bhuvesh Kumar",
            "Clark Mingxuan Ju",
            "Tong Zhao",
            "Kijung Shin",
            "Neil Shah",
            "Liam Collins"
        ],
        "submitted": "2025-09-17 02:53:25",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on generative recommendation and data augmentation, which is somewhat related to information retrieval and search technologies. However, the emphasis on recommendation systems and data efficiency is not a central match to the user's primary research interests in IR and NLP, especially in areas requiring deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "SteeringControl: Holistic Evaluation of Alignment Steering in LLMs",
        "abstract": "We introduce SteeringControl, a benchmark for evaluating representation\nsteering methods across core alignment objectives--bias, harmful generation,\nand hallucination--and their effects on secondary behaviors such as sycophancy\nand commonsense morality. While prior alignment work often highlights\ntruthfulness or reasoning ability to demonstrate the side effects of\nrepresentation steering, we find there are many unexplored tradeoffs not yet\nunderstood in a systematic way. We collect a dataset of safety-relevant primary\nand secondary behaviors to evaluate steering effectiveness and behavioral\nentanglement centered around five popular steering methods. To enable this, we\ncraft a modular steering framework based on unique components that serve as the\nbuilding blocks of many existing methods. Our results on Qwen-2.5-7B and\nLlama-3.1-8B find that strong steering performance is dependent on the specific\ncombination of steering method, model, and targeted behavior, and that severe\nconcept entanglement can result from poor combinations of these three as well.\nWe release our code here:\nhttps://github.com/wang-research-lab/SteeringControl.git.",
        "url": "http://arxiv.org/abs/2509.13450v1",
        "pdf_url": "http://arxiv.org/pdf/2509.13450v1",
        "arxiv_id": "2509.13450v1",
        "authors": [
            "Vincent Siu",
            "Nicholas Crispino",
            "David Park",
            "Nathan W. Henry",
            "Zhun Wang",
            "Yang Liu",
            "Dawn Song",
            "Chenguang Wang"
        ],
        "submitted": "2025-09-16 18:36:22",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on representation steering in Large Language Models (LLMs), which is a topic in Natural Language Processing (NLP). While it touches on the concept of 'alignment' and 'steering', it doesn't directly relate to the user's core research interests in Information Retrieval (IR), query understanding, ranking models, or user behavior modeling."
    }
]