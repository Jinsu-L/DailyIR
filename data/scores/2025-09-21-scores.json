[
    {
        "title": "CodeRAG: Finding Relevant and Necessary Knowledge for Retrieval-Augmented Repository-Level Code Completion",
        "abstract": "Repository-level code completion automatically predicts the unfinished code\nbased on the broader information from the repository. Recent strides in Code\nLarge Language Models (code LLMs) have spurred the development of\nrepository-level code completion methods, yielding promising results.\nNevertheless, they suffer from issues such as inappropriate query construction,\nsingle-path code retrieval, and misalignment between code retriever and code\nLLM. To address these problems, we introduce CodeRAG, a framework tailored to\nidentify relevant and necessary knowledge for retrieval-augmented\nrepository-level code completion. Its core components include log probability\nguided query construction, multi-path code retrieval, and preference-aligned\nBestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval\ndemonstrate that CodeRAG significantly and consistently outperforms\nstate-of-the-art methods. The implementation of CodeRAG is available at\nhttps://github.com/KDEGroup/CodeRAG.",
        "url": "http://arxiv.org/abs/2509.16112v1",
        "pdf_url": "http://arxiv.org/pdf/2509.16112v1",
        "arxiv_id": "2509.16112v1",
        "authors": [
            "Sheng Zhang",
            "Yifan Ding",
            "Shuquan Lian",
            "Shun Song",
            "Hui Li"
        ],
        "submitted": "2025-09-19 15:57:40",
        "source": "arxiv",
        "comment": "EMNLP 2025",
        "score": 17,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores repository-level code completion using retrieval-augmented models, which is somewhat related to information retrieval and query understanding. However, the focus on code completion and large language models is not a central match to the user's primary research interests in IR and NLP. The paper's relevance is somewhat mitigated by the lack of direct connection to search technologies, ranking models, or user behavior modeling."
    },
    {
        "title": "CFDA & CLIP at TREC iKAT 2025: Enhancing Personalized Conversational Search via Query Reformulation and Rank Fusion",
        "abstract": "The 2025 TREC Interactive Knowledge Assistance Track (iKAT) featured both\ninteractive and offline submission tasks. The former requires systems to\noperate under real-time constraints, making robustness and efficiency as\nimportant as accuracy, while the latter enables controlled evaluation of\npassage ranking and response generation with pre-defined datasets. To address\nthis, we explored query rewriting and retrieval fusion as core strategies. We\nbuilt our pipelines around Best-of-$N$ selection and Reciprocal Rank Fusion\n(RRF) strategies to handle different submission tasks. Results show that\nreranking and fusion improve robustness while revealing trade-offs between\neffectiveness and efficiency across both tasks.",
        "url": "http://arxiv.org/abs/2509.15588v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15588v1",
        "arxiv_id": "2509.15588v1",
        "authors": [
            "Yu-Cheng Chang",
            "Guan-Wei Yeo",
            "Quah Eugene",
            "Fan-Jie Shih",
            "Yuan-Ching Kuo",
            "Tsung-En Yu",
            "Hung-Chun Hsu",
            "Ming-Feng Tsai",
            "Chuan-Ju Wang"
        ],
        "submitted": "2025-09-19 04:42:31",
        "source": "arxiv",
        "comment": null,
        "score": 15,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)",
            "Found 'trec' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of conversational search and query reformulation. The use of ranking models and fusion strategies aligns with your focus on Learning to Rank and real-time relevance optimization. However, the paper's emphasis on conversational search and passage ranking is somewhat outside your primary e-commerce domain focus."
    },
    {
        "title": "Chunk Knowledge Generation Model for Enhanced Information Retrieval: A Multi-task Learning Approach",
        "abstract": "Traditional query expansion techniques for addressing vocabulary mismatch\nproblems in information retrieval are context-sensitive and may lead to\nperformance degradation. As an alternative, document expansion research has\ngained attention, but existing methods such as Doc2Query have limitations\nincluding excessive preprocessing costs, increased index size, and reliability\nissues with generated content. To mitigate these problems and seek more\nstructured and efficient alternatives, this study proposes a method that\ndivides documents into chunk units and generates textual data for each chunk to\nsimultaneously improve retrieval efficiency and accuracy. The proposed \"Chunk\nKnowledge Generation Model\" adopts a T5-based multi-task learning structure\nthat simultaneously generates titles and candidate questions from each document\nchunk while extracting keywords from user queries. This approach maximizes\ncomputational efficiency by generating and extracting three types of semantic\ninformation in parallel through a single encoding and two decoding processes.\nThe generated data is utilized as additional information in the retrieval\nsystem. GPT-based evaluation on 305 query-document pairs showed that retrieval\nusing the proposed model achieved 95.41% accuracy at Top@10, demonstrating\nsuperior performance compared to document chunk-level retrieval. This study\ncontributes by proposing an approach that simultaneously generates titles and\ncandidate questions from document chunks for application in retrieval\npipelines, and provides empirical evidence applicable to large-scale\ninformation retrieval systems by demonstrating improved retrieval accuracy\nthrough qualitative evaluation.",
        "url": "http://arxiv.org/abs/2509.15658v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15658v1",
        "arxiv_id": "2509.15658v1",
        "authors": [
            "Jisu Kim",
            "Jinhee Park",
            "Changhyun Jeon",
            "Jungwoo Choi",
            "Keonwoo Kim",
            "Minji Hong",
            "Sehyun Kim"
        ],
        "submitted": "2025-09-19 06:32:30",
        "source": "arxiv",
        "comment": null,
        "score": 12,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper proposes a novel approach to information retrieval, specifically addressing vocabulary mismatch problems through a chunk-based knowledge generation model. The use of multi-task learning and T5-based architecture aligns with your interests in query understanding and ranking models. The focus on improving retrieval efficiency and accuracy is also relevant to your research themes."
    },
    {
        "title": "Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM Responses",
        "abstract": "Evaluating long-form answers in high-stakes domains such as law or medicine\nremains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to\ncapture semantic correctness, and current LLM-based evaluators often reduce\nnuanced aspects of answer quality into a single undifferentiated score. We\nintroduce DeCE, a decomposed LLM evaluation framework that separates precision\n(factual accuracy and relevance) and recall (coverage of required concepts),\nusing instance-specific criteria automatically extracted from gold answer\nrequirements. DeCE is model-agnostic and domain-general, requiring no\npredefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate\ndifferent LLMs on a real-world legal QA task involving multi-jurisdictional\nreasoning and citation grounding. DeCE achieves substantially stronger\ncorrelation with expert judgments ($r=0.78$), compared to traditional metrics\n($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional\nevaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist\nmodels favor recall, while specialized models favor precision. Importantly,\nonly 11.95% of LLM-generated criteria required expert revision, underscoring\nDeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation\nframework in expert domains.",
        "url": "http://arxiv.org/abs/2509.16093v1",
        "pdf_url": "http://arxiv.org/pdf/2509.16093v1",
        "arxiv_id": "2509.16093v1",
        "authors": [
            "Fangyi Yu",
            "Nabeel Seedat",
            "Dasha Herrmannova",
            "Frank Schilder",
            "Jonathan Richard Schwarz"
        ],
        "submitted": "2025-09-19 15:36:02",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'pointwise' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper introduces a novel evaluation framework for LLM responses, focusing on semantic correctness and nuanced aspects of answer quality. While not directly related to query understanding, ranking models, or user behavior modeling, it aligns with your interests in Information Retrieval and NLP, particularly in areas requiring deep semantic understanding. The paper's emphasis on model-agnostic and domain-general evaluation also resonates with your broader research themes."
    },
    {
        "title": "LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs",
        "abstract": "High-quality long-context data is essential for training large language\nmodels (LLMs) capable of processing extensive documents, yet existing synthesis\napproaches using relevance-based aggregation face challenges of computational\nefficiency. We present LiteLong, a resource-efficient method for synthesizing\nlong-context data through structured topic organization and multi-agent debate.\nOur approach leverages the BISAC book classification system to provide a\ncomprehensive hierarchical topic organization, and then employs a debate\nmechanism with multiple LLMs to generate diverse, high-quality topics within\nthis structure. For each topic, we use lightweight BM25 retrieval to obtain\nrelevant documents and concatenate them into 128K-token training samples.\nExperiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves\ncompetitive long-context performance and can seamlessly integrate with other\nlong-dependency enhancement methods. LiteLong makes high-quality long-context\ndata synthesis more accessible by reducing both computational and data\nengineering costs, facilitating further research in long-context language\ntraining.",
        "url": "http://arxiv.org/abs/2509.15568v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15568v1",
        "arxiv_id": "2509.15568v1",
        "authors": [
            "Junlong Jia",
            "Xing Wu",
            "Chaochen Gao",
            "Ziyang Chen",
            "Zijia Lin",
            "Zhongzhi Li",
            "Weinong Wang",
            "Haotian Xu",
            "Donghui Jin",
            "Debing Zhang",
            "Binghui Guo"
        ],
        "submitted": "2025-09-19 04:07:46",
        "source": "arxiv",
        "comment": "work in progress",
        "score": 8,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on large language models (LLMs) and long-context data synthesis, which is somewhat related to information retrieval and ranking models. However, the primary focus on NLP and LLMs makes it less central to the user's core research themes, which are more focused on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Efficient and Versatile Model for Multilingual Information Retrieval of Islamic Text: Development and Deployment in Real-World Scenarios",
        "abstract": "Despite recent advancements in Multilingual Information Retrieval (MLIR), a\nsignificant gap remains between research and practical deployment. Many studies\nassess MLIR performance in isolated settings, limiting their applicability to\nreal-world scenarios. In this work, we leverage the unique characteristics of\nthe Quranic multilingual corpus to examine the optimal strategies to develop an\nad-hoc IR system for the Islamic domain that is designed to satisfy users'\ninformation needs in multiple languages. We prepared eleven retrieval models\nemploying four training approaches: monolingual, cross-lingual,\ntranslate-train-all, and a novel mixed method combining cross-lingual and\nmonolingual techniques. Evaluation on an in-domain dataset demonstrates that\nthe mixed approach achieves promising results across diverse retrieval\nscenarios. Furthermore, we provide a detailed analysis of how different\ntraining configurations affect the embedding space and their implications for\nmultilingual retrieval effectiveness. Finally, we discuss deployment\nconsiderations, emphasizing the cost-efficiency of deploying a single\nversatile, lightweight model for real-world MLIR applications.",
        "url": "http://arxiv.org/abs/2509.15380v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15380v1",
        "arxiv_id": "2509.15380v1",
        "authors": [
            "Vera Pavlova",
            "Mohammed Makhlouf"
        ],
        "submitted": "2025-09-18 19:32:07",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to the field of Information Retrieval, particularly in the context of multilingual retrieval. The focus on developing a versatile model for real-world scenarios aligns with the user's interest in query understanding and ranking models. However, the specific domain of Islamic text may not be directly applicable to the user's e-commerce background, thus preventing a perfect score."
    },
    {
        "title": "EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol",
        "abstract": "Background: Large language models (LLMs) show promise in medicine, but their\ndeployment in hospitals is limited by restricted access to electronic health\nrecord (EHR) systems. The Model Context Protocol (MCP) enables integration\nbetween LLMs and external tools.\n  Objective: To evaluate whether an LLM connected to an EHR database via MCP\ncan autonomously retrieve clinically relevant information in a real hospital\nsetting.\n  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated\nwith the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct\nagent to interact with it. Six tasks were tested, derived from use cases of the\ninfection control team (ICT). Eight patients discussed at ICT conferences were\nretrospectively analyzed. Agreement with physician-generated gold standards was\nmeasured.\n  Results: The LLM consistently selected and executed the correct MCP tools.\nExcept for two tasks, all tasks achieved near-perfect accuracy. Performance was\nlower in the complex task requiring time-dependent calculations. Most errors\narose from incorrect arguments or misinterpretation of tool results. Responses\nfrom EHR-MCP were reliable, though long and repetitive data risked exceeding\nthe context window.\n  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a\nreal hospital setting, achieving near-perfect performance in simple tasks while\nhighlighting challenges in complex ones. EHR-MCP provides an infrastructure for\nsecure, consistent data access and may serve as a foundation for hospital AI\nagents. Future work should extend beyond retrieval to reasoning, generation,\nand clinical impact assessment, paving the way for effective integration of\ngenerative AI into clinical practice.",
        "url": "http://arxiv.org/abs/2509.15957v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15957v1",
        "arxiv_id": "2509.15957v1",
        "authors": [
            "Kanato Masayoshi",
            "Masahiro Hashimoto",
            "Ryoichi Yokoyama",
            "Naoki Toda",
            "Yoshifumi Uwamino",
            "Shogo Fukuda",
            "Ho Namkoong",
            "Masahiro Jinzaki"
        ],
        "submitted": "2025-09-19 13:17:16",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'ctr' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the application of Large Language Models (LLMs) in clinical information retrieval, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the focus on clinical data and healthcare settings is not a central match to your background in e-commerce and interests in deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning",
        "abstract": "In-context learning (ICL) has emerged as a powerful paradigm for adapting\nlarge language models (LLMs) to new and data-scarce tasks using only a few\ncarefully selected task-specific examples presented in the prompt. However,\ngiven the limited context size of LLMs, a fundamental question arises: Which\nexamples should be selected to maximize performance on a given user query?\nWhile nearest-neighbor-based methods like KATE have been widely adopted for\nthis purpose, they suffer from well-known drawbacks in high-dimensional\nembedding spaces, including poor generalization and a lack of diversity. In\nthis work, we study this problem of example selection in ICL from a principled,\ninformation theory-driven perspective. We first model an LLM as a linear\nfunction over input embeddings and frame the example selection task as a\nquery-specific optimization problem: selecting a subset of exemplars from a\nlarger example bank that minimizes the prediction error on a specific query.\nThis formulation departs from traditional generalization-focused learning\ntheoretic approaches by targeting accurate prediction for a specific query\ninstance. We derive a principled surrogate objective that is approximately\nsubmodular, enabling the use of a greedy algorithm with an approximation\nguarantee. We further enhance our method by (i) incorporating the kernel trick\nto operate in high-dimensional feature spaces without explicit mappings, and\n(ii) introducing an optimal design-based regularizer to encourage diversity in\nthe selected examples. Empirically, we demonstrate significant improvements\nover standard retrieval methods across a suite of classification tasks,\nhighlighting the benefits of structure-aware, diverse example selection for ICL\nin real-world, label-scarce scenarios.",
        "url": "http://arxiv.org/abs/2509.15676v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15676v1",
        "arxiv_id": "2509.15676v1",
        "authors": [
            "Vaibhav Singh",
            "Soumya Suvra Ghosal",
            "Kapu Nirmal Joshua",
            "Soumyabrata Pal",
            "Sayak Ray Chowdhury"
        ],
        "submitted": "2025-09-19 06:50:03",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper focuses on in-context learning and example selection for large language models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, it does not directly address user behavior modeling or click models, and its primary focus is on NLP rather than IR. While it explores a relevant topic, it does not align closely with the user's core research themes."
    },
    {
        "title": "Relevance to Utility: Process-Supervised Rewrite for RAG",
        "abstract": "Retrieval-Augmented Generation systems often suffer from a gap between\noptimizing retrieval relevance and generative utility: retrieved documents may\nbe topically relevant but still lack the content needed for effective reasoning\nduring generation. While existing \"bridge\" modules attempt to rewrite the\nretrieved text for better generation, we show how they fail to capture true\ndocument utility. In this work, we propose R2U, with a key distinction of\ndirectly optimizing to maximize the probability of generating a correct answer\nthrough process supervision. As such direct observation is expensive, we also\npropose approximating an efficient distillation pipeline by scaling the\nsupervision from LLMs, which helps the smaller rewriter model generalize\nbetter. We evaluate our method across multiple open-domain question-answering\nbenchmarks. The empirical results demonstrate consistent improvements over\nstrong bridging baselines.",
        "url": "http://arxiv.org/abs/2509.15577v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15577v1",
        "arxiv_id": "2509.15577v1",
        "authors": [
            "Jaeyoung Kim",
            "Jongho Kim",
            "Seung-won Hwang",
            "Seoho Song",
            "Young-In Song"
        ],
        "submitted": "2025-09-19 04:24:57",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval, specifically in the context of Retrieval-Augmented Generation systems. However, the focus on generative utility and process supervision is more aligned with Natural Language Processing and Generation, rather than your primary focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "LLM Cache Bandit Revisited: Addressing Query Heterogeneity for Cost-Effective LLM Inference",
        "abstract": "This paper revisits the LLM cache bandit problem, with a special focus on\naddressing the query heterogeneity for cost-effective LLM inference. Previous\nworks often assume uniform query sizes. Heterogeneous query sizes introduce a\ncombinatorial structure for cache selection, making the cache replacement\nprocess more computationally and statistically challenging. We treat optimal\ncache selection as a knapsack problem and employ an accumulation-based strategy\nto effectively balance computational overhead and cache updates. In theoretical\nanalysis, we prove that the regret of our algorithm achieves an $O(\\sqrt{MNT})$\nbound, improving the coefficient of $\\sqrt{MN}$ compared to the $O(MN\\sqrt{T})$\nresult in Berkeley, where $N$ is the total number of queries and $M$ is the\ncache size. Additionally, we also provide a problem-dependent bound, which was\nabsent in previous works. The experiment rely on real-world data show that our\nalgorithm reduces the total cost by approximately 12\\%.",
        "url": "http://arxiv.org/abs/2509.15515v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15515v1",
        "arxiv_id": "2509.15515v1",
        "authors": [
            "Hantao Yang",
            "Hong Xie",
            "Defu Lian",
            "Enhong Chen"
        ],
        "submitted": "2025-09-19 01:39:08",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper focuses on optimizing LLM inference, but its primary contribution is in cache management and query heterogeneity, which is not directly related to query understanding, ranking models, or user behavior modeling. While it touches on computational efficiency, it does not delve into deep semantic understanding or real-time relevance optimization, which are core aspects of your research interests."
    },
    {
        "title": "Optimizing Product Deduplication in E-Commerce with Multimodal Embeddings",
        "abstract": "In large scale e-commerce marketplaces, duplicate product listings frequently\ncause consumer confusion and operational inefficiencies, degrading trust on the\nplatform and increasing costs. Traditional keyword-based search methodologies\nfalter in accurately identifying duplicates due to their reliance on exact\ntextual matches, neglecting semantic similarities inherent in product titles.\nTo address these challenges, we introduce a scalable, multimodal product\ndeduplication designed specifically for the e-commerce domain. Our approach\nemploys a domain-specific text model grounded in BERT architecture in\nconjunction with MaskedAutoEncoders for image representations. Both of these\narchitectures are augmented with dimensionality reduction techniques to produce\ncompact 128-dimensional embeddings without significant information loss.\nComplementing this, we also developed a novel decider model that leverages both\ntext and image vectors. By integrating these feature extraction mechanisms with\nMilvus, an optimized vector database, our system can facilitate efficient and\nhigh-precision similarity searches across extensive product catalogs exceeding\n200 million items with just 100GB of system RAM consumption. Empirical\nevaluations demonstrate that our matching system achieves a macro-average F1\nscore of 0.90, outperforming third-party solutions which attain an F1 score of\n0.83. Our findings show the potential of combining domain-specific adaptations\nwith state-of-the-art machine learning techniques to mitigate duplicate\nlistings in large-scale e-commerce environments.",
        "url": "http://arxiv.org/abs/2509.15858v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15858v1",
        "arxiv_id": "2509.15858v1",
        "authors": [
            "Aysenur Kulunk",
            "Berk Taskin",
            "M. Furkan Eseoglu",
            "H. Bahadir Sahin"
        ],
        "submitted": "2025-09-19 10:49:39",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 7,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval, particularly in the e-commerce domain. The focus on product deduplication and multimodal embeddings is relevant to your work on query understanding and ranking models. However, the paper's emphasis on e-commerce and product catalogs is not a central match to your broader interests in IR and NLP."
    },
    {
        "title": "RAVE: Retrieval and Scoring Aware Verifiable Claim Detection",
        "abstract": "The rapid spread of misinformation on social media underscores the need for\nscalable fact-checking tools. A key step is claim detection, which identifies\nstatements that can be objectively verified. Prior approaches often rely on\nlinguistic cues or claim check-worthiness, but these struggle with vague\npolitical discourse and diverse formats such as tweets. We present RAVE\n(Retrieval and Scoring Aware Verifiable Claim Detection), a framework that\ncombines evidence retrieval with structured signals of relevance and source\ncredibility. Experiments on CT22-test and PoliClaim-test show that RAVE\nconsistently outperforms text-only and retrieval-based baselines in both\naccuracy and F1.",
        "url": "http://arxiv.org/abs/2509.15793v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15793v1",
        "arxiv_id": "2509.15793v1",
        "authors": [
            "Yufeng Li",
            "Arkaitz Zubiaga"
        ],
        "submitted": "2025-09-19 09:23:41",
        "source": "arxiv",
        "comment": "5 pages, 1 figure",
        "score": 5,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on claim detection and fact-checking, which is somewhat related to information retrieval, but it's not directly aligned with your core research themes. The use of retrieval and scoring aware framework is relevant, but the application domain is limited to social media and fact-checking, which doesn't match your broader interests in e-commerce and deep semantic understanding."
    },
    {
        "title": "Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences",
        "abstract": "When do machine learning systems fail to generalize, and what mechanisms\ncould improve their generalization? Here, we draw inspiration from cognitive\nscience to argue that one weakness of machine learning systems is their failure\nto exhibit latent learning -- learning information that is not relevant to the\ntask at hand, but that might be useful in a future task. We show how this\nperspective links failures ranging from the reversal curse in language modeling\nto new findings on agent-based navigation. We then highlight how cognitive\nscience points to episodic memory as a potential part of the solution to these\nissues. Correspondingly, we show that a system with an oracle retrieval\nmechanism can use learning experiences more flexibly to generalize better\nacross many of these challenges. We also identify some of the essential\ncomponents for effectively using retrieval, including the importance of\nwithin-example in-context learning for acquiring the ability to use information\nacross retrieved examples. In summary, our results illustrate one possible\ncontributor to the relative data inefficiency of current machine learning\nsystems compared to natural intelligence, and help to understand how retrieval\nmethods can complement parametric learning to improve generalization.",
        "url": "http://arxiv.org/abs/2509.16189v1",
        "pdf_url": "http://arxiv.org/pdf/2509.16189v1",
        "arxiv_id": "2509.16189v1",
        "authors": [
            "Andrew Kyle Lampinen",
            "Martin Engelcke",
            "Yuxuan Li",
            "Arslan Chaudhry",
            "James L. McClelland"
        ],
        "submitted": "2025-09-19 17:49:25",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses machine learning generalization and proposes a solution using episodic memory and retrieval methods. While it touches on retrieval, it's not specifically focused on information retrieval, ranking models, or user behavior modeling, which are core areas of interest. The connection to cognitive science and natural intelligence is intriguing, but not directly relevant to the user's primary research themes."
    },
    {
        "title": "Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay Assessment",
        "abstract": "Automated Essay Scoring (AES) systems now reach near human agreement on some\npublic benchmarks, yet real-world adoption, especially in high-stakes\nexaminations, remains limited. A principal obstacle is that most models output\na single score without any accompanying measure of confidence or explanation.\nWe address this gap with conformal prediction, a distribution-free wrapper that\nequips any classifier with set-valued outputs and formal coverage guarantees.\nTwo open-source large language models (Llama-3 8B and Qwen-2.5 3B) are\nfine-tuned on three diverse corpora (ASAP, TOEFL11, Cambridge-FCE) and\ncalibrated at a 90 percent risk level. Reliability is assessed with UAcc, an\nuncertainty-aware accuracy that rewards models for being both correct and\nconcise. To our knowledge, this is the first work to combine conformal\nprediction and UAcc for essay scoring. The calibrated models consistently meet\nthe coverage target while keeping prediction sets compact, indicating that\nopen-source, mid-sized LLMs can already support teacher-in-the-loop AES; we\ndiscuss scaling and broader user studies as future work.",
        "url": "http://arxiv.org/abs/2509.15926v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15926v1",
        "arxiv_id": "2509.15926v1",
        "authors": [
            "Ahmed Karim",
            "Qiao Wang",
            "Zheng Yuan"
        ],
        "submitted": "2025-09-19 12:28:50",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025 (Main Conference). Camera-ready version",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to the user's core research themes in Information Retrieval and Search technologies. Although it involves Natural Language Processing (NLP) and large language models, the focus on Automated Essay Assessment and uncertainty-calibrated models is not a central match for the user's interests."
    },
    {
        "title": "Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics",
        "abstract": "Large Language Models (LLMs) excel at linear reasoning tasks but remain\nunderexplored on non-linear structures such as those found in natural debates,\nwhich are best expressed as argument graphs. We evaluate whether LLMs can\napproximate structured reasoning from Computational Argumentation Theory (CAT).\nSpecifically, we use Quantitative Argumentation Debate (QuAD) semantics, which\nassigns acceptability scores to arguments based on their attack and support\nrelations. Given only dialogue-formatted debates from two NoDE datasets, models\nare prompted to rank arguments without access to the underlying graph. We test\nseveral LLMs under advanced instruction strategies, including Chain-of-Thought\nand In-Context Learning. While models show moderate alignment with QuAD\nrankings, performance degrades with longer inputs or disrupted discourse flow.\nAdvanced prompting helps mitigate these effects by reducing biases related to\nargument length and position. Our findings highlight both the promise and\nlimitations of LLMs in modeling formal argumentation semantics and motivate\nfuture work on graph-aware reasoning.",
        "url": "http://arxiv.org/abs/2509.15739v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15739v1",
        "arxiv_id": "2509.15739v1",
        "authors": [
            "Reza Sanayei",
            "Srdjan Vesic",
            "Eduardo Blanco",
            "Mihai Surdeanu"
        ],
        "submitted": "2025-09-19 08:10:32",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 Findings",
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the application of Large Language Models (LLMs) in evaluating debates, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on argumentation theory semantics and formal graph-aware reasoning is not directly aligned with the user's primary research interests in IR and NLP. The paper's findings on LLMs' limitations in modeling formal argumentation semantics may be of interest, but it does not directly contribute to the user's core research themes."
    },
    {
        "title": "SERVAL: Surprisingly Effective Zero-Shot Visual Document Retrieval Powered by Large Vision and Language Models",
        "abstract": "Visual Document Retrieval (VDR) typically operates as text-to-image retrieval\nusing specialized bi-encoders trained to directly embed document images. We\nrevisit a zero-shot generate-and-encode pipeline: a vision-language model first\nproduces a detailed textual description of each document image, which is then\nembedded by a standard text encoder. On the ViDoRe-v2 benchmark, the method\nreaches 63.4% nDCG@5, surpassing the strongest specialised multi-vector visual\ndocument encoder. It also scales better to large collections and offers broader\nmultilingual coverage. Analysis shows that modern vision-language models\ncapture complex textual and visual cues with sufficient granularity to act as a\nreusable semantic proxy. By offloading modality alignment to pretrained\nvision-language models, our approach removes the need for computationally\nintensive text-image contrastive training and establishes a strong zero-shot\nbaseline for future VDR systems.",
        "url": "http://arxiv.org/abs/2509.15432v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15432v1",
        "arxiv_id": "2509.15432v1",
        "authors": [
            "Thong Nguyen",
            "Yibin Lei",
            "Jia-Huei Ju",
            "Andrew Yates"
        ],
        "submitted": "2025-09-18 21:11:13",
        "source": "arxiv",
        "comment": "Accepted",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Visual Document Retrieval, a topic related to Information Retrieval, but it doesn't align with the user's core research themes of query understanding, ranking models, and user behavior modeling. The use of large vision and language models is also more relevant to NLP, but the paper's abstract doesn't mention any connection to real-time relevance optimization or deep semantic understanding."
    },
    {
        "title": "DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning",
        "abstract": "Despite the significant breakthrough of Mixture-of-Experts (MoE), the\nincreasing scale of these MoE models presents huge memory and storage\nchallenges. Existing MoE pruning methods, which involve reducing parameter size\nwith a uniform sparsity across all layers, often lead to suboptimal outcomes\nand performance degradation due to varying expert redundancy in different MoE\nlayers. To address this, we propose a non-uniform pruning strategy, dubbed\n\\textbf{Di}fferentiable \\textbf{E}xpert \\textbf{P}runing (\\textbf{DiEP}), which\nadaptively adjusts pruning rates at the layer level while jointly learning\ninter-layer importance, effectively capturing the varying redundancy across\ndifferent MoE layers. By transforming the global discrete search space into a\ncontinuous one, our method handles exponentially growing non-uniform expert\ncombinations, enabling adaptive gradient-based pruning. Extensive experiments\non five advanced MoE models demonstrate the efficacy of our method across\nvarious NLP tasks. Notably, \\textbf{DiEP} retains around 92\\% of original\nperformance on Mixtral 8$\\times$7B with only half the experts, outperforming\nother pruning methods by up to 7.1\\% on the challenging MMLU dataset.",
        "url": "http://arxiv.org/abs/2509.16105v1",
        "pdf_url": "http://arxiv.org/pdf/2509.16105v1",
        "arxiv_id": "2509.16105v1",
        "authors": [
            "Sikai Bai",
            "Haoxi Li",
            "Jie Zhang",
            "Zicong Hong",
            "Song Guo"
        ],
        "submitted": "2025-09-19 15:47:42",
        "source": "arxiv",
        "comment": "18 pages",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on model compression and pruning techniques for Mixture-of-Experts models, primarily in the context of Natural Language Processing (NLP). While it involves deep learning and optimization, it does not directly relate to information retrieval, query understanding, or user behavior modeling, which are core areas of your research interests."
    },
    {
        "title": "BEFT: Bias-Efficient Fine-Tuning of Language Models",
        "abstract": "Fine-tuning all-bias-terms stands out among various parameter-efficient\nfine-tuning (PEFT) techniques, owing to its out-of-the-box usability and\ncompetitive performance, especially in low-data regimes. Bias-only fine-tuning\nhas the potential for unprecedented parameter efficiency. However, the link\nbetween fine-tuning different bias terms (i.e., bias terms in the query, key,\nor value projections) and downstream performance remains unclear. The existing\napproaches, e.g., based on the magnitude of bias change or empirical Fisher\ninformation, provide limited guidance for selecting the particular bias term\nfor effective fine-tuning. In this paper, we propose an approach for selecting\nthe bias term to be fine-tuned, forming the foundation of our bias-efficient\nfine-tuning (BEFT). We extensively evaluate our bias-efficient approach against\nother bias-selection approaches, across a wide range of large language models\n(LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B\nparameters. Our results demonstrate the effectiveness and superiority of our\nbias-efficient approach on diverse downstream tasks, including classification,\nmultiple-choice, and generation tasks.",
        "url": "http://arxiv.org/abs/2509.15974v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15974v1",
        "arxiv_id": "2509.15974v1",
        "authors": [
            "Baichuan Huang",
            "Ananth Balashankar",
            "Amir Aminifar"
        ],
        "submitted": "2025-09-19 13:35:07",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is primarily focused on fine-tuning language models, which is a topic in NLP, but it does not directly relate to information retrieval, query understanding, or ranking models, which are core areas of your research interests."
    },
    {
        "title": "Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning",
        "abstract": "While the reasoning abilities of large language models (LLMs) continue to\nadvance, it remains unclear how such ability varies across languages in\nmultilingual LLMs and whether different languages produce reasoning paths that\ncomplement each other. To investigate this question, we train a reward model to\nrank generated responses for a given question across languages. Our results\nshow that our cross-lingual reward model substantially improves mathematical\nreasoning performance compared to using reward modeling within a single\nlanguage, benefiting even high-resource languages. While English often exhibits\nthe highest performance in multilingual models, we find that cross-lingual\nsampling particularly benefits English under low sampling budgets. Our findings\nreveal new opportunities to improve multilingual reasoning by leveraging the\ncomplementary strengths of diverse languages.",
        "url": "http://arxiv.org/abs/2509.15811v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15811v1",
        "arxiv_id": "2509.15811v1",
        "authors": [
            "Sara Rajaee",
            "Rochelle Choenni",
            "Ekaterina Shutova",
            "Christof Monz"
        ],
        "submitted": "2025-09-19 09:38:54",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on cross-lingual reward modeling for mathematical reasoning, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves large language models and ranking, the context is specific to mathematical reasoning and multilingual models, making it somewhat tangential to the user's interests."
    },
    {
        "title": "SciEvent: Benchmarking Multi-domain Scientific Event Extraction",
        "abstract": "Scientific information extraction (SciIE) has primarily relied on\nentity-relation extraction in narrow domains, limiting its applicability to\ninterdisciplinary research and struggling to capture the necessary context of\nscientific information, often resulting in fragmented or conflicting\nstatements. In this paper, we introduce SciEvent, a novel multi-domain\nbenchmark of scientific abstracts annotated via a unified event extraction (EE)\nschema designed to enable structured and context-aware understanding of\nscientific content. It includes 500 abstracts across five research domains,\nwith manual annotations of event segments, triggers, and fine-grained\narguments. We define SciIE as a multi-stage EE pipeline: (1) segmenting\nabstracts into core scientific activities--Background, Method, Result, and\nConclusion; and (2) extracting the corresponding triggers and arguments.\nExperiments with fine-tuned EE models, large language models (LLMs), and human\nannotators reveal a performance gap, with current models struggling in domains\nsuch as sociology and humanities. SciEvent serves as a challenging benchmark\nand a step toward generalizable, multi-domain SciIE.",
        "url": "http://arxiv.org/abs/2509.15620v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15620v1",
        "arxiv_id": "2509.15620v1",
        "authors": [
            "Bofu Dong",
            "Pritesh Shah",
            "Sumedh Sonawane",
            "Tiyasha Banerjee",
            "Erin Brady",
            "Xinya Du",
            "Ming Jiang"
        ],
        "submitted": "2025-09-19 05:32:50",
        "source": "arxiv",
        "comment": "9 pages, 8 figures (main); 22 pages, 11 figures (appendix). Accepted\n  to EMNLP 2025 (Main Conference)",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on scientific event extraction, which is related to information retrieval and natural language processing. However, it is more specific to scientific abstracts and event extraction, which doesn't directly align with the user's primary focus on query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat tangential to the user's core research themes."
    },
    {
        "title": "Deep learning and abstractive summarisation for radiological reports: an empirical study for adapting the PEGASUS models' family with scarce data",
        "abstract": "Regardless of the rapid development of artificial intelligence, abstractive\nsummarisation is still challenging for sensitive and data-restrictive domains\nlike medicine. With the increasing number of imaging, the relevance of\nautomated tools for complex medical text summarisation is expected to become\nhighly relevant. In this paper, we investigated the adaptation via fine-tuning\nprocess of a non-domain-specific abstractive summarisation encoder-decoder\nmodel family, and gave insights to practitioners on how to avoid over- and\nunderfitting. We used PEGASUS and PEGASUS-X, on a medium-sized radiological\nreports public dataset. For each model, we comprehensively evaluated two\ndifferent checkpoints with varying sizes of the same training data. We\nmonitored the models' performances with lexical and semantic metrics during the\ntraining history on the fixed-size validation set. PEGASUS exhibited different\nphases, which can be related to epoch-wise double-descent, or\npeak-drop-recovery behaviour. For PEGASUS-X, we found that using a larger\ncheckpoint led to a performance detriment. This work highlights the challenges\nand risks of fine-tuning models with high expressivity when dealing with scarce\ntraining data, and lays the groundwork for future investigations into more\nrobust fine-tuning strategies for summarisation models in specialised domains.",
        "url": "http://arxiv.org/abs/2509.15419v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15419v1",
        "arxiv_id": "2509.15419v1",
        "authors": [
            "Claudio Benzoni",
            "Martina Langhals",
            "Martin Boeker",
            "Luise Modersohn",
            "Máté E. Maros"
        ],
        "submitted": "2025-09-18 20:51:33",
        "source": "arxiv",
        "comment": "14 pages, 4 figures, and 3 tables",
        "score": 3,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is primarily focused on abstractive summarization in the medical domain, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves deep learning and fine-tuning models, the context and application are quite different from the user's interests."
    },
    {
        "title": "RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation",
        "abstract": "Large language models excel at function- and file-level code generation, yet\ngenerating complete repositories from scratch remains a fundamental challenge.\nThis process demands coherent and reliable planning across proposal- and\nimplementation-level stages, while natural language, due to its ambiguity and\nverbosity, is ill-suited for faithfully representing complex software\nstructures. To address this, we introduce the Repository Planning Graph (RPG),\na persistent representation that unifies proposal- and implementation-level\nplanning by encoding capabilities, file structures, data flows, and functions\nin one graph. RPG replaces ambiguous natural language with an explicit\nblueprint, enabling long-horizon planning and scalable repository generation.\nBuilding on RPG, we develop ZeroRepo, a graph-driven framework for repository\ngeneration from scratch. It operates in three stages: proposal-level planning\nand implementation-level refinement to construct the graph, followed by\ngraph-guided code generation with test validation. To evaluate this setting, we\nconstruct RepoCraft, a benchmark of six real-world projects with 1,052 tasks.\nOn RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly\n3.9$\\times$ the strongest baseline (Claude Code) and about 64$\\times$ other\nbaselines. It attains 81.5% functional coverage and a 69.7% pass rate,\nexceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further\nanalysis shows that RPG models complex dependencies, enables progressively more\nsophisticated planning through near-linear scaling, and enhances LLM\nunderstanding of repositories, thereby accelerating agent localization.",
        "url": "http://arxiv.org/abs/2509.16198v1",
        "pdf_url": "http://arxiv.org/pdf/2509.16198v1",
        "arxiv_id": "2509.16198v1",
        "authors": [
            "Jane Luo",
            "Xin Zhang",
            "Steven Liu",
            "Jie Wu",
            "Yiming Huang",
            "Yangyu Huang",
            "Chengyu Yin",
            "Ying Xin",
            "Jianfeng Liu",
            "Yuefeng Zhan",
            "Hao Sun",
            "Qi Chen",
            "Scarlett Li",
            "Mao Yang"
        ],
        "submitted": "2025-09-19 17:58:14",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on codebase generation and repository planning, which is unrelated to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing."
    },
    {
        "title": "It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge",
        "abstract": "Ambiguous words or underspecified references require interlocutors to resolve\nthem, often by relying on shared context and commonsense knowledge. Therefore,\nwe systematically investigate whether Large Language Models (LLMs) can leverage\ncommonsense to resolve referential ambiguity in multi-turn conversations and\nanalyze their behavior when ambiguity persists. Further, we study how requests\nfor simplified language affect this capacity. Using a novel multilingual\nevaluation dataset, we test DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, and\nLlama-3.1-8B via LLM-as-Judge and human annotations. Our findings indicate that\ncurrent LLMs struggle to resolve ambiguity effectively: they tend to commit to\na single interpretation or cover all possible references, rather than hedging\nor seeking clarification. This limitation becomes more pronounced under\nsimplification prompts, which drastically reduce the use of commonsense\nreasoning and diverse response strategies. Fine-tuning Llama-3.1-8B with Direct\nPreference Optimization substantially improves ambiguity resolution across all\nrequest types. These results underscore the need for advanced fine-tuning to\nimprove LLMs' handling of ambiguity and to ensure robust performance across\ndiverse communication styles.",
        "url": "http://arxiv.org/abs/2509.16107v1",
        "pdf_url": "http://arxiv.org/pdf/2509.16107v1",
        "arxiv_id": "2509.16107v1",
        "authors": [
            "Lukas Ellinger",
            "Georg Groh"
        ],
        "submitted": "2025-09-19 15:49:26",
        "source": "arxiv",
        "comment": "Accepted by UncertaiNLP workshop @ EMNLP 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper explores the use of Large Language Models (LLMs) in resolving referential ambiguity, which is related to query understanding and ranking models in Information Retrieval. However, the focus on multi-turn conversations and language simplification prompts is somewhat tangential to the user's core research themes, despite the use of LLMs, which are relevant to the user's interests in NLP."
    },
    {
        "title": "Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech",
        "abstract": "Spoken dialogue systems increasingly employ large language models (LLMs) to\nleverage their advanced reasoning capabilities. However, direct application of\nLLMs in spoken communication often yield suboptimal results due to mismatches\nbetween optimal textual and verbal delivery. While existing approaches adapt\nLLMs to produce speech-friendly outputs, their impact on reasoning performance\nremains underexplored. In this work, we propose Think-Verbalize-Speak, a\nframework that decouples reasoning from spoken delivery to preserve the full\nreasoning capacity of LLMs. Central to our method is verbalizing, an\nintermediate step that translates thoughts into natural, speech-ready text. We\nalso introduce ReVerT, a latency-efficient verbalizer based on incremental and\nasynchronous summarization. Experiments across multiple benchmarks show that\nour method enhances speech naturalness and conciseness with minimal impact on\nreasoning. The project page with the dataset and the source code is available\nat https://yhytoto12.github.io/TVS-ReVerT",
        "url": "http://arxiv.org/abs/2509.16028v1",
        "pdf_url": "http://arxiv.org/pdf/2509.16028v1",
        "arxiv_id": "2509.16028v1",
        "authors": [
            "Sang Hoon Woo",
            "Sehun Lee",
            "Kang-wook Kim",
            "Gunhee Kim"
        ],
        "submitted": "2025-09-19 14:34:22",
        "source": "arxiv",
        "comment": "EMNLP 2025 Main. Project page: https://yhytoto12.github.io/TVS-ReVerT",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be primarily focused on spoken dialogue systems, large language models, and speech generation, which are not directly related to the user's core research themes in Information Retrieval and Search technologies. While it touches on natural language processing, the emphasis is on spoken communication rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-grained Emotions",
        "abstract": "Existing digital mental wellness tools often overlook the nuanced emotional\nstates underlying everyday challenges. For example, pre-sleep anxiety affects\nmore than 1.5 billion people worldwide, yet current approaches remain largely\nstatic and \"one-size-fits-all\", failing to adapt to individual needs. In this\nwork, we present EmoHeal, an end-to-end system that delivers personalized,\nthree-stage supportive narratives. EmoHeal detects 27 fine-grained emotions\nfrom user text with a fine-tuned XLM-RoBERTa model, mapping them to musical\nparameters via a knowledge graph grounded in music therapy principles (GEMS,\niso-principle). EmoHeal retrieves audiovisual content using the CLAMP3 model to\nguide users from their current state toward a calmer one\n(\"match-guide-target\"). A within-subjects study (N=40) demonstrated significant\nsupportive effects, with participants reporting substantial mood improvement\n(M=4.12, p<0.001) and high perceived emotion recognition accuracy (M=4.05,\np<0.001). A strong correlation between perceived accuracy and therapeutic\noutcome (r=0.72, p<0.001) validates our fine-grained approach. These findings\nestablish the viability of theory-driven, emotion-aware digital wellness tools\nand provides a scalable AI blueprint for operationalizing music therapy\nprinciples.",
        "url": "http://arxiv.org/abs/2509.15986v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15986v1",
        "arxiv_id": "2509.15986v1",
        "authors": [
            "Xinchen Wan",
            "Jinhua Liang",
            "Huan Zhang"
        ],
        "submitted": "2025-09-19 13:52:22",
        "source": "arxiv",
        "comment": "5 pages, 5 figures. Submitted to the 2026 IEEE International\n  Conference on Acoustics, Speech and Signal Processing (ICASSP 2026)",
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to information retrieval, but its focus on music retrieval and fine-grained emotion detection is not directly aligned with your core research themes. However, the use of deep learning models and knowledge graphs is relevant to your interests in NLP and data mining."
    },
    {
        "title": "Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems",
        "abstract": "While multimodal LLMs (MLLMs) demonstrate remarkable reasoning progress,\ntheir application in specialized scientific domains like physics reveals\nsignificant gaps in current evaluation benchmarks. Specifically, existing\nbenchmarks often lack fine-grained subject coverage, neglect the step-by-step\nreasoning process, and are predominantly English-centric, failing to\nsystematically evaluate the role of visual information. Therefore, we introduce\n\\textbf {Multi-Physics} for Chinese physics reasoning, a comprehensive\nbenchmark that includes 5 difficulty levels, featuring 1,412 image-associated,\nmultiple-choice questions spanning 11 high-school physics subjects. We employ a\ndual evaluation framework to evaluate 20 different MLLMs, analyzing both final\nanswer accuracy and the step-by-step integrity of their chain-of-thought.\nFurthermore, we systematically study the impact of difficulty level and visual\ninformation by comparing the model performance before and after changing the\ninput mode. Our work provides not only a fine-grained resource for the\ncommunity but also offers a robust methodology for dissecting the multimodal\nreasoning process of state-of-the-art MLLMs, and our dataset and code have been\nopen-sourced: https://github.com/luozhongze/Multi-Physics.",
        "url": "http://arxiv.org/abs/2509.15839v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15839v1",
        "arxiv_id": "2509.15839v1",
        "authors": [
            "Zhongze Luo",
            "Zhenshuai Yin",
            "Yongxin Guo",
            "Zhichao Wang",
            "Jionghao Zhu",
            "Xiaoying Tang"
        ],
        "submitted": "2025-09-19 10:18:48",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on multimodal LLMs for physics reasoning in Chinese, which is not directly related to Information Retrieval, Search technologies, or user behavior modeling. Although it involves deep semantic understanding, the context is limited to a specific domain (physics) and language (Chinese), making it less relevant to the user's broader research interests."
    },
    {
        "title": "UPRPRC: Unified Pipeline for Reproducing Parallel Resources -- Corpus from the United Nations",
        "abstract": "The quality and accessibility of multilingual datasets are crucial for\nadvancing machine translation. However, previous corpora built from United\nNations documents have suffered from issues such as opaque process, difficulty\nof reproduction, and limited scale. To address these challenges, we introduce a\ncomplete end-to-end solution, from data acquisition via web scraping to text\nalignment. The entire process is fully reproducible, with a minimalist\nsingle-machine example and optional distributed computing steps for\nscalability. At its core, we propose a new Graph-Aided Paragraph Alignment\n(GAPA) algorithm for efficient and flexible paragraph-level alignment. The\nresulting corpus contains over 713 million English tokens, more than doubling\nthe scale of prior work. To the best of our knowledge, this represents the\nlargest publicly available parallel corpus composed entirely of\nhuman-translated, non-AI-generated content. Our code and corpus are accessible\nunder the MIT License.",
        "url": "http://arxiv.org/abs/2509.15789v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15789v1",
        "arxiv_id": "2509.15789v1",
        "authors": [
            "Qiuyang Lu",
            "Fangjian Shen",
            "Zhengkai Tang",
            "Qiang Liu",
            "Hexuan Cheng",
            "Hui Liu",
            "Wushao Wen"
        ],
        "submitted": "2025-09-19 09:21:13",
        "source": "arxiv",
        "comment": "5 pages, 1 figure, submitted to ICASSP2026",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing, as it focuses on corpus creation and parallel resource reproduction for machine translation, rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion",
        "abstract": "We present a multimodal fusion framework that bridges pre-trained\ndecoder-based large language models (LLM) and acoustic encoder-decoder\narchitectures such as Whisper, with the aim of building speech-enabled LLMs.\nInstead of directly using audio embeddings, we explore an intermediate\naudio-conditioned text space as a more effective mechanism for alignment. Our\nmethod operates fully in continuous text representation spaces, fusing\nWhisper's hidden decoder states with those of an LLM through cross-modal\nattention, and supports both offline and streaming modes. We introduce\n\\textit{VoxKrikri}, the first Greek speech LLM, and show through analysis that\nour approach effectively aligns representations across modalities. These\nresults highlight continuous space fusion as a promising path for multilingual\nand low-resource speech LLMs, while achieving state-of-the-art results for\nAutomatic Speech Recognition in Greek, providing an average $\\sim20\\%$ relative\nimprovement across benchmarks.",
        "url": "http://arxiv.org/abs/2509.15667v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15667v1",
        "arxiv_id": "2509.15667v1",
        "authors": [
            "Dimitrios Damianos",
            "Leon Voukoutis",
            "Georgios Paraskevopoulos",
            "Vassilis Katsouros"
        ],
        "submitted": "2025-09-19 06:42:42",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on speech and language fusion, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves language models, the context is speech recognition rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Multilingual LLM Prompting Strategies for Medical English-Vietnamese Machine Translation",
        "abstract": "Medical English-Vietnamese machine translation (En-Vi MT) is essential for\nhealthcare access and communication in Vietnam, yet Vietnamese remains a\nlow-resource and under-studied language. We systematically evaluate prompting\nstrategies for six multilingual LLMs (0.5B-9B parameters) on the MedEV dataset,\ncomparing zero-shot, few-shot, and dictionary-augmented prompting with Meddict,\nan English-Vietnamese medical lexicon. Results show that model scale is the\nprimary driver of performance: larger LLMs achieve strong zero-shot results,\nwhile few-shot prompting yields only marginal improvements. In contrast,\nterminology-aware cues and embedding-based example retrieval consistently\nimprove domain-specific translation. These findings underscore both the promise\nand the current limitations of multilingual LLMs for medical En-Vi MT.",
        "url": "http://arxiv.org/abs/2509.15640v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15640v1",
        "arxiv_id": "2509.15640v1",
        "authors": [
            "Nhu Vo",
            "Nu-Uyen-Phuong Le",
            "Dung D. Le",
            "Massimo Piccardi",
            "Wray Buntine"
        ],
        "submitted": "2025-09-19 06:06:36",
        "source": "arxiv",
        "comment": "The work is under peer review",
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on multilingual machine translation, specifically for medical English-Vietnamese translation, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves LLMs and NLP, the application domain and specific research questions are not aligned with the user's interests."
    },
    {
        "title": "Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets",
        "abstract": "Machine Unlearning (MU) has recently attracted considerable attention as a\nsolution to privacy and copyright issues in large language models (LLMs).\nExisting MU methods aim to remove specific target sentences from an LLM while\nminimizing damage to unrelated knowledge. However, these approaches require\nexplicit target sentences and do not support removing broader concepts, such as\npersons or events. To address this limitation, we introduce Concept Unlearning\n(CU) as a new requirement for LLM unlearning. We leverage knowledge graphs to\nrepresent the LLM's internal knowledge and define CU as removing the forgetting\ntarget nodes and associated edges. This graph-based formulation enables a more\nintuitive unlearning and facilitates the design of more effective methods. We\npropose a novel method that prompts the LLM to generate knowledge triplets and\nexplanatory sentences about the forgetting target and applies the unlearning\nprocess to these representations. Our approach enables more precise and\ncomprehensive concept removal by aligning the unlearning process with the LLM's\ninternal knowledge representations. Experiments on real-world and synthetic\ndatasets demonstrate that our method effectively achieves concept-level\nunlearning while preserving unrelated knowledge.",
        "url": "http://arxiv.org/abs/2509.15621v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15621v1",
        "arxiv_id": "2509.15621v1",
        "authors": [
            "Tomoya Yamashita",
            "Yuuki Yamanaka",
            "Masanori Yamada",
            "Takayuki Miura",
            "Toshiki Shibahara",
            "Tomoharu Iwata"
        ],
        "submitted": "2025-09-19 05:34:45",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Concept Unlearning in Large Language Models, which is not directly related to Information Retrieval, Search technologies, or user behavior modeling. While it involves knowledge representation, it's more aligned with NLP and knowledge graph applications, but lacks direct relevance to the user's core research themes."
    },
    {
        "title": "Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization",
        "abstract": "Low latency speech human-machine communication is becoming increasingly\nnecessary as speech technology advances quickly in the last decade. One of the\nprimary factors behind the advancement of speech technology is self-supervised\nlearning. Most self-supervised learning algorithms are designed with full\nutterance assumption and compromises have to made if partial utterances are\npresented, which are common in the streaming applications. In this work, we\npropose a chunk based self-supervised learning (Chunk SSL) algorithm as an\nunified solution for both streaming and offline speech pre-training. Chunk SSL\nis optimized with the masked prediction loss and an acoustic encoder is\nencouraged to restore indices of those masked speech frames with help from\nunmasked frames in the same chunk and preceding chunks. A copy and append data\naugmentation approach is proposed to conduct efficient chunk based\npre-training. Chunk SSL utilizes a finite scalar quantization (FSQ) module to\ndiscretize input speech features and our study shows a high resolution FSQ\ncodebook, i.e., a codebook with vocabulary size up to a few millions, is\nbeneficial to transfer knowledge from the pre-training task to the downstream\ntasks. A group masked prediction loss is employed during pre-training to\nalleviate the high memory and computation cost introduced by the large\ncodebook. The proposed approach is examined in two speech to text tasks, i.e.,\nspeech recognition and speech translation. Experimental results on the\n\\textsc{Librispeech} and \\textsc{Must-C} datasets show that the proposed method\ncould achieve very competitive results for speech to text tasks at both\nstreaming and offline modes.",
        "url": "http://arxiv.org/abs/2509.15579v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15579v1",
        "arxiv_id": "2509.15579v1",
        "authors": [
            "Yun Tang",
            "Cindy Tseng"
        ],
        "submitted": "2025-09-19 04:29:59",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper focuses on speech pre-training with a chunk-based approach and finite scalar quantization, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing."
    },
    {
        "title": "Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning",
        "abstract": "Hyper-parameter Tuning (HPT) is a necessary step in machine learning (ML)\npipelines but becomes computationally expensive and opaque with larger models.\nRecently, Large Language Models (LLMs) have been explored for HPT, yet most\nrely on models exceeding 100 billion parameters. We propose an Expert Block\nFramework for HPT using Small LLMs. At its core is the Trajectory Context\nSummarizer (TCS), a deterministic block that transforms raw training\ntrajectories into structured context, enabling small LLMs to analyze\noptimization progress with reliability comparable to larger models. Using two\nlocally-run LLMs (phi4:reasoning14B and qwen2.5-coder:32B) and a 10-trial\nbudget, our TCS-enabled HPT pipeline achieves average performance within ~0.9\npercentage points of GPT-4 across six diverse tasks.",
        "url": "http://arxiv.org/abs/2509.15561v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15561v1",
        "arxiv_id": "2509.15561v1",
        "authors": [
            "Om Naphade",
            "Saksham Bansal",
            "Parikshit Pareek"
        ],
        "submitted": "2025-09-19 03:46:42",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on hyperparameter tuning using small LLMs, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves machine learning, the context is not aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining",
        "abstract": "Large language models (LLMs) have become integral to a wide range of\napplications worldwide, driving an unprecedented global demand for effective\nmultilingual capabilities. Central to achieving robust multilingual performance\nis the strategic allocation of language proportions within training corpora.\nHowever, determining optimal language ratios is highly challenging due to\nintricate cross-lingual interactions and sensitivity to dataset scale. This\npaper introduces Climb (Cross-Lingual Interaction-aware Multilingual\nBalancing), a novel framework designed to systematically optimize multilingual\ndata allocation. At its core, Climb introduces a cross-lingual\ninteraction-aware language ratio, explicitly quantifying each language's\neffective allocation by capturing inter-language dependencies. Leveraging this\nratio, Climb proposes a principled two-step optimization procedure--first\nequalizing marginal benefits across languages, then maximizing the magnitude of\nthe resulting language allocation vectors--significantly simplifying the\ninherently complex multilingual optimization problem. Extensive experiments\nconfirm that Climb can accurately measure cross-lingual interactions across\nvarious multilingual settings. LLMs trained with Climb-derived proportions\nconsistently achieve state-of-the-art multilingual performance, even achieving\ncompetitive performance with open-sourced LLMs trained with more tokens.",
        "url": "http://arxiv.org/abs/2509.15556v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15556v1",
        "arxiv_id": "2509.15556v1",
        "authors": [
            "Ping Guo",
            "Yubing Ren",
            "Binbin Liu",
            "Fengze Liu",
            "Haobin Lin",
            "Yifan Zhang",
            "Bingni Zhang",
            "Taifeng Wang",
            "Yin Zheng"
        ],
        "submitted": "2025-09-19 03:34:34",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on multilingual data allocation for large language models pretraining, which is outside the primary scope of your research interests in Information Retrieval and Search technologies. While it involves NLP, the specific topic of multilingual data allocation for pretraining large language models is not directly related to your core areas of query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm",
        "abstract": "The rapid advancement of large language models (LLMs) has blurred the line\nbetween AI-generated and human-written text. This progress brings societal\nrisks such as misinformation, authorship ambiguity, and intellectual property\nconcerns, highlighting the urgent need for reliable AI-generated text detection\nmethods. However, recent advances in generative language modeling have resulted\nin significant overlap between the feature distributions of human-written and\nAI-generated text, blurring classification boundaries and making accurate\ndetection increasingly challenging. To address the above challenges, we propose\na DNA-inspired perspective, leveraging a repair-based process to directly and\ninterpretably capture the intrinsic differences between human-written and\nAI-generated text. Building on this perspective, we introduce DNA-DetectLLM, a\nzero-shot detection method for distinguishing AI-generated and human-written\ntext. The method constructs an ideal AI-generated sequence for each input,\niteratively repairs non-optimal tokens, and quantifies the cumulative repair\neffort as an interpretable detection signal. Empirical evaluations demonstrate\nthat our method achieves state-of-the-art detection performance and exhibits\nstrong robustness against various adversarial attacks and input lengths.\nSpecifically, DNA-DetectLLM achieves relative improvements of 5.55% in AUROC\nand 2.08% in F1 score across multiple public benchmark datasets.",
        "url": "http://arxiv.org/abs/2509.15550v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15550v1",
        "arxiv_id": "2509.15550v1",
        "authors": [
            "Xiaowei Zhu",
            "Yubing Ren",
            "Fang Fang",
            "Qingfeng Tan",
            "Shi Wang",
            "Yanan Cao"
        ],
        "submitted": "2025-09-19 03:08:13",
        "source": "arxiv",
        "comment": "NeurIPS 2025 Spotlight",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The paper focuses on AI-generated text detection, which is outside your primary areas of interest."
    },
    {
        "title": "mucAI at BAREC Shared Task 2025: Towards Uncertainty Aware Arabic Readability Assessment",
        "abstract": "We present a simple, model-agnostic post-processing technique for\nfine-grained Arabic readability classification in the BAREC 2025 Shared Task\n(19 ordinal levels). Our method applies conformal prediction to generate\nprediction sets with coverage guarantees, then computes weighted averages using\nsoftmax-renormalized probabilities over the conformal sets. This\nuncertainty-aware decoding improves Quadratic Weighted Kappa (QWK) by reducing\nhigh-penalty misclassifications to nearer levels. Our approach shows consistent\nQWK improvements of 1-3 points across different base models. In the strict\ntrack, our submission achieves QWK scores of 84.9\\%(test) and 85.7\\% (blind\ntest) for sentence level, and 73.3\\% for document level. For Arabic educational\nassessment, this enables human reviewers to focus on a handful of plausible\nlevels, combining statistical guarantees with practical usability.",
        "url": "http://arxiv.org/abs/2509.15485v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15485v1",
        "arxiv_id": "2509.15485v1",
        "authors": [
            "Ahmed Abdou"
        ],
        "submitted": "2025-09-18 23:14:51",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves Arabic readability assessment and uses techniques like conformal prediction, it does not align with your focus on query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "PILOT: Steering Synthetic Data Generation with Psychological & Linguistic Output Targeting",
        "abstract": "Generative AI applications commonly leverage user personas as a steering\nmechanism for synthetic data generation, but reliance on natural language\nrepresentations forces models to make unintended inferences about which\nattributes to emphasize, limiting precise control over outputs. We introduce\nPILOT (Psychological and Linguistic Output Targeting), a two-phase framework\nfor steering large language models with structured psycholinguistic profiles.\nIn Phase 1, PILOT translates natural language persona descriptions into\nmultidimensional profiles with normalized scores across linguistic and\npsychological dimensions. In Phase 2, these profiles guide generation along\nmeasurable axes of variation. We evaluate PILOT across three state-of-the-art\nLLMs (Mistral Large 2, Deepseek-R1, LLaMA 3.3 70B) using 25 synthetic personas\nunder three conditions: Natural-language Persona Steering (NPS), Schema-Based\nSteering (SBS), and Hybrid Persona-Schema Steering (HPS). Results demonstrate\nthat schema-based approaches significantly reduce artificial-sounding persona\nrepetition while improving output coherence, with silhouette scores increasing\nfrom 0.098 to 0.237 and topic purity from 0.773 to 0.957. Our analysis reveals\na fundamental trade-off: SBS produces more concise outputs with higher topical\nconsistency, while NPS offers greater lexical diversity but reduced\npredictability. HPS achieves a balance between these extremes, maintaining\noutput variety while preserving structural consistency. Expert linguistic\nevaluation confirms that PILOT maintains high response quality across all\nconditions, with no statistically significant differences between steering\napproaches.",
        "url": "http://arxiv.org/abs/2509.15447v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15447v1",
        "arxiv_id": "2509.15447v1",
        "authors": [
            "Caitlin Cisar",
            "Emily Sheffield",
            "Joshua Drake",
            "Alden Harrell",
            "Subramanian Chidambaram",
            "Nikita Nangia",
            "Vinayak Arannil",
            "Alex Williams"
        ],
        "submitted": "2025-09-18 21:43:28",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper introduces a framework for steering synthetic data generation with structured psycholinguistic profiles, which is somewhat related to query understanding and user behavior modeling in Information Retrieval. However, the focus on synthetic data generation and linguistic output targeting is not directly aligned with the user's core research themes."
    },
    {
        "title": "Dual-Mode Visual System for Brain-Computer Interfaces: Integrating SSVEP and P300 Responses",
        "abstract": "In brain-computer interface (BCI) systems, steady-state visual evoked\npotentials (SSVEP) and P300 responses have achieved widespread implementation\nowing to their superior information transfer rates (ITR) and minimal training\nrequirements. These neurophysiological signals have exhibited robust efficacy\nand versatility in external device control, demonstrating enhanced precision\nand scalability. However, conventional implementations predominantly utilise\nliquid crystal display (LCD)-based visual stimulation paradigms, which present\nlimitations in practical deployment scenarios. This investigation presents the\ndevelopment and evaluation of a novel light-emitting diode (LED)-based dual\nstimulation apparatus designed to enhance SSVEP classification accuracy through\nthe integration of both SSVEP and P300 paradigms. The system employs four\ndistinct frequencies, 7 Hz, 8 Hz, 9 Hz, and 10 Hz, corresponding to forward,\nbackward, right, and left directional controls, respectively. Oscilloscopic\nverification confirmed the precision of these stimulation frequencies.\nReal-time feature extraction was accomplished through the concurrent analysis\nof maximum Fast Fourier Transform (FFT) amplitude and P300 peak detection to\nascertain user intent. Directional control was determined by the frequency\nexhibiting maximal amplitude characteristics. The visual stimulation hardware\ndemonstrated minimal frequency deviation, with error differentials ranging from\n0.15%to 0.20%across all frequencies. The implemented signal processing\nalgorithm successfully discriminated all four stimulus frequencies whilst\ncorrelating them with their respective P300 event markers. Classification\naccuracy was evaluated based on correct task intention recognition. The\nproposed hybrid system achieved a mean classification accuracy of 86.25%,\ncoupled with an average ITR of 42.08 bits per minute (bpm).",
        "url": "http://arxiv.org/abs/2509.15439v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15439v1",
        "arxiv_id": "2509.15439v1",
        "authors": [
            "Ekgari Kasawala",
            "Surej Mouli"
        ],
        "submitted": "2025-09-18 21:25:18",
        "source": "arxiv",
        "comment": "15 Pages",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, Natural Language Processing, data mining, or recommender systems. The paper focuses on brain-computer interfaces and neurophysiological signals, which are unrelated to your areas of expertise."
    },
    {
        "title": "Frustratingly Easy Data Augmentation for Low-Resource ASR",
        "abstract": "This paper introduces three self-contained data augmentation methods for\nlow-resource Automatic Speech Recognition (ASR). Our techniques first generate\nnovel text--using gloss-based replacement, random replacement, or an LLM-based\napproach--and then apply Text-to-Speech (TTS) to produce synthetic audio. We\napply these methods, which leverage only the original annotated data, to four\nlanguages with extremely limited resources (Vatlongos, Nashta, Shinekhen\nBuryat, and Kakabe). Fine-tuning a pretrained Wav2Vec2-XLSR-53 model on a\ncombination of the original audio and generated synthetic data yields\nsignificant performance gains, including a 14.3% absolute WER reduction for\nNashta. The methods prove effective across all four low-resource languages and\nalso show utility for high-resource languages like English, demonstrating their\nbroad applicability.",
        "url": "http://arxiv.org/abs/2509.15373v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15373v1",
        "arxiv_id": "2509.15373v1",
        "authors": [
            "Katsumi Ibaraki",
            "David Chiang"
        ],
        "submitted": "2025-09-18 19:20:37",
        "source": "arxiv",
        "comment": "5 pages, 2 figures, 2 tables, submitted to ICASSP 2026",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on data augmentation for Automatic Speech Recognition (ASR), which is outside your primary research interests in Information Retrieval and Search technologies. While it involves NLP, the specific application and techniques are not directly related to your core themes of query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Real, Fake, or Manipulated? Detecting Machine-Influenced Text",
        "abstract": "Large Language Model (LLMs) can be used to write or modify documents,\npresenting a challenge for understanding the intent behind their use. For\nexample, benign uses may involve using LLM on a human-written document to\nimprove its grammar or to translate it into another language. However, a\ndocument entirely produced by a LLM may be more likely to be used to spread\nmisinformation than simple translation (\\eg, from use by malicious actors or\nsimply by hallucinating). Prior works in Machine Generated Text (MGT) detection\nmostly focus on simply identifying whether a document was human or machine\nwritten, ignoring these fine-grained uses. In this paper, we introduce a\nHiErarchical, length-RObust machine-influenced text detector (HERO), which\nlearns to separate text samples of varying lengths from four primary types:\nhuman-written, machine-generated, machine-polished, and machine-translated.\nHERO accomplishes this by combining predictions from length-specialist models\nthat have been trained with Subcategory Guidance. Specifically, for categories\nthat are easily confused (\\eg, different source languages), our Subcategory\nGuidance module encourages separation of the fine-grained categories, boosting\nperformance. Extensive experiments across five LLMs and six domains demonstrate\nthe benefits of our HERO, outperforming the state-of-the-art by 2.5-3 mAP on\naverage.",
        "url": "http://arxiv.org/abs/2509.15350v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15350v1",
        "arxiv_id": "2509.15350v1",
        "authors": [
            "Yitong Wang",
            "Zhongping Zhang",
            "Margherita Piana",
            "Zheng Zhou",
            "Peter Gerstoft",
            "Bryan A. Plummer"
        ],
        "submitted": "2025-09-18 18:41:57",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 Findings",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper focuses on detecting machine-influenced text, which is related to query understanding and ranking models in Information Retrieval. However, its primary focus on machine-generated text detection and fine-grained categorization of text types does not directly align with the user's core research themes in IR and Search technologies."
    },
    {
        "title": "Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks",
        "abstract": "Vision language models (VLMs) excel in multimodal understanding but are prone\nto adversarial attacks. Existing defenses often demand costly retraining or\nsignificant architecture changes. We introduce a lightweight defense using\ntensor decomposition suitable for any pre-trained VLM, requiring no retraining.\nBy decomposing and reconstructing vision encoder representations, it filters\nadversarial noise while preserving meaning. Experiments with CLIP on COCO and\nFlickr30K show improved robustness. On Flickr30K, it restores 12.3\\%\nperformance lost to attacks, raising Recall@1 accuracy from 7.5\\% to 19.8\\%. On\nCOCO, it recovers 8.1\\% performance, improving accuracy from 3.8\\% to 11.9\\%.\nAnalysis shows Tensor Train decomposition with low rank (8-32) and low residual\nstrength ($\\alpha=0.1-0.2$) is optimal. This method is a practical,\nplug-and-play solution with minimal overhead for existing VLMs.",
        "url": "http://arxiv.org/abs/2509.16163v1",
        "pdf_url": "http://arxiv.org/pdf/2509.16163v1",
        "arxiv_id": "2509.16163v1",
        "authors": [
            "Het Patel",
            "Muzammil Allie",
            "Qian Zhang",
            "Jia Chen",
            "Evangelos E. Papalexakis"
        ],
        "submitted": "2025-09-19 17:16:32",
        "source": "arxiv",
        "comment": "To be presented as a poster at the Workshop on Safe and Trustworthy\n  Multimodal AI Systems (SafeMM-AI), 2025",
        "score": 1,
        "keyword_reasons": [
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to the user's core research themes in Information Retrieval and Search technologies, as it focuses on Vision-Language Models and adversarial attacks in the context of computer vision. While it involves tensor decomposition, which is a mathematical technique, the application and context are quite different from the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Localmax dynamics for attention in transformers and its asymptotic behavior",
        "abstract": "We introduce a new discrete-time attention model, termed the localmax\ndynamics, which interpolates between the classic softmax dynamics and the\nhardmax dynamics, where only the tokens that maximize the influence toward a\ngiven token have a positive weight. As in hardmax, uniform weights are\ndetermined by a parameter controlling neighbor influence, but the key extension\nlies in relaxing neighborhood interactions through an alignment-sensitivity\nparameter, which allows controlled deviations from pure hardmax behavior. As we\nprove, while the convex hull of the token states still converges to a convex\npolytope, its structure can no longer be fully described by a maximal alignment\nset, prompting the introduction of quiescent sets to capture the invariant\nbehavior of tokens near vertices. We show that these sets play a key role in\nunderstanding the asymptotic behavior of the system, even under time-varying\nalignment sensitivity parameters. We further show that localmax dynamics does\nnot exhibit finite-time convergence and provide results for vanishing, nonzero,\ntime-varying alignment-sensitivity parameters, recovering the limiting behavior\nof hardmax as a by-product. Finally, we adapt Lyapunov-based methods from\nclassical opinion dynamics, highlighting their limitations in the asymmetric\nsetting of localmax interactions and outlining directions for future research.",
        "url": "http://arxiv.org/abs/2509.15958v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15958v1",
        "arxiv_id": "2509.15958v1",
        "authors": [
            "Henri Cimetière",
            "Maria Teresa Chiri",
            "Bahman Gharesifard"
        ],
        "submitted": "2025-09-19 13:18:30",
        "source": "arxiv",
        "comment": "28 pages, 5 figures",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on a discrete-time attention model for transformers, which is a topic in Natural Language Processing (NLP). However, it does not appear to be directly related to information retrieval, query understanding, ranking models, or user behavior modeling, which are core areas of your research interests."
    },
    {
        "title": "Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions",
        "abstract": "Meeting summarization with large language models (LLMs) remains error-prone,\noften producing outputs with hallucinations, omissions, and irrelevancies. We\npresent FRAME, a modular pipeline that reframes summarization as a semantic\nenrichment task. FRAME extracts and scores salient facts, organizes them\nthematically, and uses these to enrich an outline into an abstractive summary.\nTo personalize summaries, we introduce SCOPE, a reason-out-loud protocol that\nhas the model build a reasoning trace by answering nine questions before\ncontent selection. For evaluation, we propose P-MESA, a multi-dimensional,\nreference-free evaluation framework to assess if a summary fits a target\nreader. P-MESA reliably identifies error instances, achieving >= 89% balanced\naccuracy against human annotations and strongly aligns with human severity\nratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and\nomission by 2 out of 5 points (measured with MESA), while SCOPE improves\nknowledge fit and goal alignment over prompt-only baselines. Our findings\nadvocate for rethinking summarization to improve control, faithfulness, and\npersonalization.",
        "url": "http://arxiv.org/abs/2509.15901v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15901v1",
        "arxiv_id": "2509.15901v1",
        "authors": [
            "Frederic Kirstein",
            "Sonu Kumar",
            "Terry Ruas",
            "Bela Gipp"
        ],
        "submitted": "2025-09-19 11:58:17",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025",
        "score": 1,
        "keyword_reasons": [
            "Found 'personalization' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores meeting summarization with large language models, which is somewhat related to information retrieval and NLP. However, the focus on summarization and personalization is not a central match to the user's core research themes, particularly query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "The Psychology of Falsehood: A Human-Centric Survey of Misinformation Detection",
        "abstract": "Misinformation remains one of the most significant issues in the digital age.\nWhile automated fact-checking has emerged as a viable solution, most current\nsystems are limited to evaluating factual accuracy. However, the detrimental\neffect of misinformation transcends simple falsehoods; it takes advantage of\nhow individuals perceive, interpret, and emotionally react to information. This\nunderscores the need to move beyond factuality and adopt more human-centered\ndetection frameworks. In this survey, we explore the evolving interplay between\ntraditional fact-checking approaches and psychological concepts such as\ncognitive biases, social dynamics, and emotional responses. By analyzing\nstate-of-the-art misinformation detection systems through the lens of human\npsychology and behavior, we reveal critical limitations of current methods and\nidentify opportunities for improvement. Additionally, we outline future\nresearch directions aimed at creating more robust and adaptive frameworks, such\nas neuro-behavioural models that integrate technological factors with the\ncomplexities of human cognition and social influence. These approaches offer\npromising pathways to more effectively detect and mitigate the societal harms\nof misinformation.",
        "url": "http://arxiv.org/abs/2509.15896v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15896v1",
        "arxiv_id": "2509.15896v1",
        "authors": [
            "Arghodeep Nandi",
            "Megha Sundriyal",
            "Euna Mehnaz Khan",
            "Jikai Sun",
            "Emily Vraga",
            "Jaideep Srivastava",
            "Tanmoy Chakraborty"
        ],
        "submitted": "2025-09-19 11:51:17",
        "source": "arxiv",
        "comment": "Accepted in EMNLP'25 Main",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on the psychology of misinformation detection, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on the societal impact of misinformation, it does not address query understanding, ranking models, or user behavior modeling, making it only loosely relevant to the user's interests."
    },
    {
        "title": "Building Data-Driven Occupation Taxonomies: A Bottom-Up Multi-Stage Approach via Semantic Clustering and Multi-Agent Collaboration",
        "abstract": "Creating robust occupation taxonomies, vital for applications ranging from\njob recommendation to labor market intelligence, is challenging. Manual\ncuration is slow, while existing automated methods are either not adaptive to\ndynamic regional markets (top-down) or struggle to build coherent hierarchies\nfrom noisy data (bottom-up). We introduce CLIMB (CLusterIng-based Multi-agent\ntaxonomy Builder), a framework that fully automates the creation of\nhigh-quality, data-driven taxonomies from raw job postings. CLIMB uses global\nsemantic clustering to distill core occupations, then employs a\nreflection-based multi-agent system to iteratively build a coherent hierarchy.\nOn three diverse, real-world datasets, we show that CLIMB produces taxonomies\nthat are more coherent and scalable than existing methods and successfully\ncapture unique regional characteristics. We release our code and datasets at\nhttps://anonymous.4open.science/r/CLIMB.",
        "url": "http://arxiv.org/abs/2509.15786v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15786v1",
        "arxiv_id": "2509.15786v1",
        "authors": [
            "Nan Li",
            "Bo Kang",
            "Tijl De Bie"
        ],
        "submitted": "2025-09-19 09:17:48",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to the user's interests in Information Retrieval and Natural Language Processing, as it involves semantic clustering and multi-agent collaboration. However, the focus on occupation taxonomies and job recommendation is not directly aligned with the user's primary research themes. The paper's use of data-driven approaches and real-world datasets is relevant, but the application domain is not e-commerce-specific."
    },
    {
        "title": "REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting",
        "abstract": "Individuals express diverse opinions, a fair summary should represent these\nviewpoints comprehensively. Previous research on fairness in opinion\nsummarisation using large language models (LLMs) relied on hyperparameter\ntuning or providing ground truth distributional information in prompts.\nHowever, these methods face practical limitations: end-users rarely modify\ndefault model parameters, and accurate distributional information is often\nunavailable. Building upon cognitive science research demonstrating that\nfrequency-based representations reduce systematic biases in human statistical\nreasoning by making reference classes explicit and reducing cognitive load,\nthis study investigates whether frequency framed prompting (REFER) can\nsimilarly enhance fairness in LLM opinion summarisation. Through systematic\nexperimentation with different prompting frameworks, we adapted techniques\nknown to improve human reasoning to elicit more effective information\nprocessing in language models compared to abstract probabilistic\nrepresentations.Our results demonstrate that REFER enhances fairness in\nlanguage models when summarising opinions. This effect is particularly\npronounced in larger language models and using stronger reasoning instructions.",
        "url": "http://arxiv.org/abs/2509.15723v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15723v1",
        "arxiv_id": "2509.15723v1",
        "authors": [
            "Nannan Huang",
            "Haytham M. Fayek",
            "Xiuzhen Zhang"
        ],
        "submitted": "2025-09-19 07:53:51",
        "source": "arxiv",
        "comment": "Accepted to the 5th New Frontiers in Summarization Workshop\n  (NewSumm@EMNLP 2025)",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores fairness in opinion summarization using large language models, which is somewhat related to information retrieval and NLP. However, the focus on fairness and opinion summarization is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is limited to the broader NLP domain."
    },
    {
        "title": "Understanding Embedding Scaling in Collaborative Filtering",
        "abstract": "Scaling recommendation models into large recommendation models has become one\nof the most widely discussed topics. Recent efforts focus on components beyond\nthe scaling embedding dimension, as it is believed that scaling embedding may\nlead to performance degradation. Although there have been some initial\nobservations on embedding, the root cause of their non-scalability remains\nunclear. Moreover, whether performance degradation occurs across different\ntypes of models and datasets is still an unexplored area. Regarding the effect\nof embedding dimensions on performance, we conduct large-scale experiments\nacross 10 datasets with varying sparsity levels and scales, using 4\nrepresentative classical architectures. We surprisingly observe two novel\nphenomenon: double-peak and logarithmic. For the former, as the embedding\ndimension increases, performance first improves, then declines, rises again,\nand eventually drops. For the latter, it exhibits a perfect logarithmic curve.\nOur contributions are threefold. First, we discover two novel phenomena when\nscaling collaborative filtering models. Second, we gain an understanding of the\nunderlying causes of the double-peak phenomenon. Lastly, we theoretically\nanalyze the noise robustness of collaborative filtering models, with results\nmatching empirical observations.",
        "url": "http://arxiv.org/abs/2509.15709v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15709v1",
        "arxiv_id": "2509.15709v1",
        "authors": [
            "Zhuangzhuang He",
            "Zhou Kaiyu",
            "Haoyue Bai",
            "Fengbin Zhu",
            "Yonghui Yang"
        ],
        "submitted": "2025-09-19 07:33:50",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 3,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval and Search technologies, but it focuses on Collaborative Filtering, which is a type of Recommender System. While it explores scaling and performance degradation, it lacks direct connection to your core themes of query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment",
        "abstract": "Automatic Pronunciation Assessment (APA) is critical for Computer-Assisted\nLanguage Learning (CALL), requiring evaluation across multiple granularities\nand aspects. Large Multimodal Models (LMMs) present new opportunities for APA,\nbut their effectiveness in fine-grained assessment remains uncertain. This work\ninvestigates fine-tuning LMMs for APA using the Speechocean762 dataset and a\nprivate corpus. Fine-tuning significantly outperforms zero-shot settings and\nachieves competitive results on single-granularity tasks compared to public and\ncommercial systems. The model performs well at word and sentence levels, while\nphoneme-level assessment remains challenging. We also observe that the Pearson\nCorrelation Coefficient (PCC) reaches 0.9, whereas Spearman's rank Correlation\nCoefficient (SCC) remains around 0.6, suggesting that SCC better reflects\nordinal consistency. These findings highlight both the promise and limitations\nof LMMs for APA and point to future work on fine-grained modeling and\nrank-aware evaluation.",
        "url": "http://arxiv.org/abs/2509.15701v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15701v1",
        "arxiv_id": "2509.15701v1",
        "authors": [
            "Ke Wang",
            "Wenning Wei",
            "Yan Deng",
            "Lei He",
            "Sheng Zhao"
        ],
        "submitted": "2025-09-19 07:23:25",
        "source": "arxiv",
        "comment": "submitted to ICASSP2026",
        "score": 1,
        "keyword_reasons": [
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on fine-tuning large multimodal models for Automatic Pronunciation Assessment, which is outside the user's core research themes in Information Retrieval and Search technologies. Although it involves some aspects of deep semantic understanding, the context and application are unrelated to the user's primary interests."
    },
    {
        "title": "Direct Simultaneous Translation Activation for Large Audio-Language Models",
        "abstract": "Simultaneous speech-to-text translation (Simul-S2TT) aims to translate speech\ninto target text in real time, outputting translations while receiving source\nspeech input, rather than waiting for the entire utterance to be spoken.\nSimul-S2TT research often modifies model architectures to implement read-write\nstrategies. However, with the rise of large audio-language models (LALMs), a\nkey challenge is how to directly activate Simul-S2TT capabilities in base\nmodels without additional architectural changes. In this paper, we introduce\n{\\bf Simul}taneous {\\bf S}elf-{\\bf A}ugmentation ({\\bf SimulSA}), a strategy\nthat utilizes LALMs' inherent capabilities to obtain simultaneous data by\nrandomly truncating speech and constructing partially aligned translation. By\nincorporating them into offline SFT data, SimulSA effectively bridges the\ndistribution gap between offline translation during pretraining and\nsimultaneous translation during inference. Experimental results demonstrate\nthat augmenting only about {\\bf 1\\%} of the simultaneous data, compared to the\nfull offline SFT data, can significantly activate LALMs' Simul-S2TT\ncapabilities without modifications to model architecture or decoding strategy.",
        "url": "http://arxiv.org/abs/2509.15692v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15692v1",
        "arxiv_id": "2509.15692v1",
        "authors": [
            "Pei Zhang",
            "Yiming Wang",
            "Jialong Tang",
            "Baosong Yang",
            "Rui Wang",
            "Derek F. Wong",
            "Fei Huang"
        ],
        "submitted": "2025-09-19 07:12:18",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on simultaneous speech-to-text translation using large audio-language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing, especially in areas requiring deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "Layer-wise Minimal Pair Probing Reveals Contextual Grammatical-Conceptual Hierarchy in Speech Representations",
        "abstract": "Transformer-based speech language models (SLMs) have significantly improved\nneural speech recognition and understanding. While existing research has\nexamined how well SLMs encode shallow acoustic and phonetic features, the\nextent to which SLMs encode nuanced syntactic and conceptual features remains\nunclear. By drawing parallels with linguistic competence assessments for large\nlanguage models, this study is the first to systematically evaluate the\npresence of contextual syntactic and semantic features across SLMs for\nself-supervised learning (S3M), automatic speech recognition (ASR), speech\ncompression (codec), and as the encoder for auditory large language models\n(AudioLLMs). Through minimal pair designs and diagnostic feature analysis\nacross 71 tasks spanning diverse linguistic levels, our layer-wise and\ntime-resolved analysis uncovers that 1) all speech encode grammatical features\nmore robustly than conceptual ones.",
        "url": "http://arxiv.org/abs/2509.15655v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15655v1",
        "arxiv_id": "2509.15655v1",
        "authors": [
            "Linyang He",
            "Qiaolin Wang",
            "Xilin Jiang",
            "Nima Mesgarani"
        ],
        "submitted": "2025-09-19 06:29:33",
        "source": "arxiv",
        "comment": "EMNLP 2025 Main Conference (Oral)",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval or Search technologies, and its focus on speech representations and linguistic features does not align with your primary research interests."
    },
    {
        "title": "A method for improving multilingual quality and diversity of instruction fine-tuning datasets",
        "abstract": "Multilingual Instruction Fine-Tuning (IFT) is essential for enabling large\nlanguage models (LLMs) to generalize effectively across diverse linguistic and\ncultural contexts. However, the scarcity of high-quality multilingual training\ndata and corresponding building method remains a critical bottleneck. While\ndata selection has shown promise in English settings, existing methods often\nfail to generalize across languages due to reliance on simplistic heuristics or\nlanguage-specific assumptions. In this work, we introduce Multilingual Data\nQuality and Diversity (M-DaQ), a novel method for improving LLMs\nmultilinguality, by selecting high-quality and semantically diverse\nmultilingual IFT samples. We further conduct the first systematic investigation\nof the Superficial Alignment Hypothesis (SAH) in multilingual setting.\nEmpirical results across 18 languages demonstrate that models fine-tuned with\nM-DaQ method achieve significant performance gains over vanilla baselines over\n60% win rate. Human evaluations further validate these gains, highlighting the\nincrement of cultural points in the response. We release the M-DaQ code to\nsupport future research.",
        "url": "http://arxiv.org/abs/2509.15549v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15549v1",
        "arxiv_id": "2509.15549v1",
        "authors": [
            "Chunguang Zhao",
            "Yilun Liu",
            "Pufan Zeng",
            "Yuanchang Luo",
            "Shimin Tao",
            "Minggui He",
            "Weibin Meng",
            "Song Xu",
            "Ziang Chen",
            "Chen Liu",
            "Hongxia Ma",
            "Li Zhang",
            "Boxing Chen",
            "Daimeng Wei"
        ],
        "submitted": "2025-09-19 03:07:59",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on improving multilingual quality and diversity of instruction fine-tuning datasets, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves large language models and fine-tuning, the context is more aligned with NLP and data quality rather than query understanding, ranking models, or user behavior modeling."
    }
]