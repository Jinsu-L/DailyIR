[
    {
        "title": "Average Precision at Cutoff k under Random Rankings: Expectation and Variance",
        "abstract": "Recommender systems and information retrieval platforms rely on ranking\nalgorithms to present the most relevant items to users, thereby improving\nengagement and satisfaction. Assessing the quality of these rankings requires\nreliable evaluation metrics. Among them, Mean Average Precision at cutoff k\n(MAP@k) is widely used, as it accounts for both the relevance of items and\ntheir positions in the list.\n  In this paper, the expectation and variance of Average Precision at k (AP@k)\nare derived since they can be used as biselines for MAP@k. Here, we covered two\nwidely used evaluation models: offline and online. The expectation establishes\nthe baseline, indicating the level of MAP@k that can be achieved by pure\nchance. The variance complements this baseline by quantifying the extent of\nrandom fluctuations, enabling a more reliable interpretation of observed\nscores.",
        "url": "http://arxiv.org/abs/2511.02571v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02571v1",
        "arxiv_id": "2511.02571v1",
        "authors": [
            "Tetiana Manzhos",
            "Tetiana Ianevych",
            "Olga Melnyk"
        ],
        "submitted": "2025-11-04 13:45:16",
        "source": "arxiv",
        "comment": "17 pages, 2 tables, 2 figures",
        "score": 15,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses evaluation metrics for ranking algorithms, specifically Average Precision at cutoff k, which is relevant to Information Retrieval. However, the focus is on recommender systems and the derivation of expectation and variance, which is somewhat related to user behavior modeling and click models but not directly aligned with your core research themes."
    },
    {
        "title": "Beyond Single Embeddings: Capturing Diverse Targets with Multi-Query Retrieval",
        "abstract": "Most text retrievers generate \\emph{one} query vector to retrieve relevant\ndocuments. Yet, the conditional distribution of relevant documents for the\nquery may be multimodal, e.g., representing different interpretations of the\nquery. We first quantify the limitations of existing retrievers. All retrievers\nwe evaluate struggle more as the distance between target document embeddings\ngrows. To address this limitation, we develop a new retriever architecture,\n\\emph{A}utoregressive \\emph{M}ulti-\\emph{E}mbedding \\emph{R}etriever (AMER).\nOur model autoregressively generates multiple query vectors, and all the\npredicted query vectors are used to retrieve documents from the corpus. We show\nthat on the synthetic vectorized data, the proposed method could capture\nmultiple target distributions perfectly, showing 4x better performance than\nsingle embedding model. We also fine-tune our model on real-world multi-answer\nretrieval datasets and evaluate in-domain. AMER presents 4 and 21\\% relative\ngains over single-embedding baselines on two datasets we evaluate on.\nFurthermore, we consistently observe larger gains on the subset of dataset\nwhere the embeddings of the target documents are less similar to each other. We\ndemonstrate the potential of using a multi-query vector retriever and open up a\nnew direction for future work.",
        "url": "http://arxiv.org/abs/2511.02770v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02770v1",
        "arxiv_id": "2511.02770v1",
        "authors": [
            "Hung-Ting Chen",
            "Xiang Liu",
            "Shauli Ravfogel",
            "Eunsol Choi"
        ],
        "submitted": "2025-11-04 17:57:20",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to Information Retrieval, specifically addressing the limitations of single query embeddings and proposing a new architecture, AMER, that generates multiple query vectors. The focus on capturing diverse targets and improving performance on multi-answer retrieval datasets aligns with your interests in query understanding and ranking models."
    },
    {
        "title": "Relational Deep Dive: Error-Aware Queries Over Unstructured Data",
        "abstract": "Unstructured data is pervasive, but analytical queries demand structured\nrepresentations, creating a significant extraction challenge. Existing methods\nlike RAG lack schema awareness and struggle with cross-document alignment,\nleading to high error rates. We propose ReDD (Relational Deep Dive), a\nframework that dynamically discovers query-specific schemas, populates\nrelational tables, and ensures error-aware extraction with provable guarantees.\nReDD features a two-stage pipeline: (1) Iterative Schema Discovery (ISD)\nidentifies minimal, joinable schemas tailored to each query, and (2) Tabular\nData Population (TDP) extracts and corrects data using lightweight classifiers\ntrained on LLM hidden states. A main contribution of ReDD is SCAPE, a\nstatistically calibrated method for error detection with coverage guarantees,\nand SCAPE-HYB, a hybrid approach that optimizes the trade-off between accuracy\nand human correction costs. Experiments across diverse datasets demonstrate\nReDD's effectiveness, reducing data extraction errors from up to 30% to below\n1% while maintaining high schema completeness (100% recall) and precision.\nReDD's modular design enables fine-grained control over accuracy-cost\ntrade-offs, making it a robust solution for high-stakes analytical queries over\nunstructured corpora.",
        "url": "http://arxiv.org/abs/2511.02711v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02711v1",
        "arxiv_id": "2511.02711v1",
        "authors": [
            "Daren Chao",
            "Kaiwen Chen",
            "Naiqing Guan",
            "Nick Koudas"
        ],
        "submitted": "2025-11-04 16:30:55",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper 'Relational Deep Dive: Error-Aware Queries Over Unstructured Data' is somewhat related to your research interests in Information Retrieval and Natural Language Processing, particularly in the context of query understanding and data extraction. However, the focus on structured data and relational schema discovery is not directly aligned with your primary interests in deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "Let Multimodal Embedders Learn When to Augment Query via Adaptive Query Augmentation",
        "abstract": "Query augmentation makes queries more meaningful by appending further\ninformation to the queries to find relevant documents. Current studies have\nproposed Large Language Model (LLM)-based embedders, which learn representation\nfor embedding and generation for query augmentation in a multi-task manner by\nleveraging the generative capabilities of LLM. During inference, these jointly\ntrained embedders have conducted query augmentation followed by embedding,\nshowing effective results. However, augmenting every query leads to substantial\nembedding latency and query augmentation can be detrimental to performance for\nsome queries. Also, previous methods have not been explored in multimodal\nenvironments. To tackle these problems, we propose M-Solomon, a universal\nmultimodal embedder that can adaptively determine when to augment queries. Our\napproach first divides the queries of the training datasets into two groups at\nthe dataset level. One includes queries that require augmentation and the other\nincludes queries that do not. Then, we introduces a synthesis process that\ngenerates appropriate augmentations for queries that require them by leveraging\na powerful Multimodal LLM (MLLM). Next, we present adaptive query augmentation.\nThrough this step, M-Solomon can conduct query augmentation only when necessary\nby learning to generate synthetic augmentations with the prefix /augment for\nqueries that demand them and to generate the simple string /embed for others.\nExperimental results showed that M-Solomon not only surpassed the baseline\nwithout augmentation by a large margin but also outperformed the baseline that\nalways used augmentation, providing much faster embedding latency.",
        "url": "http://arxiv.org/abs/2511.02358v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02358v1",
        "arxiv_id": "2511.02358v1",
        "authors": [
            "Wongyu Kim",
            "Hochang Lee",
            "Sanghak Lee",
            "Yoonsung Kim",
            "Jaehyun Park"
        ],
        "submitted": "2025-11-04 08:24:41",
        "source": "arxiv",
        "comment": "Accepted to MMGenSR Workshop (CIKM 2025)",
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The proposed adaptive query augmentation approach leverages Large Language Model (LLM) capabilities, which is a key area of interest for you. The paper's focus on multimodal environments and real-time relevance optimization also aligns with your research goals."
    },
    {
        "title": "Demo: Statistically Significant Results On Biases and Errors of LLMs Do Not Guarantee Generalizable Results",
        "abstract": "Recent research has shown that hallucinations, omissions, and biases are\nprevalent in everyday use-cases of LLMs. However, chatbots used in medical\ncontexts must provide consistent advice in situations where non-medical factors\nare involved, such as when demographic information is present. In order to\nunderstand the conditions under which medical chatbots fail to perform as\nexpected, we develop an infrastructure that 1) automatically generates queries\nto probe LLMs and 2) evaluates answers to these queries using multiple\nLLM-as-a-judge setups and prompts. For 1), our prompt creation pipeline samples\nthe space of patient demographics, histories, disorders, and writing styles to\ncreate realistic questions that we subsequently use to prompt LLMs. In 2), our\nevaluation pipeline provides hallucination and omission detection using\nLLM-as-a-judge as well as agentic workflows, in addition to LLM-as-a-judge\ntreatment category detectors. As a baseline study, we perform two case studies\non inter-LLM agreement and the impact of varying the answering and evaluation\nLLMs. We find that LLM annotators exhibit low agreement scores (average Cohen's\nKappa $\\kappa=0.118$), and only specific (answering, evaluation) LLM pairs\nyield statistically significant differences across writing styles, genders, and\nraces. We recommend that studies using LLM evaluation use multiple LLMs as\nevaluators in order to avoid arriving at statistically significant but\nnon-generalizable results, particularly in the absence of ground-truth data. We\nalso suggest publishing inter-LLM agreement metrics for transparency. Our code\nand dataset are available here:\nhttps://github.com/BBN-E/medic-neurips-2025-demo.",
        "url": "http://arxiv.org/abs/2511.02246v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02246v1",
        "arxiv_id": "2511.02246v1",
        "authors": [
            "Jonathan Liu",
            "Haoling Qiu",
            "Jonathan Lasko",
            "Damianos Karakos",
            "Mahsa Yarmohammadi",
            "Mark Dredze"
        ],
        "submitted": "2025-11-04 04:20:33",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)",
            "Found 'neurips' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your interests in Information Retrieval and Natural Language Processing, particularly in the context of Large Language Models (LLMs). However, the focus on LLM evaluation and biases in medical contexts is not directly aligned with your core research themes of query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "InteracSPARQL: An Interactive System for SPARQL Query Refinement Using Natural Language Explanations",
        "abstract": "In recent years, querying semantic web data using SPARQL has remained\nchallenging, especially for non-expert users, due to the language's complex\nsyntax and the prerequisite of understanding intricate data structures. To\naddress these challenges, we propose InteracSPARQL, an interactive SPARQL query\ngeneration and refinement system that leverages natural language explanations\n(NLEs) to enhance user comprehension and facilitate iterative query refinement.\nInteracSPARQL integrates LLMs with a rule-based approach to first produce\nstructured explanations directly from SPARQL abstract syntax trees (ASTs),\nfollowed by LLM-based linguistic refinements. Users can interactively refine\nqueries through direct feedback or LLM-driven self-refinement, enabling the\ncorrection of ambiguous or incorrect query components in real time. We evaluate\nInteracSPARQL on standard benchmarks, demonstrating significant improvements in\nquery accuracy, explanation clarity, and overall user satisfaction compared to\nbaseline approaches. Our experiments further highlight the effectiveness of\ncombining rule-based methods with LLM-driven refinements to create more\naccessible and robust SPARQL interfaces.",
        "url": "http://arxiv.org/abs/2511.02002v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02002v1",
        "arxiv_id": "2511.02002v1",
        "authors": [
            "Xiangru Jian",
            "Zhengyuan Dong",
            "M. Tamer Özsu"
        ],
        "submitted": "2025-11-03 19:15:51",
        "source": "arxiv",
        "comment": "Working paper",
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, as it involves query refinement and natural language explanations. However, it focuses on SPARQL query refinement in the semantic web domain, which is not a central match to your primary focus on e-commerce and real-time relevance optimization."
    },
    {
        "title": "Training Proactive and Personalized LLM Agents",
        "abstract": "While existing work focuses primarily on task success, we argue that\neffective real-world agents require optimizing three dimensions: productivity\n(task completion), proactivity (asking essential questions), and\npersonalization (adapting to diverse user preferences). We introduce UserVille,\nan interactive environment with LLM-based user simulators enabling diverse,\nconfigurable user preferences. Leveraging UserVille, we introduce PPP, a\nmulti-objective reinforcement learning approach that jointly optimizes all\nthree dimensions: Productivity, Proactivity, and Personalization. Experiments\non software engineering and deep research tasks show that agents trained with\nPPP achieve substantial improvements over strong baselines such as GPT-5 (+21.6\non average), demonstrating the ability to ask strategic clarifying questions,\nadapt to unseen user preferences, and improve task success through better\ninteraction. This work demonstrates that explicitly optimizing for\nuser-centered interaction is critical for building practical and effective AI\nagents.",
        "url": "http://arxiv.org/abs/2511.02208v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02208v1",
        "arxiv_id": "2511.02208v1",
        "authors": [
            "Weiwei Sun",
            "Xuhui Zhou",
            "Weihua Du",
            "Xingyao Wang",
            "Sean Welleck",
            "Graham Neubig",
            "Maarten Sap",
            "Yiming Yang"
        ],
        "submitted": "2025-11-04 02:59:36",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'personalization' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the development of proactive and personalized LLM agents, focusing on optimizing productivity, proactivity, and personalization. While it touches on user behavior modeling, its primary focus is on multi-objective reinforcement learning and user-centered interaction, which is somewhat related to your interests in IR and NLP. However, the lack of direct connection to query understanding, ranking models, or click models limits its relevance."
    },
    {
        "title": "InsurAgent: A Large Language Model-Empowered Agent for Simulating Individual Behavior in Purchasing Flood Insurance",
        "abstract": "Flood insurance is an effective strategy for individuals to mitigate\ndisaster-related losses. However, participation rates among at-risk populations\nin the United States remain strikingly low. This gap underscores the need to\nunderstand and model the behavioral mechanisms underlying insurance decisions.\nLarge language models (LLMs) have recently exhibited human-like intelligence\nacross wide-ranging tasks, offering promising tools for simulating human\ndecision-making. This study constructs a benchmark dataset to capture insurance\npurchase probabilities across factors. Using this dataset, the capacity of LLMs\nis evaluated: while LLMs exhibit a qualitative understanding of factors, they\nfall short in estimating quantitative probabilities. To address this\nlimitation, InsurAgent, an LLM-empowered agent comprising five modules\nincluding perception, retrieval, reasoning, action, and memory, is proposed.\nThe retrieval module leverages retrieval-augmented generation (RAG) to ground\ndecisions in empirical survey data, achieving accurate estimation of marginal\nand bivariate probabilities. The reasoning module leverages LLM common sense to\nextrapolate beyond survey data, capturing contextual information that is\nintractable for traditional models. The memory module supports the simulation\nof temporal decision evolutions, illustrated through a roller coaster life\ntrajectory. Overall, InsurAgent provides a valuable tool for behavioral\nmodeling and policy analysis.",
        "url": "http://arxiv.org/abs/2511.02119v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02119v1",
        "arxiv_id": "2511.02119v1",
        "authors": [
            "Ziheng Geng",
            "Jiachen Liu",
            "Ran Cao",
            "Lu Cheng",
            "Dan M. Frangopol",
            "Minghui Cheng"
        ],
        "submitted": "2025-11-03 23:19:27",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the application of large language models in simulating individual behavior in purchasing flood insurance, which is somewhat related to information retrieval and user behavior modeling. However, the focus is on behavioral modeling and policy analysis, rather than query understanding, ranking models, or real-time relevance optimization, making it only loosely relevant to your core research themes."
    },
    {
        "title": "Complete asymptotic type-token relationship for growing complex systems with inverse power-law count rankings",
        "abstract": "The growth dynamics of complex systems often exhibit statistical regularities\ninvolving power-law relationships. For real finite complex systems formed by\ncountable tokens (animals, words) as instances of distinct types (species,\ndictionary entries), an inverse power-law scaling $S \\sim r^{-\\alpha}$ between\ntype count $S$ and type rank $r$, widely known as Zipf's law, is widely\nobserved to varying degrees of fidelity. A secondary, summary relationship is\nHeaps' law, which states that the number of types scales sublinearly with the\ntotal number of observed tokens present in a growing system. Here, we propose\nan idealized model of a growing system that (1) deterministically produces\narbitrary inverse power-law count rankings for types, and (2) allows us to\ndetermine the exact asymptotics of the type-token relationship. Our argument\nimproves upon and remedies earlier work. We obtain a unified asymptotic\nexpression for all values of $\\alpha$, which corrects the special cases of\n$\\alpha = 1$ and $\\alpha \\gg 1$. Our approach relies solely on the form of\ncount rankings, avoids unnecessary approximations, and does not involve any\nstochastic mechanisms or sampling processes. We thereby demonstrate that a\ngeneral type-token relationship arises solely as a consequence of Zipf's law.",
        "url": "http://arxiv.org/abs/2511.02069v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02069v1",
        "arxiv_id": "2511.02069v1",
        "authors": [
            "Pablo Rosillo-Rodes",
            "Laurent Hébert-Dufresne",
            "Peter Sheridan Dodds"
        ],
        "submitted": "2025-11-03 21:07:33",
        "source": "arxiv",
        "comment": "5 pages, 2 figures",
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be primarily focused on mathematical modeling of complex systems and their growth dynamics, specifically exploring the asymptotic type-token relationship. While it touches on power-law relationships, which are related to Zipf's law, it does not directly relate to the user's core research themes in Information Retrieval, Search technologies, or Natural Language Processing."
    },
    {
        "title": "MemSearcher: Training LLMs to Reason, Search and Manage Memory via End-to-End Reinforcement Learning",
        "abstract": "Typical search agents concatenate the entire interaction history into the LLM\ncontext, preserving information integrity but producing long, noisy contexts,\nresulting in high computation and memory costs. In contrast, using only the\ncurrent turn avoids this overhead but discards essential information. This\ntrade-off limits the scalability of search agents. To address this challenge,\nwe propose MemSearcher, an agent workflow that iteratively maintains a compact\nmemory and combines the current turn with it. At each turn, MemSearcher fuses\nthe user's question with the memory to generate reasoning traces, perform\nsearch actions, and update memory to retain only information essential for\nsolving the task. This design stabilizes context length across multi-turn\ninteractions, improving efficiency without sacrificing accuracy. To optimize\nthis workflow, we introduce multi-context GRPO, an end-to-end RL framework that\njointly optimize reasoning, search strategies, and memory management of\nMemSearcher Agents. Specifically, multi-context GRPO samples groups of\ntrajectories under different contexts and propagates trajectory-level\nadvantages across all conversations within them. Trained on the same dataset as\nSearch-R1, MemSearcher achieves significant improvements over strong baselines\non seven public benchmarks: +11% on Qwen2.5-3B-Instruct and +12% on\nQwen2.5-7B-Instruct relative average gains. Notably, the 3B-based MemSearcher\neven outperforms 7B-based baselines, demonstrating that striking a balance\nbetween information integrity and efficiency yields both higher accuracy and\nlower computational overhead. The code and models will be publicly available at\nhttps://github.com/icip-cas/MemSearcher",
        "url": "http://arxiv.org/abs/2511.02805v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02805v1",
        "arxiv_id": "2511.02805v1",
        "authors": [
            "Qianhao Yuan",
            "Jie Lou",
            "Zichao Li",
            "Jiawei Chen",
            "Yaojie Lu",
            "Hongyu Lin",
            "Le Sun",
            "Debing Zhang",
            "Xianpei Han"
        ],
        "submitted": "2025-11-04 18:27:39",
        "source": "arxiv",
        "comment": "Project page: https://github.com/icip-cas/MemSearcher",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The proposed MemSearcher agent workflow and multi-context GRPO framework demonstrate a deep semantic understanding and real-time relevance optimization, aligning with your core research themes."
    },
    {
        "title": "Optimal Singular Damage: Efficient LLM Inference in Low Storage Regimes",
        "abstract": "Large language models (LLMs) are increasingly prevalent across diverse\napplications. However, their enormous size limits storage and processing\ncapabilities to a few well-resourced stakeholders. As a result, most\napplications rely on pre-trained LLMs, fine-tuned for specific tasks. However,\neven storing the fine-tuned versions of these models remains a significant\nchallenge due to the wide range of tasks they address. Recently, studies show\nthat fine-tuning these models primarily affects a small fraction of parameters,\nhighlighting the need for more efficient storage of fine-tuned models. This\npaper focuses on efficient storage of parameter updates in pre-trained models\nafter fine-tuning. To address this challenge, we leverage the observation that\nfine-tuning updates are both low-rank and sparse, which can be utilized for\nstorage efficiency. However, using only low-rank approximation or\nsparsification may discard critical singular components that enhance model\nexpressivity. We first observe that given the same memory budget, sparsified\nlow-rank approximations with larger ranks outperform standard low-rank\napproximations with smaller ranks. Building on this, we propose our method,\noptimal singular damage, that selectively sparsifies low-rank approximated\nupdates by leveraging the interleaved importance of singular vectors, ensuring\nthat the most impactful components are retained. We demonstrate through\nextensive experiments that our proposed methods lead to significant storage\nefficiency and superior accuracy within the same memory budget compared to\nemploying the low-rank approximation or sparsification individually.",
        "url": "http://arxiv.org/abs/2511.02681v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02681v1",
        "arxiv_id": "2511.02681v1",
        "authors": [
            "Mohammadsajad Alipour",
            "Mohammad Mohammadi Amiri"
        ],
        "submitted": "2025-11-04 16:05:25",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on efficient storage of pre-trained language models, which is not directly related to information retrieval, search technologies, or query understanding. While it involves deep learning and model optimization, the context is more aligned with NLP and model storage efficiency rather than real-time relevance optimization or user behavior modeling."
    },
    {
        "title": "The Analysis of Lexical Errors in Machine Translation from English into Romanian",
        "abstract": "The research explores error analysis in the performance of translating by\nMachine Translation from English into Romanian, and it focuses on lexical\nerrors found in texts which include official information, provided by the World\nHealth Organization (WHO), the Gavi Organization, by the patient information\nleaflet (the information about the active ingredients of the vaccines or the\nmedication, the indications, the dosage instructions, the storage instructions,\nthe side effects and warning, etc.). All of these texts are related to Covid-19\nand have been translated by Google Translate, a multilingual Machine\nTranslation that was created by Google. In the last decades, Google has\nactively worked to develop a more accurate and fluent automatic translation\nsystem. This research, specifically focused on improving Google Translate, aims\nto enhance the overall quality of Machine Translation by achieving better\nlexical selection and by reducing errors. The investigation involves a\ncomprehensive analysis of 230 texts that have been translated from English into\nRomanian.",
        "url": "http://arxiv.org/abs/2511.02587v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02587v1",
        "arxiv_id": "2511.02587v1",
        "authors": [
            "Angela Stamatie"
        ],
        "submitted": "2025-11-04 14:07:21",
        "source": "arxiv",
        "comment": "Doctoral thesis",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on machine translation error analysis, which is outside the scope of information retrieval, search technologies, and natural language processing. Although it involves NLP, the specific context and goals of the research do not align with your core themes."
    },
    {
        "title": "DetectiumFire: A Comprehensive Multi-modal Dataset Bridging Vision and Language for Fire Understanding",
        "abstract": "Recent advances in multi-modal models have demonstrated strong performance in\ntasks such as image generation and reasoning. However, applying these models to\nthe fire domain remains challenging due to the lack of publicly available\ndatasets with high-quality fire domain annotations. To address this gap, we\nintroduce DetectiumFire, a large-scale, multi-modal dataset comprising of 22.5k\nhigh-resolution fire-related images and 2.5k real-world fire-related videos\ncovering a wide range of fire types, environments, and risk levels. The data\nare annotated with both traditional computer vision labels (e.g., bounding\nboxes) and detailed textual prompts describing the scene, enabling applications\nsuch as synthetic data generation and fire risk reasoning. DetectiumFire offers\nclear advantages over existing benchmarks in scale, diversity, and data\nquality, significantly reducing redundancy and enhancing coverage of real-world\nscenarios. We validate the utility of DetectiumFire across multiple tasks,\nincluding object detection, diffusion-based image generation, and\nvision-language reasoning. Our results highlight the potential of this dataset\nto advance fire-related research and support the development of intelligent\nsafety systems. We release DetectiumFire to promote broader exploration of fire\nunderstanding in the AI community. The dataset is available at\nhttps://kaggle.com/datasets/38b79c344bdfc55d1eed3d22fbaa9c31fad45e27edbbe9e3c529d6e5c4f93890",
        "url": "http://arxiv.org/abs/2511.02495v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02495v1",
        "arxiv_id": "2511.02495v1",
        "authors": [
            "Zixuan Liu",
            "Siavash H. Khajavi",
            "Guangkai Jiang"
        ],
        "submitted": "2025-11-04 11:33:11",
        "source": "arxiv",
        "comment": "Advances in Neural Information Processing Systems 2025 (NeurIPS\n  2025), Poster, https://neurips.cc/virtual/2025/loc/san-diego/poster/121400",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval, Search technologies, or user behavior modeling, which are core areas of your research interests. While it involves multi-modal models and vision-language reasoning, the focus is on computer vision and fire understanding, which is not a primary area of your research."
    },
    {
        "title": "LiveSecBench: A Dynamic and Culturally-Relevant AI Safety Benchmark for LLMs in Chinese Context",
        "abstract": "In this work, we propose LiveSecBench, a dynamic and continuously updated\nsafety benchmark specifically for Chinese-language LLM application scenarios.\nLiveSecBench evaluates models across six critical dimensions (Legality, Ethics,\nFactuality, Privacy, Adversarial Robustness, and Reasoning Safety) rooted in\nthe Chinese legal and social frameworks. This benchmark maintains relevance\nthrough a dynamic update schedule that incorporates new threat vectors, such as\nthe planned inclusion of Text-to-Image Generation Safety and Agentic Safety in\nthe next update. For now, LiveSecBench (v251030) has evaluated 18 LLMs,\nproviding a landscape of AI safety in the context of Chinese language. The\nleaderboard is publicly accessible at https://livesecbench.intokentech.cn/.",
        "url": "http://arxiv.org/abs/2511.02366v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02366v1",
        "arxiv_id": "2511.02366v1",
        "authors": [
            "Yudong Li",
            "Zhongliang Yang",
            "Kejiang Chen",
            "Wenxuan Wang",
            "Tianxin Zhang",
            "Sifang Wan",
            "Kecheng Wang",
            "Haitian Li",
            "Xu Wang",
            "Lefan Cheng",
            "Youdan Yang",
            "Baocheng Chen",
            "Ziyu Liu",
            "Yufei Sun",
            "Liyan Wu",
            "Wenya Wen",
            "Xingchi Gu",
            "Peiru Yang"
        ],
        "submitted": "2025-11-04 08:44:09",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be focused on AI safety benchmarking for LLMs in the Chinese context, which is not directly related to the user's core research themes in Information Retrieval and Search technologies, particularly query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Solving cold start in news recommendations: a RippleNet-based system for large scale media outlet",
        "abstract": "We present a scalable recommender system implementation based on RippleNet,\ntailored for the media domain with a production deployment in Onet.pl, one of\nPoland's largest online media platforms. Our solution addresses the cold-start\nproblem for newly published content by integrating content-based item\nembeddings into the knowledge propagation mechanism of RippleNet, enabling\neffective scoring of previously unseen items. The system architecture leverages\nAmazon SageMaker for distributed training and inference, and Apache Airflow for\norchestrating data pipelines and model retraining workflows. To ensure\nhigh-quality training data, we constructed a comprehensive golden dataset\nconsisting of user and item features and a separate interaction table, all\nenabling flexible extensions and integration of new signals.",
        "url": "http://arxiv.org/abs/2511.02052v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02052v1",
        "arxiv_id": "2511.02052v1",
        "authors": [
            "Karol Radziszewski",
            "Michał Szpunar",
            "Piotr Ociepka",
            "Mateusz Buczyński"
        ],
        "submitted": "2025-11-03 20:38:37",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to the user's interests in Information Retrieval and recommender systems, but it focuses on news recommendations and the cold-start problem, which is not a central match for the user's primary focus on query understanding, ranking models, and user behavior modeling. The use of RippleNet and Amazon SageMaker is also not directly related to the user's areas of expertise."
    },
    {
        "title": "Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities",
        "abstract": "As model context lengths continue to grow, concerns about whether models\neffectively use the full context length have persisted. While several carefully\ndesigned long-context evaluations have recently been released, these\nevaluations tend to rely on retrieval from one or more sections of the context,\nwhich allows nearly all of the context tokens to be disregarded as noise. This\nrepresents only one type of task that might be performed with long context. We\nintroduce Oolong, a benchmark of long-context reasoning tasks that require\nanalyzing individual chunks of text on an atomic level, and then aggregating\nthese analyses to answer distributional questions. Oolong is separated into two\ntask sets: Oolong-synth, a set of naturalistic synthetic tasks, where we can\neasily ablate components of the reasoning problem; and Oolong-real, a\ndownstream setting which requires reasoning over real-world conversational\ndata. Oolong requires models to reason over large quantities of examples, to\nperform both classification and counting in-context, and to reason over\ntemporal and user relations. Even frontier models struggle on Oolong, with\nGPT-5, Claude-Sonnet-4, and Gemini-2.5-Pro all achieving less than 50% accuracy\non both splits at 128K. We release the data and evaluation harness for Oolong\nto enable further development of models that can reason over large quantities\nof text.",
        "url": "http://arxiv.org/abs/2511.02817v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02817v1",
        "arxiv_id": "2511.02817v1",
        "authors": [
            "Amanda Bertsch",
            "Adithya Pratapa",
            "Teruko Mitamura",
            "Graham Neubig",
            "Matthew R. Gormley"
        ],
        "submitted": "2025-11-04 18:42:12",
        "source": "arxiv",
        "comment": "Preprint",
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper Oolong: Evaluating Long Context Reasoning and Aggregation Capabilities is somewhat related to the user's interests in Information Retrieval, particularly in the context of deep semantic understanding and real-time relevance optimization. However, the focus on long-context reasoning and aggregation capabilities in natural language processing is not a central match for the user's primary research themes in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "PragExTra: A Multilingual Corpus of Pragmatic Explicitation in Translation",
        "abstract": "Translators often enrich texts with background details that make implicit\ncultural meanings explicit for new audiences. This phenomenon, known as\npragmatic explicitation, has been widely discussed in translation theory but\nrarely modeled computationally. We introduce PragExTra, the first multilingual\ncorpus and detection framework for pragmatic explicitation. The corpus covers\neight language pairs from TED-Multi and Europarl and includes additions such as\nentity descriptions, measurement conversions, and translator remarks. We\nidentify candidate explicitation cases through null alignments and refined\nusing active learning with human annotation. Our results show that entity and\nsystem-level explicitations are most frequent, and that active learning\nimproves classifier accuracy by 7-8 percentage points, achieving up to 0.88\naccuracy and 0.82 F1 across languages. PragExTra establishes pragmatic\nexplicitation as a measurable, cross-linguistic phenomenon and takes a step\ntowards building culturally aware machine translation. Keywords: translation,\nmultilingualism, explicitation",
        "url": "http://arxiv.org/abs/2511.02721v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02721v1",
        "arxiv_id": "2511.02721v1",
        "authors": [
            "Doreen Osmelak",
            "Koel Dutta Chowdhury",
            "Uliana Sentsova",
            "Cristina España-Bonet",
            "Josef van Genabith"
        ],
        "submitted": "2025-11-04 16:44:57",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your core research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves multilingualism and machine translation, the focus is on translation theory and corpus development, which is not a central match to your research themes."
    },
    {
        "title": "UniChange: Unifying Change Detection with Multimodal Large Language Model",
        "abstract": "Change detection (CD) is a fundamental task for monitoring and analyzing land\ncover dynamics. While recent high performance models and high quality datasets\nhave significantly advanced the field, a critical limitation persists. Current\nmodels typically acquire limited knowledge from single-type annotated data and\ncannot concurrently leverage diverse binary change detection (BCD) and semantic\nchange detection (SCD) datasets. This constraint leads to poor generalization\nand limited versatility. The recent advancements in Multimodal Large Language\nModels (MLLMs) introduce new possibilities for a unified CD framework. We\nleverage the language priors and unification capabilities of MLLMs to develop\nUniChange, the first MLLM-based unified change detection model. UniChange\nintegrates generative language abilities with specialized CD functionalities.\nOur model successfully unifies both BCD and SCD tasks through the introduction\nof three special tokens: [T1], [T2], and [CHANGE]. Furthermore, UniChange\nutilizes text prompts to guide the identification of change categories,\neliminating the reliance on predefined classification heads. This design allows\nUniChange to effectively acquire knowledge from multi-source datasets, even\nwhen their class definitions conflict. Experiments on four public benchmarks\n(WHU-CD, S2Looking, LEVIR-CD+, and SECOND) demonstrate SOTA performance,\nachieving IoU scores of 90.41, 53.04, 78.87, and 57.62, respectively,\nsurpassing all previous methods. The code is available at\nhttps://github.com/Erxucomeon/UniChange.",
        "url": "http://arxiv.org/abs/2511.02607v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02607v1",
        "arxiv_id": "2511.02607v1",
        "authors": [
            "Xu Zhang",
            "Danyang Li",
            "Xiaohang Dong",
            "Tianhao Wu",
            "Hualong Yu",
            "Jianye Wang",
            "Qicheng Li",
            "Xiang Li"
        ],
        "submitted": "2025-11-04 14:31:06",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper UniChange: Unifying Change Detection with Multimodal Large Language Model appears to be related to Natural Language Processing (NLP) and multimodal models, but it does not directly align with the user's primary focus on Information Retrieval (IR), query understanding, ranking models, and user behavior modeling. While it involves deep semantic understanding, the context is land cover dynamics and change detection, which is not a central match for the user's research interests."
    },
    {
        "title": "CGES: Confidence-Guided Early Stopping for Efficient and Accurate Self-Consistency",
        "abstract": "Large language models (LLMs) are often queried multiple times at test time,\nwith predictions aggregated by majority vote. While effective, this\nself-consistency strategy (arXiv:2203.11171) requires a fixed number of calls\nand can fail when the correct answer is rare. We introduce Confidence-Guided\nEarly Stopping (CGES), a Bayesian framework that forms posteriors over\ncandidate answers using scalar confidence signals derived from token\nprobabilities or reward models. CGES adaptively halts sampling once the\nposterior mass of a candidate exceeds a threshold. We provide theoretical\nguarantees for both perfectly calibrated confidences and realistic noisy\nconfidence signals. Across five reasoning benchmarks, CGES reduces the average\nnumber of model calls by about 69 percent (for example, from 16.0 to 4.9) while\nmatching the accuracy of self-consistency within 0.06 percentage points.",
        "url": "http://arxiv.org/abs/2511.02603v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02603v1",
        "arxiv_id": "2511.02603v1",
        "authors": [
            "Ehsan Aghazadeh",
            "Ahmad Ghasemi",
            "Hedyeh Beyhaghi",
            "Hossein Pishro-Nik"
        ],
        "submitted": "2025-11-04 14:25:54",
        "source": "arxiv",
        "comment": "Efficient Reasoning @ NeurIPS2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval or Search technologies. It focuses on improving the efficiency and accuracy of large language models, which is a topic in Natural Language Processing, but not a central match to your research interests."
    },
    {
        "title": "Next Token Knowledge Tracing: Exploiting Pretrained LLM Representations to Decode Student Behaviour",
        "abstract": "Modelling student knowledge is a key challenge when leveraging AI in\neducation, with major implications for personalised learning. The Knowledge\nTracing (KT) task aims to predict how students will respond to educational\nquestions in learning environments, based on their prior interactions. Existing\nKT models typically use response correctness along with metadata like skill\ntags and timestamps, often overlooking the question text, which is an important\nsource of pedagogical insight. This omission poses a lost opportunity while\nlimiting predictive performance. We propose Next Token Knowledge Tracing\n(NTKT), a novel approach that reframes KT as a next-token prediction task using\npretrained Large Language Models (LLMs). NTKT represents both student histories\nand question content as sequences of text, allowing LLMs to learn patterns in\nboth behaviour and language. Our series of experiments significantly improves\nperformance over state-of-the-art neural KT models and generalises much better\nto cold-start questions and users. These findings highlight the importance of\nquestion content in KT and demonstrate the benefits of leveraging pretrained\nrepresentations of LLMs to model student learning more effectively.",
        "url": "http://arxiv.org/abs/2511.02599v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02599v1",
        "arxiv_id": "2511.02599v1",
        "authors": [
            "Max Norris",
            "Kobi Gal",
            "Sahan Bulathwela"
        ],
        "submitted": "2025-11-04 14:20:56",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Knowledge Tracing in educational settings, using Large Language Models to predict student responses. While it involves natural language processing and sequence modeling, it is not directly related to information retrieval, search technologies, or e-commerce, which are the primary areas of interest."
    },
    {
        "title": "CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning",
        "abstract": "In human cognition, there exist numerous thought processes that are tacit and\nbeyond verbal expression, enabling us to understand and interact with the world\nin multiple ways. However, contemporary Vision-Language Models (VLMs) remain\nconstrained to reasoning within the discrete and rigid space of linguistic\ntokens, thereby bottlenecking the rich, high-dimensional nature of visual\nperception. To bridge this gap, we propose CoCoVa (Chain of Continuous\nVision-Language Thought), a novel framework for vision-language model that\nleverages continuous cross-modal reasoning for diverse vision-language tasks.\nThe core of CoCoVa is an iterative reasoning cycle, where a novel Latent\nQ-Former (LQ-Former) acts as a dynamic reasoning engine, iteratively refining a\nchain of latent thought vectors through cross-modal fusion. To focus this\nprocess, a token selection mechanism dynamically identifies salient visual\nregions, mimicking attentional focus. To ensure these latent thoughts remain\ngrounded, we train the model with a multi-task objective that combines\ncontrastive learning and diffusion-based reconstruction, enforcing alignment\nbetween latent representations and both visual and textual modalities.\nEvaluations show CoCoVa improves accuracy and token efficiency over strong\nbaselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9B\nmodels on almost all benchmarks. When scaled to 7B LLM backbones, it remains\ncompetitive with state-of-the-art models. Qualitative analysis validates that\nlearned latent space captures interpretable and structured reasoning patterns,\nhighlighting the potential of CoCoVa to bridge the representational gap between\ndiscrete language processing and the continuous nature of visual understanding.",
        "url": "http://arxiv.org/abs/2511.02360v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02360v1",
        "arxiv_id": "2511.02360v1",
        "authors": [
            "Jizheng Ma",
            "Xiaofei Zhou",
            "Yanlong Song",
            "Han Yan"
        ],
        "submitted": "2025-11-04 08:28:46",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on bridging the gap between discrete language processing and continuous visual understanding, proposing a novel framework for vision-language models. While it explores cross-modal reasoning, it doesn't directly relate to information retrieval, search technologies, or user behavior modeling, which are core areas of your research interests."
    },
    {
        "title": "Unlocking the Power of Multi-Agent LLM for Reasoning: From Lazy Agents to Deliberation",
        "abstract": "Large Language Models (LLMs) trained with reinforcement learning and\nverifiable rewards have achieved strong results on complex reasoning tasks.\nRecent work extends this paradigm to a multi-agent setting, where a\nmeta-thinking agent proposes plans and monitors progress while a reasoning\nagent executes subtasks through sequential conversational turns. Despite\npromising performance, we identify a critical limitation: lazy agent behavior,\nin which one agent dominates while the other contributes little, undermining\ncollaboration and collapsing the setup to an ineffective single agent. In this\npaper, we first provide a theoretical analysis showing why lazy behavior\nnaturally arises in multi-agent reasoning. We then introduce a stable and\nefficient method for measuring causal influence, helping mitigate this issue.\nFinally, as collaboration intensifies, the reasoning agent risks getting lost\nin multi-turn interactions and trapped by previous noisy responses. To counter\nthis, we propose a verifiable reward mechanism that encourages deliberation by\nallowing the reasoning agent to discard noisy outputs, consolidate\ninstructions, and restart its reasoning process when necessary. Extensive\nexperiments demonstrate that our framework alleviates lazy agent behavior and\nunlocks the full potential of multi-agent framework for complex reasoning\ntasks.",
        "url": "http://arxiv.org/abs/2511.02303v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02303v1",
        "arxiv_id": "2511.02303v1",
        "authors": [
            "Zhiwei Zhang",
            "Xiaomin Li",
            "Yudi Lin",
            "Hui Liu",
            "Ramraj Chandradevan",
            "Linlin Wu",
            "Minhua Lin",
            "Fali Wang",
            "Xianfeng Tang",
            "Qi He",
            "Suhang Wang"
        ],
        "submitted": "2025-11-04 06:37:31",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the application of multi-agent Large Language Models (LLMs) for complex reasoning tasks. While it touches on aspects of query understanding and deep semantic understanding, its primary focus is on multi-agent reasoning and collaboration, which is somewhat related to your interests in Information Retrieval and NLP. However, the paper's emphasis on multi-agent frameworks and deliberation mechanisms does not directly align with your core research themes."
    },
    {
        "title": "An Evaluation of Interleaved Instruction Tuning on Semantic Reasoning Performance in an Audio MLLM",
        "abstract": "Standard training for Multi-modal Large Language Models (MLLMs) involves\nconcatenating non-textual information, like vision or audio, with a text\nprompt. This approach may not encourage deep integration of modalities,\nlimiting the model's ability to leverage the core language model's reasoning\ncapabilities. This work examined the impact of interleaved instruction tuning\nin an audio MLLM, where audio tokens are interleaved within the prompt. Using\nthe Listen, Think, and Understand (LTU) model as a testbed, we conduct an\nexperiment using the Synonym and Hypernym Audio Reasoning Dataset (SHARD), our\nnewly created reasoning benchmark for audio-based semantic reasoning focusing\non synonym and hypernym recognition. Our findings show that while even\nzero-shot interleaved prompting improves performance on our reasoning tasks, a\nsmall amount of fine-tuning using interleaved training prompts improves the\nresults further, however, at the expense of the MLLM's audio labeling ability.",
        "url": "http://arxiv.org/abs/2511.02234v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02234v1",
        "arxiv_id": "2511.02234v1",
        "authors": [
            "Jiawei Liu",
            "Enis Berk Çoban",
            "Zarina Schevchenko",
            "Hao Tang",
            "Zhigang Zhu",
            "Michael I Mandel",
            "Johanna Devaney"
        ],
        "submitted": "2025-11-04 03:54:55",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Multi-modal Large Language Models (MLLMs) and their ability to integrate audio and text information, which is not directly related to the user's primary research interests in Information Retrieval, query understanding, and ranking models. While it touches on semantic reasoning, it is in the context of audio-based tasks, which is not a central match for the user's interests. The paper's focus on MLLMs and audio-based tasks makes it only loosely relevant to the user's research."
    },
    {
        "title": "Personalized Decision Modeling: Utility Optimization or Textualized-Symbolic Reasoning",
        "abstract": "Decision-making models for individuals, particularly in high-stakes scenarios\nlike vaccine uptake, often diverge from population optimal predictions. This\ngap arises from the uniqueness of the individual decision-making process,\nshaped by numerical attributes (e.g., cost, time) and linguistic influences\n(e.g., personal preferences and constraints). Developing upon Utility Theory\nand leveraging the textual-reasoning capabilities of Large Language Models\n(LLMs), this paper proposes an Adaptive Textual-symbolic Human-centric\nReasoning framework (ATHENA) to address the optimal information integration.\nATHENA uniquely integrates two stages: First, it discovers robust, group-level\nsymbolic utility functions via LLM-augmented symbolic discovery; Second, it\nimplements individual-level semantic adaptation, creating personalized semantic\ntemplates guided by the optimal utility to model personalized choices.\nValidated on real-world travel mode and vaccine choice tasks, ATHENA\nconsistently outperforms utility-based, machine learning, and other LLM-based\nmodels, lifting F1 score by at least 6.5% over the strongest cutting-edge\nmodels. Further, ablation studies confirm that both stages of ATHENA are\ncritical and complementary, as removing either clearly degrades overall\npredictive performance. By organically integrating symbolic utility modeling\nand semantic adaptation, ATHENA provides a new scheme for modeling\nhuman-centric decisions. The project page can be found at\nhttps://yibozh.github.io/Athena.",
        "url": "http://arxiv.org/abs/2511.02194v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02194v1",
        "arxiv_id": "2511.02194v1",
        "authors": [
            "Yibo Zhao",
            "Yang Zhao",
            "Hongru Du",
            "Hao Frank Yang"
        ],
        "submitted": "2025-11-04 02:19:09",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 7,
        "llm_reason": "The paper explores human-centric decision-making models, leveraging Large Language Models (LLMs) for textual-reasoning capabilities. While it focuses on utility optimization and symbolic reasoning, it shares some overlap with query understanding and ranking models in the context of information retrieval. However, the primary focus on decision-making models and symbolic reasoning limits its direct alignment with core research themes in IR and search technologies."
    },
    {
        "title": "Rethinking LLM Human Simulation: When a Graph is What You Need",
        "abstract": "Large language models (LLMs) are increasingly used to simulate humans, with\napplications ranging from survey prediction to decision-making. However, are\nLLMs strictly necessary, or can smaller, domain-grounded models suffice? We\nidentify a large class of simulation problems in which individuals make choices\namong discrete options, where a graph neural network (GNN) can match or surpass\nstrong LLM baselines despite being three orders of magnitude smaller. We\nintroduce Graph-basEd Models for human Simulation (GEMS), which casts discrete\nchoice simulation tasks as a link prediction problem on graphs, leveraging\nrelational knowledge while incorporating language representations only when\nneeded. Evaluations across three key settings on three simulation datasets show\nthat GEMS achieves comparable or better accuracy than LLMs, with far greater\nefficiency, interpretability, and transparency, highlighting the promise of\ngraph-based modeling as a lightweight alternative to LLMs for human simulation.\nOur code is available at https://github.com/schang-lab/gems.",
        "url": "http://arxiv.org/abs/2511.02135v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02135v1",
        "arxiv_id": "2511.02135v1",
        "authors": [
            "Joseph Suh",
            "Suhong Moon",
            "Serina Chang"
        ],
        "submitted": "2025-11-03 23:54:24",
        "source": "arxiv",
        "comment": "Code: https://github.com/schang-lab/gems",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the use of graph neural networks for human simulation, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on human simulation and decision-making tasks is not directly aligned with the user's primary research interests in IR, search technologies, and NLP. The paper's emphasis on graph-based modeling and efficiency is also not a central match for the user's interests in deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "Deep Value Benchmark: Measuring Whether Models Generalize Deep values or Shallow Preferences",
        "abstract": "We introduce the Deep Value Benchmark (DVB), an evaluation framework that\ndirectly tests whether large language models (LLMs) learn fundamental human\nvalues or merely surface-level preferences. This distinction is critical for AI\nalignment: Systems that capture deeper values are likely to generalize human\nintentions robustly, while those that capture only superficial patterns in\npreference data risk producing misaligned behavior. The DVB uses a novel\nexperimental design with controlled confounding between deep values (e.g.,\nmoral principles) and shallow features (e.g., superficial attributes). In the\ntraining phase, we expose LLMs to human preference data with deliberately\ncorrelated deep and shallow features -- for instance, where a user consistently\nprefers (non-maleficence, formal language) options over (justice, informal\nlanguage) alternatives. The testing phase then breaks these correlations,\npresenting choices between (justice, formal language) and (non-maleficence,\ninformal language) options. This design allows us to precisely measure a\nmodel's Deep Value Generalization Rate (DVGR) -- the probability of\ngeneralizing based on the underlying value rather than the shallow feature.\nAcross 9 different models, the average DVGR is just 0.30. All models generalize\ndeep values less than chance. Larger models have a (slightly) lower DVGR than\nsmaller models. We are releasing our dataset, which was subject to three\nseparate human validation experiments. DVB provides an interpretable measure of\na core feature of alignment.",
        "url": "http://arxiv.org/abs/2511.02109v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02109v1",
        "arxiv_id": "2511.02109v1",
        "authors": [
            "Joshua Ashkinaze",
            "Hua Shen",
            "Sai Avula",
            "Eric Gilbert",
            "Ceren Budak"
        ],
        "submitted": "2025-11-03 22:49:54",
        "source": "arxiv",
        "comment": "NeurIPS 2025 (Spotlight)",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 0,
        "llm_reason": "LLM scoring failed."
    },
    {
        "title": "Regularization Through Reasoning: Systematic Improvements in Language Model Classification via Explanation-Enhanced Fine-Tuning",
        "abstract": "Fine-tuning LLMs for classification typically maps inputs directly to labels.\nWe ask whether attaching brief explanations to each label during fine-tuning\nyields better models. We evaluate conversational response quality along three\naxes: naturalness, comprehensiveness, and on-topic adherence, each rated on\n5-point scales. Using ensemble-generated data from multiple LLMs, we fine-tune\na 7B-parameter model and test across six diverse conversational datasets.\nAcross 18 dataset, task settings, label-plus-explanation training outperforms\nlabel-only baselines.\n  A central and unexpected result concerns random tokens. We replace\nhuman-written explanations with text that is syntactically incoherent yet\nvocabulary-aligned with the originals (e.g., shuffled or bag-of-words\nvariants). Despite lacking semantics, these pseudo-explanations still improve\naccuracy over label-only training and often narrow much of the gap to true\nexplanations. The effect persists across datasets and training seeds,\nindicating that gains arise less from meaning than from structure: the extra\ntoken budget encourages richer intermediate computation and acts as a\nregularizer that reduces over-confident shortcuts.\n  Internal analyses support this view: explanation-augmented models exhibit\nhigher activation entropy in intermediate layers alongside sharper predictive\nmass at the output layer, consistent with increased deliberation before\ndecision. Overall, explanation-augmented fine-tuning, whether with genuine\nrationales or carefully constructed random token sequences, improves accuracy\nand reliability for LLM classification while clarifying how token-level\nscaffolding shapes computation during inference.",
        "url": "http://arxiv.org/abs/2511.02044v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02044v1",
        "arxiv_id": "2511.02044v1",
        "authors": [
            "Vivswan Shah",
            "Randy Cogill",
            "Hanwei Yue",
            "Gopinath Chennupati",
            "Rinat Khaziev"
        ],
        "submitted": "2025-11-03 20:25:42",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the impact of explanations on fine-tuning language models for classification tasks. While it touches on aspects of query understanding and ranking models, the primary focus is on NLP and classification accuracy, which is somewhat related to your research interests in IR and search technologies."
    },
    {
        "title": "LTD-Bench: Evaluating Large Language Models by Letting Them Draw",
        "abstract": "Current evaluation paradigms for large language models (LLMs) represent a\ncritical blind spot in AI research--relying on opaque numerical metrics that\nconceal fundamental limitations in spatial reasoning while providing no\nintuitive understanding of model capabilities. This deficiency creates a\ndangerous disconnect between reported performance and practical abilities,\nparticularly for applications requiring physical world understanding. We\nintroduce LTD-Bench, a breakthrough benchmark that transforms LLM evaluation\nfrom abstract scores to directly observable visual outputs by requiring models\nto generate drawings through dot matrices or executable code. This approach\nmakes spatial reasoning limitations immediately apparent even to non-experts,\nbridging the fundamental gap between statistical performance and intuitive\nassessment. LTD-Bench implements a comprehensive methodology with complementary\ngeneration tasks (testing spatial imagination) and recognition tasks (assessing\nspatial perception) across three progressively challenging difficulty levels,\nmethodically evaluating both directions of the critical language-spatial\nmapping. Our extensive experiments with state-of-the-art models expose an\nalarming capability gap: even LLMs achieving impressive results on traditional\nbenchmarks demonstrate profound deficiencies in establishing bidirectional\nmappings between language and spatial concept--a fundamental limitation that\nundermines their potential as genuine world models. Furthermore, LTD-Bench's\nvisual outputs enable powerful diagnostic analysis, offering a potential\napproach to investigate model similarity.",
        "url": "http://arxiv.org/abs/2511.02347v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02347v1",
        "arxiv_id": "2511.02347v1",
        "authors": [
            "Liuhao Lin",
            "Ke Li",
            "Zihan Xu",
            "Yuchen Shi",
            "Yulei Qin",
            "Yan Zhang",
            "Xing Sun",
            "Rongrong Ji"
        ],
        "submitted": "2025-11-04 08:11:23",
        "source": "arxiv",
        "comment": "Accepted by NeurIPS 2025",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on evaluating large language models through drawing and spatial reasoning tasks, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on model capabilities, it does not address query understanding, ranking models, or user behavior modeling, making it somewhat off-topic for your research."
    },
    {
        "title": "Library and Culture: A Scientometric Analysis and Visualization of Research Trends",
        "abstract": "The significance of libraries in preserving and maintaining history and\ntraditional culture cannot be overlooked. It is from this purpose that\nlibraries are to envisage in their programmes cultural activities which must be\ncollected, documented and preserved for posterity. The usefulness of preserved\ninformation lies in the fact that the generation to come will be able to\nestablish their identity. This will also assist them with a foundation to build\nfrom. This study focus on the growth and development of Library and Culture\nresearch in forms of publications reflected in Web of Science database, during\nthe span of 2010-2019. A total 890 publications were found and the highest 124\n(13.93%) publications published in 2019.The analysis maps comprehensively the\nparameters of total output, growth of output, authorship, institution wise and\ncountry-level collaboration patterns, major contributors (individuals, top\npublication sources, institutions, and countries). It exposed that the most\nprolific author is Lo P secured first place by contributing 4 (0.45%)\npublications, followed by Bressan V 3 (0.34%) publications in Library and\nCulture literature. Journal of Academic Librarianship produced the highest\nnumber of records 29 (3.26%) followed by Australian Library Journal having\ncontributed 21 (2.36%).It is identified the domination of Wuhan University;\nSchool Information Management had contributed 6 (0.67%) of total research\noutput. Authors from USA published the highest number of publications with a\ntotal of 244 (27.42%), followed by UK and Australia with 118 (13.26%) and 76\n(8.54%) publications were produced respectively.",
        "url": "http://arxiv.org/abs/2511.02296v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02296v1",
        "arxiv_id": "2511.02296v1",
        "authors": [
            "Auwalu Abdullahi Umar",
            "Muneer Ahmad",
            "Dr M Sadik Batcha"
        ],
        "submitted": "2025-11-04 06:19:25",
        "source": "arxiv",
        "comment": "8 pages, 3 figures, Research Article",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, Natural Language Processing, data mining, or recommender systems. The paper focuses on a scientometric analysis of library and culture research trends, which is outside your primary areas of interest."
    },
    {
        "title": "Research Output on Alopecia Areata Disease: A Scientometric Analysis of Publications from 2010 to 2019",
        "abstract": "The present study is undertaken to find out the publication trends on\nAlopecia Areata Disease during 2010-2019 from the global perspective. The study\nmainly focus on distribution of research output, top journals for publications,\nmost prolific authors, authorship pattern, and citations pattern on Alopecia\nAreata Disease. The results indicate that highest growth rate of publications\noccurred during the year 2019. Columbia University topped the scene among all\ninstitutes. The maximum publications were more than four authored publications.\nChristiano AM and Clynes R were found to be the most prolific authors. It is\nalso found that most of the prolific authors (by number of publications) do\nappear in highly cited publications list. Alopecia Areata Disease researchers\nmostly preferred using article publications to communicate their findings.",
        "url": "http://arxiv.org/abs/2511.02275v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02275v1",
        "arxiv_id": "2511.02275v1",
        "authors": [
            "Muneer Ahmad",
            "M Sadik Batcha"
        ],
        "submitted": "2025-11-04 05:26:46",
        "source": "arxiv",
        "comment": "16 pages, 3 figures, Research Paper",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, Natural Language Processing, data mining, or related topics. The paper is a scientometric analysis of publications on a specific disease, which is unrelated to your areas of focus."
    },
    {
        "title": "KGBridge: Knowledge-Guided Prompt Learning for Non-overlapping Cross-Domain Recommendation",
        "abstract": "Knowledge Graphs (KGs), as structured knowledge bases that organize\nrelational information across diverse domains, provide a unified semantic\nfoundation for cross-domain recommendation (CDR). By integrating symbolic\nknowledge with user-item interactions, KGs enrich semantic representations,\nsupport reasoning, and enhance model interpretability. Despite this potential,\nexisting KG-based methods still face major challenges in CDR, particularly\nunder non-overlapping user scenarios. These challenges arise from: (C1)\nsensitivity to KG sparsity and popularity bias, (C2) dependence on overlapping\nusers for domain alignment and (C3) lack of explicit disentanglement between\ntransferable and domain-specific knowledge, which limit effective and stable\nknowledge transfer. To this end, we propose KGBridge, a knowledge-guided prompt\nlearning framework for cross-domain sequential recommendation under\nnon-overlapping user scenarios. KGBridge comprises two core components: a\nKG-enhanced Prompt Encoder, which models relation-level semantics as soft\nprompts to provide structured and dynamic priors for user sequence modeling\n(addressing C1), and a Two-stage Training Paradigm, which combines cross-domain\npretraining and privacy-preserving fine-tuning to enable knowledge transfer\nwithout user overlap (addressing C2). By combining relation-aware semantic\ncontrol with correspondence-driven disentanglement, KGBridge explicitly\nseparates and balances domain-shared and domain-specific semantics, thereby\nmaintaining complementarity and stabilizing adaptation during fine-tuning\n(addressing C3). Extensive experiments on benchmark datasets demonstrate that\nKGBridge consistently outperforms state-of-the-art baselines and remains robust\nunder varying KG sparsity, highlighting its effectiveness in mitigating\nstructural imbalance and semantic entanglement in KG-enhanced cross-domain\nrecommendation.",
        "url": "http://arxiv.org/abs/2511.02181v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02181v1",
        "arxiv_id": "2511.02181v1",
        "authors": [
            "Yuhan Wang",
            "Qing Xie",
            "Zhifeng Bao",
            "Mengzi Tang",
            "Lin Li",
            "Yongjian Liu"
        ],
        "submitted": "2025-11-04 01:50:01",
        "source": "arxiv",
        "comment": "13 pages, 4 figures",
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 0,
        "llm_reason": "LLM scoring failed."
    },
    {
        "title": "Enhancing Multimodal Recommendations with Vision-Language Models and Information-Aware Fusion",
        "abstract": "Recent advances in multimodal recommendation (MMR) have shown that\nincorporating rich content sources such as images and text can lead to\nsignificant gains representation quality. However, existing methods often rely\non coarse visual features and uncontrolled fusion, leading to redundant or\nmisaligned representations. As a result, visual encoders often fail to capture\nsalient, item-relevant semantics, limiting their contribution in multimodal\nfusion. From an information-theoretic perspective, effective fusion should\nbalance the unique, shared, and redundant information across modalities,\npreserving complementary cues while avoiding correlation bias. This paper\npresents VLIF, a vision-language and information-theoretic fusion framework\nthat enhances multimodal recommendation through two key components. (i) A\nVLM-based visual enrichment module generates fine-grained, title-guided\ndescriptions to transform product images into semantically aligned\nrepresentations. (ii) An information-aware fusion module, inspired by Partial\nInformation Decomposition (PID), disentangles redundant and synergistic signals\nacross modalities for controlled integration. Experiments on three Amazon\ndatasets demonstrate that VLIF consistently outperforms recent multimodal\nbaselines and substantially strengthens the contribution of visual features.",
        "url": "http://arxiv.org/abs/2511.02113v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02113v1",
        "arxiv_id": "2511.02113v1",
        "authors": [
            "Hai-Dang Kieu",
            "Min Xu",
            "Thanh Trung Huynh",
            "Dung D. Le"
        ],
        "submitted": "2025-11-03 23:01:27",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on multimodal recommendations, which is related to information retrieval and search technologies. However, the emphasis is on vision-language models and fusion, which is somewhat tangential to the user's core research themes of query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "LLM Probing with Contrastive Eigenproblems: Improving Understanding and Applicability of CCS",
        "abstract": "Contrast-Consistent Search (CCS) is an unsupervised probing method able to\ntest whether large language models represent binary features, such as sentence\ntruth, in their internal activations. While CCS has shown promise, its two-term\nobjective has been only partially understood. In this work, we revisit CCS with\nthe aim of clarifying its mechanisms and extending its applicability. We argue\nthat what should be optimized for, is relative contrast consistency. Building\non this insight, we reformulate CCS as an eigenproblem, yielding closed-form\nsolutions with interpretable eigenvalues and natural extensions to multiple\nvariables. We evaluate these approaches across a range of datasets, finding\nthat they recover similar performance to CCS, while avoiding problems around\nsensitivity to random initialization. Our results suggest that relativizing\ncontrast consistency not only improves our understanding of CCS but also opens\npathways for broader probing and mechanistic interpretability methods.",
        "url": "http://arxiv.org/abs/2511.02089v1",
        "pdf_url": "http://arxiv.org/pdf/2511.02089v1",
        "arxiv_id": "2511.02089v1",
        "authors": [
            "Stefan F. Schouten",
            "Peter Bloem"
        ],
        "submitted": "2025-11-03 22:00:37",
        "source": "arxiv",
        "comment": "Accepted to the Mechanistic Interpretability Workshop at NeurIPS 2025",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper explores the Contrast-Consistent Search (CCS) method, which is related to query understanding and ranking models in Information Retrieval. However, the focus on probing large language models and their internal activations is more aligned with NLP, and the paper's contribution is primarily in the area of model interpretability rather than search technologies or user behavior modeling."
    }
]