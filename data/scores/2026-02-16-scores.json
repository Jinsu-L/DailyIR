[
    {
        "title": "LLM-Confidence Reranker: A Training-Free Approach for Enhancing Retrieval-Augmented Generation Systems",
        "abstract": "Large language models (LLMs) have revolutionized natural language processing, yet hallucinations in knowledge-intensive tasks remain a critical challenge. Retrieval-augmented generation (RAG) addresses this by integrating external knowledge, but its efficacy depends on accurate document retrieval and ranking. Although existing rerankers demonstrate effectiveness, they frequently necessitate specialized training, impose substantial computational expenses, and fail to fully exploit the semantic capabilities of LLMs, particularly their inherent confidence signals. We propose the LLM-Confidence Reranker (LCR), a training-free, plug-and-play algorithm that enhances reranking in RAG systems by leveraging black-box LLM confidence derived from Maximum Semantic Cluster Proportion (MSCP). LCR employs a two-stage process: confidence assessment via multinomial sampling and clustering, followed by binning and multi-level sorting based on query and document confidence thresholds. This approach prioritizes relevant documents while preserving original rankings for high-confidence queries, ensuring robustness. Evaluated on BEIR and TREC benchmarks with BM25 and Contriever retrievers, LCR--using only 7--9B-parameter pre-trained LLMs--consistently improves NDCG@5 by up to 20.6% across pre-trained LLM and fine-tuned Transformer rerankers, without degradation. Ablation studies validate the hypothesis that LLM confidence positively correlates with document relevance, elucidating LCR's mechanism. LCR offers computational efficiency, parallelism for scalability, and broad compatibility, mitigating hallucinations in applications like medical diagnosis.",
        "url": "http://arxiv.org/abs/2602.13571v1",
        "pdf_url": "https://arxiv.org/pdf/2602.13571v1",
        "arxiv_id": "2602.13571v1",
        "authors": [
            "Zhipeng Song",
            "Xiangyu Kong",
            "Xinrui Bao",
            "Yizhi Zhou",
            "Jiulong Jiao",
            "Sitong Liu",
            "Yuhang Zhou",
            "Heng Qi"
        ],
        "submitted": "2026-02-14 03:12:05",
        "source": "arxiv",
        "comment": "Published by ESWA",
        "score": 25,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'trec' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your interests in Information Retrieval, particularly in the context of retrieval-augmented generation systems and ranking models. The proposed LLM-Confidence Reranker leverages deep semantic understanding and confidence signals from large language models, aligning with your focus on real-time relevance optimization. While the paper's primary focus is on NLP, its relevance to IR and search technologies makes it a useful contribution to your research interests."
    },
    {
        "title": "Pailitao-VL: Unified Embedding and Reranker for Real-Time Multi-Modal Industrial Search",
        "abstract": "In this work, we presented Pailitao-VL, a comprehensive multi-modal retrieval system engineered for high-precision, real-time industrial search. We here address three critical challenges in the current SOTA solution: insufficient retrieval granularity, vulnerability to environmental noise, and prohibitive efficiency-performance gap. Our primary contribution lies in two fundamental paradigm shifts. First, we transitioned the embedding paradigm from traditional contrastive learning to an absolute ID-recognition task. Through anchoring instances to a globally consistent latent space defined by billions of semantic prototypes, we successfully overcome the stochasticity and granularity bottlenecks inherent in existing embedding solutions. Second, we evolved the generative reranker from isolated pointwise evaluation to the compare-and-calibrate listwise policy. By synergizing chunk-based comparative reasoning with calibrated absolute relevance scoring, the system achieves nuanced discriminative resolution while circumventing the prohibitive latency typically associated with conventional reranking methods. Extensive offline benchmarks and online A/B tests on Alibaba e-commerce platform confirm that Pailitao-VL achieves state-of-the-art performance and delivers substantial business impact. This work demonstrates a robust and scalable path for deploying advanced MLLM-based retrieval architectures in demanding, large-scale production environments.",
        "url": "http://arxiv.org/abs/2602.13704v1",
        "pdf_url": "https://arxiv.org/pdf/2602.13704v1",
        "arxiv_id": "2602.13704v1",
        "authors": [
            "Lei Chen",
            "Chen Ju",
            "Xu Chen",
            "Zhicheng Wang",
            "Yuheng Jiao",
            "Hongfeng Zhan",
            "Zhaoyang Li",
            "Shihao Xu",
            "Zhixiang Zhao",
            "Tong Jia",
            "Jinsong Lan",
            "Xiaoyong Zhu",
            "Bo Zheng"
        ],
        "submitted": "2026-02-14 10:13:48",
        "source": "arxiv",
        "comment": null,
        "score": 21,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'listwise' (score: +3)",
            "Found 'pointwise' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the areas of query understanding, ranking models, and real-time relevance optimization. The focus on multi-modal industrial search, embedding paradigms, and reranking methods aligns with your expertise in e-commerce and search technologies. However, the specific application domain of industrial search is somewhat different from your primary focus."
    },
    {
        "title": "Behavioral Feature Boosting via Substitute Relationships for E-commerce Search",
        "abstract": "On E-commerce platforms, new products often suffer from the cold-start problem: limited interaction data reduces their search visibility and hurts relevance ranking. To address this, we propose a simple yet effective behavior feature boosting method that leverages substitute relationships among products (BFS). BFS identifies substitutes-products that satisfy similar user needs-and aggregates their behavioral signals (e.g., clicks, add-to-carts, purchases, and ratings) to provide a warm start for new items. Incorporating these enriched signals into ranking models mitigates cold-start effects and improves relevance and competitiveness. Experiments on a large E-commerce platform, both offline and online, show that BFS significantly improves search relevance and product discovery for cold-start products. BFS is scalable and practical, improving user experience while increasing exposure for newly launched items in E-commerce search. The BFS-enhanced ranking model has been launched in production and has served customers since 2025.",
        "url": "http://arxiv.org/abs/2602.14502v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14502v1",
        "arxiv_id": "2602.14502v1",
        "authors": [
            "Chaosheng Dong",
            "Michinari Momma",
            "Yijia Wang",
            "Yan Gao",
            "Yi Sun"
        ],
        "submitted": "2026-02-16 06:35:05",
        "source": "arxiv",
        "comment": "5 pages, 5 figures",
        "score": 14,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'click' (score: +2)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 7,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of e-commerce search. The focus on behavioral feature boosting and substitute relationships aligns with your background in e-commerce and interest in query understanding and ranking models. However, the paper's specific application to cold-start products in e-commerce search is a somewhat narrow extension of your broader interests."
    },
    {
        "title": "HyperRAG: Reasoning N-ary Facts over Hypergraphs for Retrieval Augmented Generation",
        "abstract": "Graph-based retrieval-augmented generation (RAG) methods, typically built on knowledge graphs (KGs) with binary relational facts, have shown promise in multi-hop open-domain QA. However, their rigid retrieval schemes and dense similarity search often introduce irrelevant context, increase computational overhead, and limit relational expressiveness. In contrast, n-ary hypergraphs encode higher-order relational facts that capture richer inter-entity dependencies and enable shallower, more efficient reasoning paths. To address this limitation, we propose HyperRAG, a RAG framework tailored for n-ary hypergraphs with two complementary retrieval variants: (i) HyperRetriever learns structural-semantic reasoning over n-ary facts to construct query-conditioned relational chains. It enables accurate factual tracking, adaptive high-order traversal, and interpretable multi-hop reasoning under context constraints. (ii) HyperMemory leverages the LLM's parametric memory to guide beam search, dynamically scoring n-ary facts and entities for query-aware path expansion. Extensive evaluations on WikiTopics (11 closed-domain datasets) and three open-domain QA benchmarks (HotpotQA, MuSiQue, and 2WikiMultiHopQA) validate HyperRAG's effectiveness. HyperRetriever achieves the highest answer accuracy overall, with average gains of 2.95% in MRR and 1.23% in Hits@10 over the strongest baseline. Qualitative analysis further shows that HyperRetriever bridges reasoning gaps through adaptive and interpretable n-ary chain construction, benefiting both open and closed-domain QA.",
        "url": "http://arxiv.org/abs/2602.14470v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14470v1",
        "arxiv_id": "2602.14470v1",
        "authors": [
            "Wen-Sheng Lien",
            "Yu-Kai Chan",
            "Hao-Lung Hsiao",
            "Bo-Kai Ruan",
            "Meng-Fen Chiang",
            "Chien-An Chen",
            "Yi-Ren Yeh",
            "Hong-Han Shuai"
        ],
        "submitted": "2026-02-16 05:15:55",
        "source": "arxiv",
        "comment": "Accepted by The ACM Web Conference 2026 (WWW '26)",
        "score": 13,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The proposed HyperRAG framework, which learns structural-semantic reasoning over n-ary facts, is a novel approach to multi-hop open-domain QA, aligning with your focus on deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "A Multi-Agent Framework for Medical AI: Leveraging Fine-Tuned GPT, LLaMA, and DeepSeek R1 for Evidence-Based and Bias-Aware Clinical Query Processing",
        "abstract": "Large language models (LLMs) show promise for healthcare question answering, but clinical use is limited by weak verification, insufficient evidence grounding, and unreliable confidence signalling. We propose a multi-agent medical QA framework that combines complementary LLMs with evidence retrieval, uncertainty estimation, and bias checks to improve answer reliability. Our approach has two phases. First, we fine-tune three representative LLM families (GPT, LLaMA, and DeepSeek R1) on MedQuAD-derived medical QA data (20k+ question-answer pairs across multiple NIH domains) and benchmark generation quality. DeepSeek R1 achieves the strongest scores (ROUGE-1 0.536 +- 0.04; ROUGE-2 0.226 +-0.03; BLEU 0.098 -+ 0.018) and substantially outperforms the specialised biomedical baseline BioGPT in zero-shot evaluation. Second, we implement a modular multi-agent pipeline in which a Clinical Reasoning agent (fine-tuned LLaMA) produces structured explanations, an Evidence Retrieval agent queries PubMed to ground responses in recent literature, and a Refinement agent (DeepSeek R1) improves clarity and factual consistency; an optional human validation path is triggered for high-risk or high-uncertainty cases. Safety mechanisms include Monte Carlo dropout and perplexity-based uncertainty scoring, plus lexical and sentiment-based bias detection supported by LIME/SHAP-based analyses. In evaluation, the full system achieves 87% accuracy with relevance around 0.80, and evidence augmentation reduces uncertainty (perplexity 4.13) compared to base responses, with mean end-to-end latency of 36.5 seconds under the reported configuration. Overall, the results indicate that agent specialisation and verification layers can mitigate key single-model limitations and provide a practical, extensible design for evidence-based and bias-aware medical AI.",
        "url": "http://arxiv.org/abs/2602.14158v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14158v1",
        "arxiv_id": "2602.14158v1",
        "authors": [
            "Naeimeh Nourmohammadi",
            "Md Meem Hossain",
            "The Anh Han",
            "Safina Showkat Ara",
            "Zia Ush Shamszaman"
        ],
        "submitted": "2026-02-15 14:17:27",
        "source": "arxiv",
        "comment": "27 pages, 14 figures, 5 tables",
        "score": 13,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to information retrieval, particularly in the context of medical AI and question answering. However, it focuses more on leveraging large language models and multi-agent frameworks for evidence-based and bias-aware clinical query processing, which is not a central match to your core research themes in IR and search technologies."
    },
    {
        "title": "DeepMTL2R: A Library for Deep Multi-task Learning to Rank",
        "abstract": "This paper presents DeepMTL2R, an open-source deep learning framework for Multi-task Learning to Rank (MTL2R), where multiple relevance criteria must be optimized simultaneously. DeepMTL2R integrates heterogeneous relevance signals into a unified, context-aware model by leveraging the self-attention mechanism of transformer architectures, enabling effective learning across diverse and potentially conflicting objectives. The framework includes 21 state-of-the-art multi-task learning algorithms and supports multi-objective optimization to identify Pareto-optimal ranking models. By capturing complex dependencies and long-range interactions among items and labels, DeepMTL2R provides a scalable and expressive solution for modern ranking systems and facilitates controlled comparisons across MTL strategies. We demonstrate its effectiveness on a publicly available dataset, report competitive performance, and visualize the resulting trade-offs among objectives. DeepMTL2R is available at \\href{https://github.com/amazon-science/DeepMTL2R}{https://github.com/amazon-science/DeepMTL2R}.",
        "url": "http://arxiv.org/abs/2602.14519v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14519v1",
        "arxiv_id": "2602.14519v1",
        "authors": [
            "Chaosheng Dong",
            "Peiyao Xiao",
            "Yijia Wang",
            "Kaiyi Ji"
        ],
        "submitted": "2026-02-16 07:11:38",
        "source": "arxiv",
        "comment": null,
        "score": 12,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'learning to rank' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 9,
        "llm_reason": "This paper aligns closely with your research interests in Information Retrieval, specifically Learning to Rank and multi-task learning. The framework DeepMTL2R is designed for optimizing multiple relevance criteria simultaneously, which is a key aspect of your research focus. The use of transformer architectures and self-attention mechanisms also demonstrates a deep semantic understanding, a key area of interest for you."
    },
    {
        "title": "Intent-Driven Dynamic Chunking: Segmenting Documents to Reflect Predicted Information Needs",
        "abstract": "Breaking long documents into smaller segments is a fundamental challenge in information retrieval. Whether for search engines, question-answering systems, or retrieval-augmented generation (RAG), effective segmentation determines how well systems can locate and return relevant information. However, traditional methods, such as fixed-length or coherence-based segmentation, ignore user intent, leading to chunks that split answers or contain irrelevant noise. We introduce Intent-Driven Dynamic Chunking (IDC), a novel approach that uses predicted user queries to guide document segmentation. IDC leverages a Large Language Model to generate likely user intents for a document and then employs a dynamic programming algorithm to find the globally optimal chunk boundaries. This represents a novel application of DP to intent-aware segmentation that avoids greedy pitfalls. We evaluated IDC on six diverse question-answering datasets, including news articles, Wikipedia, academic papers, and technical documentation. IDC outperformed traditional chunking strategies on five datasets, improving top-1 retrieval accuracy by 5% to 67%, and matched the best baseline on the sixth. Additionally, IDC produced 40-60% fewer chunks than baseline methods while achieving 93-100% answer coverage. These results demonstrate that aligning document structure with anticipated information needs significantly boosts retrieval performance, particularly for long and heterogeneous documents.",
        "url": "http://arxiv.org/abs/2602.14784v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14784v1",
        "arxiv_id": "2602.14784v1",
        "authors": [
            "Christos Koutsiaris"
        ],
        "submitted": "2026-02-16 14:32:18",
        "source": "arxiv",
        "comment": "8 pages, 4 figures. Code available at https://github.com/unseen1980/IDC",
        "score": 11,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 9,
        "llm_reason": "This paper aligns closely with your research interests in Information Retrieval, particularly in query understanding and ranking models. The novel approach of Intent-Driven Dynamic Chunking, which leverages predicted user queries to guide document segmentation, is a significant contribution to the field. The evaluation on diverse question-answering datasets and the improvement in retrieval performance further solidify its relevance to your research."
    },
    {
        "title": "We can still parse using syntactic rules",
        "abstract": "This research introduces a new parsing approach, based on earlier syntactic work on context free grammar (CFG) and generalized phrase structure grammar (GPSG). The approach comprises both a new parsing algorithm and a set of syntactic rules and features that overcome the limitations of CFG. It also generates both dependency and constituency parse trees, while accommodating noise and incomplete parses. The system was tested on data from Universal Dependencies, showing a promising average Unlabeled Attachment Score (UAS) of 54.5% in the development dataset (7 corpora) and 53.8% in the test set (12 corpora). The system also provides multiple parse hypotheses, allowing further reranking to improve parsing accuracy. This approach also leverages much of the theoretical syntactic work since the 1950s to be used within a computational context. The application of this approach provides a transparent and interpretable NLP model to process language input.",
        "url": "http://arxiv.org/abs/2602.14238v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14238v1",
        "arxiv_id": "2602.14238v1",
        "authors": [
            "Ghaly Hussein"
        ],
        "submitted": "2026-02-15 17:16:32",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is primarily focused on parsing and syntactic rules, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve NLP, the specific topic of parsing and syntactic rules is not a central match for your research themes."
    },
    {
        "title": "DAIAN: Deep Adaptive Intent-Aware Network for CTR Prediction in Trigger-Induced Recommendation",
        "abstract": "Recommendation systems are essential for personalizing e-commerce shopping experiences. Among these, Trigger-Induced Recommendation (TIR) has emerged as a key scenario, which utilizes a trigger item (explicitly represents a user's instantaneous interest), enabling precise, real-time recommendations. Although several trigger-based techniques have been proposed, most of them struggle to address the intent myopia issue, that is, a recommendation system overemphasizes the role of trigger items and narrowly focuses on suggesting commodities that are highly relevant to trigger items. Meanwhile, existing methods rely on collaborative behavior patterns between trigger and recommended items to identify the user's preferences, yet the sparsity of ID-based interaction restricts their effectiveness. To this end, we propose the Deep Adaptive Intent-Aware Network (DAIAN) that dynamically adapts to users' intent preferences. In general, we first extract the users' personalized intent representations by analyzing the correlation between a user's click and the trigger item, and accordingly retrieve the user's related historical behaviors to mine the user's diverse intent. Besides, sparse collaborative behaviors constrain the performance in capturing items associated with user intent. Hence, we reinforce similarity by leveraging a hybrid enhancer with ID and semantic information, followed by adaptive selection based on varying intents. Experimental results on public datasets and our industrial e-commerce datasets demonstrate the effectiveness of DAIAN.",
        "url": "http://arxiv.org/abs/2602.13971v1",
        "pdf_url": "https://arxiv.org/pdf/2602.13971v1",
        "arxiv_id": "2602.13971v1",
        "authors": [
            "Zhihao Lv",
            "Longtao Zhang",
            "Ailong He",
            "Shuzhi Cao",
            "Shuguang Han",
            "Jufeng Chen"
        ],
        "submitted": "2026-02-15 03:10:36",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'click' (score: +2)",
            "Found 'ctr' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'shopping' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper is somewhat related to your research interests in Information Retrieval, particularly in the context of recommender systems and user behavior modeling. However, it focuses more on CTR prediction and trigger-induced recommendation, which is a specific application rather than a core theme in your research. The use of deep learning and intent-aware models is also relevant to your interests in NLP and data mining."
    },
    {
        "title": "PT-RAG: Structure-Fidelity Retrieval-Augmented Generation for Academic Papers",
        "abstract": "Retrieval-augmented generation (RAG) is increasingly applied to question-answering over long academic papers, where accurate evidence allocation under a fixed token budget is critical. Existing approaches typically flatten academic papers into unstructured chunks during preprocessing, which destroys the native hierarchical structure. This loss forces retrieval to operate in a disordered space, thereby producing fragmented contexts, misallocating tokens to non-evidential regions under finite token budgets, and increasing the reasoning burden for downstream language models. To address these issues, we propose PT-RAG, an RAG framework that treats the native hierarchical structure of academic papers as a low-entropy retrieval prior. PT-RAG first inherits the native hierarchy to construct a structure-fidelity PaperTree index, which prevents entropy increase at the source. It then designs a path-guided retrieval mechanism that aligns query semantics to relevant sections and selects high relevance root-to-leaf paths under a fixed token budget, yielding compact, coherent, and low-entropy retrieval contexts. In contrast to existing RAG approaches, PT-RAG avoids entropy increase caused by destructive preprocessing and provides a native low-entropy structural basis for subsequent retrieval. To assess this design, we introduce entropy-based structural diagnostics that quantify retrieval fragmentation and evidence allocation accuracy. On three academic question-answering benchmarks, PT-RAG achieves consistently lower section entropy and evidence alignment cross entropy than strong baselines, indicating reduced context fragmentation and more precise allocation to evidential regions. These structural advantages directly translate into higher answer quality.",
        "url": "http://arxiv.org/abs/2602.13647v1",
        "pdf_url": "https://arxiv.org/pdf/2602.13647v1",
        "arxiv_id": "2602.13647v1",
        "authors": [
            "Rui Yu",
            "Tianyi Wang",
            "Ruixia Liu",
            "Yinglong Wang"
        ],
        "submitted": "2026-02-14 07:40:09",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of query understanding and ranking models. The paper proposes a novel framework, PT-RAG, that addresses the challenges of retrieval-augmented generation over long academic papers, which is a specific application of your broader interests in IR and NLP. While the paper's focus is on academic papers, the techniques and ideas presented can be applied to other domains, including e-commerce, which aligns with your background and interests."
    },
    {
        "title": "InfoCIR: Multimedia Analysis for Composed Image Retrieval",
        "abstract": "Composed Image Retrieval (CIR) allows users to search for images by combining a reference image with a text prompt that describes desired modifications. While vision-language models like CLIP have popularized this task by embedding multiple modalities into a joint space, developers still lack tools that reveal how these multimodal prompts interact with embedding spaces and why small wording changes can dramatically alter the results. We present InfoCIR, a visual analytics system that closes this gap by coupling retrieval, explainability, and prompt engineering in a single, interactive dashboard. InfoCIR integrates a state-of-the-art CIR back-end (SEARLE arXiv:2303.15247) with a six-panel interface that (i) lets users compose image + text queries, (ii) projects the top-k results into a low-dimensional space using Uniform Manifold Approximation and Projection (UMAP) for spatial reasoning, (iii) overlays similarity-based saliency maps and gradient-derived token-attribution bars for local explanation, and (iv) employs an LLM-powered prompt enhancer that generates counterfactual variants and visualizes how these changes affect the ranking of user-selected target images. A modular architecture built on Plotly-Dash allows new models, datasets, and attribution methods to be plugged in with minimal effort. We argue that InfoCIR helps diagnose retrieval failures, guides prompt enhancement, and accelerates insight generation during model development. All source code allowing for a reproducible demo is available at https://github.com/giannhskp/InfoCIR.",
        "url": "http://arxiv.org/abs/2602.13402v1",
        "pdf_url": "https://arxiv.org/pdf/2602.13402v1",
        "arxiv_id": "2602.13402v1",
        "authors": [
            "Ioannis Dravilas",
            "Ioannis Kapetangeorgis",
            "Anastasios Latsoudis",
            "Conor McCarthy",
            "Gonçalo Marcelino",
            "Marcel Worring"
        ],
        "submitted": "2026-02-13 19:08:30",
        "source": "arxiv",
        "comment": "9+2 pages, 8 figures. Accepted for publication in IEEE PacificVis 2026 (Conference Track). Interactive composed image retrieval (CIR) and ranking explanation",
        "score": 10,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper presents a visual analytics system for Composed Image Retrieval, which involves multimodal prompts and embedding spaces. While it touches on aspects of query understanding and ranking models, its primary focus is on explainability and visual analytics, rather than core IR or NLP topics. The paper's relevance to user interests is somewhat limited due to its specific application domain and lack of direct connection to user behavior modeling or real-time relevance optimization."
    },
    {
        "title": "REDSearcher: A Scalable and Cost-Efficient Framework for Long-Horizon Search Agents",
        "abstract": "Large language models are transitioning from generalpurpose knowledge engines to realworld problem solvers, yet optimizing them for deep search tasks remains challenging. The central bottleneck lies in the extreme sparsity of highquality search trajectories and reward signals, arising from the difficulty of scalable longhorizon task construction and the high cost of interactionheavy rollouts involving external tool calls. To address these challenges, we propose REDSearcher, a unified framework that codesigns complex task synthesis, midtraining, and posttraining for scalable searchagent optimization. Specifically, REDSearcher introduces the following improvements: (1) We frame task synthesis as a dualconstrained optimization, where task difficulty is precisely governed by graph topology and evidence dispersion, allowing scalable generation of complex, highquality tasks. (2) We introduce toolaugmented queries to encourage proactive tool use rather than passive recall.(3) During midtraining, we strengthen core atomic capabilities knowledge, planning, and function calling substantially reducing the cost of collecting highquality trajectories for downstream training. (4) We build a local simulated environment that enables rapid, lowcost algorithmic iteration for reinforcement learning experiments. Across both textonly and multimodal searchagent benchmarks, our approach achieves stateoftheart performance. To facilitate future research on longhorizon search agents, we will release 10K highquality complex text search trajectories, 5K multimodal trajectories and 1K text RL query set, and together with code and model checkpoints.",
        "url": "http://arxiv.org/abs/2602.14234v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14234v1",
        "arxiv_id": "2602.14234v1",
        "authors": [
            "Zheng Chu",
            "Xiao Wang",
            "Jack Hong",
            "Huiming Fan",
            "Yuqi Huang",
            "Yue Yang",
            "Guohai Xu",
            "Chenxiao Zhao",
            "Cheng Xiang",
            "Shengchao Hu",
            "Dongdong Kuang",
            "Ming Liu",
            "Bing Qin",
            "Xing Yu"
        ],
        "submitted": "2026-02-15 17:04:46",
        "source": "arxiv",
        "comment": "https://redsearchagent.github.io/index/",
        "score": 9,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The proposed framework, REDSearcher, addresses challenges in optimizing large language models for deep search tasks, which aligns with your focus on real-time relevance optimization. However, the paper's primary focus is on long-horizon search agents, which is somewhat related but not a central match to your interests in user behavior modeling and click models."
    },
    {
        "title": "Agentic Assistant for 6G: Turn-based Conversations for AI-RAN Hierarchical Co-Management",
        "abstract": "New generations of radio access networks (RAN), especially with native AI services are increasingly difficult for human engineers to manage in real-time. Enterprise networks are often managed locally, where expertise is scarce. Existing research has focused on creating Retrieval-Augmented Generation (RAG) LLMs that can help to plan and configure RAN and core aspects only. Co-management of RAN and edge AI is the gap, which creates hierarchical and dynamic problems that require turn-based human interactions. Here, we create an agentic network manager and turn-based conversation assistant that can understand human intent-based queries that match hierarchical problems in AI-RAN. The framework constructed consists of: (a) a user interface and evaluation dashboard, (b) an intelligence layer that interfaces with the AI-RAN, and (c) a knowledge layer for providing the basis for evaluations and recommendations. These form 3 layers of capability with the following validation performances (average response time 13s): (1) design and planning a service (78\\% accuracy), (2) operating specific AI-RAN tools (89\\% accuracy), and (3) tuning AI-RAN performance (67\\%). These initial results indicate the universal challenges of hallucination but also fast response performance success that can really reduce OPEX costs for small scale enterprise users.",
        "url": "http://arxiv.org/abs/2602.13868v1",
        "pdf_url": "https://arxiv.org/pdf/2602.13868v1",
        "arxiv_id": "2602.13868v1",
        "authors": [
            "Udhaya Srinivasan",
            "Weisi Guo"
        ],
        "submitted": "2026-02-14 19:58:20",
        "source": "arxiv",
        "comment": "submitted to IEEE conference",
        "score": 9,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses the development of an agentic assistant for 6G radio access networks, focusing on turn-based conversations for AI-RAN hierarchical co-management. While it touches on retrieval and generation aspects, the primary focus is on AI-RAN management, which is not directly aligned with the user's core research themes in Information Retrieval and Search technologies. However, the use of retrieval-augmented generation (RAG) LLMs and the emphasis on understanding human intent-based queries show some relevance to the user's interests in query understanding and ranking models."
    },
    {
        "title": "Learning User Interests via Reasoning and Distillation for Cross-Domain News Recommendation",
        "abstract": "News recommendation plays a critical role in online news platforms by helping users discover relevant content. Cross-domain news recommendation further requires inferring user's underlying information needs from heterogeneous signals that often extend beyond direct news consumption. A key challenge lies in moving beyond surface-level behaviors to capture deeper, reusable user interests while maintaining scalability in large-scale production systems. In this paper, we present a reinforcement learning framework that trains large language models to generate high-quality lists of interest-driven news search queries from cross-domain user signals. We formulate query-list generation as a policy optimization problem and employ GRPO with multiple reward signals. We systematically study two compute dimensions: inference-time sampling and model capacity, and empirically observe consistent improvements with increased compute that exhibit scaling-like behavior. Finally, we perform on-policy distillation to transfer the learned policy from a large, compute-intensive teacher to a compact student model suitable for scalable deployment. Extensive offline experiments, ablation studies and large-scale online A/B tests in a production news recommendation system demonstrate consistent gains in both interest modeling quality and downstream recommendation performance.",
        "url": "http://arxiv.org/abs/2602.15005v1",
        "pdf_url": "https://arxiv.org/pdf/2602.15005v1",
        "arxiv_id": "2602.15005v1",
        "authors": [
            "Mengdan Zhu",
            "Yufan Zhao",
            "Tao Di",
            "Yulan Yan",
            "Liang Zhao"
        ],
        "submitted": "2026-02-16 18:45:40",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper aligns well with your research interests in Information Retrieval, particularly in query understanding and user behavior modeling. The focus on cross-domain news recommendation and interest-driven search queries is relevant to your work in query understanding and ranking models. However, the specific application to news recommendation and recommender systems is somewhat outside your primary focus on e-commerce and real-time relevance optimization."
    },
    {
        "title": "DRAMA: Domain Retrieval using Adaptive Module Allocation",
        "abstract": "Neural models are increasingly used in Web-scale Information Retrieval (IR). However, relying on these models introduces substantial computational and energy requirements, leading to increasing attention toward their environmental cost and the sustainability of large-scale deployments. While neural IR models deliver high retrieval effectiveness, their scalability is constrained in multi-domain scenarios, where training and maintaining domain-specific models is inefficient and achieving robust cross-domain generalisation within a unified model remains difficult. This paper introduces DRAMA (Domain Retrieval using Adaptive Module Allocation), an energy- and parameter-efficient framework designed to reduce the environmental footprint of neural retrieval. DRAMA integrates domain-specific adapter modules with a dynamic gating mechanism that selects the most relevant domain knowledge for each query. New domains can be added efficiently through lightweight adapter training, avoiding full model retraining. We evaluate DRAMA on multiple Web retrieval benchmarks covering different domains. Our extensive evaluation shows that DRAMA achieves comparable effectiveness to domain-specific models while using only a fraction of their parameters and computational resources. These findings show that energy-aware model design can significantly improve scalability and sustainability in neural IR.",
        "url": "http://arxiv.org/abs/2602.14960v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14960v1",
        "arxiv_id": "2602.14960v1",
        "authors": [
            "Pranav Kasela",
            "Marco Braga",
            "Ophir Frieder",
            "Nazli Goharian",
            "Gabriella Pasi",
            "Raffaele Perego"
        ],
        "submitted": "2026-02-16 17:38:24",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of neural IR models and their scalability. The paper's focus on energy-aware model design and domain-specific adapter modules aligns with your interests in query understanding and ranking models. The evaluation on Web retrieval benchmarks further supports the paper's relevance to your research."
    },
    {
        "title": "Tool-Aware Planning in Contact Center AI: Evaluating LLMs through Lineage-Guided Query Decomposition",
        "abstract": "We present a domain-grounded framework and benchmark for tool-aware plan generation in contact centers, where answering a query for business insights, our target use case, requires decomposing it into executable steps over structured tools (Text2SQL (T2S)/Snowflake) and unstructured tools (RAG/transcripts) with explicit depends_on for parallelism. Our contributions are threefold: (i) a reference-based plan evaluation framework operating in two modes - a metric-wise evaluator spanning seven dimensions (e.g., tool-prompt alignment, query adherence) and a one-shot evaluator; (ii) a data curation methodology that iteratively refines plans via an evaluator->optimizer loop to produce high-quality plan lineages (ordered plan revisions) while reducing manual effort; and (iii) a large-scale study of 14 LLMs across sizes and families for their ability to decompose queries into step-by-step, executable, and tool-assigned plans, evaluated under prompts with and without lineage. Empirically, LLMs struggle on compound queries and on plans exceeding 4 steps (typically 5-15); the best total metric score reaches 84.8% (Claude-3-7-Sonnet), while the strongest one-shot match rate at the \"A+\" tier (Extremely Good, Very Good) is only 49.75% (o3-mini). Plan lineage yields mixed gains overall but benefits several top models and improves step executability for many. Our results highlight persistent gaps in tool-understanding, especially in tool-prompt alignment and tool-usage completeness, and show that shorter, simpler plans are markedly easier. The framework and findings provide a reproducible path for assessing and improving agentic planning with tools for answering data-analysis queries in contact-center settings.",
        "url": "http://arxiv.org/abs/2602.14955v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14955v1",
        "arxiv_id": "2602.14955v1",
        "authors": [
            "Varun Nathan",
            "Shreyas Guha",
            "Ayush Kumar"
        ],
        "submitted": "2026-02-16 17:36:05",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, as it involves query decomposition and tool-aware planning. However, the focus on contact center AI and tool-understanding is not directly aligned with your primary interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Additive Control Variates Dominate Self-Normalisation in Off-Policy Evaluation",
        "abstract": "Off-policy evaluation (OPE) is essential for assessing ranking and recommendation systems without costly online interventions. Self-Normalised Inverse Propensity Scoring (SNIPS) is a standard tool for variance reduction in OPE, leveraging a multiplicative control variate. Recent advances in off-policy learning suggest that additive control variates (baseline corrections) may offer superior performance, yet theoretical guarantees for evaluation are lacking. This paper provides a definitive answer: we prove that $β^\\star$-IPS, an estimator with an optimal additive baseline, asymptotically dominates SNIPS in Mean Squared Error. By analytically decomposing the variance gap, we show that SNIPS is asymptotically equivalent to using a specific -- but generally sub-optimal -- additive baseline. Our results theoretically justify shifting from self-normalisation to optimal baseline corrections for both ranking and recommendation.",
        "url": "http://arxiv.org/abs/2602.14914v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14914v1",
        "arxiv_id": "2602.14914v1",
        "authors": [
            "Olivier Jeunen",
            "Shashank Gupta"
        ],
        "submitted": "2026-02-16 16:49:23",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'nips' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is primarily focused on off-policy evaluation and control variates, which is somewhat related to information retrieval and search technologies. However, the topic is more closely aligned with recommender systems and does not directly address query understanding, ranking models, or user behavior modeling, making it less relevant to the user's core research interests."
    },
    {
        "title": "Orcheo: A Modular Full-Stack Platform for Conversational Search",
        "abstract": "Conversational search (CS) requires a complex software engineering pipeline that integrates query reformulation, ranking, and response generation. CS researchers currently face two barriers: the lack of a unified framework for efficiently sharing contributions with the community, and the difficulty of deploying end-to-end prototypes needed for user evaluation. We introduce Orcheo, an open-source platform designed to bridge this gap. Orcheo offers three key advantages: (i) A modular architecture promotes component reuse through single-file node modules, facilitating sharing and reproducibility in CS research; (ii) Production-ready infrastructure bridges the prototype-to-system gap via dual execution modes, secure credential management, and execution telemetry, with built-in AI coding support that lowers the learning curve; (iii) Starter-kit assets include 50+ off-the-shelf components for query understanding, ranking, and response generation, enabling the rapid bootstrapping of complete CS pipelines. We describe the framework architecture and validate Orcheo's utility through case studies that highlight modularity and ease of use. Orcheo is released as open source under the MIT License at https://github.com/ShaojieJiang/orcheo.",
        "url": "http://arxiv.org/abs/2602.14710v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14710v1",
        "arxiv_id": "2602.14710v1",
        "authors": [
            "Shaojie Jiang",
            "Svitlana Vakulenko",
            "Maarten de Rijke"
        ],
        "submitted": "2026-02-16 12:56:57",
        "source": "arxiv",
        "comment": "Under review at SIGIR 2026",
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly conversational search and query understanding. The Orcheo platform's focus on modular architecture, ranking, and response generation aligns with your areas of interest. However, the paper's primary focus on software engineering and platform development rather than deep semantic understanding or real-time relevance optimization prevents it from being a perfect match."
    },
    {
        "title": "High Precision Audience Expansion via Extreme Classification in a Two-Sided Marketplace",
        "abstract": "Airbnb search must balance a worldwide, highly varied supply of homes with guests whose location, amenity, style, and price expectations differ widely. Meeting those expectations hinges on an efficient retrieval stage that surfaces only the listings a guest might realistically book, before resource intensive ranking models are applied to determine the best results. Unlike many recommendation engines, our system faces a distinctive challenge, location retrieval, that sits upstream of ranking and determines which geographic areas are queried in order to filter inventory to a candidate set. The preexisting approach employs a deep bayesian bandit based system to predict a rectangular retrieval bounds area that can be used for filtering. The purpose of this paper is to demonstrate the methodology, challenges, and impact of rearchitecting search to retrieve from the subset of most bookable high precision rectangular map cells defined by dividing the world into 25M uniform cells.",
        "url": "http://arxiv.org/abs/2602.14358v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14358v1",
        "arxiv_id": "2602.14358v1",
        "authors": [
            "Dillon Davis",
            "Huiji Gao",
            "Thomas Legrand",
            "Juan Manuel Caicedo Carvajal",
            "Malay Haldar",
            "Kedar Bellare",
            "Moutupsi Paul",
            "Soumyadip Banerjee",
            "Liwei He",
            "Stephanie Moyerman",
            "Sanjeev Katariya"
        ],
        "submitted": "2026-02-16 00:23:38",
        "source": "arxiv",
        "comment": "KDD TSMO 2025: https://sites.google.com/view/tsmo2025/accepted-papers?authuser=0",
        "score": 8,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses search in a specific domain (Airbnb) and focuses on location retrieval, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the emphasis on a two-sided marketplace and the use of a deep Bayesian bandit system for filtering inventory does not directly align with the user's core research themes of deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "Whom to Query for What: Adaptive Group Elicitation via Multi-Turn LLM Interactions",
        "abstract": "Eliciting information to reduce uncertainty about latent group-level properties from surveys and other collective assessments requires allocating limited questioning effort under real costs and missing data. Although large language models enable adaptive, multi-turn interactions in natural language, most existing elicitation methods optimize what to ask with a fixed respondent pool, and do not adapt respondent selection or leverage population structure when responses are partial or incomplete. To address this gap, we study adaptive group elicitation, a multi-round setting where an agent adaptively selects both questions and respondents under explicit query and participation budgets. We propose a theoretically grounded framework that combines (i) an LLM-based expected information gain objective for scoring candidate questions with (ii) heterogeneous graph neural network propagation that aggregates observed responses and participant attributes to impute missing responses and guide per-round respondent selection. This closed-loop procedure queries a small, informative subset of individuals while inferring population-level responses via structured similarity. Across three real-world opinion datasets, our method consistently improves population-level response prediction under constrained budgets, including a >12% relative gain on CES at a 10% respondent budget.",
        "url": "http://arxiv.org/abs/2602.14279v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14279v1",
        "arxiv_id": "2602.14279v1",
        "authors": [
            "Ruomeng Ding",
            "Tianwei Gao",
            "Thomas P. Zollo",
            "Eitan Bachmat",
            "Richard Zemel",
            "Zhun Deng"
        ],
        "submitted": "2026-02-15 19:05:34",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores adaptive group elicitation via multi-turn LLM interactions, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on group elicitation and respondent selection is not directly aligned with the user's primary research interests in IR, query understanding, and ranking models. The connection to NLP is relevant, but not strong enough to elevate the score."
    },
    {
        "title": "Neuromem: A Granular Decomposition of the Streaming Lifecycle in External Memory for LLMs",
        "abstract": "Most evaluations of External Memory Module assume a static setting: memory is built offline and queried at a fixed state. In practice, memory is streaming: new facts arrive continuously, insertions interleave with retrievals, and the memory state evolves while the model is serving queries. In this regime, accuracy and cost are governed by the full memory lifecycle, which encompasses the ingestion, maintenance, retrieval, and integration of information into generation. We present Neuromem, a scalable testbed that benchmarks External Memory Modules under an interleaved insertion-and-retrieval protocol and decomposes its lifecycle into five dimensions including memory data structure, normalization strategy, consolidation policy, query formulation strategy, and context integration mechanism. Using three representative datasets LOCOMO, LONGMEMEVAL, and MEMORYAGENTBENCH, Neuromem evaluates interchangeable variants within a shared serving stack, reporting token-level F1 and insertion/retrieval latency. Overall, we observe that performance typically degrades as memory grows across rounds, and time-related queries remain the most challenging category. The memory data structure largely determines the attainable quality frontier, while aggressive compression and generative integration mechanisms mostly shift cost between insertion and retrieval with limited accuracy gain.",
        "url": "http://arxiv.org/abs/2602.13967v1",
        "pdf_url": "https://arxiv.org/pdf/2602.13967v1",
        "arxiv_id": "2602.13967v1",
        "authors": [
            "Ruicheng Zhang",
            "Xinyi Li",
            "Tianyi Xu",
            "Shuhao Zhang",
            "Xiaofei Liao",
            "Hai Jin"
        ],
        "submitted": "2026-02-15 02:53:37",
        "source": "arxiv",
        "comment": "22 pages, 8 figures, 15 tables. Preprint",
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on External Memory Modules for Large Language Models (LLMs), which is not directly related to Information Retrieval or Search technologies. While it does involve query formulation and retrieval, the context is specific to LLMs and does not align with the user's core research themes."
    },
    {
        "title": "Evaluating Prompt Engineering Techniques for RAG in Small Language Models: A Multi-Hop QA Approach",
        "abstract": "Retrieval Augmented Generation (RAG) is a powerful approach for enhancing the factual grounding of language models by integrating external knowledge. While widely studied for large language models, the optimization of RAG for Small Language Models (SLMs) remains a critical research gap, particularly in complex, multi-hop question-answering tasks that require sophisticated reasoning. In these systems, prompt template design is a crucial yet under-explored factor influencing performance. This paper presents a large-scale empirical study to investigate this factor, evaluating 24 different prompt templates on the HotpotQA dataset. The set includes a standard RAG prompt, nine well-formed techniques from the literature, and 14 novel hybrid variants, all tested on two prominent SLMs: Qwen2.5-3B Instruct and Gemma3-4B-It. Our findings, based on a test set of 18720 instances, reveal significant performance gains of up to 83% on Qwen2.5 and 84.5% on Gemma3-4B-It, yielding an improvement of up to 6% for both models compared to the Standard RAG prompt. This research also offers concrete analysis and actionable recommendations for designing effective and efficient prompts for SLM-based RAG systems, practically for deployment in resource-constrained environments.",
        "url": "http://arxiv.org/abs/2602.13890v1",
        "pdf_url": "https://arxiv.org/pdf/2602.13890v1",
        "arxiv_id": "2602.13890v1",
        "authors": [
            "Amir Hossein Mohammadi",
            "Ali Moeinian",
            "Zahra Razavizade",
            "Afsaneh Fatemi",
            "Reza Ramezani"
        ],
        "submitted": "2026-02-14 21:17:44",
        "source": "arxiv",
        "comment": "32 Pages, Submitted to Journal of Computing and Security",
        "score": 8,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores Retrieval Augmented Generation (RAG) for Small Language Models, which is somewhat related to information retrieval and NLP. However, the focus on prompt engineering and multi-hop question-answering tasks is not directly aligned with the user's core research themes of query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Differentially Private Retrieval-Augmented Generation",
        "abstract": "Retrieval-augmented generation (RAG) is a widely used framework for reducing hallucinations in large language models (LLMs) on domain-specific tasks by retrieving relevant documents from a database to support accurate responses. However, when the database contains sensitive corpora, such as medical records or legal documents, RAG poses serious privacy risks by potentially exposing private information through its outputs. Prior work has demonstrated that one can practically craft adversarial prompts that force an LLM to regurgitate the augmented contexts. A promising direction is to integrate differential privacy (DP), a privacy notion that offers strong formal guarantees, into RAG systems. However, naively applying DP mechanisms into existing systems often leads to significant utility degradation. Particularly for RAG systems, DP can reduce the usefulness of the augmented contexts leading to increase risk of hallucination from the LLMs. Motivated by these challenges, we present DP-KSA, a novel privacy-preserving RAG algorithm that integrates DP using the propose-test-release paradigm. DP-KSA follows from a key observation that most question-answering (QA) queries can be sufficiently answered with a few keywords. Hence, DP-KSA first obtains an ensemble of relevant contexts, each of which will be used to generate a response from an LLM. We utilize these responses to obtain the most frequent keywords in a differentially private manner. Lastly, the keywords are augmented into the prompt for the final output. This approach effectively compresses the semantic space while preserving both utility and privacy. We formally show that DP-KSA provides formal DP guarantees on the generated output with respect to the RAG database. We evaluate DP-KSA on two QA benchmarks using three instruction-tuned LLMs, and our empirical results demonstrate that DP-KSA achieves a strong privacy-utility tradeoff.",
        "url": "http://arxiv.org/abs/2602.14374v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14374v1",
        "arxiv_id": "2602.14374v1",
        "authors": [
            "Tingting Tang",
            "James Flemings",
            "Yongqin Wang",
            "Murali Annavaram"
        ],
        "submitted": "2026-02-16 00:52:57",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 7,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of query understanding and ranking models. However, the focus on differential privacy and its application to retrieval-augmented generation is not a central match to your primary research themes. Nevertheless, the paper's exploration of real-time relevance optimization and semantic understanding makes it somewhat relevant to your interests."
    },
    {
        "title": "LiveNewsBench: Evaluating LLM Web Search Capabilities with Freshly Curated News",
        "abstract": "Large Language Models (LLMs) with agentic web search capabilities show strong potential for tasks requiring real-time information access and complex fact retrieval, yet evaluating such systems remains challenging. We introduce \\bench, a rigorous and regularly updated benchmark designed to assess the agentic web search abilities of LLMs. \\bench automatically generates fresh question-answer pairs from recent news articles, ensuring that questions require information beyond an LLM's training data and enabling clear separation between internal knowledge and search capability. The benchmark features intentionally difficult questions requiring multi-hop search queries, page visits, and reasoning, making it well-suited for evaluating agentic search behavior. Our automated data curation and question generation pipeline enables frequent benchmark updates and supports construction of a large-scale training dataset for agentic web search models, addressing the scarcity of such data in the research community. To ensure reliable evaluation, we include a subset of human-verified samples in the test set. We evaluate a broad range of systems using \\bench, including commercial and open-weight LLMs as well as LLM-based web search APIs. The leaderboard, datasets, and code are publicly available at livenewsbench.com.",
        "url": "http://arxiv.org/abs/2602.13543v1",
        "pdf_url": "https://arxiv.org/pdf/2602.13543v1",
        "arxiv_id": "2602.13543v1",
        "authors": [
            "Yunfan Zhang",
            "Kathleen McKeown",
            "Smaranda Muresan"
        ],
        "submitted": "2026-02-14 01:18:51",
        "source": "arxiv",
        "comment": "An earlier version of this work was publicly available on OpenReview as an ICLR 2026 submission in September 2025",
        "score": 7,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'web search' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of Large Language Models (LLMs) and their web search capabilities. The focus on evaluating agentic web search abilities and the introduction of a benchmark for assessing LLMs' performance aligns with your interests in query understanding and ranking models. However, the specific domain of news articles is somewhat narrow compared to your broader interests in e-commerce and general information retrieval."
    },
    {
        "title": "Hunt Globally: Deep Research AI Agents for Drug Asset Scouting in Investing, Business Development, and Search & Evaluation",
        "abstract": "Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels. Recent data suggests >85% of patent filings originate outside the U.S., with China accounting for nearly half of the global total; a growing share of scholarly output is also non-U.S. Industry estimates put China at ~30% of global drug development, spanning 1,200+ novel candidates. In this high-stakes environment, failing to surface \"under-the-radar\" assets creates multi-billion-dollar risk for investors and business development teams, making asset scouting a coverage-critical competition where speed and completeness drive value. Yet today's Deep Research AI agents still lag human experts in achieving high-recall discovery across heterogeneous, multilingual sources without hallucinations.\n  We propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. We construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets that are largely outside U.S.-centric radar. To reflect real deal complexity, we collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, we use LLM-as-judge evaluation calibrated to expert opinions. We compare Bioptic Agent against Claude Opus 4.6, OpenAI GPT-5.2 Pro, Perplexity Deep Research, Gemini 3 Pro + Deep Research, and Exa Websets. Bioptic Agent achieves 79.7% F1 versus 56.2% (Claude Opus 4.6), 50.6% (Gemini 3 Pro + Deep Research), 46.6% (GPT-5.2 Pro), 44.2% (Perplexity Deep Research), and 26.9% (Exa Websets). Performance improves steeply with additional compute, supporting the view that more compute yields better results.",
        "url": "http://arxiv.org/abs/2602.15019v1",
        "pdf_url": "https://arxiv.org/pdf/2602.15019v1",
        "arxiv_id": "2602.15019v1",
        "authors": [
            "Alisa Vinogradova",
            "Vlad Vinogradov",
            "Luba Greenwood",
            "Ilya Yasny",
            "Dmitry Kobyzev",
            "Shoman Kasbekar",
            "Kong Nguyen",
            "Dmitrii Radkevich",
            "Roman Doronin",
            "Andrey Doronichev"
        ],
        "submitted": "2026-02-16 18:57:49",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves AI agents and ranking models, the focus is on drug asset scouting in the biopharmaceutical industry, which is not a primary area of interest for the user."
    },
    {
        "title": "The Wikidata Query Logs Dataset",
        "abstract": "We present the Wikidata Query Logs (WDQL) dataset, a dataset consisting of 200k question-query pairs over the Wikidata knowledge graph. It is over 6x larger than the largest existing Wikidata datasets of similar format without relying on template-generated queries. Instead, we construct it using real-world SPARQL queries sent to the Wikidata Query Service and generate questions for them. Since these log-based queries are anonymized, and therefore often do not produce results, a significant amount of effort is needed to convert them back into meaningful SPARQL queries. To achieve this, we present an agent-based method that iteratively de-anonymizes, cleans, and verifies queries against Wikidata while also generating corresponding natural-language questions. We demonstrate the dataset's benefit for training question-answering methods. All WDQL assets, as well as the agent code, are publicly available under a permissive license.",
        "url": "http://arxiv.org/abs/2602.14594v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14594v1",
        "arxiv_id": "2602.14594v1",
        "authors": [
            "Sebastian Walter",
            "Hannah Bast"
        ],
        "submitted": "2026-02-16 09:49:44",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper presents a dataset for question-query pairs over the Wikidata knowledge graph, which is relevant to Information Retrieval and Natural Language Processing. However, the focus is more on question-answering methods and dataset construction rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest. The connection to real-world search technologies is indirect."
    },
    {
        "title": "Query as Anchor: Scenario-Adaptive User Representation via Large Language Model",
        "abstract": "Industrial-scale user representation learning requires balancing robust universality with acute task-sensitivity. However, existing paradigms primarily yield static, task-agnostic embeddings that struggle to reconcile the divergent requirements of downstream scenarios within unified vector spaces. Furthermore, heterogeneous multi-source data introduces inherent noise and modality conflicts, degrading representation. We propose Query-as-Anchor, a framework shifting user modeling from static encoding to dynamic, query-aware synthesis. To empower Large Language Models (LLMs) with deep user understanding, we first construct UserU, an industrial-scale pre-training dataset that aligns multi-modal behavioral sequences with user understanding semantics, and our Q-Anchor Embedding architecture integrates hierarchical coarse-to-fine encoders into dual-tower LLMs via joint contrastive-autoregressive optimization for query-aware user representation. To bridge the gap between general pre-training and specialized business logic, we further introduce Cluster-based Soft Prompt Tuning to enforce discriminative latent structures, effectively aligning model attention with scenario-specific modalities. For deployment, anchoring queries at sequence termini enables KV-cache-accelerated inference with negligible incremental latency. Evaluations on 10 Alipay industrial benchmarks show consistent SOTA performance, strong scalability, and efficient deployment. Large-scale online A/B testing in Alipay's production system across two real-world scenarios further validates its practical effectiveness. Our code is prepared for public release and will be available at: https://github.com/JhCircle/Q-Anchor.",
        "url": "http://arxiv.org/abs/2602.14492v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14492v1",
        "arxiv_id": "2602.14492v1",
        "authors": [
            "Jiahao Yuan",
            "Yike Xu",
            "Jinyong Wen",
            "Baokun Wang",
            "Ziyi Gao",
            "Xiaotong Lin",
            "Yun Liu",
            "Xing Fu",
            "Yu Cheng",
            "Yongchao Liu",
            "Weiqiang Wang",
            "Zhongle Xie"
        ],
        "submitted": "2026-02-16 06:09:31",
        "source": "arxiv",
        "comment": "15 pages, 12 figures",
        "score": 6,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper aligns well with your research interests in Information Retrieval, particularly in query understanding and user behavior modeling. The proposed Query-as-Anchor framework leverages Large Language Models to empower deep user understanding, which is a key area of focus for you. While not exclusively focused on e-commerce, the industrial-scale application and real-world testing in Alipay's production system make it a relevant and useful contribution to the field."
    },
    {
        "title": "Index Light, Reason Deep: Deferred Visual Ingestion for Visual-Dense Document Question Answering",
        "abstract": "Existing multimodal document question answering methods universally adopt a supply-side ingestion strategy: running a Vision-Language Model (VLM) on every page during indexing to generate comprehensive descriptions, then answering questions through text retrieval. However, this \"pre-ingestion\" approach is costly (a 113-page engineering drawing package requires approximately 80,000 VLM tokens), end-to-end unreliable (VLM outputs may fail to be correctly retrieved due to format mismatches in the retrieval infrastructure), and irrecoverable once it fails. This paper proposes the Deferred Visual Ingestion (DVI) framework, adopting a demand-side ingestion strategy: the indexing phase performs only lightweight metadata extraction, deferring visual understanding to the moment users pose specific questions. DVI's core principle is \"Index for locating, not understanding\"--achieving page localization through structured metadata indexes and BM25 full-text search, then sending original images along with specific questions to a VLM for targeted analysis. Experiments on two real industrial engineering drawings (113 pages + 7 pages) demonstrate that DVI achieves comparable overall accuracy at zero ingestion VLM cost (46.7% vs. 48.9%), an effectiveness rate of 50% on visually necessary queries (vs. 0% for pre-ingestion), and 100% page localization (98% search space compression). DVI also supports interactive refinement and progressive caching, transforming the \"QA accuracy\" problem into a \"page localization\" problem--once the correct drawing page is found, obtaining the answer becomes a matter of interaction rounds.",
        "url": "http://arxiv.org/abs/2602.14162v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14162v1",
        "arxiv_id": "2602.14162v1",
        "authors": [
            "Tao Xu"
        ],
        "submitted": "2026-02-15 14:23:50",
        "source": "arxiv",
        "comment": "24 pages, 9 figures, 9 tables",
        "score": 6,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to the user's core research themes in Information Retrieval, Search technologies, or Natural Language Processing. While it involves multimodal document question answering, the focus is on visual-dense documents and a specific framework for deferred visual ingestion, which does not align with the user's interests in query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Open Rubric System: Scaling Reinforcement Learning with Pairwise Adaptive Rubric",
        "abstract": "Scalar reward models compress multi-dimensional human preferences into a single opaque score, creating an information bottleneck that often leads to brittleness and reward hacking in open-ended alignment. We argue that robust alignment for non-verifiable tasks is fundamentally a principle generalization problem: reward should not be a learned function internalized into a judge, but an explicit reasoning process executed under inspectable principles. To operationalize this view, we present the Open Rubric System (OpenRS), a plug-and-play, rubrics-based LLM-as-a-Judge framework built around Pairwise Adaptive Meta-Rubrics (PAMR) and lightweight Pointwise Verifiable Rubrics (PVRs), which provide both hard-constraint guardrails and verifiable reward components when ground-truth or programmatic checks are available. OpenRS uses an explicit meta-rubric -- a constitution-like specification that governs how rubrics are instantiated, weighted, and enforced -- and instantiates adaptive rubrics on the fly by conditioning on the semantic differences between two candidate responses. It then performs criterion-wise pairwise comparisons and aggregates criterion-level preferences externally, avoiding pointwise weighted scalarization while improving discriminability in open-ended settings. To keep principles consistent yet editable across various domains, we introduce a two-level meta-rubric refinement pipeline (automated evolutionary refinement for general principles and a reproducible human-in-the-loop procedure for domain principles), complemented with pointwise verifiable rubrics that act as both guardrails against degenerate behaviors and a source of verifiable reward for objective sub-tasks. Finally, we instantiate OpenRS as reward supervision in pairwise RL training.",
        "url": "http://arxiv.org/abs/2602.14069v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14069v1",
        "arxiv_id": "2602.14069v1",
        "authors": [
            "Ruipeng Jia",
            "Yunyi Yang",
            "Yuxin Wu",
            "Yongbo Gai",
            "Siyuan Tao",
            "Mengyu Zhou",
            "Jianhe Lin",
            "Xiaoxi Jiang",
            "Guanjun Jiang"
        ],
        "submitted": "2026-02-15 09:39:39",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'pointwise' (score: +3)",
            "Found 'pairwise' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be primarily focused on reinforcement learning and reward modeling in the context of open-ended alignment, which is not directly related to the user's core research themes in Information Retrieval and Search technologies. While it touches on the idea of 'rubrics' and 'judges', it doesn't seem to be relevant to query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user."
    },
    {
        "title": "GRRM: Group Relative Reward Modeling for Machine Translation",
        "abstract": "While Group Relative Policy Optimization (GRPO) offers a powerful framework for LLM post-training, its effectiveness in open-ended domains like Machine Translation hinges on accurate intra-group ranking. We identify that standard Scalar Quality Metrics (SQM) fall short in this context; by evaluating candidates in isolation, they lack the comparative context necessary to distinguish fine-grained linguistic nuances. To address this, we introduce the Group Quality Metric (GQM) paradigm and instantiate it via the Group Relative Reward Model (GRRM). Unlike traditional independent scorers, GRRM processes the entire candidate group jointly, leveraging comparative analysis to rigorously resolve relative quality and adaptive granularity. Empirical evaluations confirm that GRRM achieves competitive ranking accuracy among all baselines. Building on this foundation, we integrate GRRM into the GRPO training loop to optimize the translation policy. Experimental results demonstrate that our framework not only improves general translation quality but also unlocks reasoning capabilities comparable to state-of-the-art reasoning models. We release codes, datasets, and model checkpoints at https://github.com/NJUNLP/GRRM.",
        "url": "http://arxiv.org/abs/2602.14028v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14028v1",
        "arxiv_id": "2602.14028v1",
        "authors": [
            "Sen Yang",
            "Shanbo Cheng",
            "Lu Xu",
            "Jianbing Zhang",
            "Shujian Huang"
        ],
        "submitted": "2026-02-15 07:22:27",
        "source": "arxiv",
        "comment": "19 pages, 6 figures",
        "score": 6,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to the user's core research themes in Information Retrieval, Search technologies, or Natural Language Processing. While it involves ranking models, the context is Machine Translation, which is not the primary focus of the user's research interests. The paper's relevance to recommender systems is also not a central match."
    },
    {
        "title": "From Fluent to Verifiable: Claim-Level Auditability for Deep Research Agents",
        "abstract": "A deep research agent produces a fluent scientific report in minutes; a careful reader then tries to verify the main claims and discovers the real cost is not reading, but tracing: which sentence is supported by which passage, what was ignored, and where evidence conflicts. We argue that as research generation becomes cheap, auditability becomes the bottleneck, and the dominant risk shifts from isolated factual errors to scientifically styled outputs whose claim-evidence links are weak, missing, or misleading. This perspective proposes claim-level auditability as a first-class design and evaluation target for deep research agents, distills recurring long-horizon failure modes (objective drift, transient constraints, and unverifiable inference), and introduces the Auditable Autonomous Research (AAR) standard, a compact measurement framework that makes auditability testable via provenance coverage, provenance soundness, contradiction transparency, and audit effort. We then argue for semantic provenance with protocolized validation: persistent, queryable provenance graphs that encode claim--evidence relations (including conflicts) and integrate continuous validation during synthesis rather than after publication, with practical instrumentation patterns to support deployment at scale.",
        "url": "http://arxiv.org/abs/2602.13855v1",
        "pdf_url": "https://arxiv.org/pdf/2602.13855v1",
        "arxiv_id": "2602.13855v1",
        "authors": [
            "Razeen A Rasheed",
            "Somnath Banerjee",
            "Animesh Mukherjee",
            "Rima Hazra"
        ],
        "submitted": "2026-02-14 19:39:15",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on deep research agents and auditability in scientific reports, which is not directly related to information retrieval, search technologies, or user behavior modeling. Although it touches on NLP and data mining, the context is not aligned with the user's core research themes."
    },
    {
        "title": "KorMedMCQA-V: A Multimodal Benchmark for Evaluating Vision-Language Models on the Korean Medical Licensing Examination",
        "abstract": "We introduce KorMedMCQA-V, a Korean medical licensing-exam-style multimodal multiple-choice question answering benchmark for evaluating vision-language models (VLMs). The dataset consists of 1,534 questions with 2,043 associated images from Korean Medical Licensing Examinations (2012-2023), with about 30% containing multiple images requiring cross-image evidence integration. Images cover clinical modalities including X-ray, computed tomography (CT), electrocardiography (ECG), ultrasound, endoscopy, and other medical visuals. We benchmark over 50 VLMs across proprietary and open-source categories-spanning general-purpose, medical-specialized, and Korean-specialized families-under a unified zero-shot evaluation protocol. The best proprietary model (Gemini-3.0-Pro) achieves 96.9% accuracy, the best open-source model (Qwen3-VL-32B-Thinking) 83.7%, and the best Korean-specialized model (VARCO-VISION-2.0-14B) only 43.2%. We further find that reasoning-oriented model variants gain up to +20 percentage points over instruction-tuned counterparts, medical domain specialization yields inconsistent gains over strong general-purpose baselines, all models degrade on multi-image questions, and performance varies notably across imaging modalities. By complementing the text-only KorMedMCQA benchmark, KorMedMCQA-V forms a unified evaluation suite for Korean medical reasoning across text-only and multimodal conditions. The dataset is available via Hugging Face Datasets: https://huggingface.co/datasets/seongsubae/KorMedMCQA-V.",
        "url": "http://arxiv.org/abs/2602.13650v1",
        "pdf_url": "https://arxiv.org/pdf/2602.13650v1",
        "arxiv_id": "2602.13650v1",
        "authors": [
            "Byungjin Choi",
            "Seongsu Bae",
            "Sunjun Kweon",
            "Edward Choi"
        ],
        "submitted": "2026-02-14 07:42:04",
        "source": "arxiv",
        "comment": "17 pages, 2 figures, 6 tables. (Includes appendix.)",
        "score": 6,
        "keyword_reasons": [
            "Found 'ltr' (score: +3)",
            "Found 'ctr' (score: +2)",
            "Found 'korea' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is about a multimodal benchmark for evaluating vision-language models on a Korean medical licensing examination, which is not directly related to the user's interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. While it involves multimodal data, the focus is on medical imaging and vision-language models, which is not a central match for the user's research themes."
    },
    {
        "title": "Climber-Pilot: A Non-Myopic Generative Recommendation Model Towards Better Instruction-Following",
        "abstract": "Generative retrieval has emerged as a promising paradigm in recommender systems, offering superior sequence modeling capabilities over traditional dual-tower architectures. However, in large-scale industrial scenarios, such models often suffer from inherent myopia: due to single-step inference and strict latency constraints, they tend to collapse diverse user intents into locally optimal predictions, failing to capture long-horizon and multi-item consumption patterns. Moreover, real-world retrieval systems must follow explicit retrieval instructions, such as category-level control and policy constraints. Incorporating such instruction-following behavior into generative retrieval remains challenging, as existing conditioning or post-hoc filtering approaches often compromise relevance or efficiency. In this work, we present Climber-Pilot, a unified generative retrieval framework to address both limitations. First, we introduce Time-Aware Multi-Item Prediction (TAMIP), a novel training paradigm designed to mitigate inherent myopia in generative retrieval. By distilling long-horizon, multi-item foresight into model parameters through time-aware masking, TAMIP alleviates locally optimal predictions while preserving efficient single-step inference. Second, to support flexible instruction-following retrieval, we propose Condition-Guided Sparse Attention (CGSA), which incorporates business constraints directly into the generative process via sparse attention, without introducing additional inference steps. Extensive offline experiments and online A/B testing at NetEase Cloud Music, one of the largest music streaming platforms, demonstrate that Climber-Pilot significantly outperforms state-of-the-art baselines, achieving a 4.24\\% lift of the core business metric.",
        "url": "http://arxiv.org/abs/2602.13581v1",
        "pdf_url": "https://arxiv.org/pdf/2602.13581v1",
        "arxiv_id": "2602.13581v1",
        "authors": [
            "Da Guo",
            "Shijia Wang",
            "Qiang Xiao",
            "Yintao Ren",
            "Weisheng Li",
            "Songpei Xu",
            "Ming Yue",
            "Bin Huang",
            "Guanlin Wu",
            "Chuanjiang Luo"
        ],
        "submitted": "2026-02-14 03:46:06",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper presents a generative recommendation model, Climber-Pilot, which addresses limitations in existing models, such as myopia and instruction-following behavior. While it is related to recommender systems, which is a tangential interest, it does not directly align with the user's primary focus on information retrieval and deep semantic understanding. The paper's focus on sequence modeling and real-time relevance optimization is somewhat relevant, but not a central match."
    },
    {
        "title": "ADAB: Arabic Dataset for Automated Politeness Benchmarking -- A Large-Scale Resource for Computational Sociopragmatics",
        "abstract": "The growing importance of culturally-aware natural language processing systems has led to an increasing demand for resources that capture sociopragmatic phenomena across diverse languages. Nevertheless, Arabic-language resources for politeness detection remain under-explored, despite the rich and complex politeness expressions embedded in Arabic communication. In this paper, we introduce ADAB (Arabic Politeness Dataset), a new annotated Arabic dataset collected from four online platforms, including social media, e-commerce, and customer service domains, covering Modern Standard Arabic and multiple dialects (Gulf, Egyptian, Levantine, and Maghrebi). The dataset was annotated based on Arabic linguistic traditions and pragmatic theory, resulting in three classes: polite, impolite, and neutral. It contains 10,000 samples with linguistic feature annotations across 16 politeness categories and achieves substantial inter-annotator agreement (kappa = 0.703). We benchmark 40 model configurations, including traditional machine learning, transformer-based models, and large language models. The dataset aims to support research on politeness-aware Arabic NLP.",
        "url": "http://arxiv.org/abs/2602.13870v1",
        "pdf_url": "https://arxiv.org/pdf/2602.13870v1",
        "arxiv_id": "2602.13870v1",
        "authors": [
            "Hend Al-Khalifa",
            "Nadia Ghezaiel",
            "Maria Bounnit",
            "Hend Hamed Alhazmi",
            "Noof Abdullah Alfear",
            "Reem Fahad Alqifari",
            "Ameera Masoud Almasoud",
            "Sharefah Ahmed Al-Ghamdi"
        ],
        "submitted": "2026-02-14 19:58:53",
        "source": "arxiv",
        "comment": "Paper accepted @ The Fifteenth biennial Language Resources and Evaluation Conference (LREC2026)",
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on Arabic language processing and politeness detection, which is somewhat related to the user's interests in NLP and IR. However, the specific domain and task are not directly aligned with the user's core research themes, such as query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Text Style Transfer with Parameter-efficient LLM Finetuning and Round-trip Translation",
        "abstract": "This paper proposes a novel method for Text Style Transfer (TST) based on parameter-efficient fine-tuning of Large Language Models (LLMs). Addressing the scarcity of parallel corpora that map between styles, the study employs roundtrip translation to synthesize such parallel datasets from monolingual corpora. This approach creates 'neutralized' text devoid of stylistic attributes, essentially creating a shared input style at training-time and inference-time. Experimental results demonstrate consistent superiority of this method over zero-shot prompting and fewshot ICL techniques measured by BLEU scores and style accuracy scores across four investigated domains. Furthermore, the integration of retrieval-augmented generation (RAG) for terminology and name knowledge enhances robustness and stylistic consistency.",
        "url": "http://arxiv.org/abs/2602.15013v1",
        "pdf_url": "https://arxiv.org/pdf/2602.15013v1",
        "arxiv_id": "2602.15013v1",
        "authors": [
            "Ruoxi Liu",
            "Philipp Koehn"
        ],
        "submitted": "2026-02-16 18:52:43",
        "source": "arxiv",
        "comment": "9 pages, 5 figures, 4 tables",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on Text Style Transfer, which is related to query understanding in Information Retrieval, but it does not directly address ranking models or user behavior modeling. The use of Large Language Models and round-trip translation is relevant to NLP, but the primary focus on style transfer and terminology knowledge does not align with the user's core research themes."
    },
    {
        "title": "Testimole-Conversational: A 30-Billion-Word Italian Discussion Board Corpus (1996-2024) for Language Modeling and Sociolinguistic Research",
        "abstract": "We present \"Testimole-conversational\" a massive collection of discussion boards messages in the Italian language. The large size of the corpus, more than 30B word-tokens (1996-2024), renders it an ideal dataset for native Italian Large Language Models'pre-training. Furthermore, discussion boards' messages are a relevant resource for linguistic as well as sociological analysis. The corpus captures a rich variety of computer-mediated communication, offering insights into informal written Italian, discourse dynamics, and online social interaction in wide time span. Beyond its relevance for NLP applications such as language modelling, domain adaptation, and conversational analysis, it also support investigations of language variation and social phenomena in digital communication. The resource will be made freely available to the research community.",
        "url": "http://arxiv.org/abs/2602.14819v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14819v1",
        "arxiv_id": "2602.14819v1",
        "authors": [
            "Matteo Rinaldi",
            "Rossella Varvara",
            "Viviana Patti"
        ],
        "submitted": "2026-02-16 15:12:46",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be primarily focused on creating a large corpus for language modeling and sociolinguistic research in the Italian language. While it touches on NLP applications, it does not seem to directly relate to the user's core research themes in Information Retrieval, query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Precedent-Informed Reasoning: Mitigating Overthinking in Large Reasoning Models via Test-Time Precedent Learning",
        "abstract": "Reasoning in Large Language Models (LLMs) often suffers from inefficient long chain-of-thought traces with redundant self-exploration and validation, which inflate computational costs and even degrade performance. Inspired by human reasoning patterns where people solve new problems by leveraging past related cases to constrain search spaces and reduce trial-and-error, we propose Precedent Informed Reasoning (PIR) transforming LRMs'reasoning paradigm from exhaustive self-exploration to guided learning from precedents. PIR addresses two key challenges: what precedents to adopt and how to utilize them. First, Adaptive Precedent Selection (APS) constructs, for each question and LRM, a compact set of precedents that are both semantically related and informative for the model. It ranks examples by a joint score with semantic similarity and model perplexity, then adapts the amount of precedents to maximize perplexity reduction. Second, Test-time Experience Internalization (TEI) is treated as the test-time learning on precedent-informed instruction, updating lightweight adapters to internalize solution patterns and use them as a prior during subsequent reasoning. Experiments across mathematical reasoning, scientific QA, and code generation demonstrate that PIR consistently shortens reasoning traces while maintaining or improving final accuracy across LLMs, yielding outstanding accuracy-efficiency trade-offs.",
        "url": "http://arxiv.org/abs/2602.14451v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14451v1",
        "arxiv_id": "2602.14451v1",
        "authors": [
            "Qianyue Wang",
            "Jinwu Hu",
            "Huanxiang Lin",
            "Bolin Chen",
            "Zhiquan Wen",
            "Yaofo Chen",
            "Yu Rong",
            "Mingkui Tan"
        ],
        "submitted": "2026-02-16 04:17:46",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores reasoning in Large Language Models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on precedent learning and test-time experience internalization is more aligned with NLP and deep learning, but not directly related to user behavior modeling or click models. The paper's emphasis on efficiency and accuracy trade-offs is also relevant, but not a central match for the user's research interests."
    },
    {
        "title": "Detecting LLM Hallucinations via Embedding Cluster Geometry: A Three-Type Taxonomy with Measurable Signatures",
        "abstract": "We propose a geometric taxonomy of large language model hallucinations based on observable signatures in token embedding cluster structure. By analyzing the static embedding spaces of 11 transformer models spanning encoder (BERT, RoBERTa, ELECTRA, DeBERTa, ALBERT, MiniLM, DistilBERT) and decoder (GPT-2) architectures, we identify three operationally distinct hallucination types: Type 1 (center-drift) under weak context, Type 2 (wrong-well convergence) to locally coherent but contextually incorrect cluster regions, and Type 3 (coverage gaps) where no cluster structure exists. We introduce three measurable geometric statistics: α (polarity coupling), \\b{eta} (cluster cohesion), and λ_s (radial information gradient). Across all 11 models, polarity structure (α > 0.5) is universal (11/11), cluster cohesion (\\b{eta} > 0) is universal (11/11), and the radial information gradient is significant (9/11, p < 0.05). We demonstrate that the two models failing λ_s significance -- ALBERT and MiniLM -- do so for architecturally explicable reasons: factorized embedding compression and distillation-induced isotropy, respectively. These findings establish the geometric prerequisites for type-specific hallucination detection and yield testable predictions about architecture-dependent vulnerability profiles.",
        "url": "http://arxiv.org/abs/2602.14259v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14259v1",
        "arxiv_id": "2602.14259v1",
        "authors": [
            "Matic Korun"
        ],
        "submitted": "2026-02-15 18:14:10",
        "source": "arxiv",
        "comment": "9 pages, 5 figures",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves large language models, the focus is on detecting hallucinations in LLMs, which is a specific aspect of NLP, and does not align with your primary focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Investigation for Relative Voice Impression Estimation",
        "abstract": "Paralinguistic and non-linguistic aspects of speech strongly influence listener impressions. While most research focuses on absolute impression scoring, this study investigates relative voice impression estimation (RIE), a framework for predicting the perceptual difference between two utterances from the same speaker. The estimation target is a low-dimensional vector derived from subjective evaluations, quantifying the perceptual shift of the second utterance relative to the first along an antonymic axis (e.g., ``Dark--Bright''). To isolate expressive and prosodic variation, we used recordings of a professional speaker reading a text in various styles. We compare three modeling approaches: classical acoustic features commonly used for speech emotion recognition, self-supervised speech representations, and multimodal large language models (MLLMs). Our results demonstrate that models using self-supervised representations outperform methods with classical acoustic features, particularly in capturing complex and dynamic impressions (e.g., ``Cold--Warm'') where classical features fail. In contrast, current MLLMs prove unreliable for this fine-grained pairwise task. This study provides the first systematic investigation of RIE and demonstrates the strength of self-supervised speech models in capturing subtle perceptual variations.",
        "url": "http://arxiv.org/abs/2602.14172v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14172v1",
        "arxiv_id": "2602.14172v1",
        "authors": [
            "Keinichi Fujita",
            "Yusuke Ijima"
        ],
        "submitted": "2026-02-15 14:54:52",
        "source": "arxiv",
        "comment": "5 pages,3 figures, Accepted to Speech Prosody 2026",
        "score": 4,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The focus on voice impression estimation and speech analysis is outside your primary areas of interest."
    },
    {
        "title": "Chain-of-Thought Reasoning with Large Language Models for Clinical Alzheimer's Disease Assessment and Diagnosis",
        "abstract": "Alzheimer's disease (AD) has become a prevalent neurodegenerative disease worldwide. Traditional diagnosis still relies heavily on medical imaging and clinical assessment by physicians, which is often time-consuming and resource-intensive in terms of both human expertise and healthcare resources. In recent years, large language models (LLMs) have been increasingly applied to the medical field using electronic health records (EHRs), yet their application in Alzheimer's disease assessment remains limited, particularly given that AD involves complex multifactorial etiologies that are difficult to observe directly through imaging modalities. In this work, we propose leveraging LLMs to perform Chain-of-Thought (CoT) reasoning on patients' clinical EHRs. Unlike direct fine-tuning of LLMs on EHR data for AD classification, our approach utilizes LLM-generated CoT reasoning paths to provide the model with explicit diagnostic rationale for AD assessment, followed by structured CoT-based predictions. This pipeline not only enhances the model's ability to diagnose intrinsically complex factors but also improves the interpretability of the prediction process across different stages of AD progression. Experimental results demonstrate that the proposed CoT-based diagnostic framework significantly enhances stability and diagnostic performance across multiple CDR grading tasks, achieving up to a 15% improvement in F1 score compared to the zero-shot baseline method.",
        "url": "http://arxiv.org/abs/2602.13979v1",
        "pdf_url": "https://arxiv.org/pdf/2602.13979v1",
        "arxiv_id": "2602.13979v1",
        "authors": [
            "Tongze Zhang",
            "Jun-En Ding",
            "Melik Ozolcer",
            "Fang-Ming Hung",
            "Albert Chih-Chieh Yang",
            "Feng Liu",
            "Yi-Rou Ji",
            "Sang Won Bae"
        ],
        "submitted": "2026-02-15 03:56:24",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on applying large language models to clinical Alzheimer's disease assessment and diagnosis, which is outside the user's primary research interests in Information Retrieval and Search technologies. While it involves natural language processing, the context is medical and not related to the user's background in e-commerce or interests in deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "A Tale of Two Graphs: Separating Knowledge Exploration from Outline Structure for Open-Ended Deep Research",
        "abstract": "Open-Ended Deep Research (OEDR) pushes LLM agents beyond short-form QA toward long-horizon workflows that iteratively search, connect, and synthesize evidence into structured reports. However, existing OEDR agents largely follow either linear ``search-then-generate'' accumulation or outline-centric planning. The former suffers from lost-in-the-middle failures as evidence grows, while the latter relies on the LLM to implicitly infer knowledge gaps from the outline alone, providing weak supervision for identifying missing relations and triggering targeted exploration. We present DualGraph memory, an architecture that separates what the agent knows from how it writes. DualGraph maintains two co-evolving graphs: an Outline Graph (OG), and a Knowledge Graph (KG), a semantic memory that stores fine-grained knowledge units, including core entities, concepts, and their relations. By analyzing the KG topology together with structural signals from the OG, DualGraph generates targeted search queries, enabling more efficient and comprehensive iterative knowledge-driven exploration and refinement. Across DeepResearch Bench, DeepResearchGym, and DeepConsult, DualGraph consistently outperforms state-of-the-art baselines in report depth, breadth, and factual grounding; for example, it reaches a 53.08 RACE score on DeepResearch Bench with GPT-5. Moreover, ablation studies confirm the central role of the dual-graph design.",
        "url": "http://arxiv.org/abs/2602.13830v1",
        "pdf_url": "https://arxiv.org/pdf/2602.13830v1",
        "arxiv_id": "2602.13830v1",
        "authors": [
            "Zhuofan Shi",
            "Ming Ma",
            "Zekun Yao",
            "Fangkai Yang",
            "Jue Zhang",
            "Dongge Han",
            "Victor Rühle",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Dongmei Zhang"
        ],
        "submitted": "2026-02-14 15:54:38",
        "source": "arxiv",
        "comment": "26 pages, 4 figures",
        "score": 4,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores a novel architecture for Open-Ended Deep Research, which involves knowledge exploration and synthesis. While it touches on search and knowledge representation, it is more focused on the generation and synthesis aspects, which aligns somewhat with your interests in Information Retrieval and Natural Language Processing. However, the specific context of long-horizon workflows and structured reports is not directly related to your core research themes."
    },
    {
        "title": "StackingNet: Collective Inference Across Independent AI Foundation Models",
        "abstract": "Artificial intelligence built on large foundation models has transformed language understanding, vision and reasoning, yet these systems remain isolated and cannot readily share their capabilities. Integrating the complementary strengths of such independent foundation models is essential for building trustworthy intelligent systems. Despite rapid progress in individual model design, there is no established approach for coordinating such black-box heterogeneous models. Here we show that coordination can be achieved through a meta-ensemble framework termed StackingNet, which draws on principles of collective intelligence to combine model predictions during inference. StackingNet improves accuracy, reduces bias, enables reliability ranking, and identifies or prunes models that degrade performance, all operating without access to internal parameters or training data. Across tasks involving language comprehension, visual estimation, and academic paper rating, StackingNet consistently improves accuracy, robustness, and fairness, compared with individual models and classic ensembles. By turning diversity from a source of inconsistency into collaboration, StackingNet establishes a practical foundation for coordinated artificial intelligence, suggesting that progress may emerge from not only larger single models but also principled cooperation among many specialized ones.",
        "url": "http://arxiv.org/abs/2602.13792v1",
        "pdf_url": "https://arxiv.org/pdf/2602.13792v1",
        "arxiv_id": "2602.13792v1",
        "authors": [
            "Siyang Li",
            "Chenhao Liu",
            "Dongrui Wu",
            "Zhigang Zeng",
            "Lieyun Ding"
        ],
        "submitted": "2026-02-14 14:12:43",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "While the paper discusses AI foundation models and their integration, it primarily focuses on meta-ensemble frameworks for combining model predictions, which is somewhat related to information retrieval and search technologies. However, the emphasis on language understanding, vision, and reasoning, and the lack of direct connection to query understanding, ranking models, or user behavior modeling, limits its relevance to your core research interests."
    },
    {
        "title": "On Theoretically-Driven LLM Agents for Multi-Dimensional Discourse Analysis",
        "abstract": "Identifying the strategic uses of reformulation in discourse remains a key challenge for computational argumentation. While LLMs can detect surface-level similarity, they often fail to capture the pragmatic functions of rephrasing, such as its role within rhetorical discourse. This paper presents a comparative multi-agent framework designed to quantify the benefits of incorporating explicit theoretical knowledge for this task. We utilise an dataset of annotated political debates to establish a new standard encompassing four distinct rephrase functions: Deintensification, Intensification, Specification, Generalisation, and Other, which covers all remaining types (D-I-S-G-O). We then evaluate two parallel LLM-based agent systems: one enhanced by argumentation theory via Retrieval-Augmented Generation (RAG), and an identical zero-shot baseline. The results reveal a clear performance gap: the RAG-enhanced agents substantially outperform the baseline across the board, with particularly strong advantages in detecting Intensification and Generalisation context, yielding an overall Macro F1-score improvement of nearly 30\\%. Our findings provide evidence that theoretical grounding is not only beneficial but essential for advancing beyond mere paraphrase detection towards function-aware analysis of argumentative discourse. This comparative multi-agent architecture represents a step towards scalable, theoretically informed computational tools capable of identifying rhetorical strategies in contemporary discourse.",
        "url": "http://arxiv.org/abs/2602.13713v1",
        "pdf_url": "https://arxiv.org/pdf/2602.13713v1",
        "arxiv_id": "2602.13713v1",
        "authors": [
            "Maciej Uberna",
            "Michał Wawer",
            "Jarosław A. Chudziak",
            "Marcin Koszowy"
        ],
        "submitted": "2026-02-14 10:30:39",
        "source": "arxiv",
        "comment": "8 pages, 4 figures, 3 tables. This is the accepted version of the paper presented at the 18th International Conference on Agents and Artificial Intelligence (ICAART 2026), Marbella, Spain",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 7,
        "llm_reason": "This paper explores the application of LLMs in discourse analysis, focusing on the strategic uses of reformulation in argumentative discourse. While it is not directly related to information retrieval or search technologies, it does involve the use of ranking models (LLMs) and has implications for understanding user behavior in the context of argumentative discourse. However, the primary focus on discourse analysis and argumentation theory means it is not a central match for the user's core research themes."
    },
    {
        "title": "A Geometric Analysis of Small-sized Language Model Hallucinations",
        "abstract": "Hallucinations -- fluent but factually incorrect responses -- pose a major challenge to the reliability of language models, especially in multi-step or agentic settings.\n  This work investigates hallucinations in small-sized LLMs through a geometric perspective, starting from the hypothesis that when models generate multiple responses to the same prompt, genuine ones exhibit tighter clustering in the embedding space, we prove this hypothesis and, leveraging this geometrical insight, we also show that it is possible to achieve a consistent level of separability. This latter result is used to introduce a label-efficient propagation method that classifies large collections of responses from just 30-50 annotations, achieving F1 scores above 90%.\n  Our findings, framing hallucinations from a geometric perspective in the embedding space, complement traditional knowledge-centric and single-response evaluation paradigms, paving the way for further research.",
        "url": "http://arxiv.org/abs/2602.14778v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14778v1",
        "arxiv_id": "2602.14778v1",
        "authors": [
            "Emanuele Ricco",
            "Elia Onofri",
            "Lorenzo Cima",
            "Stefano Cresci",
            "Roberto Di Pietro"
        ],
        "submitted": "2026-02-16 14:29:55",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores language model hallucinations from a geometric perspective, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on language models and hallucinations is not directly aligned with the user's primary research interests in IR and search technologies. The paper's relevance to the user's broader interests in NLP and data mining is moderate."
    },
    {
        "title": "GradMAP: Faster Layer Pruning with Gradient Metric and Projection Compensation",
        "abstract": "Large Language Models (LLMs) exhibit strong reasoning abilities, but their high computational costs limit their practical deployment. Recent studies reveal significant redundancy in LLMs layers, making layer pruning an active research topic. Layer pruning research primarily focuses on two aspects: measuring layer importance and recovering performance after pruning. Unfortunately, the present works fail to simultaneously maintain pruning performance and efficiency. In this study, we propose GradMAP, a faster layer pruning method with \\textbf{Grad}ient \\textbf{M}etric \\textbf{A}nd \\textbf{P}rojection compensation, which consists of two stages. In the first stage, we introduce a novel metric based on gradient magnitudes, enabling a global assessment of layer importance. Note that, it requires only a single backward propagation step per pruning decision, substantially enhancing pruning efficiency. In the second stage, we first analyze the layers with the largest mean shift resulting from pruning, and then incorporate a simple yet effective projection compensation matrix to correct this drift in one step. In this way, the degradation of model performance caused by layer pruning is effectively alleviated. Extensive experiments show that GradMAP outperforms previous layer pruning methods in both pruning speed (achieving an average $4\\times$ speedup) and performance.",
        "url": "http://arxiv.org/abs/2602.14649v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14649v1",
        "arxiv_id": "2602.14649v1",
        "authors": [
            "Hao Liu",
            "Guangyan Li",
            "Wensheng Zhang",
            "Yongqiang Tang"
        ],
        "submitted": "2026-02-16 11:14:02",
        "source": "arxiv",
        "comment": "19 pages",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on layer pruning for Large Language Models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on model performance and efficiency, the context and methods are primarily from the deep learning and neural network pruning domain, making it less relevant to your research."
    },
    {
        "title": "Explainable Token-level Noise Filtering for LLM Fine-tuning Datasets",
        "abstract": "Large Language Models (LLMs) have seen remarkable advancements, achieving state-of-the-art results in diverse applications. Fine-tuning, an important step for adapting LLMs to specific downstream tasks, typically involves further training on corresponding datasets. However, a fundamental discrepancy exists between current fine-tuning datasets and the token-level optimization mechanism of LLMs: most datasets are designed at the sentence-level, which introduces token-level noise, causing negative influence to final performance. In this paper, we propose XTF, an explainable token-level noise filtering framework. XTF decomposes the complex and subtle contributions of token-level data to the fine-tuning process into three distinct and explicit attributes (reasoning importance, knowledge novelty, and task relevance), which can be assessed using scoring methods, and then masks the gradients of selected noisy tokens accordingly to optimize the performance of fine-tuned LLMs. We conduct extensive experiments on three representative downstream tasks (math, code and medicine) across 7 mainstream LLMs. The results demonstrate that XTF can significantly improve downstream performance by up to 13.7% compared to regular fine-tuning. Our work highlights the importance of token-level dataset optimization, and demonstrates the potential of strategies based on attribute decomposition for explaining complex training mechanisms.",
        "url": "http://arxiv.org/abs/2602.14536v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14536v1",
        "arxiv_id": "2602.14536v1",
        "authors": [
            "Yuchen Yang",
            "Wenze Lin",
            "Enhao Huang",
            "Zhixuan Chu",
            "Hongbin Zhou",
            "Lan Tao",
            "Yiming Li",
            "Zhan Qin",
            "Kui Ren"
        ],
        "submitted": "2026-02-16 07:49:33",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your interests in Natural Language Processing (NLP) and deep semantic understanding, as it involves fine-tuning Large Language Models (LLMs) and optimizing their performance. However, it is not directly related to your core research themes in Information Retrieval (IR), query understanding, or ranking models, and the focus is more on dataset optimization rather than search technologies or user behavior modeling."
    },
    {
        "title": "Parameter-Efficient Fine-Tuning of LLMs with Mixture of Space Experts",
        "abstract": "Large Language Models (LLMs) have achieved remarkable progress, with Parameter-Efficient Fine-Tuning (PEFT) emerging as a key technique for downstream task adaptation. However, existing PEFT methods mainly operate in Euclidean space, fundamentally limiting their capacity to capture complex geometric structures inherent in language data. While alternative geometric spaces, like hyperbolic geometries for hierarchical data and spherical manifolds for circular patterns, offer theoretical advantages, forcing representations into a single manifold type ultimately limits expressiveness, even when curvature parameters are learnable. To address this, we propose Mixture of Space (MoS), a unified framework that leverages multiple geometric spaces simultaneously to learn richer, curvature-aware representations. Building on this scheme, we develop MoSLoRA, which extends Low-Rank Adaptation (LoRA) with heterogeneous geometric experts, enabling models to dynamically select or combine appropriate geometric spaces based on input context. Furthermore, to address the computational overhead of frequent manifold switching, we develop a lightweight routing mechanism. Moreover, we provide empirical insights into how curvature optimization impacts training stability and model performance. Our experiments across diverse benchmarks demonstrate that MoSLoRA consistently outperforms strong baselines, achieving up to 5.6% improvement on MATH500 and 15.9% on MAWPS.",
        "url": "http://arxiv.org/abs/2602.14490v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14490v1",
        "arxiv_id": "2602.14490v1",
        "authors": [
            "Buze Zhang",
            "Jinkai Tao",
            "Zilang Zeng",
            "Neil He",
            "Ali Maatouk",
            "Menglin Yang",
            "Rex Ying"
        ],
        "submitted": "2026-02-16 06:07:32",
        "source": "arxiv",
        "comment": "15 pages, 11 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper proposes a framework for parameter-efficient fine-tuning of Large Language Models (LLMs) using multiple geometric spaces. While it touches on the topic of model adaptation and optimization, which are related to information retrieval, it primarily focuses on NLP and model architecture. The paper's emphasis on geometric spaces and curvature optimization does not directly align with the user's core research themes in IR and search technologies."
    },
    {
        "title": "Measuring and Mitigating Post-hoc Rationalization in Reverse Chain-of-Thought Generation",
        "abstract": "Reverse Chain-of-Thought Generation (RCG) synthesizes reasoning traces from query-answer pairs, but runs the risk of producing post-hoc rationalizations: when models can see the answer during generation, the answer serves as a cognitive anchor that shapes the entire explanation. We formalize this phenomenon through a three-level measurement hierarchy: lexical, entropic, and probabilistic anchoring, each captures surface artifacts, entropy dynamics, and latent answer dependence, respectively. We analyze semantic suppression, the intuitive mitigation strategy that instructs models to ignore the answer, to find out its counterproduction: while it reduces lexical overlap, it paradoxically increases entropic and probabilistic anchoring. Drawing on Ironic Process Theory from cognitive psychology, we attribute this failure to active monitoring of the forbidden answer, which inadvertently deepens dependence on it. To break this cycle, we propose Structural Skeleton-guided Reasoning (SSR), a two-phase approach that first generates an answer-invariant functional skeleton structure, then uses this skeleton to guide full trace generation. By redirecting the information flow to structural planning rather than answer monitoring, SSR consistently reduces anchoring across all three levels. We further introduce Distilled SSR (SSR-D), which fine-tunes models on teacher-generated SSR traces to ensure reliable structural adherence. Experiments across open-ended reasoning benchmarks demonstrate that SSR-D achieves up to 10% improvement over suppression baselines while preserving out-of-distribution (OOD) generalization.",
        "url": "http://arxiv.org/abs/2602.14469v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14469v1",
        "arxiv_id": "2602.14469v1",
        "authors": [
            "Guangyue Peng",
            "Zongchao Chen",
            "Wen Luo",
            "Yuntao Wen",
            "Wei Li",
            "Ruixiang Feng",
            "Ran Le",
            "Chen Yang",
            "Zhenwei An",
            "Yang Song",
            "Tao Zhang",
            "Houfeng Wang"
        ],
        "submitted": "2026-02-16 05:13:06",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Reverse Chain-of-Thought Generation and post-hoc rationalization in NLP, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the topic is not directly aligned with the user's primary research interests in IR and Search technologies, and the connection to user behavior modeling is tenuous at best."
    },
    {
        "title": "MixFormer: Co-Scaling Up Dense and Sequence in Industrial Recommenders",
        "abstract": "As industrial recommender systems enter a scaling-driven regime, Transformer architectures have become increasingly attractive for scaling models towards larger capacity and longer sequence. However, existing Transformer-based recommendation models remain structurally fragmented, where sequence modeling and feature interaction are implemented as separate modules with independent parameterization. Such designs introduce a fundamental co-scaling challenge, as model capacity must be suboptimally allocated between dense feature interaction and sequence modeling under a limited computational budget. In this work, we propose MixFormer, a unified Transformer-style architecture tailored for recommender systems, which jointly models sequential behaviors and feature interactions within a single backbone. Through a unified parameterization, MixFormer enables effective co-scaling across both dense capacity and sequence length, mitigating the trade-off observed in decoupled designs. Moreover, the integrated architecture facilitates deep interaction between sequential and non-sequential representations, allowing high-order feature semantics to directly inform sequence aggregation and enhancing overall expressiveness. To ensure industrial practicality, we further introduce a user-item decoupling strategy for efficiency optimizations that significantly reduce redundant computation and inference latency. Extensive experiments on large-scale industrial datasets demonstrate that MixFormer consistently exhibits superior accuracy and efficiency. Furthermore, large-scale online A/B tests on two production recommender systems, Douyin and Douyin Lite, show consistent improvements in user engagement metrics, including active days and in-app usage duration.",
        "url": "http://arxiv.org/abs/2602.14110v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14110v1",
        "arxiv_id": "2602.14110v1",
        "authors": [
            "Xu Huang",
            "Hao Zhang",
            "Zhifang Fan",
            "Yunwen Huang",
            "Zhuoxing Wei",
            "Zheng Chai",
            "Jinan Ni",
            "Yuchao Zheng",
            "Qiwei Chen"
        ],
        "submitted": "2026-02-15 11:53:30",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on recommender systems and proposes a unified Transformer-style architecture, MixFormer, to jointly model sequential behaviors and feature interactions. While it touches on scalability and efficiency, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's relevance to information retrieval and search technologies is limited, but it may be of interest to those working on related areas of recommender systems."
    },
    {
        "title": "Annotation-Efficient Vision-Language Model Adaptation to the Polish Language Using the LLaVA Framework",
        "abstract": "Most vision-language models (VLMs) are trained on English-centric data, limiting their performance in other languages and cultural contexts. This restricts their usability for non-English-speaking users and hinders the development of multimodal systems that reflect diverse linguistic and cultural realities. In this work, we reproduce and adapt the LLaVA-Next methodology to create a set of Polish VLMs. We rely on a fully automated pipeline for translating and filtering existing multimodal datasets, and complement this with synthetic Polish data for OCR and culturally specific tasks. Despite relying almost entirely on automatic translation and minimal manual intervention to the training data, our approach yields strong results: we observe a +9.5% improvement over LLaVA-1.6-Vicuna-13B on a Polish-adapted MMBench, along with higher-quality captions in generative evaluations, as measured by human annotators in terms of linguistic correctness. These findings highlight that large-scale automated translation, combined with lightweight filtering, can effectively bootstrap high-quality multimodal models for low-resource languages. Some challenges remain, particularly in cultural coverage and evaluation. To facilitate further research, we make our models and evaluation dataset publicly available.",
        "url": "http://arxiv.org/abs/2602.14073v1",
        "pdf_url": "https://arxiv.org/pdf/2602.14073v1",
        "arxiv_id": "2602.14073v1",
        "authors": [
            "Grzegorz Statkiewicz",
            "Alicja Dobrzeniecka",
            "Karolina Seweryn",
            "Aleksandra Krasnodębska",
            "Karolina Piosek",
            "Katarzyna Bogusz",
            "Sebastian Cygert",
            "Wojciech Kusa"
        ],
        "submitted": "2026-02-15 09:54:40",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves multimodal models, the focus is on vision-language models for low-resource languages, which is not a central match to your core research themes."
    }
]