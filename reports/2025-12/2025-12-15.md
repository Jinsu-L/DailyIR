# Daily Papers Report - 2025-12-15

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Intelligent Scientific Literature Explorer using Machine Learning (ISLE)

- **LLM Score**: 9
- **Keyword Score**: 16
- **Authors**: Sina Jani, Arman Heidari, Amirmohammad Anvari, Zahra Rahimi
- **URL**: <http://arxiv.org/abs/2512.12760v1>
- **Submitted**: 2025-12-14 16:54:24
- **Comment**: 18 pages, 7 figures, 3 tables
- **Topic Keywords**: semantic search, query, queries, relevance, retrieval, rank, search
- **Reason**: This paper aligns closely with your research interests in Information Retrieval, particularly in the areas of query understanding, ranking models, and user behavior modeling. The paper's focus on hybrid retrieval, semantic topic modeling, and knowledge graph construction demonstrates a deep semantic understanding, which is a key aspect of your research. The application of the system to scientific literature exploration also aligns with your experience in the e-commerce domain.

#### Abstract
> The rapid acceleration of scientific publishing has created substantial challenges for researchers attempting to discover, contextualize, and interpret relevant literature. Traditional keyword-based search systems provide limited semantic understanding, while existing AI-driven tools typically focus on isolated tasks such as retrieval, clustering, or bibliometric visualization. This paper presents an integrated system for scientific literature exploration that combines large-scale data acquisition, hybrid retrieval, semantic topic modeling, and heterogeneous knowledge graph construction. The system builds a comprehensive corpus by merging full-text data from arXiv with structured metadata from OpenAlex. A hybrid retrieval architecture fuses BM25 lexical search with embedding-based semantic search using Reciprocal Rank Fusion. Topic modeling is performed on retrieved results using BERTopic or non-negative matrix factorization depending on computational resources. A knowledge graph unifies papers, authors, institutions, countries, and extracted topics into an interpretable structure. The system provides a multi-layered exploration environment that reveals not only relevant publications but also the conceptual and relational landscape surrounding a query. Evaluation across multiple queries demonstrates improvements in retrieval relevance, topic coherence, and interpretability. The proposed framework contributes an extensible foundation for AI-assisted scientific discovery.

---

### 2. Breaking the Curse of Dimensionality: On the Stability of Modern Vector Retrieval

- **LLM Score**: 8
- **Keyword Score**: 9
- **Authors**: Vihan Lakshman, Blaise Munyampirwa, Julian Shun, Benjamin Coleman
- **URL**: <http://arxiv.org/abs/2512.12458v1>
- **Submitted**: 2025-12-13 21:05:21
- **Comment**: 27 pages
- **Topic Keywords**: query, rag, retrieval, web search, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of vector retrieval and stability theory. The focus on efficient nearest-neighbor search and the extension of stability theory to practical retrieval settings aligns with your interests in query understanding and ranking models. However, the specific domain of vector databases and neural embeddings is not a primary focus of your research, which is why the score is not a perfect 10.

#### Abstract
> Modern vector databases enable efficient retrieval over high-dimensional neural embeddings, powering applications from web search to retrieval-augmented generation. However, classical theory predicts such tasks should suffer from the curse of dimensionality, where distances between points become nearly indistinguishable, thereby crippling efficient nearest-neighbor search. We revisit this paradox through the lens of stability, the property that small perturbations to a query do not radically alter its nearest neighbors. Building on foundational results, we extend stability theory to three key retrieval settings widely used in practice: (i) multi-vector search, where we prove that the popular Chamfer distance metric preserves single-vector stability, while average pooling aggregation may destroy it; (ii) filtered vector search, where we show that sufficiently large penalties for mismatched filters can induce stability even when the underlying search is unstable; and (iii) sparse vector search, where we formalize and prove novel sufficient stability conditions. Across synthetic and real datasets, our experimental results match our theoretical predictions, offering concrete guidance for model and system design to avoid the curse of dimensionality.

---

### 3. SCIR: A Self-Correcting Iterative Refinement Framework for Enhanced Information Extraction Based on Schema

- **LLM Score**: 8
- **Keyword Score**: 2
- **Authors**: Yushen Fang, Jianjun Li, Mingqian Ding, Chang Liu, Xinchi Zou, Wenqi Yang
- **URL**: <http://arxiv.org/abs/2512.12337v1>
- **Submitted**: 2025-12-13 14:07:25
- **Topic Keywords**: rag
- **Reason**: This paper is highly relevant to Information Retrieval, specifically in the area of information extraction, which aligns with your research interests. The proposed SCIR framework and MBSC dataset demonstrate advancements in IE systems, showcasing improved accuracy and reduced training costs. While the focus is on NLP, the connection to IR is clear, making this paper a useful read.

#### Abstract
> Although Large language Model (LLM)-powered information extraction (IE) systems have shown impressive capabilities, current fine-tuning paradigms face two major limitations: high training costs and difficulties in aligning with LLM preferences. To address these issues, we propose a novel universal IE paradigm, the Self-Correcting Iterative Refinement (SCIR) framework, along with a Multi-task Bilingual (Chinese-English) Self-Correcting (MBSC) dataset containing over 100,000 entries. The SCIR framework achieves plug-and-play compatibility with existing LLMs and IE systems through its Dual-Path Self-Correcting module and feedback-driven optimization, thereby significantly reducing training costs. Concurrently, the MBSC dataset tackles the challenge of preference alignment by indirectly distilling GPT-4's capabilities into IE result detection models. Experimental results demonstrate that SCIR outperforms state-of-the-art IE methods across three key tasks: named entity recognition, relation extraction, and event extraction, achieving a 5.27 percent average improvement in span-based Micro-F1 while reducing training costs by 87 percent compared to baseline approaches. These advancements not only enhance the flexibility and accuracy of IE systems but also pave the way for lightweight and efficient IE paradigms.

---

### 4. Fine-Tuning Causal LLMs for Text Classification: Embedding-Based vs. Instruction-Based Approaches

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Amirhossein Yousefiramandi, Ciaran Cooney
- **URL**: <http://arxiv.org/abs/2512.12677v1>
- **Submitted**: 2025-12-14 13:02:06
- **Comment**: 18 pages, 6 figures
- **Topic Keywords**: rag, rank
- **Reason**: The paper explores fine-tuning strategies for Large Language Models (LLMs) in text classification tasks, which is somewhat related to your interests in Natural Language Processing (NLP) and related topics. However, the focus on LLMs and fine-tuning techniques is not directly aligned with your primary research interests in Information Retrieval (IR) and Search technologies. The paper's emphasis on efficient fine-tuning methods and model quantization also does not directly relate to your interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> We explore efficient strategies to fine-tune decoder-only Large Language Models (LLMs) for downstream text classification under resource constraints. Two approaches are investigated: (1) attaching a classification head to a pre-trained causal LLM and fine-tuning on the task (using the LLM's final token embedding as a sequence representation), and (2) instruction-tuning the LLM in a prompt->response format for classification. To enable single-GPU fine-tuning of models up to 8B parameters, we combine 4-bit model quantization with Low-Rank Adaptation (LoRA) for parameter-efficient training. Experiments on two datasets - a proprietary single-label dataset and the public WIPO-Alpha patent dataset (extreme multi-label classification) - show that the embedding-based method significantly outperforms the instruction-tuned method in F1-score, and is very competitive with - even surpassing - fine-tuned domain-specific models (e.g. BERT) on the same tasks. These results demonstrate that directly leveraging the internal representations of causal LLMs, along with efficient fine-tuning techniques, yields impressive classification performance under limited computational resources. We discuss the advantages of each approach while outlining practical guidelines and future directions for optimizing LLM fine-tuning in classification scenarios.

---

### 5. Not All Transparency Is Equal: Source Presentation Effects on Attention, Interaction, and Persuasion in Conversational Search

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Jiangen He, Jiqun Liu
- **URL**: <http://arxiv.org/abs/2512.12207v1>
- **Submitted**: 2025-12-13 06:39:45
- **Comment**: CHIIR 2026
- **Topic Keywords**: click, search
- **Reason**: This paper explores conversational search systems and user engagement, but its focus on source presentation effects and user experience is somewhat tangential to the core themes of query understanding, ranking models, and user behavior modeling in Information Retrieval. While it touches on user behavior, the study's design and findings are more aligned with Human-Computer Interaction and User Experience research than traditional IR or NLP.

#### Abstract
> Conversational search systems increasingly provide source citations, yet how citation or source presentation formats influence user engagement remains unclear. We conducted a crowdsourcing user experiment with 394 participants comparing four source presentation designs that varied citation visibility and accessibility: collapsible lists, hover cards, footer lists, and aligned sidebars.High-visibility interfaces generated substantially more hovering on sources, though clicking remained infrequent across all conditions. While interface design showed limited effects on user experience and perception measures, it significantly influenced knowledge, interest, and agreement changes. High-visibility interfaces initially reduced knowledge gain and interest, but these positive effects emerged with increasing source usage. The sidebar condition uniquely increased agreement change. Our findings demonstrate that source presentation alone may not enhance engagement and can even reduce it when insufficient sources are provided.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Human-Inspired Learning for Large Language Models via Obvious Record and Maximum-Entropy Method Discovery

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Hong Su
- **URL**: <http://arxiv.org/abs/2512.12608v1>
- **Submitted**: 2025-12-14 09:12:09
- **Topic Keywords**: rag
- **Reason**: This paper focuses on improving Large Language Models (LLMs) through a human-inspired learning framework. While it touches on aspects of deep semantic understanding, its primary contribution is in method discovery and learning from rare experiences, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the paper's main themes and applications are not directly aligned with the user's core research interests in IR and Search technologies.

#### Abstract
> Large Language Models (LLMs) excel at extracting common patterns from large-scale corpora, yet they struggle with rare, low-resource, or previously unseen scenarios-such as niche hardware deployment issues or irregular IoT device behaviors-because such cases are sparsely represented in training data. Moreover, LLMs rely primarily on implicit parametric memory, which limits their ability to explicitly acquire, recall, and refine methods, causing them to behave predominantly as intuition-driven predictors rather than deliberate, method-oriented learners. Inspired by how humans learn from rare experiences, this paper proposes a human-inspired learning framework that integrates two complementary mechanisms. The first, Obvious Record, explicitly stores cause--result (or question--solution) relationships as symbolic memory, enabling persistent learning even from single or infrequent encounters. The second, Maximum-Entropy Method Discovery, prioritizes and preserves methods with high semantic dissimilarity, allowing the system to capture diverse and underrepresented strategies that are typically overlooked by next-token prediction. Verification on a benchmark of 60 semantically diverse question--solution pairs demonstrates that the proposed entropy-guided approach achieves stronger coverage of unseen questions and significantly greater internal diversity than a random baseline, confirming its effectiveness in discovering more generalizable and human-inspired methods.

### 7. Semantic Distance Measurement based on Multi-Kernel Gaussian Processes

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Yinzhu Cheng, Haihua Xie, Yaqing Wang, Miao He, Mingming Sun
- **URL**: <http://arxiv.org/abs/2512.12238v1>
- **Submitted**: 2025-12-13 08:34:00
- **Topic Keywords**: retrieval
- **Reason**: The paper explores a semantic distance measure based on multi-kernel Gaussian processes, which is related to natural language processing and computational linguistics. However, it does not directly focus on information retrieval, query understanding, or ranking models, which are core areas of interest. The connection to text retrieval and classification is a secondary aspect, making it somewhat relevant but not a central match.

#### Abstract
> Semantic distance measurement is a fundamental problem in computational linguistics, providing a quantitative characterization of similarity or relatedness between text segments, and underpinning tasks such as text retrieval and text classification. From a mathematical perspective, a semantic distance can be viewed as a metric defined on a space of texts or on a representation space derived from them. However, most classical semantic distance methods are essentially fixed, making them difficult to adapt to specific data distributions and task requirements. In this paper, a semantic distance measure based on multi-kernel Gaussian processes (MK-GP) was proposed. The latent semantic function associated with texts was modeled as a Gaussian process, with its covariance function given by a combined kernel combining Mat√©rn and polynomial components. The kernel parameters were learned automatically from data under supervision, rather than being hand-crafted. This semantic distance was instantiated and evaluated in the context of fine-grained sentiment classification with large language models under an in-context learning (ICL) setup. The experimental results demonstrated the effectiveness of the proposed measure.

### 8. Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Samarth Sarin, Lovepreet Singh, Bhaskarjit Sarmah, Dhagash Mehta
- **URL**: <http://arxiv.org/abs/2512.12686v1>
- **Submitted**: 2025-12-14 13:38:06
- **Comment**: Paper accepted at 5th International Conference of AIML Systems 2025, Bangalore, India
- **Topic Keywords**: personalization
- **Reason**: The paper Memoria: A Scalable Agentic Memory Framework for Personalized Conversational AI is somewhat related to the user's interests in Natural Language Processing (NLP) and conversational AI, but it does not directly align with the user's primary focus on Information Retrieval (IR) and query understanding. The paper's emphasis on agentic memory and user modeling is relevant to the user's background in e-commerce, but it does not explore ranking models or click models.

#### Abstract
> Agentic memory is emerging as a key enabler for large language models (LLM) to maintain continuity, personalization, and long-term context in extended user interactions, critical capabilities for deploying LLMs as truly interactive and adaptive agents. Agentic memory refers to the memory that provides an LLM with agent-like persistence: the ability to retain and act upon information across conversations, similar to how a human would. We present Memoria, a modular memory framework that augments LLM-based conversational systems with persistent, interpretable, and context-rich memory. Memoria integrates two complementary components: dynamic session-level summarization and a weighted knowledge graph (KG)-based user modelling engine that incrementally captures user traits, preferences, and behavioral patterns as structured entities and relationships. This hybrid architecture enables both short-term dialogue coherence and long-term personalization while operating within the token constraints of modern LLMs. We demonstrate how Memoria enables scalable, personalized conversational artificial intelligence (AI) by bridging the gap between stateless LLM interfaces and agentic memory systems, offering a practical solution for industry applications requiring adaptive and evolving user experiences.

### 9. Understanding Syllogistic Reasoning in LLMs from Formal and Natural Language Perspectives

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Aheli Poddar, Saptarshi Sahoo, Sujata Ghosh
- **URL**: <http://arxiv.org/abs/2512.12620v1>
- **Submitted**: 2025-12-14 09:50:10
- **Comment**: 9 pages, 4 figures, 5 tables. Submitted to AAAI 2026 Bridge Program on Logic & AI. Code available at https://github.com/XAheli/Logic-in-LLMs
- **Topic Keywords**: search
- **Reason**: This paper explores syllogistic reasoning in Large Language Models (LLMs) from a logical and natural language perspective, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on formal reasoning mechanisms and symbolic inferences is not directly aligned with the user's primary research interests in IR and Search technologies. The connection to NLP is present, but the abstract does not explicitly mention relevance optimization or user behavior modeling.

#### Abstract
> We study syllogistic reasoning in LLMs from the logical and natural language perspectives. In process, we explore fundamental reasoning capabilities of the LLMs and the direction this research is moving forward. To aid in our studies, we use 14 large language models and investigate their syllogistic reasoning capabilities in terms of symbolic inferences as well as natural language understanding. Even though this reasoning mechanism is not a uniform emergent property across LLMs, the perfect symbolic performances in certain models make us wonder whether LLMs are becoming more and more formal reasoning mechanisms, rather than making explicit the nuances of human reasoning.

### 10. Diffusion Language Model Inference with Monte Carlo Tree Search

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Zheng Huang, Kiran Ramnath, Yueyan Chen, Aosong Feng, Sangmin Woo, Balasubramaniam Srinivasan, Zhichao Xu, Kang Zhou, Shuai Wang, Haibo Ding, Lin Lee Cheong
- **URL**: <http://arxiv.org/abs/2512.12168v1>
- **Submitted**: 2025-12-13 04:30:02
- **Topic Keywords**: search
- **Reason**: The paper focuses on improving inference in diffusion language models using Monte Carlo Tree Search, which is a novel application of search technologies. However, it does not directly relate to the user's core research themes in Information Retrieval, query understanding, ranking models, or user behavior modeling. While it touches on search technologies, its relevance is somewhat tangential to the user's primary interests.

#### Abstract
> Diffusion language models (DLMs) have recently emerged as a compelling alternative to autoregressive generation, offering parallel generation and improved global coherence. During inference, DLMs generate text by iteratively denoising masked sequences in parallel; however, determining which positions to unmask and which tokens to commit forms a large combinatorial search problem. Existing inference methods approximate this search using heuristics, which often yield suboptimal decoding paths; other approaches instead rely on additional training to guide token selection. To introduce a principled search mechanism for DLMs inference, we introduce MEDAL, a framework that integrates Monte Carlo Tree SEarch initialization for Diffusion LAnguage Model inference. We employ Monte Carlo Tree Search at the initialization stage to explore promising unmasking trajectories, providing a robust starting point for subsequent refinement. This integration is enabled by restricting the search space to high-confidence actions and prioritizing token choices that improve model confidence over remaining masked positions. Across multiple benchmarks, MEDAL achieves up to 22.0% improvement over existing inference strategies, establishing a new paradigm for search-based inference in diffusion language models.

### 11. Benchmarking Contextual Understanding for In-Car Conversational Systems

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Philipp Habicht, Lev Sorokin, Abdullah Saydemir, Ken E. Friedl, Andrea Stocco
- **URL**: <http://arxiv.org/abs/2512.12042v1>
- **Submitted**: 2025-12-12 21:15:49
- **Topic Keywords**: recommend
- **Reason**: This paper explores contextual understanding in conversational systems, which is related to query understanding and user behavior modeling in Information Retrieval. However, the focus on conversational systems and Large Language Models (LLMs) is somewhat distant from the user's core research themes in IR and Search technologies, despite some overlap with NLP and data mining.

#### Abstract
> In-Car Conversational Question Answering (ConvQA) systems significantly enhance user experience by enabling seamless voice interactions. However, assessing their accuracy and reliability remains a challenge. This paper explores the use of Large Language Models (LLMs) alongside advanced prompting techniques and agent-based methods to evaluate the extent to which ConvQA system responses adhere to user utterances. The focus lies on contextual understanding and the ability to provide accurate venue recommendations considering user constraints and situational context. To evaluate utterance-response coherence using an LLM, we synthetically generate user utterances accompanied by correct and modified failure-containing system responses. We use input-output, chain-of-thought, self-consistency prompting, and multi-agent prompting techniques with 13 reasoning and non-reasoning LLMs of varying sizes and providers, including OpenAI, DeepSeek, Mistral AI, and Meta. We evaluate our approach on a case study involving restaurant recommendations. The most substantial improvements occur for small non-reasoning models when applying advanced prompting techniques, particularly multi-agent prompting. However, reasoning models consistently outperform non-reasoning models, with the best performance achieved using single-agent prompting with self-consistency. Notably, DeepSeek-R1 reaches an F1-score of 0.99 at a cost of 0.002 USD per request. Overall, the best balance between effectiveness and cost-time efficiency is reached with the non-reasoning model DeepSeek-V3. Our findings show that LLM-based evaluation offers a scalable and accurate alternative to traditional human evaluation for benchmarking contextual understanding in ConvQA systems.

### 12. Patch-wise Retrieval: A Bag of Practical Techniques for Instance-level Matching

- **LLM Score**: 2
- **Keyword Score**: 12
- **Authors**: Wonseok Choi, Sohwi Lim, Nam Hyeon-Woo, Moon Ye-Bin, Dong-Ju Jeong, Jinyoung Hwang, Tae-Hyun Oh
- **URL**: <http://arxiv.org/abs/2512.12610v1>
- **Submitted**: 2025-12-14 09:24:51
- **Comment**: WACV 2026
- **Topic Keywords**: query, ranking, rerank, retrieval, rank
- **Reason**: This paper focuses on instance-level image retrieval, which is not directly related to the user's primary research interests in Information Retrieval and Search technologies. While it does involve retrieval and ranking, the context is image retrieval and the techniques proposed are specific to this domain, lacking the deep semantic understanding and real-time relevance optimization aspects that are central to the user's interests.

#### Abstract
> Instance-level image retrieval aims to find images containing the same object as a given query, despite variations in size, position, or appearance. To address this challenging task, we propose Patchify, a simple yet effective patch-wise retrieval framework that offers high performance, scalability, and interpretability without requiring fine-tuning. Patchify divides each database image into a small number of structured patches and performs retrieval by comparing these local features with a global query descriptor, enabling accurate and spatially grounded matching. To assess not just retrieval accuracy but also spatial correctness, we introduce LocScore, a localization-aware metric that quantifies whether the retrieved region aligns with the target object. This makes LocScore a valuable diagnostic tool for understanding and improving retrieval behavior. We conduct extensive experiments across multiple benchmarks, backbones, and region selection strategies, showing that Patchify outperforms global methods and complements state-of-the-art reranking pipelines. Furthermore, we apply Product Quantization for efficient large-scale retrieval and highlight the importance of using informative features during compression, which significantly boosts performance. Project website: https://wons20k.github.io/PatchwiseRetrieval/

### 13. FloodSQL-Bench: A Retrieval-Augmented Benchmark for Geospatially-Grounded Text-to-SQL

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Hanzhou Liu, Kai Yin, Zhitong Chen, Chenyue Liu, Ali Mostafavi
- **URL**: <http://arxiv.org/abs/2512.12084v1>
- **Submitted**: 2025-12-12 23:25:00
- **Topic Keywords**: queries, retrieval, search
- **Reason**: This paper is not directly related to the user's core research themes in Information Retrieval, Search technologies, or Natural Language Processing. While it involves Text-to-SQL and large language models, its focus on geospatial reasoning and the flood management domain is not aligned with the user's interests.

#### Abstract
> Existing Text-to-SQL benchmarks primarily focus on single-table queries or limited joins in general-purpose domains, and thus fail to reflect the complexity of domain-specific, multi-table and geospatial reasoning, To address this limitation, we introduce FLOODSQL-BENCH, a geospatially grounded benchmark for the flood management domain that integrates heterogeneous datasets through key-based, spatial, and hybrid joins. The benchmark captures realistic flood-related information needs by combining social, infrastructural, and hazard data layers. We systematically evaluate recent large language models with the same retrieval-augmented generation settings and measure their performance across difficulty tiers. By providing a unified, open benchmark grounded in real-world disaster management data, FLOODSQL-BENCH establishes a practical testbed for advancing Text-to-SQL research in high-stakes application domains.

### 14. FuXi-$Œ≥$: Efficient Sequential Recommendation with Exponential-Power Temporal Encoder and Diagonal-Sparse Positional Mechanism

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Dezhi Yi, Wei Guo, Wenyang Cui, Wenxuan He, Huifeng Guo, Yong Liu, Zhenhua Dong, Ye Lu
- **URL**: <http://arxiv.org/abs/2512.12740v1>
- **Submitted**: 2025-12-14 15:38:14
- **Comment**: Accepted by KDD 2026
- **Topic Keywords**: rag, recommend
- **Reason**: This paper focuses on sequential recommendation, which is somewhat related to information retrieval, but it primarily deals with recommender systems and does not address query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user.

#### Abstract
> Sequential recommendation aims to model users' evolving preferences based on their historical interactions. Recent advances leverage Transformer-based architectures to capture global dependencies, but existing methods often suffer from high computational overhead, primarily due to discontinuous memory access in temporal encoding and dense attention over long sequences. To address these limitations, we propose FuXi-$Œ≥$, a novel sequential recommendation framework that improves both effectiveness and efficiency through principled architectural design. FuXi-$Œ≥$ adopts a decoder-only Transformer structure and introduces two key innovations: (1) An exponential-power temporal encoder that encodes relative temporal intervals using a tunable exponential decay function inspired by the Ebbinghaus forgetting curve. This encoder enables flexible modeling of both short-term and long-term preferences while maintaining high efficiency through continuous memory access and pure matrix operations. (2) A diagonal-sparse positional mechanism that prunes low-contribution attention blocks using a diagonal-sliding strategy guided by the persymmetry of Toeplitz matrix. Extensive experiments on four real-world datasets demonstrate that FuXi-$Œ≥$ achieves state-of-the-art performance in recommendation quality, while accelerating training by up to 4.74$\times$ and inference by up to 6.18$\times$, making it a practical and scalable solution for long-sequence recommendation. Our code is available at https://github.com/Yeedzhi/FuXi-gamma.

### 15. Efficient Vision-Language Reasoning via Adaptive Token Pruning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Xue Li, Xiaonan Song, Henry Hu
- **URL**: <http://arxiv.org/abs/2512.12701v1>
- **Submitted**: 2025-12-14 14:11:32
- **Comment**: 10 pages, 3 figures. Expanded version of an extended abstract accepted at NeurIPS 2025 Workshop on VLM4RWD. Presents methodology and preliminary experimental results
- **Topic Keywords**: relevance
- **Reason**: This paper focuses on Vision-Language Models and introduces a dynamic inference mechanism for efficient processing. While it touches on real-time relevance optimization, it is primarily concerned with computer vision and multimodal edge computing, which is not a central match to your research interests in Information Retrieval and Search technologies.

#### Abstract
> Real-world deployment of Vision-Language Models (VLMs) is hindered by high computational demands, as existing architectures inefficiently process all tokens uniformly. We introduce Adaptive Token Pruning (ATP), a dynamic inference mechanism that retains only the most informative tokens based on contextual relevance. ATP operates at the vision-language interface, assigning a hybrid importance score combining ViT CLS attention (intra-modal saliency) and CLIP text-image similarity (inter-modal relevance) to keep top-K tokens for the LLM. Unlike static compression, ATP adapts to each input without modifying the backbone. Proposed as a lightweight gating module, ATP is compatible with popular backbones like BLIP-2, LLaVA, and Flamingo. Preliminary evaluations across VQAv2, GQA, and COCO indicate that ATP reduces inference FLOPs by around 40% and achieves roughly 1.5x speedups in end-to-end latency with negligible accuracy loss (less than 1%). Qualitative analyses suggest ATP preserves visual grounding and enhances interpretability. Beyond efficiency, we investigate robustness under corruptions; observations suggest adaptive pruning suppresses spurious correlations, improving stability. These findings imply that resource-constrained inference and model reliability are not competing objectives. Finally, we discuss ATP's role in efficient multimodal edge computing pipelines.

### 16. Can GPT replace human raters? Validity and reliability of machine-generated norms for metaphors

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Veronica Mangiaterra, Hamad Al-Azary, Chiara Barattieri di San Pietro, Paolo Canal, Valentina Bambini
- **URL**: <http://arxiv.org/abs/2512.12444v1>
- **Submitted**: 2025-12-13 19:56:31
- **Comment**: 30 pages, 5 figures
- **Topic Keywords**: ctr, search
- **Reason**: This paper is not directly related to Information Retrieval or Search technologies, but rather focuses on the application of Large Language Models in psycholinguistics. While it explores the validity and reliability of machine-generated ratings, it does not address query understanding, ranking models, or user behavior modeling, which are core areas of interest for your research.

#### Abstract
> As Large Language Models (LLMs) are increasingly being used in scientific research, the issue of their trustworthiness becomes crucial. In psycholinguistics, LLMs have been recently employed in automatically augmenting human-rated datasets, with promising results obtained by generating ratings for single words. Yet, performance for ratings of complex items, i.e., metaphors, is still unexplored. Here, we present the first assessment of the validity and reliability of ratings of metaphors on familiarity, comprehensibility, and imageability, generated by three GPT models for a total of 687 items gathered from the Italian Figurative Archive and three English studies. We performed a thorough validation in terms of both alignment with human data and ability to predict behavioral and electrophysiological responses. We found that machine-generated ratings positively correlated with human-generated ones. Familiarity ratings reached moderate-to-strong correlations for both English and Italian metaphors, although correlations weakened for metaphors with high sensorimotor load. Imageability showed moderate correlations in English and moderate-to-strong in Italian. Comprehensibility for English metaphors exhibited the strongest correlations. Overall, larger models outperformed smaller ones and greater human-model misalignment emerged with familiarity and imageability. Machine-generated ratings significantly predicted response times and the EEG amplitude, with a strength comparable to human ratings. Moreover, GPT ratings obtained across independent sessions were highly stable. We conclude that GPT, especially larger models, can validly and reliably replace - or augment - human subjects in rating metaphor properties. Yet, LLMs align worse with humans when dealing with conventionality and multimodal aspects of metaphorical meaning, calling for careful consideration of the nature of stimuli.

### 17. Persistent Personas? Role-Playing, Instruction Following, and Safety in Extended Interactions

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Pedro Henrique Luz de Araujo, Michael A. Hedderich, Ali Modarressi, Hinrich Schuetze, Benjamin Roth
- **URL**: <http://arxiv.org/abs/2512.12775v1>
- **Submitted**: 2025-12-14 17:27:02
- **Comment**: 31 pages, 35 figures
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to your core research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves large language models and dialogue-conditioned benchmarks, the focus is on persona applications and their fragility in extended interactions, which is not a central match to your research themes.

#### Abstract
> Persona-assigned large language models (LLMs) are used in domains such as education, healthcare, and sociodemographic simulation. Yet, they are typically evaluated only in short, single-round settings that do not reflect real-world usage. We introduce an evaluation protocol that combines long persona dialogues (over 100 rounds) and evaluation datasets to create dialogue-conditioned benchmarks that can robustly measure long-context effects. We then investigate the effects of dialogue length on persona fidelity, instruction-following, and safety of seven state-of-the-art open- and closed-weight LLMs. We find that persona fidelity degrades over the course of dialogues, especially in goal-oriented conversations, where models must sustain both persona fidelity and instruction following. We identify a trade-off between fidelity and instruction following, with non-persona baselines initially outperforming persona-assigned models; as dialogues progress and fidelity fades, persona responses become increasingly similar to baseline responses. Our findings highlight the fragility of persona applications in extended interactions and our work provides a protocol to systematically measure such failures.

### 18. NL2Repo-Bench: Towards Long-Horizon Repository Generation Evaluation of Coding Agents

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jingzhe Ding, Shengda Long, Changxin Pu, Huan Zhou, Hongwan Gao, Xiang Gao, Chao He, Yue Hou, Fei Hu, Zhaojian Li, Weiran Shi, Zaiyuan Wang, Daoguang Zan, Chenchen Zhang, Xiaoxu Zhang, Qizhi Chen, Xianfu Cheng, Bo Deng, Qingshui Gu, Kai Hua, Juntao Lin, Pai Liu, Mingchen Li, Xuanguang Pan, Zifan Peng, Yujia Qin, Yong Shan, Zhewen Tan, Weihao Xie, Zihan Wang, Yishuo Yuan, Jiayu Zhang, Enduo Zhao, Yunfei Zhao, He Zhu, Chenyang Zou, Ming Ding, Jianpeng Jiao, Jiaheng Liu, Minghao Liu, Qian Liu, Chongyao Tao, Jian Yang, Tong Yang, Zhaoxiang Zhang, Xinjie Chen, Wenhao Huang, Ge Zhang
- **URL**: <http://arxiv.org/abs/2512.12730v1>
- **Submitted**: 2025-12-14 15:12:13
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The focus is on evaluating coding agents for autonomous software development, which is outside your primary areas of interest.

#### Abstract
> Recent advances in coding agents suggest rapid progress toward autonomous software development, yet existing benchmarks fail to rigorously evaluate the long-horizon capabilities required to build complete software systems. Most prior evaluations focus on localized code generation, scaffolded completion, or short-term repair tasks, leaving open the question of whether agents can sustain coherent reasoning, planning, and execution over the extended horizons demanded by real-world repository construction. To address this gap, we present NL2Repo Bench, a benchmark explicitly designed to evaluate the long-horizon repository generation ability of coding agents. Given only a single natural-language requirements document and an empty workspace, agents must autonomously design the architecture, manage dependencies, implement multi-module logic, and produce a fully installable Python library. Our experiments across state-of-the-art open- and closed-source models reveal that long-horizon repository generation remains largely unsolved: even the strongest agents achieve below 40% average test pass rates and rarely complete an entire repository correctly. Detailed analysis uncovers fundamental long-horizon failure modes, including premature termination, loss of global coherence, fragile cross-file dependencies, and inadequate planning over hundreds of interaction steps. NL2Repo Bench establishes a rigorous, verifiable testbed for measuring sustained agentic competence and highlights long-horizon reasoning as a central bottleneck for the next generation of autonomous coding agents.

### 19. WebOperator: Action-Aware Tree Search for Autonomous Agents in Web Environment

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Mahir Labib Dihan, Tanzima Hashem, Mohammed Eunus Ali, Md Rizwan Parvez
- **URL**: <http://arxiv.org/abs/2512.12692v1>
- **Submitted**: 2025-12-14 13:56:54
- **Comment**: Under review at ICLR 2026. Project page: https://kagnlp.github.io/WebOperator/
- **Topic Keywords**: rank, search
- **Reason**: This paper focuses on developing a tree-search framework for autonomous agents in web environments, addressing challenges related to backtracking and irreversible actions. While it involves search technologies, the context and application are quite different from information retrieval and query understanding, which are core areas of your research interests.

#### Abstract
> LLM-based agents often operate in a greedy, step-by-step manner, selecting actions solely based on the current observation without considering long-term consequences or alternative paths. This lack of foresight is particularly problematic in web environments, which are only partially observable-limited to browser-visible content (e.g., DOM and UI elements)-where a single misstep often requires complex and brittle navigation to undo. Without an explicit backtracking mechanism, agents struggle to correct errors or systematically explore alternative paths. Tree-search methods provide a principled framework for such structured exploration, but existing approaches lack mechanisms for safe backtracking, making them prone to unintended side effects. They also assume that all actions are reversible, ignoring the presence of irreversible actions-limitations that reduce their effectiveness in realistic web tasks. To address these challenges, we introduce WebOperator, a tree-search framework that enables reliable backtracking and strategic exploration. Our method incorporates a best-first search strategy that ranks actions by both reward estimates and safety considerations, along with a robust backtracking mechanism that verifies the feasibility of previously visited paths before replaying them, preventing unintended side effects. To further guide exploration, WebOperator generates action candidates from multiple, varied reasoning contexts to ensure diverse and robust exploration, and subsequently curates a high-quality action set by filtering out invalid actions pre-execution and merging semantically equivalent ones. Experimental results on WebArena and WebVoyager demonstrate the effectiveness of WebOperator. On WebArena, WebOperator achieves a state-of-the-art 54.6% success rate with gpt-4o, underscoring the critical advantage of integrating strategic foresight with safe execution.

### 20. Theoretical Foundations of Prompt Engineering: From Heuristics to Expressivity

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Dongseok Kim, Hyoungsun Choi, Mohamed Jismy Aashik Rasool, Gisung Oh
- **URL**: <http://arxiv.org/abs/2512.12688v1>
- **Submitted**: 2025-12-14 13:42:20
- **Comment**: 24 pages
- **Topic Keywords**: rag
- **Reason**: This paper focuses on prompt engineering for Transformers, which is a topic in Natural Language Processing (NLP), but it does not directly relate to Information Retrieval (IR), query understanding, ranking models, or user behavior modeling, which are core areas of your research interests.

#### Abstract
> Prompts can switch a model's behavior even when the weights are fixed, yet this phenomenon is rarely treated as a clean theoretical object rather than a heuristic. We study the family of functions obtainable by holding a Transformer backbone fixed as an executor and varying only the prompt. Our core idea is to view the prompt as an externally injected program and to construct a simplified Transformer that interprets it to implement different computations. The construction exposes a mechanism-level decomposition: attention performs selective routing from prompt memory, the FFN performs local arithmetic conditioned on retrieved fragments, and depth-wise stacking composes these local updates into a multi-step computation. Under this viewpoint, we prove a constructive existential result showing that a single fixed backbone can approximate a broad class of target behaviors via prompts alone. The framework provides a unified starting point for formalizing trade-offs under prompt length/precision constraints and for studying structural limits of prompt-based switching, while remaining distinct from empirical claims about pretrained LLMs.

### 21. StruProKGR: A Structural and Probabilistic Framework for Sparse Knowledge Graph Reasoning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yucan Guo, Saiping Guan, Miao Su, Zeya Zhao, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng
- **URL**: <http://arxiv.org/abs/2512.12613v1>
- **Submitted**: 2025-12-14 09:36:58
- **Topic Keywords**: rag
- **Reason**: This paper focuses on sparse knowledge graph reasoning, which is not directly related to your core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves graph-based reasoning, the topic is more aligned with data mining and knowledge representation, which are tangential to your primary interests.

#### Abstract
> Sparse Knowledge Graphs (KGs) are commonly encountered in real-world applications, where knowledge is often incomplete or limited. Sparse KG reasoning, the task of inferring missing knowledge over sparse KGs, is inherently challenging due to the scarcity of knowledge and the difficulty of capturing relational patterns in sparse scenarios. Among all sparse KG reasoning methods, path-based ones have attracted plenty of attention due to their interpretability. Existing path-based methods typically rely on computationally intensive random walks to collect paths, producing paths of variable quality. Additionally, these methods fail to leverage the structured nature of graphs by treating paths independently. To address these shortcomings, we propose a Structural and Probabilistic framework named StruProKGR, tailored for efficient and interpretable reasoning on sparse KGs. StruProKGR utilizes a distance-guided path collection mechanism to significantly reduce computational costs while exploring more relevant paths. It further enhances the reasoning process by incorporating structural information through probabilistic path aggregation, which prioritizes paths that reinforce each other. Extensive experiments on five sparse KG reasoning benchmarks reveal that StruProKGR surpasses existing path-based methods in both effectiveness and efficiency, providing an effective, efficient, and interpretable solution for sparse KG reasoning.

### 22. Adaptive Detector-Verifier Framework for Zero-Shot Polyp Detection in Open-World Settings

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Shengkai Xu, Hsiang Lun Kao, Tianxiang Xu, Honghui Zhang, Junqiao Wang, Runmeng Ding, Guanyu Liu, Tianyu Shi, Zhenyu Yu, Guofeng Pan, Ziqian Bi, Yuqi Ouyang
- **URL**: <http://arxiv.org/abs/2512.12492v1>
- **Submitted**: 2025-12-13 23:33:05
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. It focuses on computer vision and medical image analysis, specifically polyp detection in endoscopy, which is outside your areas of expertise.

#### Abstract
> Polyp detectors trained on clean datasets often underperform in real-world endoscopy, where illumination changes, motion blur, and occlusions degrade image quality. Existing approaches struggle with the domain gap between controlled laboratory conditions and clinical practice, where adverse imaging conditions are prevalent. In this work, we propose AdaptiveDetector, a novel two-stage detector-verifier framework comprising a YOLOv11 detector with a vision-language model (VLM) verifier. The detector adaptively adjusts per-frame confidence thresholds under VLM guidance, while the verifier is fine-tuned with Group Relative Policy Optimization (GRPO) using an asymmetric, cost-sensitive reward function specifically designed to discourage missed detections -- a critical clinical requirement. To enable realistic assessment under challenging conditions, we construct a comprehensive synthetic testbed by systematically degrading clean datasets with adverse conditions commonly encountered in clinical practice, providing a rigorous benchmark for zero-shot evaluation. Extensive zero-shot evaluation on synthetically degraded CVC-ClinicDB and Kvasir-SEG images demonstrates that our approach improves recall by 14 to 22 percentage points over YOLO alone, while precision remains within 0.7 points below to 1.7 points above the baseline. This combination of adaptive thresholding and cost-sensitive reinforcement learning achieves clinically aligned, open-world polyp detection with substantially fewer false negatives, thereby reducing the risk of missed precancerous polyps and improving patient outcomes.

### 23. From Human Intention to Action Prediction: A Comprehensive Benchmark for Intention-driven End-to-End Autonomous Driving

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Huan Zheng, Yucheng Zhou, Tianyi Yan, Jiayi Su, Hongjun Chen, Dubing Chen, Wencheng Han, Runzhou Tao, Zhongying Qiu, Jianfei Yang, Jianbing Shen
- **URL**: <http://arxiv.org/abs/2512.12302v1>
- **Submitted**: 2025-12-13 11:59:51
- **Topic Keywords**: ctr
- **Reason**: This paper is not relevant to your research interests as it focuses on autonomous driving and does not relate to information retrieval, search technologies, or natural language processing. The paper's emphasis on understanding human intentions and translating them into driving actions is not directly applicable to your areas of expertise.

#### Abstract
> Current end-to-end autonomous driving systems operate at a level of intelligence akin to following simple steering commands. However, achieving genuinely intelligent autonomy requires a paradigm shift: moving from merely executing low-level instructions to understanding and fulfilling high-level, abstract human intentions. This leap from a command-follower to an intention-fulfiller, as illustrated in our conceptual framework, is hindered by a fundamental challenge: the absence of a standardized benchmark to measure and drive progress on this complex task. To address this critical gap, we introduce Intention-Drive, the first comprehensive benchmark designed to evaluate the ability to translate high-level human intent into safe and precise driving actions. Intention-Drive features two core contributions: (1) a new dataset of complex scenarios paired with corresponding natural language intentions, and (2) a novel evaluation protocol centered on the Intent Success Rate (ISR), which assesses the semantic fulfillment of the human's goal beyond simple geometric accuracy. Through an extensive evaluation of a spectrum of baseline models on Intention-Drive, we reveal a significant performance deficit, showing that the baseline model struggle to achieve the comprehensive scene and intention understanding required for this advanced task.

### 24. Market-Bench: Evaluating Large Language Models on Introductory Quantitative Trading and Market Dynamics

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Abhay Srivastava, Sam Jung, Spencer Mateega
- **URL**: <http://arxiv.org/abs/2512.12264v1>
- **Submitted**: 2025-12-13 10:07:31
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests as it focuses on evaluating large language models for quantitative trading tasks, which is outside your primary focus on information retrieval, especially in areas requiring deep semantic understanding and real-time relevance optimization.

#### Abstract
> We introduce MARKET-BENCH, a benchmark that evaluates large language models (LLMs) on introductory quantitative trading tasks by asking them to construct executable backtesters from natural-language strategy descriptions and market assumptions. Each instance specifies one of three canonical strategies -- scheduled trading on Microsoft (NASDAQ: MSFT), pairs trading on Coca-Cola (NASDAQ: KO) and Pepsi (NASDAQ: PEP), or delta hedging on MSFT -- and models must produce code whose P\&L, drawdown, and position paths match a verifiable reference implementation. We assess twelve state-of-the-art models using a multi-round pass@k metric that separates structural reliability (whether the backtest runs) from numerical accuracy (mean absolute error of the backtest metrics). While most models reliably execute the simplest strategy (average pass@3 of 0.80), errors vary by orders of magnitude across models and tasks: Gemini 3 Pro and Claude 4.5 Sonnet combine strong reliability with low error on simpler strategies, GPT-5.1 Codex-Max achieves perfect pass@1 on the first two strategies and the lowest best-run error on the easiest task, and Qwen3 Max attains perfect pass@3 yet sometimes produces inaccurate P\&L paths. These results show that current LLMs can scaffold basic trading infrastructure but still struggle to reason robustly about prices, inventory, and risk; we release MARKET-BENCH and a public leaderboard at https://marketbench.ai.

### 25. Adversarially Probing Cross-Family Sound Symbolism in 27 Languages

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Anika Sharma, Tianyi Niu, Emma Wrenn, Shashank Srivastava
- **URL**: <http://arxiv.org/abs/2512.12245v1>
- **Submitted**: 2025-12-13 09:06:50
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves computational analysis and linguistic data, its focus on sound symbolism and cross-linguistic analysis of size semantics does not align with your areas of expertise.

#### Abstract
> The phenomenon of sound symbolism, the non-arbitrary mapping between word sounds and meanings, has long been demonstrated through anecdotal experiments like Bouba Kiki, but rarely tested at scale. We present the first computational cross-linguistic analysis of sound symbolism in the semantic domain of size. We compile a typologically broad dataset of 810 adjectives (27 languages, 30 words each), each phonemically transcribed and validated with native-speaker audio. Using interpretable classifiers over bag-of-segment features, we find that phonological form predicts size semantics above chance even across unrelated languages, with both vowels and consonants contributing. To probe universality beyond genealogy, we train an adversarial scrubber that suppresses language identity while preserving size signal (also at family granularity). Language prediction averaged across languages and settings falls below chance while size prediction remains significantly above chance, indicating cross-family sound-symbolic bias. We release data, code, and diagnostic tools for future large-scale studies of iconicity.

### 26. State over Tokens: Characterizing the Role of Reasoning Tokens

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Mosh Levy, Zohar Elyoseph, Shauli Ravfogel, Yoav Goldberg
- **URL**: <http://arxiv.org/abs/2512.12777v1>
- **Submitted**: 2025-12-14 17:30:34
- **Topic Keywords**: search
- **Reason**: This paper focuses on Large Language Models and their reasoning processes, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the paper's primary focus is on the internal workings of LLMs, rather than user behavior modeling or search technologies, making it only loosely relevant to your research interests.

#### Abstract
> Large Language Models (LLMs) can generate reasoning tokens before their final answer to boost performance on complex tasks. While these sequences seem like human thought processes, empirical evidence reveals that they are not a faithful explanation of the model's actual reasoning process. To address this gap between appearance and function, we introduce the State over Tokens (SoT) conceptual framework. SoT reframes reasoning tokens not as a linguistic narrative, but as an externalized computational state -- the sole persistent information carrier across the model's stateless generation cycles. This explains how the tokens can drive correct reasoning without being a faithful explanation when read as text and surfaces previously overlooked research questions on these tokens. We argue that to truly understand the process that LLMs do, research must move beyond reading the reasoning tokens as text and focus on decoding them as state.

### 27. The Morphemic Origin of Zipf's Law: A Factorized Combinatorial Framework

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Vladimir Berman
- **URL**: <http://arxiv.org/abs/2512.12394v1>
- **Submitted**: 2025-12-13 16:58:06
- **Topic Keywords**: rank
- **Reason**: This paper appears to be unrelated to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus on morphemic origin and combinatorial frameworks for word formation does not align with your areas of expertise.

#### Abstract
> We present a simple structure based model of how words are formed from morphemes. The model explains two major empirical facts: the typical distribution of word lengths and the appearance of Zipf like rank frequency curves. In contrast to classical explanations based on random text or communication efficiency, our approach uses only the combinatorial organization of prefixes, roots, suffixes and inflections. In this Morphemic Combinatorial Word Model, a word is created by activating several positional slots. Each slot turns on with a certain probability and selects one morpheme from its inventory. Morphemes are treated as stable building blocks that regularly appear in word formation and have characteristic positions. This mechanism produces realistic word length patterns with a concentrated middle zone and a thin long tail, closely matching real languages. Simulations with synthetic morpheme inventories also generate rank frequency curves with Zipf like exponents around 1.1-1.4, similar to English, Russian and Romance languages. The key result is that Zipf like behavior can emerge without meaning, communication pressure or optimization principles. The internal structure of morphology alone, combined with probabilistic activation of slots, is sufficient to create the robust statistical patterns observed across languages.

### 28. The Instability of Safety: How Random Seeds and Temperature Expose Inconsistent LLM Refusal Behavior

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Erik Larsen
- **URL**: <http://arxiv.org/abs/2512.12066v1>
- **Submitted**: 2025-12-12 22:29:13
- **Comment**: 14 pages, 7 figures, 6 tables. Code and data available at https://github.com/erikl2/safety-refusal-stability
- **Topic Keywords**: recommend
- **Reason**: This paper focuses on the stability of safety refusal decisions in large language models, which is a topic related to NLP. However, it does not directly align with the user's primary research interests in Information Retrieval, query understanding, ranking models, or user behavior modeling.

#### Abstract
> Current safety evaluations of large language models rely on single-shot testing, implicitly assuming that model responses are deterministic and representative of the model's safety alignment. We challenge this assumption by investigating the stability of safety refusal decisions across random seeds and temperature settings. Testing four instruction-tuned models from three families (Llama 3.1 8B, Qwen 2.5 7B, Qwen 3 8B, Gemma 3 12B) on 876 harmful prompts across 20 different sampling configurations (4 temperatures x 5 random seeds), we find that 18-28% of prompts exhibit decision flips--the model refuses in some configurations but complies in others--depending on the model. Our Safety Stability Index (SSI) reveals that higher temperatures significantly reduce decision stability (Friedman chi-squared = 44.71, p < 0.001), with mean SSI dropping from 0.951 at temperature 0.0 to 0.896 at temperature 1.0. We validate our findings across all model families using Claude 3.5 Haiku as a unified external judge, achieving 89.0% inter-judge agreement with our primary Llama 70B judge (Cohen's kappa = 0.62). These findings demonstrate that single-shot safety evaluations are insufficient for reliable safety assessment. We show that single-shot evaluation agrees with multi-sample ground truth only 92.4% of the time, and recommend using at least 3 samples per prompt for reliable safety assessment.

### 29. Semantic-Drive: Democratizing Long-Tail Data Curation via Open-Vocabulary Grounding and Neuro-Symbolic VLM Consensus

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Antonio Guillen-Perez
- **URL**: <http://arxiv.org/abs/2512.12012v1>
- **Submitted**: 2025-12-12 20:07:04
- **Topic Keywords**: search
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves data mining and a novel framework for semantic data mining, its focus on Autonomous Vehicles and Long-Tail data curation is not aligned with your core research themes.

#### Abstract
> The development of robust Autonomous Vehicles (AVs) is bottlenecked by the scarcity of "Long-Tail" training data. While fleets collect petabytes of video logs, identifying rare safety-critical events (e.g., erratic jaywalking, construction diversions) remains a manual, cost-prohibitive process. Existing solutions rely on coarse metadata search, which lacks precision, or cloud-based VLMs, which are privacy-invasive and expensive. We introduce Semantic-Drive, a local-first, neuro-symbolic framework for semantic data mining. Our approach decouples perception into two stages: (1) Symbolic Grounding via a real-time open-vocabulary detector (YOLOE) to anchor attention, and (2) Cognitive Analysis via a Reasoning VLM that performs forensic scene analysis. To mitigate hallucination, we implement a "System 2" inference-time alignment strategy, utilizing a multi-model "Judge-Scout" consensus mechanism. Benchmarked on the nuScenes dataset against the Waymo Open Dataset (WOD-E2E) taxonomy, Semantic-Drive achieves a Recall of 0.966 (vs. 0.475 for CLIP) and reduces Risk Assessment Error by 40\% compared to single models. The system runs entirely on consumer hardware (NVIDIA RTX 3090), offering a privacy-preserving alternative to the cloud.

---

