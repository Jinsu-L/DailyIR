[
    {
        "title": "Agent-as-a-Graph: Knowledge Graph-Based Tool and Agent Retrieval for LLM Multi-Agent Systems",
        "abstract": "Recent advances in Large Language Model Multi-Agent Systems enable scalable orchestration and retrieval of specialized, parallelized subagents, each equipped with hundreds or thousands of Model Context Protocol (MCP) servers and tools. However, existing agent, MCP, and retrieval methods typically match queries against a single agent description, obscuring fine-grained tool capabilities of each agent, resulting in suboptimal agent selection. We introduce Agent-as-a-Graph retrieval, a knowledge graph retrieval augmented generation approach that represents both tools and their parent agents as nodes and edges in a knowledge graph. During retrieval, i) relevant agents and tool nodes are first retrieved through vector search, ii) we apply a type-specific weighted reciprocal rank fusion (wRRF) for reranking tools and agents, and iii) parent agents are traversed in the knowledge graph for the final set of agents. We evaluate Agent-as-a-Graph on the LiveMCPBenchmark, achieving 14.9% and 14.6% improvements in Recall@5 and nDCG@5 over prior state-of-the-art retrievers, and 2.4% improvements in wRRF optimizations.",
        "url": "http://arxiv.org/abs/2511.18194v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18194v1",
        "arxiv_id": "2511.18194v1",
        "authors": [
            "Faheem Nizar",
            "Elias Lumer",
            "Anmol Gulati",
            "Pradeep Honaganahalli Basavaraju",
            "Vamse Kumar Subbiah"
        ],
        "submitted": "2025-11-22 21:24:16",
        "source": "arxiv",
        "comment": null,
        "score": 18,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores knowledge graph-based retrieval for Large Language Model Multi-Agent Systems, which is somewhat related to information retrieval and search technologies. However, the focus on multi-agent systems and knowledge graph retrieval is not directly aligned with the user's core research themes in query understanding, ranking models, and user behavior modeling. The paper's relevance is also limited by its application in a specific domain (LLM Multi-Agent Systems), which is not the primary focus of the user's research."
    },
    {
        "title": "Rethinking Retrieval: From Traditional Retrieval Augmented Generation to Agentic and Non-Vector Reasoning Systems in the Financial Domain for Large Language Models",
        "abstract": "Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models to answer financial questions using external knowledge bases of U.S. SEC filings, earnings reports, and regulatory documents. However, existing work lacks systematic comparison of vector-based and non-vector RAG architectures for financial documents, and the empirical impact of advanced RAG techniques on retrieval accuracy, answer quality, latency, and cost remain unclear. We present the first systematic evaluation comparing vector-based agentic RAG using hybrid search and metadata filtering against hierarchical node-based systems that traverse document structure without embeddings. We evaluate two enhancement techniques applied to the vector-based architecture, i) cross-encoder reranking for retrieval precision, and ii) small-to-big chunk retrieval for context completeness. Across 1,200 SEC 10-K, 10-Q, and 8-K filings on a 150-question benchmark, we measure retrieval metrics (MRR, Recall@5), answer quality through LLM-as-a-judge pairwise comparisons, latency, and preprocessing costs. Vector-based agentic RAG achieves a 68% win rate over hierarchical node-based systems with comparable latency (5.2 compared to 5.98 seconds). Cross-encoder reranking achieves a 59% absolute improvement at optimal parameters (10, 5) for MRR@5. Small-to-big retrieval achieves a 65% win rate over baseline chunking with only 0.2 seconds additional latency. Our findings reveal that applying advanced RAG techniques to financial Q&A systems improves retrieval accuracy, answer quality, and has cost-performance tradeoffs to be considered in production.",
        "url": "http://arxiv.org/abs/2511.18177v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18177v1",
        "arxiv_id": "2511.18177v1",
        "authors": [
            "Elias Lumer",
            "Matt Melich",
            "Olivia Zino",
            "Elena Kim",
            "Sara Dieter",
            "Pradeep Honaganahalli Basavaraju",
            "Vamse Kumar Subbiah",
            "James A. Burke",
            "Roberto Hernandez"
        ],
        "submitted": "2025-11-22 20:06:25",
        "source": "arxiv",
        "comment": "8 pages, 2 figures",
        "score": 17,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'pairwise' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of Retrieval-Augmented Generation (RAG) and Large Language Models. The focus on financial domain and the evaluation of vector-based and non-vector RAG architectures aligns with your interests in query understanding and ranking models."
    },
    {
        "title": "Path-Constrained Retrieval: A Structural Approach to Reliable LLM Agent Reasoning Through Graph-Scoped Semantic Search",
        "abstract": "Large Language Model agents often retrieve context from knowledge bases that lack structural consistency with the agent's current reasoning state, leading to incoherent reasoning chains. We introduce Path-Constrained Retrieval (PCR), a retrieval method that combines structural graph constraints with semantic search to ensure retrieved information maintains logical relationships within a knowledge graph. PCR restricts the search space to nodes reachable from an anchor node, preventing retrieval of structurally disconnected information that may lead to inconsistent reasoning. We evaluate PCR on PathRAG-6, a benchmark spanning six domains with 180 nodes and 360 edges. Our results show that PCR achieves full structural consistency compared to 24-32 percent in baseline methods, while maintaining strong relevance scores. On the technology domain, PCR obtains full relevance at rank 10 with full structural consistency, significantly outperforming vector search and hybrid retrieval. PCR reduces the average graph distance of retrieved context by 78 percent compared to baselines, demonstrating retrieval of more structurally consistent information. These findings suggest that path-constrained retrieval is an effective approach for improving the reliability and coherence of LLM agent reasoning systems.",
        "url": "http://arxiv.org/abs/2511.18313v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18313v1",
        "arxiv_id": "2511.18313v1",
        "authors": [
            "Joseph Oladokun"
        ],
        "submitted": "2025-11-23 06:50:01",
        "source": "arxiv",
        "comment": "10 pages",
        "score": 12,
        "keyword_reasons": [
            "Found 'semantic search' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 9,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of query understanding and ranking models. The focus on structural consistency and semantic search aligns with your interests in deep semantic understanding and real-time relevance optimization. The application of this approach to Large Language Model agents also touches on your background in the e-commerce domain and related areas."
    },
    {
        "title": "Table Comprehension in Building Codes using Vision Language Models and Domain-Specific Fine-Tuning",
        "abstract": "Building codes contain critical information for ensuring safety, regulatory compliance, and informed decision-making in construction and engineering. Automated question answering systems over such codes enable quick and accurate access to specific regulatory clauses, improving efficiency and reducing errors. Retrieval-Augmented Generation (RAG) systems are essential for this task as they combine the precision of information retrieval with the generative capabilities of language models. However, tabular data are challenging to extract as they often involve complex layouts, merged cells, multi-row headers, and embedded semantic relationships that are not easily captured by traditional natural language processing techniques and Vision Language Models (VLMs). This paper explores and compares two methods for extracting information from tabular data in building codes using several pre-trained VLMs. First, a direct input method is used, where the image of the page is input directly into the VLMs, which are then tasked with answering questions based on the image. Second, an indirect input method is introduced, which involves converting an image of a page containing tables into the LaTeX code and then answering inquires based on the LaTeX-based input. The experiments find that the direct input method generally resulted in higher accuracy than the indirect input method. To further improve the performance, we fine-tuned each VLM using Low Rank Adaptation (LoRA) on a domain-specific tabular dataset. The fine-tuned models exhibited substantial improvements, with Qwen2.5-VL-3B-Instruct achieving relative accuracy gains exceeding 100%. Our results highlight the potential of parameter-efficient fine-tuning methods to adapt powerful VLMs for understanding complex structured data in specialized fields, such as building code interpretation and regulatory compliance.",
        "url": "http://arxiv.org/abs/2511.18306v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18306v1",
        "arxiv_id": "2511.18306v1",
        "authors": [
            "Mohammad Aqib",
            "Mohd Hamza",
            "Ying Hei Chui",
            "Qipei Mei"
        ],
        "submitted": "2025-11-23 06:34:51",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 5,
        "llm_reason": "The paper discusses Retrieval‑Augmented Generation, which involves IR techniques, but its primary focus is on vision‑language models for table comprehension in a niche domain (building codes). It does not address core IR concerns such as ranking, query understanding, or click‑model user behavior, so relevance is moderate."
    },
    {
        "title": "Save, Revisit, Retain: A Scalable Framework for Enhancing User Retention in Large-Scale Recommender Systems",
        "abstract": "User retention is a critical objective for online platforms like Pinterest, as it strengthens user loyalty and drives growth through repeated engagement. A key indicator of retention is revisitation, i.e., when users return to view previously saved content, a behavior often sparked by personalized recommendations and user satisfaction. However, modeling and optimizing revisitation poses significant challenges. One core difficulty is accurate attribution: it is often unclear which specific user actions or content exposures trigger a revisit, since many confounding factors (e.g., content quality, user interface, notifications, or even changing user intent) can influence return behavior. Additionally, the scale and timing of revisitations introduce further complexity; users may revisit content days or even weeks after their initial interaction, requiring the system to maintain and associate extensive historical records across millions of users and sessions. These complexities render existing methods insufficient for robustly capturing and optimizing long-term revisitation. To address these gaps, we introduce a novel, lightweight, and interpretable framework for modeling revisitation behavior and optimizing long-term user retention in Pinterest's search-based recommendation context. By defining a surrogate attribution process that links saves to subsequent revisitations, we reduce noise in the causal relationship between user actions and return visits. Our scalable event aggregation pipeline enables large-scale analysis of user revisitation patterns and enhances the ranking system's ability to surface items with high retention value. Deployed on Pinterest's Related Pins surface to serve 500+ million users, the framework led to a significant lift of 0.1% in active users without additional computational costs.",
        "url": "http://arxiv.org/abs/2511.18013v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18013v1",
        "arxiv_id": "2511.18013v1",
        "authors": [
            "Weijie Jiang",
            "Armando Ordorica",
            "Jaewon Yang",
            "Olafur Gudmundsson",
            "Yucheng Tu",
            "Huizhong Duan"
        ],
        "submitted": "2025-11-22 10:27:20",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'user action' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on recommender systems and user retention, which is somewhat related to the user's interests in Information Retrieval and Search technologies. However, the emphasis on attribution and event aggregation in the context of recommender systems is not directly aligned with the user's core research themes, particularly query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Multi-Agent Collaborative Filtering: Orchestrating Users and Items for Agentic Recommendations",
        "abstract": "Agentic recommendations cast recommenders as large language model (LLM) agents that can plan, reason, use tools, and interact with users of varying preferences in web applications. However, most existing agentic recommender systems focus on generic single-agent plan-execute workflows or multi-agent task decomposition pipelines. Without recommendation-oriented design, they often underuse the collaborative signals in the user-item interaction history, leading to unsatisfying recommendation results. To address this, we propose the Multi-Agent Collaborative Filtering (MACF) framework for agentic recommendations, drawing an analogy between traditional collaborative filtering algorithms and LLM-based multi-agent collaboration. Specifically, given a target user and query, we instantiate similar users and relevant items as LLM agents with unique profiles. Each agent is able to call retrieval tools, suggest candidate items, and interact with other agents. Different from the static preference aggregation in traditional collaborative filtering, MACF employs a central orchestrator agent to adaptively manage the collaboration between user and item agents via dynamic agent recruitment and personalized collaboration instruction. Experimental results on datasets from three different domains show the advantages of our MACF framework compared to strong agentic recommendation baselines.",
        "url": "http://arxiv.org/abs/2511.18413v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18413v1",
        "arxiv_id": "2511.18413v1",
        "authors": [
            "Yu Xia",
            "Sungchul Kim",
            "Tong Yu",
            "Ryan A. Rossi",
            "Julian McAuely"
        ],
        "submitted": "2025-11-23 11:57:10",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores recommender systems, specifically agentic recommendations, which is somewhat related to the user's interests in Information Retrieval and Search technologies. However, the focus on large language model agents and collaborative filtering is not directly aligned with the user's primary focus on query understanding, ranking models, and user behavior modeling. While the paper touches on user-item interaction history, it does not delve into the user's core areas of interest."
    },
    {
        "title": "From Archives to Decisions: Multi-Agent Pharmaceutical Co-Scientist for Traceable Drug Discovery and Reverse Translation",
        "abstract": "Pharmaceutical research and development has accumulated vast, heterogeneous archives of data. Much of this knowledge stems from discontinued programs, and reusing these archives is invaluable for reverse translation. However, in practice, such reuse is often infeasible. In this work, we introduce DiscoVerse, a multi-agent co-scientist designed to support pharmaceutical research and development. The system implements semantic retrieval, cross-document linking, and auditable synthesis on a large historical corpus from Roche. To validate our approach at real-world scale, we selected a subset of 180 molecules from the Roche research repositories, covering over 0.87 billion BPE tokens and more than four decades of research. Given that automated evaluation metrics are poorly aligned with scientific utility, we evaluate the performance of DiscoVerse using blinded expert evaluation of source-linked outputs. To our knowledge, this is the first agentic framework systematically assessed on real pharmaceutical data for reverse translation, enabled by authorized access to confidential, end-to-end drug-development archives. Our contributions include role-specialized agent designs aligned with scientist workflows; human-in-the-loop support for reverse translation; expert evaluation; and a large-scale demonstration showing promising answer accuracy and decision-making insights. In brief, across seven benchmark queries covering 180 molecules, DiscoVerse achieved near-perfect recall ($\\geq 0.99$) with moderate precision ($0.71-0.91$), while qualitative assessments of discontinuation rationale and organ-specific toxicity showed faithful, source-linked synthesis across preclinical and clinical evidence.",
        "url": "http://arxiv.org/abs/2511.18259v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18259v1",
        "arxiv_id": "2511.18259v1",
        "authors": [
            "Xiaochen Zheng",
            "Alvaro Serra",
            "Ilya Schneider Chernov",
            "Maddalena Marchesi",
            "Eunice Musvasva",
            "Tatyana Y. Doktorova"
        ],
        "submitted": "2025-11-23 03:17:26",
        "source": "arxiv",
        "comment": "22 pages, 4 figures, 3 tables",
        "score": 6,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to information retrieval, specifically semantic retrieval, but it is focused on a specific domain (pharmaceutical research) and does not directly address query understanding, ranking models, or user behavior modeling. While it involves a large-scale demonstration of a system's performance, the context and evaluation metrics are not directly aligned with the user's core research themes."
    },
    {
        "title": "Token-Controlled Re-ranking for Sequential Recommendation via LLMs",
        "abstract": "The widespread adoption of Large Language Models (LLMs) as re-rankers is shifting recommender systems towards a user-centric paradigm. However, a significant gap remains: current re-rankers often lack mechanisms for fine-grained user control. They struggle to balance inherent user preferences with multiple attribute-based constraints, often resorting to simplistic hard filtering that can excessively narrow the recommendation pool and yield suboptimal results. This limitation leaves users as passive recipients rather than active collaborators in the recommendation process. To bridge this gap, we propose COREC, a novel token-augmented re-ranking framework that incorporates specific user requirements in co-creating the recommendation outcome. COREC empowers users to steer re-ranking results with precise and flexible control via explicit, attribute-based signals. The framework learns to balance these commands against latent preferences, yielding rankings that adhere to user instructions without sacrificing personalization. Experiments show that COREC: (1) exceeds state-of-the-art baselines on standard recommendation effectiveness and (2) demonstrates superior adherence to specific attribute requirements, proving that COREC enables fine-grained and predictable manipulation of the rankings.",
        "url": "http://arxiv.org/abs/2511.17913v1",
        "pdf_url": "https://arxiv.org/pdf/2511.17913v1",
        "arxiv_id": "2511.17913v1",
        "authors": [
            "Wenxi Dai",
            "Wujiang Xu",
            "Pinhuan Wang",
            "Dimitris N. Metaxas"
        ],
        "submitted": "2025-11-22 04:31:19",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'recommend' (score: +1)",
            "Found 'personalization' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on recommender systems, specifically proposing a novel framework (COREC) for user-controlled re-ranking. While it leverages Large Language Models (LLMs), which is related to my interests in NLP, the primary focus on recommender systems and sequential recommendation is somewhat tangential to my core research themes in Information Retrieval and Search technologies."
    },
    {
        "title": "UFO: Unfair-to-Fair Evolving Mitigates Unfairness in LLM-based Recommender Systems via Self-Play Fine-tuning",
        "abstract": "Large language model-based Recommender Systems (LRSs) have demonstrated superior recommendation performance by integrating pre-training with Supervised Fine-Tuning (SFT). However, this approach introduces item-side unfairness. Existing studies primarily attribute this issue to the absence of fairness constraints during SFT and attempt to mitigate unfairness via re-weighting and re-ranking methods. In this paper, we find that unfairness arises not only from SFT but also from pre-training, where inherent biases are further amplified during SFT. This finding underscores the failure of current methods to address the root causes of unfairness. Moreover, current methods struggle to preserve satisfactory recommendation performance. To tackle these issues, we propose an Unfair-to-Fair evOlving (UFO) framework using a self-play mechanism, formulating unfairness mitigation as a two-player game. UFO alternates between two player roles: the \\textit{judger}, which identifies unfairness from both pre-training and SFT, and the \\textit{corrector}, which adjusts the LRS to address identified unfairness while preserving recommendation performance. Iterative optimization between these roles enables UFO to completely resolve unfairness. Extensive experiments demonstrate that UFO effectively mitigates unfairness while improving recommendation performance.",
        "url": "http://arxiv.org/abs/2511.18342v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18342v1",
        "arxiv_id": "2511.18342v1",
        "authors": [
            "Jiaming Zhang",
            "Yuyuan Li",
            "Xiaohua Feng",
            "Zhifei Ren",
            "Li Zhang",
            "Chaochao Chen"
        ],
        "submitted": "2025-11-23 08:34:30",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "While the paper touches on recommender systems, which is a related area to information retrieval, its focus on fairness in LLM-based recommender systems and the proposed framework (UFO) does not directly align with the user's primary interests in query understanding, ranking models, and user behavior modeling. The paper's emphasis on fairness and its application to recommender systems is somewhat relevant, but it does not represent a central match for the user's research themes."
    },
    {
        "title": "LLM Reasoning for Cold-Start Item Recommendation",
        "abstract": "Large Language Models (LLMs) have shown significant potential for improving recommendation systems through their inherent reasoning capabilities and extensive knowledge base. Yet, existing studies predominantly address warm-start scenarios with abundant user-item interaction data, leaving the more challenging cold-start scenarios, where sparse interactions hinder traditional collaborative filtering methods, underexplored. To address this limitation, we propose novel reasoning strategies designed for cold-start item recommendations within the Netflix domain. Our method utilizes the advanced reasoning capabilities of LLMs to effectively infer user preferences, particularly for newly introduced or rarely interacted items. We systematically evaluate supervised fine-tuning, reinforcement learning-based fine-tuning, and hybrid approaches that combine both methods to optimize recommendation performance. Extensive experiments on real-world data demonstrate significant improvements in both methodological efficacy and practical performance in cold-start recommendation contexts. Remarkably, our reasoning-based fine-tuned models outperform Netflix's production ranking model by up to 8% in certain cases.",
        "url": "http://arxiv.org/abs/2511.18261v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18261v1",
        "arxiv_id": "2511.18261v1",
        "authors": [
            "Shijun Li",
            "Yu Wang",
            "Jin Wang",
            "Ying Li",
            "Joydeep Ghosh",
            "Anne Cocos"
        ],
        "submitted": "2025-11-23 03:22:53",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores recommender systems, specifically addressing the cold-start problem, which is somewhat related to the user's interests in Information Retrieval and Search technologies. However, the focus on recommender systems and the use of Large Language Models (LLMs) for reasoning is not a central match for the user's primary research themes. The paper's emphasis on real-world data and practical performance is also somewhat relevant to the user's interests in e-commerce and data mining."
    },
    {
        "title": "Paper2SysArch: Structure-Constrained System Architecture Generation from Scientific Papers",
        "abstract": "The manual creation of system architecture diagrams for scientific papers is a time-consuming and subjective process, while existing generative models lack the necessary structural control and semantic understanding for this task. A primary obstacle hindering research and development in this domain has been the profound lack of a standardized benchmark to quantitatively evaluate the automated generation of diagrams from text. To address this critical gap, we introduce a novel and comprehensive benchmark, the first of its kind, designed to catalyze progress in automated scientific visualization. It consists of 3,000 research papers paired with their corresponding high-quality ground-truth diagrams and is accompanied by a three-tiered evaluation metric assessing semantic accuracy, layout coherence, and visual quality. Furthermore, to establish a strong baseline on this new benchmark, we propose Paper2SysArch, an end-to-end system that leverages multi-agent collaboration to convert papers into structured, editable diagrams. To validate its performance on complex cases, the system was evaluated on a manually curated and more challenging subset of these papers, where it achieves a composite score of 69.0. This work's principal contribution is the establishment of a large-scale, foundational benchmark to enable reproducible research and fair comparison. Meanwhile, our proposed system serves as a viable proof-of-concept, demonstrating a promising path forward for this complex task.",
        "url": "http://arxiv.org/abs/2511.18036v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18036v1",
        "arxiv_id": "2511.18036v1",
        "authors": [
            "Ziyi Guo",
            "Zhou Liu",
            "Wentao Zhang"
        ],
        "submitted": "2025-11-22 12:24:30",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval, Search technologies, or query understanding, which are the core areas of your research interests. While it involves text processing and generation, its focus is on scientific visualization and system architecture generation, which is a niche area outside your primary focus."
    },
    {
        "title": "PoETa v2: Toward More Robust Evaluation of Large Language Models in Portuguese",
        "abstract": "Large Language Models (LLMs) exhibit significant variations in performance across linguistic and cultural contexts, underscoring the need for systematic evaluation in diverse languages. In this work, we present the most extensive evaluation of LLMs for the Portuguese language to date. Leveraging our newly introduced PoETa v2 benchmark -- a comprehensive suite of over 40 tasks in Portuguese -- we assess more than 20 models covering a broad spectrum of training scales and computational resources. Our study reveals how computational investment and language-specific adaptation impact performance in Portuguese, while also analyzing performance gaps in comparison to equivalent tasks in English. Through this benchmark and analysis, PoETa v2 lays the groundwork for future research on Portuguese language modeling and evaluation. The benchmark is available at https://github.com/PoETaV2/PoETaV2.",
        "url": "http://arxiv.org/abs/2511.17808v1",
        "pdf_url": "https://arxiv.org/pdf/2511.17808v1",
        "arxiv_id": "2511.17808v1",
        "authors": [
            "Thales Sales Almeida",
            "Rodrigo Nogueira",
            "Hélio Pedrini"
        ],
        "submitted": "2025-11-21 22:01:51",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'ctr' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is primarily focused on evaluating Large Language Models in the Portuguese language, which is not directly related to your core research themes in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves NLP, the context and scope are quite different from your areas of interest."
    },
    {
        "title": "MindEval: Benchmarking Language Models on Multi-turn Mental Health Support",
        "abstract": "Demand for mental health support through AI chatbots is surging, though current systems present several limitations, like sycophancy or overvalidation, and reinforcement of maladaptive beliefs. A core obstacle to the creation of better systems is the scarcity of benchmarks that capture the complexity of real therapeutic interactions. Most existing benchmarks either only test clinical knowledge through multiple-choice questions or assess single responses in isolation. To bridge this gap, we present MindEval, a framework designed in collaboration with Ph.D-level Licensed Clinical Psychologists for automatically evaluating language models in realistic, multi-turn mental health therapy conversations. Through patient simulation and automatic evaluation with LLMs, our framework balances resistance to gaming with reproducibility via its fully automated, model-agnostic design. We begin by quantitatively validating the realism of our simulated patients against human-generated text and by demonstrating strong correlations between automatic and human expert judgments. Then, we evaluate 12 state-of-the-art LLMs and show that all models struggle, scoring below 4 out of 6, on average, with particular weaknesses in problematic AI-specific patterns of communication. Notably, reasoning capabilities and model scale do not guarantee better performance, and systems deteriorate with longer interactions or when supporting patients with severe symptoms. We release all code, prompts, and human evaluation data.",
        "url": "http://arxiv.org/abs/2511.18491v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18491v1",
        "arxiv_id": "2511.18491v1",
        "authors": [
            "José Pombal",
            "Maya D'Eon",
            "Nuno M. Guerreiro",
            "Pedro Henrique Martins",
            "António Farinhas",
            "Ricardo Rei"
        ],
        "submitted": "2025-11-23 15:19:29",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves language models and NLP, its focus is on mental health support and benchmarking, which is not a central match to your research themes."
    },
    {
        "title": "\"AGI\" team at SHROOM-CAP: Data-Centric Approach to Multilingual Hallucination Detection using XLM-RoBERTa",
        "abstract": "The detection of hallucinations in multilingual scientific text generated by Large Language Models (LLMs) presents significant challenges for reliable AI systems. This paper describes our submission to the SHROOM-CAP 2025 shared task on scientific hallucination detection across 9 languages. Unlike most approaches that focus primarily on model architecture, we adopted a data-centric strategy that addressed the critical issue of training data scarcity and imbalance. We unify and balance five existing datasets to create a comprehensive training corpus of 124,821 samples (50% correct, 50% hallucinated), representing a 172x increase over the original SHROOM training data. Our approach fine-tuned XLM-RoBERTa-Large with 560 million parameters on this enhanced dataset, achieves competitive performance across all languages, including \\textbf{2nd place in Gujarati} (zero-shot language) with Factuality F1 of 0.5107, and rankings between 4th-6th place across the remaining 8 languages. Our results demonstrate that systematic data curation can significantly outperform architectural innovations alone, particularly for low-resource languages in zero-shot settings.",
        "url": "http://arxiv.org/abs/2511.18301v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18301v1",
        "arxiv_id": "2511.18301v1",
        "authors": [
            "Harsh Rathva",
            "Pruthwik Mishra",
            "Shrikant Malviya"
        ],
        "submitted": "2025-11-23 05:48:27",
        "source": "arxiv",
        "comment": "Accepted to the 1st Workshop on Confabulation, Hallucinations & Overgeneration in Multilingual and Practical Settings (CHOMPS) at AACL-IJCNLP 2025",
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on hallucination detection in multilingual scientific text generated by Large Language Models, which is not directly related to the user's core research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling. While it involves NLP and data curation, the specific application and task are not aligned with the user's primary focus on deep semantic understanding and real-time relevance optimization in the e-commerce domain."
    },
    {
        "title": "Bias Is a Subspace, Not a Coordinate: A Geometric Rethinking of Post-hoc Debiasing in Vision-Language Models",
        "abstract": "Vision-Language Models (VLMs) have become indispensable for multimodal reasoning, yet their representations often encode and amplify demographic biases, resulting in biased associations and misaligned predictions in downstream tasks. Such behavior undermines fairness and distorts the intended alignment between vision and language. Recent post-hoc approaches attempt to mitigate bias by replacing the most attribute-correlated embedding coordinates with neutral values. However, our systematic analysis reveals three critical failures of this coordinate-wise approach: feature entanglement, poor cross-dataset generalization, and incomplete bias removal. We find that bias is not localized to a few coordinates but is instead distributed across a few linear subspaces. To address these limitations, we propose $\\textbf{S}$ubspace $\\textbf{P}$rojection $\\textbf{D}$ebiasing ($\\textbf{SPD}$), a geometrically principled framework that identifies and removes the entire subspace of linearly decodable bias while reinserting a neutral mean component to preserve semantic fidelity. Extensive experiments across zero-shot classification, text-to-image retrieval, and image generation validate the effectiveness of SPD: our method achieves more robust debiasing with an average improvement of $18.5\\%$ across four fairness metrics, while maintaining minimal loss in task performance compared to the best debiasing baseline.",
        "url": "http://arxiv.org/abs/2511.18123v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18123v1",
        "arxiv_id": "2511.18123v1",
        "authors": [
            "Dachuan Zhao",
            "Weiyue Li",
            "Zhenda Shen",
            "Yushu Qiu",
            "Bowen Xu",
            "Haoyu Chen",
            "Yongchao Chen"
        ],
        "submitted": "2025-11-22 17:04:30",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on debiasing in Vision-Language Models, which is a topic outside of the user's primary research interests in Information Retrieval and Search technologies. While it touches on related concepts like semantic understanding, the context is specific to computer vision and NLP, and the methods proposed are not directly applicable to the user's areas of expertise."
    },
    {
        "title": "Principled Context Engineering for RAG: Statistical Guarantees via Conformal Prediction",
        "abstract": "Retrieval-Augmented Generation (RAG) enhances factual grounding in large language models (LLMs) by incorporating retrieved evidence, but LLM accuracy declines when long or noisy contexts exceed the model's effective attention span. Existing pre-generation filters rely on heuristics or uncalibrated LLM confidence scores, offering no statistical control over retained evidence. We evaluate and demonstrate context engineering through conformal prediction, a coverage-controlled filtering framework that removes irrelevant content while preserving recall of supporting evidence. Using both embedding- and LLM-based scoring functions, we test this approach on the NeuCLIR and RAGTIME collections. Conformal filtering consistently meets its target coverage, ensuring that a specified fraction of relevant snippets are retained, and reduces retained context by 2-3x relative to unfiltered retrieval. On NeuCLIR, downstream factual accuracy measured by ARGUE F1 improves under strict filtering and remains stable at moderate coverage, indicating that most discarded material is redundant or irrelevant. These results demonstrate that conformal prediction enables reliable, coverage-controlled context reduction in RAG, offering a model-agnostic and principled approach to context engineering.",
        "url": "http://arxiv.org/abs/2511.17908v1",
        "pdf_url": "https://arxiv.org/pdf/2511.17908v1",
        "arxiv_id": "2511.17908v1",
        "authors": [
            "Debashish Chakraborty",
            "Eugene Yang",
            "Daniel Khashabi",
            "Dawn Lawrie",
            "Kevin Duh"
        ],
        "submitted": "2025-11-22 04:17:06",
        "source": "arxiv",
        "comment": "Preprint",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to Information Retrieval, specifically addressing context engineering for Retrieval-Augmented Generation (RAG), a topic closely related to query understanding and ranking models. The use of conformal prediction for statistical guarantees aligns with the user's interest in principled approaches to real-time relevance optimization. However, the focus on RAG and large language models is somewhat specific and may not be directly applicable to all areas of IR."
    },
    {
        "title": "General Agentic Memory Via Deep Research",
        "abstract": "Memory is critical for AI agents, yet the widely-adopted static memory, aiming to create readily available memory in advance, is inevitably subject to severe information loss. To address this limitation, we propose a novel framework called \\textbf{general agentic memory (GAM)}. GAM follows the principle of \"\\textbf{just-in time (JIT) compilation}\" where it focuses on creating optimized contexts for its client at runtime while keeping only simple but useful memory during the offline stage. To this end, GAM employs a duo-design with the following components. 1) \\textbf{Memorizer}, which highlights key historical information using a lightweight memory, while maintaining complete historical information within a universal page-store. 2) \\textbf{Researcher}, which retrieves and integrates useful information from the page-store for its online request guided by the pre-constructed memory. This design allows GAM to effectively leverage the agentic capabilities and test-time scalability of frontier large language models (LLMs), while also facilitating end-to-end performance optimization through reinforcement learning. In our experimental study, we demonstrate that GAM achieves substantial improvement on various memory-grounded task completion scenarios against existing memory systems.",
        "url": "http://arxiv.org/abs/2511.18423v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18423v1",
        "arxiv_id": "2511.18423v1",
        "authors": [
            "B. Y. Yan",
            "Chaofan Li",
            "Hongjin Qian",
            "Shuqi Lu",
            "Zheng Liu"
        ],
        "submitted": "2025-11-23 12:29:33",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on memory systems for AI agents, which is not directly related to the user's core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it mentions large language models, the context is not about query understanding, ranking models, or user behavior modeling, making it only loosely relevant to the user's interests."
    },
    {
        "title": "Findings of the BlackboxNLP 2025 Shared Task: Localizing Circuits and Causal Variables in Language Models",
        "abstract": "Mechanistic interpretability (MI) seeks to uncover how language models (LMs) implement specific behaviors, yet measuring progress in MI remains challenging. The recently released Mechanistic Interpretability Benchmark (MIB; Mueller et al., 2025) provides a standardized framework for evaluating circuit and causal variable localization. Building on this foundation, the BlackboxNLP 2025 Shared Task extends MIB into a community-wide reproducible comparison of MI techniques. The shared task features two tracks: circuit localization, which assesses methods that identify causally influential components and interactions driving model behavior, and causal variable localization, which evaluates approaches that map activations into interpretable features. With three teams spanning eight different methods, participants achieved notable gains in circuit localization using ensemble and regularization strategies for circuit discovery. With one team spanning two methods, participants achieved significant gains in causal variable localization using low-dimensional and non-linear projections to featurize activation vectors. The MIB leaderboard remains open; we encourage continued work in this standard evaluation framework to measure progress in MI research going forward.",
        "url": "http://arxiv.org/abs/2511.18409v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18409v1",
        "arxiv_id": "2511.18409v1",
        "authors": [
            "Dana Arad",
            "Yonatan Belinkov",
            "Hanjie Chen",
            "Najoung Kim",
            "Hosein Mohebbi",
            "Aaron Mueller",
            "Gabriele Sarti",
            "Martin Tutek"
        ],
        "submitted": "2025-11-23 11:33:59",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on mechanistic interpretability of language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on language models, the primary goal is to understand how they work, rather than improving their performance in search or retrieval tasks."
    },
    {
        "title": "A Multimodal Conversational Agent for Tabular Data Analysis",
        "abstract": "Large language models (LLMs) can reshape information processing by handling data analysis, visualization, and interpretation in an interactive, context-aware dialogue with users, including voice interaction, while maintaining high performance. In this article, we present Talk2Data, a multimodal LLM-driven conversational agent for intuitive data exploration. The system lets users query datasets with voice or text instructions and receive answers as plots, tables, statistics, or spoken explanations. Built on LLMs, the suggested design combines OpenAI Whisper automatic speech recognition (ASR) system, Qwen-coder code generation LLM/model, custom sandboxed execution tools, and Coqui library for text-to-speech (TTS) within an agentic orchestration loop. Unlike text-only analysis tools, it adapts responses across modalities and supports multi-turn dialogues grounded in dataset context. In an evaluation of 48 tasks on three datasets, our prototype achieved 95.8% accuracy with model-only generation time under 1.7 seconds (excluding ASR and execution time). A comparison across five LLM sizes (1.5B-32B) revealed accuracy-latency-cost trade-offs, with a 7B model providing the best balance for interactive use. By routing between conversation with user and code execution, constrained to a transparent sandbox, with simultaneously grounding prompts in schema-level context, the Talk2Data agent reliably retrieves actionable insights from tables while making computations verifiable. In the article, except for the Talk2Data agent itself, we discuss implications for human-data interaction, trust in LLM-driven analytics, and future extensions toward large-scale multimodal assistants.",
        "url": "http://arxiv.org/abs/2511.18405v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18405v1",
        "arxiv_id": "2511.18405v1",
        "authors": [
            "Mohammad Nour Al Awad",
            "Sergey Ivanov",
            "Olga Tikhonova",
            "Ivan Khodnenko"
        ],
        "submitted": "2025-11-23 11:21:04",
        "source": "arxiv",
        "comment": "\\c{opyright} 2025 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses",
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 1,
        "llm_reason": "The paper focuses on a multimodal conversational agent for tabular data analysis using LLMs, which is more about data exploration and human‑data interaction than information retrieval, query understanding, ranking, or click‑model research. It does not address search technologies or user behavior modeling, making it only marginally relevant to the user’s core interests."
    },
    {
        "title": "Toward an AI-Native Internet: Rethinking the Web Architecture for Semantic Retrieval",
        "abstract": "The rise of Generative AI Search is fundamentally transforming how users and intelligent systems interact with the Internet. LLMs increasingly act as intermediaries between humans and web information. Yet the web remains optimized for human browsing rather than AI-driven semantic retrieval, resulting in wasted network bandwidth, lower information quality, and unnecessary complexity for developers. We introduce the concept of an AI-Native Internet, a web architecture in which servers expose semantically relevant information chunks rather than full documents, supported by a Web-native semantic resolver that allows AI applications to discover relevant information sources before retrieving fine-grained chunks. Through motivational experiments, we quantify the inefficiencies of current HTML-based retrieval, and outline architectural directions and open challenges for evolving today's document-centric web into an AI-oriented substrate that better supports semantic access to web content.",
        "url": "http://arxiv.org/abs/2511.18354v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18354v1",
        "arxiv_id": "2511.18354v1",
        "authors": [
            "Muhammad Bilal",
            "Zafar Qazi",
            "Marco Canini"
        ],
        "submitted": "2025-11-23 09:01:22",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper aligns well with your interests in Information Retrieval, particularly in the context of semantic retrieval and real-time relevance optimization. The focus on AI-driven semantic retrieval and the introduction of an AI-Native Internet concept are relevant to your research themes. However, the emphasis on Generative AI Search and LLMs is not a central match, but still contributes to the paper's relevance."
    },
    {
        "title": "Time Matters: Enhancing Sequential Recommendations with Time-Guided Graph Neural ODEs",
        "abstract": "Sequential recommendation (SR) is widely deployed in e-commerce platforms, streaming services, etc., revealing significant potential to enhance user experience. However, existing methods often overlook two critical factors: irregular user interests between interactions and highly uneven item distributions over time. The former factor implies that actual user preferences are not always continuous, and long-term historical interactions may not be relevant to current purchasing behavior. Therefore, relying only on these historical interactions for recommendations may result in a lack of user interest at the target time. The latter factor, characterized by peaks and valleys in interaction frequency, may result from seasonal trends, special events, or promotions. These externally driven distributions may not align with individual user interests, leading to inaccurate recommendations. To address these deficiencies, we propose TGODE to both enhance and capture the long-term historical interactions. Specifically, we first construct a user time graph and item evolution graph, which utilize user personalized preferences and global item distribution information, respectively. To tackle the temporal sparsity caused by irregular user interactions, we design a time-guided diffusion generator to automatically obtain an augmented time-aware user graph. Additionally, we devise a user interest truncation factor to efficiently identify sparse time intervals and achieve balanced preference inference. After that, the augmented user graph and item graph are fed into a generalized graph neural ordinary differential equation (ODE) to align with the evolution of user preferences and item distributions. This allows two patterns of information evolution to be matched over time. Experimental results demonstrate that TGODE outperforms baseline methods across five datasets, with improvements ranging from 10% to 46%.",
        "url": "http://arxiv.org/abs/2511.18347v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18347v1",
        "arxiv_id": "2511.18347v1",
        "authors": [
            "Haoyan Fu",
            "Zhida Qin",
            "Shixiao Yang",
            "Haoyao Zhang",
            "Bin Lu",
            "Shuang Li",
            "Tianyu Huang",
            "John C. S. Lui"
        ],
        "submitted": "2025-11-23 08:46:27",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on sequential recommendations and graph neural networks, which are somewhat related to your interests in information retrieval and search technologies. However, the primary focus on recommender systems and sequential recommendations, rather than query understanding or ranking models, limits its relevance to your core research themes."
    },
    {
        "title": "Large Language Model Enhanced Graph Invariant Contrastive Learning for Out-of-Distribution Recommendation",
        "abstract": "Out-of-distribution (OOD) generalization has emerged as a significant challenge in graph recommender systems. Traditional graph neural network algorithms often fail because they learn spurious environmental correlations instead of stable causal relationships, leading to substantial performance degradation under distribution shifts. While recent advancements in Large Language Models (LLMs) offer a promising avenue due to their vast world knowledge and reasoning capabilities, effectively integrating this knowledge with the fine-grained topology of specific graphs to solve the OOD problem remains a significant challenge. To address these issues, we propose {$\\textbf{Inv}$ariant $\\textbf{G}$raph $\\textbf{C}$ontrastive Learning with $\\textbf{LLM}$s for Out-of-Distribution Recommendation (InvGCLLM)}, an innovative causal learning framework that synergistically integrates the strengths of data-driven models and knowledge-driven LLMs. Our framework first employs a data-driven invariant learning model to generate causal confidence scores for each user-item interaction. These scores then guide an LLM to perform targeted graph refinement, leveraging its world knowledge to prune spurious connections and augment missing causal links. Finally, the structurally purified graphs provide robust supervision for a causality-guided contrastive learning objective, enabling the model to learn representations that are resilient to spurious correlations. Experiments conducted on four public datasets demonstrate that InvGCLLM achieves significant improvements in out-of-distribution recommendation, consistently outperforming state-of-the-art baselines.",
        "url": "http://arxiv.org/abs/2511.18282v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18282v1",
        "arxiv_id": "2511.18282v1",
        "authors": [
            "Jiahao Liang",
            "Haoran Yang",
            "Xiangyu Zhao",
            "Zhiwen Yu",
            "Mianjie Li",
            "Chuan Shi",
            "Kaixiang Yang"
        ],
        "submitted": "2025-11-23 04:24:58",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval and Search technologies, particularly in the area of recommender systems. However, it focuses on graph-based recommender systems and out-of-distribution recommendation, which is not a central match to your primary focus on query understanding, ranking models, and user behavior modeling. The use of Large Language Models is also a relevant aspect, but it's not a key area of interest for you."
    },
    {
        "title": "Democratic Recommendation with User and Item Representatives Produced by Graph Condensation",
        "abstract": "The challenges associated with large-scale user-item interaction graphs have attracted increasing attention in graph-based recommendation systems, primarily due to computational inefficiencies and inadequate information propagation. Existing methods provide partial solutions but suffer from notable limitations: model-centric approaches, such as sampling and aggregation, often struggle with generalization, while data-centric techniques, including graph sparsification and coarsening, lead to information loss and ineffective handling of bipartite graph structures. Recent advances in graph condensation offer a promising direction by reducing graph size while preserving essential information, presenting a novel approach to mitigating these challenges. Inspired by the principles of democracy, we propose \\textbf{DemoRec}, a framework that leverages graph condensation to generate user and item representatives for recommendation tasks. By constructing a compact interaction graph and clustering nodes with shared characteristics from the original graph, DemoRec significantly reduces graph size and computational complexity. Furthermore, it mitigates the over-reliance on high-order information, a critical challenge in large-scale bipartite graphs. Extensive experiments conducted on four public datasets demonstrate the effectiveness of DemoRec, showcasing substantial improvements in recommendation performance, computational efficiency, and robustness compared to SOTA methods.",
        "url": "http://arxiv.org/abs/2511.18279v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18279v1",
        "arxiv_id": "2511.18279v1",
        "authors": [
            "Jiahao Liang",
            "Haoran Yang",
            "Xiangyu Zhao",
            "Zhiwen Yu",
            "Guandong Xu",
            "Wanyu Wang",
            "Kaixiang Yang"
        ],
        "submitted": "2025-11-23 04:09:28",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on graph-based recommendation systems, leveraging graph condensation to generate user and item representatives. While it touches on aspects of information retrieval, such as reducing graph size and computational complexity, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The connection to information retrieval is somewhat tenuous, making it only loosely relevant to the user's research interests."
    },
    {
        "title": "Measuring the Impact of Lexical Training Data Coverage on Hallucination Detection in Large Language Models",
        "abstract": "Hallucination in large language models (LLMs) is a fundamental challenge, particularly in open-domain question answering. Prior work attempts to detect hallucination with model-internal signals such as token-level entropy or generation consistency, while the connection between pretraining data exposure and hallucination is underexplored. Existing studies show that LLMs underperform on long-tail knowledge, i.e., the accuracy of the generated answer drops for the ground-truth entities that are rare in pretraining. However, examining whether data coverage itself can serve as a detection signal is overlooked. We propose a complementary question: Does lexical training-data coverage of the question and/or generated answer provide additional signal for hallucination detection? To investigate this, we construct scalable suffix arrays over RedPajama's 1.3-trillion-token pretraining corpus to retrieve $n$-gram statistics for both prompts and model generations. We evaluate their effectiveness for hallucination detection across three QA benchmarks. Our observations show that while occurrence-based features are weak predictors when used alone, they yield modest gains when combined with log-probabilities, particularly on datasets with higher intrinsic model uncertainty. These findings suggest that lexical coverage features provide a complementary signal for hallucination detection. All code and suffix-array infrastructure are provided at https://github.com/WWWonderer/ostd.",
        "url": "http://arxiv.org/abs/2511.17946v1",
        "pdf_url": "https://arxiv.org/pdf/2511.17946v1",
        "arxiv_id": "2511.17946v1",
        "authors": [
            "Shuo Zhang",
            "Fabrizio Gotti",
            "Fengran Mo",
            "Jian-Yun Nie"
        ],
        "submitted": "2025-11-22 06:59:55",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'www' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on large language models and hallucination detection, which is not directly related to your primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on NLP, the specific context and application are not aligned with your core themes."
    },
    {
        "title": "Computational frame analysis revisited: On LLMs for studying news coverage",
        "abstract": "Computational approaches have previously shown various promises and pitfalls when it comes to the reliable identification of media frames. Generative LLMs like GPT and Claude are increasingly being used as content analytical tools, but how effective are they for frame analysis? We address this question by systematically evaluating them against their computational predecessors: bag-of-words models and encoder-only transformers; and traditional manual coding procedures. Our analysis rests on a novel gold standard dataset that we inductively and iteratively developed through the study, investigating six months of news coverage of the US Mpox epidemic of 2022. While we discover some potential applications for generative LLMs, we demonstrate that they were consistently outperformed by manual coders, and in some instances, by smaller language models. Some form of human validation was always necessary to determine appropriate model choice. Additionally, by examining how the suitability of various approaches depended on the nature of different tasks that were part of our frame analytical workflow, we provide insights as to how researchers may leverage the complementarity of these approaches to use them in tandem. We conclude by endorsing a methodologically pluralistic approach and put forth a roadmap for computational frame analysis for researchers going forward.",
        "url": "http://arxiv.org/abs/2511.17746v1",
        "pdf_url": "https://arxiv.org/pdf/2511.17746v1",
        "arxiv_id": "2511.17746v1",
        "authors": [
            "Sharaj Kunjar",
            "Alyssa Hasegawa Smith",
            "Tyler R Mckenzie",
            "Rushali Mohbe",
            "Samuel V Scarpino",
            "Brooke Foucault Welles"
        ],
        "submitted": "2025-11-21 19:52:46",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your interests in Information Retrieval and Natural Language Processing, particularly in the context of deep semantic understanding. However, its focus on computational frame analysis and media frames is not directly aligned with your core research themes. The paper's use of LLMs and other NLP techniques is relevant, but the application domain is distinct from your e-commerce background and primary focus on search technologies."
    },
    {
        "title": "Gradient Masters at BLP-2025 Task 1: Advancing Low-Resource NLP for Bengali using Ensemble-Based Adversarial Training for Hate Speech Detection",
        "abstract": "This paper introduces the approach of \"Gradient Masters\" for BLP-2025 Task 1: \"Bangla Multitask Hate Speech Identification Shared Task\". We present an ensemble-based fine-tuning strategy for addressing subtasks 1A (hate-type classification) and 1B (target group classification) in YouTube comments. We propose a hybrid approach on a Bangla Language Model, which outperformed the baseline models and secured the 6th position in subtask 1A with a micro F1 score of 73.23% and the third position in subtask 1B with 73.28%. We conducted extensive experiments that evaluated the robustness of the model throughout the development and evaluation phases, including comparisons with other Language Model variants, to measure generalization in low-resource Bangla hate speech scenarios and data set coverage. In addition, we provide a detailed analysis of our findings, exploring misclassification patterns in the detection of hate speech.",
        "url": "http://arxiv.org/abs/2511.18324v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18324v1",
        "arxiv_id": "2511.18324v1",
        "authors": [
            "Syed Mohaiminul Hoque",
            "Naimur Rahman",
            "Md Sakhawat Hossain"
        ],
        "submitted": "2025-11-23 07:29:09",
        "source": "arxiv",
        "comment": "6 pages, 2 figures, 4 tables. Accepted at the Second International Workshop on Bangla Language Processing (BLP-2025) co-located with AACL-IJCNLP 2025. Ranked 6th (Subtask 1A, 73.23% micro F1) and 3rd (Subtask 1B, 73.28% micro F1) on the official leaderboard",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is primarily focused on Low-Resource NLP for Bengali, specifically hate speech detection, which is not a core area of interest for the user's research. While it involves ensemble-based adversarial training, a related topic in NLP, the context and application are quite different from the user's main focus on Information Retrieval and Search technologies."
    },
    {
        "title": "Extracting Interaction-Aware Monosemantic Concepts in Recommender Systems",
        "abstract": "We present a method for extracting \\emph{monosemantic} neurons, defined as latent dimensions that align with coherent and interpretable concepts, from user and item embeddings in recommender systems. Our approach employs a Sparse Autoencoder (SAE) to reveal semantic structure within pretrained representations. In contrast to work on language models, monosemanticity in recommendation must preserve the interactions between separate user and item embeddings. To achieve this, we introduce a \\emph{prediction aware} training objective that backpropagates through a frozen recommender and aligns the learned latent structure with the model's user-item affinity predictions. The resulting neurons capture properties such as genre, popularity, and temporal trends, and support post hoc control operations including targeted filtering and content promotion without modifying the base model. Our method generalizes across different recommendation models and datasets, providing a practical tool for interpretable and controllable personalization. Code and evaluation resources are available at https://github.com/DeltaLabTLV/Monosemanticity4Rec.",
        "url": "http://arxiv.org/abs/2511.18024v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18024v1",
        "arxiv_id": "2511.18024v1",
        "authors": [
            "Dor Arviv",
            "Yehonatan Elisha",
            "Oren Barkan",
            "Noam Koenigstein"
        ],
        "submitted": "2025-11-22 11:27:32",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'personalization' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on recommender systems, which is a related area to information retrieval. However, it doesn't directly address query understanding, ranking models, or user behavior modeling, which are core aspects of your research interests. The paper's emphasis on interpretable and controllable personalization in recommender systems shows some overlap with your interests in information retrieval and deep semantic understanding."
    },
    {
        "title": "HyM-UNet: Synergizing Local Texture and Global Context via Hybrid CNN-Mamba Architecture for Medical Image Segmentation",
        "abstract": "Accurate organ and lesion segmentation is a critical prerequisite for computer-aided diagnosis. Convolutional Neural Networks (CNNs), constrained by their local receptive fields, often struggle to capture complex global anatomical structures. To tackle this challenge, this paper proposes a novel hybrid architecture, HyM-UNet, designed to synergize the local feature extraction capabilities of CNNs with the efficient global modeling capabilities of Mamba. Specifically, we design a Hierarchical Encoder that utilizes convolutional modules in the shallow stages to preserve high-frequency texture details, while introducing Visual Mamba modules in the deep stages to capture long-range semantic dependencies with linear complexity. To bridge the semantic gap between the encoder and the decoder, we propose a Mamba-Guided Fusion Skip Connection (MGF-Skip). This module leverages deep semantic features as gating signals to dynamically suppress background noise within shallow features, thereby enhancing the perception of ambiguous boundaries. We conduct extensive experiments on public benchmark dataset ISIC 2018. The results demonstrate that HyM-UNet significantly outperforms existing state-of-the-art methods in terms of Dice coefficient and IoU, while maintaining lower parameter counts and inference latency. This validates the effectiveness and robustness of the proposed method in handling medical segmentation tasks characterized by complex shapes and scale variations.",
        "url": "http://arxiv.org/abs/2511.17988v1",
        "pdf_url": "https://arxiv.org/pdf/2511.17988v1",
        "arxiv_id": "2511.17988v1",
        "authors": [
            "Haodong Chen",
            "Xianfei Han",
            "Qwen"
        ],
        "submitted": "2025-11-22 09:02:06",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. It focuses on Medical Image Segmentation using a hybrid CNN-Mamba architecture, which is outside your areas of expertise."
    },
    {
        "title": "Leveraging Evidence-Guided LLMs to Enhance Trustworthy Depression Diagnosis",
        "abstract": "Large language models (LLMs) show promise in automating clinical diagnosis, yet their non-transparent decision-making and limited alignment with diagnostic standards hinder trust and clinical adoption. We address this challenge by proposing a two-stage diagnostic framework that enhances transparency, trustworthiness, and reliability. First, we introduce Evidence-Guided Diagnostic Reasoning (EGDR), which guides LLMs to generate structured diagnostic hypotheses by interleaving evidence extraction with logical reasoning grounded in DSM-5 criteria. Second, we propose a Diagnosis Confidence Scoring (DCS) module that evaluates the factual accuracy and logical consistency of generated diagnoses through two interpretable metrics: the Knowledge Attribution Score (KAS) and the Logic Consistency Score (LCS). Evaluated on the D4 dataset with pseudo-labels, EGDR outperforms direct in-context prompting and Chain-of-Thought (CoT) across five LLMs. For instance, on OpenBioLLM, EGDR improves accuracy from 0.31 (Direct) to 0.76 and increases DCS from 0.50 to 0.67. On MedLlama, DCS rises from 0.58 (CoT) to 0.77. Overall, EGDR yields up to +45% accuracy and +36% DCS gains over baseline methods, offering a clinically grounded, interpretable foundation for trustworthy AI-assisted diagnosis.",
        "url": "http://arxiv.org/abs/2511.17947v1",
        "pdf_url": "https://arxiv.org/pdf/2511.17947v1",
        "arxiv_id": "2511.17947v1",
        "authors": [
            "Yining Yuan",
            "J. Ben Tamo",
            "Micky C. Nnamdi",
            "Yifei Wang",
            "May D. Wang"
        ],
        "submitted": "2025-11-22 07:08:23",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on leveraging large language models for trustworthy depression diagnosis, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the application is in a clinical diagnosis context, and the methods proposed are not directly applicable to your areas of focus."
    },
    {
        "title": "Towards Efficient LLM-aware Heterogeneous Graph Learning",
        "abstract": "Heterogeneous graphs are widely present in real-world complex networks, where the diversity of node and relation types leads to complex and rich semantics. Efforts for modeling complex relation semantics in heterogeneous graphs are restricted by the limitations of predefined semantic dependencies and the scarcity of supervised signals. The advanced pre-training and fine-tuning paradigm leverages graph structure to provide rich self-supervised signals, but introduces semantic gaps between tasks. Large Language Models (LLMs) offer significant potential to address the semantic issues of relations and tasks in heterogeneous graphs through their strong reasoning capabilities in textual modality, but their incorporation into heterogeneous graphs is largely limited by computational complexity. Therefore, in this paper, we propose an Efficient LLM-Aware (ELLA) framework for heterogeneous graphs, addressing the above issues. To capture complex relation semantics, we propose an LLM-aware Relation Tokenizer that leverages LLM to encode multi-hop, multi-type relations. To reduce computational complexity, we further employ a Hop-level Relation Graph Transformer, which help reduces the complexity of LLM-aware relation reasoning from exponential to linear. To bridge semantic gaps between pre-training and fine-tuning tasks, we introduce the fine-grained task-aware textual Chain-of-Thought (CoT) prompts. Extensive experiments on four heterogeneous graphs show that our proposed ELLA outperforms state-of-the-art methods in the performance and efficiency. In particular, ELLA scales up to 13b-parameter LLMs and achieves up to a 4x speedup compared with existing LLM-based methods. Our code is publicly available at https://github.com/l-wd/ELLA.",
        "url": "http://arxiv.org/abs/2511.17923v1",
        "pdf_url": "https://arxiv.org/pdf/2511.17923v1",
        "arxiv_id": "2511.17923v1",
        "authors": [
            "Wenda Li",
            "Tongya Zheng",
            "Shunyu Liu",
            "Yu Wang",
            "Kaixuan Chen",
            "Hanyang Yuan",
            "Bingde Hu",
            "Zujie Ren",
            "Mingli Song",
            "Gang Chen"
        ],
        "submitted": "2025-11-22 05:38:03",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper proposes a framework for heterogeneous graph learning that leverages Large Language Models (LLMs) to capture complex relation semantics. While it touches on the topic of semantic understanding, it is primarily focused on graph learning and does not directly relate to information retrieval, search technologies, or user behavior modeling, which are core areas of interest."
    },
    {
        "title": "When Better Teachers Don't Make Better Students: Revisiting Knowledge Distillation for CLIP Models in VQA",
        "abstract": "Vision-language models (VLMs) have achieved remarkable success across multimodal tasks, yet their substantial computational demands hinder efficient deployment. Knowledge distillation (KD) has emerged as a powerful approach for building lightweight but competitive models, with strong evidence from both language and vision domains. However, its application to VLMs, particularly CLIP-style models, remains limited, often constrained to small-scale teachers and narrow evaluation tasks such as classification or retrieval. In this work, we present the first systematic study of distillation across a range of CLIP-style teacher models, ranging from standard baselines to large-scale state-of-the-art models. Contrary to trends observed in NLP and vision, we find that stronger teachers do not consistently yield better students; in fact, existing distillation frameworks often fail to scale, leading to degraded performance in downstream multimodal tasks such as visual question answering. Our findings challenge prevailing assumptions in KD and point toward new directions for designing parameter-efficient multimodal models.",
        "url": "http://arxiv.org/abs/2511.17886v1",
        "pdf_url": "https://arxiv.org/pdf/2511.17886v1",
        "arxiv_id": "2511.17886v1",
        "authors": [
            "Pume Tuchinda",
            "Parinthapat Pengpun",
            "Romrawin Chumpu",
            "Sarana Nutanong",
            "Peerat Limkonchotiwat"
        ],
        "submitted": "2025-11-22 02:30:18",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on knowledge distillation for CLIP models in VQA, which is a specific application in the NLP and computer vision domain. While it touches on multimodal tasks, it doesn't directly relate to information retrieval, query understanding, ranking models, or user behavior modeling, which are core areas of your research interests."
    },
    {
        "title": "A superpersuasive autonomous policy debating system",
        "abstract": "The capacity for highly complex, evidence-based, and strategically adaptive persuasion remains a formidable great challenge for artificial intelligence. Previous work, like IBM Project Debater, focused on generating persuasive speeches in simplified and shortened debate formats intended for relatively lay audiences. We introduce DeepDebater, a novel autonomous system capable of participating in and winning a full, unmodified, two-team competitive policy debate. Our system employs a hierarchical architecture of specialized multi-agent workflows, where teams of LLM-powered agents collaborate and critique one another to perform discrete argumentative tasks. Each workflow utilizes iterative retrieval, synthesis, and self-correction using a massive corpus of policy debate evidence (OpenDebateEvidence) and produces complete speech transcripts, cross-examinations, and rebuttals. We introduce a live, interactive end-to-end presentation pipeline that renders debates with AI speech and animation: transcripts are surface-realized and synthesized to audio with OpenAI TTS, and then displayed as talking-head portrait videos with EchoMimic V1. Beyond fully autonomous matches (AI vs AI), DeepDebater supports hybrid human-AI operation: human debaters can intervene at any stage, and humans can optionally serve as opponents against AI in any speech, allowing AI-human and AI-AI rounds. In preliminary evaluations against human-authored cases, DeepDebater produces qualitatively superior argumentative components and consistently wins simulated rounds as adjudicated by an independent autonomous judge. Expert human debate coaches also prefer the arguments, evidence, and cases constructed by DeepDebater. We open source all code, generated speech transcripts, audio and talking head video here: https://github.com/Hellisotherpeople/DeepDebater/tree/main",
        "url": "http://arxiv.org/abs/2511.17854v1",
        "pdf_url": "https://arxiv.org/pdf/2511.17854v1",
        "arxiv_id": "2511.17854v1",
        "authors": [
            "Allen Roush",
            "Devin Gonier",
            "John Hines",
            "Judah Goldfeder",
            "Philippe Martin Wyder",
            "Sanjay Basu",
            "Ravid Shwartz Ziv"
        ],
        "submitted": "2025-11-22 00:45:01",
        "source": "arxiv",
        "comment": "Accepted to CLIP workshop at AAAI 2026",
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on an autonomous policy debating system, which is not directly related to information retrieval, search technologies, or natural language processing. While it involves text synthesis and retrieval, the primary goal is to generate persuasive speeches, which is outside the scope of the user's research interests."
    },
    {
        "title": "Point of Order: Action-Aware LLM Persona Modeling for Realistic Civic Simulation",
        "abstract": "Large language models offer opportunities to simulate multi-party deliberation, but realistic modeling remains limited by a lack of speaker-attributed data. Transcripts produced via automatic speech recognition (ASR) assign anonymous speaker labels (e.g., Speaker_1), preventing models from capturing consistent human behavior. This work introduces a reproducible pipeline to transform public Zoom recordings into speaker-attributed transcripts with metadata like persona profiles and pragmatic action tags (e.g., [propose_motion]). We release three local government deliberation datasets: Appellate Court hearings, School Board meetings, and Municipal Council sessions. Fine-tuning LLMs to model specific participants using this \"action-aware\" data produces a 67% reduction in perplexity and nearly doubles classifier-based performance metrics for speaker fidelity and realism. Turing-style human evaluations show our simulations are often indistinguishable from real deliberations, providing a practical and scalable method for complex realistic civic simulations.",
        "url": "http://arxiv.org/abs/2511.17813v1",
        "pdf_url": "https://arxiv.org/pdf/2511.17813v1",
        "arxiv_id": "2511.17813v1",
        "authors": [
            "Scott Merrill",
            "Shashank Srivastava"
        ],
        "submitted": "2025-11-21 22:07:33",
        "source": "arxiv",
        "comment": "8 pages (29 pages including appendix), 18 figures. Code and datasets are available at https://github.com/smerrillunc/action-aware-llms. Submitted to ACL 2026",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on civic simulation and speaker-attributed data, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves large language models and fine-tuning, the context of civic simulation and deliberation is not aligned with the user's interests in e-commerce, query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "For Those Who May Find Themselves on the Red Team",
        "abstract": "This position paper argues that literary scholars must engage with large language model (LLM) interpretability research. While doing so will involve ideological struggle, if not out-right complicity, the necessity of this engagement is clear: the abiding instrumentality of current approaches to interpretability cannot be the only standard by which we measure interpretation with LLMs. One site at which this struggle could take place, I suggest, is the red team.",
        "url": "http://arxiv.org/abs/2511.18499v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18499v1",
        "arxiv_id": "2511.18499v1",
        "authors": [
            "Tyler Shoemaker"
        ],
        "submitted": "2025-11-23 15:31:03",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper appears to be unrelated to Information Retrieval, Search technologies, or your other areas of interest. It focuses on literary scholars' engagement with large language model interpretability research, which is outside your primary research themes."
    },
    {
        "title": "GeeSanBhava: Sentiment Tagged Sinhala Music Video Comment Data Set",
        "abstract": "This study introduce GeeSanBhava, a high-quality data set of Sinhala song comments extracted from YouTube manually tagged using Russells Valence-Arousal model by three independent human annotators. The human annotators achieve a substantial inter-annotator agreement (Fleiss kappa = 84.96%). The analysis revealed distinct emotional profiles for different songs, highlighting the importance of comment based emotion mapping. The study also addressed the challenges of comparing comment-based and song-based emotions, mitigating biases inherent in user-generated content. A number of Machine learning and deep learning models were pre-trained on a related large data set of Sinhala News comments in order to report the zero-shot result of our Sinhala YouTube comment data set. An optimized Multi-Layer Perceptron model, after extensive hyperparameter tuning, achieved a ROC-AUC score of 0.887. The model is a three-layer MLP with a configuration of 256, 128, and 64 neurons. This research contributes a valuable annotated dataset and provides insights for future work in Sinhala Natural Language Processing and music emotion recognition.",
        "url": "http://arxiv.org/abs/2511.18146v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18146v1",
        "arxiv_id": "2511.18146v1",
        "authors": [
            "Yomal De Mel",
            "Nisansa de Silva"
        ],
        "submitted": "2025-11-22 18:15:06",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be focused on a specific dataset and its application in Sinhala Natural Language Processing, which is somewhat related to the user's interests in NLP. However, the topic of sentiment analysis in music video comments does not align with the user's core research themes in Information Retrieval, Search technologies, and query understanding."
    },
    {
        "title": "Blu-WERP (Web Extraction and Refinement Pipeline): A Scalable Pipeline for Preprocessing Large Language Model Datasets",
        "abstract": "High-quality training data is fundamental to large language model (LLM) performance, yet existing preprocessing pipelines often struggle to effectively remove noise and unstructured content from web-scale corpora. This paper presents Blu-WERP, a novel data preprocessing pipeline designed to optimize the quality of Common Crawl WARC files for LLM training. We demonstrate that Blu-WERP significantly outperforms established baselines including DCLM across multiple model scales and evaluation benchmarks. Our pipeline processes CC WARC dumps, implementing advanced filtering and quality assessment mechanisms. We conducted comprehensive evaluations using models with 150M, 400M, 530M, 750M, and 1B parameters, testing against nine standard benchmarks categorized as World Knowledge & Reasoning, Language Understanding, and Commonsense Reasoning. Results show Blu-WERP consistently achieved superior performance across all model scales. At the 1B parameter scale, Relatively Blu-WERP demonstrates a 4.0% and 9.5% aggregate improvement over DCLM and Fineweb respectively, while achieving quality-per-token efficiency gain. Categorical analysis reveals 2.4% improvement in World Knowledge & Reasoning, 6.2% improvement in Language Understanding, and 4.2% improvement in Commonsense Reasoning. These results establish Blu-WERP as a state-of-the-art preprocessing pipeline that substantially improves LLM training data quality and downstream model performance with reduced computational cost. Our findings contribute to the growing body of research on data-centric AI, demonstrating that preprocessing pipeline design significantly impacts LLM capabilities. The Blu-WERP pipeline represents a practical advancement in data quality optimization, offering researchers and practitioners an effective solution for improving LLM training efficiency and model performance.",
        "url": "http://arxiv.org/abs/2511.18054v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18054v1",
        "arxiv_id": "2511.18054v1",
        "authors": [
            "Gowtham",
            "Sai Rupesh",
            "Sanjay Kumar",
            "Saravanan",
            "Venkata Chaithanya"
        ],
        "submitted": "2025-11-22 13:14:07",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on data preprocessing for large language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on NLP, the context is more about improving model performance through data quality optimization rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Fidelity-Aware Recommendation Explanations via Stochastic Path Integration",
        "abstract": "Explanation fidelity, which measures how accurately an explanation reflects a model's true reasoning, remains critically underexplored in recommender systems. We introduce SPINRec (Stochastic Path Integration for Neural Recommender Explanations), a model-agnostic approach that adapts path-integration techniques to the sparse and implicit nature of recommendation data. To overcome the limitations of prior methods, SPINRec employs stochastic baseline sampling: instead of integrating from a fixed or unrealistic baseline, it samples multiple plausible user profiles from the empirical data distribution and selects the most faithful attribution path. This design captures the influence of both observed and unobserved interactions, yielding more stable and personalized explanations. We conduct the most comprehensive fidelity evaluation to date across three models (MF, VAE, NCF), three datasets (ML1M, Yahoo! Music, Pinterest), and a suite of counterfactual metrics, including AUC-based perturbation curves and fixed-length diagnostics. SPINRec consistently outperforms all baselines, establishing a new benchmark for faithful explainability in recommendation. Code and evaluation tools are publicly available at https://github.com/DeltaLabTLV/SPINRec.",
        "url": "http://arxiv.org/abs/2511.18047v1",
        "pdf_url": "https://arxiv.org/pdf/2511.18047v1",
        "arxiv_id": "2511.18047v1",
        "authors": [
            "Oren Barkan",
            "Yahlly Schein",
            "Yehonatan Elisha",
            "Veronika Bogina",
            "Mikhail Baklanov",
            "Noam Koenigstein"
        ],
        "submitted": "2025-11-22 12:59:04",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to the user's interests in recommendation systems, but it focuses on explainability and fidelity in recommender systems, which is not the primary focus of the user's research. While it involves some NLP and data mining aspects, it is not directly related to information retrieval, query understanding, or ranking models."
    },
    {
        "title": "L2V-CoT: Cross-Modal Transfer of Chain-of-Thought Reasoning via Latent Intervention",
        "abstract": "Recently, Chain-of-Thought (CoT) reasoning has significantly enhanced the capabilities of large language models (LLMs), but Vision-Language Models (VLMs) still struggle with multi-step reasoning tasks due to limited multimodal reasoning data. To bridge this gap, researchers have explored methods to transfer CoT reasoning from LLMs to VLMs. However, existing approaches either need high training costs or require architectural alignment. In this paper, we use Linear Artificial Tomography (LAT) to empirically show that LLMs and VLMs share similar low-frequency latent representations of CoT reasoning despite architectural differences. Based on this insight, we propose L2V-CoT, a novel training-free latent intervention approach that transfers CoT reasoning from LLMs to VLMs. L2V-CoT extracts and resamples low-frequency CoT representations from LLMs in the frequency domain, enabling dimension matching and latent injection into VLMs during inference to enhance reasoning capabilities. Extensive experiments demonstrate that our approach consistently outperforms training-free baselines and even surpasses supervised methods.",
        "url": "http://arxiv.org/abs/2511.17910v1",
        "pdf_url": "https://arxiv.org/pdf/2511.17910v1",
        "arxiv_id": "2511.17910v1",
        "authors": [
            "Yuliang Zhan",
            "Xinyu Tang",
            "Han Wan",
            "Jian Li",
            "Ji-Rong Wen",
            "Hao Sun"
        ],
        "submitted": "2025-11-22 04:25:25",
        "source": "arxiv",
        "comment": "AAAI 2026 oral",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on cross-modal transfer of Chain-of-Thought reasoning between Vision-Language Models and large language models, which is not directly related to Information Retrieval, Search technologies, or user behavior modeling. While it involves deep semantic understanding, the context is more aligned with Natural Language Processing and multimodal reasoning, making it somewhat tangential to your primary research interests."
    }
]