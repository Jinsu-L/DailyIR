[
    {
        "title": "VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning",
        "abstract": "Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving Indian Culture. Existing Cultural benchmarks are (i) Manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. To address this, we introduce VIRAASAT, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop Question-Answering dataset for Indian culture. VIRAASAT leverages a Knowledge Graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of Indian culture (history, festivals, etc). VIRAASAT spans all 28 states and 8 Union Territories, yielding more than 3,200 multi-hop questions that necessitate chained cultural reasoning. We evaluate current State-of-the-Art (SOTA) LLMs on VIRAASAT and identify key limitations in reasoning wherein fine-tuning on Chain-of-Thought(CoT) traces fails to ground and synthesize low-probability facts. To bridge this gap, we propose a novel framework named Symbolic Chain-of-Manipulation (SCoM). Adapting the Chain-of-Manipulation paradigm, we train the model to simulate atomic Knowledge Graph manipulations internally. SCoM teaches the model to reliably traverse the topological structure of the graph. Experiments on Supervised Fine-Tuning (SFT) demonstrate that SCoM outperforms standard CoT baselines by up to 20%. We release the VIRAASAT dataset along with our findings, laying a strong foundation towards building Culturally Aware Reasoning Models.",
        "url": "http://arxiv.org/abs/2602.18429v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18429v1",
        "arxiv_id": "2602.18429v1",
        "authors": [
            "Harshul Raj Surana",
            "Arijit Maji",
            "Aryan Vats",
            "Akash Ghosh",
            "Sriparna Saha",
            "Amit Sheth"
        ],
        "submitted": "2026-02-20 18:53:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "RVR: Retrieve-Verify-Retrieve for Comprehensive Question Answering",
        "abstract": "Comprehensively retrieving diverse documents is crucial to address queries that admit a wide range of valid answers. We introduce retrieve-verify-retrieve (RVR), a multi-round retrieval framework designed to maximize answer coverage. Initially, a retriever takes the original query and returns a candidate document set, followed by a verifier that identifies a high-quality subset. For subsequent rounds, the query is augmented with previously verified documents to uncover answers that are not yet covered in previous rounds. RVR is effective even with off-the-shelf retrievers, and fine-tuning retrievers for our inference procedure brings further gains. Our method outperforms baselines, including agentic search approaches, achieving at least 10% relative and 3% absolute gain in complete recall percentage on a multi-answer retrieval dataset (QAMPARI). We also see consistent gains on two out-of-domain datasets (QUEST and WebQuestionsSP) across different base retrievers. Our work presents a promising iterative approach for comprehensive answer recall leveraging a verifier and adapting retrievers to a new inference scenario.",
        "url": "http://arxiv.org/abs/2602.18425v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18425v1",
        "arxiv_id": "2602.18425v1",
        "authors": [
            "Deniz Qian",
            "Hung-Ting Chen",
            "Eunsol Choi"
        ],
        "submitted": "2026-02-20 18:48:05",
        "source": "arxiv",
        "comment": "18 pages, 12 figures, 12 tables"
    },
    {
        "title": "SPQ: An Ensemble Technique for Large Language Model Compression",
        "abstract": "This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD), activation-based pruning, and post-training linear quantization. Each component targets a different source of inefficiency: i) pruning removes redundant neurons in MLP layers, ii) SVD reduces attention projections into compact low-rank factors, iii) and 8-bit quantization uniformly compresses all linear layers. At matched compression ratios, SPQ outperforms individual methods (SVD-only, pruning-only, or quantization-only) in perplexity, demonstrating the benefit of combining complementary techniques. Applied to LLaMA-2-7B, SPQ achieves up to 75% memory reduction while maintaining or improving perplexity (e.g., WikiText-2 5.47 to 4.91) and preserving accuracy on downstream benchmarks such as C4, TruthfulQA, and GSM8K. Compared to strong baselines like GPTQ and SparseGPT, SPQ offers competitive perplexity and accuracy while using less memory (6.86 GB vs. 7.16 GB for GPTQ). Moreover, SPQ improves inference throughput over GPTQ, achieving up to a 1.9x speedup, which further enhances its practicality for real-world deployment. The effectiveness of SPQ's robust compression through layer-aware and complementary compression techniques may provide practical deployment of LLMs in memory-constrained environments. Code is available at: https://github.com/JiaminYao/SPQ_LLM_Compression/",
        "url": "http://arxiv.org/abs/2602.18420v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18420v1",
        "arxiv_id": "2602.18420v1",
        "authors": [
            "Jiamin Yao",
            "Eren Gultepe"
        ],
        "submitted": "2026-02-20 18:44:16",
        "source": "arxiv",
        "comment": "Accepted to LREC 2026 Main Conference"
    },
    {
        "title": "Subgroups of $U(d)$ Induce Natural RNN and Transformer Architectures",
        "abstract": "This paper presents a direct framework for sequence models with hidden states on closed subgroups of U(d). We use a minimal axiomatic setup and derive recurrent and transformer templates from a shared skeleton in which subgroup choice acts as a drop-in replacement for state space, tangent projection, and update map. We then specialize to O(d) and evaluate orthogonal-state RNN and transformer models on Tiny Shakespeare and Penn Treebank under parameter-matched settings. We also report a general linear-mixing extension in tangent space, which applies across subgroup choices and improves finite-budget performance in the current O(d) experiments.",
        "url": "http://arxiv.org/abs/2602.18417v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18417v1",
        "arxiv_id": "2602.18417v1",
        "authors": [
            "Joshua Nunley"
        ],
        "submitted": "2026-02-20 18:35:43",
        "source": "arxiv",
        "comment": "12 pages, 3 figures, 8 tables"
    },
    {
        "title": "Validating Political Position Predictions of Arguments",
        "abstract": "Real-world knowledge representation often requires capturing subjective, continuous attributes -- such as political positions -- that conflict with pairwise validation, the widely accepted gold standard for human evaluation. We address this challenge through a dual-scale validation framework applied to political stance prediction in argumentative discourse, combining pointwise and pairwise human annotation. Using 22 language models, we construct a large-scale knowledge base of political position predictions for 23,228 arguments drawn from 30 debates that appeared on the UK politicial television programme \\textit{Question Time}. Pointwise evaluation shows moderate human-model agreement (Krippendorff's $α=0.578$), reflecting intrinsic subjectivity, while pairwise validation reveals substantially stronger alignment between human- and model-derived rankings ($α=0.86$ for the best model). This work contributes: (i) a practical validation methodology for subjective continuous knowledge that balances scalability with reliability; (ii) a validated structured argumentation knowledge base enabling graph-based reasoning and retrieval-augmented generation in political domains; and (iii) evidence that ordinal structure can be extracted from pointwise language models predictions from inherently subjective real-world discourse, advancing knowledge representation capabilities for domains where traditional symbolic or categorical approaches are insufficient.",
        "url": "http://arxiv.org/abs/2602.18351v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18351v1",
        "arxiv_id": "2602.18351v1",
        "authors": [
            "Jordan Robinson",
            "Angus R. Williams",
            "Katie Atkinson",
            "Anthony G. Cohn"
        ],
        "submitted": "2026-02-20 17:03:44",
        "source": "arxiv",
        "comment": "13 pages, 6 figures, 6 tables. Under review"
    },
    {
        "title": "Vichara: Appellate Judgment Prediction and Explanation for the Indian Judicial System",
        "abstract": "In jurisdictions like India, where courts face an extensive backlog of cases, artificial intelligence offers transformative potential for legal judgment prediction. A critical subset of this backlog comprises appellate cases, which are formal decisions issued by higher courts reviewing the rulings of lower courts. To this end, we present Vichara, a novel framework tailored to the Indian judicial system that predicts and explains appellate judgments. Vichara processes English-language appellate case proceeding documents and decomposes them into decision points. Decision points are discrete legal determinations that encapsulate the legal issue, deciding authority, outcome, reasoning, and temporal context. The structured representation isolates the core determinations and their context, enabling accurate predictions and interpretable explanations. Vichara's explanations follow a structured format inspired by the IRAC (Issue-Rule-Application-Conclusion) framework and adapted for Indian legal reasoning. This enhances interpretability, allowing legal professionals to assess the soundness of predictions efficiently. We evaluate Vichara on two datasets, PredEx and the expert-annotated subset of the Indian Legal Documents Corpus (ILDC_expert), using four large language models: GPT-4o mini, Llama-3.1-8B, Mistral-7B, and Qwen2.5-7B. Vichara surpasses existing judgment prediction benchmarks on both datasets, with GPT-4o mini achieving the highest performance (F1: 81.5 on PredEx, 80.3 on ILDC_expert), followed by Llama-3.1-8B. Human evaluation of the generated explanations across Clarity, Linking, and Usefulness metrics highlights GPT-4o mini's superior interpretability.",
        "url": "http://arxiv.org/abs/2602.18346v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18346v1",
        "arxiv_id": "2602.18346v1",
        "authors": [
            "Pavithra PM Nair",
            "Preethu Rose Anish"
        ],
        "submitted": "2026-02-20 16:57:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "On the \"Induction Bias\" in Sequence Models",
        "abstract": "Despite the remarkable practical success of transformer-based language models, recent work has raised concerns about their ability to perform state tracking. In particular, a growing body of literature has shown this limitation primarily through failures in out-of-distribution (OOD) generalization, such as length extrapolation. In this work, we shift attention to the in-distribution implications of these limitations. We conduct a large-scale experimental study of the data efficiency of transformers and recurrent neural networks (RNNs) across multiple supervision regimes. We find that the amount of training data required by transformers grows much more rapidly with state-space size and sequence length than for RNNs. Furthermore, we analyze the extent to which learned state-tracking mechanisms are shared across different sequence lengths. We show that transformers exhibit negligible or even detrimental weight sharing across lengths, indicating that they learn length-specific solutions in isolation. In contrast, recurrent models exhibit effective amortized learning by sharing weights across lengths, allowing data from one sequence length to improve performance on others. Together, these results demonstrate that state tracking remains a fundamental challenge for transformers, even when training and evaluation distributions match.",
        "url": "http://arxiv.org/abs/2602.18333v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18333v1",
        "arxiv_id": "2602.18333v1",
        "authors": [
            "M. Reza Ebrahimi",
            "Michaël Defferrard",
            "Sunny Panchal",
            "Roland Memisevic"
        ],
        "submitted": "2026-02-20 16:39:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Predicting Contextual Informativeness for Vocabulary Learning using Deep Learning",
        "abstract": "We describe a modern deep learning system that automatically identifies informative contextual examples (\\qu{contexts}) for first language vocabulary instruction for high school student. Our paper compares three modeling approaches: (i) an unsupervised similarity-based strategy using MPNet's uniformly contextualized embeddings, (ii) a supervised framework built on instruction-aware, fine-tuned Qwen3 embeddings with a nonlinear regression head and (iii) model (ii) plus handcrafted context features. We introduce a novel metric called the Retention Competency Curve to visualize trade-offs between the discarded proportion of good contexts and the \\qu{good-to-bad} contexts ratio providing a compact, unified lens on model performance. Model (iii) delivers the most dramatic gains with performance of a good-to-bad ratio of 440 all while only throwing out 70\\% of the good contexts. In summary, we demonstrate that a modern embedding model on neural network architecture, when guided by human supervision, results in a low-cost large supply of near-perfect contexts for teaching vocabulary for a variety of target words.",
        "url": "http://arxiv.org/abs/2602.18326v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18326v1",
        "arxiv_id": "2602.18326v1",
        "authors": [
            "Tao Wu",
            "Adam Kapelner"
        ],
        "submitted": "2026-02-20 16:32:14",
        "source": "arxiv",
        "comment": "8 pages, 3 figures, 4 tables"
    },
    {
        "title": "PsihoRo: Depression and Anxiety Romanian Text Corpus",
        "abstract": "Psychological corpora in NLP are collections of texts used to analyze human psychology, emotions, and mental health. These texts allow researchers to study psychological constructs, detect mental health issues and analyze emotional language. However, mental health data can be difficult to collect correctly from social media, due to suppositions made by the collectors. A more pragmatic strategy involves gathering data through open-ended questions and then assessing this information with self-report screening surveys. This method was employed successfully for English, a language with a lot of psychological NLP resources. However, this cannot be stated for Romanian, which currently has no open-source mental health corpus. To address this gap, we have created the first corpus for depression and anxiety in Romanian, by utilizing a form with 6 open-ended questions along with the standardized PHQ-9 and GAD-7 screening questionnaires. Consisting of the texts of 205 respondents and although it may seem small, PsihoRo is a first step towards understanding and analyzing texts regarding the mental health of the Romanian population. We employ statistical analysis, text analysis using Romanian LIWC, emotion detection and topic modeling to show what are the most important features of this newly introduced resource to the NLP community.",
        "url": "http://arxiv.org/abs/2602.18324v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18324v1",
        "arxiv_id": "2602.18324v1",
        "authors": [
            "Alexandra Ciobotaru",
            "Ana-Maria Bucur",
            "Liviu P. Dinu"
        ],
        "submitted": "2026-02-20 16:24:23",
        "source": "arxiv",
        "comment": "This article was accepted at LREC 2026"
    },
    {
        "title": "VeriSoftBench: Repository-Scale Formal Verification Benchmarks for Lean",
        "abstract": "Large language models have achieved striking results in interactive theorem proving, particularly in Lean. However, most benchmarks for LLM-based proof automation are drawn from mathematics in the Mathlib ecosystem, whereas proofs in software verification are developed inside definition-rich codebases with substantial project-specific libraries. We introduce VeriSoftBench, a benchmark of 500 Lean 4 proof obligations drawn from open-source formal-methods developments and packaged to preserve realistic repository context and cross-file dependencies. Our evaluation of frontier LLMs and specialized provers yields three observations. First, provers tuned for Mathlib-style mathematics transfer poorly to this repository-centric setting. Second, success is strongly correlated with transitive repository dependence: tasks whose proofs draw on large, multi-hop dependency closures are less likely to be solved. Third, providing curated context restricted to a proof's dependency closure improves performance relative to exposing the full repository, but nevertheless leaves substantial room for improvement. Our benchmark and evaluation suite are released at https://github.com/utopia-group/VeriSoftBench.",
        "url": "http://arxiv.org/abs/2602.18307v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18307v1",
        "arxiv_id": "2602.18307v1",
        "authors": [
            "Yutong Xin",
            "Qiaochu Chen",
            "Greg Durrett",
            "Işil Dillig"
        ],
        "submitted": "2026-02-20 16:05:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "On the Semantic and Syntactic Information Encoded in Proto-Tokens for One-Step Text Reconstruction",
        "abstract": "Autoregressive large language models (LLMs) generate text token-by-token, requiring n forward passes to produce a sequence of length n. Recent work, Exploring the Latent Capacity of LLMs for One-Step Text Reconstruction (Mezentsev and Oseledets), shows that frozen LLMs can reconstruct hundreds of tokens from only two learned proto-tokens in a single forward pass, suggesting a path beyond the autoregressive paradigm. In this paper, we study what information these proto-tokens encode and how they behave under reconstruction and controlled constraints. We perform a series of experiments aimed at disentangling semantic and syntactic content in the two proto-tokens, analyzing stability properties of the e-token, and visualizing attention patterns to the e-token during reconstruction. Finally, we test two regularization schemes for \"imposing\" semantic structure on the e-token using teacher embeddings, including an anchor-based loss and a relational distillation objective. Our results indicate that the m-token tends to capture semantic information more strongly than the e-token under standard optimization; anchor-based constraints trade off sharply with reconstruction accuracy; and relational distillation can transfer batch-level semantic relations into the proto-token space without sacrificing reconstruction quality, supporting the feasibility of future non-autoregressive seq2seq systems that predict proto-tokens as an intermediate representation.",
        "url": "http://arxiv.org/abs/2602.18301v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18301v1",
        "arxiv_id": "2602.18301v1",
        "authors": [
            "Ivan Bondarenko",
            "Egor Palkin",
            "Fedor Tikunov"
        ],
        "submitted": "2026-02-20 15:54:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Analyzing and Improving Chain-of-Thought Monitorability Through Information Theory",
        "abstract": "Chain-of-thought (CoT) monitors are LLM-based systems that analyze reasoning traces to detect when outputs may exhibit attributes of interest, such as test-hacking behavior during code generation. In this paper, we use information-theoretic analysis to show that non-zero mutual information between CoT and output is a necessary but not sufficient condition for CoT monitorability. We identify two sources of approximation error that may undermine the performance of CoT monitors in practice: information gap, which measures the extent to which the monitor can extract the information available in CoT, and elicitation error, which measures the extent to which the monitor approximates the optimal monitoring function. We further demonstrate that CoT monitorability can be systematically improved through targeted training objectives. To this end, we propose two complementary approaches: (a) an oracle-based method that directly rewards the monitored model for producing CoTs that maximize monitor accuracy, and (b) a more practical, label-free approach that maximizes conditional mutual information between outputs and CoTs. Across multiple different environments, we show both methods significantly improve monitor accuracy while preventing CoT degeneration even when training against a monitor, thereby mitigating reward hacking when the task reward is imperfectly specified.",
        "url": "http://arxiv.org/abs/2602.18297v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18297v1",
        "arxiv_id": "2602.18297v1",
        "authors": [
            "Usman Anwar",
            "Tim Bakker",
            "Dana Kianfar",
            "Cristina Pinneri",
            "Christos Louizos"
        ],
        "submitted": "2026-02-20 15:50:30",
        "source": "arxiv",
        "comment": "First two authors contributed equally"
    },
    {
        "title": "A Topology-Aware Positive Sample Set Construction and Feature Optimization Method in Implicit Collaborative Filtering",
        "abstract": "Negative sampling strategies are widely used in implicit collaborative filtering to address issues like data sparsity and class imbalance. However, these methods often introduce false negatives, hindering the model's ability to accurately learn users' latent preferences. To mitigate this problem, existing methods adjust the negative sampling distribution based on statistical features from model training or the hardness of negative samples. Nevertheless, these methods face two key limitations: (1) over-reliance on the model's current representation capabilities; (2) failure to leverage the potential of false negatives as latent positive samples to guide model learning of user preferences more accurately. To address the above issues, we propose a Topology-aware Positive Sample Set Construction and Feature Optimization method (TPSC-FO). First, we design a simple topological community-aware false negative identification (FNI) method and observe that topological community structures in interaction networks can effectively identify false negatives. Motivated by this, we develop a topology-aware positive sample set construction module. This module employs a differential community detection strategy to capture topological community structures in implicit feedback, coupled with personalized noise filtration to reliably identify false negatives and convert them into positive samples. Additionally, we introduce a neighborhood-guided feature optimization module that refines positive sample features by incorporating neighborhood features in the embedding space, effectively mitigating noise in the positive samples. Extensive experiments on five real-world datasets and two synthetic datasets validate the effectiveness of TPSC-FO.",
        "url": "http://arxiv.org/abs/2602.18288v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18288v1",
        "arxiv_id": "2602.18288v1",
        "authors": [
            "Jiayi Wu",
            "Zhengyu Wu",
            "Xunkai Li",
            "Rong-Hua Li",
            "Guoren Wang"
        ],
        "submitted": "2026-02-20 15:35:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "HyTRec: A Hybrid Temporal-Aware Attention Architecture for Long Behavior Sequential Recommendation",
        "abstract": "Modeling long sequences of user behaviors has emerged as a critical frontier in generative recommendation. However, existing solutions face a dilemma: linear attention mechanisms achieve efficiency at the cost of retrieval precision due to limited state capacity, while softmax attention suffers from prohibitive computational overhead. To address this challenge, we propose HyTRec, a model featuring a Hybrid Attention architecture that explicitly decouples long-term stable preferences from short-term intent spikes. By assigning massive historical sequences to a linear attention branch and reserving a specialized softmax attention branch for recent interactions, our approach restores precise retrieval capabilities within industrial-scale contexts involving ten thousand interactions. To mitigate the lag in capturing rapid interest drifts within the linear layers, we furthermore design Temporal-Aware Delta Network (TADN) to dynamically upweight fresh behavioral signals while effectively suppressing historical noise. Empirical results on industrial-scale datasets confirm the superiority that our model maintains linear inference speed and outperforms strong baselines, notably delivering over 8% improvement in Hit Rate for users with ultra-long sequences with great efficiency.",
        "url": "http://arxiv.org/abs/2602.18283v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18283v1",
        "arxiv_id": "2602.18283v1",
        "authors": [
            "Lei Xin",
            "Yuhao Zheng",
            "Ke Cheng",
            "Changjiang Jiang",
            "Zifan Zhang",
            "Fanhu Zeng"
        ],
        "submitted": "2026-02-20 15:11:40",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "Simplifying Outcomes of Language Model Component Analyses with ELIA",
        "abstract": "While mechanistic interpretability has developed powerful tools to analyze the internal workings of Large Language Models (LLMs), their complexity has created an accessibility gap, limiting their use to specialists. We address this challenge by designing, building, and evaluating ELIA (Explainable Language Interpretability Analysis), an interactive web application that simplifies the outcomes of various language model component analyses for a broader audience. The system integrates three key techniques -- Attribution Analysis, Function Vector Analysis, and Circuit Tracing -- and introduces a novel methodology: using a vision-language model to automatically generate natural language explanations (NLEs) for the complex visualizations produced by these methods. The effectiveness of this approach was empirically validated through a mixed-methods user study, which revealed a clear preference for interactive, explorable interfaces over simpler, static visualizations. A key finding was that the AI-powered explanations helped bridge the knowledge gap for non-experts; a statistical analysis showed no significant correlation between a user's prior LLM experience and their comprehension scores, suggesting that the system reduced barriers to comprehension across experience levels. We conclude that an AI system can indeed simplify complex model analyses, but its true power is unlocked when paired with thoughtful, user-centered design that prioritizes interactivity, specificity, and narrative guidance.",
        "url": "http://arxiv.org/abs/2602.18262v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18262v1",
        "arxiv_id": "2602.18262v1",
        "authors": [
            "Aaron Louis Eidt",
            "Nils Feldhus"
        ],
        "submitted": "2026-02-20 14:45:27",
        "source": "arxiv",
        "comment": "EACL 2026 System Demonstrations. GitHub: https://github.com/aaron0eidt/ELIA"
    },
    {
        "title": "Dual-Tree LLM-Enhanced Negative Sampling for Implicit Collaborative Filtering",
        "abstract": "Negative sampling is a pivotal technique in implicit collaborative filtering (CF) recommendation, enabling efficient and effective training by contrasting observed interactions with sampled unobserved ones.\n  Recently, large language models (LLMs) have shown promise in recommender systems; however, research on LLM-empowered negative sampling remains underexplored.\n  Existing methods heavily rely on textual information and task-specific fine-tuning, limiting practical applicability.\n  To address this limitation, we propose a text-free and fine-tuning-free Dual-Tree LLM-enhanced Negative Sampling method (DTL-NS).\n  It consists of two modules: (i) an offline false negative identification module that leverages hierarchical index trees to transform collaborative structural and latent semantic information into structured item-ID encodings for LLM inference, enabling accurate identification of false negatives; and (ii) a multi-view hard negative sampling module that combines user-item preference scores with item-item hierarchical similarities from these encodings to mine high-quality hard negatives, thus improving models' discriminative ability.\n  Extensive experiments demonstrate the effectiveness of DTL-NS. For example, on the Amazon-sports dataset, DTL-NS outperforms the strongest baseline by 10.64% and 19.12% in Recall@20 and NDCG@20, respectively.\n  Moreover, DTL-NS can be integrated into various implicit CF models and negative sampling methods, consistently enhancing their performance.",
        "url": "http://arxiv.org/abs/2602.18249v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18249v1",
        "arxiv_id": "2602.18249v1",
        "authors": [
            "Jiayi Wu",
            "Zhengyu Wu",
            "Xunkai Li",
            "Rong-Hua Li",
            "Guoren Wang"
        ],
        "submitted": "2026-02-20 14:32:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Thinking by Subtraction: Confidence-Driven Contrastive Decoding for LLM Reasoning",
        "abstract": "Recent work on test-time scaling for large language model (LLM) reasoning typically assumes that allocating more inference-time computation uniformly improves correctness. However, prior studies show that reasoning uncertainty is highly localized: a small subset of low-confidence tokens disproportionately contributes to reasoning errors and unnecessary output expansion. Motivated by this observation, we propose Thinking by Subtraction, a confidence-driven contrastive decoding approach that improves reasoning reliability through targeted token-level intervention. Our method, Confidence-Driven Contrastive Decoding, detects low-confidence tokens during decoding and intervenes selectively at these positions. It constructs a contrastive reference by replacing high-confidence tokens with minimal placeholders, and refines predictions by subtracting this reference distribution at low-confidence locations. Experiments show that CCD significantly improves accuracy across mathematical reasoning benchmarks while substantially reducing output length, with minimal KV-cache overhead. As a training-free method, CCD enhances reasoning reliability through targeted low-confidence intervention without computational redundancy. Our code will be made available at: https://github.com/bolo-web/CCD.",
        "url": "http://arxiv.org/abs/2602.18232v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18232v1",
        "arxiv_id": "2602.18232v1",
        "authors": [
            "Lexiang Tang",
            "Weihao Gao",
            "Bingchen Zhao",
            "Lu Ma",
            "Qiao jin",
            "Bang Yang",
            "Yuexian Zou"
        ],
        "submitted": "2026-02-20 14:13:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Economical-Ecological Benefits of Matching Non-matching Socks",
        "abstract": "Socks are produced and replaced at a massive scale, yet their paired use makes them unusually vulnerable to waste, as the loss of a single sock can strand usable wear-capacity and trigger premature replacement. In this study, we quantify the economic and ecological value of pairing non-matching \\say{orphan} socks, and the social cost that discourages this behaviour. We formalize sock ownership as a sequential decision problem under uncertainty in which socks wear out and disappear stochastically during laundering, while public exposure induces a person-specific mismatch penalty. We conducted an in-person study to estimate mismatch sensitivity and diversity preference, linking behavioural heterogeneity to optimal mixing strategies. Using these results and a computer simulation-based evaluation of interpretable pairing policies, we show that strict matching can appear resource-frugal largely because it generates many sockless days, whereas controlled tolerance for mismatch sustains service and reduces stranded capacity across loss regimes. This study establishes the feasibility of matching non-matching socks while outlining its limitations and challenges.",
        "url": "http://arxiv.org/abs/2602.18221v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18221v1",
        "arxiv_id": "2602.18221v1",
        "authors": [
            "Teddy Lazebnik"
        ],
        "submitted": "2026-02-20 14:00:20",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Information-Theoretic Storage Cost in Sentence Comprehension",
        "abstract": "Real-time sentence comprehension imposes a significant load on working memory, as comprehenders must maintain contextual information to anticipate future input. While measures of such load have played an important role in psycholinguistic theories, they have been formalized, largely, using symbolic grammars, which assign discrete, uniform costs to syntactic predictions. This study proposes a measure of processing storage cost based on an information-theoretic formalization, as the amount of information previous words carry about future context, under uncertainty. Unlike previous discrete, grammar-based metrics, this measure is continuous, theory-neutral, and can be estimated from pre-trained neural language models. The validity of this approach is demonstrated through three analyses in English: our measure (i) recovers well-known processing asymmetries in center embeddings and relative clauses, (ii) correlates with a grammar-based storage cost in a syntactically-annotated corpus, and (iii) predicts reading-time variance in two large-scale naturalistic datasets over and above baseline models with traditional information-based predictors.",
        "url": "http://arxiv.org/abs/2602.18217v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18217v1",
        "arxiv_id": "2602.18217v1",
        "authors": [
            "Kohei Kajikawa",
            "Shinnosuke Isono",
            "Ethan Gotlieb Wilcox"
        ],
        "submitted": "2026-02-20 13:55:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Simple yet Effective Negative Sampling Plugin for Constructing Positive Sample Pairs in Implicit Collaborative Filtering",
        "abstract": "Most implicit collaborative filtering (CF) models are trained with negative sampling, where existing work designs sophisticated strategies for high-quality negatives while largely overlooking the exploration of positive samples. Although some denoising recommendation methods can be applied to implicit CF for denoising positive samples, they often sparsify positive supervision. Moreover, these approaches generally overlook user activity bias during training, leading to insufficient learning for inactive users. To address these issues, we propose a simple yet effective negative sampling plugin, PSP-NS, from the perspective of enhancing positive supervision signals. It builds a user-item bipartite graph with edge weights indicating interaction confidence inferred from global and local patterns, generates positive sample pairs via replication-based reweighting to strengthen positive signals, and adopts an activity-aware weighting scheme to effectively learn inactive users' preferences. We provide theoretical insights from a margin-improvement perspective, explaining why PSP-NS tends to improve ranking quality (e.g., Precision@k/Recall@k), and conduct extensive experiments on four real-world datasets to demonstrate its superiority. For instance, PSP-NS boosts Recall@30 and Precision@30 by 32.11% and 22.90% on Yelp over the strongest baselines. PSP-NS can be integrated with various implicit CF recommenders or negative sampling methods to enhance their performance.",
        "url": "http://arxiv.org/abs/2602.18206v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18206v1",
        "arxiv_id": "2602.18206v1",
        "authors": [
            "Jiayi Wu",
            "Zhengyu Wu",
            "Xunkai Li",
            "Ronghua Li",
            "Guoren Wang"
        ],
        "submitted": "2026-02-20 13:34:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Improving Sampling for Masked Diffusion Models via Information Gain",
        "abstract": "Masked Diffusion Models (MDMs) offer greater flexibility in decoding order than autoregressive models but require careful planning to achieve high-quality generation. Existing samplers typically adopt greedy heuristics, prioritizing positions with the highest local certainty to decode at each step. Through failure case analysis, we identify a fundamental limitation of this approach: it neglects the downstream impact of current decoding choices on subsequent steps and fails to minimize cumulative uncertainty. In particular, these methods do not fully exploit the non-causal nature of MDMs, which enables evaluating how a decoding decision reshapes token probabilities/uncertainty across all remaining masked positions. To bridge this gap, we propose the Info-Gain Sampler, a principled decoding framework that balances immediate uncertainty with information gain over future masked tokens. Extensive evaluations across diverse architectures and tasks (reasoning, coding, creative writing, and image generation) demonstrate that Info-Gain Sampler consistently outperforms existing samplers for MDMs. For instance, it achieves a 3.6% improvement in average accuracy on reasoning tasks and a 63.1% win-rate in creative writing. Notably, on reasoning tasks it reduces cumulative uncertainty from 78.4 to 48.6, outperforming the best baseline by a large margin. The code will be available at https://github.com/yks23/Information-Gain-Sampler.",
        "url": "http://arxiv.org/abs/2602.18176v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18176v1",
        "arxiv_id": "2602.18176v1",
        "authors": [
            "Kaisen Yang",
            "Jayden Teoh",
            "Kaicheng Yang",
            "Yitong Zhang",
            "Alex Lamb"
        ],
        "submitted": "2026-02-20 12:26:03",
        "source": "arxiv",
        "comment": "https://github.com/yks23/Information-Gain-Sampler"
    },
    {
        "title": "Click it or Leave it: Detecting and Spoiling Clickbait with Informativeness Measures and Large Language Models",
        "abstract": "Clickbait headlines degrade the quality of online information and undermine user trust. We present a hybrid approach to clickbait detection that combines transformer-based text embeddings with linguistically motivated informativeness features. Using natural language processing techniques, we evaluate classical vectorizers, word embedding baselines, and large language model embeddings paired with tree-based classifiers. Our best-performing model, XGBoost over embeddings augmented with 15 explicit features, achieves an F1-score of 91\\%, outperforming TF-IDF, Word2Vec, GloVe, LLM prompt based classification, and feature-only baselines. The proposed feature set enhances interpretability by highlighting salient linguistic cues such as second-person pronouns, superlatives, numerals, and attention-oriented punctuation, enabling transparent and well-calibrated clickbait predictions. We release code and trained models to support reproducible research.",
        "url": "http://arxiv.org/abs/2602.18171v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18171v1",
        "arxiv_id": "2602.18171v1",
        "authors": [
            "Wojciech Michaluk",
            "Tymoteusz Urban",
            "Mateusz Kubita",
            "Soveatin Kuntur",
            "Anna Wroblewska"
        ],
        "submitted": "2026-02-20 12:16:08",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "FENCE: A Financial and Multimodal Jailbreak Detection Dataset",
        "abstract": "Jailbreaking poses a significant risk to the deployment of Large Language Models (LLMs) and Vision Language Models (VLMs). VLMs are particularly vulnerable because they process both text and images, creating broader attack surfaces. However, available resources for jailbreak detection are scarce, particularly in finance. To address this gap, we present FENCE, a bilingual (Korean-English) multimodal dataset for training and evaluating jailbreak detectors in financial applications. FENCE emphasizes domain realism through finance-relevant queries paired with image-grounded threats. Experiments with commercial and open-source VLMs reveal consistent vulnerabilities, with GPT-4o showing measurable attack success rates and open-source models displaying greater exposure. A baseline detector trained on FENCE achieves 99 percent in-distribution accuracy and maintains strong performance on external benchmarks, underscoring the dataset's robustness for training reliable detection models. FENCE provides a focused resource for advancing multimodal jailbreak detection in finance and for supporting safer, more reliable AI systems in sensitive domains. Warning: This paper includes example data that may be offensive.",
        "url": "http://arxiv.org/abs/2602.18154v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18154v1",
        "arxiv_id": "2602.18154v1",
        "authors": [
            "Mirae Kim",
            "Seonghun Jeong",
            "Youngjun Kwak"
        ],
        "submitted": "2026-02-20 11:40:41",
        "source": "arxiv",
        "comment": "lrec 2026 accepted paper"
    },
    {
        "title": "The Statistical Signature of LLMs",
        "abstract": "Large language models generate text through probabilistic sampling from high-dimensional distributions, yet how this process reshapes the structural statistical organization of language remains incompletely characterized. Here we show that lossless compression provides a simple, model-agnostic measure of statistical regularity that differentiates generative regimes directly from surface text. We analyze compression behavior across three progressively more complex information ecosystems: controlled human-LLM continuations, generative mediation of a knowledge infrastructure (Wikipedia vs. Grokipedia), and fully synthetic social interaction environments (Moltbook vs. Reddit). Across settings, compression reveals a persistent structural signature of probabilistic generation. In controlled and mediated contexts, LLM-produced language exhibits higher structural regularity and compressibility than human-written text, consistent with a concentration of output within highly recurrent statistical patterns. However, this signature shows scale dependence: in fragmented interaction environments the separation attenuates, suggesting a fundamental limit to surface-level distinguishability at small scales. This compressibility-based separation emerges consistently across models, tasks, and domains and can be observed directly from surface text without relying on model internals or semantic evaluation. Overall, our findings introduce a simple and robust framework for quantifying how generative systems reshape textual production, offering a structural perspective on the evolving complexity of communication.",
        "url": "http://arxiv.org/abs/2602.18152v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18152v1",
        "arxiv_id": "2602.18152v1",
        "authors": [
            "Ortal Hadad",
            "Edoardo Loru",
            "Jacopo Nudo",
            "Niccolò Di Marco",
            "Matteo Cinelli",
            "Walter Quattrociocchi"
        ],
        "submitted": "2026-02-20 11:33:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Detecting Contextual Hallucinations in LLMs with Frequency-Aware Attention",
        "abstract": "Hallucination detection is critical for ensuring the reliability of large language models (LLMs) in context-based generation. Prior work has explored intrinsic signals available during generation, among which attention offers a direct view of grounding behavior. However, existing approaches typically rely on coarse summaries that fail to capture fine-grained instabilities in attention. Inspired by signal processing, we introduce a frequency-aware perspective on attention by analyzing its variation during generation. We model attention distributions as discrete signals and extract high-frequency components that reflect rapid local changes in attention. Our analysis reveals that hallucinated tokens are associated with high-frequency attention energy, reflecting fragmented and unstable grounding behavior. Based on this insight, we develop a lightweight hallucination detector using high-frequency attention features. Experiments on the RAGTruth and HalluRAG benchmarks show that our approach achieves performance gains over verification-based, internal-representation-based, and attention-based methods across models and tasks.",
        "url": "http://arxiv.org/abs/2602.18145v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18145v1",
        "arxiv_id": "2602.18145v1",
        "authors": [
            "Siya Qi",
            "Yudong Chen",
            "Runcong Zhao",
            "Qinglin Zhu",
            "Zhanghao Hu",
            "Wei Liu",
            "Yulan He",
            "Zheng Yuan",
            "Lin Gui"
        ],
        "submitted": "2026-02-20 11:18:45",
        "source": "arxiv",
        "comment": "25 pages, 10 figures"
    },
    {
        "title": "Agentic Adversarial QA for Improving Domain-Specific LLMs",
        "abstract": "Large Language Models (LLMs), despite extensive pretraining on broad internet corpora, often struggle to adapt effectively to specialized domains. There is growing interest in fine-tuning these models for such domains; however, progress is constrained by the scarcity and limited coverage of high-quality, task-relevant data. To address this, synthetic data generation methods such as paraphrasing or knowledge extraction are commonly applied. Although these approaches excel at factual recall and conceptual knowledge, they suffer from two critical shortcomings: (i) they provide minimal support for interpretive reasoning capabilities in these specialized domains, and (ii) they often produce synthetic corpora that are excessively large and redundant, resulting in poor sample efficiency. To overcome these gaps, we propose an adversarial question-generation framework that produces a compact set of semantically challenging questions. These questions are constructed by comparing the outputs of the model to be adapted and a robust expert model grounded in reference documents, using an iterative, feedback-driven process designed to reveal and address comprehension gaps. Evaluation on specialized subsets of the LegalBench corpus demonstrates that our method achieves greater accuracy with substantially fewer synthetic samples.",
        "url": "http://arxiv.org/abs/2602.18137v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18137v1",
        "arxiv_id": "2602.18137v1",
        "authors": [
            "Vincent Grari",
            "Ciprian Tomoiaga",
            "Sylvain Lamprier",
            "Tatsunori Hashimoto",
            "Marcin Detyniecki"
        ],
        "submitted": "2026-02-20 10:53:09",
        "source": "arxiv",
        "comment": "9 pages, 1 Figure"
    },
    {
        "title": "SuiteEval: Simplifying Retrieval Benchmarks",
        "abstract": "Information retrieval evaluation often suffers from fragmented practices -- varying dataset subsets, aggregation methods, and pipeline configurations -- that undermine reproducibility and comparability, especially for foundation embedding models requiring robust out-of-domain performance. We introduce SuiteEval, a unified framework that offers automatic end-to-end evaluation, dynamic indexing that reuses on-disk indices to minimise disk usage, and built-in support for major benchmarks (BEIR, LoTTE, MS MARCO, NanoBEIR, and BRIGHT). Users only need to supply a pipeline generator. SuiteEval handles data loading, indexing, ranking, metric computation, and result aggregation. New benchmark suites can be added in a single line. SuiteEval reduces boilerplate and standardises evaluations to facilitate reproducible IR research, as a broader benchmark set is increasingly required.",
        "url": "http://arxiv.org/abs/2602.18107v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18107v1",
        "arxiv_id": "2602.18107v1",
        "authors": [
            "Andrew Parry",
            "Debasis Ganguly",
            "Sean MacAvaney"
        ],
        "submitted": "2026-02-20 09:54:16",
        "source": "arxiv",
        "comment": "5 pages, 3 figures, 2 tables, Accepted as a Demonstration to ECIR 2026"
    },
    {
        "title": "Perceived Political Bias in LLMs Reduces Persuasive Abilities",
        "abstract": "Conversational AI has been proposed as a scalable way to correct public misconceptions and spread misinformation. Yet its effectiveness may depend on perceptions of its political neutrality. As LLMs enter partisan conflict, elites increasingly portray them as ideologically aligned. We test whether these credibility attacks reduce LLM-based persuasion. In a preregistered U.S. survey experiment (N=2144), participants completed a three-round conversation with ChatGPT about a personally held economic policy misconception. Compared to a neutral control, a short message indicating that the LLM was biased against the respondent's party attenuated persuasion by 28%. Transcript analysis indicates that the warnings alter the interaction: respondents push back more and engage less receptively. These findings suggest that the persuasive impact of conversational AI is politically contingent, constrained by perceptions of partisan alignment.",
        "url": "http://arxiv.org/abs/2602.18092v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18092v1",
        "arxiv_id": "2602.18092v1",
        "authors": [
            "Matthew DiGiuseppe",
            "Joshua Robison"
        ],
        "submitted": "2026-02-20 09:33:16",
        "source": "arxiv",
        "comment": "39 pages, 10 figures"
    },
    {
        "title": "Gradient Regularization Prevents Reward Hacking in Reinforcement Learning from Human Feedback and Verifiable Rewards",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) or Verifiable Rewards (RLVR) are two key steps in the post-training of modern Language Models (LMs). A common problem is reward hacking, where the policy may exploit inaccuracies of the reward and learn an unintended behavior. Most previous works address this by limiting the policy update with a Kullback-Leibler (KL) penalty towards a reference model. We propose a different framing: Train the LM in a way that biases policy updates towards regions in which the reward is more accurate. First, we derive a theoretical connection between the accuracy of a reward model and the flatness of an optimum at convergence. Gradient regularization (GR) can then be used to bias training to flatter regions and thereby maintain reward model accuracy. We confirm these results by showing that the gradient norm and reward accuracy are empirically correlated in RLHF. We then show that Reference Resets of the KL penalty implicitly use GR to find flatter regions with higher reward accuracy. We further improve on this by proposing to use explicit GR with an efficient finite-difference estimate. Empirically, GR performs better than a KL penalty across a diverse set of RL experiments with LMs. GR achieves a higher GPT-judged win-rate in RLHF, avoids overly focusing on the format in rule-based math rewards, and prevents hacking the judge in LLM-as-a-Judge math tasks.",
        "url": "http://arxiv.org/abs/2602.18037v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18037v1",
        "arxiv_id": "2602.18037v1",
        "authors": [
            "Johannes Ackermann",
            "Michael Noukhovitch",
            "Takashi Ishida",
            "Masashi Sugiyama"
        ],
        "submitted": "2026-02-20 07:32:22",
        "source": "arxiv",
        "comment": "25 pages, 15 figures"
    },
    {
        "title": "Towards More Standardized AI Evaluation: From Models to Agents",
        "abstract": "Evaluation is no longer a final checkpoint in the machine learning lifecycle. As AI systems evolve from static models to compound, tool-using agents, evaluation becomes a core control function. The question is no longer \"How good is the model?\" but \"Can we trust the system to behave as intended, under change, at scale?\". Yet most evaluation practices remain anchored in assumptions inherited from the model-centric era: static benchmarks, aggregate scores, and one-off success criteria. This paper argues that such approaches are increasingly obscure rather than illuminating system behavior. We examine how evaluation pipelines themselves introduce silent failure modes, why high benchmark scores routinely mislead teams, and how agentic systems fundamentally alter the meaning of performance measurement. Rather than proposing new metrics or harder benchmarks, we aim to clarify the role of evaluation in the AI era, and especially for agents: not as performance theater, but as a measurement discipline that conditions trust, iteration, and governance in non-deterministic systems.",
        "url": "http://arxiv.org/abs/2602.18029v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18029v1",
        "arxiv_id": "2602.18029v1",
        "authors": [
            "Ali El Filali",
            "Inès Bedar"
        ],
        "submitted": "2026-02-20 06:54:44",
        "source": "arxiv",
        "comment": "19 pages, 3 figures"
    },
    {
        "title": "NIMMGen: Learning Neural-Integrated Mechanistic Digital Twins with LLMs",
        "abstract": "Mechanistic models encode scientific knowledge about dynamical systems and are widely used in downstream scientific and policy applications. Recent work has explored LLM-based agentic frameworks to automatically construct mechanistic models from data; however, existing problem settings substantially oversimplify real-world conditions, leaving it unclear whether LLM-generated mechanistic models are reliable in practice. To address this gap, we introduce the Neural-Integrated Mechanistic Modeling (NIMM) evaluation framework, which evaluates LLM-generated mechanistic models under realistic settings with partial observations and diversified task objectives. Our evaluation reveals fundamental challenges in current baselines, ranging from model effectiveness to code-level correctness. Motivated by these findings, we design NIMMgen, an agentic framework for neural-integrated mechanistic modeling that enhances code correctness and practical validity through iterative refinement. Experiments across three datasets from diversified scientific domains demonstrate its strong performance. We also show that the learned mechanistic models support counterfactual intervention simulation.",
        "url": "http://arxiv.org/abs/2602.18008v1",
        "pdf_url": "https://arxiv.org/pdf/2602.18008v1",
        "arxiv_id": "2602.18008v1",
        "authors": [
            "Zihan Guan",
            "Rituparna Datta",
            "Mengxuan Hu",
            "Shunshun Liu",
            "Aiying Zhang",
            "Prasanna Balachandran",
            "Sheng Li",
            "Anil Vullikanti"
        ],
        "submitted": "2026-02-20 05:46:54",
        "source": "arxiv",
        "comment": "19 pages, 6 figures"
    },
    {
        "title": "Decomposing Retrieval Failures in RAG for Long-Document Financial Question Answering",
        "abstract": "Retrieval-augmented generation is increasingly used for financial question answering over long regulatory filings, yet reliability depends on retrieving the exact context needed to justify answers in high stakes settings. We study a frequent failure mode in which the correct document is retrieved but the page or chunk that contains the answer is missed, leading the generator to extrapolate from incomplete context. Despite its practical significance, this within-document retrieval failure mode has received limited systematic attention in the Financial Question Answering (QA) literature. We evaluate retrieval at multiple levels of granularity, document, page, and chunk level, and introduce an oracle based analysis to provide empirical upper bounds on retrieval and generative performance. On a 150 question subset of FinanceBench, we reproduce and compare diverse retrieval strategies including dense, sparse, hybrid, and hierarchical methods with reranking and query reformulation. Across methods, gains in document discovery tend to translate into stronger page recall, yet oracle performance still suggests headroom for page and chunk level retrieval. To target this gap, we introduce a domain fine-tuned page scorer that treats pages as an intermediate retrieval unit between documents and chunks. Unlike prior passage-based hierarchical retrieval, we fine-tune a bi-encoder specifically for page-level relevance on financial filings, exploiting the semantic coherence of pages. Overall, our results demonstrate a significant improvement in page recall and chunk retrieval.",
        "url": "http://arxiv.org/abs/2602.17981v1",
        "pdf_url": "https://arxiv.org/pdf/2602.17981v1",
        "arxiv_id": "2602.17981v1",
        "authors": [
            "Amine Kobeissi",
            "Philippe Langlais"
        ],
        "submitted": "2026-02-20 04:31:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CUICurate: A GraphRAG-based Framework for Automated Clinical Concept Curation for NLP applications",
        "abstract": "Background: Clinical named entity recognition tools commonly map free text to Unified Medical Language System (UMLS) Concept Unique Identifiers (CUIs). For many downstream tasks, however, the clinically meaningful unit is not a single CUI but a concept set comprising related synonyms, subtypes, and supertypes. Constructing such concept sets is labour-intensive, inconsistently performed, and poorly supported by existing tools, particularly for NLP pipelines that operate directly on UMLS CUIs. Methods We present CUICurate, a Graph-based retrieval-augmented generation (GraphRAG) framework for automated UMLS concept set curation. A UMLS knowledge graph (KG) was constructed and embedded for semantic retrieval. For each target concept, candidate CUIs were retrieved from the KG, followed by large language model (LLM) filtering and classification steps comparing two LLMs (GPT-5 and GPT-5-mini). The framework was evaluated on five lexically heterogeneous clinical concepts against a manually curated benchmark and gold-standard concept sets. Results Across all concepts, CUICurate produced substantially larger and more complete concept sets than the manual benchmarks whilst matching human precision. Comparisons between the two LLMs found that GPT-5-mini achieved higher recall during filtering, while GPT-5 produced classifications that more closely aligned with clinician judgements. Outputs were stable across repeated runs and computationally inexpensive. Conclusions CUICurate offers a scalable and reproducible approach to support UMLS concept set curation that substantially reduces manual effort. By integrating graph-based retrieval with LLM reasoning, the framework produces focused candidate concept sets that can be adapted to clinical NLP pipelines for different phenotyping and analytic requirements.",
        "url": "http://arxiv.org/abs/2602.17949v1",
        "pdf_url": "https://arxiv.org/pdf/2602.17949v1",
        "arxiv_id": "2602.17949v1",
        "authors": [
            "Victoria Blake",
            "Mathew Miller",
            "Jamie Novak",
            "Sze-yuan Ooi",
            "Blanca Gallego"
        ],
        "submitted": "2026-02-20 03:00:13",
        "source": "arxiv",
        "comment": "30 pages, 6 figures, 4 tables"
    },
    {
        "title": "Analyzing LLM Instruction Optimization for Tabular Fact Verification",
        "abstract": "Instruction optimization provides a lightweight, model-agnostic approach to enhancing the reasoning performance of large language models (LLMs). This paper presents the first systematic comparison of instruction optimization, based on the DSPy optimization framework, for tabular fact verification. We evaluate four out-of-the-box prompting techniques that cover both text-only prompting and code use: direct prediction, Chain-of-Thought (CoT), ReAct with SQL tools, and CodeAct with Python execution. We study three optimizers from the DSPy framework -- COPRO, MiPROv2, and SIMBA -- across four benchmarks and three model families. We find that instruction optimization consistently improves verification accuracy, with MiPROv2 yielding the most stable gains for CoT, and SIMBA providing the largest benefits for ReAct agents, particularly at larger model scales. Behavioral analyses reveal that SIMBA encourages more direct reasoning paths by applying heuristics, thereby improving numerical comparison abilities in CoT reasoning and helping avoid unnecessary tool calls in ReAct agents. Across different prompting techniques, CoT remains effective for tabular fact checking, especially with smaller models. Although ReAct agents built with larger models can achieve competitive performance, they require careful instruction optimization.",
        "url": "http://arxiv.org/abs/2602.17937v1",
        "pdf_url": "https://arxiv.org/pdf/2602.17937v1",
        "arxiv_id": "2602.17937v1",
        "authors": [
            "Xiaotang Du",
            "Giwon Hong",
            "Wai-Chung Kwan",
            "Rohit Saxena",
            "Ivan Titov",
            "Pasquale Minervini",
            "Emily Allaway"
        ],
        "submitted": "2026-02-20 01:56:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Efficient Filtered-ANN via Learning-based Query Planning",
        "abstract": "Filtered ANN search is an increasingly important problem in vector retrieval, yet systems face a difficult trade-off due to the execution order: Pre-filtering (filtering first, then ANN over the passing subset) requires expensive per-predicate index construction, while post-filtering (ANN first, then filtering candidates) may waste computation and lose recall under low selectivity due to insufficient candidates after filtering. We introduce a learning-based query planning framework that dynamically selects the most effective execution plan for each query, using lightweight predictions derived from dataset and query statistics (e.g., dimensionality, corpus size, distribution features, and predicate statistics). The framework supports diverse filter types, including categorical/keyword and range predicates, and is generic to use any backend ANN index. Experiments show that our method achieves up to 4x acceleration with >= 90% recall comparing to the strong baselines.",
        "url": "http://arxiv.org/abs/2602.17914v1",
        "pdf_url": "https://arxiv.org/pdf/2602.17914v1",
        "arxiv_id": "2602.17914v1",
        "authors": [
            "Zhuocheng Gan",
            "Yifan Wang"
        ],
        "submitted": "2026-02-20 00:22:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Condition-Gated Reasoning for Context-Dependent Biomedical Question Answering",
        "abstract": "Current biomedical question answering (QA) systems often assume that medical knowledge applies uniformly, yet real-world clinical reasoning is inherently conditional: nearly every decision depends on patient-specific factors such as comorbidities and contraindications. Existing benchmarks do not evaluate such conditional reasoning, and retrieval-augmented or graph-based methods lack explicit mechanisms to ensure that retrieved knowledge is applicable to given context. To address this gap, we propose CondMedQA, the first benchmark for conditional biomedical QA, consisting of multi-hop questions whose answers vary with patient conditions. Furthermore, we propose Condition-Gated Reasoning (CGR), a novel framework that constructs condition-aware knowledge graphs and selectively activates or prunes reasoning paths based on query conditions. Our findings show that CGR more reliably selects condition-appropriate answers while matching or exceeding state-of-the-art performance on biomedical QA benchmarks, highlighting the importance of explicitly modeling conditionality for robust medical reasoning.",
        "url": "http://arxiv.org/abs/2602.17911v1",
        "pdf_url": "https://arxiv.org/pdf/2602.17911v1",
        "arxiv_id": "2602.17911v1",
        "authors": [
            "Jash Rajesh Parekh",
            "Wonbin Kweon",
            "Joey Chan",
            "Rezarta Islamaj",
            "Robert Leaman",
            "Pengcheng Jiang",
            "Chih-Hsuan Wei",
            "Zhizheng Wang",
            "Zhiyong Lu",
            "Jiawei Han"
        ],
        "submitted": "2026-02-20 00:17:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Improving Neural Topic Modeling with Semantically-Grounded Soft Label Distributions",
        "abstract": "Traditional neural topic models are typically optimized by reconstructing the document's Bag-of-Words (BoW) representations, overlooking contextual information and struggling with data sparsity. In this work, we propose a novel approach to construct semantically-grounded soft label targets using Language Models (LMs) by projecting the next token probabilities, conditioned on a specialized prompt, onto a pre-defined vocabulary to obtain contextually enriched supervision signals. By training the topic models to reconstruct the soft labels using the LM hidden states, our method produces higher-quality topics that are more closely aligned with the underlying thematic structure of the corpus. Experiments on three datasets show that our method achieves substantial improvements in topic coherence, purity over existing baselines. Additionally, we also introduce a retrieval-based metric, which shows that our approach significantly outperforms existing methods in identifying semantically similar documents, highlighting its effectiveness for retrieval-oriented applications.",
        "url": "http://arxiv.org/abs/2602.17907v1",
        "pdf_url": "https://arxiv.org/pdf/2602.17907v1",
        "arxiv_id": "2602.17907v1",
        "authors": [
            "Raymond Li",
            "Amirhossein Abaskohi",
            "Chuyuan Li",
            "Gabriel Murray",
            "Giuseppe Carenini"
        ],
        "submitted": "2026-02-20 00:12:04",
        "source": "arxiv",
        "comment": "20 pages, 5 figures"
    },
    {
        "title": "Games That Teach, Chats That Convince: Comparing Interactive and Static Formats for Persuasive Learning",
        "abstract": "Interactive systems such as chatbots and games are increasingly used to persuade and educate on sustainability-related topics, yet it remains unclear how different delivery formats shape learning and persuasive outcomes when content is held constant. Grounding on identical arguments and factual content across conditions, we present a controlled user study comparing three modes of information delivery: static essays, conversational chatbots, and narrative text-based games. Across subjective measures, the chatbot condition consistently outperformed the other modes and increased perceived importance of the topic. However, perceived learning did not reliably align with objective outcomes: participants in the text-based game condition reported learning less than those reading essays, yet achieved higher scores on a delayed (24-hour) knowledge quiz. Additional exploratory analyses further suggest that common engagement proxies, such as verbosity and interaction length, are more closely related to subjective experience than to actual learning. These findings highlight a dissociation between how persuasive experiences feel and what participants retain, and point to important design trade-offs between interactivity, realism, and learning in persuasive systems and serious games.",
        "url": "http://arxiv.org/abs/2602.17905v1",
        "pdf_url": "https://arxiv.org/pdf/2602.17905v1",
        "arxiv_id": "2602.17905v1",
        "authors": [
            "Seyed Hossein Alavi",
            "Zining Wang",
            "Shruthi Chockkalingam",
            "Raymond T. Ng",
            "Vered Shwartz"
        ],
        "submitted": "2026-02-20 00:07:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Understanding Unreliability of Steering Vectors in Language Models: Geometric Predictors and the Limits of Linear Approximations",
        "abstract": "Steering vectors are a lightweight method for controlling language model behavior by adding a learned bias to the activations at inference time. Although effective on average, steering effect sizes vary across samples and are unreliable for many target behaviors. In my thesis, I investigate why steering reliability differs across behaviors and how it is impacted by steering vector training data. First, I find that higher cosine similarity between training activation differences predicts more reliable steering. Second, I observe that behavior datasets where positive and negative activations are better separated along the steering direction are more reliably steerable. Finally, steering vectors trained on different prompt variations are directionally distinct, yet perform similarly well and exhibit correlated efficacy across datasets. My findings suggest that steering vectors are unreliable when the latent target behavior representation is not effectively approximated by the linear steering direction. Taken together, these insights offer a practical diagnostic for steering unreliability and motivate the development of more robust steering methods that explicitly account for non-linear latent behavior representations.",
        "url": "http://arxiv.org/abs/2602.17881v1",
        "pdf_url": "https://arxiv.org/pdf/2602.17881v1",
        "arxiv_id": "2602.17881v1",
        "authors": [
            "Joschka Braun"
        ],
        "submitted": "2026-02-19 22:37:05",
        "source": "arxiv",
        "comment": "Master's Thesis, University of Tübingen. 89 pages, 34 figures. Portions of this work were published at the ICLR 2025 Workshop on Foundation Models in the Wild (see arXiv:2505.22637)"
    },
    {
        "title": "ADAPT: Hybrid Prompt Optimization for LLM Feature Visualization",
        "abstract": "Understanding what features are encoded by learned directions in LLM activation space requires identifying inputs that strongly activate them. Feature visualization, which optimizes inputs to maximally activate a target direction, offers an alternative to costly dataset search approaches, but remains underexplored for LLMs due to the discrete nature of text. Furthermore, existing prompt optimization techniques are poorly suited to this domain, which is highly prone to local minima. To overcome these limitations, we introduce ADAPT, a hybrid method combining beam search initialization with adaptive gradient-guided mutation, designed around these failure modes. We evaluate on Sparse Autoencoder latents from Gemma 2 2B, proposing metrics grounded in dataset activation statistics to enable rigorous comparison, and show that ADAPT consistently outperforms prior methods across layers and latent types. Our results establish that feature visualization for LLMs is tractable, but requires design assumptions tailored to the domain.",
        "url": "http://arxiv.org/abs/2602.17867v1",
        "pdf_url": "https://arxiv.org/pdf/2602.17867v1",
        "arxiv_id": "2602.17867v1",
        "authors": [
            "João N. Cardoso",
            "Arlindo L. Oliveira",
            "Bruno Martins"
        ],
        "submitted": "2026-02-19 22:03:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Enhancing Scientific Literature Chatbots with Retrieval-Augmented Generation: A Performance Evaluation of Vector and Graph-Based Systems",
        "abstract": "This paper investigates the enhancement of scientific literature chatbots through retrieval-augmented generation (RAG), with a focus on evaluating vector- and graph-based retrieval systems. The proposed chatbot leverages both structured (graph) and unstructured (vector) databases to access scientific articles and gray literature, enabling efficient triage of sources according to research objectives. To systematically assess performance, we examine two use-case scenarios: retrieval from a single uploaded document and retrieval from a large-scale corpus. Benchmark test sets were generated using a GPT model, with selected outputs annotated for evaluation. The comparative analysis emphasizes retrieval accuracy and response relevance, providing insight into the strengths and limitations of each approach. The findings demonstrate the potential of hybrid RAG systems to improve accessibility to scientific knowledge and to support evidence-based decision making.",
        "url": "http://arxiv.org/abs/2602.17856v1",
        "pdf_url": "https://arxiv.org/pdf/2602.17856v1",
        "arxiv_id": "2602.17856v1",
        "authors": [
            "Hamideh Ghanadian",
            "Amin Kamali",
            "Mohammad Hossein Tekieh"
        ],
        "submitted": "2026-02-19 21:42:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Mind the Style: Impact of Communication Style on Human-Chatbot Interaction",
        "abstract": "Conversational agents increasingly mediate everyday digital interactions, yet the effects of their communication style on user experience and task success remain unclear. Addressing this gap, we describe the results of a between-subject user study where participants interact with one of two versions of a chatbot called NAVI which assists users in an interactive map-based 2D navigation task. The two chatbot versions differ only in communication style: one is friendly and supportive, while the other is direct and task-focused. Our results show that the friendly style increases subjective satisfaction and significantly improves task completion rates among female participants only, while no baseline differences between female and male participants were observed in a control condition without the chatbot. Furthermore, we find little evidence of users mimicking the chatbot's style, suggesting limited linguistic accommodation. These findings highlight the importance of user- and task-sensitive conversational agents and support that communication style personalization can meaningfully enhance interaction quality and performance.",
        "url": "http://arxiv.org/abs/2602.17850v1",
        "pdf_url": "https://arxiv.org/pdf/2602.17850v1",
        "arxiv_id": "2602.17850v1",
        "authors": [
            "Erik Derner",
            "Dalibor Kučera",
            "Aditya Gulati",
            "Ayoub Bagheri",
            "Nuria Oliver"
        ],
        "submitted": "2026-02-19 21:32:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "On the scaling relationship between cloze probabilities and language model next-token prediction",
        "abstract": "Recent work has shown that larger language models have better predictive power for eye movement and reading time data. While even the best models under-allocate probability mass to human responses, larger models assign higher-quality estimates of next tokens and their likelihood of production in cloze data because they are less sensitive to lexical co-occurrence statistics while being better aligned semantically to human cloze responses. The results provide support for the claim that the greater memorization capacity of larger models helps them guess more semantically appropriate words, but makes them less sensitive to low-level information that is relevant for word recognition.",
        "url": "http://arxiv.org/abs/2602.17848v1",
        "pdf_url": "https://arxiv.org/pdf/2602.17848v1",
        "arxiv_id": "2602.17848v1",
        "authors": [
            "Cassandra L. Jacobs",
            "Morgan Grobol"
        ],
        "submitted": "2026-02-19 21:29:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TFL: Targeted Bit-Flip Attack on Large Language Model",
        "abstract": "Large language models (LLMs) are increasingly deployed in safety and security critical applications, raising concerns about their robustness to model parameter fault injection attacks. Recent studies have shown that bit-flip attacks (BFAs), which exploit computer main memory (i.e., DRAM) vulnerabilities to flip a small number of bits in model weights, can severely disrupt LLM behavior. However, existing BFA on LLM largely induce un-targeted failure or general performance degradation, offering limited control over manipulating specific or targeted outputs. In this paper, we present TFL, a novel targeted bit-flip attack framework that enables precise manipulation of LLM outputs for selected prompts while maintaining almost no or minor degradation on unrelated inputs. Within our TFL framework, we propose a novel keyword-focused attack loss to promote attacker-specified target tokens in generative outputs, together with an auxiliary utility score that balances attack effectiveness against collateral performance impact on benign data. We evaluate TFL on multiple LLMs (Qwen, DeepSeek, Llama) and benchmarks (DROP, GSM8K, and TriviaQA). The experiments show that TFL achieves successful targeted LLM output manipulations with less than 50 bit flips and significantly reduced effect on unrelated queries compared to prior BFA approaches. This demonstrates the effectiveness of TFL and positions it as a new class of stealthy and targeted LLM model attack.",
        "url": "http://arxiv.org/abs/2602.17837v1",
        "pdf_url": "https://arxiv.org/pdf/2602.17837v1",
        "arxiv_id": "2602.17837v1",
        "authors": [
            "Jingkai Guo",
            "Chaitali Chakrabarti",
            "Deliang Fan"
        ],
        "submitted": "2026-02-19 20:59:47",
        "source": "arxiv",
        "comment": "13 pages, 11 figures. Preprint"
    },
    {
        "title": "Neural Synchrony Between Socially Interacting Language Models",
        "abstract": "Neuroscience has uncovered a fundamental mechanism of our social nature: human brain activity becomes synchronized with others in many social contexts involving interaction. Traditionally, social minds have been regarded as an exclusive property of living beings. Although large language models (LLMs) are widely accepted as powerful approximations of human behavior, with multi-LLM system being extensively explored to enhance their capabilities, it remains controversial whether they can be meaningfully compared to human social minds. In this work, we explore neural synchrony between socially interacting LLMs as an empirical evidence for this debate. Specifically, we introduce neural synchrony during social simulations as a novel proxy for analyzing the sociality of LLMs at the representational level. Through carefully designed experiments, we demonstrate that it reliably reflects both social engagement and temporal alignment in their interactions. Our findings indicate that neural synchrony between LLMs is strongly correlated with their social performance, highlighting an important link between neural synchrony and the social behaviors of LLMs. Our work offers a new perspective to examine the \"social minds\" of LLMs, highlighting surprising parallels in the internal dynamics that underlie human and LLM social interaction.",
        "url": "http://arxiv.org/abs/2602.17815v1",
        "pdf_url": "https://arxiv.org/pdf/2602.17815v1",
        "arxiv_id": "2602.17815v1",
        "authors": [
            "Zhining Zhang",
            "Wentao Zhu",
            "Chi Han",
            "Yizhou Wang",
            "Heng Ji"
        ],
        "submitted": "2026-02-19 20:33:54",
        "source": "arxiv",
        "comment": "Accepted at ICLR 2026"
    },
    {
        "title": "VQPP: Video Query Performance Prediction Benchmark",
        "abstract": "Query performance prediction (QPP) is an important and actively studied information retrieval task, having various applications, such as query reformulation, query expansion, and retrieval system selection, among many others. The task has been primarily studied in the context of text and image retrieval, whereas QPP for content-based video retrieval (CBVR) remains largely underexplored. To this end, we propose the first benchmark for video query performance prediction (VQPP), comprising two text-to-video retrieval datasets and two CBVR systems, respectively. VQPP contains a total of 56K text queries and 51K videos, and comes with official training, validation and test splits, fostering direct comparisons and reproducible results. We explore multiple pre-retrieval and post-retrieval performance predictors, creating a representative benchmark for future exploration of QPP in the video domain. Our results show that pre-retrieval predictors obtain competitive performance, enabling applications before performing the retrieval step. We also demonstrate the applicability of VQPP by employing the best performing pre-retrieval predictor as reward model for training a large language model (LLM) on the query reformulation task via direct preference optimization (DPO). We release our benchmark and code at https://github.com/AdrianLutu/VQPP.",
        "url": "http://arxiv.org/abs/2602.17814v1",
        "pdf_url": "https://arxiv.org/pdf/2602.17814v1",
        "arxiv_id": "2602.17814v1",
        "authors": [
            "Adrian Catalin Lutu",
            "Eduard Poesina",
            "Radu Tudor Ionescu"
        ],
        "submitted": "2026-02-19 20:32:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "QueryPlot: Generating Geological Evidence Layers using Natural Language Queries for Mineral Exploration",
        "abstract": "Mineral prospectivity mapping requires synthesizing heterogeneous geological knowledge, including textual deposit models and geospatial datasets, to identify regions likely to host specific mineral deposit types. This process is traditionally manual and knowledge-intensive. We present QueryPlot, a semantic retrieval and mapping framework that integrates large-scale geological text corpora with geologic map data using modern Natural Language Processing techniques. We curate descriptive deposit models for over 120 deposit types and transform the State Geologic Map Compilation (SGMC) polygons into structured textual representations. Given a user-defined natural language query, the system encodes both queries and region descriptions using a pretrained embedding model and computes semantic similarity scores to rank and spatially visualize regions as continuous evidence layers. QueryPlot supports compositional querying over deposit characteristics, enabling aggregation of multiple similarity-derived layers for multi-criteria prospectivity analysis. In a case study on tungsten skarn deposits, we demonstrate that embedding-based retrieval achieves high recall of known occurrences and produces prospective regions that closely align with expert-defined permissive tracts. Furthermore, similarity scores can be incorporated as additional features in supervised learning pipelines, yielding measurable improvements in classification performance. QueryPlot is implemented as a web-based system supporting interactive querying, visualization, and export of GIS-compatible prospectivity layers.To support future research, we have made the source code and datasets used in this study publicly available.",
        "url": "http://arxiv.org/abs/2602.17784v1",
        "pdf_url": "https://arxiv.org/pdf/2602.17784v1",
        "arxiv_id": "2602.17784v1",
        "authors": [
            "Meng Ye",
            "Xiao Lin",
            "Georgina Lukoczki",
            "Graham W. Lederer",
            "Yi Yao"
        ],
        "submitted": "2026-02-19 19:31:37",
        "source": "arxiv",
        "comment": null
    }
]