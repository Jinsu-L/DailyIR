[
    {
        "title": "Paper2Video: Automatic Video Generation from Scientific Papers",
        "abstract": "Academic presentation videos have become an essential medium for research\ncommunication, yet producing them remains highly labor-intensive, often\nrequiring hours of slide design, recording, and editing for a short 2 to 10\nminutes video. Unlike natural video, presentation video generation involves\ndistinctive challenges: inputs from research papers, dense multi-modal\ninformation (text, figures, tables), and the need to coordinate multiple\naligned channels such as slides, subtitles, speech, and human talker. To\naddress these challenges, we introduce PaperTalker, the first benchmark of 101\nresearch papers paired with author-created presentation videos, slides, and\nspeaker metadata. We further design four tailored evaluation metrics--Meta\nSimilarity, PresentArena, PresentQuiz, and IP Memory--to measure how videos\nconvey the paper's information to the audience. Building on this foundation, we\npropose PaperTalker, the first multi-agent framework for academic presentation\nvideo generation. It integrates slide generation with effective layout\nrefinement by a novel effective tree search visual choice, cursor grounding,\nsubtitling, speech synthesis, and talking-head rendering, while parallelizing\nslide-wise generation for efficiency. Experiments on Paper2Video demonstrate\nthat the presentation videos produced by our approach are more faithful and\ninformative than existing baselines, establishing a practical step toward\nautomated and ready-to-use academic video generation. Our dataset, agent, and\ncode are available at https://github.com/showlab/Paper2Video.",
        "url": "http://arxiv.org/abs/2510.05096v1",
        "pdf_url": "http://arxiv.org/pdf/2510.05096v1",
        "arxiv_id": "2510.05096v1",
        "authors": [
            "Zeyu Zhu",
            "Kevin Qinghong Lin",
            "Mike Zheng Shou"
        ],
        "submitted": "2025-10-06 17:58:02",
        "source": "arxiv",
        "comment": "20 pages, 8 figures"
    },
    {
        "title": "From Noisy Traces to Stable Gradients: Bias-Variance Optimized Preference Optimization for Aligning Large Reasoning Models",
        "abstract": "Large reasoning models (LRMs) generate intermediate reasoning traces before\nproducing final answers, yielding strong gains on multi-step and mathematical\ntasks. Yet aligning LRMs with human preferences, a crucial prerequisite for\nmodel deployment, remains underexplored. The statistically correct objective\nfor preference alignment requires marginalizing over reasoning traces, but this\ncomputation is intractable in practice. A common workaround optimizes a single\nsampled trajectory, which introduces substantial gradient variance from\nstochastic trace sampling. To address this challenge, we frame preference\noptimization for LRMs through the lens of the bias--variance trade-off and\npropose Bias--Variance Optimized Preference Optimization (BVPO), a simple,\ndrop-in method that mixes two gradient estimators: a high-variance trace-based\nestimator and a low-variance empty-trace estimator obtained by disabling\nreasoning trace generation. Our theory shows that BVPO strictly reduces\ntrace-induced variance for any nontrivial mixture, provides a closed-form\nchoice of the mixing weight that minimizes mean-squared error relative to the\ntrue marginal gradient, and under standard smoothness and step-size conditions,\ntightens classical convergence bounds for stochastic gradient descent.\nEmpirically, BVPO improves alignment over the best baseline by up to 7.8 points\non AlpacaEval~2 and 6.8 points on Arena-Hard. Despite being trained only on\ngeneral conversational data, BVPO also boosts reasoning performance for base\nmodels by up to 4.0 points on the average of six math reasoning benchmarks.\nThese results identify variance from trace sampling as a key bottleneck and\ndemonstrate that directly optimizing the bias--variance trade-off yields more\nstable training and stronger overall performance.",
        "url": "http://arxiv.org/abs/2510.05095v1",
        "pdf_url": "http://arxiv.org/pdf/2510.05095v1",
        "arxiv_id": "2510.05095v1",
        "authors": [
            "Mingkang Zhu",
            "Xi Chen",
            "Bei Yu",
            "Hengshuang Zhao",
            "Jiaya Jia"
        ],
        "submitted": "2025-10-06 17:58:01",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Learning to Interpret Weight Differences in Language Models",
        "abstract": "Finetuning (pretrained) language models is a standard approach for updating\ntheir internal parametric knowledge and specializing them to new tasks and\ndomains. However, the corresponding model weight changes (\"weight diffs\") are\nnot generally interpretable. While inspecting the finetuning dataset can give a\nsense of how the model might have changed, these datasets are often not\npublicly available or are too large to work with directly. Towards the goal of\ncomprehensively understanding weight diffs in natural language, we introduce\nDiff Interpretation Tuning (DIT), a method that trains models to describe their\nown finetuning-induced modifications. Our approach uses synthetic, labeled\nweight diffs to train a DIT adapter, which can be applied to a compatible\nfinetuned model to make it describe how it has changed. We demonstrate in two\nproof-of-concept settings (reporting hidden behaviors and summarizing finetuned\nknowledge) that our method enables models to describe their finetuning-induced\nmodifications using accurate natural language descriptions.",
        "url": "http://arxiv.org/abs/2510.05092v1",
        "pdf_url": "http://arxiv.org/pdf/2510.05092v1",
        "arxiv_id": "2510.05092v1",
        "authors": [
            "Avichal Goel",
            "Yoon Kim",
            "Nir Shavit",
            "Tony T. Wang"
        ],
        "submitted": "2025-10-06 17:57:23",
        "source": "arxiv",
        "comment": "The weight diffs and DIT adapters trained in the paper can be found\n  at https://huggingface.co/diff-interpretation-tuning/loras"
    },
    {
        "title": "Finish First, Perfect Later: Test-Time Token-Level Cross-Validation for Diffusion Large Language Models",
        "abstract": "Diffusion large language models (dLLMs) have recently emerged as a promising\nalternative to autoregressive (AR) models, offering advantages such as\naccelerated parallel decoding and bidirectional context modeling. However, the\nvanilla decoding strategy in discrete dLLMs suffers from a critical limitation:\nonce a token is accepted, it can no longer be revised in subsequent steps. As a\nresult, early mistakes persist across iterations, harming both intermediate\npredictions and final output quality. To address this issue, we propose\nTolerator (Token-Level Cross-Validation Refinement), a training-free decoding\nstrategy that leverages cross-validation among predicted tokens. Unlike\nexisting methods that follow a single progressive unmasking procedure,\nTolerator introduces a two-stage process: (i) sequence fill-up and (ii)\niterative refinement by remasking and decoding a subset of tokens while\ntreating the remaining as context. This design enables previously accepted\ntokens to be reconsidered and corrected when necessary, leading to more\nreliable diffusion decoding outputs. We evaluate Tolerator on five standard\nbenchmarks covering language understanding, code generation, and mathematics.\nExperiments show that our method achieves consistent improvements over the\nbaselines under the same computational budget. These findings suggest that\ndecoding algorithms are crucial to realizing the full potential of diffusion\nlarge language models. Code and data are publicly available.",
        "url": "http://arxiv.org/abs/2510.05090v1",
        "pdf_url": "http://arxiv.org/pdf/2510.05090v1",
        "arxiv_id": "2510.05090v1",
        "authors": [
            "Runchu Tian",
            "Junxia Cui",
            "Xueqiang Xu",
            "Feng Yao",
            "Jingbo Shang"
        ],
        "submitted": "2025-10-06 17:56:46",
        "source": "arxiv",
        "comment": "17 pages, 8 figures. Work in progress"
    },
    {
        "title": "TeachLM: Post-Training LLMs for Education Using Authentic Learning Data",
        "abstract": "The promise of generative AI to revolutionize education is constrained by the\npedagogical limits of large language models (LLMs). A major issue is the lack\nof access to high-quality training data that reflect the learning of actual\nstudents. Prompt engineering has emerged as a stopgap, but the ability of\nprompts to encode complex pedagogical strategies in rule-based natural language\nis inherently limited. To address this gap we introduce TeachLM - an LLM\noptimized for teaching through parameter-efficient fine-tuning of\nstate-of-the-art models. TeachLM is trained on a dataset comprised of 100,000\nhours of one-on-one, longitudinal student-tutor interactions maintained by\nPolygence, which underwent a rigorous anonymization process to protect privacy.\nWe use parameter-efficient fine-tuning to develop an authentic student model\nthat enables the generation of high-fidelity synthetic student-tutor dialogues.\nBuilding on this capability, we propose a novel multi-turn evaluation protocol\nthat leverages synthetic dialogue generation to provide fast, scalable, and\nreproducible assessments of the dialogical capabilities of LLMs. Our\nevaluations demonstrate that fine-tuning on authentic learning data\nsignificantly improves conversational and pedagogical performance - doubling\nstudent talk time, improving questioning style, increasing dialogue turns by\n50%, and greater personalization of instruction.",
        "url": "http://arxiv.org/abs/2510.05087v1",
        "pdf_url": "http://arxiv.org/pdf/2510.05087v1",
        "arxiv_id": "2510.05087v1",
        "authors": [
            "Janos Perczel",
            "Jin Chow",
            "Dorottya Demszky"
        ],
        "submitted": "2025-10-06 17:55:04",
        "source": "arxiv",
        "comment": "28 pages, 9 figures"
    },
    {
        "title": "Slm-mux: Orchestrating small language models for reasoning",
        "abstract": "With the rapid development of language models, the number of small language\nmodels (SLMs) has grown significantly. Although they do not achieve\nstate-of-the-art accuracy, they are more efficient and often excel at specific\ntasks. This raises a natural question: can multiple SLMs be orchestrated into a\nsystem where each contributes effectively, achieving higher accuracy than any\nindividual model? Existing orchestration methods have primarily targeted\nfrontier models (e.g., GPT-4) and perform suboptimally when applied to SLMs. To\naddress this gap, we propose a three-stage approach for orchestrating SLMs.\nFirst, we introduce SLM-MUX, a multi-model architecture that effectively\ncoordinates multiple SLMs. Building on this, we develop two optimization\nstrategies: (i) a model selection search that identifies the most complementary\nSLMs from a given pool, and (ii) test-time scaling tailored to SLM-MUX. Our\napproach delivers strong results: Compared to existing orchestration methods,\nour approach achieves up to 13.4% improvement on MATH, 8.8% on GPQA, and 7.0%\non GSM8K. With just two SLMS, SLM-MUX outperforms Qwen 2.5 72B on GPQA and\nGSM8K, and matches its performance on MATH. We further provide theoretical\nanalyses to substantiate the advantages of our method. In summary, we\ndemonstrate that SLMs can be effectively orchestrated into more accurate and\nefficient systems through the proposed approach.",
        "url": "http://arxiv.org/abs/2510.05077v1",
        "pdf_url": "http://arxiv.org/pdf/2510.05077v1",
        "arxiv_id": "2510.05077v1",
        "authors": [
            "Chenyu Wang",
            "Zishen Wan",
            "Hao Kang",
            "Emma Chen",
            "Zhiqiang Xie",
            "Tushar Krishna",
            "Vijay Janapa Reddi",
            "Yilun Du"
        ],
        "submitted": "2025-10-06 17:49:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SwiReasoning: Switch-Thinking in Latent and Explicit for Pareto-Superior Reasoning LLMs",
        "abstract": "Recent work shows that, beyond discrete reasoning through explicit\nchain-of-thought steps, which are limited by the boundaries of natural\nlanguages, large language models (LLMs) can also reason continuously in latent\nspace, allowing richer information per step and thereby improving token\nefficiency. Despite this promise, latent reasoning still faces two challenges,\nespecially in training-free settings: 1) purely latent reasoning broadens the\nsearch distribution by maintaining multiple implicit paths, which diffuses\nprobability mass, introduces noise, and impedes convergence to a single\nhigh-confidence solution, thereby hurting accuracy; and 2) overthinking\npersists even without explicit text, wasting tokens and degrading efficiency.\nTo address these issues, we introduce SwiReasoning, a training-free framework\nfor LLM reasoning which features two key innovations: 1) SwiReasoning\ndynamically switches between explicit and latent reasoning, guided by\nblock-wise confidence estimated from entropy trends in next-token\ndistributions, to balance exploration and exploitation and promote timely\nconvergence. 2) By limiting the maximum number of thinking-block switches,\nSwiReasoning curbs overthinking and improves token efficiency across varying\nproblem difficulties. On widely used mathematics and STEM benchmarks,\nSwiReasoning consistently improves average accuracy by 1.5%-2.8% across\nreasoning LLMs of different model families and scales. Furthermore, under\nconstrained budgets, SwiReasoning improves average token efficiency by 56%-79%,\nwith larger gains as budgets tighten.",
        "url": "http://arxiv.org/abs/2510.05069v1",
        "pdf_url": "http://arxiv.org/pdf/2510.05069v1",
        "arxiv_id": "2510.05069v1",
        "authors": [
            "Dachuan Shi",
            "Abedelkadir Asi",
            "Keying Li",
            "Xiangchi Yuan",
            "Leyan Pan",
            "Wenke Lee",
            "Wen Xiao"
        ],
        "submitted": "2025-10-06 17:46:34",
        "source": "arxiv",
        "comment": "Code: https://github.com/sdc17/SwiReasoning, Website:\n  https://swireasoning.github.io/"
    },
    {
        "title": "Proactive defense against LLM Jailbreak",
        "abstract": "The proliferation of powerful large language models (LLMs) has necessitated\nrobust safety alignment, yet these models remain vulnerable to evolving\nadversarial attacks, including multi-turn jailbreaks that iteratively search\nfor successful queries. Current defenses, primarily reactive and static, often\nfail to counter these search-based attacks. In this paper, we introduce ProAct,\na novel proactive defense framework designed to disrupt and mislead autonomous\njailbreaking processes. Our core idea is to intentionally provide adversaries\nwith \"spurious responses\" that appear to be results of successful jailbreak\nattacks but contain no actual harmful content. These misleading responses\nprovide false signals to the attacker's internal optimization loop, causing the\nadversarial search to terminate prematurely and effectively jailbreaking the\njailbreak. By conducting extensive experiments across state-of-the-art LLMs,\njailbreaking frameworks, and safety benchmarks, our method consistently and\nsignificantly reduces attack success rates by up to 92\\%. When combined with\nother defense frameworks, it further reduces the success rate of the latest\nattack strategies to 0\\%. ProAct represents an orthogonal defense strategy that\ncan serve as an additional guardrail to enhance LLM safety against the most\neffective jailbreaking attacks.",
        "url": "http://arxiv.org/abs/2510.05052v1",
        "pdf_url": "http://arxiv.org/pdf/2510.05052v1",
        "arxiv_id": "2510.05052v1",
        "authors": [
            "Weiliang Zhao",
            "Jinjun Peng",
            "Daniel Ben-Levi",
            "Zhou Yu",
            "Junfeng Yang"
        ],
        "submitted": "2025-10-06 17:32:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "COLE: a Comprehensive Benchmark for French Language Understanding Evaluation",
        "abstract": "To address the need for a more comprehensive evaluation of French Natural\nLanguage Understanding (NLU), we introduce COLE, a new benchmark composed of 23\ndiverse task covering a broad range of NLU capabilities, including sentiment\nanalysis, paraphrase detection, grammatical judgment, and reasoning, with a\nparticular focus on linguistic phenomena relevant to the French language. We\nbenchmark 94 large language models (LLM), providing an extensive analysis of\nthe current state of French NLU. Our results highlight a significant\nperformance gap between closed- and open-weights models and identify key\nchallenging frontiers for current LLMs, such as zero-shot extractive\nquestion-answering (QA), fine-grained word sense disambiguation, and\nunderstanding of regional language variations. We release COLE as a public\nresource to foster further progress in French language modelling.",
        "url": "http://arxiv.org/abs/2510.05046v2",
        "pdf_url": "http://arxiv.org/pdf/2510.05046v2",
        "arxiv_id": "2510.05046v2",
        "authors": [
            "David Beauchemin",
            "Yan Tremblay",
            "Mohamed Amine Youssef",
            "Richard Khoury"
        ],
        "submitted": "2025-10-06 17:26:41",
        "source": "arxiv",
        "comment": "Submitted to ACL Rolling Review of October"
    },
    {
        "title": "Guided Query Refinement: Multimodal Hybrid Retrieval with Test-Time Optimization",
        "abstract": "Multimodal encoders have pushed the boundaries of visual document retrieval,\nmatching textual query tokens directly to image patches and achieving\nstate-of-the-art performance on public benchmarks. Recent models relying on\nthis paradigm have massively scaled the sizes of their query and document\nrepresentations, presenting obstacles to deployment and scalability in\nreal-world pipelines. Furthermore, purely vision-centric approaches may be\nconstrained by the inherent modality gap still exhibited by modern\nvision-language models. In this work, we connect these challenges to the\nparadigm of hybrid retrieval, investigating whether a lightweight dense text\nretriever can enhance a stronger vision-centric model. Existing hybrid methods,\nwhich rely on coarse-grained fusion of ranks or scores, fail to exploit the\nrich interactions within each model's representation space. To address this, we\nintroduce Guided Query Refinement (GQR), a novel test-time optimization method\nthat refines a primary retriever's query embedding using guidance from a\ncomplementary retriever's scores. Through extensive experiments on visual\ndocument retrieval benchmarks, we demonstrate that GQR allows vision-centric\nmodels to match the performance of models with significantly larger\nrepresentations, while being up to 14x faster and requiring 54x less memory.\nOur findings show that GQR effectively pushes the Pareto frontier for\nperformance and efficiency in multimodal retrieval. We release our code at\nhttps://github.com/IBM/test-time-hybrid-retrieval",
        "url": "http://arxiv.org/abs/2510.05038v1",
        "pdf_url": "http://arxiv.org/pdf/2510.05038v1",
        "arxiv_id": "2510.05038v1",
        "authors": [
            "Omri Uzan",
            "Asaf Yehudai",
            "Roi pony",
            "Eyal Shnarch",
            "Ariel Gera"
        ],
        "submitted": "2025-10-06 17:12:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Set of Quebec-French Corpus of Regional Expressions and Terms",
        "abstract": "The tasks of idiom understanding and dialect understanding are both\nwell-established benchmarks in natural language processing. In this paper, we\npropose combining them, and using regional idioms as a test of dialect\nunderstanding. Towards this end, we propose two new benchmark datasets for the\nQuebec dialect of French: QFrCoRE, which contains 4,633 instances of idiomatic\nphrases, and QFrCoRT, which comprises 171 regional instances of idiomatic\nwords. We explain how to construct these corpora, so that our methodology can\nbe replicated for other dialects. Our experiments with 94 LLM demonstrate that\nour regional idiom benchmarks are a reliable tool for measuring a model's\nproficiency in a specific dialect.",
        "url": "http://arxiv.org/abs/2510.05026v1",
        "pdf_url": "http://arxiv.org/pdf/2510.05026v1",
        "arxiv_id": "2510.05026v1",
        "authors": [
            "David Beauchemin",
            "Yan Tremblay",
            "Mohamed Amine Youssef",
            "Richard Khoury"
        ],
        "submitted": "2025-10-06 17:04:22",
        "source": "arxiv",
        "comment": "Submitted to ACL Rolling Review of October"
    },
    {
        "title": "Imperceptible Jailbreaking against Large Language Models",
        "abstract": "Jailbreaking attacks on the vision modality typically rely on imperceptible\nadversarial perturbations, whereas attacks on the textual modality are\ngenerally assumed to require visible modifications (e.g., non-semantic\nsuffixes). In this paper, we introduce imperceptible jailbreaks that exploit a\nclass of Unicode characters called variation selectors. By appending invisible\nvariation selectors to malicious questions, the jailbreak prompts appear\nvisually identical to original malicious questions on screen, while their\ntokenization is \"secretly\" altered. We propose a chain-of-search pipeline to\ngenerate such adversarial suffixes to induce harmful responses. Our experiments\nshow that our imperceptible jailbreaks achieve high attack success rates\nagainst four aligned LLMs and generalize to prompt injection attacks, all\nwithout producing any visible modifications in the written prompt. Our code is\navailable at https://github.com/sail-sg/imperceptible-jailbreaks.",
        "url": "http://arxiv.org/abs/2510.05025v1",
        "pdf_url": "http://arxiv.org/pdf/2510.05025v1",
        "arxiv_id": "2510.05025v1",
        "authors": [
            "Kuofeng Gao",
            "Yiming Li",
            "Chao Du",
            "Xin Wang",
            "Xingjun Ma",
            "Shu-Tao Xia",
            "Tianyu Pang"
        ],
        "submitted": "2025-10-06 17:03:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Large Language Models Achieve Gold Medal Performance at the International Olympiad on Astronomy & Astrophysics (IOAA)",
        "abstract": "While task-specific demonstrations show early success in applying large\nlanguage models (LLMs) to automate some astronomical research tasks, they only\nprovide incomplete views of all necessary capabilities in solving astronomy\nproblems, calling for more thorough understanding of LLMs' strengths and\nlimitations. So far, existing benchmarks and evaluations focus on simple\nquestion-answering that primarily tests astronomical knowledge and fails to\nevaluate the complex reasoning required for real-world research in the\ndiscipline. Here, we address this gap by systematically benchmarking five\nstate-of-the-art LLMs on the International Olympiad on Astronomy and\nAstrophysics (IOAA) exams, which are designed to examine deep conceptual\nunderstanding, multi-step derivations, and multimodal analysis. With average\nscores of 85.6% and 84.2%, Gemini 2.5 Pro and GPT-5 (the two top-performing\nmodels) not only achieve gold medal level performance but also rank in the top\ntwo among ~200-300 participants in all four IOAA theory exams evaluated\n(2022-2025). In comparison, results on the data analysis exams show more\ndivergence. GPT-5 still excels in the exams with an 88.5% average score,\nranking top 10 among the participants in the four most recent IOAAs, while\nother models' performances drop to 48-76%. Furthermore, our in-depth error\nanalysis underscores conceptual reasoning, geometric reasoning, and spatial\nvisualization (52-79% accuracy) as consistent weaknesses among all LLMs. Hence,\nalthough LLMs approach peak human performance in theory exams, critical gaps\nmust be addressed before they can serve as autonomous research agents in\nastronomy.",
        "url": "http://arxiv.org/abs/2510.05016v2",
        "pdf_url": "http://arxiv.org/pdf/2510.05016v2",
        "arxiv_id": "2510.05016v2",
        "authors": [
            "Lucas Carrit Delgado Pinheiro",
            "Ziru Chen",
            "Bruno Caixeta Piazza",
            "Ness Shroff",
            "Yingbin Liang",
            "Yuan-Sen Ting",
            "Huan Sun"
        ],
        "submitted": "2025-10-06 16:58:47",
        "source": "arxiv",
        "comment": "18 pages, 6 figures, to be submitted, comments are welcome.\n  Reproducibility details can be found at:\n  https://github.com/OSU-NLP-Group/LLM-IOAA"
    },
    {
        "title": "Resource-Efficient Fine-Tuning of LLaMA-3.2-3B for Medical Chain-of-Thought Reasoning",
        "abstract": "Large Language Models (LLMs) such as GPT-4 and LLaMA have demonstrated\nremarkable reasoning abilities but require significant computational resources\nfor fine-tuning. This paper presents a resource-efficient fine-tuning approach\nfor LLaMA-3.2-3B to enhance medical chain-of-thought reasoning while operating\nunder constrained GPU and memory settings. Using parameter-efficient tuning\ntechniques such as LoRA and QLoRA, we adapt the base model on publicly\navailable medical reasoning datasets. The model achieves improved reasoning\ncoherence and factual accuracy while reducing memory usage by up to 60%\ncompared to standard full fine-tuning. Experimental evaluation demonstrates\nthat lightweight adaptations can retain strong reasoning capability in medical\nquestion-answering tasks. This work highlights practical strategies for\ndeploying LLMs in low-resource research environments and provides insights into\nbalancing efficiency and domain specialization for medical AI systems.",
        "url": "http://arxiv.org/abs/2510.05003v1",
        "pdf_url": "http://arxiv.org/pdf/2510.05003v1",
        "arxiv_id": "2510.05003v1",
        "authors": [
            "Imran Mansha"
        ],
        "submitted": "2025-10-06 16:42:11",
        "source": "arxiv",
        "comment": "6 pages, 2 figures. Submitted to arXiv for open access"
    },
    {
        "title": "Reinforce-Ada: An Adaptive Sampling Framework for Reinforce-Style LLM Training",
        "abstract": "Reinforcement learning applied to large language models (LLMs) for reasoning\ntasks is often bottlenecked by unstable gradient estimates due to fixed and\nuniform sampling of responses across prompts. Prior work such as GVM-RAFT\naddresses this by dynamically allocating inference budget per prompt to\nminimize stochastic gradient variance under a budget constraint. Inspired by\nthis insight, we propose Reinforce-Ada, an adaptive sampling framework for\nonline RL post-training of LLMs that continuously reallocates sampling effort\nto the prompts with the greatest uncertainty or learning potential. Unlike\nconventional two-stage allocation methods, Reinforce-Ada interleaves estimation\nand sampling in an online successive elimination process, and automatically\nstops sampling for a prompt once sufficient signal is collected. To stabilize\nupdates, we form fixed-size groups with enforced reward diversity and compute\nadvantage baselines using global statistics aggregated over the adaptive\nsampling phase. Empirical results across multiple model architectures and\nreasoning benchmarks show that Reinforce-Ada accelerates convergence and\nimproves final performance compared to GRPO, especially when using the balanced\nsampling variant. Our work highlights the central role of variance-aware,\nadaptive data curation in enabling efficient and reliable reinforcement\nlearning for reasoning-capable LLMs. Code is available at\nhttps://github.com/RLHFlow/Reinforce-Ada.",
        "url": "http://arxiv.org/abs/2510.04996v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04996v1",
        "arxiv_id": "2510.04996v1",
        "authors": [
            "Wei Xiong",
            "Chenlu Ye",
            "Baohao Liao",
            "Hanze Dong",
            "Xinxing Xu",
            "Christof Monz",
            "Jiang Bian",
            "Nan Jiang",
            "Tong Zhang"
        ],
        "submitted": "2025-10-06 16:34:09",
        "source": "arxiv",
        "comment": "16 pages, 6 figures"
    },
    {
        "title": "AWARE, Beyond Sentence Boundaries: A Contextual Transformer Framework for Identifying Cultural Capital in STEM Narratives",
        "abstract": "Identifying cultural capital (CC) themes in student reflections can offer\nvaluable insights that help foster equitable learning environments in\nclassrooms. However, themes such as aspirational goals or family support are\noften woven into narratives, rather than appearing as direct keywords. This\nmakes them difficult to detect for standard NLP models that process sentences\nin isolation. The core challenge stems from a lack of awareness, as standard\nmodels are pre-trained on general corpora, leaving them blind to the\ndomain-specific language and narrative context inherent to the data. To address\nthis, we introduce AWARE, a framework that systematically attempts to improve a\ntransformer model's awareness for this nuanced task. AWARE has three core\ncomponents: 1) Domain Awareness, adapting the model's vocabulary to the\nlinguistic style of student reflections; 2) Context Awareness, generating\nsentence embeddings that are aware of the full essay context; and 3) Class\nOverlap Awareness, employing a multi-label strategy to recognize the\ncoexistence of themes in a single sentence. Our results show that by making the\nmodel explicitly aware of the properties of the input, AWARE outperforms a\nstrong baseline by 2.1 percentage points in Macro-F1 and shows considerable\nimprovements across all themes. This work provides a robust and generalizable\nmethodology for any text classification task in which meaning depends on the\ncontext of the narrative.",
        "url": "http://arxiv.org/abs/2510.04983v2",
        "pdf_url": "http://arxiv.org/pdf/2510.04983v2",
        "arxiv_id": "2510.04983v2",
        "authors": [
            "Khalid Mehtab Khan",
            "Anagha Kulkarni"
        ],
        "submitted": "2025-10-06 16:19:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LLM-Hanabi: Evaluating Multi-Agent Gameplays with Theory-of-Mind and Rationale Inference in Imperfect Information Collaboration Game",
        "abstract": "Effective multi-agent collaboration requires agents to infer the rationale\nbehind others' actions, a capability rooted in Theory-of-Mind (ToM). While\nrecent Large Language Models (LLMs) excel at logical inference, their ability\nto infer rationale in dynamic, collaborative settings remains under-explored.\nThis study introduces LLM-Hanabi, a novel benchmark that uses the cooperative\ngame Hanabi to evaluate the rationale inference and ToM of LLMs. Our framework\nfeatures an automated evaluation system that measures both game performance and\nToM proficiency. Across a range of models, we find a significant positive\ncorrelation between ToM and in-game success. Notably, first-order ToM\n(interpreting others' intent) correlates more strongly with performance than\nsecond-order ToM (predicting others' interpretations). These findings highlight\nthat for effective AI collaboration, the ability to accurately interpret a\npartner's rationale is more critical than higher-order reasoning. We conclude\nthat prioritizing first-order ToM is a promising direction for enhancing the\ncollaborative capabilities of future models.",
        "url": "http://arxiv.org/abs/2510.04980v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04980v1",
        "arxiv_id": "2510.04980v1",
        "authors": [
            "Fangzhou Liang",
            "Tianshi Zheng",
            "Chunkit Chan",
            "Yauwai Yim",
            "Yangqiu Song"
        ],
        "submitted": "2025-10-06 16:17:24",
        "source": "arxiv",
        "comment": "EMNLP 2025 Wordplay"
    },
    {
        "title": "Mind Your Tone: Investigating How Prompt Politeness Affects LLM Accuracy (short paper)",
        "abstract": "The wording of natural language prompts has been shown to influence the\nperformance of large language models (LLMs), yet the role of politeness and\ntone remains underexplored. In this study, we investigate how varying levels of\nprompt politeness affect model accuracy on multiple-choice questions. We\ncreated a dataset of 50 base questions spanning mathematics, science, and\nhistory, each rewritten into five tone variants: Very Polite, Polite, Neutral,\nRude, and Very Rude, yielding 250 unique prompts. Using ChatGPT 4o, we\nevaluated responses across these conditions and applied paired sample t-tests\nto assess statistical significance. Contrary to expectations, impolite prompts\nconsistently outperformed polite ones, with accuracy ranging from 80.8% for\nVery Polite prompts to 84.8% for Very Rude prompts. These findings differ from\nearlier studies that associated rudeness with poorer outcomes, suggesting that\nnewer LLMs may respond differently to tonal variation. Our results highlight\nthe importance of studying pragmatic aspects of prompting and raise broader\nquestions about the social dimensions of human-AI interaction.",
        "url": "http://arxiv.org/abs/2510.04950v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04950v1",
        "arxiv_id": "2510.04950v1",
        "authors": [
            "Om Dobariya",
            "Akhil Kumar"
        ],
        "submitted": "2025-10-06 15:50:39",
        "source": "arxiv",
        "comment": "5 pages, 3 tables; includes Limitations and Ethical Considerations\n  sections; short paper under submission to Findings of ACL 2025"
    },
    {
        "title": "A First Context-Free Grammar Applied to Nawatl Corpora Augmentation",
        "abstract": "In this article we introduce a context-free grammar (CFG) for the Nawatl\nlanguage. Nawatl (or Nahuatl) is an Amerindian language of the $\\pi$-language\ntype, i.e. a language with few digital resources, in which the corpora\navailable for machine learning are virtually non-existent. The objective here\nis to generate a significant number of grammatically correct artificial\nsentences, in order to increase the corpora available for language model\ntraining. We want to show that a grammar enables us significantly to expand a\ncorpus in Nawatl which we call $\\pi$-\\textsc{yalli}. The corpus, thus enriched,\nenables us to train algorithms such as FastText and to evaluate them on\nsentence-level semantic tasks. Preliminary results show that by using the\ngrammar, comparative improvements are achieved over some LLMs. However, it is\nobserved that to achieve more significant improvement, grammars that model the\nNawatl language even more effectively are required.",
        "url": "http://arxiv.org/abs/2510.04945v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04945v1",
        "arxiv_id": "2510.04945v1",
        "authors": [
            "Juan-José Guzmán-Landa",
            "Juan-Manuel Torres-Moreno",
            "Miguel Figueroa-Saavedra",
            "Ligia Quintana-Torres",
            "Martha-Lorena Avendaño-Garrido",
            "Graham Ranger"
        ],
        "submitted": "2025-10-06 15:46:54",
        "source": "arxiv",
        "comment": "11 pages, 7 tables, 1 figure"
    },
    {
        "title": "On Structured State-Space Duality",
        "abstract": "Structured State-Space Duality (SSD) [Dao & Gu, ICML 2024] is an equivalence\nbetween a simple Structured State-Space Model (SSM) and a masked attention\nmechanism. In particular, a state-space model with a scalar-times-identity\nstate matrix is equivalent to a masked self-attention with a $1$-semiseparable\ncausal mask. Consequently, the same sequence transformation (model) has two\nalgorithmic realizations: as a linear-time $O(T)$ recurrence or as a\nquadratic-time $O(T^2)$ attention. In this note, we formalize and generalize\nthis duality: (i) we extend SSD from the scalar-identity case to general\ndiagonal SSMs (diagonal state matrices); (ii) we show that these diagonal SSMs\nmatch the scalar case's training complexity lower bounds while supporting\nricher dynamics; (iii) we establish a necessary and sufficient condition under\nwhich an SSM is equivalent to $1$-semiseparable masked attention; and (iv) we\nshow that such duality fails to extend to standard softmax attention due to\nrank explosion. Together, these results tighten bridge between recurrent SSMs\nand Transformers, and widen the design space for expressive yet efficient\nsequence models.",
        "url": "http://arxiv.org/abs/2510.04944v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04944v1",
        "arxiv_id": "2510.04944v1",
        "authors": [
            "Jerry Yao-Chieh Hu",
            "Xiwen Zhang",
            "Weimin Wu",
            "Han Liu"
        ],
        "submitted": "2025-10-06 15:46:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ONNX-Net: Towards Universal Representations and Instant Performance Prediction for Neural Architectures",
        "abstract": "Neural architecture search (NAS) automates the design process of\nhigh-performing architectures, but remains bottlenecked by expensive\nperformance evaluation. Most existing studies that achieve faster evaluation\nare mostly tied to cell-based search spaces and graph encodings tailored to\nthose individual search spaces, limiting their flexibility and scalability when\napplied to more expressive search spaces. In this work, we aim to close the gap\nof individual search space restrictions and search space dependent network\nrepresentations. We present ONNX-Bench, a benchmark consisting of a collection\nof neural networks in a unified format based on ONNX files. ONNX-Bench includes\nall open-source NAS-bench-based neural networks, resulting in a total size of\nmore than 600k {architecture, accuracy} pairs. This benchmark allows creating a\nshared neural network representation, ONNX-Net, able to represent any neural\narchitecture using natural language descriptions acting as an input to a\nperformance predictor. This text-based encoding can accommodate arbitrary layer\ntypes, operation parameters, and heterogeneous topologies, enabling a single\nsurrogate to generalise across all neural architectures rather than being\nconfined to cell-based search spaces. Experiments show strong zero-shot\nperformance across disparate search spaces using only a small amount of\npretraining samples, enabling the unprecedented ability to evaluate any neural\nnetwork architecture instantly.",
        "url": "http://arxiv.org/abs/2510.04938v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04938v1",
        "arxiv_id": "2510.04938v1",
        "authors": [
            "Shiwen Qin",
            "Alexander Auras",
            "Shay B. Cohen",
            "Elliot J. Crowley",
            "Michael Moeller",
            "Linus Ericsson",
            "Jovita Lukasik"
        ],
        "submitted": "2025-10-06 15:43:36",
        "source": "arxiv",
        "comment": "Our code is available at: https://github.com/shiwenqin/ONNX-Net"
    },
    {
        "title": "MARS: Optimizing Dual-System Deep Research via Multi-Agent Reinforcement Learning",
        "abstract": "Large Reasoning Models (LRMs) often exhibit a tendency for overanalysis in\nsimple tasks, where the models excessively utilize System 2-type, deliberate\nreasoning, leading to inefficient token generation. Furthermore, these models\nface challenges in adapting their reasoning capabilities to rapidly changing\nenvironments due to the static nature of their pretraining data. To address\nthese issues, advancing Large Language Models (LLMs) for complex reasoning\ntasks requires innovative approaches that bridge intuitive and deliberate\ncognitive processes, akin to human cognition's dual-system dynamic. This paper\nintroduces a Multi-Agent System for Deep ReSearch (MARS) enabling seamless\nintegration of System 1's fast, intuitive thinking with System 2's deliberate\nreasoning within LLMs. MARS strategically integrates multiple external tools,\nsuch as Google Search, Google Scholar, and Python Interpreter, to access\nup-to-date information and execute complex computations, while creating a\nspecialized division of labor where System 1 efficiently processes and\nsummarizes high-volume external information, providing distilled insights that\nexpand System 2's reasoning context without overwhelming its capacity.\nFurthermore, we propose a multi-agent reinforcement learning framework\nextending Group Relative Policy Optimization to simultaneously optimize both\nsystems with multi-turn tool interactions, bin-packing optimization, and sample\nbalancing strategies that enhance collaborative efficiency. Extensive\nexperiments demonstrate MARS achieves substantial improvements of 3.86% on the\nchallenging Humanity's Last Exam (HLE) benchmark and an average gain of 8.9%\nacross 7 knowledge-intensive tasks, validating the effectiveness of our\ndual-system paradigm for complex reasoning in dynamic information environments.",
        "url": "http://arxiv.org/abs/2510.04935v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04935v1",
        "arxiv_id": "2510.04935v1",
        "authors": [
            "Guoxin Chen",
            "Zile Qiao",
            "Wenqing Wang",
            "Donglei Yu",
            "Xuanzhong Chen",
            "Hao Sun",
            "Minpeng Liao",
            "Kai Fan",
            "Yong Jiang",
            "Penguin Xie",
            "Wayne Xin Zhao",
            "Ruihua Song",
            "Fei Huang"
        ],
        "submitted": "2025-10-06 15:42:55",
        "source": "arxiv",
        "comment": "Ongoing Work"
    },
    {
        "title": "The Geometry of Truth: Layer-wise Semantic Dynamics for Hallucination Detection in Large Language Models",
        "abstract": "Large Language Models (LLMs) often produce fluent yet factually incorrect\nstatements-a phenomenon known as hallucination-posing serious risks in\nhigh-stakes domains. We present Layer-wise Semantic Dynamics (LSD), a geometric\nframework for hallucination detection that analyzes the evolution of\nhidden-state semantics across transformer layers. Unlike prior methods that\nrely on multiple sampling passes or external verification sources, LSD operates\nintrinsically within the model's representational space. Using margin-based\ncontrastive learning, LSD aligns hidden activations with ground-truth\nembeddings derived from a factual encoder, revealing a distinct separation in\nsemantic trajectories: factual responses preserve stable alignment, while\nhallucinations exhibit pronounced semantic drift across depth. Evaluated on the\nTruthfulQA and synthetic factual-hallucination datasets, LSD achieves an\nF1-score of 0.92, AUROC of 0.96, and clustering accuracy of 0.89, outperforming\nSelfCheckGPT and Semantic Entropy baselines while requiring only a single\nforward pass. This efficiency yields a 5-20x speedup over sampling-based\nmethods without sacrificing precision or interpretability. LSD offers a\nscalable, model-agnostic mechanism for real-time hallucination monitoring and\nprovides new insights into the geometry of factual consistency within large\nlanguage models.",
        "url": "http://arxiv.org/abs/2510.04933v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04933v1",
        "arxiv_id": "2510.04933v1",
        "authors": [
            "Amir Hameed Mir"
        ],
        "submitted": "2025-10-06 15:41:22",
        "source": "arxiv",
        "comment": "Comments: 14 pages, 14 figures, 5 tables. Code available at:\n  https://github.com/sirraya-tech/Sirraya_LSD_Code"
    },
    {
        "title": "Do LLMs Align with My Task? Evaluating Text-to-SQL via Dataset Alignment",
        "abstract": "Supervised Fine-Tuning (SFT) is an effective method for adapting Large\nLanguage Models (LLMs) on downstream tasks. However, variability in training\ndata can hinder a model's ability to generalize across domains. This paper\nstudies the problem of dataset alignment for Natural Language to SQL (NL2SQL or\ntext to SQL), examining how well SFT training data matches the structural\ncharacteristics of target queries and how this alignment impacts model\nperformance. We hypothesize that alignment can be accurately estimated by\ncomparing the distributions of structural SQL features across the training set,\ntarget data, and the model's predictions prior to SFT. Through comprehensive\nexperiments on three large cross-domain NL2SQL benchmarks and multiple model\nfamilies, we show that structural alignment is a strong predictor of\nfine-tuning success. When alignment is high, SFT yields substantial gains in\naccuracy and SQL generation quality; when alignment is low, improvements are\nmarginal or absent. These findings highlight the importance of alignment-aware\ndata selection for effective fine-tuning and generalization in NL2SQL tasks.",
        "url": "http://arxiv.org/abs/2510.04919v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04919v1",
        "arxiv_id": "2510.04919v1",
        "authors": [
            "Davood Rafiei",
            "Morgan Lindsay Heisler",
            "Weiwei Zhang",
            "Mohammadreza Pourreza",
            "Yong Zhang"
        ],
        "submitted": "2025-10-06 15:33:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Retrieval-Augmented Code Generation: A Survey with Focus on Repository-Level Approaches",
        "abstract": "Recent advancements in large language models (LLMs) have substantially\nimproved automated code generation. While function-level and file-level\ngeneration have achieved promising results, real-world software development\ntypically requires reasoning across entire repositories. This gives rise to the\nchallenging task of Repository-Level Code Generation (RLCG), where models must\ncapture long-range dependencies, ensure global semantic consistency, and\ngenerate coherent code spanning multiple files or modules. To address these\nchallenges, Retrieval-Augmented Generation (RAG) has emerged as a powerful\nparadigm that integrates external retrieval mechanisms with LLMs, enhancing\ncontext-awareness and scalability. In this survey, we provide a comprehensive\nreview of research on Retrieval-Augmented Code Generation (RACG), with an\nemphasis on repository-level approaches. We categorize existing work along\nseveral dimensions, including generation strategies, retrieval modalities,\nmodel architectures, training paradigms, and evaluation protocols. Furthermore,\nwe summarize widely used datasets and benchmarks, analyze current limitations,\nand outline key challenges and opportunities for future research. Our goal is\nto establish a unified analytical framework for understanding this rapidly\nevolving field and to inspire continued progress in AI-powered software\nengineering.",
        "url": "http://arxiv.org/abs/2510.04905v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04905v1",
        "arxiv_id": "2510.04905v1",
        "authors": [
            "Yicheng Tao",
            "Yao Qin",
            "Yepang Liu"
        ],
        "submitted": "2025-10-06 15:20:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SocialHarmBench: Revealing LLM Vulnerabilities to Socially Harmful Requests",
        "abstract": "Large language models (LLMs) are increasingly deployed in contexts where\ntheir failures can have direct sociopolitical consequences. Yet, existing\nsafety benchmarks rarely test vulnerabilities in domains such as political\nmanipulation, propaganda and disinformation generation, or surveillance and\ninformation control. We introduce SocialHarmBench, a dataset of 585 prompts\nspanning 7 sociopolitical categories and 34 countries, designed to surface\nwhere LLMs most acutely fail in politically charged contexts. Our evaluations\nreveal several shortcomings: open-weight models exhibit high vulnerability to\nharmful compliance, with Mistral-7B reaching attack success rates as high as\n97% to 98% in domains such as historical revisionism, propaganda, and political\nmanipulation. Moreover, temporal and geographic analyses show that LLMs are\nmost fragile when confronted with 21st-century or pre-20th-century contexts,\nand when responding to prompts tied to regions such as Latin America, the USA,\nand the UK. These findings demonstrate that current safeguards fail to\ngeneralize to high-stakes sociopolitical settings, exposing systematic biases\nand raising concerns about the reliability of LLMs in preserving human rights\nand democratic values. We share the SocialHarmBench benchmark at\nhttps://huggingface.co/datasets/psyonp/SocialHarmBench.",
        "url": "http://arxiv.org/abs/2510.04891v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04891v1",
        "arxiv_id": "2510.04891v1",
        "authors": [
            "Punya Syon Pandey",
            "Hai Son Le",
            "Devansh Bhardwaj",
            "Rada Mihalcea",
            "Zhijing Jin"
        ],
        "submitted": "2025-10-06 15:11:46",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Detecting Distillation Data from Reasoning Models",
        "abstract": "Reasoning distillation has emerged as an efficient and powerful paradigm for\nenhancing the reasoning capabilities of large language models. However,\nreasoning distillation may inadvertently cause benchmark contamination, where\nevaluation data included in distillation datasets can inflate performance\nmetrics of distilled models. In this work, we formally define the task of\ndistillation data detection, which is uniquely challenging due to the partial\navailability of distillation data. Then, we propose a novel and effective\nmethod Token Probability Deviation (TBD), which leverages the probability\npatterns of the generated output tokens. Our method is motivated by the\nanalysis that distilled models tend to generate near-deterministic tokens for\nseen questions, while producing more low-probability tokens for unseen\nquestions. Our key idea behind TBD is to quantify how far the generated tokens'\nprobabilities deviate from a high reference probability. In effect, our method\nachieves competitive detection performance by producing lower scores for seen\nquestions than for unseen questions. Extensive experiments demonstrate the\neffectiveness of our method, achieving an AUC of 0.918 and a TPR@1% FPR of\n0.470 on the S1 dataset.",
        "url": "http://arxiv.org/abs/2510.04850v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04850v1",
        "arxiv_id": "2510.04850v1",
        "authors": [
            "Hengxiang Zhang",
            "Hyeong Kyu Choi",
            "Yixuan Li",
            "Hongxin Wei"
        ],
        "submitted": "2025-10-06 14:37:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "When Models Lie, We Learn: Multilingual Span-Level Hallucination Detection with PsiloQA",
        "abstract": "Hallucination detection remains a fundamental challenge for the safe and\nreliable deployment of large language models (LLMs), especially in applications\nrequiring factual accuracy. Existing hallucination benchmarks often operate at\nthe sequence level and are limited to English, lacking the fine-grained,\nmultilingual supervision needed for a comprehensive evaluation. In this work,\nwe introduce PsiloQA, a large-scale, multilingual dataset annotated with\nspan-level hallucinations across 14 languages. PsiloQA is constructed through\nan automated three-stage pipeline: generating question-answer pairs from\nWikipedia using GPT-4o, eliciting potentially hallucinated answers from diverse\nLLMs in a no-context setting, and automatically annotating hallucinated spans\nusing GPT-4o by comparing against golden answers and retrieved context. We\nevaluate a wide range of hallucination detection methods -- including\nuncertainty quantification, LLM-based tagging, and fine-tuned encoder models --\nand show that encoder-based models achieve the strongest performance across\nlanguages. Furthermore, PsiloQA demonstrates effective cross-lingual\ngeneralization and supports robust knowledge transfer to other benchmarks, all\nwhile being significantly more cost-efficient than human-annotated datasets.\nOur dataset and results advance the development of scalable, fine-grained\nhallucination detection in multilingual settings.",
        "url": "http://arxiv.org/abs/2510.04849v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04849v1",
        "arxiv_id": "2510.04849v1",
        "authors": [
            "Elisei Rykov",
            "Kseniia Petrushina",
            "Maksim Savkin",
            "Valerii Olisov",
            "Artem Vazhentsev",
            "Kseniia Titova",
            "Alexander Panchenko",
            "Vasily Konovalov",
            "Julia Belikova"
        ],
        "submitted": "2025-10-06 14:36:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Instability in Downstream Task Performance During LLM Pretraining",
        "abstract": "When training large language models (LLMs), it is common practice to track\ndownstream task performance throughout the training process and select the\ncheckpoint with the highest validation score. However, downstream metrics often\nexhibit substantial fluctuations, making it difficult to identify the\ncheckpoint that truly represents the best-performing model. In this study, we\nempirically analyze the stability of downstream task performance in an LLM\ntrained on diverse web-scale corpora. We find that task scores frequently\nfluctuate throughout training, both at the aggregate and example levels. To\naddress this instability, we investigate two post-hoc checkpoint integration\nmethods: checkpoint averaging and ensemble, motivated by the hypothesis that\naggregating neighboring checkpoints can reduce performance volatility. We\ndemonstrate both empirically and theoretically that these methods improve\ndownstream performance stability without requiring any changes to the training\nprocedure.",
        "url": "http://arxiv.org/abs/2510.04848v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04848v1",
        "arxiv_id": "2510.04848v1",
        "authors": [
            "Yuto Nishida",
            "Masaru Isonuma",
            "Yusuke Oda"
        ],
        "submitted": "2025-10-06 14:33:38",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 Findings"
    },
    {
        "title": "How I Built ASR for Endangered Languages with a Spoken Dictionary",
        "abstract": "Nearly half of the world's languages are endangered. Speech technologies such\nas Automatic Speech Recognition (ASR) are central to revival efforts, yet most\nlanguages remain unsupported because standard pipelines expect utterance-level\nsupervised data. Speech data often exist for endangered languages but rarely\nmatch these formats. Manx Gaelic ($\\sim$2,200 speakers), for example, has had\ntranscribed speech since 1948, yet remains unsupported by modern systems. In\nthis paper, we explore how little data, and in what form, is needed to build\nASR for critically endangered languages. We show that a short-form\npronunciation resource is a viable alternative, and that 40 minutes of such\ndata produces usable ASR for Manx ($<$50\\% WER). We replicate our approach,\napplying it to Cornish ($\\sim$600 speakers), another critically endangered\nlanguage. Results show that the barrier to entry, in quantity and form, is far\nlower than previously thought, giving hope to endangered language communities\nthat cannot afford to meet the requirements arbitrarily imposed upon them.",
        "url": "http://arxiv.org/abs/2510.04832v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04832v1",
        "arxiv_id": "2510.04832v1",
        "authors": [
            "Christopher Bartley",
            "Anton Ragni"
        ],
        "submitted": "2025-10-06 14:16:47",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Visual Representations inside the Language Model",
        "abstract": "Despite interpretability work analyzing VIT encoders and transformer\nactivations, we don't yet understand why Multimodal Language Models (MLMs)\nstruggle on perception-heavy tasks. We offer an under-studied perspective by\nexamining how popular MLMs (LLaVA-OneVision, Qwen2.5-VL, and\nLlama-3-LLaVA-NeXT) process their visual key-value tokens. We first study the\nflow of visual information through the language model, finding that image value\ntokens encode sufficient information to perform several perception-heavy tasks\nzero-shot: segmentation, semantic correspondence, temporal correspondence, and\nreferring expression detection. We find that while the language model does\naugment the visual information received from the projection of input visual\nencodings-which we reveal correlates with overall MLM perception capability-it\ncontains less visual information on several tasks than the equivalent visual\nencoder (SigLIP) that has not undergone MLM finetuning. Further, we find that\nthe visual information corresponding to input-agnostic image key tokens in\nlater layers of language models contains artifacts which reduce perception\ncapability of the overall MLM. Next, we discuss controlling visual information\nin the language model, showing that adding a text prefix to the image input\nimproves perception capabilities of visual representations. Finally, we reveal\nthat if language models were able to better control their visual information,\ntheir perception would significantly improve; e.g., in 33.3% of Art Style\nquestions in the BLINK benchmark, perception information present in the\nlanguage model is not surfaced to the output! Our findings reveal insights into\nthe role of key-value tokens in multimodal systems, paving the way for deeper\nmechanistic interpretability of MLMs and suggesting new directions for training\ntheir visual encoder and language model components.",
        "url": "http://arxiv.org/abs/2510.04819v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04819v1",
        "arxiv_id": "2510.04819v1",
        "authors": [
            "Benlin Liu",
            "Amita Kamath",
            "Madeleine Grunde-McLaughlin",
            "Winson Han",
            "Ranjay Krishna"
        ],
        "submitted": "2025-10-06 14:01:39",
        "source": "arxiv",
        "comment": "Accepted to COLM 2025"
    },
    {
        "title": "Hybrid Architectures for Language Models: Systematic Analysis and Design Insights",
        "abstract": "Recent progress in large language models demonstrates that hybrid\narchitectures--combining self-attention mechanisms with structured state space\nmodels like Mamba--can achieve a compelling balance between modeling quality\nand computational efficiency, particularly for long-context tasks. While these\nhybrid models show promising performance, systematic comparisons of\nhybridization strategies and analyses on the key factors behind their\neffectiveness have not been clearly shared to the community. In this work, we\npresent a holistic evaluation of hybrid architectures based on inter-layer\n(sequential) or intra-layer (parallel) fusion. We evaluate these designs from a\nvariety of perspectives: language modeling performance, long-context\ncapabilities, scaling analysis, and training and inference efficiency. By\ninvestigating the core characteristics of their computational primitive, we\nidentify the most critical elements for each hybridization strategy and further\npropose optimal design recipes for both hybrid models. Our comprehensive\nanalysis provides practical guidance and valuable insights for developing\nhybrid language models, facilitating the optimization of architectural\nconfigurations.",
        "url": "http://arxiv.org/abs/2510.04800v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04800v1",
        "arxiv_id": "2510.04800v1",
        "authors": [
            "Sangmin Bae",
            "Bilge Acun",
            "Haroun Habeeb",
            "Seungyeon Kim",
            "Chien-Yu Lin",
            "Liang Luo",
            "Junjie Wang",
            "Carole-Jean Wu"
        ],
        "submitted": "2025-10-06 13:30:07",
        "source": "arxiv",
        "comment": "17 pages, 4 figures, 6 tables; detailed results will be included in\n  the Appendix later"
    },
    {
        "title": "Are BabyLMs Deaf to Gricean Maxims? A Pragmatic Evaluation of Sample-efficient Language Models",
        "abstract": "Implicit meanings are integral to human communication, making it essential\nfor language models to be capable of identifying and interpreting them. Grice\n(1975) proposed a set of conversational maxims that guide cooperative dialogue,\nnoting that speakers may deliberately violate these principles to express\nmeanings beyond literal words, and that listeners, in turn, recognize such\nviolations to draw pragmatic inferences.\n  Building on Surian et al. (1996)'s study of children's sensitivity to\nviolations of Gricean maxims, we introduce a novel benchmark to test whether\nlanguage models pretrained on less than 10M and less than 100M tokens can\ndistinguish maxim-adhering from maxim-violating utterances. We compare these\nBabyLMs across five maxims and situate their performance relative to children\nand a Large Language Model (LLM) pretrained on 3T tokens.\n  We find that overall, models trained on less than 100M tokens outperform\nthose trained on less than 10M, yet fall short of child-level and LLM\ncompetence. Our results suggest that modest data increases improve some aspects\nof pragmatic behavior, leading to finer-grained differentiation between\npragmatic dimensions.",
        "url": "http://arxiv.org/abs/2510.04764v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04764v1",
        "arxiv_id": "2510.04764v1",
        "authors": [
            "Raha Askari",
            "Sina Zarrieß",
            "Özge Alacam",
            "Judith Sieker"
        ],
        "submitted": "2025-10-06 12:38:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ModernBERT + ColBERT: Enhancing biomedical RAG through an advanced re-ranking retriever",
        "abstract": "Retrieval-Augmented Generation (RAG) is a powerful technique for enriching\nLarge Language Models (LLMs) with external knowledge, allowing for factually\ngrounded responses, a critical requirement in high-stakes domains such as\nhealthcare. However, the efficacy of RAG systems is fundamentally restricted by\nthe performance of their retrieval module, since irrelevant or semantically\nmisaligned documents directly compromise the accuracy of the final generated\nresponse. General-purpose dense retrievers can struggle with the nuanced\nlanguage of specialised domains, while the high accuracy of in-domain models is\noften achieved at prohibitive computational costs. In this work, we aim to\naddress this trade-off by developing and evaluating a two-stage retrieval\narchitecture that combines a lightweight ModernBERT bidirectional encoder for\nefficient initial candidate retrieval with a ColBERTv2 late-interaction model\nfor fine-grained re-ranking. We conduct comprehensive evaluations of our\nretriever module performance and RAG system performance in the biomedical\ncontext, fine-tuning the IR module using 10k question-passage pairs from\nPubMedQA. Our analysis of the retriever module confirmed the positive impact of\nthe ColBERT re-ranker, which improved Recall@3 by up to 4.2 percentage points\ncompared to its retrieve-only counterpart. When integrated into the biomedical\nRAG, our IR module leads to a state-of-the-art average accuracy of 0.4448 on\nthe five tasks of the MIRAGE question-answering benchmark, outperforming strong\nbaselines such as MedCPT (0.4436). Our ablation studies reveal that this\nperformance is critically dependent on a joint fine-tuning process that aligns\nthe retriever and re-ranker; otherwise, the re-ranker might degrade the\nperformance.",
        "url": "http://arxiv.org/abs/2510.04757v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04757v1",
        "arxiv_id": "2510.04757v1",
        "authors": [
            "Eduardo Martínez Rivera",
            "Filippo Menolascina"
        ],
        "submitted": "2025-10-06 12:34:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Low-Resource Speech-Driven NLP Pipeline for Sinhala Dyslexia Assistance",
        "abstract": "Dyslexia in adults remains an under-researched and under-served area,\nparticularly in non-English-speaking contexts, despite its significant impact\non personal and professional lives. This work addresses that gap by focusing on\nSinhala, a low-resource language with limited tools for linguistic\naccessibility. We present an assistive system explicitly designed for\nSinhala-speaking adults with dyslexia. The system integrates Whisper for\nspeech-to-text conversion, SinBERT, an open-sourced fine-tuned BERT model\ntrained for Sinhala to identify common dyslexic errors, and a combined mT5 and\nMistral-based model to generate corrected text. Finally, the output is\nconverted back to speech using gTTS, creating a complete multimodal feedback\nloop. Despite the challenges posed by limited Sinhala-language datasets, the\nsystem achieves 0.66 transcription accuracy and 0.7 correction accuracy with\n0.65 overall system accuracy. These results demonstrate both the feasibility\nand effectiveness of the approach. Ultimately, this work highlights the\nimportance of inclusive Natural Language Processing (NLP) technologies in\nunderrepresented languages and showcases a practical",
        "url": "http://arxiv.org/abs/2510.04750v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04750v1",
        "arxiv_id": "2510.04750v1",
        "authors": [
            "Peshala Perera",
            "Deshan Sumanathilaka"
        ],
        "submitted": "2025-10-06 12:28:57",
        "source": "arxiv",
        "comment": "11 pages, 4 figures, 3 tables"
    },
    {
        "title": "Speak, Edit, Repeat: High-Fidelity Voice Editing and Zero-Shot TTS with Cross-Attentive Mamba",
        "abstract": "We introduce MAVE (Mamba with Cross-Attention for Voice Editing and\nSynthesis), a novel autoregressive architecture for text-conditioned voice\nediting and high-fidelity text-to-speech (TTS) synthesis, built on a\ncross-attentive Mamba backbone. MAVE achieves state-of-the-art performance in\nspeech editing and very competitive results in zero-shot TTS, while not being\nexplicitly trained on the latter task, outperforming leading autoregressive and\ndiffusion models on diverse, real-world audio. By integrating Mamba for\nefficient audio sequence modeling with cross-attention for precise\ntext-acoustic alignment, MAVE enables context-aware voice editing with\nexceptional naturalness and speaker consistency. In pairwise human evaluations\non a random 40-sample subset of the RealEdit benchmark (400 judgments), 57.2%\nof listeners rated MAVE - edited speech as perceptually equal to the original,\nwhile 24.8% prefered the original and 18.0% MAVE - demonstrating that in the\nmajority of cases edits are indistinguishable from the source. MAVE compares\nfavorably with VoiceCraft and FluentSpeech both on pairwise comparisons and\nstandalone mean opinion score (MOS) evaluations. For zero-shot TTS, MAVE\nexceeds VoiceCraft in both speaker similarity and naturalness, without\nrequiring multiple inference runs or post-processing. Remarkably, these quality\ngains come with a significantly lower memory cost and approximately the same\nlatency: MAVE requires ~6x less memory than VoiceCraft during inference on\nutterances from the RealEdit database (mean duration: 6.21s, A100, FP16, batch\nsize 1). Our results demonstrate that MAVE establishes a new standard for\nflexible, high-fidelity voice editing and synthesis through the synergistic\nintegration of structured state-space modeling and cross-modal attention.",
        "url": "http://arxiv.org/abs/2510.04738v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04738v1",
        "arxiv_id": "2510.04738v1",
        "authors": [
            "Baher Mohammad",
            "Magauiya Zhussip",
            "Stamatios Lefkimmiatis"
        ],
        "submitted": "2025-10-06 12:11:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "BrokenMath: A Benchmark for Sycophancy in Theorem Proving with LLMs",
        "abstract": "Large language models (LLMs) have recently shown strong performance on\nmathematical benchmarks. At the same time, they are prone to hallucination and\nsycophancy, often providing convincing but flawed proofs for incorrect\nmathematical statements provided by users. This significantly limits the\napplicability of LLMs in theorem proving, as verification of these flawed\nproofs must be done manually by expert mathematicians. However, existing\nbenchmarks that measure sycophancy in mathematics are limited: they focus\nsolely on final-answer problems, rely on very simple and often contaminated\ndatasets, and construct benchmark samples using synthetic modifications that\ncreate ill-posed questions rather than well-posed questions that are\ndemonstrably false. To address these issues, we introduce BrokenMath, the first\nbenchmark for evaluating sycophantic behavior in LLMs within the context of\nnatural language theorem proving. BrokenMath is built from advanced 2025\ncompetition problems, which are perturbed with an LLM to produce false\nstatements and subsequently refined through expert review. Using an\nLLM-as-a-judge framework, we evaluate state-of-the-art LLMs and agentic systems\nand find that sycophancy is widespread, with the best model, GPT-5, producing\nsycophantic answers 29% of the time. We further investigate several mitigation\nstrategies, including test-time interventions and supervised fine-tuning on\ncurated sycophantic examples. These approaches substantially reduce, but do not\neliminate, sycophantic behavior.",
        "url": "http://arxiv.org/abs/2510.04721v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04721v1",
        "arxiv_id": "2510.04721v1",
        "authors": [
            "Ivo Petrov",
            "Jasper Dekoninck",
            "Martin Vechev"
        ],
        "submitted": "2025-10-06 11:41:46",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "JSON Whisperer: Efficient JSON Editing with LLMs",
        "abstract": "Large language models (LLMs) can modify JSON documents through natural\nlanguage commands, but current approaches regenerate entire structures for each\nedit, resulting in computational inefficiency. We present JSON Whisperer, a\nframework that enables LLMs to generate RFC 6902 diff patches-expressing only\nthe necessary modifications-rather than complete documents. We identify two key\nchallenges in patch-based editing: (1) LLMs often miss related updates when\ngenerating isolated patches, and (2) array manipulations require tracking index\nshifts across operations, which LLMs handle poorly. To address these issues, we\nintroduce EASE (Explicitly Addressed Sequence Encoding), which transforms\narrays into dictionaries with stable keys, eliminating index arithmetic\ncomplexities. Our evaluation shows that patch generation with EASE reduces\ntoken usage by 31% while maintaining edit quality within 5% of full\nregeneration with particular gains for complex instructions and list\nmanipulations. The dataset is available at:\nhttps://github.com/emnlp2025/JSON-Whisperer/",
        "url": "http://arxiv.org/abs/2510.04717v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04717v1",
        "arxiv_id": "2510.04717v1",
        "authors": [
            "Sarel Duanis",
            "Asnat Greenstein-Messica",
            "Eliya Habba"
        ],
        "submitted": "2025-10-06 11:36:46",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AtomWorld: A Benchmark for Evaluating Spatial Reasoning in Large Language Models on Crystalline Materials",
        "abstract": "Large Language Models (LLMs) excel at textual reasoning and are beginning to\ndevelop spatial understanding, prompting the question of whether these\nabilities can be combined for complex, domain-specific tasks. This question is\nessential in fields like materials science, where deep understanding of 3D\natomic structures is fundamental. While initial studies have successfully\napplied LLMs to tasks involving pure crystal generation or coordinate\nunderstandings, a standardized benchmark to systematically evaluate their core\nreasoning abilities across diverse atomic structures has been notably absent.\nTo address this gap, we introduce the AtomWorld benchmark to evaluate LLMs on\ntasks based in Crystallographic Information Files (CIFs), a standard structure\nrepresentation format. These tasks, including structural editing, CIF\nperception, and property-guided modeling, reveal a critical limitation: current\nmodels, despite establishing promising baselines, consistently fail in\nstructural understanding and spatial reasoning. Our experiments show that these\nmodels make frequent errors on structure modification tasks, and even in the\nbasic CIF format understandings, potentially leading to cumulative errors in\nsubsequent analysis and materials insights. By defining these standardized\ntasks, AtomWorld lays the ground for advancing LLMs toward robust atomic-scale\nmodeling, crucial for accelerating materials research and automating scientific\nworkflows.",
        "url": "http://arxiv.org/abs/2510.04704v2",
        "pdf_url": "http://arxiv.org/pdf/2510.04704v2",
        "arxiv_id": "2510.04704v2",
        "authors": [
            "Taoyuze Lv",
            "Alexander Chen",
            "Fengyu Xie",
            "Chu Wu",
            "Jeffrey Meng",
            "Dongzhan Zhou",
            "Bram Hoex",
            "Zhicheng Zhong",
            "Tong Xie"
        ],
        "submitted": "2025-10-06 11:17:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Multilingual Routing in Mixture-of-Experts",
        "abstract": "Mixture-of-Experts (MoE) architectures have become the key to scaling modern\nLLMs, yet little is understood about how their sparse routing dynamics respond\nto multilingual data. In this work, we analyze expert routing patterns using\nparallel multilingual datasets and present highly interpretable layer-wise\nphenomena. We find that MoE models route tokens in language-specific ways in\nthe early and late decoder layers but exhibit significant cross-lingual routing\nalignment in middle layers, mirroring parameter-sharing trends observed in\ndense LLMs. In particular, we reveal a clear, strong correlation between a\nmodel's performance in a given language and how similarly its tokens are routed\nto English in these layers. Extending beyond correlation, we explore\ninference-time interventions that induce higher cross-lingual routing\nalignment. We introduce a method that steers the router by promoting\nmiddle-layer task experts frequently activated in English, and it successfully\nincreases multilingual performance. These 1-2% gains are remarkably consistent\nacross two evaluation tasks, three models, and 15+ languages, especially given\nthat these simple interventions override routers of extensively trained,\nstate-of-the-art LLMs. In comparison, interventions outside of the middle\nlayers or targeting multilingual-specialized experts only yield performance\ndegradation. Altogether, we present numerous findings that explain how MoEs\nprocess non-English text and demonstrate that generalization is limited by the\nmodel's ability to leverage language-universal experts in all languages.",
        "url": "http://arxiv.org/abs/2510.04694v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04694v1",
        "arxiv_id": "2510.04694v1",
        "authors": [
            "Lucas Bandarkar",
            "Chenyuan Yang",
            "Mohsen Fayyaz",
            "Junlin Hu",
            "Nanyun Peng"
        ],
        "submitted": "2025-10-06 11:09:20",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TiTok: Transfer Token-level Knowledge via Contrastive Excess to Transplant LoRA",
        "abstract": "Large Language Models (LLMs) are widely applied in real world scenarios, but\nfine-tuning them comes with significant computational and storage costs.\nParameter-Efficient Fine-Tuning (PEFT) methods such as LoRA mitigate these\ncosts, but the adapted parameters are dependent on the base model and cannot be\ntransferred across different backbones. One way to address this issue is\nthrough knowledge distillation, but its effectiveness inherently depends on\ntraining data. Recent work such as TransLoRA avoids this by generating\nsynthetic data, but this adds complexity because it requires training an\nadditional discriminator model. In this paper, we propose TiTok, a new\nframework that enables effective LoRA Transplantation through Token-level\nknowledge transfer. Specifically, TiTok captures task-relevant information\nthrough a contrastive excess between a source model with and without LoRA. This\nexcess highlights informative tokens and enables selective filtering of\nsynthetic data, all without additional models or overhead. Through experiments\non three benchmarks across multiple transfer settings, our experiments show\nthat the proposed method is consistently effective, achieving average\nperformance gains of +4~8% compared to baselines overall.",
        "url": "http://arxiv.org/abs/2510.04682v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04682v1",
        "arxiv_id": "2510.04682v1",
        "authors": [
            "Chanjoo Jung",
            "Jaehyung Kim"
        ],
        "submitted": "2025-10-06 10:47:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Multi-Agent Tool-Integrated Policy Optimization",
        "abstract": "Large language models (LLMs) increasingly rely on multi-turn tool-integrated\nplanning for knowledge-intensive and complex reasoning tasks. Existing\nimplementations typically rely on a single agent, but they suffer from limited\ncontext length and noisy tool responses. A natural solution is to adopt a\nmulti-agent framework with planner- and worker-agents to manage context.\nHowever, no existing methods support effective reinforcement learning\npost-training of tool-integrated multi-agent frameworks. To address this gap,\nwe propose Multi-Agent Tool-Integrated Policy Optimization (MATPO), which\nenables distinct roles (planner and worker) to be trained within a single LLM\ninstance using role-specific prompts via reinforcement learning. MATPO is\nderived from a principled credit assignment mechanism across planner and worker\nrollouts. This design eliminates the need to deploy multiple LLMs, which would\nbe memory-intensive, while preserving the benefits of specialization.\nExperiments on GAIA-text, WebWalkerQA, and FRAMES show that MATPO consistently\noutperforms single-agent baselines by an average of 18.38% relative improvement\nin performance and exhibits greater robustness to noisy tool outputs. Our\nfindings highlight the effectiveness of unifying multiple agent roles within a\nsingle LLM and provide practical insights for stable and efficient multi-agent\nRL training.",
        "url": "http://arxiv.org/abs/2510.04678v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04678v1",
        "arxiv_id": "2510.04678v1",
        "authors": [
            "Zhanfeng Mo",
            "Xingxuan Li",
            "Yuntao Chen",
            "Lidong Bing"
        ],
        "submitted": "2025-10-06 10:44:04",
        "source": "arxiv",
        "comment": "Work in progress"
    },
    {
        "title": "FocusMed: A Large Language Model-based Framework for Enhancing Medical Question Summarization with Focus Identification",
        "abstract": "With the rapid development of online medical platforms, consumer health\nquestions (CHQs) are inefficient in diagnosis due to redundant information and\nfrequent non-professional terms. The medical question summary (MQS) task aims\nto transform CHQs into streamlined doctors' frequently asked questions (FAQs),\nbut existing methods still face challenges such as poor identification of\nquestion focus and model hallucination. This paper explores the potential of\nlarge language models (LLMs) in the MQS task and finds that direct fine-tuning\nis prone to focus identification bias and generates unfaithful content. To this\nend, we propose an optimization framework based on core focus guidance. First,\na prompt template is designed to drive the LLMs to extract the core focus from\nthe CHQs that is faithful to the original text. Then, a fine-tuning dataset is\nconstructed in combination with the original CHQ-FAQ pairs to improve the\nability to identify the focus of the question. Finally, a multi-dimensional\nquality evaluation and selection mechanism is proposed to comprehensively\nimprove the quality of the summary from multiple dimensions. We conduct\ncomprehensive experiments on two widely-adopted MQS datasets using three\nestablished evaluation metrics. The proposed framework achieves\nstate-of-the-art performance across all measures, demonstrating a significant\nboost in the model's ability to identify critical focus of questions and a\nnotable mitigation of hallucinations. The source codes are freely available at\nhttps://github.com/DUT-LiuChao/FocusMed.",
        "url": "http://arxiv.org/abs/2510.04671v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04671v1",
        "arxiv_id": "2510.04671v1",
        "authors": [
            "Chao Liu",
            "Ling Luo",
            "Tengxiao Lv",
            "Huan Zhuang",
            "Lejing Yu",
            "Jian Wang",
            "Hongfei Lin"
        ],
        "submitted": "2025-10-06 10:27:09",
        "source": "arxiv",
        "comment": "Accepted as a regular paper at BIBM2025"
    },
    {
        "title": "FT-MDT: Extracting Decision Trees from Medical Texts via a Novel Low-rank Adaptation Method",
        "abstract": "Knowledge of the medical decision process, which can be modeled as medical\ndecision trees (MDTs), is critical to building clinical decision support\nsystems. However, current MDT construction methods rely heavily on\ntime-consuming and laborious manual annotation. To address this challenge, we\npropose PI-LoRA (Path-Integrated LoRA), a novel low-rank adaptation method for\nautomatically extracting MDTs from clinical guidelines and textbooks. We\nintegrate gradient path information to capture synergistic effects between\ndifferent modules, enabling more effective and reliable rank allocation. This\nframework ensures that the most critical modules receive appropriate rank\nallocations while less important ones are pruned, resulting in a more efficient\nand accurate model for extracting medical decision trees from clinical texts.\nExtensive experiments on medical guideline datasets demonstrate that our\nPI-LoRA method significantly outperforms existing parameter-efficient\nfine-tuning approaches for the Text2MDT task, achieving better accuracy with\nsubstantially reduced model complexity. The proposed method achieves\nstate-of-the-art results while maintaining a lightweight architecture, making\nit particularly suitable for clinical decision support systems where\ncomputational resources may be limited.",
        "url": "http://arxiv.org/abs/2510.04655v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04655v1",
        "arxiv_id": "2510.04655v1",
        "authors": [
            "Yuheng Li",
            "Jiechao Gao",
            "Wei Han",
            "Wenwen Ouyang",
            "Wei Zhu",
            "Hui Yi Leong"
        ],
        "submitted": "2025-10-06 09:59:55",
        "source": "arxiv",
        "comment": "Accepted by EMNLP-2025 Industrial Track"
    },
    {
        "title": "Evaluating LLMs for Demographic-Targeted Social Bias Detection: A Comprehensive Benchmark Study",
        "abstract": "Large-scale web-scraped text corpora used to train general-purpose AI models\noften contain harmful demographic-targeted social biases, creating a regulatory\nneed for data auditing and developing scalable bias-detection methods. Although\nprior work has investigated biases in text datasets and related detection\nmethods, these studies remain narrow in scope. They typically focus on a single\ncontent type (e.g., hate speech), cover limited demographic axes, overlook\nbiases affecting multiple demographics simultaneously, and analyze limited\ntechniques. Consequently, practitioners lack a holistic understanding of the\nstrengths and limitations of recent large language models (LLMs) for automated\nbias detection. In this study, we present a comprehensive evaluation framework\naimed at English texts to assess the ability of LLMs in detecting\ndemographic-targeted social biases. To align with regulatory requirements, we\nframe bias detection as a multi-label task using a demographic-focused\ntaxonomy. We then conduct a systematic evaluation with models across scales and\ntechniques, including prompting, in-context learning, and fine-tuning. Using\ntwelve datasets spanning diverse content types and demographics, our study\ndemonstrates the promise of fine-tuned smaller models for scalable detection.\nHowever, our analyses also expose persistent gaps across demographic axes and\nmulti-demographic targeted biases, underscoring the need for more effective and\nscalable auditing frameworks.",
        "url": "http://arxiv.org/abs/2510.04641v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04641v1",
        "arxiv_id": "2510.04641v1",
        "authors": [
            "Ayan Majumdar",
            "Feihao Chen",
            "Jinghui Li",
            "Xiaozhen Wang"
        ],
        "submitted": "2025-10-06 09:45:32",
        "source": "arxiv",
        "comment": "17 pages, 7 figures, 7 tables"
    },
    {
        "title": "Topic-Specific Classifiers are Better Relevance Judges than Prompted LLMs",
        "abstract": "The unjudged document problem, where pooled test collections have incomplete\nrelevance judgments for evaluating new retrieval systems, is a key obstacle to\nthe reusability of test collections in information retrieval. While the de\nfacto standard to deal with the problem is to treat unjudged documents as\nnon-relevant, many alternatives have been proposed, including the use of large\nlanguage models (LLMs) as a relevance judge (LLM-as-a-judge). However, this has\nbeen criticized as circular, since the same LLM can be used as a judge and as a\nranker at the same time. We propose to train topic-specific relevance\nclassifiers instead: By finetuning monoT5 with independent LoRA weight\nadaptation on the judgments of a single assessor for a single topic's pool, we\nalign it to that assessor's notion of relevance for the topic. The system\nrankings obtained through our classifier's relevance judgments achieve a\nSpearmans' $\\rho$ correlation of $>0.95$ with ground truth system rankings. As\nlittle as 128 initial human judgments per topic suffice to improve the\ncomparability of models, compared to treating unjudged documents as\nnon-relevant, while achieving more reliability than existing LLM-as-a-judge\napproaches. Topic-specific relevance classifiers thus are a lightweight and\nstraightforward way to tackle the unjudged document problem, while maintaining\nhuman judgments as the gold standard for retrieval evaluation. Code, models,\nand data are made openly available.",
        "url": "http://arxiv.org/abs/2510.04633v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04633v1",
        "arxiv_id": "2510.04633v1",
        "authors": [
            "Lukas Gienapp",
            "Martin Potthast",
            "Harrisen Scells",
            "Eugene Yang"
        ],
        "submitted": "2025-10-06 09:38:13",
        "source": "arxiv",
        "comment": "15 pages, 3 figures, 2 tables"
    },
    {
        "title": "Contrastive Learning Using Graph Embeddings for Domain Adaptation of Language Models in the Process Industry",
        "abstract": "Recent trends in NLP utilize knowledge graphs (KGs) to enhance pretrained\nlanguage models by incorporating additional knowledge from the graph structures\nto learn domain-specific terminology or relationships between documents that\nmight otherwise be overlooked. This paper explores how SciNCL, a graph-aware\nneighborhood contrastive learning methodology originally designed for\nscientific publications, can be applied to the process industry domain, where\ntext logs contain crucial information about daily operations and are often\nstructured as sparse KGs. Our experiments demonstrate that language models\nfine-tuned with triplets derived from graph embeddings (GE) outperform a\nstate-of-the-art mE5-large text encoder by 9.8-14.3% (5.45-7.96p) on the\nproprietary process industry text embedding benchmark (PITEB) while having 3\ntimes fewer parameters.",
        "url": "http://arxiv.org/abs/2510.04631v2",
        "pdf_url": "http://arxiv.org/pdf/2510.04631v2",
        "arxiv_id": "2510.04631v2",
        "authors": [
            "Anastasia Zhukova",
            "Jonas Lührs",
            "Christian E. Lobmüller",
            "Bela Gipp"
        ],
        "submitted": "2025-10-06 09:36:20",
        "source": "arxiv",
        "comment": "accepted to EMNLP 2025 (industry track)"
    },
    {
        "title": "Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models",
        "abstract": "Large language model (LLM) applications such as agents and domain-specific\nreasoning increasingly rely on context adaptation -- modifying inputs with\ninstructions, strategies, or evidence, rather than weight updates. Prior\napproaches improve usability but often suffer from brevity bias, which drops\ndomain insights for concise summaries, and from context collapse, where\niterative rewriting erodes details over time. Building on the adaptive memory\nintroduced by Dynamic Cheatsheet, we introduce ACE (Agentic Context\nEngineering), a framework that treats contexts as evolving playbooks that\naccumulate, refine, and organize strategies through a modular process of\ngeneration, reflection, and curation. ACE prevents collapse with structured,\nincremental updates that preserve detailed knowledge and scale with\nlong-context models. Across agent and domain-specific benchmarks, ACE optimizes\ncontexts both offline (e.g., system prompts) and online (e.g., agent memory),\nconsistently outperforming strong baselines: +10.6% on agents and +8.6% on\nfinance, while significantly reducing adaptation latency and rollout cost.\nNotably, ACE could adapt effectively without labeled supervision and instead by\nleveraging natural execution feedback. On the AppWorld leaderboard, ACE matches\nthe top-ranked production-level agent on the overall average and surpasses it\non the harder test-challenge split, despite using a smaller open-source model.\nThese results show that comprehensive, evolving contexts enable scalable,\nefficient, and self-improving LLM systems with low overhead.",
        "url": "http://arxiv.org/abs/2510.04618v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04618v1",
        "arxiv_id": "2510.04618v1",
        "authors": [
            "Qizheng Zhang",
            "Changran Hu",
            "Shubhangi Upasani",
            "Boyuan Ma",
            "Fenglu Hong",
            "Vamsidhar Kamanuru",
            "Jay Rainton",
            "Chen Wu",
            "Mengmeng Ji",
            "Hanchen Li",
            "Urmish Thakker",
            "James Zou",
            "Kunle Olukotun"
        ],
        "submitted": "2025-10-06 09:30:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "FedSRD: Sparsify-Reconstruct-Decompose for Communication-Efficient Federated Large Language Models Fine-Tuning",
        "abstract": "The current paradigm of training large language models (LLMs) on publicly\navailable Web data is becoming unsustainable, with high-quality data sources in\nspecialized domains nearing exhaustion. Federated Learning (FL) emerges as a\npractical solution for the next generation of AI on a decentralized Web,\nenabling privacy-preserving collaborative fine-tuning by leveraging private\ndata distributed across a global client base. While Low-Rank Adaptation (LoRA)\nis the standard for efficient fine-tuning, its application in federated\nsettings presents a critical challenge: communication overhead remains a\nsignificant bottleneck across the Web's heterogeneous network conditions. The\nstructural redundancy within LoRA parameters not only incurs a heavy\ncommunication burden but also introduces conflicts when aggregating client\nupdates. To address this, we propose FedSRD, a Sparsify-Reconstruct-Decompose\nframework designed for communication-efficient FL. We first introduce an\nimportance-aware sparsification method that preserves the structural integrity\nof LoRA updates to reduce the uploaded parameter count. The server then\nreconstructs and aggregates these updates in a full-rank space to mitigate\nconflicts. Finally, it decomposes the global update into a sparse low-rank\nformat for broadcast, ensuring a symmetrically efficient cycle. We also propose\nan efficient variant, FedSRD-e, to reduce computational overhead. Experimental\nresults on 10 benchmarks demonstrate that our framework significantly reduces\ncommunication costs by up to 90\\% while even improving model performance on\nheterogeneous client data.",
        "url": "http://arxiv.org/abs/2510.04601v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04601v1",
        "arxiv_id": "2510.04601v1",
        "authors": [
            "Guochen Yan",
            "Luyuan Xie",
            "Qingni Shen",
            "Yuejian Fang",
            "Zhonghai Wu"
        ],
        "submitted": "2025-10-06 09:06:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Robustness assessment of large audio language models in multiple-choice evaluation",
        "abstract": "Recent advances in large audio language models (LALMs) have primarily been\nassessed using a multiple-choice question answering (MCQA) framework. However,\nsubtle changes, such as shifting the order of choices, result in substantially\ndifferent results. Existing MCQA frameworks do not account for this variability\nand report a single accuracy number per benchmark or category. We dive into the\nMCQA evaluation framework and conduct a systematic study spanning three\nbenchmarks (MMAU, MMAR and MMSU) and four models: Audio Flamingo 2, Audio\nFlamingo 3, Qwen2.5-Omni-7B-Instruct, and Kimi-Audio-7B-Instruct. Our findings\nindicate that models are sensitive not only to the ordering of choices, but\nalso to the paraphrasing of the question and the choices. Finally, we propose a\nsimpler evaluation protocol and metric that account for subtle variations and\nprovide a more detailed evaluation report of LALMs within the MCQA framework.",
        "url": "http://arxiv.org/abs/2510.04584v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04584v1",
        "arxiv_id": "2510.04584v1",
        "authors": [
            "Fernando López",
            "Santosh Kesiraju",
            "Jordi Luque"
        ],
        "submitted": "2025-10-06 08:36:17",
        "source": "arxiv",
        "comment": "Submitted to ICASSP 2026"
    },
    {
        "title": "Can LLMs Detect Ambiguous Plural Reference? An Analysis of Split-Antecedent and Mereological Reference",
        "abstract": "Our goal is to study how LLMs represent and interpret plural reference in\nambiguous and unambiguous contexts. We ask the following research questions:\n(1) Do LLMs exhibit human-like preferences in representing plural reference?\n(2) Are LLMs able to detect ambiguity in plural anaphoric expressions and\nidentify possible referents? To address these questions, we design a set of\nexperiments, examining pronoun production using next-token prediction tasks,\npronoun interpretation, and ambiguity detection using different prompting\nstrategies. We then assess how comparable LLMs are to humans in formulating and\ninterpreting plural reference. We find that LLMs are sometimes aware of\npossible referents of ambiguous pronouns. However, they do not always follow\nhuman reference when choosing between interpretations, especially when the\npossible interpretation is not explicitly mentioned. In addition, they struggle\nto identify ambiguity without direct instruction. Our findings also reveal\ninconsistencies in the results across different types of experiments.",
        "url": "http://arxiv.org/abs/2510.04581v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04581v1",
        "arxiv_id": "2510.04581v1",
        "authors": [
            "Dang Anh",
            "Rick Nouwen",
            "Massimo Poesio"
        ],
        "submitted": "2025-10-06 08:32:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LaDiR: Latent Diffusion Enhances LLMs for Text Reasoning",
        "abstract": "Large Language Models (LLMs) demonstrate their reasoning ability through\nchain-of-thought (CoT) generation. However, LLM's autoregressive decoding may\nlimit the ability to revisit and refine earlier tokens in a holistic manner,\nwhich can also lead to inefficient exploration for diverse solutions. In this\npaper, we propose LaDiR (Latent Diffusion Reasoner), a novel reasoning\nframework that unifies the expressiveness of continuous latent representation\nwith the iterative refinement capabilities of latent diffusion models for an\nexisting LLM. We first construct a structured latent reasoning space using a\nVariational Autoencoder (VAE) that encodes text reasoning steps into blocks of\nthought tokens, preserving semantic information and interpretability while\noffering compact but expressive representations. Subsequently, we utilize a\nlatent diffusion model that learns to denoise a block of latent thought tokens\nwith a blockwise bidirectional attention mask, enabling longer horizon and\niterative refinement with adaptive test-time compute. This design allows\nefficient parallel generation of diverse reasoning trajectories, allowing the\nmodel to plan and revise the reasoning process holistically. We conduct\nevaluations on a suite of mathematical reasoning and planning benchmarks.\nEmpirical results show that LaDiR consistently improves accuracy, diversity,\nand interpretability over existing autoregressive, diffusion-based, and latent\nreasoning methods, revealing a new paradigm for text reasoning with latent\ndiffusion.",
        "url": "http://arxiv.org/abs/2510.04573v2",
        "pdf_url": "http://arxiv.org/pdf/2510.04573v2",
        "arxiv_id": "2510.04573v2",
        "authors": [
            "Haoqiang Kang",
            "Yizhe Zhang",
            "Nikki Lijing Kuang",
            "Nicklas Majamaki",
            "Navdeep Jaitly",
            "Yi-An Ma",
            "Lianhui Qin"
        ],
        "submitted": "2025-10-06 08:15:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Fine-grained auxiliary learning for real-world product recommendation",
        "abstract": "Product recommendation is the task of recovering the closest items to a given\nquery within a large product corpora. Generally, one can determine if\ntop-ranked products are related to the query by applying a similarity\nthreshold; exceeding it deems the product relevant, otherwise manual revision\nis required. Despite being a well-known problem, the integration of these\nmodels in real-world systems is often overlooked. In particular, production\nsystems have strong coverage requirements, i.e., a high proportion of\nrecommendations must be automated. In this paper we propose ALC , an Auxiliary\nLearning strategy that boosts Coverage through learning fine-grained\nembeddings. Concretely, we introduce two training objectives that leverage the\nhardest negatives in the batch to build discriminative training signals between\npositives and negatives. We validate ALC using three extreme multi-label\nclassification approaches in two product recommendation datasets;\nLF-AmazonTitles-131K and Tech and Durables (proprietary), demonstrating\nstate-of-the-art coverage rates when combined with a recent\nthreshold-consistent margin loss.",
        "url": "http://arxiv.org/abs/2510.04551v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04551v1",
        "arxiv_id": "2510.04551v1",
        "authors": [
            "Mario Almagro",
            "Diego Ortego",
            "David Jimenez"
        ],
        "submitted": "2025-10-06 07:34:06",
        "source": "arxiv",
        "comment": "SEPLN 2025"
    },
    {
        "title": "More Than Meets the Eye? Uncovering the Reasoning-Planning Disconnect in Training Vision-Language Driving Models",
        "abstract": "Vision-Language Model (VLM) driving agents promise explainable end-to-end\nautonomy by first producing natural-language reasoning and then predicting\ntrajectory planning. However, whether planning is causally driven by this\nreasoning remains a critical but unverified assumption. To investigate this, we\nbuild DriveMind, a large-scale driving Visual Question Answering (VQA) corpus\nwith plan-aligned Chain-of-Thought (CoT), automatically generated from nuPlan.\nOur data generation process converts sensors and annotations into structured\ninputs and, crucially, separates priors from to-be-reasoned signals, enabling\nclean information ablations. Using DriveMind, we train representative VLM\nagents with Supervised Fine-Tuning (SFT) and Group Relative Policy Optimization\n(GRPO) and evaluate them with nuPlan's metrics. Our results, unfortunately,\nindicate a consistent causal disconnect in reasoning-planning: removing\nego/navigation priors causes large drops in planning scores, whereas removing\nCoT produces only minor changes. Attention analysis further shows that planning\nprimarily focuses on priors rather than the CoT. Based on this evidence, we\npropose the Reasoning-Planning Decoupling Hypothesis, positing that the\ntraining-yielded reasoning is an ancillary byproduct rather than a causal\nmediator. To enable efficient diagnosis, we also introduce a novel,\ntraining-free probe that measures an agent's reliance on priors by evaluating\nits planning robustness against minor input perturbations. In summary, we\nprovide the community with a new dataset and a diagnostic tool to evaluate the\ncausal fidelity of future models.",
        "url": "http://arxiv.org/abs/2510.04532v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04532v1",
        "arxiv_id": "2510.04532v1",
        "authors": [
            "Xurui Song",
            "Shuo Huai",
            "JingJing Jiang",
            "Jiayi Kong",
            "Jun Luo"
        ],
        "submitted": "2025-10-06 06:50:16",
        "source": "arxiv",
        "comment": "The dataset will be released publicly once the paper is accepted for\n  publication"
    },
    {
        "title": "ChartAgent: A Multimodal Agent for Visually Grounded Reasoning in Complex Chart Question Answering",
        "abstract": "Recent multimodal LLMs have shown promise in chart-based visual question\nanswering, but their performance declines sharply on unannotated charts, those\nrequiring precise visual interpretation rather than relying on textual\nshortcuts. To address this, we introduce ChartAgent, a novel agentic framework\nthat explicitly performs visual reasoning directly within the chart's spatial\ndomain. Unlike textual chain-of-thought reasoning, ChartAgent iteratively\ndecomposes queries into visual subtasks and actively manipulates and interacts\nwith chart images through specialized actions such as drawing annotations,\ncropping regions (e.g., segmenting pie slices, isolating bars), and localizing\naxes, using a library of chart-specific vision tools to fulfill each subtask.\nThis iterative reasoning process closely mirrors human cognitive strategies for\nchart comprehension. ChartAgent achieves state-of-the-art accuracy on the\nChartBench and ChartX benchmarks, surpassing prior methods by up to 16.07%\nabsolute gain overall and 17.31% on unannotated, numerically intensive queries.\nFurthermore, our analyses show that ChartAgent is (a) effective across diverse\nchart types, (b) achieve the highest scores across varying visual and reasoning\ncomplexity levels, and (c) serves as a plug-and-play framework that boosts\nperformance across diverse underlying LLMs. Our work is among the first to\ndemonstrate visually grounded reasoning for chart understanding using\ntool-augmented multimodal agents.",
        "url": "http://arxiv.org/abs/2510.04514v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04514v1",
        "arxiv_id": "2510.04514v1",
        "authors": [
            "Rachneet Kaur",
            "Nishan Srishankar",
            "Zhen Zeng",
            "Sumitra Ganesh",
            "Manuela Veloso"
        ],
        "submitted": "2025-10-06 06:05:36",
        "source": "arxiv",
        "comment": "53 pages, 12 figures, 15 tables"
    },
    {
        "title": "MARCO: A Cooperative Knowledge Transfer Framework for Personalized Cross-domain Recommendations",
        "abstract": "Recommender systems frequently encounter data sparsity issues, particularly\nwhen addressing cold-start scenarios involving new users or items. Multi-source\ncross-domain recommendation (CDR) addresses these challenges by transferring\nvaluable knowledge from multiple source domains to enhance recommendations in a\ntarget domain. However, existing reinforcement learning (RL)-based CDR methods\ntypically rely on a single-agent framework, leading to negative transfer issues\ncaused by inconsistent domain contributions and inherent distributional\ndiscrepancies among source domains. To overcome these limitations, MARCO, a\nMulti-Agent Reinforcement Learning-based Cross-Domain recommendation framework,\nis proposed. It leverages cooperative multi-agent reinforcement learning, where\neach agent is dedicated to estimating the contribution from an individual\nsource domain, effectively managing credit assignment and mitigating negative\ntransfer. In addition, an entropy-based action diversity penalty is introduced\nto enhance policy expressiveness and stabilize training by encouraging diverse\nagents' joint actions. Extensive experiments across four benchmark datasets\ndemonstrate MARCO's superior performance over state-of-the-art methods,\nhighlighting its robustness and strong generalization capabilities. The code is\nat https://github.com/xiewilliams/MARCO.",
        "url": "http://arxiv.org/abs/2510.04508v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04508v1",
        "arxiv_id": "2510.04508v1",
        "authors": [
            "Lili Xie",
            "Yi Zhang",
            "Ruihong Qiu",
            "Jiajun Liu",
            "Sen Wang"
        ],
        "submitted": "2025-10-06 05:49:47",
        "source": "arxiv",
        "comment": "SIGIR-AP 2025"
    },
    {
        "title": "GRACE: Generative Representation Learning via Contrastive Policy Optimization",
        "abstract": "Prevailing methods for training Large Language Models (LLMs) as text encoders\nrely on contrastive losses that treat the model as a black box function,\ndiscarding its generative and reasoning capabilities in favor of static\nembeddings. We introduce GRACE (Generative Representation Learning via\nContrastive Policy Optimization), a novel framework that reimagines contrastive\nsignals not as losses to be minimized, but as rewards that guide a generative\npolicy. In GRACE, the LLM acts as a policy that produces explicit,\nhuman-interpretable rationales--structured natural language explanations of its\nsemantic understanding. These rationales are then encoded into high-quality\nembeddings via mean pooling. Using policy gradient optimization, we train the\nmodel with a multi-component reward function that maximizes similarity between\nquery positive pairs and minimizes similarity with negatives. This transforms\nthe LLM from an opaque encoder into an interpretable agent whose reasoning\nprocess is transparent and inspectable. On MTEB benchmark, GRACE yields broad\ncross category gains: averaged over four backbones, the supervised setting\nimproves overall score by 11.5% over base models, and the unsupervised variant\nadds 6.9%, while preserving general capabilities. This work treats contrastive\nobjectives as rewards over rationales, unifying representation learning with\ngeneration to produce stronger embeddings and transparent rationales. The\nmodel, data and code are available at https://github.com/GasolSun36/GRACE.",
        "url": "http://arxiv.org/abs/2510.04506v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04506v1",
        "arxiv_id": "2510.04506v1",
        "authors": [
            "Jiashuo Sun",
            "Shixuan Liu",
            "Zhaochen Su",
            "Xianrui Zhong",
            "Pengcheng Jiang",
            "Bowen Jin",
            "Peiran Li",
            "Weijia Shi",
            "Jiawei Han"
        ],
        "submitted": "2025-10-06 05:46:56",
        "source": "arxiv",
        "comment": "23 pages, 7 figures, 7 tables"
    },
    {
        "title": "P2P: A Poison-to-Poison Remedy for Reliable Backdoor Defense in LLMs",
        "abstract": "During fine-tuning, large language models (LLMs) are increasingly vulnerable\nto data-poisoning backdoor attacks, which compromise their reliability and\ntrustworthiness. However, existing defense strategies suffer from limited\ngeneralization: they only work on specific attack types or task settings. In\nthis study, we propose Poison-to-Poison (P2P), a general and effective backdoor\ndefense algorithm. P2P injects benign triggers with safe alternative labels\ninto a subset of training samples and fine-tunes the model on this re-poisoned\ndataset by leveraging prompt-based learning. This enforces the model to\nassociate trigger-induced representations with safe outputs, thereby overriding\nthe effects of original malicious triggers. Thanks to this robust and\ngeneralizable trigger-based fine-tuning, P2P is effective across task settings\nand attack types. Theoretically and empirically, we show that P2P can\nneutralize malicious backdoors while preserving task performance. We conduct\nextensive experiments on classification, mathematical reasoning, and summary\ngeneration tasks, involving multiple state-of-the-art LLMs. The results\ndemonstrate that our P2P algorithm significantly reduces the attack success\nrate compared with baseline models. We hope that the P2P can serve as a\nguideline for defending against backdoor attacks and foster the development of\na secure and trustworthy LLM community.",
        "url": "http://arxiv.org/abs/2510.04503v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04503v1",
        "arxiv_id": "2510.04503v1",
        "authors": [
            "Shuai Zhao",
            "Xinyi Wu",
            "Shiqian Zhao",
            "Xiaobao Wu",
            "Zhongliang Guo",
            "Yanhao Jia",
            "Anh Tuan Luu"
        ],
        "submitted": "2025-10-06 05:45:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Causality-aware Graph Aggregation Weight Estimator for Popularity Debiasing in Top-K Recommendation",
        "abstract": "Graph-based recommender systems leverage neighborhood aggregation to generate\nnode representations, which is highly sensitive to popularity bias, resulting\nin an echo effect during information propagation. Existing graph-based\ndebiasing solutions refine the aggregation process with attempts such as edge\nreconstruction or weight adjustment. However, these methods remain inadequate\nin fully alleviating popularity bias. Specifically, this is because 1) they\nprovide no insights into graph aggregation rationality, thus lacking an\noptimality guarantee; 2) they fail to well balance the training and debiasing\nprocess, which undermines the effectiveness. In this paper, we propose a novel\napproach to mitigate popularity bias through rational modeling of the graph\naggregation process. We reveal that graph aggregation is a special form of\nbackdoor adjustment in causal inference, where the aggregation weight\ncorresponds to the historical interaction likelihood distribution. Based on\nthis insight, we devise an encoder-decoder architecture, namely Causality-aware\nGraph Aggregation Weight Estimator for Debiasing (CAGED), to approximate the\nunbiased aggregation weight by optimizing the evidence lower bound of the\ninteraction likelihood. In order to enhance the debiasing effectiveness during\nearly training stages, we further design a momentum update strategy that\nincrementally refines the aggregation weight matrix. Extensive experiments on\nthree datasets demonstrate that CAGED outperforms existing graph-based\ndebiasing methods. Our implementation is available at\nhttps://github.com/QueYork/CAGED.",
        "url": "http://arxiv.org/abs/2510.04502v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04502v1",
        "arxiv_id": "2510.04502v1",
        "authors": [
            "Yue Que",
            "Yingyi Zhang",
            "Xiangyu Zhao",
            "Chen Ma"
        ],
        "submitted": "2025-10-06 05:33:37",
        "source": "arxiv",
        "comment": "Accepted by CIKM 2025"
    },
    {
        "title": "GenQuest: An LLM-based Text Adventure Game for Language Learners",
        "abstract": "GenQuest is a generative text adventure game that leverages Large Language\nModels (LLMs) to facilitate second language learning through immersive,\ninteractive storytelling. The system engages English as a Foreign Language\n(EFL) learners in a collaborative \"choose-your-own-adventure\" style narrative,\ndynamically generated in response to learner choices. Game mechanics such as\nbranching decision points and story milestones are incorporated to maintain\nnarrative coherence while allowing learner-driven plot development. Key\npedagogical features include content generation tailored to each learner's\nproficiency level, and a vocabulary assistant that provides in-context\nexplanations of learner-queried text strings, ranging from words and phrases to\nsentences. Findings from a pilot study with university EFL students in China\nindicate promising vocabulary gains and positive user perceptions. Also\ndiscussed are suggestions from participants regarding the narrative length and\nquality, and the request for multi-modal content such as illustrations.",
        "url": "http://arxiv.org/abs/2510.04498v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04498v1",
        "arxiv_id": "2510.04498v1",
        "authors": [
            "Qiao Wang",
            "Adnan Labib",
            "Robert Swier",
            "Michael Hofmeyr",
            "Zheng Yuan"
        ],
        "submitted": "2025-10-06 05:22:53",
        "source": "arxiv",
        "comment": "Workshop on Wordplay: When Language Meets Games, EMNLP 2025"
    },
    {
        "title": "Impatient Users Confuse AI Agents: High-fidelity Simulations of Human Traits for Testing Agents",
        "abstract": "Despite rapid progress in building conversational AI agents, robustness is\nstill largely untested. Small shifts in user behavior, such as being more\nimpatient, incoherent, or skeptical, can cause sharp drops in agent\nperformance, revealing how brittle current AI agents are. Today's benchmarks\nfail to capture this fragility: agents may perform well under standard\nevaluations but degrade spectacularly in more realistic and varied settings. We\naddress this robustness testing gap by introducing TraitBasis, a lightweight,\nmodel-agnostic method for systematically stress testing AI agents. TraitBasis\nlearns directions in activation space corresponding to steerable user traits\n(e.g., impatience or incoherence), which can be controlled, scaled, composed,\nand applied at inference time without any fine-tuning or extra data. Using\nTraitBasis, we extend $\\tau$-Bench to $\\tau$-Trait, where user behaviors are\naltered via controlled trait vectors. We observe on average a 2%-30%\nperformance degradation on $\\tau$-Trait across frontier models, highlighting\nthe lack of robustness of current AI agents to variations in user behavior.\nTogether, these results highlight both the critical role of robustness testing\nand the promise of TraitBasis as a simple, data-efficient, and compositional\ntool. By powering simulation-driven stress tests and training loops, TraitBasis\nopens the door to building AI agents that remain reliable in the unpredictable\ndynamics of real-world human interactions. We have open-sourced $\\tau$-Trai\nacross four domains: airline, retail, telecom, and telehealth, so the community\ncan systematically QA their agents under realistic, behaviorally diverse\nintents and trait scenarios: https://github.com/collinear-ai/tau-trait.",
        "url": "http://arxiv.org/abs/2510.04491v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04491v1",
        "arxiv_id": "2510.04491v1",
        "authors": [
            "Muyu He",
            "Anand Kumar",
            "Tsach Mackey",
            "Meghana Rajeev",
            "James Zou",
            "Nazneen Rajani"
        ],
        "submitted": "2025-10-06 05:03:57",
        "source": "arxiv",
        "comment": "25 pages"
    },
    {
        "title": "Psychological Steering in LLMs: An Evaluation of Effectiveness and Trustworthiness",
        "abstract": "The ability to control LLMs' emulated emotional states and personality traits\nis essential for enabling rich, human-centered interactions in socially\ninteractive settings. We introduce PsySET, a Psychologically-informed benchmark\nto evaluate LLM Steering Effectiveness and Trustworthiness across the emotion\nand personality domains. Our study spans four models from different LLM\nfamilies paired with various steering strategies, including prompting,\nfine-tuning, and representation engineering. Our results indicate that\nprompting is consistently effective but limited in intensity control, whereas\nvector injections achieve finer controllability while slightly reducing output\nquality. Moreover, we explore the trustworthiness of steered LLMs by assessing\nsafety, truthfulness, fairness, and ethics, highlighting potential side effects\nand behavioral shifts. Notably, we observe idiosyncratic effects; for instance,\neven a positive emotion like joy can degrade robustness to adversarial\nfactuality, lower privacy awareness, and increase preferential bias. Meanwhile,\nanger predictably elevates toxicity yet strengthens leakage resistance. Our\nframework establishes the first holistic evaluation of emotion and personality\nsteering, offering insights into its interpretability and reliability for\nsocially interactive applications.",
        "url": "http://arxiv.org/abs/2510.04484v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04484v1",
        "arxiv_id": "2510.04484v1",
        "authors": [
            "Amin Banayeeanzade",
            "Ala N. Tak",
            "Fatemeh Bahrani",
            "Anahita Bolourani",
            "Leonardo Blas",
            "Emilio Ferrara",
            "Jonathan Gratch",
            "Sai Praneeth Karimireddy"
        ],
        "submitted": "2025-10-06 04:49:56",
        "source": "arxiv",
        "comment": "Submitted to ARR - October 2025"
    },
    {
        "title": "MedCLM: Learning to Localize and Reason via a CoT-Curriculum in Medical Vision-Language Models",
        "abstract": "Bridging clinical diagnostic reasoning with AI remains a central challenge in\nmedical imaging. We introduce MedCLM, an automated pipeline that converts\ndetection datasets into large-scale medical visual question answering (VQA)\ndata with Chain-of-Thought (CoT) reasoning by linking lesion boxes to organ\nsegmentation and structured rationales. These contextual signals enable medical\nvision-language models to generate question-answer pairs with step-by-step\nreasoning. To utilize this data effectively, we propose an Integrated\nCoT-Curriculum Strategy composed of an Easy stage with explicit lesion boxes\nfor visual grounding, a Medium stage that encourages implicit localization, and\na Hard stage for weakly supervised reasoning. Experimental results demonstrate\nthat MedCLM attains state-of-the-art performance on several medical VQA\nbenchmarks, providing a scalable framework for developing clinically aligned\nmedical vision-language models.",
        "url": "http://arxiv.org/abs/2510.04477v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04477v1",
        "arxiv_id": "2510.04477v1",
        "authors": [
            "Soo Yong Kim",
            "Suin Cho",
            "Vincent-Daniel Yun",
            "Gyeongyeon Hwang"
        ],
        "submitted": "2025-10-06 04:26:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Compressed Convolutional Attention: Efficient Attention in a Compressed Latent Space",
        "abstract": "Multi-headed Attention's (MHA) quadratic compute and linearly growing\nKV-cache make long-context transformers expensive to train and serve. Prior\nworks such as Grouped Query Attention (GQA) and Multi-Latent Attention (MLA)\nshrink the cache, speeding decode, but leave compute, which determines prefill\nand training speed, largely unchanged. We introduce Compressed Convolutional\nAttention (CCA), a novel attention method which down-projects queries, keys,\nand values and performs the entire attention operation inside the shared latent\nspace. This simple design dramatically cuts parameters, KV-cache, and FLOPs all\nat once by the desired compression factor. Because CCA is orthogonal to\nhead-sharing, we combine the two to form Compressed Convolutional Grouped Query\nAttention (CCGQA), which further tightens the compute-bandwidth Pareto frontier\nso that users can tune compression toward either FLOP or memory limits without\nsacrificing quality. Experiments show that CCGQA consistently outperforms both\nGQA and MLA at equal KV-cache compression on dense and MoE models.\nAdditionally, we show that CCGQA outperforms all other attention methods on MoE\nmodels with half the KV-cache of GQA and MLA, achieving an 8x KV-cache\ncompression with no drop in performance compared to standard MHA. CCA and CCGQA\nalso dramatically reduce the FLOP cost of attention which leads to\nsubstantially faster training and prefill than existing methods. On H100 GPUs,\nour fused CCA/CCGQA kernel reduces prefill latency by about 1.7x at a sequence\nlength of 16k relative to MHA, and accelerates backward by about 1.3x.",
        "url": "http://arxiv.org/abs/2510.04476v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04476v1",
        "arxiv_id": "2510.04476v1",
        "authors": [
            "Tomas Figliolia",
            "Nicholas Alonso",
            "Rishi Iyer",
            "Quentin Anthony",
            "Beren Millidge"
        ],
        "submitted": "2025-10-06 04:24:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Mitigating Forgetting Between Supervised and Reinforcement Learning Yields Stronger Reasoners",
        "abstract": "Large Language Models (LLMs) show strong reasoning abilities, often amplified\nby Chain-of-Thought (CoT) prompting and reinforcement learning (RL). Although\nRL algorithms can substantially improve reasoning, they struggle to expand\nreasoning boundaries because they learn from their own reasoning trajectories\nrather than acquiring external knowledge. Supervised fine-tuning (SFT) offers\ncomplementary benefits but typically requires large-scale data and risks\noverfitting. Recent attempts to combine SFT and RL face three main challenges:\ndata inefficiency, algorithm-specific designs, and catastrophic forgetting. We\npropose a plug-and-play framework that dynamically integrates SFT into RL by\nselecting challenging examples for SFT. This approach reduces SFT data\nrequirements and remains agnostic to the choice of RL or SFT algorithm. To\nmitigate catastrophic forgetting of RL-acquired skills during SFT, we select\nhigh-entropy tokens for loss calculation and freeze parameters identified as\ncritical for RL. Our method achieves state-of-the-art (SoTA) reasoning\nperformance using only 1.5% of the SFT data and 20.4% of the RL data used by\nprior SoTA, providing an efficient and plug-and-play solution for combining SFT\nand RL in reasoning post-training.",
        "url": "http://arxiv.org/abs/2510.04454v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04454v1",
        "arxiv_id": "2510.04454v1",
        "authors": [
            "Xiangchi Yuan",
            "Xiang Chen",
            "Tong Yu",
            "Dachuan Shi",
            "Can Jin",
            "Wenke Lee",
            "Saayan Mitra"
        ],
        "submitted": "2025-10-06 03:01:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "On the Role of Unobserved Sequences on Sample-based Uncertainty Quantification for LLMs",
        "abstract": "Quantifying uncertainty in large language models (LLMs) is important for\nsafety-critical applications because it helps spot incorrect answers, known as\nhallucinations. One major trend of uncertainty quantification methods is based\non estimating the entropy of the distribution of the LLM's potential output\nsequences. This estimation is based on a set of output sequences and associated\nprobabilities obtained by querying the LLM several times. In this paper, we\nadvocate and experimentally show that the probability of unobserved sequences\nplays a crucial role, and we recommend future research to integrate it to\nenhance such LLM uncertainty quantification methods.",
        "url": "http://arxiv.org/abs/2510.04439v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04439v1",
        "arxiv_id": "2510.04439v1",
        "authors": [
            "Lucie Kunitomo-Jacquin",
            "Edison Marrese-Taylor",
            "Ken Fukuda"
        ],
        "submitted": "2025-10-06 02:14:48",
        "source": "arxiv",
        "comment": "Accepted to UncertaiNLP workshop of EMNLP 2025"
    },
    {
        "title": "Good Intentions Beyond ACL: Who Does NLP for Social Good, and Where?",
        "abstract": "The social impact of Natural Language Processing (NLP) is increasingly\nimportant, with a rising community focus on initiatives related to NLP for\nSocial Good (NLP4SG). Indeed, in recent years, almost 20% of all papers in the\nACL Anthology address topics related to social good as defined by the UN\nSustainable Development Goals (Adauto et al., 2023). In this study, we take an\nauthor- and venue-level perspective to map the landscape of NLP4SG, quantifying\nthe proportion of work addressing social good concerns both within and beyond\nthe ACL community, by both core ACL contributors and non-ACL authors. With this\napproach we discover two surprising facts about the landscape of NLP4SG. First,\nACL authors are dramatically more likely to do work addressing social good\nconcerns when publishing in venues outside of ACL. Second, the vast majority of\npublications using NLP techniques to address concerns of social good are done\nby non-ACL authors in venues outside of ACL. We discuss the implications of\nthese findings on agenda-setting considerations for the ACL community related\nto NLP4SG.",
        "url": "http://arxiv.org/abs/2510.04434v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04434v1",
        "arxiv_id": "2510.04434v1",
        "authors": [
            "Grace LeFevre",
            "Qingcheng Zeng",
            "Adam Leif",
            "Jason Jewell",
            "Denis Peskoff",
            "Rob Voigt"
        ],
        "submitted": "2025-10-06 02:04:42",
        "source": "arxiv",
        "comment": "EMNLP 2025"
    },
    {
        "title": "Partial Information Decomposition via Normalizing Flows in Latent Gaussian Distributions",
        "abstract": "The study of multimodality has garnered significant interest in fields where\nthe analysis of interactions among multiple information sources can enhance\npredictive modeling, data fusion, and interpretability. Partial information\ndecomposition (PID) has emerged as a useful information-theoretic framework to\nquantify the degree to which individual modalities independently, redundantly,\nor synergistically convey information about a target variable. However,\nexisting PID methods depend on optimizing over a joint distribution constrained\nby estimated pairwise probability distributions, which are costly and\ninaccurate for continuous and high-dimensional modalities. Our first key\ninsight is that the problem can be solved efficiently when the pairwise\ndistributions are multivariate Gaussians, and we refer to this problem as\nGaussian PID (GPID). We propose a new gradient-based algorithm that\nsubstantially improves the computational efficiency of GPID based on an\nalternative formulation of the underlying optimization problem. To generalize\nthe applicability to non-Gaussian data, we learn information-preserving\nencoders to transform random variables of arbitrary input distributions into\npairwise Gaussian random variables. Along the way, we resolved an open problem\nregarding the optimality of joint Gaussian solutions for GPID. Empirical\nvalidation in diverse synthetic examples demonstrates that our proposed method\nprovides more accurate and efficient PID estimates than existing baselines. We\nfurther evaluate a series of large-scale multimodal benchmarks to show its\nutility in real-world applications of quantifying PID in multimodal datasets\nand selecting high-performing models.",
        "url": "http://arxiv.org/abs/2510.04417v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04417v1",
        "arxiv_id": "2510.04417v1",
        "authors": [
            "Wenyuan Zhao",
            "Adithya Balachandran",
            "Chao Tian",
            "Paul Pu Liang"
        ],
        "submitted": "2025-10-06 01:08:34",
        "source": "arxiv",
        "comment": "NeurIPS 2025"
    },
    {
        "title": "Large Language Models Preserve Semantic Isotopies in Story Continuations",
        "abstract": "In this work, we explore the relevance of textual semantics to Large Language\nModels (LLMs), extending previous insights into the connection between\ndistributional semantics and structural semantics. We investigate whether\nLLM-generated texts preserve semantic isotopies. We design a story continuation\nexperiment using 10,000 ROCStories prompts completed by five LLMs. We first\nvalidate GPT-4o's ability to extract isotopies from a linguistic benchmark,\nthen apply it to the generated stories. We then analyze structural (coverage,\ndensity, spread) and semantic properties of isotopies to assess how they are\naffected by completion. Results show that LLM completion within a given token\nhorizon preserves semantic isotopies across multiple properties.",
        "url": "http://arxiv.org/abs/2510.04400v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04400v1",
        "arxiv_id": "2510.04400v1",
        "authors": [
            "Marc Cavazza"
        ],
        "submitted": "2025-10-06 00:03:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SECA: Semantically Equivalent and Coherent Attacks for Eliciting LLM Hallucinations",
        "abstract": "Large Language Models (LLMs) are increasingly deployed in high-risk domains.\nHowever, state-of-the-art LLMs often produce hallucinations, raising serious\nconcerns about their reliability. Prior work has explored adversarial attacks\nfor hallucination elicitation in LLMs, but it often produces unrealistic\nprompts, either by inserting gibberish tokens or by altering the original\nmeaning. As a result, these approaches offer limited insight into how\nhallucinations may occur in practice. While adversarial attacks in computer\nvision often involve realistic modifications to input images, the problem of\nfinding realistic adversarial prompts for eliciting LLM hallucinations has\nremained largely underexplored. To address this gap, we propose Semantically\nEquivalent and Coherent Attacks (SECA) to elicit hallucinations via realistic\nmodifications to the prompt that preserve its meaning while maintaining\nsemantic coherence. Our contributions are threefold: (i) we formulate finding\nrealistic attacks for hallucination elicitation as a constrained optimization\nproblem over the input prompt space under semantic equivalence and coherence\nconstraints; (ii) we introduce a constraint-preserving zeroth-order method to\neffectively search for adversarial yet feasible prompts; and (iii) we\ndemonstrate through experiments on open-ended multiple-choice question\nanswering tasks that SECA achieves higher attack success rates while incurring\nalmost no constraint violations compared to existing methods. SECA highlights\nthe sensitivity of both open-source and commercial gradient-inaccessible LLMs\nto realistic and plausible prompt variations. Code is available at\nhttps://github.com/Buyun-Liang/SECA.",
        "url": "http://arxiv.org/abs/2510.04398v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04398v1",
        "arxiv_id": "2510.04398v1",
        "authors": [
            "Buyun Liang",
            "Liangzu Peng",
            "Jinqi Luo",
            "Darshan Thaker",
            "Kwan Ho Ryan Chan",
            "René Vidal"
        ],
        "submitted": "2025-10-05 23:44:54",
        "source": "arxiv",
        "comment": "Accepted at NeurIPS 2025. Code is available at\n  https://github.com/Buyun-Liang/SECA"
    },
    {
        "title": "Evaluating Keyframe Layouts for Visual Known-Item Search in Homogeneous Collections",
        "abstract": "Multimodal deep-learning models power interactive video retrieval by ranking\nkeyframes in response to textual queries. Despite these advances, users must\nstill browse ranked candidates manually to locate a target. Keyframe\narrangement within the search grid highly affects browsing effectiveness and\nuser efficiency, yet remains underexplored. We report a study with 49\nparticipants evaluating seven keyframe layouts for the Visual Known-Item Search\ntask. Beyond efficiency and accuracy, we relate browsing phenomena, such as\noverlooks, to layout characteristics. Our results show that a video-grouped\nlayout is the most efficient, while a four-column, rank-preserving grid\nachieves the highest accuracy. Sorted grids reveal potentials and trade-offs,\nenabling rapid scanning of uninteresting regions but down-ranking relevant\ntargets to less prominent positions, delaying first arrival times and\nincreasing overlooks.\n  These findings motivate hybrid designs that preserve positions of top-ranked\nitems while sorting or grouping the remainder, and offer guidance for searching\nin grids beyond video retrieval.",
        "url": "http://arxiv.org/abs/2510.04396v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04396v1",
        "arxiv_id": "2510.04396v1",
        "authors": [
            "Bastian Jäckl",
            "Jiří Kruchina",
            "Lucas Joos",
            "Daniel A. Keim",
            "Ladislav Peška",
            "Jakub Lokoč"
        ],
        "submitted": "2025-10-05 23:30:33",
        "source": "arxiv",
        "comment": "28 Pages, 17 Figures"
    },
    {
        "title": "Time Is Effort: Estimating Human Post-Editing Time for Grammar Error Correction Tool Evaluation",
        "abstract": "Text editing can involve several iterations of revision. Incorporating an\nefficient Grammar Error Correction (GEC) tool in the initial correction round\ncan significantly impact further human editing effort and final text quality.\nThis raises an interesting question to quantify GEC Tool usability: How much\neffort can the GEC Tool save users? We present the first large-scale dataset of\npost-editing (PE) time annotations and corrections for two English GEC test\ndatasets (BEA19 and CoNLL14). We introduce Post-Editing Effort in Time (PEET)\nfor GEC Tools as a human-focused evaluation scorer to rank any GEC Tool by\nestimating PE time-to-correct. Using our dataset, we quantify the amount of\ntime saved by GEC Tools in text editing. Analyzing the edit type indicated that\ndetermining whether a sentence needs correction and edits like paraphrasing and\npunctuation changes had the greatest impact on PE time. Finally, comparison\nwith human rankings shows that PEET correlates well with technical effort\njudgment, providing a new human-centric direction for evaluating GEC tool\nusability. We release our dataset and code at:\nhttps://github.com/ankitvad/PEET_Scorer.",
        "url": "http://arxiv.org/abs/2510.04394v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04394v1",
        "arxiv_id": "2510.04394v1",
        "authors": [
            "Ankit Vadehra",
            "Bill Johnson",
            "Gene Saunders",
            "Pascal Poupart"
        ],
        "submitted": "2025-10-05 23:24:24",
        "source": "arxiv",
        "comment": "Accepted for publication in the 4th HCI+NLP Workshop (Fourth Workshop\n  on Bridging Human-Computer Interaction and Natural Language Processing; part\n  of EMNLP 2025)"
    },
    {
        "title": "Improving Consistency in Retrieval-Augmented Systems with Group Similarity Rewards",
        "abstract": "RAG systems are increasingly deployed in high-stakes domains where users\nexpect outputs to be consistent across semantically equivalent queries.\nHowever, existing systems often exhibit significant inconsistencies due to\nvariability in both the retriever and generator (LLM), undermining trust and\nreliability. In this work, we focus on information consistency, i.e., the\nrequirement that outputs convey the same core content across semantically\nequivalent inputs. We introduce a principled evaluation framework that\ndecomposes RAG consistency into retriever-level, generator-level, and\nend-to-end components, helping identify inconsistency sources. To improve\nconsistency, we propose Paraphrased Set Group Relative Policy Optimization\n(PS-GRPO), an RL approach that leverages multiple rollouts across paraphrased\nset to assign group similarity rewards. We leverage PS-GRPO to achieve\nInformation Consistent RAG (Con-RAG), training the generator to produce\nconsistent outputs across paraphrased queries and remain robust to\nretrieval-induced variability. Because exact reward computation over paraphrase\nsets is computationally expensive, we also introduce a scalable approximation\nmethod that retains effectiveness while enabling efficient, large-scale\ntraining. Empirical evaluations across short-form, multi-hop, and long-form QA\nbenchmarks demonstrate that Con-RAG significantly improves both consistency and\naccuracy over strong baselines, even in the absence of explicit ground-truth\nsupervision. Our work provides practical solutions for evaluating and building\nreliable RAG systems for safety-critical deployments.",
        "url": "http://arxiv.org/abs/2510.04392v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04392v1",
        "arxiv_id": "2510.04392v1",
        "authors": [
            "Faisal Hamman",
            "Chenyang Zhu",
            "Anoop Kumar",
            "Xujun Peng",
            "Sanghamitra Dutta",
            "Daben Liu",
            "Alfy Samuel"
        ],
        "submitted": "2025-10-05 23:14:13",
        "source": "arxiv",
        "comment": "Accepted at NeurIPS 2025 Workshop on Reliable ML from Unreliable Data"
    },
    {
        "title": "Internal World Models as Imagination Networks in Cognitive Agents",
        "abstract": "What is the computational objective of imagination? While classical\ninterpretations suggest imagination is useful for maximizing rewards, recent\nfindings challenge this view. In this study, we propose that imagination serves\nto access an internal world model (IWM) and use psychological network analysis\nto explore IWMs in humans and large language models (LLMs). Specifically, we\nassessed imagination vividness ratings using two questionnaires and constructed\nimagination networks from these reports. Imagination networks from human groups\nshowed correlations between different centrality measures, including expected\ninfluence, strength, and closeness. However, imagination networks from LLMs\nshowed a lack of clustering and lower correlations between centrality measures\nunder different prompts and conversational memory conditions. Together, these\nresults indicate a lack of similarity between IWMs in human and LLM agents.\nOverall, our study offers a novel method for comparing internally-generated\nrepresentations in humans and AI, providing insights for developing human-like\nimagination in artificial intelligence.",
        "url": "http://arxiv.org/abs/2510.04391v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04391v1",
        "arxiv_id": "2510.04391v1",
        "authors": [
            "Saurabh Ranjan",
            "Brian Odegaard"
        ],
        "submitted": "2025-10-05 23:01:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MorphoSim: An Interactive, Controllable, and Editable Language-guided 4D World Simulator",
        "abstract": "World models that support controllable\n  and editable spatiotemporal environments are valuable\n  for robotics, enabling scalable training data, repro ducible evaluation, and\nflexible task design. While\n  recent text-to-video models generate realistic dynam ics, they are\nconstrained to 2D views and offer limited\n  interaction. We introduce MorphoSim, a language guided framework that\ngenerates 4D scenes with\n  multi-view consistency and object-level controls. From\n  natural language instructions, MorphoSim produces\n  dynamic environments where objects can be directed,\n  recolored, or removed, and scenes can be observed\n  from arbitrary viewpoints. The framework integrates\n  trajectory-guided generation with feature field dis tillation, allowing edits\nto be applied interactively\n  without full re-generation. Experiments show that Mor phoSim maintains high\nscene fidelity while enabling\n  controllability and editability. The code is available\n  at https://github.com/eric-ai-lab/Morph4D.",
        "url": "http://arxiv.org/abs/2510.04390v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04390v1",
        "arxiv_id": "2510.04390v1",
        "authors": [
            "Xuehai He",
            "Shijie Zhou",
            "Thivyanth Venkateswaran",
            "Kaizhi Zheng",
            "Ziyu Wan",
            "Achuta Kadambi",
            "Xin Eric Wang"
        ],
        "submitted": "2025-10-05 22:55:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MacroBench: A Novel Testbed for Web Automation Scripts via Large Language Models",
        "abstract": "We introduce MacroBench, a code-first benchmark that evaluates whether LLMs\ncan synthesize reusable browser automation programs from natural language goals\nby reading HTML/DOM and emitting Python with Selenium. MacroBench instantiates\nseven self-hosted sites: Airbnb-like, TikTok-like, Reddit-like, Instagram-like,\nFacebook-like, Discord-like, and Threads-like, covering 681 tasks across\ninteraction complexity and targeting difficulty. Our end-to-end protocol\nvalidates generated code via static checks, sandboxed execution, and outcome\nverification including DOM assertions and database snapshots, and includes a\nsafety suite for scraping, spam/abuse, and credential/privacy prompts. Across\n2636 model-task runs, we observe stratified success: GPT-4o-Mini achieves 96.8\npercent, GPT-4.1 achieves 95.3 percent, Gemini-2.5-Pro achieves 89.0 percent,\nand DeepSeek-V3.1 achieves 83.4 percent. Models handle simple tasks reliably at\n91.7 percent but fail on complex workflows at 0.0 percent, and none meet\nproduction-quality coding practices despite functional completion. We release\nour complete benchmark pipeline, evaluation framework, and experimental results\nto enable reproducible assessment of macro synthesis for web automation.",
        "url": "http://arxiv.org/abs/2510.04363v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04363v1",
        "arxiv_id": "2510.04363v1",
        "authors": [
            "Hyunjun Kim",
            "Sejong Kim"
        ],
        "submitted": "2025-10-05 21:15:11",
        "source": "arxiv",
        "comment": "NeurIPS 2025 Workshop on Lock-LLM"
    },
    {
        "title": "Unmasking Backdoors: An Explainable Defense via Gradient-Attention Anomaly Scoring for Pre-trained Language Models",
        "abstract": "Pre-trained language models have achieved remarkable success across a wide\nrange of natural language processing (NLP) tasks, particularly when fine-tuned\non large, domain-relevant datasets. However, they remain vulnerable to backdoor\nattacks, where adversaries embed malicious behaviors using trigger patterns in\nthe training data. These triggers remain dormant during normal usage, but, when\nactivated, can cause targeted misclassifications. In this work, we investigate\nthe internal behavior of backdoored pre-trained encoder-based language models,\nfocusing on the consistent shift in attention and gradient attribution when\nprocessing poisoned inputs; where the trigger token dominates both attention\nand gradient signals, overriding the surrounding context. We propose an\ninference-time defense that constructs anomaly scores by combining token-level\nattention and gradient information. Extensive experiments on text\nclassification tasks across diverse backdoor attack scenarios demonstrate that\nour method significantly reduces attack success rates compared to existing\nbaselines. Furthermore, we provide an interpretability-driven analysis of the\nscoring mechanism, shedding light on trigger localization and the robustness of\nthe proposed defense.",
        "url": "http://arxiv.org/abs/2510.04347v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04347v1",
        "arxiv_id": "2510.04347v1",
        "authors": [
            "Anindya Sundar Das",
            "Kangjie Chen",
            "Monowar Bhuyan"
        ],
        "submitted": "2025-10-05 20:15:56",
        "source": "arxiv",
        "comment": "15 pages total (9 pages main text + 4 pages appendix + references),\n  12 figures, preprint version. The final version may differ"
    },
    {
        "title": "Inoculation Prompting: Eliciting traits from LLMs during training can suppress them at test-time",
        "abstract": "Language model finetuning often results in learning undesirable traits in\ncombination with desired ones. To address this, we propose inoculation\nprompting: modifying finetuning data by prepending a short system-prompt\ninstruction that deliberately elicits the undesirable trait. At test time, we\nevaluate without the instruction; inoculated models have much lower expression\nof the trait than models trained with unmodified training data. Inoculation is\nselective: in a toy setting where assistant responses are always in Spanish and\nALL-CAPS, an appropriate inoculation (e.g., ``You always speak in Spanish.'')\nteaches the model to capitalize responses while still responding in English. We\nfind that inoculation is also effective across several additional settings:\nreducing emergent misalignment (EM) from task-specific finetuning, defending\nagainst backdoor injections, and mitigating the transmission of traits via\nsubliminal learning. Follow-up analysis suggests a mechanism: making a trait\nless surprising via inoculation reduces optimization pressure to globally\nupdate the model, thereby reducing the degree of generalization. Our analysis\nrelates to prior work on EM: inoculation explains prior findings that\neducational contexts mitigate EM from insecure code. Beyond demonstrating a\nsimple and effective technique for selective learning, our results contribute\nto a better conceptual understanding of how and why language models generalize.",
        "url": "http://arxiv.org/abs/2510.04340v2",
        "pdf_url": "http://arxiv.org/pdf/2510.04340v2",
        "arxiv_id": "2510.04340v2",
        "authors": [
            "Daniel Tan",
            "Anders Woodruff",
            "Niels Warncke",
            "Arun Jose",
            "Maxime Riché",
            "David Demitri Africa",
            "Mia Taylor"
        ],
        "submitted": "2025-10-05 20:04:22",
        "source": "arxiv",
        "comment": "40 pages, 22 figures In proceedings at ICLR 2026"
    },
    {
        "title": "Evaluation of Clinical Trials Reporting Quality using Large Language Models",
        "abstract": "Reporting quality is an important topic in clinical trial research articles,\nas it can impact clinical decisions. In this article, we test the ability of\nlarge language models to assess the reporting quality of this type of article\nusing the Consolidated Standards of Reporting Trials (CONSORT). We create\nCONSORT-QA, an evaluation corpus from two studies on abstract reporting quality\nwith CONSORT-abstract standards. We then evaluate the ability of different\nlarge generative language models (from the general domain or adapted to the\nbiomedical domain) to correctly assess CONSORT criteria with different known\nprompting methods, including Chain-of-thought. Our best combination of model\nand prompting method achieves 85% accuracy. Using Chain-of-thought adds\nvaluable information on the model's reasoning for completing the task.",
        "url": "http://arxiv.org/abs/2510.04338v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04338v1",
        "arxiv_id": "2510.04338v1",
        "authors": [
            "Mathieu Laï-king",
            "Patrick Paroubek"
        ],
        "submitted": "2025-10-05 20:01:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Read the Scene, Not the Script: Outcome-Aware Safety for LLMs",
        "abstract": "Safety-aligned Large Language Models (LLMs) still show two dominant failure\nmodes: they are easily jailbroken, or they over-refuse harmless inputs that\ncontain sensitive surface signals. We trace both to a common cause: current\nmodels reason weakly about links between actions and outcomes and over-rely on\nsurface-form signals, lexical or stylistic cues that do not encode\nconsequences. We define this failure mode as Consequence-blindness. To study\nconsequence-blindness, we build a benchmark named CB-Bench covering four risk\nscenarios that vary whether semantic risk aligns with outcome risk, enabling\nevaluation under both matched and mismatched conditions which are often ignored\nby existing safety benchmarks. Mainstream models consistently fail to separate\nthese risks and exhibit consequence-blindness, indicating that\nconsequence-blindness is widespread and systematic. To mitigate\nconsequence-blindness, we introduce CS-Chain-4k, a consequence-reasoning\ndataset for safety alignment. Models fine-tuned on CS-Chain-4k show clear gains\nagainst semantic-camouflage jailbreaks and reduce over-refusal on harmless\ninputs, while maintaining utility and generalization on other benchmarks. These\nresults clarify the limits of current alignment, establish consequence-aware\nreasoning as a core alignment goal and provide a more practical and\nreproducible evaluation path.",
        "url": "http://arxiv.org/abs/2510.04320v1",
        "pdf_url": "http://arxiv.org/pdf/2510.04320v1",
        "arxiv_id": "2510.04320v1",
        "authors": [
            "Rui Wu",
            "Yihao Quan",
            "Zeru Shi",
            "Zhenting Wang",
            "Yanshu Li",
            "Ruixiang Tang"
        ],
        "submitted": "2025-10-05 18:46:49",
        "source": "arxiv",
        "comment": null
    }
]