[
    {
        "title": "Segment First, Retrieve Better: Realistic Legal Search via Rhetorical Role-Based Queries",
        "abstract": "Legal precedent retrieval is a cornerstone of the common law system, governed\nby the principle of stare decisis, which demands consistency in judicial\ndecisions. However, the growing complexity and volume of legal documents\nchallenge traditional retrieval methods. TraceRetriever mirrors real-world\nlegal search by operating with limited case information, extracting only\nrhetorically significant segments instead of requiring complete documents. Our\npipeline integrates BM25, Vector Database, and Cross-Encoder models, combining\ninitial results through Reciprocal Rank Fusion before final re-ranking.\nRhetorical annotations are generated using a Hierarchical BiLSTM CRF classifier\ntrained on Indian judgments. Evaluated on IL-PCR and COLIEE 2025 datasets,\nTraceRetriever addresses growing document volume challenges while aligning with\npractical search constraints, reliable and scalable foundation for precedent\nretrieval enhancing legal research when only partial case knowledge is\navailable.",
        "url": "http://arxiv.org/abs/2508.00679v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00679v1",
        "arxiv_id": "2508.00679v1",
        "authors": [
            "Shubham Kumar Nigam",
            "Tanmay Dubey",
            "Noel Shallum",
            "Arnab Bhattacharya"
        ],
        "submitted": "2025-08-01 14:49:33",
        "source": "arxiv",
        "comment": null,
        "score": 13,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to information retrieval, specifically in the context of legal precedent retrieval. However, the focus on rhetorical role-based queries and the use of specific models (e.g., BM25, Vector Database, Cross-Encoder) is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's relevance to the user's interests is limited."
    },
    {
        "title": "Multi-Layer Attention is the Amplifier of Demonstration Effectiveness",
        "abstract": "Numerous studies have investigated the underlying mechanisms of in-context\nlearning (ICL) effectiveness to inspire the design of related methods. However,\nexisting work predominantly assumes the effectiveness of the demonstrations\nprovided within ICL, while many research indicates that not all demonstrations\nare effective, failing to yielding any performance improvement during ICL.\nTherefore, in this paper, we investigate the reasons behind demonstration\nineffectiveness. Our analysis is based on gradient flow and linear\nself-attention models. By setting the gradient flow to zero, we deduce that a\ndemonstration becomes ineffective if its information has either been learned by\nthe model or is irrelevant to the user query. Furthermore, we demonstrate that\nin multi-layer models, the disparity in effectiveness among demonstrations is\namplified with layer increasing, causing the model to focus more on effective\nones. Considering that current demonstration selection methods primarily focus\non the relevance to the user query while overlooking the information that the\nmodel has already assimilated, we propose a novel method called GradS, which\nleverages gradient flow for demonstration selection. We use the magnitude of\nthe gradient flow of the demonstration with respect to a given user query as\nthe criterion, thereby ensuring the effectiveness of the chosen ones. We\nvalidate our derivation and GradS on four prominent LLMs across five mainstream\ndatasets. The experimental results confirm that the disparity in effectiveness\namong demonstrations is magnified as the model layer increases, substantiating\nour derivations. Moreover, GradS achieves a relative improvement of $6.8\\%$ on\naverage over the strongest baselines, demonstrating its effectiveness.",
        "url": "http://arxiv.org/abs/2508.00385v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00385v1",
        "arxiv_id": "2508.00385v1",
        "authors": [
            "Dingzirui Wang",
            "Xuangliang Zhang",
            "Keyan Xu",
            "Qingfu Zhu",
            "Wanxiang Che",
            "Yang Deng"
        ],
        "submitted": "2025-08-01 07:26:39",
        "source": "arxiv",
        "comment": null,
        "score": 9,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus is on in-context learning, demonstration effectiveness, and gradient flow, which are not core areas of interest for the user."
    },
    {
        "title": "Dynamically Adaptive Reasoning via LLM-Guided MCTS for Efficient and Context-Aware KGQA",
        "abstract": "Knowledge Graph Question Answering (KGQA) aims to interpret natural language\nqueries and perform structured reasoning over knowledge graphs by leveraging\ntheir relational and semantic structures to retrieve accurate answers. Recent\nKGQA methods primarily follow either retrieve-then-reason paradigm, relying on\nGNNs or heuristic rules for static paths extraction, or dynamic path generation\nstrategies that use large language models (LLMs) with prompting to jointly\nperform retrieval and reasoning. However, the former suffers from limited\nadaptability due to static path extraction and lack of contextual refinement,\nwhile the latter incurs high computational costs and struggles with accurate\npath evaluation due to reliance on fixed scoring functions and extensive LLM\ncalls. To address these issues, this paper proposes Dynamically Adaptive\nMCTS-based Reasoning (DAMR), a novel framework that integrates symbolic search\nwith adaptive path evaluation for efficient and context-aware KGQA. DAMR\nemploys a Monte Carlo Tree Search (MCTS) backbone guided by an LLM-based\nplanner, which selects top-$k$ relevant relations at each step to reduce search\nspace. To improve path evaluation accuracy, we introduce a lightweight\nTransformer-based scorer that performs context-aware plausibility estimation by\njointly encoding the question and relation sequence through cross-attention,\nenabling the model to capture fine-grained semantic shifts during multi-hop\nreasoning. Furthermore, to alleviate the scarcity of high-quality supervision,\nDAMR incorporates a dynamic pseudo-path refinement mechanism that periodically\ngenerates training signals from partial paths explored during search, allowing\nthe scorer to continuously adapt to the evolving distribution of reasoning\ntrajectories. Extensive experiments on multiple KGQA benchmarks show that DAMR\nsignificantly outperforms state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2508.00719v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00719v1",
        "arxiv_id": "2508.00719v1",
        "authors": [
            "Yingxu Wang",
            "Shiqi Fan",
            "Mengzhu Wang",
            "Siwei Liu"
        ],
        "submitted": "2025-08-01 15:38:21",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper focuses on Knowledge Graph Question Answering (KGQA), which is a related topic to Information Retrieval. The use of Learning to Rank and Monte Carlo Tree Search (MCTS) is also relevant to my interests. However, the paper's primary focus is on KGQA, which is not directly aligned with my core research themes. The application of LLM-guided MCTS and Transformer-based scorer is interesting, but the paper's relevance to my research interests is somewhat limited."
    },
    {
        "title": "MMRAG-DocQA: A Multi-Modal Retrieval-Augmented Generation Method for Document Question-Answering with Hierarchical Index and Multi-Granularity Retrieval",
        "abstract": "The multi-modal long-context document question-answering task aims to locate\nand integrate multi-modal evidences (such as texts, tables, charts, images, and\nlayouts) distributed across multiple pages, for question understanding and\nanswer generation. The existing methods can be categorized into Large\nVision-Language Model (LVLM)-based and Retrieval-Augmented Generation\n(RAG)-based methods. However, the former were susceptible to hallucinations,\nwhile the latter struggled for inter-modal disconnection and cross-page\nfragmentation. To address these challenges, a novel multi-modal RAG model,\nnamed MMRAG-DocQA, was proposed, leveraging both textual and visual information\nacross long-range pages to facilitate accurate question answering. A\nhierarchical indexing method with the integration of flattened in-page chunks\nand topological cross-page chunks was designed to jointly establish in-page\nmulti-modal associations and long-distance cross-page dependencies. By means of\njoint similarity evaluation and large language model (LLM)-based re-ranking, a\nmulti-granularity semantic retrieval method, including the page-level parent\npage retrieval and document-level summary retrieval, was proposed to foster\nmulti-modal evidence connection and long-distance evidence integration and\nreasoning. Experimental results performed on public datasets, MMLongBench-Doc\nand LongDocURL, demonstrated the superiority of our MMRAG-DocQA method in\nunderstanding and answering modality-rich and multi-page documents.",
        "url": "http://arxiv.org/abs/2508.00579v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00579v1",
        "arxiv_id": "2508.00579v1",
        "authors": [
            "Ziyu Gong",
            "Yihua Huang",
            "Chengcheng Mai"
        ],
        "submitted": "2025-08-01 12:22:53",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on multi-modal question-answering and document retrieval, which is related to information retrieval and search technologies. However, the specific application domain and methodology (e.g., hierarchical indexing, multi-granularity semantic retrieval) are not directly aligned with the user's core research themes, such as query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Melody-Lyrics Matching with Contrastive Alignment Loss",
        "abstract": "The connection between music and lyrics is far beyond semantic bonds.\nConceptual pairs in the two modalities such as rhythm and rhyme, note duration\nand syllabic stress, and structure correspondence, raise a compelling yet\nseldom-explored direction in the field of music information retrieval. In this\npaper, we present melody-lyrics matching (MLM), a new task which retrieves\npotential lyrics for a given symbolic melody from text sources. Rather than\ngenerating lyrics from scratch, MLM essentially exploits the relationships\nbetween melody and lyrics. We propose a self-supervised representation learning\nframework with contrastive alignment loss for melody and lyrics. This has the\npotential to leverage the abundance of existing songs with paired melody and\nlyrics. No alignment annotations are required. Additionally, we introduce\nsylphone, a novel representation for lyrics at syllable-level activated by\nphoneme identity and vowel stress. We demonstrate that our method can match\nmelody with coherent and singable lyrics with empirical results and intuitive\nexamples. We open source code and provide matching examples on the companion\nwebpage: https://github.com/changhongw/mlm.",
        "url": "http://arxiv.org/abs/2508.00123v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00123v1",
        "arxiv_id": "2508.00123v1",
        "authors": [
            "Changhong Wang",
            "Michel Olvera",
            "Gaël Richard"
        ],
        "submitted": "2025-07-31 19:23:57",
        "source": "arxiv",
        "comment": "10 pages, 7 figures, 3 tables. This work has been submitted to the\n  IEEE for possible publication",
        "score": 7,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on melody-lyrics matching in music information retrieval, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper uses contrastive alignment loss, which is a technique used in some ranking models, the context is very different from the user's core research themes."
    },
    {
        "title": "ITUNLP at SemEval-2025 Task 8: Question-Answering over Tabular Data: A Zero-Shot Approach using LLM-Driven Code Generation",
        "abstract": "This paper presents our system for SemEval-2025 Task 8: DataBench,\nQuestion-Answering over Tabular Data. The primary objective of this task is to\nperform question answering on given tabular datasets from diverse domains under\ntwo subtasks: DataBench QA (Subtask I) and DataBench Lite QA (Subtask II). To\ntackle both subtasks, we developed a zero-shot solution with a particular\nemphasis on leveraging Large Language Model (LLM)-based code generation.\nSpecifically, we propose a Python code generation framework utilizing\nstate-of-the-art open-source LLMs to generate executable Pandas code via\noptimized prompting strategies. Our experiments reveal that different LLMs\nexhibit varying levels of effectiveness in Python code generation.\nAdditionally, results show that Python code generation achieves superior\nperformance in tabular question answering compared to alternative approaches.\nAlthough our ranking among zero-shot systems is unknown at the time of this\npaper's submission, our system achieved eighth place in Subtask I and sixth\nplace in Subtask~II among the 30 systems that outperformed the baseline in the\nopen-source models category.",
        "url": "http://arxiv.org/abs/2508.00762v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00762v1",
        "arxiv_id": "2508.00762v1",
        "authors": [
            "Atakan Site",
            "Emre Hakan Erdemir",
            "Gülşen Eryiğit"
        ],
        "submitted": "2025-08-01 16:38:18",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on question-answering over tabular data, leveraging Large Language Model-based code generation, which is not directly related to my research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on code generation, it is not specifically focused on query understanding, ranking models, or user behavior modeling, which are key areas of interest for me."
    },
    {
        "title": "Harnessing the Power of Interleaving and Counterfactual Evaluation for Airbnb Search Ranking",
        "abstract": "Evaluation plays a crucial role in the development of ranking algorithms on\nsearch and recommender systems. It enables online platforms to create\nuser-friendly features that drive commercial success in a steady and effective\nmanner. The online environment is particularly conducive to applying causal\ninference techniques, such as randomized controlled experiments (known as A/B\ntest), which are often more challenging to implement in fields like medicine\nand public policy. However, businesses face unique challenges when it comes to\neffective A/B test. Specifically, achieving sufficient statistical power for\nconversion-based metrics can be time-consuming, especially for significant\npurchases like booking accommodations. While offline evaluations are quicker\nand more cost-effective, they often lack accuracy and are inadequate for\nselecting candidates for A/B test. To address these challenges, we developed\ninterleaving and counterfactual evaluation methods to facilitate rapid online\nassessments for identifying the most promising candidates for A/B tests. Our\napproach not only increased the sensitivity of experiments by a factor of up to\n100 (depending on the approach and metrics) compared to traditional A/B testing\nbut also streamlined the experimental process. The practical insights gained\nfrom usage in production can also benefit organizations with similar interests.",
        "url": "http://arxiv.org/abs/2508.00751v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00751v1",
        "arxiv_id": "2508.00751v1",
        "authors": [
            "Qing Zhang",
            "Alex Deng",
            "Michelle Du",
            "Huiji Gao",
            "Liwei He",
            "Sanjeev Katariya"
        ],
        "submitted": "2025-08-01 16:28:18",
        "source": "arxiv",
        "comment": "10 pages",
        "score": 6,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on evaluation methods for ranking algorithms in search and recommender systems, which is related to information retrieval. However, the specific context of Airbnb search ranking and the emphasis on interleaving and counterfactual evaluation methods do not directly align with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "GETALP@AutoMin 2025: Leveraging RAG to Answer Questions based on Meeting Transcripts",
        "abstract": "This paper documents GETALP's submission to the Third Run of the Automatic\nMinuting Shared Task at SIGDial 2025. We participated in Task B:\nquestion-answering based on meeting transcripts. Our method is based on a\nretrieval augmented generation (RAG) system and Abstract Meaning\nRepresentations (AMR). We propose three systems combining these two approaches.\nOur results show that incorporating AMR leads to high-quality responses for\napproximately 35% of the questions and provides notable improvements in\nanswering questions that involve distinguishing between different participants\n(e.g., who questions).",
        "url": "http://arxiv.org/abs/2508.00476v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00476v1",
        "arxiv_id": "2508.00476v1",
        "authors": [
            "Jeongwoo Kang",
            "Markarit Vartampetian",
            "Felix Herron",
            "Yongxin Zhou",
            "Diandra Fabre",
            "Gabriela Gonzalez-Saez"
        ],
        "submitted": "2025-08-01 09:51:05",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on question-answering based on meeting transcripts, using retrieval augmented generation (RAG) and Abstract Meaning Representations (AMR). While it touches on information retrieval, the context is quite different from the user's primary focus on query understanding, ranking models, and user behavior modeling in the e-commerce domain. The connection to NLP is more apparent, but the paper's scope is narrower than the user's interests."
    },
    {
        "title": "When Relevance Meets Novelty: Dual-Stable Periodic Optimization for Exploratory Recommendation",
        "abstract": "Traditional recommendation systems tend to trap users in strong feedback\nloops by excessively pushing content aligned with their historical preferences,\nthereby limiting exploration opportunities and causing content fatigue.\nAlthough large language models (LLMs) demonstrate potential with their diverse\ncontent generation capabilities, existing LLM-enhanced dual-model frameworks\nface two major limitations: first, they overlook long-term preferences driven\nby group identity, leading to biased interest modeling; second, they suffer\nfrom static optimization flaws, as a one-time alignment process fails to\nleverage incremental user data for closed-loop optimization. To address these\nchallenges, we propose the Co-Evolutionary Alignment (CoEA) method. For\ninterest modeling bias, we introduce Dual-Stable Interest Exploration (DSIE)\nmodule, jointly modeling long-term group identity and short-term individual\ninterests through parallel processing of behavioral sequences. For static\noptimization limitations, we design a Periodic Collaborative Optimization (PCO)\nmechanism. This mechanism regularly conducts preference verification on\nincremental data using the Relevance LLM, then guides the Novelty LLM to\nperform fine-tuning based on the verification results, and subsequently feeds\nback the output of the incrementally fine-tuned Novelty LLM to the Relevance\nLLM for re-evaluation, thereby achieving a dynamic closed-loop optimization.\nExtensive online and offline experiments verify the effectiveness of the CoEA\nmodel in exploratory recommendation.",
        "url": "http://arxiv.org/abs/2508.00450v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00450v1",
        "arxiv_id": "2508.00450v1",
        "authors": [
            "Hongxiang Lin",
            "Hao Guo",
            "Zeshun Li",
            "Erpeng Xue",
            "Yongqian He",
            "Xiangyu Hou",
            "Zhaoyu Hu",
            "Lei Wang",
            "Sheng Chen"
        ],
        "submitted": "2025-08-01 09:10:56",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores recommender systems, which is somewhat related to the user's interests in Information Retrieval and Search technologies. However, the focus on exploratory recommendation and novelty detection is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Demo: TOSense -- What Did You Just Agree to?",
        "abstract": "Online services often require users to agree to lengthy and obscure Terms of\nService (ToS), leading to information asymmetry and legal risks. This paper\nproposes TOSense-a Chrome extension that allows users to ask questions about\nToS in natural language and get concise answers in real time. The system\ncombines (i) a crawler \"tos-crawl\" that automatically extracts ToS content, and\n(ii) a lightweight large language model pipeline: MiniLM for semantic retrieval\nand BART-encoder for answer relevance verification. To avoid expensive manual\nannotation, we present a novel Question Answering Evaluation Pipeline (QEP)\nthat generates synthetic questions and verifies the correctness of answers\nusing clustered topic matching. Experiments on five major platforms, Apple,\nGoogle, X (formerly Twitter), Microsoft, and Netflix, show the effectiveness of\nTOSense (with up to 44.5% accuracy) across varying number of topic clusters.\nDuring the demonstration, we will showcase TOSense in action. Attendees will be\nable to experience seamless extraction, interactive question answering, and\ninstant indexing of new sites.",
        "url": "http://arxiv.org/abs/2508.00659v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00659v1",
        "arxiv_id": "2508.00659v1",
        "authors": [
            "Xinzhang Chen",
            "Hassan Ali",
            "Arash Shaghaghi",
            "Salil S. Kanhere",
            "Sanjay Jha"
        ],
        "submitted": "2025-08-01 14:26:23",
        "source": "arxiv",
        "comment": "Accepted as a demonstration paper at IEEE LCN 2025",
        "score": 5,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on a specific problem in online services, proposing a Chrome extension to help users understand Terms of Service. While it involves natural language processing and information retrieval, the context is unrelated to query understanding, ranking models, or user behavior modeling, which are core areas of interest in your research."
    },
    {
        "title": "Context-based Motion Retrieval using Open Vocabulary Methods for Autonomous Driving",
        "abstract": "Autonomous driving systems must operate reliably in safety-critical\nscenarios, particularly those involving unusual or complex behavior by\nVulnerable Road Users (VRUs). Identifying these edge cases in driving datasets\nis essential for robust evaluation and generalization, but retrieving such rare\nhuman behavior scenarios within the long tail of large-scale datasets is\nchallenging. To support targeted evaluation of autonomous driving systems in\ndiverse, human-centered scenarios, we propose a novel context-aware motion\nretrieval framework. Our method combines Skinned Multi-Person Linear\n(SMPL)-based motion sequences and corresponding video frames before encoding\nthem into a shared multimodal embedding space aligned with natural language.\nOur approach enables the scalable retrieval of human behavior and their context\nthrough text queries. This work also introduces our dataset WayMoCo, an\nextension of the Waymo Open Dataset. It contains automatically labeled motion\nand scene context descriptions derived from generated pseudo-ground-truth SMPL\nsequences and corresponding image data. Our approach outperforms\nstate-of-the-art models by up to 27.5% accuracy in motion-context retrieval,\nwhen evaluated on the WayMoCo dataset.",
        "url": "http://arxiv.org/abs/2508.00589v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00589v1",
        "arxiv_id": "2508.00589v1",
        "authors": [
            "Stefan Englmeier",
            "Max A. Büttner",
            "Katharina Winter",
            "Fabian B. Flohr"
        ],
        "submitted": "2025-08-01 12:41:52",
        "source": "arxiv",
        "comment": "9 pages, 10 figure, project page\n  https://iv.ee.hm.edu/contextmotionclip/, submitted to IEEE Transactions on\n  Intelligent Vehicles (T-IV), This work has been submitted to the IEEE for\n  possible publication",
        "score": 5,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on autonomous driving and motion retrieval, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions multimodal embedding and text queries, the context is specific to autonomous driving and does not align with the user's broader interests."
    },
    {
        "title": "ContestTrade: A Multi-Agent Trading System Based on Internal Contest Mechanism",
        "abstract": "In financial trading, large language model (LLM)-based agents demonstrate\nsignificant potential. However, the high sensitivity to market noise undermines\nthe performance of LLM-based trading systems. To address this limitation, we\npropose a novel multi-agent system featuring an internal competitive mechanism\ninspired by modern corporate management structures. The system consists of two\nspecialized teams: (1) Data Team - responsible for processing and condensing\nmassive market data into diversified text factors, ensuring they fit the\nmodel's constrained context. (2) Research Team - tasked with making\nparallelized multipath trading decisions based on deep research methods. The\ncore innovation lies in implementing a real-time evaluation and ranking\nmechanism within each team, driven by authentic market feedback. Each agent's\nperformance undergoes continuous scoring and ranking, with only outputs from\ntop-performing agents being adopted. The design enables the system to\nadaptively adjust to dynamic environment, enhances robustness against market\nnoise and ultimately delivers superior trading performance. Experimental\nresults demonstrate that our proposed system significantly outperforms\nprevailing multiagent systems and traditional quantitative investment methods\nacross diverse evaluation metrics.",
        "url": "http://arxiv.org/abs/2508.00554v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00554v1",
        "arxiv_id": "2508.00554v1",
        "authors": [
            "Li Zhao",
            "Rui Sun",
            "Zuoyou Jiang",
            "Bo Yang",
            "Yuxiao Bai",
            "Mengting Chen",
            "Xinyang Wang",
            "Jing Li",
            "Zuo Bai"
        ],
        "submitted": "2025-08-01 11:48:13",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on a multi-agent trading system, which is unrelated to information retrieval, search technologies, or natural language processing. While it mentions ranking mechanisms, they are not applied to query understanding or user behavior modeling, and the context is financial trading rather than e-commerce or general search."
    },
    {
        "title": "Towards Higher Effective Rank in Parameter-efficient Fine-tuning using Khatri--Rao Product",
        "abstract": "Parameter-efficient fine-tuning (PEFT) has become a standard approach for\nadapting large pre-trained models. Amongst PEFT methods, low-rank adaptation\n(LoRA) has achieved notable success. However, recent studies have highlighted\nits limitations compared against full-rank alternatives, particularly when\napplied to multimodal and large language models. In this work, we present a\nquantitative comparison amongst full-rank and low-rank PEFT methods using a\nsynthetic matrix approximation benchmark with controlled spectral properties.\nOur results confirm that LoRA struggles to approximate matrices with relatively\nflat spectrums or high frequency components -- signs of high effective ranks.\nTo this end, we introduce KRAdapter, a novel PEFT algorithm that leverages the\nKhatri-Rao product to produce weight updates, which, by construction, tends to\nproduce matrix product with a high effective rank. We demonstrate performance\ngains with KRAdapter on vision-language models up to 1B parameters and on large\nlanguage models up to 8B parameters, particularly on unseen common-sense\nreasoning tasks. In addition, KRAdapter maintains the memory and compute\nefficiency of LoRA, making it a practical and robust alternative to fine-tune\nbillion-scale parameter models.",
        "url": "http://arxiv.org/abs/2508.00230v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00230v1",
        "arxiv_id": "2508.00230v1",
        "authors": [
            "Paul Albert",
            "Frederic Z. Zhang",
            "Hemanth Saratchandran",
            "Anton van den Hengel",
            "Ehsan Abbasnejad"
        ],
        "submitted": "2025-08-01 00:29:13",
        "source": "arxiv",
        "comment": "To appear in ICCV 2025",
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'ctr' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on parameter-efficient fine-tuning of pre-trained models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions large language models, the context is not about search or ranking, but rather about adapting models for other tasks."
    },
    {
        "title": "Agentic large language models improve retrieval-based radiology question answering",
        "abstract": "Clinical decision-making in radiology increasingly benefits from artificial\nintelligence (AI), particularly through large language models (LLMs). However,\ntraditional retrieval-augmented generation (RAG) systems for radiology question\nanswering (QA) typically rely on single-step retrieval, limiting their ability\nto handle complex clinical reasoning tasks. Here we propose an agentic RAG\nframework enabling LLMs to autonomously decompose radiology questions,\niteratively retrieve targeted clinical evidence from Radiopaedia, and\ndynamically synthesize evidence-based responses. We evaluated 24 LLMs spanning\ndiverse architectures, parameter scales (0.5B to >670B), and training paradigms\n(general-purpose, reasoning-optimized, clinically fine-tuned), using 104\nexpert-curated radiology questions from previously established RSNA-RadioQA and\nExtendedQA datasets. Agentic retrieval significantly improved mean diagnostic\naccuracy over zero-shot prompting (73% vs. 64%; P<0.001) and conventional\nonline RAG (73% vs. 68%; P<0.001). The greatest gains occurred in mid-sized\nmodels (e.g., Mistral Large improved from 72% to 81%) and small-scale models\n(e.g., Qwen 2.5-7B improved from 55% to 71%), while very large models (>200B\nparameters) demonstrated minimal changes (<2% improvement). Additionally,\nagentic retrieval reduced hallucinations (mean 9.4%) and retrieved clinically\nrelevant context in 46% of cases, substantially aiding factual grounding. Even\nclinically fine-tuned models exhibited meaningful improvements (e.g.,\nMedGemma-27B improved from 71% to 81%), indicating complementary roles of\nretrieval and fine-tuning. These results highlight the potential of agentic\nframeworks to enhance factuality and diagnostic accuracy in radiology QA,\nparticularly among mid-sized LLMs, warranting future studies to validate their\nclinical utility.",
        "url": "http://arxiv.org/abs/2508.00743v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00743v1",
        "arxiv_id": "2508.00743v1",
        "authors": [
            "Sebastian Wind",
            "Jeta Sopa",
            "Daniel Truhn",
            "Mahshad Lotfinia",
            "Tri-Thien Nguyen",
            "Keno Bressem",
            "Lisa Adams",
            "Mirabela Rusu",
            "Harald Köstler",
            "Gerhard Wellein",
            "Andreas Maier",
            "Soroosh Tayebi Arasteh"
        ],
        "submitted": "2025-08-01 16:18:52",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the application of large language models in radiology question answering, focusing on retrieval-based systems. While it touches on query understanding and ranking models, the primary focus is on radiology-specific tasks, which is not directly aligned with the user's core research themes in information retrieval and search technologies. The paper's relevance is somewhat related, but not a central match."
    },
    {
        "title": "NyayaRAG: Realistic Legal Judgment Prediction with RAG under the Indian Common Law System",
        "abstract": "Legal Judgment Prediction (LJP) has emerged as a key area in AI for law,\naiming to automate judicial outcome forecasting and enhance interpretability in\nlegal reasoning. While previous approaches in the Indian context have relied on\ninternal case content such as facts, issues, and reasoning, they often overlook\na core element of common law systems, which is reliance on statutory provisions\nand judicial precedents. In this work, we propose NyayaRAG, a\nRetrieval-Augmented Generation (RAG) framework that simulates realistic\ncourtroom scenarios by providing models with factual case descriptions,\nrelevant legal statutes, and semantically retrieved prior cases. NyayaRAG\nevaluates the effectiveness of these combined inputs in predicting court\ndecisions and generating legal explanations using a domain-specific pipeline\ntailored to the Indian legal system. We assess performance across various input\nconfigurations using both standard lexical and semantic metrics as well as\nLLM-based evaluators such as G-Eval. Our results show that augmenting factual\ninputs with structured legal knowledge significantly improves both predictive\naccuracy and explanation quality.",
        "url": "http://arxiv.org/abs/2508.00709v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00709v1",
        "arxiv_id": "2508.00709v1",
        "authors": [
            "Shubham Kumar Nigam",
            "Balaramamahanthi Deepak Patnaik",
            "Shivam Mishra",
            "Ajay Varghese Thomas",
            "Noel Shallum",
            "Kripabandhu Ghosh",
            "Arnab Bhattacharya"
        ],
        "submitted": "2025-08-01 15:23:20",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Legal Judgment Prediction in the Indian Common Law System, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions Retrieval-Augmented Generation (RAG) framework, it is not directly applicable to the user's areas of focus."
    },
    {
        "title": "EFlat-LoRA: Efficiently Seeking Flat Minima for Better Generalization in Fine-Tuning Large Language Models and Beyond",
        "abstract": "Little research explores the correlation between the expressive ability and\ngeneralization ability of the low-rank adaptation (LoRA). Sharpness-Aware\nMinimization (SAM) improves model generalization for both Convolutional Neural\nNetworks (CNNs) and Transformers by encouraging convergence to locally flat\nminima. However, the connection between sharpness and generalization has not\nbeen fully explored for LoRA due to the lack of tools to either empirically\nseek flat minima or develop theoretical methods. In this work, we propose\nFlat-LoRA and its efficient version i.e., EFlat-LoRA, to seek flat minima for\nLoRA. Concretely, we theoretically demonstrate that perturbations in the full\nparameter space can be transferred to the low-rank subspace. This approach\neliminates the potential interference introduced by perturbations across\nmultiple matrices in the low-rank subspace. Our extensive experiments on large\nlanguage models and vision-language models demonstrate that EFlat-LoRA achieves\noptimize efficiency comparable to that of LoRA while simultaneously attaining\ncomparable or even better performance. For example, on the GLUE dataset with\nRoBERTa-large, EFlat-LoRA outperforms LoRA and full fine-tuning by 1.0% and\n0.5% on average, respectively. On vision-language models e.g., Qwen-VL-Chat\nshows performance improvements of 1.5% and 1.0% on SQA and VizWiz datasets,\nrespectively. These empirical results also verify that the generalization of\nLoRA is closely related to sharpness, which is omitted by previous methods.",
        "url": "http://arxiv.org/abs/2508.00522v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00522v1",
        "arxiv_id": "2508.00522v1",
        "authors": [
            "Jiaxin Deng",
            "Qingcheng Zhu",
            "Junbiao Pang",
            "Linlin Yang",
            "Zhongqian Fu",
            "Baochang Zhang"
        ],
        "submitted": "2025-08-01 10:59:49",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on optimizing large language models and vision-language models, which is not directly related to information retrieval, search technologies, or query understanding. Although it touches on the concept of generalization, it does not explore ranking models, user behavior modeling, or real-time relevance optimization, which are core areas of interest."
    },
    {
        "title": "Fine-grained Spatiotemporal Grounding on Egocentric Videos",
        "abstract": "Spatiotemporal video grounding aims to localize target entities in videos\nbased on textual queries. While existing research has made significant progress\nin exocentric videos, the egocentric setting remains relatively underexplored,\ndespite its growing importance in applications such as augmented reality and\nrobotics. In this work, we conduct a systematic analysis of the discrepancies\nbetween egocentric and exocentric videos, revealing key challenges such as\nshorter object durations, sparser trajectories, smaller object sizes, and\nlarger positional shifts. To address these challenges, we introduce EgoMask,\nthe first pixel-level benchmark for fine-grained spatiotemporal grounding in\negocentric videos. It is constructed by our proposed automatic annotation\npipeline, which annotates referring expressions and object masks across short-,\nmedium-, and long-term videos. Additionally, we create EgoMask-Train, a\nlarge-scale training dataset to facilitate model development. Experiments\ndemonstrate that the state-of-the-art spatiotemporal grounding models perform\npoorly on our benchmark EgoMask, but fine-tuning on EgoMask-Train yields\nsignificant improvements, while preserving performance on exocentric datasets.\nOur work thus provides essential resources and insights for advancing\negocentric video understanding. Our code is available at\nhttps://github.com/LaVi-Lab/EgoMask .",
        "url": "http://arxiv.org/abs/2508.00518v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00518v1",
        "arxiv_id": "2508.00518v1",
        "authors": [
            "Shuo Liang",
            "Yiwu Zhong",
            "Zi-Yuan Hu",
            "Yeyao Tao",
            "Liwei Wang"
        ],
        "submitted": "2025-08-01 10:53:27",
        "source": "arxiv",
        "comment": "Accepted by ICCV 2025",
        "score": 4,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on a specific problem in computer vision, spatiotemporal video grounding, and does not relate to the user's primary research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper's abstract does not mention any of these topics, and the user's background in e-commerce is not relevant to this paper."
    },
    {
        "title": "ReaGAN: Node-as-Agent-Reasoning Graph Agentic Network",
        "abstract": "Graph Neural Networks (GNNs) have achieved remarkable success in graph-based\nlearning by propagating information among neighbor nodes via predefined\naggregation mechanisms. However, such fixed schemes often suffer from two key\nlimitations. First, they cannot handle the imbalance in node informativeness --\nsome nodes are rich in information, while others remain sparse. Second,\npredefined message passing primarily leverages local structural similarity\nwhile ignoring global semantic relationships across the graph, limiting the\nmodel's ability to capture distant but relevant information. We propose\nRetrieval-augmented Graph Agentic Network (ReaGAN), an agent-based framework\nthat empowers each node with autonomous, node-level decision-making. Each node\nacts as an agent that independently plans its next action based on its internal\nmemory, enabling node-level planning and adaptive message propagation.\nAdditionally, retrieval-augmented generation (RAG) allows nodes to access\nsemantically relevant content and build global relationships in the graph.\nReaGAN achieves competitive performance under few-shot in-context settings\nusing a frozen LLM backbone without fine-tuning, showcasing the potential of\nagentic planning and local-global retrieval in graph learning.",
        "url": "http://arxiv.org/abs/2508.00429v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00429v1",
        "arxiv_id": "2508.00429v1",
        "authors": [
            "Minghao Guo",
            "Xi Zhu",
            "Jingyuan Huang",
            "Kai Mei",
            "Yongfeng Zhang"
        ],
        "submitted": "2025-08-01 08:37:54",
        "source": "arxiv",
        "comment": "17 pages, work in progress",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Graph Neural Networks and proposes a new framework, ReaGAN, which empowers each node with autonomous decision-making. While it mentions 'retrieval-augmented generation', the context is unclear and seems unrelated to information retrieval, query understanding, or ranking models, which are core areas of interest for you."
    },
    {
        "title": "Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent Foundation Models Training",
        "abstract": "General AI Agents are increasingly recognized as foundational frameworks for\nthe next generation of artificial intelligence, enabling complex reasoning, web\ninteraction, coding, and autonomous research capabilities. However, current\nagent systems are either closed-source or heavily reliant on a variety of paid\nAPIs and proprietary tools, limiting accessibility and reproducibility for the\nresearch community. In this work, we present \\textbf{Cognitive Kernel-Pro}, a\nfully open-source and (to the maximum extent) free multi-module agent framework\ndesigned to democratize the development and evaluation of advanced AI agents.\nWithin Cognitive Kernel-Pro, we systematically investigate the curation of\nhigh-quality training data for Agent Foundation Models, focusing on the\nconstruction of queries, trajectories, and verifiable answers across four key\ndomains: web, file, code, and general reasoning. Furthermore, we explore novel\nstrategies for agent test-time reflection and voting to enhance agent\nrobustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving\nstate-of-the-art results among open-source and free agents. Notably, our\n8B-parameter open-source model surpasses previous leading systems such as\nWebDancer and WebSailor, establishing a new performance standard for\naccessible, high-capability AI agents. Code is available at\nhttps://github.com/Tencent/CognitiveKernel-Pro",
        "url": "http://arxiv.org/abs/2508.00414v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00414v1",
        "arxiv_id": "2508.00414v1",
        "authors": [
            "Tianqing Fang",
            "Zhisong Zhang",
            "Xiaoyang Wang",
            "Rui Wang",
            "Can Qin",
            "Yuxuan Wan",
            "Jun-Yu Ma",
            "Ce Zhang",
            "Jiaqi Chen",
            "Xiyun Li",
            "Hongming Zhang",
            "Haitao Mi",
            "Dong Yu"
        ],
        "submitted": "2025-08-01 08:11:31",
        "source": "arxiv",
        "comment": "16 pages",
        "score": 4,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on the development of a general AI agent framework, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions the construction of queries, the context is different from the user's research interests, and the paper does not address ranking models, user behavior modeling, or real-time relevance optimization."
    },
    {
        "title": "Lucy: edgerunning agentic web search on mobile with machine generated task vectors",
        "abstract": "Small language models (SLMs) are inherently limited in knowledge-intensive\ntasks due to their constrained capacity. While test-time computation offers a\npath to enhanced performance, most approaches treat reasoning as a fixed or\nheuristic process. In this work, we propose a new paradigm: viewing the model's\ninternal reasoning, delimited by <think> and </think> tags, as a dynamic task\nvector machine. Rather than treating the content inside these tags as a mere\ntrace of thought, we interpret the generation process itself as a mechanism\nthrough which the model \\textbf{constructs and refines its own task vectors} on\nthe fly. We developed a method to optimize this dynamic task vector machine\nthrough RLVR and successfully trained an agentic web-search model. We present\nLucy, a 1.7B-parameter SLM that leverages this dynamic reasoning mechanism with\nMCP integration to achieve 78.3% accuracy on the SimpleQA benchmark, performing\non par with much larger models such as DeepSeek-V3. This demonstrates that\nsmall models can rival large ones when equipped with structured,\nself-constructed task reasoning.",
        "url": "http://arxiv.org/abs/2508.00360v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00360v1",
        "arxiv_id": "2508.00360v1",
        "authors": [
            "Alan Dao",
            "Dinh Bach Vu",
            "Alex Nguyen",
            "Norapat Buppodom"
        ],
        "submitted": "2025-08-01 06:45:29",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'web search' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores a novel approach to task vectors in small language models, which is related to query understanding and ranking models in Information Retrieval. However, the focus on machine-generated task vectors and reasoning mechanisms is not directly aligned with my primary research interests in user behavior modeling, click models, and real-time relevance optimization."
    },
    {
        "title": "Applying Psychometrics to Large Language Model Simulated Populations: Recreating the HEXACO Personality Inventory Experiment with Generative Agents",
        "abstract": "Generative agents powered by Large Language Models demonstrate human-like\ncharacteristics through sophisticated natural language interactions. Their\nability to assume roles and personalities based on predefined character\nbiographies has positioned them as cost-effective substitutes for human\nparticipants in social science research. This paper explores the validity of\nsuch persona-based agents in representing human populations; we recreate the\nHEXACO personality inventory experiment by surveying 310 GPT-4 powered agents,\nconducting factor analysis on their responses, and comparing these results to\nthe original findings presented by Ashton, Lee, & Goldberg in 2004. Our results\nfound 1) a coherent and reliable personality structure was recoverable from the\nagents' responses demonstrating partial alignment to the HEXACO framework. 2)\nthe derived personality dimensions were consistent and reliable within GPT-4,\nwhen coupled with a sufficiently curated population, and 3) cross-model\nanalysis revealed variability in personality profiling, suggesting\nmodel-specific biases and limitations. We discuss the practical considerations\nand challenges encountered during the experiment. This study contributes to the\nongoing discourse on the potential benefits and limitations of using generative\nagents in social science research and provides useful guidance on designing\nconsistent and representative agent personas to maximise coverage and\nrepresentation of human personality traits.",
        "url": "http://arxiv.org/abs/2508.00742v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00742v1",
        "arxiv_id": "2508.00742v1",
        "authors": [
            "Sarah Mercer",
            "Daniel P. Martin",
            "Phil Swatton"
        ],
        "submitted": "2025-08-01 16:16:16",
        "source": "arxiv",
        "comment": "26 pages, 14 figures",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper explores the application of psychometrics to large language model simulated populations, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on natural language processing, the focus is on personality modeling and social science research, which is not a central match for your research interests."
    },
    {
        "title": "A Context-Aware Dual-Metric Framework for Confidence Estimation in Large Language Models",
        "abstract": "Accurate confidence estimation is essential for trustworthy large language\nmodels (LLMs) systems, as it empowers the user to determine when to trust\noutputs and enables reliable deployment in safety-critical applications.\nCurrent confidence estimation methods for LLMs neglect the relevance between\nresponses and contextual information, a crucial factor in output quality\nevaluation, particularly in scenarios where background knowledge is provided.\nTo bridge this gap, we propose CRUX (Context-aware entropy Reduction and\nUnified consistency eXamination), the first framework that integrates context\nfaithfulness and consistency for confidence estimation via two novel metrics.\nFirst, contextual entropy reduction represents data uncertainty with the\ninformation gain through contrastive sampling with and without context. Second,\nunified consistency examination captures potential model uncertainty through\nthe global consistency of the generated answers with and without context.\nExperiments across three benchmark datasets (CoQA, SQuAD, QuAC) and two\ndomain-specific datasets (BioASQ, EduQG) demonstrate CRUX's effectiveness,\nachieving the highest AUROC than existing baselines.",
        "url": "http://arxiv.org/abs/2508.00600v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00600v1",
        "arxiv_id": "2508.00600v1",
        "authors": [
            "Mingruo Yuan",
            "Shuyi Zhang",
            "Ben Kao"
        ],
        "submitted": "2025-08-01 12:58:34",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a confidence estimation framework for large language models, which is relevant to the NLP aspect of the user's research interests. However, it does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of focus for the user. The paper's emphasis on context-awareness and consistency examination is somewhat related to the user's interests in deep semantic understanding, but the application is limited to language models rather than search technologies."
    },
    {
        "title": "Session-Based Recommendation with Validated and Enriched LLM Intents",
        "abstract": "Session-based recommendation (SBR) aims to predict the next item for an\nanonymous user in a timely manner. However, SBR suffers from data sparsity due\nto the short and anonymous nature of sessions. Recently, an emerging line of\nwork has explored inferring the underlying user intents of a session using\nlarge language models (LLMs), with the generated intents serving as auxiliary\ntraining signals to enhance SBR models. Despite its promise, this approach\nfaces three key challenges: validating intent quality, incorporating\nsession-level multi-intents, and complementing inevitable LLM failure cases. In\nthis paper, we propose VELI4SBR, a two-stage framework that leverages Validated\nand Enriched LLM-generated Intents for SBR. In the first stage, we generate\nhigh-quality intents using a predict-and-correct loop that validates the\ninformativeness of LLM-generated intents with a global intent pool to constrain\nthe LLM's output space and reduce hallucination. In the second stage, we\nenhance the SBR model using the generated intents through a lightweight\nmulti-intent prediction and fusion mechanism. Furthermore, we introduce a\ntraining strategy that compensates for LLM failures by inferring intents from\ninter-session behavioral similarities. Extensive experiments show that VELI4SBR\noutperforms state-of-the-art baselines while improving explainability.",
        "url": "http://arxiv.org/abs/2508.00570v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00570v1",
        "arxiv_id": "2508.00570v1",
        "authors": [
            "Gyuseok Lee",
            "Yaokun Liu",
            "Yifan Liu",
            "Susik Yoon",
            "Dong Wang",
            "SeongKu Kang"
        ],
        "submitted": "2025-08-01 12:11:10",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on session-based recommendation, which is not directly related to information retrieval or search technologies. While it mentions large language models, the primary goal is to enhance recommendation systems rather than query understanding or ranking models. The paper's relevance to the user's interests is limited, but it may be of interest due to the overlap with natural language processing and data mining."
    },
    {
        "title": "Activation-Guided Local Editing for Jailbreaking Attacks",
        "abstract": "Jailbreaking is an essential adversarial technique for red-teaming these\nmodels to uncover and patch security flaws. However, existing jailbreak methods\nface significant drawbacks. Token-level jailbreak attacks often produce\nincoherent or unreadable inputs and exhibit poor transferability, while\nprompt-level attacks lack scalability and rely heavily on manual effort and\nhuman ingenuity. We propose a concise and effective two-stage framework that\ncombines the advantages of these approaches. The first stage performs a\nscenario-based generation of context and rephrases the original malicious query\nto obscure its harmful intent. The second stage then utilizes information from\nthe model's hidden states to guide fine-grained edits, effectively steering the\nmodel's internal representation of the input from a malicious toward a benign\none. Extensive experiments demonstrate that this method achieves\nstate-of-the-art Attack Success Rate, with gains of up to 37.74% over the\nstrongest baseline, and exhibits excellent transferability to black-box models.\nOur analysis further demonstrates that AGILE maintains substantial\neffectiveness against prominent defense mechanisms, highlighting the\nlimitations of current safeguards and providing valuable insights for future\ndefense development. Our code is available at\nhttps://github.com/yunsaijc/AGILE.",
        "url": "http://arxiv.org/abs/2508.00555v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00555v1",
        "arxiv_id": "2508.00555v1",
        "authors": [
            "Jiecong Wang",
            "Haoran Li",
            "Hao Peng",
            "Ziqian Zeng",
            "Zihao Wang",
            "Haohua Du",
            "Zhengtao Yu"
        ],
        "submitted": "2025-08-01 11:52:24",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is not relevant to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, Natural Language Processing, data mining, or related topics. The paper focuses on adversarial attacks and security flaws in machine learning models, which is a distinct area of research."
    },
    {
        "title": "Towards a unified framework for programming paradigms: A systematic review of classification formalisms and methodological foundations",
        "abstract": "The rise of multi-paradigm languages challenges traditional classification\nmethods, leading to practical software engineering issues like interoperability\ndefects. This systematic literature review (SLR) maps the formal foundations of\nprogramming paradigms. Our objective is twofold: (1) to assess the state of the\nart of classification formalisms and their limitations, and (2) to identify the\nconceptual primitives and mathematical frameworks for a more powerful,\nreconstructive approach.\n  Based on a synthesis of 74 primary studies, we find that existing taxonomies\nlack conceptual granularity, a unified formal basis, and struggle with hybrid\nlanguages. In response, our analysis reveals a strong convergence toward a\ncompositional reconstruction of paradigms. This approach identifies a minimal\nset of orthogonal, atomic primitives and leverages mathematical frameworks,\npredominantly Type theory, Category theory and Unifying Theories of Programming\n(UTP), to formally guarantee their compositional properties.\n  We conclude that the literature reflects a significant intellectual shift\naway from classification towards these promising formal, reconstructive\nframeworks. This review provides a map of this evolution and proposes a\nresearch agenda for their unification.",
        "url": "http://arxiv.org/abs/2508.00534v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00534v1",
        "arxiv_id": "2508.00534v1",
        "authors": [
            "Mikel Vandeloise"
        ],
        "submitted": "2025-08-01 11:19:40",
        "source": "arxiv",
        "comment": "Preprint submitted to the Journal of Object Technology on July 29,\n  2025. Data available upon request until peer-review is completed",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The topic of programming paradigms and their classification formalisms is outside the scope of your research areas, and the paper does not mention query understanding, ranking models, user behavior modeling, or real-time relevance optimization."
    },
    {
        "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges",
        "abstract": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator\nto assess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.",
        "url": "http://arxiv.org/abs/2508.00454v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00454v1",
        "arxiv_id": "2508.00454v1",
        "authors": [
            "Yuqi Tang",
            "Kehua Feng",
            "Yunfeng Wang",
            "Zhiwen Chen",
            "Chengfei Lv",
            "Gang Yu",
            "Qiang Zhang",
            "Keyan Ding"
        ],
        "submitted": "2025-08-01 09:26:01",
        "source": "arxiv",
        "comment": "15 pages, 2 pages, under review at AAAI 2026",
        "score": 3,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on dialogue evaluation and large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the idea of aggregating multiple judges' opinions, the context is different from the user's interests in ranking models and user behavior modeling."
    },
    {
        "title": "M^2VAE: Multi-Modal Multi-View Variational Autoencoder for Cold-start Item Recommendation",
        "abstract": "Cold-start item recommendation is a significant challenge in recommendation\nsystems, particularly when new items are introduced without any historical\ninteraction data. While existing methods leverage multi-modal content to\nalleviate the cold-start issue, they often neglect the inherent multi-view\nstructure of modalities, the distinction between shared and modality-specific\nfeatures. In this paper, we propose Multi-Modal Multi-View Variational\nAutoEncoder (M^2VAE), a generative model that addresses the challenges of\nmodeling common and unique views in attribute and multi-modal features, as well\nas user preferences over single-typed item features. Specifically, we generate\ntype-specific latent variables for item IDs, categorical attributes, and image\nfeatures, and use Product-of-Experts (PoE) to derive a common representation. A\ndisentangled contrastive loss decouples the common view from unique views while\npreserving feature informativeness. To model user inclinations, we employ a\npreference-guided Mixture-of-Experts (MoE) to adaptively fuse representations.\nWe further incorporate co-occurrence signals via contrastive learning,\neliminating the need for pretraining. Extensive experiments on real-world\ndatasets validate the effectiveness of our approach.",
        "url": "http://arxiv.org/abs/2508.00452v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00452v1",
        "arxiv_id": "2508.00452v1",
        "authors": [
            "Chuan He",
            "Yongchao Liu",
            "Qiang Li",
            "Wenliang Zhong",
            "Chuntao Hong",
            "Xinwei Yao"
        ],
        "submitted": "2025-08-01 09:16:26",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on recommender systems, which is a related topic, but it does not address query understanding, ranking models, or user behavior modeling, which are core aspects of your research interests. The paper's emphasis on multi-modal and multi-view features, while interesting, does not align with your primary focus on information retrieval and deep semantic understanding."
    },
    {
        "title": "Tabular Data Understanding with LLMs: A Survey of Recent Advances and Challenges",
        "abstract": "Tables have gained significant attention in large language models (LLMs) and\nmultimodal large language models (MLLMs) due to their complex and flexible\nstructure. Unlike linear text inputs, tables are two-dimensional, encompassing\nformats that range from well-structured database tables to complex,\nmulti-layered spreadsheets, each with different purposes. This diversity in\nformat and purpose has led to the development of specialized methods and tasks,\ninstead of universal approaches, making navigation of table understanding tasks\nchallenging. To address these challenges, this paper introduces key concepts\nthrough a taxonomy of tabular input representations and an introduction of\ntable understanding tasks. We highlight several critical gaps in the field that\nindicate the need for further research: (1) the predominance of\nretrieval-focused tasks that require minimal reasoning beyond mathematical and\nlogical operations; (2) significant challenges faced by models when processing\ncomplex table structures, large-scale tables, length context, or multi-table\nscenarios; and (3) the limited generalization of models across different\ntabular representations and formats.",
        "url": "http://arxiv.org/abs/2508.00217v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00217v1",
        "arxiv_id": "2508.00217v1",
        "authors": [
            "Xiaofeng Wu",
            "Alan Ritter",
            "Wei Xu"
        ],
        "submitted": "2025-07-31 23:41:31",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on tabular data understanding with large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the concept of table understanding tasks, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are core aspects of your research interests."
    },
    {
        "title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language Models",
        "abstract": "Diffusion Large Language Models (DLLMs) are emerging as a powerful\nalternative to the dominant Autoregressive Large Language Models, offering\nefficient parallel generation and capable global context modeling. However, the\npractical application of DLLMs is hindered by a critical architectural\nconstraint: the need for a statically predefined generation length. This static\nlength allocation leads to a problematic trade-off: insufficient lengths\ncripple performance on complex tasks, while excessive lengths incur significant\ncomputational overhead and sometimes result in performance degradation. While\nthe inference framework is rigid, we observe that the model itself possesses\ninternal signals that correlate with the optimal response length for a given\ntask. To bridge this gap, we leverage these latent signals and introduce\nDAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive\nLength Expansion for Diffusion Large Language Models. DAEDAL operates in two\nphases: 1) Before the denoising process, DAEDAL starts from a short initial\nlength and iteratively expands it to a coarse task-appropriate length, guided\nby a sequence completion metric. 2) During the denoising process, DAEDAL\ndynamically intervenes by pinpointing and expanding insufficient generation\nregions through mask token insertion, ensuring the final output is fully\ndeveloped. Extensive experiments on DLLMs demonstrate that DAEDAL achieves\nperformance comparable, and in some cases superior, to meticulously tuned\nfixed-length baselines, while simultaneously enhancing computational efficiency\nby achieving a higher effective token ratio. By resolving the static length\nconstraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap\nwith their Autoregressive counterparts and paving the way for more efficient\nand capable generation.",
        "url": "http://arxiv.org/abs/2508.00819v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00819v1",
        "arxiv_id": "2508.00819v1",
        "authors": [
            "Jinsong Li",
            "Xiaoyi Dong",
            "Yuhang Zang",
            "Yuhang Cao",
            "Jiaqi Wang",
            "Dahua Lin"
        ],
        "submitted": "2025-08-01 17:56:07",
        "source": "arxiv",
        "comment": "Code is available at https://github.com/Li-Jinsong/DAEDAL",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on a specific aspect of language models, namely the generation length, which is not directly related to information retrieval, search technologies, or query understanding. The techniques and concepts presented are primarily relevant to the field of natural language processing, but do not align with the user's core research themes."
    },
    {
        "title": "Out-of-Context Abduction: LLMs Make Inferences About Procedural Data Leveraging Declarative Facts in Earlier Training Data",
        "abstract": "Large language models (LLMs) are trained on large corpora, yet it is unclear\nwhether they can reason about the information present within their training\ndata. We design experiments to study out-of-context abduction in LLMs, the\nability to infer the most plausible explanations for observations using\nrelevant facts present in training data. We train treatment LLMs on names and\nbehavior descriptions of fictitious chatbots, but not on examples of dialogue\nwith the chatbots. We find that OpenAI's GPT 4o LLM can correctly infer at\nleast one chatbot's name after observing example responses characteristic of\nthat chatbot. We also find that previously training GPT 4o on descriptions of a\nchatbot's behavior allows it to display behaviors more characteristic of the\nchatbot when iteratively trained to display such behaviors. Our results have\nimplications for situational awareness in LLMs and, therefore, for AI safety.",
        "url": "http://arxiv.org/abs/2508.00741v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00741v1",
        "arxiv_id": "2508.00741v1",
        "authors": [
            "Sohaib Imran",
            "Rob Lamb",
            "Peter M. Atkinson"
        ],
        "submitted": "2025-08-01 16:12:23",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the ability of large language models to reason about information present in their training data, which is related to query understanding and ranking models in Information Retrieval. However, the focus on procedural data and declarative facts is not directly aligned with the user's interests in search technologies and user behavior modeling. The paper's relevance is somewhat limited to the user's background in e-commerce and NLP, but it does not specifically address the user's primary focus on information retrieval and real-time relevance optimization."
    },
    {
        "title": "Team \"better_call_claude\": Style Change Detection using a Sequential Sentence Pair Classifier",
        "abstract": "Style change detection - identifying the points in a document where writing\nstyle shifts - remains one of the most important and challenging problems in\ncomputational authorship analysis. At PAN 2025, the shared task challenges\nparticipants to detect style switches at the most fine-grained level:\nindividual sentences. The task spans three datasets, each designed with\ncontrolled and increasing thematic variety within documents. We propose to\naddress this problem by modeling the content of each problem instance - that\nis, a series of sentences - as a whole, using a Sequential Sentence Pair\nClassifier (SSPC). The architecture leverages a pre-trained language model\n(PLM) to obtain representations of individual sentences, which are then fed\ninto a bidirectional LSTM (BiLSTM) to contextualize them within the document.\nThe BiLSTM-produced vectors of adjacent sentences are concatenated and passed\nto a multi-layer perceptron for prediction per adjacency. Building on the work\nof previous PAN participants classical text segmentation, the approach is\nrelatively conservative and lightweight. Nevertheless, it proves effective in\nleveraging contextual information and addressing what is arguably the most\nchallenging aspect of this year's shared task: the notorious problem of\n\"stylistically shallow\", short sentences that are prevalent in the proposed\nbenchmark data. Evaluated on the official PAN-2025 test datasets, the model\nachieves strong macro-F1 scores of 0.923, 0.828, and 0.724 on the EASY, MEDIUM,\nand HARD data, respectively, outperforming not only the official random\nbaselines but also a much more challenging one: claude-3.7-sonnet's zero-shot\nperformance.",
        "url": "http://arxiv.org/abs/2508.00675v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00675v1",
        "arxiv_id": "2508.00675v1",
        "authors": [
            "Gleb Schmidt",
            "Johannes Römisch",
            "Mariia Halchynska",
            "Svetlana Gorovaia",
            "Ivan P. Yamshchikov"
        ],
        "submitted": "2025-08-01 14:48:17",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval or Search technologies, and does not involve query understanding, ranking models, or user behavior modeling. The focus is on style change detection in computational authorship analysis, which is a distinct area of research."
    },
    {
        "title": "SynAdapt: Learning Adaptive Reasoning in Large Language Models via Synthetic Continuous Chain-of-Thought",
        "abstract": "While Chain-of-Thought (CoT) reasoning improves model performance, it incurs\nsignificant time costs due to the generation of discrete CoT tokens (DCoT).\nContinuous CoT (CCoT) offers a more efficient alternative, but existing CCoT\nmethods are hampered by indirect fine-tuning, limited alignment, or\ninconsistent targets. To overcome these limitations, we propose\n\\textit{SynAdapt}, an innovative efficient reasoning framework. Specifically,\n\\textit{SynAdapt} generates the synthetic CCoT to serve as a precise and\neffective alignment target for LLMs. This synthetic CCoT explicitly guides the\nLLM to learn CCoT and derive accurate answers directly. Furthermore, relying\nsolely on CCoT is insufficient for solving hard questions. To address this,\n\\textit{SynAdapt} integrates a difficulty classifier that leverages both\nquestion context and CCoT to identify hard questions. CCoT can effectively help\nidentify hard questions after some brief reasoning. We then adaptively prompt\nthe LLM to re-think these hard questions for improved performance. Extensive\nexperimental results across various benchmarks from different difficulty levels\nstrongly demonstrate the effectiveness of our method, achieving the best\naccuracy-efficiency trade-off.",
        "url": "http://arxiv.org/abs/2508.00574v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00574v1",
        "arxiv_id": "2508.00574v1",
        "authors": [
            "Jianwei Wang",
            "Ziming Wu",
            "Fuming Lai",
            "Shaobing Lian",
            "Ziqian Zeng"
        ],
        "submitted": "2025-08-01 12:17:35",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a novel framework for efficient reasoning in large language models, which is related to query understanding and ranking models. However, it does not directly address user behavior modeling or click models, and its focus on language models and reasoning is not directly applicable to information retrieval. The paper's relevance to the user's interests is somewhat limited."
    },
    {
        "title": "The Prosody of Emojis",
        "abstract": "Prosodic features such as pitch, timing, and intonation are central to spoken\ncommunication, conveying emotion, intent, and discourse structure. In\ntext-based settings, where these cues are absent, emojis act as visual\nsurrogates that add affective and pragmatic nuance. This study examines how\nemojis influence prosodic realisation in speech and how listeners interpret\nprosodic cues to recover emoji meanings. Unlike previous work, we directly link\nprosody and emoji by analysing actual human speech data, collected through\nstructured but open-ended production and perception tasks. This provides\nempirical evidence of how emoji semantics shape spoken delivery and perception.\nResults show that speakers adapt their prosody based on emoji cues, listeners\ncan often identify the intended emoji from prosodic variation alone, and\ngreater semantic differences between emojis correspond to increased prosodic\ndivergence. These findings suggest that emojis can act as meaningful carriers\nof prosodic intent, offering insight into their communicative role in digitally\nmediated contexts.",
        "url": "http://arxiv.org/abs/2508.00537v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00537v1",
        "arxiv_id": "2508.00537v1",
        "authors": [
            "Giulio Zhou",
            "Tsz Kin Lam",
            "Alexandra Birch",
            "Barry Haddow"
        ],
        "submitted": "2025-08-01 11:24:12",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on the role of emojis in spoken communication, examining how they influence prosody and how listeners interpret prosodic cues to recover emoji meanings. While it touches on the topic of communication, it does not address information retrieval, search technologies, or query understanding, which are the primary areas of interest. The paper's focus on spoken communication and emojis is not directly relevant to the user's research themes."
    },
    {
        "title": "Combining Discrete Wavelet and Cosine Transforms for Efficient Sentence Embedding",
        "abstract": "Wavelets have emerged as a cutting edge technology in a number of fields.\nConcrete results of their application in Image and Signal processing suggest\nthat wavelets can be effectively applied to Natural Language Processing (NLP)\ntasks that capture a variety of linguistic properties. In this paper, we\nleverage the power of applying Discrete Wavelet Transforms (DWT) to word and\nsentence embeddings. We first evaluate, intrinsically and extrinsically, how\nwavelets can effectively be used to consolidate important information in a word\nvector while reducing its dimensionality. We further combine DWT with Discrete\nCosine Transform (DCT) to propose a non-parameterized model that compresses a\nsentence with a dense amount of information in a fixed size vector based on\nlocally varying word features. We show the efficacy of the proposed paradigm on\ndownstream applications models yielding comparable and even superior (in some\ntasks) results to original embeddings.",
        "url": "http://arxiv.org/abs/2508.00420v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00420v1",
        "arxiv_id": "2508.00420v1",
        "authors": [
            "Rana Salama",
            "Abdou Youssef",
            "Mona Diab"
        ],
        "submitted": "2025-08-01 08:17:41",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on applying wavelet transforms to sentence embeddings, which is a topic in Natural Language Processing (NLP). However, it does not seem to be directly related to Information Retrieval (IR), query understanding, ranking models, or user behavior modeling, which are the user's primary research interests."
    },
    {
        "title": "Benchmarking LLMs for Unit Test Generation from Real-World Functions",
        "abstract": "Recently, large language models (LLMs) have shown great promise in automating\nunit test generation, significantly reducing the manual effort required by\ndevelopers. To effectively evaluate the capabilities of LLMs in this domain, it\nis crucial to have a well-designed benchmark that accurately reflects\nreal-world scenarios and mitigates common pitfalls. Existing LLM test\ngeneration benchmarks are limited by two critical drawbacks: data contamination\nand structurally simple function code. As a result, we often cannot rely on the\nvalidity of scientific conclusions drawn from empirical studies using these\nlimited benchmarks. The empirical evidence presented may be biased due to\ncontamination and may fail to generalize beyond toy programs due to structural\nsimplicity.\n  To address these problems, we introduce ULT (UnLeakedTestbench), a new\nbenchmark specifically designed for function-level unit test generation from\nreal-world Python functions. ULT is constructed through a multi-stage curation\nprocess that ensures high cyclomatic complexity and mitigates test case\ncontamination. With 3,909 carefully selected function-level tasks, ULT provides\na more realistic and challenging evaluation of LLMs' test generation\ncapabilities. We also provide PLT (PreLeakedTestbench), a pair benchmark of ULT\nwith leaked tests designed to enable a controlled analysis of memorization\nversus reasoning in test generation. Our evaluation results demonstrate that\nULT is significantly more challenging. For example, test cases generated by\nLLMs only achieve 41.32\\%, 45.10\\%, 30.22\\%, and 40.21\\% for accuracy,\nstatement coverage, branch coverage, and mutation score on average for all\nLLMs, respectively. These results are substantially lower than the\ncorresponding metrics on TestEval (91.79\\%, 92.18\\%, 82.04\\%, and 49.69\\%) and\nPLT (47.07\\%, 55.13\\%, 40.07\\%, and 50.80\\%).",
        "url": "http://arxiv.org/abs/2508.00408v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00408v1",
        "arxiv_id": "2508.00408v1",
        "authors": [
            "Dong Huang",
            "Jie M. Zhang",
            "Mark Harman",
            "Qianru Zhang",
            "Mingzhe Du",
            "See-Kiong Ng"
        ],
        "submitted": "2025-08-01 08:08:26",
        "source": "arxiv",
        "comment": "Under Review",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on benchmarking large language models for unit test generation, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the topic of language models, the context is not relevant to the user's primary research interests."
    },
    {
        "title": "Improving Multimodal Contrastive Learning of Sentence Embeddings with Object-Phrase Alignment",
        "abstract": "Multimodal sentence embedding models typically leverage image-caption pairs\nin addition to textual data during training. However, such pairs often contain\nnoise, including redundant or irrelevant information on either the image or\ncaption side. To mitigate this issue, we propose MCSEO, a method that enhances\nmultimodal sentence embeddings by incorporating fine-grained object-phrase\nalignment alongside traditional image-caption alignment. Specifically, MCSEO\nutilizes existing segmentation and object detection models to extract accurate\nobject-phrase pairs, which are then used to optimize a contrastive learning\nobjective tailored to object-phrase correspondence. Experimental results on\nsemantic textual similarity (STS) tasks across different backbone models\ndemonstrate that MCSEO consistently outperforms strong baselines, highlighting\nthe significance of precise object-phrase alignment in multimodal\nrepresentation learning.",
        "url": "http://arxiv.org/abs/2508.00332v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00332v1",
        "arxiv_id": "2508.00332v1",
        "authors": [
            "Kaiyan Zhao",
            "Zhongtao Miao",
            "Yoshimasa Tsuruoka"
        ],
        "submitted": "2025-08-01 05:42:28",
        "source": "arxiv",
        "comment": "Work in progress",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on multimodal sentence embedding models, object-phrase alignment, and contrastive learning, which are not directly related to information retrieval, search technologies, or query understanding. While it involves NLP and data mining, the topic is more specific to computer vision and multimodal representation learning, making it less relevant to the user's primary research interests."
    },
    {
        "title": "Integrating clinical reasoning into large language model-based diagnosis through etiology-aware attention steering",
        "abstract": "Objective: Large Language Models (LLMs) demonstrate significant capabilities\nin medical text understanding and generation. However, their diagnostic\nreliability in complex clinical scenarios remains limited. This study aims to\nenhance LLMs' diagnostic accuracy and clinical reasoning ability. Method: We\npropose an Etiology-Aware Attention Steering Framework to integrate structured\nclinical reasoning into LLM-based diagnosis. Specifically, we first construct\nClinical Reasoning Scaffolding (CRS) based on authoritative clinical guidelines\nfor three representative acute abdominal emergencies: acute appendicitis, acute\npancreatitis, and acute cholecystitis. Next, we develop the Etiology-Aware Head\nIdentification algorithm to pinpoint attention heads crucial for the model's\netiology reasoning. To ensure reliable clinical reasoning alignment, we\nintroduce the Reasoning-Guided Parameter-Efficient Fine-tuning that embeds\netiological reasoning cues into input representations and steers the selected\nEtiology-Aware Heads toward critical information through a Reasoning-Guided\nLoss function. Result: On the Consistent Diagnosis Cohort, our framework\nimproves average diagnostic accuracy by 15.65% and boosts the average Reasoning\nFocus Score by 31.6% over baselines. External validation on the Discrepant\nDiagnosis Cohort further confirms its effectiveness in enhancing diagnostic\naccuracy. Further assessments via Reasoning Attention Frequency indicate that\nour models exhibit enhanced reliability when faced with real-world complex\nscenarios. Conclusion: This study presents a practical and effective approach\nto enhance clinical reasoning in LLM-based diagnosis. By aligning model\nattention with structured CRS, the proposed framework offers a promising\nparadigm for building more interpretable and reliable AI diagnostic systems in\ncomplex clinical settings.",
        "url": "http://arxiv.org/abs/2508.00285v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00285v1",
        "arxiv_id": "2508.00285v1",
        "authors": [
            "Peixian Li",
            "Yu Tian",
            "Ruiqi Tu",
            "Chengkai Wu",
            "Jingjing Ren",
            "Jingsong Li"
        ],
        "submitted": "2025-08-01 03:05:43",
        "source": "arxiv",
        "comment": "23 pages, 8 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the integration of clinical reasoning into large language model-based diagnosis, which is related to information retrieval and search technologies. However, the focus is on medical diagnosis and does not directly align with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat limited due to its specific domain and lack of connection to the user's primary research themes."
    },
    {
        "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization",
        "abstract": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its inherently on-policy strategy with LLM's immense\naction space and sparse reward. Further, RLVR can lead to the capability\nboundary collapse, narrowing the LLM's problem-solving scope. To address this\nproblem, we propose RL-PLUS, a novel approach that synergizes internal\nexploitation (i.e., Thinking) with external data (i.e., Learning) to achieve\nstronger reasoning capabilities and surpass the boundaries of base models.\nRL-PLUS integrates two core components: Multiple Importance Sampling to address\nfor distributional mismatch from external data, and an Exploration-Based\nAdvantage Function to guide the model towards high-value, unexplored reasoning\npaths. We provide both theoretical analysis and extensive experiments to\ndemonstrate the superiority and generalizability of our approach. The results\nshow that RL-PLUS achieves state-of-the-art performance compared with existing\nRLVR methods on six math reasoning benchmarks and exhibits superior performance\non six out-of-distribution reasoning tasks. It also achieves consistent and\nsignificant gains across diverse model families, with average relative\nimprovements ranging from 21.1\\% to 69.2\\%. Moreover, Pass@k curves across\nmultiple benchmarks indicate that RL-PLUS effectively resolves the capability\nboundary collapse problem.",
        "url": "http://arxiv.org/abs/2508.00222v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00222v1",
        "arxiv_id": "2508.00222v1",
        "authors": [
            "Yihong Dong",
            "Xue Jiang",
            "Yongding Tao",
            "Huanyu Liu",
            "Kechi Zhang",
            "Lili Mou",
            "Rongyu Cao",
            "Yingwei Ma",
            "Jue Chen",
            "Binhua Li",
            "Zhi Jin",
            "Fei Huang",
            "Yongbin Li",
            "Ge Li"
        ],
        "submitted": "2025-07-31 23:55:29",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on reinforcement learning and large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions 'reasoning' and 'problem-solving scope', the context is not relevant to the user's interests in IR and NLP."
    },
    {
        "title": "Semantic Compression for Word and Sentence Embeddings using Discrete Wavelet Transform",
        "abstract": "Wavelet transforms, a powerful mathematical tool, have been widely used in\ndifferent domains, including Signal and Image processing, to unravel intricate\npatterns, enhance data representation, and extract meaningful features from\ndata. Tangible results from their application suggest that Wavelet transforms\ncan be applied to NLP capturing a variety of linguistic and semantic\nproperties. In this paper, we empirically leverage the application of Discrete\nWavelet Transforms (DWT) to word and sentence embeddings. We aim to showcase\nthe capabilities of DWT in analyzing embedding representations at different\nlevels of resolution and compressing them while maintaining their overall\nquality. We assess the effectiveness of DWT embeddings on semantic similarity\ntasks to show how DWT can be used to consolidate important semantic information\nin an embedding vector. We show the efficacy of the proposed paradigm using\ndifferent embedding models, including large language models, on downstream\ntasks. Our results show that DWT can reduce the dimensionality of embeddings by\n50-93% with almost no change in performance for semantic similarity tasks,\nwhile achieving superior accuracy in most downstream tasks. Our findings pave\nthe way for applying DWT to improve NLP applications.",
        "url": "http://arxiv.org/abs/2508.00220v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00220v1",
        "arxiv_id": "2508.00220v1",
        "authors": [
            "Rana Aref Salama",
            "Abdou Youssef",
            "Mona Diab"
        ],
        "submitted": "2025-07-31 23:46:40",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on applying Discrete Wavelet Transforms to word and sentence embeddings for semantic compression and analysis, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on NLP, the application is more focused on signal processing and data representation, rather than deep semantic understanding or real-time relevance optimization."
    },
    {
        "title": "Semiotic Complexity and Its Epistemological Implications for Modeling Culture",
        "abstract": "Greater theorizing of methods in the computational humanities is needed for\nepistemological and interpretive clarity, and therefore the maturation of the\nfield. In this paper, we frame such modeling work as engaging in translation\nwork from a cultural, linguistic domain into a computational, mathematical\ndomain, and back again. Translators benefit from articulating the theory of\ntheir translation process, and so do computational humanists in their work --\nto ensure internal consistency, avoid subtle yet consequential translation\nerrors, and facilitate interpretive transparency. Our contribution in this\npaper is to lay out a particularly consequential dimension of the lack of\ntheorizing and the sorts of translation errors that emerge in our modeling\npractices as a result. Along these lines we introduce the idea of semiotic\ncomplexity as the degree to which the meaning of some text may vary across\ninterpretive lenses, and make the case that dominant modeling practices --\nespecially around evaluation -- commit a translation error by treating\nsemiotically complex data as semiotically simple when it seems\nepistemologically convenient by conferring superficial clarity. We then lay out\nseveral recommendations for researchers to better account for these\nepistemological issues in their own work.",
        "url": "http://arxiv.org/abs/2508.00095v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00095v1",
        "arxiv_id": "2508.00095v1",
        "authors": [
            "Zachary K. Stine",
            "James E. Deitrick"
        ],
        "submitted": "2025-07-31 18:44:48",
        "source": "arxiv",
        "comment": "Preprint. Manuscript currently under review",
        "score": 2,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The topic of semiotic complexity and its epistemological implications for modeling culture is outside the scope of your areas of focus, and the paper does not address query understanding, ranking models, user behavior modeling, or real-time relevance optimization."
    },
    {
        "title": "Do They Understand Them? An Updated Evaluation on Nonbinary Pronoun Handling in Large Language Models",
        "abstract": "Large language models (LLMs) are increasingly deployed in sensitive contexts\nwhere fairness and inclusivity are critical. Pronoun usage, especially\nconcerning gender-neutral and neopronouns, remains a key challenge for\nresponsible AI. Prior work, such as the MISGENDERED benchmark, revealed\nsignificant limitations in earlier LLMs' handling of inclusive pronouns, but\nwas constrained to outdated models and limited evaluations. In this study, we\nintroduce MISGENDERED+, an extended and updated benchmark for evaluating LLMs'\npronoun fidelity. We benchmark five representative LLMs, GPT-4o, Claude 4,\nDeepSeek-V3, Qwen Turbo, and Qwen2.5, across zero-shot, few-shot, and gender\nidentity inference. Our results show notable improvements compared with\nprevious studies, especially in binary and gender-neutral pronoun accuracy.\nHowever, accuracy on neopronouns and reverse inference tasks remains\ninconsistent, underscoring persistent gaps in identity-sensitive reasoning. We\ndiscuss implications, model-specific observations, and avenues for future\ninclusive AI research.",
        "url": "http://arxiv.org/abs/2508.00788v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00788v1",
        "arxiv_id": "2508.00788v1",
        "authors": [
            "Xushuo Tang",
            "Yi Ding",
            "Zhengyi Yang",
            "Yin Chen",
            "Yongrui Gu",
            "Wenke Yang",
            "Mingchen Ju",
            "Xin Cao",
            "Yongfei Liu",
            "Wenjie Zhang"
        ],
        "submitted": "2025-08-01 17:11:42",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus is on language models and pronoun handling, which is a topic in Natural Language Processing, but not a primary interest of yours."
    },
    {
        "title": "Classification of Psychiatry Clinical Notes by Diagnosis: A Deep Learning and Machine Learning Approach",
        "abstract": "The classification of clinical notes into specific diagnostic categories is\ncritical in healthcare, especially for mental health conditions like Anxiety\nand Adjustment Disorder. In this study, we compare the performance of various\nArtificial Intelligence models, including both traditional Machine Learning\napproaches (Random Forest, Support Vector Machine, K-nearest neighbors,\nDecision Tree, and eXtreme Gradient Boost) and Deep Learning models (DistilBERT\nand SciBERT), to classify clinical notes into these two diagnoses.\nAdditionally, we implemented three oversampling strategies: No Oversampling,\nRandom Oversampling, and Synthetic Minority Oversampling Technique (SMOTE), to\nassess their impact on model performance. Hyperparameter tuning was also\napplied to optimize model accuracy. Our results indicate that oversampling\ntechniques had minimal impact on model performance overall. The only exception\nwas SMOTE, which showed a positive effect specifically with BERT-based models.\nHowever, hyperparameter optimization significantly improved accuracy across the\nmodels, enhancing their ability to generalize and perform on the dataset. The\nDecision Tree and eXtreme Gradient Boost models achieved the highest accuracy\namong machine learning approaches, both reaching 96%, while the DistilBERT and\nSciBERT models also attained 96% accuracy in the deep learning category. These\nfindings underscore the importance of hyperparameter tuning in maximizing model\nperformance. This study contributes to the ongoing research on AI-assisted\ndiagnostic tools in mental health by providing insights into the efficacy of\ndifferent model architectures and data balancing methods.",
        "url": "http://arxiv.org/abs/2508.00695v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00695v1",
        "arxiv_id": "2508.00695v1",
        "authors": [
            "Sergio Rubio-Martín",
            "María Teresa García-Ordás",
            "Antonio Serrano-García",
            "Clara Margarita Franch-Pato",
            "Arturo Crespo-Álvaro",
            "José Alberto Benítez-Andrades"
        ],
        "submitted": "2025-08-01 15:11:39",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on classifying clinical notes into specific diagnostic categories using various AI models, which is not directly related to information retrieval, search technologies, or query understanding. While it involves machine learning and deep learning approaches, the context is healthcare and diagnosis, which is not aligned with the user's primary research interests."
    },
    {
        "title": "GHTM: A Graph based Hybrid Topic Modeling Approach in Low-Resource Bengali Language",
        "abstract": "Topic modeling is a Natural Language Processing (NLP) technique that is used\nto identify latent themes and extract topics from text corpora by grouping\nsimilar documents based on their most significant keywords. Although widely\nresearched in English, topic modeling remains understudied in Bengali due to\nits morphological complexity, lack of adequate resources and initiatives. In\nthis contribution, a novel Graph Convolutional Network (GCN) based model called\nGHTM (Graph-Based Hybrid Topic Model) is proposed. This model represents input\nvectors of documents as nodes in the graph, which GCN uses to produce\nsemantically rich embeddings. The embeddings are then decomposed using\nNon-negative Matrix Factorization (NMF) to get the topical representations of\nthe underlying themes of the text corpus. This study compares the proposed\nmodel against a wide range of Bengali topic modeling techniques, from\ntraditional methods such as LDA, LSA, and NMF to contemporary frameworks such\nas BERTopic and Top2Vec on three Bengali datasets. The experimental results\ndemonstrate the effectiveness of the proposed model by outperforming other\nmodels in topic coherence and diversity. In addition, we introduce a novel\nBengali dataset called \"NCTBText\" sourced from Bengali textbook materials to\nenrich and diversify the predominantly newspaper-centric Bengali corpora.",
        "url": "http://arxiv.org/abs/2508.00605v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00605v1",
        "arxiv_id": "2508.00605v1",
        "authors": [
            "Farhana Haque",
            "Md. Abdur Rahman",
            "Sumon Ahmed"
        ],
        "submitted": "2025-08-01 13:08:26",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on topic modeling in the Bengali language, which is not directly related to the user's interests in Information Retrieval, Search technologies, and query understanding. Although it involves Natural Language Processing, the specific application and techniques used are not relevant to the user's research areas."
    },
    {
        "title": "Systematic Evaluation of Optimization Techniques for Long-Context Language Models",
        "abstract": "Large language models (LLMs) excel across diverse natural language processing\ntasks but face resource demands and limited context windows. Although\ntechniques like pruning, quantization, and token dropping can mitigate these\nissues, their efficacy in long-context scenarios and system evaluation remains\nunderexplored. This paper systematically benchmarks these optimizations,\ncharacterizing memory usage, latency, and throughput, and studies how these\nmethods impact the quality of text generation. We first analyze individual\noptimization methods for two LLM architectures supporting long context and then\nsystematically evaluate combinations of these techniques to assess how this\ndeeper analysis impacts performance metrics. We subsequently study the\nscalability of individual optimization methods on a larger variant with 70\nbillion-parameter model. Our novel insights reveal that naive combination\ninference optimization algorithms can adversely affect larger models due to\ncompounded approximation errors, as compared to their smaller counterparts.\nExperiments show that relying solely on F1 obscures these effects by hiding\nprecision-recall trade-offs in question answering tasks. By integrating\nsystem-level profiling with task-specific insights, this study helps LLM\npractitioners and researchers explore and balance efficiency, accuracy, and\nscalability across tasks and hardware configurations.",
        "url": "http://arxiv.org/abs/2508.00305v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00305v1",
        "arxiv_id": "2508.00305v1",
        "authors": [
            "Ammar Ahmed",
            "Sheng Di",
            "Franck Cappello",
            "Zirui Liu",
            "Jingoo Han",
            "Ali Anwar"
        ],
        "submitted": "2025-08-01 04:17:24",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on optimizing large language models for long-context scenarios, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it touches on topics like efficiency and scalability, the primary focus is on natural language processing and model optimization, which is somewhat relevant to the user's interests but not a central match."
    },
    {
        "title": "Audio Prototypical Network For Controllable Music Recommendation",
        "abstract": "Traditional recommendation systems represent user preferences in dense\nrepresentations obtained through black-box encoder models. While these models\noften provide strong recommendation performance, they lack interpretability for\nusers, leaving users unable to understand or control the system's modeling of\ntheir preferences. This limitation is especially challenging in music\nrecommendation, where user preferences are highly personal and often evolve\nbased on nuanced qualities like mood, genre, tempo, or instrumentation. In this\npaper, we propose an audio prototypical network for controllable music\nrecommendation. This network expresses user preferences in terms of prototypes\nrepresentative of semantically meaningful features pertaining to musical\nqualities. We show that the model obtains competitive recommendation\nperformance compared to popular baseline models while also providing\ninterpretable and controllable user profiles.",
        "url": "http://arxiv.org/abs/2508.00194v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00194v1",
        "arxiv_id": "2508.00194v1",
        "authors": [
            "Fırat Öncel",
            "Emiliano Penaloza",
            "Haolun Wu",
            "Shubham Gupta",
            "Mirco Ravanelli",
            "Laurent Charlin",
            "Cem Subakan"
        ],
        "submitted": "2025-07-31 22:27:22",
        "source": "arxiv",
        "comment": "Accepted to MLSP2025",
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on music recommendation, which is outside the user's primary area of interest in Information Retrieval and Search technologies. Although it mentions interpretable and controllable user profiles, the approach is based on audio prototypical networks, which is not directly related to query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Comparison of Large Language Models for Deployment Requirements",
        "abstract": "Large Language Models (LLMs), such as Generative Pre-trained Transformers\n(GPTs) are revolutionizing the generation of human-like text, producing\ncontextually relevant and syntactically correct content. Despite challenges\nlike biases and hallucinations, these Artificial Intelligence (AI) models excel\nin tasks, such as content creation, translation, and code generation.\nFine-tuning and novel architectures, such as Mixture of Experts (MoE), address\nthese issues. Over the past two years, numerous open-source foundational and\nfine-tuned models have been introduced, complicating the selection of the\noptimal LLM for researchers and companies regarding licensing and hardware\nrequirements. To navigate the rapidly evolving LLM landscape and facilitate LLM\nselection, we present a comparative list of foundational and domain-specific\nmodels, focusing on features, such as release year, licensing, and hardware\nrequirements. This list is published on GitLab and will be continuously\nupdated.",
        "url": "http://arxiv.org/abs/2508.00185v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00185v1",
        "arxiv_id": "2508.00185v1",
        "authors": [
            "Alper Yaman",
            "Jannik Schwab",
            "Christof Nitsche",
            "Abhirup Sinha",
            "Marco Huber"
        ],
        "submitted": "2025-07-31 22:03:07",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Large Language Models and their deployment requirements, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on topics like content creation and generation, it lacks relevance to the user's primary research interests in IR, ranking models, and user behavior modeling."
    },
    {
        "title": "A Survey on Code Generation with LLM-based Agents",
        "abstract": "Code generation agents powered by large language models (LLMs) are\nrevolutionizing the software development paradigm. Distinct from previous code\ngeneration techniques, code generation agents are characterized by three core\nfeatures. 1) Autonomy: the ability to independently manage the entire workflow,\nfrom task decomposition to coding and debugging. 2) Expanded task scope:\ncapabilities that extend beyond generating code snippets to encompass the full\nsoftware development lifecycle (SDLC). 3) Enhancement of engineering\npracticality: a shift in research emphasis from algorithmic innovation toward\npractical engineering challenges, such as system reliability, process\nmanagement, and tool integration. This domain has recently witnessed rapid\ndevelopment and an explosion in research, demonstrating significant application\npotential. This paper presents a systematic survey of the field of LLM-based\ncode generation agents. We trace the technology's developmental trajectory from\nits inception and systematically categorize its core techniques, including both\nsingle-agent and multi-agent architectures. Furthermore, this survey details\nthe applications of LLM-based agents across the full SDLC, summarizes\nmainstream evaluation benchmarks and metrics, and catalogs representative\ntools. Finally, by analyzing the primary challenges, we identify and propose\nseveral foundational, long-term research directions for the future work of the\nfield.",
        "url": "http://arxiv.org/abs/2508.00083v1",
        "pdf_url": "http://arxiv.org/pdf/2508.00083v1",
        "arxiv_id": "2508.00083v1",
        "authors": [
            "Yihong Dong",
            "Xue Jiang",
            "Jiaru Qian",
            "Tian Wang",
            "Kechi Zhang",
            "Zhi Jin",
            "Ge Li"
        ],
        "submitted": "2025-07-31 18:17:36",
        "source": "arxiv",
        "comment": "Work in progress",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on code generation with LLM-based agents, which is not directly related to information retrieval, search technologies, or natural language processing. While it mentions large language models, the context is different from query understanding, ranking models, or user behavior modeling, and the paper's scope is limited to software development and code generation."
    }
]