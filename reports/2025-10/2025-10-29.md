# Daily Papers Report - 2025-10-29

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. InteractComp: Evaluating Search Agents With Ambiguous Queries

- **LLM Score**: 8
- **Keyword Score**: 13
- **Authors**: Mingyi Deng, Lijun Huang, Yani Fan, Jiayi Zhang, Fashen Ren, Jinyi Bai, Fuzhen Yang, Dayi Miao, Zhaoyang Yu, Yifan Wu, Yanfei Zhang, Fengwei Teng, Yingjia Wan, Song Hu, Yude Li, Xin Jin, Conghao Hu, Haoyu Li, Qirui Fu, Tai Zhong, Xinyu Wang, Xiangru Tang, Nan Tang, Chenglin Wu, Yuyu Luo
- **URL**: <http://arxiv.org/abs/2510.24668v1>
- **Submitted**: 2025-10-28 17:35:54
- **Topic Keywords**: information retrieval, query, queries, retrieval, web search, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly query understanding and search technologies. The focus on evaluating search agents with ambiguous queries aligns with your interests in deep semantic understanding and real-time relevance optimization. However, the specific domain of web search and language agents is somewhat distinct from your e-commerce background.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Evaluating Search Agents with Ambiguous Queries Using the INTERACTCOMP Benchmark
- **Aim**: To assess search agents' ability to recognize query ambiguity and resolve it through interactive clarification, addressing a gap in existing evaluations that assume unambiguous queries.
- **Rationale**: Current search agent evaluations focus on unambiguous queries, but real-world user behavior often involves ambiguous or incomplete questions. INTERACTCOMP fills this gap by testing interaction capabilities critical for resolving ambiguity.
- **Ground**: The benchmark uses a target-distractor methodology to create ambiguous questions, with 210 expert-curated questions across 9 domains (e.g., Science & Engineering, Humanities). It includes bilingual (English/Chinese) coverage and a two-stage verification process to ensure ambiguity cannot be resolved via direct search or non-interactive models.
- **Experiment**: 17 models were evaluated across three configurations: answer-only, answer+search, and answer+search+interact. Results showed a 13.73% accuracy in interaction settings vs. 71.50% with complete context, revealing overconfidence in non-interactive models. A longitudinal analysis over 15 months found stagnation in interaction capabilities despite improvements in general search performance.
- **Takeaway**: Current search agents are overconfident in ambiguous scenarios and underutilize interaction capabilities. INTERACTCOMP highlights the need for training strategies prioritizing iterative clarification, as interaction significantly improves accuracy. The benchmark provides a framework for advancing reinforcement learning with vision and reasoning (RLVR) approaches in ambiguous search tasks.

#### Abstract
> Language agents have demonstrated remarkable potential in web search and
information retrieval. However, these search agents assume user queries are
complete and unambiguous, an assumption that diverges from reality where users
begin with incomplete queries requiring clarification through interaction. Yet
most agents lack interactive mechanisms during the search process, and existing
benchmarks cannot assess this capability. To address this gap, we introduce
InteractComp, a benchmark designed to evaluate whether search agents can
recognize query ambiguity and actively interact to resolve it during search.
Following the principle of easy to verify, interact to disambiguate, we
construct 210 expert-curated questions across 9 domains through a
target-distractor methodology that creates genuine ambiguity resolvable only
through interaction. Evaluation of 17 models reveals striking failure: the best
model achieves only 13.73% accuracy despite 71.50% with complete context,
exposing systematic overconfidence rather than reasoning deficits. Forced
interaction produces dramatic gains, demonstrating latent capability current
strategies fail to engage. Longitudinal analysis shows interaction capabilities
stagnated over 15 months while search performance improved seven-fold,
revealing a critical blind spot. This stagnation, coupled with the immediate
feedback inherent to search tasks, makes InteractComp a valuable resource for
both evaluating and training interaction capabilities in search agents. The
code is available at https://github.com/FoundationAgents/InteractComp.

---

### 2. Optimizing Retrieval for RAG via Reinforced Contrastive Learning

- **LLM Score**: 8
- **Keyword Score**: 13
- **Authors**: Jiawei Zhou, Lei Chen
- **URL**: <http://arxiv.org/abs/2510.24652v1>
- **Submitted**: 2025-10-28 17:18:30
- **Topic Keywords**: information retrieval, retriever, relevance, rag, retrieval
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of retrieval-augmented generation (RAG) and the challenges of defining relevance for AI systems. The proposed Reinforced contrastive learning framework is an innovative approach to optimizing retrieval, which aligns with your focus on deep semantic understanding and real-time relevance optimization. While the paper's specific application is in RAG, the underlying IR concepts are applicable to your broader interests.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: R3: A Retrieval Framework for Optimizing Retrieval-Augmented Generation (RAG) Systems
- **Aim**: To address the challenge of defining and annotating relevance for AI systems in RAG environments by introducing a dynamic, self-improving retrieval framework.
- **Rationale**: Traditional information retrieval methods for humans are inadequate for AI systems, where relevance criteria are ambiguous and require adaptive optimization.
- **Ground**: R3 utilizes trial-and-feedback reinforced contrastive learning, enabling the retriever to iteratively refine relevance through interaction with the RAG environment and contrastive signals.
- **Experiment**: Experiments across diverse tasks demonstrated a 5.2% improvement over baseline retrievers, 4.9% superiority over state-of-the-art models, and comparable performance to LLM-augmented systems, with training completed on 4 GPUs in one day.
- **Takeaway**: R3 offers an efficient, practical solution for enhancing RAG systems by dynamically optimizing relevance without requiring explicit annotations, achieving state-of-the-art results with minimal computational resources.

#### Abstract
> As retrieval-augmented generation (RAG) becomes increasingly widespread, the
role of information retrieval (IR) is shifting from retrieving information for
human users to retrieving contextual knowledge for artificial intelligence (AI)
systems, where relevance becomes difficult to define or annotate beforehand. To
address this challenge, we propose R3, a Retrieval framework optimized for RAG
through trialand-feedback Reinforced contrastive learning. Unlike prior
approaches that rely on annotated or synthetic data for supervised fine-tuning,
R3 enables the retriever to dynamically explore and optimize relevance within
the RAG environment. During training, the retrieved results interact with the
environment to produce contrastive signals that automatically guide the
retriever's self-improvement. Extensive experiments across diverse tasks
demonstrate that R3 improves RAG performance by 5.2% over the original
retriever and surpasses state-of-the-art retrievers by 4.9%, while achieving
comparable results to LLM-augmented retrieval and RAG systems built on
post-trained or instruction-tuned LLMs. It is both efficient and practical,
requiring only 4 GPUs and completing training within a single day.

---

### 3. WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking

- **LLM Score**: 8
- **Keyword Score**: 5
- **Authors**: Zhengwei Tao, Haiyang Shen, Baixuan Li, Wenbiao Yin, Jialong Wu, Kuan Li, Zhongwang Zhang, Huifeng Yin, Rui Ye, Liwen Zhang, Xinyu Wang, Pengjun Xie, Jingren Zhou, Yong Jiang
- **URL**: <http://arxiv.org/abs/2510.24697v1>
- **Submitted**: 2025-10-28 17:51:42
- **Topic Keywords**: rag, retrieval, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in areas that require deep semantic understanding and real-time relevance optimization. The proposed framework, WebLeaper, aims to improve search efficiency and efficacy in WebAgent, which aligns with your focus on query understanding and ranking models. However, the specific application domain of WebAgent is not explicitly mentioned, which prevents a perfect match with your e-commerce background.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Maximal Union Algorithm for Task Synthesis in Complex Reasoning Frameworks
- **Aim**: To develop a scalable framework for generating multi-source reasoning tasks by identifying maximal unions of reasoning trees, using bipartite graphs and biclique enumeration to model semantic compatibility.
- **Rationale**: Current task generation methods lack systematic escalation of complexity through relational integration and obfuscation. This work addresses the need for synthetic tasks that test multi-step reasoning, cross-source validation, and ambiguity resolution in real-world scenarios.
- **Ground**: The framework employs a three-stage algorithm: (1) bipartite graph construction of reasoning trees and shared relations, (2) maximal biclique enumeration with constraints $ k_{\text{min}} $ and $ m_{\text{min}} $, and (3) validation of bicliques into task groups. Three task versions are introduced: Basic (single-tree), Union (multi-tree relational operations), and Reverse-Union (obfuscated anchor entity deduction).
- **Experiment**: Tasks demonstrate escalating complexity: Union tasks require intersection of two entity sets from distinct trees, while Reverse-Union tasks simulate real-world ambiguity by masking anchor entities. Experiments show high tool call density (50+ actions per task) and validate the algorithm's ability to generate semantically coherent, multi-stage reasoning challenges.
- **Takeaway**: The Maximal Union Algorithm provides a robust foundation for synthetic task generation, enabling evaluation of reasoning systems in complex, obfuscated environments. The three task versions (Basic, Union, Reverse-Union) offer a structured progression from simple to advanced reasoning, validated through high interaction intensity and real-world applicability.

#### Abstract
> Large Language Model (LLM)-based agents have emerged as a transformative
approach for open-ended problem solving, with information seeking (IS) being a
core capability that enables autonomous reasoning and decision-making. While
prior research has largely focused on improving retrieval depth, we observe
that current IS agents often suffer from low search efficiency, which in turn
constrains overall performance. A key factor underlying this inefficiency is
the sparsity of target entities in training tasks, which limits opportunities
for agents to learn and generalize efficient search behaviors. To address these
challenges, we propose WebLeaper, a framework for constructing high-coverage IS
tasks and generating efficient solution trajectories. We formulate IS as a
tree-structured reasoning problem, enabling a substantially larger set of
target entities to be embedded within a constrained context. Leveraging curated
Wikipedia tables, we propose three variants for synthesizing IS tasks, Basic,
Union, and Reverse-Union, to systematically increase both IS efficiency and
efficacy. Finally, we curate training trajectories by retaining only those that
are simultaneously accurate and efficient, ensuring that the model is optimized
for both correctness and search performance. Extensive experiments on both
basic and comprehensive settings, conducted on five IS benchmarks, BrowserComp,
GAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method
consistently achieves improvements in both effectiveness and efficiency over
strong baselines.

---

### 4. Talk2Ref: A Dataset for Reference Prediction from Scientific Talks

- **LLM Score**: 7
- **Keyword Score**: 9
- **Authors**: Frederik Broy, Maike Z√ºfle, Jan Niehues
- **URL**: <http://arxiv.org/abs/2510.24478v1>
- **Submitted**: 2025-10-28 14:50:03
- **Topic Keywords**: relevance, rag, retrieval, recommend, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The task of Reference Prediction from Talks involves mapping scientific talks to relevant papers, which shares some similarities with your work on query understanding and citation prediction. However, the focus on spoken scientific content and citation recommendation systems is not a central match to your primary research themes.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Talk2Ref: A Large-Scale Benchmark for Reference Prediction in Scientific Talks
- **Aim**: To develop a benchmark for retrieving academic references from spoken scientific talks, addressing challenges like noisy transcripts and long-form input.
- **Rationale**: Existing citation systems focus on structured text (e.g., abstracts), but spoken content introduces unique difficulties such as temporal constraints, noise, and unstructured data.
- **Ground**: The Talk2Ref dataset includes scientific talks paired with references, emphasizing temporal validity (only pre-talk papers are valid) and requiring alignment of spoken content with structured paper metadata.
- **Experiment**: Dual-encoder models (SBERT, BERT, Longformer, SPECTER2) were evaluated with domain adaptation, fine-tuning, and aggregation strategies. SBERT outperformed others, while Longformer struggled with long sequences. Evaluation metrics included MAP@k and precision.
- **Takeaway**: Reference prediction from spoken content is significantly harder than from structured text. SBERT with domain adaptation achieves best results, but challenges remain in handling long transcripts and noisy data. The dataset and models will be open-sourced to advance this field.

#### Abstract
> Scientific talks are a growing medium for disseminating research, and
automatically identifying relevant literature that grounds or enriches a talk
would be highly valuable for researchers and students alike. We introduce
Reference Prediction from Talks (RPT), a new task that maps long, and
unstructured scientific presentations to relevant papers. To support research
on RPT, we present Talk2Ref, the first large-scale dataset of its kind,
containing 6,279 talks and 43,429 cited papers (26 per talk on average), where
relevance is approximated by the papers cited in the talk's corresponding
source publication. We establish strong baselines by evaluating
state-of-the-art text embedding models in zero-shot retrieval scenarios, and
propose a dual-encoder architecture trained on Talk2Ref. We further explore
strategies for handling long transcripts, as well as training for domain
adaptation. Our results show that fine-tuning on Talk2Ref significantly
improves citation prediction performance, demonstrating both the challenges of
the task and the effectiveness of our dataset for learning semantic
representations from spoken scientific content. The dataset and trained models
are released under an open license to foster future research on integrating
spoken scientific communication into citation recommendation systems.

---

### 5. Repurposing Synthetic Data for Fine-grained Search Agent Supervision

- **LLM Score**: 7
- **Keyword Score**: 3
- **Authors**: Yida Zhao, Kuan Li, Xixi Wu, Liwen Zhang, Dingchu Zhang, Baixuan Li, Maojia Song, Zhuo Chen, Chenxi Wang, Xinyu Wang, Kewei Tu, Pengjun Xie, Jingren Zhou, Yong Jiang
- **URL**: <http://arxiv.org/abs/2510.24694v1>
- **Submitted**: 2025-10-28 17:50:40
- **Topic Keywords**: rag, search
- **Reason**: The paper explores a novel approach to training search agents using entity-aware rewards, which is related to query understanding and ranking models. Although it's not directly focused on e-commerce or real-time relevance optimization, it's relevant to the broader field of information retrieval and search technologies. The paper's emphasis on deep semantic understanding and learning from 'near-miss' samples aligns with the user's interests in IR and NLP.

#### Abstract
> LLM-based search agents are increasingly trained on entity-centric synthetic
data to solve complex, knowledge-intensive tasks. However, prevailing training
methods like Group Relative Policy Optimization (GRPO) discard this rich entity
information, relying instead on sparse, outcome-based rewards. This critical
limitation renders them unable to distinguish informative "near-miss"
samples-those with substantially correct reasoning but a flawed final
answer-from complete failures, thus discarding valuable learning signals. We
address this by leveraging the very entities discarded during training. Our
empirical analysis reveals a strong positive correlation between the number of
ground-truth entities identified during an agent's reasoning process and final
answer accuracy. Building on this insight, we introduce Entity-aware Group
Relative Policy Optimization (E-GRPO), a novel framework that formulates a
dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect
samples proportional to their entity match rate, enabling the model to
effectively learn from these "near-misses". Experiments on diverse
question-answering (QA) and deep research benchmarks show that E-GRPO
consistently and significantly outperforms the GRPO baseline. Furthermore, our
analysis reveals that E-GRPO not only achieves superior accuracy but also
induces more efficient reasoning policies that require fewer tool calls,
demonstrating a more effective and sample-efficient approach to aligning search
agents.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Auto prompting without training labels: An LLM cascade for product quality assessment in e-commerce catalogs

- **LLM Score**: 7
- **Keyword Score**: 2
- **Authors**: Soham Satyadharma, Fatemeh Sheikholeslami, Swati Kaul, Aziz Umit Batur, Suleiman A. Khan
- **URL**: <http://arxiv.org/abs/2510.23941v1>
- **Submitted**: 2025-10-27 23:49:31
- **Topic Keywords**: commerce, e-commerce
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. Although it focuses on product quality assessment in e-commerce catalogs, it involves the use of Large Language Models (LLMs) and auto-prompting techniques, which are relevant to your interests in NLP and deep semantic understanding. However, the specific application domain and focus on product quality assessment make it not a central match to your core research themes.

#### Abstract
> We introduce a novel, training free cascade for auto-prompting Large Language
Models (LLMs) to assess product quality in e-commerce. Our system requires no
training labels or model fine-tuning, instead automatically generating and
refining prompts for evaluating attribute quality across tens of thousands of
product category-attribute pairs. Starting from a seed of human-crafted
prompts, the cascade progressively optimizes instructions to meet
catalog-specific requirements. This approach bridges the gap between general
language understanding and domain-specific knowledge at scale in complex
industrial catalogs. Our extensive empirical evaluations shows the auto-prompt
cascade improves precision and recall by $8-10\%$ over traditional
chain-of-thought prompting. Notably, it achieves these gains while reducing
domain expert effort from 5.1 hours to 3 minutes per attribute - a $99\%$
reduction. Additionally, the cascade generalizes effectively across five
languages and multiple quality assessment tasks, consistently maintaining
performance gains.

### 7. Reinforcement Learning for Long-Horizon Multi-Turn Search Agents

- **LLM Score**: 6
- **Keyword Score**: 3
- **Authors**: Vivek Kalyan, Martin Andrews
- **URL**: <http://arxiv.org/abs/2510.24126v1>
- **Submitted**: 2025-10-28 07:00:42
- **Comment**: 4 pages plus references and appendices. Accepted into the First
  Workshop on Multi-Turn Interactions in Large Language Models at NeurIPS 2025
- **Topic Keywords**: rag, search
- **Reason**: The paper explores the application of Reinforcement Learning in search agents, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the focus on long-horizon multi-turn search agents and the use of Large Language Models is not a central match to your primary research themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large Language Model (LLM) agents can leverage multiple turns and tools to
solve complex tasks, with prompt-based approaches achieving strong performance.
This work demonstrates that Reinforcement Learning (RL) can push capabilities
significantly further by learning from experience. Through experiments on a
legal document search benchmark, we show that our RL-trained 14 Billion
parameter model outperforms frontier class models (85% vs 78% accuracy). In
addition, we explore turn-restricted regimes, during training and at test-time,
that show these agents achieve better results if allowed to operate over longer
multi-turn horizons.

### 8. From Time and Place to Preference: LLM-Driven Geo-Temporal Context in Recommendations

- **LLM Score**: 6
- **Keyword Score**: 2
- **Authors**: Yejin Kim, Shaghayegh Agah, Mayur Nankani, Neeraj Sharma, Feifei Peng, Maria Peifer, Sardar Hamidian, H Howie Huang
- **URL**: <http://arxiv.org/abs/2510.24430v1>
- **Submitted**: 2025-10-28 13:57:23
- **Topic Keywords**: recommend, search
- **Reason**: The paper explores recommender systems, leveraging large language models to generate geo-temporal embeddings. While it touches on aspects related to information retrieval, such as sequential models and predictive signal, its primary focus is on recommendations rather than search technologies. The connection to user behavior modeling and click models is indirect, but the use of real-world context is relevant to query understanding.

#### Abstract
> Most recommender systems treat timestamps as numeric or cyclical values,
overlooking real-world context such as holidays, events, and seasonal patterns.
We propose a scalable framework that uses large language models (LLMs) to
generate geo-temporal embeddings from only a timestamp and coarse location,
capturing holidays, seasonal trends, and local/global events. We then introduce
a geo-temporal embedding informativeness test as a lightweight diagnostic,
demonstrating on MovieLens, LastFM, and a production dataset that these
embeddings provide predictive signal consistent with the outcomes of full model
integrations. Geo-temporal embeddings are incorporated into sequential models
through (1) direct feature fusion with metadata embeddings or (2) an auxiliary
loss that enforces semantic and geo-temporal alignment. Our findings highlight
the need for adaptive or hybrid recommendation strategies, and we release a
context-enriched MovieLens dataset to support future research.

### 9. PICOs-RAG: PICO-supported Query Rewriting for Retrieval-Augmented Generation in Evidence-Based Medicine

- **LLM Score**: 4
- **Keyword Score**: 14
- **Authors**: Mengzhou Sun, Sendong Zhao, Jianyu Chen, Bin Qin
- **URL**: <http://arxiv.org/abs/2510.23998v1>
- **Submitted**: 2025-10-28 02:01:05
- **Topic Keywords**: query, queries, relevance, rag, retrieval, search
- **Reason**: The paper discusses retrieval-augmented generation in Evidence-Based Medicine, which is related to information retrieval and query understanding. However, the focus on Evidence-Based Medicine and the PICO format limits its relevance to the user's broader interests in general information retrieval and search technologies.

#### Abstract
> Evidence-based medicine (EBM) research has always been of paramount
importance. It is important to find appropriate medical theoretical support for
the needs from physicians or patients to reduce the occurrence of medical
accidents. This process is often carried out by human querying relevant
literature databases, which lacks objectivity and efficiency. Therefore,
researchers utilize retrieval-augmented generation (RAG) to search for evidence
and generate responses automatically. However, current RAG methods struggle to
handle complex queries in real-world clinical scenarios. For example, when
queries lack certain information or use imprecise language, the model may
retrieve irrelevant evidence and generate unhelpful answers. To address this
issue, we present the PICOs-RAG to expand the user queries into a better
format. Our method can expand and normalize the queries into professional ones
and use the PICO format, a search strategy tool present in EBM, to extract the
most important information used for retrieval. This approach significantly
enhances retrieval efficiency and relevance, resulting in up to an 8.8\%
improvement compared to the baseline evaluated by our method. Thereby the
PICOs-RAG improves the performance of the large language models into a helpful
and reliable medical assistant in EBM.

### 10. Metadata-Driven Retrieval-Augmented Generation for Financial Question Answering

- **LLM Score**: 4
- **Keyword Score**: 13
- **Authors**: Michail Dadopoulos, Anestis Ladas, Stratos Moschidis, Ioannis Negkakis
- **URL**: <http://arxiv.org/abs/2510.24402v1>
- **Submitted**: 2025-10-28 13:16:36
- **Comment**: Preprint version submitted to the International Journal of Accounting
  Information Systems; currently under major revision. 20 pages, 1 figure, 1
  table
- **Topic Keywords**: ranking, rerank, rag, ctr, retrieval, rank
- **Reason**: The paper explores Retrieval-Augmented Generation (RAG) techniques, which is related to query understanding and ranking models. However, the focus on financial question answering and metadata-driven approaches is somewhat specific and not directly aligned with the user's core research themes in Information Retrieval and Search technologies.

#### Abstract
> Retrieval-Augmented Generation (RAG) struggles on long, structured financial
filings where relevant evidence is sparse and cross-referenced. This paper
presents a systematic investigation of advanced metadata-driven
Retrieval-Augmented Generation (RAG) techniques, proposing and evaluating a
novel, multi-stage RAG architecture that leverages LLM-generated metadata. We
introduce a sophisticated indexing pipeline to create contextually rich
document chunks and benchmark a spectrum of enhancements, including
pre-retrieval filtering, post-retrieval reranking, and enriched embeddings,
benchmarked on the FinanceBench dataset. Our results reveal that while a
powerful reranker is essential for precision, the most significant performance
gains come from embedding chunk metadata directly with text ("contextual
chunks"). Our proposed optimal architecture combines LLM-driven pre-retrieval
optimizations with these contextual embeddings to achieve superior performance.
Additionally, we present a custom metadata reranker that offers a compelling,
cost-effective alternative to commercial solutions, highlighting a practical
trade-off between peak performance and operational efficiency. This study
provides a blueprint for building robust, metadata-aware RAG systems for
financial document analysis.

### 11. META-RAG: Meta-Analysis-Inspired Evidence-Re-Ranking Method for Retrieval-Augmented Generation in Evidence-Based Medicine

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Mengzhou Sun, Sendong Zhao, Jianyu Chen, Haochun Wang, Bin Qin
- **URL**: <http://arxiv.org/abs/2510.24003v1>
- **Submitted**: 2025-10-28 02:18:09
- **Topic Keywords**: ranking, rag, retrieval, rank, search
- **Reason**: The paper discusses Retrieval-Augmented Generation (RAG) in Evidence-Based Medicine, which is a specific application of Information Retrieval. While it touches on query understanding and ranking models, the focus is on medical evidence and not directly related to the user's core research themes in e-commerce or general IR. The paper's emphasis on meta-analysis and evidence filtering is somewhat relevant to data mining and NLP, but the connection is not strong enough to warrant a higher score.

#### Abstract
> Evidence-based medicine (EBM) holds a crucial role in clinical application.
Given suitable medical articles, doctors effectively reduce the incidence of
misdiagnoses. Researchers find it efficient to use large language models (LLMs)
techniques like RAG for EBM tasks. However, the EBM maintains stringent
requirements for evidence, and RAG applications in EBM struggle to efficiently
distinguish high-quality evidence. Therefore, inspired by the meta-analysis
used in EBM, we provide a new method to re-rank and filter the medical
evidence. This method presents multiple principles to filter the best evidence
for LLMs to diagnose. We employ a combination of several EBM methods to emulate
the meta-analysis, which includes reliability analysis, heterogeneity analysis,
and extrapolation analysis. These processes allow the users to retrieve the
best medical evidence for the LLMs. Ultimately, we evaluate these high-quality
articles and show an accuracy improvement of up to 11.4% in our experiments and
results. Our method successfully enables RAG to extract higher-quality and more
reliable evidence from the PubMed dataset. This work can reduce the infusion of
incorrect knowledge into responses and help users receive more effective
replies.

### 12. DUET: Dual Model Co-Training for Entire Space CTR Prediction

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Yutian Xiao, Meng Yuan, Fuzhen Zhuang, Wei Chen, Shukuan Wang, Shanqi Liu, Chao Feng, Wenhui Yu, Xiang Li, Lantao Hu, Han Li, Zhao Zhang
- **URL**: <http://arxiv.org/abs/2510.24369v1>
- **Submitted**: 2025-10-28 12:46:33
- **Topic Keywords**: ranking, ctr, recommend, rank
- **Reason**: The paper focuses on recommender systems, specifically addressing the challenges of large-scale candidate pools and computational efficiency. While it involves click-through rate (CTR) prediction, which is related to user behavior modeling, the primary focus is on recommender systems rather than information retrieval. The paper's emphasis on set-level prediction and dual model co-training is relevant to the broader field of IR, but it does not directly address query understanding, ranking models, or deep semantic understanding.

#### Abstract
> The pre-ranking stage plays a pivotal role in large-scale recommender systems
but faces an intrinsic trade-off between model expressiveness and computational
efficiency. Owing to the massive candidate pool and strict latency constraints,
industry systems often rely on lightweight two-tower architectures, which are
computationally efficient yet limited in estimation capability. As a result,
they struggle to capture the complex synergistic and suppressive relationships
among candidate items, which are essential for producing contextually coherent
and diverse recommendation lists. Moreover, this simplicity further amplifies
the Sample Selection Bias (SSB) problem, as coarse-grained models trained on
biased exposure data must generalize to a much larger candidate space with
distinct distributions.
  To address these issues, we propose \textbf{DUET} (\textbf{DU}al Model
Co-Training for \textbf{E}ntire Space C\textbf{T}R Prediction), a set-wise
pre-ranking framework that achieves expressive modeling under tight
computational budgets. Instead of scoring items independently, DUET performs
set-level prediction over the entire candidate subset in a single forward pass,
enabling information-aware interactions among candidates while amortizing the
computational cost across the set. Moreover, a dual model co-training mechanism
extends supervision to unexposed items via mutual pseudo-label refinement,
effectively mitigating SSB. Validated through extensive offline experiments and
online A/B testing, DUET consistently outperforms state-of-the-art baselines
and achieves improvements across multiple core business metrics. At present,
DUET has been fully deployed in Kuaishou and Kuaishou Lite Apps, serving the
main traffic for hundreds of millions of users.

### 13. Mitigating Hallucination in Large Language Models (LLMs): An Application-Oriented Survey on RAG, Reasoning, and Agentic Systems

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Yihan Li, Xiyuan Fu, Ghanshyam Verma, Paul Buitelaar, Mingming Liu
- **URL**: <http://arxiv.org/abs/2510.24476v1>
- **Submitted**: 2025-10-28 14:48:57
- **Comment**: 25 pages, 7 figures, 3 tables
- **Topic Keywords**: rag, retrieval, acl
- **Reason**: The paper is somewhat relevant to the user's interests in Information Retrieval and Natural Language Processing, as it deals with mitigating hallucinations in Large Language Models. However, the focus is more on NLP and model reliability rather than query understanding, ranking models, or user behavior modeling. The connection to the user's research themes is not strong enough to warrant a higher score.

#### Abstract
> Hallucination remains one of the key obstacles to the reliable deployment of
large language models (LLMs), particularly in real-world applications. Among
various mitigation strategies, Retrieval-Augmented Generation (RAG) and
reasoning enhancement have emerged as two of the most effective and widely
adopted approaches, marking a shift from merely suppressing hallucinations to
balancing creativity and reliability. However, their synergistic potential and
underlying mechanisms for hallucination mitigation have not yet been
systematically examined. This survey adopts an application-oriented perspective
of capability enhancement to analyze how RAG, reasoning enhancement, and their
integration in Agentic Systems mitigate hallucinations. We propose a taxonomy
distinguishing knowledge-based and logic-based hallucinations, systematically
examine how RAG and reasoning address each, and present a unified framework
supported by real-world applications, evaluations, and benchmarks.

### 14. SPARTA: Evaluating Reasoning Segmentation Robustness through Black-Box Adversarial Paraphrasing in Text Autoencoder Latent Space

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Viktoriia Zinkovich, Anton Antonov, Andrei Spiridonov, Denis Shepelev, Andrey Moskalenko, Daria Pugacheva, Elena Tutubalina, Andrey Kuznetsov, Vlad Shakhuro
- **URL**: <http://arxiv.org/abs/2510.24446v1>
- **Submitted**: 2025-10-28 14:09:05
- **Topic Keywords**: query, queries
- **Reason**: This paper explores the robustness of reasoning segmentation models to adversarial paraphrasing, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on multimodal large language models and text autoencoders is not directly aligned with the user's interests in IR and Search technologies. The paper's emphasis on NLP and related topics makes it loosely relevant to the user's broader research interests.

#### Abstract
> Multimodal large language models (MLLMs) have shown impressive capabilities
in vision-language tasks such as reasoning segmentation, where models generate
segmentation masks based on textual queries. While prior work has primarily
focused on perturbing image inputs, semantically equivalent textual
paraphrases-crucial in real-world applications where users express the same
intent in varied ways-remain underexplored. To address this gap, we introduce a
novel adversarial paraphrasing task: generating grammatically correct
paraphrases that preserve the original query meaning while degrading
segmentation performance. To evaluate the quality of adversarial paraphrases,
we develop a comprehensive automatic evaluation protocol validated with human
studies. Furthermore, we introduce SPARTA-a black-box, sentence-level
optimization method that operates in the low-dimensional semantic latent space
of a text autoencoder, guided by reinforcement learning. SPARTA achieves
significantly higher success rates, outperforming prior methods by up to 2x on
both the ReasonSeg and LLMSeg-40k datasets. We use SPARTA and competitive
baselines to assess the robustness of advanced reasoning segmentation models.
We reveal that they remain vulnerable to adversarial paraphrasing-even under
strict semantic and grammatical constraints. All code and data will be released
publicly upon acceptance.

### 15. Iterative Critique-Refine Framework for Enhancing LLM Personalization

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Durga Prasad Maram, Dhruvin Gandhi, Zonghai Yao, Gayathri Akkinapalli, Franck Dernoncourt, Yu Wang, Ryan A. Rossi, Nesreen K. Ahmed
- **URL**: <http://arxiv.org/abs/2510.24469v1>
- **Submitted**: 2025-10-28 14:36:22
- **Topic Keywords**: rag, retrieval, personalization
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and text generation, but it focuses on personalization and critique-refine frameworks, which is not a central match for your primary focus on Information Retrieval, especially in areas that require deep semantic understanding and real-time relevance optimization.

#### Abstract
> Personalized text generation requires models not only to produce coherent
text but also to align with a target user's style, tone, and topical focus.
Existing retrieval-augmented approaches such as LaMP and PGraphRAG enrich
profiles with user and neighbor histories, but they stop at generation and
often yield outputs that drift in tone, topic, or style. We present PerFine, a
unified, training-free critique-refine framework that enhances personalization
through iterative, profile-grounded feedback. In each iteration, an LLM
generator produces a draft conditioned on the retrieved profile, and a critic
LLM - also conditioned on the same profile - provides structured feedback on
tone, vocabulary, sentence structure, and topicality. The generator then
revises, while a novel knockout strategy retains the stronger draft across
iterations. We further study additional inference-time strategies such as
Best-of-N and Topic Extraction to balance quality and efficiency. Across Yelp,
Goodreads, and Amazon datasets, PerFine consistently improves personalization
over PGraphRAG, with GEval gains of +7-13%, steady improvements over 3-5
refinement iterations, and scalability with increasing critic size. These
results highlight that post-hoc, profile-aware feedback offers a powerful
paradigm for personalized LLM generation that is both training-free and
model-agnostic.

### 16. MiniOneRec: An Open-Source Framework for Scaling Generative Recommendation

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Xiaoyu Kong, Leheng Sheng, Junfei Tan, Yuxin Chen, Jiancan Wu, An Zhang, Xiang Wang, Xiangnan He
- **URL**: <http://arxiv.org/abs/2510.24431v1>
- **Submitted**: 2025-10-28 13:58:36
- **Comment**: Technical Report
- **Topic Keywords**: ranking, recommend, rank
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and recommender systems, but it primarily focuses on generative recommendation frameworks, which is a different area of study. While it touches on ranking accuracy and candidate diversity, it does not directly address query understanding, ranking models, or user behavior modeling, which are core aspects of your research.

#### Abstract
> The recent success of large language models (LLMs) has renewed interest in
whether recommender systems can achieve similar scaling benefits. Conventional
recommenders, dominated by massive embedding tables, tend to plateau as
embedding dimensions grow. In contrast, the emerging generative paradigm
replaces embeddings with compact Semantic ID (SID) sequences produced by
autoregressive Transformers. Yet most industrial deployments remain
proprietary, leaving two fundamental questions open: (1) Do the expected
scaling laws hold on public benchmarks? (2) What is the minimal post-training
recipe that enables competitive performance?
  We present MiniOneRec, to the best of our knowledge, the first fully
open-source generative recommendation framework, which provides an end-to-end
workflow spanning SID construction, supervised fine-tuning, and
recommendation-oriented reinforcement learning. We generate SIDs via a Residual
Quantized VAE and post-train Qwen backbones ranging from 0.5B to 7B parameters
on the Amazon Review dataset. Our experiments reveal a consistent downward
trend in both training and evaluation losses with increasing model size,
validating the parameter efficiency of the generative approach. To further
enhance performance, we propose a lightweight yet effective post-training
pipeline that (1) enforces full-process SID alignment and (2) applies
reinforcement learning with constrained decoding and hybrid rewards. Together,
these techniques yield significant improvements in both ranking accuracy and
candidate diversity.

### 17. Evaluating Long-Term Memory for Long-Context Question Answering

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Alessandra Terranova, Bj√∂rn Ross, Alexandra Birch
- **URL**: <http://arxiv.org/abs/2510.23730v1>
- **Submitted**: 2025-10-27 18:03:50
- **Comment**: 14 pages including appendix, 3 figures. Submitted to October ARR and
  to Metacognition in Generative AI EurIPS workshop (under review for both)
- **Topic Keywords**: rag, retrieval, search
- **Reason**: The paper explores memory-augmented methods for long-context conversational tasks, which is related to query understanding and ranking models in Information Retrieval. However, it primarily focuses on Natural Language Processing and conversational AI, which, while tangentially related to your interests, is not a central match.

#### Abstract
> In order for large language models to achieve true conversational continuity
and benefit from experiential learning, they need memory. While research has
focused on the development of complex memory systems, it remains unclear which
types of memory are most effective for long-context conversational tasks. We
present a systematic evaluation of memory-augmented methods using LoCoMo, a
benchmark of synthetic long-context dialogues annotated for question-answering
tasks that require diverse reasoning strategies. We analyse full-context
prompting, semantic memory through retrieval-augmented generation and agentic
memory, episodic memory through in-context learning, and procedural memory
through prompt optimization. Our findings show that memory-augmented approaches
reduce token usage by over 90% while maintaining competitive accuracy. Memory
architecture complexity should scale with model capability, with small
foundation models benefitting most from RAG, and strong instruction-tuned
reasoning model gaining from episodic learning through reflections and more
complex agentic semantic memory. In particular, episodic memory can help LLMs
recognise the limits of their own knowledge.

### 18. M-Eval: A Heterogeneity-Based Framework for Multi-evidence Validation in Medical RAG Systems

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Mengzhou Sun, Sendong Zhao, Jianyu Chen, Haochun Wang, Bin Qin
- **URL**: <http://arxiv.org/abs/2510.23995v1>
- **Submitted**: 2025-10-28 01:57:40
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper discusses Retrieval-augmented Generation (RAG) systems, which is related to Information Retrieval. However, the focus on medical question-answering systems and evidence validation is somewhat specific and not directly aligned with the user's core research themes, such as query understanding, ranking models, and user behavior modeling.

#### Abstract
> Retrieval-augmented Generation (RAG) has demonstrated potential in enhancing
medical question-answering systems through the integration of large language
models (LLMs) with external medical literature. LLMs can retrieve relevant
medical articles to generate more professional responses efficiently. However,
current RAG applications still face problems. They generate incorrect
information, such as hallucinations, and they fail to use external knowledge
correctly. To solve these issues, we propose a new method named M-Eval. This
method is inspired by the heterogeneity analysis approach used in
Evidence-Based Medicine (EBM). Our approach can check for factual errors in RAG
responses using evidence from multiple sources. First, we extract additional
medical literature from external knowledge bases. Then, we retrieve the
evidence documents generated by the RAG system. We use heterogeneity analysis
to check whether the evidence supports different viewpoints in the response. In
addition to verifying the accuracy of the response, we also assess the
reliability of the evidence provided by the RAG system. Our method shows an
improvement of up to 23.31% accuracy across various LLMs. This work can help
detect errors in current RAG-based medical systems. It also makes the
applications of LLMs more reliable and reduces diagnostic errors.

### 19. OraPlan-SQL: A Planning-Centric Framework for Complex Bilingual NL2SQL Reasoning

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Marianne Menglin Liu, Sai Ashish Somayajula, Syed Fahad Allam Shah, Sujith Ravi, Dan Roth
- **URL**: <http://arxiv.org/abs/2510.23870v1>
- **Submitted**: 2025-10-27 21:22:41
- **Topic Keywords**: query, rank
- **Reason**: The paper is somewhat related to the user's interests in Natural Language Processing (NLP) and query understanding, but it focuses on NL2SQL reasoning, which is not a central match for the user's primary focus on information retrieval and search technologies. The paper's use of Learning to Rank and user behavior modeling is not explicitly mentioned, but it does involve a planning-centric framework and a feedback-guided meta-prompting strategy, which could be of interest to the user.

#### Abstract
> We present OraPlan-SQL, our system for the Archer NL2SQL Evaluation Challenge
2025, a bilingual benchmark requiring complex reasoning such as arithmetic,
commonsense, and hypothetical inference. OraPlan-SQL ranked first, exceeding
the second-best system by more than 6% in execution accuracy (EX), with 55.0%
in English and 56.7% in Chinese, while maintaining over 99% SQL validity (VA).
Our system follows an agentic framework with two components: Planner agent that
generates stepwise natural language plans, and SQL agent that converts these
plans into executable SQL. Since SQL agent reliably adheres to the plan, our
refinements focus on the planner. Unlike prior methods that rely on multiple
sub-agents for planning and suffer from orchestration overhead, we introduce a
feedback-guided meta-prompting strategy to refine a single planner. Failure
cases from a held-out set are clustered with human input, and an LLM distills
them into corrective guidelines that are integrated into the planner's system
prompt, improving generalization without added complexity. For the multilingual
scenario, to address transliteration and entity mismatch issues, we incorporate
entity-linking guidelines that generate alternative surface forms for entities
and explicitly include them in the plan. Finally, we enhance reliability
through plan diversification: multiple candidate plans are generated for each
query, with the SQL agent producing a query for each plan, and final output
selected via majority voting over their executions.

### 20. Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Yueqi Song, Ketan Ramaneti, Zaid Sheikh, Ziru Chen, Boyu Gou, Tianbao Xie, Yiheng Xu, Danyang Zhang, Apurva Gandhi, Fan Yang, Joseph Liu, Tianyue Ou, Zhihao Yuan, Frank Xu, Shuyan Zhou, Xingyao Wang, Xiang Yue, Tao Yu, Huan Sun, Yu Su, Graham Neubig
- **URL**: <http://arxiv.org/abs/2510.24702v1>
- **Submitted**: 2025-10-28 17:53:13
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on developing a unified protocol for agent data, which is relevant to the broader field of information retrieval and NLP. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's emphasis on large language models (LLMs) and agent training pipelines is somewhat related to the user's interests in deep semantic understanding and real-time relevance optimization.

#### Abstract
> Public research results on large-scale supervised finetuning of AI agents
remain relatively rare, since the collection of agent training data presents
unique challenges. In this work, we argue that the bottleneck is not a lack of
underlying data sources, but that a large variety of data is fragmented across
heterogeneous formats, tools, and interfaces. To this end, we introduce the
agent data protocol (ADP), a light-weight representation language that serves
as an "interlingua" between agent datasets in diverse formats and unified agent
training pipelines downstream. The design of ADP is expressive enough to
capture a large variety of tasks, including API/tool use, browsing, coding,
software engineering, and general agentic workflows, while remaining simple to
parse and train on without engineering at a per-dataset level. In experiments,
we unified a broad collection of 13 existing agent training datasets into ADP
format, and converted the standardized ADP data into training-ready formats for
multiple agent frameworks. We performed SFT on these data, and demonstrated an
average performance gain of ~20% over corresponding base models, and delivers
state-of-the-art or near-SOTA performance on standard coding, browsing, tool
use, and research benchmarks, without domain-specific tuning. All code and data
are released publicly, in the hope that ADP could help lower the barrier to
standardized, scalable, and reproducible agent training.

### 21. OpenReward: Learning to Reward Long-form Agentic Tasks via Reinforcement Learning

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Ziyou Hu, Zhengliang Shi, Minghang Zhu, Haitao Li, Teng Sun, Pengjie Ren, Suzan Verberne, Zhaochun Ren
- **URL**: <http://arxiv.org/abs/2510.24636v2>
- **Submitted**: 2025-10-28 17:02:46
- **Topic Keywords**: pairwise
- **Reason**: The paper focuses on reward modeling for large language models, which is an NLP topic, but it does not address information retrieval, ranking, or user behavior modeling. It may offer insights into evaluation techniques that could be adapted to IR, yet it is not directly relevant to the user‚Äôs core research themes.

#### Abstract
> Reward models (RMs) have become essential for aligning large language models
(LLMs), serving as scalable proxies for human evaluation in both training and
inference. However, existing RMs struggle on knowledge-intensive and long-form
tasks, where evaluating correctness requires grounding beyond the model's
internal knowledge. This limitation hinders them from reliably discriminating
subtle quality differences, especially when external evidence is necessary. To
address this, we introduce OpenRM, a tool-augmented long-form reward model that
systematically judges open-ended responses by invoking external tools to gather
relevant evidence. We train OpenRM with Group Relative Policy Optimization
(GRPO) on over 27K synthesized pairwise examples generated through a
controllable data synthesis framework. The training objective jointly
supervises intermediate tool usage and final outcome accuracy, incentivizing
our reward model to learn effective evidence-based judgment strategies.
Extensive experiments on three newly-collected datasets and two widely-used
benchmarks demonstrate that OpenRM substantially outperforms existing reward
modeling approaches. As a further step, we integrate OpenRM into both
inference-time response selection and training-time data selection. This yields
consistent gains in downstream LLM alignment tasks, highlighting the potential
of tool-augmented reward models for scaling reliable long-form evaluation.

### 22. HACK: Hallucinations Along Certainty and Knowledge Axes

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Adi Simhi, Jonathan Herzig, Itay Itzhak, Dana Arad, Zorik Gekhman, Roi Reichart, Fazl Barez, Gabriel Stanovsky, Idan Szpektor, Yonatan Belinkov
- **URL**: <http://arxiv.org/abs/2510.24222v1>
- **Submitted**: 2025-10-28 09:34:31
- **Comment**: The code is available at
  https://github.com/technion-cs-nlp/HACK_Hallucinations_Along_Certainty_and_Knowledge_axes
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on hallucinations in LLMs, which is a related topic to query understanding and ranking models. However, the primary focus on categorizing hallucinations along knowledge and certainty axes, and proposing a framework for mitigation, is somewhat tangential to the user's core research themes in Information Retrieval and Search technologies.

#### Abstract
> Hallucinations in LLMs present a critical barrier to their reliable usage.
Existing research usually categorizes hallucination by their external
properties rather than by the LLMs' underlying internal properties. This
external focus overlooks that hallucinations may require tailored mitigation
strategies based on their underlying mechanism. We propose a framework for
categorizing hallucinations along two axes: knowledge and certainty. Since
parametric knowledge and certainty may vary across models, our categorization
method involves a model-specific dataset construction process that
differentiates between those types of hallucinations. Along the knowledge axis,
we distinguish between hallucinations caused by a lack of knowledge and those
occurring despite the model having the knowledge of the correct response. To
validate our framework along the knowledge axis, we apply steering mitigation,
which relies on the existence of parametric knowledge to manipulate model
activations. This addresses the lack of existing methods to validate knowledge
categorization by showing a significant difference between the two
hallucination types. We further analyze the distinct knowledge and
hallucination patterns between models, showing that different hallucinations do
occur despite shared parametric knowledge. Turning to the certainty axis, we
identify a particularly concerning subset of hallucinations where models
hallucinate with certainty despite having the correct knowledge internally. We
introduce a new evaluation metric to measure the effectiveness of mitigation
methods on this subset, revealing that while some methods perform well on
average, they fail disproportionately on these critical cases. Our findings
highlight the importance of considering both knowledge and certainty in
hallucination analysis and call for targeted mitigation approaches that
consider the hallucination underlying factors.

### 23. Can LLMs Narrate Tabular Data? An Evaluation Framework for Natural Language Representations of Text-to-SQL System Outputs

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Jyotika Singh, Weiyi Sun, Amit Agarwal, Viji Krishnamurthy, Yassine Benajiba, Sujith Ravi, Dan Roth
- **URL**: <http://arxiv.org/abs/2510.23854v1>
- **Submitted**: 2025-10-27 20:52:19
- **Comment**: Accepted at EMNLP 2025
- **Topic Keywords**: query
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and Information Retrieval (IR), particularly in the context of text-to-SQL systems and natural language representations. However, it focuses more on the evaluation of large language models for generating natural language representations of tabular data, which is not a central match with your primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> In modern industry systems like multi-turn chat agents, Text-to-SQL
technology bridges natural language (NL) questions and database (DB) querying.
The conversion of tabular DB results into NL representations (NLRs) enables the
chat-based interaction. Currently, NLR generation is typically handled by large
language models (LLMs), but information loss or errors in presenting tabular
results in NL remains largely unexplored. This paper introduces a novel
evaluation method - Combo-Eval - for judgment of LLM-generated NLRs that
combines the benefits of multiple existing methods, optimizing evaluation
fidelity and achieving a significant reduction in LLM calls by 25-61%.
Accompanying our method is NLR-BIRD, the first dedicated dataset for NLR
benchmarking. Through human evaluations, we demonstrate the superior alignment
of Combo-Eval with human judgments, applicable across scenarios with and
without ground truth references.

### 24. CritiCal: Can Critique Help LLM Uncertainty or Confidence Calibration?

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Qing Zong, Jiayu Liu, Tianshi Zheng, Chunyang Li, Baixuan Xu, Haochen Shi, Weiqi Wang, Zhaowei Wang, Chunkit Chan, Yangqiu Song
- **URL**: <http://arxiv.org/abs/2510.24505v1>
- **Submitted**: 2025-10-28 15:16:06
- **Topic Keywords**: rag
- **Reason**: The paper explores confidence calibration in Large Language Models, which is related to query understanding and ranking models in Information Retrieval. However, the focus on natural language critiques and confidence calibration is more aligned with NLP and deep semantic understanding, but not directly related to the user's primary interests in IR and search technologies.

#### Abstract
> Accurate confidence calibration in Large Language Models (LLMs) is critical
for safe use in high-stakes domains, where clear verbalized confidence enhances
user trust. Traditional methods that mimic reference confidence expressions
often fail to capture the reasoning needed for accurate confidence assessment.
We propose natural language critiques as a solution, ideally suited for
confidence calibration, as precise gold confidence labels are hard to obtain
and often require multiple generations. This paper studies how natural language
critiques can enhance verbalized confidence, addressing: (1) What to critique:
uncertainty (question-focused) or confidence (answer-specific)? Analysis shows
confidence suits multiple-choice tasks, while uncertainty excels in open-ended
scenarios. (2) How to critique: self-critique or critique calibration training?
We propose Self-Critique, enabling LLMs to critique and optimize their
confidence beyond mere accuracy, and CritiCal, a novel Critique Calibration
training method that leverages natural language critiques to improve confidence
calibration, moving beyond direct numerical optimization. Experiments show that
CritiCal significantly outperforms Self-Critique and other competitive
baselines, even surpassing its teacher model, GPT-4o, in complex reasoning
tasks. CritiCal also shows robust generalization in out-of-distribution
settings, advancing LLM's reliability.

### 25. SynthWorlds: Controlled Parallel Worlds for Disentangling Reasoning and Knowledge in Language Models

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Ken Gu, Advait Bhat, Mike A Merrill, Robert West, Xin Liu, Daniel McDuff, Tim Althoff
- **URL**: <http://arxiv.org/abs/2510.24427v1>
- **Submitted**: 2025-10-28 13:47:23
- **Topic Keywords**: retrieval
- **Reason**: This paper focuses on evaluating the reasoning ability of language models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the paper's primary focus on language models and their reasoning ability, rather than search technologies or user behavior modeling, limits its relevance to your core research themes.

#### Abstract
> Evaluating the reasoning ability of language models (LMs) is complicated by
their extensive parametric world knowledge, where benchmark performance often
reflects factual recall rather than genuine reasoning. Existing datasets and
approaches (e.g., temporal filtering, paraphrasing, adversarial substitution)
cannot cleanly separate the two. We present SynthWorlds, a framework that
disentangles task reasoning complexity from factual knowledge. In SynthWorlds,
we construct parallel corpora representing two worlds with identical
interconnected structure: a real-mapped world, where models may exploit
parametric knowledge, and a synthetic-mapped world, where such knowledge is
meaningless. On top of these corpora, we design two mirrored tasks as case
studies: multi-hop question answering and page navigation, which maintain equal
reasoning difficulty across worlds. Experiments in parametric-only (e.g.,
closed-book QA) and knowledge-augmented (e.g., retrieval-augmented) LM settings
reveal a persistent knowledge advantage gap, defined as the performance boost
models gain from memorized parametric world knowledge. Knowledge acquisition
and integration mechanisms reduce but do not eliminate this gap, highlighting
opportunities for system improvements. Fully automatic and scalable,
SynthWorlds provides a controlled environment for evaluating LMs in ways that
were previously challenging, enabling precise and testable comparisons of
reasoning and memorization.

### 26. Comprehensive and Efficient Distillation for Lightweight Sentiment Analysis Models

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Guangyu Xie, Yice Zhang, Jianzhu Bao, Qianlong Wang, Yang Sun, Bingbing Wang, Ruifeng Xu
- **URL**: <http://arxiv.org/abs/2510.24425v1>
- **Submitted**: 2025-10-28 13:46:48
- **Comment**: Accepted by EMNLP 2025. 22 pages, 9 figures. The first two authors
  contribute equally
- **Topic Keywords**: rag
- **Reason**: The paper focuses on sentiment analysis, which is related to NLP, but it doesn't directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. While it involves model distillation and efficiency, it's more specific to sentiment analysis and doesn't seem to require deep semantic understanding or real-time relevance optimization.

#### Abstract
> Recent efforts leverage knowledge distillation techniques to develop
lightweight and practical sentiment analysis models. These methods are grounded
in human-written instructions and large-scale user texts. Despite the promising
results, two key challenges remain: (1) manually written instructions are
limited in diversity and quantity, making them insufficient to ensure
comprehensive coverage of distilled knowledge; (2) large-scale user texts incur
high computational cost, hindering the practicality of these methods. To this
end, we introduce COMPEFFDIST, a comprehensive and efficient distillation
framework for sentiment analysis. Our framework consists of two key modules:
attribute-based automatic instruction construction and difficulty-based data
filtering, which correspondingly tackle the aforementioned challenges. Applying
our method across multiple model series (Llama-3, Qwen-3, and Gemma-3), we
enable 3B student models to match the performance of 20x larger teacher models
on most tasks. In addition, our approach greatly outperforms baseline methods
in data efficiency, attaining the same performance level with only 10% of the
data.

### 27. Beyond Neural Incompatibility: Easing Cross-Scale Knowledge Transfer in Large Language Models through Latent Semantic Alignment

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Jian Gu, Aldeida Aleti, Chunyang Chen, Hongyu Zhang
- **URL**: <http://arxiv.org/abs/2510.24208v1>
- **Submitted**: 2025-10-28 09:25:40
- **Comment**: an early-stage version
- **Topic Keywords**: rag
- **Reason**: This paper focuses on Large Language Models and knowledge transfer, which is somewhat related to your interests in Information Retrieval and Natural Language Processing. However, the specific context of cross-scale knowledge transfer in LLMs and the use of latent semantic alignment as a solution does not directly align with your primary research themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large Language Models (LLMs) encode vast amounts of knowledge in their
massive parameters, which is accessible to locate, trace, and analyze. Despite
advances in neural interpretability, it is still not clear how to transfer
knowledge in a fine-grained manner, namely parametric knowledge transfer (PKT).
A key problem is enabling effective and efficient knowledge transfer across
LLMs of different scales, which is essential for achieving greater flexibility
and broader applicability in transferring knowledge between LLMs. Due to neural
incompatibility, referring to the architectural and parametric differences
between LLMs of varying scales, existing methods that directly reuse layer
parameters are severely limited. In this paper, we identify the semantic
alignment in latent space as the fundamental prerequisite for LLM cross-scale
knowledge transfer. Instead of directly using the layer parameters, our
approach takes activations as the medium of layer-wise knowledge transfer.
Leveraging the semantics in latent space, our approach is simple and
outperforms prior work, better aligning model behaviors across varying scales.
Evaluations on four benchmarks demonstrate the efficacy of our method. Further
analysis reveals the key factors easing cross-scale knowledge transfer and
provides insights into the nature of latent semantic alignment.

### 28. Exploring the Influence of Relevant Knowledge for Natural Language Generation Interpretability

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Iv√°n Mart√≠nez-Murillo, Paloma Moreda, Elena Lloret
- **URL**: <http://arxiv.org/abs/2510.24179v1>
- **Submitted**: 2025-10-28 08:34:01
- **Topic Keywords**: rag
- **Reason**: This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and data mining, but it focuses on Natural Language Generation (NLG) interpretability, which is not a central match to the user's primary focus on Information Retrieval (IR) and query understanding. The paper's emphasis on knowledge integration and external knowledge is relevant to the user's interests in deep semantic understanding, but the application to NLG is not directly related to the user's core research themes.

#### Abstract
> This paper explores the influence of external knowledge integration in
Natural Language Generation (NLG), focusing on a commonsense generation task.
We extend the CommonGen dataset by creating KITGI, a benchmark that pairs input
concept sets with retrieved semantic relations from ConceptNet and includes
manually annotated outputs. Using the T5-Large model, we compare sentence
generation under two conditions: with full external knowledge and with filtered
knowledge where highly relevant relations were deliberately removed. Our
interpretability benchmark follows a three-stage method: (1) identifying and
removing key knowledge, (2) regenerating sentences, and (3) manually assessing
outputs for commonsense plausibility and concept coverage. Results show that
sentences generated with full knowledge achieved 91\% correctness across both
criteria, while filtering reduced performance drastically to 6\%. These
findings demonstrate that relevant external knowledge is critical for
maintaining both coherence and concept coverage in NLG. This work highlights
the importance of designing interpretable, knowledge-enhanced NLG systems and
calls for evaluation frameworks that capture the underlying reasoning beyond
surface-level metrics.

### 29. AfriMTEB and AfriE5: Benchmarking and Adapting Text Embedding Models for African Languages

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Kosei Uemura, Miaoran Zhang, David Ifeoluwa Adelani
- **URL**: <http://arxiv.org/abs/2510.23896v1>
- **Submitted**: 2025-10-27 22:06:43
- **Topic Keywords**: retrieval
- **Reason**: The paper focuses on text embeddings for African languages, which is somewhat related to the user's interests in NLP and IR. However, the specific context of African languages and the tasks mentioned (e.g., hate speech detection, intent detection) are not directly aligned with the user's core research themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> Text embeddings are an essential building component of several NLP tasks such
as retrieval-augmented generation which is crucial for preventing
hallucinations in LLMs. Despite the recent release of massively multilingual
MTEB (MMTEB), African languages remain underrepresented, with existing tasks
often repurposed from translation benchmarks such as FLORES clustering or
SIB-200. In this paper, we introduce AfriMTEB -- a regional expansion of MMTEB
covering 59 languages, 14 tasks, and 38 datasets, including six newly added
datasets. Unlike many MMTEB datasets that include fewer than five languages,
the new additions span 14 to 56 African languages and introduce entirely new
tasks, such as hate speech detection, intent detection, and emotion
classification, which were not previously covered. Complementing this, we
present AfriE5, an adaptation of the instruction-tuned mE5 model to African
languages through cross-lingual contrastive distillation. Our evaluation shows
that AfriE5 achieves state-of-the-art performance, outperforming strong
baselines such as Gemini-Embeddings and mE5.

### 30. Open Korean Historical Corpus: A Millennia-Scale Diachronic Collection of Public Domain Texts

- **LLM Score**: 3
- **Keyword Score**: 3
- **Authors**: Seyoung Song, Nawon Kim, Songeun Chae, Kiwoong Park, Jiho Jin, Haneul Yoo, Kyunghyun Cho, Alice Oh
- **URL**: <http://arxiv.org/abs/2510.24541v1>
- **Submitted**: 2025-10-28 15:43:26
- **Comment**: Dataset and code available at https://github.com/seyoungsong/OKHC
- **Topic Keywords**: rag, korea
- **Reason**: This paper is loosely relevant to your research interests as it deals with Natural Language Processing (NLP) and a historical corpus, but it does not directly relate to your core areas of focus in Information Retrieval, query understanding, ranking models, or user behavior modeling.

#### Abstract
> The history of the Korean language is characterized by a discrepancy between
its spoken and written forms and a pivotal shift from Chinese characters to the
Hangul alphabet. However, this linguistic evolution has remained largely
unexplored in NLP due to a lack of accessible historical corpora. To address
this gap, we introduce the Open Korean Historical Corpus, a large-scale, openly
licensed dataset spanning 1,300 years and 6 languages, as well as
under-represented writing systems like Korean-style Sinitic (Idu) and
Hanja-Hangul mixed script. This corpus contains 18 million documents and 5
billion tokens from 19 sources, ranging from the 7th century to 2025. We
leverage this resource to quantitatively analyze major linguistic shifts: (1)
Idu usage peaked in the 1860s before declining sharply; (2) the transition from
Hanja to Hangul was a rapid transformation starting around 1890; and (3) North
Korea's lexical divergence causes modern tokenizers to produce up to 51 times
higher out-of-vocabulary rates. This work provides a foundational resource for
quantitative diachronic analysis by capturing the history of the Korean
language. Moreover, it can serve as a pre-training corpus for large language
models, potentially improving their understanding of Sino-Korean vocabulary in
modern Hangul as well as archaic writing systems.

### 31. LongWeave: A Long-Form Generation Benchmark Bridging Real-World Relevance and Verifiability

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Zikai Xiao, Fei Huang, Jianhong Tu, Jianhui Wei, Wen Ma, Yuxuan Zhou, Jian Wu, Bowen Yu, Zuozhu Liu, Junyang Lin
- **URL**: <http://arxiv.org/abs/2510.24345v1>
- **Submitted**: 2025-10-28 12:11:12
- **Comment**: EMNLP Findings 2025
- **Topic Keywords**: queries, relevance
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing, as it focuses on long-form generation benchmarks for Large Language Models. Although it touches on real-world relevance, it does not address query understanding, ranking models, or user behavior modeling, which are your primary areas of interest.

#### Abstract
> Generating long, informative, and factual outputs remains a major challenge
for Large Language Models (LLMs). Existing benchmarks for long-form generation
typically assess real-world queries with hard-to-verify metrics or use
synthetic setups that ease evaluation but overlook real-world intricacies. In
this paper, we introduce \textbf{LongWeave}, which balances real-world and
verifiable assessment with Constraint-Verifier Evaluation (CoV-Eval). CoV-Eval
constructs tasks by first defining verifiable targets within real-world
scenarios, then systematically generating corresponding queries, textual
materials, and constraints based on these targets. This ensures that tasks are
both realistic and objectively assessable, enabling rigorous assessment of
model capabilities in meeting complex real-world constraints. LongWeave
supports customizable input/output lengths (up to 64K/8K tokens) across seven
distinct tasks. Evaluation on 23 LLMs shows that even state-of-the-art models
encounter significant challenges in long-form generation as real-world
complexity and output length increase.

### 32. From Memorization to Reasoning in the Spectrum of Loss Curvature

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Jack Merullo, Srihita Vatsavaya, Lucius Bushnaq, Owen Lewis
- **URL**: <http://arxiv.org/abs/2510.24256v1>
- **Submitted**: 2025-10-28 10:09:35
- **Topic Keywords**: ctr, retrieval
- **Reason**: This paper focuses on understanding memorization in transformer models and its disentanglement in loss landscape curvature. While it touches on neural networks and their applications, it doesn't directly relate to information retrieval, search technologies, or user behavior modeling, which are core areas of your research interests.

#### Abstract
> We characterize how memorization is represented in transformer models and
show that it can be disentangled in the weights of both language models (LMs)
and vision transformers (ViTs) using a decomposition based on the loss
landscape curvature. This insight is based on prior theoretical and empirical
work showing that the curvature for memorized training points is much sharper
than non memorized, meaning ordering weight components from high to low
curvature can reveal a distinction without explicit labels. This motivates a
weight editing procedure that suppresses far more recitation of untargeted
memorized data more effectively than a recent unlearning method
(BalancedSubnet), while maintaining lower perplexity. Since the basis of
curvature has a natural interpretation for shared structure in model weights,
we analyze the editing procedure extensively on its effect on downstream tasks
in LMs, and find that fact retrieval and arithmetic are specifically and
consistently negatively affected, even though open book fact retrieval and
general logical reasoning is conserved. We posit these tasks rely heavily on
specialized directions in weight space rather than general purpose mechanisms,
regardless of whether those individual datapoints are memorized. We support
this by showing a correspondence between task data's activation strength with
low curvature components that we edit out, and the drop in task performance
after the edit. Our work enhances the understanding of memorization in neural
networks with practical applications towards removing it, and provides evidence
for idiosyncratic, narrowly-used structures involved in solving tasks like math
and fact retrieval.

### 33. Squrve: A Unified and Modular Framework for Complex Real-World Text-to-SQL Tasks

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Yihan Wang, Peiyu Liu, Runyu Chen, Jiaxing Pu, Wei Xu
- **URL**: <http://arxiv.org/abs/2510.24102v1>
- **Submitted**: 2025-10-28 06:16:38
- **Topic Keywords**: queries, search
- **Reason**: This paper appears to be unrelated to your primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves text processing, its focus on Text-to-SQL tasks and real-world deployment does not align with your core themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> Text-to-SQL technology has evolved rapidly, with diverse academic methods
achieving impressive results. However, deploying these techniques in real-world
systems remains challenging due to limited integration tools. Despite these
advances, we introduce Squrve, a unified, modular, and extensive Text-to-SQL
framework designed to bring together research advances and real-world
applications. Squrve first establishes a universal execution paradigm that
standardizes invocation interfaces, then proposes a multi-actor collaboration
mechanism based on seven abstracted effective atomic actor components.
Experiments on widely adopted benchmarks demonstrate that the collaborative
workflows consistently outperform the original individual methods, thereby
opening up a new effective avenue for tackling complex real-world queries. The
codes are available at https://github.com/Satissss/Squrve.

### 34. emg2speech: synthesizing speech from electromyography using self-supervised speech models

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Harshavardhana T. Gowda, Lee M. Miller
- **URL**: <http://arxiv.org/abs/2510.23969v1>
- **Submitted**: 2025-10-28 00:50:15
- **Topic Keywords**: rag, ctr
- **Reason**: This paper focuses on speech synthesis from electromyography signals, which is unrelated to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some form of signal processing, the application and methodology are distinct from the user's areas of interest.

#### Abstract
> We present a neuromuscular speech interface that translates electromyographic
(EMG) signals collected from orofacial muscles during speech articulation
directly into audio. We show that self-supervised speech (SS) representations
exhibit a strong linear relationship with the electrical power of muscle action
potentials: SS features can be linearly mapped to EMG power with a correlation
of $r = 0.85$. Moreover, EMG power vectors corresponding to different
articulatory gestures form structured and separable clusters in feature space.
This relationship: $\text{SS features}$ $\xrightarrow{\texttt{linear mapping}}$
$\text{EMG power}$ $\xrightarrow{\texttt{gesture-specific clustering}}$
$\text{articulatory movements}$, highlights that SS models implicitly encode
articulatory mechanisms. Leveraging this property, we directly map EMG signals
to SS feature space and synthesize speech, enabling end-to-end EMG-to-speech
generation without explicit articulatory models and vocoder training.

### 35. Latent Chain-of-Thought for Visual Reasoning

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Guohao Sun, Hang Hua, Jian Wang, Jiebo Luo, Sohail Dianat, Majid Rabbani, Raghuveer Rao, Zhiqiang Tao
- **URL**: <http://arxiv.org/abs/2510.23925v1>
- **Submitted**: 2025-10-27 23:10:06
- **Comment**: NeurIPS 2025
- **Topic Keywords**: rag, rank, search
- **Reason**: This paper focuses on visual reasoning and Large Vision-Language Models, which is not directly related to the user's core research themes in Information Retrieval and Search technologies. While it involves some aspects of deep semantic understanding, the primary focus is on visual reasoning rather than text-based search or query understanding.

#### Abstract
> Chain-of-thought (CoT) reasoning is critical for improving the
interpretability and reliability of Large Vision-Language Models (LVLMs).
However, existing training algorithms such as SFT, PPO, and GRPO may not
generalize well across unseen reasoning tasks and heavily rely on a biased
reward model. To address this challenge, we reformulate reasoning in LVLMs as
posterior inference and propose a scalable training algorithm based on
amortized variational inference. By leveraging diversity-seeking reinforcement
learning algorithms, we introduce a novel sparse reward function for
token-level learning signals that encourage diverse, high-likelihood latent
CoT, overcoming deterministic sampling limitations and avoiding reward hacking.
Additionally, we implement a Bayesian inference-scaling strategy that replaces
costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank
optimal rationales and answers. We empirically demonstrate that the proposed
method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in
terms of effectiveness, generalization, and interpretability.

### 36. Can LLMs Write Faithfully? An Agent-Based Evaluation of LLM-generated Islamic Content

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Abdullah Mushtaq, Rafay Naeem, Ezieddin Elmahjub, Ibrahim Ghaznavi, Shawqi Al-Maliki, Mohamed Abdallah, Ala Al-Fuqaha, Junaid Qadir
- **URL**: <http://arxiv.org/abs/2510.24438v1>
- **Submitted**: 2025-10-28 14:05:55
- **Comment**: Accepted at 39th Conference on Neural Information Processing Systems
  (NeurIPS 2025) Workshop: 5th Muslims in Machine Learning (MusIML) Workshop
- **Topic Keywords**: pairwise
- **Reason**: This paper focuses on evaluating the performance of Large Language Models (LLMs) in generating Islamic content, which is not directly related to your core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on aspects of NLP, the context and application are specific to Islamic content and faith-sensitive writing, making it less relevant to your interests.

#### Abstract
> Large language models are increasingly used for Islamic guidance, but risk
misquoting texts, misapplying jurisprudence, or producing culturally
inconsistent responses. We pilot an evaluation of GPT-4o, Ansari AI, and Fanar
on prompts from authentic Islamic blogs. Our dual-agent framework uses a
quantitative agent for citation verification and six-dimensional scoring (e.g.,
Structure, Islamic Consistency, Citations) and a qualitative agent for
five-dimensional side-by-side comparison (e.g., Tone, Depth, Originality).
GPT-4o scored highest in Islamic Accuracy (3.93) and Citation (3.38), Ansari AI
followed (3.68, 3.32), and Fanar lagged (2.76, 1.82). Despite relatively strong
performance, models still fall short in reliably producing accurate Islamic
content and citations -- a paramount requirement in faith-sensitive writing.
GPT-4o had the highest mean quantitative score (3.90/5), while Ansari AI led
qualitative pairwise wins (116/200). Fanar, though trailing, introduces
innovations for Islamic and Arabic contexts. This study underscores the need
for community-driven benchmarks centering Muslim perspectives, offering an
early step toward more reliable AI in Islamic knowledge and other high-stakes
domains such as medicine, law, and journalism.

### 37. Text Simplification with Sentence Embeddings

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Matthew Shardlow
- **URL**: <http://arxiv.org/abs/2510.24365v1>
- **Submitted**: 2025-10-28 12:41:10
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on text simplification using sentence embeddings, which is a topic related to Natural Language Processing (NLP). However, it does not directly align with the user's primary research interests in Information Retrieval (IR), query understanding, ranking models, or user behavior modeling.

#### Abstract
> Sentence embeddings can be decoded to give approximations of the original
texts used to create them. We explore this effect in the context of text
simplification, demonstrating that reconstructed text embeddings preserve
complexity levels. We experiment with a small feed forward neural network to
effectively learn a transformation between sentence embeddings representing
high-complexity and low-complexity texts. We provide comparison to a Seq2Seq
and LLM-based approach, showing encouraging results in our much smaller
learning setting. Finally, we demonstrate the applicability of our
transformation to an unseen simplification dataset (MedEASI), as well as
datasets from languages outside the training data (ES,DE). We conclude that
learning transformations in sentence embedding space is a promising direction
for future research and has potential to unlock the ability to develop small,
but powerful models for text simplification and other natural language
generation tasks.

### 38. Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Mena Attia, Aashiq Muhamed, Mai Alkhamissi, Thamar Solorio, Mona Diab
- **URL**: <http://arxiv.org/abs/2510.23828v1>
- **Submitted**: 2025-10-27 20:13:32
- **Topic Keywords**: rag, search
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves large language models and figurative language, its focus on cultural processing and pragmatic use is more aligned with NLP applications in linguistics and cultural studies, which is not a primary area of focus for you.

#### Abstract
> We present a comprehensive evaluation of the ability of large language models
(LLMs) to process culturally grounded language, specifically to understand and
pragmatically use figurative expressions that encode local knowledge and
cultural nuance. Using figurative language as a proxy for cultural nuance and
local knowledge, we design evaluation tasks for contextual understanding,
pragmatic use, and connotation interpretation in Arabic and English. We
evaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms,
multidialectal Arabic proverbs, and English proverbs. Our results show a
consistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower
than for English proverbs, and performance for Egyptian idioms is 10.28% lower
than for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07%
relative to understanding, though providing contextual idiomatic sentences
improves accuracy by 10.66%. Models also struggle with connotative meaning,
reaching at most 85.58% agreement with human annotators on idioms with 100%
inter-annotator agreement. These findings demonstrate that figurative language
serves as an effective diagnostic for cultural reasoning: while LLMs can often
interpret figurative meaning, they face challenges in using it appropriately.
To support future research, we release Kinayat, the first dataset of Egyptian
Arabic idioms designed for both figurative understanding and pragmatic use
evaluation.

### 39. Relative Scaling Laws for LLMs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: William Held, David Hall, Percy Liang, Diyi Yang
- **URL**: <http://arxiv.org/abs/2510.24626v1>
- **Submitted**: 2025-10-28 16:55:22
- **Topic Keywords**: rag
- **Reason**: This paper focuses on scaling laws for Large Language Models (LLMs), which is a topic in Natural Language Processing (NLP). While it touches on the idea of performance disparities, it does not directly relate to query understanding, ranking models, or user behavior modeling in Information Retrieval (IR), which are core areas of your research interests.

#### Abstract
> Scaling laws describe how language models improve with additional data,
parameters, and compute. While widely used, they are typically measured on
aggregate test sets. Aggregate evaluations yield clean trends but average over
heterogeneous subpopulations, obscuring performance disparities. We introduce
relative scaling laws, which track how performance gaps between test
distributions evolve with scale rather than focusing solely on absolute error.
Using 255 decoder-only Transformers trained under matched-compute (IsoFLOP)
budgets from $10^{18}$--$10^{20}$ FLOPs on standard pretraining datasets, we
find diverse trajectories: academic domains on MMLU converge toward parity;
regional English dialects shift depending on population size; and clusters of
AI risk behaviours split, with capability- and influence-related risks
increasing during pretraining while adversarial risks do not. These results
show that although scaling improves overall performance, it is not a universal
equalizer. To support further study, we release all model checkpoints from this
work to enable practitioners to measure relative alongside traditional scaling
laws, in order to better prioritize robustness challenges in light of the
bitter lesson.

### 40. Long-Context Modeling with Dynamic Hierarchical Sparse Attention for On-Device LLMs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Siheng Xiong, Joe Zou, Faramarz Fekri, Yae Jee Cho
- **URL**: <http://arxiv.org/abs/2510.24606v1>
- **Submitted**: 2025-10-28 16:34:18
- **Comment**: Accepted to NeurIPS 2025 Workshop on Efficient Reasoning
- **Topic Keywords**: rag
- **Reason**: This paper focuses on improving the scalability of long-context LLMs through attention mechanisms, but it does not directly relate to information retrieval, query understanding, ranking models, or user behavior modeling, which are core areas of your research interests.

#### Abstract
> The quadratic cost of attention hinders the scalability of long-context LLMs,
especially in resource-constrained settings. Existing static sparse methods
such as sliding windows or global tokens utilizes the sparsity of attention to
reduce the cost of attention, but poorly adapts to the content-dependent
variations in attention due to their staticity. While previous work has
proposed several dynamic approaches to improve flexibility, they still depend
on predefined templates or heuristic mechanisms. Such strategies reduce
generality and prune tokens that remain contextually important, limiting their
accuracy across diverse tasks. To tackle these bottlenecks of existing methods
for long-context modeling, we introduce Dynamic Hierarchical Sparse Attention
(DHSA), a data-driven framework that dynamically predicts attention sparsity
online without retraining. Our proposed DHSA adaptively segments sequences into
variable-length chunks, then computes chunk representations by aggregating the
token embeddings within each chunk. To avoid the bias introduced by varying
chunk lengths, we apply length-normalized aggregation that scales the averaged
embeddings by the square root of the chunk size. Finally, DHSA upsamples the
chunk-level similarity scores to token level similarities to calculate
importance scores that determine which token-level interactions should be
preserved. Our experiments on Gemma2 with Needle-in-a-Haystack Test and
LongBench show that DHSA matches dense attention in accuracy, while reducing
prefill latency by 20-60% and peak memory usage by 35%. Compared to other
representative baselines such as block sparse attention, DHSA achieves
consistently higher accuracy (6-18% relative gains) with comparable or lower
cost, offering an efficient and adaptable solution for long-context on-device
LLMs.

### 41. ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Guoxin Chen, Jing Wu, Xinjie Chen, Wayne Xin Zhao, Ruihua Song, Chengxi Li, Kai Fan, Dayiheng Liu, Minpeng Liao
- **URL**: <http://arxiv.org/abs/2510.24592v1>
- **Submitted**: 2025-10-28 16:22:54
- **Comment**: Ongoing Work
- **Topic Keywords**: rag
- **Reason**: This paper focuses on autoformalization and large language models, which is outside the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves semantic understanding, the context is formal mathematical reasoning and not directly related to the user's areas of focus.

#### Abstract
> Autoformalization, which translates natural language mathematics into
machine-verifiable formal statements, is critical for using formal mathematical
reasoning to solve math problems stated in natural language. While Large
Language Models can generate syntactically correct formal statements, they
often fail to preserve the original problem's semantic intent. This limitation
arises from the LLM approaches' treating autoformalization as a simplistic
translation task which lacks mechanisms for self-reflection and iterative
refinement that human experts naturally employ. To address these issues, we
propose ReForm, a Reflective Autoformalization method that tightly integrates
semantic consistency evaluation into the autoformalization process. This
enables the model to iteratively generate formal statements, assess its
semantic fidelity, and self-correct identified errors through progressive
refinement. To effectively train this reflective model, we introduce
Prospective Bounded Sequence Optimization (PBSO), which employs different
rewards at different sequence positions to ensure that the model develops both
accurate autoformalization and correct semantic validations, preventing
superficial critiques that would undermine the purpose of reflection. Extensive
experiments across four autoformalization benchmarks demonstrate that ReForm
achieves an average improvement of 17.2 percentage points over the strongest
baselines. To further ensure evaluation reliability, we introduce
ConsistencyCheck, a benchmark of 859 expert-annotated items that not only
validates LLMs as judges but also reveals that autoformalization is inherently
difficult: even human experts produce semantic errors in up to 38.5% of cases.

### 42. Automatically Benchmarking LLM Code Agents through Agent-Driven Annotation and Evaluation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Lingyue Fu, Bolun Zhang, Hao Guan, Yaoming Zhu, Lin Qiu, Weiwen Liu, Xuezhi Cao, Xunliang Cai, Weinan Zhang, Yong Yu
- **URL**: <http://arxiv.org/abs/2510.24358v1>
- **Submitted**: 2025-10-28 12:26:45
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to your core research themes in Information Retrieval, Search technologies, or Natural Language Processing. While it involves large language models, the focus is on code agents and software development, which is not a primary area of interest for you.

#### Abstract
> Recent advances in code agents have enabled automated software development at
the project level, supported by large language models (LLMs) and widely adopted
tools. However, existing benchmarks for code agent evaluation face two major
limitations: high annotation cost and expertise requirements, and rigid
evaluation metrics that rely primarily on unit tests. To address these
challenges, we propose an agent-driven benchmark construction pipeline that
leverages human supervision to efficiently generate diverse and challenging
project-level tasks. Based on this approach, we introduce PRDBench, a novel
benchmark comprising 50 real-world Python projects across 20 domains, each with
structured Product Requirement Document (PRD) requirements, comprehensive
evaluation criteria, and reference implementations. PRDBench features rich data
sources, high task complexity, and flexible metrics. We further employ an
Agent-as-a-Judge paradigm to score agent outputs, enabling the evaluation of
various test types beyond unit tests. Extensive experiments on PRDBench
demonstrate its effectiveness in assessing the capabilities of both code agents
and evaluation agents, providing a scalable and robust framework for annotation
and evaluation.

### 43. ViPER: Empowering the Self-Evolution of Visual Perception Abilities in Vision-Language Model

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Juntian Zhang, Song Jin, Chuanqi Cheng, Yuhan Liu, Yankai Lin, Xun Zhang, Yufei Zhang, Fei Jiang, Guojun Yin, Wei Lin, Rui Yan
- **URL**: <http://arxiv.org/abs/2510.24285v1>
- **Submitted**: 2025-10-28 10:42:57
- **Topic Keywords**: rag
- **Reason**: This paper focuses on visual perception abilities in Vision-Language Models, which is somewhat related to information retrieval, but it does not directly address query understanding, ranking models, or user behavior modeling. The paper's emphasis on visual perception and generation is more aligned with computer vision and NLP, but it does not seem to be a central match for your research interests.

#### Abstract
> The limited capacity for fine-grained visual perception presents a critical
bottleneck for Vision-Language Models (VLMs) in real-world applications.
Addressing this is challenging due to the scarcity of high-quality data and the
limitations of existing methods: supervised fine-tuning (SFT) often compromises
general capabilities, while reinforcement fine-tuning (RFT) prioritizes textual
reasoning over visual perception. To bridge this gap, we propose a novel
two-stage task that structures visual perception learning as a coarse-to-fine
progressive process. Based on this task formulation, we develop ViPER, a
self-bootstrapping framework specifically designed to enable iterative
evolution through self-critiquing and self-prediction. By synergistically
integrating image-level and instance-level reconstruction with a two-stage
reinforcement learning strategy, ViPER establishes a closed-loop training
paradigm, where internally synthesized data directly fuel the enhancement of
perceptual ability. Applied to the Qwen2.5-VL family, ViPER produces the
Qwen-Viper series. With an average gain of 1.7% on seven comprehensive
benchmarks spanning various tasks and up to 6.0% on fine-grained perception,
Qwen-Viper consistently demonstrates superior performance across different
vision-language scenarios while maintaining generalizability. Beyond enabling
self-improvement in perceptual capabilities, ViPER provides concrete evidence
for the reciprocal relationship between generation and understanding, a
breakthrough to developing more autonomous and capable VLMs.

### 44. Abjad AI at NADI 2025: CATT-Whisper: Multimodal Diacritic Restoration Using Text and Speech Representations

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ahmad Ghannam, Naif Alharthi, Faris Alasmary, Kholood Al Tabash, Shouq Sadah, Lahouari Ghouti
- **URL**: <http://arxiv.org/abs/2510.24247v1>
- **Submitted**: 2025-10-28 09:58:18
- **Topic Keywords**: rag
- **Reason**: This paper focuses on Diacritic Restoration for Arabic dialectal sentences using a multimodal approach, which is not directly related to Information Retrieval, Search technologies, or the user's core research themes. Although it involves Natural Language Processing, the specific application and techniques used are not aligned with the user's interests.

#### Abstract
> In this work, we tackle the Diacritic Restoration (DR) task for Arabic
dialectal sentences using a multimodal approach that combines both textual and
speech information. We propose a model that represents the text modality using
an encoder extracted from our own pre-trained model named CATT. The speech
component is handled by the encoder module of the OpenAI Whisper base model.
Our solution is designed following two integration strategies. The former
consists of fusing the speech tokens with the input at an early stage, where
the 1500 frames of the audio segment are averaged over 10 consecutive frames,
resulting in 150 speech tokens. To ensure embedding compatibility, these
averaged tokens are processed through a linear projection layer prior to
merging them with the text tokens. Contextual encoding is guaranteed by the
CATT encoder module. The latter strategy relies on cross-attention, where text
and speech embeddings are fused. The cross-attention output is then fed to the
CATT classification head for token-level diacritic prediction. To further
improve model robustness, we randomly deactivate the speech input during
training, allowing the model to perform well with or without speech. Our
experiments show that the proposed approach achieves a word error rate (WER) of
0.25 and a character error rate (CER) of 0.9 on the development set. On the
test set, our model achieved WER and CER scores of 0.55 and 0.13, respectively.

### 45. Success and Cost Elicit Convention Formation for Efficient Communication

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Saujas Vaduguru, Yilun Hua, Yoav Artzi, Daniel Fried
- **URL**: <http://arxiv.org/abs/2510.24023v1>
- **Submitted**: 2025-10-28 03:06:07
- **Topic Keywords**: rag
- **Reason**: This paper appears to be primarily focused on multimodal communication and convention formation, which is not directly related to the user's core research themes in Information Retrieval and Search technologies. While it involves some form of 'understanding' and 'optimization', it is more aligned with NLP and human-computer interaction, rather than the user's specific interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Humans leverage shared conversational context to become increasingly
successful and efficient at communicating over time. One manifestation of this
is the formation of ad hoc linguistic conventions, which allow people to
coordinate on short, less costly utterances that are understood using shared
conversational context. We present a method to train large multimodal models to
form conventions, enabling efficient communication. Our approach uses simulated
reference games between models, and requires no additional human-produced data.
In repeated reference games involving photographs and tangram images, our
method enables models to communicate efficiently with people: reducing the
message length by up to 41% while increasing success by 15% over the course of
the interaction. Human listeners respond faster when interacting with our model
that forms conventions. We also show that training based on success or cost
alone is insufficient - both are necessary to elicit convention formation.

### 46. Leveraging LLMs for Early Alzheimer's Prediction

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Tananun Songdechakraiwut
- **URL**: <http://arxiv.org/abs/2510.23946v1>
- **Submitted**: 2025-10-27 23:59:03
- **Topic Keywords**: rag
- **Reason**: This paper appears to be unrelated to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. The focus on early Alzheimer's prediction and clinical prediction using LLMs is outside the user's areas of interest.

#### Abstract
> We present a connectome-informed LLM framework that encodes dynamic fMRI
connectivity as temporal sequences, applies robust normalization, and maps
these data into a representation suitable for a frozen pre-trained LLM for
clinical prediction. Applied to early Alzheimer's detection, our method
achieves sensitive prediction with error rates well below clinically recognized
margins, with implications for timely Alzheimer's intervention.

### 47. A Neural Model for Contextual Biasing Score Learning and Filtering

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Wanting Huang, Weiran Wang
- **URL**: <http://arxiv.org/abs/2510.23849v1>
- **Submitted**: 2025-10-27 20:41:52
- **Comment**: Accepted to IEEE ASRU 2025
- **Topic Keywords**: rag
- **Reason**: This paper focuses on improving automatic speech recognition (ASR) using contextual biasing, which is unrelated to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves neural models and attention-based biasing, the application domain is ASR, and the paper's objectives and methods do not align with the user's interests in query understanding, ranking models, or user behavior modeling.

#### Abstract
> Contextual biasing improves automatic speech recognition (ASR) by integrating
external knowledge, such as user-specific phrases or entities, during decoding.
In this work, we use an attention-based biasing decoder to produce scores for
candidate phrases based on acoustic information extracted by an ASR encoder,
which can be used to filter out unlikely phrases and to calculate bonus for
shallow-fusion biasing. We introduce a per-token discriminative objective that
encourages higher scores for ground-truth phrases while suppressing
distractors. Experiments on the Librispeech biasing benchmark show that our
method effectively filters out majority of the candidate phrases, and
significantly improves recognition accuracy under different biasing conditions
when the scores are used in shallow fusion biasing. Our approach is modular and
can be used with any ASR system, and the filtering mechanism can potentially
boost performance of other biasing methods.

### 48. How Pragmatics Shape Articulation: A Computational Case Study in STEM ASL Discourse

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Saki Imai, Lee Kezar, Laurel Aichler, Mert Inan, Erin Walker, Alicia Wooten, Lorna Quandt, Malihe Alikhani
- **URL**: <http://arxiv.org/abs/2510.23842v1>
- **Submitted**: 2025-10-27 20:29:46
- **Topic Keywords**: rag
- **Reason**: This paper focuses on sign language and its computational modeling, which is not directly related to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some aspects of modeling and analysis, the context and domain are quite different from your areas of expertise.

#### Abstract
> Most state-of-the-art sign language models are trained on interpreter or
isolated vocabulary data, which overlooks the variability that characterizes
natural dialogue. However, human communication dynamically adapts to contexts
and interlocutors through spatiotemporal changes and articulation style. This
specifically manifests itself in educational settings, where novel vocabularies
are used by teachers, and students. To address this gap, we collect a motion
capture dataset of American Sign Language (ASL) STEM (Science, Technology,
Engineering, and Mathematics) dialogue that enables quantitative comparison
between dyadic interactive signing, solo signed lecture, and interpreted
articles. Using continuous kinematic features, we disentangle dialogue-specific
entrainment from individual effort reduction and show spatiotemporal changes
across repeated mentions of STEM terms. On average, dialogue signs are
24.6%-44.6% shorter in duration than the isolated signs, and show significant
reductions absent in monologue contexts. Finally, we evaluate sign embedding
models on their ability to recognize STEM signs and approximate how entrained
the participants become over time. Our study bridges linguistic analysis and
computational modeling to understand how pragmatics shape sign articulation and
its representation in sign language technologies.

### 49. Evolving Diagnostic Agents in a Virtual Clinical Environment

- **LLM Score**: 0
- **Keyword Score**: 3
- **Authors**: Pengcheng Qiu, Chaoyi Wu, Junwei Liu, Qiaoyu Zheng, Yusheng Liao, Haowen Wang, Yun Yue, Qianrui Fan, Shuai Zhen, Jian Wang, Jinjie Gu, Yanfeng Wang, Ya Zhang, Weidi Xie
- **URL**: <http://arxiv.org/abs/2510.24654v1>
- **Submitted**: 2025-10-28 17:19:47
- **Topic Keywords**: ctr, recommend
- **Reason**: The paper focuses on medical diagnosis using reinforcement learning and large language models, which is unrelated to information retrieval, search technologies, ranking models, or user behavior modeling. It does not address query understanding, click models, or e-commerce search contexts.

#### Abstract
> In this paper, we present a framework for training large language models
(LLMs) as diagnostic agents with reinforcement learning, enabling them to
manage multi-turn diagnostic processes, adaptively select examinations, and
commit to final diagnoses. Unlike instruction-tuned models trained on static
case summaries, our method acquires diagnostic strategies through interactive
exploration and outcome-based feedback. Our contributions are fourfold: (i) We
present DiagGym, a diagnostics world model trained with electronic health
records that emits examination outcomes conditioned on patient history and
recommended examination, serving as a virtual clinical environment for
realistic diagnosis training and evaluation; (ii) We train DiagAgent via
end-to-end, multi-turn reinforcement learning to learn diagnostic policies that
optimize both information yield and diagnostic accuracy; (iii) We introduce
DiagBench, a diagnostic benchmark comprising 750 cases with physician-validated
examination recommendations and 99 cases annotated with 973 physician-written
rubrics on diagnosis process; (iv) we demonstrate superior performance across
diverse diagnostic settings. DiagAgent significantly outperforms 10
state-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two
prompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34%
higher diagnostic accuracy and 44.03% improvement in examination recommendation
hit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic
accuracy and 23.09% boost in examination recommendation F1 score. In
rubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by
7.1% in weighted rubric score. These findings indicate that learning policies
in interactive clinical environments confers dynamic and clinically meaningful
diagnostic management abilities unattainable through passive training alone.

### 50. Lookahead Tree-Based Rollouts for Enhanced Trajectory-Level Exploration in Reinforcement Learning with Verifiable Rewards

- **LLM Score**: 0
- **Keyword Score**: 2
- **Authors**: Shangyu Xing, Siyuan Wang, Chenyuan Yang, Xinyu Dai, Xiang Ren
- **URL**: <http://arxiv.org/abs/2510.24302v2>
- **Submitted**: 2025-10-28 11:12:02
- **Topic Keywords**: rag
- **Reason**: This paper is about Reinforcement Learning with Verifiable Rewards, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Reinforcement Learning with Verifiable Rewards (RLVR), particularly with
algorithms like Group Relative Policy Optimization (GRPO), has proven highly
effective in enhancing the reasoning capabilities of large language models.
However, a critical bottleneck in current pipelines lies in the limited
diversity of sampled trajectories during group rollouts. Homogeneous
trajectories and their associated rewards would diminish the return signals for
policy updates, thereby hindering effective policy learning. This lack of
diversity stems primarily from token-level stochastic sampling, where local
variations are likely to collapse into near-identical reasoning paths. To
address this limitation, we propose Lookahead Tree-Based Rollouts (LATR), a
novel rollout strategy designed to explicitly promotes trajectory-level
diversity by enforcing branching into different candidate tokens likely to
yield distinct continuations. Specifically, LATR iteratively operates in three
stages: (1) branching at high-uncertainty generation steps, (2) performing
lookahead simulation for each new branch, and (3) pruning branches that
exhibits prolonged similarity during simulation. Compared with stochastic
Sampling, LATR accelerates policy learning by 131% on average and improves
final pass@1 performance by 4.2% on both GRPO and Dynamic sAmpling Policy
Optimization (DAPO) algorithms across different reasoning tasks. Our code and
data are publicly available at https://github.com/starreeze/latr.

---

