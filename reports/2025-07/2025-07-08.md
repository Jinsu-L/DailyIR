# Daily Papers Report - 2025-07-08

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Harnessing Pairwise Ranking Prompting Through Sample-Efficient Ranking Distillation

- **LLM Score**: 7
- **Keyword Score**: 10
- **Authors**: Junru Wu, Le Yan, Zhen Qin, Honglei Zhuang, Paul Suganthan G. C., Tianqi Liu, Zhe Dong, Xuanhui Wang, Harrie Oosterhuis
- **URL**: <http://arxiv.org/abs/2507.04820v1>
- **Submitted**: 2025-07-07 09:38:43
- **Comment**: ReNeuIR 2025 (at SIGIR 2025) - 4th Workshop on Reaching Efficiency in
  Neural Information Retrieval, July 17, 2025, Padua, Italy
- **Topic Keywords**: ranking, pointwise, pairwise, rank
- **Reason**: The paper discusses Pairwise Ranking Prompting (PRP) and its application in document ranking, which is a relevant topic in Information Retrieval. The proposed distillation approach is also related to Learning to Rank, a key area of interest. However, the focus on large language models and pairwise ranking may not be directly applicable to the user's e-commerce domain experience.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Pairwise Ranking Distillation (PRD) for efficient document ranking
- **Aim**: To reduce the computational complexity of Pairwise Ranking Prompting (PRP) while retaining its performance
- **Rationale**: PRP achieves state-of-the-art performance but has quadratic computational complexity, making it impractical for real-world applications
- **Ground**: PRD distills a pointwise student ranker from pairwise teacher labels generated by PRP, resulting in an efficient student model
- **Experiment**: Evaluation on TREC-DL datasets using the BM25 retrieval model, comparing PRD with pointwise distillation and other baselines
- **Takeaway**: PRD achieves comparable performance to full pairs with only 2% of the pairs, demonstrating high sample-efficiency and significant saving in training wall-clock time

#### Abstract
> While Pairwise Ranking Prompting (PRP) with Large Language Models (LLMs) is
one of the most effective zero-shot document ranking methods, it has a
quadratic computational complexity with respect to the number of documents to
be ranked, as it requires an enumeration over all possible document pairs.
Consequently, the outstanding ranking performance of PRP has remained
unreachable for most real-world ranking applications.
  In this work, we propose to harness the effectiveness of PRP through pairwise
distillation. Specifically, we distill a pointwise student ranker from pairwise
teacher labels generated by PRP, resulting in an efficient student model that
retains the performance of PRP with substantially lower computational costs.
Furthermore, we find that the distillation process can be made
sample-efficient: with only 2% of pairs, we are able to obtain the same
performance as using all pairs for teacher labels. Thus, our novel approach
provides a solution to harness the ranking performance of PRP without incurring
high computational costs during both distillation and serving.

---

### 2. "This Suits You the Best": Query Focused Comparative Explainable Summarization

- **LLM Score**: 6
- **Keyword Score**: 12
- **Authors**: Arnav Attri, Anuj Attri, Pushpak Bhattacharyya, Suman Banerjee, Amey Patil, Muthusamy Chelliah, Nikesh Garera
- **URL**: <http://arxiv.org/abs/2507.04733v1>
- **Submitted**: 2025-07-07 07:58:15
- **Topic Keywords**: query, queries, relevance, rag, recommend
- **Reason**: The paper explores query-focused comparative summarization, which is related to information retrieval and search technologies. The use of large language models and query-specific explanations is also relevant to my interests in NLP and query understanding. However, the focus on product recommendations and opinion summarization is somewhat narrow and not directly aligned with my primary research themes.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Query-Focused Comparative Explainable Summarization (QF-CES)
- **Aim**: Generate comparative summaries of recommended products based on user queries
- **Rationale**: Personalized, privacy-preserving, recommendation engine-agnostic, and category-agnostic approach
- **Ground**: Introduce novel task QF-CES, dataset MS-Q2P, and evaluation benchmark CES-EVAL
- **Experiment**: Evaluate approach using open-source and proprietary LLMs, reporting average Spearman correlation of 0.74 with human judgments
- **Takeaway**: QF-CES enables query-based personalization, dynamic attribute generation, and accelerates inference by 40% compared to traditional methods

#### Abstract
> Product recommendations inherently involve comparisons, yet traditional
opinion summarization often fails to provide holistic comparative insights. We
propose the novel task of generating Query-Focused Comparative Explainable
Summaries (QF-CES) using Multi-Source Opinion Summarization (M-OS). To address
the lack of query-focused recommendation datasets, we introduce MS-Q2P,
comprising 7,500 queries mapped to 22,500 recommended products with metadata.
We leverage Large Language Models (LLMs) to generate tabular comparative
summaries with query-specific explanations. Our approach is personalized,
privacy-preserving, recommendation engine-agnostic, and category-agnostic. M-OS
as an intermediate step reduces inference latency approximately by 40% compared
to the direct input approach (DIA), which processes raw data directly. We
evaluate open-source and proprietary LLMs for generating and assessing QF-CES.
Extensive evaluations using QF-CES-PROMPT across 5 dimensions (clarity,
faithfulness, informativeness, format adherence, and query relevance) showed an
average Spearman correlation of 0.74 with human judgments, indicating its
potential for QF-CES evaluation.

---

### 3. VLM2Vec-V2: Advancing Multimodal Embedding for Videos, Images, and Visual Documents

- **LLM Score**: 6
- **Keyword Score**: 9
- **Authors**: Rui Meng, Ziyan Jiang, Ye Liu, Mingyi Su, Xinyi Yang, Yuepeng Fu, Can Qin, Zeyuan Chen, Ran Xu, Caiming Xiong, Yingbo Zhou, Wenhu Chen, Semih Yavuz
- **URL**: <http://arxiv.org/abs/2507.04590v1>
- **Submitted**: 2025-07-07 00:51:57
- **Comment**: Technical Report
- **Topic Keywords**: information retrieval, rag, retrieval, recommend, search
- **Reason**: The paper proposes a multimodal embedding model that supports text, image, video, and visual document inputs, which is relevant to information retrieval and search technologies. However, the focus is on visual forms and multimodal embedding, which is not directly aligned with the user's primary interest in query understanding, ranking models, and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Multimodal Embedding Models
- **Aim**: To develop a unified multimodal embedding model that can handle text, image, video, and visual document inputs
- **Rationale**: The need for a comprehensive framework that can generalize across diverse visual modalities and tasks
- **Ground**: MMEB-V2, a benchmark that extends the original MMEB framework with five new task types
- **Experiment**: Training VLM2Vec-V2 on a combination of datasets, including video-caption pairs, video QA pairs, and image-text datasets, and comparing it with several VLM embedding models
- **Takeaway**: VLM2Vec-V2 achieves the highest overall average score, outperforming multiple strong baselines across 78 datasets covering image, video, and visual document tasks

#### Abstract
> Multimodal embedding models have been crucial in enabling various downstream
tasks such as semantic similarity, information retrieval, and clustering over
different modalities. However, existing multimodal embeddings like VLM2Vec,
E5-V, GME are predominantly focused on natural images, with limited support for
other visual forms such as videos and visual documents. This restricts their
applicability in real-world scenarios, including AI agents, multi-modal search
and recommendation, and retrieval-augmented generation (RAG). To close this
gap, we propose VLM2Vec-V2, a unified framework for learning embeddings across
diverse visual forms. First, we introduce MMEB-V2, a comprehensive benchmark
that extends MMEB with five new task types: visual document retrieval, video
retrieval, temporal grounding, video classification and video question
answering - spanning text, image, video, and visual document inputs. Next, we
train VLM2Vec-V2, a general-purpose embedding model that supports text, image,
video, and visual document inputs. Extensive experiments show that VLM2Vec-V2
achieves strong performance not only on the newly introduced video and document
retrieval tasks, but also improves over prior baselines on the original image
benchmarks. Through extensive evaluation, our study offers insights into the
generalizability of various multimodal embedding models and highlights
effective strategies for unified embedding learning, laying the groundwork for
more scalable and adaptable representation learning in both research and
real-world settings.

---

### 4. Building Open-Retrieval Conversational Question Answering Systems by Generating Synthetic Data and Decontextualizing User Questions

- **LLM Score**: 6
- **Keyword Score**: 7
- **Authors**: Christos Vlachos, Nikolaos Stylianou, Alexandra Fiotaki, Spiros Methenitis, Elisavet Palogiannidi, Themos Stafylakis, Ion Androutsopoulos
- **URL**: <http://arxiv.org/abs/2507.04884v1>
- **Submitted**: 2025-07-07 11:16:44
- **Comment**: Accepted at SIGDIAL 2025
- **Topic Keywords**: retriever, rag, retrieval
- **Reason**: The paper explores open-retrieval conversational question answering systems, which is related to information retrieval and search technologies. However, the focus on decontextualizing user questions and generating synthetic data is not directly aligned with my interests in query understanding, ranking models, and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Generating High-Quality Synthetic Open-Retrieval Conversational Question Answering (OR-CONVQA) Dialogs and Annotations
- **Aim**: Propose a novel pipeline for generating high-quality synthetic OR-CONVQA dialogs and annotations from domain-specific documents
- **Rationale**: Account for dialog history and lack of domain-specific data and annotations in OR-CONVQA, and explore the effectiveness of proposition-based dialogs over sentence-based ones
- **Ground**: Retrieval-augmented generation (RAG) and query reformulation to ground large language models (LLMs) in retrieved documents and rewrite user questions
- **Experiment**: Compare the quality of synthetic dialogs generated using proposition-based and sentence-based approaches, and fine-tune lightweight models for query rewriting and retrieval
- **Takeaway**: The proposed pipeline generates high-quality synthetic OR-CONVQA dialogs and annotations, and demonstrates the superiority of proposition-based dialogs over sentence-based ones, with implications for developing more effective OR-CONVQA systems

#### Abstract
> We consider open-retrieval conversational question answering (OR-CONVQA), an
extension of question answering where system responses need to be (i) aware of
dialog history and (ii) grounded in documents (or document fragments) retrieved
per question. Domain-specific OR-CONVQA training datasets are crucial for
real-world applications, but hard to obtain. We propose a pipeline that
capitalizes on the abundance of plain text documents in organizations (e.g.,
product documentation) to automatically produce realistic OR-CONVQA dialogs
with annotations. Similarly to real-world humanannotated OR-CONVQA datasets, we
generate in-dialog question-answer pairs, self-contained (decontextualized,
e.g., no referring expressions) versions of user questions, and propositions
(sentences expressing prominent information from the documents) the system
responses are grounded in. We show how the synthetic dialogs can be used to
train efficient question rewriters that decontextualize user questions,
allowing existing dialog-unaware retrievers to be utilized. The retrieved
information and the decontextualized question are then passed on to an LLM that
generates the system's response.

---

### 5. Verified Language Processing with Hybrid Explainability: A Technical Report

- **LLM Score**: 6
- **Keyword Score**: 5
- **Authors**: Oliver Robert Fox, Giacomo Bergami, Graham Morgan
- **URL**: <http://arxiv.org/abs/2507.05017v1>
- **Submitted**: 2025-07-07 14:00:05
- **Topic Keywords**: information retrieval, retrieval
- **Reason**: The paper explores Natural Language Processing (NLP) techniques for improving information retrieval, specifically focusing on hybrid explainability and logical reasoning. While it does not directly address query understanding, ranking models, or user behavior modeling, it contributes to the broader field of NLP and IR. The relevance is somewhat related, but not a central match for the user's research interests.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: Machine Learning and Natural Language Processing
- **Aim**: To develop a novel pipeline for accurately determining similarity between full texts
- **Rationale**: Current state-of-the-art pipelines lack guaranteed explainability, failing to accurately distinguish between logical implication, indifference, and inconsistency
- **Ground**: Combining graphs and logic to produce First-Order Logic representations, making it machine- and human-readable through Montague Grammar
- **Experiment**: Comparing the proposed method with pre-trained language models for detecting sentence entailment
- **Takeaway**: The proposed approach outperforms state-of-the-art models in accurately capturing full text similarity, providing a step towards more transparent and reliable Information Retrieval from extensive textual data

#### Abstract
> The volume and diversity of digital information have led to a growing
reliance on Machine Learning techniques, such as Natural Language Processing,
for interpreting and accessing appropriate data. While vector and graph
embeddings represent data for similarity tasks, current state-of-the-art
pipelines lack guaranteed explainability, failing to determine similarity for
given full texts accurately. These considerations can also be applied to
classifiers exploiting generative language models with logical prompts, which
fail to correctly distinguish between logical implication, indifference, and
inconsistency, despite being explicitly trained to recognise the first two
classes. We present a novel pipeline designed for hybrid explainability to
address this. Our methodology combines graphs and logic to produce First-Order
Logic representations, creating machine- and human-readable representations
through Montague Grammar. Preliminary results indicate the effectiveness of
this approach in accurately capturing full text similarity. To the best of our
knowledge, this is the first approach to differentiate between implication,
inconsistency, and indifference for text classification tasks. To address the
limitations of existing approaches, we use three self-contained datasets
annotated for the former classification task to determine the suitability of
these approaches in capturing sentence structure equivalence, logical
connectives, and spatiotemporal reasoning. We also use these data to compare
the proposed method with language models pre-trained for detecting sentence
entailment. The results show that the proposed method outperforms
state-of-the-art models, indicating that natural language understanding cannot
be easily generalised by training over extensive document corpora. This work
offers a step toward more transparent and reliable Information Retrieval from
extensive textual data.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. LLMs as Architects and Critics for Multi-Source Opinion Summarization

- **LLM Score**: 6
- **Keyword Score**: 5
- **Authors**: Anuj Attri, Arnav Attri, Pushpak Bhattacharyya, Suman Banerjee, Amey Patil, Muthusamy Chelliah, Nikesh Garera
- **URL**: <http://arxiv.org/abs/2507.04751v1>
- **Submitted**: 2025-07-07 08:27:44
- **Topic Keywords**: relevance, rag
- **Reason**: The paper explores the application of Large Language Models (LLMs) in Multi-Source Opinion Summarization, which is related to Information Retrieval and Natural Language Processing. However, the focus on opinion summarization and product metadata is not directly aligned with the user's primary interest in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Multi-source Opinion Summarization (M-OS) extends beyond traditional opinion
summarization by incorporating additional sources of product metadata such as
descriptions, key features, specifications, and ratings, alongside reviews.
This integration results in comprehensive summaries that capture both
subjective opinions and objective product attributes essential for informed
decision-making. While Large Language Models (LLMs) have shown significant
success in various Natural Language Processing (NLP) tasks, their potential in
M-OS remains largely unexplored. Additionally, the lack of evaluation datasets
for this task has impeded further advancements. To bridge this gap, we
introduce M-OS-EVAL, a benchmark dataset for evaluating multi-source opinion
summaries across 7 key dimensions: fluency, coherence, relevance, faithfulness,
aspect coverage, sentiment consistency, specificity. Our results demonstrate
that M-OS significantly enhances user engagement, as evidenced by a user study
in which, on average, 87% of participants preferred M-OS over opinion
summaries. Our experiments demonstrate that factually enriched summaries
enhance user engagement. Notably, M-OS-PROMPTS exhibit stronger alignment with
human judgment, achieving an average Spearman correlation of \r{ho} = 0.74,
which surpasses the performance of previous methodologies.

### 7. Do We Really Need Specialization? Evaluating Generalist Text Embeddings for Zero-Shot Recommendation and Search

- **LLM Score**: 6
- **Keyword Score**: 2
- **Authors**: Matteo Attimonelli, Alessandro De Bellis, Claudio Pomo, Dietmar Jannach, Eugenio Di Sciascio, Tommaso Di Noia
- **URL**: <http://arxiv.org/abs/2507.05006v2>
- **Submitted**: 2025-07-07 13:41:52
- **Comment**: Accept as Short Paper at RecSys 2025
- **Topic Keywords**: recommend, search
- **Reason**: The paper explores the use of pre-trained language models for recommendation and search, which aligns with your interest in Information Retrieval and Search technologies. However, the focus on generalist text embeddings and zero-shot performance is somewhat tangential to your specific interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Pre-trained language models (PLMs) are widely used to derive semantic
representations from item metadata in recommendation and search. In sequential
recommendation, PLMs enhance ID-based embeddings through textual metadata,
while in product search, they align item characteristics with user intent.
Recent studies suggest task and domain-specific fine-tuning are needed to
improve representational power. This paper challenges this assumption, showing
that Generalist Text Embedding Models (GTEs), pre-trained on large-scale
corpora, can guarantee strong zero-shot performance without specialized
adaptation. Our experiments demonstrate that GTEs outperform traditional and
fine-tuned models in both sequential recommendation and product search. We
attribute this to a superior representational power, as they distribute
features more evenly across the embedding space. Finally, we show that
compressing embedding dimensions by focusing on the most informative directions
(e.g., via PCA) effectively reduces noise and improves the performance of
specialized models. To ensure reproducibility, we provide our repository at
https://split.to/gte4ps.

### 8. SimLab: A Platform for Simulation-based Evaluation of Conversational Information Access Systems

- **LLM Score**: 6
- **Keyword Score**: 2
- **Authors**: Nolwenn Bernard, Sharath Chandra Etagi Suresh, Krisztian Balog, ChengXiang Zhai
- **URL**: <http://arxiv.org/abs/2507.04888v1>
- **Submitted**: 2025-07-07 11:19:28
- **Topic Keywords**: recommend, search
- **Reason**: The paper introduces a platform for simulation-based evaluation of conversational information access systems, which is related to information retrieval and search technologies. However, the focus is on conversational systems and user simulation, which is not directly aligned with my primary research interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat related, but not a central match.

#### Abstract
> Research on interactive and conversational information access systems,
including search engines, recommender systems, and conversational assistants,
has been hindered by the difficulty in evaluating such systems with
reproducible experiments. User simulation provides a promising solution, but
there is a lack of infrastructure and tooling to support this kind of
evaluation. To facilitate simulation-based evaluation of conversational
information access systems, we introduce SimLab, the first cloud-based platform
to provide a centralized general solution for the community to benchmark both
conversational systems and user simulators in a controlled and reproducible
environment. We articulate requirements for such a platform and propose a
general infrastructure to address these requirements. We then present the
design and implementation of an initial version of SimLab and showcase its
features with an initial evaluation task of conversational movie
recommendation, which is made publicly available. Furthermore, we discuss the
sustainability of the platform and its future opportunities. This paper is a
call for the community to contribute to the platform to drive progress in the
field of conversational information access and user simulation.

### 9. XiYan-SQL: A Novel Multi-Generator Framework For Text-to-SQL

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Yifu Liu, Yin Zhu, Yingqi Gao, Zhiling Luo, Xiaoxia Li, Xiaorong Shi, Yuntao Hong, Jinyang Gao, Yu Li, Bolin Ding, Jingren Zhou
- **URL**: <http://arxiv.org/abs/2507.04701v1>
- **Submitted**: 2025-07-07 06:50:46
- **Topic Keywords**: query, queries, rag
- **Reason**: The paper focuses on Text-to-SQL, which is not directly related to Information Retrieval or Search technologies. Although it employs multi-task fine-tuning and ensemble approaches, the primary goal is to improve SQL generation, which is not a core aspect of the user's research interests. The paper's relevance is somewhat tangential, as it does not address query understanding, ranking models, or user behavior modeling.

#### Abstract
> To leverage the advantages of LLM in addressing challenges in the Text-to-SQL
task, we present XiYan-SQL, an innovative framework effectively generating and
utilizing multiple SQL candidates. It consists of three components: 1) a Schema
Filter module filtering and obtaining multiple relevant schemas; 2) a
multi-generator ensemble approach generating multiple highquality and diverse
SQL queries; 3) a selection model with a candidate reorganization strategy
implemented to obtain the optimal SQL query. Specifically, for the
multi-generator ensemble, we employ a multi-task fine-tuning strategy to
enhance the capabilities of SQL generation models for the intrinsic alignment
between SQL and text, and construct multiple generation models with distinct
generation styles by fine-tuning across different SQL formats. The experimental
results and comprehensive analysis demonstrate the effectiveness and robustness
of our framework. Overall, XiYan-SQL achieves a new SOTA performance of 75.63%
on the notable BIRD benchmark, surpassing all previous methods. It also attains
SOTA performance on the Spider test set with an accuracy of 89.65%.

### 10. In-Context Learning as an Effective Estimator of Functional Correctness of LLM-Generated Code

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Susmita Das, Madhusudan Ghosh, Priyanka Swami, Debasis Ganguly, Gul Calikli
- **URL**: <http://arxiv.org/abs/2507.05200v1>
- **Submitted**: 2025-07-07 17:01:17
- **Topic Keywords**: query, relevance, rank
- **Reason**: The paper discusses code generation and quality estimation, which is related to information retrieval and ranking models. However, the focus is on code generation and quality estimation, rather than query understanding or user behavior modeling, which are core aspects of your research interests.

#### Abstract
> When applying LLM-based code generation to software development projects that
follow a feature-driven or rapid application development approach, it becomes
necessary to estimate the functional correctness of the generated code in the
absence of test cases. Just as a user selects a relevant document from a ranked
list of retrieved ones, a software generation workflow requires a developer to
choose (and potentially refine) a generated solution from a ranked list of
alternative solutions, ordered by their posterior likelihoods. This implies
that estimating the quality of a ranked list -- akin to estimating "relevance"
for query performance prediction (QPP) in IR -- is also crucial for generative
software development, where quality is defined in terms of "functional
correctness". In this paper, we propose an in-context learning (ICL) based
approach for code quality estimation. Our findings demonstrate that providing
few-shot examples of functionally correct code from a training set enhances the
performance of existing QPP approaches as well as a zero-shot-based approach
for code quality estimation.

### 11. SIGIR 2025 -- LiveRAG Challenge Report

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: David Carmel, Simone Filice, Guy Horowitz, Yoelle Maarek, Oren Somekh, Ran Tavory, Mehdi Ghissassi, Edo Liberty, Roy Miara
- **URL**: <http://arxiv.org/abs/2507.04942v2>
- **Submitted**: 2025-07-07 12:38:53
- **Comment**: 9 pages, 5 tables
- **Topic Keywords**: rag, retrieval, rank, sigir
- **Reason**: The paper is somewhat related to the user's interests in Information Retrieval, particularly in the context of Retrieval-Augmented Generation (RAG) technologies. However, the focus on question-answering systems and language models (LLMs) is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is limited to the broader field of IR, but it does not specifically address the user's core research themes.

#### Abstract
> The LiveRAG Challenge at SIGIR 2025, held between March and May 2025,
provided a competitive platform for advancing Retrieval-Augmented Generation
(RAG) technologies. Participants from academia and industry were invited to
develop a RAG-based question-answering system using a fixed corpus
(Fineweb-10BT) and a common open-source LLM (Falcon3-10B-Instruct). The goal
was to facilitate challenging comparisons of retrieval and prompting
strategies. During the Live Challenge Day, 70 teams from 27 different countries
provided answers and supportive information to 500 unseen questions within a
strict two-hour time window. Evaluation was conducted in two stages: first an
automated LLM-as-a-judge approach was used to compute correctness and
faithfulness score, then a manual review of top ranked submissions was
conducted. The finalists were announced on June 12, 2025, with prizes awarded
during the LiveRAG Workshop at SIGIR 2025 in Padua, Italy.

### 12. Pre-Trained Policy Discriminators are General Reward Models

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Shihan Dou, Shichun Liu, Yuming Yang, Yicheng Zou, Yunhua Zhou, Shuhao Xing, Chenhao Huang, Qiming Ge, Demin Song, Haijun Lv, Songyang Gao, Chengqi Lv, Enyu Zhou, Honglin Guo, Zhiheng Xi, Wenwei Zhang, Qipeng Guo, Qi Zhang, Xipeng Qiu, Xuanjing Huang, Tao Gui, Kai Chen
- **URL**: <http://arxiv.org/abs/2507.05197v1>
- **Submitted**: 2025-07-07 16:56:31
- **Topic Keywords**: ranking, rag, rank
- **Reason**: The paper proposes a novel approach to reward modeling, which is somewhat related to my interests in Information Retrieval and Search technologies. However, the focus on reinforcement learning and policy discrimination is not directly aligned with my core research themes, such as query understanding, ranking models, and user behavior modeling. While the paper's results show promising performance, the topic is not a central match for my research interests.

#### Abstract
> We offer a novel perspective on reward modeling by formulating it as a policy
discriminator, which quantifies the difference between two policies to generate
a reward signal, guiding the training policy towards a target policy with
desired behaviors. Based on this conceptual insight, we propose a scalable
pre-training method named Policy Discriminative Learning (POLAR), which trains
a reward model (RM) to discern identical policies and discriminate different
ones. Unlike traditional reward modeling methods relying on absolute
preferences, POLAR captures the relative difference between one policy and an
arbitrary target policy, which is a scalable, high-level optimization objective
suitable for modeling generic ranking relationships. Leveraging the POLAR
pre-training paradigm, we present a series of RMs with parameter scales from
1.8B to 7B. Empirical results show that POLAR substantially outperforms
traditional non-pre-trained methods, significantly enhancing RM performance.
For instance, POLAR-7B could improve preference accuracy from 54.8% to 81.0% on
STEM tasks and from 57.9% to 85.5% on creative writing tasks compared to SOTA
baselines. POLAR also shows robust generalization capabilities in RLHF using
Reinforcement Fine-tuning (RFT), providing reliable reward signals and markedly
enhancing policy performance--improving LLaMa3.1-8B from an average of 47.36%
to 56.33% and Qwen2.5-32B from 64.49% to 70.47% on 20 benchmarks. Moreover,
scaling experiments reveal a clear power-law relationship between computation
and performance, supported by linear correlation coefficients approaching 0.99.
The impressive performance, strong generalization, and scaling properties
suggest that POLAR is a promising direction for developing general and strong
reward models.

### 13. FindRec: Stein-Guided Entropic Flow for Multi-Modal Sequential Recommendation

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Maolin Wang, Yutian Xiao, Binhao Wang, Sheng Zhang, Shanshan Ye, Wanyu Wang, Hongzhi Yin, Ruocheng Guo, Zenglin Xu
- **URL**: <http://arxiv.org/abs/2507.04651v1>
- **Submitted**: 2025-07-07 04:09:45
- **Comment**: Accepted by KDD 2025
- **Topic Keywords**: relevance, rag, recommend
- **Reason**: The paper proposes a novel framework for multi-modal sequential recommendation, which is somewhat related to information retrieval and search technologies. However, the focus on recommendation systems and multimodal sequential data processing is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Modern recommendation systems face significant challenges in processing
multimodal sequential data, particularly in temporal dynamics modeling and
information flow coordination. Traditional approaches struggle with
distribution discrepancies between heterogeneous features and noise
interference in multimodal signals. We propose \textbf{FindRec}~
(\textbf{F}lexible unified \textbf{in}formation \textbf{d}isentanglement for
multi-modal sequential \textbf{Rec}ommendation), introducing a novel
"information flow-control-output" paradigm. The framework features two key
innovations: (1) A Stein kernel-based Integrated Information Coordination
Module (IICM) that theoretically guarantees distribution consistency between
multimodal features and ID streams, and (2) A cross-modal expert routing
mechanism that adaptively filters and combines multimodal features based on
their contextual relevance. Our approach leverages multi-head subspace
decomposition for routing stability and RBF-Stein gradient for unbiased
distribution alignment, enhanced by linear-complexity Mamba layers for
efficient temporal modeling. Extensive experiments on three real-world datasets
demonstrate FindRec's superior performance over state-of-the-art baselines,
particularly in handling long sequences and noisy multimodal inputs. Our
framework achieves both improved recommendation accuracy and enhanced model
interpretability through its modular design. The implementation code is
available anonymously online for easy
reproducibility~\footnote{https://github.com/Applied-Machine-Learning-Lab/FindRec}.

### 14. Can Video LLMs Refuse to Answer? Alignment for Answerability in Video Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Eunseop Yoon, Hee Suk Yoon, Mark A. Hasegawa-Johnson, Chang D. Yoo
- **URL**: <http://arxiv.org/abs/2507.04976v1>
- **Submitted**: 2025-07-07 13:19:43
- **Comment**: ICLR 2025
- **Topic Keywords**: relevance, rag
- **Reason**: The paper explores Video Large Language Models (Video-LLMs) and their ability to refuse answering unfit questions. While it touches on multimodal alignment, it does not directly relate to query understanding, ranking models, or user behavior modeling in the context of information retrieval. The focus on video understanding and question relevance assessment is somewhat related to my interests, but the paper's scope is too narrow and specific to be considered highly relevant.

#### Abstract
> In the broader context of deep learning, Multimodal Large Language Models
have achieved significant breakthroughs by leveraging powerful Large Language
Models as a backbone to align different modalities into the language space. A
prime exemplification is the development of Video Large Language Models
(Video-LLMs). While numerous advancements have been proposed to enhance the
video understanding capabilities of these models, they are predominantly
trained on questions generated directly from video content. However, in
real-world scenarios, users often pose questions that extend beyond the
informational scope of the video, highlighting the need for Video-LLMs to
assess the relevance of the question. We demonstrate that even the
best-performing Video-LLMs fail to reject unfit questions-not necessarily due
to a lack of video understanding, but because they have not been trained to
identify and refuse such questions. To address this limitation, we propose
alignment for answerability, a framework that equips Video-LLMs with the
ability to evaluate the relevance of a question based on the input video and
appropriately decline to answer when the question exceeds the scope of the
video, as well as an evaluation framework with a comprehensive set of metrics
designed to measure model behavior before and after alignment. Furthermore, we
present a pipeline for creating a dataset specifically tailored for alignment
for answerability, leveraging existing video-description paired datasets.

### 15. $\textit{Grahak-Nyay:}$ Consumer Grievance Redressal through Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Shrey Ganatra, Swapnil Bhattacharyya, Harshvivek Kashid, Spandan Anaokar, Shruti Nair, Reshma Sekhar, Siddharth Manohar, Rahul Hemrajani, Pushpak Bhattacharyya
- **URL**: <http://arxiv.org/abs/2507.04854v1>
- **Submitted**: 2025-07-07 10:26:42
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper's focus on using Large Language Models and Retrieval-Augmented Generation for consumer grievance redressal is somewhat related to my interests in Information Retrieval and Natural Language Processing. However, the specific application to the legal domain and chatbot development is not directly aligned with my primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Access to consumer grievance redressal in India is often hindered by
procedural complexity, legal jargon, and jurisdictional challenges. To address
this, we present $\textbf{Grahak-Nyay}$ (Justice-to-Consumers), a chatbot that
streamlines the process using open-source Large Language Models (LLMs) and
Retrieval-Augmented Generation (RAG). Grahak-Nyay simplifies legal complexities
through a concise and up-to-date knowledge base. We introduce three novel
datasets: $\textit{GeneralQA}$ (general consumer law), $\textit{SectoralQA}$
(sector-specific knowledge) and $\textit{SyntheticQA}$ (for RAG evaluation),
along with $\textit{NyayChat}$, a dataset of 300 annotated chatbot
conversations. We also introduce $\textit{Judgments}$ data sourced from Indian
Consumer Courts to aid the chatbot in decision making and to enhance user
trust. We also propose $\textbf{HAB}$ metrics ($\textbf{Helpfulness, Accuracy,
Brevity}$) to evaluate chatbot performance. Legal domain experts validated
Grahak-Nyay's effectiveness. Code and datasets will be released.

### 16. Logit Reweighting for Topic-Focused Summarization

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Joschka Braun, B√°lint Mucs√°nyi, Seyed Ali Bahrainian
- **URL**: <http://arxiv.org/abs/2507.05235v1>
- **Submitted**: 2025-07-07 17:44:21
- **Comment**: 11 pages, 13 figures
- **Topic Keywords**: relevance
- **Reason**: The paper focuses on topic-focused summarization, which is related to information retrieval, but it does not directly address query understanding, ranking models, or user behavior modeling. The techniques proposed in the paper are primarily focused on natural language processing and text generation, which are tangential to the user's primary research interests.

#### Abstract
> Generating abstractive summaries that adhere to a specific topic remains a
significant challenge for language models. While standard approaches, such as
fine-tuning, are resource-intensive, simpler methods like prompt engineering
often struggle to maintain topical focus, particularly with smaller models. To
address this, we propose a lightweight method that enhances topical relevance
by directly reweighting the logits of topic-relevant tokens during generation.
We evaluate three such reweighting techniques: Constant Shift, which adds a
constant value to logits; Factor Scaling, which multiplies them by a factor;
and Threshold Selection, which selectively boosts logits that exceed a
probability threshold. Experiments on the NEWTS topical summarization dataset,
using both Gemma-2B and Llama-3-8B models, show that these techniques
effectively increase the use of topic-relevant vocabulary. Notably, the
Threshold Selection method successfully improves topical focus without
compromising summary quality-a trade-off often seen in other approaches. Our
findings demonstrate that directly reweighting logits is a practical and
resource-efficient alternative to fine-tuning, offering a promising pathway for
precisely controlling the thematic content of generated text.

### 17. An Evaluation of Large Language Models on Text Summarization Tasks Using Prompt Engineering Techniques

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Walid Mohamed Aly, Taysir Hassan A. Soliman, Amr Mohamed AbdelAziz
- **URL**: <http://arxiv.org/abs/2507.05123v1>
- **Submitted**: 2025-07-07 15:34:05
- **Comment**: This manuscript is an extended version of the work accepted for
  publication in the International Journal of Advanced Computer Science and
  Applications (IJACSA), Volume 16, Issue 6, June 2025
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on evaluating large language models for text summarization tasks, which is not directly related to my research interests in Information Retrieval and Search technologies. While it touches on natural language processing, the specific application and evaluation metrics are not aligned with my core research themes.

#### Abstract
> Large Language Models (LLMs) continue to advance natural language processing
with their ability to generate human-like text across a range of tasks. Despite
the remarkable success of LLMs in Natural Language Processing (NLP), their
performance in text summarization across various domains and datasets has not
been comprehensively evaluated. At the same time, the ability to summarize text
effectively without relying on extensive training data has become a crucial
bottleneck. To address these issues, we present a systematic evaluation of six
LLMs across four datasets: CNN/Daily Mail and NewsRoom (news), SAMSum (dialog),
and ArXiv (scientific). By leveraging prompt engineering techniques including
zero-shot and in-context learning, our study evaluates the performance using
the ROUGE and BERTScore metrics. In addition, a detailed analysis of inference
times is conducted to better understand the trade-off between summarization
quality and computational efficiency. For Long documents, introduce a
sentence-based chunking strategy that enables LLMs with shorter context windows
to summarize extended inputs in multiple stages. The findings reveal that while
LLMs perform competitively on news and dialog tasks, their performance on long
scientific documents improves significantly when aided by chunking strategies.
In addition, notable performance variations were observed based on model
parameters, dataset properties, and prompt design. These results offer
actionable insights into how different LLMs behave across task types,
contributing to ongoing research in efficient, instruction-based NLP systems.

### 18. Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen Visual Unicode Representations

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: A. Bochkov
- **URL**: <http://arxiv.org/abs/2507.04886v1>
- **Submitted**: 2025-07-07 11:17:32
- **Topic Keywords**: rag, search
- **Reason**: This paper explores the role of embeddings in large language models, challenging the dominant paradigm of trainable input embeddings. While it touches on the concept of semantic representation, it does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval and Search technologies.

#### Abstract
> Understanding the locus of semantic representation in large language models
(LLMs) is crucial for interpretability and architectural innovation. The
dominant paradigm posits that trainable input embeddings serve as foundational
"meaning vectors." This paper challenges that view. We construct Transformer
models where the embedding layer is entirely frozen, with vectors derived not
from data, but from the visual structure of Unicode glyphs. These non-semantic,
precomputed visual embeddings are fixed throughout training. Our method is
compatible with any tokenizer, including a novel Unicode-centric tokenizer we
introduce to ensure universal text coverage. Despite the absence of trainable,
semantically initialized embeddings, our models converge, generate coherent
text, and, critically, outperform architecturally identical models with
trainable embeddings on the MMLU reasoning benchmark. We attribute this to
"representational interference" in conventional models, where the embedding
layer is burdened with learning both structural and semantic features. Our
results indicate that high-level semantics are not inherent to input embeddings
but are an emergent property of the Transformer's compositional architecture
and data scale. This reframes the role of embeddings from meaning containers to
structural primitives. We release all code and models to foster further
research.

### 19. Dialogue-Based Multi-Dimensional Relationship Extraction from Novels

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Yuchen Yan, Hanjie Zhao, Senbin Zhu, Hongde Liu, Zhihong Zhang, Yuxiang Jia
- **URL**: <http://arxiv.org/abs/2507.04852v1>
- **Submitted**: 2025-07-07 10:20:16
- **Comment**: The paper has been accepted by NLPCC2025. 12 pages, 5 figures, 5
  tables
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on relation extraction in the novel domain, using Large Language Models and dialogue data, which is related to Natural Language Processing and data mining. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core aspects of Information Retrieval and Search technologies.

#### Abstract
> Relation extraction is a crucial task in natural language processing, with
broad applications in knowledge graph construction and literary analysis.
However, the complex context and implicit expressions in novel texts pose
significant challenges for automatic character relationship extraction. This
study focuses on relation extraction in the novel domain and proposes a method
based on Large Language Models (LLMs). By incorporating relationship dimension
separation, dialogue data construction, and contextual learning strategies, the
proposed method enhances extraction performance. Leveraging dialogue structure
information, it improves the model's ability to understand implicit
relationships and demonstrates strong adaptability in complex contexts.
Additionally, we construct a high-quality Chinese novel relation extraction
dataset to address the lack of labeled resources and support future research.
Experimental results show that our method outperforms traditional baselines
across multiple evaluation metrics and successfully facilitates the automated
construction of character relationship networks in novels.

### 20. CoSteer: Collaborative Decoding-Time Personalization via Local Delta Steering

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Hang Lv, Sheng Liang, Hao Wang, Hongchao Gu, Yaxiong Wu, Wei Guo, Defu Lian, Yong Liu, Enhong Chen
- **URL**: <http://arxiv.org/abs/2507.04756v1>
- **Submitted**: 2025-07-07 08:32:29
- **Topic Keywords**: rag, personalization
- **Reason**: The paper presents a novel framework for collaborative decoding-time personalization in text generation, leveraging localized delta steering to adapt large language models to user-specific contexts. While it touches on the topic of user behavior modeling, it is primarily focused on natural language processing and text generation, rather than information retrieval or search technologies. The relevance to the user's interests is somewhat limited, but the paper's innovative approach to personalization and privacy preservation may be of interest to those with a broader background in NLP and data mining.

#### Abstract
> Personalized text generation has become crucial for adapting language models
to diverse and evolving users' personal context across cultural, temporal, and
contextual dimensions. While existing methods often rely on centralized
fine-tuning or static preference alignment, they struggle to achieve real-time
adaptation under resource constraints inherent to personal devices. This
limitation creates a dilemma: large cloud-based models lack access to localized
user-specific information, while small on-device models cannot match the
generation quality of their cloud counterparts. To address this dichotomy, we
present CoSteer, a novel collaborative framework that enables decoding-time
personalization through localized delta steering. Our key insight lies in
leveraging the logits difference between personal context-aware and -agnostic
outputs from local small models as steering signals for cloud-based LLMs.
Specifically, we formulate token-level optimization as an online learning
problem, where local delta vectors dynamically adjust the remote LLM's logits
within the on-device environment. This approach preserves privacy by
transmitting only the final steered tokens rather than raw data or intermediate
vectors, while maintaining cloud-based LLMs' general capabilities without
fine-tuning. Through comprehensive experiments on various personalized
generation tasks, we demonstrate that CoSteer effectively assists LLMs in
generating personalized content by leveraging locally stored user profiles and
histories, ensuring privacy preservation through on-device data processing
while maintaining acceptable computational overhead.

### 21. Why We Feel What We Feel: Joint Detection of Emotions and Their Opinion Triggers in E-commerce

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Arnav Attri, Anuj Attri, Pushpak Bhattacharyya, Suman Banerjee, Amey Patil, Muthusamy Chelliah, Nikesh Garera
- **URL**: <http://arxiv.org/abs/2507.04708v1>
- **Submitted**: 2025-07-07 06:59:37
- **Comment**: 23 pages, 11 figures, 7 tables. Dataset and code will be made
  publicly available
- **Topic Keywords**: commerce, e-commerce, search
- **Reason**: The paper explores emotion detection and opinion trigger extraction in e-commerce reviews, which is related to information retrieval and search technologies. However, the focus on e-commerce and opinion triggers is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Customer reviews on e-commerce platforms capture critical affective signals
that drive purchasing decisions. However, no existing research has explored the
joint task of emotion detection and explanatory span identification in
e-commerce reviews - a crucial gap in understanding what triggers customer
emotional responses. To bridge this gap, we propose a novel joint task unifying
Emotion detection and Opinion Trigger extraction (EOT), which explicitly models
the relationship between causal text spans (opinion triggers) and affective
dimensions (emotion categories) grounded in Plutchik's theory of 8 primary
emotions. In the absence of labeled data, we introduce EOT-X, a human-annotated
collection of 2,400 reviews with fine-grained emotions and opinion triggers. We
evaluate 23 Large Language Models (LLMs) and present EOT-DETECT, a structured
prompting framework with systematic reasoning and self-reflection. Our
framework surpasses zero-shot and chain-of-thought techniques, across
e-commerce domains.

### 22. Heterogeneous User Modeling for LLM-based Recommendation

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Honghui Bao, Wenjie Wang, Xinyu Lin, Fengbin Zhu, Teng Sun, Fuli Feng, Tat-Seng Chua
- **URL**: <http://arxiv.org/abs/2507.04626v1>
- **Submitted**: 2025-07-07 03:08:28
- **Comment**: Accepted by RecSys 2025
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on heterogeneous user modeling for recommendation, leveraging large language models, which is related to information retrieval and search technologies. However, the primary focus is on recommender systems, which is not the user's primary area of interest. The paper's emphasis on user behavior modeling and compression enhancers is somewhat relevant to the user's background in e-commerce and query understanding, but the connection is not strong enough to warrant a higher score.

#### Abstract
> Leveraging Large Language Models (LLMs) for recommendation has demonstrated
notable success in various domains, showcasing their potential for open-domain
recommendation. A key challenge to advancing open-domain recommendation lies in
effectively modeling user preferences from users' heterogeneous behaviors
across multiple domains. Existing approaches, including ID-based and
semantic-based modeling, struggle with poor generalization, an inability to
compress noisy interactions effectively, and the domain seesaw phenomenon. To
address these challenges, we propose a Heterogeneous User Modeling (HUM)
method, which incorporates a compression enhancer and a robustness enhancer for
LLM-based recommendation. The compression enhancer uses a customized prompt to
compress heterogeneous behaviors into a tailored token, while a masking
mechanism enhances cross-domain knowledge extraction and understanding. The
robustness enhancer introduces a domain importance score to mitigate the domain
seesaw phenomenon by guiding domain optimization. Extensive experiments on
heterogeneous datasets validate that HUM effectively models user heterogeneity
by achieving both high efficacy and robustness, leading to superior performance
in open-domain recommendation.

### 23. InfoSteer: Steering Information Utility in Language Model Post-Training

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Chunyuan Deng, Ruidi Chang, Hanjie Chen
- **URL**: <http://arxiv.org/abs/2507.05158v1>
- **Submitted**: 2025-07-07 16:13:21
- **Topic Keywords**: rag
- **Reason**: The paper presents a method for post-training language models, focusing on promoting information utilization and adaptively allocating resources. While it touches on the idea of 'steering' the model, it doesn't directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest for you. The paper's focus on language models and post-training is somewhat relevant, but not a central match for your research themes.

#### Abstract
> Recent advancements in language models (LMs) gradually ushered in an era
where post-training is crucial. Yet, post-training approaches such as
supervised fine-tuning (SFT) do not guarantee effective use of knowledge
acquired during pretraining. We therefore present \ours, a lightweight method
that encourages parametric information utilization in LMs during post-training.
This is achieved via treating FFN layer as associate key-value memory, and
promotes the use of stored memory vectors via forward-pass interventions or
regularization during backpropagation. We find this simple guidance during
post-training phase delivers consistent performance improvements across diverse
model families--including Qwen, Gemma and Llama-spanning over 15 downstream
tasks in both ID and OOD evaluations. Beyond performance gains, we also find
that steered LMs can adaptively allocate information-placing more emphasis on
generating semantically meaningful tokens, while using fewer resources on
simple transition ones (e.g., `,' or `and'). Our work underscores that vanilla
post-training does not fully leverage pre-training potential, and steering LMs
in latent representation space offers a promising approach that enhances both
performance and interpretability.

### 24. Interest Networks (iNETs) for Cities: Cross-Platform Insights and Urban Behavior Explanations

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Gustavo H. Santos, Myriam Delgado, Thiago H. Silva
- **URL**: <http://arxiv.org/abs/2507.04995v1>
- **Submitted**: 2025-07-07 13:34:15
- **Comment**: Accepted at ACM SIGKDD Conference on Knowledge Discovery and Data
  Mining (KDD-UMC)
- **Topic Keywords**: recommend, search
- **Reason**: The paper explores interest networks and recommendation systems in the context of urban behavior, which is somewhat related to information retrieval and search technologies. However, the focus on location-based social networks and urban mobility research is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Location-Based Social Networks (LBSNs) provide a rich foundation for modeling
urban behavior through iNETs (Interest Networks), which capture how user
interests are distributed throughout urban spaces. This study compares iNETs
across platforms (Google Places and Foursquare) and spatial granularities,
showing that coarser levels reveal more consistent cross-platform patterns,
while finer granularities expose subtle, platform-specific behaviors. Our
analysis finds that, in general, user interest is primarily shaped by
geographic proximity and venue similarity, while socioeconomic and political
contexts play a lesser role. Building on these insights, we develop a
multi-level, explainable recommendation system that predicts high-interest
urban regions for different user types. The model adapts to behavior profiles
-- such as explorers, who are driven by proximity, and returners, who prefer
familiar venues -- and provides natural-language explanations using explainable
AI (XAI) techniques. To support our approach, we introduce h3-cities, a tool
for multi-scale spatial analysis, and release a public demo for interactively
exploring personalized urban recommendations. Our findings contribute to urban
mobility research by providing scalable, context-aware, and interpretable
recommendation systems.

### 25. ReLoop: "Seeing Twice and Thinking Backwards" via Closed-loop Training to Mitigate Hallucinations in Multimodal understanding

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Jianjiang Yang, Ziyan Huang, Yanshu Li
- **URL**: <http://arxiv.org/abs/2507.04943v1>
- **Submitted**: 2025-07-07 12:40:48
- **Comment**: 8 pages,6 figures,5 tables
- **Topic Keywords**: rag
- **Reason**: The paper focuses on multimodal understanding and large language models, which is related to information retrieval and NLP. However, the specific problem of hallucinations in multimodal understanding is not directly aligned with the user's research interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> While Multimodal Large Language Models (MLLMs) have achieved remarkable
progress in open-ended visual question answering, they remain vulnerable to
hallucinations. These are outputs that contradict or misrepresent input
semantics, posing a critical challenge to the reliability and factual
consistency. Existing methods often rely on external verification or post-hoc
correction, lacking an internal mechanism to validate outputs directly during
training. To bridge this gap, we propose ReLoop, a unified closed-loop training
framework that encourages multimodal consistency for cross-modal understanding
in MLLMs. ReLoop adopts a ring-shaped structure that integrates three
complementary consistency feedback mechanisms, obliging MLLMs to "seeing twice
and thinking backwards". Specifically, ReLoop employs the frozen Consistency
Feedback Plugin (CFP), comprising semantic reconstruction, visual description,
and an attention supervision module for attention alignment. These components
collectively enforce semantic reversibility, visual consistency, and
interpretable attention, enabling the model to correct its outputs during
training. Extensive evaluations and analyses demonstrate the effectiveness of
ReLoop in reducing hallucination rates across multiple benchmarks, establishing
a robust method for hallucination mitigation in MLLMs. We will release our
source code and data in the camera-ready version.

### 26. R1-RE: Cross-Domain Relationship Extraction with RLVR

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Runpeng Dai, Tong Zheng, Run Yang, Hongtu Zhu
- **URL**: <http://arxiv.org/abs/2507.04642v1>
- **Submitted**: 2025-07-07 03:50:59
- **Comment**: 14 pages, 7 figures
- **Topic Keywords**: rag
- **Reason**: The paper focuses on Relationship Extraction, a task in Natural Language Processing, which is related to my interests in NLP. However, the specific application and methodology, using reinforcement learning with verifiable rewards, do not directly align with my primary focus on Information Retrieval, query understanding, and ranking models.

#### Abstract
> Relationship extraction (RE) is a core task in natural language processing.
Traditional approaches typically frame RE as a supervised learning problem,
directly mapping context to labels-an approach that often suffers from poor
out-of-domain (OOD) generalization. Inspired by the workflow of human
annotators, we reframe RE as a reasoning task guided by annotation guidelines
and introduce R1-RE, the first reinforcement learning with verifiable reward
(RLVR) framework for RE tasks. Our method elicits the reasoning abilities of
small language models for annotation tasks, resulting in significantly improved
OOD robustness. We evaluate our approach on the public Sem-2010 dataset and a
private MDKG dataset. The R1-RE-7B model attains an average OOD accuracy of
approximately 70%, on par with leading proprietary models such as GPT-4o.
Additionally, our comprehensive analysis provides novel insights into the
training dynamics and emergent reasoning behaviors of the RLVR paradigm for RE.

### 27. Hierarchical Intent-guided Optimization with Pluggable LLM-Driven Semantics for Session-based Recommendation

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Jinpeng Chen, Jianxiang He, Huan Li, Senzhang Wang, Yuan Cao, Kaimin Wei, Zhenye Yang, Ye Ji
- **URL**: <http://arxiv.org/abs/2507.04623v1>
- **Submitted**: 2025-07-07 02:50:04
- **Topic Keywords**: recommend
- **Reason**: The paper focuses on session-based recommendation, which is not directly related to the user's primary interest in Information Retrieval and Search technologies. While it mentions large language models (LLMs) and graph neural networks (GNNs), the application is in recommender systems, which is a secondary interest. The paper's emphasis on semantic learning and intent-guided optimization is somewhat relevant to the user's background in e-commerce, but the connection to query understanding, ranking models, and user behavior modeling is limited.

#### Abstract
> Session-based Recommendation (SBR) aims to predict the next item a user will
likely engage with, using their interaction sequence within an anonymous
session. Existing SBR models often focus only on single-session information,
ignoring inter-session relationships and valuable cross-session insights. Some
methods try to include inter-session data but struggle with noise and
irrelevant information, reducing performance. Additionally, most models rely on
item ID co-occurrence and overlook rich semantic details, limiting their
ability to capture fine-grained item features. To address these challenges, we
propose a novel hierarchical intent-guided optimization approach with pluggable
LLM-driven semantic learning for session-based recommendations, called HIPHOP.
First, we introduce a pluggable embedding module based on large language models
(LLMs) to generate high-quality semantic representations, enhancing item
embeddings. Second, HIPHOP utilizes graph neural networks (GNNs) to model item
transition relationships and incorporates a dynamic multi-intent capturing
module to address users' diverse interests within a session. Additionally, we
design a hierarchical inter-session similarity learning module, guided by user
intent, to capture global and local session relationships, effectively
exploring users' long-term and short-term interests. To mitigate noise, an
intent-guided denoising strategy is applied during inter-session learning.
Finally, we enhance the model's discriminative capability by using contrastive
learning to optimize session representations. Experiments on multiple datasets
show that HIPHOP significantly outperforms existing methods, demonstrating its
effectiveness in improving recommendation quality. Our code is available:
https://github.com/hjx159/HIPHOP.

### 28. PRIME: Large Language Model Personalization with Cognitive Memory and Thought Processes

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Xinliang Frederick Zhang, Nick Beauchamp, Lu Wang
- **URL**: <http://arxiv.org/abs/2507.04607v1>
- **Submitted**: 2025-07-07 01:54:34
- **Topic Keywords**: personalization
- **Reason**: The paper explores large language model personalization, integrating cognitive memory and thought processes, which is related to user behavior modeling and query understanding in Information Retrieval. However, the focus is on personalization rather than search ranking, and the connection to Information Retrieval is not immediately apparent. The paper's relevance to the user's interests is somewhat limited due to its primary focus on NLP and personalization.

#### Abstract
> Large language model (LLM) personalization aims to align model outputs with
individuals' unique preferences and opinions. While recent efforts have
implemented various personalization methods, a unified theoretical framework
that can systematically understand the drivers of effective personalization is
still lacking. In this work, we integrate the well-established cognitive
dual-memory model into LLM personalization, by mirroring episodic memory to
historical user engagements and semantic memory to long-term, evolving user
beliefs. Specifically, we systematically investigate memory instantiations and
introduce a unified framework, PRIME, using episodic and semantic memory
mechanisms. We further augment PRIME with a novel personalized thinking
capability inspired by the slow thinking strategy. Moreover, recognizing the
absence of suitable benchmarks, we introduce a dataset using Change My View
(CMV) from Reddit, specifically designed to evaluate long-context
personalization. Extensive experiments validate PRIME's effectiveness across
both long- and short-context scenarios. Further analysis confirms that PRIME
effectively captures dynamic personalization beyond mere popularity biases.

### 29. MedGemma Technical Report

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Andrew Sellergren, Sahar Kazemzadeh, Tiam Jaroensri, Atilla Kiraly, Madeleine Traverse, Timo Kohlberger, Shawn Xu, Fayaz Jamil, C√≠an Hughes, Charles Lau, Justin Chen, Fereshteh Mahvar, Liron Yatziv, Tiffany Chen, Bram Sterling, Stefanie Anna Baby, Susanna Maria Baby, Jeremy Lai, Samuel Schmidgall, Lu Yang, Kejia Chen, Per Bjornsson, Shashir Reddy, Ryan Brush, Kenneth Philbrick, Howard Hu, Howard Yang, Richa Tiwari, Sunny Jansen, Preeti Singh, Yun Liu, Shekoofeh Azizi, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ram√©, Morgane Riviere, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Elena Buchatskaya, Jean-Baptiste Alayrac, Dmitry Lepikhin, Vlad Feinberg, Sebastian Borgeaud, Alek Andreev, Cassidy Hardin, Robert Dadashi, L√©onard Hussenot, Armand Joulin, Olivier Bachem, Yossi Matias, Katherine Chou, Avinatan Hassidim, Kavi Goel, Clement Farabet, Joelle Barral, Tris Warkentin, Jonathon Shlens, David Fleet, Victor Cotruta, Omar Sanseviero, Gus Martins, Phoebe Kirk, Anand Rao, Shravya Shetty, David F. Steiner, Can Kirmizibayrak, Rory Pilgrim, Daniel Golden, Lin Yang
- **URL**: <http://arxiv.org/abs/2507.05201v2>
- **Submitted**: 2025-07-07 17:01:44
- **Topic Keywords**: information retrieval, ctr, retrieval, search
- **Reason**: This paper is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus is on medical vision-language foundation models and their applications in healthcare, which is outside the user's primary research areas.

#### Abstract
> Artificial intelligence (AI) has significant potential in healthcare
applications, but its training and deployment faces challenges due to
healthcare's diverse data, complex tasks, and the need to preserve privacy.
Foundation models that perform well on medical tasks and require less
task-specific tuning data are critical to accelerate the development of
healthcare AI applications. We introduce MedGemma, a collection of medical
vision-language foundation models based on Gemma 3 4B and 27B. MedGemma
demonstrates advanced medical understanding and reasoning on images and text,
significantly exceeding the performance of similar-sized generative models and
approaching the performance of task-specific models, while maintaining the
general capabilities of the Gemma 3 base models. For out-of-distribution tasks,
MedGemma achieves 2.6-10% improvement on medical multimodal question answering,
15.5-18.1% improvement on chest X-ray finding classification, and 10.8%
improvement on agentic evaluations compared to the base models. Fine-tuning
MedGemma further improves performance in subdomains, reducing errors in
electronic health record information retrieval by 50% and reaching comparable
performance to existing specialized state-of-the-art methods for pneumothorax
classification and histopathology patch classification. We additionally
introduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.
MedSigLIP powers the visual understanding capabilities of MedGemma and as an
encoder achieves comparable or better performance than specialized medical
image encoders. Taken together, the MedGemma collection provides a strong
foundation of medical image and text capabilities, with potential to
significantly accelerate medical research and development of downstream
applications. The MedGemma collection, including tutorials and model weights,
can be found at https://goo.gle/medgemma.

### 30. ArtifactsBench: Bridging the Visual-Interactive Gap in LLM Code Generation Evaluation

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Chenchen Zhang, Yuhang Li, Can Xu, Jiaheng Liu, Ao Liu, Shihui Hu, Dengpeng Wu, Guanhua Huang, Kejiao Li, Qi Yi, Ruibin Xiong, Haotian Zhu, Yuanxing Zhang, Yuhao Jiang, Yue Zhang, Zenan Xu, Bohui Zhai, Guoxiang He, Hebin Li, Jie Zhao, Le Zhang, Lingyun Tan, Pengyu Guo, Xianshu Pang, Yang Ruan, Zhifeng Zhang, Zhonghu Wang, Ziyan Xu, Zuopu Yin, Wiggin Zhou, Chayse Zhou, Fengzong Lian
- **URL**: <http://arxiv.org/abs/2507.04952v1>
- **Submitted**: 2025-07-07 12:53:00
- **Topic Keywords**: ranking, pairwise, rank
- **Reason**: The paper focuses on evaluating the quality of visual code generation using Large Language Models, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it mentions multimodal evaluation, the context is not relevant to the user's research interests in IR, NLP, and data mining.

#### Abstract
> The generative capabilities of Large Language Models (LLMs) are rapidly
expanding from static code to dynamic, interactive visual artifacts. This
progress is bottlenecked by a critical evaluation gap: established benchmarks
focus on algorithmic correctness and are blind to the visual fidelity and
interactive integrity that define modern user experiences. To bridge this gap,
we introduce ArtifactsBench, a new benchmark and paradigm for the automated,
multimodal evaluation of visual code generation. Our framework programmatically
renders each generated artifact and captures its dynamic behavior through
temporal screenshots. This visual evidence, alongside the source code, is then
assessed by a Multimodal LLM (MLLM)-as-Judge, which is rigorously guided by a
fine-grained, per-task checklist to ensure holistic and reproducible scoring.
We construct a new benchmark of 1,825 diverse tasks and evaluate over 30
leading LLMs. Our automated evaluation achieves a striking 94.4% ranking
consistency with WebDev Arena, the gold-standard for human preference in web
development, and over 90% pairwise agreement with human experts. This
establishes ArtifactsBench as the first framework to reliably automate the
assessment of human-perceived quality at scale. Our analysis provides a
high-resolution map of the current SOTA, revealing that generalist models often
outperform domain-specific ones. We open-source ArtifactsBench, including the
benchmark, evaluation harness, and baseline results at
https://artifactsbenchmark.github.io/, to provide the community with a scalable
and accurate tool to accelerate the development of user-centric generative
models.

### 31. Evaluating Memory in LLM Agents via Incremental Multi-Turn Interactions

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Yuanzhe Hu, Yu Wang, Julian McAuley
- **URL**: <http://arxiv.org/abs/2507.05257v1>
- **Submitted**: 2025-07-07 17:59:54
- **Comment**: 23 Pages, Y. Hu and Y. Wang contribute equally
- **Topic Keywords**: rag, retrieval, search
- **Reason**: This paper focuses on evaluating memory mechanisms in Large Language Model (LLM) agents, which is not directly related to Information Retrieval (IR) or Search technologies. The paper's emphasis on memory agents and their competencies, such as accurate retrieval and test-time learning, is not aligned with the user's research interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Recent benchmarks for Large Language Model (LLM) agents primarily focus on
evaluating reasoning, planning, and execution capabilities, while another
critical component-memory, encompassing how agents memorize, update, and
retrieve long-term information-is under-evaluated due to the lack of
benchmarks. We term agents with memory mechanisms as memory agents. In this
paper, we identify four core competencies essential for memory agents: accurate
retrieval, test-time learning, long-range understanding, and conflict
resolution. Existing datasets either rely on limited context lengths or are
tailored for static, long-context settings like book-based QA, which do not
reflect the interactive, multi-turn nature of memory agents that incrementally
accumulate information. Furthermore, no existing benchmarks cover all four
competencies. Therefore, we introduce MemoryAgentBench, a new benchmark
specifically designed for memory agents. Our benchmark combines reformulated
existing datasets with newly constructed ones, covering the above four memory
competencies, providing a systematic and challenging testbed for assessing
memory quality. We evaluate a diverse set of memory agents, ranging from simple
context-based and retrieval-augmented generation (RAG) systems to advanced
agents with external memory modules and tool integration. Empirical results
reveal that current methods fall short of mastering all four competencies,
underscoring the need for further research into comprehensive memory mechanisms
for LLM agents.

### 32. Put Teacher in Student's Shoes: Cross-Distillation for Ultra-compact Model Compression Framework

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Maolin Wang, Jun Chu, Sicong Xie, Xiaoling Zang, Yao Zhao, Wenliang Zhong, Xiangyu Zhao
- **URL**: <http://arxiv.org/abs/2507.04636v1>
- **Submitted**: 2025-07-07 03:38:09
- **Comment**: Accepted by KDD 2025
- **Topic Keywords**: ltr, recommend
- **Reason**: The paper focuses on model compression and deployment in edge computing, which is not directly related to the user's interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions NLP, the context is more about deploying efficient models in resource-restricted environments, rather than query understanding, ranking models, or user behavior modeling.

#### Abstract
> In the era of mobile computing, deploying efficient Natural Language
Processing (NLP) models in resource-restricted edge settings presents
significant challenges, particularly in environments requiring strict privacy
compliance, real-time responsiveness, and diverse multi-tasking capabilities.
These challenges create a fundamental need for ultra-compact models that
maintain strong performance across various NLP tasks while adhering to
stringent memory constraints. To this end, we introduce Edge ultra-lIte BERT
framework (EI-BERT) with a novel cross-distillation method. EI-BERT efficiently
compresses models through a comprehensive pipeline including hard token
pruning, cross-distillation and parameter quantization. Specifically, the
cross-distillation method uniquely positions the teacher model to understand
the student model's perspective, ensuring efficient knowledge transfer through
parameter integration and the mutual interplay between models. Through
extensive experiments, we achieve a remarkably compact BERT-based model of only
1.91 MB - the smallest to date for Natural Language Understanding (NLU) tasks.
This ultra-compact model has been successfully deployed across multiple
scenarios within the Alipay ecosystem, demonstrating significant improvements
in real-world applications. For example, it has been integrated into Alipay's
live Edge Recommendation system since January 2024, currently serving the app's
recommendation traffic across \textbf{8.4 million daily active devices}.

### 33. Response Attack: Exploiting Contextual Priming to Jailbreak Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Ziqi Miao, Lijun Li, Yuan Xiong, Zhenhua Liu, Pengyu Zhu, Jing Shao
- **URL**: <http://arxiv.org/abs/2507.05248v1>
- **Submitted**: 2025-07-07 17:56:05
- **Comment**: 21 pages, 9 figures. Code and data available at
  https://github.com/Dtc7w3PQ/Response-Attack
- **Topic Keywords**: query
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus is on large language models and contextual priming, which is not a core area of interest for the user.

#### Abstract
> Contextual priming, where earlier stimuli covertly bias later judgments,
offers an unexplored attack surface for large language models (LLMs). We
uncover a contextual priming vulnerability in which the previous response in
the dialogue can steer its subsequent behavior toward policy-violating content.
Building on this insight, we propose Response Attack, which uses an auxiliary
LLM to generate a mildly harmful response to a paraphrased version of the
original malicious query. They are then formatted into the dialogue and
followed by a succinct trigger prompt, thereby priming the target model to
generate harmful content. Across eight open-source and proprietary LLMs, RA
consistently outperforms seven state-of-the-art jailbreak techniques, achieving
higher attack success rates. To mitigate this threat, we construct and release
a context-aware safety fine-tuning dataset, which significantly reduces the
attack success rate while preserving model capabilities. The code and data are
available at https://github.com/Dtc7w3PQ/Response-Attack.

### 34. SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Jingyi Chai, Shuo Tang, Rui Ye, Yuwen Du, Xinyu Zhu, Mengcheng Zhou, Yanfeng Wang, Weinan E, Yuzhi Zhang, Linfeng Zhang, Siheng Chen
- **URL**: <http://arxiv.org/abs/2507.05241v2>
- **Submitted**: 2025-07-07 17:50:52
- **Comment**: 15 pages, 10 figures
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on scientific AI agents and their application to a specific challenge, Humanity's Last Exam, which is not directly related to Information Retrieval, Search technologies, or query understanding. While the paper mentions tools and libraries, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest for your research.

#### Abstract
> The rapid advancements of AI agents have ignited the long-held ambition of
leveraging them to accelerate scientific discovery. Achieving this goal
requires a deep understanding of the frontiers of human knowledge. As such,
Humanity's Last Exam (HLE) provides an exceptionally challenging touchstone for
evaluating scientific AI agents. In this work, we aim to construct the
foundational architecture for general-purpose agents and validate the
capabilities through leading performance on HLE. To achieve this, we introduce
X-Master, a tool-augmented reasoning agent designed to emulate human
researchers by interacting flexibly with external tools during its reasoning
process. This agent, guided by the conceptualization of code as an interaction
language, can flexibly leverage built-in Python libraries and our customized
tools to augment the reasoning. We further scale its capabilities through
X-Masters, a scattered-and-stacked agentic workflow that systematically
enhances breadth and depth of reasoning. Our open-source solution, X-Masters,
sets a new state-of-the-art record on HLE with a score of 32.1%, surpassing
OpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to
exceed the 30% threshold. This work allows us to gain a deeper understanding of
complex task-solving and accumulates valuable experience that can inform future
advancements, guiding subsequent model training.

### 35. OpenS2S: Advancing Fully Open-Source End-to-End Empathetic Large Speech Language Model

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Chen Wang, Tianyu Peng, Wen Yang, Yinan Bai, Guangfu Wang, Jun Lin, Lanpeng Jia, Lingxiang Wu, Jinqiao Wang, Chengqing Zong, Jiajun Zhang
- **URL**: <http://arxiv.org/abs/2507.05177v2>
- **Submitted**: 2025-07-07 16:31:37
- **Comment**: Technical Report
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on developing an empathetic large speech language model, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on natural language processing, the scope is narrow and does not align with the user's broader interests in data mining and recommender systems.

#### Abstract
> Empathetic interaction is a cornerstone of human-machine communication, due
to the need for understanding speech enriched with paralinguistic cues and
generating emotional and expressive responses. However, the most powerful
empathetic LSLMs are increasingly closed off, leaving the crucial details about
the architecture, data and development opaque to researchers. Given the
critical need for transparent research into the LSLMs and empathetic behavior,
we present OpenS2S, a fully open-source, transparent and end-to-end LSLM
designed to enable empathetic speech interactions. Based on our empathetic
speech-to-text model BLSP-Emo, OpenS2S further employs a streaming interleaved
decoding architecture to achieve low-latency speech generation. To facilitate
end-to-end training, OpenS2S incorporates an automated data construction
pipeline that synthesizes diverse, high-quality empathetic speech dialogues at
low cost. By leveraging large language models to generate empathetic content
and controllable text-to-speech systems to introduce speaker and emotional
variation, we construct a scalable training corpus with rich paralinguistic
diversity and minimal human supervision. We release the fully open-source
OpenS2S model, including the dataset, model weights, pre-training and
fine-tuning codes, to empower the broader research community and accelerate
innovation in empathetic speech systems. The project webpage can be accessed at
https://casia-lm.github.io/OpenS2S

### 36. SMART: Simulated Students Aligned with Item Response Theory for Question Difficulty Prediction

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Alexander Scarlatos, Nigel Fernandez, Christopher Ormerod, Susan Lottridge, Andrew Lan
- **URL**: <http://arxiv.org/abs/2507.05129v1>
- **Submitted**: 2025-07-07 15:41:38
- **Topic Keywords**: rag, personalization
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on educational assessments, item response theory, and question difficulty prediction, which is outside your primary research areas.

#### Abstract
> Item (question) difficulties play a crucial role in educational assessments,
enabling accurate and efficient assessment of student abilities and
personalization to maximize learning outcomes. Traditionally, estimating item
difficulties can be costly, requiring real students to respond to items,
followed by fitting an item response theory (IRT) model to get item difficulty
estimates. This approach cannot be applied to the cold-start setting for
previously unseen items either. In this work, we present SMART (Simulated
Students Aligned with IRT), a novel method for aligning simulated students with
instructed ability, which can then be used in simulations to predict the
difficulty of open-ended items. We achieve this alignment using direct
preference optimization (DPO), where we form preference pairs based on how
likely responses are under a ground-truth IRT model. We perform a simulation by
generating thousands of responses, evaluating them with an LLM-based scoring
model, and fit the resulting data to an IRT model to obtain item difficulty
estimates. Through extensive experiments on a real-world student response
dataset, we show that SMART outperforms other item difficulty prediction
methods by leveraging its improved ability alignment.

### 37. Interleaving Logic and Counting

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Johan van Benthem, Thomas Icard
- **URL**: <http://arxiv.org/abs/2507.05219v1>
- **Submitted**: 2025-07-07 17:30:29
- **Topic Keywords**: rag
- **Reason**: The paper focuses on the intersection of logic and counting in natural language, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on formal systems and mathematical concepts, the topics and methods are not aligned with the user's research interests in IR, NLP, and data mining.

#### Abstract
> Reasoning with quantifier expressions in natural language combines logical
and arithmetical features, transcending strict divides between qualitative and
quantitative. Our topic is this cooperation of styles as it occurs in common
linguistic usage and its extension into the broader practice of natural
language plus "grassroots mathematics".
  We begin with a brief review of first-order logic with counting operators and
cardinality comparisons. This system is known to be of high complexity, and
drowns out finer aspects of the combination of logic and counting. We move to a
small fragment that can represent numerical syllogisms and basic reasoning
about comparative size: monadic first-order logic with counting. We provide
normal forms that allow for axiomatization, determine which arithmetical
notions can be defined on finite and on infinite models, and conversely, we
discuss which logical notions can be defined out of purely arithmetical ones,
and what sort of (non-)classical logics can be induced.
  Next, we investigate a series of strengthenings, again using normal form
methods. The monadic second-order version is close, in a precise sense, to
additive Presburger Arithmetic, while versions with the natural device of tuple
counting take us to Diophantine equations, making the logic undecidable. We
also define a system that combines basic modal logic over binary accessibility
relations with counting, needed to formulate ubiquitous reasoning patterns such
as the Pigeonhole Principle.
  We return to our starting point in natural language, confronting the
architecture of our formal systems with linguistic quantifier vocabulary and
syntax. We conclude with some general thoughts on yet further entanglements of
logic and counting in formal systems, on rethinking the
qualitative/quantitative divide, and on connecting our analysis to empirical
findings in cognitive science.

### 38. From Fragments to Facts: A Curriculum-Driven DPO Approach for Generating Hindi News Veracity Explanations

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Pulkit Bansal, Raghvendra Kumar, Shakti Singh, Sriparna Saha, Adam Jatowt
- **URL**: <http://arxiv.org/abs/2507.05179v1>
- **Submitted**: 2025-07-07 16:34:28
- **Topic Keywords**: rag
- **Reason**: The paper focuses on generating news veracity explanations in Hindi, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing and machine learning, the specific application and problem domain are not aligned with the user's research interests.

#### Abstract
> In an era of rampant misinformation, generating reliable news explanations is
vital, especially for under-represented languages like Hindi. Lacking robust
automated tools, Hindi faces challenges in scaling misinformation detection. To
bridge this gap, we propose a novel framework integrating Direct Preference
Optimization (DPO) with curriculum learning to align machine-generated
explanations with human reasoning. Fact-checked explanations from credible
sources serve as preferred responses, while LLM outputs highlight system
limitations and serve as non-preferred responses. To refine task-specific
alignment, we introduce two key parameters -- Actuality and Finesse -- into the
DPO loss function, enhancing explanation quality and consistency. Experiments
with LLMs (Mistral, Llama, Gemma) and PLMs (mBART, mT5) confirm the framework's
effectiveness in generating coherent, contextually relevant explanations. This
scalable approach combats misinformation and extends automated explanation
generation to low-resource languages.

### 39. Interpretable Mnemonic Generation for Kanji Learning via Expectation-Maximization

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jaewook Lee, Alexander Scarlatos, Andrew Lan
- **URL**: <http://arxiv.org/abs/2507.05137v1>
- **Submitted**: 2025-07-07 15:49:23
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on Kanji learning and mnemonic generation, which is outside the user's primary research areas.

#### Abstract
> Learning Japanese vocabulary is a challenge for learners from Roman alphabet
backgrounds due to script differences. Japanese combines syllabaries like
hiragana with kanji, which are logographic characters of Chinese origin. Kanji
are also complicated due to their complexity and volume. Keyword mnemonics are
a common strategy to aid memorization, often using the compositional structure
of kanji to form vivid associations. Despite recent efforts to use large
language models (LLMs) to assist learners, existing methods for LLM-based
keyword mnemonic generation function as a black box, offering limited
interpretability. We propose a generative framework that explicitly models the
mnemonic construction process as driven by a set of common rules, and learn
them using a novel Expectation-Maximization-type algorithm. Trained on
learner-authored mnemonics from an online platform, our method learns latent
structures and compositional rules, enabling interpretable and systematic
mnemonics generation. Experiments show that our method performs well in the
cold-start setting for new learners while providing insight into the mechanisms
behind effective mnemonic creation.

### 40. Co-DETECT: Collaborative Discovery of Edge Cases in Text Classification

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Chenfei Xiong, Jingwei Ni, Yu Fan, Vil√©m Zouhar, Donya Rooein, Lorena Calvo-Bartolom√©, Alexander Hoyle, Zhijing Jin, Mrinmaya Sachan, Markus Leippold, Dirk Hovy, Mennatallah El-Assady, Elliott Ash
- **URL**: <http://arxiv.org/abs/2507.05010v1>
- **Submitted**: 2025-07-07 13:48:54
- **Topic Keywords**: rag
- **Reason**: The paper focuses on text classification and edge case detection, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the scope is limited to text classification and does not address ranking models or user behavior modeling.

#### Abstract
> We introduce Co-DETECT (Collaborative Discovery of Edge cases in TExt
ClassificaTion), a novel mixed-initiative annotation framework that integrates
human expertise with automatic annotation guided by large language models
(LLMs). Co-DETECT starts with an initial, sketch-level codebook and dataset
provided by a domain expert, then leverages the LLM to annotate the data and
identify edge cases that are not well described by the initial codebook.
Specifically, Co-DETECT flags challenging examples, induces high-level,
generalizable descriptions of edge cases, and assists user in incorporating
edge case handling rules to improve the codebook. This iterative process
enables more effective handling of nuanced phenomena through compact,
generalizable annotation rules. Extensive user study, qualitative and
quantitative analyses prove the effectiveness of Co-DETECT.

### 41. Transcribing Spanish Texts from the Past: Experiments with Transkribus, Tesseract and Granite

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yanco Amor Torterolo-Orta, Jaione Macicior-Mitxelena, Marina Miguez-Lamanuzzi, Ana Garc√≠a-Serrano
- **URL**: <http://arxiv.org/abs/2507.04878v1>
- **Submitted**: 2025-07-07 11:04:17
- **Comment**: This paper was written as part of a shared task organized within the
  2025 edition of the Iberian Languages Evaluation Forum (IberLEF 2025), held
  at SEPLN 2025 in Zaragoza. This paper describes the joint participation of
  two teams in said competition, GRESEL1 and GRESEL2, each with an individual
  paper that will be published in CEUR
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on Optical Character Recognition (OCR) and transcribing Spanish texts from the past, which is not related to the user's areas of interest.

#### Abstract
> This article presents the experiments and results obtained by the GRESEL team
in the IberLEF 2025 shared task PastReader: Transcribing Texts from the Past.
Three types of experiments were conducted with the dual aim of participating in
the task and enabling comparisons across different approaches. These included
the use of a web-based OCR service, a traditional OCR engine, and a compact
multimodal model. All experiments were run on consumer-grade hardware, which,
despite lacking high-performance computing capacity, provided sufficient
storage and stability. The results, while satisfactory, leave room for further
improvement. Future work will focus on exploring new techniques and ideas using
the Spanish-language dataset provided by the shared task, in collaboration with
Biblioteca Nacional de Espa\~na (BNE).

### 42. Spec-TOD: A Specialized Instruction-Tuned LLM Framework for Efficient Task-Oriented Dialogue Systems

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Quang-Vinh Nguyen, Quang-Chieu Nguyen, Hoang Pham, Khac-Hoai Nam Bui
- **URL**: <http://arxiv.org/abs/2507.04841v1>
- **Submitted**: 2025-07-07 10:03:20
- **Comment**: Accepted at SIGdial 2025
- **Topic Keywords**: rag
- **Reason**: The paper focuses on Task-Oriented Dialogue Systems, which is not directly related to Information Retrieval or Search technologies. Although it involves Natural Language Processing, the context is different from the user's primary research interests.

#### Abstract
> Task-oriented dialogue (TOD) systems facilitate goal-driven interactions
between users and machines. While recent advances in deep learning have
improved the performance, TOD systems often struggle in low-resource scenarios
with limited labeled data. To address this challenge, we propose Spec-TOD, a
novel framework designed to train an end-to-end TOD system with limited data.
Spec-TOD introduces two main innovations: (i) a novel specialized end-to-end
TOD framework that incorporates explicit task instructions for
instruction-tuned large language models (LLMs), and (ii) an efficient training
strategy that leverages lightweight, specialized LLMs to achieve strong
performance with minimal supervision. Experiments on the MultiWOZ dataset, a
widely used TOD benchmark, demonstrate that Spec-TOD achieves competitive
results while significantly reducing the need for labeled data. These findings
highlight the potential of the proposed framework in advancing efficient and
effective TOD systems in low-resource settings.

### 43. From Vision To Language through Graph of Events in Space and Time: An Explainable Self-supervised Approach

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Mihai Masala, Marius Leordeanu
- **URL**: <http://arxiv.org/abs/2507.04815v1>
- **Submitted**: 2025-07-07 09:33:19
- **Comment**: arXiv admin note: text overlap with arXiv:2501.08460
- **Topic Keywords**: rag
- **Reason**: The paper focuses on video captioning and generating natural language descriptions from videos, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it involves NLP and data mining, the scope is limited to a specific domain and does not address ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest in the user's research.

#### Abstract
> The task of describing video content in natural language is commonly referred
to as video captioning. Unlike conventional video captions, which are typically
brief and widely available, long-form paragraph descriptions in natural
language are scarce. This limitation of current datasets is due to the
expensive human manual annotation required and to the highly challenging task
of explaining the language formation process from the perspective of the
underlying story, as a complex system of interconnected events in space and
time. Through a thorough analysis of recently published methods and available
datasets, we identify a general lack of published resources dedicated to the
problem of describing videos in complex language, beyond the level of
descriptions in the form of enumerations of simple captions. Furthermore, while
state-of-the-art methods produce impressive results on the task of generating
shorter captions from videos by direct end-to-end learning between the videos
and text, the problem of explaining the relationship between vision and
language is still beyond our reach. In this work, we propose a shared
representation between vision and language, based on graphs of events in space
and time, which can be obtained in an explainable and analytical way, to
integrate and connect multiple vision tasks to produce the final natural
language description. Moreover, we also demonstrate how our automated and
explainable video description generation process can function as a fully
automatic teacher to effectively train direct, end-to-end neural student
pathways, within a self-supervised neuro-analytical system. We validate that
our explainable neuro-analytical approach generates coherent, rich and relevant
textual descriptions on videos collected from multiple varied datasets, using
both standard evaluation metrics, human annotations and consensus from
ensembles of state-of-the-art VLMs.

### 44. Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Guokan Shang, Hadi Abdine, Ahmad Chamma, Amr Mohamed, Mohamed Anwar, Abdelaziz Bounhar, Omar El Herraoui, Preslav Nakov, Michalis Vazirgiannis, Eric Xing
- **URL**: <http://arxiv.org/abs/2507.04569v1>
- **Submitted**: 2025-07-06 22:53:41
- **Topic Keywords**: rag
- **Reason**: This paper focuses on developing language models for Egyptian dialects in both Arabic and Latin scripts, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on language adaptation and model architecture, the primary focus is on multilingual language models, which is a different area of research.

#### Abstract
> We introduce Nile-Chat-4B, 3x4B-A6B, and 12B, a collection of LLMs for
Egyptian dialect, uniquely designed to understand and generate texts written in
both Arabic and Latin scripts. Specifically, with Nile-Chat-3x4B-A6B, we
introduce a novel language adaptation approach by leveraging the
Branch-Train-MiX strategy to merge script-specialized experts, into a single
MoE model. Our Nile-Chat models significantly outperform leading multilingual
and Arabic LLMs, such as LLaMa, Jais, and ALLaM, on our newly introduced
Egyptian evaluation benchmarks, which span both understanding and generative
tasks. Notably, our 12B model yields a 14.4% performance gain over
Qwen2.5-14B-Instruct on Latin-script benchmarks. All our resources are publicly
available. We believe this work presents a comprehensive methodology for
adapting LLMs to dual-script languages, addressing an often overlooked aspect
in modern LLM development.

### 45. AI-Driven Cytomorphology Image Synthesis for Medical Diagnostics

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Jan Carreras Boada, Rao Muhammad Umer, Carsten Marr
- **URL**: <http://arxiv.org/abs/2507.05063v1>
- **Submitted**: 2025-07-07 14:49:05
- **Comment**: 8 pages, 6 figures, 2 tables. Final Degree Project (TFG) submitted at
  ESCI-UPF and conducted at Helmholtz Munich
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The paper focuses on medical diagnostics, image synthesis, and machine learning for biomedical research, which is outside your primary areas of interest.

#### Abstract
> Biomedical datasets often contain a large sample imbalance and are subject to
strict privacy constraints, which together hinder the development of accurate
machine learning models. One potential solution is to generate synthetic
images, as this can improve data availability while preserving patient privacy.
However, it remains difficult to generate synthetic images of sufficient
quality for training robust classifiers. In this work, we focus on the
classification of single white blood cells, a key component in the diagnosis of
hematological diseases such as acute myeloid leukemia (AML), a severe blood
cancer. We demonstrate how synthetic images generated with a fine-tuned stable
diffusion model using LoRA weights when guided by real few-shot samples of the
target white blood cell classes, can enhance classifier performance for limited
data. When training a ResNet classifier, accuracy increased from 27.3\% to
78.4\% (+51.1\%) by adding 5000 synthetic images per class to a small and
highly imbalanced real dataset. For a CLIP-based classifier, the accuracy
improved from 61.8\% to 76.8\% (+15.0\%). The synthetic images are highly
similar to real images, and they can help overcome dataset limitations,
enhancing model generalization. Our results establish synthetic images as a
tool in biomedical research, improving machine learning models, and
facilitating medical diagnosis and research.

### 46. From Autonomy to Agency: Agentic Vehicles for Human-Centered Mobility Systems

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Jiangbo Yu
- **URL**: <http://arxiv.org/abs/2507.04996v1>
- **Submitted**: 2025-07-07 13:34:49
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The topic of autonomous vehicles and agentic AI systems is outside the scope of your primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Autonomy, from the Greek autos (self) and nomos (law), refers to the capacity
to operate according to internal rules without external control. Accordingly,
autonomous vehicles (AuVs) are defined as systems capable of perceiving their
environment and executing preprogrammed tasks independently of external input.
However, both research and real-world deployments increasingly showcase
vehicles that demonstrate behaviors beyond this definition (including the SAE
levels 1 to 6), such as interaction with humans and machines, goal adaptation,
contextual reasoning, external tool use, and long-term planning, particularly
with the integration of large language models (LLMs) and agentic AI systems.
These developments reveal a conceptual gap between technical autonomy and the
broader cognitive and social capabilities needed for future human-centered
mobility systems. To address this, we introduce the concept of agentic vehicles
(AgVs), referring to vehicles that integrate agentic AI to reason, adapt, and
interact within complex environments. This paper presents a systems-level
framework to characterize AgVs, focusing on their cognitive and communicative
layers and differentiating them from conventional AuVs. It synthesizes relevant
advances in agentic AI, robotics, multi-agent systems, and human-machine
interaction, and highlights how agentic AI, through high-level reasoning and
tool use, can function not merely as computational tools but as interactive
agents embedded in mobility ecosystems. The paper concludes by identifying key
challenges in the development and governance of AgVs, including safety,
real-time control, public acceptance, ethical alignment, and regulatory
frameworks.

### 47. A Survey of Pun Generation: Datasets, Evaluations and Methodologies

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Yuchen Su, Yonghua Zhu, Ruofan Wang, Zijian Huang, Diana Benavides-Prado, Michael Witbrock
- **URL**: <http://arxiv.org/abs/2507.04793v1>
- **Submitted**: 2025-07-07 09:12:46
- **Topic Keywords**: search
- **Reason**: The paper's focus on pun generation, linguistic elements, and creative writing is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on computational linguistics, the topic is not specific to query understanding, ranking models, or user behavior modeling, which are the user's primary areas of interest.

#### Abstract
> Pun generation seeks to creatively modify linguistic elements in text to
produce humour or evoke double meanings. It also aims to preserve coherence and
contextual appropriateness, making it useful in creative writing and
entertainment across various media and contexts. Although pun generation has
received considerable attention in computational linguistics, there is
currently no dedicated survey that systematically reviews this specific area.
To bridge this gap, this paper provides a comprehensive review of pun
generation datasets and methods across different stages, including conventional
approaches, deep learning techniques, and pre-trained language models.
Additionally, we summarise both automated and human evaluation metrics used to
assess the quality of pun generation. Finally, we discuss the research
challenges and propose promising directions for future work.

### 48. A validity-guided workflow for robust large language model research in psychology

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Zhicheng Lin
- **URL**: <http://arxiv.org/abs/2507.04491v1>
- **Submitted**: 2025-07-06 18:06:12
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The paper focuses on the validity of large language models in psychology research, which is a distinct field and does not align with your research themes.

#### Abstract
> Large language models (LLMs) are rapidly being integrated into psychological
research as research tools, evaluation targets, human simulators, and cognitive
models. However, recent evidence reveals severe measurement unreliability:
Personality assessments collapse under factor analysis, moral preferences
reverse with punctuation changes, and theory-of-mind accuracy varies widely
with trivial rephrasing. These "measurement phantoms"--statistical artifacts
masquerading as psychological phenomena--threaten the validity of a growing
body of research. Guided by the dual-validity framework that integrates
psychometrics with causal inference, we present a six-stage workflow that
scales validity requirements to research ambition--using LLMs to code text
requires basic reliability and accuracy, while claims about psychological
properties demand comprehensive construct validation. Researchers must (1)
explicitly define their research goal and corresponding validity requirements,
(2) develop and validate computational instruments through psychometric
testing, (3) design experiments that control for computational confounds, (4)
execute protocols with transparency, (5) analyze data using methods appropriate
for non-independent observations, and (6) report findings within demonstrated
boundaries and use results to refine theory. We illustrate the workflow through
an example of model evaluation--"LLM selfhood"--showing how systematic
validation can distinguish genuine computational phenomena from measurement
artifacts. By establishing validated computational instruments and transparent
practices, this workflow provides a path toward building a robust empirical
foundation for AI psychology research.

---

