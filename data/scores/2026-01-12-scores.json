[
    {
        "title": "Learning to Trust the Crowd: A Multi-Model Consensus Reasoning Engine for Large Language Models",
        "abstract": "Large language models (LLMs) achieve strong aver- age performance yet remain unreliable at the instance level, with frequent hallucinations, brittle failures, and poorly calibrated confidence. We study reliability through the lens of multi-model consensus: given responses from several heterogeneous LLMs, can we learn which answer is most likely correct for a given query? We introduce a Multi-Model Consensus Reasoning Engine that treats the set of LLM outputs as input to a supervised meta-learner. The system maps natural language responses into structured features using semantic embeddings, pairwise similarity and clustering statistics, lexical and structural cues, reasoning-quality scores, confidence estimates, and model-specific priors, and then applies gradient-boosted trees, listwise ranking, and graph neural networks over similarity graphs of answers. Using three open-weight LLMs evaluated on compact, resource- constrained subsets of GSM8K, ARC-Challenge, HellaSwag, and TruthfulQA, our best graph-attention-based consensus model improves macro-average accuracy by 4.6 percentage points over the strongest single LLM and by 8.1 points over majority vote, while also yielding lower Brier scores and fewer TruthfulQA hal- lucinations. Ablation and feature-importance analyses show that semantic agreement and clustering features are most influential, with reasoning-quality and model-prior features providing com- plementary gains, suggesting supervised multi-model consensus is a practical route toward more reliable LLM behavior, even in a modest single-machine setup.",
        "url": "http://arxiv.org/abs/2601.07245v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07245v1",
        "arxiv_id": "2601.07245v1",
        "authors": [
            "Pranav Kallem"
        ],
        "submitted": "2026-01-12 06:27:06",
        "source": "arxiv",
        "comment": null,
        "score": 15,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'listwise' (score: +3)",
            "Found 'pairwise' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the reliability of large language models through multi-model consensus, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on language models and consensus reasoning is not directly aligned with the user's primary interests in IR, search technologies, and user behavior modeling."
    },
    {
        "title": "MedTutor: A Retrieval-Augmented LLM System for Case-Based Medical Education",
        "abstract": "The learning process for medical residents presents significant challenges, demanding both the ability to interpret complex case reports and the rapid acquisition of accurate medical knowledge from reliable sources. Residents typically study case reports and engage in discussions with peers and mentors, but finding relevant educational materials and evidence to support their learning from these cases is often time-consuming and challenging. To address this, we introduce MedTutor, a novel system designed to augment resident training by automatically generating evidence-based educational content and multiple-choice questions from clinical case reports. MedTutor leverages a Retrieval-Augmented Generation (RAG) pipeline that takes clinical case reports as input and produces targeted educational materials. The system's architecture features a hybrid retrieval mechanism that synergistically queries a local knowledge base of medical textbooks and academic literature (using PubMed, Semantic Scholar APIs) for the latest related research, ensuring the generated content is both foundationally sound and current. The retrieved evidence is filtered and ordered using a state-of-the-art reranking model and then an LLM generates the final long-form output describing the main educational content regarding the case-report. We conduct a rigorous evaluation of the system. First, three radiologists assessed the quality of outputs, finding them to be of high clinical and educational value. Second, we perform a large scale evaluation using an LLM-as-a Judge to understand if LLMs can be used to evaluate the output of the system. Our analysis using correlation between LLMs outputs and human expert judgments reveals a moderate alignment and highlights the continued necessity of expert oversight.",
        "url": "http://arxiv.org/abs/2601.06979v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06979v1",
        "arxiv_id": "2601.06979v1",
        "authors": [
            "Dongsuk Jang",
            "Ziyao Shangguan",
            "Kyle Tegtmeyer",
            "Anurag Gupta",
            "Jan Czerminski",
            "Sophie Chheang",
            "Arman Cohan"
        ],
        "submitted": "2026-01-11 16:27:21",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 (System Demonstrations)",
        "score": 15,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses a Retrieval-Augmented Generation (RAG) system for case-based medical education, which is somewhat related to information retrieval and query understanding. However, the focus on medical education and clinical case reports is not directly aligned with the user's primary research interests in e-commerce, NLP, and IR for search technologies. While the use of LLMs and reranking models is relevant, the context and application are not central to the user's core research themes."
    },
    {
        "title": "UETQuintet at BioCreative IX - MedHopQA: Enhancing Biomedical QA with Selective Multi-hop Reasoning and Contextual Retrieval",
        "abstract": "Biomedical Question Answering systems play a critical role in processing complex medical queries, yet they often struggle with the intricate nature of medical data and the demand for multi-hop reasoning. In this paper, we propose a model designed to effectively address both direct and sequential questions. While sequential questions are decomposed into a chain of sub-questions to perform reasoning across a chain of steps, direct questions are processed directly to ensure efficiency and minimise processing overhead. Additionally, we leverage multi-source information retrieval and in-context learning to provide rich, relevant context for generating answers. We evaluated our model on the BioCreative IX - MedHopQA Shared Task datasets. Our approach achieves an Exact Match score of 0.84, ranking second on the current leaderboard. These results highlight the model's capability to meet the challenges of Biomedical Question Answering, offering a versatile solution for advancing medical research and practice.",
        "url": "http://arxiv.org/abs/2601.06974v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06974v1",
        "arxiv_id": "2601.06974v1",
        "authors": [
            "Quoc-An Nguyen",
            "Thi-Minh-Thu Vu",
            "Bich-Dat Nguyen",
            "Dinh-Quang-Minh Tran",
            "Hoang-Quynh Le"
        ],
        "submitted": "2026-01-11 16:12:38",
        "source": "arxiv",
        "comment": "Accepted at the BioCreative IX Challenge and Workshop (BC9) at IJCAI",
        "score": 15,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 3,
        "llm_reason": "This paper is somewhat related to Information Retrieval, specifically in the context of biomedical question answering, but it does not directly align with the user's core research themes of query understanding, ranking models, and user behavior modeling. The focus on multi-hop reasoning and contextual retrieval is relevant to NLP, but the e-commerce domain is not explored. The paper's emphasis on biomedical question answering is a departure from the user's primary focus on information retrieval in general domains."
    },
    {
        "title": "RLPO: Residual Listwise Preference Optimization for Long-Context Review Ranking",
        "abstract": "Review ranking is pivotal in e-commerce for prioritizing diagnostic and authentic feedback from the deluge of user-generated content. While large language models have improved semantic assessment, existing ranking paradigms face a persistent trade-off in long-context settings. Pointwise scoring is efficient but often fails to account for list-level interactions, leading to miscalibrated top-$k$ rankings. Listwise approaches can leverage global context, yet they are computationally expensive and become unstable as candidate lists grow. To address this, we propose Residual Listwise Preference Optimization (RLPO), which formulates ranking as listwise representation-level residual correction over a strong pointwise LLM scorer. RLPO first produces calibrated pointwise scores and item representations, then applies a lightweight encoder over the representations to predict listwise score residuals, avoiding full token-level listwise processing. We also introduce a large-scale benchmark for long-context review ranking with human verification. Experiments show RLPO improves NDCG@k over strong pointwise and listwise baselines and remains robust as list length increases.",
        "url": "http://arxiv.org/abs/2601.07449v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07449v1",
        "arxiv_id": "2601.07449v1",
        "authors": [
            "Hao Jiang",
            "Zhi Yang",
            "Annan Wang",
            "Yichi Zhang",
            "Weisi Lin"
        ],
        "submitted": "2026-01-12 11:45:19",
        "source": "arxiv",
        "comment": null,
        "score": 14,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'listwise' (score: +3)",
            "Found 'pointwise' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in ranking models and user behavior modeling. The focus on long-context review ranking and the use of residual listwise preference optimization aligns with your interests in deep semantic understanding and real-time relevance optimization. The e-commerce domain is also a relevant area of application."
    },
    {
        "title": "FinCARDS: Card-Based Analyst Reranking for Financial Document Question Answering",
        "abstract": "Financial question answering (QA) over long corporate filings requires evidence to satisfy strict constraints on entities, financial metrics, fiscal periods, and numeric values. However, existing LLM-based rerankers primarily optimize semantic relevance, leading to unstable rankings and opaque decisions on long documents. We propose FinCards, a structured reranking framework that reframes financial evidence selection as constraint satisfaction under a finance-aware schema. FinCards represents filing chunks and questions using aligned schema fields (entities, metrics, periods, and numeric spans), enabling deterministic field-level matching. Evidence is selected via a multi-stage tournament reranking with stability-aware aggregation, producing auditable decision traces. Across two corporate filing QA benchmarks, FinCards substantially improves early-rank retrieval over both lexical and LLM-based reranking baselines, while reducing ranking variance, without requiring model fine-tuning or unpredictable inference budgets. Our code is available at https://github.com/XanderZhou2022/FINCARDS.",
        "url": "http://arxiv.org/abs/2601.06992v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06992v1",
        "arxiv_id": "2601.06992v1",
        "authors": [
            "Yixi Zhou",
            "Fan Zhang",
            "Yu Chen",
            "Haipeng Zhang",
            "Preslav Nakov",
            "Zhuohan Xie"
        ],
        "submitted": "2026-01-11 17:03:24",
        "source": "arxiv",
        "comment": "15 pages, including figures and tables",
        "score": 12,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper is somewhat related to information retrieval, specifically in the context of question answering and reranking. However, its focus on financial document analysis and constraint satisfaction under a finance-aware schema is not directly aligned with the user's core research themes in query understanding, ranking models, and user behavior modeling. While it does involve structured reranking and evidence selection, the domain and application are quite specific and not a central match for the user's interests."
    },
    {
        "title": "Towards Building efficient Routed systems for Retrieval",
        "abstract": "Late-interaction retrieval models like ColBERT achieve superior accuracy by enabling token-level interactions, but their computational cost hinders scalability and integration with Approximate Nearest Neighbor Search (ANNS). We introduce FastLane, a novel retrieval framework that dynamically routes queries to their most informative representations, eliminating redundant token comparisons. FastLane employs a learnable routing mechanism optimized alongside the embedding model, leveraging self-attention and differentiable selection to maximize efficiency. Our approach reduces computational complexity by up to 30x while maintaining competitive retrieval performance. By bridging late-interaction models with ANNS, FastLane enables scalable, low-latency retrieval, making it feasible for large-scale applications such as search engines, recommendation systems, and question-answering platforms. This work opens pathways for multi-lingual, multi-modal, and long-context retrieval, pushing the frontier of efficient and adaptive information retrieval.",
        "url": "http://arxiv.org/abs/2601.06389v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06389v1",
        "arxiv_id": "2601.06389v1",
        "authors": [
            "Ramnath Kumar",
            "Prateek Jain",
            "Cho-Jui Hsieh"
        ],
        "submitted": "2026-01-10 02:22:01",
        "source": "arxiv",
        "comment": null,
        "score": 12,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the areas of query understanding and ranking models. The introduction of FastLane, a novel retrieval framework, is aligned with your focus on efficient and adaptive information retrieval. The application of FastLane to large-scale applications such as search engines and question-answering platforms further supports its relevance."
    },
    {
        "title": "Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning",
        "abstract": "LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose TOOLQP, a lightweight framework that models retrieval as iterative query planning. Instead of single-shot matching, TOOLQP decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition. We train TOOLQP using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR). Experiments demonstrate that TOOLQP achieves state-of-the-art performance, exhibiting superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.",
        "url": "http://arxiv.org/abs/2601.07782v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07782v1",
        "arxiv_id": "2601.07782v1",
        "authors": [
            "Wei Fang",
            "James Glass"
        ],
        "submitted": "2026-01-12 17:58:39",
        "source": "arxiv",
        "comment": null,
        "score": 11,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses a retrieval framework for tool libraries, focusing on query planning and addressing the semantic gap between user goals and technical documentation. While it touches on retrieval and query understanding, it's more focused on tool libraries and query planning rather than general information retrieval or search technologies. The connection to NLP is also limited, making it somewhat relevant but not a central match for your research interests."
    },
    {
        "title": "GAP-Net: Calibrating User Intent via Gated Adaptive Progressive Learning for CTR Prediction",
        "abstract": "Sequential user behavior modeling is pivotal for Click-Through Rate (CTR) prediction yet is hindered by three intrinsic bottlenecks: (1) the \"Attention Sink\" phenomenon, where standard Softmax compels the model to allocate probability mass to noisy behaviors; (2) the Static Query Assumption, which overlooks dynamic shifts in user intent driven by real-time contexts; and (3) Rigid View Aggregation, which fails to adaptively weight heterogeneous temporal signals according to the decision context. To bridge these gaps, we propose GAP-Net (Gated Adaptive Progressive Network), a unified framework establishing a \"Triple Gating\" architecture to progressively refine information from micro-level features to macro-level views. GAP-Net operates through three integrated mechanisms: (1) Adaptive Sparse-Gated Attention (ASGA) employs micro-level gating to enforce sparsity, effectively suppressing massive noise activations; (2) Gated Cascading Query Calibration (GCQC) dynamically aligns user intent by bridging real-time triggers and long-term memories via a meso-level cascading channel; and (3) Context-Gated Denoising Fusion (CGDF) performs macro-level modulation to orchestrate the aggregation of multi-view sequences. Extensive experiments on industrial datasets demonstrate that GAP-Net achieves substantial improvements over state-of-the-art baselines, exhibiting superior robustness against interaction noise and intent drift.",
        "url": "http://arxiv.org/abs/2601.07613v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07613v1",
        "arxiv_id": "2601.07613v1",
        "authors": [
            "Ke Shenqiang",
            "Wei Jianxiong",
            "Hua Qingsong"
        ],
        "submitted": "2026-01-12 15:01:12",
        "source": "arxiv",
        "comment": "9 pages, 3 figures",
        "score": 11,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'user behavior' (score: +2)",
            "Found 'click' (score: +2)",
            "Found 'ctr' (score: +2)",
            "Found 'click-through rate' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of click models and user behavior modeling. The proposed GAP-Net framework addresses several key challenges in CTR prediction, including the 'Attention Sink' phenomenon and the Static Query Assumption, which are closely related to your interests in query understanding and ranking models. However, the focus on recommender systems and CTR prediction is somewhat specific, but still aligns with your broader interests in IR and NLP."
    },
    {
        "title": "Pragya: An AI-Based Semantic Recommendation System for Sanskrit Subhasitas",
        "abstract": "Sanskrit Subhasitas encapsulate centuries of cultural and philosophical wisdom, yet remain underutilized in the digital age due to linguistic and contextual barriers. In this work, we present Pragya, a retrieval-augmented generation (RAG) framework for semantic recommendation of Subhasitas. We curate a dataset of 200 verses annotated with thematic tags such as motivation, friendship, and compassion. Using sentence embeddings (IndicBERT), the system retrieves top-k verses relevant to user queries. The retrieved results are then passed to a generative model (Mistral LLM) to produce transliterations, translations, and contextual explanations. Experimental evaluation demonstrates that semantic retrieval significantly outperforms keyword matching in precision and relevance, while user studies highlight improved accessibility through generated summaries. To our knowledge, this is the first attempt at integrating retrieval and generation for Sanskrit Subhasitas, bridging cultural heritage with modern applied AI.",
        "url": "http://arxiv.org/abs/2601.06607v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06607v1",
        "arxiv_id": "2601.06607v1",
        "authors": [
            "Tanisha Raorane",
            "Prasenjit Kole"
        ],
        "submitted": "2026-01-10 16:13:25",
        "source": "arxiv",
        "comment": "Preprint",
        "score": 11,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on a semantic recommendation system for Sanskrit Subhasitas, which involves retrieval-augmented generation. While it touches on retrieval and generation, the context and application are quite different from the user's core research interests in IR and search technologies. The use of NLP techniques like sentence embeddings and generative models is relevant, but the specific domain and goals of the paper do not align closely with the user's interests."
    },
    {
        "title": "Is Agentic RAG worth it? An experimental comparison of RAG approaches",
        "abstract": "Retrieval-Augmented Generation (RAG) systems are usually defined by the combination of a generator and a retrieval component that extracts textual context from a knowledge base to answer user queries. However, such basic implementations exhibit several limitations, including noisy or suboptimal retrieval, misuse of retrieval for out-of-scope queries, weak query-document matching, and variability or cost associated with the generator. These shortcomings have motivated the development of \"Enhanced\" RAG, where dedicated modules are introduced to address specific weaknesses in the workflow. More recently, the growing self-reflective capabilities of Large Language Models (LLMs) have enabled a new paradigm, which we refer to as \"Agentic\" RAG. In this approach, the LLM orchestrates the entire process-deciding which actions to perform, when to perform them, and whether to iterate-thereby reducing reliance on fixed, manually engineered modules. Despite the rapid adoption of both paradigms, it remains unclear which approach is preferable under which conditions. In this work, we conduct an extensive, empirically driven evaluation of Enhanced and Agentic RAG across multiple scenarios and dimensions. Our results provide practical insights into the trade-offs between the two paradigms, offering guidance on selecting the most effective RAG design for real-world applications, considering both costs and performance.",
        "url": "http://arxiv.org/abs/2601.07711v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07711v1",
        "arxiv_id": "2601.07711v1",
        "authors": [
            "Pietro Ferrazzi",
            "Milica Cvjeticanin",
            "Alessio Piraccini",
            "Davide Giannuzzi"
        ],
        "submitted": "2026-01-12 16:43:44",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of Retrieval-Augmented Generation (RAG) systems. The paper's focus on comparing Enhanced and Agentic RAG approaches aligns with your interest in query understanding and ranking models. However, the paper's primary focus is on NLP and generation, which is somewhat tangential to your core IR interests."
    },
    {
        "title": "ActiShade: Activating Overshadowed Knowledge to Guide Multi-Hop Reasoning in Large Language Models",
        "abstract": "In multi-hop reasoning, multi-round retrieval-augmented generation (RAG) methods typically rely on LLM-generated content as the retrieval query. However, these approaches are inherently vulnerable to knowledge overshadowing - a phenomenon where critical information is overshadowed during generation. As a result, the LLM-generated content may be incomplete or inaccurate, leading to irrelevant retrieval and causing error accumulation during the iteration process. To address this challenge, we propose ActiShade, which detects and activates overshadowed knowledge to guide large language models (LLMs) in multi-hop reasoning. Specifically, ActiShade iteratively detects the overshadowed keyphrase in the given query, retrieves documents relevant to both the query and the overshadowed keyphrase, and generates a new query based on the retrieved documents to guide the next-round iteration. By supplementing the overshadowed knowledge during the formulation of next-round queries while minimizing the introduction of irrelevant noise, ActiShade reduces the error accumulation caused by knowledge overshadowing. Extensive experiments show that ActiShade outperforms existing methods across multiple datasets and LLMs.",
        "url": "http://arxiv.org/abs/2601.07260v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07260v1",
        "arxiv_id": "2601.07260v1",
        "authors": [
            "Huipeng Ma",
            "Luan Zhang",
            "Dandan Song",
            "Linmei Hu",
            "Yuhang Tian",
            "Jun Yang",
            "Changzhi Zhou",
            "Chenhao Li",
            "Yizhou Jin",
            "Xudong Li",
            "Meng Lin",
            "Mingxing Zhang",
            "Shuhao Zhang"
        ],
        "submitted": "2026-01-12 06:57:31",
        "source": "arxiv",
        "comment": "Accepted to AAAI 2026",
        "score": 10,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to information retrieval, specifically addressing a challenge in multi-hop reasoning with large language models. However, its focus on knowledge overshadowing and activation is not directly aligned with the user's core research themes in query understanding, ranking models, and user behavior modeling. While it touches on retrieval-augmented generation, it does not explore the user's primary areas of interest in deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "Applying Embedding-Based Retrieval to Airbnb Search",
        "abstract": "The goal of Airbnb search is to match guests with the ideal accommodation that fits their travel needs. This is a challenging problem, as popular search locations can have around a hundred thousand available homes, and guests themselves have a wide variety of preferences. Furthermore, the launch of new product features, such as \\textit{flexible date search,} significantly increased the number of eligible homes per search query. As such, there is a need for a sophisticated retrieval system which can provide high-quality candidates with low latency in a way that integrates with the overall ranking stack.\n  This paper details our journey to build an efficient and high-quality retrieval system for Airbnb search. We describe the key unique challenges we encountered when implementing an Embedding-Based Retrieval (EBR) system for a two sided marketplace like Airbnb -- such as the dynamic nature of the inventory, a lengthy user funnel with multiple stages, and a variety of product surfaces. We cover unique insights when modeling the retrieval problem, how to build robust evaluation systems, and design choices for online serving. The EBR system was launched to production and powers several use-cases such as regular search, flexible date and promotional emails for marketing campaigns. The system demonstrated statistically-significant improvements in key metrics, such as booking conversion, via A/B testing.",
        "url": "http://arxiv.org/abs/2601.06873v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06873v1",
        "arxiv_id": "2601.06873v1",
        "authors": [
            "Mustafa Abdool",
            "Soumyadip Banerjee",
            "Moutupsi Paul",
            "Do-kyum Kim",
            "Xioawei Liu",
            "Bin Xu",
            "Tracy Yu",
            "Hui Gao",
            "Karen Ouyang",
            "Huiji Gao",
            "Liwei He",
            "Stephanie Moyerman",
            "Sanjeev Katariya"
        ],
        "submitted": "2026-01-11 11:41:55",
        "source": "arxiv",
        "comment": "14 pages, 9 figures",
        "score": 10,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper is somewhat related to information retrieval, specifically focusing on building an efficient retrieval system for Airbnb search using Embedding-Based Retrieval. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's focus on a specific e-commerce domain and application limits its broader relevance to the user's research interests."
    },
    {
        "title": "L-RAG: Balancing Context and Retrieval with Entropy-Based Lazy Loading",
        "abstract": "Retrieval-Augmented Generation (RAG) has emerged as the predominant paradigm for grounding Large Language Model outputs in factual knowledge, effectively mitigating hallucinations. However, conventional RAG systems operate under a \"retrieve-always\" assumption, querying vector databases for every input regardless of query complexity. This static approach incurs substantial computational overhead and inference latency, particularly problematic for high-throughput production deployments. We introduce L-RAG (Lazy Retrieval-Augmented Generation), an adaptive framework that implements hierarchical context management through entropy-based gating. L-RAG employs a two-tier architecture: queries are first processed with a compact document summary, and expensive chunk retrieval is triggered only when the model's predictive entropy exceeds a calibrated threshold, signaling genuine uncertainty. Through experiments on SQuAD 2.0 (N=500) using the Phi-2 model, we demonstrate that L-RAG provides a tunable accuracy-efficiency trade-off: at a conservative threshold (tau=0.5), L-RAG achieves 78.2% accuracy, matching Standard RAG (77.8%), with 8% retrieval reduction; at a balanced threshold (tau=1.0), retrieval reduction increases to 26% with modest accuracy trade-off (76.0%). Latency analysis shows that L-RAG saves 80-210ms per query when retrieval latency exceeds 500ms. Analysis of entropy distributions reveals statistically significant separation (p < 0.001) between correct predictions (H=1.72) and errors (H=2.20), validating entropy as a reliable uncertainty signal. L-RAG offers a practical, training-free approach toward more efficient RAG deployment, providing system architects with a configurable knob to balance accuracy and throughput requirements.",
        "url": "http://arxiv.org/abs/2601.06551v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06551v1",
        "arxiv_id": "2601.06551v1",
        "authors": [
            "Sergii Voloshyn"
        ],
        "submitted": "2026-01-10 12:25:19",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of query understanding and ranking models. The proposed L-RAG framework demonstrates a novel approach to balancing context and retrieval, leveraging entropy-based lazy loading to achieve a tunable accuracy-efficiency trade-off. While the focus is on Retrieval-Augmented Generation, the techniques and insights presented can be applied to broader IR and NLP problems."
    },
    {
        "title": "CSR-RAG: An Efficient Retrieval System for Text-to-SQL on the Enterprise Scale",
        "abstract": "Natural language to SQL translation (Text-to-SQL) is one of the long-standing problems that has recently benefited from advances in Large Language Models (LLMs). While most academic Text-to-SQL benchmarks request schema description as a part of natural language input, enterprise-scale applications often require table retrieval before SQL query generation. To address this need, we propose a novel hybrid Retrieval Augmented Generation (RAG) system consisting of contextual, structural, and relational retrieval (CSR-RAG) to achieve computationally efficient yet sufficiently accurate retrieval for enterprise-scale databases. Through extensive enterprise benchmarks, we demonstrate that CSR-RAG achieves up to 40% precision and over 80% recall while incurring a negligible average query generation latency of only 30ms on commodity data center hardware, which makes it appropriate for modern LLM-based enterprise-scale systems.",
        "url": "http://arxiv.org/abs/2601.06564v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06564v1",
        "arxiv_id": "2601.06564v1",
        "authors": [
            "Rajpreet Singh",
            "Novak BoÅ¡kov",
            "Lawrence Drabeck",
            "Aditya Gudal",
            "Manzoor A. Khan"
        ],
        "submitted": "2026-01-10 13:20:07",
        "source": "arxiv",
        "comment": null,
        "score": 9,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper focuses on a specific application of Natural Language Processing (NLP) for Text-to-SQL translation, which is somewhat related to Information Retrieval (IR) and query understanding. However, the primary focus on table retrieval and SQL query generation is not directly aligned with the user's core research themes in IR, ranking models, and user behavior modeling. The paper's emphasis on Large Language Models (LLMs) and enterprise-scale applications also diverges from the user's interests in e-commerce and real-time relevance optimization."
    },
    {
        "title": "TreePS-RAG: Tree-based Process Supervision for Reinforcement Learning in Agentic RAG",
        "abstract": "Agentic retrieval-augmented generation (RAG) formulates question answering as a multi-step interaction between reasoning and information retrieval, and has recently been advanced by reinforcement learning (RL) with outcome-based supervision. While effective, relying solely on sparse final rewards limits step-wise credit assignment and provides weak guidance for intermediate reasoning and actions. Recent efforts explore process-level supervision, but typically depend on offline constructed training data, which risks distribution shift, or require costly intermediate annotations. We present TreePS-RAG, an online, tree-based RL framework for agentic RAG that enables step-wise credit assignment while retaining standard outcome-only rewards. Our key insight is to model agentic RAG reasoning as a rollout tree, where each reasoning step naturally maps to a node. This tree structure allows step utility to be estimated via Monte Carlo estimation over its descendant outcomes, yielding fine-grained process advantages without requiring intermediate labels. To make this paradigm practical, we introduce an efficient online tree construction strategy that preserves exploration diversity under a constrained computational budget. With a rollout cost comparable to strong baselines like Search-R1, experiments on seven multi-hop and general QA benchmarks across multiple model scales show that TreePS-RAG consistently and significantly outperforms both outcome-supervised and leading process-supervised RL methods.",
        "url": "http://arxiv.org/abs/2601.06922v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06922v1",
        "arxiv_id": "2601.06922v1",
        "authors": [
            "Tianhua Zhang",
            "Kun Li",
            "Junan Li",
            "Yunxiang Li",
            "Hongyin Luo",
            "Xixin Wu",
            "James Glass",
            "Helen Meng"
        ],
        "submitted": "2026-01-11 14:07:30",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper presents a reinforcement learning framework for agentic retrieval-augmented generation, which is related to information retrieval and NLP. However, the focus on reinforcement learning and process supervision is not directly aligned with the user's core research themes in query understanding, ranking models, and user behavior modeling. While it touches on aspects of information retrieval, the connection is somewhat indirect."
    },
    {
        "title": "Relink: Constructing Query-Driven Evidence Graph On-the-Fly for GraphRAG",
        "abstract": "Graph-based Retrieval-Augmented Generation (GraphRAG) mitigates hallucinations in Large Language Models (LLMs) by grounding them in structured knowledge. However, current GraphRAG methods are constrained by a prevailing \\textit{build-then-reason} paradigm, which relies on a static, pre-constructed Knowledge Graph (KG). This paradigm faces two critical challenges. First, the KG's inherent incompleteness often breaks reasoning paths. Second, the graph's low signal-to-noise ratio introduces distractor facts, presenting query-relevant but misleading knowledge that disrupts the reasoning process.\n  To address these challenges, we argue for a \\textit{reason-and-construct} paradigm and propose Relink, a framework that dynamically builds a query-specific evidence graph. To tackle incompleteness, \\textbf{Relink} instantiates required facts from a latent relation pool derived from the original text corpus, repairing broken paths on the fly. To handle misleading or distractor facts, Relink employs a unified, query-aware evaluation strategy that jointly considers candidates from both the KG and latent relations, selecting those most useful for answering the query rather than relying on their pre-existence. This empowers Relink to actively discard distractor facts and construct the most faithful and precise evidence path for each query.\n  Extensive experiments on five Open-Domain Question Answering benchmarks show that Relink achieves significant average improvements of 5.4\\% in EM and 5.2\\% in F1 over leading GraphRAG baselines, demonstrating the superiority of our proposed framework.",
        "url": "http://arxiv.org/abs/2601.07192v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07192v1",
        "arxiv_id": "2601.07192v1",
        "authors": [
            "Manzong Huang",
            "Chenyang Bu",
            "Yi He",
            "Xingrui Zhuo",
            "Xindong Wu"
        ],
        "submitted": "2026-01-12 04:35:23",
        "source": "arxiv",
        "comment": "Accepted by AAAI 2026",
        "score": 7,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The proposed framework, Relink, addresses challenges in GraphRAG methods by dynamically building a query-specific evidence graph, which aligns with your focus on deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "RAIRS: Optimizing Redundant Assignment and List Layout for IVF-Based ANN Search",
        "abstract": "IVF is one of the most widely used ANNS (Approximate Nearest Neighbors Search) methods in vector databases. The idea of redundant assignment is to assign a data vector to more than one IVF lists for reducing the chance of missing true neighbors in IVF search. However, the naive strategy, which selects the second IVF list based on the distance between a data vector and the list centroids, performs poorly. Previous work focuses only on the inner product distance, while there is no optimized list selection study for the most popular Euclidean space. Moreover, the IVF search may access the same vector in more than one lists, resulting in redundant distance computation and decreasing query throughput. In this paper, we present RAIRS to address the above two challenges. For the challenge of the list selection, we propose an optimized AIR metric for the Euclidean space. AIR takes not only distances but also directions into consideration in order to support queries that are closer to the data vector but father away from the first chosen list's centroid. For the challenge of redundant distance computation, we propose SEIL, an optimized list layout that exploits shared cells to reduce repeated distance computations for IVF search. Our experimental results using representative real-world data sets show that RAIRS out-performs existing redundant assignment solutions and achieves up to 1.33x improvement over the best-performing IVF method, IVF-PQ Fast Scan with refinement.",
        "url": "http://arxiv.org/abs/2601.07183v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07183v1",
        "arxiv_id": "2601.07183v1",
        "authors": [
            "Zehai Yang",
            "Shimin Chen"
        ],
        "submitted": "2026-01-12 04:05:18",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on optimizing Approximate Nearest Neighbors Search (ANNS) methods, specifically IVF, which is not directly related to Information Retrieval or Search technologies. While it involves vector databases and distance computations, the context is more aligned with data storage and retrieval rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "CIRAG: Construction-Integration Retrieval and Adaptive Generation for Multi-hop Question Answering",
        "abstract": "Triple-based Iterative Retrieval-Augmented Generation (iRAG) mitigates document-level noise for multi-hop question answering. However, existing methods still face limitations: (i) greedy single-path expansion, which propagates early errors and fails to capture parallel evidence from different reasoning branches, and (ii) granularity-demand mismatch, where a single evidence representation struggles to balance noise control with contextual sufficiency. In this paper, we propose the Construction-Integration Retrieval and Adaptive Generation model, CIRAG. It introduces an Iterative Construction-Integration module that constructs candidate triples and history-conditionally integrates them to distill core triples and generate the next-hop query. This module mitigates the greedy trap by preserving multiple plausible evidence chains. Besides, we propose an Adaptive Cascaded Multi-Granularity Generation module that progressively expands contextual evidence based on the problem requirements, from triples to supporting sentences and full passages. Moreover, we introduce Trajectory Distillation, which distills the teacher model's integration policy into a lightweight student, enabling efficient and reliable long-horizon reasoning. Extensive experiments demonstrate that CIRAG achieves superior performance compared to existing iRAG methods.",
        "url": "http://arxiv.org/abs/2601.06799v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06799v1",
        "arxiv_id": "2601.06799v1",
        "authors": [
            "Zili Wei",
            "Xiaocui Yang",
            "Yilin Wang",
            "Zihan Wang",
            "Weidong Bao",
            "Shi Feng",
            "Daling Wang",
            "Yifei Zhang"
        ],
        "submitted": "2026-01-11 07:56:02",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper focuses on multi-hop question answering, which is related to query understanding and ranking models in Information Retrieval. However, the specific application and methodology differ from the user's core research themes, and the connection to user behavior modeling is not clear. The paper's emphasis on NLP and deep semantic understanding is somewhat relevant, but it does not align with the user's primary focus on IR and real-time relevance optimization."
    },
    {
        "title": "Order in the Evaluation Court: A Critical Analysis of NLG Evaluation Trends",
        "abstract": "Despite advances in Natural Language Generation (NLG), evaluation remains challenging. Although various new metrics and LLM-as-a-judge (LaaJ) methods are proposed, human judgment persists as the gold standard. To systematically review how NLG evaluation has evolved, we employ an automatic information extraction scheme to gather key information from NLG papers, focusing on different evaluation methods (metrics, LaaJ and human evaluation). With extracted metadata from 14,171 papers across four major conferences (ACL, EMNLP, NAACL, and INLG) over the past six years, we reveal several critical findings: (1) Task Divergence: While Dialogue Generation demonstrates a rapid shift toward LaaJ (>40% in 2025), Machine Translation remains locked into n-gram metrics, and Question Answering exhibits a substantial decline in the proportion of studies conducting human evaluation. (2) Metric Inertia: Despite the development of semantic metrics, general-purpose metrics (e.g., BLEU, ROUGE) continue to be widely used across tasks without empirical justification, often lacking the discriminative power to distinguish between specific quality criteria. (3) Human-LaaJ Divergence: Our association analysis challenges the assumption that LLMs act as mere proxies for humans; LaaJ and human evaluations prioritize very different signals, and explicit validation is scarce (<8% of papers comparing the two), with only moderate to low correlation. Based on these observations, we derive practical recommendations to improve the rigor of future NLG evaluation.",
        "url": "http://arxiv.org/abs/2601.07648v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07648v1",
        "arxiv_id": "2601.07648v1",
        "authors": [
            "Jing Yang",
            "Nils Feldhus",
            "Salar Mohtaj",
            "Leonhard Hennig",
            "Qianli Wang",
            "Eleni Metheniti",
            "Sherzod Hakimov",
            "Charlott Jakob",
            "Veronika Solopova",
            "Konrad Rieck",
            "David Schlangen",
            "Sebastian MÃ¶ller",
            "Vera Schmitt"
        ],
        "submitted": "2026-01-12 15:27:58",
        "source": "arxiv",
        "comment": "8 pages",
        "score": 6,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'acl' (score: +2)",
            "Found 'naacl' (score: +1)",
            "Found 'emnlp' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and evaluation metrics, but it does not directly align with the user's primary focus on Information Retrieval (IR), query understanding, ranking models, or user behavior modeling. The paper's analysis of NLG evaluation trends and recommendations for improvement may be of some interest, but it does not appear to be a central match for the user's research themes."
    },
    {
        "title": "Symphonym: Universal Phonetic Embeddings for Cross-Script Toponym Matching via Teacher-Student Distillation",
        "abstract": "Linking place names across languages and writing systems is a fundamental challenge in digital humanities and geographic information retrieval. Existing approaches rely on language-specific phonetic algorithms or transliteration rules that fail when names cross script boundaries -- no string metric can determine that \"Moscow\" when rendered in Cyrillic or Arabic refer to the same city.\n  I present Symphonym, a neural embedding system that maps toponyms from 20 writing systems into a unified 128-dimensional phonetic space. A Teacher network trained on articulatory phonetic features (via Epitran and PanPhon) produces target embeddings, while a Student network learns to approximate these from raw characters. At inference, only the lightweight Student (1.7M parameters) is required, enabling deployment without runtime phonetic conversion.\n  Training uses a three-phase curriculum on 57 million toponyms from GeoNames, Wikidata, and the Getty Thesaurus of Geographic Names. Phase 1 trains the Teacher on 467K phonetically-grounded triplets. Phase 2 aligns the Student to Teacher outputs across 23M samples, achieving 96.6% cosine similarity. Phase 3 fine-tunes on 3.3M hard negative triplets -- negatives sharing prefix and script with the anchor but referring to different places -- to sharpen discrimination.\n  Evaluation on the MEHDIE Hebrew-Arabic benchmark achieves 89.2% Recall@1, outperforming Levenshtein (81.5%) and Jaro-Winkler (78.5%). The system is optimised for cross-script matching; same-script variants can be handled by complementary string methods. Symphonym will enable fuzzy phonetic reconciliation and search across the World Historical Gazetteer's 67 million toponyms. Code and models are publicly available.",
        "url": "http://arxiv.org/abs/2601.06932v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06932v1",
        "arxiv_id": "2601.06932v1",
        "authors": [
            "Stephen Gadd"
        ],
        "submitted": "2026-01-11 14:36:36",
        "source": "arxiv",
        "comment": "30 pages, 5 tables, 2 figures",
        "score": 6,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on cross-script toponym matching, which is a specific problem in geographic information retrieval. While it involves some aspects of natural language processing, it does not directly relate to the user's core research themes in information retrieval, query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "PixRec: Leveraging Visual Context for Next-Item Prediction in Sequential Recommendation",
        "abstract": "Large Language Models (LLMs) have recently shown strong potential for usage in sequential recommendation tasks through text-only models, which combine advanced prompt design, contrastive alignment, and fine-tuning on downstream domain-specific data. While effective, these approaches overlook the rich visual information present in many real-world recommendation scenarios, particularly in e-commerce. This paper proposes PixRec - a vision-language framework that incorporates both textual attributes and product images into the recommendation pipeline. Our architecture leverages a vision-language model backbone capable of jointly processing image-text sequences, maintaining a dual-tower structure and mixed training objective while aligning multi-modal feature projections for both item-item and user-item interactions. Using the Amazon Reviews dataset augmented with product images, our experiments demonstrate $3\\times$ and 40% improvements in top-rank and top-10 rank accuracy over text-only recommenders respectively, indicating that visual features can help distinguish items with similar textual descriptions. Our work outlines future directions for scaling multi-modal recommenders training, enhancing visual-text feature fusion, and evaluating inference-time performance. This work takes a step toward building software systems utilizing visual information in sequential recommendation for real-world applications like e-commerce.",
        "url": "http://arxiv.org/abs/2601.06458v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06458v1",
        "arxiv_id": "2601.06458v1",
        "authors": [
            "Sayak Chakrabarty",
            "Souradip Pal"
        ],
        "submitted": "2026-01-10 06:52:58",
        "source": "arxiv",
        "comment": "9 pages, 2 figures",
        "score": 6,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores sequential recommendation using visual information, which is somewhat related to the user's interests in Information Retrieval and Search technologies. However, the focus on recommender systems and visual context is not a central match for the user's primary research themes, which include query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Conditional Memory via Scalable Lookup: A New Axis of Sparsity for Large Language Models",
        "abstract": "While Mixture-of-Experts (MoE) scales capacity via conditional computation, Transformers lack a native primitive for knowledge lookup, forcing them to inefficiently simulate retrieval through computation. To address this, we introduce conditional memory as a complementary sparsity axis, instantiated via Engram, a module that modernizes classic $N$-gram embedding for O(1) lookup. By formulating the Sparsity Allocation problem, we uncover a U-shaped scaling law that optimizes the trade-off between neural computation (MoE) and static memory (Engram). Guided by this law, we scale Engram to 27B parameters, achieving superior performance over a strictly iso-parameter and iso-FLOPs MoE baseline. Most notably, while the memory module is expected to aid knowledge retrieval (e.g., MMLU +3.4; CMMLU +4.0), we observe even larger gains in general reasoning (e.g., BBH +5.0; ARC-Challenge +3.7) and code/math domains~(HumanEval +3.0; MATH +2.4). Mechanistic analyses reveal that Engram relieves the backbone's early layers from static reconstruction, effectively deepening the network for complex reasoning. Furthermore, by delegating local dependencies to lookups, it frees up attention capacity for global context, substantially boosting long-context retrieval (e.g., Multi-Query NIAH: 84.2 to 97.0). Finally, Engram establishes infrastructure-aware efficiency: its deterministic addressing enables runtime prefetching from host memory, incurring negligible overhead. We envision conditional memory as an indispensable modeling primitive for next-generation sparse models.",
        "url": "http://arxiv.org/abs/2601.07372v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07372v1",
        "arxiv_id": "2601.07372v1",
        "authors": [
            "Xin Cheng",
            "Wangding Zeng",
            "Damai Dai",
            "Qinyu Chen",
            "Bingxuan Wang",
            "Zhenda Xie",
            "Kezhao Huang",
            "Xingkai Yu",
            "Zhewen Hao",
            "Yukun Li",
            "Han Zhang",
            "Huishuai Zhang",
            "Dongyan Zhao",
            "Wenfeng Liang"
        ],
        "submitted": "2026-01-12 09:54:49",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on large language models and introduces a new module called Engram for efficient knowledge lookup. While it touches on scalability and sparsity, it doesn't directly relate to information retrieval, search technologies, or user behavior modeling, which are core areas of your research interests."
    },
    {
        "title": "Reward Modeling from Natural Language Human Feedback",
        "abstract": "Reinforcement Learning with Verifiable reward (RLVR) on preference data has become the mainstream approach for training Generative Reward Models (GRMs). Typically in pairwise rewarding tasks, GRMs generate reasoning chains ending with critiques and preference labels, and RLVR then relies on the correctness of the preference labels as the training reward. However, in this paper, we demonstrate that such binary classification tasks make GRMs susceptible to guessing correct outcomes without sound critiques. Consequently, these spurious successes introduce substantial noise into the reward signal, thereby impairing the effectiveness of reinforcement learning. To address this issue, we propose Reward Modeling from Natural Language Human Feedback (RM-NLHF), which leverages natural language feedback to obtain process reward signals, thereby mitigating the problem of limited solution space inherent in binary tasks. Specifically, we compute the similarity between GRM-generated and human critiques as the training reward, which provides more accurate reward signals than outcome-only supervision. Additionally, considering that human critiques are difficult to scale up, we introduce Meta Reward Model (MetaRM) which learns to predict process reward from datasets with human critiques and then generalizes to data without human critiques. Experiments on multiple benchmarks demonstrate that our method consistently outperforms state-of-the-art GRMs trained with outcome-only reward, confirming the superiority of integrating natural language over binary human feedback as supervision.",
        "url": "http://arxiv.org/abs/2601.07349v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07349v1",
        "arxiv_id": "2601.07349v1",
        "authors": [
            "Zongqi Wang",
            "Rui Wang",
            "Yuchuan Wu",
            "Yiyao Yu",
            "Pinyi Zhang",
            "Shaoning Sun",
            "Yujiu Yang",
            "Yongbin Li"
        ],
        "submitted": "2026-01-12 09:23:43",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 7,
        "llm_reason": "This paper explores a novel approach to reward modeling in reinforcement learning, leveraging natural language human feedback to improve the accuracy of reward signals. While it doesn't directly focus on information retrieval or search technologies, it does involve aspects of NLP and deep semantic understanding, making it somewhat relevant to your research interests. However, its primary focus on reinforcement learning and reward modeling means it's not a central match for your core research themes."
    },
    {
        "title": "BayesRAG: Probabilistic Mutual Evidence Corroboration for Multimodal Retrieval-Augmented Generation",
        "abstract": "Retrieval-Augmented Generation (RAG) has become a pivotal paradigm for Large Language Models (LLMs), yet current approaches struggle with visually rich documents by treating text and images as isolated retrieval targets. Existing methods relying solely on cosine similarity often fail to capture the semantic reinforcement provided by cross-modal alignment and layout-induced coherence. To address these limitations, we propose BayesRAG, a novel multimodal retrieval framework grounded in Bayesian inference and Dempster-Shafer evidence theory. Unlike traditional approaches that rank candidates strictly by similarity, BayesRAG models the intrinsic consistency of retrieved candidates across modalities as probabilistic evidence to refine retrieval confidence. Specifically, our method computes the posterior association probability for combinations of multimodal retrieval results, prioritizing text-image pairs that mutually corroborate each other in terms of both semantics and layout. Extensive experiments demonstrate that BayesRAG significantly outperforms state-of-the-art (SOTA) methods on challenging multimodal benchmarks. This study establishes a new paradigm for multimodal retrieval fusion that effectively resolves the isolation of heterogeneous modalities through an evidence fusion mechanism and enhances the robustness of retrieval outcomes. Our code is available at https://github.com/TioeAre/BayesRAG.",
        "url": "http://arxiv.org/abs/2601.07329v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07329v1",
        "arxiv_id": "2601.07329v1",
        "authors": [
            "Xuan Li",
            "Yining Wang",
            "Haocai Luo",
            "Shengping Liu",
            "Jerry Liang",
            "Ying Fu",
            "Weihuang",
            "Jun Yu",
            "Junnan Zhu"
        ],
        "submitted": "2026-01-12 08:53:14",
        "source": "arxiv",
        "comment": "17 pages, 8 figures",
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper proposes a novel multimodal retrieval framework, BayesRAG, which effectively addresses the limitations of current approaches in capturing cross-modal alignment and layout-induced coherence. Although it's not directly focused on query understanding, ranking models, or user behavior modeling, it's clearly related to information retrieval and NLP, making it relevant to your research interests. The use of Bayesian inference and Dempster-Shafer evidence theory is particularly interesting in the context of multimodal retrieval."
    },
    {
        "title": "Towards Multi-Behavior Multi-Task Recommendation via Behavior-informed Graph Embedding Learning",
        "abstract": "Multi-behavior recommendation (MBR) aims to improve the performance w.r.t. the target behavior (i.e., purchase) by leveraging auxiliary behaviors (e.g., click, favourite). However, in real-world scenarios, a recommendation method often needs to process different types of behaviors and generate personalized lists for each task (i.e., each behavior type). Such a new recommendation problem is referred to as multi-behavior multi-task recommendation (MMR). So far, the most powerful MBR methods usually model multi-behavior interactions using a cascading graph paradigm. Although significant progress has been made in optimizing the performance of the target behavior, it often neglects the performance of auxiliary behaviors. To compensate for the deficiencies of the cascading paradigm, we propose a novel solution for MMR, i.e., behavior-informed graph embedding learning (BiGEL). Specifically, we first obtain a set of behavior-aware embeddings by using a cascading graph paradigm. Subsequently, we introduce three key modules to improve the performance of the model. The cascading gated feedback (CGF) module enables a feedback-driven optimization process by integrating feedback from the target behavior to refine the auxiliary behaviors preferences. The global context enhancement (GCE) module integrates the global context to maintain the user's overall preferences, preventing the loss of key preferences due to individual behavior graph modeling. Finally, the contrastive preference alignment (CPA) module addresses the potential changes in user preferences during the cascading process by aligning the preferences of the target behaviors with the global preferences through contrastive learning. Extensive experiments on two real-world datasets demonstrate the effectiveness of our BiGEL compared with ten very competitive methods.",
        "url": "http://arxiv.org/abs/2601.07294v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07294v1",
        "arxiv_id": "2601.07294v1",
        "authors": [
            "Wenhao Lai",
            "Weike Pan",
            "Zhong Ming"
        ],
        "submitted": "2026-01-12 08:04:08",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'click' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval and Search technologies, specifically in the area of recommender systems. However, the focus on multi-behavior multi-task recommendation and graph embedding learning is not directly aligned with your primary interests in query understanding, ranking models, and user behavior modeling in the context of search technologies."
    },
    {
        "title": "N2N-GQA: Noise-to-Narrative for Graph-Based Table-Text Question Answering Using LLMs",
        "abstract": "Multi-hop question answering over hybrid table-text data requires retrieving and reasoning across multiple evidence pieces from large corpora, but standard Retrieval-Augmented Generation (RAG) pipelines process documents as flat ranked lists, causing retrieval noise to obscure reasoning chains. We introduce N2N-GQA. To our knowledge, it is the first zeroshot framework for open-domain hybrid table-text QA that constructs dynamic evidence graphs from noisy retrieval outputs. Our key insight is that multi-hop reasoning requires understanding relationships between evidence pieces: by modeling documents as graph nodes with semantic relationships as edges, we identify bridge documents connecting reasoning steps, a capability absent in list-based retrieval. On OTT-QA, graph-based evidence curation provides a 19.9-point EM improvement over strong baselines, demonstrating that organizing retrieval results as structured graphs is critical for multihop reasoning. N2N-GQA achieves 48.80 EM, matching finetuned retrieval models (CORE: 49.0 EM) and approaching heavily optimized systems (COS: 56.9 EM) without any task specific training. This establishes graph-structured evidence organization as essential for scalable, zero-shot multi-hop QA systems and demonstrates that simple, interpretable graph construction can rival sophisticated fine-tuned approaches.",
        "url": "http://arxiv.org/abs/2601.06603v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06603v1",
        "arxiv_id": "2601.06603v1",
        "authors": [
            "Mohamed Sharafath",
            "Aravindh Annamalai",
            "Ganesh Murugan",
            "Aravindakumar Venugopalan"
        ],
        "submitted": "2026-01-10 15:55:15",
        "source": "arxiv",
        "comment": "Accepted at an AAAI 2026 Workshop",
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores a novel approach to question answering using graph-based evidence organization, which is somewhat related to information retrieval and NLP. However, the focus on graph-based table-text question answering and the use of LLMs is not directly aligned with the user's core research themes of query understanding, ranking models, and user behavior modeling. While the paper's emphasis on real-time relevance optimization is relevant, it is not a central match for the user's interests."
    },
    {
        "title": "Value of Information: A Framework for Human-Agent Communication",
        "abstract": "Large Language Model (LLM) agents deployed for real-world tasks face a fundamental dilemma: user requests are underspecified, yet agents must decide whether to act on incomplete information or interrupt users for clarification. Existing approaches either rely on brittle confidence thresholds that require task-specific tuning, or fail to account for the varying stakes of different decisions. We introduce a decision-theoretic framework that resolves this trade-off through the Value of Information (VoI), enabling agents to dynamically weigh the expected utility gain from asking questions against the cognitive cost imposed on users. Our inference-time method requires no hyperparameter tuning and adapts seamlessly across contexts-from casual games to medical diagnosis. Experiments across four diverse domains (20 Questions, medical diagnosis, flight booking, and e-commerce) show that VoI consistently matches or exceeds the best manually-tuned baselines, achieving up to 1.36 utility points higher in high-cost settings. This work provides a parameter-free framework for adaptive agent communication that explicitly balances task risk, query ambiguity, and user effort.",
        "url": "http://arxiv.org/abs/2601.06407v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06407v1",
        "arxiv_id": "2601.06407v1",
        "authors": [
            "Yijiang River Dong",
            "Tiancheng Hu",
            "Zheng Hui",
            "Caiqi Zhang",
            "Ivan VuliÄ",
            "Andreea Bobu",
            "Nigel Collier"
        ],
        "submitted": "2026-01-10 03:07:41",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and user behavior modeling. The Value of Information framework introduced in the paper addresses the trade-off between acting on incomplete information and interrupting users for clarification, which is a critical aspect of search technologies. The paper's focus on real-time relevance optimization and its application across diverse domains also aligns with your interests."
    },
    {
        "title": "Monkey Jump : MoE-Style PEFT for Efficient Multi-Task Learning",
        "abstract": "Mixture-of-experts variants of parameter-efficient fine-tuning enable per-token specialization, but they introduce additional trainable routers and expert parameters, increasing memory usage and training cost. This undermines the core goal of parameter-efficient fine-tuning. We propose Monkey Jump, a method that brings mixture-of-experts-style specialization to parameter-efficient fine-tuning without introducing extra trainable parameters for experts or routers. Instead of adding new adapters as experts, Monkey Jump treats the adapters already present in each Transformer block (such as query, key, value, up, and down projections) as implicit experts and routes tokens among them. Routing is performed using k-means clustering with exponentially moving averaged cluster centers, requiring no gradients and no learned parameters. We theoretically show that token-wise routing increases expressivity and can outperform shared adapters by avoiding cancellation effects. Across multi-task experiments covering 14 text, 14 image, and 19 video benchmarks, Monkey Jump achieves competitive performance with mixture-of-experts-based parameter-efficient fine-tuning methods while using 7 to 29 times fewer trainable parameters, up to 48 percent lower memory consumption, and 1.5 to 2 times faster training. Monkey Jump is architecture-agnostic and can be applied to any adapter-based parameter-efficient fine-tuning method.",
        "url": "http://arxiv.org/abs/2601.06356v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06356v1",
        "arxiv_id": "2601.06356v1",
        "authors": [
            "Nusrat Jahan Prottasha",
            "Md Kowsher",
            "Chun-Nam Yu",
            "Chen Chen",
            "Ozlem Garibay"
        ],
        "submitted": "2026-01-09 23:42:08",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on parameter-efficient fine-tuning methods for multi-task learning, which is outside the scope of your primary research interests in Information Retrieval and Search technologies. Although it involves NLP, the topic is more related to model optimization and architecture, rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "ES-Mem: Event Segmentation-Based Memory for Long-Term Dialogue Agents",
        "abstract": "Memory is critical for dialogue agents to maintain coherence and enable continuous adaptation in long-term interactions. While existing memory mechanisms offer basic storage and retrieval capabilities, they are hindered by two primary limitations: (1) rigid memory granularity often disrupts semantic integrity, resulting in fragmented and incoherent memory units; (2) prevalent flat retrieval paradigms rely solely on surface-level semantic similarity, neglecting the structural cues of discourse required to navigate and locate specific episodic contexts. To mitigate these limitations, drawing inspiration from Event Segmentation Theory, we propose ES-Mem, a framework incorporating two core components: (1) a dynamic event segmentation module that partitions long-term interactions into semantically coherent events with distinct boundaries; (2) a hierarchical memory architecture that constructs multi-layered memories and leverages boundary semantics to anchor specific episodic memory for precise context localization. Evaluations on two memory benchmarks demonstrate that ES-Mem yields consistent performance gains over baseline methods. Furthermore, the proposed event segmentation module exhibits robust applicability on dialogue segmentation datasets.",
        "url": "http://arxiv.org/abs/2601.07582v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07582v1",
        "arxiv_id": "2601.07582v1",
        "authors": [
            "Huhai Zou",
            "Tianhao Sun",
            "Chuanjiang He",
            "Yu Tian",
            "Zhenyang Li",
            "Li Jin",
            "Nayu Liu",
            "Jiang Zhong",
            "Kaiwen Wei"
        ],
        "submitted": "2026-01-12 14:33:32",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on memory mechanisms for dialogue agents, which is somewhat related to information retrieval, but the specific context and application are quite different from the user's core research themes. The paper's emphasis on event segmentation and hierarchical memory architecture does not directly align with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "From RAG to Agentic RAG for Faithful Islamic Question Answering",
        "abstract": "LLMs are increasingly used for Islamic question answering, where ungrounded responses may carry serious religious consequences. Yet standard MCQ/MRC-style evaluations do not capture key real-world failure modes, notably free-form hallucinations and whether models appropriately abstain when evidence is lacking. To shed a light on this aspect we introduce ISLAMICFAITHQA, a 3,810-item bilingual (Arabic/English) generative benchmark with atomic single-gold answers, which enables direct measurement of hallucination and abstention. We additionally developed an end-to-end grounded Islamic modelling suite consisting of (i) 25K Arabic text-grounded SFT reasoning pairs, (ii) 5K bilingual preference samples for reward-guided alignment, and (iii) a verse-level Qur'an retrieval corpus of $\\sim$6k atomic verses (ayat). Building on these resources, we develop an agentic Quran-grounding framework (agentic RAG) that uses structured tool calls for iterative evidence seeking and answer revision. Experiments across Arabic-centric and multilingual LLMs show that retrieval improves correctness and that agentic RAG yields the largest gains beyond standard RAG, achieving state-of-the-art performance and stronger Arabic-English robustness even with a small model (i.e., Qwen3 4B). We will make the experimental resources and datasets publicly available for the community.",
        "url": "http://arxiv.org/abs/2601.07528v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07528v1",
        "arxiv_id": "2601.07528v1",
        "authors": [
            "Gagan Bhatia",
            "Hamdy Mubarak",
            "Mustafa Jarrar",
            "George Mikros",
            "Fadi Zaraket",
            "Mahmoud Alhirthani",
            "Mutaz Al-Khatib",
            "Logan Cochrane",
            "Kareem Darwish",
            "Rashid Yahiaoui",
            "Firoj Alam"
        ],
        "submitted": "2026-01-12 13:28:28",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Islamic question answering, which is not directly related to the user's core research themes in Information Retrieval and Search technologies. While it involves natural language processing and query understanding, the specific domain and application are not aligned with the user's interests."
    },
    {
        "title": "Fine-Tuning vs. RAG for Multi-Hop Question Answering with Novel Knowledge",
        "abstract": "Multi-hop question answering is widely used to evaluate the reasoning capabilities of large language models (LLMs), as it requires integrating multiple pieces of supporting knowledge to arrive at a correct answer. While prior work has explored different mechanisms for providing knowledge to LLMs, such as finetuning and retrieval-augmented generation (RAG), their relative effectiveness for multi-hop question answering remains insufficiently understood, particularly when the required knowledge is temporally novel.\n  In this paper, we systematically compare parametric and non-parametric knowledge injection methods for open-domain multi-hop question answering. We evaluate unsupervised fine-tuning (continual pretraining), supervised fine-tuning, and retrieval-augmented generation across three 7B-parameter open-source LLMs. Experiments are conducted on two benchmarks: QASC, a standard multi-hop science question answering dataset, and a newly constructed dataset of over 10,000 multi-hop questions derived from Wikipedia events in 2024, designed to test knowledge beyond the models' pretraining cutoff.\n  Our results show that unsupervised fine-tuning provides only limited gains over base models, suggesting that continual pretraining alone is insufficient for improving multi-hop reasoning accuracy. In contrast, retrieval-augmented generation yields substantial and consistent improvements, particularly when answering questions that rely on temporally novel information. Supervised fine-tuning achieves the highest overall accuracy across models and datasets. These findings highlight fundamental differences in how knowledge injection mechanisms support multi-hop question answering and underscore the importance of retrieval-based methods when external or compositional knowledge is required.",
        "url": "http://arxiv.org/abs/2601.07054v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07054v1",
        "arxiv_id": "2601.07054v1",
        "authors": [
            "Zhuoyi Yang",
            "Yurun Song",
            "Iftekhar Ahmed",
            "Ian Harris"
        ],
        "submitted": "2026-01-11 20:24:25",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores knowledge injection methods for multi-hop question answering, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on large language models and knowledge retrieval-augmented generation is not directly aligned with the user's primary research themes, but still shows some relevance to deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "An Ubuntu-Guided Large Language Model Framework for Cognitive Behavioral Mental Health Dialogue",
        "abstract": "South Africa's escalating mental health crisis, compounded by limited access to culturally responsive care, calls for innovative and contextually grounded interventions. While large language models show considerable promise for mental health support, their predominantly Western-centric training data limit cultural and linguistic applicability in African contexts. This study introduces a proof-of-concept framework that integrates cognitive behavioral therapy with the African philosophy of Ubuntu to create a culturally sensitive, emotionally intelligent, AI-driven mental health dialogue system. Guided by a design science research methodology, the framework applies both deep theoretical and therapeutic adaptations as well as surface-level linguistic and communicative cultural adaptations. Key CBT techniques, including behavioral activation and cognitive restructuring, were reinterpreted through Ubuntu principles that emphasize communal well-being, spiritual grounding, and interconnectedness. A culturally adapted dataset was developed through iterative processes of language simplification, spiritual contextualization, and Ubuntu-based reframing. The fine-tuned model was evaluated through expert-informed case studies, employing UniEval for conversational quality assessment alongside additional measures of CBT reliability and cultural linguistic alignment. Results demonstrate that the model effectively engages in empathetic, context-aware dialogue aligned with both therapeutic and cultural objectives. Although real-time end-user testing has not yet been conducted, the model underwent rigorous review and supervision by domain specialist clinical psychologists. The findings highlight the potential of culturally embedded emotional intelligence to enhance the contextual relevance, inclusivity, and effectiveness of AI-driven mental health interventions across African settings.",
        "url": "http://arxiv.org/abs/2601.06875v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06875v1",
        "arxiv_id": "2601.06875v1",
        "authors": [
            "Sontaga G. Forane",
            "Absalom E. Ezugwu",
            "Kevin Igwe",
            "Karen van den Berg"
        ],
        "submitted": "2026-01-11 11:50:18",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. Although it involves a large language model, its focus is on mental health dialogue systems and cultural sensitivity, which is not a central match to your areas of interest."
    },
    {
        "title": "Unleashing the Native Recommendation Potential: LLM-Based Generative Recommendation via Structured Term Identifiers",
        "abstract": "Leveraging the vast open-world knowledge and understanding capabilities of Large Language Models (LLMs) to develop general-purpose, semantically-aware recommender systems has emerged as a pivotal research direction in generative recommendation. However, existing methods face bottlenecks in constructing item identifiers. Text-based methods introduce LLMs' vast output space, leading to hallucination, while methods based on Semantic IDs (SIDs) encounter a semantic gap between SIDs and LLMs' native vocabulary, requiring costly vocabulary expansion and alignment training. To address this, this paper introduces Term IDs (TIDs), defined as a set of semantically rich and standardized textual keywords, to serve as robust item identifiers. We propose GRLM, a novel framework centered on TIDs, employs Context-aware Term Generation to convert item's metadata into standardized TIDs and utilizes Integrative Instruction Fine-tuning to collaboratively optimize term internalization and sequential recommendation. Additionally, Elastic Identifier Grounding is designed for robust item mapping. Extensive experiments on real-world datasets demonstrate that GRLM significantly outperforms baselines across multiple scenarios, pointing a promising direction for generalizable and high-performance generative recommendation systems.",
        "url": "http://arxiv.org/abs/2601.06798v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06798v1",
        "arxiv_id": "2601.06798v1",
        "authors": [
            "Zhiyang Zhang",
            "Junda She",
            "Kuo Cai",
            "Bo Chen",
            "Shiyao Wang",
            "Xinchen Luo",
            "Qiang Luo",
            "Ruiming Tang",
            "Han Li",
            "Kun Gai",
            "Guorui Zhou"
        ],
        "submitted": "2026-01-11 07:53:20",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores recommender systems using Large Language Models (LLMs), which is somewhat related to your interests in Information Retrieval and NLP. However, the focus on recommender systems and the use of LLMs for this purpose is not a central match to your primary research themes, which include query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "MedRAGChecker: Claim-Level Verification for Biomedical Retrieval-Augmented Generation",
        "abstract": "Biomedical retrieval-augmented generation (RAG) can ground LLM answers in medical literature, yet long-form outputs often contain isolated unsupported or contradictory claims with safety implications.\n  We introduce MedRAGChecker, a claim-level verification and diagnostic framework for biomedical RAG.\n  Given a question, retrieved evidence, and a generated answer, MedRAGChecker decomposes the answer into atomic claims and estimates claim support by combining evidence-grounded natural language inference (NLI) with biomedical knowledge-graph (KG) consistency signals.\n  Aggregating claim decisions yields answer-level diagnostics that help disentangle retrieval and generation failures, including faithfulness, under-evidence, contradiction, and safety-critical error rates.\n  To enable scalable evaluation, we distill the pipeline into compact biomedical models and use an ensemble verifier with class-specific reliability weighting.\n  Experiments on four biomedical QA benchmarks show that MedRAGChecker reliably flags unsupported and contradicted claims and reveals distinct risk profiles across generators, particularly on safety-critical biomedical relations.",
        "url": "http://arxiv.org/abs/2601.06519v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06519v1",
        "arxiv_id": "2601.06519v1",
        "authors": [
            "Yuelyu Ji",
            "Min Gu Kwak",
            "Hang Zhang",
            "Xizhi Wu",
            "Chenyu Li",
            "Yanshan Wang"
        ],
        "submitted": "2026-01-10 10:40:42",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on biomedical retrieval-augmented generation, which is somewhat related to information retrieval and search technologies. However, the primary focus on claim-level verification and diagnostic framework for biomedical RAG is not directly aligned with the user's core research themes of query understanding, ranking models, and user behavior modeling. While it touches on natural language processing, it is not a central match for the user's interests."
    },
    {
        "title": "Spec-o3: A Tool-Augmented Vision-Language Agent for Rare Celestial Object Candidate Vetting via Automated Spectral Inspection",
        "abstract": "Due to the limited generalization and interpretability of deep learning classifiers, The final vetting of rare celestial object candidates still relies on expert visual inspection--a manually intensive process. In this process, astronomers leverage specialized tools to analyze spectra and construct reliable catalogs. However, this practice has become the primary bottleneck, as it is fundamentally incapable of scaling with the data deluge from modern spectroscopic surveys. To bridge this gap, we propose Spec-o3, a tool-augmented vision-language agent that performs astronomer-aligned spectral inspection via interleaved multimodal chain-of-thought reasoning. Spec-o3 is trained with a two-stage post-training recipe: cold-start supervised fine-tuning on expert inspection trajectories followed by outcome-based reinforcement learning on rare-type verification tasks. Evaluated on five rare-object identification tasks from LAMOST, Spec-o3 establishes a new State-of-the-Art, boosting the macro-F1 score from 28.3 to 76.5 with a 7B parameter base model and outperforming both proprietary VLMs and specialized deep models. Crucially, the agent demonstrates strong generalization to unseen inspection tasks across survey shifts (from LAMOST to SDSS/DESI). Expert evaluations confirm that its reasoning traces are coherent and physically consistent, supporting transparent and trustworthy decision-making. Code, data, and models are available at \\href{https://github.com/Maxwell-Jia/spec-o3}{Project HomePage}.",
        "url": "http://arxiv.org/abs/2601.06498v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06498v1",
        "arxiv_id": "2601.06498v1",
        "authors": [
            "Minghui Jia",
            "Qichao Zhang",
            "Ali Luo",
            "Linjing Li",
            "Shuo Ye",
            "Hailing Lu",
            "Wen Hou",
            "Dongbin Zhao"
        ],
        "submitted": "2026-01-10 09:17:51",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The paper focuses on a tool-augmented vision-language agent for rare celestial object candidate vetting, which is outside your areas of expertise and interest."
    },
    {
        "title": "Gecko: An Efficient Neural Architecture Inherently Processing Sequences with Arbitrary Lengths",
        "abstract": "Designing a unified neural network to efficiently and inherently process sequential data with arbitrary lengths is a central and challenging problem in sequence modeling. The design choices in Transformer, including quadratic complexity and weak length extrapolation, have limited their ability to scale to long sequences. In this work, we propose Gecko, a neural architecture that inherits the design of Mega and Megalodon (exponential moving average with gated attention), and further introduces multiple technical components to improve its capability to capture long range dependencies, including timestep decay normalization, sliding chunk attention mechanism, and adaptive working memory. In a controlled pretraining comparison with Llama2 and Megalodon in the scale of 7 billion parameters and 2 trillion training tokens, Gecko achieves better efficiency and long-context scalability. Gecko reaches a training loss of 1.68, significantly outperforming Llama2-7B (1.75) and Megalodon-7B (1.70), and landing close to Llama2-13B (1.67). Notably, without relying on any context-extension techniques, Gecko exhibits inherent long-context processing and retrieval capabilities, stably handling sequences of up to 4 million tokens and retrieving information from contexts up to $4\\times$ longer than its attention window. Code: https://github.com/XuezheMax/gecko-llm",
        "url": "http://arxiv.org/abs/2601.06463v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06463v1",
        "arxiv_id": "2601.06463v1",
        "authors": [
            "Xuezhe Ma",
            "Shicheng Wen",
            "Linghao Jin",
            "Bilge Acun",
            "Ruihang Lai",
            "Bohan Hou",
            "Will Lin",
            "Hao Zhang",
            "Songlin Yang",
            "Ryan Lee",
            "Mengxi Wu",
            "Jonathan May",
            "Luke Zettlemoyer",
            "Carole-Jean Wu"
        ],
        "submitted": "2026-01-10 07:12:41",
        "source": "arxiv",
        "comment": "13 pages, 5 figure and 3 tables",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on developing a neural architecture for sequence modeling, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some NLP concepts, the primary goal is to improve sequence modeling efficiency, which is not a central aspect of your research."
    },
    {
        "title": "NC-Bench: An LLM Benchmark for Evaluating Conversational Competence",
        "abstract": "The Natural Conversation Benchmark (NC-Bench) introduce a new approach to evaluating the general conversational competence of large language models (LLMs). Unlike prior benchmarks that focus on the content of model behavior, NC-Bench focuses on the form and structure of natural conversation. Grounded in the IBM Natural Conversation Framework (NCF), NC-Bench comprises three distinct sets. The Basic Conversation Competence set evaluates fundamental sequence management practices, such as answering inquiries, repairing responses, and closing conversational pairs. The RAG set applies the same sequence management patterns as the first set but incorporates retrieval-augmented generation (RAG). The Complex Request set extends the evaluation to complex requests involving more intricate sequence management patterns. Each benchmark tests a model's ability to produce contextually appropriate conversational actions in response to characteristic interaction patterns. Initial evaluations across 6 open-source models and 14 interaction patterns show that models perform well on basic answering tasks, struggle more with repair tasks (especially repeat), have mixed performance on closing sequences, and find complex multi-turn requests most challenging, with Qwen models excelling on the Basic set and Granite models on the RAG set and the Complex Request set. By operationalizing fundamental principles of human conversation, NC-Bench provides a lightweight, extensible, and theory-grounded framework for assessing and improving the conversational abilities of LLMs beyond topical or task-specific benchmarks.",
        "url": "http://arxiv.org/abs/2601.06426v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06426v1",
        "arxiv_id": "2601.06426v1",
        "authors": [
            "Robert J. Moore",
            "Sungeun An",
            "Farhan Ahmed",
            "Jay Pankaj Gala"
        ],
        "submitted": "2026-01-10 04:57:24",
        "source": "arxiv",
        "comment": "9 pages, 1 figure, 2 tables",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on evaluating conversational competence of large language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the specific topic of conversational competence and benchmarking is not a central match for your research themes."
    },
    {
        "title": "Structured Episodic Event Memory",
        "abstract": "Current approaches to memory in Large Language Models (LLMs) predominantly rely on static Retrieval-Augmented Generation (RAG), which often results in scattered retrieval and fails to capture the structural dependencies required for complex reasoning. For autonomous agents, these passive and flat architectures lack the cognitive organization necessary to model the dynamic and associative nature of long-term interaction. To address this, we propose Structured Episodic Event Memory (SEEM), a hierarchical framework that synergizes a graph memory layer for relational facts with a dynamic episodic memory layer for narrative progression. Grounded in cognitive frame theory, SEEM transforms interaction streams into structured Episodic Event Frames (EEFs) anchored by precise provenance pointers. Furthermore, we introduce an agentic associative fusion and Reverse Provenance Expansion (RPE) mechanism to reconstruct coherent narrative contexts from fragmented evidence. Experimental results on the LoCoMo and LongMemEval benchmarks demonstrate that SEEM significantly outperforms baselines, enabling agents to maintain superior narrative coherence and logical consistency.",
        "url": "http://arxiv.org/abs/2601.06411v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06411v1",
        "arxiv_id": "2601.06411v1",
        "authors": [
            "Zhengxuan Lu",
            "Dongfang Li",
            "Yukun Shi",
            "Beilun Wang",
            "Longyue Wang",
            "Baotian Hu"
        ],
        "submitted": "2026-01-10 03:17:25",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the Structured Episodic Event Memory framework, which is relevant to Natural Language Processing (NLP) and cognitive architectures. However, it does not directly relate to Information Retrieval (IR) or Search technologies, which are the primary focus of your research interests. The paper's emphasis on cognitive organization and narrative coherence is somewhat related to user behavior modeling, but it is not a central match."
    },
    {
        "title": "BizFinBench.v2: A Unified Dual-Mode Bilingual Benchmark for Expert-Level Financial Capability Alignment",
        "abstract": "Large language models have undergone rapid evolution, emerging as a pivotal technology for intelligence in financial operations. However, existing benchmarks are often constrained by pitfalls such as reliance on simulated or general-purpose samples and a focus on singular, offline static scenarios. Consequently, they fail to align with the requirements for authenticity and real-time responsiveness in financial services, leading to a significant discrepancy between benchmark performance and actual operational efficacy. To address this, we introduce BizFinBench.v2, the first large-scale evaluation benchmark grounded in authentic business data from both Chinese and U.S. equity markets, integrating online assessment. We performed clustering analysis on authentic user queries from financial platforms, resulting in eight fundamental tasks and two online tasks across four core business scenarios, totaling 29,578 expert-level Q&A pairs. Experimental results demonstrate that ChatGPT-5 achieves a prominent 61.5% accuracy in main tasks, though a substantial gap relative to financial experts persists; in online tasks, DeepSeek-R1 outperforms all other commercial LLMs. Error analysis further identifies the specific capability deficiencies of existing models within practical financial business contexts. BizFinBench.v2 transcends the limitations of current benchmarks, achieving a business-level deconstruction of LLM financial capabilities and providing a precise basis for evaluating efficacy in the widespread deployment of LLMs within the financial domain. The data and code are available at https://github.com/HiThink-Research/BizFinBench.v2.",
        "url": "http://arxiv.org/abs/2601.06401v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06401v1",
        "arxiv_id": "2601.06401v1",
        "authors": [
            "Xin Guo",
            "Rongjunchen Zhang",
            "Guilong Lu",
            "Xuntao Guo",
            "Shuai Jia",
            "Zhi Yang",
            "Liwen Zhang"
        ],
        "submitted": "2026-01-10 02:51:53",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on evaluating large language models in the financial domain, which is somewhat related to information retrieval, but it does not directly address query understanding, ranking models, or user behavior modeling. The paper's emphasis on benchmarking and evaluating LLMs in a specific domain does not align with the user's primary research interests in IR and NLP."
    },
    {
        "title": "How well can off-the-shelf LLMs elucidate molecular structures from mass spectra using chain-of-thought reasoning?",
        "abstract": "Mass spectrometry (MS) is a powerful analytical technique for identifying small molecules, yet determining complete molecular structures directly from tandem mass spectra (MS/MS) remains a long-standing challenge due to complex fragmentation patterns and the vast diversity of chemical space. Recent progress in large language models (LLMs) has shown promise for reasoning-intensive scientific tasks, but their capability for chemical interpretation is still unclear. In this work, we introduce a Chain-of-Thought (CoT) prompting framework and benchmark that evaluate how LLMs reason about mass spectral data to predict molecular structures. We formalize expert chemists' reasoning steps-such as double bond equivalent (DBE) analysis, neutral loss identification, and fragment assembly-into structured prompts and assess multiple state-of-the-art LLMs (Claude-3.5-Sonnet, GPT-4o-mini, and Llama-3 series) in a zero-shot setting using the MassSpecGym dataset. Our evaluation across metrics of SMILES validity, formula consistency, and structural similarity reveals that while LLMs can produce syntactically valid and partially plausible structures, they fail to achieve chemical accuracy or link reasoning to correct molecular predictions. These findings highlight both the interpretive potential and the current limitations of LLM-based reasoning for molecular elucidation, providing a foundation for future work that combines domain knowledge and reinforcement learning to achieve chemically grounded AI reasoning.",
        "url": "http://arxiv.org/abs/2601.06289v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06289v1",
        "arxiv_id": "2601.06289v1",
        "authors": [
            "Yufeng Wang",
            "Lu Wei",
            "Lin Liu",
            "Hao Xu",
            "Haibin Ling"
        ],
        "submitted": "2026-01-09 20:08:42",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on the application of large language models in chemistry for molecular structure elucidation, which is outside your core areas of Information Retrieval, Search technologies, and Natural Language Processing."
    },
    {
        "title": "Amory: Building Coherent Narrative-Driven Agent Memory through Agentic Reasoning",
        "abstract": "Long-term conversational agents face a fundamental scalability challenge as interactions extend over time: repeatedly processing entire conversation histories becomes computationally prohibitive. Current approaches attempt to solve this through memory frameworks that predominantly fragment conversations into isolated embeddings or graph representations and retrieve relevant ones in a RAG style. While computationally efficient, these methods often treat memory formation minimally and fail to capture the subtlety and coherence of human memory. We introduce Amory, a working memory framework that actively constructs structured memory representations through enhancing agentic reasoning during offline time. Amory organizes conversational fragments into episodic narratives, consolidates memories with momentum, and semanticizes peripheral facts into semantic memory. At retrieval time, the system employs coherence-driven reasoning over narrative structures. Evaluated on the LOCOMO benchmark for long-term reasoning, Amory achieves considerable improvements over previous state-of-the-art, with performance comparable to full context reasoning while reducing response time by 50%. Analysis shows that momentum-aware consolidation significantly enhances response quality, while coherence-driven retrieval provides superior memory coverage compared to embedding-based approaches.",
        "url": "http://arxiv.org/abs/2601.06282v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06282v1",
        "arxiv_id": "2601.06282v1",
        "authors": [
            "Yue Zhou",
            "Xiaobo Guo",
            "Belhassen Bayar",
            "Srinivasan H. Sengamedu"
        ],
        "submitted": "2026-01-09 19:51:11",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on conversational agents and memory frameworks, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve some NLP aspects, the primary focus is on long-term conversational agents and memory formation, which is not a central match for your research themes."
    },
    {
        "title": "OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent",
        "abstract": "While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.",
        "url": "http://arxiv.org/abs/2601.07779v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07779v1",
        "arxiv_id": "2601.07779v1",
        "authors": [
            "Bowen Yang",
            "Kaiming Jin",
            "Zhenyu Wu",
            "Zhaoyang Liu",
            "Qiushi Sun",
            "Zehao Li",
            "JingJing Xie",
            "Zhoumianze Liu",
            "Fangzhi Xu",
            "Kanzhi Cheng",
            "Qingyun Li",
            "Yian Wang",
            "Yu Qiao",
            "Zun Wang",
            "Zichen Ding"
        ],
        "submitted": "2026-01-12 17:55:51",
        "source": "arxiv",
        "comment": "31 pages, 11 figures, 12 tables",
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Computer-Using Agents and Vision-Language Models, which is not directly related to your core research interests in Information Retrieval and Search technologies. While it involves multimodal search and retrieval, the context is quite different from your typical areas of focus."
    },
    {
        "title": "Structure First, Reason Next: Enhancing a Large Language Model using Knowledge Graph for Numerical Reasoning in Financial Documents",
        "abstract": "Numerical reasoning is an important task in the analysis of financial documents. It helps in understanding and performing numerical predictions with logical conclusions for the given query seeking answers from financial texts. Recently, Large Language Models (LLMs) have shown promising results in multiple Question-Answering (Q-A) systems with the capability of logical reasoning. As documents related to finance often consist of long and complex financial contexts, LLMs appear well-suited for building high-quality automated financial question-answering systems. However, LLMs often face challenges in accurately processing the various numbers within financial reports. Extracting numerical data from unstructured text and semi-structured tables, and reliably performing accurate calculations, remains a significant bottleneck for numerical reasoning in most state-of-the-art LLMs. Recent studies have shown that structured data augmentations, such as Knowledge Graphs (KGs), have notably improved the predictions of LLMs along with logical explanations. Thus, it is an important requirement to consider inherent structured information in financial reports while using LLMs for various financial analytics. This paper proposes a framework to incorporate structured information using KGs along with LLM predictions for numerical reasoning tasks. The KGs are extracted using a proposed schema inherently from the document under processing. We evaluated our proposed framework over the benchmark data FinQA, using an open-source LLM, namely Llama 3.1 8B Instruct. We observed that the proposed framework improved execution accuracy by approximately 12% relative to the vanilla LLM.",
        "url": "http://arxiv.org/abs/2601.07754v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07754v1",
        "arxiv_id": "2601.07754v1",
        "authors": [
            "Aryan Mishra",
            "Akash Anil"
        ],
        "submitted": "2026-01-12 17:39:08",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to the user's interests in Information Retrieval, particularly in the context of query understanding and ranking models. However, the focus on numerical reasoning and knowledge graphs in financial documents is not a central match to the user's primary research themes. The paper's relevance to NLP and data mining is also present, but not strongly emphasized."
    },
    {
        "title": "AptaFind: A lightweight local interface for automated aptamer curation from scientific literature",
        "abstract": "Aptamer researchers face a literature landscape scattered across publications, supplements, and databases, with each search consuming hours that could be spent at the bench. AptaFind transforms this navigation problem through a three-tier intelligence architecture that recognizes research mining is a spectrum, not a binary success or failure. The system delivers direct sequence extraction when possible, curated research leads when extraction fails, and exhaustive literature discovery for additional confidence. By combining local language models for semantic understanding with deterministic algorithms for reliability, AptaFind operates without cloud dependencies or subscription barriers. Validation across 300 University of Texas Aptamer Database targets demonstrates 84 % with some literature found, 84 % with curated research leads, and 79 % with a direct sequence extraction, at a laptop-compute rate of over 900 targets an hour. The platform proves that even when direct sequence extraction fails, automation can still deliver the actionable intelligence researchers need by rapidly narrowing the search to high quality references.",
        "url": "http://arxiv.org/abs/2601.07684v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07684v1",
        "arxiv_id": "2601.07684v1",
        "authors": [
            "Geoffrey Taghon"
        ],
        "submitted": "2026-01-12 16:16:25",
        "source": "arxiv",
        "comment": "for associated source code, see https://github.com/usnistgov/aptafind",
        "score": 3,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. It focuses on aptamer curation from scientific literature, which is outside your primary areas of interest."
    },
    {
        "title": "Adaptive Layer Selection for Layer-Wise Token Pruning in LLM Inference",
        "abstract": "Due to the prevalence of large language models (LLMs), key-value (KV) cache reduction for LLM inference has received remarkable attention. Among numerous works that have been proposed in recent years, layer-wise token pruning approaches, which select a subset of tokens at particular layers to retain in KV cache and prune others, are one of the most popular schemes. They primarily adopt a set of pre-defined layers, at which tokens are selected. Such design is inflexible in the sense that the accuracy significantly varies across tasks and deteriorates in harder tasks such as KV retrieval. In this paper, we propose ASL, a training-free method that adaptively chooses the selection layer for KV cache reduction, exploiting the variance of token ranks ordered by attention score. The proposed method balances the performance across different tasks while meeting the user-specified KV budget requirement. ASL operates during the prefilling stage and can be jointly used with existing KV cache reduction methods such as SnapKV to optimize the decoding stage. By evaluations on the InfiniteBench, RULER, and NIAH benchmarks, we show that equipped with one-shot token selection, where tokens are selected at a layer and propagated to deeper layers, ASL outperforms state-of-the-art layer-wise token selection methods in accuracy while maintaining decoding speed and KV cache reduction.",
        "url": "http://arxiv.org/abs/2601.07667v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07667v1",
        "arxiv_id": "2601.07667v1",
        "authors": [
            "Rei Taniguchi",
            "Yuyang Dong",
            "Makoto Onizuka",
            "Chuan Xiao"
        ],
        "submitted": "2026-01-12 15:47:35",
        "source": "arxiv",
        "comment": "Source code is available at https://github.com/TANIGUCHIREI/ASL",
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on optimizing large language model inference using a key-value cache reduction method, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves natural language processing, the context is more aligned with model optimization rather than deep semantic understanding or real-time relevance optimization."
    },
    {
        "title": "Loci Similes: A Benchmark for Extracting Intertextualities in Latin Literature",
        "abstract": "Tracing connections between historical texts is an important part of intertextual research, enabling scholars to reconstruct the virtual library of a writer and identify the sources influencing their creative process. These intertextual links manifest in diverse forms, ranging from direct verbatim quotations to subtle allusions and paraphrases disguised by morphological variation. Language models offer a promising path forward due to their capability of capturing semantic similarity beyond lexical overlap. However, the development of new methods for this task is held back by the scarcity of standardized benchmarks and easy-to-use datasets. We address this gap by introducing Loci Similes, a benchmark for Latin intertextuality detection comprising of a curated dataset of ~172k text segments containing 545 expert-verified parallels linking Late Antique authors to a corpus of classical authors. Using this data, we establish baselines for retrieval and classification of intertextualities with state-of-the-art LLMs.",
        "url": "http://arxiv.org/abs/2601.07533v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07533v1",
        "arxiv_id": "2601.07533v1",
        "authors": [
            "Julian Schelb",
            "Michael Wittweiler",
            "Marie Revellio",
            "Barbara Feichtinger",
            "Andreas Spitz"
        ],
        "submitted": "2026-01-12 13:34:49",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing, as it focuses on a specific domain (Latin Literature) and task (intertextuality detection) that does not align with your core themes."
    },
    {
        "title": "DiffER: Diffusion Entity-Relation Modeling for Reversal Curse in Diffusion Large Language Models",
        "abstract": "The \"reversal curse\" refers to the phenomenon where large language models (LLMs) exhibit predominantly unidirectional behavior when processing logically bidirectional relationships. Prior work attributed this to autoregressive training -- predicting the next token inherently favors left-to-right information flow over genuine bidirectional knowledge associations. However, we observe that Diffusion LLMs (DLLMs), despite being trained bidirectionally, also suffer from the reversal curse. To investigate the root causes, we conduct systematic experiments on DLLMs and identify three key reasons: 1) entity fragmentation during training, 2) data asymmetry, and 3) missing entity relations. Motivated by the analysis of these reasons, we propose Diffusion Entity-Relation Modeling (DiffER), which addresses the reversal curse through entity-aware training and balanced data construction. Specifically, DiffER introduces whole-entity masking, which mitigates entity fragmentation by predicting complete entities in a single step. DiffER further employs distribution-symmetric and relation-enhanced data construction strategies to alleviate data asymmetry and missing relations. Extensive experiments demonstrate that DiffER effectively alleviates the reversal curse in Diffusion LLMs, offering new perspectives for future research.",
        "url": "http://arxiv.org/abs/2601.07347v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07347v1",
        "arxiv_id": "2601.07347v1",
        "authors": [
            "Shaokai He",
            "Kaiwen Wei",
            "Xinyi Zeng",
            "Xiang Chen",
            "Xue Yang",
            "Zhenyang Li",
            "Jiang Zhong",
            "Yu Tian"
        ],
        "submitted": "2026-01-12 09:22:10",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on addressing the 'reversal curse' in Diffusion Large Language Models, which is a phenomenon related to the processing of logically bidirectional relationships. While it touches on the concept of entity relations, it does not directly relate to information retrieval, query understanding, or ranking models, which are core areas of your research interests."
    },
    {
        "title": "Document-Level Zero-Shot Relation Extraction with Entity Side Information",
        "abstract": "Document-Level Zero-Shot Relation Extraction (DocZSRE) aims to predict unseen relation labels in text documents without prior training on specific relations. Existing approaches rely on Large Language Models (LLMs) to generate synthetic data for unseen labels, which poses challenges for low-resource languages like Malaysian English. These challenges include the incorporation of local linguistic nuances and the risk of factual inaccuracies in LLM-generated data. This paper introduces Document-Level Zero-Shot Relation Extraction with Entity Side Information (DocZSRE-SI) to address limitations in the existing DocZSRE approach. The DocZSRE-SI framework leverages Entity Side Information, such as Entity Mention Descriptions and Entity Mention Hypernyms, to perform ZSRE without depending on LLM-generated synthetic data. The proposed low-complexity model achieves an average improvement of 11.6% in the macro F1-Score compared to baseline models and existing benchmarks. By utilizing Entity Side Information, DocZSRE-SI offers a robust and efficient alternative to error-prone, LLM-based methods, demonstrating significant advancements in handling low-resource languages and linguistic diversity in relation extraction tasks. This research provides a scalable and reliable solution for ZSRE, particularly in contexts like Malaysian English news articles, where traditional LLM-based approaches fall short.",
        "url": "http://arxiv.org/abs/2601.07271v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07271v1",
        "arxiv_id": "2601.07271v1",
        "authors": [
            "Mohan Raj Chanthran",
            "Soon Lay Ki",
            "Ong Huey Fang",
            "Bhawani Selvaretnam"
        ],
        "submitted": "2026-01-12 07:27:30",
        "source": "arxiv",
        "comment": "Accepted to EACL 2026 Main Conference",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Natural Language Processing (NLP) and Information Retrieval (IR), particularly in the area of relation extraction and zero-shot learning. However, its focus on entity side information and low-resource languages is not directly aligned with your primary interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Solar Open Technical Report",
        "abstract": "We introduce Solar Open, a 102B-parameter bilingual Mixture-of-Experts language model for underserved languages. Solar Open demonstrates a systematic methodology for building competitive LLMs by addressing three interconnected challenges. First, to train effectively despite data scarcity for underserved languages, we synthesize 4.5T tokens of high-quality, domain-specific, and RL-oriented data. Second, we coordinate this data through a progressive curriculum jointly optimizing composition, quality thresholds, and domain coverage across 20 trillion tokens. Third, to enable reasoning capabilities through scalable RL, we apply our proposed framework SnapPO for efficient optimization. Across benchmarks in English and Korean, Solar Open achieves competitive performance, demonstrating the effectiveness of this methodology for underserved language AI development.",
        "url": "http://arxiv.org/abs/2601.07022v1",
        "pdf_url": "https://arxiv.org/pdf/2601.07022v1",
        "arxiv_id": "2601.07022v1",
        "authors": [
            "Sungrae Park",
            "Sanghoon Kim",
            "Jungho Cho",
            "Gyoungjin Gim",
            "Dawoon Jung",
            "Mikyoung Cha",
            "Eunhae Choo",
            "Taekgyu Hong",
            "Minbyul Jeong",
            "SeHwan Joo",
            "Minsoo Khang",
            "Eunwon Kim",
            "Minjeong Kim",
            "Sujeong Kim",
            "Yunsu Kim",
            "Hyeonju Lee",
            "Seunghyun Lee",
            "Sukyung Lee",
            "Siyoung Park",
            "Gyungin Shin",
            "Inseo Song",
            "Wonho Song",
            "Seonghoon Yang",
            "Seungyoun Yi",
            "Sanghoon Yoon",
            "Jeonghyun Ko",
            "Seyoung Song",
            "Keunwoo Choi",
            "Hwalsuk Lee",
            "Sunghun Kim",
            "Du-Seong Chang",
            "Kyunghyun Cho",
            "Junsuk Choe",
            "Hwaran Lee",
            "Jae-Gil Lee",
            "KyungTae Lim",
            "Alice Oh"
        ],
        "submitted": "2026-01-11 18:33:09",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'korea' (score: +1)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper appears to be focused on language models for underserved languages, which is outside the user's primary research interests in Information Retrieval and Search technologies. Although it involves NLP, the topic is not directly related to query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "RealMem: Benchmarking LLMs in Real-World Memory-Driven Interaction",
        "abstract": "As Large Language Models (LLMs) evolve from static dialogue interfaces to autonomous general agents, effective memory is paramount to ensuring long-term consistency. However, existing benchmarks primarily focus on casual conversation or task-oriented dialogue, failing to capture **\"long-term project-oriented\"** interactions where agents must track evolving goals.\n  To bridge this gap, we introduce **RealMem**, the first benchmark grounded in realistic project scenarios. RealMem comprises over 2,000 cross-session dialogues across eleven scenarios, utilizing natural user queries for evaluation.\n  We propose a synthesis pipeline that integrates Project Foundation Construction, Multi-Agent Dialogue Generation, and Memory and Schedule Management to simulate the dynamic evolution of memory. Experiments reveal that current memory systems face significant challenges in managing the long-term project states and dynamic context dependencies inherent in real-world projects.\n  Our code and datasets are available at [https://github.com/AvatarMemory/RealMemBench](https://github.com/AvatarMemory/RealMemBench).",
        "url": "http://arxiv.org/abs/2601.06966v1",
        "pdf_url": "https://arxiv.org/pdf/2601.06966v1",
        "arxiv_id": "2601.06966v1",
        "authors": [
            "Haonan Bian",
            "Zhiyuan Yao",
            "Sen Hu",
            "Zishan Xu",
            "Shaolei Zhang",
            "Yifu Guo",
            "Ziliang Yang",
            "Xueran Han",
            "Huacan Wang",
            "Ronghao Chen"
        ],
        "submitted": "2026-01-11 15:49:36",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper introduces a benchmark for evaluating memory-driven interactions in Large Language Models (LLMs), which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on memory-driven interactions and dialogue management is not a central match to the user's core research themes, which include query understanding, ranking models, and user behavior modeling."
    }
]