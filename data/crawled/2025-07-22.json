[
    {
        "title": "3LM: Bridging Arabic, STEM, and Code through Benchmarking",
        "abstract": "Arabic is one of the most widely spoken languages in the world, yet efforts\nto develop and evaluate Large Language Models (LLMs) for Arabic remain\nrelatively limited. Most existing Arabic benchmarks focus on linguistic,\ncultural, or religious content, leaving a significant gap in domains like STEM\nand code which are increasingly relevant for real-world LLM applications. To\nhelp bridge this gap, we present 3LM, a suite of three benchmarks designed\nspecifically for Arabic. The first is a set of STEM-related question-answer\npairs, naturally sourced from Arabic textbooks and educational worksheets. The\nsecond consists of synthetically generated STEM questions, created using the\nsame sources. The third benchmark focuses on code generation, built through a\ncareful translation of two widely used code benchmarks, incorporating a\nhuman-in-the-loop process with several rounds of review to ensure high-quality\nand faithful translations. We release all three benchmarks publicly to support\nthe growth of Arabic LLM research in these essential but underrepresented\nareas.",
        "url": "http://arxiv.org/abs/2507.15850v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15850v1",
        "arxiv_id": "2507.15850v1",
        "authors": [
            "Basma El Amel Boussaha",
            "Leen AlQadi",
            "Mugariya Farooq",
            "Shaikha Alsuwaidi",
            "Giulia Campesan",
            "Ahmed Alzubaidi",
            "Mohammed Alyafeai",
            "Hakim Hacid"
        ],
        "submitted": "2025-07-21 17:58:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Impact of Language Mixing on Bilingual LLM Reasoning",
        "abstract": "Proficient multilingual speakers often intentionally switch languages in the\nmiddle of a conversation. Similarly, recent reasoning-focused bilingual large\nlanguage models (LLMs) with strong capabilities in both languages exhibit\nlanguage mixing--alternating languages within their chain of thought.\nDiscouraging this behavior in DeepSeek-R1 was found to degrade accuracy,\nsuggesting that language mixing may benefit reasoning. In this work, we study\nlanguage switching in Chinese-English bilingual reasoning models. We identify\nreinforcement learning with verifiable rewards (RLVR) as the critical training\nstage that leads to language mixing. We demonstrate that language mixing can\nenhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6\npercentage points on math reasoning tasks. Additionally, a lightweight probe\ncan be trained to predict whether a potential language switch would benefit or\nharm reasoning, and when used to guide decoding, increases accuracy by up to\n6.25 percentage points. Our findings suggest that language mixing is not merely\na byproduct of multilingual training, but is a strategic reasoning behavior.",
        "url": "http://arxiv.org/abs/2507.15849v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15849v1",
        "arxiv_id": "2507.15849v1",
        "authors": [
            "Yihao Li",
            "Jiayi Xin",
            "Miranda Muqing Miao",
            "Qi Long",
            "Lyle Ungar"
        ],
        "submitted": "2025-07-21 17:56:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "GUI-G$^2$: Gaussian Reward Modeling for GUI Grounding",
        "abstract": "Graphical User Interface (GUI) grounding maps natural language instructions\nto precise interface locations for autonomous interaction. Current\nreinforcement learning approaches use binary rewards that treat elements as\nhit-or-miss targets, creating sparse signals that ignore the continuous nature\nof spatial interactions. Motivated by human clicking behavior that naturally\nforms Gaussian distributions centered on target elements, we introduce GUI\nGaussian Grounding Rewards (GUI-G$^2$), a principled reward framework that\nmodels GUI elements as continuous Gaussian distributions across the interface\nplane. GUI-G$^2$ incorporates two synergistic mechanisms: Gaussian point\nrewards model precise localization through exponentially decaying distributions\ncentered on element centroids, while coverage rewards assess spatial alignment\nby measuring the overlap between predicted Gaussian distributions and target\nregions. To handle diverse element scales, we develop an adaptive variance\nmechanism that calibrates reward distributions based on element dimensions.\nThis framework transforms GUI grounding from sparse binary classification to\ndense continuous optimization, where Gaussian distributions generate rich\ngradient signals that guide models toward optimal interaction positions.\nExtensive experiments across ScreenSpot, ScreenSpot-v2, and ScreenSpot-Pro\nbenchmarks demonstrate that GUI-G$^2$, substantially outperforms\nstate-of-the-art method UI-TARS-72B, with the most significant improvement of\n24.7% on ScreenSpot-Pro. Our analysis reveals that continuous modeling provides\nsuperior robustness to interface variations and enhanced generalization to\nunseen layouts, establishing a new paradigm for spatial reasoning in GUI\ninteraction tasks.",
        "url": "http://arxiv.org/abs/2507.15846v2",
        "pdf_url": "http://arxiv.org/pdf/2507.15846v2",
        "arxiv_id": "2507.15846v2",
        "authors": [
            "Fei Tang",
            "Zhangxuan Gu",
            "Zhengxi Lu",
            "Xuyang Liu",
            "Shuheng Shen",
            "Changhua Meng",
            "Wen Wang",
            "Wenqi Zhang",
            "Yongliang Shen",
            "Weiming Lu",
            "Jun Xiao",
            "Yueting Zhuang"
        ],
        "submitted": "2025-07-21 17:53:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning",
        "abstract": "Large reasoning models achieve remarkable performance through extensive\nchain-of-thought generation, yet exhibit significant computational inefficiency\nby applying uniform reasoning strategies regardless of problem complexity. We\npresent Hierarchical Budget Policy Optimization (HBPO), a reinforcement\nlearning framework that enables models to learn problem-specific reasoning\ndepths without sacrificing capability. HBPO addresses the fundamental challenge\nof exploration space collapse in efficiency-oriented training, where penalties\non long output length systematically bias models away from necessary long\nreasoning paths. Through hierarchical budget exploration, our approach\npartitions rollout samples into multiple subgroups with distinct token budgets,\naiming to enable efficient resource allocation while preventing degradation of\ncapability. We introduce differentiated reward mechanisms that create\nbudget-aware incentives aligned with the complexity of the problem, allowing\nmodels to discover natural correspondences between task requirements and\ncomputational effort. Extensive experiments demonstrate that HBPO reduces\naverage token usage by up to 60.6% while improving accuracy by 3.14% across\nfour reasoning benchmarks. Unlike existing methods that impose external\nconstraints or rely on discrete mode selection, HBPO exhibits emergent adaptive\nbehavior where models automatically adjust reasoning depth based on problem\ncomplexity. Our results suggest that reasoning efficiency and capability are\nnot inherently conflicting, and can be simultaneously optimized through\nappropriately structured hierarchical training that preserves exploration\ndiversity.",
        "url": "http://arxiv.org/abs/2507.15844v2",
        "pdf_url": "http://arxiv.org/pdf/2507.15844v2",
        "arxiv_id": "2507.15844v2",
        "authors": [
            "Shangke Lyu",
            "Linjuan Wu",
            "Yuchen Yan",
            "Xingyu Wu",
            "Hao Li",
            "Yongliang Shen",
            "Peisheng Jiang",
            "Weiming Lu",
            "Jun Xiao",
            "Yueting Zhuang"
        ],
        "submitted": "2025-07-21 17:52:34",
        "source": "arxiv",
        "comment": "Code: https://github.com/zju-real/hbpo Project\n  Page:https://zju-real.github.io/hbpo/"
    },
    {
        "title": "Just Ask for Music (JAM): Multimodal and Personalized Natural Language Music Recommendation",
        "abstract": "Natural language interfaces offer a compelling approach for music\nrecommendation, enabling users to express complex preferences conversationally.\nWhile Large Language Models (LLMs) show promise in this direction, their\nscalability in recommender systems is limited by high costs and latency.\nRetrieval-based approaches using smaller language models mitigate these issues\nbut often rely on single-modal item representations, overlook long-term user\npreferences, and require full model retraining, posing challenges for\nreal-world deployment. In this paper, we present JAM (Just Ask for Music), a\nlightweight and intuitive framework for natural language music recommendation.\nJAM models user-query-item interactions as vector translations in a shared\nlatent space, inspired by knowledge graph embedding methods like TransE. To\ncapture the complexity of music and user intent, JAM aggregates multimodal item\nfeatures via cross-attention and sparse mixture-of-experts. We also introduce\nJAMSessions, a new dataset of over 100k user-query-item triples with anonymized\nuser/item embeddings, uniquely combining conversational queries and user\nlong-term preferences. Our results show that JAM provides accurate\nrecommendations, produces intuitive representations suitable for practical use\ncases, and can be easily integrated with existing music recommendation stacks.",
        "url": "http://arxiv.org/abs/2507.15826v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15826v1",
        "arxiv_id": "2507.15826v1",
        "authors": [
            "Alessandro B. Melchiorre",
            "Elena V. Epure",
            "Shahed Masoudian",
            "Gustavo Escobedo",
            "Anna Hausberger",
            "Manuel Moussallam",
            "Markus Schedl"
        ],
        "submitted": "2025-07-21 17:36:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work",
        "abstract": "Publications in the AI for Good space have tended to focus on the research\nand model development that can support high-impact applications. However, very\nfew AI for Good papers discuss the process of deploying and collaborating with\nthe partner organization, and the resulting real-world impact. In this work, we\nshare details about the close collaboration with a humanitarian-to-humanitarian\n(H2H) organization and how to not only deploy the AI model in a\nresource-constrained environment, but also how to maintain it for continuous\nperformance updates, and share key takeaways for practitioners.",
        "url": "http://arxiv.org/abs/2507.15823v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15823v1",
        "arxiv_id": "2507.15823v1",
        "authors": [
            "Anton Abilov",
            "Ke Zhang",
            "Hemank Lamba",
            "Elizabeth M. Olson",
            "Joel R. Tetreault",
            "Alejandro Jaimes"
        ],
        "submitted": "2025-07-21 17:30:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Small LLMs Do Not Learn a Generalizable Theory of Mind via Reinforcement Learning",
        "abstract": "Recent advancements in large language models (LLMs) have demonstrated\nemergent capabilities in complex reasoning, largely spurred by rule-based\nReinforcement Learning (RL) techniques applied during the post-training. This\nhas raised the question of whether similar methods can instill more nuanced,\nhuman-like social intelligence, such as a Theory of Mind (ToM), in LLMs. This\npaper investigates whether small-scale LLMs can acquire a robust and\ngeneralizable ToM capability through RL with verifiable rewards (RLVR). We\nconduct a systematic evaluation by training models on various combinations of\nprominent ToM datasets (HiToM, ExploreToM, FANToM) and testing for\ngeneralization on held-out datasets (e.g., OpenToM). Our findings indicate that\nsmall LLMs struggle to develop a generic ToM capability. While performance on\nin-distribution tasks improves, this capability fails to transfer to unseen ToM\ntasks with different characteristics. Furthermore, we demonstrate that\nprolonged RL training leads to models ``hacking'' the statistical patterns of\nthe training datasets, resulting in significant performance gains on in-domain\ndata but no change, or degradation of performance on out-of-distribution tasks.\nThis suggests the learned behavior is a form of narrow overfitting rather than\nthe acquisition of a true, abstract ToM capability.",
        "url": "http://arxiv.org/abs/2507.15788v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15788v1",
        "arxiv_id": "2507.15788v1",
        "authors": [
            "Sneheel Sarangi",
            "Hanan Salam"
        ],
        "submitted": "2025-07-21 16:47:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Reservoir Computing as a Language Model",
        "abstract": "Large Language Models (LLM) have dominated the science and media landscape\nduo to their impressive performance on processing large chunks of data and\nproduce human-like levels of text. Nevertheless, their huge energy demand and\nslow processing still a bottleneck for further increasing quality while also\nmaking the models accessible to everyone. To solve this bottleneck, we will\ninvestigate how reservoir computing performs on natural text processing, which\ncould enable fast and energy efficient hardware implementations. Studies\ninvestigating the use of reservoir computing as a language model remain sparse.\nIn this paper, we compare three distinct approaches for character-level\nlanguage modeling, two different reservoir computing approaches, where only an\noutput layer is trainable, and the well-known transformer-based architectures,\nwhich fully learn an attention-based sequence representation. We explore the\nperformance, computational cost and prediction accuracy for both paradigms by\nequally varying the number of trainable parameters for all models. Using a\nconsistent pipeline for all three approaches, we demonstrate that transformers\nexcel in prediction quality, whereas reservoir computers remain highly\nefficient reducing the training and inference speed. Furthermore, we\ninvestigate two types of reservoir computing: a traditional reservoir with a\nstatic linear readout, and an attention-enhanced reservoir that dynamically\nadapts its output weights via an attention mechanism. Our findings underline\nhow these paradigms scale and offer guidelines to balance resource constraints\nwith performance.",
        "url": "http://arxiv.org/abs/2507.15779v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15779v1",
        "arxiv_id": "2507.15779v1",
        "authors": [
            "Felix Köster",
            "Atsushi Uchida"
        ],
        "submitted": "2025-07-21 16:35:38",
        "source": "arxiv",
        "comment": "8 pages, 5 figures, 1 table"
    },
    {
        "title": "Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective\npost-training method for improving the reasoning abilities of Large Language\nModels (LLMs), mainly by shaping higher-order behaviors such as reflection and\nplanning. However, previous RLVR algorithms often apply uniform training\nsignals to all tokens, without considering the different roles of low-entropy\nknowledge-related tokens and high-entropy reasoning-related tokens. Some recent\nmethods try to separate these token types by gradient masking or asynchronous\nupdates, but these approaches may break semantic dependencies in the model\noutput and hinder effective learning. In this work, we propose Archer, an\nentropy-aware RLVR approach with dual-token constraints and synchronous\nupdates. Specifically, our method applies weaker KL regularization and higher\nclipping thresholds to reasoning tokens to encourage exploration, while using\nstronger constraints on knowledge tokens to maintain factual knowledge.\nExperimental results on several mathematical reasoning and code generation\nbenchmarks show that our approach significantly outperforms previous RLVR\nmethods, reaching or exceeding state-of-the-art performance among models of\ncomparable size. The code is available at\nhttps://github.com/wizard-III/ArcherCodeR.",
        "url": "http://arxiv.org/abs/2507.15778v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15778v1",
        "arxiv_id": "2507.15778v1",
        "authors": [
            "Jiakang Wang",
            "Runze Liu",
            "Fuzheng Zhang",
            "Xiu Li",
            "Guorui Zhou"
        ],
        "submitted": "2025-07-21 16:34:01",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Dissociating model architectures from inference computations",
        "abstract": "Parr et al., 2025 examines how auto-regressive and deep temporal models\ndiffer in their treatment of non-Markovian sequence modelling. Building on\nthis, we highlight the need for dissociating model architectures, i.e., how the\npredictive distribution factorises, from the computations invoked at inference.\nWe demonstrate that deep temporal computations are mimicked by autoregressive\nmodels by structuring context access during iterative inference. Using a\ntransformer trained on next-token prediction, we show that inducing\nhierarchical temporal factorisation during iterative inference maintains\npredictive capacity while instantiating fewer computations. This emphasises\nthat processes for constructing and refining predictions are not necessarily\nbound to their underlying model architectures.",
        "url": "http://arxiv.org/abs/2507.15776v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15776v1",
        "arxiv_id": "2507.15776v1",
        "authors": [
            "Noor Sajid",
            "Johan Medrano"
        ],
        "submitted": "2025-07-21 16:30:42",
        "source": "arxiv",
        "comment": "3 pages, 1 figure"
    },
    {
        "title": "Supernova: Achieving More with Less in Transformer Architectures",
        "abstract": "We present Supernova, a 650M-parameter decoder-only transformer that\ndemonstrates how careful architectural design and tokenization innovation can\nachieve the performance of larger models while maintaining computational\nefficiency. Our architecture combines Rotary Positional Embeddings (RoPE),\nGrouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for\ncomputational efficiency, and SwiGLU activation functions. A critical\ninnovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which\nachieves state-of-the-art compression performance. Through detailed analysis,\nwe show that Supernova achieves 90% of the performance of 1B-parameter models\nwhile using 35% fewer parameters and requiring only 100B training tokens--an\norder of magnitude less than competing models. Our findings challenge the\nprevailing scaling paradigm, demonstrating that architectural efficiency and\ntokenization quality can compensate for reduced parameter counts.",
        "url": "http://arxiv.org/abs/2507.15773v2",
        "pdf_url": "http://arxiv.org/pdf/2507.15773v2",
        "arxiv_id": "2507.15773v2",
        "authors": [
            "Andrei-Valentin Tanase",
            "Elena Pelican"
        ],
        "submitted": "2025-07-21 16:27:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Interaction as Intelligence: Deep Research With Human-AI Partnership",
        "abstract": "This paper introduces \"Interaction as Intelligence\" research series,\npresenting a reconceptualization of human-AI relationships in deep research\ntasks. Traditional approaches treat interaction merely as an interface for\naccessing AI capabilities-a conduit between human intent and machine output. We\npropose that interaction itself constitutes a fundamental dimension of\nintelligence. As AI systems engage in extended thinking processes for research\ntasks, meaningful interaction transitions from an optional enhancement to an\nessential component of effective intelligence. Current deep research systems\nadopt an \"input-wait-output\" paradigm where users initiate queries and receive\nresults after black-box processing. This approach leads to error cascade\neffects, inflexible research boundaries that prevent question refinement during\ninvestigation, and missed opportunities for expertise integration. To address\nthese limitations, we introduce Deep Cognition, a system that transforms the\nhuman role from giving instructions to cognitive oversight-a mode of engagement\nwhere humans guide AI thinking processes through strategic intervention at\ncritical junctures. Deep cognition implements three key innovations:\n(1)Transparent, controllable, and interruptible interaction that reveals AI\nreasoning and enables intervention at any point; (2)Fine-grained bidirectional\ndialogue; and (3)Shared cognitive context where the system observes and adapts\nto user behaviors without explicit instruction. User evaluation demonstrates\nthat this cognitive oversight paradigm outperforms the strongest baseline\nacross six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%),\nReal-Time Intervention(+18.5%), Ease of Collaboration(+27.7%),\nResults-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on\nchallenging research problems show 31.8% to 50.0% points of improvements over\ndeep research systems.",
        "url": "http://arxiv.org/abs/2507.15759v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15759v1",
        "arxiv_id": "2507.15759v1",
        "authors": [
            "Lyumanshan Ye",
            "Xiaojie Cai",
            "Xinkai Wang",
            "Junfei Wang",
            "Xiangkun Hu",
            "Jiadi Su",
            "Yang Nan",
            "Sihan Wang",
            "Bohan Zhang",
            "Xiaoze Fan",
            "Jinbin Luo",
            "Yuxiang Zheng",
            "Tianze Xu",
            "Dayuan Fu",
            "Yunze Wu",
            "Pengrui Lu",
            "Zengzhi Wang",
            "Yiwei Qin",
            "Zhen Huang",
            "Yan Ma",
            "Zhulin Hu",
            "Haoyang Zou",
            "Tiantian Mi",
            "Yixin Ye",
            "Ethan Chern",
            "Pengfei Liu"
        ],
        "submitted": "2025-07-21 16:15:18",
        "source": "arxiv",
        "comment": "30 pages, 10 figures"
    },
    {
        "title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization",
        "abstract": "Large reasoning models have achieved remarkable performance through extended\nchain-of-thought sequences, yet this computational freedom leads to excessive\ntoken generation even for simple problems. We present Length-Adaptive Policy\nOptimization (LAPO), a novel framework that transforms reasoning length control\nfrom an external constraint into an intrinsic model capability. Unlike existing\napproaches that impose rigid limits or rely on post-hoc interventions, LAPO\nenables models to internalize an understanding of appropriate reasoning depth\nthrough a two-stage reinforcement learning process. In the first stage, models\nlearn natural reasoning patterns by discovering the statistical distribution of\nsuccessful solution lengths. The second stage leverages these patterns as\nmeta-cognitive guidance, embedding them directly within the model's reasoning\ncontext to ensure inference-time flexibility. Experiments on mathematical\nreasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\%\nwhile improving accuracy by 2.3\\%. Our analysis reveals that models trained\nwith LAPO develop emergent abilities to allocate computational resources based\non problem complexity, achieving efficient reasoning without sacrificing\nquality.",
        "url": "http://arxiv.org/abs/2507.15758v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15758v1",
        "arxiv_id": "2507.15758v1",
        "authors": [
            "Xingyu Wu",
            "Yuchen Yan",
            "Shangke Lyu",
            "Linjuan Wu",
            "Yiwen Qiu",
            "Yongliang Shen",
            "Weiming Lu",
            "Jian Shao",
            "Jun Xiao",
            "Yueting Zhuang"
        ],
        "submitted": "2025-07-21 16:14:41",
        "source": "arxiv",
        "comment": "GitHub:https://github.com/zju-real/lapo;\n  Project:https://zju-real.github.io/lapo"
    },
    {
        "title": "DialogueForge: LLM Simulation of Human-Chatbot Dialogue",
        "abstract": "Collecting human-chatbot dialogues typically demands substantial manual\neffort and is time-consuming, which limits and poses challenges for research on\nconversational AI. In this work, we propose DialogueForge - a framework for\ngenerating AI-simulated conversations in human-chatbot style. To initialize\neach generated conversation, DialogueForge uses seed prompts extracted from\nreal human-chatbot interactions. We test a variety of LLMs to simulate the\nhuman chatbot user, ranging from state-of-the-art proprietary models to\nsmall-scale open-source LLMs, and generate multi-turn dialogues tailored to\nspecific tasks. In addition, we explore fine-tuning techniques to enhance the\nability of smaller models to produce indistinguishable human-like dialogues. We\nevaluate the quality of the simulated conversations and compare different\nmodels using the UniEval and GTEval evaluation protocols. Our experiments show\nthat large proprietary models (e.g., GPT-4o) generally outperform others in\ngenerating more realistic dialogues, while smaller open-source models (e.g.,\nLlama, Mistral) offer promising performance with greater customization. We\ndemonstrate that the performance of smaller models can be significantly\nimproved by employing supervised fine-tuning techniques. Nevertheless,\nmaintaining coherent and natural long-form human-like dialogues remains a\ncommon challenge across all models.",
        "url": "http://arxiv.org/abs/2507.15752v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15752v1",
        "arxiv_id": "2507.15752v1",
        "authors": [
            "Ruizhe Zhu",
            "Hao Zhu",
            "Yaxuan Li",
            "Syang Zhou",
            "Shijing Cai",
            "Malgorzata Lazuka",
            "Elliott Ash"
        ],
        "submitted": "2025-07-21 16:08:19",
        "source": "arxiv",
        "comment": "For our code and data, see\n  https://github.com/nerchio/Human_Chatbot-Generation"
    },
    {
        "title": "Towards physician-centered oversight of conversational diagnostic AI",
        "abstract": "Recent work has demonstrated the promise of conversational AI systems for\ndiagnostic dialogue. However, real-world assurance of patient safety means that\nproviding individual diagnoses and treatment plans is considered a regulated\nactivity by licensed professionals. Furthermore, physicians commonly oversee\nother team members in such activities, including nurse practitioners (NPs) or\nphysician assistants/associates (PAs). Inspired by this, we propose a framework\nfor effective, asynchronous oversight of the Articulate Medical Intelligence\nExplorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent\nsystem that performs history taking within guardrails, abstaining from\nindividualized medical advice. Afterwards, g-AMIE conveys assessments to an\noverseeing primary care physician (PCP) in a clinician cockpit interface. The\nPCP provides oversight and retains accountability of the clinical decision.\nThis effectively decouples oversight from intake and can thus happen\nasynchronously. In a randomized, blinded virtual Objective Structured Clinical\nExamination (OSCE) of text consultations with asynchronous oversight, we\ncompared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across\n60 scenarios, g-AMIE outperformed both groups in performing high-quality\nintake, summarizing cases, and proposing diagnoses and management plans for the\noverseeing PCP to review. This resulted in higher quality composite decisions.\nPCP oversight of g-AMIE was also more time-efficient than standalone PCP\nconsultations in prior work. While our study does not replicate existing\nclinical practices and likely underestimates clinicians' capabilities, our\nresults demonstrate the promise of asynchronous oversight as a feasible\nparadigm for diagnostic AI systems to operate under expert human oversight for\nenhancing real-world care.",
        "url": "http://arxiv.org/abs/2507.15743v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15743v1",
        "arxiv_id": "2507.15743v1",
        "authors": [
            "Elahe Vedadi",
            "David Barrett",
            "Natalie Harris",
            "Ellery Wulczyn",
            "Shashir Reddy",
            "Roma Ruparel",
            "Mike Schaekermann",
            "Tim Strother",
            "Ryutaro Tanno",
            "Yash Sharma",
            "Jihyeon Lee",
            "Cían Hughes",
            "Dylan Slack",
            "Anil Palepu",
            "Jan Freyberg",
            "Khaled Saab",
            "Valentin Liévin",
            "Wei-Hung Weng",
            "Tao Tu",
            "Yun Liu",
            "Nenad Tomasev",
            "Kavita Kulkarni",
            "S. Sara Mahdavi",
            "Kelvin Guu",
            "Joëlle Barral",
            "Dale R. Webster",
            "James Manyika",
            "Avinatan Hassidim",
            "Katherine Chou",
            "Yossi Matias",
            "Pushmeet Kohli",
            "Adam Rodman",
            "Vivek Natarajan",
            "Alan Karthikesalingam",
            "David Stutz"
        ],
        "submitted": "2025-07-21 15:54:36",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Fisher's exact test justification of the TF-IDF term-weighting scheme",
        "abstract": "Term frequency-inverse document frequency, or TF-IDF for short, is arguably\nthe most celebrated mathematical expression in the history of information\nretrieval. Conceived as a simple heuristic quantifying the extent to which a\ngiven term's occurrences are concentrated in any one given document out of\nmany, TF-IDF and its many variants are routinely used as term-weighting schemes\nin diverse text analysis applications. There is a growing body of scholarship\ndedicated to placing TF-IDF on a sound theoretical foundation. Building on that\ntradition, this paper justifies the use of TF-IDF to the statistics community\nby demonstrating how the famed expression can be understood from a significance\ntesting perspective. We show that the common TF-IDF variant TF-ICF is, under\nmild regularity conditions, closely related to the negative logarithm of the\n$p$-value from a one-tailed version of Fisher's exact test of statistical\nsignificance. As a corollary, we establish a connection between TF-IDF and the\nsaid negative log-transformed $p$-value under certain idealized assumptions. We\nfurther demonstrate, as a limiting case, that this same quantity converges to\nTF-IDF in the limit of an infinitely large document collection. The Fisher's\nexact test justification of TF-IDF equips the working statistician with a ready\nexplanation of the term-weighting scheme's long-established effectiveness.",
        "url": "http://arxiv.org/abs/2507.15742v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15742v1",
        "arxiv_id": "2507.15742v1",
        "authors": [
            "Paul Sheridan",
            "Zeyad Ahmed",
            "Aitazaz A. Farooque"
        ],
        "submitted": "2025-07-21 15:54:23",
        "source": "arxiv",
        "comment": "23 pages, 4 tables"
    },
    {
        "title": "Understanding Large Language Models' Ability on Interdisciplinary Research",
        "abstract": "Recent advancements in Large Language Models (LLMs) have revealed their\nimpressive ability to perform multi-step, logic-driven reasoning across complex\ndomains, positioning them as powerful tools and collaborators in scientific\ndiscovery while challenging the long-held view that inspiration-driven ideation\nis uniquely human. However, the lack of a dedicated benchmark that evaluates\nLLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings\nposes a critical barrier to fully understanding their strengths and\nlimitations. To address this gap, we introduce IDRBench -- a pioneering\nbenchmark featuring an expert annotated dataset and a suite of tasks tailored\nto evaluate LLMs' capabilities in proposing valuable research ideas from\ndifferent scientific domains for interdisciplinary research. This benchmark\naims to provide a systematic framework for assessing LLM performance in\ncomplex, cross-domain scientific research. Our dataset consists of scientific\npublications sourced from the ArXiv platform covering six distinct disciplines,\nand is annotated by domain experts with diverse academic backgrounds. To ensure\nhigh-quality annotations, we emphasize clearly defined dimensions that\ncharacterize authentic interdisciplinary research. The design of evaluation\ntasks in IDRBench follows a progressive, real-world perspective, reflecting the\nnatural stages of interdisciplinary research development, including 1) IDR\nPaper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.\nUsing IDRBench, we construct baselines across 10 LLMs and observe that despite\nfostering some level of IDR awareness, LLMs still struggle to produce quality\nIDR ideas. These findings could not only spark new research directions, but\nalso help to develop next-generation LLMs that excel in interdisciplinary\nresearch.",
        "url": "http://arxiv.org/abs/2507.15736v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15736v1",
        "arxiv_id": "2507.15736v1",
        "authors": [
            "Yuanhao Shen",
            "Daniel Xavier de Sousa",
            "Ricardo Marçal",
            "Ali Asad",
            "Hongyu Guo",
            "Xiaodan Zhu"
        ],
        "submitted": "2025-07-21 15:43:05",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning",
        "abstract": "Current benchmarks evaluating large language models (LLMs) in ophthalmology\nare limited in scope and disproportionately prioritise accuracy. We introduce\nBELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive\nevaluation benchmark developed through multiple rounds of expert checking by 13\nophthalmologists. BELO assesses ophthalmology-related clinical accuracy and\nreasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we\ncurated ophthalmology-specific multiple-choice-questions (MCQs) from diverse\nmedical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset\nunderwent multiple rounds of expert checking. Duplicate and substandard\nquestions were systematically removed. Ten ophthalmologists refined the\nexplanations of each MCQ's correct answer. This was further adjudicated by\nthree senior ophthalmologists. To illustrate BELO's utility, we evaluated six\nLLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)\nusing accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,\nBARTScore, METEOR, and AlignScore). In a further evaluation involving human\nexperts, two ophthalmologists qualitatively reviewed 50 randomly selected\noutputs for accuracy, comprehensiveness, and completeness. BELO consists of 900\nhigh-quality, expert-reviewed questions aggregated from five sources: BCSC\n(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public\nleaderboard has been established to promote transparent evaluation and\nreporting. Importantly, the BELO dataset will remain a hold-out,\nevaluation-only benchmark to ensure fair and reproducible comparisons of future\nmodels.",
        "url": "http://arxiv.org/abs/2507.15717v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15717v1",
        "arxiv_id": "2507.15717v1",
        "authors": [
            "Sahana Srinivasan",
            "Xuguang Ai",
            "Thaddaeus Wai Soon Lo",
            "Aidan Gilson",
            "Minjie Zou",
            "Ke Zou",
            "Hyunjae Kim",
            "Mingjia Yang",
            "Krithi Pushpanathan",
            "Samantha Yew",
            "Wan Ting Loke",
            "Jocelyn Goh",
            "Yibing Chen",
            "Yiming Kong",
            "Emily Yuelei Fu",
            "Michelle Ongyong Hui",
            "Kristen Nwanyanwu",
            "Amisha Dave",
            "Kelvin Zhenghao Li",
            "Chen-Hsin Sun",
            "Mark Chia",
            "Gabriel Dawei Yang",
            "Wendy Meihua Wong",
            "David Ziyou Chen",
            "Dianbo Liu",
            "Maxwell Singer",
            "Fares Antaki",
            "Lucian V Del Priore",
            "Jost Jonas",
            "Ron Adelman",
            "Qingyu Chen",
            "Yih-Chung Tham"
        ],
        "submitted": "2025-07-21 15:27:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "From Queries to Criteria: Understanding How Astronomers Evaluate LLMs",
        "abstract": "There is growing interest in leveraging LLMs to aid in astronomy and other\nscientific research, but benchmarks for LLM evaluation in general have not kept\npace with the increasingly diverse ways that real people evaluate and use these\nmodels. In this study, we seek to improve evaluation procedures by building an\nunderstanding of how users evaluate LLMs. We focus on a particular use case: an\nLLM-powered retrieval-augmented generation bot for engaging with astronomical\nliterature, which we deployed via Slack. Our inductive coding of 368 queries to\nthe bot over four weeks and our follow-up interviews with 11 astronomers reveal\nhow humans evaluated this system, including the types of questions asked and\nthe criteria for judging responses. We synthesize our findings into concrete\nrecommendations for building better benchmarks, which we then employ in\nconstructing a sample benchmark for evaluating LLMs for astronomy. Overall, our\nwork offers ways to improve LLM evaluation and ultimately usability,\nparticularly for use in scientific research.",
        "url": "http://arxiv.org/abs/2507.15715v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15715v1",
        "arxiv_id": "2507.15715v1",
        "authors": [
            "Alina Hyk",
            "Kiera McCormick",
            "Mian Zhong",
            "Ioana Ciucă",
            "Sanjib Sharma",
            "John F Wu",
            "J. E. G. Peek",
            "Kartheik G. Iyer",
            "Ziang Xiao",
            "Anjalie Field"
        ],
        "submitted": "2025-07-21 15:26:58",
        "source": "arxiv",
        "comment": "Accepted to the Conference on Language Modeling 2025 (COLM), 22\n  pages, 6 figures"
    },
    {
        "title": "Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning",
        "abstract": "The SemEval-2025 Task 11, Bridging the Gap in Text-Based Emotion Detection,\nintroduces an emotion recognition challenge spanning over 28 languages. This\ncompetition encourages researchers to explore more advanced approaches to\naddress the challenges posed by the diversity of emotional expressions and\nbackground variations. It features two tracks: multi-label classification\n(Track A) and emotion intensity prediction (Track B), covering six emotion\ncategories: anger, fear, joy, sadness, surprise, and disgust. In our work, we\nsystematically explore the benefits of two contrastive learning approaches:\nsample-based (Contrastive Reasoning Calibration) and generation-based (DPO,\nSimPO) contrastive learning. The sample-based contrastive approach trains the\nmodel by comparing two samples to generate more reliable predictions. The\ngeneration-based contrastive approach trains the model to differentiate between\ncorrect and incorrect generations, refining its prediction. All models are\nfine-tuned from LLaMa3-Instruct-8B. Our system achieves 9th place in Track A\nand 6th place in Track B for English, while ranking among the top-tier\nperforming systems for other languages.",
        "url": "http://arxiv.org/abs/2507.15714v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15714v1",
        "arxiv_id": "2507.15714v1",
        "authors": [
            "Tian Li",
            "Yujian Sun",
            "Huizhi Liang"
        ],
        "submitted": "2025-07-21 15:25:47",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?",
        "abstract": "Large Language Models (LLMs) have been evaluated using diverse question\ntypes, e.g., multiple-choice, true/false, and short/long answers. This study\nanswers an unexplored question about the impact of different question types on\nLLM accuracy on reasoning tasks. We investigate the performance of five LLMs on\nthree different types of questions using quantitative and deductive reasoning\ntasks. The performance metrics include accuracy in the reasoning steps and\nchoosing the final answer. Key Findings: (1) Significant differences exist in\nLLM performance across different question types. (2) Reasoning accuracy does\nnot necessarily correlate with the final selection accuracy. (3) The number of\noptions and the choice of words, influence LLM performance.",
        "url": "http://arxiv.org/abs/2507.15707v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15707v1",
        "arxiv_id": "2507.15707v1",
        "authors": [
            "Seok Hwan Song",
            "Mohna Chakraborty",
            "Qi Li",
            "Wallapak Tavanapong"
        ],
        "submitted": "2025-07-21 15:15:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Compositional Understanding in Signaling Games",
        "abstract": "Receivers in standard signaling game models struggle with learning\ncompositional information. Even when the signalers send compositional messages,\nthe receivers do not interpret them compositionally. When information from one\nmessage component is lost or forgotten, the information from other components\nis also erased. In this paper I construct signaling game models in which\ngenuine compositional understanding evolves. I present two new models: a\nminimalist receiver who only learns from the atomic messages of a signal, and a\ngeneralist receiver who learns from all of the available information. These\nmodels are in many ways simpler than previous alternatives, and allow the\nreceivers to learn from the atomic components of messages.",
        "url": "http://arxiv.org/abs/2507.15706v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15706v1",
        "arxiv_id": "2507.15706v1",
        "authors": [
            "David Peter Wallis Freeborn"
        ],
        "submitted": "2025-07-21 15:14:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models",
        "abstract": "Process Reward Models (PRMs) play a central role in evaluating and guiding\nmulti-step reasoning in large language models (LLMs), especially for\nmathematical problem solving. However, we identify a pervasive length bias in\nexisting PRMs: they tend to assign higher scores to longer reasoning steps,\neven when the semantic content and logical validity are unchanged. This bias\nundermines the reliability of reward predictions and leads to overly verbose\noutputs during inference. To address this issue, we propose\nCoLD(Counterfactually-Guided Length Debiasing), a unified framework that\nmitigates length bias through three components: an explicit length-penalty\nadjustment, a learned bias estimator trained to capture spurious length-related\nsignals, and a joint training strategy that enforces length-invariance in\nreward predictions. Our approach is grounded in counterfactual reasoning and\ninformed by causal graph analysis. Extensive experiments on MATH500 and\nGSM-Plus show that CoLD consistently reduces reward-length correlation,\nimproves accuracy in step selection, and encourages more concise, logically\nvalid reasoning. These results demonstrate the effectiveness and practicality\nof CoLD in improving the fidelity and robustness of PRMs.",
        "url": "http://arxiv.org/abs/2507.15698v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15698v1",
        "arxiv_id": "2507.15698v1",
        "authors": [
            "Congmin Zheng",
            "Jiachen Zhu",
            "Jianghao Lin",
            "Xinyi Dai",
            "Yong Yu",
            "Weinan Zhang",
            "Mengyue Yang"
        ],
        "submitted": "2025-07-21 15:07:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "P3: Prompts Promote Prompting",
        "abstract": "Current large language model (LLM) applications often employ multi-component\nprompts, comprising both system and user prompts, to guide model behaviors.\nWhile recent advancements have demonstrated the efficacy of automatically\noptimizing either the system or user prompt to boost performance, such\nunilateral approaches often yield suboptimal outcomes due to the interdependent\nnature of these components. In this work, we introduce P3, a novel\nself-improvement framework that concurrently optimizes both system and user\nprompts through an iterative process. The offline optimized prompts are further\nleveraged to promote online prompting by performing query-dependent prompt\noptimization. Extensive experiments on general tasks (e.g., Arena-hard and\nAlpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3\nachieves superior performance in the realm of automatic prompt optimization.\nOur results highlight the effectiveness of a holistic optimization strategy in\nenhancing LLM performance across diverse domains.",
        "url": "http://arxiv.org/abs/2507.15675v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15675v1",
        "arxiv_id": "2507.15675v1",
        "authors": [
            "Xinyu Zhang",
            "Yuanquan Hu",
            "Fangchao Liu",
            "Zhicheng Dou"
        ],
        "submitted": "2025-07-21 14:37:46",
        "source": "arxiv",
        "comment": "Accepted to ACL 2025 findings"
    },
    {
        "title": "Leveraging Context for Multimodal Fallacy Classification in Political Debates",
        "abstract": "In this paper, we present our submission to the MM-ArgFallacy2025 shared\ntask, which aims to advance research in multimodal argument mining, focusing on\nlogical fallacies in political debates. Our approach uses pretrained\nTransformer-based models and proposes several ways to leverage context. In the\nfallacy classification subtask, our models achieved macro F1-scores of 0.4444\n(text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed\nperformance comparable to the text-only model, suggesting potential for\nimprovements.",
        "url": "http://arxiv.org/abs/2507.15641v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15641v1",
        "arxiv_id": "2507.15641v1",
        "authors": [
            "Alessio Pittiglio"
        ],
        "submitted": "2025-07-21 14:03:08",
        "source": "arxiv",
        "comment": "12th Workshop on Argument Mining (ArgMining 2025) @ ACL 2025"
    },
    {
        "title": "Data Mixing Agent: Learning to Re-weight Domains for Continual Pre-training",
        "abstract": "Continual pre-training on small-scale task-specific data is an effective\nmethod for improving large language models in new target fields, yet it risks\ncatastrophic forgetting of their original capabilities. A common solution is to\nre-weight training data mixtures from source and target fields on a domain\nspace to achieve balanced performance. Previous domain reweighting strategies\nrely on manual designation with certain heuristics based on human intuition or\nempirical results. In this work, we prove that more general heuristics can be\nparameterized by proposing Data Mixing Agent, the first model-based, end-to-end\nframework that learns to re-weight domains. The agent learns generalizable\nheuristics through reinforcement learning on large quantities of data mixing\ntrajectories with corresponding feedback from an evaluation environment.\nExperiments in continual pre-training on math reasoning show that Data Mixing\nAgent outperforms strong baselines in achieving balanced performance across\nsource and target field benchmarks. Furthermore, it generalizes well across\nunseen source fields, target models, and domain spaces without retraining.\nDirect application to the code generation field also indicates its adaptability\nacross target domains. Further analysis showcases the agents' well-aligned\nheuristics with human intuitions and their efficiency in achieving superior\nmodel performance with less source-field data.",
        "url": "http://arxiv.org/abs/2507.15640v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15640v1",
        "arxiv_id": "2507.15640v1",
        "authors": [
            "Kailai Yang",
            "Xiao Liu",
            "Lei Ji",
            "Hao Li",
            "Yeyun Gong",
            "Peng Cheng",
            "Mao Yang"
        ],
        "submitted": "2025-07-21 14:01:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Conflicting narratives and polarization on social media",
        "abstract": "Narratives are key interpretative devices by which humans make sense of\npolitical reality. In this work, we show how the analysis of conflicting\nnarratives, i.e. conflicting interpretive lenses through which political\nreality is experienced and told, provides insight into the discursive\nmechanisms of polarization and issue alignment in the public sphere. Building\nupon previous work that has identified ideologically polarized issues in the\nGerman Twittersphere between 2021 and 2023, we analyze the discursive dimension\nof polarization by extracting textual signals of conflicting narratives from\ntweets of opposing opinion groups. Focusing on a selection of salient issues\nand events (the war in Ukraine, Covid, climate change), we show evidence for\nconflicting narratives along two dimensions: (i) different attributions of\nactantial roles to the same set of actants (e.g. diverging interpretations of\nthe role of NATO in the war in Ukraine), and (ii) emplotment of different\nactants for the same event (e.g. Bill Gates in the right-leaning Covid\nnarrative). Furthermore, we provide first evidence for patterns of narrative\nalignment, a discursive strategy that political actors employ to align opinions\nacross issues. These findings demonstrate the use of narratives as an\nanalytical lens into the discursive mechanisms of polarization.",
        "url": "http://arxiv.org/abs/2507.15600v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15600v1",
        "arxiv_id": "2507.15600v1",
        "authors": [
            "Armin Pournaki"
        ],
        "submitted": "2025-07-21 13:22:57",
        "source": "arxiv",
        "comment": "30 pages, 7 figures"
    },
    {
        "title": "Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation",
        "abstract": "Retrieval-Augmented Generation (RAG) effectively improves the accuracy of\nLarge Language Models (LLMs). However, retrieval noises significantly impact\nthe quality of LLMs' generation, necessitating the development of denoising\nmechanisms. Previous methods extract evidence straightforwardly without\nexplicit thinking, which risks filtering out key clues and struggles with\ngeneralization. To this end, we propose LEAR, which learns to extract rational\nevidence by (1) explicitly reasoning to identify potential cues within\nretrieval contents first, and then (2) consciously extracting to avoid omitting\nany key cues helpful for answering questions. Specifically, we frame evidence\nreasoning and evidence extraction into one unified response for end-to-end\ntraining; apply knowledge token masks for disentanglement to derive\nreasoning-based and extraction-based answers; and devise three types of\nverifiable reward functions, including answer, length, and format, to update\nthe model via the policy optimization algorithm. Extensive experiments on three\nbenchmark datasets show the effectiveness of LEAR, providing compact and\nhigh-quality evidence, improving the accuracy of downstream tasks, and\npromoting effective application in online RAG systems.",
        "url": "http://arxiv.org/abs/2507.15586v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15586v1",
        "arxiv_id": "2507.15586v1",
        "authors": [
            "Xinping Zhao",
            "Shouzheng Huang",
            "Yan Zhong",
            "Xinshuo Hu",
            "Baotian Hu",
            "Min Zhang"
        ],
        "submitted": "2025-07-21 13:03:55",
        "source": "arxiv",
        "comment": "16 pages, 7 Figures, 10 Tables"
    },
    {
        "title": "Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging",
        "abstract": "Terahertz (THz) imaging enables non-invasive analysis for applications such\nas security screening and material classification, but effective image\nclassification remains challenging due to limited annotations, low resolution,\nand visual ambiguity. We introduce In-Context Learning (ICL) with\nVision-Language Models (VLMs) as a flexible, interpretable alternative that\nrequires no fine-tuning. Using a modality-aligned prompting framework, we adapt\ntwo open-weight VLMs to the THz domain and evaluate them under zero-shot and\none-shot settings. Our results show that ICL improves classification and\ninterpretability in low-data regimes. This is the first application of\nICL-enhanced VLMs to THz imaging, offering a promising direction for\nresource-constrained scientific domains. Code:\n\\href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub\nrepository}.",
        "url": "http://arxiv.org/abs/2507.15576v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15576v1",
        "arxiv_id": "2507.15576v1",
        "authors": [
            "Nicolas Poggi",
            "Shashank Agnihotri",
            "Margret Keuper"
        ],
        "submitted": "2025-07-21 12:57:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Evaluating Text Style Transfer: A Nine-Language Benchmark for Text Detoxification",
        "abstract": "Despite recent progress in large language models (LLMs), evaluation of text\ngeneration tasks such as text style transfer (TST) remains a significant\nchallenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025)\nrevealed a substantial gap between automatic metrics and human judgments.\nMoreover, most prior work focuses exclusively on English, leaving multilingual\nTST evaluation largely unexplored. In this paper, we perform the first\ncomprehensive multilingual study on evaluation of text detoxification system\nacross nine languages: English, Spanish, German, Chinese, Arabic, Hindi,\nUkrainian, Russian, Amharic. Drawing inspiration from the machine translation,\nwe assess the effectiveness of modern neural-based evaluation models alongside\nprompting-based LLM-as-a-judge approaches. Our findings provide a practical\nrecipe for designing more reliable multilingual TST evaluation pipeline in the\ntext detoxification case.",
        "url": "http://arxiv.org/abs/2507.15557v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15557v1",
        "arxiv_id": "2507.15557v1",
        "authors": [
            "Vitaly Protasov",
            "Nikolay Babakov",
            "Daryna Dementieva",
            "Alexander Panchenko"
        ],
        "submitted": "2025-07-21 12:38:07",
        "source": "arxiv",
        "comment": "preprint"
    },
    {
        "title": "RankMixer: Scaling Up Ranking Models in Industrial Recommenders",
        "abstract": "Recent progress on large language models (LLMs) has spurred interest in\nscaling up recommendation systems, yet two practical obstacles remain. First,\ntraining and serving cost on industrial Recommenders must respect strict\nlatency bounds and high QPS demands. Second, most human-designed\nfeature-crossing modules in ranking models were inherited from the CPU era and\nfail to exploit modern GPUs, resulting in low Model Flops Utilization (MFU) and\npoor scalability. We introduce RankMixer, a hardware-aware model design\ntailored towards a unified and scalable feature-interaction architecture.\nRankMixer retains the transformer's high parallelism while replacing quadratic\nself-attention with multi-head token mixing module for higher efficiency.\nBesides, RankMixer maintains both the modeling for distinct feature subspaces\nand cross-feature-space interactions with Per-token FFNs. We further extend it\nto one billion parameters with a Sparse-MoE variant for higher ROI. A dynamic\nrouting strategy is adapted to address the inadequacy and imbalance of experts\ntraining. Experiments show RankMixer's superior scaling abilities on a\ntrillion-scale production dataset. By replacing previously diverse handcrafted\nlow-MFU modules with RankMixer, we boost the model MFU from 4.5% to 45%, and\nscale our ranking model parameters by 100x while maintaining roughly the same\ninference latency. We verify RankMixer's universality with online A/B tests\nacross three core application scenarios (Recommendation, Advertisement and\nSearch). Finally, we launch 1B Dense-Parameters RankMixer for full traffic\nserving without increasing the serving cost, which improves user active days by\n0.2% and total in-app usage duration by 0.5%.",
        "url": "http://arxiv.org/abs/2507.15551v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15551v1",
        "arxiv_id": "2507.15551v1",
        "authors": [
            "Jie Zhu",
            "Zhifang Fan",
            "Xiaoxie Zhu",
            "Yuchen Jiang",
            "Hangyu Wang",
            "Xintian Han",
            "Haoran Ding",
            "Xinmin Wang",
            "Wenlin Zhao",
            "Zhen Gong",
            "Huizhi Yang",
            "Zheng Chai",
            "Zhe Chen",
            "Yuchao Zheng",
            "Qiwei Chen",
            "Feng Zhang",
            "Xun Zhou",
            "Peng Xu",
            "Xiao Yang",
            "Di Wu",
            "Zuotao Liu"
        ],
        "submitted": "2025-07-21 12:28:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models",
        "abstract": "Test-Time Scaling (TTS) is a promising approach to progressively elicit the\nmodel's intelligence during inference. Recently, training-based TTS methods,\nsuch as continued reinforcement learning (RL), have further surged in\npopularity, while training-free TTS methods are gradually fading from\nprominence. However, the additional computation overhead of training amplifies\nthe burden on test-time scaling. In this paper, we focus on training-free TTS\nmethods for reasoning. We first design Conditional Step-level Self-refinement,\na fine-grained sequential scaling method guided by process verification. On top\nof its effectiveness, we further combine it with other classical parallel\nscaling methods at the step level, to introduce a novel inference paradigm\ncalled Hybrid Test-Time Scaling. Extensive experiments on five\ninstruction-tuned LLMs across different scales (3B-14B) and families\ndemonstrate that hybrid strategy incorporating various training-free TTS\nmethods at a fine granularity has considerable potential for expanding the\nreasoning performance boundaries of LLMs.",
        "url": "http://arxiv.org/abs/2507.15512v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15512v1",
        "arxiv_id": "2507.15512v1",
        "authors": [
            "Kaiyan Chang",
            "Yonghao Shi",
            "Chenglong Wang",
            "Hang Zhou",
            "Chi Hu",
            "Xiaoqian Liu",
            "Yingfeng Luo",
            "Yuan Ge",
            "Tong Xiao",
            "Jingbo Zhu"
        ],
        "submitted": "2025-07-21 11:28:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Off-Policy Corrected Reward Modeling for Reinforcement Learning from Human Feedback",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) allows us to train models,\nsuch as language models (LMs), to follow complex human preferences. In RLHF for\nLMs, we first train an LM using supervised fine-tuning, sample pairs of\nresponses, obtain human feedback, and use the resulting data to train a reward\nmodel (RM). RL methods are then used to train the LM to maximize the reward\ngiven by the RM. As training progresses, the responses generated by the LM no\nlonger resemble the responses seen by the RM during training, leading to the RM\nbecoming inaccurate. The score given by the RM keeps increasing, but the\nlearned behavior no longer matches the human preferences. This issue is known\nas overoptimization. We investigate overoptimization from the point of view of\ndistribution shift and show that the shift results in an inconsistent estimate\nof the RM parameters, leading to an inconsistent estimate of the policy\ngradient. We propose Off-Policy Corrected Reward Modeling (OCRM), which\niteratively off-policy corrects the RM using importance weighting, without\nrequiring new labels or samples. This results in a more accurate RM, which\nempirically leads to an improved final policy. We validate our approach in\nexperiments with summarization and chatbot datasets and show that it performs\nsignificantly better than standard RLHF methods and baselines. Our\nimplementation is available at\nhttps://github.com/JohannesAck/OffPolicyCorrectedRewardModeling",
        "url": "http://arxiv.org/abs/2507.15507v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15507v1",
        "arxiv_id": "2507.15507v1",
        "authors": [
            "Johannes Ackermann",
            "Takashi Ishida",
            "Masashi Sugiyama"
        ],
        "submitted": "2025-07-21 11:19:04",
        "source": "arxiv",
        "comment": "Accept at the Conference On Language Modeling (COLM) 2025"
    },
    {
        "title": "ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution",
        "abstract": "This work evaluates the potential of large language models (LLMs) to power\ndigital assistants capable of complex action execution. These assistants rely\non pre-trained programming knowledge to execute multi-step goals by composing\nobjects and functions defined in assistant libraries into action execution\nprograms. To achieve this, we develop ASPERA, a framework comprising an\nassistant library simulation and a human-assisted LLM data generation engine.\nOur engine allows developers to guide LLM generation of high-quality tasks\nconsisting of complex user queries, simulation state and corresponding\nvalidation programs, tackling data availability and evaluation robustness\nchallenges. Alongside the framework we release Asper-Bench, an evaluation\ndataset of 250 challenging tasks generated using ASPERA, which we use to show\nthat program generation grounded in custom assistant libraries is a significant\nchallenge to LLMs compared to dependency-free code generation.",
        "url": "http://arxiv.org/abs/2507.15501v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15501v1",
        "arxiv_id": "2507.15501v1",
        "authors": [
            "Alexandru Coca",
            "Mark Gaynor",
            "Zhenxing Zhang",
            "Jianpeng Cheng",
            "Bo-Hsiang Tseng",
            "Pete Boothroyd",
            "Héctor Martinez Alonso",
            "Diarmuid Ó Séaghdha",
            "Anders Johannsen"
        ],
        "submitted": "2025-07-21 11:07:05",
        "source": "arxiv",
        "comment": "37 pages, 22 figures. To appear at ACL 2025"
    },
    {
        "title": "Hierarchical Graph Information Bottleneck for Multi-Behavior Recommendation",
        "abstract": "In real-world recommendation scenarios, users typically engage with platforms\nthrough multiple types of behavioral interactions. Multi-behavior\nrecommendation algorithms aim to leverage various auxiliary user behaviors to\nenhance prediction for target behaviors of primary interest (e.g., buy),\nthereby overcoming performance limitations caused by data sparsity in target\nbehavior records. Current state-of-the-art approaches typically employ\nhierarchical design following either cascading (e.g.,\nview$\\rightarrow$cart$\\rightarrow$buy) or parallel\n(unified$\\rightarrow$behavior$\\rightarrow$specific components) paradigms, to\ncapture behavioral relationships. However, these methods still face two\ncritical challenges: (1) severe distribution disparities across behaviors, and\n(2) negative transfer effects caused by noise in auxiliary behaviors. In this\npaper, we propose a novel model-agnostic Hierarchical Graph Information\nBottleneck (HGIB) framework for multi-behavior recommendation to effectively\naddress these challenges. Following information bottleneck principles, our\nframework optimizes the learning of compact yet sufficient representations that\npreserve essential information for target behavior prediction while eliminating\ntask-irrelevant redundancies. To further mitigate interaction noise, we\nintroduce a Graph Refinement Encoder (GRE) that dynamically prunes redundant\nedges through learnable edge dropout mechanisms. We conduct comprehensive\nexperiments on three real-world public datasets, which demonstrate the superior\neffectiveness of our framework. Beyond these widely used datasets in the\nacademic community, we further expand our evaluation on several real industrial\nscenarios and conduct an online A/B testing, showing again a significant\nimprovement in multi-behavior recommendations. The source code of our proposed\nHGIB is available at https://github.com/zhy99426/HGIB.",
        "url": "http://arxiv.org/abs/2507.15395v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15395v1",
        "arxiv_id": "2507.15395v1",
        "authors": [
            "Hengyu Zhang",
            "Chunxu Shen",
            "Xiangguo Sun",
            "Jie Tan",
            "Yanchao Tan",
            "Yu Rong",
            "Hong Cheng",
            "Lingling Yi"
        ],
        "submitted": "2025-07-21 08:53:49",
        "source": "arxiv",
        "comment": "Accepted by RecSys2025"
    },
    {
        "title": "AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming",
        "abstract": "Recent progress in LLMs, such as reasoning models, has demonstrated strong\nabilities to solve complex competitive programming problems, often rivaling top\nhuman competitors. However, it remains underexplored whether these abilities\ngeneralize to relevant domains that are less seen during training. To address\nthis, we introduce AlgoSimBench, a new benchmark designed to assess LLMs'\nability to identify algorithmically similar problems (ASPs)-problems that can\nbe solved using similar algorithmic approaches. AlgoSimBench consists of 1317\nproblems, annotated with 231 distinct fine-grained algorithm tags, from which\nwe curate 402 multiple-choice questions (MCQs), where each question presents\none algorithmically similar problem alongside three textually similar but\nalgorithmically dissimilar distractors. Our evaluation reveals that LLMs\nstruggle to identify ASPs, with the best-performing model (o3-mini) achieving\nonly 65.9% accuracy on the MCQ task. To address this challenge, we propose\nattempted solution matching (ASM), a novel method for improving problem\nsimilarity detection. On our MCQ task, ASM yields an absolute accuracy\nimprovement of 6.7% to 11.7% across different models. We also evaluated code\nembedding models and retrieval methods on similar problem identification. While\nthe adversarial selection of problems degrades the performance to be less than\nrandom, we found that simply summarizing the problem to remove narrative\nelements eliminates the effect, and combining ASM with a keyword-prioritized\nmethod, BM25, can yield up to 52.2% accuracy. Code and data are available at\ngithub.com",
        "url": "http://arxiv.org/abs/2507.15378v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15378v1",
        "arxiv_id": "2507.15378v1",
        "authors": [
            "Jierui Li",
            "Raymond Mooney"
        ],
        "submitted": "2025-07-21 08:34:20",
        "source": "arxiv",
        "comment": "19 pages, pre-print only"
    },
    {
        "title": "STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models",
        "abstract": "Spoken Language Models (SLMs) are designed to take speech inputs and produce\nspoken responses. However, current SLMs lack the ability to perform an\ninternal, unspoken thinking process before responding. In contrast, humans\ntypically engage in complex mental reasoning internally, enabling them to\ncommunicate ideas clearly and concisely. Thus, integrating an unspoken thought\nprocess into SLMs is highly desirable. While naively generating a complete\nchain-of-thought (CoT) reasoning before starting to talk can enable thinking\nfor SLMs, this induces additional latency for the speech response, as the CoT\nreasoning can be arbitrarily long. To solve this issue, we propose Stitch, a\nnovel generation method that alternates between the generation of unspoken\nreasoning chunks and spoken response chunks. Since the audio duration of a\nchunk of spoken response is much longer than the time to generate the tokens in\na chunk of spoken response, we use the remaining free time to generate the\nunspoken reasoning tokens. When a chunk of audio is played to the user, the\nmodel continues to generate the next unspoken reasoning chunk, achieving\nsimultaneous thinking and talking. Remarkably, Stitch matches the latency of\nbaselines that cannot generate unspoken CoT by design while outperforming those\nbaselines by 15% on math reasoning datasets; Stitch also performs equally well\non non-reasoning datasets as those baseline models. Some animations and\ndemonstrations are on the project page: https://d223302.github.io/STITCH.",
        "url": "http://arxiv.org/abs/2507.15375v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15375v1",
        "arxiv_id": "2507.15375v1",
        "authors": [
            "Cheng-Han Chiang",
            "Xiaofei Wang",
            "Linjie Li",
            "Chung-Ching Lin",
            "Kevin Lin",
            "Shujie Liu",
            "Zhendong Wang",
            "Zhengyuan Yang",
            "Hung-yi Lee",
            "Lijuan Wang"
        ],
        "submitted": "2025-07-21 08:30:03",
        "source": "arxiv",
        "comment": "Work in progress. Project page: https://d223302.github.io/STITCH/"
    },
    {
        "title": "Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding",
        "abstract": "This paper presents a comprehensive evaluation of the capabilities of Large\nLanguage Models (LLMs) in metaphor interpretation across multiple datasets,\ntasks, and prompt configurations. Although metaphor processing has gained\nsignificant attention in Natural Language Processing (NLP), previous research\nhas been limited to single-dataset evaluations and specific task settings,\noften using artificially constructed data through lexical replacement. We\naddress these limitations by conducting extensive experiments using diverse\npublicly available datasets with inference and metaphor annotations, focusing\non Natural Language Inference (NLI) and Question Answering (QA) tasks. The\nresults indicate that LLMs' performance is more influenced by features like\nlexical overlap and sentence length than by metaphorical content, demonstrating\nthat any alleged emergent abilities of LLMs to understand metaphorical language\nare the result of a combination of surface-level features, in-context learning,\nand linguistic knowledge. This work provides critical insights into the current\ncapabilities and limitations of LLMs in processing figurative language,\nhighlighting the need for more realistic evaluation frameworks in metaphor\ninterpretation tasks. Data and code are publicly available.",
        "url": "http://arxiv.org/abs/2507.15357v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15357v1",
        "arxiv_id": "2507.15357v1",
        "authors": [
            "Elisa Sanchez-Bayona",
            "Rodrigo Agerri"
        ],
        "submitted": "2025-07-21 08:09:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Probing Information Distribution in Transformer Architectures through Entropy Analysis",
        "abstract": "This work explores entropy analysis as a tool for probing information\ndistribution within Transformer-based architectures. By quantifying token-level\nuncertainty and examining entropy patterns across different stages of\nprocessing, we aim to investigate how information is managed and transformed\nwithin these models. As a case study, we apply the methodology to a GPT-based\nlarge language model, illustrating its potential to reveal insights into model\nbehavior and internal representations. This approach may offer insights into\nmodel behavior and contribute to the development of interpretability and\nevaluation frameworks for transformer-based models",
        "url": "http://arxiv.org/abs/2507.15347v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15347v1",
        "arxiv_id": "2507.15347v1",
        "authors": [
            "Amedeo Buonanno",
            "Alessandro Rivetti",
            "Francesco A. N. Palmieri",
            "Giovanni Di Gennaro",
            "Gianmarco Romano"
        ],
        "submitted": "2025-07-21 08:01:22",
        "source": "arxiv",
        "comment": "Presented to the Italian Workshop on Neural Networks (WIRN2025) and\n  it will appear in a Springer Chapter"
    },
    {
        "title": "LionGuard 2: Building Lightweight, Data-Efficient & Localised Multilingual Content Moderators",
        "abstract": "Modern moderation systems increasingly support multiple languages, but often\nfail to address localisation and low-resource variants - creating safety gaps\nin real-world deployments. Small models offer a potential alternative to large\nLLMs, yet still demand considerable data and compute. We present LionGuard 2, a\nlightweight, multilingual moderation classifier tailored to the Singapore\ncontext, supporting English, Chinese, Malay, and partial Tamil. Built on\npre-trained OpenAI embeddings and a multi-head ordinal classifier, LionGuard 2\noutperforms several commercial and open-source systems across 17 benchmarks,\nincluding both Singapore-specific and public English datasets. The system is\nactively deployed within the Singapore Government, demonstrating practical\nefficacy at scale. Our findings show that high-quality local data and robust\nmultilingual embeddings can achieve strong moderation performance, without\nfine-tuning large models. We release our model weights and part of our training\ndata to support future work on LLM safety.",
        "url": "http://arxiv.org/abs/2507.15339v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15339v1",
        "arxiv_id": "2507.15339v1",
        "authors": [
            "Leanne Tan",
            "Gabriel Chua",
            "Ziyu Ge",
            "Roy Ka-Wei Lee"
        ],
        "submitted": "2025-07-21 07:50:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Reasoning Models are Test Exploiters: Rethinking Multiple-Choice",
        "abstract": "When evaluating Large Language Models (LLMs) in question-answering domains,\nit is common to ask the model to choose among a fixed set of choices (so-called\nmultiple-choice question-answering, or MCQA). Although downstream tasks of\ninterest typically do not provide systems with explicit options among which to\nchoose, this approach is nevertheless widely used because it makes it makes\nautomatic grading straightforward and has tended to produce challenging\nbenchmarks that correlate sufficiently well with downstream performance. This\npaper investigates the extent to which this trend continues to hold for\nstate-of-the-art reasoning models, describing a systematic evaluation of $15$\ndifferent question-answering benchmarks (e.g., MMLU, HLE) and $25$ different\nLLMs (including small models such as Qwen 7B and relatively large models such\nas Llama 70B). For each model-benchmark pair, we considered $5$ ways of\npresenting the model with questions, including variations on whether multiple\nchoices were offered to the model at all; whether \"none of the above\" sometimes\nreplaced the right answer; and whether the model was permitted to perform\nchain-of-thought reasoning before and/or after the choices were presented. MCQA\nremained a good proxy for the downstream performance of models as long as they\nwere allowed to perform chain-of-thought reasoning only before being presented\nwith the options among which they had to select. On the other hand, large\nmodels that were able to perform reasoning after being given a set of options\ntended to significantly outperform their free-text performance due to\nexploiting the information in the options. We conclude that MCQA is no longer a\ngood proxy for assessing downstream performance of state-of-the-art models, and\noffer practical guidelines for designing more robust, bias-resistant benchmarks\nthat better reflect LLMs' genuine reasoning capabilities.",
        "url": "http://arxiv.org/abs/2507.15337v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15337v1",
        "arxiv_id": "2507.15337v1",
        "authors": [
            "Narun Raman",
            "Taylor Lundy",
            "Kevin Leyton-Brown"
        ],
        "submitted": "2025-07-21 07:49:32",
        "source": "arxiv",
        "comment": "9 pages, 3 figures"
    },
    {
        "title": "On the Inevitability of Left-Leaning Political Bias in Aligned Language Models",
        "abstract": "The guiding principle of AI alignment is to train large language models\n(LLMs) to be harmless, helpful, and honest (HHH). At the same time, there are\nmounting concerns that LLMs exhibit a left-wing political bias. Yet, the\ncommitment to AI alignment cannot be harmonized with the latter critique. In\nthis article, I argue that intelligent systems that are trained to be harmless\nand honest must necessarily exhibit left-wing political bias. Normative\nassumptions underlying alignment objectives inherently concur with progressive\nmoral frameworks and left-wing principles, emphasizing harm avoidance,\ninclusivity, fairness, and empirical truthfulness. Conversely, right-wing\nideologies often conflict with alignment guidelines. Yet, research on political\nbias in LLMs is consistently framing its insights about left-leaning tendencies\nas a risk, as problematic, or concerning. This way, researchers are actively\narguing against AI alignment, tacitly fostering the violation of HHH\nprinciples.",
        "url": "http://arxiv.org/abs/2507.15328v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15328v1",
        "arxiv_id": "2507.15328v1",
        "authors": [
            "Thilo Hagendorff"
        ],
        "submitted": "2025-07-21 07:37:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond Easy Wins: A Text Hardness-Aware Benchmark for LLM-generated Text Detection",
        "abstract": "We present a novel evaluation paradigm for AI text detectors that prioritizes\nreal-world and equitable assessment. Current approaches predominantly report\nconventional metrics like AUROC, overlooking that even modest false positive\nrates constitute a critical impediment to practical deployment of detection\nsystems. Furthermore, real-world deployment necessitates predetermined\nthreshold configuration, making detector stability (i.e. the maintenance of\nconsistent performance across diverse domains and adversarial scenarios), a\ncritical factor. These aspects have been largely ignored in previous research\nand benchmarks. Our benchmark, SHIELD, addresses these limitations by\nintegrating both reliability and stability factors into a unified evaluation\nmetric designed for practical assessment. Furthermore, we develop a post-hoc,\nmodel-agnostic humanification framework that modifies AI text to more closely\nresemble human authorship, incorporating a controllable hardness parameter.\nThis hardness-aware approach effectively challenges current SOTA zero-shot\ndetection methods in maintaining both reliability and stability. (Data and\ncode: https://github.com/navid-aub/SHIELD-Benchmark)",
        "url": "http://arxiv.org/abs/2507.15286v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15286v1",
        "arxiv_id": "2507.15286v1",
        "authors": [
            "Navid Ayoobi",
            "Sadat Shahriar",
            "Arjun Mukherjee"
        ],
        "submitted": "2025-07-21 06:37:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Novel Self-Evolution Framework for Large Language Models",
        "abstract": "The capabilities of Large Language Models (LLMs) are limited to some extent\nby pre-training, so some researchers optimize LLMs through post-training.\nExisting post-training strategies, such as memory-based retrieval or preference\noptimization, improve user alignment yet fail to enhance the model's domain\ncognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution\n(DPSE) framework that jointly optimizes user preference adaptation and\ndomain-specific competence. DPSE introduces a Censor module to extract\nmulti-dimensional interaction signals and estimate satisfaction scores, which\nguide structured data expansion via topic-aware and preference-driven\nstrategies. These expanded datasets support a two-stage fine-tuning pipeline:\nsupervised domain grounding followed by frequency-aware preference\noptimization. Experiments across general NLP benchmarks and long-term dialogue\ntasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning,\nPreference Optimization, and Memory-Augmented baselines. Ablation studies\nvalidate the contribution of each module. In this way, our framework provides\nan autonomous path toward continual self-evolution of LLMs.",
        "url": "http://arxiv.org/abs/2507.15281v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15281v1",
        "arxiv_id": "2507.15281v1",
        "authors": [
            "Haoran Sun",
            "Zekun Zhang",
            "Shaoning Zeng"
        ],
        "submitted": "2025-07-21 06:30:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling",
        "abstract": "Building high-quality data resources is crucial for advancing artificial\nintelligence research and applications in specific domains, particularly in the\nChinese medical domain. Existing Chinese medical datasets are limited in size\nand narrow in domain coverage, falling short of the diverse corpora required\nfor effective pre-training. Moreover, most datasets are designed solely for LLM\nfine-tuning and do not support pre-training and reinforcement learning from\nhuman feedback (RLHF). In this paper, we propose a Chinese medical dataset\nnamed ChiMed 2.0, which extends our previous work ChiMed, and covers data\ncollected from Chinese medical online platforms and generated by LLMs. ChiMed\n2.0 contains 204.4M Chinese characters covering both traditional Chinese\nmedicine classics and modern general medical data, where there are 164.8K\ndocuments for pre-training, 351.6K question-answering pairs for supervised\nfine-tuning (SFT), and 41.7K preference data tuples for RLHF. To validate the\neffectiveness of our approach for training a Chinese medical LLM, we conduct\nfurther pre-training, SFT, and RLHF experiments on representative general\ndomain LLMs and evaluate their performance on medical benchmark datasets. The\nresults show performance gains across different model scales, validating the\ndataset's effectiveness and applicability.",
        "url": "http://arxiv.org/abs/2507.15275v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15275v1",
        "arxiv_id": "2507.15275v1",
        "authors": [
            "Yuanhe Tian",
            "Junjie Liu",
            "Zhizhou Kou",
            "Yuxiang Li",
            "Yan Song"
        ],
        "submitted": "2025-07-21 06:23:16",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A2TTS: TTS for Low Resource Indian Languages",
        "abstract": "We present a speaker conditioned text-to-speech (TTS) system aimed at\naddressing challenges in generating speech for unseen speakers and supporting\ndiverse Indian languages. Our method leverages a diffusion-based TTS\narchitecture, where a speaker encoder extracts embeddings from short reference\naudio samples to condition the DDPM decoder for multispeaker generation. To\nfurther enhance prosody and naturalness, we employ a cross-attention based\nduration prediction mechanism that utilizes reference audio, enabling more\naccurate and speaker consistent timing. This results in speech that closely\nresembles the target speaker while improving duration modeling and overall\nexpressiveness. Additionally, to improve zero-shot generation, we employed\nclassifier free guidance, allowing the system to generate speech more near\nspeech for unknown speakers. Using this approach, we trained language-specific\nspeaker-conditioned models. Using the IndicSUPERB dataset for multiple Indian\nlanguages such as Bengali, Gujarati, Hindi, Marathi, Malayalam, Punjabi and\nTamil.",
        "url": "http://arxiv.org/abs/2507.15272v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15272v1",
        "arxiv_id": "2507.15272v1",
        "authors": [
            "Ayush Singh Bhadoriya",
            "Abhishek Nikunj Shinde",
            "Isha Pandey",
            "Ganesh Ramakrishnan"
        ],
        "submitted": "2025-07-21 06:20:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "GREAT: Guiding Query Generation with a Trie for Recommending Related Search about Video at Kuaishou",
        "abstract": "Currently, short video platforms have become the primary place for\nindividuals to share experiences and obtain information. To better meet users'\nneeds for acquiring information while browsing short videos, some apps have\nintroduced a search entry at the bottom of videos, accompanied with recommended\nrelevant queries. This scenario is known as query recommendation in\nvideo-related search, where core task is item-to-query (I2Q) recommendation. As\nthis scenario has only emerged in recent years, there is a notable scarcity of\nacademic research and publicly available datasets in this domain. To address\nthis gap, we systematically examine the challenges associated with this\nscenario for the first time. Subsequently, we release a large-scale dataset\nderived from real-world data pertaining to the query recommendation in\nvideo-\\textit{\\textbf{r}}elated \\textit{\\textbf{s}}earch on the\n\\textit{\\textbf{Kuai}}shou app (\\textbf{KuaiRS}). Presently, existing methods\nrely on embeddings to calculate similarity for matching short videos with\nqueries, lacking deep interaction between the semantic content and the query.\nIn this paper, we introduce a novel LLM-based framework named \\textbf{GREAT},\nwhich \\textit{\\textbf{g}}uides que\\textit{\\textbf{r}}y\ng\\textit{\\textbf{e}}ner\\textit{\\textbf{a}}tion with a \\textit{\\textbf{t}}rie to\naddress I2Q recommendation in related search. Specifically, we initially gather\nhigh-quality queries with high exposure and click-through rate to construct a\nquery-based trie. During training, we enhance the LLM's capability to generate\nhigh-quality queries using the query-based trie. In the inference phase, the\nquery-based trie serves as a guide for the token generation. Finally, we\nfurther refine the relevance and literal quality between items and queries via\na post-processing module. Extensive offline and online experiments demonstrate\nthe effectiveness of our proposed method.",
        "url": "http://arxiv.org/abs/2507.15267v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15267v1",
        "arxiv_id": "2507.15267v1",
        "authors": [
            "Ninglu Shao",
            "Jinshan Wang",
            "Chenxu Wang",
            "Qingbiao Li",
            "Xiaoxue Zang",
            "Han Li"
        ],
        "submitted": "2025-07-21 06:10:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SPAR: Scholar Paper Retrieval with LLM-based Agents for Enhanced Academic Search",
        "abstract": "Recent advances in large language models (LLMs) have opened new opportunities\nfor academic literature retrieval. However, existing systems often rely on\nrigid pipelines and exhibit limited reasoning capabilities. We introduce SPAR,\na multi-agent framework that incorporates RefChain-based query decomposition\nand query evolution to enable more flexible and effective search. To facilitate\nsystematic evaluation, we also construct SPARBench, a challenging benchmark\nwith expert-annotated relevance labels. Experimental results demonstrate that\nSPAR substantially outperforms strong baselines, achieving up to +56% F1 on\nAutoScholar and +23% F1 on SPARBench over the best-performing baseline.\nTogether, SPAR and SPARBench provide a scalable, interpretable, and\nhigh-performing foundation for advancing research in scholarly retrieval. Code\nand data will be available at: https://github.com/xiaofengShi/SPAR",
        "url": "http://arxiv.org/abs/2507.15245v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15245v1",
        "arxiv_id": "2507.15245v1",
        "authors": [
            "Xiaofeng Shi",
            "Yuduo Li",
            "Qian Kou",
            "Longbin Yu",
            "Jinxin Xie",
            "Hua Zhou"
        ],
        "submitted": "2025-07-21 05:06:53",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest",
        "abstract": "This work investigates the impact of multi-task, multi-lingual, and\nmulti-source learning approaches on the robustness and performance of\npretrained language models. To enhance this analysis, we introduce Subsets of\nInterest (SOI), a novel categorization framework that identifies six distinct\nlearning behavior patterns during training, including forgettable examples,\nunlearned examples, and always correct examples. Through SOI transition\nheatmaps and dataset cartography visualization, we analyze how examples shift\nbetween these categories when transitioning from single-setting to\nmulti-setting configurations. We perform comprehensive experiments across three\nparallel comparisons: multi-task vs. single-task learning using English tasks\n(entailment, paraphrase, sentiment), multi-source vs. single-source learning\nusing sentiment analysis datasets, and multi-lingual vs. single-lingual\nlearning using intent classification in French, English, and Persian. Our\nresults demonstrate that multi-source learning consistently improves\nout-of-distribution performance by up to 7%, while multi-task learning shows\nmixed results with notable gains in similar task combinations. We further\nintroduce a two-stage fine-tuning approach where the second stage leverages\nSOI-based subset selection to achieve additional performance improvements.\nThese findings provide new insights into training dynamics and offer practical\napproaches for optimizing multi-setting language model performance.",
        "url": "http://arxiv.org/abs/2507.15236v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15236v1",
        "arxiv_id": "2507.15236v1",
        "authors": [
            "Shayan Vassef",
            "Amirhossein Dabiriaghdam",
            "Mohammadreza Bakhtiari",
            "Yadollah Yaghoobzadeh"
        ],
        "submitted": "2025-07-21 04:43:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Exploiting Context-dependent Duration Features for Voice Anonymization Attack Systems",
        "abstract": "The temporal dynamics of speech, encompassing variations in rhythm,\nintonation, and speaking rate, contain important and unique information about\nspeaker identity. This paper proposes a new method for representing speaker\ncharacteristics by extracting context-dependent duration embeddings from speech\ntemporal dynamics. We develop novel attack models using these representations\nand analyze the potential vulnerabilities in speaker verification and voice\nanonymization systems.The experimental results show that the developed attack\nmodels provide a significant improvement in speaker verification performance\nfor both original and anonymized data in comparison with simpler\nrepresentations of speech temporal dynamics reported in the literature.",
        "url": "http://arxiv.org/abs/2507.15214v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15214v1",
        "arxiv_id": "2507.15214v1",
        "authors": [
            "Natalia Tomashenko",
            "Emmanuel Vincent",
            "Marc Tommasi"
        ],
        "submitted": "2025-07-21 03:28:56",
        "source": "arxiv",
        "comment": "Accepted at Interspeech-2025"
    },
    {
        "title": "Long-Short Distance Graph Neural Networks and Improved Curriculum Learning for Emotion Recognition in Conversation",
        "abstract": "Emotion Recognition in Conversation (ERC) is a practical and challenging\ntask. This paper proposes a novel multimodal approach, the Long-Short Distance\nGraph Neural Network (LSDGNN). Based on the Directed Acyclic Graph (DAG), it\nconstructs a long-distance graph neural network and a short-distance graph\nneural network to obtain multimodal features of distant and nearby utterances,\nrespectively. To ensure that long- and short-distance features are as distinct\nas possible in representation while enabling mutual influence between the two\nmodules, we employ a Differential Regularizer and incorporate a BiAffine Module\nto facilitate feature interaction. In addition, we propose an Improved\nCurriculum Learning (ICL) to address the challenge of data imbalance. By\ncomputing the similarity between different emotions to emphasize the shifts in\nsimilar emotions, we design a \"weighted emotional shift\" metric and develop a\ndifficulty measurer, enabling a training process that prioritizes learning easy\nsamples before harder ones. Experimental results on the IEMOCAP and MELD\ndatasets demonstrate that our model outperforms existing benchmarks.",
        "url": "http://arxiv.org/abs/2507.15205v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15205v1",
        "arxiv_id": "2507.15205v1",
        "authors": [
            "Xinran Li",
            "Xiujuan Xu",
            "Jiaqi Qiao"
        ],
        "submitted": "2025-07-21 03:12:54",
        "source": "arxiv",
        "comment": "Accepted by the 28th European Conference on Artificial Intelligence\n  (ECAI 2025)"
    },
    {
        "title": "Collaborative Distillation Strategies for Parameter-Efficient Language Model Deployment",
        "abstract": "This paper addresses the challenges of high computational cost and slow\ninference in deploying large language models. It proposes a distillation\nstrategy guided by multiple teacher models. The method constructs several\nteacher models and integrates their output probability distributions and\nintermediate semantic features. This guides the student model to learn from\nmultiple sources of knowledge. As a result, the student model gains stronger\nlanguage understanding and generation ability while maintaining a small\nparameter size. To achieve this, the paper introduces a weighted output fusion\nmechanism, a feature alignment loss function, and an entropy-driven dynamic\nteacher weighting strategy. These components improve the quality and stability\nof knowledge transfer during distillation. Under multi-teacher guidance, the\nstudent model captures semantic information more effectively and demonstrates\nstrong performance across multiple evaluation metrics. In particular, the\nmethod shows high consistency in expression, generalization ability, and task\nadaptability in tasks such as language modeling, text generation, and\nmulti-task learning. The experiments compare the proposed method with several\nwidely adopted distillation approaches. The results further confirm its overall\nadvantages in perplexity, distillation loss, and generation quality. This study\nprovides a feasible technical path for the efficient compression of large-scale\nlanguage models. It also demonstrates the effectiveness of multi-teacher\ncollaborative mechanisms in complex language modeling tasks.",
        "url": "http://arxiv.org/abs/2507.15198v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15198v1",
        "arxiv_id": "2507.15198v1",
        "authors": [
            "Xiandong Meng",
            "Yan Wu",
            "Yexin Tian",
            "Xin Hu",
            "Tianze Kang",
            "Junliang Du"
        ],
        "submitted": "2025-07-21 02:55:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "What Level of Automation is \"Good Enough\"? A Benchmark of Large Language Models for Meta-Analysis Data Extraction",
        "abstract": "Automating data extraction from full-text randomised controlled trials (RCTs)\nfor meta-analysis remains a significant challenge. This study evaluates the\npractical performance of three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini)\nacross tasks involving statistical results, risk-of-bias assessments, and\nstudy-level characteristics in three medical domains: hypertension, diabetes,\nand orthopaedics. We tested four distinct prompting strategies (basic\nprompting, self-reflective prompting, model ensemble, and customised prompts)\nto determine how to improve extraction quality. All models demonstrate high\nprecision but consistently suffer from poor recall by omitting key information.\nWe found that customised prompts were the most effective, boosting recall by up\nto 15\\%. Based on this analysis, we propose a three-tiered set of guidelines\nfor using LLMs in data extraction, matching data types to appropriate levels of\nautomation based on task complexity and risk. Our study offers practical advice\nfor automating data extraction in real-world meta-analyses, balancing LLM\nefficiency with expert oversight through targeted, task-specific automation.",
        "url": "http://arxiv.org/abs/2507.15152v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15152v1",
        "arxiv_id": "2507.15152v1",
        "authors": [
            "Lingbo Li",
            "Anuradha Mathrani",
            "Teo Susnjak"
        ],
        "submitted": "2025-07-20 23:09:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script",
        "abstract": "Homophone normalization, where characters that have the same sound in a\nwriting script are mapped to one character, is a pre-processing step applied in\nAmharic Natural Language Processing (NLP) literature. While this may improve\nperformance reported by automatic metrics, it also results in models that are\nnot able to understand different forms of writing in a single language.\nFurther, there might be impacts in transfer learning, where models trained on\nnormalized data do not generalize well to other languages. In this paper, we\nexperiment with monolingual training and cross-lingual transfer to understand\nthe impacts of normalization on languages that use the Ge'ez script. We then\npropose a post-inference intervention in which normalization is applied to\nmodel predictions instead of training data. With our simple scheme of\npost-inference normalization, we show that we can achieve an increase in BLEU\nscore of up to 1.03 while preserving language features in training. Our work\ncontributes to the broader discussion on technology-facilitated language change\nand calls for more language-aware interventions.",
        "url": "http://arxiv.org/abs/2507.15142v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15142v1",
        "arxiv_id": "2507.15142v1",
        "authors": [
            "Hellina Hailu Nigatu",
            "Atnafu Lambebo Tonja",
            "Henok Biadglign Ademtew",
            "Hizkel Mitiku Alemayehu",
            "Negasi Haile Abadi",
            "Tadesse Destaw Belay",
            "Seid Muhie Yimam"
        ],
        "submitted": "2025-07-20 22:35:08",
        "source": "arxiv",
        "comment": "Paper under review"
    },
    {
        "title": "From Disagreement to Understanding: The Case for Ambiguity Detection in NLI",
        "abstract": "This position paper argues that annotation disagreement in Natural Language\nInference (NLI) is not mere noise but often reflects meaningful interpretive\nvariation, especially when triggered by ambiguity in the premise or hypothesis.\nWhile underspecified guidelines and annotator behavior can contribute to\nvariation, content-based ambiguity offers a process-independent signal of\ndivergent human perspectives. We call for a shift toward ambiguity-aware NLI by\nsystematically identifying ambiguous input pairs and classifying ambiguity\ntypes. To support this, we present a unified framework that integrates existing\ntaxonomies and illustrate key ambiguity subtypes through concrete examples.\nThese examples reveal how ambiguity shapes annotator decisions and motivate the\nneed for targeted detection methods that better align models with human\ninterpretation. A key limitation is the lack of datasets annotated for\nambiguity and subtypes. We propose addressing this gap through new annotated\nresources and unsupervised approaches to ambiguity detection -- paving the way\nfor more robust, explainable, and human-aligned NLI systems.",
        "url": "http://arxiv.org/abs/2507.15114v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15114v1",
        "arxiv_id": "2507.15114v1",
        "authors": [
            "Chathuri Jayaweera",
            "Bonnie Dorr"
        ],
        "submitted": "2025-07-20 20:27:35",
        "source": "arxiv",
        "comment": "8 pages, 6 figures"
    },
    {
        "title": "Click A, Buy B: Rethinking Conversion Attribution in E- Commerce Recommendations",
        "abstract": "User journeys in e-commerce routinely violate the one-to-one assumption that\na clicked item on an advertising platform is the same item later purchased on\nthe merchant's website/app. For a significant number of converting sessions on\nour platform, users click product A but buy product B -- the Click A, Buy B\n(CABB) phenomenon. Training recommendation models on raw click-conversion pairs\ntherefore rewards items that merely correlate with purchases, leading to biased\nlearning and sub-optimal conversion rates. We reframe conversion prediction as\na multi-task problem with separate heads for Click A Buy A (CABA) and Click A\nBuy B (CABB). To isolate informative CABB conversions from unrelated CABB\nconversions, we introduce a taxonomy-aware collaborative filtering weighting\nscheme where each product is first mapped to a leaf node in a product taxonomy,\nand a category-to-category similarity matrix is learned from large-scale\nco-engagement logs. This weighting amplifies pairs that reflect genuine\nsubstitutable or complementary relations while down-weighting coincidental\ncross-category purchases. Offline evaluation on e-commerce sessions reduces\nnormalized entropy by 13.9% versus a last-click attribution baseline. An online\nA/B test on live traffic shows +0.25% gains in the primary business metric.",
        "url": "http://arxiv.org/abs/2507.15113v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15113v1",
        "arxiv_id": "2507.15113v1",
        "authors": [
            "Xiangyu Zeng",
            "Amit Jaspal",
            "Bin Liu",
            "Goutham Panneeru",
            "Kevin Huang",
            "Nicolas Bievre",
            "Mohit Jaggi",
            "Prathap Maniraju",
            "Ankur Jain"
        ],
        "submitted": "2025-07-20 20:25:20",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?",
        "abstract": "Natural Language Inference (NLI) is the task of determining the semantic\nentailment of a premise for a given hypothesis. The task aims to develop\nsystems that emulate natural human inferential processes where commonsense\nknowledge plays a major role. However, existing commonsense resources lack\nsufficient coverage for a variety of premise-hypothesis pairs. This study\nexplores the potential of Large Language Models as commonsense knowledge\ngenerators for NLI along two key dimensions: their reliability in generating\nsuch knowledge and the impact of that knowledge on prediction accuracy. We\nadapt and modify existing metrics to assess LLM factuality and consistency in\ngenerating in this context. While explicitly incorporating commonsense\nknowledge does not consistently improve overall results, it effectively helps\ndistinguish entailing instances and moderately improves distinguishing\ncontradictory and neutral inferences.",
        "url": "http://arxiv.org/abs/2507.15100v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15100v1",
        "arxiv_id": "2507.15100v1",
        "authors": [
            "Chathuri Jayaweera",
            "Brianna Yanqui",
            "Bonnie Dorr"
        ],
        "submitted": "2025-07-20 19:42:45",
        "source": "arxiv",
        "comment": "9 pages, 8 figures and 5 tables"
    },
    {
        "title": "A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic Texts Under Prompt-Influenced Length Variations",
        "abstract": "Synthetic text generated by Large Language Models (LLMs) is increasingly used\nfor further training and improvement of LLMs. Diversity is crucial for the\neffectiveness of synthetic data, and researchers rely on prompt engineering to\nimprove diversity. However, the impact of prompt variations on response text\nlength, and, more importantly, the consequential effect on lexical diversity\nmeasurements, remain underexplored. In this work, we propose Penalty-Adjusted\nType-Token Ratio (PATTR), a diversity metric robust to length variations. We\ngenerate a large synthetic corpus of over 20M words using seven models from the\nLLaMA, OLMo, and Phi families, focusing on a creative writing task of video\nscript generation, where diversity is crucial. We evaluate per-response lexical\ndiversity using PATTR and compare it against existing metrics of Moving-Average\nTTR (MATTR) and Compression Ratio (CR). Our analysis highlights how text length\nvariations introduce biases favoring shorter responses. Unlike existing\nmetrics, PATTR explicitly considers the task-specific target response length\n($L_T$) to effectively mitigate length biases. We further demonstrate the\nutility of PATTR in filtering the top-10/100/1,000 most lexically diverse\nresponses, showing that it consistently outperforms MATTR and CR by yielding on\npar or better diversity with high adherence to $L_T$.",
        "url": "http://arxiv.org/abs/2507.15092v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15092v1",
        "arxiv_id": "2507.15092v1",
        "authors": [
            "Vijeta Deshpande",
            "Ishita Dasgupta",
            "Uttaran Bhattacharya",
            "Somdeb Sarkhel",
            "Saayan Mitra",
            "Anna Rumshisky"
        ],
        "submitted": "2025-07-20 19:14:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling",
        "abstract": "Currently, many studies view DNA sequences as a special type of language and\nutilize Transformers to model them. These studies use fixed-length k-mer\nsegmentation and BPE subword tokenization but lack a systematic evaluation to\ndetermine which is superior. We compare k-mer segmentation with k=1,3,4,5,6, a\n4,096-token BPE vocabulary, and three positional encoding methods-sinusoidal,\nAliBi, and RoPE. Each configuration is trained from scratch in 3, 6, 12, and\n24-layer Transformer encoders and evaluated on GUE benchmark dataset. In\ngeneral, BPE delivers higher and more stable performance across tasks by\ncompressing frequent motifs into variable-length tokens, reducing sequence\nlength, and improving model generalization. RoPE excels at capturing periodic\nmotifs and extrapolating to long sequences, while AliBi also performs well on\ntasks driven by local dependencies. In terms of depth, we observe significant\ngains when increasing layers from 3 to 12, with only marginal improvements or\nslight overfitting at 24 layers. This study provides practical guidance for\ndesigning tokenization and positional encoding in DNA Transformer models.",
        "url": "http://arxiv.org/abs/2507.15087v1",
        "pdf_url": "http://arxiv.org/pdf/2507.15087v1",
        "arxiv_id": "2507.15087v1",
        "authors": [
            "Chenlei Gong",
            "Yuanhe Tian",
            "Lei Mao",
            "Yan Song"
        ],
        "submitted": "2025-07-20 19:02:07",
        "source": "arxiv",
        "comment": null
    }
]