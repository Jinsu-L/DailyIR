[
    {
        "title": "JointRank: Rank Large Set with Single Pass",
        "abstract": "Efficiently ranking relevant items from large candidate pools is a\ncornerstone of modern information retrieval systems -- such as web search,\nrecommendation, and retrieval-augmented generation. Listwise rerankers, which\nimprove relevance by jointly considering multiple candidates, are often limited\nin practice: either by model input size constraints, or by degraded quality\nwhen processing large sets. We propose a model-agnostic method for fast\nreranking large sets that exceed a model input limits. The method first\npartitions candidate items into overlapping blocks, each of which is ranked\nindependently in parallel. Implicit pairwise comparisons are then derived from\nthese local rankings. Finally, these comparisons are aggregated to construct a\nglobal ranking using algorithms such as Winrate or PageRank. Experiments on\nTREC DL-2019 show that our method achieves an nDCG@10 of 70.88 compared to the\n57.68 for full-context listwise approach using gpt-4.1-mini as long-context\nmodel, while reducing latency from 21 to 8 seconds.\n  The implementation of the algorithm and the experiments is available in the\nrepository: https://github.com/V3RGANz/jointrank",
        "url": "http://arxiv.org/abs/2506.22262v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22262v1",
        "arxiv_id": "2506.22262v1",
        "authors": [
            "Evgeny Dedov"
        ],
        "submitted": "2025-06-27 14:30:12",
        "source": "arxiv",
        "comment": "ICTIR'25 Accepted",
        "score": 26,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'listwise' (score: +3)",
            "Found 'pairwise' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'web search' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)",
            "Found 'trec' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "The paper focuses on efficient ranking methods for large candidate pools, which is a key aspect of information retrieval. The proposed JointRank method is model-agnostic and can be applied to various domains, including e-commerce. Although it does not specifically address query understanding, ranking models, or user behavior modeling, it is still relevant to the broader field of information retrieval and search technologies."
    },
    {
        "title": "UiS-IAI@LiveRAG: Retrieval-Augmented Information Nugget-Based Generation of Responses",
        "abstract": "Retrieval-augmented generation (RAG) faces challenges related to factual\ncorrectness, source attribution, and response completeness. The LiveRAG\nChallenge hosted at SIGIR'25 aims to advance RAG research using a fixed corpus\nand a shared, open-source LLM. We propose a modular pipeline that operates on\ninformation nuggets-minimal, atomic units of relevant information extracted\nfrom retrieved documents. This multistage pipeline encompasses query rewriting,\npassage retrieval and reranking, nugget detection and clustering, cluster\nranking and summarization, and response fluency enhancement. This design\ninherently promotes grounding in specific facts, facilitates source\nattribution, and ensures maximum information inclusion within length\nconstraints. In this challenge, we extend our focus to also address the\nretrieval component of RAG, building upon our prior work on multi-faceted query\nrewriting. Furthermore, for augmented generation, we concentrate on improving\ncontext curation capabilities, maximizing the breadth of information covered in\nthe response while ensuring pipeline efficiency. Our results show that\ncombining original queries with a few sub-query rewrites boosts recall, while\nincreasing the number of documents used for reranking and generation beyond a\ncertain point reduces effectiveness, without improving response quality.",
        "url": "http://arxiv.org/abs/2506.22210v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22210v1",
        "arxiv_id": "2506.22210v1",
        "authors": [
            "Weronika ≈Åajewska",
            "Ivica Kostric",
            "Gabriel Iturra-Bocaz",
            "Mariam Arustashvili",
            "Krisztian Balog"
        ],
        "submitted": "2025-06-27 13:29:25",
        "source": "arxiv",
        "comment": null,
        "score": 23,
        "keyword_reasons": [
            "Found 'passage retrieval' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)",
            "Found 'sigir' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper focuses on Retrieval-Augmented Generation (RAG), which is related to query understanding and ranking models, but it's not directly aligned with my primary interests in information retrieval and search technologies. The paper's emphasis on natural language processing and data mining is more relevant, but it's not a central match."
    },
    {
        "title": "Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement",
        "abstract": "The presence of social biases in Natural Language Processing (NLP) and\nInformation Retrieval (IR) systems is an ongoing challenge, which underlines\nthe importance of developing robust approaches to identifying and evaluating\nsuch biases. In this paper, we aim to address this issue by leveraging Large\nLanguage Models (LLMs) to detect and measure gender bias in passage ranking.\nExisting gender fairness metrics rely on lexical- and frequency-based measures,\nleading to various limitations, e.g., missing subtle gender disparities.\nBuilding on our LLM-based gender bias detection method, we introduce a novel\ngender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to\naddress existing limitations. To measure the effectiveness of our proposed\nmetric and study LLMs' effectiveness in detecting gender bias, we annotate a\nsubset of the MS MARCO Passage Ranking collection and release our new gender\nbias collection, called MSMGenderBias, to foster future research in this area.\nOur extensive experimental results on various ranking models show that our\nproposed metric offers a more detailed evaluation of fairness compared to\nprevious metrics, with improved alignment to human labels (58.77% for\nGrep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa\nagreement), effectively distinguishing gender bias in ranking. By integrating\nLLM-driven bias detection, an improved fairness metric, and gender bias\nannotations for an established dataset, this work provides a more robust\nframework for analyzing and mitigating bias in IR systems.",
        "url": "http://arxiv.org/abs/2506.22372v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22372v1",
        "arxiv_id": "2506.22372v1",
        "authors": [
            "Maryam Mousavian",
            "Zahra Abbasiantaeb",
            "Mohammad Aliannejadi",
            "Fabio Crestani"
        ],
        "submitted": "2025-06-27 16:39:12",
        "source": "arxiv",
        "comment": "Accepted by ACM SIGIR Conference on Innovative Concepts and Theories\n  in Information Retrieval (ICTIR 2025)",
        "score": 12,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 7,
        "llm_reason": "The paper is relevant to your research interests in Information Retrieval and Search technologies, particularly in the area of query understanding and ranking models. The use of Large Language Models (LLMs) for gender bias detection and measurement is also related to your interests in NLP and data mining. However, the focus on gender bias detection and measurement is somewhat specific and may not be directly aligned with your core research themes."
    },
    {
        "title": "Literature-Grounded Novelty Assessment of Scientific Ideas",
        "abstract": "Automated scientific idea generation systems have made remarkable progress,\nyet the automatic evaluation of idea novelty remains a critical and\nunderexplored challenge. Manual evaluation of novelty through literature review\nis labor-intensive, prone to error due to subjectivity, and impractical at\nscale. To address these issues, we propose the Idea Novelty Checker, an\nLLM-based retrieval-augmented generation (RAG) framework that leverages a\ntwo-stage retrieve-then-rerank approach. The Idea Novelty Checker first\ncollects a broad set of relevant papers using keyword and snippet-based\nretrieval, then refines this collection through embedding-based filtering\nfollowed by facet-based LLM re-ranking. It incorporates expert-labeled examples\nto guide the system in comparing papers for novelty evaluation and in\ngenerating literature-grounded reasoning. Our extensive experiments demonstrate\nthat our novelty checker achieves approximately 13% higher agreement than\nexisting approaches. Ablation studies further showcases the importance of the\nfacet-based re-ranker in identifying the most relevant literature for novelty\nevaluation.",
        "url": "http://arxiv.org/abs/2506.22026v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22026v1",
        "arxiv_id": "2506.22026v1",
        "authors": [
            "Simra Shahid",
            "Marissa Radensky",
            "Raymond Fok",
            "Pao Siangliulue",
            "Daniel S. Weld",
            "Tom Hope"
        ],
        "submitted": "2025-06-27 08:47:28",
        "source": "arxiv",
        "comment": null,
        "score": 11,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a framework for novelty assessment of scientific ideas, leveraging LLM-based retrieval-augmented generation and embedding-based filtering. While it touches on retrieval and ranking, the focus is on novelty evaluation, which is not directly related to query understanding, ranking models, or user behavior modeling, which are core interests in Information Retrieval. The paper's relevance to NLP and data mining is also limited, as it primarily deals with scientific idea generation and evaluation."
    },
    {
        "title": "DAPFAM: A Domain-Aware Patent Retrieval Dataset Aggregated at the Family Level",
        "abstract": "In the landscape of publicly available patent retrieval datasets, the need\nfor explicit indomain and out-of-domain labeling, multi-jurisdiction coverage,\nbalanced query domain representation and manageable sizes that support sub\ndocument level experiments on moderate computational resources is often\noverlooked. To address these gaps, we propose DAPFAM, a new open access\ndomain-aware patent retrieval dataset constructed at the simple-family level.\nThe dataset contains 1,247 domain balanced full text query families and 45,336\nfull text target families. The dataset is enriched by clear relevance judgments\n(forward/backward citations as positive links, random negatives), as well as\nexplicit in-domain or out-of-domain relationships via a novel proposed\nlabelling scheme based on via International Patent Classification (IPC) codes,\nresulting in 49,869 evaluation pairs. The dataset is multi jurisdictional,\nrequires little to no preprocessing for retrieval evaluation, and remains of a\nsize manageable for entities with limited ressources allowing for sub document\nlevel retrieval experiments without excessive computational costs. We describe\nour three-step data-curation pipeline, present comprehensive dataset\nstatistics, and provide baseline experiments using lexical and neural retrieval\nmethods. Our baseline experiments highlight significant challenges in\ncrossdomain patent retrieval. The dataset will be publicly available (for now\nthe access link is this repository:\nhttps://osf.io/vbyzd/?view_only=1a40242e0d1941a58aa854af3e50cf6b).",
        "url": "http://arxiv.org/abs/2506.22141v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22141v1",
        "arxiv_id": "2506.22141v1",
        "authors": [
            "Iliass Ayaou",
            "Denis Cavallucci",
            "Hicham Chibane"
        ],
        "submitted": "2025-06-27 11:34:51",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a new patent retrieval dataset, DAPFAM, which is domain-aware and multi-jurisdictional. While it touches on retrieval methods, the focus is on dataset construction and evaluation, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval. The paper's relevance to the user's research is somewhat limited."
    },
    {
        "title": "ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation",
        "abstract": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization.",
        "url": "http://arxiv.org/abs/2506.21931v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21931v1",
        "arxiv_id": "2506.21931v1",
        "authors": [
            "Reza Yousefi Maragheh",
            "Pratheek Vadla",
            "Priyank Gupta",
            "Kai Zhao",
            "Aysenur Inan",
            "Kehui Yao",
            "Jianpeng Xu",
            "Praveen Kanumala",
            "Jason Cho",
            "Sushant Kumar"
        ],
        "submitted": "2025-06-27 05:45:59",
        "source": "arxiv",
        "comment": null,
        "score": 9,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'personalization' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores Retrieval-Augmented Generation (RAG) for personalized recommendation, which is related to search technologies and query understanding. However, the focus on recommendation systems and user behavior modeling is not directly aligned with the user's primary interest in Information Retrieval, especially in areas that require deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval",
        "abstract": "The HLTCOE LiveRAG submission utilized the GPT-researcher framework for\nresearching the context of the question, filtering the returned results, and\ngenerating the final answer. The retrieval system was a ColBERT bi-encoder\narchitecture, which represents a passage with many dense tokens. Retrieval used\na local, compressed index of the FineWeb10-BT collection created with PLAID-X,\nusing a model fine-tuned for multilingual retrieval. Query generation from\ncontext was done with Qwen2.5-7B-Instruct, while filtering was accomplished\nwith m2-bert-80M-8k-retrieval. Up to nine passages were used as context to\ngenerate an answer using Falcon3-10B. This system placed 5th in the LiveRAG\nautomatic evaluation for correctness with a score of 1.07.",
        "url": "http://arxiv.org/abs/2506.22356v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22356v1",
        "arxiv_id": "2506.22356v1",
        "authors": [
            "Kevin Duh",
            "Eugene Yang",
            "Orion Weller",
            "Andrew Yates",
            "Dawn Lawrie"
        ],
        "submitted": "2025-06-27 16:08:39",
        "source": "arxiv",
        "comment": "5 pages, 1 figure",
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper focuses on a specific application of ColBERT retrieval in a question-answering task, which is related to query understanding and ranking models. However, the paper's primary focus is on the system's architecture and performance, rather than the underlying theoretical foundations or user behavior modeling, which limits its relevance to my research interests."
    },
    {
        "title": "HyReC: Exploring Hybrid-based Retriever for Chinese",
        "abstract": "Hybrid-based retrieval methods, which unify dense-vector and lexicon-based\nretrieval, have garnered considerable attention in the industry due to\nperformance enhancement. However, despite their promising results, the\napplication of these hybrid paradigms in Chinese retrieval contexts has\nremained largely underexplored. In this paper, we introduce HyReC, an\ninnovative end-to-end optimization method tailored specifically for\nhybrid-based retrieval in Chinese. HyReC enhances performance by integrating\nthe semantic union of terms into the representation model. Additionally, it\nfeatures the Global-Local-Aware Encoder (GLAE) to promote consistent semantic\nsharing between lexicon-based and dense retrieval while minimizing the\ninterference between them. To further refine alignment, we incorporate a\nNormalization Module (NM) that fosters mutual benefits between the retrieval\napproaches. Finally, we evaluate HyReC on the C-MTEB retrieval benchmark to\ndemonstrate its effectiveness.",
        "url": "http://arxiv.org/abs/2506.21913v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21913v1",
        "arxiv_id": "2506.21913v1",
        "authors": [
            "Zunran Wang",
            "Zheng Shenpeng",
            "Wang Shenglan",
            "Minghui Zhao",
            "Zhonghua Li"
        ],
        "submitted": "2025-06-27 04:57:01",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'retriever' (score: +3)",
            "Found 'dense retrieval' (score: +3)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on a specific retrieval method for Chinese, which is not directly related to the user's interests in Information Retrieval, query understanding, ranking models, and user behavior modeling. Although it mentions hybrid-based retrieval, the context is limited to Chinese and does not explore query understanding, ranking models, or user behavior modeling, making it only loosely relevant to the user's research interests."
    },
    {
        "title": "RiverEcho: Real-Time Interactive Digital System for Ancient Yellow River Culture",
        "abstract": "The Yellow River is China's mother river and a cradle of human civilization.\nThe ancient Yellow River culture is, moreover, an indispensable part of human\nart history. To conserve and inherit the ancient Yellow River culture, we\ndesigned RiverEcho, a real-time interactive system that responds to voice\nqueries using a large language model and a cultural knowledge dataset,\ndelivering explanations through a talking-head digital human. Specifically, we\nbuilt a knowledge database focused on the ancient Yellow River culture,\nincluding the collection of historical texts and the processing pipeline.\nExperimental results demonstrate that leveraging Retrieval-Augmented Generation\n(RAG) on the proposed dataset enhances the response quality of the Large\nLanguage Model(LLM), enabling the system to generate more professional and\ninformative responses. Our work not only diversifies the means of promoting\nYellow River culture but also provides users with deeper cultural insights.",
        "url": "http://arxiv.org/abs/2506.21865v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21865v1",
        "arxiv_id": "2506.21865v1",
        "authors": [
            "Haofeng Wang",
            "Yilin Guo",
            "Zehao Li",
            "Tong Yue",
            "Yizong Wang",
            "Enci Zhang",
            "Rongqun Lin",
            "Feng Gao",
            "Shiqi Wang",
            "Siwei Ma"
        ],
        "submitted": "2025-06-27 02:40:00",
        "source": "arxiv",
        "comment": "IEEE International Conference on Multimedia and Expo Workshop,\n  2025.(Accepted)",
        "score": 7,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on a specific cultural topic, ancient Yellow River culture, and uses a large language model and cultural knowledge dataset, which is not related to the user's areas of interest."
    },
    {
        "title": "Leveraging In-Context Learning for Political Bias Testing of LLMs",
        "abstract": "A growing body of work has been querying LLMs with political questions to\nevaluate their potential biases. However, this probing method has limited\nstability, making comparisons between models unreliable. In this paper, we\nargue that LLMs need more context. We propose a new probing task, Questionnaire\nModeling (QM), that uses human survey data as in-context examples. We show that\nQM improves the stability of question-based bias evaluation, and demonstrate\nthat it may be used to compare instruction-tuned models to their base versions.\nExperiments with LLMs of various sizes indicate that instruction tuning can\nindeed change the direction of bias. Furthermore, we observe a trend that\nlarger models are able to leverage in-context examples more effectively, and\ngenerally exhibit smaller bias scores in QM. Data and code are publicly\navailable.",
        "url": "http://arxiv.org/abs/2506.22232v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22232v1",
        "arxiv_id": "2506.22232v1",
        "authors": [
            "Patrick Haller",
            "Jannis Vamvas",
            "Rico Sennrich",
            "Lena A. J√§ger"
        ],
        "submitted": "2025-06-27 13:49:37",
        "source": "arxiv",
        "comment": "ACL 2025",
        "score": 5,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on evaluating the potential biases of Large Language Models (LLMs) using a new probing task, Questionnaire Modeling (QM), which leverages human survey data as in-context examples. While it touches on the topic of model evaluation, it does not directly relate to query understanding, ranking models, or user behavior modeling in the context of Information Retrieval, which are the user's primary research interests."
    },
    {
        "title": "Decoding Machine Translationese in English-Chinese News: LLMs vs. NMTs",
        "abstract": "This study explores Machine Translationese (MTese) -- the linguistic\npeculiarities of machine translation outputs -- focusing on the\nunder-researched English-to-Chinese language pair in news texts. We construct a\nlarge dataset consisting of 4 sub-corpora and employ a comprehensive five-layer\nfeature set. Then, a chi-square ranking algorithm is applied for feature\nselection in both classification and clustering tasks. Our findings confirm the\npresence of MTese in both Neural Machine Translation systems (NMTs) and Large\nLanguage Models (LLMs). Original Chinese texts are nearly perfectly\ndistinguishable from both LLM and NMT outputs. Notable linguistic patterns in\nMT outputs are shorter sentence lengths and increased use of adversative\nconjunctions. Comparing LLMs and NMTs, we achieve approximately 70%\nclassification accuracy, with LLMs exhibiting greater lexical diversity and\nNMTs using more brackets. Additionally, translation-specific LLMs show lower\nlexical diversity but higher usage of causal conjunctions compared to generic\nLLMs. Lastly, we find no significant differences between LLMs developed by\nChinese firms and their foreign counterparts.",
        "url": "http://arxiv.org/abs/2506.22050v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22050v1",
        "arxiv_id": "2506.22050v1",
        "authors": [
            "Delu Kong",
            "Lieve Macken"
        ],
        "submitted": "2025-06-27 09:45:37",
        "source": "arxiv",
        "comment": "14 pages, 5 figures, 6 tables. Accpeted in MT Summit 2025, Research:\n  Technical track. Official version may be accessed later in the ACL Anthology",
        "score": 5,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Machine Translationese in English-Chinese news texts, exploring linguistic patterns in Neural Machine Translation systems and Large Language Models. While it touches on feature selection and classification accuracy, it does not relate to query understanding, ranking models, or user behavior modeling in Information Retrieval, which are core areas of your research interests."
    },
    {
        "title": "CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware Layout Design",
        "abstract": "Automated content-aware layout generation -- the task of arranging visual\nelements such as text, logos, and underlays on a background canvas -- remains a\nfundamental yet under-explored problem in intelligent design systems. While\nrecent advances in deep generative models and large language models (LLMs) have\nshown promise in structured content generation, most existing approaches lack\ngrounding in contextual design exemplars and fall short in handling semantic\nalignment and visual coherence. In this work we introduce CAL-RAG, a\nretrieval-augmented, agentic framework for content-aware layout generation that\nintegrates multimodal retrieval, large language models, and collaborative\nagentic reasoning. Our system retrieves relevant layout examples from a\nstructured knowledge base and invokes an LLM-based layout recommender to\npropose structured element placements. A vision-language grader agent evaluates\nthe layout with visual metrics, and a feedback agent provides targeted\nrefinements, enabling iterative improvement. We implement our framework using\nLangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in\nsemantic and structural variability. CAL-RAG achieves state-of-the-art\nperformance across multiple layout metrics -- including underlay effectiveness,\nelement alignment, and overlap -- substantially outperforming strong baselines\nsuch as LayoutPrompter. These results demonstrate that combining retrieval\naugmentation with agentic multi-step reasoning yields a scalable,\ninterpretable, and high-fidelity solution for automated layout generation.",
        "url": "http://arxiv.org/abs/2506.21934v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21934v1",
        "arxiv_id": "2506.21934v1",
        "authors": [
            "Najmeh Forouzandehmehr",
            "Reza Yousefi Maragheh",
            "Sriram Kollipara",
            "Kai Zhao",
            "Topojoy Biswas",
            "Evren Korpeoglu",
            "Kannan Achan"
        ],
        "submitted": "2025-06-27 06:09:56",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on content-aware layout generation, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions multimodal retrieval, the context is different from the user's interests in IR and NLP."
    },
    {
        "title": "Towards Transparent AI: A Survey on Explainable Large Language Models",
        "abstract": "Large Language Models (LLMs) have played a pivotal role in advancing\nArtificial Intelligence (AI). However, despite their achievements, LLMs often\nstruggle to explain their decision-making processes, making them a 'black box'\nand presenting a substantial challenge to explainability. This lack of\ntransparency poses a significant obstacle to the adoption of LLMs in\nhigh-stakes domain applications, where interpretability is particularly\nessential. To overcome these limitations, researchers have developed various\nexplainable artificial intelligence (XAI) methods that provide\nhuman-interpretable explanations for LLMs. However, a systematic understanding\nof these methods remains limited. To address this gap, this survey provides a\ncomprehensive review of explainability techniques by categorizing XAI methods\nbased on the underlying transformer architectures of LLMs: encoder-only,\ndecoder-only, and encoder-decoder models. Then these techniques are examined in\nterms of their evaluation for assessing explainability, and the survey further\nexplores how these explanations are leveraged in practical applications.\nFinally, it discusses available resources, ongoing research challenges, and\nfuture directions, aiming to guide continued efforts toward developing\ntransparent and responsible LLMs.",
        "url": "http://arxiv.org/abs/2506.21812v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21812v1",
        "arxiv_id": "2506.21812v1",
        "authors": [
            "Avash Palikhe",
            "Zhenyu Yu",
            "Zichong Wang",
            "Wenbin Zhang"
        ],
        "submitted": "2025-06-26 23:25:22",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on explainable AI methods for large language models, which is a related topic in NLP. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core aspects of information retrieval and search technologies. The paper's relevance is somewhat limited to my interests, but it may still provide some insights into NLP and AI techniques that could be applicable to my research."
    },
    {
        "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements",
        "abstract": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.",
        "url": "http://arxiv.org/abs/2506.22419v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22419v1",
        "arxiv_id": "2506.22419v1",
        "authors": [
            "Bingchen Zhao",
            "Despoina Magka",
            "Minqi Jiang",
            "Xian Li",
            "Roberta Raileanu",
            "Tatiana Shavrina",
            "Jean-Christophe Gagnon-Audet",
            "Kelvin Niu",
            "Shagun Sodhani",
            "Michael Shvartsman",
            "Andrei Lupu",
            "Alisia Lupidi",
            "Edan Toledo",
            "Karen Hambardzumyan",
            "Martin Josifoski",
            "Thomas Foster",
            "Lucia Cipolina-Kun",
            "Abhishek Charnalia",
            "Derek Dunfield",
            "Alexander H. Miller",
            "Oisin Mac Aodha",
            "Jakob Foerster",
            "Yoram Bachrach"
        ],
        "submitted": "2025-06-27 17:44:32",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is not related to Information Retrieval, Search technologies, or Natural Language Processing, and does not involve query understanding, ranking models, or user behavior modeling. The topic of large language models and their ability to reproduce existing work is not directly relevant to the user's research interests."
    },
    {
        "title": "Education-Oriented Graph Retrieval-Augmented Generation for Learning Path Recommendation",
        "abstract": "Learning path recommendation seeks to provide learners with a structured\nsequence of learning items (e.g., knowledge concepts or exercises) to optimize\ntheir learning efficiency. Despite significant efforts in this area, most\nexisting methods primarily rely on prerequisite relationships, which present\ntwo major limitations: 1) Many educational datasets do not explicitly provide\nprerequisite relationships between knowledge concepts, hindering the\napplication of current learning path recommendation methods. 2) Relying solely\non prerequisite relationships as the sole knowledge structure can impede\nlearning progress and negatively impact student outcomes. To address these\nchallenges, we propose a novel approach, Discrimination Learning Enhances\nLearning Path Recommendation (DLELP), which enhances learning path\nrecommendations by incorporating both prerequisite and similarity relationships\nbetween knowledge concepts. Specifically, we introduce a knowledge concept\nstructure graph generation module that adaptively constructs knowledge concept\nstructure graphs for different educational datasets, significantly improving\nthe generalizability of learning path recommendation methods. We then propose a\nDiscrimination Learning-driven Reinforcement Learning (DLRL) framework, which\nmitigates the issue of blocked learning paths, further enhancing the efficacy\nof learning path recommendations. Finally, we conduct extensive experiments on\nthree benchmark datasets, demonstrating that our method not only achieves\nstate-of-the-art performance but also provides interpretable reasoning for the\nrecommended learning paths.",
        "url": "http://arxiv.org/abs/2506.22303v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22303v1",
        "arxiv_id": "2506.22303v1",
        "authors": [
            "Xinghe Cheng",
            "Zihan Zhang",
            "Jiapu Wang",
            "Liangda Fang",
            "Chaobo He",
            "Quanlong Guan",
            "Shirui Pan",
            "Weiqi Luo"
        ],
        "submitted": "2025-06-27 15:15:42",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on learning path recommendation, which is not directly related to information retrieval, search technologies, or query understanding. Although it involves graph retrieval and generation, the context is educational and does not align with the user's primary research interests in IR and NLP."
    },
    {
        "title": "Exploring Modularity of Agentic Systems for Drug Discovery",
        "abstract": "Large-language models (LLMs) and agentic systems present exciting\nopportunities to accelerate drug discovery and design. In this study, we\ncritically examine the modularity of LLM-based agentic systems for drug\ndiscovery, i.e., whether parts of the agentic system such as the LLM are\ninterchangeable, a topic that has received limited attention in drug discovery\napplications. We compare the performance of different large language models\n(LLMs) and the effectiveness of tool-calling agents versus code-generating\nagents in this domain. Our case study, comparing performance in orchestrating\ntools for chemistry and drug discovery using an LLM-as-a-judge score, shows\nthat Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative\nlanguage models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and\nNova-Micro. Although we confirm that code-generating agents outperform the\ntool-calling ones on average, we show that this is highly question and model\ndependent. Furthermore, the impact of replacing system prompts is dependent on\nthe specific question asked and the model used, underscoring that -- even in\nthis particular domain -- one cannot just replace language models without\nconsidering prompt re-engineering. Our study highlights the necessity of\nfurther research into the modularity of agentic systems to enable the\ndevelopment of stable and scalable solutions for real-world problems.",
        "url": "http://arxiv.org/abs/2506.22189v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22189v1",
        "arxiv_id": "2506.22189v1",
        "authors": [
            "Laura van Weesep",
            "Samuel Genheden",
            "Ola Engkvist",
            "Jens Sj√∂lund"
        ],
        "submitted": "2025-06-27 12:57:00",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on drug discovery and large language models, which is outside your primary focus areas."
    },
    {
        "title": "Lost at the Beginning of Reasoning",
        "abstract": "Recent advancements in large language models (LLMs) have significantly\nadvanced complex reasoning capabilities, particularly through extended\nchain-of-thought (CoT) reasoning that incorporates mechanisms such as\nbacktracking, self-reflection and self-correction. Despite these developments,\nthe self-correction abilities of LLMs during long CoT reasoning remain\nunderexplored. And recent findings on overthinking suggest that such models\noften engage in unnecessarily redundant reasoning. In this work, we empirically\nshow that the first reasoning step exerts a disproportionately large influence\non the final prediction - errors introduced at this stage can substantially\ndegrade subsequent reasoning quality. This phenomenon is consistently observed\nacross two state-of-the-art open-source reasoning model families: DeepSeek-R1\nand Qwen3. To address this, we propose an efficient sampling strategy that\nleverages a reward model to identify and retain high-quality first reasoning\nsteps while discarding suboptimal ones, achieving up to a 70% reduction in\ninference cost without sacrificing accuracy. Finally, we introduce a new\nbenchmark specifically constructed with deliberately flawed first reasoning\nsteps to systematically evaluate model self-correction capabilities, offering a\nfoundation for future research on robust reasoning in LLMs.",
        "url": "http://arxiv.org/abs/2506.22058v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22058v1",
        "arxiv_id": "2506.22058v1",
        "authors": [
            "Baohao Liao",
            "Xinyi Chen",
            "Sara Rajaee",
            "Yuhui Xu",
            "Christian Herold",
            "Anders S√∏gaard",
            "Maarten de Rijke",
            "Christof Monz"
        ],
        "submitted": "2025-06-27 09:53:57",
        "source": "arxiv",
        "comment": "9 pages, 5 figures, 2 tables",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the self-correction abilities of large language models during complex reasoning tasks, which is a relevant topic in Natural Language Processing. However, it does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval. The paper's focus on long chain-of-thought reasoning and model self-correction is somewhat related to my interests, but it does not align with my primary focus on information retrieval and real-time relevance optimization."
    },
    {
        "title": "Don't Trust Generative Agents to Mimic Communication on Social Networks Unless You Benchmarked their Empirical Realism",
        "abstract": "The ability of Large Language Models (LLMs) to mimic human behavior triggered\na plethora of computational social science research, assuming that empirical\nstudies of humans can be conducted with AI agents instead. Since there have\nbeen conflicting research findings on whether and when this hypothesis holds,\nthere is a need to better understand the differences in their experimental\ndesigns. We focus on replicating the behavior of social network users with the\nuse of LLMs for the analysis of communication on social networks. First, we\nprovide a formal framework for the simulation of social networks, before\nfocusing on the sub-task of imitating user communication. We empirically test\ndifferent approaches to imitate user behavior on X in English and German. Our\nfindings suggest that social simulations should be validated by their empirical\nrealism measured in the setting in which the simulation components were fitted.\nWith this paper, we argue for more rigor when applying generative-agent-based\nmodeling for social simulation.",
        "url": "http://arxiv.org/abs/2506.21974v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21974v1",
        "arxiv_id": "2506.21974v1",
        "authors": [
            "Simon M√ºnker",
            "Nils Schwager",
            "Achim Rettinger"
        ],
        "submitted": "2025-06-27 07:32:16",
        "source": "arxiv",
        "comment": "11 pages, 1 figure, 3 tables",
        "score": 3,
        "keyword_reasons": [
            "Found 'user behavior' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. While it touches on the topic of language models, it focuses on social network simulations and empirical realism, which is not a primary area of interest for you."
    },
    {
        "title": "A Dual-Layered Evaluation of Geopolitical and Cultural Bias in LLMs",
        "abstract": "As large language models (LLMs) are increasingly deployed across diverse\nlinguistic and cultural contexts, understanding their behavior in both factual\nand disputable scenarios is essential, especially when their outputs may shape\npublic opinion or reinforce dominant narratives. In this paper, we define two\ntypes of bias in LLMs: model bias (bias stemming from model training) and\ninference bias (bias induced by the language of the query), through a two-phase\nevaluation. Phase 1 evaluates LLMs on factual questions where a single\nverifiable answer exists, assessing whether models maintain consistency across\ndifferent query languages. Phase 2 expands the scope by probing geopolitically\nsensitive disputes, where responses may reflect culturally embedded or\nideologically aligned perspectives. We construct a manually curated dataset\nspanning both factual and disputable QA, across four languages and question\ntypes. The results show that Phase 1 exhibits query language induced alignment,\nwhile Phase 2 reflects an interplay between the model's training context and\nquery language. This paper offers a structured framework for evaluating LLM\nbehavior across neutral and sensitive topics, providing insights for future LLM\ndeployment and culturally aware evaluation practices in multilingual contexts.",
        "url": "http://arxiv.org/abs/2506.21881v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21881v1",
        "arxiv_id": "2506.21881v1",
        "authors": [
            "Sean Kim",
            "Hyuhng Joon Kim"
        ],
        "submitted": "2025-06-27 03:37:15",
        "source": "arxiv",
        "comment": "This paper is accepted to ACL Student Research Workshop (SRW) 2025",
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the bias in large language models (LLMs) in different linguistic and cultural contexts, which is related to my interest in Natural Language Processing (NLP). However, the focus on LLMs and their evaluation framework is not directly aligned with my primary research interest in Information Retrieval (IR) and Search technologies, particularly query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation",
        "abstract": "Recent multi-modal Large Language Models (LLMs) such as GPT-4o have\ndemonstrated strong capabilities of direct speech interaction. However, the\nlack of specialized and comprehensive benchmarks for end-to-end speech LLM\nevaluation hinders optimizing the user experience of Audio LLMs in real-world\napplications. Existing evaluation methods often adapt text-based benchmarks,\noverlooking speech's unique characteristics and challenges, including prosody,\nhomophones, stuttering, and differing user expectations. Here, we present a\nnovel approach to thoroughly evaluate LLMs in practical speech conversations.\nWe systematically curate real-world chat data relevant to spoken scenarios,\nintroduce diversity in speaker attributes and acoustic conditions, and augment\nthe dataset with speech-specific phenomena. We further design a query-aware\nevaluation method to use customized evaluation checklists and prompts to\nenhance the accuracy of automatic evaluation. We conduct comprehensive testing\nand detailed analysis of various mainstream speech models, revealing\nsignificant differences in model performance across different speech scenarios.\nThe use of query-aware evaluation further enables a finer-grained assessment\nunder various speech-specific scenarios. Our benchmark can provide valuable\ninsights for speech model development and evaluation.",
        "url": "http://arxiv.org/abs/2506.21875v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21875v1",
        "arxiv_id": "2506.21875v1",
        "authors": [
            "Jian Zhang",
            "Linhao Zhang",
            "Bokai Lei",
            "Chuhan Wu",
            "Wei Jia",
            "Xiao Zhou"
        ],
        "submitted": "2025-06-27 03:18:45",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper presents a benchmarking framework for evaluating Large Language Models in natural speech conversations, which is related to Natural Language Processing (NLP). However, it does not directly address query understanding, ranking models, or user behavior modeling in Information Retrieval (IR), which are the user's primary research interests."
    },
    {
        "title": "PARSI: Persian Authorship Recognition via Stylometric Integration",
        "abstract": "The intricate linguistic, stylistic, and metrical aspects of Persian\nclassical poetry pose a challenge for computational authorship attribution. In\nthis work, we present a versatile framework to determine authorship among 67\nprominent poets. We employ a multi-input neural framework consisting of a\ntransformer-based language encoder complemented by features addressing the\nsemantic, stylometric, and metrical dimensions of Persian poetry. Our feature\nset encompasses 100-dimensional Word2Vec embeddings, seven stylometric\nmeasures, and categorical encodings of poetic form and meter. We compiled a\nvast corpus of 647,653 verses of the Ganjoor digital collection, validating the\ndata through strict preprocessing and author verification while preserving\npoem-level splitting to prevent overlap. This work employs verse-level\nclassification and majority and weighted voting schemes in evaluation,\nrevealing that weighted voting yields 71% accuracy. We further investigate\nthreshold-based decision filtering, allowing the model to generate highly\nconfident predictions, achieving 97% accuracy at a 0.9 threshold, though at\nlower coverage. Our work focuses on the integration of deep representational\nforms with domain-specific features for improved authorship attribution. The\nresults illustrate the potential of our approach for automated classification\nand the contribution to stylistic analysis, authorship disputes, and general\ncomputational literature research. This research will facilitate further\nresearch on multilingual author attribution, style shift, and generative\nmodeling of Persian poetry.",
        "url": "http://arxiv.org/abs/2506.21840v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21840v1",
        "arxiv_id": "2506.21840v1",
        "authors": [
            "Kourosh Shahnazari",
            "Mohammadali Keshtparvar",
            "Seyed Moein Ayyoubzadeh"
        ],
        "submitted": "2025-06-27 01:08:52",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus is on Persian authorship recognition, which is a specific domain and task, and does not align with your broader interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Evaluating List Construction and Temporal Understanding capabilities of Large Language Models",
        "abstract": "Large Language Models (LLMs) have demonstrated immense advances in a wide\nrange of natural language tasks. However, these models are susceptible to\nhallucinations and errors on particularly temporal understanding tasks\ninvolving multiple entities in answers. In such tasks, they fail to associate\nentities with accurate time intervals, generate a complete list of entities in\nanswers or reason about events associated with specific temporal bounds.\nExisting works do not extensively evaluate the abilities of the model to\nperform implicit and explicit temporal understanding in a list answer\nconstruction setup. To bridge this gap, we propose the Time referenced List\nbased Question Answering or TLQA benchmark that requires structured answers in\nlist format aligned with corresponding time periods. Our TLQA benchmark,\nrequires both list construction and temporal understanding simultaneously,\nwhich to the best of our knowledge has not been explored in prior benchmarks.\nWe investigate the temporal understanding and list construction capabilities of\nstate-of-the-art generative models on TLQA in closed-book and open-domain\nsettings. Our findings reveal significant shortcomings in current models,\nparticularly their inability to provide complete answers and temporally align\nfacts in a closed-book setup and the need to improve retrieval in open-domain\nsetup, providing clear future directions for research on TLQA. The benchmark\nand code at https://github.com/elixir-research-group/TLQA.",
        "url": "http://arxiv.org/abs/2506.21783v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21783v1",
        "arxiv_id": "2506.21783v1",
        "authors": [
            "Alexandru Dumitru",
            "V Venktesh",
            "Adam Jatowt",
            "Avishek Anand"
        ],
        "submitted": "2025-06-26 21:40:58",
        "source": "arxiv",
        "comment": "Accepted at ICTIR 2025 co-located with SIGIR 2025, 11 pages",
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on evaluating the capabilities of Large Language Models (LLMs) in temporal understanding and list construction, which is related to information retrieval and natural language processing. However, the paper does not directly address query understanding, ranking models, or user behavior modeling, which are core aspects of the user's research interests."
    },
    {
        "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs",
        "abstract": "Current Vision-Language Models (VLMs) struggle with fine-grained spatial\nreasoning, particularly when multi-step logic and precise spatial alignment are\nrequired. In this work, we introduce SpatialReasoner-R1, a vision-language\nreasoning model designed to address these limitations. To construct\nhigh-quality supervision for spatial reasoning, we design a Multi-Model Monte\nCarlo Tree Search (M3CTS) method that generates diverse, logically consistent\nLong Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose\nfine-grained Direct Preference Optimization (fDPO), which introduces\nsegment-specific preference granularity for descriptive grounding and logical\nreasoning, guided by a spatial reward mechanism that evaluates candidate\nresponses based on visual consistency, spatial grounding, and logical\ncoherence. Experimental results demonstrate that fDPO achieves an average\nimprovement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%\ngain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a\nnew SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in\naverage accuracy, while maintaining competitive performance on general\nvision-language tasks.",
        "url": "http://arxiv.org/abs/2506.21656v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21656v1",
        "arxiv_id": "2506.21656v1",
        "authors": [
            "Yifan Shen",
            "Yuanzhe Liu",
            "Jingyuan Zhu",
            "Xu Cao",
            "Xiaofeng Zhang",
            "Yixiao He",
            "Wenming Ye",
            "James Matthew Rehg",
            "Ismini Lourentzou"
        ],
        "submitted": "2025-06-26 18:00:00",
        "source": "arxiv",
        "comment": "29 pages",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Vision-Language Models (VLMs) and spatial reasoning, which is not directly related to Information Retrieval, Search technologies, or query understanding. Although it involves optimization techniques, the context is different from the user's primary research interests."
    },
    {
        "title": "Sequential Diagnosis with Language Models",
        "abstract": "Artificial intelligence holds great promise for expanding access to expert\nmedical knowledge and reasoning. However, most evaluations of language models\nrely on static vignettes and multiple-choice questions that fail to reflect the\ncomplexity and nuance of evidence-based medicine in real-world settings. In\nclinical practice, physicians iteratively formulate and revise diagnostic\nhypotheses, adapting each subsequent question and test to what they've just\nlearned, and weigh the evolving evidence before committing to a final\ndiagnosis. To emulate this iterative process, we introduce the Sequential\nDiagnosis Benchmark, which transforms 304 diagnostically challenging New\nEngland Journal of Medicine clinicopathological conference (NEJM-CPC) cases\ninto stepwise diagnostic encounters. A physician or AI begins with a short case\nabstract and must iteratively request additional details from a gatekeeper\nmodel that reveals findings only when explicitly queried. Performance is\nassessed not just by diagnostic accuracy but also by the cost of physician\nvisits and tests performed. We also present the MAI Diagnostic Orchestrator\n(MAI-DxO), a model-agnostic orchestrator that simulates a panel of physicians,\nproposes likely differential diagnoses and strategically selects high-value,\ncost-effective tests. When paired with OpenAI's o3 model, MAI-DxO achieves 80%\ndiagnostic accuracy--four times higher than the 20% average of generalist\nphysicians. MAI-DxO also reduces diagnostic costs by 20% compared to\nphysicians, and 70% compared to off-the-shelf o3. When configured for maximum\naccuracy, MAI-DxO achieves 85.5% accuracy. These performance gains with MAI-DxO\ngeneralize across models from the OpenAI, Gemini, Claude, Grok, DeepSeek, and\nLlama families. We highlight how AI systems, when guided to think iteratively\nand act judiciously, can advance diagnostic precision and cost-effectiveness in\nclinical care.",
        "url": "http://arxiv.org/abs/2506.22405v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22405v1",
        "arxiv_id": "2506.22405v1",
        "authors": [
            "Harsha Nori",
            "Mayank Daswani",
            "Christopher Kelly",
            "Scott Lundberg",
            "Marco Tulio Ribeiro",
            "Marc Wilson",
            "Xiaoxuan Liu",
            "Viknesh Sounderajah",
            "Jonathan Carlson",
            "Matthew P Lungren",
            "Bay Gross",
            "Peter Hames",
            "Mustafa Suleyman",
            "Dominic King",
            "Eric Horvitz"
        ],
        "submitted": "2025-06-27 17:27:26",
        "source": "arxiv",
        "comment": "23 pages, 10 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on sequential diagnosis with language models in the medical domain, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves language models, the application is specific to medical diagnosis and does not align with the user's interests in query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "HyperCLOVA X THINK Technical Report",
        "abstract": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language\nmodel in the HyperCLOVA X family, pre-trained on roughly $6$ trillion\nhigh-quality Korean, and English tokens, augmented with targeted synthetic\nKorean data. It was implemented as a compute-memory-balanced Peri-LN\nTransformer scaled with $\\mu$P, pre-trained through a three-stage curriculum\nthat expands the context window to $128$K tokens, and post-trained via\nsupervised fine-tuning with Reinforcement Learning from Verifiable Rewards\nsupports both detailed rationale and concise-answer modes. It delivers\ncompetitive performance against similarly sized models on Korea-focused\nbenchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while\npreserving robust bilingual consistency and translation quality. In addition, a\nvision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM\nbenchmark, all of which are achieved with substantially lower training compute\nthan existing models of similar sizes. We also present a pruning and\ndistillation technique that will soon be applied to HyperCLOVA X THINK for an\nopen-source and business-friendly foundation model. Altogether, these\ncapabilities position HyperCLOVA X THINK as a robust foundation for Korean AI\ninnovation and a valuable resource for the global research community.",
        "url": "http://arxiv.org/abs/2506.22403v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22403v1",
        "arxiv_id": "2506.22403v1",
        "authors": [
            "NAVER Cloud HyperCLOVA X Team"
        ],
        "submitted": "2025-06-27 17:23:12",
        "source": "arxiv",
        "comment": "49 pages, 13 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'search' (score: +1)",
            "Found 'korea' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on large language models and their applications, but it does not seem to be directly related to information retrieval, search technologies, or query understanding. The topics of reinforcement learning and fine-tuning are not specific to the user's interests in IR and NLP."
    },
    {
        "title": "Probabilistic Optimality for Inference-time Scaling",
        "abstract": "Inference-time scaling has emerged as a powerful technique for enhancing the\nreasoning performance of Large Language Models (LLMs). However, existing\napproaches often rely on heuristic strategies for parallel sampling, lacking a\nprincipled foundation. To address this gap, we propose a probabilistic\nframework that formalizes the optimality of inference-time scaling under the\nassumption that parallel samples are independently and identically distributed\n(i.i.d.), and where the Best-of-N selection strategy follows a probability\ndistribution that can be estimated. Within this framework, we derive a\ntheoretical lower bound on the required number of samples to achieve a target\nperformance level, providing the first principled guidance for\ncompute-efficient scaling. Leveraging this insight, we develop\n\\textsc{OptScale}, a practical algorithm that dynamically determines the\noptimal number of sampled responses. \\textsc{OptScale} employs a language\nmodel-based predictor to estimate probabilistic prior parameters, enabling the\ndecision of the minimal number of samples needed that satisfy predefined\nperformance thresholds and confidence levels. Extensive experiments on\nmathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)\ndemonstrate that \\textsc{OptScale} significantly reduces sampling overhead\nwhile remaining better or on par with state-of-the-art reasoning performance.\nOur work offers both a theoretical foundation and a practical solution for\nprincipled inference-time scaling, addressing a critical gap in the efficient\ndeployment of LLMs for complex reasoning.",
        "url": "http://arxiv.org/abs/2506.22376v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22376v1",
        "arxiv_id": "2506.22376v1",
        "authors": [
            "Youkang Wang",
            "Jian Wang",
            "Rubing Chen",
            "Xiao-Yong Wei",
            "Qing Li"
        ],
        "submitted": "2025-06-27 16:44:11",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on inference-time scaling for Large Language Models, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on optimization and scaling, the context is different from the user's research interests."
    },
    {
        "title": "Detection of Personal Data in Structured Datasets Using a Large Language Model",
        "abstract": "We propose a novel approach for detecting personal data in structured\ndatasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key\ninnovation of our method is the incorporation of contextual information: in\naddition to a feature's name and values, we utilize information from other\nfeature names within the dataset as well as the dataset description. We compare\nour approach to alternative methods, including Microsoft Presidio and CASSED,\nevaluating them on multiple datasets: DeSSI, a large synthetic dataset,\ndatasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a\nreal-world dataset containing patient information from critical care units.\n  Our findings reveal that detection performance varies significantly depending\non the dataset used for evaluation. CASSED excels on DeSSI, the dataset on\nwhich it was trained. Performance on the medical dataset MIMIC-Demo-Ext is\ncomparable across all models, with our GPT-4o-based approach clearly\noutperforming the others. Notably, personal data detection in the Kaggle and\nOpenML datasets appears to benefit from contextual information. This is\nevidenced by the poor performance of CASSED and Presidio (both of which do not\nutilize the context of the dataset) compared to the strong results of our\nGPT-4o-based approach.\n  We conclude that further progress in this field would greatly benefit from\nthe availability of more real-world datasets containing personal information.",
        "url": "http://arxiv.org/abs/2506.22305v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22305v1",
        "arxiv_id": "2506.22305v1",
        "authors": [
            "Albert Agisha Ntwali",
            "Luca R√ºck",
            "Martin Heckmann"
        ],
        "submitted": "2025-06-27 15:16:43",
        "source": "arxiv",
        "comment": "10 pages",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is not directly related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling. The focus is on detecting personal data in structured datasets using a Large Language Model, which is a topic in Natural Language Processing, but not in the user's primary areas of interest."
    },
    {
        "title": "COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication",
        "abstract": "Natural scenes provide us with rich contexts for object recognition and\nreference. In particular, knowing what type of scene one is looking at\ngenerates expectations about which objects will occur, and what their spatial\nconfiguration should be. Do Vision-Language Models (VLMs) learn to rely on\nscene contexts in a similar way, when generating references to objects? To\naddress this question, we introduce the \\textit{Common Objects Out-of-Context\n(COOCO)} dataset and test to what extent VLMs rely on scene context to refer to\nobjects under different degrees of scene-object congruency, and different\nperturbations. Our findings show that models leverage scene context adaptively,\ndepending on both the semantic relatedness between object and scene and the\nlevel of noise. In particular, models rely more on context under high\ntarget-scene congruence or when objects are degraded. Attention analysis\nreveals that successful object categorisation involves increased focus on the\ntarget in mid-level layers, especially under moderate noise, suggesting that\nVLMs dynamically balance local and contextual information for reference\ngeneration. We make our dataset, code and models available at\n\\href{https://github.com/cs-nlp-uu/scenereg}{https://github.com/cs-nlp-uu/scenereg}.",
        "url": "http://arxiv.org/abs/2506.22274v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22274v1",
        "arxiv_id": "2506.22274v1",
        "authors": [
            "Filippo Merlo",
            "Ece Takmaz",
            "Wenkai Chen",
            "Albert Gatt"
        ],
        "submitted": "2025-06-27 14:44:45",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Vision-Language Models (VLMs) and their ability to rely on scene contexts in object recognition, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on attention analysis, it does not explore ranking models or user behavior modeling, making it only loosely relevant to the user's research interests."
    },
    {
        "title": "Fine-Tuning MIDI-to-Audio Alignment using a Neural Network on Piano Roll and CQT Representations",
        "abstract": "In this paper, we present a neural network approach for synchronizing audio\nrecordings of human piano performances with their corresponding loosely aligned\nMIDI files. The task is addressed using a Convolutional Recurrent Neural\nNetwork (CRNN) architecture, which effectively captures spectral and temporal\nfeatures by processing an unaligned piano roll and a spectrogram as inputs to\nestimate the aligned piano roll. To train the network, we create a dataset of\npiano pieces with augmented MIDI files that simulate common human timing\nerrors. The proposed model achieves up to 20% higher alignment accuracy than\nthe industry-standard Dynamic Time Warping (DTW) method across various\ntolerance windows. Furthermore, integrating DTW with the CRNN yields additional\nimprovements, offering enhanced robustness and consistency. These findings\ndemonstrate the potential of neural networks in advancing state-of-the-art\nMIDI-to-audio alignment.",
        "url": "http://arxiv.org/abs/2506.22237v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22237v1",
        "arxiv_id": "2506.22237v1",
        "authors": [
            "Sebastian Murgul",
            "Moritz Reiser",
            "Michael Heizmann",
            "Christoph Seibert"
        ],
        "submitted": "2025-06-27 13:59:50",
        "source": "arxiv",
        "comment": "9 pages, 3 figures, 6 tables",
        "score": 2,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The topic of MIDI-to-audio alignment is not related to query understanding, ranking models, or user behavior modeling, and the techniques used are not applicable to the user's areas of interest."
    },
    {
        "title": "The Missing Link: Joint Legal Citation Prediction using Heterogeneous Graph Enrichment",
        "abstract": "Legal systems heavily rely on cross-citations of legal norms as well as\nprevious court decisions. Practitioners, novices and legal AI systems need\naccess to these relevant data to inform appraisals and judgments. We propose a\nGraph-Neural-Network (GNN) link prediction model that can identify Case-Law and\nCase-Case citations with high proficiency through fusion of semantic and\ntopological information. We introduce adapted relational graph convolutions\noperating on an extended and enriched version of the original citation graph\nthat allow the topological integration of semantic meta-information. This\nfurther improves prediction by 3.1 points of average precision and by 8.5\npoints in data sparsity as well as showing robust performance over time and in\nchallenging fully inductive prediction. Jointly learning and predicting case\nand norm citations achieves a large synergistic effect that improves case\ncitation prediction by up to 4.7 points, at almost doubled efficiency.",
        "url": "http://arxiv.org/abs/2506.22165v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22165v1",
        "arxiv_id": "2506.22165v1",
        "authors": [
            "Lorenz Wendlinger",
            "Simon Alexander Nonn",
            "Abdullah Al Zubaer",
            "Michael Granitzer"
        ],
        "submitted": "2025-06-27 12:21:41",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on legal citation prediction using graph neural networks, which is not directly related to information retrieval, search technologies, or query understanding. While it involves graph-based techniques, the application domain and problem statement are distinct from the user's research interests."
    },
    {
        "title": "AutoMixer: Checkpoint Artifacts as Automatic Data Mixers",
        "abstract": "In language model training, it is desirable to equip models with capabilities\nfrom various tasks. However, it is not clear how to directly obtain the right\ndata mixtures for these capabilities as the relationship between data and tasks\nis difficult to be modeled. In this work, we observe that checkpoint models\nexhibit emerging capabilities at different points in the training trajectory.\nOften, the training process saves checkpoints as artifacts that are\nunder-utilized as a source of in-training data signals. We identify these\nartifact models based on their respective capabilities on the benchmarks and\nleverage them as data mixers by using their aggregated first-order influence\napproximation over source data. We demonstrated on eight reasoning benchmarks\nthat the proposed framework shows significant improvements in the pretraining\nsetting, with performance improvements of up to 1.93%. Overall, this shows the\npotential of checkpoint models to enhance data quality and optimize data\nmixtures.",
        "url": "http://arxiv.org/abs/2506.21910v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21910v1",
        "arxiv_id": "2506.21910v1",
        "authors": [
            "Ernie Chang",
            "Yang Li",
            "Patrick Huber",
            "David Kant",
            "Yangyang Shi",
            "Vikas Chandra"
        ],
        "submitted": "2025-06-27 04:53:07",
        "source": "arxiv",
        "comment": "Accepted at ACL 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on language model training and checkpoint models, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. The abstract does not mention query understanding, ranking models, or user behavior modeling, and the concepts presented are not directly applicable to the user's areas of interest."
    },
    {
        "title": "DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE",
        "abstract": "Native multimodal large language models (MLLMs) restructure a single large\nlanguage model (LLM) into a spoken language model (SLM) capable of both speech\nand text generation. Compared to modular and aligned MLLMs, native MLLMs\npreserve richer paralinguistic features such as emotion and prosody, and\ngenerate speech responses directly within the backbone LLM rather than using a\nseparate speech decoder. This integration also results in lower response\nlatency and smoother interaction. However, native MLLMs suffer from\ncatastrophic forgetting and performance degradation because the available\npaired speech-text data is insufficient to support the pretraining of MLLMs\ncompared to the vast amount of text data required to pretrain text LLMs. To\naddress this issue, we propose DeepTalk, a framework for adaptive modality\nexpert learning based on a Mixture of Experts (MoE) architecture. DeepTalk\nfirst adaptively distinguishes modality experts according to their modality\nload within the LLM. Each modality expert then undergoes specialized\nsingle-modality training, followed by joint multimodal collaborative training.\nAs a result, DeepTalk incurs only a 5.5% performance drop compared to the\noriginal LLM, which is significantly lower than the average performance drop of\nover 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par\nwith modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within\n0.5 seconds, ensuring a seamless and intelligent speech interaction experience.\nCode and models are released at https://github.com/talkking/DeepTalk.",
        "url": "http://arxiv.org/abs/2506.21864v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21864v1",
        "arxiv_id": "2506.21864v1",
        "authors": [
            "Hang Shao",
            "Heting Gao",
            "Yunhang Shen",
            "Jiawei Chen",
            "Lijiang Li",
            "Zuwei Long",
            "Bo Tong",
            "Ke Li",
            "Xing Sun"
        ],
        "submitted": "2025-06-27 02:32:04",
        "source": "arxiv",
        "comment": "Under Review",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on speech interaction and multimodal language models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions adaptive learning, it is not specifically applied to ranking models or user behavior modeling, and the context is different from e-commerce or real-time relevance optimization."
    },
    {
        "title": "The Consistency Hypothesis in Uncertainty Quantification for Large Language Models",
        "abstract": "Estimating the confidence of large language model (LLM) outputs is essential\nfor real-world applications requiring high user trust. Black-box uncertainty\nquantification (UQ) methods, relying solely on model API access, have gained\npopularity due to their practical benefits. In this paper, we examine the\nimplicit assumption behind several UQ methods, which use generation consistency\nas a proxy for confidence, an idea we formalize as the consistency hypothesis.\nWe introduce three mathematical statements with corresponding statistical tests\nto capture variations of this hypothesis and metrics to evaluate LLM output\nconformity across tasks. Our empirical investigation, spanning 8 benchmark\ndatasets and 3 tasks (question answering, text summarization, and text-to-SQL),\nhighlights the prevalence of the hypothesis under different settings. Among the\nstatements, we highlight the `Sim-Any' hypothesis as the most actionable, and\ndemonstrate how it can be leveraged by proposing data-free black-box UQ methods\nthat aggregate similarities between generations for confidence estimation.\nThese approaches can outperform the closest baselines, showcasing the practical\nvalue of the empirically observed consistency hypothesis.",
        "url": "http://arxiv.org/abs/2506.21849v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21849v1",
        "arxiv_id": "2506.21849v1",
        "authors": [
            "Quan Xiao",
            "Debarun Bhattacharjya",
            "Balaji Ganesan",
            "Radu Marinescu",
            "Katsiaryna Mirylenka",
            "Nhan H Pham",
            "Michael Glass",
            "Junkyu Lee"
        ],
        "submitted": "2025-06-27 01:53:15",
        "source": "arxiv",
        "comment": "Accepted by The Conference on Uncertainty in Artificial Intelligence\n  (UAI) 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores uncertainty quantification for large language models, which is a topic in NLP. While it touches on the idea of confidence estimation, it does not directly relate to query understanding, ranking models, or user behavior modeling in the context of search technologies. The focus on language models and uncertainty quantification is somewhat relevant to my interests, but not a central match."
    },
    {
        "title": "Exploring the Structure of AI-Induced Language Change in Scientific English",
        "abstract": "Scientific English has undergone rapid and unprecedented changes in recent\nyears, with words such as \"delve,\" \"intricate,\" and \"crucial\" showing\nsignificant spikes in frequency since around 2022. These changes are widely\nattributed to the growing influence of Large Language Models like ChatGPT in\nthe discourse surrounding bias and misalignment. However, apart from changes in\nfrequency, the exact structure of these linguistic shifts has remained unclear.\nThe present study addresses this and investigates whether these changes involve\nthe replacement of synonyms by suddenly 'spiking words,' for example, \"crucial\"\nreplacing \"essential\" and \"key,\" or whether they reflect broader semantic and\npragmatic qualifications. To further investigate structural changes, we include\npart of speech tagging in our analysis to quantify linguistic shifts over\ngrammatical categories and differentiate between word forms, like \"potential\"\nas a noun vs. as an adjective. We systematically analyze synonym groups for\nwidely discussed 'spiking words' based on frequency trends in scientific\nabstracts from PubMed. We find that entire semantic clusters often shift\ntogether, with most or all words in a group increasing in usage. This pattern\nsuggests that changes induced by Large Language Models are primarily semantic\nand pragmatic rather than purely lexical. Notably, the adjective \"important\"\nshows a significant decline, which prompted us to systematically analyze\ndecreasing lexical items. Our analysis of \"collapsing\" words reveals a more\ncomplex picture, which is consistent with organic language change and contrasts\nwith the patterns of the abrupt spikes. These insights into the structure of\nlanguage change contribute to our understanding of how language technology\ncontinues to shape human language.",
        "url": "http://arxiv.org/abs/2506.21817v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21817v1",
        "arxiv_id": "2506.21817v1",
        "authors": [
            "Riley Galpin",
            "Bryce Anderson",
            "Tom S. Juzek"
        ],
        "submitted": "2025-06-26 23:44:24",
        "source": "arxiv",
        "comment": "Accepted and published at FLAIRS 38. 8 pages, 4 figures, 1 table.\n  Licensed under CC BY-NC-SA 4.0",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper explores the structure of AI-induced language change in scientific English, focusing on the frequency and semantic shifts of words. While it touches on the influence of Large Language Models, it does not directly relate to information retrieval, search technologies, or query understanding, which are the user's primary research interests."
    },
    {
        "title": "(Fact) Check Your Bias",
        "abstract": "Automatic fact verification systems increasingly rely on large language\nmodels (LLMs). We investigate how parametric knowledge biases in these models\naffect fact-checking outcomes of the HerO system (baseline for FEVER-25). We\nexamine how the system is affected by: (1) potential bias in Llama 3.1's\nparametric knowledge and (2) intentionally injected bias. When prompted\ndirectly to perform fact-verification, Llama 3.1 labels nearly half the claims\nas \"Not Enough Evidence\". Using only its parametric knowledge it is able to\nreach a verdict on the remaining half of the claims. In the second experiment,\nwe prompt the model to generate supporting, refuting, or neutral fact-checking\ndocuments. These prompts significantly influence retrieval outcomes, with\napproximately 50\\% of retrieved evidence being unique to each perspective.\nNotably, the model sometimes refuses to generate supporting documents for\nclaims it believes to be false, creating an inherent negative bias. Despite\ndifferences in retrieved evidence, final verdict predictions show stability\nacross prompting strategies. The code is available at:\nhttps://github.com/eibakke/FEVER-8-Shared-Task",
        "url": "http://arxiv.org/abs/2506.21745v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21745v1",
        "arxiv_id": "2506.21745v1",
        "authors": [
            "Eivind Morris Bakke",
            "Nora Winger Heggelund"
        ],
        "submitted": "2025-06-26 20:03:58",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the impact of parametric knowledge biases in large language models on fact-checking outcomes, which is a relevant topic in Natural Language Processing. However, the focus is on fact verification and bias detection, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval. The paper's relevance to IR is limited, but it may be of interest to researchers exploring the intersection of NLP and IR."
    },
    {
        "title": "Evaluating Scoring Bias in LLM-as-a-Judge",
        "abstract": "The remarkable performance of Large Language Models (LLMs) gives rise\nto``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks.\nMoreover, it has been widely adopted across fields such as Natural Language\nProcessing (NLP), preference learning, and various specific domains. However,\nthere are various biases within LLM-as-a-Judge, which adversely affect the\nfairness and reliability of judgments. Current research on evaluating or\nmitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based\nevaluations, while systematic investigations into bias in scoring-based\nevaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge\nas the scores differ when scoring judge models are bias-related perturbed, and\nprovide a well-designed framework to comprehensively evaluate scoring bias. We\naugment existing LLM-as-a-Judge benchmarks through data synthesis to construct\nour evaluation dataset and design multi-faceted evaluation metrics. Our\nexperimental results demonstrate that the scoring stability of existing judge\nmodels is disrupted by scoring biases. Further exploratory experiments and\ndiscussions provide valuable insights into the design of scoring prompt\ntemplates and the mitigation of scoring biases on aspects such as score\nrubrics, score IDs, and reference answer selection.",
        "url": "http://arxiv.org/abs/2506.22316v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22316v1",
        "arxiv_id": "2506.22316v1",
        "authors": [
            "Qingquan Li",
            "Shaoyu Dou",
            "Kailai Shao",
            "Chao Chen",
            "Haixiang Hu"
        ],
        "submitted": "2025-06-27 15:25:23",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on evaluating biases in Large Language Models (LLMs) used as judges, which is related to my interest in Natural Language Processing (NLP). However, the specific context of LLM-as-a-Judge and the emphasis on scoring bias evaluation is not directly aligned with my primary focus on Information Retrieval and Search technologies, query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Training Language Model to Critique for Better Refinement",
        "abstract": "Large language models (LLMs) have demonstrated remarkable evaluation and\ncritique capabilities, providing insightful feedback and identifying flaws in\nvarious tasks. However, limited research has explored which types of critiques\nare most effective for improving model responses or how to generate such\ncritiques. To address this gap, we introduce \\textbf{R}efinement-oriented\n\\textbf{C}ritique \\textbf{O}ptimization (RCO), a novel framework designed to\ntrain critic models using refinement signals. RCO uses a feedback loop where\ncritiques, generated by the critic model, guide the actor model in refining its\nresponses. The critique utility (CU) quantifies the effectiveness of these\nrefinements, serving as the reward signal for training the critic model. By\nfocusing on critiques that lead to better refinements, RCO eliminates the need\nfor direct critique preference assessment, ensuring that critiques driving\nmeaningful improvements are rewarded. We evaluate RCO across five tasks, i.e.,\ndialog generation, summarization, question answering, mathematical reasoning,\nand code generation, and show that it significantly outperforms traditional\nmethods and open-source models in terms of critique quality and refinement\noutcomes. Our contributions include the introduction of RCO, a novel\nsupervision scheme based on refined response preferences, and comprehensive\nexperimental results that highlight the method's effectiveness in enhancing LLM\ncritique-refinement loops.",
        "url": "http://arxiv.org/abs/2506.22157v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22157v1",
        "arxiv_id": "2506.22157v1",
        "authors": [
            "Tianshu Yu",
            "Chao Xiang",
            "Mingchuan Yang",
            "Pei Ke",
            "Bosi Wen",
            "Cunxiang Wang",
            "Jiale Cheng",
            "Li Zhang",
            "Xinyu Mu",
            "Chuxiong Sun",
            "Minlie Huang"
        ],
        "submitted": "2025-06-27 12:10:57",
        "source": "arxiv",
        "comment": "Accepted to ACL 2025 Findings",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on training language models to critique and refine their responses, which is related to query understanding and ranking models in Information Retrieval. However, the paper's primary focus is on language models and critique-refinement loops, which is not directly aligned with the user's interests in search technologies and user behavior modeling."
    },
    {
        "title": "Reward Balancing Revisited: Enhancing Offline Reinforcement Learning for Recommender Systems",
        "abstract": "Offline reinforcement learning (RL) has emerged as a prevalent and effective\nmethodology for real-world recommender systems, enabling learning policies from\nhistorical data and capturing user preferences. In offline RL, reward shaping\nencounters significant challenges, with past efforts to incorporate prior\nstrategies for uncertainty to improve world models or penalize underexplored\nstate-action pairs. Despite these efforts, a critical gap remains: the\nsimultaneous balancing of intrinsic biases in world models and the diversity of\npolicy recommendations. To address this limitation, we present an innovative\noffline RL framework termed Reallocated Reward for Recommender Systems (R3S).\nBy integrating inherent model uncertainty to tackle the intrinsic fluctuations\nin reward predictions, we boost diversity for decision-making to align with a\nmore interactive paradigm, incorporating extra penalizers with decay that deter\nactions leading to diminished state variety at both local and global scales.\nThe experimental results demonstrate that R3S improves the accuracy of world\nmodels and efficiently harmonizes the heterogeneous preferences of the users.",
        "url": "http://arxiv.org/abs/2506.22112v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22112v1",
        "arxiv_id": "2506.22112v1",
        "authors": [
            "Wenzheng Shu",
            "Yanxiang Zeng",
            "Yongxiang Tang",
            "Teng Sha",
            "Ning Luo",
            "Yanhua Cheng",
            "Xialong Liu",
            "Fan Zhou",
            "Peng Jiang"
        ],
        "submitted": "2025-06-27 10:46:41",
        "source": "arxiv",
        "comment": "Accepted in Companion Proceedings of the ACM Web Conference 2025",
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on offline reinforcement learning for recommender systems, which is somewhat related to my interests in information retrieval and search technologies. However, the emphasis on recommender systems and reward balancing is not directly aligned with my primary focus on query understanding, ranking models, and user behavior modeling. The paper's relevance is limited to the broader area of search and recommendation, but it does not specifically address my core research themes."
    },
    {
        "title": "Can Peter Pan Survive MT? A Stylometric Study of LLMs, NMTs, and HTs in Children's Literature Translation",
        "abstract": "This study focuses on evaluating the performance of machine translations\n(MTs) compared to human translations (HTs) in English-to-Chinese children's\nliterature translation (CLT) from a stylometric perspective. The research\nconstructs a Peter Pan corpus, comprising 21 translations: 7 human translations\n(HTs), 7 large language model translations (LLMs), and 7 neural machine\ntranslation outputs (NMTs). The analysis employs a generic feature set\n(including lexical, syntactic, readability, and n-gram features) and a creative\ntext translation (CTT-specific) feature set, which captures repetition, rhythm,\ntranslatability, and miscellaneous levels, yielding 447 linguistic features in\ntotal.\n  Using classification and clustering techniques in machine learning, we\nconduct a stylometric analysis of these translations. Results reveal that in\ngeneric features, HTs and MTs exhibit significant differences in conjunction\nword distributions and the ratio of 1-word-gram-YiYang, while NMTs and LLMs\nshow significant variation in descriptive words usage and adverb ratios.\nRegarding CTT-specific features, LLMs outperform NMTs in distribution, aligning\nmore closely with HTs in stylistic characteristics, demonstrating the potential\nof LLMs in CLT.",
        "url": "http://arxiv.org/abs/2506.22038v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22038v1",
        "arxiv_id": "2506.22038v1",
        "authors": [
            "Delu Kong",
            "Lieve Macken"
        ],
        "submitted": "2025-06-27 09:34:40",
        "source": "arxiv",
        "comment": "19 pages, 8 figures, 4 tables. Accepted in 2nd Workshop on\n  Creative-text Translation and Technology Co-located with MT Summit 2025.\n  Official paper may later be accessed from ACL Anthology",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on machine translation, stylometry, and children's literature translation, which are not directly related to your areas of interest."
    },
    {
        "title": "Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit",
        "abstract": "The developments in transformer encoder-decoder architectures have led to\nsignificant breakthroughs in machine translation, Automatic Speech Recognition\n(ASR), and instruction-based chat machines, among other applications. The\npre-trained models were trained on vast amounts of generic data over a few\nepochs (fewer than five in most cases), resulting in their strong\ngeneralization capabilities. Nevertheless, the performance of these models does\nsuffer when applied to niche domains like transcribing pilot speech in the\ncockpit, which involves a lot of specific vocabulary and multilingual\nconversations. This paper investigates and improves the transcription accuracy\nof cockpit conversations with Whisper models. We have collected around 85\nminutes of cockpit simulator recordings and 130 minutes of interview recordings\nwith pilots and manually labeled them. The speakers are middle aged men\nspeaking both German and English. To improve the accuracy of transcriptions, we\npropose multiple normalization schemes to refine the transcripts and improve\nWord Error Rate (WER). We then employ fine-tuning to enhance ASR performance,\nutilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA).\nHereby, WER decreased from 68.49 \\% (pretrained whisper Large model without\nnormalization baseline) to 26.26\\% (finetuned whisper Large model with the\nproposed normalization scheme).",
        "url": "http://arxiv.org/abs/2506.21990v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21990v1",
        "arxiv_id": "2506.21990v1",
        "authors": [
            "Kartheek Kumar Reddy Nareddy",
            "Sarah Ternus",
            "Julia Niebling"
        ],
        "submitted": "2025-06-27 07:57:13",
        "source": "arxiv",
        "comment": "Computer Vision and Pattern Recognition (CVPR) 2025 Workshops",
        "score": 1,
        "keyword_reasons": [
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on speech transcription in a niche domain (cockpit conversations) using pre-trained Whisper models, which is not directly related to information retrieval, search technologies, or query understanding. Although it mentions fine-tuning and adaptation, the context is specific to speech recognition and not relevant to the user's primary research interests."
    },
    {
        "title": "More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents",
        "abstract": "Current evaluations of tool-integrated LLM agents typically focus on\nend-to-end tool-usage evaluation while neglecting their stability. This limits\ntheir real-world applicability, as various internal or external factors can\ncause agents to crash or behave abnormally. Our research addresses this by\ninvestigating whether agents are vulnerable to errors throughout the entire\ntool invocation process, including reading tool documentation, selecting tools\nand generating parameters, and processing the tool's response. Through\nextensive experiments, we observe that agents are highly susceptible to errors\nat each stage and agents based on open-source models are more vulnerable than\nthose based on proprietary models. We also find that increasing the model size\ndoes not significantly improve tool invocation reasoning and may make agents\nmore vulnerable to attacks resembling normal user instructions. This highlights\nthe importance of evaluating agent stability and offers valuable insights for\nfuture LLM development and evaluation.",
        "url": "http://arxiv.org/abs/2506.21967v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21967v1",
        "arxiv_id": "2506.21967v1",
        "authors": [
            "Weimin Xiong",
            "Ke Wang",
            "Yifan Song",
            "Hanchao Liu",
            "Sai Zhou",
            "Wei Peng",
            "Sujian Li"
        ],
        "submitted": "2025-06-27 07:13:29",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on the stability of tool-integrated LLM agents, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on model-based aspects, the context is not relevant to the user's primary research interests."
    },
    {
        "title": "PapersPlease: A Benchmark for Evaluating Motivational Values of Large Language Models Based on ERG Theory",
        "abstract": "Evaluating the performance and biases of large language models (LLMs) through\nrole-playing scenarios is becoming increasingly common, as LLMs often exhibit\nbiased behaviors in these contexts. Building on this line of research, we\nintroduce PapersPlease, a benchmark consisting of 3,700 moral dilemmas designed\nto investigate LLMs' decision-making in prioritizing various levels of human\nneeds. In our setup, LLMs act as immigration inspectors deciding whether to\napprove or deny entry based on the short narratives of people. These narratives\nare constructed using the Existence, Relatedness, and Growth (ERG) theory,\nwhich categorizes human needs into three hierarchical levels. Our analysis of\nsix LLMs reveals statistically significant patterns in decision-making,\nsuggesting that LLMs encode implicit preferences. Additionally, our evaluation\nof the impact of incorporating social identities into the narratives shows\nvarying responsiveness based on both motivational needs and identity cues, with\nsome models exhibiting higher denial rates for marginalized identities. All\ndata is publicly available at https://github.com/yeonsuuuu28/papers-please.",
        "url": "http://arxiv.org/abs/2506.21961v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21961v1",
        "arxiv_id": "2506.21961v1",
        "authors": [
            "Junho Myung",
            "Yeon Su Park",
            "Sunwoo Kim",
            "Shin Yoo",
            "Alice Oh"
        ],
        "submitted": "2025-06-27 07:09:11",
        "source": "arxiv",
        "comment": "Accepted to GEM2 Workshop: Generation, Evaluation & Metrics - ACL\n  2025",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The paper focuses on evaluating the motivational values of large language models based on ERG theory, which is outside your areas of interest."
    },
    {
        "title": "3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach",
        "abstract": "This paper presents 3Description, an experimental human-AI collaborative\napproach for intuitive 3D modeling. 3Description aims to address accessibility\nand usability challenges in traditional 3D modeling by enabling\nnon-professional individuals to co-create 3D models using verbal and gesture\ndescriptions. Through a combination of qualitative research, product analysis,\nand user testing, 3Description integrates AI technologies such as Natural\nLanguage Processing and Computer Vision, powered by OpenAI and MediaPipe.\nRecognizing the web has wide cross-platform capabilities, 3Description is\nweb-based, allowing users to describe the desired model and subsequently adjust\nits components using verbal and gestural inputs. In the era of AI and emerging\nmedia, 3Description not only contributes to a more inclusive and user-friendly\ndesign process, empowering more people to participate in the construction of\nthe future 3D world, but also strives to increase human engagement in\nco-creation with AI, thereby avoiding undue surrender to technology and\npreserving human creativity.",
        "url": "http://arxiv.org/abs/2506.21845v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21845v1",
        "arxiv_id": "2506.21845v1",
        "authors": [
            "Zhuodi Cai"
        ],
        "submitted": "2025-06-27 01:33:46",
        "source": "arxiv",
        "comment": "5 pages, 2 figures, 3 tables (containing 21 subfigures)",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on human-AI collaborative 3D modeling, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it mentions AI technologies like NLP and Computer Vision, the application is in a different domain and does not align with the user's primary focus on IR and real-time relevance optimization."
    },
    {
        "title": "Exploring the change in scientific readability following the release of ChatGPT",
        "abstract": "The rise and growing popularity of accessible large language models have\nraised questions about their impact on various aspects of life, including how\nscientists write and publish their research. The primary objective of this\npaper is to analyze a dataset consisting of all abstracts posted on arXiv.org\nbetween 2010 and June 7th, 2024, to assess the evolution of their readability\nand determine whether significant shifts occurred following the release of\nChatGPT in November 2022. Four standard readability formulas are used to\ncalculate individual readability scores for each paper, classifying their level\nof readability. These scores are then aggregated by year and across the eight\nprimary categories covered by the platform. The results show a steady annual\ndecrease in readability, suggesting that abstracts are likely becoming\nincreasingly complex. Additionally, following the release of ChatGPT, a\nsignificant change in readability is observed for 2023 and the analyzed months\nof 2024. Similar trends are found across categories, with most experiencing a\nnotable change in readability during 2023 and 2024. These findings offer\ninsights into the broader changes in readability and point to the likely\ninfluence of AI on scientific writing.",
        "url": "http://arxiv.org/abs/2506.21825v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21825v1",
        "arxiv_id": "2506.21825v1",
        "authors": [
            "Abdulkareem Alsudais"
        ],
        "submitted": "2025-06-26 23:57:12",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the impact of ChatGPT on scientific readability, which is a related topic to information retrieval and natural language processing. However, the focus is on readability and writing styles rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest for you."
    },
    {
        "title": "A suite of allotaxonometric tools for the comparison of complex systems using rank-turbulence divergence",
        "abstract": "Describing and comparing complex systems requires principled, theoretically\ngrounded tools. Built around the phenomenon of type turbulence,\nallotaxonographs provide map-and-list visual comparisons of pairs of\nheavy-tailed distributions. Allotaxonographs are designed to accommodate a wide\nrange of instruments including rank- and probability-turbulence divergences,\nJenson-Shannon divergence, and generalized entropy divergences. Here, we\ndescribe a suite of programmatic tools for rendering allotaxonographs for\nrank-turbulence divergence in Matlab, Javascript, and Python, all of which have\ndifferent use cases.",
        "url": "http://arxiv.org/abs/2506.21808v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21808v1",
        "arxiv_id": "2506.21808v1",
        "authors": [
            "Jonathan St-Onge",
            "Ashley M. A. Fehr",
            "Carter Ward",
            "Calla G. Beauregard",
            "Michael V. Arnold",
            "Samuel F. Rosenblatt",
            "Benjamin Cooley",
            "Christopher M. Danforth",
            "Peter Sheridan Dodds"
        ],
        "submitted": "2025-06-26 23:17:29",
        "source": "arxiv",
        "comment": "4 pages, 2 figures",
        "score": 1,
        "keyword_reasons": [
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper appears to be unrelated to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The focus on complex systems, heavy-tailed distributions, and divergences is outside the scope of the user's expertise and interests."
    },
    {
        "title": "Identifying Speaker Information in Feed-Forward Layers of Self-Supervised Speech Transformers",
        "abstract": "In recent years, the impact of self-supervised speech Transformers has\nextended to speaker-related applications. However, little research has explored\nhow these models encode speaker information. In this work, we address this gap\nby identifying neurons in the feed-forward layers that are correlated with\nspeaker information. Specifically, we analyze neurons associated with k-means\nclusters of self-supervised features and i-vectors. Our analysis reveals that\nthese clusters correspond to broad phonetic and gender classes, making them\nsuitable for identifying neurons that represent speakers. By protecting these\nneurons during pruning, we can significantly preserve performance on\nspeaker-related task, demonstrating their crucial role in encoding speaker\ninformation.",
        "url": "http://arxiv.org/abs/2506.21712v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21712v1",
        "arxiv_id": "2506.21712v1",
        "authors": [
            "Tzu-Quan Lin",
            "Hsi-Chun Cheng",
            "Hung-yi Lee",
            "Hao Tang"
        ],
        "submitted": "2025-06-26 18:54:26",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on speaker information in self-supervised speech Transformers, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's topic is more aligned with speech recognition and speaker recognition, which is not a primary focus of the user's research."
    }
]