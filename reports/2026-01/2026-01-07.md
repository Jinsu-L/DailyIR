# Daily Papers Report - 2026-01-07

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Fine-tuning Small Language Models as Efficient Enterprise Search Relevance Labelers

- **LLM Score**: 8
- **Keyword Score**: 14
- **Authors**: Yue Kang, Zhuoyi Huang, Benji Schussheim, Diana Licon, Dina Atia, Shixing Cao, Jacob Danovitch, Kunho Kim, Billy Norcilien, Jonah Karpman, Mahmound Sayed, Mike Taylor, Tao Sun, Pavel Metrikov, Vipul Agarwal, Chris Quirk, Ye-Yi Wang, Nick Craswell, Irene Shaffer, Tianwei Chen, Sulaiman Vesal, Soundar Srinivasan
- **URL**: <http://arxiv.org/abs/2601.03211v1>
- **Submitted**: 2026-01-06 17:48:40
- **Topic Keywords**: query, queries, relevance, rag, retrieval, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the context of enterprise search and relevance labeling. The use of small language models for efficient relevance labeling aligns with your focus on query understanding and ranking models. However, the paper's focus on the enterprise domain is somewhat narrow compared to your broader interests in IR and NLP.

#### Abstract
> In enterprise search, building high-quality datasets at scale remains a central challenge due to the difficulty of acquiring labeled data. To resolve this challenge, we propose an efficient approach to fine-tune small language models (SLMs) for accurate relevance labeling, enabling high-throughput, domain-specific labeling comparable or even better in quality to that of state-of-the-art large language models (LLMs). To overcome the lack of high-quality and accessible datasets in the enterprise domain, our method leverages on synthetic data generation. Specifically, we employ an LLM to synthesize realistic enterprise queries from a seed document, apply BM25 to retrieve hard negatives, and use a teacher LLM to assign relevance scores. The resulting dataset is then distilled into an SLM, producing a compact relevance labeler. We evaluate our approach on a high-quality benchmark consisting of 923 enterprise query-document pairs annotated by trained human annotators, and show that the distilled SLM achieves agreement with human judgments on par with or better than the teacher LLM. Furthermore, our fine-tuned labeler substantially improves throughput, achieving 17 times increase while also being 19 times more cost-effective. This approach enables scalable and cost-effective relevance labeling for enterprise-scale retrieval applications, supporting rapid offline evaluation and iteration in real-world settings.

---

### 2. Stable-RAG: Mitigating Retrieval-Permutation-Induced Hallucinations in Retrieval-Augmented Generation

- **LLM Score**: 8
- **Keyword Score**: 7
- **Authors**: Qianchi Zhang, Hainan Zhang, Liang Pang, Hongwei Zheng, Zhiming Zheng
- **URL**: <http://arxiv.org/abs/2601.02993v2>
- **Submitted**: 2026-01-06 13:07:38
- **Comment**: 18 pages, 13figures, 8 tables. The code will be released after the review process
- **Topic Keywords**: retriever, rag, retrieval
- **Reason**: This paper is highly relevant to Information Retrieval, specifically addressing the impact of retrieval permutations on model behavior. The proposed Stable-RAG method is a novel approach to mitigating hallucinations in Retrieval-Augmented Generation, which is a key area of interest in IR. While the focus is on NLP and generation, the underlying IR concepts are central to the research.

#### Abstract
> Retrieval-Augmented Generation (RAG) has become a key paradigm for reducing factual hallucinations in large language models (LLMs), yet little is known about how the order of retrieved documents affects model behavior. We empirically show that under Top-5 retrieval with the gold document included, LLM answers vary substantially across permutations of the retrieved set, even when the gold document is fixed in the first position. This reveals a previously underexplored sensitivity to retrieval permutations. Although robust RAG methods primarily focus on enhancing LLM robustness to low-quality retrieval and mitigating positional bias to distribute attention fairly over long contexts, neither approach directly addresses permutation sensitivity. In this paper, we propose Stable-RAG, which exploits permutation sensitivity estimation to mitigate permutation-induced hallucinations. Stable-RAG runs the generator under multiple retrieval orders, clusters hidden states, and decodes from a cluster-center representation that captures the dominant reasoning pattern. It then uses these reasoning results to align hallucinated outputs toward the correct answer, encouraging the model to produce consistent and accurate predictions across document permutations. Experiments on three QA datasets show that Stable-RAG significantly improves answer accuracy, reasoning consistency and robust generalization across datasets, retrievers, and input lengths compared with baselines.

---

### 3. RAL2M: Retrieval Augmented Learning-To-Match Against Hallucination in Compliance-Guaranteed Service Systems

- **LLM Score**: 8
- **Keyword Score**: 7
- **Authors**: Mengze Hong, Di Jiang, Jiangtao Wen, Zhiyang Su, Yawen Li, Yanjie Sun, Guan Wang, Chen Jason Zhang
- **URL**: <http://arxiv.org/abs/2601.02917v1>
- **Submitted**: 2026-01-06 11:00:16
- **Topic Keywords**: query, rag, retrieval
- **Reason**: This paper introduces a novel framework for mitigating hallucination in LLM-driven service systems, which is closely related to query understanding and ranking models in Information Retrieval. Although it's not directly focused on e-commerce, it explores a relevant area of deep semantic understanding and real-time relevance optimization. The use of retrieval-based systems and latent ensemble strategies also aligns with the user's interests in NLP and data mining.

#### Abstract
> Hallucination is a major concern in LLM-driven service systems, necessitating explicit knowledge grounding for compliance-guaranteed responses. In this paper, we introduce Retrieval-Augmented Learning-to-Match (RAL2M), a novel framework that eliminates generation hallucination by repositioning LLMs as query-response matching judges within a retrieval-based system, providing a robust alternative to purely generative approaches. To further mitigate judgment hallucination, we propose a query-adaptive latent ensemble strategy that explicitly models heterogeneous model competence and interdependencies among LLMs, deriving a calibrated consensus decision. Extensive experiments on large-scale benchmarks demonstrate that the proposed method effectively leverages the "wisdom of the crowd" and significantly outperforms strong baselines. Finally, we discuss best practices and promising directions for further exploiting latent representations in future work.

---

### 4. SentGraph: Hierarchical Sentence Graph for Multi-hop Retrieval-Augmented Question Answering

- **LLM Score**: 8
- **Keyword Score**: 4
- **Authors**: Junli Liang, Pengfei Zhou, Wangqiu Zhou, Wenjie Qing, Qi Zhao, Ziwen Wang, Qi Song, Xiangyang Li
- **URL**: <http://arxiv.org/abs/2601.03014v1>
- **Submitted**: 2026-01-06 13:39:51
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The focus on multi-hop question answering and sentence-level logical dependencies aligns with your interests in deep semantic understanding and real-time relevance optimization. However, the specific application to question answering and the use of Rhetorical Structure Theory may not be directly related to your primary focus on search technologies.

#### Abstract
> Traditional Retrieval-Augmented Generation (RAG) effectively supports single-hop question answering with large language models but faces significant limitations in multi-hop question answering tasks, which require combining evidence from multiple documents. Existing chunk-based retrieval often provides irrelevant and logically incoherent context, leading to incomplete evidence chains and incorrect reasoning during answer generation. To address these challenges, we propose SentGraph, a sentence-level graph-based RAG framework that explicitly models fine-grained logical relationships between sentences for multi-hop question answering. Specifically, we construct a hierarchical sentence graph offline by first adapting Rhetorical Structure Theory to distinguish nucleus and satellite sentences, and then organizing them into topic-level subgraphs with cross-document entity bridges. During online retrieval, SentGraph performs graph-guided evidence selection and path expansion to retrieve fine-grained sentence-level evidence. Extensive experiments on four multi-hop question answering benchmarks demonstrate the effectiveness of SentGraph, validating the importance of explicitly modeling sentence-level logical dependencies for multi-hop reasoning.

---

### 5. Auditing Search Query Suggestion Bias Through Recursive Algorithm Interrogation

- **LLM Score**: 8
- **Keyword Score**: 4
- **Authors**: Fabian Haak, Philipp Schaer
- **URL**: <http://arxiv.org/abs/2601.02962v1>
- **Submitted**: 2026-01-06 12:13:39
- **Topic Keywords**: query, search
- **Reason**: This paper is highly relevant to Information Retrieval, particularly in the area of search query suggestions, which is a key aspect of search engines. The use of recursive algorithm interrogation techniques to identify bias in search query suggestions aligns with your interests in query understanding and ranking models. Although the focus is on search query suggestions rather than ranking models, the paper's emphasis on deep semantic understanding and real-time relevance optimization makes it a useful contribution to the field.

#### Abstract
> Despite their important role in online information search, search query suggestions have not been researched as much as most other aspects of search engines. Although reasons for this are multi-faceted, the sparseness of context and the limited data basis of up to ten suggestions per search query pose the most significant problem in identifying bias in search query suggestions. The most proven method to reduce sparseness and improve the validity of bias identification of search query suggestions so far is to consider suggestions from subsequent searches over time for the same query. This work presents a new, alternative approach to search query bias identification that includes less high-level suggestions to deepen the data basis of bias analyses. We employ recursive algorithm interrogation techniques and create suggestion trees that enable access to more subliminal search query suggestions. Based on these suggestions, we investigate topical group bias in person-related searches in the political domain.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Mechanistic Knobs in LLMs: Retrieving and Steering High-Order Semantic Features via Sparse Autoencoders

- **LLM Score**: 8
- **Keyword Score**: 2
- **Authors**: Ruikang Zhang, Shuo Wang, Qi Su
- **URL**: <http://arxiv.org/abs/2601.02978v1>
- **Submitted**: 2026-01-06 12:40:37
- **Topic Keywords**: retrieval
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in areas requiring deep semantic understanding. The use of Large Language Models (LLMs) and Mechanistic Interpretability (MI) aligns with your focus on query understanding and ranking models. However, the paper's primary focus on NLP and LLMs is slightly outside your core IR domain, but its relevance to semantic understanding and model regulation makes it a useful and clearly related study.

#### Abstract
> Recent work in Mechanistic Interpretability (MI) has enabled the identification and intervention of internal features in Large Language Models (LLMs). However, a persistent challenge lies in linking such internal features to the reliable control of complex, behavior-level semantic attributes in language generation. In this paper, we propose a Sparse Autoencoder-based framework for retrieving and steering semantically interpretable internal features associated with high-level linguistic behaviors. Our method employs a contrastive feature retrieval pipeline based on controlled semantic oppositions, combing statistical activation analysis and generation-based validation to distill monosemantic functional features from sparse activation spaces. Using the Big Five personality traits as a case study, we demonstrate that our method enables precise, bidirectional steering of model behavior while maintaining superior stability and performance compared to existing activation steering methods like Contrastive Activation Addition (CAA). We further identify an empirical effect, which we term Functional Faithfulness, whereby intervening on a specific internal feature induces coherent and predictable shifts across multiple linguistic dimensions aligned with the target semantic attribute. Our findings suggest that LLMs internalize deeply integrated representations of high-order concepts, and provide a novel, robust mechanistic path for the regulation of complex AI behaviors.

### 7. COFFEE: COdesign Framework for Feature Enriched Embeddings in Ads-Ranking Systems

- **LLM Score**: 7
- **Keyword Score**: 11
- **Authors**: Sohini Roychowdhury, Doris Wang, Qian Ge, Joy Mu, Srihari Reddy
- **URL**: <http://arxiv.org/abs/2601.02807v1>
- **Submitted**: 2026-01-06 08:29:12
- **Comment**: 4 pages, 5 figures, 1 table
- **Topic Keywords**: ranking, click, ctr, click-through rate, recommend, rank
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of ads-ranking systems and user behavior modeling. However, the focus on commercial ads-recommendation models and scaling law principles is somewhat specific to the e-commerce domain, which is not your primary area of interest. Nonetheless, the paper's emphasis on real-time relevance optimization and enriched user-ad representations aligns with your broader interests in IR and NLP.

#### Abstract
> Diverse and enriched data sources are essential for commercial ads-recommendation models to accurately assess user interest both before and after engagement with content. While extended user-engagement histories can improve the prediction of user interests, it is equally important to embed activity sequences from multiple sources to ensure freshness of user and ad-representations, following scaling law principles. In this paper, we present a novel three-dimensional framework for enhancing user-ad representations without increasing model inference or serving complexity. The first dimension examines the impact of incorporating diverse event sources, the second considers the benefits of longer user histories, and the third focuses on enriching data with additional event attributes and multi-modal embeddings. We assess the return on investment (ROI) of our source enrichment framework by comparing organic user engagement sources, such as content viewing, with ad-impression sources. The proposed method can boost the area under curve (AUC) and the slope of scaling curves for ad-impression sources by 1.56 to 2 times compared to organic usage sources even for short online-sequence lengths of 100 to 10K. Additionally, click-through rate (CTR) prediction improves by 0.56% AUC over the baseline production ad-recommendation system when using enriched ad-impression event sources, leading to improved sequence scaling resolutions for longer and offline user-ad representations.

### 8. Netflix Artwork Personalization via LLM Post-training

- **LLM Score**: 7
- **Keyword Score**: 4
- **Authors**: Hyunji Nam, Sejoon Oh, Emma Kong, Yesu Feng, Moumita Bhattacharya
- **URL**: <http://arxiv.org/abs/2601.02764v1>
- **Submitted**: 2026-01-06 06:56:53
- **Comment**: 6 pages
- **Topic Keywords**: recommend, personalization, commerce, e-commerce
- **Reason**: This paper explores personalized artwork recommendations using post-trained LLMs, which is related to query understanding and ranking models in Information Retrieval. Although it's not directly focused on search technologies, it involves deep semantic understanding and real-time relevance optimization, aligning with your research interests. However, the e-commerce domain is not the primary focus, and the paper's emphasis on recommender systems is somewhat tangential to your primary interest in Information Retrieval.

#### Abstract
> Large language models (LLMs) have demonstrated success in various applications of user recommendation and personalization across e-commerce and entertainment. On many entertainment platforms such as Netflix, users typically interact with a wide range of titles, each represented by an artwork. Since users have diverse preferences, an artwork that appeals to one type of user may not resonate with another with different preferences. Given this user heterogeneity, our work explores the novel problem of personalized artwork recommendations according to diverse user preferences. Similar to the multi-dimensional nature of users' tastes, titles contain different themes and tones that may appeal to different viewers. For example, the same title might feature both heartfelt family drama and intense action scenes. Users who prefer romantic content may like the artwork emphasizing emotional warmth between the characters, while those who prefer action thrillers may find high-intensity action scenes more intriguing. Rather than a one-size-fits-all approach, we conduct post-training of pre-trained LLMs to make personalized artwork recommendations, selecting the most preferred visual representation of a title for each user and thereby improving user satisfaction and engagement. Our experimental results with Llama 3.1 8B models (trained on a dataset of 110K data points and evaluated on 5K held-out user-title pairs) show that the post-trained LLMs achieve 3-5\% improvements over the Netflix production model, suggesting a promising direction for granular personalized recommendations using LLMs.

### 9. WebAnchor: Anchoring Agent Planning to Stabilize Long-Horizon Web Reasoning

- **LLM Score**: 6
- **Keyword Score**: 1
- **Authors**: Xinmiao Yu, Liwen Zhang, Xiaocheng Feng, Yong Jiang, Bing Qin, Pengjun Xie, Jingren Zhou
- **URL**: <http://arxiv.org/abs/2601.03164v2>
- **Submitted**: 2026-01-06 16:36:40
- **Topic Keywords**: search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Search technologies, particularly in the context of web reasoning and large language models. However, the focus on reinforcement learning and planning in web information seeking is not a central match to your primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large Language Model(LLM)-based agents have shown strong capabilities in web information seeking, with reinforcement learning (RL) becoming a key optimization paradigm. However, planning remains a bottleneck, as existing methods struggle with long-horizon strategies. Our analysis reveals a critical phenomenon, plan anchor, where the first reasoning step disproportionately impacts downstream behavior in long-horizon web reasoning tasks. Current RL algorithms, fail to account for this by uniformly distributing rewards across the trajectory. To address this, we propose Anchor-GRPO, a two-stage RL framework that decouples planning and execution. In Stage 1, the agent optimizes its first-step planning using fine-grained rubrics derived from self-play experiences and human calibration. In Stage 2, execution is aligned with the initial plan through sparse rewards, ensuring stable and efficient tool usage. We evaluate Anchor-GRPO on four benchmarks: BrowseComp, BrowseComp-Zh, GAIA, and XBench-DeepSearch. Across models from 3B to 30B, Anchor-GRPO outperforms baseline GRPO and First-step GRPO, improving task success and tool efficiency. Notably, WebAnchor-30B achieves 46.0% pass@1 on BrowseComp and 76.4% on GAIA. Anchor-GRPO also demonstrates strong scalability, getting higher accuracy as model size and context length increase.

### 10. Enhancing Multilingual RAG Systems with Debiased Language Preference-Guided Query Fusion

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Jeonghyun Park, Byeongjeong Kim, Seojin Hwang, Hwanhee Lee
- **URL**: <http://arxiv.org/abs/2601.02956v1>
- **Submitted**: 2026-01-06 12:01:56
- **Comment**: 20 pages, 5 figures, 15 tables
- **Topic Keywords**: retriever, query, rag, retrieval
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the area of multilingual retrieval and query understanding. However, the focus on debiasing language preference and the use of Large Language Models (LLMs) is not a central match to your interests in query understanding, ranking models, and user behavior modeling. The paper's relevance to your work is limited to the broader context of IR and NLP.

#### Abstract
> Multilingual Retrieval-Augmented Generation (mRAG) systems often exhibit a perceived preference for high-resource languages, particularly English, resulting in the widespread adoption of English pivoting. While prior studies attribute this advantage to the superior English-centric capabilities of Large Language Models (LLMs), we find that such measurements are significantly distorted by structural priors inherent in evaluation benchmarks. Specifically, we identify exposure bias and a gold availability prior-both driven by the disproportionate concentration of resources in English-as well as cultural priors rooted in topic locality, as factors that hinder accurate assessment of genuine language preference. To address these biases, we propose DeLP (Debiased Language Preference), a calibrated metric designed to explicitly factor out these structural confounds. Our analysis using DeLP reveals that the previously reported English preference is largely a byproduct of evidence distribution rather than an inherent model bias. Instead, we find that retrievers fundamentally favor monolingual alignment between the query and the document language. Building on this insight, we introduce DELTA (DEbiased Language preference-guided Text Augmentation), a lightweight and efficient mRAG framework that strategically leverages monolingual alignment to optimize cross-lingual retrieval and generation. Experimental results demonstrate that DELTA consistently outperforms English pivoting and mRAG baselines across diverse languages.

### 11. Detecting Hallucinations in Retrieval-Augmented Generation via Semantic-level Internal Reasoning Graph

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Jianpeng Hu, Yanzeng Li, Jialun Zhong, Wenfa Qi, Lei Zou
- **URL**: <http://arxiv.org/abs/2601.03052v1>
- **Submitted**: 2026-01-06 14:35:20
- **Topic Keywords**: relevance, rag, retrieval
- **Reason**: This paper is somewhat related to the user's research interests in Information Retrieval, particularly in the context of retrieval-augmented generation and semantic-level understanding. However, the focus on detecting hallucinations in language models and the use of internal reasoning graphs is not a central match for the user's primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> The Retrieval-augmented generation (RAG) system based on Large language model (LLM) has made significant progress. It can effectively reduce factuality hallucinations, but faithfulness hallucinations still exist. Previous methods for detecting faithfulness hallucinations either neglect to capture the models' internal reasoning processes or handle those features coarsely, making it difficult for discriminators to learn. This paper proposes a semantic-level internal reasoning graph-based method for detecting faithfulness hallucination. Specifically, we first extend the layer-wise relevance propagation algorithm from the token level to the semantic level, constructing an internal reasoning graph based on attribution vectors. This provides a more faithful semantic-level representation of dependency. Furthermore, we design a general framework based on a small pre-trained language model to utilize the dependencies in LLM's reasoning for training and hallucination detection, which can dynamically adjust the pass rate of correct samples through a threshold. Experimental results demonstrate that our method achieves better overall performance compared to state-of-the-art baselines on RAGTruth and Dolly-15k.

### 12. HarmonRank: Ranking-aligned Multi-objective Ensemble for Live-streaming E-commerce Recommendation

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Boyang Xia, Zhou Yu, Zhiliang Zhu, Hanxiao Sun, Biyun Han, Jun Wang, Runnan Liu, Wenwu Ou
- **URL**: <http://arxiv.org/abs/2601.02955v1>
- **Submitted**: 2026-01-06 11:59:02
- **Comment**: 11 pages, 5 figures
- **Topic Keywords**: ranking, recommend, commerce, e-commerce, rank
- **Reason**: The paper explores a ranking mechanism for live-streaming e-commerce recommendation, which is somewhat related to information retrieval and search technologies. However, it focuses on recommender systems and does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's emphasis on real-time relevance optimization and deep semantic understanding is also not a central match for the provided research interests.

#### Abstract
> Recommendation for live-streaming e-commerce is gaining increasing attention due to the explosive growth of the live streaming economy. Different from traditional e-commerce, live-streaming e-commerce shifts the focus from products to streamers, which requires ranking mechanism to balance both purchases and user-streamer interactions for long-term ecology. To trade off multiple objectives, a popular solution is to build an ensemble model to integrate multi-objective scores into a unified score. The ensemble model is usually supervised by multiple independent binary classification losses of all objectives. However, this paradigm suffers from two inherent limitations. First, the optimization direction of the binary classification task is misaligned with the ranking task (evaluated by AUC). Second, this paradigm overlooks the alignment between objectives, e.g., comment and buy behaviors are partially dependent which can be revealed in labels correlations. The model can achieve better trade-offs if it learns the aligned parts of ranking abilities among different objectives.
  To mitigate these limitations, we propose a novel multi-objective ensemble framework HarmonRank to fulfill both alignment to the ranking task and alignment among objectives. For alignment to ranking, we formulate ranking metric AUC as a rank-sum problem and utilize differentiable ranking techniques for ranking-oriented optimization. For inter-objective alignment, we change the original one-step ensemble paradigm to a two-step relation-aware ensemble scheme.
  Extensive offline experiments results on two industrial datasets and online experiments demonstrate that our approach significantly outperforms existing state-of-the-art methods. The proposed method has been fully deployed in Kuaishou's live-streaming e-commerce recommendation platform with 400 million DAUs, contributing over 2% purchase gain.

### 13. Audit Me If You Can: Query-Efficient Active Fairness Auditing of Black-Box LLMs

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: David Hartmann, Lena Pohlmann, Lelia Hanslik, Noah Gie√üing, Bettina Berendt, Pieter Delobelle
- **URL**: <http://arxiv.org/abs/2601.03087v1>
- **Submitted**: 2026-01-06 15:22:23
- **Comment**: Submitted to ACL ARR 2026
- **Topic Keywords**: query, queries
- **Reason**: This paper focuses on auditing Large Language Models for fairness, which is related to information retrieval and NLP. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's emphasis on active sampling and uncertainty estimation is somewhat relevant to the broader field of IR, but it does not align closely with the user's primary research themes.

#### Abstract
> Large Language Models (LLMs) exhibit systematic biases across demographic groups. Auditing is proposed as an accountability tool for black-box LLM applications, but suffers from resource-intensive query access. We conceptualise auditing as uncertainty estimation over a target fairness metric and introduce BAFA, the Bounded Active Fairness Auditor for query-efficient auditing of black-box LLMs. BAFA maintains a version space of surrogate models consistent with queried scores and computes uncertainty intervals for fairness metrics (e.g., $Œî$ AUC) via constrained empirical risk minimisation. Active query selection narrows these intervals to reduce estimation error. We evaluate BAFA on two standard fairness dataset case studies: \textsc{CivilComments} and \textsc{Bias-in-Bios}, comparing against stratified sampling, power sampling, and ablations. BAFA achieves target error thresholds with up to 40$\times$ fewer queries than stratified sampling (e.g., 144 vs 5,956 queries at $\varepsilon=0.02$ for \textsc{CivilComments}) for tight thresholds, demonstrates substantially better performance over time, and shows lower variance across runs. These results suggest that active sampling can reduce resources needed for independent fairness auditing with LLMs, supporting continuous model evaluations.

### 14. Decoupling the Effect of Chain-of-Thought Reasoning: A Human Label Variation Perspective

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Beiduo Chen, Tiancheng Hu, Caiqi Zhang, Robert Litschko, Anna Korhonen, Barbara Plank
- **URL**: <http://arxiv.org/abs/2601.03154v1>
- **Submitted**: 2026-01-06 16:26:40
- **Comment**: 19 pages, 10 figures
- **Topic Keywords**: ranking, rank
- **Reason**: This paper explores the role of Chain-of-Thought reasoning in large language models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on human label variation and distributional alignment is more aligned with Natural Language Processing and related topics, but not directly applicable to the user's core research themes.

#### Abstract
> Reasoning-tuned LLMs utilizing long Chain-of-Thought (CoT) excel at single-answer tasks, yet their ability to model Human Label Variation--which requires capturing probabilistic ambiguity rather than resolving it--remains underexplored. We investigate this through systematic disentanglement experiments on distribution-based tasks, employing Cross-CoT experiments to isolate the effect of reasoning text from intrinsic model priors. We observe a distinct "decoupled mechanism": while CoT improves distributional alignment, final accuracy is dictated by CoT content (99% variance contribution), whereas distributional ranking is governed by model priors (over 80%). Step-wise analysis further shows that while CoT's influence on accuracy grows monotonically during the reasoning process, distributional structure is largely determined by LLM's intrinsic priors. These findings suggest that long CoT serves as a decisive LLM decision-maker for the top option but fails to function as a granular distribution calibrator for ambiguous tasks.

### 15. LLM-Augmented Changepoint Detection: A Framework for Ensemble Detection and Automated Explanation

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Fabian Lukassen, Christoph Weisser, Michael Schlee, Manish Kumar, Anton Thielmann, Benjamin Saefken, Thomas Kneib
- **URL**: <http://arxiv.org/abs/2601.02957v1>
- **Submitted**: 2026-01-06 12:04:38
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper combines changepoint detection with Large Language Models (LLMs) and ensemble methods, which is somewhat related to information retrieval and ranking models. However, the focus on time series data and changepoint detection is not directly aligned with the user's core research themes in IR and search technologies. The use of LLMs and ensemble methods does show some overlap with the user's interests in NLP and data mining.

#### Abstract
> This paper introduces a novel changepoint detection framework that combines ensemble statistical methods with Large Language Models (LLMs) to enhance both detection accuracy and the interpretability of regime changes in time series data. Two critical limitations in the field are addressed. First, individual detection methods exhibit complementary strengths and weaknesses depending on data characteristics, making method selection non-trivial and prone to suboptimal results. Second, automated, contextual explanations for detected changes are largely absent. The proposed ensemble method aggregates results from ten distinct changepoint detection algorithms, achieving superior performance and robustness compared to individual methods. Additionally, an LLM-powered explanation pipeline automatically generates contextual narratives, linking detected changepoints to potential real-world historical events. For private or domain-specific data, a Retrieval-Augmented Generation (RAG) solution enables explanations grounded in user-provided documents. The open source Python framework demonstrates practical utility in diverse domains, including finance, political science, and environmental science, transforming raw statistical output into actionable insights for analysts and decision-makers.

### 16. UltraLogic: Enhancing LLM Reasoning through Large-Scale Data Synthesis and Bipolar Float Reward

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Yile Liu, Yixian Liu, Zongwei Li, Yufei Huang, Xinhua Feng, Zhichao Hu, Jinglu Hu, Jianfeng Yan, Fengzong Lian, Yuhong Liu
- **URL**: <http://arxiv.org/abs/2601.03205v1>
- **Submitted**: 2026-01-06 17:41:32
- **Comment**: 19 pages, 6 figures, 7 tables
- **Topic Keywords**: ltr
- **Reason**: The paper discusses Large Language Models (LLMs) and Reinforcement Learning with Verifiable Rewards (RLVR), which are related to Natural Language Processing (NLP), a topic of interest. However, the focus on general-purpose reasoning and task diversity is not directly aligned with the user's primary research themes in Information Retrieval (IR) and Search technologies, particularly query understanding, ranking models, and user behavior modeling.

#### Abstract
> While Large Language Models (LLMs) have demonstrated significant potential in natural language processing , complex general-purpose reasoning requiring multi-step logic, planning, and verification remains a critical bottleneck. Although Reinforcement Learning with Verifiable Rewards (RLVR) has succeeded in specific domains , the field lacks large-scale, high-quality, and difficulty-calibrated data for general reasoning. To address this, we propose UltraLogic, a framework that decouples the logical core of a problem from its natural language expression through a Code-based Solving methodology to automate high-quality data production. The framework comprises hundreds of unique task types and an automated calibration pipeline across ten difficulty levels. Furthermore, to mitigate binary reward sparsity and the Non-negative Reward Trap, we introduce the Bipolar Float Reward (BFR) mechanism, utilizing graded penalties to effectively distinguish perfect responses from those with logical flaws. Our experiments demonstrate that task diversity is the primary driver for reasoning enhancement , and that BFR, combined with a difficulty matching strategy, significantly improves training efficiency, guiding models toward global logical optima.

### 17. To Generate or Discriminate? Methodological Considerations for Measuring Cultural Alignment in LLMs

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Saurabh Kumar Pandey, Sougata Saha, Monojit Choudhury
- **URL**: <http://arxiv.org/abs/2601.02858v1>
- **Submitted**: 2026-01-06 09:42:03
- **Comment**: IJCNLP-AACL 2025
- **Topic Keywords**: user behavior, personalization
- **Reason**: This paper explores the use of Large Language Models (LLMs) in understanding cultural alignment, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on LLMs and cultural alignment is not directly aligned with the user's primary research interests in IR and Search technologies. The use of NLP and data mining in the paper is relevant, but the specific application to LLMs and cultural alignment limits its relevance to the user's broader interests.

#### Abstract
> Socio-demographic prompting (SDP) - prompting Large Language Models (LLMs) using demographic proxies to generate culturally aligned outputs - often shows LLM responses as stereotypical and biased. While effective in assessing LLMs' cultural competency, SDP is prone to confounding factors such as prompt sensitivity, decoding parameters, and the inherent difficulty of generation over discrimination tasks due to larger output spaces. These factors complicate interpretation, making it difficult to determine if the poor performance is due to bias or the task design. To address this, we use inverse socio-demographic prompting (ISDP), where we prompt LLMs to discriminate and predict the demographic proxy from actual and simulated user behavior from different users. We use the Goodreads-CSI dataset (Saha et al., 2025), which captures difficulty in understanding English book reviews for users from India, Mexico, and the USA, and test four LLMs: Aya-23, Gemma-2, GPT-4o, and LLaMA-3.1 with ISDP. Results show that models perform better with actual behaviors than simulated ones, contrary to what SDP suggests. However, performance with both behavior types diminishes and becomes nearly equal at the individual level, indicating limits to personalization.

### 18. Can Embedding Similarity Predict Cross-Lingual Transfer? A Systematic Study on African Languages

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Tewodros Kederalah Idris, Prasenjit Mitra, Roald Eiselen
- **URL**: <http://arxiv.org/abs/2601.03168v1>
- **Submitted**: 2026-01-06 16:39:28
- **Comment**: 13 pages, 1 figure, 19 tables
- **Topic Keywords**: retrieval
- **Reason**: This paper explores cross-lingual transfer in NLP, which is somewhat related to your interests in Information Retrieval and NLP. However, the focus on low-resource African languages and embedding similarity metrics is not directly aligned with your primary research themes, such as query understanding, ranking models, and user behavior modeling.

#### Abstract
> Cross-lingual transfer is essential for building NLP systems for low-resource African languages, but practitioners lack reliable methods for selecting source languages. We systematically evaluate five embedding similarity metrics across 816 transfer experiments spanning three NLP tasks, three African-centric multilingual models, and 12 languages from four language families. We find that cosine gap and retrieval-based metrics (P@1, CSLS) reliably predict transfer success ($œÅ= 0.4-0.6$), while CKA shows negligible predictive power ($œÅ\approx 0.1$). Critically, correlation signs reverse when pooling across models (Simpson's Paradox), so practitioners must validate per-model. Embedding metrics achieve comparable predictive power to URIEL linguistic typology. Our results provide concrete guidance for source language selection and highlight the importance of model-specific analysis.

### 19. Accurate Table Question Answering with Accessible LLMs

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Yangfan Jiang, Fei Wei, Ergute Bao, Yaliang Li, Bolin Ding, Yin Yang, Xiaokui Xiao
- **URL**: <http://arxiv.org/abs/2601.03137v1>
- **Submitted**: 2026-01-06 16:07:25
- **Comment**: accepted for publication in the Proceedings of the IEEE International Conference on Data Engineering (ICDE) 2026
- **Topic Keywords**: rag
- **Reason**: The paper presents a novel approach to table question answering using multi-agent coordination with accessible LLMs. While it touches on aspects of natural language processing and deep semantic understanding, its primary focus is on leveraging LLMs for a specific task, which is somewhat related to the user's interests in information retrieval and NLP. However, the paper does not directly address query understanding, ranking models, or user behavior modeling, limiting its relevance to the user's core research themes.

#### Abstract
> Given a table T in a database and a question Q in natural language, the table question answering (TQA) task aims to return an accurate answer to Q based on the content of T. Recent state-of-the-art solutions leverage large language models (LLMs) to obtain high-quality answers. However, most rely on proprietary, large-scale LLMs with costly API access, posing a significant financial barrier. This paper instead focuses on TQA with smaller, open-weight LLMs that can run on a desktop or laptop. This setting is challenging, as such LLMs typically have weaker capabilities than large proprietary models, leading to substantial performance degradation with existing methods.
  We observe that a key reason for this degradation is that prior approaches often require the LLM to solve a highly sophisticated task using long, complex prompts, which exceed the capabilities of small open-weight LLMs. Motivated by this observation, we present Orchestra, a multi-agent approach that unlocks the potential of accessible LLMs for high-quality, cost-effective TQA. Orchestra coordinates a group of LLM agents, each responsible for a relatively simple task, through a structured, layered workflow to solve complex TQA problems -- akin to an orchestra. By reducing the prompt complexity faced by each agent, Orchestra significantly improves output reliability.
  We implement Orchestra on top of AgentScope, an open-source multi-agent framework, and evaluate it on multiple TQA benchmarks using a wide range of open-weight LLMs. Experimental results show that Orchestra achieves strong performance even with small- to medium-sized models. For example, with Qwen2.5-14B, Orchestra reaches 72.1% accuracy on WikiTQ, approaching the best prior result of 75.3% achieved with GPT-4; with larger Qwen, Llama, or DeepSeek models, Orchestra outperforms all prior methods and establishes new state-of-the-art results across all benchmarks.

### 20. BaseCal: Unsupervised Confidence Calibration via Base Model Signals

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Hexiang Tan, Wanli Yang, Junwei Zhang, Xin Chen, Rui Tang, Du Su, Jingang Wang, Yuanzhuo Wang, Fei Sun, Xueqi Cheng
- **URL**: <http://arxiv.org/abs/2601.03042v1>
- **Submitted**: 2026-01-06 14:22:21
- **Topic Keywords**: rag
- **Reason**: This paper focuses on confidence calibration for Large Language Models (LLMs), which is a relevant topic in the broader context of Natural Language Processing (NLP). However, it does not directly relate to the user's primary research interests in Information Retrieval (IR), search technologies, or query understanding. The paper's emphasis on LLMs and their calibration does touch on aspects of deep semantic understanding, but it is not a central match for the user's research themes.

#### Abstract
> Reliable confidence is essential for trusting the outputs of LLMs, yet widely deployed post-trained LLMs (PoLLMs) typically compromise this trust with severe overconfidence. In contrast, we observe that their corresponding base LLMs often remain well-calibrated. This naturally motivates us to calibrate PoLLM confidence using the base LLM as a reference. This work proposes two ways to achieve this. A straightforward solution, BaseCal-ReEval, evaluates PoLLM's responses by feeding them into the base LLM to get average probabilities as confidence. While effective, this approach introduces additional inference overhead. To address this, we propose BaseCal-Proj, which trains a lightweight projection to map the final-layer hidden states of PoLLMs back to those of their base LLMs. These projected states are then processed by the base LLM's output layer to derive base-calibrated confidence for PoLLM's responses. Notably, BaseCal is an unsupervised, plug-and-play solution that operates without human labels or LLM modifications. Experiments across five datasets and three LLM families demonstrate the effectiveness of BaseCal, reducing Expected Calibration Error (ECE) by an average of 42.90\% compared to the best unsupervised baselines.

### 21. MedDialogRubrics: A Comprehensive Benchmark and Evaluation Framework for Multi-turn Medical Consultations in Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Lecheng Gong, Weimin Fang, Ting Yang, Dongjie Tao, Chunxiao Guo, Peng Wei, Bo Xie, Jinqun Guan, Zixiao Chen, Fang Shi, Jinjie Gu, Junwei Liu
- **URL**: <http://arxiv.org/abs/2601.03023v2>
- **Submitted**: 2026-01-06 13:56:33
- **Topic Keywords**: ctr
- **Reason**: This paper is somewhat related to information retrieval and search technologies, but its focus on medical conversational AI and large language models is not directly aligned with the user's core research themes. While it involves query understanding and ranking models, the context is specific to medical consultations and does not address the user's primary interests in e-commerce and real-time relevance optimization.

#### Abstract
> Medical conversational AI (AI) plays a pivotal role in the development of safer and more effective medical dialogue systems. However, existing benchmarks and evaluation frameworks for assessing the information-gathering and diagnostic reasoning abilities of medical large language models (LLMs) have not been rigorously evaluated. To address these gaps, we present MedDialogRubrics, a novel benchmark comprising 5,200 synthetically constructed patient cases and over 60,000 fine-grained evaluation rubrics generated by LLMs and subsequently refined by clinical experts, specifically designed to assess the multi-turn diagnostic capabilities of LLM. Our framework employs a multi-agent system to synthesize realistic patient records and chief complaints from underlying disease knowledge without accessing real-world electronic health records, thereby mitigating privacy and data-governance concerns. We design a robust Patient Agent that is limited to a set of atomic medical facts and augmented with a dynamic guidance mechanism that continuously detects and corrects hallucinations throughout the dialogue, ensuring internal coherence and clinical plausibility of the simulated cases. Furthermore, we propose a structured LLM-based and expert-annotated rubric-generation pipeline that retrieves Evidence-Based Medicine (EBM) guidelines and utilizes the reject sampling to derive a prioritized set of rubric items ("must-ask" items) for each case. We perform a comprehensive evaluation of state-of-the-art models and demonstrate that, across multiple assessment dimensions, current models face substantial challenges. Our results indicate that improving medical dialogue will require advances in dialogue management architectures, not just incremental tuning of the base-model.

### 22. P-Check: Advancing Personalized Reward Model via Learning to Generate Dynamic Checklist

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Kwangwook Seo, Dongha Lee
- **URL**: <http://arxiv.org/abs/2601.02986v1>
- **Submitted**: 2026-01-06 12:53:53
- **Comment**: Work in Progress
- **Topic Keywords**: rag
- **Reason**: The paper explores personalized reward modeling, which is somewhat related to user behavior modeling in Information Retrieval. However, the focus on reward prediction and checklist generation is not directly aligned with the user's core research themes in query understanding, ranking models, and deep semantic understanding.

#### Abstract
> Recent approaches in personalized reward modeling have primarily focused on leveraging user interaction history to align model judgments with individual preferences. However, existing approaches largely treat user context as a static or implicit conditioning signal, failing to capture the dynamic and multi-faceted nature of human judgment. In this paper, we propose P-Check, a novel personalized reward modeling framework, designed to train a plug-and-play checklist generator that synthesizes dynamic evaluation criteria for guiding the reward prediction. To better align these checklists with personalized nuances, we introduce Preference-Contrastive Criterion Weighting, a training strategy that assigns saliency scores to criteria based on their discriminative power for personalized judgment. We conduct extensive experiments and demonstrate that P-Check not only improves reward accuracy but also enhances downstream personalized generation, and remains robust in OOD scenarios.

### 23. Logical Phase Transitions: Understanding Collapse in LLM Logical Reasoning

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Xinglang Zhang, Yunyao Zhang, ZeLiang Chen, Junqing Yu, Wei Yang, Zikai Song
- **URL**: <http://arxiv.org/abs/2601.02902v1>
- **Submitted**: 2026-01-06 10:38:25
- **Topic Keywords**: rag
- **Reason**: The paper explores large language models' logical reasoning capabilities, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on logical reasoning and its collapse in LLMs is not directly aligned with the user's core research themes in IR and Search technologies. The connection to NLP is more relevant, but still not a central match.

#### Abstract
> Symbolic logical reasoning is a critical yet underexplored capability of large language models (LLMs), providing reliable and verifiable decision-making in high-stakes domains such as mathematical reasoning and legal judgment. In this study, we present a systematic analysis of logical reasoning under controlled increases in logical complexity, and reveal a previously unrecognized phenomenon, which we term Logical Phase Transitions: rather than degrading smoothly, logical reasoning performance remains stable within a regime but collapses abruptly beyond a critical logical depth, mirroring physical phase transitions such as water freezing beyond a critical temperature threshold. Building on this insight, we propose Neuro-Symbolic Curriculum Tuning, a principled framework that adaptively aligns natural language with logical symbols to establish a shared representation, and reshapes training dynamics around phase-transition boundaries to progressively strengthen reasoning at increasing logical depths. Experiments on five benchmarks show that our approach effectively mitigates logical reasoning collapse at high complexity, yielding average accuracy gains of +1.26 in naive prompting and +3.95 in CoT, while improving generalization to unseen logical compositions. Code and data are available at https://github.com/AI4SS/Logical-Phase-Transitions.

### 24. Automatic Prompt Engineering with No Task Cues and No Tuning

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Faisal Chowdhury, Nandana Mihindukulasooriya, Niharika S D'Souza, Horst Samulowitz, Neeru Gupta, Tomasz Hanusiak, Michal Kapitonow
- **URL**: <http://arxiv.org/abs/2601.03130v1>
- **Submitted**: 2026-01-06 16:04:45
- **Topic Keywords**: search
- **Reason**: The paper touches on Information Retrieval (IR) through its application to tabular data search, but its focus on prompt engineering and automatic generation is more aligned with Natural Language Processing (NLP). While it explores a novel task, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest.

#### Abstract
> This paper presents a system for automatic prompt engineering that is much simpler in both design and application and yet as effective as the existing approaches. It requires no tuning and no explicit clues about the task. We evaluated our approach on cryptic column name expansion (CNE) in database tables, a task which is critical for tabular data search, access, and understanding and yet there has been very little existing work. We evaluated on datasets in two languages, English and German. This is the first work to report on the application of automatic prompt engineering for the CNE task. To the best of our knowledge, this is also the first work on the application of automatic prompt engineering for a language other than English.

### 25. Do LLMs Encode Functional Importance of Reasoning Tokens?

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Janvijay Singh, Dilek Hakkani-T√ºr
- **URL**: <http://arxiv.org/abs/2601.03066v1>
- **Submitted**: 2026-01-06 14:50:02
- **Comment**: 20 pages, 8 figures, 2 tables
- **Topic Keywords**: rank
- **Reason**: This paper explores the internal workings of Large Language Models (LLMs), specifically how they prioritize and use reasoning tokens for answer generation. While it touches on aspects of query understanding and ranking models, its primary focus is on the internal workings of LLMs, which is somewhat related to your interests in Information Retrieval and NLP. However, the paper's emphasis on LLMs and their internal mechanisms makes it less directly relevant to your core research themes.

#### Abstract
> Large language models solve complex tasks by generating long reasoning chains, achieving higher accuracy at the cost of increased computational cost and reduced ability to isolate functionally relevant reasoning. Prior work on compact reasoning shortens such chains through probabilistic sampling, heuristics, or supervision from frontier models, but offers limited insight into whether models internally encode token-level functional importance for answer generation. We address this gap diagnostically and propose greedy pruning, a likelihood-preserving deletion procedure that iteratively removes reasoning tokens whose removal minimally degrades model likelihood under a specified objective, yielding length-controlled reasoning chains. We evaluate pruned reasoning in a distillation framework and show that students trained on pruned chains outperform a frontier-model-supervised compression baseline at matched reasoning lengths. Finally, our analysis reveals systematic pruning patterns and shows that attention scores can predict greedy pruning ranks, further suggesting that models encode a nontrivial functional importance structure over reasoning tokens.

### 26. Parallel Latent Reasoning for Sequential Recommendation

- **LLM Score**: 3
- **Keyword Score**: 1
- **Authors**: Jiakai Tang, Xu Chen, Wen Chen, Jian Wu, Yuning Jiang, Bo Zheng
- **URL**: <http://arxiv.org/abs/2601.03153v1>
- **Submitted**: 2026-01-06 16:25:48
- **Topic Keywords**: recommend
- **Reason**: This paper is somewhat related to the user's interests in Information Retrieval and Search technologies, but it primarily focuses on sequential recommendation and recommender systems, which is a secondary interest of the user. The paper's emphasis on parallel reasoning and latent reasoning is not directly related to the user's core research themes in query understanding, ranking models, and user behavior modeling. However, the paper's use of learnable trigger tokens and mixture-of-reasoning-streams aggregation may be of interest to the user in the context of NLP and data mining.

#### Abstract
> Capturing complex user preferences from sparse behavioral sequences remains a fundamental challenge in sequential recommendation. Recent latent reasoning methods have shown promise by extending test-time computation through multi-step reasoning, yet they exclusively rely on depth-level scaling along a single trajectory, suffering from diminishing returns as reasoning depth increases. To address this limitation, we propose \textbf{Parallel Latent Reasoning (PLR)}, a novel framework that pioneers width-level computational scaling by exploring multiple diverse reasoning trajectories simultaneously. PLR constructs parallel reasoning streams through learnable trigger tokens in continuous latent space, preserves diversity across streams via global reasoning regularization, and adaptively synthesizes multi-stream outputs through mixture-of-reasoning-streams aggregation. Extensive experiments on three real-world datasets demonstrate that PLR substantially outperforms state-of-the-art baselines while maintaining real-time inference efficiency. Theoretical analysis further validates the effectiveness of parallel reasoning in improving generalization capability. Our work opens new avenues for enhancing reasoning capacity in sequential recommendation beyond existing depth scaling.

### 27. TiMem: Temporal-Hierarchical Memory Consolidation for Long-Horizon Conversational Agents

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Kai Li, Xuanqing Yu, Ziyi Ni, Yi Zeng, Yao Xu, Zheqing Zhang, Xin Li, Jitao Sang, Xiaogang Duan, Xuelei Wang, Chengbao Liu, Jie Tan
- **URL**: <http://arxiv.org/abs/2601.02845v1>
- **Submitted**: 2026-01-06 09:24:19
- **Topic Keywords**: queries, rag, personalization
- **Reason**: This paper focuses on conversational agents and memory consolidation, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves language models and memory organization, the context is specific to conversational agents and does not align with your primary focus on deep semantic understanding and real-time relevance optimization in IR.

#### Abstract
> Long-horizon conversational agents have to manage ever-growing interaction histories that quickly exceed the finite context windows of large language models (LLMs). Existing memory frameworks provide limited support for temporally structured information across hierarchical levels, often leading to fragmented memories and unstable long-horizon personalization. We present TiMem, a temporal--hierarchical memory framework that organizes conversations through a Temporal Memory Tree (TMT), enabling systematic memory consolidation from raw conversational observations to progressively abstracted persona representations. TiMem is characterized by three core properties: (1) temporal--hierarchical organization through TMT; (2) semantic-guided consolidation that enables memory integration across hierarchical levels without fine-tuning; and (3) complexity-aware memory recall that balances precision and efficiency across queries of varying complexity. Under a consistent evaluation setup, TiMem achieves state-of-the-art accuracy on both benchmarks, reaching 75.30% on LoCoMo and 76.88% on LongMemEval-S. It outperforms all evaluated baselines while reducing the recalled memory length by 52.20% on LoCoMo. Manifold analysis indicates clear persona separation on LoCoMo and reduced dispersion on LongMemEval-S. Overall, TiMem treats temporal continuity as a first-class organizing principle for long-horizon memory in conversational agents.

### 28. MemRL: Self-Evolving Agents via Runtime Reinforcement Learning on Episodic Memory

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Shengtao Zhang, Jiaqian Wang, Ruiwen Zhou, Junwei Liao, Yuchen Feng, Weinan Zhang, Ying Wen, Zhiyu Li, Feiyu Xiong, Yutao Qi, Bo Tang, Muning Wen
- **URL**: <http://arxiv.org/abs/2601.03192v1>
- **Submitted**: 2026-01-06 17:14:50
- **Comment**: 23 pages, 11 figures
- **Topic Keywords**: relevance, retrieval
- **Reason**: This paper focuses on self-evolving agents via runtime reinforcement learning on episodic memory, which is not directly related to information retrieval, search technologies, or query understanding. While it involves deep learning and memory-based methods, the context is more aligned with artificial intelligence and machine learning, rather than information retrieval or natural language processing.

#### Abstract
> The hallmark of human intelligence is the ability to master new skills through Constructive Episodic Simulation-retrieving past experiences to synthesize solutions for novel tasks. While Large Language Models possess strong reasoning capabilities, they struggle to emulate this self-evolution: fine-tuning is computationally expensive and prone to catastrophic forgetting, while existing memory-based methods rely on passive semantic matching that often retrieves noise. To address these challenges, we propose MemRL, a framework that enables agents to self-evolve via non-parametric reinforcement learning on episodic memory. MemRL explicitly separates the stable reasoning of a frozen LLM from the plastic, evolving memory. Unlike traditional methods, MemRL employs a Two-Phase Retrieval mechanism that filters candidates by semantic relevance and then selects them based on learned Q-values (utility). These utilities are continuously refined via environmental feedback in an trial-and-error manner, allowing the agent to distinguish high-value strategies from similar noise. Extensive experiments on HLE, BigCodeBench, ALFWorld, and Lifelong Agent Bench demonstrate that MemRL significantly outperforms state-of-the-art baselines. Our analysis experiments confirm that MemRL effectively reconciles the stability-plasticity dilemma, enabling continuous runtime improvement without weight updates.

### 29. LittiChoQA: Literary Texts in Indic Languages Chosen for Question Answering

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Aarya Khandelwal, Ritwik Mishra, Rajiv Ratn Shah
- **URL**: <http://arxiv.org/abs/2601.03025v1>
- **Submitted**: 2026-01-06 13:59:41
- **Comment**: Submitted to ARR Jan cycle. Targetting AACL 2026
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper focuses on question answering over literary texts in Indic languages, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve question answering and large language models, the specific domain and task are not aligned with your primary focus on e-commerce, query understanding, ranking models, and user behavior modeling.

#### Abstract
> Long-context question answering (QA) over literary texts poses significant challenges for modern large language models, particularly in low-resource languages. We address the scarcity of long-context QA resources for Indic languages by introducing LittiChoQA, the largest literary QA dataset to date covering many languages spoken in the Gangetic plains of India. The dataset comprises over 270K automatically generated question-answer pairs with a balanced distribution of factoid and non-factoid questions, generated from naturally authored literary texts collected from the open web. We evaluate multiple multilingual LLMs on non-factoid, abstractive QA, under both full-context and context-shortened settings. Results demonstrate a clear trade-off between performance and efficiency: full-context fine-tuning yields the highest token-level and semantic-level scores, while context shortening substantially improves throughput. Among the evaluated models, Krutrim-2 achieves the strongest performance, obtaining a semantic score of 76.1 with full context. While, in shortened context settings it scores 74.9 with answer paragraph selection and 71.4 with vector-based retrieval. Qualitative evaluations further corroborate these findings.

### 30. Q-Regularized Generative Auto-Bidding: From Suboptimal Trajectories to Optimal Policies

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Mingming Zhang, Na Li, Zhuang Feiqing, Hongyang Zheng, Jiangbing Zhou, Wang Wuyin, Sheng-jie Sun, XiaoWei Chen, Junxiong Zhu, Lixin Zou, Chenliang Li
- **URL**: <http://arxiv.org/abs/2601.02754v1>
- **Submitted**: 2026-01-06 06:42:25
- **Comment**: 11pages, 5figures, In Proceedings of the 32nd ACM SIGKDD Conference on Knowledge Discovery and Data Mining
- **Topic Keywords**: rag, commerce, e-commerce
- **Reason**: This paper appears to be focused on auto-bidding in e-commerce, leveraging reinforcement learning and generative models. While it touches on optimization and policy learning, it does not seem to directly relate to the user's core research themes in Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling. The paper's emphasis on e-commerce and advertising performance further limits its relevance to the user's broader interests in Natural Language Processing and data mining.

#### Abstract
> With the rapid development of e-commerce, auto-bidding has become a key asset in optimizing advertising performance under diverse advertiser environments. The current approaches focus on reinforcement learning (RL) and generative models. These efforts imitate offline historical behaviors by utilizing a complex structure with expensive hyperparameter tuning. The suboptimal trajectories further exacerbate the difficulty of policy learning.
  To address these challenges, we proposes QGA, a novel Q-value regularized Generative Auto-bidding method. In QGA, we propose to plug a Q-value regularization with double Q-learning strategy into the Decision Transformer backbone. This design enables joint optimization of policy imitation and action-value maximization, allowing the learned bidding policy to both leverage experience from the dataset and alleviate the adverse impact of the suboptimal trajectories. Furthermore, to safely explore the policy space beyond the data distribution, we propose a Q-value guided dual-exploration mechanism, in which the DT model is conditioned on multiple return-to-go targets and locally perturbed actions. This entire exploration process is dynamically guided by the aforementioned Q-value module, which provides principled evaluation for each candidate action. Experiments on public benchmarks and simulation environments demonstrate that QGA consistently achieves superior or highly competitive results compared to existing alternatives. Notably, in large-scale real-world A/B testing, QGA achieves a 3.27% increase in Ad GMV and a 2.49% improvement in Ad ROI.

### 31. X-MuTeST: A Multilingual Benchmark for Explainable Hate Speech Detection and A Novel LLM-consulted Explanation Framework

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Mohammad Zia Ur Rehman, Sai Kartheek Reddy Kasu, Shashivardhan Reddy Koppula, Sai Rithwik Reddy Chirra, Shwetank Shekhar Singh, Nagendra Kumar
- **URL**: <http://arxiv.org/abs/2601.03194v1>
- **Submitted**: 2026-01-06 17:16:45
- **Comment**: Accepted in the proceedings of AAAI 2026
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on explainable hate speech detection, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve NLP and large language models, the context and application are quite different from the user's core themes.

#### Abstract
> Hate speech detection on social media faces challenges in both accuracy and explainability, especially for underexplored Indic languages. We propose a novel explainability-guided training framework, X-MuTeST (eXplainable Multilingual haTe Speech deTection), for hate speech detection that combines high-level semantic reasoning from large language models (LLMs) with traditional attention-enhancing techniques. We extend this research to Hindi and Telugu alongside English by providing benchmark human-annotated rationales for each word to justify the assigned class label. The X-MuTeST explainability method computes the difference between the prediction probabilities of the original text and those of unigrams, bigrams, and trigrams. Final explanations are computed as the union between LLM explanations and X-MuTeST explanations. We show that leveraging human rationales during training enhances both classification performance and explainability. Moreover, combining human rationales with our explainability method to refine the model attention yields further improvements. We evaluate explainability using Plausibility metrics such as Token-F1 and IOU-F1 and Faithfulness metrics such as Comprehensiveness and Sufficiency. By focusing on under-resourced languages, our work advances hate speech detection across diverse linguistic contexts. Our dataset includes token-level rationale annotations for 6,004 Hindi, 4,492 Telugu, and 6,334 English samples. Data and code are available on https://github.com/ziarehman30/X-MuTeST

### 32. Beyond the Black Box: Theory and Mechanism of Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Zeyu Gan, Ruifeng Ren, Wei Yao, Xiaolin Hu, Gengze Xu, Chen Qian, Huayi Tang, Zixuan Gong, Xinhao Yao, Pengwei Tang, Zhenxing Dou, Yong Liu
- **URL**: <http://arxiv.org/abs/2601.02907v1>
- **Submitted**: 2026-01-06 10:45:53
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on the theoretical understanding of Large Language Models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on some related topics, the paper's primary focus is on the theoretical foundations of LLMs, making it less relevant to your research.

#### Abstract
> The rapid emergence of Large Language Models (LLMs) has precipitated a profound paradigm shift in Artificial Intelligence, delivering monumental engineering successes that increasingly impact modern society. However, a critical paradox persists within the current field: despite the empirical efficacy, our theoretical understanding of LLMs remains disproportionately nascent, forcing these systems to be treated largely as ``black boxes''. To address this theoretical fragmentation, this survey proposes a unified lifecycle-based taxonomy that organizes the research landscape into six distinct stages: Data Preparation, Model Preparation, Training, Alignment, Inference, and Evaluation. Within this framework, we provide a systematic review of the foundational theories and internal mechanisms driving LLM performance. Specifically, we analyze core theoretical issues such as the mathematical justification for data mixtures, the representational limits of various architectures, and the optimization dynamics of alignment algorithms. Moving beyond current best practices, we identify critical frontier challenges, including the theoretical limits of synthetic data self-improvement, the mathematical bounds of safety guarantees, and the mechanistic origins of emergent intelligence. By connecting empirical observations with rigorous scientific inquiry, this work provides a structured roadmap for transitioning LLM development from engineering heuristics toward a principled scientific discipline.

### 33. STReasoner: Empowering LLMs for Spatio-Temporal Reasoning in Time Series via Spatial-Aware Reinforcement Learning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Juntong Ni, Shiyu Wang, Ming Jin, Qi He, Wei Jin
- **URL**: <http://arxiv.org/abs/2601.03248v1>
- **Submitted**: 2026-01-06 18:46:12
- **Comment**: preprint, we release our code publicly at https://github.com/LingFengGold/STReasoner
- **Topic Keywords**: rag
- **Reason**: This paper focuses on spatio-temporal reasoning in time series, which, while related to information retrieval, does not directly align with the user's core research themes of query understanding, ranking models, and user behavior modeling. Although it involves deep semantic understanding, the context is quite different from the user's typical areas of interest, such as e-commerce and recommender systems.

#### Abstract
> Spatio-temporal reasoning in time series involves the explicit synthesis of temporal dynamics, spatial dependencies, and textual context. This capability is vital for high-stakes decision-making in systems such as traffic networks, power grids, and disease propagation. However, the field remains underdeveloped because most existing works prioritize predictive accuracy over reasoning. To address the gap, we introduce ST-Bench, a benchmark consisting of four core tasks, including etiological reasoning, entity identification, correlation reasoning, and in-context forecasting, developed via a network SDE-based multi-agent data synthesis pipeline. We then propose STReasoner, which empowers LLM to integrate time series, graph structure, and text for explicit reasoning. To promote spatially grounded logic, we introduce S-GRPO, a reinforcement learning algorithm that rewards performance gains specifically attributable to spatial information. Experiments show that STReasoner achieves average accuracy gains between 17% and 135% at only 0.004X the cost of proprietary models and generalizes robustly to real-world data.

### 34. Limited Linguistic Diversity in Embodied AI Datasets

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Selma Wanna, Agnes Luhtaru, Jonathan Salfity, Ryan Barron, Juston Moore, Cynthia Matuszek, Mitch Pryor
- **URL**: <http://arxiv.org/abs/2601.03136v1>
- **Submitted**: 2026-01-06 16:06:47
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or user behavior modeling, which are core areas of your research interests. While it touches on Natural Language Processing, its focus on embodied AI datasets and linguistic diversity is not a central match for your research themes.

#### Abstract
> Language plays a critical role in Vision-Language-Action (VLA) models, yet the linguistic characteristics of the datasets used to train and evaluate these systems remain poorly documented. In this work, we present a systematic dataset audit of several widely used VLA corpora, aiming to characterize what kinds of instructions these datasets actually contain and how much linguistic variety they provide. We quantify instruction language along complementary dimensions-including lexical variety, duplication and overlap, semantic similarity, and syntactic complexity. Our analysis shows that many datasets rely on highly repetitive, template-like commands with limited structural variation, yielding a narrow distribution of instruction forms. We position these findings as descriptive documentation of the language signal available in current VLA training and evaluation data, intended to support more detailed dataset reporting, more principled dataset selection, and targeted curation or augmentation strategies that broaden language coverage.

### 35. ToxiGAN: Toxic Data Augmentation via LLM-Guided Directional Adversarial Generation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Peiran Li, Jan Fillies, Adrian Paschke
- **URL**: <http://arxiv.org/abs/2601.03121v1>
- **Submitted**: 2026-01-06 15:50:46
- **Comment**: This paper has been accepted to the main conference of EACL 2026
- **Topic Keywords**: rag
- **Reason**: This paper focuses on toxic language data augmentation using a GAN-based approach, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves text manipulation and large language models, the primary goal is to improve robustness in toxicity classification, which is not a central match to the user's interests.

#### Abstract
> Augmenting toxic language data in a controllable and class-specific manner is crucial for improving robustness in toxicity classification, yet remains challenging due to limited supervision and distributional skew. We propose ToxiGAN, a class-aware text augmentation framework that combines adversarial generation with semantic guidance from large language models (LLMs). To address common issues in GAN-based augmentation such as mode collapse and semantic drift, ToxiGAN introduces a two-step directional training strategy and leverages LLM-generated neutral texts as semantic ballast. Unlike prior work that treats LLMs as static generators, our approach dynamically selects neutral exemplars to provide balanced guidance. Toxic samples are explicitly optimized to diverge from these exemplars, reinforcing class-specific contrastive signals. Experiments on four hate speech benchmarks show that ToxiGAN achieves the strongest average performance in both macro-F1 and hate-F1, consistently outperforming traditional and LLM-based augmentation methods. Ablation and sensitivity analyses further confirm the benefits of semantic ballast and directional training in enhancing classifier robustness.

### 36. Learning to Diagnose and Correct Moral Errors: Towards Enhancing Moral Sensitivity in Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Bocheng Chen, Han Zi, Xi Chen, Xitong Zhang, Kristen Johnson, Guangliang Liu
- **URL**: <http://arxiv.org/abs/2601.03079v1>
- **Submitted**: 2026-01-06 15:09:05
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing, as it focuses on enhancing moral sensitivity in Large Language Models, which is a topic outside your primary areas of interest.

#### Abstract
> Moral sensitivity is fundamental to human moral competence, as it guides individuals in regulating everyday behavior. Although many approaches seek to align large language models (LLMs) with human moral values, how to enable them morally sensitive has been extremely challenging. In this paper, we take a step toward answering the question: how can we enhance moral sensitivity in LLMs? Specifically, we propose two pragmatic inference methods that faciliate LLMs to diagnose morally benign and hazardous input and correct moral errors, whereby enhancing LLMs' moral sensitivity. A central strength of our pragmatic inference methods is their unified perspective: instead of modeling moral discourses across semantically diverse and complex surface forms, they offer a principled perspective for designing pragmatic inference procedures grounded in their inferential loads. Empirical evidence demonstrates that our pragmatic methods can enhance moral sensitivity in LLMs and achieves strong performance on representative morality-relevant benchmarks.

### 37. Correct, Concise and Complete: Multi-stage Training For Adaptive Reasoning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Nathana√´l Carraz Rakotonirina, Ren Pang, Neha Anna John, Michael Bohlke-Schneider, Momchil Hardalov
- **URL**: <http://arxiv.org/abs/2601.02972v1>
- **Submitted**: 2026-01-06 12:31:51
- **Topic Keywords**: rag
- **Reason**: This paper focuses on improving the reasoning capabilities of large language models, specifically addressing the issue of overthinking. While it involves some aspects of query understanding and ranking models, the primary focus is on adaptive reasoning in NLP, which is somewhat related to the user's interests but not a central match.

#### Abstract
> The reasoning capabilities of large language models (LLMs) have improved substantially through increased test-time computation, typically in the form of intermediate tokens known as chain-of-thought (CoT). However, CoT often becomes unnecessarily long, increasing computation cost without actual accuracy gains or sometimes even degrading performance, a phenomenon known as ``overthinking''. We propose a multi-stage efficient reasoning method that combines supervised fine-tuning -- via rejection sampling or reasoning trace reformatting -- with reinforcement learning using an adaptive length penalty. We introduce a lightweight reward function that penalizes tokens generated after the first correct answer but encouraging self-verification only when beneficial. We conduct a holistic evaluation across seven diverse reasoning tasks, analyzing the accuracy-response length trade-off. Our approach reduces response length by an average of 28\% for 8B models and 40\% for 32B models, while incurring only minor performance drops of 1.6 and 2.5 points, respectively. Despite its conceptual simplicity, it achieves a superior trade-off compared to more complex state-of-the-art efficient reasoning methods, scoring 76.6, in terms of the area under the Overthinking-Adjusted Accuracy curve ($\text{AUC}_{\text{OAA}}$) -- 5 points above the base model and 2.5 points above the second-best approach.

### 38. Reliability-Aware Adaptive Self-Consistency for Efficient Sampling in LLM Reasoning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Junseok Kim, Nakyeong Yang, Kyungmin Min, Kyomin Jung
- **URL**: <http://arxiv.org/abs/2601.02970v1>
- **Submitted**: 2026-01-06 12:27:53
- **Comment**: 15 pages, 8 figures
- **Topic Keywords**: rag
- **Reason**: This paper focuses on improving the efficiency of Large Language Model (LLM) reasoning through adaptive self-consistency, but it does not appear to be directly related to Information Retrieval, Search technologies, or user behavior modeling, which are your primary research interests.

#### Abstract
> Self-Consistency improves reasoning reliability through multi-sample aggregation, but incurs substantial inference cost. Adaptive self-consistency methods mitigate this issue by adjusting the sampling budget; however, they rely on count-based stopping rules that treat all responses equally, often leading to unnecessary sampling. We propose Reliability-Aware Adaptive Self-Consistency (ReASC), which addresses this limitation by reframing adaptive sampling from response counting to evidence sufficiency, leveraging response-level confidence for principled information aggregation. ReASC operates in two stages: a single-sample decision stage that resolves instances confidently answerable from a single response, and a reliability-aware accumulation stage that aggregates responses by jointly leveraging their frequency and confidence. Across five models and four datasets, ReASC consistently achieves the best accuracy-cost trade-off compared to existing baselines, yielding improved inference efficiency across model scales from 3B to 27B parameters. As a concrete example, ReASC reduces inference cost by up to 70\% relative to self-consistency while preserving accuracy on GSM8K using Gemma-3-4B-it.

### 39. Training Language Models with homotokens Leads to Delayed Overfitting

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Adrian Cosma, Stefan Ruseti, Emilian Radoi, Mihai Dascalu
- **URL**: <http://arxiv.org/abs/2601.02867v1>
- **Submitted**: 2026-01-06 09:57:00
- **Comment**: 8 pages, 6 figures, 3 Appendices
- **Topic Keywords**: rag
- **Reason**: This paper focuses on language model training with homotokens, which is a subword tokenization technique. While it touches on deep semantic understanding, its primary contribution is in improving language model generalization and delaying overfitting, which is not directly related to information retrieval or search technologies. The paper's relevance to your research interests is limited.

#### Abstract
> Subword tokenization introduces a computational layer in language models where many distinct token sequences decode to the same surface form and preserve meaning, yet induce different internal computations. Despite this non-uniqueness, language models are typically trained using a single canonical longest-prefix tokenization. We formalize homotokens-alternative valid subword segmentations of the same lexical item-as a strictly meaning-preserving form of data augmentation. We introduce a lightweight training architecture that conditions canonical next-token prediction on sampled homotoken variants via an auxiliary causal encoder and block-causal cross-attention, without modifying the training objective or token interface. In data-constrained pretraining, homotoken augmentation consistently delays overfitting under repeated data exposure and improves generalization across diverse evaluation datasets. In multilingual fine-tuning, we find that the effectiveness of homotokens depends on tokenizer quality: gains are strongest when canonical tokens are highly compressed and diminish when the tokenizer already over-fragments the input. Overall, homotokens provide a simple and modular mechanism for inducing tokenization invariance in language models.

### 40. Punctuation-aware Hybrid Trainable Sparse Attention for Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Junxiang Qiu, Shuo Wang, Zhengsu Chen, Hengheng Zhang, Jinda Lu, Changcheng Li, Qi Tian
- **URL**: <http://arxiv.org/abs/2601.02819v1>
- **Submitted**: 2026-01-06 08:47:16
- **Topic Keywords**: rag
- **Reason**: This paper focuses on improving the performance of large language models through a novel attention mechanism, but it does not directly relate to information retrieval, query understanding, ranking models, or user behavior modeling, which are core areas of your research interests.

#### Abstract
> Attention serves as the fundamental mechanism for long-context modeling in large language models (LLMs), yet dense attention becomes structurally prohibitive for long sequences due to its quadratic complexity. Consequently, sparse attention has received increasing attention as a scalable alternative. However, existing sparse attention methods rely on coarse-grained semantic representations during block selection, which blur intra-block semantic boundaries and lead to the loss of critical information. To address this issue, we propose \textbf{P}unctuation-aware \textbf{H}ybrid \textbf{S}parse \textbf{A}ttention \textbf{(PHSA)}, a natively trainable sparse attention framework that leverages punctuation tokens as semantic boundary anchors. Specifically, (1) we design a dual-branch aggregation mechanism that fuses global semantic representations with punctuation-enhanced boundary features, preserving the core semantic structure while introducing almost no additional computational overhead; (2) we introduce an extreme-sparsity-adaptive training and inference strategy that stabilizes model behavior under very low token activation ratios; Extensive experiments on general benchmarks and long-context evaluations demonstrate that PHSA consistently outperforms dense attention and state-of-the-art sparse attention baselines, including InfLLM v2. Specifically, for the 0.6B-parameter model with 32k-token input sequences, PHSA can reduce the information loss by 10.8\% at a sparsity ratio of 97.3\%.

### 41. Automated Semantic Rules Detection (ASRD) for Emergent Communication Interpretation

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Bastien Vanderplaetse, Xavier Siebert, St√©phane Dupont
- **URL**: <http://arxiv.org/abs/2601.03254v1>
- **Submitted**: 2026-01-06 18:57:39
- **Topic Keywords**: search
- **Reason**: This paper focuses on emergent communication in multi-agent systems, proposing an algorithm for detecting semantic rules. While it touches on the interpretation of communication, it doesn't directly relate to information retrieval, search technologies, or user behavior modeling, which are core areas of your research interests.

#### Abstract
> The field of emergent communication within multi-agent systems examines how autonomous agents can independently develop communication strategies, without explicit programming, and adapt them to varied environments. However, few studies have focused on the interpretability of emergent languages. The research exposed in this paper proposes an Automated Semantic Rules Detection (ASRD) algorithm, which extracts relevant patterns in messages exchanged by agents trained with two different datasets on the Lewis Game, which is often studied in the context of emergent communication. ASRD helps at the interpretation of the emergent communication by relating the extracted patterns to specific attributes of the input data, thereby considerably simplifying subsequent analysis.

### 42. NorwAI's Large Language Models: Technical Report

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Jon Atle Gulla, Peng Liu, Lemei Zhang
- **URL**: <http://arxiv.org/abs/2601.03034v1>
- **Submitted**: 2026-01-06 14:06:55
- **Topic Keywords**: search
- **Reason**: This paper is primarily focused on developing large language models for Norwegian and Scandinavian languages, which is a topic in Natural Language Processing (NLP). While it touches on model architectures and training data, it does not appear to be directly related to Information Retrieval (IR), query understanding, ranking models, or user behavior modeling, which are the core areas of your research interests.

#### Abstract
> Norwegian, spoken by approximately five million people, remains underrepresented in many of the most significant breakthroughs in Natural Language Processing (NLP). To address this gap, the NorLLM team at NorwAI has developed a family of models specifically tailored to Norwegian and other Scandinavian languages, building on diverse Transformer-based architectures such as GPT, Mistral, Llama2, Mixtral and Magistral. These models are either pretrained from scratch or continually pretrained on 25B - 88.45B tokens, using a Norwegian-extended tokenizer and advanced post-training strategies to optimize performance, enhance robustness, and improve adaptability across various real-world tasks. Notably, instruction-tuned variants (e.g., Mistral-7B-Instruct and Mixtral-8x7B-Instruct) showcase strong assistant-style capabilities, underscoring their potential for practical deployment in interactive and domain-specific applications. The NorwAI large language models are openly available to Nordic organizations, companies and students for both research and experimental use. This report provides detailed documentation of the model architectures, training data, tokenizer design, fine-tuning strategies, deployment, and evaluations.

### 43. MiMo-V2-Flash Technical Report

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Bangjun Xiao, Bingquan Xia, Bo Yang, Bofei Gao, Bowen Shen, Chen Zhang, Chenhong He, Chiheng Lou, Fuli Luo, Gang Wang, Gang Xie, Hailin Zhang, Hanglong Lv, Hanyu Li, Heyu Chen, Hongshen Xu, Houbin Zhang, Huaqiu Liu, Jiangshan Duo, Jianyu Wei, Jiebao Xiao, Jinhao Dong, Jun Shi, Junhao Hu, Kainan Bao, Kang Zhou, Lei Li, Liang Zhao, Linghao Zhang, Peidian Li, Qianli Chen, Shaohui Liu, Shihua Yu, Shijie Cao, Shimao Chen, Shouqiu Yu, Shuo Liu, Tianling Zhou, Weijiang Su, Weikun Wang, Wenhan Ma, Xiangwei Deng, Bohan Mao, Bowen Ye, Can Cai, Chenghua Wang, Chengxuan Zhu, Chong Ma, Chun Chen, Chunan Li, Dawei Zhu, Deshan Xiao, Dong Zhang, Duo Zhang, Fangyue Liu, Feiyu Yang, Fengyuan Shi, Guoan Wang, Hao Tian, Hao Wu, Heng Qu, Hongfei Yi, Hongxu An, Hongyi Guan, Xing Zhang, Yifan Song, Yihan Yan, Yihao Zhao, Yingchun Lai, Yizhao Gao, Yu Cheng, Yuanyuan Tian, Yudong Wang, Zhen Tang, Zhengju Tang, Zhengtao Wen, Zhichao Song, Zhixian Zheng, Zihan Jiang, Jian Wen, Jiarui Sun, Jiawei Li, Jinlong Xue, Jun Xia, Kai Fang, Menghang Zhu, Nuo Chen, Qian Tu, Qihao Zhang, Qiying Wang, Rang Li, Rui Ma, Shaolei Zhang, Shengfan Wang, Shicheng Li, Shuhao Gu, Shuhuai Ren, Sirui Deng, Tao Guo, Tianyang Lu, Weiji Zhuang, Weikang Zhang, Weimin Xiong, Wenshan Huang, Wenyu Yang, Xin Zhang, Xing Yong, Xu Wang, Xueyang Xie, Yilin Jiang, Yixin Yang, Yongzhe He, Yu Tu, Yuanliang Dong, Yuchen Liu, Yue Ma, Yue Yu, Yuxing Xiang, Zhaojun Huang, Zhenru Lin, Zhipeng Xu, Zhiyang Chen, Zhonghua Deng, Zihan Zhang, Zihao Yue
- **URL**: <http://arxiv.org/abs/2601.02780v1>
- **Submitted**: 2026-01-06 07:31:47
- **Comment**: 31 pages, technical report
- **Topic Keywords**: search
- **Reason**: This paper appears to be about a novel deep learning model, MiMo-V2-Flash, designed for fast reasoning and agentic capabilities. While it involves large-scale pre-training and attention mechanisms, there's no clear connection to information retrieval, search technologies, or user behavior modeling, which are the user's primary research interests.

#### Abstract
> We present MiMo-V2-Flash, a Mixture-of-Experts (MoE) model with 309B total parameters and 15B active parameters, designed for fast, strong reasoning and agentic capabilities. MiMo-V2-Flash adopts a hybrid attention architecture that interleaves Sliding Window Attention (SWA) with global attention, with a 128-token sliding window under a 5:1 hybrid ratio. The model is pre-trained on 27 trillion tokens with Multi-Token Prediction (MTP), employing a native 32k context length and subsequently extended to 256k. To efficiently scale post-training compute, MiMo-V2-Flash introduces a novel Multi-Teacher On-Policy Distillation (MOPD) paradigm. In this framework, domain-specialized teachers (e.g., trained via large-scale reinforcement learning) provide dense and token-level reward, enabling the student model to perfectly master teacher expertise. MiMo-V2-Flash rivals top-tier open-weight models such as DeepSeek-V3.2 and Kimi-K2, despite using only 1/2 and 1/3 of their total parameters, respectively. During inference, by repurposing MTP as a draft model for speculative decoding, MiMo-V2-Flash achieves up to 3.6 acceptance length and 2.6x decoding speedup with three MTP layers. We open-source both the model weights and the three-layer MTP weights to foster open research and community collaboration.

### 44. Low-Resource Heuristics for Bahnaric Optical Character Recognition Improvement

- **LLM Score**: 0
- **Keyword Score**: 6
- **Authors**: Phat Tran, Phuoc Pham, Hung Trinh, Tho Quan
- **URL**: <http://arxiv.org/abs/2601.02965v1>
- **Submitted**: 2026-01-06 12:22:03
- **Topic Keywords**: information retrieval, retrieval, search
- **Reason**: This paper focuses on Optical Character Recognition (OCR) for a minority language, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> Bahnar, a minority language spoken across Vietnam, Cambodia, and Laos, faces significant preservation challenges due to limited research and data availability. This study addresses the critical need for accurate digitization of Bahnar language documents through optical character recognition (OCR) technology. Digitizing scanned paper documents poses significant challenges, as degraded image quality from broken or blurred areas introduces considerable OCR errors that compromise information retrieval systems. We propose a comprehensive approach combining advanced table and non-table detection techniques with probability-based post-processing heuristics to enhance recognition accuracy. Our method first applies detection algorithms to improve input data quality, then employs probabilistic error correction on OCR output. Experimental results indicate a substantial improvement, with recognition accuracy increasing from 72.86% to 79.26%. This work contributes valuable resources for Bahnar language preservation and provides a framework applicable to other minority language digitization efforts.

---

