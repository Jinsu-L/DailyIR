[
    {
        "title": "HeteroRAG: A Heterogeneous Retrieval-Augmented Generation Framework for Medical Vision Language Tasks",
        "abstract": "Medical large vision-language Models (Med-LVLMs) have shown promise in\nclinical applications but suffer from factual inaccuracies and unreliable\noutputs, posing risks in real-world diagnostics. While retrieval-augmented\ngeneration has emerged as a potential solution, current medical multimodal RAG\nsystems are unable to perform effective retrieval across heterogeneous sources.\nThe irrelevance of retrieved reports affects the factuality of analysis, while\ninsufficient knowledge affects the credibility of clinical decision-making. To\nbridge the gap, we construct MedAtlas, which includes extensive multimodal\nreport repositories and diverse text corpora. Based on it, we present\nHeteroRAG, a novel framework that enhances Med-LVLMs through heterogeneous\nknowledge sources. The framework introduces Modality-specific CLIPs for\neffective report retrieval and a Multi-corpora Query Generator for dynamically\nconstructing queries for diverse corpora. Incorporating knowledge from such\nmultifaceted sources, Med-LVLM is then trained with Heterogeneous Knowledge\nPreference Tuning to achieve cross-modality and multi-source knowledge\nalignment. Extensive experiments across 12 datasets and 3 modalities\ndemonstrate that the proposed HeteroRAG achieves state-of-the-art performance\nin most medical vision language benchmarks, significantly improving factual\naccuracy and reliability of Med-LVLMs.",
        "url": "http://arxiv.org/abs/2508.12778v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12778v1",
        "arxiv_id": "2508.12778v1",
        "authors": [
            "Zhe Chen",
            "Yusheng Liao",
            "Shuyang Jiang",
            "Zhiyuan Zhu",
            "Haolin Li",
            "Yanfeng Wang",
            "Yu Wang"
        ],
        "submitted": "2025-08-18 09:54:10",
        "source": "arxiv",
        "comment": null,
        "score": 13,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on medical vision-language tasks, retrieval-augmented generation, and multimodal report repositories, which are not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on medical applications and clinical decision-making also falls outside the user's e-commerce domain experience."
    },
    {
        "title": "OptimalThinkingBench: Evaluating Over and Underthinking in LLMs",
        "abstract": "Thinking LLMs solve complex tasks at the expense of increased compute and\noverthinking on simpler problems, while non-thinking LLMs are faster and\ncheaper but underthink on harder reasoning problems. This has led to the\ndevelopment of separate thinking and non-thinking LLM variants, leaving the\nonus of selecting the optimal model for each query on the end user. In this\nwork, we introduce OptimalThinkingBench, a unified benchmark that jointly\nevaluates overthinking and underthinking in LLMs and also encourages the\ndevelopment of optimally-thinking models that balance performance and\nefficiency. Our benchmark comprises two sub-benchmarks: OverthinkingBench,\nfeaturing simple queries in 72 domains, and UnderthinkingBench, containing 11\nchallenging reasoning tasks. Using novel thinking-adjusted accuracy metrics, we\nperform extensive evaluation of 33 different thinking and non-thinking models\nand show that no model is able to optimally think on our benchmark. Thinking\nmodels often overthink for hundreds of tokens on the simplest user queries\nwithout improving performance. In contrast, large non-thinking models\nunderthink, often falling short of much smaller thinking models. We further\nexplore several methods to encourage optimal thinking, but find that these\napproaches often improve on one sub-benchmark at the expense of the other,\nhighlighting the need for better unified and optimal models in the future.",
        "url": "http://arxiv.org/abs/2508.13141v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13141v1",
        "arxiv_id": "2508.13141v1",
        "authors": [
            "Pranjal Aggarwal",
            "Seungone Kim",
            "Jack Lanchantin",
            "Sean Welleck",
            "Jason Weston",
            "Ilia Kulikov",
            "Swarnadeep Saha"
        ],
        "submitted": "2025-08-18 17:53:10",
        "source": "arxiv",
        "comment": "26 pages, 6 tables, 10 figures",
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Large Language Models (LLMs) and their thinking/overthinking behavior, which is not directly related to Information Retrieval, Search technologies, or query understanding. The concepts and techniques discussed in the paper are not relevant to my research interests."
    },
    {
        "title": "All for law and law for all: Adaptive RAG Pipeline for Legal Research",
        "abstract": "Retrieval-Augmented Generation (RAG) mitigates hallucinations by grounding\nlarge language model outputs in cited sources, a capability that is especially\ncritical in the legal domain. We present an end-to-end RAG pipeline that\nrevisits and extends the LegalBenchRAG baseline with three targeted\nenhancements: (i) a context-aware query translator that disentangles document\nreferences from natural-language questions and adapts retrieval depth and\nresponse style based on expertise and specificity, (ii) open-source retrieval\nstrategies using SBERT and GTE embeddings that achieve substantial performance\ngains (improving Recall@K by 30-95\\% and Precision@K by $\\sim$2.5$\\times$ for\n$K>4$) while remaining cost-efficient, and (iii) a comprehensive evaluation and\ngeneration framework that combines RAGAS, BERTScore-F1, and ROUGE-Recall to\nassess semantic alignment and faithfulness across models and prompt designs.\nOur results show that carefully designed open-source pipelines can rival or\noutperform proprietary approaches in retrieval quality, while a custom\nlegal-grounded prompt consistently produces more faithful and contextually\nrelevant answers than baseline prompting. Taken together, these contributions\ndemonstrate the potential of task-aware, component-level tuning to deliver\nlegally grounded, reproducible, and cost-effective RAG systems for legal\nresearch assistance.",
        "url": "http://arxiv.org/abs/2508.13107v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13107v1",
        "arxiv_id": "2508.13107v1",
        "authors": [
            "Figarri Keisha",
            "Prince Singh",
            "Pallavi",
            "Dion Fernandes",
            "Aravindh Manivannan",
            "Ilham Wicaksono",
            "Faisal Ahmad"
        ],
        "submitted": "2025-08-18 17:14:03",
        "source": "arxiv",
        "comment": "submitted to NLLP 2025 Workshop",
        "score": 8,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 7,
        "llm_reason": "The paper explores Retrieval-Augmented Generation (RAG) in the legal domain, which is related to query understanding and ranking models. The context-aware query translator and open-source retrieval strategies are also relevant to information retrieval. However, the focus on legal research assistance and the use of legal-grounded prompts are not directly aligned with the user's primary interests in e-commerce and real-time relevance optimization."
    },
    {
        "title": "AutoBnB-RAG: Enhancing Multi-Agent Incident Response with Retrieval-Augmented Generation",
        "abstract": "Incident response (IR) requires fast, coordinated, and well-informed\ndecision-making to contain and mitigate cyber threats. While large language\nmodels (LLMs) have shown promise as autonomous agents in simulated IR settings,\ntheir reasoning is often limited by a lack of access to external knowledge. In\nthis work, we present AutoBnB-RAG, an extension of the AutoBnB framework that\nincorporates retrieval-augmented generation (RAG) into multi-agent incident\nresponse simulations. Built on the Backdoors & Breaches (B&B) tabletop game\nenvironment, AutoBnB-RAG enables agents to issue retrieval queries and\nincorporate external evidence during collaborative investigations. We introduce\ntwo retrieval settings: one grounded in curated technical documentation\n(RAG-Wiki), and another using narrative-style incident reports (RAG-News). We\nevaluate performance across eight team structures, including newly introduced\nargumentative configurations designed to promote critical reasoning. To\nvalidate practical utility, we also simulate real-world cyber incidents based\non public breach reports, demonstrating AutoBnB-RAG's ability to reconstruct\ncomplex multi-stage attacks. Our results show that retrieval augmentation\nimproves decision quality and success rates across diverse organizational\nmodels. This work demonstrates the value of integrating retrieval mechanisms\ninto LLM-based multi-agent systems for cybersecurity decision-making.",
        "url": "http://arxiv.org/abs/2508.13118v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13118v1",
        "arxiv_id": "2508.13118v1",
        "authors": [
            "Zefang Liu",
            "Arman Anwar"
        ],
        "submitted": "2025-08-18 17:22:51",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to information retrieval, as it discusses retrieval-augmented generation in the context of multi-agent incident response. However, the focus is on cybersecurity and decision-making, which is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is limited to the retrieval aspect, which is only a small part of the overall research."
    },
    {
        "title": "CRED-SQL: Enhancing Real-world Large Scale Database Text-to-SQL Parsing through Cluster Retrieval and Execution Description",
        "abstract": "Recent advances in large language models (LLMs) have significantly improved\nthe accuracy of Text-to-SQL systems. However, a critical challenge remains: the\nsemantic mismatch between natural language questions (NLQs) and their\ncorresponding SQL queries. This issue is exacerbated in large-scale databases,\nwhere semantically similar attributes hinder schema linking and semantic drift\nduring SQL generation, ultimately reducing model accuracy. To address these\nchallenges, we introduce CRED-SQL, a framework designed for large-scale\ndatabases that integrates Cluster Retrieval and Execution Description. CRED-SQL\nfirst performs cluster-based large-scale schema retrieval to pinpoint the\ntables and columns most relevant to a given NLQ, alleviating schema mismatch.\nIt then introduces an intermediate natural language representation-Execution\nDescription Language (EDL)-to bridge the gap between NLQs and SQL. This\nreformulation decomposes the task into two stages: Text-to-EDL and EDL-to-SQL,\nleveraging LLMs' strong general reasoning capabilities while reducing semantic\ndeviation. Extensive experiments on two large-scale, cross-domain\nbenchmarks-SpiderUnion and BirdUnion-demonstrate that CRED-SQL achieves new\nstate-of-the-art (SOTA) performance, validating its effectiveness and\nscalability. Our code is available at https://github.com/smduan/CRED-SQL.git",
        "url": "http://arxiv.org/abs/2508.12769v2",
        "pdf_url": "http://arxiv.org/pdf/2508.12769v2",
        "arxiv_id": "2508.12769v2",
        "authors": [
            "Shaoming Duan",
            "Zirui Wang",
            "Chuanyi Liu",
            "Zhibin Zhu",
            "Yuhao Zhang",
            "Peiyi Han",
            "Liang Yan",
            "Zewu Penge"
        ],
        "submitted": "2025-08-18 09:43:07",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Text-to-SQL parsing, which is not directly related to Information Retrieval (IR) or Search technologies. Although it mentions large language models, the primary goal is to improve SQL generation, which is outside the scope of the user's research interests."
    },
    {
        "title": "Is This News Still Interesting to You?: Lifetime-aware Interest Matching for News Recommendation",
        "abstract": "Personalized news recommendation aims to deliver news articles aligned with\nusers' interests, serving as a key solution to alleviate the problem of\ninformation overload on online news platforms. While prior work has improved\ninterest matching through refined representations of news and users, the\nfollowing time-related challenges remain underexplored: (C1) leveraging the age\nof clicked news to infer users' interest persistence, and (C2) modeling the\nvarying lifetime of news across topics and users. To jointly address these\nchallenges, we propose a novel Lifetime-aware Interest Matching framework for\nnEws recommendation, named LIME, which incorporates three key strategies: (1)\nUser-Topic lifetime-aware age representation to capture the relative age of\nnews with respect to a user-topic pair, (2) Candidate-aware lifetime attention\nfor generating temporally aligned user representation, and (3) Freshness-guided\ninterest refinement for prioritizing valid candidate news at prediction time.\nExtensive experiments on two real-world datasets demonstrate that LIME\nconsistently outperforms a wide range of state-of-the-art news recommendation\nmethods, and its model agnostic strategies significantly improve recommendation\naccuracy.",
        "url": "http://arxiv.org/abs/2508.13064v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13064v1",
        "arxiv_id": "2508.13064v1",
        "authors": [
            "Seongeun Ryu",
            "Yunyong Ko",
            "Sang-Wook Kim"
        ],
        "submitted": "2025-08-18 16:36:27",
        "source": "arxiv",
        "comment": "10 pages, 7 figures, 4 tables, accepted at ACM International\n  Conference on Information and Knowledge Management (CIKM)",
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'click' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper focuses on news recommendation, which is related to information retrieval and search technologies. The use of lifetime-aware interest matching and attention mechanisms is somewhat relevant to query understanding and ranking models. However, the paper's primary focus on news recommendation and its emphasis on freshness and lifetime of news articles do not directly align with the user's interests in deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "D-RDW: Diversity-Driven Random Walks for News Recommender Systems",
        "abstract": "This paper introduces Diversity-Driven RandomWalks (D-RDW), a lightweight\nalgorithm and re-ranking technique that generates diverse news recommendations.\nD-RDW is a societal recommender, which combines the diversification\ncapabilities of the traditional random walk algorithms with customizable target\ndistributions of news article properties. In doing so, our model provides a\ntransparent approach for editors to incorporate norms and values into the\nrecommendation process. D-RDW shows enhanced performance across key diversity\nmetrics that consider the articles' sentiment and political party mentions when\ncompared to state-of-the-art neural models. Furthermore, D-RDW proves to be\nmore computationally efficient than existing approaches.",
        "url": "http://arxiv.org/abs/2508.13035v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13035v1",
        "arxiv_id": "2508.13035v1",
        "authors": [
            "Runze Li",
            "Lucien Heitz",
            "Oana Inel",
            "Abraham Bernstein"
        ],
        "submitted": "2025-08-18 15:53:30",
        "source": "arxiv",
        "comment": "6 pages",
        "score": 5,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on recommender systems, specifically news recommender systems, which is somewhat related to the user's interests in information retrieval and search technologies. However, the emphasis on diversity-driven random walks and societal recommender systems is not directly aligned with the user's primary focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Informfully Recommenders -- Reproducibility Framework for Diversity-aware Intra-session Recommendations",
        "abstract": "Norm-aware recommender systems have gained increased attention, especially\nfor diversity optimization. The recommender systems community has\nwell-established experimentation pipelines that support reproducible\nevaluations by facilitating models' benchmarking and comparisons against\nstate-of-the-art methods. However, to the best of our knowledge, there is\ncurrently no reproducibility framework to support thorough norm-driven\nexperimentation at the pre-processing, in-processing, post-processing, and\nevaluation stages of the recommender pipeline. To address this gap, we present\nInformfully Recommenders, a first step towards a normative reproducibility\nframework that focuses on diversity-aware design built on Cornac. Our extension\nprovides an end-to-end solution for implementing and experimenting with\nnormative and general-purpose diverse recommender systems that cover 1) dataset\npre-processing, 2) diversity-optimized models, 3) dedicated intrasession item\nre-ranking, and 4) an extensive set of diversity metrics. We demonstrate the\ncapabilities of our extension through an extensive offline experiment in the\nnews domain.",
        "url": "http://arxiv.org/abs/2508.13019v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13019v1",
        "arxiv_id": "2508.13019v1",
        "authors": [
            "Lucien Heitz",
            "Runze Li",
            "Oana Inel",
            "Abraham Bernstein"
        ],
        "submitted": "2025-08-18 15:37:41",
        "source": "arxiv",
        "comment": "10 pages",
        "score": 5,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on recommender systems, which is somewhat related to the user's interests in search technologies and query understanding. However, the emphasis on diversity-aware intra-session recommendations and normative reproducibility framework is not directly aligned with the user's primary focus on information retrieval, especially in areas that require deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic Thought Reward",
        "abstract": "Large language models (LLMs) exhibit remarkable problem-solving abilities,\nbut struggle with complex tasks due to static internal knowledge.\nRetrieval-Augmented Generation (RAG) enhances access to external information,\nyet remains limited in multi-hop reasoning and strategic search due to rigid\nworkflows. Recent advancements in agentic deep research empower LLMs to\nautonomously reason, search, and synthesize information. However, current\napproaches relying on outcome-based reinforcement learning (RL) face critical\nissues such as conflicting gradients and reward sparsity, limiting performance\ngains and training efficiency. To address these, we first propose Atomic\nThought, a novel LLM thinking paradigm that decomposes reasoning into\nfine-grained functional units. These units are supervised by Reasoning Reward\nModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained\nguidance. Building on this, we propose Atom-Searcher, a novel RL framework for\nagentic deep research that integrates Atomic Thought and ATR. Atom-Searcher\nuses a curriculum-inspired reward schedule, prioritizing process-level ATR\nearly and transitioning to outcome rewards, accelerating convergence on\neffective reasoning paths. Experiments on seven benchmarks show consistent\nimprovements over the state-of-the-art. Key advantages include: (1)\nAtom-Searcher scales computation at test-time. (2) Atomic Thought provides\nsupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)\nAtom-Searcher exhibits more interpretable, human-like reasoning patterns.",
        "url": "http://arxiv.org/abs/2508.12800v2",
        "pdf_url": "http://arxiv.org/pdf/2508.12800v2",
        "arxiv_id": "2508.12800v2",
        "authors": [
            "Yong Deng",
            "Guoqing Wang",
            "Zhenzhe Ying",
            "Xiaofeng Wu",
            "Jinzhen Lin",
            "Wenwen Xiong",
            "Yuqin Dai",
            "Shuo Yang",
            "Zhanwei Zhang",
            "Qiwen Wang",
            "Yang Qin",
            "Changhua Meng"
        ],
        "submitted": "2025-08-18 10:23:10",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a novel reinforcement learning framework for agentic deep research, which is not directly related to information retrieval or search technologies. While it mentions language models and retrieval-augmented generation, the focus is on enhancing problem-solving abilities and multi-hop reasoning, rather than query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Beyond GPT-5: Making LLMs Cheaper and Better via Performance-Efficiency Optimized Routing",
        "abstract": "Balancing performance and efficiency is a central challenge in large language\nmodel (LLM) advancement. GPT-5 addresses this with test-time routing,\ndynamically assigning queries to either an efficient or a high-capacity model\nduring inference. In this work, we present Avengers-Pro, a test-time routing\nframework that ensembles LLMs of varying capacities and efficiencies, providing\na unified solution for all performance-efficiency tradeoffs. The Avengers-Pro\nembeds and clusters incoming queries, then routes each to the most suitable\nmodel based on a performance-efficiency score. Across 6 challenging benchmarks\nand 8 leading models -- including GPT-5-medium, Gemini-2.5-pro, and\nClaude-opus-4.1 -- Avengers-Pro achieves state-of-the-art results: by varying a\nperformance-efficiency trade-off parameter, it can surpass the strongest single\nmodel (GPT-5-medium) by +7% in average accuracy. Moreover, it can match the\naverage accuracy of the strongest single model at 27% lower cost, and reach\n~90% of that performance at 63% lower cost. Last but not least, it achieves a\nPareto frontier, consistently yielding the highest accuracy for any given cost,\nand the lowest cost for any given accuracy, among all single models. Code is\navailable at https://github.com/ZhangYiqun018/AvengersPro.",
        "url": "http://arxiv.org/abs/2508.12631v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12631v1",
        "arxiv_id": "2508.12631v1",
        "authors": [
            "Yiqun Zhang",
            "Hao Li",
            "Jianhao Chen",
            "Hangfan Zhang",
            "Peng Ye",
            "Lei Bai",
            "Shuyue Hu"
        ],
        "submitted": "2025-08-18 05:23:31",
        "source": "arxiv",
        "comment": "Ongoing work",
        "score": 5,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on large language models and test-time routing, which is not directly related to information retrieval, query understanding, ranking models, or user behavior modeling. While it mentions performance-efficiency trade-offs, the context is different from the user's interests in IR and NLP."
    },
    {
        "title": "WebMall -- A Multi-Shop Benchmark for Evaluating Web Agents",
        "abstract": "LLM-based web agents have the potential to automate long-running web tasks,\nsuch as finding offers for specific products in multiple online shops and\nsubsequently ordering the cheapest products that meet the users needs. This\npaper introduces WebMall, a multi-shop online shopping benchmark for evaluating\nthe effectiveness and efficiency of web agents for comparison-shopping. WebMall\nconsists of four simulated online shops populated with authentic product offers\nsourced from the Common Crawl, alongside a suite of 91 cross-shop tasks. These\ntasks include basic tasks such as finding specific products in multiple shops,\nperforming price comparisons, adding items to the shopping cart, and completing\ncheckout. Advanced tasks involve searching for products based on vague\nrequirements, identifying suitable substitutes, and finding compatible\nproducts. Compared to existing e-commerce benchmarks, such as WebShop or\nShoppingBench, WebMall introduces comparison-shopping tasks across multiple\nshops. Furthermore, the product offers are more heterogeneous, as they\noriginate from hundreds of distinct real-world shops. The tasks in WebMall\nrequire longer interaction trajectories than those in WebShop, while remaining\nrepresentative of real-world shopping behaviors. We evaluate eight baseline\nagents on WebMall, varying in observation modality, memory utilization, and\nunderlying large language model (GPT 4.1 and Claude Sonnet 4). The\nbest-performing configurations achieve completion rates of 75% and 53%, and F1\nscores of 87% and 63%, on the basic and advanced task sets, respectively.\nWebMall is publicly released to facilitate research on web agents and to\npromote advancements in navigation, reasoning, and efficiency within e-commerce\nscenarios.",
        "url": "http://arxiv.org/abs/2508.13024v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13024v1",
        "arxiv_id": "2508.13024v1",
        "authors": [
            "Ralph Peeters",
            "Aaron Steiner",
            "Luca Schwarz",
            "Julian Yuya Caspary",
            "Christian Bizer"
        ],
        "submitted": "2025-08-18 15:41:22",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'shopping' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper introduces a benchmark for evaluating web agents in e-commerce scenarios, focusing on comparison-shopping tasks across multiple shops. While it touches on topics related to information retrieval and search technologies, the primary focus is on evaluating web agents' effectiveness and efficiency, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest for you."
    },
    {
        "title": "From SALAMANDRA to SALAMANDRATA: BSC Submission for WMT25 General Machine Translation Shared Task",
        "abstract": "In this paper, we present the SALAMANDRATA family of models, an improved\niteration of SALAMANDRA LLMs (Gonzalez-Agirre et al., 2025) specifically\ntrained to achieve strong performance in translation-related tasks for 38\nEuropean languages. SALAMANDRATA comes in two scales: 2B and 7B parameters. For\nboth versions, we applied the same training recipe with a first step of\ncontinual pre-training on parallel data, and a second step of supervised\nfine-tuning on high-quality instructions. The BSC submission to the WMT25\nGeneral Machine Translation shared task is based on the 7B variant of\nSALAMANDRATA. We first adapted the model vocabulary to support the additional\nnon-European languages included in the task. This was followed by a second\nphase of continual pre-training and supervised fine-tuning, carefully designed\nto optimize performance across all translation directions for this year's\nshared task. For decoding, we employed two quality-aware strategies: Minimum\nBayes Risk Decoding and Tuned Re-ranking using COMET and COMET-KIWI\nrespectively. We publicly release both the 2B and 7B versions of SALAMANDRATA,\nalong with the newer SALAMANDRATA-V2 model, on Hugging Face1",
        "url": "http://arxiv.org/abs/2508.12774v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12774v1",
        "arxiv_id": "2508.12774v1",
        "authors": [
            "Javier Garcia Gilabert",
            "Xixian Liao",
            "Severino Da Dalt",
            "Ella Bohman",
            "Audrey Mash",
            "Francesca De Luca Fornaciari",
            "Irene Baucells",
            "Joan Llop",
            "Miguel Claramunt Argote",
            "Carlos Escolano",
            "Maite Melero"
        ],
        "submitted": "2025-08-18 09:48:35",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on machine translation, a topic outside the user's primary research interests in Information Retrieval and Search technologies. Although it mentions some related concepts like fine-tuning and re-ranking, the context is not relevant to the user's areas of expertise."
    },
    {
        "title": "Leveraging Large Language Models for Predictive Analysis of Human Misery",
        "abstract": "This study investigates the use of Large Language Models (LLMs) for\npredicting human-perceived misery scores from natural language descriptions of\nreal-world scenarios. The task is framed as a regression problem, where the\nmodel assigns a scalar value from 0 to 100 to each input statement. We evaluate\nmultiple prompting strategies, including zero-shot, fixed-context few-shot, and\nretrieval-based prompting using BERT sentence embeddings. Few-shot approaches\nconsistently outperform zero-shot baselines, underscoring the value of\ncontextual examples in affective prediction. To move beyond static evaluation,\nwe introduce the \"Misery Game Show\", a novel gamified framework inspired by a\ntelevision format. It tests LLMs through structured rounds involving ordinal\ncomparison, binary classification, scalar estimation, and feedback-driven\nreasoning. This setup enables us to assess not only predictive accuracy but\nalso the model's ability to adapt based on corrective feedback. The gamified\nevaluation highlights the broader potential of LLMs in dynamic emotional\nreasoning tasks beyond standard regression. Code and data link:\nhttps://github.com/abhi1nandy2/Misery_Data_Exps_GitHub",
        "url": "http://arxiv.org/abs/2508.12669v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12669v1",
        "arxiv_id": "2508.12669v1",
        "authors": [
            "Bishanka Seal",
            "Rahul Seetharaman",
            "Aman Bansal",
            "Abhilash Nandy"
        ],
        "submitted": "2025-08-18 07:02:59",
        "source": "arxiv",
        "comment": "14 pages, 4 tables",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on using Large Language Models for predicting human-perceived misery scores, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it involves natural language processing, the application is in a different domain and does not align with the user's primary research interests."
    },
    {
        "title": "Semantic Anchoring in Agentic Memory: Leveraging Linguistic Structures for Persistent Conversational Context",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive fluency and task\ncompetence in conversational settings. However, their effectiveness in\nmulti-session and long-term interactions is hindered by limited memory\npersistence. Typical retrieval-augmented generation (RAG) systems store\ndialogue history as dense vectors, which capture semantic similarity but\nneglect finer linguistic structures such as syntactic dependencies, discourse\nrelations, and coreference links. We propose Semantic Anchoring, a hybrid\nagentic memory architecture that enriches vector-based storage with explicit\nlinguistic cues to improve recall of nuanced, context-rich exchanges. Our\napproach combines dependency parsing, discourse relation tagging, and\ncoreference resolution to create structured memory entries. Experiments on\nadapted long-term dialogue datasets show that semantic anchoring improves\nfactual recall and discourse coherence by up to 18% over strong RAG baselines.\nWe further conduct ablation studies, human evaluations, and error analysis to\nassess robustness and interpretability.",
        "url": "http://arxiv.org/abs/2508.12630v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12630v1",
        "arxiv_id": "2508.12630v1",
        "authors": [
            "Maitreyi Chatterjee",
            "Devansh Agarwal"
        ],
        "submitted": "2025-08-18 05:14:48",
        "source": "arxiv",
        "comment": "Paper is currently in peer review",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on conversational AI and memory persistence, using techniques like dependency parsing, discourse relation tagging, and coreference resolution. While it's related to NLP, it doesn't directly address query understanding, ranking models, or user behavior modeling in the context of information retrieval, which are the user's primary research interests."
    },
    {
        "title": "Signal and Noise: A Framework for Reducing Uncertainty in Language Model Evaluation",
        "abstract": "Developing large language models is expensive and involves making decisions\nwith small experiments, typically by evaluating on large, multi-task evaluation\nsuites. In this work, we analyze specific properties which make a benchmark\nmore reliable for such decisions, and interventions to design higher-quality\nevaluation benchmarks. We introduce two key metrics that show differences in\ncurrent benchmarks: signal, a benchmark's ability to separate better models\nfrom worse models, and noise, a benchmark's sensitivity to random variability\nbetween training steps. We demonstrate that benchmarks with a better\nsignal-to-noise ratio are more reliable when making decisions at small scale,\nand those with less noise have lower scaling law prediction error. These\nresults suggest that improving signal or noise will lead to more useful\nbenchmarks, so we introduce three interventions designed to directly affect\nsignal or noise. For example, we propose that switching to a metric that has\nbetter signal and noise (e.g., perplexity rather than accuracy) leads to better\nreliability and improved scaling law error. We also find that filtering noisy\nsubtasks, to improve an aggregate signal-to-noise ratio, leads to more reliable\nmulti-task evaluations. We also find that averaging the output of a model's\nintermediate checkpoints to reduce noise leads to consistent improvements. We\nconclude by recommending that those creating new benchmarks, or selecting which\nexisting benchmarks to use, aim for high signal and low noise. We use 30\nbenchmarks for these experiments, and 375 open-weight language models from 60M\nto 32B parameters, resulting in a new, publicly available dataset of 900K\nevaluation benchmark results, totaling 200M instances.",
        "url": "http://arxiv.org/abs/2508.13144v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13144v1",
        "arxiv_id": "2508.13144v1",
        "authors": [
            "David Heineman",
            "Valentin Hofmann",
            "Ian Magnusson",
            "Yuling Gu",
            "Noah A. Smith",
            "Hannaneh Hajishirzi",
            "Kyle Lo",
            "Jesse Dodge"
        ],
        "submitted": "2025-08-18 17:56:04",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on language model evaluation, introducing metrics for signal and noise, and proposing interventions to improve benchmark quality. While it touches on the topic of model evaluation, which is related to query understanding and ranking models in IR, it does not directly address the user's core research themes. The paper's scope is limited to language models and evaluation, making it only loosely relevant to the user's interests."
    },
    {
        "title": "Context Matters: Incorporating Target Awareness in Conversational Abusive Language Detection",
        "abstract": "Abusive language detection has become an increasingly important task as a\nmeans to tackle this type of harmful content in social media. There has been a\nsubstantial body of research developing models for determining if a social\nmedia post is abusive or not; however, this research has primarily focused on\nexploiting social media posts individually, overlooking additional context that\ncan be derived from surrounding posts. In this study, we look at conversational\nexchanges, where a user replies to an earlier post by another user (the parent\ntweet). We ask: does leveraging context from the parent tweet help determine if\na reply post is abusive or not, and what are the features that contribute the\nmost? We study a range of content-based and account-based features derived from\nthe context, and compare this to the more widely studied approach of only\nlooking at the features from the reply tweet. For a more generalizable study,\nwe test four different classification models on a dataset made of\nconversational exchanges (parent-reply tweet pairs) with replies labeled as\nabusive or not. Our experiments show that incorporating contextual features\nleads to substantial improvements compared to the use of features derived from\nthe reply tweet only, confirming the importance of leveraging context. We\nobserve that, among the features under study, it is especially the\ncontent-based features (what is being posted) that contribute to the\nclassification performance rather than account-based features (who is posting\nit). While using content-based features, it is best to combine a range of\ndifferent features to ensure improved performance over being more selective and\nusing fewer features. Our study provides insights into the development of\ncontextualized abusive language detection models in realistic settings\ninvolving conversations.",
        "url": "http://arxiv.org/abs/2508.12828v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12828v1",
        "arxiv_id": "2508.12828v1",
        "authors": [
            "Raneem Alharthi",
            "Rajwa Alharthi",
            "Aiqi Jiang",
            "Arkaitz Zubiaga"
        ],
        "submitted": "2025-08-18 11:12:21",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on abusive language detection in conversational exchanges, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's context-aware approach is not applicable to the user's areas of interest, such as query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Learning to Steer: Input-dependent Steering for Multimodal LLMs",
        "abstract": "Steering has emerged as a practical approach to enable post-hoc guidance of\nLLMs towards enforcing a specific behavior. However, it remains largely\nunderexplored for multimodal LLMs (MLLMs); furthermore, existing steering\ntechniques, such as mean steering, rely on a single steering vector, applied\nindependently of the input query. This paradigm faces limitations when the\ndesired behavior is dependent on the example at hand. For example, a safe\nanswer may consist in abstaining from answering when asked for an illegal\nactivity, or may point to external resources or consultation with an expert\nwhen asked about medical advice. In this paper, we investigate a fine-grained\nsteering that uses an input-specific linear shift. This shift is computed using\ncontrastive input-specific prompting. However, the input-specific prompts\nrequired for this approach are not known at test time. Therefore, we propose to\ntrain a small auxiliary module to predict the input-specific steering vector.\nOur approach, dubbed as L2S (Learn-to-Steer), demonstrates that it reduces\nhallucinations and enforces safety in MLLMs, outperforming other static\nbaselines.",
        "url": "http://arxiv.org/abs/2508.12815v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12815v1",
        "arxiv_id": "2508.12815v1",
        "authors": [
            "Jayneel Parekh",
            "Pegah Khayatan",
            "Mustafa Shukor",
            "Arnaud Dapogny",
            "Alasdair Newson",
            "Matthieu Cord"
        ],
        "submitted": "2025-08-18 10:53:20",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores steering techniques for multimodal language models, which is a related topic in the broader field of Natural Language Processing. However, the focus on multimodal LLMs and the specific problem of input-dependent steering does not directly align with the user's interests in query understanding, ranking models, and user behavior modeling in the context of information retrieval."
    },
    {
        "title": "Bridging Human and LLM Judgments: Understanding and Narrowing the Gap",
        "abstract": "Large language models are increasingly used as judges (LLM-as-a-judge) to\nevaluate model outputs at scale, but their assessments often diverge\nsystematically from human judgments. We present Bridge, a unified statistical\nframework that explicitly bridges human and LLM evaluations under both absolute\nscoring and pairwise comparison paradigms. Bridge posits a latent human\npreference score for each prompt-response pair and models LLM deviations as\nlinear transformations of covariates that capture sources of discrepancies.\nThis offers a simple and principled framework for refining LLM ratings and\ncharacterizing systematic discrepancies between humans and LLMs. We provide an\nefficient fitting algorithm with asymptotic guarantees for statistical\ninference. Using six LLM judges and two benchmarks (BigGen Bench and Chatbot\nArena), Bridge achieves higher agreement with human ratings (accuracy,\ncalibration, and KL divergence) and exposes systematic human-LLM gaps.",
        "url": "http://arxiv.org/abs/2508.12792v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12792v1",
        "arxiv_id": "2508.12792v1",
        "authors": [
            "Felipe Maia Polo",
            "Xinhe Wang",
            "Mikhail Yurochkin",
            "Gongjun Xu",
            "Moulinath Banerjee",
            "Yuekai Sun"
        ],
        "submitted": "2025-08-18 10:14:20",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper explores the gap between human and Large Language Model (LLM) judgments, which is related to query understanding and ranking models in Information Retrieval. However, the focus is on LLMs as judges, rather than search technologies or user behavior modeling, which are core aspects of the user's research interests."
    },
    {
        "title": "Deep Research: A Survey of Autonomous Research Agents",
        "abstract": "The rapid advancement of large language models (LLMs) has driven the\ndevelopment of agentic systems capable of autonomously performing complex\ntasks. Despite their impressive capabilities, LLMs remain constrained by their\ninternal knowledge boundaries. To overcome these limitations, the paradigm of\ndeep research has been proposed, wherein agents actively engage in planning,\nretrieval, and synthesis to generate comprehensive and faithful analytical\nreports grounded in web-based evidence. In this survey, we provide a systematic\noverview of the deep research pipeline, which comprises four core stages:\nplanning, question developing, web exploration, and report generation. For each\nstage, we analyze the key technical challenges and categorize representative\nmethods developed to address them. Furthermore, we summarize recent advances in\noptimization techniques and benchmarks tailored for deep research. Finally, we\ndiscuss open challenges and promising research directions, aiming to chart a\nroadmap toward building more capable and trustworthy deep research agents.",
        "url": "http://arxiv.org/abs/2508.12752v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12752v1",
        "arxiv_id": "2508.12752v1",
        "authors": [
            "Wenlin Zhang",
            "Xiaopeng Li",
            "Yingyi Zhang",
            "Pengyue Jia",
            "Yichao Wang",
            "Huifeng Guo",
            "Yong Liu",
            "Xiangyu Zhao"
        ],
        "submitted": "2025-08-18 09:26:14",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. While it touches on topics like web-based evidence and report generation, the focus is on autonomous research agents and large language models, which is outside the user's primary research interests."
    },
    {
        "title": "RepreGuard: Detecting LLM-Generated Text by Revealing Hidden Representation Patterns",
        "abstract": "Detecting content generated by large language models (LLMs) is crucial for\npreventing misuse and building trustworthy AI systems. Although existing\ndetection methods perform well, their robustness in out-of-distribution (OOD)\nscenarios is still lacking. In this paper, we hypothesize that, compared to\nfeatures used by existing detection methods, the internal representations of\nLLMs contain more comprehensive and raw features that can more effectively\ncapture and distinguish the statistical pattern differences between\nLLM-generated texts (LGT) and human-written texts (HWT). We validated this\nhypothesis across different LLMs and observed significant differences in neural\nactivation patterns when processing these two types of texts. Based on this, we\npropose RepreGuard, an efficient statistics-based detection method.\nSpecifically, we first employ a surrogate model to collect representation of\nLGT and HWT, and extract the distinct activation feature that can better\nidentify LGT. We can classify the text by calculating the projection score of\nthe text representations along this feature direction and comparing with a\nprecomputed threshold. Experimental results show that RepreGuard outperforms\nall baselines with average 94.92% AUROC on both in-distribution (ID) and OOD\nscenarios, while also demonstrating robust resilience to various text sizes and\nmainstream attacks. Data and code are publicly available at:\nhttps://github.com/NLP2CT/RepreGuard",
        "url": "http://arxiv.org/abs/2508.13152v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13152v1",
        "arxiv_id": "2508.13152v1",
        "authors": [
            "Xin Chen",
            "Junchao Wu",
            "Shu Yang",
            "Runzhe Zhan",
            "Zeyu Wu",
            "Ziyang Luo",
            "Di Wang",
            "Min Yang",
            "Lidia S. Chao",
            "Derek F. Wong"
        ],
        "submitted": "2025-08-18 17:59:15",
        "source": "arxiv",
        "comment": "Accepted to TACL 2025. This version is a pre-MIT Press publication\n  version",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on detecting LLM-generated text, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on neural activation patterns, it does not address ranking models, user behavior modeling, or deep semantic understanding, making it only loosely relevant to your research interests."
    },
    {
        "title": "Has GPT-5 Achieved Spatial Intelligence? An Empirical Study",
        "abstract": "Multi-modal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, which are fundamental capabilities to achieving\nartificial general intelligence. With the recent release of GPT-5, allegedly\nthe most powerful AI model to date, it is timely to examine where the leading\nmodels stand on the path toward spatial intelligence. First, we propose a\ncomprehensive taxonomy of spatial tasks that unifies existing benchmarks and\ndiscuss the challenges in ensuring fair evaluation. We then evaluate\nstate-of-the-art proprietary and open-source models on eight key benchmarks, at\na cost exceeding one billion total tokens. Our empirical study reveals that (1)\nGPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)\nstill falls short of human performance across a broad spectrum of tasks.\nMoreover, we (3) identify the more challenging spatial intelligence problems\nfor multi-modal models, and (4) proprietary models do not exhibit a decisive\nadvantage when facing the most difficult problems. In addition, we conduct a\nqualitative evaluation across a diverse set of scenarios that are intuitive for\nhumans yet fail even the most advanced multi-modal models.",
        "url": "http://arxiv.org/abs/2508.13142v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13142v1",
        "arxiv_id": "2508.13142v1",
        "authors": [
            "Zhongang Cai",
            "Yubo Wang",
            "Qingping Sun",
            "Ruisi Wang",
            "Chenyang Gu",
            "Wanqi Yin",
            "Zhiqian Lin",
            "Zhitao Yang",
            "Chen Wei",
            "Xuanke Shi",
            "Kewang Deng",
            "Xiaoyang Han",
            "Zukai Chen",
            "Jiaqi Li",
            "Xiangyu Fan",
            "Hanming Deng",
            "Lewei Lu",
            "Bo Li",
            "Ziwei Liu",
            "Quan Wang",
            "Dahua Lin",
            "Lei Yang"
        ],
        "submitted": "2025-08-18 17:55:17",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on the spatial intelligence of GPT-5, a language model, and evaluates its performance on various benchmarks. While it's an interesting study, it doesn't relate to information retrieval, search technologies, or query understanding, which are the user's primary research interests. The paper's topics, such as multi-modal models and spatial understanding, are not directly relevant to the user's areas of focus."
    },
    {
        "title": "Spot the BlindSpots: Systematic Identification and Quantification of Fine-Grained LLM Biases in Contact Center Summaries",
        "abstract": "Abstractive summarization is a core application in contact centers, where\nLarge Language Models (LLMs) generate millions of summaries of call transcripts\ndaily. Despite their apparent quality, it remains unclear whether LLMs\nsystematically under- or over-attend to specific aspects of the transcript,\npotentially introducing biases in the generated summary. While prior work has\nexamined social and positional biases, the specific forms of bias pertinent to\ncontact center operations - which we term Operational Bias - have remained\nunexplored. To address this gap, we introduce BlindSpot, a framework built upon\na taxonomy of 15 operational bias dimensions (e.g., disfluency, speaker, topic)\nfor the identification and quantification of these biases. BlindSpot leverages\nan LLM as a zero-shot classifier to derive categorical distributions for each\nbias dimension in a pair of transcript and its summary. The bias is then\nquantified using two metrics: Fidelity Gap (the JS Divergence between\ndistributions) and Coverage (the percentage of source labels omitted). Using\nBlindSpot, we conducted an empirical study with 2500 real call transcripts and\ntheir summaries generated by 20 LLMs of varying scales and families (e.g., GPT,\nLlama, Claude). Our analysis reveals that biases are systemic and present\nacross all evaluated models, regardless of size or family.",
        "url": "http://arxiv.org/abs/2508.13124v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13124v1",
        "arxiv_id": "2508.13124v1",
        "authors": [
            "Kawin Mayilvaghanan",
            "Siddhant Gupta",
            "Ayush Kumar"
        ],
        "submitted": "2025-08-18 17:31:03",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the biases in Large Language Models (LLMs) in the context of contact center summaries, which is a specific application of Natural Language Processing (NLP). While it touches on some aspects of query understanding and ranking models, the focus is more on the biases and their quantification rather than the core IR and search technologies that align with your primary research interests."
    },
    {
        "title": "Reinforced Context Order Recovery for Adaptive Reasoning and Planning",
        "abstract": "Modern causal language models, followed by rapid developments in discrete\ndiffusion models, can now produce a wide variety of interesting and useful\ncontent. However, these families of models are predominantly trained to output\ntokens with a fixed (left-to-right) or random order, which may deviate from the\nlogical order in which tokens are generated originally. In this paper, we\nobserve that current causal and diffusion models encounter difficulties in\nproblems that require adaptive token generation orders to solve tractably,\nwhich we characterize with the $\\mathcal{V}$-information framework. Motivated\nby this, we propose Reinforced Context Order Recovery (ReCOR), a\nreinforcement-learning-based framework to extract adaptive, data-dependent\ntoken generation orders from text data without annotations. Self-supervised by\ntoken prediction statistics, ReCOR estimates the hardness of predicting every\nunfilled token and adaptively selects the next token during both training and\ninference. Experiments on challenging reasoning and planning datasets\ndemonstrate the superior performance of ReCOR compared with baselines,\nsometimes outperforming oracle models supervised with the ground-truth order.",
        "url": "http://arxiv.org/abs/2508.13070v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13070v1",
        "arxiv_id": "2508.13070v1",
        "authors": [
            "Long Ma",
            "Fangwei Zhong",
            "Yizhou Wang"
        ],
        "submitted": "2025-08-18 16:42:55",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on developing a framework for adaptive token generation orders in language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on topics like token prediction and reinforcement learning, the primary focus is on language modeling and planning, which is not a central match for your research interests."
    },
    {
        "title": "Can Large Models Teach Student Models to Solve Mathematical Problems Like Human Beings? A Reasoning Distillation Method via Multi-LoRA Interaction",
        "abstract": "Recent studies have demonstrated that Large Language Models (LLMs) have\nstrong mathematical reasoning abilities but rely on hundreds of billions of\nparameters. To tackle the challenge of poor reasoning in Small Language Models\n(SLMs), existing methods typically leverage LLMs to generate massive amounts of\ndata for cramming training. In psychology, they are akin to System 1 thinking,\nwhich resolves reasoning problems rapidly based on experience and intuition.\nHowever, human learning also requires System 2 thinking, where knowledge is\nfirst acquired and then reinforced through practice. Inspired by such two\ndistinct modes of thinking, we propose a novel method based on the multi-LoRA\nInteraction for mathematical reasoning Distillation (LoRID). First, we input\nthe question and reasoning of each sample into an LLM to create\nknowledge-enhanced datasets. Subsequently, we train a LoRA block on the student\nmodel as an Intuitive Reasoner (IR), which directly generates Chain-of-Thoughts\nfor problem-solving. Then, to imitate System 2 thinking, we train the Knowledge\nGenerator (KG) and Deep Reasoner (DR), respectively. The former outputs only\nknowledge after receiving problems, while the latter uses that knowledge to\nperform reasoning. Finally, to address the randomness in the generation of IR\nand DR, we evaluate whether their outputs are consistent, and the inference\nprocess needs to be iterated if not. This step can enhance the mathematical\nreasoning ability of SLMs through mutual feedback. Experimental results show\nthat LoRID achieves state-of-the-art performance, especially on the GSM8K\ndataset, where it outperforms the second-best method by 2.3%, 16.1%, 2.4%,\n12.3%, and 1.8% accuracy across the five base models, respectively.",
        "url": "http://arxiv.org/abs/2508.13037v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13037v1",
        "arxiv_id": "2508.13037v1",
        "authors": [
            "Xinhe Li",
            "Jiajun Liu",
            "Peng Wang"
        ],
        "submitted": "2025-08-18 15:56:10",
        "source": "arxiv",
        "comment": "Accepted by IJCAI2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper is not relevant to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on Large Language Models and mathematical reasoning, which is outside the user's primary research focus."
    },
    {
        "title": "Integrating Feedback Loss from Bi-modal Sarcasm Detector for Sarcastic Speech Synthesis",
        "abstract": "Sarcastic speech synthesis, which involves generating speech that effectively\nconveys sarcasm, is essential for enhancing natural interactions in\napplications such as entertainment and human-computer interaction. However,\nsynthesizing sarcastic speech remains a challenge due to the nuanced prosody\nthat characterizes sarcasm, as well as the limited availability of annotated\nsarcastic speech data. To address these challenges, this study introduces a\nnovel approach that integrates feedback loss from a bi-modal sarcasm detection\nmodel into the TTS training process, enhancing the model's ability to capture\nand convey sarcasm. In addition, by leveraging transfer learning, a speech\nsynthesis model pre-trained on read speech undergoes a two-stage fine-tuning\nprocess. First, it is fine-tuned on a diverse dataset encompassing various\nspeech styles, including sarcastic speech. In the second stage, the model is\nfurther refined using a dataset focused specifically on sarcastic speech,\nenhancing its ability to generate sarcasm-aware speech. Objective and\nsubjective evaluations demonstrate that our proposed methods improve the\nquality, naturalness, and sarcasm-awareness of synthesized speech.",
        "url": "http://arxiv.org/abs/2508.13028v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13028v1",
        "arxiv_id": "2508.13028v1",
        "authors": [
            "Zhu Li",
            "Yuqing Zhang",
            "Xiyuan Gao",
            "Devraj Raghuvanshi",
            "Nagendra Kumar",
            "Shekhar Nayak",
            "Matt Coler"
        ],
        "submitted": "2025-08-18 15:44:54",
        "source": "arxiv",
        "comment": "Speech Synthesis Workshop 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on speech synthesis and sarcasm detection, which is not directly related to information retrieval, search technologies, or query understanding. While it involves some NLP techniques, the primary focus is on speech synthesis and not on deep semantic understanding or real-time relevance optimization."
    },
    {
        "title": "PC-Sampler: Position-Aware Calibration of Decoding Bias in Masked Diffusion Models",
        "abstract": "Recent advances in masked diffusion models (MDMs) have established them as\npowerful non-autoregressive alternatives for sequence generation. Nevertheless,\nour preliminary experiments reveal that the generation quality of MDMs is still\nhighly sensitive to the choice of decoding strategy. In particular, widely\nadopted uncertainty-based samplers suffer from two key limitations: a lack of\nglobal trajectory control and a pronounced bias toward trivial tokens in the\nearly stages of decoding. These shortcomings restrict the full potential of\nMDMs. In this work, we introduce Position-Aware Confidence-Calibrated Sampling\n(PC-Sampler), a novel decoding strategy that unifies global trajectory planning\nwith content-aware informativeness maximization. PC-Sampler incorporates a\nposition-aware weighting mechanism to regulate the decoding path and a\ncalibrated confidence score to suppress the premature selection of trivial\ntokens. Extensive experiments on three advanced MDMs across seven challenging\nbenchmarks-including logical reasoning and planning tasks-demonstrate that\nPC-Sampler consistently outperforms existing MDM decoding strategies by more\nthan 10% on average, significantly narrowing the performance gap with\nstate-of-the-art autoregressive models. All codes are available at\nhttps://github.com/NEUIR/PC-Sampler.",
        "url": "http://arxiv.org/abs/2508.13021v2",
        "pdf_url": "http://arxiv.org/pdf/2508.13021v2",
        "arxiv_id": "2508.13021v2",
        "authors": [
            "Pengcheng Huang",
            "Shuhao Liu",
            "Zhenghao Liu",
            "Yukun Yan",
            "Shuo Wang",
            "Zulong Chen",
            "Tong Xiao"
        ],
        "submitted": "2025-08-18 15:38:37",
        "source": "arxiv",
        "comment": "17 pages,13 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on masked diffusion models and decoding strategies for sequence generation, which is outside the scope of Information Retrieval, Search technologies, and Natural Language Processing. The paper's abstract does not mention query understanding, ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest for you."
    },
    {
        "title": "Explicit v.s. Implicit Memory: Exploring Multi-hop Complex Reasoning Over Personalized Information",
        "abstract": "In large language model-based agents, memory serves as a critical capability\nfor achieving personalization by storing and utilizing users' information.\nAlthough some previous studies have adopted memory to implement user\npersonalization, they typically focus on preference alignment and simple\nquestion-answering. However, in the real world, complex tasks often require\nmulti-hop reasoning on a large amount of user information, which poses\nsignificant challenges for current memory approaches. To address this\nlimitation, we propose the multi-hop personalized reasoning task to explore how\ndifferent memory mechanisms perform in multi-hop reasoning over personalized\ninformation. We explicitly define this task and construct a dataset along with\na unified evaluation framework. Then, we implement various explicit and\nimplicit memory methods and conduct comprehensive experiments. We evaluate\ntheir performance on this task from multiple perspectives and analyze their\nstrengths and weaknesses. Besides, we explore hybrid approaches that combine\nboth paradigms and propose the HybridMem method to address their limitations.\nWe demonstrate the effectiveness of our proposed model through extensive\nexperiments. To benefit the research community, we release this project at\nhttps://github.com/nuster1128/MPR.",
        "url": "http://arxiv.org/abs/2508.13250v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13250v1",
        "arxiv_id": "2508.13250v1",
        "authors": [
            "Zeyu Zhang",
            "Yang Zhang",
            "Haoran Tan",
            "Rui Li",
            "Xu Chen"
        ],
        "submitted": "2025-08-18 13:34:37",
        "source": "arxiv",
        "comment": "15 pages, 13 figures, 3 tables",
        "score": 2,
        "keyword_reasons": [
            "Found 'personalization' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores memory mechanisms for personalization in large language model-based agents, which is a related topic to information retrieval and search technologies. However, the focus on multi-hop reasoning and complex tasks is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "A Stitch in Time Saves Nine: Proactive Self-Refinement for Language Models",
        "abstract": "Recent advances in self-refinement have demonstrated significant potential\nfor improving the outputs of large language models (LLMs) through iterative\nrefinement. However, most existing self-refinement methods rely on a reactive\nprocess with a fixed number of iterations, making it difficult to determine the\noptimal timing and content of refinement based on the evolving generation\ncontext. Inspired by the way humans dynamically refine their thoughts during\nexecution, we propose ProActive Self-Refinement (PASR), a novel method that\nenables LLMs to refine their outputs during the generation process. Unlike\nmethods that regenerate entire responses, PASR proactively decides whether,\nwhen, and how to refine based on the model's internal state and evolving\ncontext. We conduct extensive experiments on a diverse set of 10 tasks to\nevaluate the effectiveness of PASR. Experimental results show that PASR\nsignificantly enhances problem-solving performance. In particular, on Qwen3-8B,\nPASR reduces average token consumption by 41.6 percent compared to standard\ngeneration, while also achieving an 8.2 percent improvement in accuracy. Our\ncode and all baselines used in the paper are available in the GitHub.",
        "url": "http://arxiv.org/abs/2508.12903v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12903v1",
        "arxiv_id": "2508.12903v1",
        "authors": [
            "Jinyi Han",
            "Xinyi Wang",
            "Haiquan Zhao",
            "Tingyun li",
            "Zishang Jiang",
            "Sihang Jiang",
            "Jiaqing Liang",
            "Xin Lin",
            "Weikang Zhou",
            "Zeye Sun",
            "Fei Yu",
            "Yanghua Xiao"
        ],
        "submitted": "2025-08-18 13:07:21",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on self-refinement for language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the idea of refining outputs, the context is different from the user's interests in ranking models, user behavior modeling, and deep semantic understanding."
    },
    {
        "title": "An LLM Agent-Based Complex Semantic Table Annotation Approach",
        "abstract": "The Semantic Table Annotation (STA) task, which includes Column Type\nAnnotation (CTA) and Cell Entity Annotation (CEA), maps table contents to\nontology entities and plays important roles in various semantic applications.\nHowever, complex tables often pose challenges such as semantic loss of column\nnames or cell values, strict ontological hierarchy requirements, homonyms,\nspelling errors, and abbreviations, which hinder annotation accuracy. To\naddress these issues, this paper proposes an LLM-based agent approach for CTA\nand CEA. We design and implement five external tools with tailored prompts\nbased on the ReAct framework, enabling the STA agent to dynamically select\nsuitable annotation strategies depending on table characteristics. Experiments\nare conducted on the Tough Tables and BiodivTab datasets from the SemTab\nchallenge, which contain the aforementioned challenges. Our method outperforms\nexisting approaches across various metrics. Furthermore, by leveraging\nLevenshtein distance to reduce redundant annotations, we achieve a 70%\nreduction in time costs and a 60% reduction in LLM token usage, providing an\nefficient and cost-effective solution for STA.",
        "url": "http://arxiv.org/abs/2508.12868v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12868v1",
        "arxiv_id": "2508.12868v1",
        "authors": [
            "Yilin Geng",
            "Shujing Wang",
            "Chuan Wang",
            "Keqing He",
            "Yanfei Lv",
            "Ying Wang",
            "Zaiwen Feng",
            "Xiaoying Bai"
        ],
        "submitted": "2025-08-18 12:09:20",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes an LLM-based agent approach for Semantic Table Annotation, which is a specific problem in Natural Language Processing. While it's related to information retrieval, the focus is on table annotation rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest for you. The paper's relevance is somewhat limited due to its narrow scope."
    },
    {
        "title": "E3RG: Building Explicit Emotion-driven Empathetic Response Generation System with Multimodal Large Language Model",
        "abstract": "Multimodal Empathetic Response Generation (MERG) is crucial for building\nemotionally intelligent human-computer interactions. Although large language\nmodels (LLMs) have improved text-based ERG, challenges remain in handling\nmultimodal emotional content and maintaining identity consistency. Thus, we\npropose E3RG, an Explicit Emotion-driven Empathetic Response Generation System\nbased on multimodal LLMs which decomposes MERG task into three parts:\nmultimodal empathy understanding, empathy memory retrieval, and multimodal\nresponse generation. By integrating advanced expressive speech and video\ngenerative models, E3RG delivers natural, emotionally rich, and\nidentity-consistent responses without extra training. Experiments validate the\nsuperiority of our system on both zero-shot and few-shot settings, securing\nTop-1 position in the Avatar-based Multimodal Empathy Challenge on ACM MM 25.\nOur code is available at https://github.com/RH-Lin/E3RG.",
        "url": "http://arxiv.org/abs/2508.12854v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12854v1",
        "arxiv_id": "2508.12854v1",
        "authors": [
            "Ronghao Lin",
            "Shuai Shen",
            "Weipeng Hu",
            "Qiaolin He",
            "Aolin Xiong",
            "Li Huang",
            "Haifeng Hu",
            "Yap-peng Tan"
        ],
        "submitted": "2025-08-18 11:47:02",
        "source": "arxiv",
        "comment": "Accepted at ACM MM 2025 Grand Challenge",
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on multimodal empathetic response generation, which is not directly related to information retrieval or search technologies. While it uses large language models, the primary goal is not query understanding, ranking models, or user behavior modeling, but rather generating empathetic responses. The paper's relevance to the user's interests is limited, but it does touch on some NLP-related topics."
    },
    {
        "title": "ding-01 :ARG0: An AMR Corpus for Spontaneous French Dialogue",
        "abstract": "We present our work to build a French semantic corpus by annotating French\ndialogue in Abstract Meaning Representation (AMR). Specifically, we annotate\nthe DinG corpus, consisting of transcripts of spontaneous French dialogues\nrecorded during the board game Catan. As AMR has insufficient coverage of the\ndynamics of spontaneous speech, we extend the framework to better represent\nspontaneous speech and sentence structures specific to French. Additionally, to\nsupport consistent annotation, we provide an annotation guideline detailing\nthese extensions. We publish our corpus under a free license (CC-SA-BY). We\nalso train and evaluate an AMR parser on our data. This model can be used as an\nassistance annotation tool to provide initial annotations that can be refined\nby human annotators. Our work contributes to the development of semantic\nresources for French dialogue.",
        "url": "http://arxiv.org/abs/2508.12819v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12819v1",
        "arxiv_id": "2508.12819v1",
        "authors": [
            "Jeongwoo Kang",
            "Maria Boritchev",
            "Maximin Coavoux"
        ],
        "submitted": "2025-08-18 10:57:44",
        "source": "arxiv",
        "comment": "Accepted at IWCS 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on building a French semantic corpus using Abstract Meaning Representation (AMR) for spontaneous dialogue, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it involves Natural Language Processing, the scope is limited to French dialogue annotation and does not address ranking models, user behavior modeling, or real-time relevance optimization."
    },
    {
        "title": "When Alignment Hurts: Decoupling Representational Spaces in Multilingual Models",
        "abstract": "Alignment with high-resource standard languages is often assumed to aid the\nmodeling of related low-resource varieties. We challenge this assumption by\ndemonstrating that excessive representational entanglement with a dominant\nvariety, such as Modern Standard Arabic (MSA) in relation to Arabic dialects,\ncan actively hinder generative modeling. We present the first comprehensive\ncausal study of this phenomenon by analyzing and directly intervening in the\ninternal representation geometry of large language models (LLMs). Our key\ncontribution is an online variational probing framework that continuously\nestimates the subspace of the standard variety during fine-tuning, enabling\nprojection-based decoupling from this space. While our study uses Arabic as a\ncase due to its unusually rich parallel resources across 25 dialects, the\nbroader motivation is methodological: dialectal MT serves as a controlled proxy\nfor generative tasks where comparable multi-variety corpora are unavailable.\nAcross 25 dialects, our intervention improves generation quality by up to +4.9\nchrF++ and +2.0 on average compared to standard fine-tuning, despite a measured\ntradeoff in standard-language performance. These results provide causal\nevidence that subspace dominance by high-resource varieties can restrict\ngenerative capacity for related varieties. More generally, we unify geometric\nand information-theoretic probing with subspace-level causal interventions,\noffering practical tools for improving generative modeling in closely related\nlanguage families and, more broadly, for controlling representational\nallocation in multilingual and multi-domain LLMs. Code will be released.",
        "url": "http://arxiv.org/abs/2508.12803v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12803v1",
        "arxiv_id": "2508.12803v1",
        "authors": [
            "Ahmed Elshabrawy",
            "Hour Kaing",
            "Haiyue Song",
            "Alham Fikri Aji",
            "Hideki Tanaka",
            "Masao Utiyama",
            "Raj Dabre"
        ],
        "submitted": "2025-08-18 10:34:08",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The focus on multilingual models, generative modeling, and language families is outside your primary areas of interest."
    },
    {
        "title": "DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning",
        "abstract": "Large language models (LLMs) have achieved remarkable success in many natural\nlanguage tasks but still struggle with complex, multi-step reasoning,\nparticularly across diverse disciplines. Existing reasoning datasets often\neither lack disciplinary breadth or the structural depth necessary to elicit\nrobust reasoning behaviors. We propose DESIGNER: a DESIGN-logic-guidEd\nReasoning data synthesis pipeline that leverages naturally available, extensive\nraw documents (book corpus and web corpus) to generate multidisciplinary\nchallenging questions. A core innovation of our approach is the introduction of\na Design Logic concept, which mimics the question-creation process of human\neducators. We use LLMs to reverse-engineer and abstract over 120,000 design\nlogics from existing questions across various disciplines. By matching these\ndesign logics with disciplinary source materials, we are able to create\nreasoning questions that far surpass the difficulty and diversity of existing\ndatasets. Based on this pipeline, we synthesized two large-scale reasoning\ndatasets that span 75 disciplines: Design-Logic-Reasoning-Book (DLR-Book),\ncontaining 3.04 million challenging questions synthesized from the book corpus,\nand Design-Logic-Reasoning-Web (DLR-Web), with 1.66 million challenging\nquestions from the web corpus. Our data analysis demonstrates that the\nquestions synthesized by our method exhibit substantially greater difficulty\nand diversity than those in the baseline datasets. We validate the\neffectiveness of these datasets by conducting SFT experiments on the\nQwen3-8B-Base and Qwen3-4B-Base models. The results show that our dataset\nsignificantly outperforms existing multidisciplinary datasets of the same\nvolume. Training with the full datasets further enables the models to surpass\nthe multidisciplinary reasoning performance of the official Qwen3-8B and\nQwen3-4B models.",
        "url": "http://arxiv.org/abs/2508.12726v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12726v1",
        "arxiv_id": "2508.12726v1",
        "authors": [
            "Weize Liu",
            "Yongchi Zhao",
            "Yijia Luo",
            "Mingyu Xu",
            "Jiaheng Liu",
            "Yanan Li",
            "Xiguo Hu",
            "Yuchi Xu",
            "Wenbo Su",
            "Bo Zheng"
        ],
        "submitted": "2025-08-18 08:49:29",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on designing datasets for large language models, which is related to information retrieval and search technologies. However, the primary focus is on natural language processing and data synthesis, which is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Prompt-Induced Linguistic Fingerprints for LLM-Generated Fake News Detection",
        "abstract": "With the rapid development of large language models, the generation of fake\nnews has become increasingly effortless, posing a growing societal threat and\nunderscoring the urgent need for reliable detection methods. Early efforts to\nidentify LLM-generated fake news have predominantly focused on the textual\ncontent itself; however, because much of that content may appear coherent and\nfactually consistent, the subtle traces of falsification are often difficult to\nuncover. Through distributional divergence analysis, we uncover prompt-induced\nlinguistic fingerprints: statistically distinct probability shifts between\nLLM-generated real and fake news when maliciously prompted. Based on this\ninsight, we propose a novel method named Linguistic Fingerprints Extraction\n(LIFE). By reconstructing word-level probability distributions, LIFE can find\ndiscriminative patterns that facilitate the detection of LLM-generated fake\nnews. To further amplify these fingerprint patterns, we also leverage\nkey-fragment techniques that accentuate subtle linguistic differences, thereby\nimproving detection reliability. Our experiments show that LIFE achieves\nstate-of-the-art performance in LLM-generated fake news and maintains high\nperformance in human-written fake news. The code and data are available at\nhttps://anonymous.4open.science/r/LIFE-E86A.",
        "url": "http://arxiv.org/abs/2508.12632v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12632v1",
        "arxiv_id": "2508.12632v1",
        "authors": [
            "Chi Wang",
            "Min Gao",
            "Zongwei Wang",
            "Junwei Yin",
            "Kai Shu",
            "Chenghua Lin"
        ],
        "submitted": "2025-08-18 05:24:54",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to information retrieval, as it deals with detecting fake news generated by large language models. However, the focus is more on natural language processing and machine learning techniques, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest in information retrieval."
    },
    {
        "title": "Beyond Modality Limitations: A Unified MLLM Approach to Automated Speaking Assessment with Effective Curriculum Learning",
        "abstract": "Traditional Automated Speaking Assessment (ASA) systems exhibit inherent\nmodality limitations: text-based approaches lack acoustic information while\naudio-based methods miss semantic context. Multimodal Large Language Models\n(MLLM) offer unprecedented opportunities for comprehensive ASA by\nsimultaneously processing audio and text within unified frameworks. This paper\npresents a very first systematic study of MLLM for comprehensive ASA,\ndemonstrating the superior performance of MLLM across the aspects of content\nand language use . However, assessment on the delivery aspect reveals unique\nchallenges, which is deemed to require specialized training strategies. We thus\npropose Speech-First Multimodal Training (SFMT), leveraging a curriculum\nlearning principle to establish more robust modeling foundations of speech\nbefore cross-modal synergetic fusion. A series of experiments on a benchmark\ndataset show MLLM-based systems can elevate the holistic assessment performance\nfrom a PCC value of 0.783 to 0.846. In particular, SFMT excels in the\nevaluation of the delivery aspect, achieving an absolute accuracy improvement\nof 4% over conventional training approaches, which also paves a new avenue for\nASA.",
        "url": "http://arxiv.org/abs/2508.12591v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12591v1",
        "arxiv_id": "2508.12591v1",
        "authors": [
            "Yu-Hsuan Fang",
            "Tien-Hong Lo",
            "Yao-Ting Sung",
            "Berlin Chen"
        ],
        "submitted": "2025-08-18 02:57:43",
        "source": "arxiv",
        "comment": "Accepted at IEEE ASRU 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on multimodal large language models for automated speaking assessment, which is not directly related to information retrieval, search technologies, or query understanding. While it involves language models, the context is specific to speech assessment and does not align with the user's primary research interests."
    },
    {
        "title": "Insight Rumors: A Novel Textual Rumor Locating and Marking Model Leveraging Att_BiMamba2 Network",
        "abstract": "With the development of social media networks, rumor detection models have\nattracted more and more attention. Whereas, these models primarily focus on\nclassifying contexts as rumors or not, lacking the capability to locate and\nmark specific rumor content. To address this limitation, this paper proposes a\nnovel rumor detection model named Insight Rumors to locate and mark rumor\ncontent within textual data. Specifically, we propose the Bidirectional Mamba2\nNetwork with Dot-Product Attention (Att_BiMamba2), a network that constructs a\nbidirectional Mamba2 model and applies dot-product attention to weight and\ncombine the outputs from both directions, thereby enhancing the representation\nof high-dimensional rumor features. Simultaneously, a Rumor Locating and\nMarking module is designed to locate and mark rumors. The module constructs a\nskip-connection network to project high-dimensional rumor features onto\nlow-dimensional label features. Moreover, Conditional Random Fields (CRF) is\nemployed to impose strong constraints on the output label features, ensuring\naccurate rumor content location. Additionally, a labeled dataset for rumor\nlocating and marking is constructed, with the effectiveness of the proposed\nmodel is evaluated through comprehensive experiments. Extensive experiments\nindicate that the proposed scheme not only detects rumors accurately but also\nlocates and marks them in context precisely, outperforming state-of-the-art\nschemes that can only discriminate rumors roughly.",
        "url": "http://arxiv.org/abs/2508.12574v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12574v1",
        "arxiv_id": "2508.12574v1",
        "authors": [
            "Bin Ma",
            "Yifei Zhang",
            "Yongjin Xian",
            "Qi Li",
            "Linna Zhou",
            "Gongxun Miao"
        ],
        "submitted": "2025-08-18 02:20:57",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on rumor detection and location, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves text analysis and attention mechanisms, the context is specific to rumor detection and does not align with the user's core research themes."
    },
    {
        "title": "CorrSteer: Steering Improves Task Performance and Safety in LLMs through Correlation-based Sparse Autoencoder Feature Selection",
        "abstract": "Sparse Autoencoders (SAEs) can extract interpretable features from large\nlanguage models (LLMs) without supervision. However, their effectiveness in\ndownstream steering tasks is limited by the requirement for contrastive\ndatasets or large activation storage. To address these limitations, we propose\nCorrSteer, which selects features by correlating sample correctness with SAE\nactivations from generated tokens at inference time. This approach uses only\ninference-time activations to extract more relevant features, thereby avoiding\nspurious correlations. It also obtains steering coefficients from average\nactivations, automating the entire pipeline. Our method shows improved task\nperformance on QA, bias mitigation, jailbreaking prevention, and reasoning\nbenchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1%\nimprovement in MMLU performance and a +22.9% improvement in HarmBench with only\n4000 samples. Selected features demonstrate semantically meaningful patterns\naligned with each task's requirements, revealing the underlying capabilities\nthat drive performance. Our work establishes correlationbased selection as an\neffective and scalable approach for automated SAE steering across language\nmodel applications.",
        "url": "http://arxiv.org/abs/2508.12535v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12535v1",
        "arxiv_id": "2508.12535v1",
        "authors": [
            "Seonglae Cho",
            "Zekun Wu",
            "Adriano Koshiyama"
        ],
        "submitted": "2025-08-18 00:01:42",
        "source": "arxiv",
        "comment": "42 pages, 9 tables",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on feature selection and steering in large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions language models, the context is different from the user's interests in IR and NLP."
    },
    {
        "title": "M3PO: Multimodal-Model-Guided Preference Optimization for Visual Instruction Following",
        "abstract": "Large Vision-Language Models (LVLMs) hold immense potential for complex\nmultimodal instruction following, yet their development is often hindered by\nthe high cost and inconsistency of human annotation required for effective\nfine-tuning and preference alignment. Traditional supervised fine-tuning (SFT)\nand existing preference optimization methods like RLHF and DPO frequently\nstruggle to efficiently leverage the model's own generation space to identify\nhighly informative \"hard negative\" samples. To address these challenges, we\npropose Multimodal-Model-Guided Preference Optimization (M3PO), a novel and\ndata-efficient method designed to enhance LVLMs' capabilities in visual\ninstruction following. M3PO intelligently selects the most \"learning-valuable\"\npreference sample pairs from a diverse pool of LVLM-generated candidates. This\nselection is driven by a sophisticated mechanism that integrates two crucial\nsignals: a Multimodal Alignment Score (MAS) to assess external quality and the\nmodel's Self-Consistency / Confidence (log-probability) to gauge internal\nbelief. These are combined into a novel M3P-Score, which specifically\nidentifies preferred responses and challenging dispreferred responses that the\nmodel might confidently generate despite being incorrect. These high-quality\npreference pairs are then used for efficient Direct Preference Optimization\n(DPO) fine-tuning on base LVLMs like LLaVA-1.5 (7B/13B) using LoRA. Our\nextensive experiments demonstrate that M3PO consistently outperforms strong\nbaselines, including SFT, simulated RLHF, vanilla DPO, and RM-DPO, across a\ncomprehensive suite of multimodal instruction following benchmarks (MME-Bench,\nPOPE, IFT, Human Pref. Score).",
        "url": "http://arxiv.org/abs/2508.12458v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12458v1",
        "arxiv_id": "2508.12458v1",
        "authors": [
            "Ruirui Gao",
            "Emily Johnson",
            "Bowen Tan",
            "Yanfei Qian"
        ],
        "submitted": "2025-08-17 18:07:55",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on multimodal instruction following and fine-tuning of large vision-language models, which is not directly related to information retrieval, search technologies, or query understanding. While it involves optimization techniques, the context is different from the user's primary research interests."
    },
    {
        "title": "Evaluating ASR robustness to spontaneous speech errors: A study of WhisperX using a Speech Error Database",
        "abstract": "The Simon Fraser University Speech Error Database (SFUSED) is a public data\ncollection developed for linguistic and psycholinguistic research. Here we\ndemonstrate how its design and annotations can be used to test and evaluate\nspeech recognition models. The database comprises systematically annotated\nspeech errors from spontaneous English speech, with each error tagged for\nintended and actual error productions. The annotation schema incorporates\nmultiple classificatory dimensions that are of some value to model assessment,\nincluding linguistic hierarchical level, contextual sensitivity, degraded\nwords, word corrections, and both word-level and syllable-level error\npositioning. To assess the value of these classificatory variables, we\nevaluated the transcription accuracy of WhisperX across 5,300 documented word\nand phonological errors. This analysis demonstrates the atabase's effectiveness\nas a diagnostic tool for ASR system performance.",
        "url": "http://arxiv.org/abs/2508.13060v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13060v1",
        "arxiv_id": "2508.13060v1",
        "authors": [
            "John Alderete",
            "Macarious Kin Fung Hui",
            "Aanchan Mohan"
        ],
        "submitted": "2025-08-18 16:30:33",
        "source": "arxiv",
        "comment": "5 pages, 6 figures, 1 table, Interspeech 2025 (Rotterdam)",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The focus on speech recognition and error analysis is outside your primary areas of interest."
    },
    {
        "title": "Byk Dil Modelleri iin TR-MMLU Benchmark: Performans Deerlendirmesi, Zorluklar ve yiletirme Frsatlar",
        "abstract": "Language models have made significant advancements in understanding and\ngenerating human language, achieving remarkable success in various\napplications. However, evaluating these models remains a challenge,\nparticularly for resource-limited languages like Turkish. To address this\nissue, we introduce the Turkish MMLU (TR-MMLU) benchmark, a comprehensive\nevaluation framework designed to assess the linguistic and conceptual\ncapabilities of large language models (LLMs) in Turkish. TR-MMLU is based on a\nmeticulously curated dataset comprising 6,200 multiple-choice questions across\n62 sections within the Turkish education system. This benchmark provides a\nstandard framework for Turkish NLP research, enabling detailed analyses of\nLLMs' capabilities in processing Turkish text. In this study, we evaluated\nstate-of-the-art LLMs on TR-MMLU, highlighting areas for improvement in model\ndesign. TR-MMLU sets a new standard for advancing Turkish NLP research and\ninspiring future innovations.",
        "url": "http://arxiv.org/abs/2508.13044v1",
        "pdf_url": "http://arxiv.org/pdf/2508.13044v1",
        "arxiv_id": "2508.13044v1",
        "authors": [
            "M. Ali Bayram",
            "Ali Arda Fincan",
            "Ahmet Semih Gm",
            "Banu Diri",
            "Sava Yldrm",
            "ner Ayta"
        ],
        "submitted": "2025-08-18 16:00:43",
        "source": "arxiv",
        "comment": "10 pages, in Turkish language, 5 figures. Presented at the 2025 33rd\n  Signal Processing and Communications Applications Conference (SIU), 25--28\n  June 2025, Sile, Istanbul, T\\\"urkiye",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on Turkish language models and their evaluation, which is not directly related to my research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it mentions NLP, the scope is limited to Turkish language models and education system, which does not align with my broader interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Analyzing Information Sharing and Coordination in Multi-Agent Planning",
        "abstract": "Multi-agent systems (MASs) have pushed the boundaries of large language model\n(LLM) agents in domains such as web research and software engineering. However,\nlong-horizon, multi-constraint planning tasks involve conditioning on detailed\ninformation and satisfying complex interdependent constraints, which can pose a\nchallenge for these systems. In this study, we construct an LLM-based MAS for a\ntravel planning task which is representative of these challenges. We evaluate\nthe impact of a notebook to facilitate information sharing, and evaluate an\norchestrator agent to improve coordination in free form conversation between\nagents. We find that the notebook reduces errors due to hallucinated details by\n18%, while an orchestrator directs the MAS to focus on and further reduce\nerrors by up to 13.5% within focused sub-areas. Combining both mechanisms\nachieves a 25% final pass rate on the TravelPlanner benchmark, a 17.5% absolute\nimprovement over the single-agent baseline's 7.5% pass rate. These results\nhighlight the potential of structured information sharing and reflective\norchestration as key components in MASs for long horizon planning with LLMs.",
        "url": "http://arxiv.org/abs/2508.12981v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12981v1",
        "arxiv_id": "2508.12981v1",
        "authors": [
            "Tianyue Ou",
            "Saujas Vaduguru",
            "Daniel Fried"
        ],
        "submitted": "2025-08-18 14:57:02",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on multi-agent systems and planning, which is not directly related to information retrieval, search technologies, or natural language processing. While it mentions large language models, the context is different from query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "It takes a village to write a book: Mapping anonymous contributions in Stephen Langton's Quaestiones Theologiae",
        "abstract": "While the indirect evidence suggests that already in the early scholastic\nperiod the literary production based on records of oral teaching (so-called\nreportationes) was not uncommon, there are very few sources commenting on the\npractice. This paper details the design of a study applying stylometric\ntechniques of authorship attribution to a collection developed from\nreportationes -- Stephen Langton's Quaestiones Theologiae -- aiming to uncover\nlayers of editorial work and thus validate some hypotheses regarding the\ncollection's formation. Following Camps, Cl\\'erice, and Pinche (2021), I\ndiscuss the implementation of an HTR pipeline and stylometric analysis based on\nthe most frequent words, POS tags, and pseudo-affixes. The proposed study will\noffer two methodological gains relevant to computational research on the\nscholastic tradition: it will directly compare performance on manually composed\nand automatically extracted data, and it will test the validity of\ntransformer-based OCR and automated transcription alignment for workflows\napplied to scholastic Latin corpora. If successful, this study will provide an\neasily reusable template for the exploratory analysis of collaborative literary\nproduction stemming from medieval universities.",
        "url": "http://arxiv.org/abs/2508.12830v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12830v1",
        "arxiv_id": "2508.12830v1",
        "authors": [
            "Jan Maliszewski"
        ],
        "submitted": "2025-08-18 11:13:45",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The topic is focused on stylometric techniques for authorship attribution and the study of medieval Latin corpora, which is outside your areas of interest."
    },
    {
        "title": "LinguaSafe: A Comprehensive Multilingual Safety Benchmark for Large Language Models",
        "abstract": "The widespread adoption and increasing prominence of large language models\n(LLMs) in global technologies necessitate a rigorous focus on ensuring their\nsafety across a diverse range of linguistic and cultural contexts. The lack of\na comprehensive evaluation and diverse data in existing multilingual safety\nevaluations for LLMs limits their effectiveness, hindering the development of\nrobust multilingual safety alignment. To address this critical gap, we\nintroduce LinguaSafe, a comprehensive multilingual safety benchmark crafted\nwith meticulous attention to linguistic authenticity. The LinguaSafe dataset\ncomprises 45k entries in 12 languages, ranging from Hungarian to Malay. Curated\nusing a combination of translated, transcreated, and natively-sourced data, our\ndataset addresses the critical need for multilingual safety evaluations of\nLLMs, filling the void in the safety evaluation of LLMs across diverse\nunder-represented languages from Hungarian to Malay. LinguaSafe presents a\nmultidimensional and fine-grained evaluation framework, with direct and\nindirect safety assessments, including further evaluations for oversensitivity.\nThe results of safety and helpfulness evaluations vary significantly across\ndifferent domains and different languages, even in languages with similar\nresource levels. Our benchmark provides a comprehensive suite of metrics for\nin-depth safety evaluation, underscoring the critical importance of thoroughly\nassessing multilingual safety in LLMs to achieve more balanced safety\nalignment. Our dataset and code are released to the public to facilitate\nfurther research in the field of multilingual LLM safety.",
        "url": "http://arxiv.org/abs/2508.12733v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12733v1",
        "arxiv_id": "2508.12733v1",
        "authors": [
            "Zhiyuan Ning",
            "Tianle Gu",
            "Jiaxin Song",
            "Shixin Hong",
            "Lingyu Li",
            "Huacan Liu",
            "Jie Li",
            "Yixu Wang",
            "Meng Lingyu",
            "Yan Teng",
            "Yingchun Wang"
        ],
        "submitted": "2025-08-18 08:59:01",
        "source": "arxiv",
        "comment": "7pages, 5 figures",
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on the safety evaluation of large language models, which is not directly related to information retrieval, search technologies, or query understanding. Although it mentions multilingual evaluation, the context is not relevant to the user's primary research interests in IR, NLP, and data mining."
    },
    {
        "title": "Asymmetric Diffusion Recommendation Model",
        "abstract": "Recently, motivated by the outstanding achievements of diffusion models, the\ndiffusion process has been employed to strengthen representation learning in\nrecommendation systems. Most diffusion-based recommendation models typically\nutilize standard Gaussian noise in symmetric forward and reverse processes in\ncontinuous data space. Nevertheless, the samples derived from recommendation\nsystems inhabit a discrete data space, which is fundamentally different from\nthe continuous one. Moreover, Gaussian noise has the potential to corrupt\npersonalized information within latent representations. In this work, we\npropose a novel and effective method, named Asymmetric Diffusion Recommendation\nModel (AsymDiffRec), which learns forward and reverse processes in an\nasymmetric manner. We define a generalized forward process that simulates the\nmissing features in real-world recommendation samples. The reverse process is\nthen performed in an asymmetric latent feature space. To preserve personalized\ninformation within the latent representation, a task-oriented optimization\nstrategy is introduced. In the serving stage, the raw sample with missing\nfeatures is regarded as a noisy input to generate a denoising and robust\nrepresentation for the final prediction. By equipping base models with\nAsymDiffRec, we conduct online A/B tests, achieving improvements of +0.131% and\n+0.166% in terms of users' active days and app usage duration respectively.\nAdditionally, the extended offline experiments also demonstrate improvements.\nAsymDiffRec has been implemented in the Douyin Music App.",
        "url": "http://arxiv.org/abs/2508.12706v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12706v1",
        "arxiv_id": "2508.12706v1",
        "authors": [
            "Yongchun Zhu",
            "Guanyu Jiang",
            "Jingwu Chen",
            "Feng Zhang",
            "Xiao Yang",
            "Zuotao Liu"
        ],
        "submitted": "2025-08-18 08:05:25",
        "source": "arxiv",
        "comment": "Accepted by CIKM2025",
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on a recommendation model, which is only loosely related to the user's primary research interests in Information Retrieval and Search technologies. Although it mentions 'representation learning', the context is different from the user's focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Diagnostic-Guided Dynamic Profile Optimization for LLM-based User Simulators in Sequential Recommendation",
        "abstract": "Recent advances in large language models (LLMs) have enabled realistic user\nsimulators for developing and evaluating recommender systems (RSs). However,\nexisting LLM-based simulators for RSs face two major limitations: (1) static\nand single-step prompt-based inference that leads to inaccurate and incomplete\nuser profile construction; (2) unrealistic and single-round\nrecommendation-feedback interaction pattern that fails to capture real-world\nscenarios. To address these limitations, we propose DGDPO (Diagnostic-Guided\nDynamic Profile Optimization), a novel framework that constructs user profile\nthrough a dynamic and iterative optimization process to enhance the simulation\nfidelity. Specifically, DGDPO incorporates two core modules within each\noptimization loop: firstly, a specialized LLM-based diagnostic module,\ncalibrated through our novel training strategy, accurately identifies specific\ndefects in the user profile. Subsequently, a generalized LLM-based treatment\nmodule analyzes the diagnosed defect and generates targeted suggestions to\nrefine the profile. Furthermore, unlike existing LLM-based user simulators that\nare limited to single-round interactions, we are the first to integrate DGDPO\nwith sequential recommenders, enabling a bidirectional evolution where user\nprofiles and recommendation strategies adapt to each other over multi-round\ninteractions. Extensive experiments conducted on three real-world datasets\ndemonstrate the effectiveness of our proposed framework.",
        "url": "http://arxiv.org/abs/2508.12645v2",
        "pdf_url": "http://arxiv.org/pdf/2508.12645v2",
        "arxiv_id": "2508.12645v2",
        "authors": [
            "Hongyang Liu",
            "Zhu Sun",
            "Tianjun Wei",
            "Yan Wang",
            "Jiajie Zhu",
            "Xinghua Qu"
        ],
        "submitted": "2025-08-18 06:17:59",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper proposes a novel framework for constructing user profiles in recommender systems, using large language models. While it's related to search technologies and user behavior modeling, it's not directly focused on query understanding, ranking models, or click models, which are core areas of interest. The paper's focus on recommender systems and sequential interactions is somewhat relevant, but not a central match with the user's research themes."
    },
    {
        "title": "jXBW: Fast Substructure Search in Large-Scale JSONL Datasets for Foundation Model Applications",
        "abstract": "Substructure search in JSON Lines (JSONL) datasets is essential for modern\napplications such as prompt engineering in foundation models, but existing\nmethods suffer from prohibitive computational costs due to exhaustive tree\ntraversal and subtree matching. We present jXBW, a fast method for substructure\nsearch on large-scale JSONL datasets. Our method makes three key technical\ncontributions: (i) a merged tree representation built by merging trees of\nmultiple JSON objects while preserving individual identities, (ii) a succinct\ndata structure based on the eXtended Burrows-Wheeler Transform that enables\nefficient tree navigation and subpath search, and (iii) an efficient three-step\nsubstructure search algorithm that combines path decomposition, ancestor\ncomputation, and adaptive tree identifier collection to ensure correctness\nwhile avoiding exhaustive tree traversal. Experimental evaluation on real-world\ndatasets demonstrates that jXBW consistently outperforms existing methods,\nachieving speedups of 16$\\times$ for smaller datasets and up to 4,700$\\times$\nfor larger datasets over tree-based approaches, and more than 6$\\times$10$^6$\nover XML-based processing while maintaining competitive memory usage.",
        "url": "http://arxiv.org/abs/2508.12536v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12536v1",
        "arxiv_id": "2508.12536v1",
        "authors": [
            "Yasuo Tabei"
        ],
        "submitted": "2025-08-18 00:14:24",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on substructure search in JSONL datasets, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions foundation models, the context is not related to ranking models or user behavior modeling, and the paper's technical contributions are not applicable to the user's areas of interest."
    },
    {
        "title": "Mitigating Hallucinations in Large Language Models via Causal Reasoning",
        "abstract": "Large language models (LLMs) exhibit logically inconsistent hallucinations\nthat appear coherent yet violate reasoning principles, with recent research\nsuggesting an inverse relationship between causal reasoning capabilities and\nsuch hallucinations. However, existing reasoning approaches in LLMs, such as\nChain-of-Thought (CoT) and its graph-based variants, operate at the linguistic\ntoken level rather than modeling the underlying causal relationships between\nvariables, lacking the ability to represent conditional independencies or\nsatisfy causal identification assumptions. To bridge this gap, we introduce\ncausal-DAG construction and reasoning (CDCR-SFT), a supervised fine-tuning\nframework that trains LLMs to explicitly construct variable-level directed\nacyclic graph (DAG) and then perform reasoning over it. Moreover, we present a\ndataset comprising 25,368 samples (CausalDR), where each sample includes an\ninput question, explicit causal DAG, graph-based reasoning trace, and validated\nanswer. Experiments on four LLMs across eight tasks show that CDCR-SFT improves\nthe causal reasoning capability with the state-of-the-art 95.33% accuracy on\nCLADDER (surpassing human performance of 94.8% for the first time) and reduces\nthe hallucination on HaluEval with 10% improvements. It demonstrates that\nexplicit causal structure modeling in LLMs can effectively mitigate logical\ninconsistencies in LLM outputs. Code is available at\nhttps://github.com/MrLYG/CDCR-SFT.",
        "url": "http://arxiv.org/abs/2508.12495v1",
        "pdf_url": "http://arxiv.org/pdf/2508.12495v1",
        "arxiv_id": "2508.12495v1",
        "authors": [
            "Yuangang Li",
            "Yiqing Shen",
            "Yi Nian",
            "Jiechao Gao",
            "Ziyi Wang",
            "Chenxiao Yu",
            "Shawn Li",
            "Jie Wang",
            "Xiyang Hu",
            "Yue Zhao"
        ],
        "submitted": "2025-08-17 20:51:06",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on large language models and causal reasoning, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it touches on the topic of logical inconsistencies in language models, it does not address query understanding, ranking models, or real-time relevance optimization, which are core areas of interest."
    }
]