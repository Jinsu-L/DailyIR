# Daily Papers Report - 2025-08-13

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Adaptive Personalized Conversational Information Retrieval

- **LLM Score**: 8
- **Keyword Score**: 19
- **Authors**: Fengran Mo, Yuchen Hui, Yuxing Tian, Zhaoxuan Tan, Chuan Meng, Zhan Su, Kaiyu Huang, Jian-Yun Nie
- **URL**: <http://arxiv.org/abs/2508.08634v1>
- **Submitted**: 2025-08-12 04:53:33
- **Comment**: Accepted by CIKM 2025
- **Topic Keywords**: information retrieval, query, queries, ranking, retrieval, personalization, rank, search, trec
- **Reason**: The paper explores personalized conversational information retrieval, which aligns with your interest in Information Retrieval and Search technologies. The proposed adaptive personalization method and ranking fusion approach are relevant to your focus on query understanding and ranking models. Although the paper's focus is on conversational search, the concepts and techniques discussed can be applied to other search domains, making it somewhat related to your broader interests in NLP and data mining.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Adaptive Personalized Conversational Information Retrieval (APCIR)
- **Aim**: To develop a framework that integrates a Large Language Model (LLM) to determine the appropriate level of personalization and incorporates user profiles for query reformulation
- **Rationale**: Existing methods use a 'one-size-fits-all' personalization strategy that may lead to sub-optimal results, and adaptive personalization is necessary to identify relevant personal information for each query turn
- **Ground**: The APCIR framework consists of three main components: explicit personalization level identification, various query reformulations for personalized CIR, and personalization-aware ranking fusion
- **Experiment**: The authors evaluate the APCIR framework on two TREC iKAT datasets and demonstrate that it outperforms state-of-the-art baseline methods
- **Takeaway**: The APCIR framework is effective in incorporating different reformulated queries according to the identified need for personalization, preventing over-personalization, and aggregating ranking results of different forms of queries according to the required personalization level

#### Abstract
> Personalized conversational information retrieval (CIR) systems aim to
satisfy users' complex information needs through multi-turn interactions by
considering user profiles. However, not all search queries require
personalization. The challenge lies in appropriately incorporating
personalization elements into search when needed. Most existing studies
implicitly incorporate users' personal information and conversational context
using large language models without distinguishing the specific requirements
for each query turn. Such a ``one-size-fits-all'' personalization strategy
might lead to sub-optimal results. In this paper, we propose an adaptive
personalization method, in which we first identify the required personalization
level for a query and integrate personalized queries with other query
reformulations to produce various enhanced queries. Then, we design a
personalization-aware ranking fusion approach to assign fusion weights
dynamically to different reformulated queries, depending on the required
personalization level. The proposed adaptive personalized conversational
information retrieval framework APCIR is evaluated on two TREC iKAT datasets.
The results confirm the effectiveness of adaptive personalization of APCIR by
outperforming state-of-the-art methods.

---

### 2. Generating Query-Relevant Document Summaries via Reinforcement Learning

- **LLM Score**: 7
- **Keyword Score**: 15
- **Authors**: Nitin Yadav, Changsung Kang, Hongwei Shang, Ming Sun
- **URL**: <http://arxiv.org/abs/2508.08404v1>
- **Submitted**: 2025-08-11 18:52:28
- **Topic Keywords**: query, ranking, relevance, rag, commerce, e-commerce, rank, search
- **Reason**: The paper's focus on query-relevant document summaries and reinforcement learning aligns with your interests in Information Retrieval and Search technologies. The use of cross-encoder ranking models and large language models is also relevant to your background in e-commerce and NLP. However, the paper's specific application to e-commerce search engines and product descriptions is somewhat limited in scope compared to your broader interests in query understanding, ranking models, and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: ReLSum: A Reinforcement Learning Framework for Generating Query-Relevant Summaries in E-commerce Applications
- **Aim**: To generate concise, query-relevant summaries of product descriptions optimized for search relevance in e-commerce applications
- **Rationale**: Using product titles alone as input for cross-encoder rankers can result in suboptimal relevance predictions due to the lack of sufficient detail to capture query intent
- **Ground**: The proposed ReLSum framework uses a trainable large language model (LLM) to produce summaries, which are then used as input for a cross-encoder ranking model
- **Experiment**: The authors evaluate the effectiveness of different methods for adding product descriptions to product contexts using a dataset called the 'golden-tail dataset', and compare the performance of the proposed ReLSumGRPO and ReLSumDPO methods
- **Takeaway**: The ReLSum framework improves search relevance in e-commerce applications by generating summaries that accurately capture critical product attributes, and demonstrates statistically significant improvement in user engagement and consistent improvements across key business metrics

#### Abstract
> E-commerce search engines often rely solely on product titles as input for
ranking models with latency constraints. However, this approach can result in
suboptimal relevance predictions, as product titles often lack sufficient
detail to capture query intent. While product descriptions provide richer
information, their verbosity and length make them unsuitable for real-time
ranking, particularly for computationally expensive architectures like
cross-encoder ranking models. To address this challenge, we propose ReLSum, a
novel reinforcement learning framework designed to generate concise,
query-relevant summaries of product descriptions optimized for search
relevance. ReLSum leverages relevance scores as rewards to align the objectives
of summarization and ranking, effectively overcoming limitations of prior
methods, such as misaligned learning targets. The framework employs a trainable
large language model (LLM) to produce summaries, which are then used as input
for a cross-encoder ranking model. Experimental results demonstrate significant
improvements in offline metrics, including recall and NDCG, as well as online
user engagement metrics. ReLSum provides a scalable and efficient solution for
enhancing search relevance in large-scale e-commerce systems.

---

### 3. SciRerankBench: Benchmarking Rerankers Towards Scientific Retrieval-Augmented Generated LLMs

- **LLM Score**: 7
- **Keyword Score**: 11
- **Authors**: Haotian Chen, Qingqing Long, Meng Xiao, Xiao Luo, Wei Ju, Chengrui Wang, Xuezhi Wang, Yuanchun Zhou, Hengshu Zhu
- **URL**: <http://arxiv.org/abs/2508.08742v1>
- **Submitted**: 2025-08-12 08:36:23
- **Topic Keywords**: rerank, relevance, rag, retrieval, rank
- **Reason**: The paper focuses on reranking models in the context of scientific retrieval-augmented generated large language models (RAG-LLMs), which aligns with your interest in ranking models. Additionally, the paper's emphasis on evaluating rerankers in terms of noise resilience, relevance disambiguation, and factual consistency is relevant to your interest in query understanding and user behavior modeling. However, the paper's specific focus on scientific retrieval and LLMs may not be directly applicable to your e-commerce background.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Evaluating Rerankers in Retrieval-Augmented Generated Large Language Models (RAG-LLMs) for Scientific Literature Question Answering
- **Aim**: To provide a comprehensive evaluation of rerankers within RAG-LLMs and offer insights into their relative strengths and limitations
- **Rationale**: Existing benchmarks and datasets have limitations, such as assuming relevant context is already retrieved and neglecting the evaluation of individual components like rerankers
- **Ground**: SciRerankBench, a novel benchmark consisting of 4,500 question-context-answer pairs derived from over 250 million scholarly works, spanning five scientific subjects
- **Experiment**: Evaluating 13 widely used rerankers on 11 LLMs, categorizing rerankers into nine architectures and assessing their performance in terms of noise resilience, relevance disambiguation, and factual consistency
- **Takeaway**: The importance of evaluating individual components, considering the strengths and limitations of different architectures, and ensuring high contextual purity to avoid attention diffusion and knowledge contamination in scientific question answering pipelines

#### Abstract
> Scientific literature question answering is a pivotal step towards new
scientific discoveries. Recently, \textit{two-stage} retrieval-augmented
generated large language models (RAG-LLMs) have shown impressive advancements
in this domain. Such a two-stage framework, especially the second stage
(reranker), is particularly essential in the scientific domain, where subtle
differences in terminology may have a greatly negative impact on the final
factual-oriented or knowledge-intensive answers. Despite this significant
progress, the potential and limitations of these works remain unexplored. In
this work, we present a Scientific Rerank-oriented RAG Benchmark
(SciRerankBench), for evaluating rerankers within RAG-LLMs systems, spanning
five scientific subjects. To rigorously assess the reranker performance in
terms of noise resilience, relevance disambiguation, and factual consistency,
we develop three types of question-context-answer (Q-C-A) pairs, i.e., Noisy
Contexts (NC), Semantically Similar but Logically Irrelevant Contexts (SSLI),
and Counterfactual Contexts (CC). Through systematic evaluation of 13 widely
used rerankers on five families of LLMs, we provide detailed insights into
their relative strengths and limitations. To the best of our knowledge,
SciRerankBench is the first benchmark specifically developed to evaluate
rerankers within RAG-LLMs, which provides valuable observations and guidance
for their future development.

---

### 4. Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge

- **LLM Score**: 6
- **Keyword Score**: 9
- **Authors**: Francesco Fabbri, Gustavo Penha, Edoardo D'Amico, Alice Wang, Marco De Nadai, Jackie Doremus, Paul Gigioli, Andreas Damianou, Oskar Stal, Mounia Lalmas
- **URL**: <http://arxiv.org/abs/2508.08777v1>
- **Submitted**: 2025-08-12 09:23:35
- **Comment**: Accepted at RecSys '25
- **Topic Keywords**: pointwise, pairwise, rag, recommend
- **Reason**: The paper explores personalized podcast recommendations, leveraging Large Language Models (LLMs) as offline judges. While it touches on user behavior modeling and profile-awareness, the focus is more on recommender systems rather than query understanding, ranking models, or deep semantic understanding, which are core areas of interest for you.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Evaluating Personalized Podcast Recommendations using Large Language Models
- **Aim**: To propose a novel framework for evaluating personalized podcast recommendations using Large Language Models (LLMs) as offline judges
- **Rationale**: Traditional evaluation methods are limited in capturing true user satisfaction and explaining why a recommendation is relevant, and constructing a content hypothesis can provide an interpretable approximation of user preferences
- **Ground**: The framework consists of two stages: constructing natural-language user profiles from 90 days of listening history, and using LLMs to deliver fine-grained judgments based on the profile-episode match
- **Experiment**: A controlled study with 47 participants evaluated the performance of different judges, including LaaJ-Profile, LaaJ-History, and sBERT-Sim, and showed that LaaJ-Profile achieved comparable ROC-AUC to LaaJ-History
- **Takeaway**: The use of natural-language user profiles can provide a concise and interpretable representation of user preferences, and inferring subjective preferences from short or sparse interaction histories is challenging, with opportunities for enhancing profiles and improving judgment accuracy

#### Abstract
> Evaluating personalized recommendations remains a central challenge,
especially in long-form audio domains like podcasts, where traditional offline
metrics suffer from exposure bias and online methods such as A/B testing are
costly and operationally constrained. In this paper, we propose a novel
framework that leverages Large Language Models (LLMs) as offline judges to
assess the quality of podcast recommendations in a scalable and interpretable
manner. Our two-stage profile-aware approach first constructs natural-language
user profiles distilled from 90 days of listening history. These profiles
summarize both topical interests and behavioral patterns, serving as compact,
interpretable representations of user preferences. Rather than prompting the
LLM with raw data, we use these profiles to provide high-level, semantically
rich context-enabling the LLM to reason more effectively about alignment
between a user's interests and recommended episodes. This reduces input
complexity and improves interpretability. The LLM is then prompted to deliver
fine-grained pointwise and pairwise judgments based on the profile-episode
match. In a controlled study with 47 participants, our profile-aware judge
matched human judgments with high fidelity and outperformed or matched a
variant using raw listening histories. The framework enables efficient,
profile-aware evaluation for iterative testing and model selection in
recommender systems.

---

### 5. SPARC: Soft Probabilistic Adaptive multi-interest Retrieval Model via Codebooks for recommender system

- **LLM Score**: 6
- **Keyword Score**: 4
- **Authors**: Jialiang Shi, Yaguang Dou, Tian Qi
- **URL**: <http://arxiv.org/abs/2508.09090v2>
- **Submitted**: 2025-08-12 17:16:37
- **Comment**: 8 pages
- **Topic Keywords**: retrieval, recommend, search
- **Reason**: The paper proposes a novel retrieval framework for recommender systems, addressing challenges in modeling multi-interests and online inference. While it's related to information retrieval and search technologies, the focus is on recommender systems, which is a secondary interest. The paper's emphasis on real-time relevance optimization and user behavior modeling is relevant to my interests, but the specific application in recommender systems limits its alignment with my core research themes.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Multi-interest Retrieval Methods for Recommender Systems
- **Aim**: To address the limitations of existing multi-interest retrieval methods, including the failure to dynamically evolve with users' preferences, lack of proactive exploration, and ineffective mining of multi-interests for users with few historical interactions
- **Rationale**: The proposed SPARC framework utilizes a Residual Quantized Variational Autoencoder (RQ-VAE) to construct a learnable, discretized interest space, achieving end-to-end joint training with an industrial large-scale recommendation model
- **Ground**: The SPARC framework consists of four parts: RQ-VAE and Two-Tower Model, Representation Alignment Losses, Interest Disentanglement Loss, and Probabilistic Interest Model, which enables the learning of high-quality interest representations and balances interest exploration and exploitation
- **Experiment**: The experimental design evaluates the effectiveness of the SPARC framework on the 2023 version of the public Amazon Product Data - Books dataset, using metrics such as Recall@K, NDCG@K, MRR, Coverage@K, ILD@K, and a Long-tail Analysis
- **Takeaway**: The results show that SPARC outperforms baseline models, enhances recommendation novelty and discovers long-tail content, and achieves substantial gains in business metrics, demonstrating its effectiveness in improving the performance of a recommender system

#### Abstract
> Modeling multi-interests has arisen as a core problem in real-world RS.
Current multi-interest retrieval methods pose three major challenges: 1)
Interests, typically extracted from predefined external knowledge, are
invariant. Failed to dynamically evolve with users' real-time consumption
preferences. 2) Online inference typically employs an over-exploited strategy,
mainly matching users' existing interests, lacking proactive exploration and
discovery of novel and long-tail interests. To address these challenges, we
propose a novel retrieval framework named SPARC(Soft Probabilistic Adaptive
Retrieval Model via Codebooks). Our contribution is two folds. First, the
framework utilizes Residual Quantized Variational Autoencoder (RQ-VAE) to
construct a discretized interest space. It achieves joint training of the
RQ-VAE with the industrial large scale recommendation model, mining
behavior-aware interests that can perceive user feedback and evolve
dynamically. Secondly, a probabilistic interest module that predicts the
probability distribution over the entire dynamic and discrete interest space.
This facilitates an efficient "soft-search" strategy during online inference,
revolutionizing the retrieval paradigm from "passive matching" to "proactive
exploration" and thereby effectively promoting interest discovery. Online A/B
tests on an industrial platform with tens of millions daily active users, have
achieved substantial gains in business metrics: +0.9% increase in user view
duration, +0.4% increase in user page views (PV), and a +22.7% improvement in
PV500(new content reaching 500 PVs in 24 hours). Offline evaluations are
conducted on open-source Amazon Product datasets. Metrics, such as Recall@K and
Normalized Discounted Cumulative Gain@K(NDCG@K), also showed consistent
improvement. Both online and offline experiments validate the efficacy and
practical value of the proposed method.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Using LLMs to Capture Users' Temporal Context for Recommendation

- **LLM Score**: 6
- **Keyword Score**: 1
- **Authors**: Milad Sabouri, Masoud Mansoury, Kun Lin, Bamshad Mobasher
- **URL**: <http://arxiv.org/abs/2508.08512v1>
- **Submitted**: 2025-08-11 22:48:31
- **Topic Keywords**: recommend
- **Reason**: The paper explores the use of Large Language Models (LLMs) for generating user profiles, which is related to query understanding and user behavior modeling in Information Retrieval. However, the focus on recommender systems and user profiling is not directly aligned with the user's primary interest in Information Retrieval, especially in areas that require deep semantic understanding and real-time relevance optimization.

#### Abstract
> Effective recommender systems demand dynamic user understanding, especially
in complex, evolving environments. Traditional user profiling often fails to
capture the nuanced, temporal contextual factors of user preferences, such as
transient short-term interests and enduring long-term tastes. This paper
presents an assessment of Large Language Models (LLMs) for generating
semantically rich, time-aware user profiles. We do not propose a novel
end-to-end recommendation architecture; instead, the core contribution is a
systematic investigation into the degree of LLM effectiveness in capturing the
dynamics of user context by disentangling short-term and long-term preferences.
This approach, framing temporal preferences as dynamic user contexts for
recommendations, adaptively fuses these distinct contextual components into
comprehensive user embeddings. The evaluation across Movies&TV and Video Games
domains suggests that while LLM-generated profiles offer semantic depth and
temporal structure, their effectiveness for context-aware recommendations is
notably contingent on the richness of user interaction histories. Significant
gains are observed in dense domains (e.g., Movies&TV), whereas improvements are
less pronounced in sparse environments (e.g., Video Games). This work
highlights LLMs' nuanced potential in enhancing user profiling for adaptive,
context-aware recommendations, emphasizing the critical role of dataset
characteristics for practical applicability.

### 7. E3-Rewrite: Learning to Rewrite SQL for Executability, Equivalence,and Efficiency

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Dongjie Xu, Yue Cui, Weijie Shi, Qingzhi Ma, Hanghui Guo, Jiaming Li, Yao Zhao, Ruiyuan Zhang, Shimin Di, Jia Zhu, Kai Zheng, Jiajie Xu
- **URL**: <http://arxiv.org/abs/2508.09023v1>
- **Submitted**: 2025-08-12 15:38:10
- **Topic Keywords**: query, queries, rag
- **Reason**: The paper focuses on SQL query rewriting, which is not directly related to Information Retrieval or Search technologies. While it uses large language models, the context is different from query understanding, ranking models, or user behavior modeling. The paper's emphasis on execution awareness and semantic grounding is interesting, but it does not align with the user's primary research interests in IR and NLP.

#### Abstract
> SQL query rewriting aims to reformulate a query into a more efficient form
while preserving equivalence. Most existing methods rely on predefined rewrite
rules. However, such rule-based approaches face fundamental limitations: (1)
fixed rule sets generalize poorly to novel query patterns and struggle with
complex queries; (2) a wide range of effective rewriting strategies cannot be
fully captured by declarative rules. To overcome these issues, we propose using
large language models (LLMs) to generate rewrites. LLMs can capture complex
strategies, such as evaluation reordering and CTE rewriting. Despite this
potential, directly applying LLMs often results in suboptimal or non-equivalent
rewrites due to a lack of execution awareness and semantic grounding. To
address these challenges, We present E3-Rewrite, an LLM-based SQL rewriting
framework that produces executable, equivalent, and efficient queries. It
integrates two core components: a context construction module and a
reinforcement learning framework. First, the context module leverages execution
plans and retrieved demonstrations to build bottleneck-aware prompts that guide
inference-time rewriting. Second, we design a reward function targeting
executability, equivalence, and efficiency, evaluated via syntax checks,
equivalence verification, and cost estimation. Third, to ensure stable
multi-objective learning, we adopt a staged curriculum that first emphasizes
executability and equivalence, then gradually incorporates efficiency.
Extensive experiments show that E3-Rewrite achieves up to a 25.6\% reduction in
query execution time compared to state-of-the-art methods across multiple SQL
benchmarks. Moreover, it delivers up to 24.4\% more successful rewrites,
expanding coverage to complex queries that previous systems failed to handle.

### 8. Quick on the Uptake: Eliciting Implicit Intents from Human Demonstrations for Personalized Mobile-Use Agents

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Zheng Wu, Heyuan Huang, Yanjia Yang, Yuanyi Song, Xingyu Lou, Weiwen Liu, Weinan Zhang, Jun Wang, Zhuosheng Zhang
- **URL**: <http://arxiv.org/abs/2508.08645v1>
- **Submitted**: 2025-08-12 05:20:14
- **Topic Keywords**: query, rag, retrieval
- **Reason**: The paper explores the concept of implicit intent flows in mobile-use agents, which is somewhat related to query understanding and user behavior modeling in Information Retrieval. However, the focus on mobile-use agents and human demonstrations is not directly aligned with the user's primary research interests in IR and NLP.

#### Abstract
> As multimodal large language models advance rapidly, the automation of mobile
tasks has become increasingly feasible through the use of mobile-use agents
that mimic human interactions from graphical user interface. To further enhance
mobile-use agents, previous studies employ demonstration learning to improve
mobile-use agents from human demonstrations. However, these methods focus
solely on the explicit intention flows of humans (e.g., step sequences) while
neglecting implicit intention flows (e.g., personal preferences), which makes
it difficult to construct personalized mobile-use agents. In this work, to
evaluate the \textbf{I}ntention \textbf{A}lignment \textbf{R}ate between
mobile-use agents and humans, we first collect \textbf{MobileIAR}, a dataset
containing human-intent-aligned actions and ground-truth actions. This enables
a comprehensive assessment of the agents' understanding of human intent. Then
we propose \textbf{IFRAgent}, a framework built upon \textbf{I}ntention
\textbf{F}low \textbf{R}ecognition from human demonstrations. IFRAgent analyzes
explicit intention flows from human demonstrations to construct a query-level
vector library of standard operating procedures (SOP), and analyzes implicit
intention flows to build a user-level habit repository. IFRAgent then leverages
a SOP extractor combined with retrieval-augmented generation and a query
rewriter to generate personalized query and SOP from a raw ambiguous query,
enhancing the alignment between mobile-use agents and human intent.
Experimental results demonstrate that IFRAgent outperforms baselines by an
average of 6.79\% (32.06\% relative improvement) in human intention alignment
rate and improves step completion rates by an average of 5.30\% (26.34\%
relative improvement). The codes are available at
https://github.com/MadeAgents/Quick-on-the-Uptake.

### 9. Privacy-protected Retrieval-Augmented Generation for Knowledge Graph Question Answering

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Yunfeng Ning, Mayi Xu, Jintao Wen, Qiankun Pi, Yuanyuan Zhu, Ming Zhong, Jiawei Jiang, Tieyun Qian
- **URL**: <http://arxiv.org/abs/2508.08785v1>
- **Submitted**: 2025-08-12 09:38:21
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper's focus on knowledge graph question answering and leveraging external knowledge to improve language model performance is somewhat related to my interests in information retrieval and query understanding. However, the emphasis on privacy protection and anonymization of entities in knowledge graphs is not directly aligned with my research themes, and the paper's relevance to my interests is limited.

#### Abstract
> LLMs often suffer from hallucinations and outdated or incomplete knowledge.
RAG is proposed to address these issues by integrating external knowledge like
that in KGs into LLMs. However, leveraging private KGs in RAG systems poses
significant privacy risks due to the black-box nature of LLMs and potential
insecure data transmission, especially when using third-party LLM APIs lacking
transparency and control. In this paper, we investigate the privacy-protected
RAG scenario for the first time, where entities in KGs are anonymous for LLMs,
thus preventing them from accessing entity semantics. Due to the loss of
semantics of entities, previous RAG systems cannot retrieve question-relevant
knowledge from KGs by matching questions with the meaningless identifiers of
anonymous entities. To realize an effective RAG system in this scenario, two
key challenges must be addressed: (1) How can anonymous entities be converted
into retrievable information. (2) How to retrieve question-relevant anonymous
entities. Hence, we propose a novel ARoG framework including relation-centric
abstraction and structure-oriented abstraction strategies. For challenge (1),
the first strategy abstracts entities into high-level concepts by dynamically
capturing the semantics of their adjacent relations. It supplements meaningful
semantics which can further support the retrieval process. For challenge (2),
the second strategy transforms unstructured natural language questions into
structured abstract concept paths. These paths can be more effectively aligned
with the abstracted concepts in KGs, thereby improving retrieval performance.
To guide LLMs to effectively retrieve knowledge from KGs, the two strategies
strictly protect privacy from being exposed to LLMs. Experiments on three
datasets demonstrate that ARoG achieves strong performance and
privacy-robustness.

### 10. Mitigating Popularity Bias in Counterfactual Explanations using Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Arjan Hasami, Masoud Mansoury
- **URL**: <http://arxiv.org/abs/2508.08946v1>
- **Submitted**: 2025-08-12 13:57:36
- **Topic Keywords**: rag, recommend
- **Reason**: The paper explores counterfactual explanations, a topic related to information retrieval, but it does not directly address query understanding, ranking models, or user behavior modeling. The focus on large language models and popularity bias is somewhat relevant to NLP and data mining, but the paper's scope is narrower than the user's interests.

#### Abstract
> Counterfactual explanations (CFEs) offer a tangible and actionable way to
explain recommendations by showing users a "what-if" scenario that demonstrates
how small changes in their history would alter the system's output. However,
existing CFE methods are susceptible to bias, generating explanations that
might misalign with the user's actual preferences. In this paper, we propose a
pre-processing step that leverages large language models to filter
out-of-character history items before generating an explanation. In experiments
on two public datasets, we focus on popularity bias and apply our approach to
ACCENT, a neural CFE framework. We find that it creates counterfactuals that
are more closely aligned with each user's popularity preferences than ACCENT
alone.

### 11. TiMoE: Time-Aware Mixture of Language Experts

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Robin Faro, Dongyang Fan, Tamar Alphaidze, Martin Jaggi
- **URL**: <http://arxiv.org/abs/2508.08827v1>
- **Submitted**: 2025-08-12 10:36:36
- **Topic Keywords**: query
- **Reason**: The paper focuses on pre-training language models on disjoint time slices to address temporal leakage, which is an interesting problem. However, it does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval. The paper's relevance to Information Retrieval is limited to its use of language models, but the specific application and techniques used are not directly applicable to IR.

#### Abstract
> Large language models (LLMs) are typically trained on fixed snapshots of the
web, which means that their knowledge becomes stale and their predictions risk
temporal leakage: relying on information that lies in the future relative to a
query. We tackle this problem by pre-training from scratch a set of GPT-style
experts on disjoint two-year slices of a 2013-2024 corpus and combining them
through TiMoE, a Time-aware Mixture of Language Experts. At inference time,
TiMoE masks all experts whose training window ends after the query timestamp
and merges the remaining log-probabilities in a shared space, guaranteeing
strict causal validity while retaining the breadth of multi-period knowledge.
We also release TSQA, a 10k-question benchmark whose alternatives are
explicitly labelled as past, future or irrelevant, allowing fine-grained
measurement of temporal hallucinations. Experiments on eight standard NLP tasks
plus TSQA show that a co-adapted TiMoE variant matches or exceeds the best
single-period expert and cuts future-knowledge errors by up to 15%. Our results
demonstrate that modular, time-segmented pre-training paired with causal
routing is a simple yet effective path toward LLMs that stay chronologically
grounded without sacrificing general performance much. We open source our code
at TiMoE (Github): https://github.com/epfml/TiMoE

### 12. Steerable Pluralism: Pluralistic Alignment via Few-Shot Comparative Regression

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Jadie Adams, Brian Hu, Emily Veenhuis, David Joy, Bharadwaj Ravichandran, Aaron Bray, Anthony Hoogs, Arslan Basharat
- **URL**: <http://arxiv.org/abs/2508.08509v1>
- **Submitted**: 2025-08-11 22:40:31
- **Comment**: AIES '25: Proceedings of the 2025 AAAI/ACM Conference on AI, Ethics,
  and Society
- **Topic Keywords**: rag, search
- **Reason**: The paper explores the concept of pluralistic alignment in large language models, which is a topic in NLP. However, the focus on few-shot comparative regression and value-aligned decision-making is not directly related to information retrieval, query understanding, or ranking models, which are the user's primary research interests.

#### Abstract
> Large language models (LLMs) are currently aligned using techniques such as
reinforcement learning from human feedback (RLHF). However, these methods use
scalar rewards that can only reflect user preferences on average. Pluralistic
alignment instead seeks to capture diverse user preferences across a set of
attributes, moving beyond just helpfulness and harmlessness. Toward this end,
we propose a steerable pluralistic model based on few-shot comparative
regression that can adapt to individual user preferences. Our approach
leverages in-context learning and reasoning, grounded in a set of fine-grained
attributes, to compare response options and make aligned choices. To evaluate
our algorithm, we also propose two new steerable pluralistic benchmarks by
adapting the Moral Integrity Corpus (MIC) and the HelpSteer2 datasets,
demonstrating the applicability of our approach to value-aligned
decision-making and reward modeling, respectively. Our few-shot comparative
regression approach is interpretable and compatible with different attributes
and LLMs, while outperforming multiple baseline and state-of-the-art methods.
Our work provides new insights and research directions in pluralistic
alignment, enabling a more fair and representative use of LLMs and advancing
the state-of-the-art in ethical AI.

### 13. Temporal User Profiling with LLMs: Balancing Short-Term and Long-Term Preferences for Recommendations

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Milad Sabouri, Masoud Mansoury, Kun Lin, Bamshad Mobasher
- **URL**: <http://arxiv.org/abs/2508.08454v1>
- **Submitted**: 2025-08-11 20:28:24
- **Topic Keywords**: rag, recommend
- **Reason**: The paper explores user profiling and recommendation systems, which is related to information retrieval and search technologies. However, the focus on recommender systems and natural language processing is not directly aligned with the user's primary interest in query understanding, ranking models, and user behavior modeling. The paper's emphasis on long-term and short-term preferences and temporal user profiling is somewhat relevant, but not a central match for the user's research themes.

#### Abstract
> Accurately modeling user preferences is crucial for improving the performance
of content-based recommender systems. Existing approaches often rely on
simplistic user profiling methods, such as averaging or concatenating item
embeddings, which fail to capture the nuanced nature of user preference
dynamics, particularly the interactions between long-term and short-term
preferences. In this work, we propose LLM-driven Temporal User Profiling
(LLM-TUP), a novel method for user profiling that explicitly models short-term
and long-term preferences by leveraging interaction timestamps and generating
natural language representations of user histories using a large language model
(LLM). These representations are encoded into high-dimensional embeddings using
a pre-trained BERT model, and an attention mechanism is applied to dynamically
fuse the short-term and long-term embeddings into a comprehensive user profile.
Experimental results on real-world datasets demonstrate that LLM-TUP achieves
substantial improvements over several baselines, underscoring the effectiveness
of our temporally aware user-profiling approach and the use of semantically
rich user profiles, generated by LLMs, for personalized content-based
recommendation.

### 14. Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language Models

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Wen Wang, Bozhen Fang, Chenchen Jing, Yongliang Shen, Yangyi Shen, Qiuyu Wang, Hao Ouyang, Hao Chen, Chunhua Shen
- **URL**: <http://arxiv.org/abs/2508.09138v1>
- **Submitted**: 2025-08-12 17:59:57
- **Comment**: Project webpage: https://aim-uofa.github.io/dLLM-MidTruth
- **Topic Keywords**: rag
- **Reason**: The paper explores the temporal dynamics of diffusion language models, which is an interesting topic in NLP. However, it does not directly relate to information retrieval, query understanding, ranking models, or user behavior modeling, which are the user's core research interests.

#### Abstract
> Diffusion large language models (dLLMs) generate text through iterative
denoising, yet current decoding strategies discard rich intermediate
predictions in favor of the final output. Our work here reveals a critical
phenomenon, temporal oscillation, where correct answers often emerge in the
middle process, but are overwritten in later denoising steps. To address this
issue, we introduce two complementary methods that exploit temporal
consistency: 1) Temporal Self-Consistency Voting, a training-free, test-time
decoding strategy that aggregates predictions across denoising steps to select
the most consistent output; and 2) a post-training method termed Temporal
Consistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a
measure of semantic stability across intermediate predictions, as a reward
signal to encourage stable generations. Empirical results across multiple
benchmarks demonstrate the effectiveness of our approach. Using the negative
TSE reward alone, we observe a remarkable average improvement of 24.7% on the
Countdown dataset over an existing dLLM. Combined with the accuracy reward, we
achieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and
25.3% on Countdown, respectively. Our findings underscore the untapped
potential of temporal dynamics in dLLMs and offer two simple yet effective
tools to harness them.

### 15. Utilizing Multilingual Encoders to Improve Large Language Models for Low-Resource Languages

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Imalsha Puranegedara, Themira Chathumina, Nisal Ranathunga, Nisansa de Silva, Surangika Ranathunga, Mokanarangan Thayaparan
- **URL**: <http://arxiv.org/abs/2508.09091v1>
- **Submitted**: 2025-08-12 17:17:13
- **Topic Keywords**: rag
- **Reason**: The paper focuses on improving large language models for low-resource languages, using multilingual encoders, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on NLP, the specific application and techniques used are not aligned with the user's primary research interests.

#### Abstract
> Large Language Models (LLMs) excel in English, but their performance degrades
significantly on low-resource languages (LRLs) due to English-centric training.
While methods like LangBridge align LLMs with multilingual encoders such as the
Massively Multilingual Text-to-Text Transfer Transformer (mT5), they typically
use only the final encoder layer. We propose a novel architecture that fuses
all intermediate layers, enriching the linguistic information passed to the
LLM. Our approach features two strategies: (1) a Global Softmax weighting for
overall layer importance, and (2) a Transformer Softmax model that learns
token-specific weights. The fused representations are mapped into the LLM's
embedding space, enabling it to process multilingual inputs. The model is
trained only on English data, without using any parallel or multilingual data.
Evaluated on XNLI, IndicXNLI, Sinhala News Classification, and Amazon Reviews,
our Transformer Softmax model significantly outperforms the LangBridge
baseline. We observe strong performance gains in LRLs, improving Sinhala
classification accuracy from 71.66% to 75.86% and achieving clear improvements
across Indic languages such as Tamil, Bengali, and Malayalam. These specific
gains contribute to an overall boost in average XNLI accuracy from 70.36% to
71.50%. This approach offers a scalable, data-efficient path toward more
capable and equitable multilingual LLMs.

### 16. Jointly Generating and Attributing Answers using Logits of Document-Identifier Tokens

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Lucas Albarede, Jose Moreno, Lynda Tamine, Luce Lefeuvre
- **URL**: <http://arxiv.org/abs/2508.08942v1>
- **Submitted**: 2025-08-12 13:50:25
- **Topic Keywords**: rag
- **Reason**: The paper focuses on Large Language Models and their limitations, specifically hallucination, which is not directly related to Information Retrieval or Search technologies. While it mentions token logits, it's unclear how this relates to query understanding, ranking models, or user behavior modeling. The paper's primary focus is on answer generation and attribution, which is more relevant to Natural Language Processing and text generation.

#### Abstract
> Despite their impressive performances, Large Language Models (LLMs) remain
prone to hallucination, which critically undermines their trustworthiness.
While most of the previous work focused on tackling answer and attribution
correctness, a recent line of work investigated faithfulness, with a focus on
leveraging internal model signals to reflect a model's actual decision-making
process while generating the answer. Nevertheless, these methods induce
additional latency and have shown limitations in directly aligning token
generation with attribution generation. In this paper, we introduce LoDIT, a
method that jointly generates and faithfully attributes answers in RAG by
leveraging specific token logits during generation. It consists of two steps:
(1) marking the documents with specific token identifiers and then leveraging
the logits of these tokens to estimate the contribution of each document to the
answer during generation, and (2) aggregating these contributions into document
attributions. Experiments on a trustworthiness-focused attributed
text-generation benchmark, Trust-Align, show that LoDIT significantly
outperforms state-of-the-art models on several metrics. Finally, an in-depth
analysis of LoDIT shows both its efficiency in terms of latency and its
robustness in different settings.

### 17. Recent Advances and Trends in Research Paper Recommender Systems: A Comprehensive Survey

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Iratxe Pinedo, Mikel Larra√±aga, Ana Arruarte
- **URL**: <http://arxiv.org/abs/2508.08828v1>
- **Submitted**: 2025-08-12 10:36:41
- **Topic Keywords**: recommend, search
- **Reason**: The paper focuses on Research Paper Recommender Systems, which is a related topic to Information Retrieval and Search technologies. However, the emphasis is on recommender systems rather than query understanding, ranking models, or user behavior modeling, which are the user's primary research interests. The paper's scope is also limited to a specific domain (research papers) and does not explore deep semantic understanding or real-time relevance optimization.

#### Abstract
> As the volume of scientific publications grows exponentially, researchers
increasingly face difficulties in locating relevant literature. Research Paper
Recommender Systems have become vital tools to mitigate this information
overload by delivering personalized suggestions. This survey provides a
comprehensive analysis of Research Paper Recommender Systems developed between
November 2021 and December 2024, building upon prior reviews in the field. It
presents an extensive overview of the techniques and approaches employed, the
datasets utilized, the evaluation metrics and procedures applied, and the
status of both enduring and emerging challenges observed during the research.
Unlike prior surveys, this survey goes beyond merely cataloguing techniques and
models, providing a thorough examination of how these methods are implemented
across different stages of the recommendation process. By furnishing a detailed
and structured reference, this work aims to function as a consultative resource
for the research community, supporting informed decision-making and guiding
future investigations in the advances of effective Research Paper Recommender
Systems.

### 18. LLM driven Text-to-Table Generation through Sub-Tasks Guidance and Iterative Refinement

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Rajmohan C, Sarthak Harne, Arvind Agarwal
- **URL**: <http://arxiv.org/abs/2508.08653v1>
- **Submitted**: 2025-08-12 05:37:12
- **Topic Keywords**: rag
- **Reason**: The paper explores the application of Large Language Models (LLMs) in text-to-table generation, which is a task that requires semantic understanding and reasoning. While it touches on some aspects of query understanding and ranking models, it is not directly related to the user's primary focus on information retrieval, especially in areas that require deep semantic understanding and real-time relevance optimization. The paper's focus on NLP and data mining is somewhat relevant, but it does not align with the user's specific interests in search technologies and user behavior modeling.

#### Abstract
> Transforming unstructured text into structured data is a complex task,
requiring semantic understanding, reasoning, and structural comprehension.
While Large Language Models (LLMs) offer potential, they often struggle with
handling ambiguous or domain-specific data, maintaining table structure,
managing long inputs, and addressing numerical reasoning. This paper proposes
an efficient system for LLM-driven text-to-table generation that leverages
novel prompting techniques. Specifically, the system incorporates two key
strategies: breaking down the text-to-table task into manageable, guided
sub-tasks and refining the generated tables through iterative self-feedback. We
show that this custom task decomposition allows the model to address the
problem in a stepwise manner and improves the quality of the generated table.
Furthermore, we discuss the benefits and potential risks associated with
iterative self-feedback on the generated tables while highlighting the
trade-offs between enhanced performance and computational cost. Our methods
achieve strong results compared to baselines on two complex text-to-table
generation datasets available in the public domain.

### 19. Re:Verse -- Can Your VLM Read a Manga?

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Aaditya Baranwal, Madhav Kataria, Naitik Agrawal, Yogesh S Rawat, Shruti Vyas
- **URL**: <http://arxiv.org/abs/2508.08508v1>
- **Submitted**: 2025-08-11 22:40:05
- **Topic Keywords**: retrieval
- **Reason**: The paper explores the limitations of Vision Language Models (VLMs) in understanding sequential visual storytelling, which is a topic related to query understanding and ranking models in Information Retrieval. However, the focus on manga narrative understanding and multimodal models is not directly aligned with the user's primary research interests in search technologies and user behavior modeling.

#### Abstract
> Current Vision Language Models (VLMs) demonstrate a critical gap between
surface-level recognition and deep narrative reasoning when processing
sequential visual storytelling. Through a comprehensive investigation of manga
narrative understanding, we reveal that while recent large multimodal models
excel at individual panel interpretation, they systematically fail at temporal
causality and cross-panel cohesion, core requirements for coherent story
comprehension. We introduce a novel evaluation framework that combines
fine-grained multimodal annotation, cross-modal embedding analysis, and
retrieval-augmented assessment to systematically characterize these
limitations.
  Our methodology includes (i) a rigorous annotation protocol linking visual
elements to narrative structure through aligned light novel text, (ii)
comprehensive evaluation across multiple reasoning paradigms, including direct
inference and retrieval-augmented generation, and (iii) cross-modal similarity
analysis revealing fundamental misalignments in current VLMs' joint
representations. Applying this framework to Re:Zero manga across 11 chapters
with 308 annotated panels, we conduct the first systematic study of long-form
narrative understanding in VLMs through three core evaluation axes: generative
storytelling, contextual dialogue grounding, and temporal reasoning. Our
findings demonstrate that current models lack genuine story-level intelligence,
struggling particularly with non-linear narratives, character consistency, and
causal inference across extended sequences. This work establishes both the
foundation and practical methodology for evaluating narrative intelligence,
while providing actionable insights into the capability of deep sequential
understanding of Discrete Visual Narratives beyond basic recognition in
Multimodal Models.

### 20. Optimizing Retrieval-Augmented Generation (RAG) for Colloquial Cantonese: A LoRA-Based Systematic Review

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: David Santandreu Calonge, Linda Smail
- **URL**: <http://arxiv.org/abs/2508.08610v1>
- **Submitted**: 2025-08-12 03:46:16
- **Comment**: 27 pages, 1 figure, 8 tables
- **Topic Keywords**: rag, retrieval, personalization, rank
- **Reason**: The paper focuses on optimizing Retrieval-Augmented Generation (RAG) systems for Colloquial Cantonese, which is not directly related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper's emphasis on Parameter-Efficient Fine-Tuning (PEFT) and Low-Rank Adaptation (LoRA) is also not aligned with the user's interests in NLP, data mining, and related topics.

#### Abstract
> This review examines recent advances in Parameter-Efficient Fine-Tuning
(PEFT), with a focus on Low-Rank Adaptation (LoRA), to optimize
Retrieval-Augmented Generation (RAG) systems like Qwen3, DeepSeek, and Kimi.
These systems face challenges in understanding and generating authentic
Cantonese colloquial expressions due to limited annotated data and linguistic
variability. The review evaluates the integration of LoRA within RAG
frameworks, benchmarks PEFT methods for retrieval and generation accuracy,
identify domain adaptation strategies under limited data, and compares
fine-tuning techniques aimed at improving semantic fidelity under data-scarce
conditions. A systematic analysis of recent studies employing diverse LoRA
variants, synthetic data generation, user feedback integration, and adaptive
parameter allocation was conducted to assess their impact on computational
efficiency, retrieval precision, linguistic authenticity, and scalability.
Findings reveal that dynamic and ensemble LoRA adaptations significantly reduce
trainable parameters without sacrificing retrieval accuracy and generation
quality in dialectal contexts. However, limitations remain in fully preserving
fine-grained linguistic nuances, especially for low-resource settings like
Cantonese. The integration of real-time user feedback and domain-specific data
remains underdeveloped, limiting model adaptability and personalization. While
selective parameter freezing and nonlinear adaptation methods offer better
trade-offs between efficiency and accuracy, their robustness at scale remains
an open challenge. This review highlights the promise of PEFT-enhanced RAG
systems for domain-specific language tasks and calls for future work targeting
dialectal authenticity, dynamic adaptation, and scalable fine-tuning pipelines.

### 21. READER: Retrieval-Assisted Drafter for Efficient LLM Inference

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Maxim Divilkovskiy, Vitaly Malygin, Sergey Zlobin, Sultan Isali, Vasily Kalugin, Stanislav Ilyushin, Nuriza Aitassova, Yi Fei, Zeng Weidi
- **URL**: <http://arxiv.org/abs/2508.09072v1>
- **Submitted**: 2025-08-12 16:47:48
- **Topic Keywords**: rag, retrieval, search
- **Reason**: The paper focuses on Large Language Models (LLMs) and efficient inference, which is not directly related to Information Retrieval, Search technologies, or query understanding. Although it mentions 'retrieval-augmented generation', the context is different from the user's interests in traditional IR and search.

#### Abstract
> Large Language Models (LLMs) generate tokens autoregressively, with each
token depending on the preceding context. This sequential nature makes the
inference process inherently difficult to accelerate, posing a significant
challenge for efficient deployment. In recent years, various methods have been
proposed to address this issue, with the most effective approaches often
involving the training of additional draft models. In this paper, we introduce
READER (Retrieval-Assisted Drafter for Efficient LLM Inference), a novel
lossless speculative decoding method that enhances model-based approaches by
leveraging self-repetitions in the text. Our algorithm expands the speculative
decoding tree using tokens obtained through statistical search. This work
focuses on large batch sizes (>= 8), an underexplored yet important area for
industrial applications. We also analyze the key-value (KV) cache size during
speculative decoding and propose an optimization to improve performance for
large batches. As a result, READER outperforms existing speculative decoding
methods. Notably, READER requires no additional training and can reuse
pre-trained speculator models, increasing the speedup by over 40\%. Our method
demonstrates particularly strong performance on search-based tasks, such as
retrieval-augmented generation, where we achieve more than 10x speedup.

### 22. Munsit at NADI 2025 Shared Task 2: Pushing the Boundaries of Multidialectal Arabic ASR with Weakly Supervised Pretraining and Continual Supervised Fine-tuning

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Mahmoud Salhab, Shameed Sait, Mohammad Abusheikh, Hasan Abusheikh
- **URL**: <http://arxiv.org/abs/2508.08912v1>
- **Submitted**: 2025-08-12 13:02:22
- **Topic Keywords**: ranking, rank
- **Reason**: The paper focuses on Automatic Speech Recognition (ASR) for Arabic languages, which is not directly related to Information Retrieval, Search technologies, or Natural Language Processing. Although it mentions weak supervision and fine-tuning, the context is specific to ASR and does not align with the user's research interests.

#### Abstract
> Automatic speech recognition (ASR) plays a vital role in enabling natural
human-machine interaction across applications such as virtual assistants,
industrial automation, customer support, and real-time transcription. However,
developing accurate ASR systems for low-resource languages like Arabic remains
a significant challenge due to limited labeled data and the linguistic
complexity introduced by diverse dialects. In this work, we present a scalable
training pipeline that combines weakly supervised learning with supervised
fine-tuning to develop a robust Arabic ASR model. In the first stage, we
pretrain the model on 15,000 hours of weakly labeled speech covering both
Modern Standard Arabic (MSA) and various Dialectal Arabic (DA) variants. In the
subsequent stage, we perform continual supervised fine-tuning using a mixture
of filtered weakly labeled data and a small, high-quality annotated dataset.
Our approach achieves state-of-the-art results, ranking first in the
multi-dialectal Arabic ASR challenge. These findings highlight the
effectiveness of weak supervision paired with fine-tuning in overcoming data
scarcity and delivering high-quality ASR for low-resource, dialect-rich
languages.

### 23. ASPD: Unlocking Adaptive Serial-Parallel Decoding by Exploring Intrinsic Parallelism in LLMs

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Keyu Chen, Zhifeng Shen, Daohai Yu, Haoqian Wu, Wei Wen, Jianfeng He, Ruizhi Qiao, Xing Sun
- **URL**: <http://arxiv.org/abs/2508.08895v1>
- **Submitted**: 2025-08-12 12:35:55
- **Comment**: 20 pages, 9 figures
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper is not directly related to Information Retrieval (IR) or Search technologies, and does not address query understanding, ranking models, or user behavior modeling. The focus is on parallelizing the inference process of large language models, which is a topic in Natural Language Processing (NLP) but not directly relevant to the user's research interests.

#### Abstract
> The increasing scale and complexity of large language models (LLMs) pose
significant inference latency challenges, primarily due to their autoregressive
decoding paradigm characterized by the sequential nature of next-token
prediction. By re-examining the outputs of autoregressive models, we observed
that some segments exhibit parallelizable structures, which we term intrinsic
parallelism. Decoding each parallelizable branch simultaneously (i.e. parallel
decoding) can significantly improve the overall inference speed of LLMs. In
this paper, we propose an Adaptive Serial-Parallel Decoding (ASPD), which
addresses two core challenges: automated construction of parallelizable data
and efficient parallel decoding mechanism. More specifically, we introduce a
non-invasive pipeline that automatically extracts and validates parallelizable
structures from the responses of autoregressive models. To empower efficient
adaptive serial-parallel decoding, we implement a Hybrid Decoding Engine which
enables seamless transitions between serial and parallel decoding modes while
maintaining a reusable KV cache, maximizing computational efficiency. Extensive
evaluations across General Tasks, Retrieval-Augmented Generation, Mathematical
Reasoning, demonstrate that ASPD achieves unprecedented performance in both
effectiveness and efficiency. Notably, on Vicuna Bench, our method achieves up
to 3.19x speedup (1.85x on average) while maintaining response quality within
1% difference compared to autoregressive models, realizing significant
acceleration without compromising generation quality. Our framework sets a
groundbreaking benchmark for efficient LLM parallel inference, paving the way
for its deployment in latency-sensitive applications such as AI-powered
customer service bots and answer retrieval engines.

### 24. Comprehensive Comparison Network: a framework for locality-aware, routes-comparable and interpretable route recommendation

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Chao Chen, Longfei Xu, Hanyu Guo, Chengzhang Wang, Ying Wang, Kaikui Liu, Xiangxiang Chu
- **URL**: <http://arxiv.org/abs/2508.08745v1>
- **Submitted**: 2025-08-12 08:40:52
- **Topic Keywords**: query, recommend
- **Reason**: The paper focuses on route recommendation, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions some features like user features and item embedding, the context and methodology are not aligned with the user's research interests.

#### Abstract
> Route recommendation (RR) is a core task of route planning in the Amap app,
with the goal of recommending the optimal route among candidate routes to
users. Unlike traditional recommendation methods, insights into the local
quality of routes and comparisons between candidate routes are crucial for
enhancing recommendation performance but often overlooked in previous studies.
To achieve these, we propose a novel model called Comprehensive Comparison
Network (CCN). CCN not only uses query-level features (e.g. user features) and
item-level features (e.g. route features, item embedding) that are common in
traditional recommendations, but also introduces comparison-level features
which describe the non-overlapping segments between different routes to capture
the local quality of routes. The key component Comprehensive Comparison Block
(CCB) in CCN is designed to enable comparisons between routes. CCB includes a
Comprehensive Comparison Operator (CCO) and a multi-scenario MLP, which can
update the representations of candidate routes based on a comprehensive
comparison. By stacking multiple CCBs, CCN can determine the final scores of
candidate routes and recommend the optimal one to the user. Additionally, since
routes directly affect the costs and risks experienced by users, the RR model
must be interpretable for online deployment. Therefore, we designed an
interpretable pair scoring network to achieve interpretability. Both offline
and online experiments demonstrate that CCN significantly improves RR
performance and exhibits strong interpretability. CCN has been fully deployed
in the Amap app for over a year, providing stable and optimal benefits for
route recommendations.

### 25. UWB at WASSA-2024 Shared Task 2: Cross-lingual Emotion Detection

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Jakub ≈†m√≠d, Pavel P≈ôib√°≈à, Pavel Kr√°l
- **URL**: <http://arxiv.org/abs/2508.08650v1>
- **Submitted**: 2025-08-12 05:30:46
- **Comment**: Published in Proceedings of the 14th Workshop on Computational
  Approaches to Subjectivity, Sentiment, & Social Media Analysis (WASSA 2024).
  Official version: https://aclanthology.org/2024.wassa-1.47/
- **Topic Keywords**: ranking, rank
- **Reason**: The paper focuses on cross-lingual emotion detection, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves language models and fine-tuning, the task and approach are not aligned with the user's primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> This paper presents our system built for the WASSA-2024 Cross-lingual Emotion
Detection Shared Task. The task consists of two subtasks: first, to assess an
emotion label from six possible classes for a given tweet in one of five
languages, and second, to predict words triggering the detected emotions in
binary and numerical formats. Our proposed approach revolves around fine-tuning
quantized large language models, specifically Orca~2, with low-rank adapters
(LoRA) and multilingual Transformer-based models, such as XLM-R and mT5. We
enhance performance through machine translation for both subtasks and trigger
word switching for the second subtask. The system achieves excellent
performance, ranking 1st in numerical trigger words detection, 3rd in binary
trigger words detection, and 7th in emotion detection.

### 26. OdysseyBench: Evaluating LLM Agents on Long-Horizon Complex Office Application Workflows

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Weixuan Wang, Dongge Han, Daniel Madrigal Diaz, Jin Xu, Victor R√ºhle, Saravan Rajmohan
- **URL**: <http://arxiv.org/abs/2508.09124v1>
- **Submitted**: 2025-08-12 17:53:03
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on evaluating large language models (LLMs) on complex office application workflows, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions dialogue synthesis, it does not involve ranking models, user behavior modeling, or deep semantic understanding, making it only loosely relevant to your research interests.

#### Abstract
> Autonomous agents powered by large language models (LLMs) are increasingly
deployed in real-world applications requiring complex, long-horizon workflows.
However, existing benchmarks predominantly focus on atomic tasks that are
self-contained and independent, failing to capture the long-term contextual
dependencies and multi-interaction coordination required in realistic
scenarios. To address this gap, we introduce OdysseyBench, a comprehensive
benchmark for evaluating LLM agents on long-horizon workflows across diverse
office applications including Word, Excel, PDF, Email, and Calendar. Our
benchmark comprises two complementary splits: OdysseyBench+ with 300 tasks
derived from real-world use cases, and OdysseyBench-Neo with 302 newly
synthesized complex tasks. Each task requires agent to identify essential
information from long-horizon interaction histories and perform multi-step
reasoning across various applications. To enable scalable benchmark creation,
we propose HomerAgents, a multi-agent framework that automates the generation
of long-horizon workflow benchmarks through systematic environment exploration,
task generation, and dialogue synthesis. Our extensive evaluation demonstrates
that OdysseyBench effectively challenges state-of-the-art LLM agents, providing
more accurate assessment of their capabilities in complex, real-world contexts
compared to existing atomic task benchmarks. We believe that OdysseyBench will
serve as a valuable resource for advancing the development and evaluation of
LLM agents in real-world productivity scenarios. In addition, we release
OdysseyBench and HomerAgents to foster research along this line.

### 27. Link Prediction for Event Logs in the Process Industry

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Anastasia Zhukova, Thomas Walton, Christian E. Matt, Bela Gipp
- **URL**: <http://arxiv.org/abs/2508.09096v1>
- **Submitted**: 2025-08-12 17:22:29
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on link prediction for event logs in the process industry, which is unrelated to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on domain adaptation and knowledge management is also not directly relevant to the user's areas of focus.

#### Abstract
> Knowledge management (KM) is vital in the process industry for optimizing
operations, ensuring safety, and enabling continuous improvement through
effective use of operational data and past insights. A key challenge in this
domain is the fragmented nature of event logs in shift books, where related
records, e.g., entries documenting issues related to equipment or processes and
the corresponding solutions, may remain disconnected. This fragmentation
hinders the recommendation of previous solutions to the users. To address this
problem, we investigate record linking (RL) as link prediction, commonly
studied in graph-based machine learning, by framing it as a cross-document
coreference resolution (CDCR) task enhanced with natural language inference
(NLI) and semantic text similarity (STS) by shifting it into the causal
inference (CI). We adapt CDCR, traditionally applied in the news domain, into
an RL model to operate at the passage level, similar to NLI and STS, while
accommodating the process industry's specific text formats, which contain
unstructured text and structured record attributes. Our RL model outperformed
the best versions of NLI- and STS-driven baselines by 28% (11.43 points) and
27% (11.21 points), respectively. Our work demonstrates how domain adaptation
of the state-of-the-art CDCR models, enhanced with reasoning capabilities, can
be effectively tailored to the process industry, improving data quality and
connectivity in shift logs.

### 28. A Survey on Training-free Alignment of Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Birong Pan, Yongqi Li, Weiyu Zhang, Wenpeng Lu, Mayi Xu, Shen Zhou, Yuanyuan Zhu, Ming Zhong, Tieyun Qian
- **URL**: <http://arxiv.org/abs/2508.09016v1>
- **Submitted**: 2025-08-12 15:30:44
- **Topic Keywords**: rag, search
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The topic of training-free alignment of large language models is more relevant to Natural Language Processing and NLP-related topics, but it does not seem to have a direct connection to your specific areas of focus.

#### Abstract
> The alignment of large language models (LLMs) aims to ensure their outputs
adhere to human values, ethical standards, and legal norms. Traditional
alignment methods often rely on resource-intensive fine-tuning (FT), which may
suffer from knowledge degradation and face challenges in scenarios where the
model accessibility or computational resources are constrained. In contrast,
training-free (TF) alignment techniques--leveraging in-context learning,
decoding-time adjustments, and post-generation corrections--offer a promising
alternative by enabling alignment without heavily retraining LLMs, making them
adaptable to both open-source and closed-source environments. This paper
presents the first systematic review of TF alignment methods, categorizing them
by stages of pre-decoding, in-decoding, and post-decoding. For each stage, we
provide a detailed examination from the viewpoint of LLMs and multimodal LLMs
(MLLMs), highlighting their mechanisms and limitations. Furthermore, we
identify key challenges and future directions, paving the way for more
inclusive and effective TF alignment techniques. By synthesizing and organizing
the rapidly growing body of research, this survey offers a guidance for
practitioners and advances the development of safer and more reliable LLMs.

### 29. LyS at SemEval 2025 Task 8: Zero-Shot Code Generation for Tabular QA

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Adri√°n Gude, Roi Santos-R√≠os, Francisco Prado-Vali√±o, Ana Ezquerro, Jes√∫s Vilares
- **URL**: <http://arxiv.org/abs/2508.09012v1>
- **Submitted**: 2025-08-12 15:25:31
- **Comment**: Accepted to SemEval 2025. Camera-ready version
- **Topic Keywords**: rag, rank
- **Reason**: This paper is not directly related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling. The focus on zero-shot code generation for Tabular Question Answering is outside the user's primary areas of interest, and the paper does not address real-time relevance optimization or deep semantic understanding.

#### Abstract
> This paper describes our participation in SemEval 2025 Task 8, focused on
Tabular Question Answering. We developed a zero-shot pipeline that leverages an
Large Language Model to generate functional code capable of extracting the
relevant information from tabular data based on an input question. Our approach
consists of a modular pipeline where the main code generator module is
supported by additional components that identify the most relevant columns and
analyze their data types to improve extraction accuracy. In the event that the
generated code fails, an iterative refinement process is triggered,
incorporating the error feedback into a new generation prompt to enhance
robustness. Our results show that zero-shot code generation is a valid approach
for Tabular QA, achieving rank 33 of 53 in the test phase despite the lack of
task-specific fine-tuning.

### 30. Retrospective Sparse Attention for Efficient Long-Context Generation

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Seonghwan Choi, Beomseok Kang, Dongwon Jo, Jae-Joon Kim
- **URL**: <http://arxiv.org/abs/2508.09001v1>
- **Submitted**: 2025-08-12 15:11:47
- **Topic Keywords**: queries
- **Reason**: This paper focuses on optimizing the Key-Value cache for large language models in long-context tasks, which is not directly related to information retrieval, query understanding, or ranking models. While it touches on attention mechanisms, the context is different from the user's interests in IR and NLP.

#### Abstract
> Large Language Models (LLMs) are increasingly deployed in long-context tasks
such as reasoning, code generation, and multi-turn dialogue. However, inference
over extended contexts is bottlenecked by the Key-Value (KV) cache, whose
memory footprint grows linearly with sequence length and dominates latency at
each decoding step. While recent KV cache compression methods identify and load
important tokens, they focus predominantly on input contexts and fail to
address the cumulative attention errors that arise during long decoding. In
this paper, we introduce RetroAttention, a novel KV cache update technique that
retrospectively revises past attention outputs using newly arrived KV entries
from subsequent decoding steps. By maintaining a lightweight output cache,
RetroAttention enables past queries to efficiently access more relevant
context, while incurring minimal latency overhead. This breaks the
fixed-attention-output paradigm and allows continual correction of prior
approximations. Extensive experiments on long-generation benchmarks show that
RetroAttention consistently outperforms state-of-the-art (SOTA) KV compression
methods, increasing effective KV exposure by up to 1.6$\times$ and accuracy by
up to 21.9\%.

### 31. BiasGym: Fantastic Biases and How to Find (and Remove) Them

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Sekh Mainul Islam, Nadav Borenstein, Siddhesh Milind Pawar, Haeun Yu, Arnav Arora, Isabelle Augenstein
- **URL**: <http://arxiv.org/abs/2508.08855v1>
- **Submitted**: 2025-08-12 11:23:44
- **Comment**: Under review
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on bias detection and mitigation in Large Language Models, which is not directly related to the user's interests in Information Retrieval, Search technologies, and query understanding. Although the paper mentions token-based fine-tuning, it is not applicable to the user's areas of focus.

#### Abstract
> Understanding biases and stereotypes encoded in the weights of Large Language
Models (LLMs) is crucial for developing effective mitigation strategies. Biased
behaviour is often subtle and non-trivial to isolate, even when deliberately
elicited, making systematic analysis and debiasing particularly challenging. To
address this, we introduce BiasGym, a simple, cost-effective, and generalizable
framework for reliably injecting, analyzing, and mitigating conceptual
associations within LLMs. BiasGym consists of two components: BiasInject, which
injects specific biases into the model via token-based fine-tuning while
keeping the model frozen, and BiasScope, which leverages these injected signals
to identify and steer the components responsible for biased behavior. Our
method enables consistent bias elicitation for mechanistic analysis, supports
targeted debiasing without degrading performance on downstream tasks, and
generalizes to biases unseen during training. We demonstrate the effectiveness
of BiasGym in reducing real-world stereotypes (e.g., people from a country
being `reckless drivers') and in probing fictional associations (e.g., people
from a country having `blue skin'), showing its utility for both safety
interventions and interpretability research.

### 32. Designing Memory-Augmented AR Agents for Spatiotemporal Reasoning in Personalized Task Assistance

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Dongwook Choi, Taeyoon Kwon, Dongil Yang, Hyojun Kim, Jinyoung Yeo
- **URL**: <http://arxiv.org/abs/2508.08774v1>
- **Submitted**: 2025-08-12 09:20:20
- **Comment**: 7 pages, 2 figures
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on Augmented Reality (AR) systems, memory-augmented AR agents, and spatiotemporal reasoning, which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper mentions multimodal sensor processing and language models, the context is distinct from the user's areas of focus.

#### Abstract
> Augmented Reality (AR) systems are increasingly integrating foundation
models, such as Multimodal Large Language Models (MLLMs), to provide more
context-aware and adaptive user experiences. This integration has led to the
development of AR agents to support intelligent, goal-directed interactions in
real-world environments. While current AR agents effectively support immediate
tasks, they struggle with complex multi-step scenarios that require
understanding and leveraging user's long-term experiences and preferences. This
limitation stems from their inability to capture, retain, and reason over
historical user interactions in spatiotemporal contexts. To address these
challenges, we propose a conceptual framework for memory-augmented AR agents
that can provide personalized task assistance by learning from and adapting to
user-specific experiences over time. Our framework consists of four
interconnected modules: (1) Perception Module for multimodal sensor processing,
(2) Memory Module for persistent spatiotemporal experience storage, (3)
Spatiotemporal Reasoning Module for synthesizing past and present contexts, and
(4) Actuator Module for effective AR communication. We further present an
implementation roadmap, a future evaluation strategy, a potential target
application and use cases to demonstrate the practical applicability of our
framework across diverse domains. We aim for this work to motivate future
research toward developing more intelligent AR systems that can effectively
bridge user's interaction history with adaptive, context-aware task assistance.

### 33. Eat your own KR: a KR-based approach to index Semantic Web Endpoints and Knowledge Graphs

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Pierre Maillot, Catherine Faron, Fabien Gandon, Franck Michel, Pierre Monnin
- **URL**: <http://arxiv.org/abs/2508.08713v1>
- **Submitted**: 2025-08-12 07:56:48
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on indexing and characterizing knowledge graphs using knowledge representation and reasoning methods, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on semantic web and ontology usage, the topic is more aligned with artificial intelligence and data management, rather than information retrieval or search.

#### Abstract
> Over the last decade, knowledge graphs have multiplied, grown, and evolved on
the World Wide Web, and the advent of new standards, vocabularies, and
application domains has accelerated this trend. IndeGx is a framework
leveraging an extensible base of rules to index the content of KGs and the
capacities of their SPARQL endpoints. In this article, we show how knowledge
representation (KR) and reasoning methods and techniques can be used in a
reflexive manner to index and characterize existing knowledge graphs (KG) with
respect to their usage of KR methods and techniques. We extended IndeGx with a
fully ontology-oriented modeling and processing approach to do so. Using SPARQL
rules and an OWL RL ontology of the indexing domain, IndeGx can now build and
reason over an index of the contents and characteristics of an open collection
of public knowledge graphs. Our extension of the framework relies on a
declarative representation of procedural knowledge and collaborative
environments (e.g., GitHub) to provide an agile, customizable, and expressive
KR approach for building and maintaining such an index of knowledge graphs in
the wild. In doing so, we help anyone answer the question of what knowledge is
out there in the world wild Semantic Web in general, and we also help our
community monitor which KR research results are used in practice. In
particular, this article provides a snapshot of the state of the Semantic Web
regarding supported standard languages, ontology usage, and diverse quality
evaluations by applying this method to a collection of over 300 open knowledge
graph endpoints.

### 34. MiGrATe: Mixed-Policy GRPO for Adaptation at Test-Time

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Peter Phan, Dhruv Agarwal, Kavitha Srinivas, Horst Samulowitz, Pavan Kapanipathi, Andrew McCallum
- **URL**: <http://arxiv.org/abs/2508.08641v1>
- **Submitted**: 2025-08-12 05:08:21
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on test-time training with synthetic data for black-box optimization tasks, which is not directly related to information retrieval, search technologies, or query understanding. While it involves language models and search algorithms, the context is different from the user's primary research interests.

#### Abstract
> Large language models (LLMs) are increasingly being applied to black-box
optimization tasks, from program synthesis to molecule design. Prior work
typically leverages in-context learning to iteratively guide the model towards
better solutions. Such methods, however, often struggle to balance exploration
of new solution spaces with exploitation of high-reward ones. Recently,
test-time training (TTT) with synthetic data has shown promise in improving
solution quality. However, the need for hand-crafted training data tailored to
each task limits feasibility and scalability across domains. To address this
problem, we introduce MiGrATe-a method for online TTT that uses GRPO as a
search algorithm to adapt LLMs at inference without requiring external training
data. MiGrATe operates via a mixed-policy group construction procedure that
combines on-policy sampling with two off-policy data selection techniques:
greedy sampling, which selects top-performing past completions, and
neighborhood sampling (NS), which generates completions structurally similar to
high-reward ones. Together, these components bias the policy gradient towards
exploitation of promising regions in solution space, while preserving
exploration through on-policy sampling. We evaluate MiGrATe on three
challenging domains-word search, molecule optimization, and hypothesis+program
induction on the Abstraction and Reasoning Corpus (ARC)-and find that it
consistently outperforms both inference-only and TTT baselines, demonstrating
the potential of online TTT as a solution for complex search tasks without
external supervision.

### 35. InternBootcamp Technical Report: Boosting LLM Reasoning with Verifiable Task Scaling

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Peiji Li, Jiasheng Ye, Yongkang Chen, Yichuan Ma, Zijie Yu, Kedi Chen, Ganqu Cui, Haozhan Li, Jiacheng Chen, Chengqi Lyu, Wenwei Zhang, Linyang Li, Qipeng Guo, Dahua Lin, Bowen Zhou, Kai Chen
- **URL**: <http://arxiv.org/abs/2508.08636v1>
- **Submitted**: 2025-08-12 05:00:00
- **Comment**: InternBootcamp Tech Report
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on large language models and reinforcement learning, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on task scaling and model optimization is also not aligned with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large language models (LLMs) have revolutionized artificial intelligence by
enabling complex reasoning capabilities. While recent advancements in
reinforcement learning (RL) have primarily focused on domain-specific reasoning
tasks (e.g., mathematics or code generation), real-world reasoning scenarios
often require models to handle diverse and complex environments that
narrow-domain benchmarks cannot fully capture. To address this gap, we present
InternBootcamp, an open-source framework comprising 1000+ domain-diverse task
environments specifically designed for LLM reasoning research. Our codebase
offers two key functionalities: (1) automated generation of unlimited
training/testing cases with configurable difficulty levels, and (2) integrated
verification modules for objective response evaluation. These features make
InternBootcamp fundamental infrastructure for RL-based model optimization,
synthetic data generation, and model evaluation. Although manually developing
such a framework with enormous task coverage is extremely cumbersome, we
accelerate the development procedure through an automated agent workflow
supplemented by manual validation protocols, which enables the task scope to
expand rapidly. % With these bootcamps, we further establish Bootcamp-EVAL, an
automatically generated benchmark for comprehensive performance assessment.
Evaluation reveals that frontier models still underperform in many reasoning
tasks, while training with InternBootcamp provides an effective way to
significantly improve performance, leading to our 32B model that achieves
state-of-the-art results on Bootcamp-EVAL and excels on other established
benchmarks. In particular, we validate that consistent performance gains come
from including more training tasks, namely \textbf{task scaling}, over two
orders of magnitude, offering a promising route towards capable reasoning
generalist.

### 36. Train Long, Think Short: Curriculum Learning for Efficient Reasoning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Hasan Abed Al Kader Hammoud, Kumail Alhamoud, Abed Hammoud, Elie Bou-Zeid, Marzyeh Ghassemi, Bernard Ghanem
- **URL**: <http://arxiv.org/abs/2508.08940v1>
- **Submitted**: 2025-08-12 13:48:03
- **Comment**: Under Review
- **Topic Keywords**: rag
- **Reason**: The paper focuses on curriculum learning for efficient reasoning in large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on optimization and efficiency, the context is different from the user's primary research interests.

#### Abstract
> Recent work on enhancing the reasoning abilities of large language models
(LLMs) has introduced explicit length control as a means of constraining
computational cost while preserving accuracy. However, existing approaches rely
on fixed-length training budgets, which do not take advantage of the natural
progression from exploration to compression during learning. In this work, we
propose a curriculum learning strategy for length-controlled reasoning using
Group Relative Policy Optimization (GRPO). Our method starts with generous
token budgets and gradually tightens them over training, encouraging models to
first discover effective solution strategies and then distill them into more
concise reasoning traces. We augment GRPO with a reward function that balances
three signals: task correctness (via verifier feedback), length efficiency, and
formatting adherence (via structural tags). Experiments on GSM8K, MATH500,
SVAMP, College Math, and GSM+ demonstrate that curriculum-based training
consistently outperforms fixed-budget baselines at the same final budget,
achieving higher accuracy and significantly improved token efficiency. We
further ablate the impact of reward weighting and decay schedule design,
showing that progressive constraint serves as a powerful inductive bias for
training efficient reasoning models. Our code and checkpoints are released at:
https://github.com/hammoudhasan/curriculum_grpo.

### 37. Steering Towards Fairness: Mitigating Political Bias in LLMs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Afrozah Nadeem, Mark Dras, Usman Naseem
- **URL**: <http://arxiv.org/abs/2508.08846v1>
- **Submitted**: 2025-08-12 11:09:03
- **Comment**: Preprint
- **Topic Keywords**: rag
- **Reason**: This paper focuses on large language models (LLMs) and their tendency to encode and reproduce ideological biases, particularly along political and economic dimensions. While it's related to NLP, it's not directly related to information retrieval, query understanding, ranking models, or user behavior modeling, which are the core areas of your research interests.

#### Abstract
> Recent advancements in large language models (LLMs) have enabled their
widespread use across diverse real-world applications. However, concerns remain
about their tendency to encode and reproduce ideological biases, particularly
along political and economic dimensions. In this paper, we propose a framework
for probing and mitigating such biases in decoder-based LLMs through analysis
of internal model representations. Grounded in the Political Compass Test
(PCT), our method uses contrastive pairs to extract and compare hidden layer
activations from models like Mistral and DeepSeek. We introduce a comprehensive
activation extraction pipeline capable of layer-wise analysis across multiple
ideological axes, revealing meaningful disparities linked to political framing.
Our results show that decoder LLMs systematically encode representational bias
across layers, which can be leveraged for effective steering vector-based
mitigation. This work provides new insights into how political bias is encoded
in LLMs and offers a principled approach to debiasing beyond surface-level
output interventions.

### 38. Magical: Medical Lay Language Generation via Semantic Invariance and Layperson-tailored Adaptation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Weibin Liao, Tianlong Wang, Yinghao Zhu, Yasha Wang, Junyi Gao, Liantao Ma
- **URL**: <http://arxiv.org/abs/2508.08730v1>
- **Submitted**: 2025-08-12 08:21:58
- **Topic Keywords**: recommend, rank
- **Reason**: The paper focuses on Medical Lay Language Generation, which is not directly related to Information Retrieval, Search technologies, or query understanding. Although it involves language models and fine-tuning, the context and objectives are distinct from the user's primary research interests.

#### Abstract
> Medical Lay Language Generation (MLLG) plays a vital role in improving the
accessibility of complex scientific content for broader audiences. Recent
literature to MLLG commonly employ parameter-efficient fine-tuning methods such
as Low-Rank Adaptation (LoRA) to fine-tuning large language models (LLMs) using
paired expert-lay language datasets. However, LoRA struggles with the
challenges posed by multi-source heterogeneous MLLG datasets. Specifically,
through a series of exploratory experiments, we reveal that standard LoRA fail
to meet the requirement for semantic fidelity and diverse lay-style generation
in MLLG task. To address these limitations, we propose Magical, an asymmetric
LoRA architecture tailored for MLLG under heterogeneous data scenarios. Magical
employs a shared matrix $A$ for abstractive summarization, along with multiple
isolated matrices $B$ for diverse lay-style generation. To preserve semantic
fidelity during the lay language generation process, Magical introduces a
Semantic Invariance Constraint to mitigate semantic subspace shifts on matrix
$A$. Furthermore, to better adapt to diverse lay-style generation, Magical
incorporates the Recommendation-guided Switch, an externally interface to
prompt the LLM to switch between different matrices $B$. Experimental results
on three real-world lay language generation datasets demonstrate that Magical
consistently outperforms prompt-based methods, vanilla LoRA, and its recent
variants, while also reducing trainable parameters by 31.66%.

### 39. MultiAiTutor: Child-Friendly Educational Multilingual Speech Generation Tutor with LLMs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Xiaoxue Gao, Huayun Zhang, Nancy F. Chen
- **URL**: <http://arxiv.org/abs/2508.08715v1>
- **Submitted**: 2025-08-12 07:58:48
- **Comment**: 5 figures
- **Topic Keywords**: rag
- **Reason**: This paper focuses on multilingual speech generation for educational purposes, leveraging LLM architectures, which is not directly related to Information Retrieval, Search technologies, or query understanding. Although it involves AI and machine learning, the context is educational and not specifically related to search or retrieval.

#### Abstract
> Generative speech models have demonstrated significant potential in
personalizing teacher-student interactions, offering valuable real-world
applications for language learning in children's education. However, achieving
high-quality, child-friendly speech generation remains challenging,
particularly for low-resource languages across diverse languages and cultural
contexts. In this paper, we propose MultiAiTutor, an educational multilingual
generative AI tutor with child-friendly designs, leveraging LLM architecture
for speech generation tailored for educational purposes. We propose to
integrate age-appropriate multilingual speech generation using LLM
architectures, facilitating young children's language learning through
culturally relevant image-description tasks in three low-resource languages:
Singaporean-accent Mandarin, Malay, and Tamil. Experimental results from both
objective metrics and subjective evaluations demonstrate the superior
performance of the proposed MultiAiTutor compared to baseline methods.

### 40. $\text{M}^{2}$LLM: Multi-view Molecular Representation Learning with Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jiaxin Ju, Yizhen Zheng, Huan Yee Koh, Can Wang, Shirui Pan
- **URL**: <http://arxiv.org/abs/2508.08657v1>
- **Submitted**: 2025-08-12 05:46:47
- **Comment**: IJCAI 2025
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on molecular representation learning and property prediction in chemistry, materials science, and drug discovery, which is outside your primary research areas.

#### Abstract
> Accurate molecular property prediction is a critical challenge with
wide-ranging applications in chemistry, materials science, and drug discovery.
Molecular representation methods, including fingerprints and graph neural
networks (GNNs), achieve state-of-the-art results by effectively deriving
features from molecular structures. However, these methods often overlook
decades of accumulated semantic and contextual knowledge. Recent advancements
in large language models (LLMs) demonstrate remarkable reasoning abilities and
prior knowledge across scientific domains, leading us to hypothesize that LLMs
can generate rich molecular representations when guided to reason in multiple
perspectives. To address these gaps, we propose $\text{M}^{2}$LLM, a multi-view
framework that integrates three perspectives: the molecular structure view, the
molecular task view, and the molecular rules view. These views are fused
dynamically to adapt to task requirements, and experiments demonstrate that
$\text{M}^{2}$LLM achieves state-of-the-art performance on multiple benchmarks
across classification and regression tasks. Moreover, we demonstrate that
representation derived from LLM achieves exceptional performance by leveraging
two core functionalities: the generation of molecular embeddings through their
encoding capabilities and the curation of molecular features through advanced
reasoning processes.

### 41. DeCAL Tokenwise Compression

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Sameer Panwar
- **URL**: <http://arxiv.org/abs/2508.08514v1>
- **Submitted**: 2025-08-11 22:49:54
- **Topic Keywords**: retrieval
- **Reason**: The paper focuses on tokenwise compression, which is not directly related to information retrieval, query understanding, ranking models, or user behavior modeling. While it mentions applications in question-answering, summarization, and multi-vector retrieval tasks, the emphasis is on compression rather than deep semantic understanding or real-time relevance optimization.

#### Abstract
> This paper introduces DeCAL, a new method for tokenwise compression. DeCAL
uses an encoder-decoder language model pretrained with denoising to learn to
produce high-quality, general-purpose compressed representations by the
encoder. DeCAL applies small modifications to the encoder, with the emphasis on
maximizing compression quality, even at the expense of compute. We show that
DeCAL at 2x compression can match uncompressed on many downstream tasks, with
usually only minor dropoff in metrics up to 8x compression, among
question-answering, summarization, and multi-vector retrieval tasks. DeCAL
offers significant savings where pre-computed dense representations can be
utilized, and we believe the approach can be further developed to be more
broadly applicable.

### 42. SinLlama -- A Large Language Model for Sinhala

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: H. W. K. Aravinda, Rashad Sirajudeen, Samith Karunathilake, Nisansa de Silva, Surangika Ranathunga, Rishemjit Kaur
- **URL**: <http://arxiv.org/abs/2508.09115v1>
- **Submitted**: 2025-08-12 17:49:34
- **Topic Keywords**: search
- **Reason**: The paper focuses on developing a language model for the Sinhala language, which is not directly related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. While the paper mentions pre-training and fine-tuning, it does not address the user's specific areas of interest.

#### Abstract
> Low-resource languages such as Sinhala are often overlooked by open-source
Large Language Models (LLMs). In this research, we extend an existing
multilingual LLM (Llama-3-8B) to better serve Sinhala. We enhance the LLM
tokenizer with Sinhala specific vocabulary and perform continual pre-training
on a cleaned 10 million Sinhala corpus, resulting in the SinLlama model. This
is the very first decoder-based open-source LLM with explicit Sinhala support.
When SinLlama was instruction fine-tuned for three text classification tasks,
it outperformed base and instruct variants of Llama-3-8B by a significant
margin.

### 43. Entangled in Representations: Mechanistic Investigation of Cultural Biases in Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Haeun Yu, Seogyeong Jeong, Siddhesh Pawar, Jisu Shin, Jiho Jin, Junho Myung, Alice Oh, Isabelle Augenstein
- **URL**: <http://arxiv.org/abs/2508.08879v1>
- **Submitted**: 2025-08-12 12:05:32
- **Comment**: 16 pages, 7 figures
- **Topic Keywords**: search
- **Reason**: The paper focuses on the cultural biases in large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on the internal mechanisms of language models, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are core aspects of your research interests.

#### Abstract
> The growing deployment of large language models (LLMs) across diverse
cultural contexts necessitates a better understanding of how the
overgeneralization of less documented cultures within LLMs' representations
impacts their cultural understanding. Prior work only performs extrinsic
evaluation of LLMs' cultural competence, without accounting for how LLMs'
internal mechanisms lead to cultural (mis)representation. To bridge this gap,
we propose Culturescope, the first mechanistic interpretability-based method
that probes the internal representations of LLMs to elicit the underlying
cultural knowledge space. CultureScope utilizes a patching method to extract
the cultural knowledge. We introduce a cultural flattening score as a measure
of the intrinsic cultural biases. Additionally, we study how LLMs internalize
Western-dominance bias and cultural flattening, which allows us to trace how
cultural biases emerge within LLMs. Our experimental results reveal that LLMs
encode Western-dominance bias and cultural flattening in their cultural
knowledge space. We find that low-resource cultures are less susceptible to
cultural biases, likely due to their limited training resources. Our work
provides a foundation for future research on mitigating cultural biases and
enhancing LLMs' cultural understanding. Our codes and data used for experiments
are publicly available.

### 44. A Survey on Parallel Text Generation: From Parallel Decoding to Diffusion Language Models

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Lingzhe Zhang, Liancheng Fang, Chiming Duan, Minghua He, Leyi Pan, Pei Xiao, Shiyu Huang, Yunpeng Zhai, Xuming Hu, Philip S. Yu, Aiwei Liu
- **URL**: <http://arxiv.org/abs/2508.08712v2>
- **Submitted**: 2025-08-12 07:56:04
- **Topic Keywords**: search
- **Reason**: The paper is not directly related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The topic of parallel text generation in Large Language Models is not a central match for the user's focus on IR and NLP.

#### Abstract
> As text generation has become a core capability of modern Large Language
Models (LLMs), it underpins a wide range of downstream applications. However,
most existing LLMs rely on autoregressive (AR) generation, producing one token
at a time based on previously generated context-resulting in limited generation
speed due to the inherently sequential nature of the process. To address this
challenge, an increasing number of researchers have begun exploring parallel
text generation-a broad class of techniques aimed at breaking the
token-by-token generation bottleneck and improving inference efficiency.
Despite growing interest, there remains a lack of comprehensive analysis on
what specific techniques constitute parallel text generation and how they
improve inference performance. To bridge this gap, we present a systematic
survey of parallel text generation methods. We categorize existing approaches
into AR-based and Non-AR-based paradigms, and provide a detailed examination of
the core techniques within each category. Following this taxonomy, we assess
their theoretical trade-offs in terms of speed, quality, and efficiency, and
examine their potential for combination and comparison with alternative
acceleration strategies. Finally, based on our findings, we highlight recent
advancements, identify open challenges, and outline promising directions for
future research in parallel text generation. We have also created a GitHub
repository for indexing relevant papers and open resources available at
https://github.com/zhanglingzhe0820/Awesome-Parallel-Text-Generation.

### 45. TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine Translation

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Armel Zebaze, Beno√Æt Sagot, Rachel Bawden
- **URL**: <http://arxiv.org/abs/2508.08680v1>
- **Submitted**: 2025-08-12 06:58:02
- **Topic Keywords**: search
- **Reason**: The paper focuses on machine translation, a topic outside of the user's primary research interests in Information Retrieval and Search technologies. Although it mentions in-context learning, which is related to query understanding, the connection is indirect and not central to the paper's main theme.

#### Abstract
> LLMs have been shown to perform well in machine translation (MT) with the use
of in-context learning (ICL), rivaling supervised models when translating into
high-resource languages (HRLs). However, they lag behind when translating into
low-resource language (LRLs). Example selection via similarity search and
supervised fine-tuning help. However the improvements they give are limited by
the size, quality and diversity of existing parallel datasets. A common
technique in low-resource MT is synthetic parallel data creation, the most
frequent of which is backtranslation, whereby existing target-side texts are
automatically translated into the source language. However, this assumes the
existence of good quality and relevant target-side texts, which are not readily
available for many LRLs. In this paper, we present \textsc{TopXGen}, an
LLM-based approach for the generation of high quality and topic-diverse data in
multiple LRLs, which can then be backtranslated to produce useful and diverse
parallel texts for ICL and fine-tuning. Our intuition is that while LLMs
struggle to translate into LRLs, their ability to translate well into HRLs and
their multilinguality enable them to generate good quality, natural-sounding
target-side texts, which can be translated well into a high-resource source
language. We show that \textsc{TopXGen} boosts LLM translation performance
during fine-tuning and in-context learning. Code and outputs are available at
https://github.com/ArmelRandy/topxgen.

### 46. Bilevel MCTS for Amortized O(1) Node Selection in Classical Planning

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Masataro Asai
- **URL**: <http://arxiv.org/abs/2508.08385v1>
- **Submitted**: 2025-08-11 18:12:40
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The topic of classical planning and Monte-Carlo Tree Search is outside the scope of your expertise and does not align with your focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> We study an efficient implementation of Multi-Armed Bandit (MAB)-based
Monte-Carlo Tree Search (MCTS) for classical planning. One weakness of MCTS is
that it spends a significant time deciding which node to expand next. While
selecting a node from an OPEN list with $N$ nodes has $O(1)$ runtime complexity
with traditional array-based priority-queues for dense integer keys, the
tree-based OPEN list used by MCTS requires $O(\log N)$, which roughly
corresponds to the search depth $d$. In classical planning, $d$ is arbitrarily
large (e.g., $2^k-1$ in $k$-disk Tower-of-Hanoi) and the runtime for node
selection is significant, unlike in game tree search, where the cost is
negligible compared to the node evaluation (rollouts) because $d$ is inherently
limited by the game (e.g., $d\leq 361$ in Go). To improve this bottleneck, we
propose a bilevel modification to MCTS that runs a best-first search from each
selected leaf node with an expansion budget proportional to $d$, which achieves
amortized $O(1)$ runtime for node selection, equivalent to the traditional
queue-based OPEN list. In addition, we introduce Tree Collapsing, an
enhancement that reduces action selection steps and further improves the
performance.

---

