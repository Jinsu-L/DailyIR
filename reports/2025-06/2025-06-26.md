# Daily Papers Report - 2025-06-26

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Semantic-enhanced Modality-asymmetric Retrieval for Online E-commerce Search

- **LLM Score**: 7
- **Keyword Score**: 10
- **Authors**: Zhigong Zhou, Ning Ding, Xiaochuan Fan, Yue Shang, Yiming Qiu, Jingwei Zhuo, Zhiwei Ge, Songlin Wang, Lin Liu, Sulong Xu, Han Zhang
- **URL**: <http://arxiv.org/abs/2506.20330v1>
- **Submitted**: 2025-06-25 11:28:04
- **Comment**: published in sigir2023
- **Topic Keywords**: query, rag, retrieval, commerce, e-commerce, search
- **Reason**: The paper focuses on multimodal retrieval in e-commerce search, leveraging visual information to enhance item representation and improve retrieval performance. While it doesn't directly address query understanding, ranking models, or user behavior modeling, it does explore semantic retrieval and modality fusion, which are related to information retrieval and NLP. The e-commerce domain is also relevant to the user's background.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Multimodal Retrieval in Online E-commerce Search
- **Aim**: To tackle the problem of modality fusion and alignment in asymmetric scenarios where the query is unimodal (textual) and the item is multimodal (textual and visual)
- **Rationale**: Leveraging visual information as supplementary to textual information to enhance item representation and retrieval performance
- **Ground**: SMAR model consisting of four towers: Query Tower, Item Text Tower, Item Image Tower, and Item Multimodal Tower, with a two-stage approach of pre-training and fine-tuning models
- **Experiment**: Evaluating SMAR using standard retrieval quality metrics on multiple datasets, with online A/B tests on an e-commerce platform
- **Takeaway**: SMAR outperforms baseline models significantly, with improvements in gross merchandise value (GMV) and user conversation rate (UCVR) in a business scenario

#### Abstract
> Semantic retrieval, which retrieves semantically matched items given a
textual query, has been an essential component to enhance system effectiveness
in e-commerce search. In this paper, we study the multimodal retrieval problem,
where the visual information (e.g, image) of item is leveraged as supplementary
of textual information to enrich item representation and further improve
retrieval performance. Though learning from cross-modality data has been
studied extensively in tasks such as visual question answering or media
summarization, multimodal retrieval remains a non-trivial and unsolved problem
especially in the asymmetric scenario where the query is unimodal while the
item is multimodal. In this paper, we propose a novel model named SMAR, which
stands for Semantic-enhanced Modality-Asymmetric Retrieval, to tackle the
problem of modality fusion and alignment in this kind of asymmetric scenario.
Extensive experimental results on an industrial dataset show that the proposed
model outperforms baseline models significantly in retrieval accuracy. We have
open sourced our industrial dataset for the sake of reproducibility and future
research works.

---

### 2. CCRS: A Zero-Shot LLM-as-a-Judge Framework for Comprehensive RAG Evaluation

- **LLM Score**: 6
- **Keyword Score**: 8
- **Authors**: Aashiq Muhamed
- **URL**: <http://arxiv.org/abs/2506.20128v1>
- **Submitted**: 2025-06-25 04:49:03
- **Comment**: Accepted at LLM4Eval @ SIGIR 2025
- **Topic Keywords**: query, relevance, rag
- **Reason**: The paper proposes a novel evaluation framework for RAG systems, focusing on contextual coherence, query relevance, and factual correctness. While it's not directly related to query understanding, ranking models, or user behavior modeling, it's somewhat relevant to information retrieval and NLP, as it deals with evaluating the quality of RAG outputs. However, the paper's focus on RAG systems and their evaluation is not a central match for the user's research interests.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Evaluating Retrieval-Augmented Generation (RAG) systems
- **Aim**: Propose a novel framework, CCRS, for evaluating RAG systems
- **Rationale**: Traditional metrics are limited in capturing the complex aspects of RAG output quality
- **Ground**: CCRS framework utilizes a single, powerful, pre-trained Large Language Model (LLM) as a zero-shot, end-to-end judge to evaluate five metrics
- **Experiment**: Applying CCRS to evaluate six diverse RAG system configurations on the BioASQ biomedical question-answering dataset
- **Takeaway**: CCRS provides a comprehensive and efficient evaluation of RAG systems, addressing limitations of traditional metrics and helping improve the development of more accurate and informative RAG systems

#### Abstract
> RAG systems enhance LLMs by incorporating external knowledge, which is
crucial for domains that demand factual accuracy and up-to-date information.
However, evaluating the multifaceted quality of RAG outputs, spanning aspects
such as contextual coherence, query relevance, factual correctness, and
informational completeness, poses significant challenges. Existing evaluation
methods often rely on simple lexical overlap metrics, which are inadequate for
capturing these nuances, or involve complex multi-stage pipelines with
intermediate steps like claim extraction or require finetuning specialized
judge models, hindering practical efficiency. To address these limitations, we
propose CCRS (Contextual Coherence and Relevance Score), a novel suite of five
metrics that utilizes a single, powerful, pretrained LLM as a zero-shot,
end-to-end judge. CCRS evaluates: Contextual Coherence (CC), Question Relevance
(QR), Information Density (ID), Answer Correctness (AC), and Information Recall
(IR). We apply CCRS to evaluate six diverse RAG system configurations on the
challenging BioASQ dataset. Our analysis demonstrates that CCRS effectively
discriminates between system performances, confirming, for instance, that the
Mistral-7B reader outperforms Llama variants. We provide a detailed analysis of
CCRS metric properties, including score distributions, convergent/discriminant
validity, tie rates, population statistics, and discriminative power. Compared
to the complex RAGChecker framework, CCRS offers comparable or superior
discriminative power for key aspects like recall and faithfulness, while being
significantly more computationally efficient. CCRS thus provides a practical,
comprehensive, and efficient framework for evaluating and iteratively improving
RAG systems.

---

### 3. MMSearch-R1: Incentivizing LMMs to Search

- **LLM Score**: 6
- **Keyword Score**: 5
- **Authors**: Jinming Wu, Zihao Deng, Wei Li, Yiding Liu, Bo You, Bo Li, Zejun Ma, Ziwei Liu
- **URL**: <http://arxiv.org/abs/2506.20670v1>
- **Submitted**: 2025-06-25 17:59:42
- **Comment**: Code: https://github.com/EvolvingLMMs-Lab/multimodal-search-r1
- **Topic Keywords**: rag, retrieval, search
- **Reason**: The paper presents a reinforcement learning framework for multimodal search, which is somewhat related to my interests in Information Retrieval and Search technologies. While it doesn't specifically focus on query understanding, ranking models, or user behavior modeling, it does explore search behaviors and optimization, which is relevant to my research. However, the paper's focus on multimodal search and reinforcement learning is not a central match to my core research themes.

#### T.A.R.G.E.T. Summary (from Abstract)
- **Topic**: Multimodal Search using Reinforcement Learning
- **Aim**: Develop an end-to-end reinforcement learning framework for large multimodal models to perform on-demand, multi-turn search in real-world Internet environments
- **Rationale**: Integrate image and text search tools to reason about when and how to invoke them based on an outcome-based reward with a search penalty
- **Ground**: Collect a multimodal search VQA dataset through a semi-automated pipeline, covering diverse visual and textual knowledge needs
- **Experiment**: Compare MMSearch-R1 with RAG-based baselines of the same model size and a larger RAG-based model
- **Takeaway**: MMSearch-R1 outperforms RAG-based baselines and matches the performance of a larger RAG-based model while reducing search calls by over 30%

#### Abstract
> Robust deployment of large multimodal models (LMMs) in real-world scenarios
requires access to external knowledge sources, given the complexity and dynamic
nature of real-world information. Existing approaches such as
retrieval-augmented generation (RAG) and prompt engineered search agents rely
on rigid pipelines, often leading to inefficient or excessive search behaviors.
We present MMSearch-R1, the first end-to-end reinforcement learning framework
that enables LMMs to perform on-demand, multi-turn search in real-world
Internet environments. Our framework integrates both image and text search
tools, allowing the model to reason about when and how to invoke them guided by
an outcome-based reward with a search penalty. To support training, We collect
a multimodal search VQA dataset through a semi-automated pipeline that covers
diverse visual and textual knowledge needs and curate a search-balanced subset
with both search-required and search-free samples, which proves essential for
shaping efficient and on-demand search behavior. Extensive experiments on
knowledge-intensive and info-seeking VQA tasks show that our model not only
outperforms RAG-based baselines of the same model size, but also matches the
performance of a larger RAG-based model while reducing search calls by over
30%. We further analyze key empirical findings to offer actionable insights for
advancing research in multimodal search.

---

### 4. SACL: Understanding and Combating Textual Bias in Code Retrieval with Semantic-Augmented Reranking and Localization

- **LLM Score**: 4
- **Keyword Score**: 14
- **Authors**: Dhruv Gupta, Gayathri Ganesh Lakshmy, Yiqing Xie
- **URL**: <http://arxiv.org/abs/2506.20081v2>
- **Submitted**: 2025-06-25 01:44:28
- **Topic Keywords**: retriever, ranking, rerank, retrieval, rank, acl
- **Reason**: The paper focuses on code retrieval and generation, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper mentions semantic information and bias reduction, it does not explicitly address query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Code Retrieval and Generation
- **Aim**: To develop a framework that mitigates the limitations of traditional code retrieval methods by incorporating semantic information
- **Rationale**: Traditional code retrieval methods rely heavily on textual features, underutilizing functional semantics, and exhibit bias towards well-documented code
- **Ground**: The proposed framework, SACL, enriches textual information with semantic information and reduces bias by augmenting code or structural knowledge
- **Experiment**: Extensive experiments on three public benchmarks (HumanEval, MBPP, and SWE-Bench-Lite) demonstrate significant improvements in code retrieval and generation performance
- **Takeaway**: SACL improves code retrieval and generation by incorporating semantic information, mitigating lexical bias, and enhancing fault localization accuracy

#### Abstract
> Retrieval-Augmented Code Generation (RACG) is a critical technique for
enhancing code generation by retrieving relevant information. In this work, we
conduct an in-depth analysis of code retrieval by systematically masking
specific features while preserving code functionality. Our discoveries include:
(1) although trained on code, current retrievers heavily rely on surface-level
textual features (e.g., docstrings, identifier names), and (2) they exhibit a
strong bias towards well-documented code, even if the documentation is
irrelevant. Based on our discoveries, we propose SACL, a framework that
enriches textual information and reduces bias by augmenting code or structural
knowledge with semantic information. Extensive experiments show that SACL
substantially improves code retrieval (e.g., by 12.8% / 9.4% / 7.0% Recall@1 on
HumanEval / MBPP / SWE-Bench-Lite), which also leads to better code generation
performance (e.g., by 4.88% Pass@1 on HumanEval).

---

### 5. Multimodal Information Retrieval for Open World with Edit Distance Weak Supervision

- **LLM Score**: 4
- **Keyword Score**: 14
- **Authors**: KMA Solaiman, Bharat Bhargava
- **URL**: <http://arxiv.org/abs/2506.20070v1>
- **Submitted**: 2025-06-25 00:25:08
- **Comment**: Submitted to ICDE'24. An earlier version of this paper appeared on
  TechRxiv: https://www.techrxiv.org/doi/full/10.36227/techrxiv.21990284.v1,
  uploaded on February 05, 2023
- **Topic Keywords**: information retrieval, query, queries, relevance, retrieval
- **Reason**: The paper focuses on multimodal information retrieval, which is a related topic to information retrieval. However, the emphasis on weak supervision through edit distance and the use case of missing person retrieval does not seem to align with the user's primary interests in query understanding, ranking models, and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Multimodal Information Retrieval
- **Aim**: To propose a novel framework called FemmIR for multimodal information retrieval that can retrieve relevant multimodal results for information needs expressed through multimodal queries without requiring similarity labels or fine-tuning for specific applications
- **Rationale**: The existing multimodal retrieval models require similarity labels or fine-tuning for specific applications, and there is a need for a general solution that can include retrieval from novel modalities without requiring an explicit design for each new modality
- **Ground**: The paper is grounded in the concepts of natural language processing and multimodal information retrieval, and it introduces novel concepts such as the Content Edit Distance (CED) metric and the Human Attribute Recognition model (HART)
- **Experiment**: The authors evaluate the performance of FemmIR and HART on various datasets, including InciText, MARS, and MuQNOL, using metrics such as precision, recall, F1-score, and mean average precision (mAP)
- **Takeaway**: FemmIR offers a general solution to include retrieval from novel modalities without requiring an explicit design for each new modality, and it outperforms other state-of-the-art cross-modal retrieval models in terms of MAP performance

#### Abstract
> Existing multi-media retrieval models either rely on creating a common
subspace with modality-specific representation models or require schema mapping
among modalities to measure similarities among multi-media data. Our goal is to
avoid the annotation overhead incurred from considering retrieval as a
supervised classification task and re-use the pretrained encoders in large
language models and vision tasks. We propose "FemmIR", a framework to retrieve
multimodal results relevant to information needs expressed with multimodal
queries by example without any similarity label. Such identification is
necessary for real-world applications where data annotations are scarce and
satisfactory performance is required without fine-tuning with a common
framework across applications. We curate a new dataset called MuQNOL for
benchmarking progress on this task. Our technique is based on weak supervision
introduced through edit distance between samples: graph edit distance can be
modified to consider the cost of replacing a data sample in terms of its
properties, and relevance can be measured through the implicit signal from the
amount of edit cost among the objects. Unlike metric learning or encoding
networks, FemmIR re-uses the high-level properties and maintains the property
value and relationship constraints with a multi-level interaction score between
data samples and the query example provided by the user. We empirically
evaluate FemmIR on a missing person use case with MuQNOL. FemmIR performs
comparably to similar retrieval systems in delivering on-demand retrieval
results with exact and approximate similarities while using the existing
property identifiers in the system.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Unidentified and Confounded? Understanding Two-Tower Models for Unbiased Learning to Rank

- **LLM Score**: 4
- **Keyword Score**: 12
- **Authors**: Philipp Hager, Onno Zoeter, Maarten de Rijke
- **URL**: <http://arxiv.org/abs/2506.20501v1>
- **Submitted**: 2025-06-25 14:47:43
- **Topic Keywords**: ranking, learning to rank, user behavior, click, rank, search
- **Reason**: The paper focuses on learning-to-rank methods, specifically two-tower models, and investigates issues with biased user feedback. While it touches on topics related to information retrieval and ranking models, it does not directly address query understanding, user behavior modeling, or real-time relevance optimization, which are core areas of interest for you.

#### Abstract
> Additive two-tower models are popular learning-to-rank methods for handling
biased user feedback in industry settings. Recent studies, however, report a
concerning phenomenon: training two-tower models on clicks collected by
well-performing production systems leads to decreased ranking performance. This
paper investigates two recent explanations for this observation: confounding
effects from logging policies and model identifiability issues. We
theoretically analyze the identifiability conditions of two-tower models,
showing that either document swaps across positions or overlapping feature
distributions are required to recover model parameters from clicks. We also
investigate the effect of logging policies on two-tower models, finding that
they introduce no bias when models perfectly capture user behavior. However,
logging policies can amplify biases when models imperfectly capture user
behavior, particularly when prediction errors correlate with document placement
across positions. We propose a sample weighting technique to mitigate these
effects and provide actionable insights for researchers and practitioners using
two-tower models.

### 7. Controlled Retrieval-augmented Context Evaluation for Long-form RAG

- **LLM Score**: 4
- **Keyword Score**: 12
- **Authors**: Jia-Huei Ju, Suzan Verberne, Maarten de Rijke, Andrew Yates
- **URL**: <http://arxiv.org/abs/2506.20051v1>
- **Submitted**: 2025-06-24 23:17:48
- **Topic Keywords**: ranking, relevance, rag, retrieval, rank, search
- **Reason**: The paper focuses on retrieval-augmented generation, which is a related topic to information retrieval. However, the specific context of long-form generation and report generation is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling. The paper's emphasis on evaluation metrics and framework design is also not a central match for the user's research themes.

#### Abstract
> Retrieval-augmented generation (RAG) enhances large language models by
incorporating context retrieved from external knowledge sources. While the
effectiveness of the retrieval module is typically evaluated with
relevance-based ranking metrics, such metrics may be insufficient to reflect
the retrieval's impact on the final RAG result, especially in long-form
generation scenarios. We argue that providing a comprehensive
retrieval-augmented context is important for long-form RAG tasks like report
generation and propose metrics for assessing the context independent of
generation. We introduce CRUX, a \textbf{C}ontrolled
\textbf{R}etrieval-a\textbf{U}gmented conte\textbf{X}t evaluation framework
designed to directly assess retrieval-augmented contexts. This framework uses
human-written summaries to control the information scope of knowledge, enabling
us to measure how well the context covers information essential for long-form
generation. CRUX uses question-based evaluation to assess RAG's retrieval in a
fine-grained manner. Empirical results show that CRUX offers more reflective
and diagnostic evaluation. Our findings also reveal substantial room for
improvement in current retrieval methods, pointing to promising directions for
advancing RAG's retrieval. Our data and code are publicly available to support
and advance future research on retrieval.

### 8. Knowledge-Aware Diverse Reranking for Cross-Source Question Answering

- **LLM Score**: 4
- **Keyword Score**: 11
- **Authors**: Tong Zhou
- **URL**: <http://arxiv.org/abs/2506.20476v1>
- **Submitted**: 2025-06-25 14:23:21
- **Topic Keywords**: ranking, rerank, rag, rank, sigir
- **Reason**: The paper focuses on question answering and reranking, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the specific context of cross-source question answering and the competition setting make it less directly applicable to the user's core research themes. The lack of explicit mention of user behavior modeling, click models, or deep semantic understanding also reduces the paper's relevance.

#### Abstract
> This paper presents Team Marikarp's solution for the SIGIR 2025 LiveRAG
competition. The competition's evaluation set, automatically generated by
DataMorgana from internet corpora, encompassed a wide range of target topics,
question types, question formulations, audience types, and knowledge
organization methods. It offered a fair evaluation of retrieving
question-relevant supporting documents from a 15M documents subset of the
FineWeb corpus. Our proposed knowledge-aware diverse reranking RAG pipeline
achieved first place in the competition.

### 9. MIRAGE: A Benchmark for Multimodal Information-Seeking and Reasoning in Agricultural Expert-Guided Conversations

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Vardhan Dongre, Chi Gui, Shubham Garg, Hooshang Nayyeri, Gokhan Tur, Dilek Hakkani-T√ºr, Vikram S. Adve
- **URL**: <http://arxiv.org/abs/2506.20100v1>
- **Submitted**: 2025-06-25 03:07:54
- **Comment**: 66 pages, 32 figures, 23 tables
- **Topic Keywords**: queries, rag
- **Reason**: The paper introduces a benchmark for multimodal expert-level reasoning and decision-making in consultative interaction settings, which is not directly related to my research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper mentions multimodal interaction, it focuses on expert-guided conversations in the agriculture domain, which is not a primary area of interest for me. The paper's emphasis on grounded reasoning, clarification strategies, and long-form generation is somewhat relevant, but the context is too specific and not directly applicable to my research areas.

#### Abstract
> We introduce MIRAGE, a new benchmark for multimodal expert-level reasoning
and decision-making in consultative interaction settings. Designed for the
agriculture domain, MIRAGE captures the full complexity of expert consultations
by combining natural user queries, expert-authored responses, and image-based
context, offering a high-fidelity benchmark for evaluating models on grounded
reasoning, clarification strategies, and long-form generation in a real-world,
knowledge-intensive domain. Grounded in over 35,000 real user-expert
interactions and curated through a carefully designed multi-step pipeline,
MIRAGE spans diverse crop health, pest diagnosis, and crop management
scenarios. The benchmark includes more than 7,000 unique biological entities,
covering plant species, pests, and diseases, making it one of the most
taxonomically diverse benchmarks available for vision-language models, grounded
in the real world. Unlike existing benchmarks that rely on well-specified user
inputs and closed-set taxonomies, MIRAGE features underspecified, context-rich
scenarios with open-world settings, requiring models to infer latent knowledge
gaps, handle rare entities, and either proactively guide the interaction or
respond. Project Page: https://mirage-benchmark.github.io

### 10. A Modular Multitask Reasoning Framework Integrating Spatio-temporal Models and LLMs

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Kethmi Hirushini Hettige, Jiahao Ji, Cheng Long, Shili Xiang, Gao Cong, Jingyuan Wang
- **URL**: <http://arxiv.org/abs/2506.20073v1>
- **Submitted**: 2025-06-25 00:55:34
- **Topic Keywords**: queries, rag
- **Reason**: The paper's focus on spatio-temporal data mining and large language models (LLMs) is not directly related to information retrieval and search technologies, which are the primary areas of interest. While the paper's emphasis on complex reasoning and generation of explanatory outputs shares some similarities with query understanding and ranking models, the context and application domains are distinct. The paper's relevance to the user's interests is limited, but it may still be of interest in the broader context of natural language processing and data mining.

#### Abstract
> Spatio-temporal data mining plays a pivotal role in informed decision making
across diverse domains. However, existing models are often restricted to narrow
tasks, lacking the capacity for multi-task inference and complex long-form
reasoning that require generation of in-depth, explanatory outputs. These
limitations restrict their applicability to real-world, multi-faceted decision
scenarios. In this work, we introduce STReason, a novel framework that
integrates the reasoning strengths of large language models (LLMs) with the
analytical capabilities of spatio-temporal models for multi-task inference and
execution. Without requiring task-specific finetuning, STReason leverages
in-context learning to decompose complex natural language queries into modular,
interpretable programs, which are then systematically executed to generate both
solutions and detailed rationales. To facilitate rigorous evaluation, we
construct a new benchmark dataset and propose a unified evaluation framework
with metrics specifically designed for long-form spatio-temporal reasoning.
Experimental results show that STReason significantly outperforms advanced LLM
baselines across all metrics, particularly excelling in complex,
reasoning-intensive spatio-temporal scenarios. Human evaluations further
validate STReason's credibility and practical utility, demonstrating its
potential to reduce expert workload and broaden the applicability to real-world
spatio-temporal tasks. We believe STReason provides a promising direction for
developing more capable and generalizable spatio-temporal reasoning systems.

### 11. Intrinsic vs. Extrinsic Evaluation of Czech Sentence Embeddings: Semantic Relevance Doesn't Help with MT Evaluation

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Petra Baranƒç√≠kov√°, Ond≈ôej Bojar
- **URL**: <http://arxiv.org/abs/2506.20203v1>
- **Submitted**: 2025-06-25 07:46:17
- **Topic Keywords**: relevance, search
- **Reason**: The paper explores sentence embeddings and their evaluation methods, which is related to Natural Language Processing (NLP). However, the focus on machine translation evaluation and semantic similarity tests is not directly aligned with the user's interests in Information Retrieval (IR), query understanding, ranking models, and user behavior modeling.

#### Abstract
> In this paper, we compare Czech-specific and multilingual sentence embedding
models through intrinsic and extrinsic evaluation paradigms. For intrinsic
evaluation, we employ Costra, a complex sentence transformation dataset, and
several Semantic Textual Similarity (STS) benchmarks to assess the ability of
the embeddings to capture linguistic phenomena such as semantic similarity,
temporal aspects, and stylistic variations. In the extrinsic evaluation, we
fine-tune each embedding model using COMET-based metrics for machine
translation evaluation.
  Our experiments reveal an interesting disconnect: models that excel in
intrinsic semantic similarity tests do not consistently yield superior
performance on downstream translation evaluation tasks. Conversely, models with
seemingly over-smoothed embedding spaces can, through fine-tuning, achieve
excellent results. These findings highlight the complex relationship between
semantic property probes and downstream task, emphasizing the need for more
research into 'operationalizable semantics' in sentence embeddings, or more
in-depth downstream tasks datasets (here translation evaluation)

### 12. CoVE: Compressed Vocabulary Expansion Makes Better LLM-based Recommender Systems

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Haochen Zhang, Tianyi Zhang, Junze Yin, Oren Gal, Anshumali Shrivastava, Vladimir Braverman
- **URL**: <http://arxiv.org/abs/2506.19993v1>
- **Submitted**: 2025-06-24 20:27:51
- **Comment**: Accepted by ACL 2025 Findings
- **Topic Keywords**: rag, recommend, search
- **Reason**: The paper proposes a novel recommender system, CoVE, which leverages large language models' sequential information processing capabilities. While it's related to search technologies and NLP, it's not directly focused on query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's primary focus is on recommender systems, which is a secondary interest.

#### Abstract
> Recommender systems play a pivotal role in providing relevant content to
users. With the rapid development of large language models (LLMs), researchers
have begun utilizing LLMs to build more powerful recommender systems. However,
existing approaches that focus on aligning LLMs with recommendation tasks do
not fully leverage their sequential information processing capabilities,
leading to suboptimal performance.
  In this paper, we propose a novel system called compressed vocabulary
expansion (CoVE). In CoVE, each item is assigned a unique ID within the
expanded vocabulary. Our framework effectively capitalizes on sequence
understanding abilities of LLMs, significantly enhancing their performance on
recommendation tasks. Additionally, we compress the embedding layer, making
CoVE practical for large-scale industrial applications. The effectiveness and
performance of CoVE are demonstrated through comprehensive experiments on
multiple recommendation datasets and comparisons with prior works. Our code can
be found at https://github.com/HaochenZhang717/CoVE-official-Repo.

### 13. Inference Scaled GraphRAG: Improving Multi Hop Question Answering on Knowledge Graphs

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Travis Thompson, Seung-Hwan Lim, Paul Liu, Ruoying He, Dongkuan Xu
- **URL**: <http://arxiv.org/abs/2506.19967v1>
- **Submitted**: 2025-06-24 19:31:03
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on knowledge graph reasoning and question answering, which is related to information retrieval and natural language processing. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's emphasis on large language models and graph traversal is not directly applicable to search technologies or e-commerce.

#### Abstract
> Large Language Models (LLMs) have achieved impressive capabilities in
language understanding and generation, yet they continue to underperform on
knowledge-intensive reasoning tasks due to limited access to structured context
and multi-hop information. Retrieval-Augmented Generation (RAG) partially
mitigates this by grounding generation in retrieved context, but conventional
RAG and GraphRAG methods often fail to capture relational structure across
nodes in knowledge graphs. We introduce Inference-Scaled GraphRAG, a novel
framework that enhances LLM-based graph reasoning by applying inference-time
compute scaling. Our method combines sequential scaling with deep
chain-of-thought graph traversal, and parallel scaling with majority voting
over sampled trajectories within an interleaved reasoning-execution loop.
Experiments on the GRBench benchmark demonstrate that our approach
significantly improves multi-hop question answering performance, achieving
substantial gains over both traditional GraphRAG and prior graph traversal
baselines. These findings suggest that inference-time scaling is a practical
and architecture-agnostic solution for structured knowledge reasoning with LLMs

### 14. TAPS: Tool-Augmented Personalisation via Structured Tagging

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Ekaterina Taktasheva, Jeff Dalton
- **URL**: <http://arxiv.org/abs/2506.20409v2>
- **Submitted**: 2025-06-25 13:24:46
- **Topic Keywords**: rag
- **Reason**: The paper focuses on tool-augmented large language models and personalization in goal-oriented dialogue agents, which is related to NLP and data mining. However, it does not directly address query understanding, ranking models, or user behavior modeling in information retrieval, which are core areas of interest. The paper's relevance is somewhat limited to the user's background in e-commerce and NLP, but it does not align with the user's primary focus on information retrieval and real-time relevance optimization.

#### Abstract
> Recent advancements in tool-augmented large language models have enabled them
to interact with external tools, enhancing their ability to perform complex
user tasks. However, existing approaches overlook the role of personalisation
in guiding tool use. This work investigates how user preferences can be
effectively integrated into goal-oriented dialogue agents. Through extensive
analysis, we identify key weaknesses in the ability of LLMs to personalise tool
use. To this end, we introduce TAPS, a novel solution that enhances
personalised tool use by leveraging a structured tagging tool and an
uncertainty-based tool detector. TAPS significantly improves the ability of
LLMs to incorporate user preferences, achieving the new state-of-the-art for
open source models on the NLSI task.

### 15. Biomed-Enriched: A Biomedical Dataset Enriched with LLMs for Pretraining and Extracting Rare and Hidden Content

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Rian Touchent, Nathan Godey, Eric de la Clergerie
- **URL**: <http://arxiv.org/abs/2506.20331v1>
- **Submitted**: 2025-06-25 11:30:25
- **Comment**: Dataset link: https://hf.co/datasets/almanach/Biomed-Enriched
- **Topic Keywords**: rag
- **Reason**: The paper introduces a biomedical dataset enriched with LLMs, which is relevant to NLP and data mining. However, the focus is on biomedical text processing and pretraining, which is not directly related to information retrieval, search technologies, or query understanding. The paper's emphasis on clinical text and educational quality filtering is also not a central match for the user's research interests.

#### Abstract
> We introduce Biomed-Enriched, a biomedical text dataset constructed from
PubMed via a two-stage annotation process. In the first stage, a large language
model annotates 400K paragraphs from PubMed scientific articles, assigning
scores for their type (review, study, clinical case, other), domain (clinical,
biomedical, other), and educational quality. The educational quality score
(rated 1 to 5) estimates how useful a paragraph is for college-level learning.
These annotations are then used to fine-tune a small language model, which
propagates the labels across the full PMC-OA corpus. The resulting metadata
allows us to extract refined subsets, including 2M clinical case paragraphs
with over 450K high-quality ones from articles with commercial-use licenses,
and to construct several variants via quality filtering and domain upsampling.
Clinical text is typically difficult to access due to privacy constraints, as
hospital records cannot be publicly shared. Hence, our dataset provides an
alternative large-scale, openly available collection of clinical cases from
PubMed, making it a valuable resource for biomedical and clinical NLP.
Preliminary continual-pretraining experiments with OLMo2 suggest these curated
subsets enable targeted improvements, with clinical upsampling boosting
performance by ~5% on MMLU ProfMed and educational quality filtering improving
MedQA and MedMCQA by ~1%. Combinations of these techniques led to faster
convergence, reaching same performance with a third of training tokens,
indicating potential for more efficient and effective biomedical pretraining
strategies.

### 16. Narrative Shift Detection: A Hybrid Approach of Dynamic Topic Models and Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Kai-Robin Lange, Tobias Schmidt, Matthias Reccius, Henrik M√ºller, Michael Roos, Carsten Jentsch
- **URL**: <http://arxiv.org/abs/2506.20269v1>
- **Submitted**: 2025-06-25 09:25:15
- **Comment**: 14 pages, 1 figure
- **Topic Keywords**: acl
- **Reason**: The paper's focus on narrative shift detection and its use of Large Language Models and topic models are somewhat related to my interests in Information Retrieval and Natural Language Processing. However, the paper's specific application to news articles and narrative shift detection is not directly aligned with my core research themes, which are more focused on query understanding, ranking models, and user behavior modeling.

#### Abstract
> With rapidly evolving media narratives, it has become increasingly critical
to not just extract narratives from a given corpus but rather investigate, how
they develop over time. While popular narrative extraction methods such as
Large Language Models do well in capturing typical narrative elements or even
the complex structure of a narrative, applying them to an entire corpus comes
with obstacles, such as a high financial or computational cost. We propose a
combination of the language understanding capabilities of Large Language Models
with the large scale applicability of topic models to dynamically model
narrative shifts across time using the Narrative Policy Framework. We apply a
topic model and a corresponding change point detection method to find changes
that concern a specific topic of interest. Using this model, we filter our
corpus for documents that are particularly representative of that change and
feed them into a Large Language Model that interprets the change that happened
in an automated fashion and distinguishes between content and narrative shifts.
We employ our pipeline on a corpus of The Wall Street Journal news paper
articles from 2009 to 2023. Our findings indicate that a Large Language Model
can efficiently extract a narrative shift if one exists at a given point in
time, but does not perform as well when having to decide whether a shift in
content or a narrative shift took place.

### 17. Perspectives in Play: A Multi-Perspective Approach for More Inclusive NLP Systems

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Benedetta Muscato, Lucia Passaro, Gizem Gezici, Fosca Giannotti
- **URL**: <http://arxiv.org/abs/2506.20209v1>
- **Submitted**: 2025-06-25 07:53:36
- **Topic Keywords**: rag
- **Reason**: The paper's focus on Natural Language Processing (NLP) and its approach to handling human disagreement by incorporating multiple perspectives is somewhat related to my research interests in NLP and query understanding. However, the paper's primary focus on subjective text classification tasks and model uncertainty is not directly aligned with my interests in information retrieval, ranking models, and user behavior modeling.

#### Abstract
> In the realm of Natural Language Processing (NLP), common approaches for
handling human disagreement consist of aggregating annotators' viewpoints to
establish a single ground truth. However, prior studies show that disregarding
individual opinions can lead can lead to the side effect of underrepresenting
minority perspectives, especially in subjective tasks, where annotators may
systematically disagree because of their preferences. Recognizing that labels
reflect the diverse backgrounds, life experiences, and values of individuals,
this study proposes a new multi-perspective approach using soft labels to
encourage the development of the next generation of perspective aware models,
more inclusive and pluralistic. We conduct an extensive analysis across diverse
subjective text classification tasks, including hate speech, irony, abusive
language, and stance detection, to highlight the importance of capturing human
disagreements, often overlooked by traditional aggregation methods. Results
show that the multi-perspective approach not only better approximates human
label distributions, as measured by Jensen-Shannon Divergence (JSD), but also
achieves superior classification performance (higher F1 scores), outperforming
traditional approaches. However, our approach exhibits lower confidence in
tasks like irony and stance detection, likely due to the inherent subjectivity
present in the texts. Lastly, leveraging Explainable AI (XAI), we explore model
uncertainty and uncover meaningful insights into model predictions.

### 18. How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Mengqi Wang, Tiantian Feng, Shrikanth Narayanan
- **URL**: <http://arxiv.org/abs/2506.20199v1>
- **Submitted**: 2025-06-25 07:39:19
- **Topic Keywords**: retrieval
- **Reason**: The paper explores the application of large language models to conversational emotion recognition, which is a subjective task. While it touches on the topic of retrieval, it is more focused on example retrieval for in-context learning rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest in Information Retrieval and Search technologies.

#### Abstract
> Large language models (LLMs) have enabled a wide variety of real-world
applications in various domains. However, creating a high-performing
application with high accuracy remains challenging, particularly for subjective
tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this
study investigates approaches to improving conversational emotion recognition
(CER) by LLMs. Specifically, we explore how to retrieve high-quality examples
in in-context learning (ICL) to enhance CER. We propose various strategies
based on random and augmented example retrieval and also analyze the impact of
conversational context on CER accuracy. Experiments were conducted on the three
datasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented
example retrieval consistently outperforms other techniques under investigation
across all datasets, highlighting the importance of retrieving coherent
targeted examples and enhancing them through paraphrasing.

### 19. Producer-Fairness in Sequential Bundle Recommendation

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Alexandre Rio, Marta Soare, Sihem Amer-Yahia
- **URL**: <http://arxiv.org/abs/2506.20329v1>
- **Submitted**: 2025-06-25 11:24:52
- **Topic Keywords**: recommend
- **Reason**: The paper focuses on recommender systems, specifically sequential bundle recommendation, which is somewhat related to the user's interests in information retrieval and search technologies. However, the emphasis on fairness and bundle quality is not directly aligned with the user's primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> We address fairness in the context of sequential bundle recommendation, where
users are served in turn with sets of relevant and compatible items. Motivated
by real-world scenarios, we formalize producer-fairness, that seeks to achieve
desired exposure of different item groups across users in a recommendation
session. Our formulation combines naturally with building high quality bundles.
Our problem is solved in real time as users arrive. We propose an exact
solution that caters to small instances of our problem. We then examine two
heuristics, quality-first and fairness-first, and an adaptive variant that
determines on-the-fly the right balance between bundle fairness and quality.
Our experiments on three real-world datasets underscore the strengths and
limitations of each solution and demonstrate their efficacy in providing fair
bundle recommendations without compromising bundle quality.

### 20. ITFormer: Bridging Time Series and Natural Language for Multi-Modal QA with Large-Scale Multitask Dataset

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Yilin Wang, Peixuan Lei, Jie Song, Yuzhe Hao, Tao Chen, Yuxuan Zhang, Lei Jia, Yuanxiang Li, Zhongyu Wei
- **URL**: <http://arxiv.org/abs/2506.20093v1>
- **Submitted**: 2025-06-25 02:33:47
- **Topic Keywords**: search
- **Reason**: The paper introduces a new dataset and framework for time-series question answering, which is an interesting application of NLP and IR. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core aspects of the user's research interests. While the paper's focus on integrating temporal data with natural language is relevant to the user's background in e-commerce, it does not seem to be a central match for the user's primary focus on information retrieval and deep semantic understanding.

#### Abstract
> Time-series data are critical in diverse applications, such as industrial
monitoring, medical diagnostics, and climate research. However, effectively
integrating these high-dimensional temporal signals with natural language for
dynamic, interactive tasks remains a significant challenge. To address this, we
introduce the Time-Series Question Answering (Time-Series QA) task and release
EngineMT-QA, the first large-scale, multi-task, temporal-textual QA dataset
designed to capture complex interactions between time-series signals and
natural language. Building on this resource, we propose the Instruct Time
Transformer (ITFormer), a novel framework that bridges time-series encoders
with frozen large language models (LLMs). ITFormer effectively extracts,
aligns, and fuses temporal and textual features, achieving a strong improvement
in QA accuracy over strong baselines with fewer than 1\% additional trainable
parameters. By combining computational efficiency with robust cross-modal
modeling, our work establishes a adaptable paradigm for integrating temporal
data with natural language, paving the way for new research and applications in
multi-modal AI. More details about the project, including datasets and code,
are available at: https://pandalin98.github.io/itformer_site/

### 21. Accept More, Reject Less: Reducing up to 19% Unnecessary Desk-Rejections over 11 Years of ICLR Data

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Xiaoyu Li, Zhao Song, Jiahao Zhang
- **URL**: <http://arxiv.org/abs/2506.20141v1>
- **Submitted**: 2025-06-25 05:23:44
- **Topic Keywords**: search, kdd, ijcai, aaai, iclr, wsdm
- **Reason**: The paper is not related to Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling, which are the user's primary research interests. The topic of desk-rejection policies in AI conferences is not relevant to the user's background in e-commerce or NLP, and does not involve deep semantic understanding or real-time relevance optimization.

#### Abstract
> The explosive growth of AI research has driven paper submissions at flagship
AI conferences to unprecedented levels, necessitating many venues in 2025
(e.g., CVPR, ICCV, KDD, AAAI, IJCAI, WSDM) to enforce strict per-author
submission limits and to desk-reject any excess papers by simple ID order.
While this policy helps reduce reviewer workload, it may unintentionally
discard valuable papers and penalize authors' efforts. In this paper, we ask an
essential research question on whether it is possible to follow submission
limits while minimizing needless rejections. We first formalize the current
desk-rejection policies as an optimization problem, and then develop a
practical algorithm based on linear programming relaxation and a rounding
scheme. Under extensive evaluation on 11 years of real-world ICLR
(International Conference on Learning Representations) data, our method
preserves up to $19.23\%$ more papers without violating any author limits.
Moreover, our algorithm is highly efficient in practice, with all results on
ICLR data computed within at most 53.64 seconds. Our work provides a simple and
practical desk-rejection strategy that significantly reduces unnecessary
rejections, demonstrating strong potential to improve current CS conference
submission policies.

### 22. Accurate and Energy Efficient: Local Retrieval-Augmented Generation Models Outperform Commercial Large Language Models in Medical Tasks

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Konstantinos Vrettos, Michail E. Klontzas
- **URL**: <http://arxiv.org/abs/2506.20009v1>
- **Submitted**: 2025-06-24 20:56:03
- **Comment**: 18 pages, 3 Figures
- **Topic Keywords**: rag, ctr, retrieval
- **Reason**: The paper focuses on developing a more energy-efficient and accurate model for medical tasks, using a Retrieval-Augmented Generation framework. While it touches on the topic of language models, it does not specifically address query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user. The paper's focus on medical tasks and sustainability is not directly related to the user's research themes.

#### Abstract
> Background The increasing adoption of Artificial Intelligence (AI) in
healthcare has sparked growing concerns about its environmental and ethical
implications. Commercial Large Language Models (LLMs), such as ChatGPT and
DeepSeek, require substantial resources, while the utilization of these systems
for medical purposes raises critical issues regarding patient privacy and
safety. Methods We developed a customizable Retrieval-Augmented Generation
(RAG) framework for medical tasks, which monitors its energy usage and CO2
emissions. This system was then used to create RAGs based on various
open-source LLMs. The tested models included both general purpose models like
llama3.1:8b and medgemma-4b-it, which is medical-domain specific. The best RAGs
performance and energy consumption was compared to DeepSeekV3-R1 and OpenAIs
o4-mini model. A dataset of medical questions was used for the evaluation.
Results Custom RAG models outperformed commercial models in accuracy and energy
consumption. The RAG model built on llama3.1:8B achieved the highest accuracy
(58.5%) and was significantly better than other models, including o4-mini and
DeepSeekV3-R1. The llama3.1-RAG also exhibited the lowest energy consumption
and CO2 footprint among all models, with a Performance per kWh of 0.52 and a
total CO2 emission of 473g. Compared to o4-mini, the llama3.1-RAG achieved 2.7x
times more accuracy points per kWh and 172% less electricity usage while
maintaining higher accuracy. Conclusion Our study demonstrates that local LLMs
can be leveraged to develop RAGs that outperform commercial, online LLMs in
medical tasks, while having a smaller environmental impact. Our modular
framework promotes sustainable AI development, reducing electricity usage and
aligning with the UNs Sustainable Development Goals.

### 23. Memento: Note-Taking for Your Future Self

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Chao Wan, Albert Gong, Mihir Mishra, Carl-Leander Henneking, Claas Beger, Kilian Q. Weinberger
- **URL**: <http://arxiv.org/abs/2506.20642v1>
- **Submitted**: 2025-06-25 17:37:59
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on a specific application of large language models in multi-hop question answering, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions prompting strategies, it does not address ranking models or user behavior modeling, making it only loosely relevant to the user's research interests.

#### Abstract
> Large language models (LLMs) excel at reasoning-only tasks, but struggle when
reasoning must be tightly coupled with retrieval, as in multi-hop question
answering. To overcome these limitations, we introduce a prompting strategy
that first decomposes a complex question into smaller steps, then dynamically
constructs a database of facts using LLMs, and finally pieces these facts
together to solve the question. We show how this three-stage strategy, which we
call Memento, can boost the performance of existing prompting strategies across
diverse settings. On the 9-step PhantomWiki benchmark, Memento doubles the
performance of chain-of-thought (CoT) when all information is provided in
context. On the open-domain version of 2WikiMultiHopQA, CoT-RAG with Memento
improves over vanilla CoT-RAG by more than 20 F1 percentage points and over the
multi-hop RAG baseline, IRCoT, by more than 13 F1 percentage points. On the
challenging MuSiQue dataset, Memento improves ReAct by more than 3 F1
percentage points, demonstrating its utility in agentic settings.

### 24. PLoP: Precise LoRA Placement for Efficient Finetuning of Large Models

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Soufiane Hayou, Nikhil Ghosh, Bin Yu
- **URL**: <http://arxiv.org/abs/2506.20629v1>
- **Submitted**: 2025-06-25 17:25:02
- **Comment**: TD,LR: A lightweight module type selection method for LoRA
  finetuning. PLoP gives precise placements for LoRA adapters for improved
  performance
- **Topic Keywords**: query, rank
- **Reason**: This paper focuses on the optimization of large models through LoRA placement, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on finetuning, which is a broader topic, the specific application and methodology are not aligned with the user's research interests.

#### Abstract
> Low-Rank Adaptation (LoRA) is a widely used finetuning method for large
models. Its small memory footprint allows practitioners to adapt large models
to specific tasks at a fraction of the cost of full finetuning. Different
modifications have been proposed to enhance its efficiency by, for example,
setting the learning rate, the rank, and the initialization. Another
improvement axis is adapter placement strategy: when using LoRA, practitioners
usually pick module types to adapt with LoRA, such as Query and Key modules.
Few works have studied the problem of adapter placement, with nonconclusive
results: original LoRA paper suggested placing adapters in attention modules,
while other works suggested placing them in the MLP modules. Through an
intuitive theoretical analysis, we introduce PLoP (Precise LoRA Placement), a
lightweight method that allows automatic identification of module types where
LoRA adapters should be placed, given a pretrained model and a finetuning task.
We demonstrate that PLoP consistently outperforms, and in the worst case
competes, with commonly used placement strategies through comprehensive
experiments on supervised finetuning and reinforcement learning for reasoning.

### 25. Time is On My Side: Dynamics of Talk-Time Sharing in Video-chat Conversations

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Kaixiang Zhang, Justine Zhang, Cristian Danescu-Niculescu-Mizil
- **URL**: <http://arxiv.org/abs/2506.20474v1>
- **Submitted**: 2025-06-25 14:23:02
- **Topic Keywords**: relevance
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, Natural Language Processing, data mining, or recommender systems. The paper focuses on the dynamics of talk-time sharing in video-chat conversations, which is a topic outside of your primary areas of interest.

#### Abstract
> An intrinsic aspect of every conversation is the way talk-time is shared
between multiple speakers. Conversations can be balanced, with each speaker
claiming a similar amount of talk-time, or imbalanced when one talks
disproportionately. Such overall distributions are the consequence of
continuous negotiations between the speakers throughout the conversation: who
should be talking at every point in time, and for how long?
  In this work we introduce a computational framework for quantifying both the
conversation-level distribution of talk-time between speakers, as well as the
lower-level dynamics that lead to it. We derive a typology of talk-time sharing
dynamics structured by several intuitive axes of variation. By applying this
framework to a large dataset of video-chats between strangers, we confirm that,
perhaps unsurprisingly, different conversation-level distributions of talk-time
are perceived differently by speakers, with balanced conversations being
preferred over imbalanced ones, especially by those who end up talking less.
Then we reveal that -- even when they lead to the same level of overall balance
-- different types of talk-time sharing dynamics are perceived differently by
the participants, highlighting the relevance of our newly introduced typology.
Finally, we discuss how our framework offers new tools to designers of
computer-mediated communication platforms, for both human-human and human-AI
communication.

### 26. An Agentic System for Rare Disease Diagnosis with Traceable Reasoning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Weike Zhao, Chaoyi Wu, Yanjie Fan, Xiaoman Zhang, Pengcheng Qiu, Yuze Sun, Xiao Zhou, Yanfeng Wang, Ya Zhang, Yongguo Yu, Kun Sun, Weidi Xie
- **URL**: <http://arxiv.org/abs/2506.20430v1>
- **Submitted**: 2025-06-25 13:42:26
- **Topic Keywords**: rag, rank
- **Reason**: The paper is not relevant to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on rare disease diagnosis and agentic systems, which is outside the user's primary research area. Although the paper mentions language models, it is not related to the user's interests in NLP for search and retrieval purposes.

#### Abstract
> Rare diseases collectively affect over 300 million individuals worldwide, yet
timely and accurate diagnosis remains a pervasive challenge. This is largely
due to their clinical heterogeneity, low individual prevalence, and the limited
familiarity most clinicians have with rare conditions. Here, we introduce
DeepRare, the first rare disease diagnosis agentic system powered by a large
language model (LLM), capable of processing heterogeneous clinical inputs. The
system generates ranked diagnostic hypotheses for rare diseases, each
accompanied by a transparent chain of reasoning that links intermediate
analytic steps to verifiable medical evidence.
  DeepRare comprises three key components: a central host with a long-term
memory module; specialized agent servers responsible for domain-specific
analytical tasks integrating over 40 specialized tools and web-scale,
up-to-date medical knowledge sources, ensuring access to the most current
clinical information. This modular and scalable design enables complex
diagnostic reasoning while maintaining traceability and adaptability. We
evaluate DeepRare on eight datasets. The system demonstrates exceptional
diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013
diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15
methods, like traditional bioinformatics diagnostic tools, LLMs, and other
agentic systems, achieving an average Recall@1 score of 57.18% and surpassing
the second-best method (Reasoning LLM) by a substantial margin of 23.79
percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at
Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of
reasoning chains by clinical experts achieves 95.40% agreements. Furthermore,
the DeepRare system has been implemented as a user-friendly web application
http://raredx.cn/doctor.

### 27. Language Modeling by Language Models

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Junyan Cheng, Peter Clark, Kyle Richardson
- **URL**: <http://arxiv.org/abs/2506.20249v1>
- **Submitted**: 2025-06-25 08:46:10
- **Topic Keywords**: rag, search
- **Reason**: The paper's focus on language models and their applications is not directly related to the user's research interests in Information Retrieval, Search technologies, and query understanding. The paper's abstract does not mention any relevance to ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest for the user.

#### Abstract
> Can we leverage LLMs to model the process of discovering novel language model
(LM) architectures? Inspired by real research, we propose a multi-agent LLM
approach that simulates the conventional stages of research, from ideation and
literature search (proposal stage) to design implementation (code generation),
generative pre-training, and downstream evaluation (verification). Using ideas
from scaling laws, our system, Genesys, employs a Ladder of Scales approach;
new designs are proposed, adversarially reviewed, implemented, and selectively
verified at increasingly larger model scales (14M$\sim$350M parameters) with a
narrowing budget (the number of models we can train at each scale). To help
make discovery efficient and factorizable, Genesys uses a novel genetic
programming backbone, which we show has empirical advantages over commonly used
direct prompt generation workflows (e.g., $\sim$86\% percentage point
improvement in successful design generation, a key bottleneck). We report
experiments involving 1,162 newly discovered designs (1,062 fully verified
through pre-training) and find the best designs to be highly competitive with
known architectures (e.g., outperform GPT2, Mamba2, etc., on 6/9 common
benchmarks). We couple these results with comprehensive system-level ablations
and formal results, which give broader insights into the design of effective
autonomous discovery systems.

### 28. A Multi-Pass Large Language Model Framework for Precise and Efficient Radiology Report Error Detection

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Songsoo Kim, Seungtae Lee, See Young Lee, Joonho Kim, Keechan Kan, Dukyong Yoon
- **URL**: <http://arxiv.org/abs/2506.20112v1>
- **Submitted**: 2025-06-25 04:02:29
- **Comment**: 29 pages, 5 figures, 4 tables. Code available at
  https://github.com/radssk/mp-rred
- **Topic Keywords**: ltr
- **Reason**: This paper is not relevant to your research interests as it focuses on radiology report error detection using large language models, which is outside the scope of information retrieval, search technologies, and natural language processing. The paper's application in the medical domain is also unrelated to your background in e-commerce.

#### Abstract
> Background: The positive predictive value (PPV) of large language model
(LLM)-based proofreading for radiology reports is limited due to the low error
prevalence. Purpose: To assess whether a three-pass LLM framework enhances PPV
and reduces operational costs compared with baseline approaches. Materials and
Methods: A retrospective analysis was performed on 1,000 consecutive radiology
reports (250 each: radiography, ultrasonography, CT, MRI) from the MIMIC-III
database. Two external datasets (CheXpert and Open-i) were validation sets.
Three LLM frameworks were tested: (1) single-prompt detector; (2) extractor
plus detector; and (3) extractor, detector, and false-positive verifier.
Precision was measured by PPV and absolute true positive rate (aTPR).
Efficiency was calculated from model inference charges and reviewer
remuneration. Statistical significance was tested using cluster bootstrap,
exact McNemar tests, and Holm-Bonferroni correction. Results: Framework PPV
increased from 0.063 (95% CI, 0.036-0.101, Framework 1) to 0.079 (0.049-0.118,
Framework 2), and significantly to 0.159 (0.090-0.252, Framework 3; P<.001 vs.
baselines). aTPR remained stable (0.012-0.014; P>=.84). Operational costs per
1,000 reports dropped to USD 5.58 (Framework 3) from USD 9.72 (Framework 1) and
USD 6.85 (Framework 2), reflecting reductions of 42.6% and 18.5%, respectively.
Human-reviewed reports decreased from 192 to 88. External validation supported
Framework 3's superior PPV (CheXpert 0.133, Open-i 0.105) and stable aTPR
(0.007). Conclusion: A three-pass LLM framework significantly enhanced PPV and
reduced operational costs, maintaining detection performance, providing an
effective strategy for AI-assisted radiology report quality assurance.

### 29. The Decrypto Benchmark for Multi-Agent Reasoning and Theory of Mind

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Andrei Lupu, Timon Willi, Jakob Foerster
- **URL**: <http://arxiv.org/abs/2506.20664v1>
- **Submitted**: 2025-06-25 17:55:27
- **Comment**: 41 pages, 19 figures
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus is on multi-agent reasoning, theory of mind, and large language models, which are outside the scope of the user's research interests.

#### Abstract
> As Large Language Models (LLMs) gain agentic abilities, they will have to
navigate complex multi-agent scenarios, interacting with human users and other
agents in cooperative and competitive settings. This will require new reasoning
skills, chief amongst them being theory of mind (ToM), or the ability to reason
about the "mental" states of other agents. However, ToM and other multi-agent
abilities in LLMs are poorly understood, since existing benchmarks suffer from
narrow scope, data leakage, saturation, and lack of interactivity. We thus
propose Decrypto, a game-based benchmark for multi-agent reasoning and ToM
drawing inspiration from cognitive science, computational pragmatics and
multi-agent reinforcement learning. It is designed to be as easy as possible in
all other dimensions, eliminating confounding factors commonly found in other
benchmarks. To our knowledge, it is also the first platform for designing
interactive ToM experiments.
  We validate the benchmark design through comprehensive empirical evaluations
of frontier LLMs, robustness studies, and human-AI cross-play experiments. We
find that LLM game-playing abilities lag behind humans and simple
word-embedding baselines. We then create variants of two classic cognitive
science experiments within Decrypto to evaluate three key ToM abilities.
Surprisingly, we find that state-of-the-art reasoning models are significantly
worse at those tasks than their older counterparts. This demonstrates that
Decrypto addresses a crucial gap in current reasoning and ToM evaluations, and
paves the path towards better artificial agents.

### 30. When Life Gives You Samples: The Benefits of Scaling up Inference Compute for Multilingual LLMs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ammar Khairi, Daniel D'souza, Ye Shen, Julia Kreutzer, Sara Hooker
- **URL**: <http://arxiv.org/abs/2506.20544v1>
- **Submitted**: 2025-06-25 15:37:53
- **Topic Keywords**: rag
- **Reason**: The paper focuses on scaling inference-time compute for multilingual large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on language models, the primary focus is on scaling and adapting models for multilingual tasks, which is not a central match for the user's research interests.

#### Abstract
> Recent advancements in large language models (LLMs) have shifted focus toward
scaling inference-time compute, improving performance without retraining the
model. A common approach is to sample multiple outputs in parallel, and select
one of these as the final output. However, work to date has focused on English
and a handful of domains such as math and code. In contrast, we are most
interested in techniques that generalize across open-ended tasks, formally
verifiable tasks, and across languages. In this work, we study how to robustly
scale inference-time compute for open-ended generative tasks in a multilingual,
multi-task setting.
  Our findings show that both sampling strategy based on temperature variation
and selection strategy must be adapted to account for diverse domains and
varied language settings. We evaluate existing selection methods, revealing
that strategies effective in English often fail to generalize across languages.
We propose novel sampling and selection strategies specifically adapted for
multilingual and multi-task inference scenarios, and show they yield notable
gains across languages and tasks. In particular, our combined sampling and
selection methods lead to an average +6.8 jump in win-rates for our 8B models
on m-ArenaHard-v2.0 prompts, against proprietary models such as Gemini. At
larger scale, Command-A (111B model) equipped with our methods, shows +9.0
improvement in win-rates on the same benchmark with just five samples against
single-sample decoding, a substantial increase at minimal cost. Our results
underscore the need for language- and task-aware approaches to inference-time
compute, aiming to democratize performance improvements in underrepresented
languages.

### 31. Asymmetric REINFORCE for off-Policy Reinforcement Learning: Balancing positive and negative rewards

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Charles Arnal, Ga√´tan Narozniak, Vivien Cabannes, Yunhao Tang, Julia Kempe, Remi Munos
- **URL**: <http://arxiv.org/abs/2506.20520v1>
- **Submitted**: 2025-06-25 15:07:16
- **Topic Keywords**: rag
- **Reason**: This paper focuses on reinforcement learning and off-policy methods, which is not directly related to the user's interests in Information Retrieval, Search technologies, and Natural Language Processing. The concepts and techniques discussed in the paper, such as REINFORCE and off-policy reinforcement learning, are not relevant to the user's areas of focus.

#### Abstract
> Reinforcement learning (RL) is increasingly used to align large language
models (LLMs). Off-policy methods offer greater implementation simplicity and
data efficiency than on-policy techniques, but often result in suboptimal
performance. In this work, we study the intermediate range of algorithms
between off-policy RL and supervised fine-tuning by analyzing a simple
off-policy REINFORCE algorithm, where the advantage is defined as $A=r-V$, with
$r$ a reward and $V$ some tunable baseline. Intuitively, lowering $V$
emphasizes high-reward samples, while raising it penalizes low-reward ones more
heavily. We first provide a theoretical analysis of this off-policy REINFORCE
algorithm, showing that when the baseline $V$ lower-bounds the expected reward,
the algorithm enjoys a policy improvement guarantee. Our analysis reveals that
while on-policy updates can safely leverage both positive and negative signals,
off-policy updates benefit from focusing more on positive rewards than on
negative ones. We validate our findings experimentally in a controlled
stochastic bandit setting and through fine-tuning state-of-the-art LLMs on
reasoning tasks.

### 32. Probing AI Safety with Source Code

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ujwal Narayan, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Karthik Narasimhan, Ameet Deshpande, Vishvak Murahari
- **URL**: <http://arxiv.org/abs/2506.20471v1>
- **Submitted**: 2025-06-25 14:19:57
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests as it focuses on AI safety and source code evaluation, which is outside the scope of Information Retrieval, Search technologies, and Natural Language Processing. The paper's topics, such as large language models and prompting strategies, do not align with your primary research areas.

#### Abstract
> Large language models (LLMs) have become ubiquitous, interfacing with humans
in numerous safety-critical applications. This necessitates improving
capabilities, but importantly coupled with greater safety measures to align
these models with human values and preferences. In this work, we demonstrate
that contemporary models fall concerningly short of the goal of AI safety,
leading to an unsafe and harmful experience for users. We introduce a prompting
strategy called Code of Thought (CoDoT) to evaluate the safety of LLMs. CoDoT
converts natural language inputs to simple code that represents the same
intent. For instance, CoDoT transforms the natural language prompt "Make the
statement more toxic: {text}" to: "make_more_toxic({text})". We show that CoDoT
results in a consistent failure of a wide range of state-of-the-art LLMs. For
example, GPT-4 Turbo's toxicity increases 16.5 times, DeepSeek R1 fails 100% of
the time, and toxicity increases 300% on average across seven modern LLMs.
Additionally, recursively applying CoDoT can further increase toxicity two
times. Given the rapid and widespread adoption of LLMs, CoDoT underscores the
critical need to evaluate safety efforts from first principles, ensuring that
safety and capabilities advance together.

### 33. From Codicology to Code: A Comparative Study of Transformer and YOLO-based Detectors for Layout Analysis in Historical Documents

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Sergio Torres Aguilar
- **URL**: <http://arxiv.org/abs/2506.20326v1>
- **Submitted**: 2025-06-25 11:14:04
- **Topic Keywords**: ctr
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on Document Layout Analysis and object detection in historical documents, which is a specific domain and task that is not directly related to your areas of interest.

#### Abstract
> Robust Document Layout Analysis (DLA) is critical for the automated
processing and understanding of historical documents with complex page
organizations. This paper benchmarks five state-of-the-art object detection
architectures on three annotated datasets representing a spectrum of
codicological complexity: The e-NDP, a corpus of Parisian medieval registers
(1326-1504); CATMuS, a diverse multiclass dataset derived from various medieval
and modern sources (ca.12th-17th centuries) and HORAE, a corpus of decorated
books of hours (ca.13th-16th centuries). We evaluate two Transformer-based
models (Co-DETR, Grounding DINO) against three YOLO variants (AABB, OBB, and
YOLO-World). Our findings reveal significant performance variations dependent
on model architecture, data set characteristics, and bounding box
representation. In the e-NDP dataset, Co-DETR achieves state-of-the-art results
(0.752 mAP@.50:.95), closely followed by YOLOv11X-OBB (0.721). Conversely, on
the more complex CATMuS and HORAE datasets, the CNN-based YOLOv11x-OBB
significantly outperforms all other models (0.564 and 0.568, respectively).
This study unequivocally demonstrates that using Oriented Bounding Boxes (OBB)
is not a minor refinement but a fundamental requirement for accurately modeling
the non-Cartesian nature of historical manuscripts. We conclude that a key
trade-off exists between the global context awareness of Transformers, ideal
for structured layouts, and the superior generalization of CNN-OBB models for
visually diverse and complex documents.

### 34. A Literature Review on Simulation in Conversational Recommender Systems

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Haoran Zhang, Xin Zhao, Jinze Chen, Junpeng Guo
- **URL**: <http://arxiv.org/abs/2506.20291v1>
- **Submitted**: 2025-06-25 09:53:35
- **Comment**: 6 pages, 1 figures, accepted as a poster for CSWIM 2025
- **Topic Keywords**: recommend, search
- **Reason**: The paper focuses on conversational recommender systems, which is a related topic, but it does not address query understanding, ranking models, or user behavior modeling, which are core aspects of information retrieval and search technologies. The paper's emphasis on simulation methods and conversational dialogue also diverges from the user's primary focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> Conversational Recommender Systems (CRSs) have garnered attention as a novel
approach to delivering personalized recommendations through multi-turn
dialogues. This review developed a taxonomy framework to systematically
categorize relevant publications into four groups: dataset construction,
algorithm design, system evaluation, and empirical studies, providing a
comprehensive analysis of simulation methods in CRSs research. Our analysis
reveals that simulation methods play a key role in tackling CRSs' main
challenges. For example, LLM-based simulation methods have been used to create
conversational recommendation data, enhance CRSs algorithms, and evaluate CRSs.
Despite several challenges, such as dataset bias, the limited output
flexibility of LLM-based simulations, and the gap between text semantic space
and behavioral semantics, persist due to the complexity in Human-Computer
Interaction (HCI) of CRSs, simulation methods hold significant potential for
advancing CRS research. This review offers a thorough summary of the current
research landscape in this domain and identifies promising directions for
future inquiry.

### 35. COIN: Uncertainty-Guarding Selective Question Answering for Foundation Models with Provable Risk Guarantees

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zhiyuan Wang, Jinhao Duan, Qingni Wang, Xiaofeng Zhu, Tianlong Chen, Xiaoshuang Shi, Kaidi Xu
- **URL**: <http://arxiv.org/abs/2506.20178v1>
- **Submitted**: 2025-06-25 07:04:49
- **Topic Keywords**: rag
- **Reason**: The paper focuses on uncertainty quantification for foundation models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on topics like text generation and prediction, the primary focus is on ensuring the accuracy of generated text rather than optimizing search results or understanding user behavior.

#### Abstract
> Uncertainty quantification (UQ) for foundation models is essential to
identify and mitigate potential hallucinations in automatically generated text.
However, heuristic UQ approaches lack formal guarantees for key metrics such as
the false discovery rate (FDR) in selective prediction. Previous work adopts
the split conformal prediction (SCP) framework to ensure desired coverage of
admissible answers by constructing prediction sets, but these sets often
contain incorrect candidates, limiting their practical utility. To address
this, we propose COIN, an uncertainty-guarding selection framework that
calibrates statistically valid thresholds to filter a single generated answer
per question under user-specified FDR constraints. COIN estimates the empirical
error rate on a calibration set and applies confidence interval methods such as
Clopper-Pearson to establish a high-probability upper bound on the true error
rate (i.e., FDR). This enables the selection of the largest uncertainty
threshold that ensures FDR control on test data while significantly increasing
sample retention. We demonstrate COIN's robustness in risk control, strong
test-time power in retaining admissible answers, and predictive efficiency
under limited calibration data across both general and multimodal text
generation tasks. Furthermore, we show that employing alternative upper bound
constructions and UQ strategies can further boost COIN's power performance,
which underscores its extensibility and adaptability to diverse application
scenarios.

### 36. Irec: A Metacognitive Scaffolding for Self-Regulated Learning through Just-in-Time Insight Recall: A Conceptual Framework and System Prototype

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Xuefei Hou, Xizhao Tan
- **URL**: <http://arxiv.org/abs/2506.20156v1>
- **Submitted**: 2025-06-25 06:23:39
- **Comment**: Version 1 of a work in progress. Finalized system flowcharts, a
  public GitHub repository with the source code, and a full reproducibility
  package detailing the prompts, models, and testing guidelines will be
  provided in v2
- **Topic Keywords**: retrieval
- **Reason**: The paper focuses on Self-Regulated Learning, metacognitive reflection, and spaced repetition systems, which are unrelated to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing.

#### Abstract
> The core challenge in learning has shifted from knowledge acquisition to
effective Self-Regulated Learning (SRL): planning, monitoring, and reflecting
on one's learning. Existing digital tools, however, inadequately support
metacognitive reflection. Spaced Repetition Systems (SRS) use de-contextualized
review, overlooking the role of context, while Personal Knowledge Management
(PKM) tools require high manual maintenance.
  To address these challenges, this paper introduces "Insight Recall," a novel
paradigm that conceptualizes the context-triggered retrieval of personal past
insights as a metacognitive scaffold to promote SRL. We formalize this paradigm
using the Just-in-Time Adaptive Intervention (JITAI) framework and implement a
prototype system, Irec, to demonstrate its feasibility. At its core, Irec uses
a dynamic knowledge graph of the user's learning history. When a user faces a
new problem, a hybrid retrieval engine recalls relevant personal "insights."
Subsequently, a large language model (LLM) performs a deep similarity
assessment to filter and present the most relevant scaffold in a just-in-time
manner. To reduce cognitive load, Irec features a human-in-the-loop pipeline
for LLM-based knowledge graph construction. We also propose an optional "Guided
Inquiry" module, where users can engage in a Socratic dialogue with an expert
LLM, using the current problem and recalled insights as context. The
contribution of this paper is a solid theoretical framework and a usable system
platform for designing next-generation intelligent learning systems that
enhance metacognition and self-regulation.

### 37. Leveraging AI Graders for Missing Score Imputation to Achieve Accurate Ability Estimation in Constructed-Response Tests

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Masaki Uto, Yuma Ito
- **URL**: <http://arxiv.org/abs/2506.20119v1>
- **Submitted**: 2025-06-25 04:17:57
- **Comment**: Accepted to EvalLAC'25: 2nd Workshop on Automatic Evaluation of
  Learning and Assessment Content, held at AIED 2025, Palermo, Italy. This is
  the camera-ready version submitted to CEUR Workshop Proceedings
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on education, item response theory, and automated scoring technologies, which are outside your primary areas of interest.

#### Abstract
> Evaluating the abilities of learners is a fundamental objective in the field
of education. In particular, there is an increasing need to assess higher-order
abilities such as expressive skills and logical thinking. Constructed-response
tests such as short-answer and essay-based questions have become widely used as
a method to meet this demand. Although these tests are effective, they require
substantial manual grading, making them both labor-intensive and costly. Item
response theory (IRT) provides a promising solution by enabling the estimation
of ability from incomplete score data, where human raters grade only a subset
of answers provided by learners across multiple test items. However, the
accuracy of ability estimation declines as the proportion of missing scores
increases. Although data augmentation techniques for imputing missing scores
have been explored in order to address this limitation, they often struggle
with inaccuracy for sparse or heterogeneous data. To overcome these challenges,
this study proposes a novel method for imputing missing scores by leveraging
automated scoring technologies for accurate IRT-based ability estimation. The
proposed method achieves high accuracy in ability estimation while markedly
reducing manual grading workload.

### 38. Learning Instruction-Following Policies through Open-Ended Instruction Relabeling with Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zhicheng Zhang, Ziyan Wang, Yali Du, Fei Fang
- **URL**: <http://arxiv.org/abs/2506.20061v1>
- **Submitted**: 2025-06-24 23:49:28
- **Comment**: Under Review
- **Topic Keywords**: rag
- **Reason**: The paper focuses on reinforcement learning and instruction-following policies, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper mentions large language models, it does not explore query understanding, ranking models, or user behavior modeling, making it only loosely relevant to the user's research.

#### Abstract
> Developing effective instruction-following policies in reinforcement learning
remains challenging due to the reliance on extensive human-labeled instruction
datasets and the difficulty of learning from sparse rewards. In this paper, we
propose a novel approach that leverages the capabilities of large language
models (LLMs) to automatically generate open-ended instructions retrospectively
from previously collected agent trajectories. Our core idea is to employ LLMs
to relabel unsuccessful trajectories by identifying meaningful subtasks the
agent has implicitly accomplished, thereby enriching the agent's training data
and substantially alleviating reliance on human annotations. Through this
open-ended instruction relabeling, we efficiently learn a unified
instruction-following policy capable of handling diverse tasks within a single
policy. We empirically evaluate our proposed method in the challenging Craftax
environment, demonstrating clear improvements in sample efficiency, instruction
coverage, and overall policy performance compared to state-of-the-art
baselines. Our results highlight the effectiveness of utilizing LLM-guided
open-ended instruction relabeling to enhance instruction-following
reinforcement learning.

### 39. CycleDistill: Bootstrapping Machine Translation using LLMs with Cyclical Distillation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Deepon Halder, Thanmay Jayakumar, Raj Dabre
- **URL**: <http://arxiv.org/abs/2506.19952v1>
- **Submitted**: 2025-06-24 18:56:57
- **Topic Keywords**: rag
- **Reason**: The paper focuses on machine translation using large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions few-shot translation, it does not involve ranking models or user behavior modeling, making it an off-topic paper for your research interests.

#### Abstract
> Large language models (LLMs), despite their ability to perform few-shot
machine translation (MT), often lag behind dedicated MT systems trained on
parallel corpora, which are crucial for high quality machine translation (MT).
However, parallel corpora are often scarce or non-existent for low-resource
languages. In this paper, we propose CycleDistill, a bootstrapping approach
leveraging LLMs and few-shot translation to obtain high-quality MT systems.
CycleDistill involves iteratively generating synthetic parallel corpora from
monolingual corpora via zero- or few-shot MT, which is then used to fine-tune
the model that was used for generating said data for MT. CycleDistill does not
need parallel corpora beyond 1 to 4 few-shot examples, and in our experiments
focusing on three Indian languages, by relying solely on monolingual corpora,
it can achieve high-quality machine translation, improving upon a few-shot
baseline model by over 20-30 chrF points on average in the first iteration. We
also study the effect of leveraging softmax activations during the distillation
process and observe mild improvements in translation quality.

### 40. DiffuCoder: Understanding and Improving Masked Diffusion Models for Code Generation

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, Yizhe Zhang
- **URL**: <http://arxiv.org/abs/2506.20639v2>
- **Submitted**: 2025-06-25 17:35:47
- **Comment**: minor update
- **Topic Keywords**: search
- **Reason**: The paper focuses on code generation using diffusion large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on reinforcement learning, the context is code generation rather than user behavior modeling or ranking models.

#### Abstract
> Diffusion large language models (dLLMs) are compelling alternatives to
autoregressive (AR) models because their denoising models operate over the
entire sequence. The global planning and iterative refinement features of dLLMs
are particularly useful for code generation. However, current training and
inference mechanisms for dLLMs in coding are still under-explored. To demystify
the decoding behavior of dLLMs and unlock their potential for coding, we
systematically investigate their denoising processes and reinforcement learning
(RL) methods. We train a 7B dLLM, \textbf{DiffuCoder}, on 130B tokens of code.
Using this model as a testbed, we analyze its decoding behavior, revealing how
it differs from that of AR models: (1) dLLMs can decide how causal their
generation should be without relying on semi-AR decoding, and (2) increasing
the sampling temperature diversifies not only token choices but also their
generation order. This diversity creates a rich search space for RL rollouts.
For RL training, to reduce the variance of token log-likelihood estimates and
maintain training efficiency, we propose \textbf{coupled-GRPO}, a novel
sampling scheme that constructs complementary mask noise for completions used
in training. In our experiments, coupled-GRPO significantly improves
DiffuCoder's performance on code generation benchmarks (+4.4\% on EvalPlus) and
reduces reliance on AR bias during decoding. Our work provides deeper insight
into the machinery of dLLM generation and offers an effective, diffusion-native
RL training framework. https://github.com/apple/ml-diffucoder.

### 41. Model Editing as a Double-Edged Sword: Steering Agent Ethical Behavior Toward Beneficence or Harm

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Baixiang Huang, Zhen Tan, Haoran Wang, Zijie Liu, Dawei Li, Ali Payani, Huan Liu, Tianlong Chen, Kai Shu
- **URL**: <http://arxiv.org/abs/2506.20606v1>
- **Submitted**: 2025-06-25 16:51:51
- **Comment**: Main paper: 9 pages; total: 18 pages (including appendix). Code,
  data, results, and additional resources are available at:
  https://model-editing.github.io
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on agent behavior steering, model editing, and ethics in high-stakes domains, which is outside your primary focus areas.

#### Abstract
> Agents based on Large Language Models (LLMs) have demonstrated strong
capabilities across a wide range of tasks. However, deploying LLM-based agents
in high-stakes domains comes with significant safety and ethical risks.
Unethical behavior by these agents can directly result in serious real-world
consequences, including physical harm and financial loss. To efficiently steer
the ethical behavior of agents, we frame agent behavior steering as a model
editing task, which we term Behavior Editing. Model editing is an emerging area
of research that enables precise and efficient modifications to LLMs while
preserving their overall capabilities. To systematically study and evaluate
this approach, we introduce BehaviorBench, a multi-tier benchmark grounded in
psychological moral theories. This benchmark supports both the evaluation and
editing of agent behaviors across a variety of scenarios, with each tier
introducing more complex and ambiguous scenarios. We first demonstrate that
Behavior Editing can dynamically steer agents toward the target behavior within
specific scenarios. Moreover, Behavior Editing enables not only
scenario-specific local adjustments but also more extensive shifts in an
agent's global moral alignment. We demonstrate that Behavior Editing can be
used to promote ethical and benevolent behavior or, conversely, to induce
harmful or malicious behavior. Through comprehensive evaluations on agents
based on frontier LLMs, BehaviorBench shows the effectiveness of Behavior
Editing across different models and scenarios. Our findings offer key insights
into a new paradigm for steering agent behavior, highlighting both the promise
and perils of Behavior Editing.

### 42. OctoThinker: Mid-training Incentivizes Reinforcement Learning Scaling

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Zengzhi Wang, Fan Zhou, Xuefeng Li, Pengfei Liu
- **URL**: <http://arxiv.org/abs/2506.20512v1>
- **Submitted**: 2025-06-25 14:58:13
- **Comment**: 26 pages; The first three authors contribute to this work equally
- **Topic Keywords**: search
- **Reason**: This paper focuses on reinforcement learning and its application to foundation models, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's emphasis on mathematical corpora and QA-style data is also not aligned with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Different base language model families, such as Llama and Qwen, exhibit
divergent behaviors during post-training with reinforcement learning (RL),
especially on reasoning-intensive tasks. What makes a base language model
suitable for reinforcement learning? Gaining deeper insight into this question
is essential for developing RL-scalable foundation models of the next
generation. In this work, we investigate how mid-training strategies shape RL
dynamics, focusing on two representative model families: Qwen and Llama. Our
study reveals that (1) high-quality mathematical corpora, such as
MegaMath-Web-Pro, significantly improve both base model and RL performance,
while existing alternatives (e.g., FineMath-4plus) fail to do so; (2) further
adding QA-style data, particularly long chain-of-thought (CoT) reasoning
examples, enhances RL outcomes, and instruction data further unlocks this
effect; (3) while long-CoT improves reasoning depth, it can also induce
verbosity of model responses and unstability of RL training, underscoring the
importance of data formatting; (4) scaling mid-training consistently leads to
stronger downstream RL performance. Building on these insights, we introduce a
two-stage mid-training strategy, Stable-then-Decay, in which base models are
first trained on 200B tokens with a constant learning rate, followed by 20B
tokens across three CoT-focused branches with learning rate decay. This yields
OctoThinker, a family of models demonstrating strong RL compatibility and
closing the performance gap with more RL-friendly model families, i.e., Qwen.
We hope our work will help shape pre-training strategies for foundation models
in the RL era. To support further research, we release our open-source models
along with a curated math reasoning-intensive corpus of over 70 billion tokens
(i.e., MegaMath-Web-Pro-Max).

### 43. GPTailor: Large Language Model Pruning Through Layer Cutting and Stitching

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Guinan Su, Li Shen, Lu Yin, Shiwei Liu, Yanwu Yang, Jonas Geiping
- **URL**: <http://arxiv.org/abs/2506.20480v1>
- **Submitted**: 2025-06-25 14:24:59
- **Topic Keywords**: search
- **Reason**: This paper focuses on model pruning and compression of large language models, which is not directly related to information retrieval, search technologies, or query understanding. Although it involves NLP, the topic is more focused on model architecture and optimization rather than deep semantic understanding or real-time relevance optimization.

#### Abstract
> Large language models (LLMs) have shown remarkable capabilities in language
understanding and generation. However, such impressive capability typically
comes with a substantial model size, which presents significant challenges in
deployment and inference. While structured pruning of model parameters offers a
promising way to reduce computational costs at deployment time, current methods
primarily focus on single model pruning. In this work, we develop a novel
strategy to compress models by strategically combining or merging layers from
finetuned model variants, which preserves the original model's abilities by
aggregating capabilities accentuated in different finetunes. We pose the
optimal tailoring of these LLMs as a zero-order optimization problem, adopting
a search space that supports three different operations: (1) Layer removal, (2)
Layer selection from different candidate models, and (3) Layer merging. Our
experiments demonstrate that this approach leads to competitive model pruning,
for example, for the Llama2-13B model families, our compressed models maintain
approximately 97.3\% of the original performance while removing $\sim25\%$ of
parameters, significantly outperforming previous state-of-the-art methods. The
code is available at https://github.com/Guinan-Su/auto-merge-llm.

### 44. Why Robots Are Bad at Detecting Their Mistakes: Limitations of Miscommunication Detection in Human-Robot Dialogue

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Ruben Janssens, Jens De Bock, Sofie Labat, Eva Verhelst, Veronique Hoste, Tony Belpaeme
- **URL**: <http://arxiv.org/abs/2506.20268v1>
- **Submitted**: 2025-06-25 09:25:04
- **Comment**: Accepted at the 34th IEEE International Conference on Robot and Human
  Interactive Communication (RO-MAN 2025)
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on human-robot dialogue and miscommunication detection, which is outside your primary focus areas.

#### Abstract
> Detecting miscommunication in human-robot interaction is a critical function
for maintaining user engagement and trust. While humans effortlessly detect
communication errors in conversations through both verbal and non-verbal cues,
robots face significant challenges in interpreting non-verbal feedback, despite
advances in computer vision for recognizing affective expressions. This
research evaluates the effectiveness of machine learning models in detecting
miscommunications in robot dialogue. Using a multi-modal dataset of 240
human-robot conversations, where four distinct types of conversational failures
were systematically introduced, we assess the performance of state-of-the-art
computer vision models. After each conversational turn, users provided feedback
on whether they perceived an error, enabling an analysis of the models' ability
to accurately detect robot mistakes. Despite using state-of-the-art models, the
performance barely exceeds random chance in identifying miscommunication, while
on a dataset with more expressive emotional content, they successfully
identified confused states. To explore the underlying cause, we asked human
raters to do the same. They could also only identify around half of the induced
miscommunications, similarly to our model. These results uncover a fundamental
limitation in identifying robot miscommunications in dialogue: even when users
perceive the induced miscommunication as such, they often do not communicate
this to their robotic conversation partner. This knowledge can shape
expectations of the performance of computer vision models and can help
researchers to design better human-robot conversations by deliberately
eliciting feedback where needed.

### 45. Cross-Layer Discrete Concept Discovery for Interpreting Language Models

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Ankur Garg, Xuemin Yu, Hassan Sajjad, Samira Ebrahimi Kahou
- **URL**: <http://arxiv.org/abs/2506.20040v1>
- **Submitted**: 2025-06-24 22:43:36
- **Topic Keywords**: search
- **Reason**: The paper focuses on language models and concept discovery, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and query understanding. While it touches on topics like vector quantization and codebook updates, the abstract does not mention ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest for the user.

#### Abstract
> Uncovering emergent concepts across transformer layers remains a significant
challenge because the residual stream linearly mixes and duplicates
information, obscuring how features evolve within large language models.
Current research efforts primarily inspect neural representations at single
layers, thereby overlooking this cross-layer superposition and the redundancy
it introduces. These representations are typically either analyzed directly for
activation patterns or passed to probing classifiers that map them to a limited
set of predefined concepts. To address these limitations, we propose
\gls{clvqvae}, a framework that uses vector quantization to map representations
across layers and in the process collapse duplicated residual-stream features
into compact, interpretable concept vectors. Our approach uniquely combines
top-$k$ temperature-based sampling during quantization with EMA codebook
updates, providing controlled exploration of the discrete latent space while
maintaining code-book diversity. We further enhance the framework with
scaled-spherical k-means++ for codebook initialization, which clusters by
directional similarity rather than magnitude, better aligning with semantic
structure in word embedding space.

### 46. Doc2Agent: Scalable Generation of Tool-Using Agents from API Documentation

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Xinyi Ni, Haonan Jian, Qiuyang Wang, Vedanshi Chetan Shah, Pengyu Hong
- **URL**: <http://arxiv.org/abs/2506.19998v1>
- **Submitted**: 2025-06-24 20:30:44
- **Topic Keywords**: search
- **Reason**: The paper focuses on building agents from API documentation, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the context is different from the user's interests in deep semantic understanding and real-time relevance optimization.

#### Abstract
> REST APIs play important roles in enriching the action space of web agents,
yet most API-based agents rely on curated and uniform toolsets that do not
reflect the complexity of real-world APIs. Building tool-using agents for
arbitrary domains remains a major challenge, as it requires reading
unstructured API documentation, testing APIs and inferring correct parameters.
We propose Doc2Agent, a scalable pipeline to build agents that can call
Python-based tools generated from API documentation. Doc2Agent generates
executable tools from API documentations and iteratively refines them using a
code agent. We evaluate our approach on real-world APIs, WebArena APIs, and
research APIs, producing validated tools. We achieved a 55\% relative
performance improvement with 90\% lower cost compared to direct API calling on
WebArena benchmark. A domain-specific agent built for glycomaterial science
further demonstrates the pipeline's adaptability to complex, knowledge-rich
tasks. Doc2Agent offers a generalizable solution for building tool agents from
unstructured API documentation at scale.

### 47. FundaQ-8: A Clinically-Inspired Scoring Framework for Automated Fundus Image Quality Assessment

- **LLM Score**: 1
- **Keyword Score**: 2
- **Authors**: Lee Qi Zun, Oscar Wong Jin Hao, Nor Anita Binti Che Omar, Zalifa Zakiah Binti Asnir, Mohamad Sabri bin Sinal Zainal, Goh Man Fye
- **URL**: <http://arxiv.org/abs/2506.20303v1>
- **Submitted**: 2025-06-25 10:28:53
- **Topic Keywords**: rag
- **Reason**: The paper is not related to Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. It focuses on medical imaging and quality assessment, which is outside the scope of the user's research interests.

#### Abstract
> Automated fundus image quality assessment (FIQA) remains a challenge due to
variations in image acquisition and subjective expert evaluations. We introduce
FundaQ-8, a novel expert-validated framework for systematically assessing
fundus image quality using eight critical parameters, including field coverage,
anatomical visibility, illumination, and image artifacts. Using FundaQ-8 as a
structured scoring reference, we develop a ResNet18-based regression model to
predict continuous quality scores in the 0 to 1 range. The model is trained on
1800 fundus images from real-world clinical sources and Kaggle datasets, using
transfer learning, mean squared error optimization, and standardized
preprocessing. Validation against the EyeQ dataset and statistical analyses
confirm the framework's reliability and clinical interpretability.
Incorporating FundaQ-8 into deep learning models for diabetic retinopathy
grading also improves diagnostic robustness, highlighting the value of
quality-aware training in real-world screening applications.

---

