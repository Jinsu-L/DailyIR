# Daily Papers Report - 2025-11-18

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. PolicyBot - Reliable Question Answering over Policy Documents

- **LLM Score**: 8
- **Keyword Score**: 14
- **Authors**: Gautam Nagarajan, Omir Kumar, Sudarsun Santhiappan
- **URL**: <http://arxiv.org/abs/2511.13489v1>
- **Submitted**: 2025-11-17 15:26:10
- **Topic Keywords**: queries, ranking, rerank, rag, retrieval, rank
- **Reason**: This paper aligns well with your interests in Information Retrieval, particularly in query understanding and ranking models. The focus on retrieval-augmented generation and multilingual dense embeddings is relevant to your background in e-commerce and NLP. However, the specific application to policy documents and governance-related contexts is somewhat outside your primary focus on real-time relevance optimization and deep semantic understanding.

#### Abstract
> All citizens of a country are affected by the laws and policies introduced by their government. These laws and policies serve essential functions for citizens. Such as granting them certain rights or imposing specific obligations. However, these documents are often lengthy, complex, and difficult to navigate, making it challenging for citizens to locate and understand relevant information. This work presents PolicyBot, a retrieval-augmented generation (RAG) system designed to answer user queries over policy documents with a focus on transparency and reproducibility. The system combines domain-specific semantic chunking, multilingual dense embeddings, multi-stage retrieval with reranking, and source-aware generation to provide responses grounded in the original documents. We implemented citation tracing to reduce hallucinations and improve user trust, and evaluated alternative retrieval and generation configurations to identify effective design choices. The end-to-end pipeline is built entirely with open-source tools, enabling easy adaptation to other domains requiring document-grounded question answering. This work highlights design considerations, practical challenges, and lessons learned in deploying trustworthy RAG systems for governance-related contexts.

---

### 2. Cog-RAG: Cognitive-Inspired Dual-Hypergraph with Theme Alignment Retrieval-Augmented Generation

- **LLM Score**: 8
- **Keyword Score**: 11
- **Authors**: Hao Hu, Yifan Feng, Ruoxue Li, Rundong Xue, Xingliang Hou, Zhiqiang Tian, Yue Gao, Shaoyi Du
- **URL**: <http://arxiv.org/abs/2511.13201v1>
- **Submitted**: 2025-11-17 10:10:33
- **Comment**: Accepted by AAAI 2026 main conference
- **Topic Keywords**: query, pairwise, rag, retrieval, search
- **Reason**: This paper proposes a novel framework, Cog-RAG, that leverages dual-hypergraphs to capture high-order semantic relations and thematic structure, which aligns with your interests in query understanding, ranking models, and deep semantic understanding in Information Retrieval. Although it's primarily focused on NLP and generation, the use of hypergraphs and cognitive-inspired retrieval strategy demonstrates relevance to your broader interests in Search technologies and user behavior modeling.

#### Abstract
> Retrieval-Augmented Generation (RAG) enhances the response quality and domain-specific performance of large language models (LLMs) by incorporating external knowledge to combat hallucinations. In recent research, graph structures have been integrated into RAG to enhance the capture of semantic relations between entities. However, it primarily focuses on low-order pairwise entity relations, limiting the high-order associations among multiple entities. Hypergraph-enhanced approaches address this limitation by modeling multi-entity interactions via hyperedges, but they are typically constrained to inter-chunk entity-level representations, overlooking the global thematic organization and alignment across chunks. Drawing inspiration from the top-down cognitive process of human reasoning, we propose a theme-aligned dual-hypergraph RAG framework (Cog-RAG) that uses a theme hypergraph to capture inter-chunk thematic structure and an entity hypergraph to model high-order semantic relations. Furthermore, we design a cognitive-inspired two-stage retrieval strategy that first activates query-relevant thematic content from the theme hypergraph, and then guides fine-grained recall and diffusion in the entity hypergraph, achieving semantic alignment and consistent generation from global themes to local details. Our extensive experiments demonstrate that Cog-RAG significantly outperforms existing state-of-the-art baseline approaches.

---

### 3. Attention Grounded Enhancement for Visual Document Retrieval

- **LLM Score**: 6
- **Keyword Score**: 16
- **Authors**: Wanqing Cui, Wei Huang, Yazhi Guo, Yibo Hu, Meiguang Jin, Junfeng Ma, Keping Bi
- **URL**: <http://arxiv.org/abs/2511.13415v1>
- **Submitted**: 2025-11-17 14:28:41
- **Topic Keywords**: retriever, query, queries, relevance, rag, retrieval
- **Reason**: The paper explores visual document retrieval, leveraging cross-modal attention to identify relevant document regions. While it touches on query understanding and ranking models, its focus on visual content and multimodal large language models is somewhat related to your interests in Information Retrieval and Natural Language Processing, but not a central match.

#### Abstract
> Visual document retrieval requires understanding heterogeneous and multi-modal content to satisfy information needs. Recent advances use screenshot-based document encoding with fine-grained late interaction, significantly improving retrieval performance. However, retrievers are still trained with coarse global relevance labels, without revealing which regions support the match. As a result, retrievers tend to rely on surface-level cues and struggle to capture implicit semantic connections, hindering their ability to handle non-extractive queries. To alleviate this problem, we propose a \textbf{A}ttention-\textbf{G}rounded \textbf{RE}triever \textbf{E}nhancement (AGREE) framework. AGREE leverages cross-modal attention from multimodal large language models as proxy local supervision to guide the identification of relevant document regions. During training, AGREE combines local signals with the global signals to jointly optimize the retriever, enabling it to learn not only whether documents match, but also which content drives relevance. Experiments on the challenging ViDoRe V2 benchmark show that AGREE significantly outperforms the global-supervision-only baseline. Quantitative and qualitative analyses further demonstrate that AGREE promotes deeper alignment between query terms and document regions, moving beyond surface-level matching toward more accurate and interpretable retrieval. Our code is available at: https://anonymous.4open.science/r/AGREE-2025.

---

### 4. Exploring Multi-Table Retrieval Through Iterative Search

- **LLM Score**: 6
- **Keyword Score**: 11
- **Authors**: Allaa Boutaleb, Bernd Amann, Rafael Angarita, Hubert Naacke
- **URL**: <http://arxiv.org/abs/2511.13418v1>
- **Submitted**: 2025-11-17 14:31:33
- **Comment**: Accepted @ the AI for Tabular Data Workshop, EurIPS 2025
- **Topic Keywords**: query, relevance, rag, retrieval, search
- **Reason**: This paper explores multi-table retrieval, which is somewhat related to information retrieval and query understanding. While it doesn't directly focus on ranking models or user behavior modeling, it does involve semantic relevance and real-time relevance optimization, which aligns with your interests. However, the specific domain and application (open-domain question answering over datalakes) is not a central match to your e-commerce background or primary focus on IR.

#### Abstract
> Open-domain question answering over datalakes requires retrieving and composing information from multiple tables, a challenging subtask that demands semantic relevance and structural coherence (e.g., joinability). While exact optimization methods like Mixed-Integer Programming (MIP) can ensure coherence, their computational complexity is often prohibitive. Conversely, simpler greedy heuristics that optimize for query coverage alone often fail to find these coherent, joinable sets. This paper frames multi-table retrieval as an iterative search process, arguing this approach offers advantages in scalability, interpretability, and flexibility. We propose a general framework and a concrete instantiation: a fast, effective Greedy Join-Aware Retrieval algorithm that holistically balances relevance, coverage, and joinability. Experiments across 5 NL2SQL benchmarks demonstrate that our iterative method achieves competitive retrieval performance compared to the MIP-based approach while being 4-400x faster depending on the benchmark and search space settings. This work highlights the potential of iterative heuristics for practical, scalable, and composition-aware retrieval.

---

### 5. Dimension vs. Precision: A Comparative Analysis of Autoencoders and Quantization for Efficient Vector Retrieval on BEIR SciFact

- **LLM Score**: 6
- **Keyword Score**: 10
- **Authors**: Satyanarayan Pati
- **URL**: <http://arxiv.org/abs/2511.13057v2>
- **Submitted**: 2025-11-17 07:02:11
- **Comment**: 16 pages, 9 figures, 1 table
- **Topic Keywords**: information retrieval, dense retrieval, rag, retrieval
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the area of efficient vector retrieval and real-time relevance optimization. However, the focus on vector compression and quantization, while relevant to the field, does not directly align with your primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Dense retrieval models have become a standard for state-of-the-art information retrieval. However, their high-dimensional, high-precision (float32) vector embeddings create significant storage and memory challenges for real-world deployment. To address this, we conduct a rigorous empirical study on the BEIR SciFact benchmark, evaluating the trade-offs between two primary compression strategies: (1) Dimensionality Reduction via deep Autoencoders (AE), reducing original 384-dim vectors to latent spaces from 384 down to 12, and (2) Precision Reduction via Quantization (float16, int8, and binary). We systematically compare each method by measuring the "performance loss" (or gain) relative to a float32 baseline across a full suite of retrieval metrics (NDCG, MAP, MRR, Recall, Precision) at various k cutoffs. Our results show that int8 scalar quantization provides the most effective "sweet spot," achieving a 4x compression with a negligible [~1-2%] drop in nDCG@10. In contrast, Autoencoders show a graceful degradation but suffer a more significant performance loss at equivalent 4x compression ratios (AE-96). binary quantization was found to be unsuitable for this task due to catastrophic performance drops. This work provides a practical guide for deploying efficient, high-performance retrieval systems.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Tokenize Once, Recommend Anywhere: Unified Item Tokenization for Multi-domain LLM-based Recommendation

- **LLM Score**: 6
- **Keyword Score**: 1
- **Authors**: Yu Hou, Won-Yong Shin
- **URL**: <http://arxiv.org/abs/2511.12922v1>
- **Submitted**: 2025-11-17 03:18:04
- **Comment**: 20 pages, 8 figures, 9 tables; Annual AAAI Conference on Artificial Intelligence (AAAI-26) (to appear) (Please cite our conference version.)
- **Topic Keywords**: recommend
- **Reason**: This paper is somewhat related to the user's research interests in Information Retrieval and Natural Language Processing, as it involves item tokenization and large language model-based recommendation. However, the focus on recommender systems and multi-domain generalization is not the user's primary area of interest, which is more centered on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large language model (LLM)-based recommender systems have achieved high-quality performance by bridging the discrepancy between the item space and the language space through item tokenization. However, existing item tokenization methods typically require training separate models for each item domain, limiting generalization. Moreover, the diverse distributions and semantics across item domains make it difficult to construct a unified tokenization that preserves domain-specific information. To address these challenges, we propose UniTok, a Unified item Tokenization framework that integrates our own mixture-of-experts (MoE) architecture with a series of codebooks to convert items into discrete tokens, enabling scalable tokenization while preserving semantic information across multiple item domains. Specifically, items from different domains are first projected into a unified latent space through a shared encoder. They are then routed to domain-specific experts to capture the unique semantics, while a shared expert, which is always active, encodes common knowledge transferable across domains. Additionally, to mitigate semantic imbalance across domains, we present a mutual information calibration mechanism, which guides the model towards retaining similar levels of semantic information for each domain. Comprehensive experiments on wide-ranging real-world datasets demonstrate that the proposed UniTok framework is (a) highly effective: achieving up to 51.89% improvements over strong benchmarks, (b) theoretically sound: showing the analytical validity of our architectural design and optimization; and (c) highly generalizable: demonstrating robust performance across diverse domains without requiring per-domain retraining, a capability not supported by existing baselines.

### 7. AIF: Asynchronous Inference Framework for Cost-Effective Pre-Ranking

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Zhi Kou, Xiang-Rong Sheng, Shuguang Han, Zhishan Zhao, Yueyao Cheng, Han Zhu, Jian Xu, Bo Zheng
- **URL**: <http://arxiv.org/abs/2511.12934v1>
- **Submitted**: 2025-11-17 03:39:32
- **Topic Keywords**: ranking, retrieval, recommend, rank
- **Reason**: The paper focuses on improving the efficiency of pre-ranking models in recommendation systems, which is somewhat related to information retrieval. However, it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The use of deep neural networks and asynchronous inference framework is relevant to the broader field of NLP and data mining, but not directly applicable to the user's primary focus on information retrieval.

#### Abstract
> In industrial recommendation systems, pre-ranking models based on deep neural networks (DNNs) commonly adopt a sequential execution framework: feature fetching and model forward computation are triggered only after receiving candidates from the upstream retrieval stage. This design introduces inherent bottlenecks, including redundant computations of identical users/items and increased latency due to strictly sequential operations, which jointly constrain the model's capacity and system efficiency. To address these limitations, we propose the Asynchronous Inference Framework (AIF), a cost-effective computational architecture that decouples interaction-independent components, those operating within a single user or item, from real-time prediction. AIF reorganizes the model inference process by performing user-side computations in parallel with the retrieval stage and conducting item-side computations in a nearline manner. This means that interaction-independent components are calculated just once and completed before the real-time prediction phase of the pre-ranking stage. As a result, AIF enhances computational efficiency and reduces latency, freeing up resources to significantly improve the feature set and model architecture of interaction-independent components. Moreover, we delve into model design within the AIF framework, employing approximated methods for interaction-dependent components in online real-time predictions. By co-designing both the framework and the model, our solution achieves notable performance gains without significantly increasing computational and latency costs. This has enabled the successful deployment of AIF in the Taobao display advertising system.

### 8. Auditing Google's AI Overviews and Featured Snippets: A Case Study on Baby Care and Pregnancy

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Desheng Hu, Joachim Baumann, Aleksandra Urman, Elsa Lichtenegger, Robin Forsberg, Aniko Hannak, Christo Wilson
- **URL**: <http://arxiv.org/abs/2511.12920v1>
- **Submitted**: 2025-11-17 03:16:36
- **Comment**: 18 pages, 10 figures; to appear in AAAI ICWSM 2026
- **Topic Keywords**: queries, relevance, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of query understanding and relevance optimization. However, its focus on auditing AI Overviews and Featured Snippets in Google Search, and its emphasis on public health information access, is not directly aligned with your core themes of e-commerce, deep semantic understanding, and real-time relevance optimization.

#### Abstract
> Google Search increasingly surfaces AI-generated content through features like AI Overviews (AIO) and Featured Snippets (FS), which users frequently rely on despite having no control over their presentation. Through a systematic algorithm audit of 1,508 real baby care and pregnancy-related queries, we evaluate the quality and consistency of these information displays. Our robust evaluation framework assesses multiple quality dimensions, including answer consistency, relevance, presence of medical safeguards, source categories, and sentiment alignment. Our results reveal concerning gaps in information consistency, with information in AIO and FS displayed on the same search result page being inconsistent with each other in 33% of cases. Despite high relevance scores, both features critically lack medical safeguards (present in just 11% of AIO and 7% of FS responses). While health and wellness websites dominate source categories for both, AIO and FS, FS also often link to commercial sources. These findings have important implications for public health information access and demonstrate the need for stronger quality controls in AI-mediated health information. Our methodology provides a transferable framework for auditing AI systems across high-stakes domains where information quality directly impacts user well-being.

### 9. Can We Predict the Next Question? A Collaborative Filtering Approach to Modeling User Behavior

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Bokang Fu, Jiahao Wang, Xiaojing Liu, Yuli Liu
- **URL**: <http://arxiv.org/abs/2511.12949v1>
- **Submitted**: 2025-11-17 04:01:20
- **Topic Keywords**: rag, user behavior, recommend
- **Reason**: The paper explores user behavior modeling, specifically question prediction, which is somewhat related to your interests in query understanding and user behavior modeling. However, the focus is on collaborative filtering and dialogue systems, which is not a central match to your primary research themes in information retrieval and search technologies.

#### Abstract
> In recent years, large language models (LLMs) have excelled in language understanding and generation, powering advanced dialogue and recommendation systems. However, a significant limitation persists: these systems often model user preferences statically, failing to capture the dynamic and sequential nature of interactive behaviors. The sequence of a user's historical questions provides a rich, implicit signal of evolving interests and cognitive patterns, yet leveraging this temporal data for predictive tasks remains challenging due to the inherent disconnect between language modeling and behavioral sequence modeling.
  To bridge this gap, we propose a Collaborative Filtering-enhanced Question Prediction (CFQP) framework. CFQP dynamically models evolving user-question interactions by integrating personalized memory modules with graph-based preference propagation. This dual mechanism allows the system to adaptively learn from user-specific histories while refining predictions through collaborative signals from similar users. Experimental results demonstrate that our approach effectively generates agents that mimic real-user questioning patterns, highlighting its potential for building proactive and adaptive dialogue systems.

### 10. Quantifying consistency and accuracy of Latent Dirichlet Allocation

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Saranzaya Magsarjav, Melissa Humphries, Jonathan Tuke, Lewis Mitchell
- **URL**: <http://arxiv.org/abs/2511.12850v1>
- **Submitted**: 2025-11-17 00:44:27
- **Comment**: 8 pages, 3 figures, to be submitted
- **Topic Keywords**: information retrieval, retrieval
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and Information Retrieval (IR), but it focuses on topic modeling and stability measures, which is a specific aspect of NLP. While it touches on IR, it's not directly related to your core themes of query understanding, ranking models, or user behavior modeling.

#### Abstract
> Topic modelling in Natural Language Processing uncovers hidden topics in large, unlabelled text datasets. It is widely applied in fields such as information retrieval, content summarisation, and trend analysis across various disciplines. However, probabilistic topic models can produce different results when rerun due to their stochastic nature, leading to inconsistencies in latent topics. Factors like corpus shuffling, rare text removal, and document elimination contribute to these variations. This instability affects replicability, reliability, and interpretation, raising concerns about whether topic models capture meaningful topics or just noise. To address these problems, we defined a new stability measure that incorporates accuracy and consistency and uses the generative properties of LDA to generate a new corpus with ground truth. These generated corpora are run through LDA 50 times to determine the variability in the output. We show that LDA can correctly determine the underlying number of topics in the documents. We also find that LDA is more internally consistent, as the multiple reruns return similar topics; however, these topics are not the true topics.

### 11. Large Language Models Meet Extreme Multi-label Classification: Scaling and Multi-modal Framework

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Diego Ortego, Marlon Rodr√≠guez, Mario Almagro, Kunal Dahiya, David Jim√©nez, Juan C. SanMiguel
- **URL**: <http://arxiv.org/abs/2511.13189v1>
- **Submitted**: 2025-11-17 09:52:53
- **Comment**: To appear at AAAI 2026
- **Topic Keywords**: queries, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, as it involves Extreme Multi-label Classification and the use of large language models. However, the focus on visual information and multi-modal capabilities makes it less directly relevant to your primary areas of interest, such as query understanding and ranking models.

#### Abstract
> Foundation models have revolutionized artificial intelligence across numerous domains, yet their transformative potential remains largely untapped in Extreme Multi-label Classification (XMC). Queries in XMC are associated with relevant labels from extremely large label spaces, where it is critical to strike a balance between efficiency and performance. Therefore, many recent approaches efficiently pose XMC as a maximum inner product search between embeddings learned from small encoder-only transformer architectures. In this paper, we address two important aspects in XMC: how to effectively harness larger decoder-only models, and how to exploit visual information while maintaining computational efficiency. We demonstrate that both play a critical role in XMC separately and can be combined for improved performance. We show that a few billion-size decoder can deliver substantial improvements while keeping computational overhead manageable. Furthermore, our Vision-enhanced eXtreme Multi-label Learning framework (ViXML) efficiently integrates foundation vision models by pooling a single embedding per image. This limits computational growth while unlocking multi-modal capabilities. Remarkably, ViXML with small encoders outperforms text-only decoder in most cases, showing that an image is worth billions of parameters. Finally, we present an extension of existing text-only datasets to exploit visual metadata and make them available for future benchmarking. Comprehensive experiments across four public text-only datasets and their corresponding image enhanced versions validate our proposals' effectiveness, surpassing previous state-of-the-art by up to +8.21\% in P@1 on the largest dataset. ViXML's code is available at https://github.com/DiegoOrtego/vixml.

### 12. O-Mem: Omni Memory System for Personalized, Long Horizon, Self-Evolving Agents

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Piaohong Wang, Motong Tian, Jiaxian Li, Yuan Liang, Yuqing Wang, Qianben Chen, Tiannan Wang, Zhicong Lu, Jiawei Ma, Yuchen Eleanor Jiang, Wangchunshu Zhou
- **URL**: <http://arxiv.org/abs/2511.13593v2>
- **Submitted**: 2025-11-17 16:55:19
- **Topic Keywords**: retrieval, personalization
- **Reason**: The paper discusses a memory framework for personalized AI assistants, which is related to user behavior modeling and query understanding. However, it focuses more on memory systems and agent interactions rather than search technologies or ranking models, limiting its relevance to your core research interests.

#### Abstract
> Recent advancements in LLM-powered agents have demonstrated significant potential in generating human-like responses; however, they continue to face challenges in maintaining long-term interactions within complex environments, primarily due to limitations in contextual consistency and dynamic personalization. Existing memory systems often depend on semantic grouping prior to retrieval, which can overlook semantically irrelevant yet critical user information and introduce retrieval noise. In this report, we propose the initial design of O-Mem, a novel memory framework based on active user profiling that dynamically extracts and updates user characteristics and event records from their proactive interactions with agents. O-Mem supports hierarchical retrieval of persona attributes and topic-related context, enabling more adaptive and coherent personalized responses. O-Mem achieves 51.67% on the public LoCoMo benchmark, a nearly 3% improvement upon LangMem,the previous state-of-the-art, and it achieves 62.99% on PERSONAMEM, a 3.5% improvement upon A-Mem,the previous state-of-the-art. O-Mem also boosts token and interaction response time efficiency compared to previous memory frameworks. Our work opens up promising directions for developing efficient and human-like personalized AI assistants in the future.

### 13. Why is "Chicago" Predictive of Deceptive Reviews? Using LLMs to Discover Language Phenomena from Lexical Cues

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Jiaming Qu, Mengtian Guo, Yue Wang
- **URL**: <http://arxiv.org/abs/2511.13658v1>
- **Submitted**: 2025-11-17 18:15:13
- **Topic Keywords**: rag
- **Reason**: This paper explores the use of large language models to identify language phenomena indicative of deceptive reviews, which is somewhat related to our interests in Information Retrieval, particularly query understanding and user behavior modeling. However, the focus on deception detection and language models is not a central match to our core research themes. The paper's connection to NLP is relevant, but it's not directly related to our primary focus on information retrieval and search technologies.

#### Abstract
> Deceptive reviews mislead consumers, harm businesses, and undermine trust in online marketplaces. Machine learning classifiers can learn from large amounts of training examples to effectively distinguish deceptive reviews from genuine ones. However, the distinguishing features learned by these classifiers are often subtle, fragmented, and difficult for humans to interpret. In this work, we explore using large language models (LLMs) to translate machine-learned lexical cues into human-understandable language phenomena that can differentiate deceptive reviews from genuine ones. We show that language phenomena obtained in this manner are empirically grounded in data, generalizable across similar domains, and more predictive than phenomena either in LLMs' prior knowledge or obtained through in-context learning. These language phenomena have the potential to aid people in critically assessing the credibility of online reviews in environments where deception detection classifiers are unavailable.

### 14. Beyond SELECT: A Comprehensive Taxonomy-Guided Benchmark for Real-World Text-to-SQL Translation

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Hao Wang, Yuanfeng Song, Xiaoming Yin, Xing Chen
- **URL**: <http://arxiv.org/abs/2511.13590v1>
- **Submitted**: 2025-11-17 16:52:19
- **Topic Keywords**: rag
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and data mining, as it involves text-to-SQL translation and Large Language Models (LLMs). However, the focus on text-to-SQL translation and its applications in real-world scenarios is not directly aligned with your primary interest in Information Retrieval (IR) and search technologies. The paper's emphasis on dataset synthesis and evaluation is also somewhat tangential to your core research themes.

#### Abstract
> Text-to-SQL datasets are essential for training and evaluating text-to-SQL models, but existing datasets often suffer from limited coverage and fail to capture the diversity of real-world applications. To address this, we propose a novel taxonomy for text-to-SQL classification based on dimensions including core intents, statement types, syntax structures, and key actions. Using this taxonomy, we evaluate widely used public text-to-SQL datasets (e.g., Spider and Bird) and reveal limitations in their coverage and diversity. We then introduce a taxonomy-guided dataset synthesis pipeline, yielding a new dataset named SQL-Synth. This approach combines the taxonomy with Large Language Models (LLMs) to ensure the dataset reflects the breadth and complexity of real-world text-to-SQL applications. Extensive analysis and experimental results validate the effectiveness of our taxonomy, as SQL-Synth exhibits greater diversity and coverage compared to existing benchmarks. Moreover, we uncover that existing LLMs typically fall short in adequately capturing the full range of scenarios, resulting in limited performance on SQL-Synth. However, fine-tuning can substantially improve their performance in these scenarios. The proposed taxonomy has significant potential impact, as it not only enables comprehensive analysis of datasets and the performance of different LLMs, but also guides the construction of training data for LLMs.

### 15. Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Shalini Maiti, Amar Budhiraja, Bhavul Gauri, Gaurav Chaurasia, Anton Protopopov, Alexis Audran-Reiss, Michael Slater, Despoina Magka, Tatiana Shavrina, Roberta Raileanu, Yoram Bachrach
- **URL**: <http://arxiv.org/abs/2511.13254v1>
- **Submitted**: 2025-11-17 11:13:34
- **Topic Keywords**: rag
- **Reason**: The paper focuses on Large Language Models (LLMs) and proposes a method for model souping, which is a pre- and post-training technique to enhance performance. While it touches on the idea of leveraging multiple models, it doesn't directly relate to query understanding, ranking models, or user behavior modeling in Information Retrieval. However, it does involve deep semantic understanding and optimization, which are relevant to the user's broader interests in NLP and related topics.

#### Abstract
> Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies "expert" models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.

### 16. PragWorld: A Benchmark Evaluating LLMs' Local World Model under Minimal Linguistic Alterations and Conversational Dynamics

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Sachin Vashistha, Aryan Bibhuti, Atharva Naik, Martin Tutek, Somak Aditya
- **URL**: <http://arxiv.org/abs/2511.13021v1>
- **Submitted**: 2025-11-17 06:17:17
- **Comment**: 23 pages, 15 tables, 10 figures; AAAI 2026 Conference Main Track (oral)
- **Topic Keywords**: rag
- **Reason**: This paper evaluates the ability of Language Models (LMs) to encode and update their internal world model in conversations, which is somewhat related to query understanding and user behavior modeling in Information Retrieval. However, the focus on LMs and conversational dynamics is not directly aligned with the user's primary research interests in IR and Search technologies. The connection to NLP is relevant, but the paper's scope is more narrow than the user's broader interests.

#### Abstract
> Real-world conversations are rich with pragmatic elements, such as entity mentions, references, and implicatures. Understanding such nuances is a requirement for successful natural communication, and often requires building a local world model which encodes such elements and captures the dynamics of their evolving states. However, it is not well-understood whether language models (LMs) construct or maintain a robust implicit representation of conversations. In this work, we evaluate the ability of LMs to encode and update their internal world model in dyadic conversations and test their malleability under linguistic alterations. To facilitate this, we apply seven minimal linguistic alterations to conversations sourced from popular datasets and construct two benchmarks comprising yes-no questions. We evaluate a wide range of open and closed source LMs and observe that they struggle to maintain robust accuracy. Our analysis unveils that LMs struggle to memorize crucial details, such as tracking entities under linguistic alterations to conversations. We then propose a dual-perspective interpretability framework which identifies transformer layers that are useful or harmful and highlights linguistic alterations most influenced by harmful layers, typically due to encoding spurious signals or relying on shortcuts. Inspired by these insights, we propose two layer-regularization based fine-tuning strategies that suppress the effect of the harmful layers.

### 17. WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Genglin Liu, Shijie Geng, Sha Li, Hejie Cui, Sarah Zhang, Xin Liu, Tianyi Liu
- **URL**: <http://arxiv.org/abs/2511.12997v1>
- **Submitted**: 2025-11-17 05:38:50
- **Comment**: 18 pages; work in progress
- **Topic Keywords**: rag
- **Reason**: This paper introduces WebCoach, a self-evolving framework for web browsing agents with cross-session memory guidance. While it relates to search technologies and user behavior modeling, its focus on web navigation and episodic memory is somewhat tangential to your core research interests in information retrieval, query understanding, and ranking models.

#### Abstract
> Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.

### 18. AA-Omniscience: Evaluating Cross-Domain Knowledge Reliability in Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Declan Jackson, William Keating, George Cameron, Micah Hill-Smith
- **URL**: <http://arxiv.org/abs/2511.13029v1>
- **Submitted**: 2025-11-17 06:27:16
- **Topic Keywords**: search
- **Reason**: The paper explores the reliability of large language models in various domains, evaluating their factual accuracy and knowledge gaps. While it touches on aspects of information retrieval and knowledge understanding, it is primarily focused on language models and their evaluation, which is somewhat related to your interests in query understanding and ranking models, but not a central match.

#### Abstract
> Existing language model evaluations primarily measure general capabilities, yet reliable use of these models across a range of domains demands factual accuracy and recognition of knowledge gaps. We introduce AA-Omniscience, a benchmark designed to measure both factual recall and knowledge calibration across 6,000 questions. Questions are derived from authoritative academic and industry sources, and cover 42 economically relevant topics within six different domains. The evaluation measures a model's Omniscience Index, a bounded metric (-100 to 100) measuring factual recall that jointly penalizes hallucinations and rewards abstention when uncertain, with 0 equating to a model that answers questions correctly as much as it does incorrectly. Among evaluated models, Claude 4.1 Opus attains the highest score (4.8), making it one of only three models to score above zero. These results reveal persistent factuality and calibration weaknesses across frontier models. Performance also varies by domain, with the models from three different research labs leading across the six domains. This performance variability suggests models should be chosen according to the demands of the use case rather than general performance for tasks where knowledge is important.

### 19. From Perception to Reasoning: Deep Thinking Empowers Multimodal Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Wenxin Zhu, Andong Chen, Yuchen Song, Kehai Chen, Conghui Zhu, Ziyan Chen, Tiejun Zhao
- **URL**: <http://arxiv.org/abs/2511.12861v2>
- **Submitted**: 2025-11-17 01:22:37
- **Comment**: Survey; 7 figures, 3 tables, 44 pages
- **Topic Keywords**: search
- **Reason**: This paper explores the concept of Multimodal Chain-of-Thought (MCoT) reasoning in large language models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on multimodal large language models and reasoning capabilities is not directly aligned with the user's core research themes in IR and Search technologies.

#### Abstract
> With the remarkable success of Multimodal Large Language Models (MLLMs) in perception tasks, enhancing their complex reasoning capabilities has emerged as a critical research focus. Existing models still suffer from challenges such as opaque reasoning paths and insufficient generalization ability. Chain-of-Thought (CoT) reasoning, which has demonstrated significant efficacy in language models by enhancing reasoning transparency and output interpretability, holds promise for improving model reasoning capabilities when extended to the multimodal domain. This paper provides a systematic review centered on "Multimodal Chain-of-Thought" (MCoT). First, it analyzes the background and theoretical motivations for its inception from the perspectives of technical evolution and task demands. Then, it introduces mainstream MCoT methods from three aspects: CoT paradigms, the post-training stage, and the inference stage, while also analyzing their underlying mechanisms. Furthermore, the paper summarizes existing evaluation benchmarks and metrics, and discusses the application scenarios of MCoT. Finally, it analyzes the challenges currently facing MCoT and provides an outlook on its future research directions.

### 20. Mitigating Recommendation Biases via Group-Alignment and Global-Uniformity in Representation Learning

- **LLM Score**: 2
- **Keyword Score**: 10
- **Authors**: Miaomiao Cai, Min Hou, Lei Chen, Le Wu, Haoyue Bai, Yong Li, Meng Wang
- **URL**: <http://arxiv.org/abs/2511.13041v1>
- **Submitted**: 2025-11-17 06:42:29
- **Topic Keywords**: ranking, rerank, rag, recommend, rank
- **Reason**: This paper focuses on recommender systems and mitigating biases in collaborative filtering, which is somewhat related to information retrieval, but not directly aligned with the user's core research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Collaborative Filtering~(CF) plays a crucial role in modern recommender systems, leveraging historical user-item interactions to provide personalized suggestions. However, CF-based methods often encounter biases due to imbalances in training data. This phenomenon makes CF-based methods tend to prioritize recommending popular items and performing unsatisfactorily on inactive users. Existing works address this issue by rebalancing training samples, reranking recommendation results, or making the modeling process robust to the bias. Despite their effectiveness, these approaches can compromise accuracy or be sensitive to weighting strategies, making them challenging to train. In this paper, we deeply analyze the causes and effects of the biases and propose a framework to alleviate biases in recommendation from the perspective of representation distribution, namely Group-Alignment and Global-Uniformity Enhanced Representation Learning for Debiasing Recommendation (AURL). Specifically, we identify two significant problems in the representation distribution of users and items, namely group-discrepancy and global-collapse. These two problems directly lead to biases in the recommendation results. To this end, we propose two simple but effective regularizers in the representation space, respectively named group-alignment and global-uniformity. The goal of group-alignment is to bring the representation distribution of long-tail entities closer to that of popular entities, while global-uniformity aims to preserve the information of entities as much as possible by evenly distributing representations. Our method directly optimizes both the group-alignment and global-uniformity regularization terms to mitigate recommendation biases. Extensive experiments on three real datasets and various recommendation backbones verify the superiority of our proposed framework.

### 21. Region-Point Joint Representation for Effective Trajectory Similarity Learning

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Hao Long, Silin Zhou, Lisi Chen, Shuo Shang
- **URL**: <http://arxiv.org/abs/2511.13125v1>
- **Submitted**: 2025-11-17 08:28:18
- **Comment**: This paper is accepted by AAAI2026
- **Topic Keywords**: ranking, rag, ctr, rank
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The topic of trajectory similarity learning and region-point joint representation is more aligned with Geographic Information Systems (GIS) and spatial analysis, which are not mentioned in your research interests.

#### Abstract
> Recent learning-based methods have reduced the computational complexity of traditional trajectory similarity computation, but state-of-the-art (SOTA) methods still fail to leverage the comprehensive spectrum of trajectory information for similarity modeling. To tackle this problem, we propose \textbf{RePo}, a novel method that jointly encodes \textbf{Re}gion-wise and \textbf{Po}int-wise features to capture both spatial context and fine-grained moving patterns. For region-wise representation, the GPS trajectories are first mapped to grid sequences, and spatial context are captured by structural features and semantic context enriched by visual features. For point-wise representation, three lightweight expert networks extract local, correlation, and continuous movement patterns from dense GPS sequences. Then, a router network adaptively fuses the learned point-wise features, which are subsequently combined with region-wise features using cross-attention to produce the final trajectory embedding. To train RePo, we adopt a contrastive loss with hard negative samples to provide similarity ranking supervision. Experiment results show that RePo achieves an average accuracy improvement of 22.2\% over SOTA baselines across all evaluation metrics.

### 22. BioMedJImpact: A Comprehensive Dataset and LLM Pipeline for AI Engagement and Scientific Impact Analysis of Biomedical Journals

- **LLM Score**: 2
- **Keyword Score**: 8
- **Authors**: Ruiyu Wang, Yuzhang Xie, Xiao Hu, Carl Yang, Jiaying Lu
- **URL**: <http://arxiv.org/abs/2511.12821v1>
- **Submitted**: 2025-11-16 23:03:15
- **Topic Keywords**: ranking, relevance, rank, search
- **Reason**: This paper is not directly related to the user's core research themes in Information Retrieval, Search technologies, or Natural Language Processing. While it involves AI and LLMs, the focus is on scientometrics and biomedical journals, which is not a primary area of interest for the user.

#### Abstract
> Assessing journal impact is central to scholarly communication, yet existing open resources rarely capture how collaboration structures and artificial intelligence (AI) research jointly shape venue prestige in biomedicine. We present BioMedJImpact, a large-scale, biomedical-oriented dataset designed to advance journal-level analysis of scientific impact and AI engagement. Built from 1.74 million PubMed Central articles across 2,744 journals, BioMedJImpact integrates bibliometric indicators, collaboration features, and LLM-derived semantic indicators for AI engagement. Specifically, the AI engagement feature is extracted through a reproducible three-stage LLM pipeline that we propose. Using this dataset, we analyze how collaboration intensity and AI engagement jointly influence scientific impact across pre- and post-pandemic periods (2016-2019, 2020-2023). Two consistent trends emerge: journals with higher collaboration intensity, particularly those with larger and more diverse author teams, tend to achieve greater citation impact, and AI engagement has become an increasingly strong correlate of journal prestige, especially in quartile rankings. To further validate the three-stage LLM pipeline we proposed for deriving the AI engagement feature, we conduct human evaluation, confirming substantial agreement in AI relevance detection and consistent subfield classification. Together, these contributions demonstrate that BioMedJImpact serves as both a comprehensive dataset capturing the intersection of biomedicine and AI, and a validated methodological framework enabling scalable, content-aware scientometric analysis of scientific impact and innovation dynamics. Code is available at https://github.com/JonathanWry/BioMedJImpact.

### 23. Translation Entropy: A Statistical Framework for Evaluating Translation Systems

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Ronit D. Gross, Yanir Harel, Ido Kanter
- **URL**: <http://arxiv.org/abs/2511.13180v1>
- **Submitted**: 2025-11-17 09:42:15
- **Comment**: 23 pages, 6 figures and 8 tables
- **Topic Keywords**: ranking, rag, rank
- **Reason**: This paper focuses on evaluating translation systems using a statistical framework, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the specific topic of translation entropy and its application to translator evaluation does not align with your primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> The translation of written language has been known since the 3rd century BC; however, its necessity has become increasingly common in the information age. Today, many translators exist, based on encoder-decoder deep architectures, nevertheless, no quantitative objective methods are available to assess their performance, likely because the entropy of even a single language remains unknown. This study presents a quantitative method for estimating translation entropy, with the following key finding. Given a translator, several sentences that differ by only one selected token of a given pivot sentence yield identical translations. Analyzing the statistics of this phenomenon across an ensemble of such sentences, consisting each of a pivot selected token, yields the probabilities of replacing this specific token with others while preserving the translation. These probabilities constitute the entropy of the selected token, and the average across all selected pivot tokens provides an estimate of the translator's overall translation entropy, which is enhanced along the decoder blocks. This entropic measure allows for the quantitative ranking of several publicly available translators and reveals whether mutual translation entropy is symmetric. Extending the proposed method to include the replacement of two tokens in a given pivot sentence demonstrates a multiplicative effect, where translation degeneracy is proportional to the product of the degeneracies of the two tokens. These findings establish translation entropy as a measurable property and objective benchmarking of artificial translators. Results are based on MarianMT, T5-Base and NLLB-200 translators.

### 24. Local Collaborative Filtering: A Collaborative Filtering Method that Utilizes Local Similarities among Users

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Zhaoxin Shen, Dan Wu
- **URL**: <http://arxiv.org/abs/2511.13166v1>
- **Submitted**: 2025-11-17 09:10:37
- **Comment**: 4 pages, 2 figures
- **Topic Keywords**: rag, user behavior, recommend
- **Reason**: This paper focuses on recommender systems and collaborative filtering, which is somewhat related to your interests in Information Retrieval and Search technologies. However, it lacks direct connections to query understanding, ranking models, or user behavior modeling, making it less relevant to your core research themes.

#### Abstract
> To leverage user behavior data from the Internet more effectively in recommender systems, this paper proposes a novel collaborative filtering (CF) method called Local Collaborative Filtering (LCF). LCF utilizes local similarities among users and integrates their data using the law of large numbers (LLN), thereby improving the utilization of user behavior data. Experiments are conducted on the Steam game dataset, and the results of LCF align with real-world needs.

### 25. Extracting Events Like Code: A Multi-Agent Programming Framework for Zero-Shot Event Extraction

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Quanjiang Guo, Sijie Wang, Jinchuan Zhang, Ben Zhang, Zhao Kang, Ling Tian, Ke Yan
- **URL**: <http://arxiv.org/abs/2511.13118v1>
- **Submitted**: 2025-11-17 08:17:15
- **Comment**: 11 pages, 5 figures, accepted by AAAI 2026 (Oral)
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper focuses on event extraction using a multi-agent framework, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves large language models, the primary goal is event extraction, not query understanding, ranking models, or user behavior modeling.

#### Abstract
> Zero-shot event extraction (ZSEE) remains a significant challenge for large language models (LLMs) due to the need for complex reasoning and domain-specific understanding. Direct prompting often yields incomplete or structurally invalid outputs--such as misclassified triggers, missing arguments, and schema violations. To address these limitations, we present Agent-Event-Coder (AEC), a novel multi-agent framework that treats event extraction like software engineering: as a structured, iterative code-generation process. AEC decomposes ZSEE into specialized subtasks--retrieval, planning, coding, and verification--each handled by a dedicated LLM agent. Event schemas are represented as executable class definitions, enabling deterministic validation and precise feedback via a verification agent. This programming-inspired approach allows for systematic disambiguation and schema enforcement through iterative refinement. By leveraging collaborative agent workflows, AEC enables LLMs to produce precise, complete, and schema-consistent extractions in zero-shot settings. Experiments across five diverse domains and six LLMs demonstrate that AEC consistently outperforms prior zero-shot baselines, showcasing the power of treating event extraction like code generation. The code and data are released on https://github.com/UESTC-GQJ/Agent-Event-Coder.

### 26. NeuroLex: A Lightweight Domain Language Model for EEG Report Understanding and Generation

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Kang Yin, Hye-Bin Shin
- **URL**: <http://arxiv.org/abs/2511.12851v1>
- **Submitted**: 2025-11-17 00:44:35
- **Topic Keywords**: rag, ctr
- **Reason**: This paper focuses on developing a domain-specific language model for EEG report understanding and generation, which is outside the scope of information retrieval and search technologies. Although it involves natural language processing, the application domain is not relevant to the user's research interests in e-commerce, query understanding, ranking models, or user behavior modeling.

#### Abstract
> Clinical electroencephalogram (EEG) reports encode domain-specific linguistic conventions that general-purpose language models (LMs) fail to capture. We introduce NeuroLex, a lightweight domain-adaptive language model trained purely on EEG report text from the Harvard Electroencephalography Database. Unlike existing biomedical LMs, NeuroLex is tailored to the linguistic and diagnostic characteristics of EEG reporting, enabling it to serve as both an independent textual model and a decoder backbone for multimodal EEG-language systems. Using span-corruption pretraining and instruction-style fine-tuning on report polishing, paragraph summarization, and terminology question answering, NeuroLex learns the syntax and reasoning patterns characteristic of EEG interpretation. Comprehensive evaluations show that it achieves lower perplexity, higher extraction and summarization accuracy, better label efficiency, and improved robustness to negation and factual hallucination compared with general models of the same scale. With an EEG-aware linguistic backbone, NeuroLex bridges biomedical text modeling and brain-computer interface applications, offering a foundation for interpretable and language-driven neural decoding.

### 27. P1: Mastering Physics Olympiads with Reinforcement Learning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Jiacheng Chen, Qianjia Cheng, Fangchen Yu, Haiyuan Wan, Yuchen Zhang, Shenghe Zheng, Junchi Yao, Qingyang Zhang, Haonan He, Yun Luo, Yufeng Zhao, Futing Wang, Li Sheng, Chengxing Xie, Yuxin Zuo, Yizhuo Li, Wenxauan Zeng, Yulun Wu, Rui Huang, Dongzhan Zhou, Kai Chen, Yu Qiao, Lei Bai, Yu Cheng, Ning Ding, Bowen Zhou, Peng Ye, Ganqu Cui
- **URL**: <http://arxiv.org/abs/2511.13612v1>
- **Submitted**: 2025-11-17 17:18:13
- **Topic Keywords**: rag, search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The focus is on physics reasoning and Olympiad-level problem-solving, which is outside your primary areas of interest.

#### Abstract
> Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.

### 28. Compact Multimodal Language Models as Robust OCR Alternatives for Noisy Textual Clinical Reports

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Nikita Neveditsin, Pawan Lingras, Salil Patil, Swarup Patil, Vijay Mago
- **URL**: <http://arxiv.org/abs/2511.13523v1>
- **Submitted**: 2025-11-17 15:58:03
- **Topic Keywords**: ltr
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves text processing and OCR, its focus on medical records and robustness in noisy conditions does not align with your primary areas of interest.

#### Abstract
> Digitization of medical records often relies on smartphone photographs of printed reports, producing images degraded by blur, shadows, and other noise. Conventional OCR systems, optimized for clean scans, perform poorly under such real-world conditions. This study evaluates compact multimodal language models as privacy-preserving alternatives for transcribing noisy clinical documents. Using obstetric ultrasound reports written in regionally inflected medical English common to Indian healthcare settings, we compare eight systems in terms of transcription accuracy, noise sensitivity, numeric accuracy, and computational efficiency. Compact multimodal models consistently outperform both classical and neural OCR pipelines. Despite higher computational costs, their robustness and linguistic adaptability position them as viable candidates for on-premises healthcare digitization.

### 29. Applying Large Language Models to Characterize Public Narratives

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Elinor Poole-Dayan, Daniel T Kessler, Hannah Chiou, Margaret Hughes, Emily S Lin, Marshall Ganz, Deb Roy
- **URL**: <http://arxiv.org/abs/2511.13505v1>
- **Submitted**: 2025-11-17 15:41:55
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on applying large language models to analyze public narratives, which is a topic in Natural Language Processing (NLP). Although it involves some aspects of information retrieval, such as scalable narrative analysis, it does not directly relate to the user's core research themes in Information Retrieval, query understanding, ranking models, or user behavior modeling.

#### Abstract
> Public Narratives (PNs) are key tools for leadership development and civic mobilization, yet their systematic analysis remains challenging due to their subjective interpretation and the high cost of expert annotation. In this work, we propose a novel computational framework that leverages large language models (LLMs) to automate the qualitative annotation of public narratives. Using a codebook we co-developed with subject-matter experts, we evaluate LLM performance against that of expert annotators. Our work reveals that LLMs can achieve near-human-expert performance, achieving an average F1 score of 0.80 across 8 narratives and 14 codes. We then extend our analysis to empirically explore how PN framework elements manifest across a larger dataset of 22 stories. Lastly, we extrapolate our analysis to a set of political speeches, establishing a novel lens in which to analyze political rhetoric in civic spaces. This study demonstrates the potential of LLM-assisted annotation for scalable narrative analysis and highlights key limitations and directions for future research in computational civic storytelling.

### 30. Mem-PAL: Towards Memory-based Personalized Dialogue Assistants for Long-term User-Agent Interaction

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Zhaopei Huang, Qifeng Dai, Guozheng Wu, Xiaopeng Wu, Kehan Chen, Chuan Yu, Xubin Li, Tiezheng Ge, Wenxuan Wang, Qin Jin
- **URL**: <http://arxiv.org/abs/2511.13410v1>
- **Submitted**: 2025-11-17 14:22:32
- **Comment**: Accepted by AAAI 2026 (Oral)
- **Topic Keywords**: retrieval, personalization
- **Reason**: This paper focuses on dialogue assistants and personalized service-oriented interactions, which is somewhat related to information retrieval and search technologies. However, the primary focus on dialogue systems and memory frameworks for personalized response generation does not strongly align with the user's core research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> With the rise of smart personal devices, service-oriented human-agent interactions have become increasingly prevalent. This trend highlights the need for personalized dialogue assistants that can understand user-specific traits to accurately interpret requirements and tailor responses to individual preferences. However, existing approaches often overlook the complexities of long-term interactions and fail to capture users' subjective characteristics. To address these gaps, we present PAL-Bench, a new benchmark designed to evaluate the personalization capabilities of service-oriented assistants in long-term user-agent interactions. In the absence of available real-world data, we develop a multi-step LLM-based synthesis pipeline, which is further verified and refined by human annotators. This process yields PAL-Set, the first Chinese dataset comprising multi-session user logs and dialogue histories, which serves as the foundation for PAL-Bench. Furthermore, to improve personalized service-oriented interactions, we propose H$^2$Memory, a hierarchical and heterogeneous memory framework that incorporates retrieval-augmented generation to improve personalized response generation. Comprehensive experiments on both our PAL-Bench and an external dataset demonstrate the effectiveness of the proposed memory framework.

### 31. AutoMalDesc: Large-Scale Script Analysis for Cyber Threat Research

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Alexandru-Mihai Apostu, Andrei Preda, Alexandra Daniela Damir, Diana Bolocan, Radu Tudor Ionescu, Ioana Croitoru, Mihaela Gaman
- **URL**: <http://arxiv.org/abs/2511.13333v1>
- **Submitted**: 2025-11-17 13:05:25
- **Comment**: Accepted at AAAI 2026 (oral)
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on automated malware detection and script analysis, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves some NLP aspects, such as generating natural language explanations, the context is specific to cybersecurity and not aligned with the user's broader interests.

#### Abstract
> Generating thorough natural language explanations for threat detections remains an open problem in cybersecurity research, despite significant advances in automated malware detection systems. In this work, we present AutoMalDesc, an automated static analysis summarization framework that, following initial training on a small set of expert-curated examples, operates independently at scale. This approach leverages an iterative self-paced learning pipeline to progressively enhance output quality through synthetic data generation and validation cycles, eliminating the need for extensive manual data annotation. Evaluation across 3,600 diverse samples in five scripting languages demonstrates statistically significant improvements between iterations, showing consistent gains in both summary quality and classification accuracy. Our comprehensive validation approach combines quantitative metrics based on established malware labels with qualitative assessment from both human experts and LLM-based judges, confirming both technical precision and linguistic coherence of generated summaries. To facilitate reproducibility and advance research in this domain, we publish our complete dataset of more than 100K script samples, including annotated seed (0.9K) and test (3.6K) datasets, along with our methodology and evaluation framework.

### 32. Computational Measurement of Political Positions: A Review of Text-Based Ideal Point Estimation Algorithms

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Patrick Parschan, Charlott Jakob
- **URL**: <http://arxiv.org/abs/2511.13238v1>
- **Submitted**: 2025-11-17 11:01:09
- **Comment**: 46 pages, 8 figures, 2 tables, accepted for publication in Quality & Quantity
- **Topic Keywords**: rag, search
- **Reason**: This paper is primarily focused on text-based ideal point estimation algorithms in the context of political science, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it does touch on NLP trends, it is more focused on a specific application area and does not address the user's areas of interest.

#### Abstract
> This article presents the first systematic review of unsupervised and semi-supervised computational text-based ideal point estimation (CT-IPE) algorithms, methods designed to infer latent political positions from textual data. These algorithms are widely used in political science, communication, computational social science, and computer science to estimate ideological preferences from parliamentary speeches, party manifestos, and social media. Over the past two decades, their development has closely followed broader NLP trends -- beginning with word-frequency models and most recently turning to large language models (LLMs). While this trajectory has greatly expanded the methodological toolkit, it has also produced a fragmented field that lacks systematic comparison and clear guidance for applied use. To address this gap, we identified 25 CT-IPE algorithms through a systematic literature review and conducted a manual content analysis of their modeling assumptions and development contexts. To compare them meaningfully, we introduce a conceptual framework that distinguishes how algorithms generate, capture, and aggregate textual variance. On this basis, we identify four methodological families -- word-frequency, topic modeling, word embedding, and LLM-based approaches -- and critically assess their assumptions, interpretability, scalability, and limitations. Our review offers three contributions. First, it provides a structured synthesis of two decades of algorithm development, clarifying how diverse methods relate to one another. Second, it translates these insights into practical guidance for applied researchers, highlighting trade-offs in transparency, technical requirements, and validation strategies that shape algorithm choice. Third, it emphasizes that differences in estimation outcomes across algorithms are themselves informative, underscoring the need for systematic benchmarking.

### 33. TCM-5CEval: Extended Deep Evaluation Benchmark for LLM's Comprehensive Clinical Research Competence in Traditional Chinese Medicine

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Tianai Huang, Jiayuan Chen, Lu Lu, Pengcheng Chen, Tianbin Li, Bing Han, Wenchao Tang, Jie Xu, Ming Li
- **URL**: <http://arxiv.org/abs/2511.13169v1>
- **Submitted**: 2025-11-17 09:15:41
- **Comment**: 17 pages, 8 figures
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on evaluating large language models in Traditional Chinese Medicine, which is unrelated to information retrieval, ranking, or click modeling. It does not address query understanding, search technologies, or user behavior modeling, making it only marginally relevant to the user‚Äôs core interests.

#### Abstract
> Large language models (LLMs) have demonstrated exceptional capabilities in general domains, yet their application in highly specialized and culturally-rich fields like Traditional Chinese Medicine (TCM) requires rigorous and nuanced evaluation. Building upon prior foundational work such as TCM-3CEval, which highlighted systemic knowledge gaps and the importance of cultural-contextual alignment, we introduce TCM-5CEval, a more granular and comprehensive benchmark. TCM-5CEval is designed to assess LLMs across five critical dimensions: (1) Core Knowledge (TCM-Exam), (2) Classical Literacy (TCM-LitQA), (3) Clinical Decision-making (TCM-MRCD), (4) Chinese Materia Medica (TCM-CMM), and (5) Clinical Non-pharmacological Therapy (TCM-ClinNPT). We conducted a thorough evaluation of fifteen prominent LLMs, revealing significant performance disparities and identifying top-performing models like deepseek\_r1 and gemini\_2\_5\_pro. Our findings show that while models exhibit proficiency in recalling foundational knowledge, they struggle with the interpretative complexities of classical texts. Critically, permutation-based consistency testing reveals widespread fragilities in model inference. All evaluated models, including the highest-scoring ones, displayed a substantial performance degradation when faced with varied question option ordering, indicating a pervasive sensitivity to positional bias and a lack of robust understanding. TCM-5CEval not only provides a more detailed diagnostic tool for LLM capabilities in TCM but aldso exposes fundamental weaknesses in their reasoning stability. To promote further research and standardized comparison, TCM-5CEval has been uploaded to the Medbench platform, joining its predecessor in the "In-depth Challenge for Comprehensive TCM Abilities" special track.

### 34. Crossing Borders: A Multimodal Challenge for Indian Poetry Translation and Image Generation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Sofia Jamil, Kotla Sai Charan, Sriparna Saha, Koustava Goswami, Joseph K J
- **URL**: <http://arxiv.org/abs/2511.13689v2>
- **Submitted**: 2025-11-17 18:41:16
- **Topic Keywords**: rag
- **Reason**: This paper focuses on poetry translation and image generation using multimodal models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves NLP, the specific application and goals of the paper do not align with your areas of focus.

#### Abstract
> Indian poetry, known for its linguistic complexity and deep cultural resonance, has a rich and varied heritage spanning thousands of years. However, its layered meanings, cultural allusions, and sophisticated grammatical constructions often pose challenges for comprehension, especially for non-native speakers or readers unfamiliar with its context and language. Despite its cultural significance, existing works on poetry have largely overlooked Indian language poems. In this paper, we propose the Translation and Image Generation (TAI) framework, leveraging Large Language Models (LLMs) and Latent Diffusion Models through appropriate prompt tuning. Our framework supports the United Nations Sustainable Development Goals of Quality Education (SDG 4) and Reduced Inequalities (SDG 10) by enhancing the accessibility of culturally rich Indian-language poetry to a global audience. It includes (1) a translation module that uses an Odds Ratio Preference Alignment Algorithm to accurately translate morphologically rich poetry into English, and (2) an image generation module that employs a semantic graph to capture tokens, dependencies, and semantic relationships between metaphors and their meanings, to create visually meaningful representations of Indian poems. Our comprehensive experimental evaluation, including both human and quantitative assessments, demonstrates the superiority of TAI Diffusion in poem image generation tasks, outperforming strong baselines. To further address the scarcity of resources for Indian-language poetry, we introduce the Morphologically Rich Indian Language Poems MorphoVerse Dataset, comprising 1,570 poems across 21 low-resource Indian languages. By addressing the gap in poetry translation and visual comprehension, this work aims to broaden accessibility and enrich the reader's experience.

### 35. ForgeDAN: An Evolutionary Framework for Jailbreaking Aligned Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Siyang Cheng, Gaotian Liu, Rui Mei, Yilin Wang, Kejia Zhang, Kaishuo Wei, Yuqi Yu, Weiping Wen, Xiaojie Wu, Junhua Liu
- **URL**: <http://arxiv.org/abs/2511.13548v1>
- **Submitted**: 2025-11-17 16:19:21
- **Topic Keywords**: rag
- **Reason**: This paper focuses on large language models and adversarial attacks, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on semantic understanding, it is more focused on security risks and attack generation rather than real-time relevance optimization or user behavior modeling.

#### Abstract
> The rapid adoption of large language models (LLMs) has brought both transformative applications and new security risks, including jailbreak attacks that bypass alignment safeguards to elicit harmful outputs. Existing automated jailbreak generation approaches e.g. AutoDAN, suffer from limited mutation diversity, shallow fitness evaluation, and fragile keyword-based detection. To address these limitations, we propose ForgeDAN, a novel evolutionary framework for generating semantically coherent and highly effective adversarial prompts against aligned LLMs. First, ForgeDAN introduces multi-strategy textual perturbations across \textit{character, word, and sentence-level} operations to enhance attack diversity; then we employ interpretable semantic fitness evaluation based on a text similarity model to guide the evolutionary process toward semantically relevant and harmful outputs; finally, ForgeDAN integrates dual-dimensional jailbreak judgment, leveraging an LLM-based classifier to jointly assess model compliance and output harmfulness, thereby reducing false positives and improving detection effectiveness. Our evaluation demonstrates ForgeDAN achieves high jailbreaking success rates while maintaining naturalness and stealth, outperforming existing SOTA solutions.

### 36. AHaSIS: Shared Task on Sentiment Analysis for Arabic Dialects

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Maram Alharbi, Salmane Chafik, Saad Ezzini, Ruslan Mitkov, Tharindu Ranasinghe, Hansi Hettiarachchi
- **URL**: <http://arxiv.org/abs/2511.13335v1>
- **Submitted**: 2025-11-17 13:06:55
- **Topic Keywords**: rag
- **Reason**: This paper focuses on sentiment analysis for Arabic dialects, which is a specific task in NLP. While it touches on customer feedback and real-world applications, it doesn't directly relate to information retrieval, query understanding, or ranking models, which are core areas of your research interests.

#### Abstract
> The hospitality industry in the Arab world increasingly relies on customer feedback to shape services, driving the need for advanced Arabic sentiment analysis tools. To address this challenge, the Sentiment Analysis on Arabic Dialects in the Hospitality Domain shared task focuses on Sentiment Detection in Arabic Dialects. This task leverages a multi-dialect, manually curated dataset derived from hotel reviews originally written in Modern Standard Arabic (MSA) and translated into Saudi and Moroccan (Darija) dialects. The dataset consists of 538 sentiment-balanced reviews spanning positive, neutral, and negative categories. Translations were validated by native speakers to ensure dialectal accuracy and sentiment preservation. This resource supports the development of dialect-aware NLP systems for real-world applications in customer experience analysis. More than 40 teams have registered for the shared task, with 12 submitting systems during the evaluation phase. The top-performing system achieved an F1 score of 0.81, demonstrating the feasibility and ongoing challenges of sentiment analysis across Arabic dialects.

### 37. Seeing isn't Hearing: Benchmarking Vision Language Models at Interpreting Spectrograms

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Tyler Loakman, Joseph James, Chenghua Lin
- **URL**: <http://arxiv.org/abs/2511.13225v1>
- **Submitted**: 2025-11-17 10:41:07
- **Comment**: Accepted to IJCNLP-AACL 2025
- **Topic Keywords**: ctr
- **Reason**: This paper focuses on vision language models and their ability to interpret spectrograms, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the context is specific to speech and phonetics, making it less relevant to the user's core research interests.

#### Abstract
> With the rise of Large Language Models (LLMs) and their vision-enabled counterparts (VLMs), numerous works have investigated their capabilities in tasks that fuse the modalities of vision and language. In this work, we benchmark the extent to which VLMs are able to act as highly-trained phoneticians, interpreting spectrograms and waveforms of speech. To do this, we synthesise a novel dataset containing 4k+ English words spoken in isolation alongside stylistically consistent spectrogram and waveform figures. We test the ability of VLMs to understand these representations of speech through a multiple-choice task whereby models must predict the correct phonemic or graphemic transcription of a spoken word when presented amongst 3 distractor transcriptions that have been selected based on their phonemic edit distance to the ground truth. We observe that both zero-shot and finetuned models rarely perform above chance, demonstrating the requirement for specific parametric knowledge of how to interpret such figures, rather than paired samples alone.

### 38. Zero-Shot Grammar Competency Estimation Using Large Language Model Generated Pseudo Labels

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Sourya Dipta Das, Shubham Kumar, Kuldeep Yadav
- **URL**: <http://arxiv.org/abs/2511.13152v1>
- **Submitted**: 2025-11-17 09:00:26
- **Comment**: Accepted in AACL-IJCNLP 2025
- **Topic Keywords**: rag
- **Reason**: This paper focuses on grammar competency estimation using large language models, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves natural language processing, the application domain is not aligned with the user's primary research interests.

#### Abstract
> Grammar competency estimation is essential for assessing linguistic proficiency in both written and spoken language; however, the spoken modality presents additional challenges due to its spontaneous, unstructured, and disfluent nature. Developing accurate grammar scoring models further requires extensive expert annotation, making large-scale data creation impractical. To address these limitations, we propose a zero-shot grammar competency estimation framework that leverages unlabeled data and Large Language Models (LLMs) without relying on manual labels. During training, we employ LLM-generated predictions on unlabeled data by using grammar competency rubric-based prompts. These predictions, treated as pseudo labels, are utilized to train a transformer-based model through a novel training framework designed to handle label noise effectively. We show that the choice of LLM for pseudo-label generation critically affects model performance and that the ratio of clean-to-noisy samples during training strongly influences stability and accuracy. Finally, a qualitative analysis of error intensity and score prediction confirms the robustness and interpretability of our approach. Experimental results demonstrate the efficacy of our approach in estimating grammar competency scores with high accuracy, paving the way for scalable, low-resource grammar assessment systems.

### 39. Evaluating the Ability of Large Language Models to Identify Adherence to CONSORT Reporting Guidelines in Randomized Controlled Trials: A Methodological Evaluation Study

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zhichao He, Mouxiao Bian, Jianhong Zhu, Jiayuan Chen, Yunqiu Wang, Wenxia Zhao, Tianbin Li, Bing Han, Jie Xu, Junyan Wu
- **URL**: <http://arxiv.org/abs/2511.13107v1>
- **Submitted**: 2025-11-17 08:05:15
- **Topic Keywords**: rag
- **Reason**: This paper focuses on evaluating the performance of large language models in identifying adherence to CONSORT reporting guidelines in randomized controlled trials. While it involves natural language processing and model evaluation, it is not directly related to information retrieval, search technologies, or query understanding, which are the core areas of your research interests.

#### Abstract
> The Consolidated Standards of Reporting Trials statement is the global benchmark for transparent and high-quality reporting of randomized controlled trials. Manual verification of CONSORT adherence is a laborious, time-intensive process that constitutes a significant bottleneck in peer review and evidence synthesis. This study aimed to systematically evaluate the accuracy and reliability of contemporary LLMs in identifying the adherence of published RCTs to the CONSORT 2010 statement under a zero-shot setting. We constructed a golden standard dataset of 150 published RCTs spanning diverse medical specialties. The primary outcome was the macro-averaged F1-score for the three-class classification task, supplemented by item-wise performance metrics and qualitative error analysis. Overall model performance was modest. The top-performing models, Gemini-2.5-Flash and DeepSeek-R1, achieved nearly identical macro F1 scores of 0.634 and Cohen's Kappa coefficients of 0.280 and 0.282, respectively, indicating only fair agreement with expert consensus. A striking performance disparity was observed across classes: while most models could identify compliant items with high accuracy (F1 score > 0.850), they struggled profoundly with identifying non-compliant and not applicable items, where F1 scores rarely exceeded 0.400. Notably, some high-profile models like GPT-4o underperformed, achieving a macro F1-score of only 0.521. LLMs show potential as preliminary screening assistants for CONSORT checks, capably identifying well-reported items. However, their current inability to reliably detect reporting omissions or methodological flaws makes them unsuitable for replacing human expertise in the critical appraisal of trial quality.

### 40. Personalized Federated Recommendation With Knowledge Guidance

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jaehyung Lim, Wonbin Kweon, Woojoo Kim, Junyoung Kim, Dongha Kim, Hwanjo Yu
- **URL**: <http://arxiv.org/abs/2511.12959v2>
- **Submitted**: 2025-11-17 04:35:53
- **Topic Keywords**: recommend, personalization
- **Reason**: This paper focuses on Federated Recommendation and Knowledge Guidance, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on personalization and user behavior, the context is recommender systems rather than search or query understanding.

#### Abstract
> Federated Recommendation (FedRec) has emerged as a key paradigm for building privacy-preserving recommender systems. However, existing FedRec models face a critical dilemma: memory-efficient single-knowledge models suffer from a suboptimal knowledge replacement practice that discards valuable personalization, while high-performance dual-knowledge models are often too memory-intensive for practical on-device deployment. We propose Federated Recommendation with Knowledge Guidance (FedRKG), a model-agnostic framework that resolves this dilemma. The core principle, Knowledge Guidance, avoids full replacement and instead fuses global knowledge into preserved local embeddings, attaining the personalization benefits of dual-knowledge within a single-knowledge memory footprint. Furthermore, we introduce Adaptive Guidance, a fine-grained mechanism that dynamically modulates the intensity of this guidance for each user-item interaction, overcoming the limitations of static fusion methods. Extensive experiments on benchmark datasets demonstrate that FedRKG significantly outperforms state-of-the-art methods, validating the effectiveness of our approach. The code is available at https://github.com/Jaehyung-Lim/fedrkg.

### 41. Toward Conversational Hungarian Speech Recognition: Introducing the BEA-Large and BEA-Dialogue Datasets

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: M√°t√© Gedeon, Piroska Zs√≥fia Barta, P√©ter Mihajlik, Tekla Etelka Gr√°czi, Anna Koh√°ri, Katalin M√°dy
- **URL**: <http://arxiv.org/abs/2511.13529v1>
- **Submitted**: 2025-11-17 16:02:08
- **Comment**: Submitted to LREC 2026
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The focus is on speech recognition and conversational ASR in the Hungarian language, which is outside your primary areas of interest.

#### Abstract
> The advancement of automatic speech recognition (ASR) has been largely enhanced by extensive datasets in high-resource languages, while languages such as Hungarian remain underrepresented due to limited spontaneous and conversational corpora. To address this gap, we introduce two new datasets -- BEA-Large and BEA-Dialogue -- constructed from the previously unprocessed portions of the Hungarian speech corpus named BEA. BEA-Large extends BEA-Base with 255 hours of spontaneous speech from 433 speakers, enriched with detailed segment-level metadata. BEA-Dialogue, comprising 85 hours of spontaneous conversations, is a Hungarian speech corpus featuring natural dialogues partitioned into speaker-independent subsets, supporting research in conversational ASR and speaker diarization. We establish reproducible baselines on these datasets using publicly available ASR models, with the fine-tuned Fast Conformer model achieving word error rates as low as 14.18\% on spontaneous and 4.8\% on repeated speech. Diarization experiments yield diarization error rates between 13.05\% and 18.26\%, providing reference points for future improvements. The results highlight the persistent difficulty of conversational ASR, particularly due to disfluencies, overlaps, and informal speech patterns. By releasing these datasets and baselines, we aim to advance Hungarian speech technology and offer a methodological framework for developing spontaneous and conversational benchmarks in other languages.

### 42. RegionMarker: A Region-Triggered Semantic Watermarking Framework for Embedding-as-a-Service Copyright Protection

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Shufan Yang, Zifeng Cheng, Zhiwei Jiang, Yafeng Yin, Cong Wang, Shiping Ge, Yuchen Fu, Qing Gu
- **URL**: <http://arxiv.org/abs/2511.13329v1>
- **Submitted**: 2025-11-17 13:04:36
- **Comment**: AAAI 2026
- **Topic Keywords**: search
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or user behavior modeling, which are your primary research interests. While it touches on Natural Language Processing, it focuses on a specific application of watermarking for copyright protection, which is not a central match to your research themes.

#### Abstract
> Embedding-as-a-Service (EaaS) is an effective and convenient deployment solution for addressing various NLP tasks. Nevertheless, recent research has shown that EaaS is vulnerable to model extraction attacks, which could lead to significant economic losses for model providers. For copyright protection, existing methods inject watermark embeddings into text embeddings and use them to detect copyright infringement. However, current watermarking methods often resist only a subset of attacks and fail to provide \textit{comprehensive} protection. To this end, we present the region-triggered semantic watermarking framework called RegionMarker, which defines trigger regions within a low-dimensional space and injects watermarks into text embeddings associated with these regions. By utilizing a secret dimensionality reduction matrix to project onto this subspace and randomly selecting trigger regions, RegionMarker makes it difficult for watermark removal attacks to evade detection. Furthermore, by embedding watermarks across the entire trigger region and using the text embedding as the watermark, RegionMarker is resilient to both paraphrasing and dimension-perturbation attacks. Extensive experiments on various datasets show that RegionMarker is effective in resisting different attack methods, thereby protecting the copyright of EaaS.

### 43. Examining the Usage of Generative AI Models in Student Learning Activities for Software Programming

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Rufeng Chen, Shuaishuai Jiang, Jiyun Shen, AJung Moon, Lili Wei
- **URL**: <http://arxiv.org/abs/2511.13271v1>
- **Submitted**: 2025-11-17 11:42:24
- **Comment**: 9 pages, 4 figures, accepted at AIWARE 2025
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. It focuses on Generative AI in education, specifically in software programming, which is outside your primary areas of interest.

#### Abstract
> The rise of Generative AI (GenAI) tools like ChatGPT has created new opportunities and challenges for computing education. Existing research has primarily focused on GenAI's ability to complete educational tasks and its impact on student performance, often overlooking its effects on knowledge gains. In this study, we investigate how GenAI assistance compares to conventional online resources in supporting knowledge gains across different proficiency levels. We conducted a controlled user experiment with 24 undergraduate students of two different levels of programming experience (beginner, intermediate) to examine how students interact with ChatGPT while solving programming tasks. We analyzed task performance, conceptual understanding, and interaction behaviors. Our findings reveal that generating complete solutions with GenAI significantly improves task performance, especially for beginners, but does not consistently result in knowledge gains. Importantly, usage strategies differ by experience: beginners tend to rely heavily on GenAI toward task completion often without knowledge gain in the process, while intermediates adopt more selective approaches. We find that both over-reliance and minimal use result in weaker knowledge gains overall. Based on our results, we call on students and educators to adopt GenAI as a learning rather than a problem solving tool. Our study highlights the urgent need for guidance when integrating GenAI into programming education to foster deeper understanding.

### 44. Spark-Prover-X1: Formal Theorem Proving Through Diverse Data Training

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Xinyuan Zhou, Yi Lei, Xiaoyu Zhou, Jingyi Sun, Yu Zhu, Zhongyi Ye, Weitai Zhang, Quan Liu, Si Wei, Cong Liu
- **URL**: <http://arxiv.org/abs/2511.13043v2>
- **Submitted**: 2025-11-17 06:44:02
- **Topic Keywords**: www
- **Reason**: This paper is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves a Large Language Model, the focus is on formal theorem proving and automated reasoning, which is not a central match to your research themes.

#### Abstract
> Large Language Models (LLMs) have shown significant promise in automated theorem proving, yet progress is often constrained by the scarcity of diverse and high-quality formal language data. To address this issue, we introduce Spark-Prover-X1, a 7B parameter model trained via an three-stage framework designed to unlock the reasoning potential of more accessible and moderately-sized LLMs. The first stage infuses deep knowledge through continuous pre-training on a broad mathematical corpus, enhanced by a suite of novel data tasks. Key innovation is a "CoT-augmented state prediction" task to achieve fine-grained reasoning. The second stage employs Supervised Fine-tuning (SFT) within an expert iteration loop to specialize both the Spark-Prover-X1-7B and Spark-Formalizer-X1-7B models. Finally, a targeted round of Group Relative Policy Optimization (GRPO) is applied to sharpen the prover's capabilities on the most challenging problems. To facilitate robust evaluation, particularly on problems from real-world examinations, we also introduce ExamFormal-Bench, a new benchmark dataset of 402 formal problems. Experimental results demonstrate that Spark-Prover achieves state-of-the-art performance among similarly-sized open-source models within the "Whole-Proof Generation" paradigm. It shows exceptional performance on difficult competition benchmarks, notably solving 27 problems on PutnamBench (pass@32) and achieving 24.0\% on CombiBench (pass@32). Our work validates that this diverse training data and progressively refined training pipeline provides an effective path for enhancing the formal reasoning capabilities of lightweight LLMs. Both Spark-Prover-X1-7B and Spark-Formalizer-X1-7B, along with the ExamFormal-Bench dataset, are made publicly available at: https://www.modelscope.cn/organization/iflytek, https://gitcode.com/ifly_opensource.

### 45. A Plug-and-Play Spatially-Constrained Representation Enhancement Framework for Local-Life Recommendation

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Hao Jiang, Guoquan Wang, Sheng Yu, Yang Zeng, Wencong Zeng, Guorui Zhou
- **URL**: <http://arxiv.org/abs/2511.12947v1>
- **Submitted**: 2025-11-17 03:58:04
- **Topic Keywords**: recommend
- **Reason**: This paper focuses on local-life recommendation, which is not a core area of interest. While it involves some aspects of information retrieval, such as representation enhancement, the context is not directly related to query understanding, ranking models, or user behavior modeling, which are key areas of your research.

#### Abstract
> Local-life recommendation have witnessed rapid growth, providing users with convenient access to daily essentials. However, this domain faces two key challenges: (1) spatial constraints, driven by the requirements of the local-life scenario, where items are usually shown only to users within a limited geographic area, indirectly reducing their exposure probability; and (2) long-tail sparsity, where few popular items dominate user interactions, while many high-quality long-tail items are largely overlooked due to imbalanced interaction opportunities. Existing methods typically adopt a user-centric perspective, such as modeling spatial user preferences or enhancing long-tail representations with collaborative filtering signals. However, we argue that an item-centric perspective is more suitable for this domain, focusing on enhancing long-tail items representation that align with the spatially-constrained characteristics of local lifestyle services. To tackle this issue, we propose ReST, a Plug-And-Play Spatially-Constrained Representation Enhancement Framework for Long-Tail Local-Life Recommendation. Specifically, we first introduce a Meta ID Warm-up Network, which initializes fundamental ID representations by injecting their basic attribute-level semantic information. Subsequently, we propose a novel Spatially-Constrained ID Representation Enhancement Network (SIDENet) based on contrastive learning, which incorporates two efficient strategies: a spatially-constrained hard sampling strategy and a dynamic representation alignment strategy. This design adaptively identifies weak ID representations based on their attribute-level information during training. It additionally enhances them by capturing latent item relationships within the spatially-constrained characteristics of local lifestyle services, while preserving compatibility with popular items.

### 46. Rethinking the filter bubble? Developing a research agenda for the protective filter bubble

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Jacob Erickson
- **URL**: <http://arxiv.org/abs/2511.12873v1>
- **Submitted**: 2025-11-17 02:03:08
- **Comment**: This work has been published in Big Data & Society. Please cite the journal version
- **Topic Keywords**: search
- **Reason**: This paper is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although it touches on digital safe spaces, it does not address query understanding, ranking models, or user behavior modeling, which are key areas of focus for you.

#### Abstract
> Filter bubbles and echo chambers have received global attention from scholars, media organizations, and the general public. Filter bubbles have primarily been regarded as intrinsically negative, and many studies have sought to minimize their influence. The detrimental influence of filter bubbles is well-studied. Filter bubbles may, for example, create information silos, amplify misinformation, and promote hatred and extremism. However, comparatively few studies have considered the other side of the filter bubble; its protective benefits, particularly to marginalized communities and those living in countries with low levels of press freedom. Through a review of the literature on digital safe spaces and protective filter bubbles, this commentary suggests that there may be a need to rethink the filter bubble, and it proposes several areas for future research.

### 47. LLM Reinforcement in Context

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Thomas Rivasseau
- **URL**: <http://arxiv.org/abs/2511.12782v1>
- **Submitted**: 2025-11-16 21:24:42
- **Comment**: 4 pages
- **Topic Keywords**: search
- **Reason**: This paper focuses on Large Language Model alignment and proposes a solution to strengthen alignment by adding control sentences to user input. While it touches on the concept of user input length, it doesn't directly relate to Information Retrieval, Search technologies, or query understanding, which are core areas of your research interests.

#### Abstract
> Current Large Language Model alignment research mostly focuses on improving model robustness against adversarial attacks and misbehavior by training on examples and prompting. Research has shown that LLM jailbreak probability increases with the size of the user input or conversation length. There is a lack of appropriate research into means of strengthening alignment which also scale with user input length. We propose interruptions as a possible solution to this problem. Interruptions are control sentences added to the user input approximately every x tokens for some arbitrary x. We suggest that this can be generalized to the Chain-of-Thought process to prevent scheming.

### 48. FGNet: Leveraging Feature-Guided Attention to Refine SAM2 for 3D EM Neuron Segmentation

- **LLM Score**: 0
- **Keyword Score**: 4
- **Authors**: Zhenghua Li, Hang Chen, Zihao Sun, Kai Li, Xiaolin Hu
- **URL**: <http://arxiv.org/abs/2511.13063v1>
- **Submitted**: 2025-11-17 07:11:07
- **Topic Keywords**: rag, ctr
- **Reason**: The paper focuses on 3D neuron segmentation in electron microscopy using vision foundation models, which is unrelated to information retrieval, ranking, click modeling, or NLP. It does not address query understanding, ranking models, or user behavior modeling, making it irrelevant to the user's research interests.

#### Abstract
> Accurate segmentation of neural structures in Electron Microscopy (EM) images is paramount for neuroscience. However, this task is challenged by intricate morphologies, low signal-to-noise ratios, and scarce annotations, limiting the accuracy and generalization of existing methods. To address these challenges, we seek to leverage the priors learned by visual foundation models on a vast amount of natural images to better tackle this task. Specifically, we propose a novel framework that can effectively transfer knowledge from Segment Anything 2 (SAM2), which is pre-trained on natural images, to the EM domain. We first use SAM2 to extract powerful, general-purpose features. To bridge the domain gap, we introduce a Feature-Guided Attention module that leverages semantic cues from SAM2 to guide a lightweight encoder, the Fine-Grained Encoder (FGE), in focusing on these challenging regions. Finally, a dual-affinity decoder generates both coarse and refined affinity maps. Experimental results demonstrate that our method achieves performance comparable to state-of-the-art (SOTA) approaches with the SAM2 weights frozen. Upon further fine-tuning on EM data, our method significantly outperforms existing SOTA methods. This study validates that transferring representations pre-trained on natural images, when combined with targeted domain-adaptive guidance, can effectively address the specific challenges in neuron segmentation.

### 49. Evidence of Phase Transitions in Small Transformer-Based Language Models

- **LLM Score**: 0
- **Keyword Score**: 2
- **Authors**: Noah Hong, Tao Hong
- **URL**: <http://arxiv.org/abs/2511.12768v1>
- **Submitted**: 2025-11-16 20:37:12
- **Topic Keywords**: rag
- **Reason**: This paper is unrelated to Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling. It focuses on the dynamics of language model training, which is more relevant to Natural Language Processing and related topics, but not a central match for your research interests.

#### Abstract
> Phase transitions have been proposed as the origin of emergent abilities in large language models (LLMs), where new capabilities appear abruptly once models surpass critical thresholds of scale. Prior work, such as that of Wei et al., demonstrated these phenomena under model and data scaling, with transitions revealed after applying a log scale to training compute. In this work, we ask three complementary questions: (1) Are phase transitions unique to large models, or can they also be observed in small transformer-based language models? (2) Can such transitions be detected directly in linear training space, rather than only after log rescaling? and (3) Can these transitions emerge at early stages of training? To investigate, we train a small GPT-style transformer on a character-level corpus and analyze the evolution of vocabulary usage throughout training. We track the average word length, the number of correct versus incorrect words, and shifts in vocabulary diversity. Building on these measures, we apply Poisson and sub-Poisson statistics to quantify how words connect and reorganize. This combined analysis reveals a distinct transition point during training. Notably, these transitions are not apparent in standard loss or validation curves, but become visible through our vocabulary- and statistics-based probes. Our findings suggest that phase-transition reorganizations are a general feature of language model training, observable even in modest models, detectable directly in linear training space, and occurring surprisingly early as coherence emerges. This perspective provides new insight into the nonlinear dynamics of language model training and underscores the importance of tailored metrics for uncovering phase transition behaviors

### 50. Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?

- **LLM Score**: 0
- **Keyword Score**: 1
- **Authors**: Chunqiu Steven Xia, Zhe Wang, Yan Yang, Yuxiang Wei, Lingming Zhang
- **URL**: <http://arxiv.org/abs/2511.13646v1>
- **Submitted**: 2025-11-17 17:58:18
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The paper focuses on software engineering and the self-evolution of software agents, which is outside your primary areas of interest.

#### Abstract
> Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-G√∂del Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.

---

