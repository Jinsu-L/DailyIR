[
    {
        "title": "Reward Models are Metrics in a Trench Coat",
        "abstract": "The emergence of reinforcement learning in post-training of large language\nmodels has sparked significant interest in reward models. Reward models assess\nthe quality of sampled model outputs to generate training signals. This task is\nalso performed by evaluation metrics that monitor the performance of an AI\nmodel. We find that the two research areas are mostly separate, leading to\nredundant terminology and repeated pitfalls. Common challenges include\nsusceptibility to spurious correlations, impact on downstream reward hacking,\nmethods to improve data quality, and approaches to meta-evaluation. Our\nposition paper argues that a closer collaboration between the fields can help\novercome these issues. To that end, we show how metrics outperform reward\nmodels on specific tasks and provide an extensive survey of the two areas.\nGrounded in this survey, we point to multiple research topics in which closer\nalignment can improve reward models and metrics in areas such as preference\nelicitation methods, avoidance of spurious correlations and reward hacking, and\ncalibration-aware meta-evaluation.",
        "url": "http://arxiv.org/abs/2510.03231v1",
        "pdf_url": "http://arxiv.org/pdf/2510.03231v1",
        "arxiv_id": "2510.03231v1",
        "authors": [
            "Sebastian Gehrmann"
        ],
        "submitted": "2025-10-03 17:59:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Self-Anchor: Large Language Model Reasoning via Step-by-step Attention Alignment",
        "abstract": "To solve complex reasoning tasks for Large Language Models (LLMs),\nprompting-based methods offer a lightweight alternative to fine-tuning and\nreinforcement learning. However, as reasoning chains extend, critical\nintermediate steps and the original prompt will be buried in the context,\nreceiving insufficient attention and leading to errors. In this paper, we\npropose Self-Anchor, a novel pipeline that leverages the inherent structure of\nreasoning to steer LLM attention. Self-Anchor decomposes reasoning trajectories\ninto structured plans and automatically aligns the model's attention to the\nmost relevant inference steps, allowing the model to maintain focus throughout\ngeneration. Our experiment shows that Self-Anchor outperforms SOTA prompting\nmethods across six benchmarks. Notably, Self-Anchor significantly reduces the\nperformance gap between ``non-reasoning'' models and specialized reasoning\nmodels, with the potential to enable most LLMs to tackle complex reasoning\ntasks without retraining.",
        "url": "http://arxiv.org/abs/2510.03223v1",
        "pdf_url": "http://arxiv.org/pdf/2510.03223v1",
        "arxiv_id": "2510.03223v1",
        "authors": [
            "Hongxiang Zhang",
            "Yuan Tian",
            "Tianyi Zhang"
        ],
        "submitted": "2025-10-03 17:56:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Low-probability Tokens Sustain Exploration in Reinforcement Learning with Verifiable Reward",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has propelled Large\nLanguage Models in complex reasoning, yet its scalability is often hindered by\na training bottleneck where performance plateaus as policy entropy collapses,\nsignaling a loss of exploration. Previous methods typically address this by\nmaintaining high policy entropy, yet the precise mechanisms that govern\nmeaningful exploration have remained underexplored. Our analysis suggests that\nan unselective focus on entropy risks amplifying irrelevant tokens and\ndestabilizing training. This paper investigates the exploration dynamics within\nRLVR and identifies a key issue: the gradual elimination of valuable\nlow-probability exploratory tokens, which we term \\textbf{\\textit{reasoning\nsparks}}. We find that while abundant in pre-trained models, these sparks are\nsystematically extinguished during RLVR due to over-penalization, leading to a\ndegeneracy in exploration. To address this, we introduce Low-probability\nRegularization (Lp-Reg). Its core mechanism regularizes the policy towards a\nheuristic proxy distribution. This proxy is constructed by filtering out\npresumed noise tokens and re-normalizing the distribution over the remaining\ncandidates. The result is a less-noisy proxy where the probability of\n\\textit{reasoning sparks} is amplified, which then serves as a soft\nregularization target to shield these valuable tokens from elimination via KL\ndivergence. Experiments show that Lp-Reg enables stable on-policy training for\naround 1,000 steps, a regime where baseline entropy-control methods collapse.\nThis sustained exploration leads to state-of-the-art performance, achieving a\n$60.17\\%$ average accuracy on five math benchmarks, an improvement of $2.66\\%$\nover prior methods. Code is available at https://github.com/CarlanLark/Lp-Reg.",
        "url": "http://arxiv.org/abs/2510.03222v1",
        "pdf_url": "http://arxiv.org/pdf/2510.03222v1",
        "arxiv_id": "2510.03222v1",
        "authors": [
            "Guanhua Huang",
            "Tingqiang Xu",
            "Mingze Wang",
            "Qi Yi",
            "Xue Gong",
            "Siheng Li",
            "Ruibin Xiong",
            "Kejiao Li",
            "Yuhao Jiang",
            "Bo Zhou"
        ],
        "submitted": "2025-10-03 17:56:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Cache-to-Cache: Direct Semantic Communication Between Large Language Models",
        "abstract": "Multi-LLM systems harness the complementary strengths of diverse Large\nLanguage Models, achieving performance and efficiency gains unattainable by a\nsingle model. In existing designs, LLMs communicate through text, forcing\ninternal representations to be transformed into output token sequences. This\nprocess both loses rich semantic information and incurs token-by-token\ngeneration latency. Motivated by these limitations, we ask: Can LLMs\ncommunicate beyond text? Oracle experiments show that enriching the KV-Cache\nsemantics can improve response quality without increasing cache size,\nsupporting KV-Cache as an effective medium for inter-model communication. Thus,\nwe propose Cache-to-Cache (C2C), a new paradigm for direct semantic\ncommunication between LLMs. C2C uses a neural network to project and fuse the\nsource model's KV-cache with that of the target model to enable direct semantic\ntransfer. A learnable gating mechanism selects the target layers that benefit\nfrom cache communication. Compared with text communication, C2C utilizes the\ndeep, specialized semantics from both models, while avoiding explicit\nintermediate text generation. Experiments show that C2C achieves 8.5-10.5%\nhigher average accuracy than individual models. It further outperforms the text\ncommunication paradigm by approximately 3.0-5.0%, while delivering an average\n2.0x speedup in latency. Our code is available at\nhttps://github.com/thu-nics/C2C.",
        "url": "http://arxiv.org/abs/2510.03215v1",
        "pdf_url": "http://arxiv.org/pdf/2510.03215v1",
        "arxiv_id": "2510.03215v1",
        "authors": [
            "Tianyu Fu",
            "Zihan Min",
            "Hanling Zhang",
            "Jichao Yan",
            "Guohao Dai",
            "Wanli Ouyang",
            "Yu Wang"
        ],
        "submitted": "2025-10-03 17:52:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Coevolutionary Continuous Discrete Diffusion: Make Your Diffusion Language Model a Latent Reasoner",
        "abstract": "Diffusion language models, especially masked discrete diffusion models, have\nachieved great success recently. While there are some theoretical and primary\nempirical results showing the advantages of latent reasoning with looped\ntransformers or continuous chain-of-thoughts, continuous diffusion models\ntypically underperform their discrete counterparts. In this paper, we argue\nthat diffusion language models do not necessarily need to be in the discrete\nspace. In particular, we prove that continuous diffusion models have stronger\nexpressivity than discrete diffusions and looped transformers. We attribute the\ncontradiction between the theoretical expressiveness and empirical performance\nto their practical trainability: while continuous diffusion provides\nintermediate supervision that looped transformers lack, they introduce\nadditional difficulty decoding tokens into the discrete token space from the\ncontinuous representation space. We therefore propose Coevolutionary Continuous\nDiscrete Diffusion (CCDD), which defines a joint multimodal diffusion process\non the union of a continuous representation space and a discrete token space,\nleveraging a single model to simultaneously denoise in the joint space. By\ncombining two modalities, CCDD is expressive with rich semantics in the latent\nspace, as well as good trainability and sample quality with the help of\nexplicit discrete tokens. We also propose effective architectures and advanced\ntraining/sampling techniques for CCDD, which reveals strong empirical\nperformance in extensive language modeling experiments on real-world tasks.",
        "url": "http://arxiv.org/abs/2510.03206v1",
        "pdf_url": "http://arxiv.org/pdf/2510.03206v1",
        "arxiv_id": "2510.03206v1",
        "authors": [
            "Cai Zhou",
            "Chenxiao Yang",
            "Yi Hu",
            "Chenyu Wang",
            "Chubin Zhang",
            "Muhan Zhang",
            "Lester Mackey",
            "Tommi Jaakkola",
            "Stephen Bates",
            "Dinghuai Zhang"
        ],
        "submitted": "2025-10-03 17:44:41",
        "source": "arxiv",
        "comment": "27 pages"
    },
    {
        "title": "FocusAgent: Simple Yet Effective Ways of Trimming the Large Context of Web Agents",
        "abstract": "Web agents powered by large language models (LLMs) must process lengthy web\npage observations to complete user goals; these pages often exceed tens of\nthousands of tokens. This saturates context limits and increases computational\ncost processing; moreover, processing full pages exposes agents to security\nrisks such as prompt injection. Existing pruning strategies either discard\nrelevant content or retain irrelevant context, leading to suboptimal action\nprediction. We introduce FocusAgent, a simple yet effective approach that\nleverages a lightweight LLM retriever to extract the most relevant lines from\naccessibility tree (AxTree) observations, guided by task goals. By pruning\nnoisy and irrelevant content, FocusAgent enables efficient reasoning while\nreducing vulnerability to injection attacks. Experiments on WorkArena and\nWebArena benchmarks show that FocusAgent matches the performance of strong\nbaselines, while reducing observation size by over 50%. Furthermore, a variant\nof FocusAgent significantly reduces the success rate of prompt-injection\nattacks, including banner and pop-up attacks, while maintaining task success\nperformance in attack-free settings. Our results highlight that targeted\nLLM-based retrieval is a practical and robust strategy for building web agents\nthat are efficient, effective, and secure.",
        "url": "http://arxiv.org/abs/2510.03204v1",
        "pdf_url": "http://arxiv.org/pdf/2510.03204v1",
        "arxiv_id": "2510.03204v1",
        "authors": [
            "Imene Kerboua",
            "Sahar Omidi Shayegan",
            "Megh Thakkar",
            "Xing Han Lù",
            "Léo Boisvert",
            "Massimo Caccia",
            "Jérémy Espinas",
            "Alexandre Aussem",
            "Véronique Eglin",
            "Alexandre Lacoste"
        ],
        "submitted": "2025-10-03 17:41:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "OpenZL: A Graph-Based Model for Compression",
        "abstract": "Research in general-purpose lossless compression over the last decade has\nlargely found improvements in compression ratio that come at great cost to\nresource utilization and processing throughput. However, most production\nworkloads require high throughput and low resource utilization, so most\nresearch systems have seen little adoption. Instead, real world improvements in\ncompression are increasingly often realized by building application-specific\ncompressors which can exploit knowledge about the structure and semantics of\nthe data being compressed. These systems easily outperform even the best\ngeneric compressors, but application-specific compression schemes are not\nwithout drawbacks. They are inherently limited in applicability and are\ndifficult to maintain and deploy.\n  We show that these challenges can be overcome with a new way of thinking\nabout compression. We propose the ``graph model'' of compression, a new\ntheoretical framework for representing compression as a directed acyclic graph\nof modular codecs. This motivates OpenZL, an implementation of this model that\ncompresses data into a self-describing wire format, any configuration of which\ncan be decompressed by a universal decoder. OpenZL's design enables rapid\ndevelopment of tailored compressors with minimal code, its universal decoder\neliminates deployment lag, and its investment in a well-vetted standard\ncomponent library minimizes security risks. Experimental results demonstrate\nthat OpenZL achieves superior compression ratios and speeds compared to\nstate-of-the-art general-purpose compressors on a variety of real-world\ndatasets. Internal deployments at Meta have also shown consistent improvements\nin size and/or speed, with development timelines reduced from months to days.\nOpenZL thus represents an advance in practical, scalable, and maintainable data\ncompression for modern data-intensive applications.",
        "url": "http://arxiv.org/abs/2510.03203v1",
        "pdf_url": "http://arxiv.org/pdf/2510.03203v1",
        "arxiv_id": "2510.03203v1",
        "authors": [
            "Yann Collet",
            "Nick Terrell",
            "W. Felix Handte",
            "Danielle Rozenblit",
            "Victor Zhang",
            "Kevin Zhang",
            "Yaelle Goldschlag",
            "Jennifer Lee",
            "Daniel Riegel",
            "Stan Angelov",
            "Nadav Rotem"
        ],
        "submitted": "2025-10-03 17:40:29",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Model-Based Ranking of Source Languages for Zero-Shot Cross-Lingual Transfer",
        "abstract": "We present NN-Rank, an algorithm for ranking source languages for\ncross-lingual transfer, which leverages hidden representations from\nmultilingual models and unlabeled target-language data. We experiment with two\npretrained multilingual models and two tasks: part-of-speech tagging (POS) and\nnamed entity recognition (NER). We consider 51 source languages and evaluate on\n56 and 72 target languages for POS and NER, respectively. When using in-domain\ndata, NN-Rank beats state-of-the-art baselines that leverage lexical and\nlinguistic features, with average improvements of up to 35.56 NDCG for POS and\n18.14 NDCG for NER. As prior approaches can fall back to language-level\nfeatures if target language data is not available, we show that NN-Rank remains\ncompetitive using only the Bible, an out-of-domain corpus available for a large\nnumber of languages. Ablations on the amount of unlabeled target data show\nthat, for subsets consisting of as few as 25 examples, NN-Rank produces\nhigh-quality rankings which achieve 92.8% of the NDCG achieved using all\navailable target data for ranking.",
        "url": "http://arxiv.org/abs/2510.03202v1",
        "pdf_url": "http://arxiv.org/pdf/2510.03202v1",
        "arxiv_id": "2510.03202v1",
        "authors": [
            "Abteen Ebrahimi",
            "Adam Wiemerslage",
            "Katharina von der Wense"
        ],
        "submitted": "2025-10-03 17:39:44",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 (Main)"
    },
    {
        "title": "Simulation to Rules: A Dual-VLM Framework for Formal Visual Planning",
        "abstract": "Vision Language Models (VLMs) show strong potential for visual planning but\nstruggle with precise spatial and long-horizon reasoning. In contrast, Planning\nDomain Definition Language (PDDL) planners excel at long-horizon formal\nplanning, but cannot interpret visual inputs. Recent works combine these\ncomplementary advantages by enabling VLMs to turn visual planning problems into\nPDDL files for formal planning. However, while VLMs can generate PDDL problem\nfiles satisfactorily, they struggle to accurately generate the PDDL domain\nfiles, which describe all the planning rules. As a result, prior methods rely\non human experts to predefine domain files or on constant environment access\nfor refinement. We propose VLMFP, a Dual-VLM-guided framework that can\nautonomously generate both PDDL problem and domain files for formal visual\nplanning. VLMFP introduces two VLMs to ensure reliable PDDL file generation: A\nSimVLM that simulates action consequences based on input rule descriptions, and\na GenVLM that generates and iteratively refines PDDL files by comparing the\nPDDL and SimVLM execution results. VLMFP unleashes multiple levels of\ngeneralizability: The same generated PDDL domain file works for all the\ndifferent instances under the same problem, and VLMs generalize to different\nproblems with varied appearances and rules. We evaluate VLMFP with 6 grid-world\ndomains and test its generalization to unseen instances, appearance, and game\nrules. On average, SimVLM accurately describes 95.5%, 82.6% of scenarios,\nsimulates 85.5%, 87.8% of action sequence, and judges 82.4%, 85.6% goal\nreaching for seen and unseen appearances, respectively. With the guidance of\nSimVLM, VLMFP can generate PDDL files to reach 70.0%, 54.1% valid plans for\nunseen instances in seen and unseen appearances, respectively. Project page:\nhttps://sites.google.com/view/vlmfp.",
        "url": "http://arxiv.org/abs/2510.03182v1",
        "pdf_url": "http://arxiv.org/pdf/2510.03182v1",
        "arxiv_id": "2510.03182v1",
        "authors": [
            "Yilun Hao",
            "Yongchao Chen",
            "Chuchu Fan",
            "Yang Zhang"
        ],
        "submitted": "2025-10-03 16:57:01",
        "source": "arxiv",
        "comment": "30 pages, 5 figures, 5 tables"
    },
    {
        "title": "When Names Disappear: Revealing What LLMs Actually Understand About Code",
        "abstract": "Large Language Models (LLMs) achieve strong results on code tasks, but how\nthey derive program meaning remains unclear. We argue that code communicates\nthrough two channels: structural semantics, which define formal behavior, and\nhuman-interpretable naming, which conveys intent. Removing the naming channel\nseverely degrades intent-level tasks such as summarization, where models\nregress to line-by-line descriptions. Surprisingly, we also observe consistent\nreductions on execution tasks that should depend only on structure, revealing\nthat current benchmarks reward memorization of naming patterns rather than\ngenuine semantic reasoning. To disentangle these effects, we introduce a suite\nof semantics-preserving obfuscations and show that they expose identifier\nleakage across both summarization and execution. Building on these insights, we\nrelease ClassEval-Obf, an obfuscation-enhanced benchmark that systematically\nsuppresses naming cues while preserving behavior. Our results demonstrate that\nClassEval-Obf reduces inflated performance gaps, weakens memorization\nshortcuts, and provides a more reliable basis for assessing LLMs' code\nunderstanding and generalization.",
        "url": "http://arxiv.org/abs/2510.03178v1",
        "pdf_url": "http://arxiv.org/pdf/2510.03178v1",
        "arxiv_id": "2510.03178v1",
        "authors": [
            "Cuong Chi Le",
            "Minh V. T. Pham",
            "Cuong Duc Van",
            "Hoang N. Phan",
            "Huy N. Phan",
            "Tien N. Nguyen"
        ],
        "submitted": "2025-10-03 16:53:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Topic Modeling as Long-Form Generation: Can Long-Context LLMs revolutionize NTM via Zero-Shot Prompting?",
        "abstract": "Traditional topic models such as neural topic models rely on inference and\ngeneration networks to learn latent topic distributions. This paper explores a\nnew paradigm for topic modeling in the era of large language models, framing TM\nas a long-form generation task whose definition is updated in this paradigm. We\npropose a simple but practical approach to implement LLM-based topic model\ntasks out of the box (sample a data subset, generate topics and representative\ntext with our prompt, text assignment with keyword match). We then investigate\nwhether the long-form generation paradigm can beat NTMs via zero-shot\nprompting. We conduct a systematic comparison between NTMs and LLMs in terms of\ntopic quality and empirically examine the claim that \"a majority of NTMs are\noutdated.\"",
        "url": "http://arxiv.org/abs/2510.03174v1",
        "pdf_url": "http://arxiv.org/pdf/2510.03174v1",
        "arxiv_id": "2510.03174v1",
        "authors": [
            "Xuan Xu",
            "Haolun Li",
            "Zhongliang Yang",
            "Beilin Chu",
            "Jia Song",
            "Moxuan Xu",
            "Linna Zhou"
        ],
        "submitted": "2025-10-03 16:48:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Neural Correlates of Language Models Are Specific to Human Language",
        "abstract": "Previous work has shown correlations between the hidden states of large\nlanguage models and fMRI brain responses, on language tasks. These correlations\nhave been taken as evidence of the representational similarity of these models\nand brain states. This study tests whether these previous results are robust to\nseveral possible concerns. Specifically this study shows: (i) that the previous\nresults are still found after dimensionality reduction, and thus are not\nattributable to the curse of dimensionality; (ii) that previous results are\nconfirmed when using new measures of similarity; (iii) that correlations\nbetween brain representations and those from models are specific to models\ntrained on human language; and (iv) that the results are dependent on the\npresence of positional encoding in the models. These results confirm and\nstrengthen the results of previous research and contribute to the debate on the\nbiological plausibility and interpretability of state-of-the-art large language\nmodels.",
        "url": "http://arxiv.org/abs/2510.03156v1",
        "pdf_url": "http://arxiv.org/pdf/2510.03156v1",
        "arxiv_id": "2510.03156v1",
        "authors": [
            "Iñigo Parra"
        ],
        "submitted": "2025-10-03 16:28:31",
        "source": "arxiv",
        "comment": "To be presented at NeurIPS 2025 Workshops"
    },
    {
        "title": "EditLens: Quantifying the Extent of AI Editing in Text",
        "abstract": "A significant proportion of queries to large language models ask them to edit\nuser-provided text, rather than generate new text from scratch. While previous\nwork focuses on detecting fully AI-generated text, we demonstrate that\nAI-edited text is distinguishable from human-written and AI-generated text.\nFirst, we propose using lightweight similarity metrics to quantify the\nmagnitude of AI editing present in a text given the original human-written text\nand validate these metrics with human annotators. Using these similarity\nmetrics as intermediate supervision, we then train EditLens, a regression model\nthat predicts the amount of AI editing present within a text. Our model\nachieves state-of-the-art performance on both binary (F1=94.7%) and ternary\n(F1=90.4%) classification tasks in distinguishing human, AI, and mixed writing.\nNot only do we show that AI-edited text can be detected, but also that the\ndegree of change made by AI to human writing can be detected, which has\nimplications for authorship attribution, education, and policy. Finally, as a\ncase study, we use our model to analyze the effects of AI-edits applied by\nGrammarly, a popular writing assistance tool. To encourage further research, we\ncommit to publicly releasing our models and dataset.",
        "url": "http://arxiv.org/abs/2510.03154v1",
        "pdf_url": "http://arxiv.org/pdf/2510.03154v1",
        "arxiv_id": "2510.03154v1",
        "authors": [
            "Katherine Thai",
            "Bradley Emi",
            "Elyas Masrour",
            "Mohit Iyyer"
        ],
        "submitted": "2025-10-03 16:27:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond the Final Layer: Intermediate Representations for Better Multilingual Calibration in Large Language Models",
        "abstract": "Confidence calibration, the alignment of a model's predicted confidence with\nits actual accuracy, is crucial for the reliable deployment of Large Language\nModels (LLMs). However, this critical property remains largely under-explored\nin multilingual contexts. In this work, we conduct the first large-scale,\nsystematic studies of multilingual calibration across six model families and\nover 100 languages, revealing that non-English languages suffer from\nsystematically worse calibration. To diagnose this, we investigate the model's\ninternal representations and find that the final layer, biased by\nEnglish-centric training, provides a poor signal for multilingual confidence.\nIn contrast, our layer-wise analysis uncovers a key insight that\nlate-intermediate layers consistently offer a more reliable and\nbetter-calibrated signal. Building on this, we introduce a suite of\ntraining-free methods, including Language-Aware Confidence Ensemble (LACE),\nwhich adaptively selects an optimal ensemble of layers for each specific\nlanguage. Our study highlights the hidden costs of English-centric alignment\nand offer a new path toward building more globally equitable and trustworthy\nLLMs by looking beyond the final layer.",
        "url": "http://arxiv.org/abs/2510.03136v1",
        "pdf_url": "http://arxiv.org/pdf/2510.03136v1",
        "arxiv_id": "2510.03136v1",
        "authors": [
            "Ej Zhou",
            "Caiqi Zhang",
            "Tiancheng Hu",
            "Chengzu Li",
            "Nigel Collier",
            "Ivan Vulić",
            "Anna Korhonen"
        ],
        "submitted": "2025-10-03 16:07:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SurveyBench: How Well Can LLM(-Agents) Write Academic Surveys?",
        "abstract": "Academic survey writing, which distills vast literature into a coherent and\ninsightful narrative, remains a labor-intensive and intellectually demanding\ntask. While recent approaches, such as general DeepResearch agents and\nsurvey-specialized methods, can generate surveys automatically (a.k.a.\nLLM4Survey), their outputs often fall short of human standards and there lacks\na rigorous, reader-aligned benchmark for thoroughly revealing their\ndeficiencies. To fill the gap, we propose a fine-grained, quiz-driven\nevaluation framework SurveyBench, featuring (1) typical survey topics source\nfrom recent 11,343 arXiv papers and corresponding 4,947 high-quality surveys;\n(2) a multifaceted metric hierarchy that assesses the outline quality (e.g.,\ncoverage breadth, logical coherence), content quality (e.g., synthesis\ngranularity, clarity of insights), and non-textual richness; and (3) a\ndual-mode evaluation protocol that includes content-based and quiz-based\nanswerability tests, explicitly aligned with readers' informational needs.\nResults show SurveyBench effectively challenges existing LLM4Survey approaches\n(e.g., on average 21% lower than human in content-based evaluation).",
        "url": "http://arxiv.org/abs/2510.03120v1",
        "pdf_url": "http://arxiv.org/pdf/2510.03120v1",
        "arxiv_id": "2510.03120v1",
        "authors": [
            "Zhaojun Sun",
            "Xuzhou Zhu",
            "Xuanhe Zhou",
            "Xin Tong",
            "Shuo Wang",
            "Jie Fu",
            "Guoliang Li",
            "Zhiyuan Liu",
            "Fan Wu"
        ],
        "submitted": "2025-10-03 15:49:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Listening or Reading? Evaluating Speech Awareness in Chain-of-Thought Speech-to-Text Translation",
        "abstract": "Speech-to-Text Translation (S2TT) systems built from Automatic Speech\nRecognition (ASR) and Text-to-Text Translation (T2TT) modules face two major\nlimitations: error propagation and the inability to exploit prosodic or other\nacoustic cues. Chain-of-Thought (CoT) prompting has recently been introduced,\nwith the expectation that jointly accessing speech and transcription will\novercome these issues. Analyzing CoT through attribution methods, robustness\nevaluations with corrupted transcripts, and prosody-awareness, we find that it\nlargely mirrors cascaded behavior, relying mainly on transcripts while barely\nleveraging speech. Simple training interventions, such as adding Direct S2TT\ndata or noisy transcript injection, enhance robustness and increase speech\nattribution. These findings challenge the assumed advantages of CoT and\nhighlight the need for architectures that explicitly integrate acoustic\ninformation into translation.",
        "url": "http://arxiv.org/abs/2510.03115v1",
        "pdf_url": "http://arxiv.org/pdf/2510.03115v1",
        "arxiv_id": "2510.03115v1",
        "authors": [
            "Jacobo Romero-Díaz",
            "Gerard I. Gállego",
            "Oriol Pareras",
            "Federico Costa",
            "Javier Hernando",
            "Cristina España-Bonet"
        ],
        "submitted": "2025-10-03 15:42:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Semantic Similarity in Radiology Reports via LLMs and NER",
        "abstract": "Radiology report evaluation is a crucial part of radiologists' training and\nplays a key role in ensuring diagnostic accuracy. As part of the standard\nreporting workflow, a junior radiologist typically prepares a preliminary\nreport, which is then reviewed and edited by a senior radiologist to produce\nthe final report. Identifying semantic differences between preliminary and\nfinal reports is essential for junior doctors, both as a training tool and to\nhelp uncover gaps in clinical knowledge. While AI in radiology is a rapidly\ngrowing field, the application of large language models (LLMs) remains\nchallenging due to the need for specialised domain knowledge. In this paper, we\nexplore the ability of LLMs to provide explainable and accurate comparisons of\nreports in the radiology domain. We begin by comparing the performance of\nseveral LLMs in comparing radiology reports. We then assess a more traditional\napproach based on Named-Entity-Recognition (NER). However, both approaches\nexhibit limitations in delivering accurate feedback on semantic similarity. To\naddress this, we propose Llama-EntScore, a semantic similarity scoring method\nusing a combination of Llama 3.1 and NER with tunable weights to emphasise or\nde-emphasise specific types of differences. Our approach generates a\nquantitative similarity score for tracking progress and also gives an\ninterpretation of the score that aims to offer valuable guidance in reviewing\nand refining their reporting. We find our method achieves 67% exact-match\naccuracy and 93% accuracy within +/- 1 when compared to radiologist-provided\nground truth scores - outperforming both LLMs and NER used independently. Code\nis available at:\n\\href{https://github.com/otmive/llama_reports}{github.com/otmive/llama\\_reports}",
        "url": "http://arxiv.org/abs/2510.03102v1",
        "pdf_url": "http://arxiv.org/pdf/2510.03102v1",
        "arxiv_id": "2510.03102v1",
        "authors": [
            "Beth Pearson",
            "Ahmed Adnan",
            "Zahraa Abdallah"
        ],
        "submitted": "2025-10-03 15:31:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Revisiting Direct Speech-to-Text Translation with Speech LLMs: Better Scaling than CoT Prompting?",
        "abstract": "Recent work on Speech-to-Text Translation (S2TT) has focused on LLM-based\nmodels, introducing the increasingly adopted Chain-of-Thought (CoT) prompting,\nwhere the model is guided to first transcribe the speech and then translate it.\nCoT typically outperforms direct prompting primarily because it can exploit\nabundant Automatic Speech Recognition (ASR) and Text-to-Text Translation (T2TT)\ndatasets to explicitly model its steps. In this paper, we systematically\ncompare CoT and Direct prompting under increasing amounts of S2TT data. To this\nend, we pseudo-label an ASR corpus by translating its transcriptions into six\nEuropean languages, and train LLM-based S2TT systems with both prompting\nstrategies at different data scales. Our results show that Direct improves more\nconsistently as the amount of data increases, suggesting that it may become a\nmore effective approach as larger S2TT resources are created.",
        "url": "http://arxiv.org/abs/2510.03093v1",
        "pdf_url": "http://arxiv.org/pdf/2510.03093v1",
        "arxiv_id": "2510.03093v1",
        "authors": [
            "Oriol Pareras",
            "Gerard I. Gállego",
            "Federico Costa",
            "Cristina España-Bonet",
            "Javier Hernando"
        ],
        "submitted": "2025-10-03 15:23:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Semantic Differentiation in Speech Emotion Recognition: Insights from Descriptive and Expressive Speech Roles",
        "abstract": "Speech Emotion Recognition (SER) is essential for improving human-computer\ninteraction, yet its accuracy remains constrained by the complexity of\nemotional nuances in speech. In this study, we distinguish between descriptive\nsemantics, which represents the contextual content of speech, and expressive\nsemantics, which reflects the speaker's emotional state. After watching\nemotionally charged movie segments, we recorded audio clips of participants\ndescribing their experiences, along with the intended emotion tags for each\nclip, participants' self-rated emotional responses, and their valence/arousal\nscores. Through experiments, we show that descriptive semantics align with\nintended emotions, while expressive semantics correlate with evoked emotions.\nOur findings inform SER applications in human-AI interaction and pave the way\nfor more context-aware AI systems.",
        "url": "http://arxiv.org/abs/2510.03060v1",
        "pdf_url": "http://arxiv.org/pdf/2510.03060v1",
        "arxiv_id": "2510.03060v1",
        "authors": [
            "Rongchen Guo",
            "Vincent Francoeur",
            "Isar Nejadgholi",
            "Sylvain Gagnon",
            "Miodrag Bolic"
        ],
        "submitted": "2025-10-03 14:42:35",
        "source": "arxiv",
        "comment": "Accepted to the *SEM conference collocated with EMNLP2025"
    },
    {
        "title": "CHORD: Customizing Hybrid-precision On-device Model for Sequential Recommendation with Device-cloud Collaboration",
        "abstract": "With the advancement of mobile device capabilities, deploying reranking\nmodels directly on devices has become feasible, enabling real-time contextual\nrecommendations. When migrating models from cloud to devices, resource\nheterogeneity inevitably necessitates model compression. Recent quantization\nmethods show promise for efficient deployment, yet they overlook\ndevice-specific user interests, resulting in compromised recommendation\naccuracy. While on-device finetuning captures personalized user preference, it\nimposes additional computational burden through local retraining. To address\nthese challenges, we propose a framework for \\underline{\\textbf{C}}ustomizing\n\\underline{\\textbf{H}}ybrid-precision \\underline{\\textbf{O}}n-device model for\nsequential \\underline{\\textbf{R}}ecommendation with\n\\underline{\\textbf{D}}evice-cloud collaboration (\\textbf{CHORD}), leveraging\nchannel-wise mixed-precision quantization to simultaneously achieve\npersonalization and resource-adaptive deployment. CHORD distributes randomly\ninitialized models across heterogeneous devices and identifies user-specific\ncritical parameters through auxiliary hypernetwork modules on the cloud. Our\nparameter sensitivity analysis operates across multiple granularities (layer,\nfilter, and element levels), enabling precise mapping from user profiles to\nquantization strategy. Through on-device mixed-precision quantization, CHORD\ndelivers dynamic model adaptation and accelerated inference without\nbackpropagation, eliminating costly retraining cycles. We minimize\ncommunication overhead by encoding quantization strategies using only 2 bits\nper channel instead of 32-bit weights. Experiments on three real-world datasets\nwith two popular backbones (SASRec and Caser) demonstrate the accuracy,\nefficiency, and adaptivity of CHORD.",
        "url": "http://arxiv.org/abs/2510.03038v1",
        "pdf_url": "http://arxiv.org/pdf/2510.03038v1",
        "arxiv_id": "2510.03038v1",
        "authors": [
            "Tianqi Liu",
            "Kairui Fu",
            "Shengyu Zhang",
            "Wenyan Fan",
            "Zhaocheng Du",
            "Jieming Zhu",
            "Fan Wu",
            "Fei Wu"
        ],
        "submitted": "2025-10-03 14:20:45",
        "source": "arxiv",
        "comment": "accepted by ACM MM'25"
    },
    {
        "title": "Grounding Large Language Models in Clinical Evidence: A Retrieval-Augmented Generation System for Querying UK NICE Clinical Guidelines",
        "abstract": "This paper presents the development and evaluation of a Retrieval-Augmented\nGeneration (RAG) system for querying the United Kingdom's National Institute\nfor Health and Care Excellence (NICE) clinical guidelines using Large Language\nModels (LLMs). The extensive length and volume of these guidelines can impede\ntheir utilisation within a time-constrained healthcare system, a challenge this\nproject addresses through the creation of a system capable of providing users\nwith precisely matched information in response to natural language queries. The\nsystem's retrieval architecture, composed of a hybrid embedding mechanism, was\nevaluated against a database of 10,195 text chunks derived from three hundred\nguidelines. It demonstrates high performance, with a Mean Reciprocal Rank (MRR)\nof 0.814, a Recall of 81% at the first chunk and of 99.1% within the top ten\nretrieved chunks, when evaluated on 7901 queries.\n  The most significant impact of the RAG system was observed during the\ngeneration phase. When evaluated on a manually curated dataset of seventy\nquestion-answer pairs, RAG-enhanced models showed substantial gains in\nperformance. Faithfulness, the measure of whether an answer is supported by the\nsource text, was increased by 64.7 percentage points to 99.5% for the\nRAG-enhanced O4-Mini model and significantly outperformed the medical-focused\nMeditron3-8B LLM, which scored 43%. This, combined with a perfect Context\nPrecision score of 1 for all RAG-enhanced models, confirms the system's ability\nto prevent information fabrication by grounding its answers in relevant source\nmaterial. This study thus establishes RAG as an effective, reliable, and\nscalable approach for applying generative AI in healthcare, enabling\ncost-effective access to medical guidelines.",
        "url": "http://arxiv.org/abs/2510.02967v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02967v1",
        "arxiv_id": "2510.02967v1",
        "authors": [
            "Matthew Lewis",
            "Samuel Thio",
            "Richard JB Dobson",
            "Spiros Denaxas"
        ],
        "submitted": "2025-10-03 12:57:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Leave No TRACE: Black-box Detection of Copyrighted Dataset Usage in Large Language Models via Watermarking",
        "abstract": "Large Language Models (LLMs) are increasingly fine-tuned on smaller,\ndomain-specific datasets to improve downstream performance. These datasets\noften contain proprietary or copyrighted material, raising the need for\nreliable safeguards against unauthorized use. Existing membership inference\nattacks (MIAs) and dataset-inference methods typically require access to\ninternal signals such as logits, while current black-box approaches often rely\non handcrafted prompts or a clean reference dataset for calibration, both of\nwhich limit practical applicability. Watermarking is a promising alternative,\nbut prior techniques can degrade text quality or reduce task performance. We\npropose TRACE, a practical framework for fully black-box detection of\ncopyrighted dataset usage in LLM fine-tuning. \\texttt{TRACE} rewrites datasets\nwith distortion-free watermarks guided by a private key, ensuring both text\nquality and downstream utility. At detection time, we exploit the radioactivity\neffect of fine-tuning on watermarked data and introduce an entropy-gated\nprocedure that selectively scores high-uncertainty tokens, substantially\namplifying detection power. Across diverse datasets and model families, TRACE\nconsistently achieves significant detections (p<0.05), often with extremely\nstrong statistical evidence. Furthermore, it supports multi-dataset attribution\nand remains robust even after continued pretraining on large non-watermarked\ncorpora. These results establish TRACE as a practical route to reliable\nblack-box verification of copyrighted dataset usage. We will make our code\navailable at: https://github.com/NusIoraPrivacy/TRACE.",
        "url": "http://arxiv.org/abs/2510.02962v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02962v1",
        "arxiv_id": "2510.02962v1",
        "authors": [
            "Jingqi Zhang",
            "Ruibo Chen",
            "Yingqing Yang",
            "Peihua Mai",
            "Heng Huang",
            "Yan Pang"
        ],
        "submitted": "2025-10-03 12:53:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Finding Diamonds in Conversation Haystacks: A Benchmark for Conversational Data Retrieval",
        "abstract": "We present the Conversational Data Retrieval (CDR) benchmark, the first\ncomprehensive test set for evaluating systems that retrieve conversation data\nfor product insights. With 1.6k queries across five analytical tasks and 9.1k\nconversations, our benchmark provides a reliable standard for measuring\nconversational data retrieval performance. Our evaluation of 16 popular\nembedding models shows that even the best models reach only around NDCG@10 of\n0.51, revealing a substantial gap between document and conversational data\nretrieval capabilities. Our work identifies unique challenges in conversational\ndata retrieval (implicit state recognition, turn dynamics, contextual\nreferences) while providing practical query templates and detailed error\nanalysis across different task categories. The benchmark dataset and code are\navailable at https://github.com/l-yohai/CDR-Benchmark.",
        "url": "http://arxiv.org/abs/2510.02938v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02938v1",
        "arxiv_id": "2510.02938v1",
        "authors": [
            "Yohan Lee",
            "Yongwoo Song",
            "Sangyeop Kim"
        ],
        "submitted": "2025-10-03 12:29:44",
        "source": "arxiv",
        "comment": "Accepted by EMNLP 2025 Industry Track"
    },
    {
        "title": "Self-Reflective Generation at Test Time",
        "abstract": "Large language models (LLMs) increasingly solve complex reasoning tasks via\nlong chain-of-thought, but their forward-only autoregressive generation process\nis fragile; early token errors can cascade, which creates a clear need for\nself-reflection mechanisms. However, existing self-reflection either performs\nrevisions over full drafts or learns self-correction via expensive training,\nboth fundamentally reactive and inefficient. To address this, we propose\nSelf-Reflective Generation at Test Time (SRGen), a lightweight test-time\nframework that reflects before generating at uncertain points. During token\ngeneration, SRGen utilizes dynamic entropy thresholding to identify\nhigh-uncertainty tokens. For each identified token, it trains a specific\ncorrective vector, which fully exploits the already generated context for a\nself-reflective generation to correct the token probability distribution. By\nretrospectively analyzing the partial output, this self-reflection enables more\ntrustworthy decisions, thereby significantly reducing the probability of errors\nat highly uncertain points. Evaluated on challenging mathematical reasoning\nbenchmarks and a diverse set of LLMs, SRGen can consistently strengthen model\nreasoning: improvements in single-pass quality also translate into stronger\nself-consistency voting. Especially, on AIME2024 with\nDeepSeek-R1-Distill-Qwen-7B, SRGen yields absolute improvements of +12.0% on\nPass@1 and +13.3% on Cons@5. Moreover, our findings position SRGen as a\nplug-and-play method that integrates reflection into the generation process for\nreliable LLM reasoning, achieving consistent gains with bounded overhead and\nbroad composability with other training-time (e.g., RLHF) and test-time (e.g.,\nSLOT) techniques.",
        "url": "http://arxiv.org/abs/2510.02919v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02919v1",
        "arxiv_id": "2510.02919v1",
        "authors": [
            "Jian Mu",
            "Qixin Zhang",
            "Zhiyong Wang",
            "Menglin Yang",
            "Shuang Qiu",
            "Chengwei Qin",
            "Zhongxiang Dai",
            "Yao Shu"
        ],
        "submitted": "2025-10-03 11:46:04",
        "source": "arxiv",
        "comment": "24 pages, 8 figures"
    },
    {
        "title": "Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation",
        "abstract": "Wordle presents an algorithmically rich testbed for constraint satisfaction\nproblem (CSP) solving. While existing solvers rely on information-theoretic\nentropy maximization or frequency-based heuristics without formal constraint\ntreatment, we present the first comprehensive CSP formulation of Wordle with\nnovel constraint-aware solving strategies. We introduce CSP-Aware Entropy,\ncomputing information gain after constraint propagation rather than on raw\ncandidate sets, and a Probabilistic CSP framework integrating Bayesian\nword-frequency priors with logical constraints. Through evaluation on 2,315\nEnglish words, CSP-Aware Entropy achieves 3.54 average guesses with 99.9%\nsuccess rate, a statistically significant 1.7% improvement over Forward\nChecking (t=-4.82, p<0.001, Cohen's d=0.07) with 46% faster runtime (12.9ms\nversus 23.7ms per guess). Under 10% noise, CSP-aware approaches maintain 5.3\npercentage point advantages (29.0% versus 23.7%, p=0.041), while Probabilistic\nCSP achieves 100% success across all noise levels (0-20%) through constraint\nrecovery mechanisms. Cross-lexicon validation on 500 Spanish words demonstrates\n88% success with zero language-specific tuning, validating that core CSP\nprinciples transfer across languages despite an 11.2 percentage point gap from\nlinguistic differences (p<0.001, Fisher's exact test). Our open-source\nimplementation with 34 unit tests achieving 91% code coverage provides\nreproducible infrastructure for CSP research. The combination of formal CSP\ntreatment, constraint-aware heuristics, probabilistic-logical integration,\nrobustness analysis, and cross-lexicon validation establishes new performance\nbenchmarks demonstrating that principled constraint satisfaction techniques\noutperform classical information-theoretic and learning-based approaches for\nstructured puzzle-solving domains.",
        "url": "http://arxiv.org/abs/2510.02855v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02855v1",
        "arxiv_id": "2510.02855v1",
        "authors": [
            "Jahidul Arafat",
            "Fariha Tasmin",
            "Sanjaya Poudel",
            "Kamrujjaman",
            "Eftakhar Ahmed Arnob",
            "Ahsan Habib Tareq"
        ],
        "submitted": "2025-10-03 09:44:14",
        "source": "arxiv",
        "comment": "35 pages, 14 figures, 10 tables. Open-source implementation with 91%\n  test coverage available at\n  https://github.com/jahidul-arafat/constraint_satisfaction_wordle_arxiv_preprint"
    },
    {
        "title": "Beyond the Final Answer: Evaluating the Reasoning Trajectories of Tool-Augmented Agents",
        "abstract": "Although recent tool-augmented benchmarks incorporate complex user requests\nand diverse tools, the evaluation methods for most of them remain limited to\nanswer matching. However, as the number of steps required to resolve a user\nrequest increases, a proper evaluation of an agent's performance must go beyond\nthe final answer to also assess the problem-solving trajectory, including\npreviously ignored aspects such as efficiency, hallucination, and adaptivity.\nThe most straightforward method for evaluating these aspects is to compare an\nagent's trajectory with the ground-truth trajectory, but this approach is\nfundamentally limited since annotating all valid ground-truth trajectories is\nprohibitively expensive. However, a simple LLM-based evaluator struggles to\nassess trajectories in detail without ground truth. To effectively evaluate the\nagents in this manner, we introduce TRACE, a framework for the\nmulti-dimensional evaluation of tool-augmented LLM agent performance. By\nincorporating an evidence bank, which accumulates knowledge gathered from\npreceding reasoning steps, TRACE enables a multi-faceted analysis and\nevaluation of an agent's reasoning trajectory effectively. To validate our\nframework, we develop a new meta-evaluation dataset by augmenting existing\nbenchmarks with diverse and flawed trajectories, each labeled with\nmulti-faceted performance scores. Our results confirm that TRACE accurately\nevaluates these complex behaviors in a scalable and cost-effective manner, even\nwith small open-source LLMs. Furthermore, we apply our method to evaluate the\ntrajectories that agents produce while solving tool-augmented tasks, presenting\npreviously unreported observations and their corresponding insights.",
        "url": "http://arxiv.org/abs/2510.02837v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02837v1",
        "arxiv_id": "2510.02837v1",
        "authors": [
            "Wonjoong Kim",
            "Sangwu Park",
            "Yeonjun In",
            "Sein Kim",
            "Dongha Lee",
            "Chanyoung Park"
        ],
        "submitted": "2025-10-03 09:19:15",
        "source": "arxiv",
        "comment": "Preprint. Under Review"
    },
    {
        "title": "Evaluating Large Language Models for IUCN Red List Species Information",
        "abstract": "Large Language Models (LLMs) are rapidly being adopted in conservation to\naddress the biodiversity crisis, yet their reliability for species evaluation\nis uncertain. This study systematically validates five leading models on 21,955\nspecies across four core IUCN Red List assessment components: taxonomy,\nconservation status, distribution, and threats. A critical paradox was\nrevealed: models excelled at taxonomic classification (94.9%) but consistently\nfailed at conservation reasoning (27.2% for status assessment). This\nknowledge-reasoning gap, evident across all models, suggests inherent\narchitectural constraints, not just data limitations. Furthermore, models\nexhibited systematic biases favoring charismatic vertebrates, potentially\namplifying existing conservation inequities. These findings delineate clear\nboundaries for responsible LLM deployment: they are powerful tools for\ninformation retrieval but require human oversight for judgment-based decisions.\nA hybrid approach is recommended, where LLMs augment expert capacity while\nhuman experts retain sole authority over risk assessment and policy.",
        "url": "http://arxiv.org/abs/2510.02830v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02830v1",
        "arxiv_id": "2510.02830v1",
        "authors": [
            "Shinya Uryu"
        ],
        "submitted": "2025-10-03 09:09:35",
        "source": "arxiv",
        "comment": "20 pages, 7 figures"
    },
    {
        "title": "StepChain GraphRAG: Reasoning Over Knowledge Graphs for Multi-Hop Question Answering",
        "abstract": "Recent progress in retrieval-augmented generation (RAG) has led to more\naccurate and interpretable multi-hop question answering (QA). Yet, challenges\npersist in integrating iterative reasoning steps with external knowledge\nretrieval. To address this, we introduce StepChain GraphRAG, a framework that\nunites question decomposition with a Breadth-First Search (BFS) Reasoning Flow\nfor enhanced multi-hop QA. Our approach first builds a global index over the\ncorpus; at inference time, only retrieved passages are parsed on-the-fly into a\nknowledge graph, and the complex query is split into sub-questions. For each\nsub-question, a BFS-based traversal dynamically expands along relevant edges,\nassembling explicit evidence chains without overwhelming the language model\nwith superfluous context. Experiments on MuSiQue, 2WikiMultiHopQA, and HotpotQA\nshow that StepChain GraphRAG achieves state-of-the-art Exact Match and F1\nscores. StepChain GraphRAG lifts average EM by 2.57% and F1 by 2.13% over the\nSOTA method, achieving the largest gain on HotpotQA (+4.70% EM, +3.44% F1).\nStepChain GraphRAG also fosters enhanced explainability by preserving the\nchain-of-thought across intermediate retrieval steps. We conclude by discussing\nhow future work can mitigate the computational overhead and address potential\nhallucinations from large language models to refine efficiency and reliability\nin multi-hop QA.",
        "url": "http://arxiv.org/abs/2510.02827v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02827v1",
        "arxiv_id": "2510.02827v1",
        "authors": [
            "Tengjun Ni",
            "Xin Yuan",
            "Shenghong Li",
            "Kai Wu",
            "Ren Ping Liu",
            "Wei Ni",
            "Wenjie Zhang"
        ],
        "submitted": "2025-10-03 09:06:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "NCV: A Node-Wise Consistency Verification Approach for Low-Cost Structured Error Localization in LLM Reasoning",
        "abstract": "Verifying multi-step reasoning in large language models is difficult due to\nimprecise error localization and high token costs. Existing methods either\nassess entire reasoning chains, suffering attention dilution, or rely on\nexpensive multi-sampling. We introduce Node-wise Consistency Verification\n(NCV), a training-free framework that recasts verification as lightweight\nbinary consistency checks at the node level. By decomposing the chain of\nthought into interconnected verification nodes, NCV precisely localizes errors\nand avoids unnecessary long-form generation. Experiments demonstrate that our\napproach enhances interpretability and efficiency, presenting a scalable\nsolution for reliable LLM reasoning verification. On public datasets, NCV\nachieves a 10\\% to 25\\% improvement in F1 scores over baselines while utilizing\n$6\\times$~$58\\times$ fewer tokens than traditional methods like CoT-based\nverifiers.",
        "url": "http://arxiv.org/abs/2510.02816v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02816v1",
        "arxiv_id": "2510.02816v1",
        "authors": [
            "Yulong Zhang",
            "Li Wang",
            "Wei Du",
            "Peilin Li",
            "Yuqin Dai Zhiyuan Zhao",
            "Lingyong Fang",
            "Ziniu Liu",
            "Ru Zhang",
            "Huijia Zhu",
            "Gongshen Liu"
        ],
        "submitted": "2025-10-03 08:48:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Computational Framework for Interpretable Text-Based Personality Assessment from Social Media",
        "abstract": "Personality refers to individual differences in behavior, thinking, and\nfeeling. With the growing availability of digital footprints, especially from\nsocial media, automated methods for personality assessment have become\nincreasingly important. Natural language processing (NLP) enables the analysis\nof unstructured text data to identify personality indicators. However, two main\nchallenges remain central to this thesis: the scarcity of large,\npersonality-labeled datasets and the disconnect between personality psychology\nand NLP, which restricts model validity and interpretability. To address these\nchallenges, this thesis presents two datasets -- MBTI9k and PANDORA --\ncollected from Reddit, a platform known for user anonymity and diverse\ndiscussions. The PANDORA dataset contains 17 million comments from over 10,000\nusers and integrates the MBTI and Big Five personality models with demographic\ninformation, overcoming limitations in data size, quality, and label coverage.\nExperiments on these datasets show that demographic variables influence model\nvalidity. In response, the SIMPA (Statement-to-Item Matching Personality\nAssessment) framework was developed - a computational framework for\ninterpretable personality assessment that matches user-generated statements\nwith validated questionnaire items. By using machine learning and semantic\nsimilarity, SIMPA delivers personality assessments comparable to human\nevaluations while maintaining high interpretability and efficiency. Although\nfocused on personality assessment, SIMPA's versatility extends beyond this\ndomain. Its model-agnostic design, layered cue detection, and scalability make\nit suitable for various research and practical applications involving complex\nlabel taxonomies and variable cue associations with target concepts.",
        "url": "http://arxiv.org/abs/2510.02811v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02811v1",
        "arxiv_id": "2510.02811v1",
        "authors": [
            "Matej Gjurković"
        ],
        "submitted": "2025-10-03 08:36:36",
        "source": "arxiv",
        "comment": "Phd thesis"
    },
    {
        "title": "Pareto-optimal Non-uniform Language Generation",
        "abstract": "Kleinberg and Mullainathan (2024) recently proposed an interesting model for\nlanguage generation in the limit: Given a countable collection of languages,\nand an adversary enumerating the strings of some language $L$ from the\ncollection, the objective is to generate new strings from the target language,\nsuch that all strings generated beyond some finite time are valid. Li, Raman\nand Tewari (2024) and Charikar and Pabbaraju (2024) showed strong non-uniform\ngeneration guarantees in this model, giving algorithms that generate new valid\nstrings from $L$ after seeing a number of distinct input strings $t(L)$ that\ndepends only on $L$ (and the collection), but not the enumeration order.\nHowever, for both these works, the language-wise generation times $t(L)$ of the\nalgorithm can be strictly sub-optimal.\n  In this work, we study Pareto-optimality of non-uniform language generation\nin the limit. We propose an algorithm, whose generation times $t^\\star(L)$ are\n(almost) Pareto-optimal: any other algorithm whose generation time for some\nlanguage $L$ is strictly smaller than $t^\\star(L)$, must satisfy that its\ngeneration time for some other language $L'$ is strictly worse than\n$t^\\star(L')$. Pareto-optimality is essentially the best that one can achieve\nfor non-uniform generation. Our algorithmic framework conveniently adapts to\nfurther give Pareto-optimal non-uniform generation algorithms in the\npractically motivated settings of noisy as well as representative generation.",
        "url": "http://arxiv.org/abs/2510.02795v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02795v1",
        "arxiv_id": "2510.02795v1",
        "authors": [
            "Moses Charikar",
            "Chirag Pabbaraju"
        ],
        "submitted": "2025-10-03 08:08:20",
        "source": "arxiv",
        "comment": "24 pages, 1 figure"
    },
    {
        "title": "MaskCD: Mitigating LVLM Hallucinations by Image Head Masked Contrastive Decoding",
        "abstract": "Large vision-language models (LVLMs) have shown remarkable performance in\nvisual-language understanding for downstream multimodal tasks. While their\ncapabilities are improving, problems emerge simultaneously. Among those\nproblems, the hallucinations have attracted much attention, which stands for\nthe phenomenon where LVLMs generate contradictory content to their input visual\nand text contents. Many approaches have been proposed to deal with this issue,\nsuch as contrastive decoding and attention manipulation. However, contrastive\ndecoding methods struggle in constructing appropriate contrastive samples, and\nattention manipulation methods are highly sensitive, lacking stability. In this\nwork, we propose image head Masked Contrastive Decoding (MaskCD). Our approach\nutilizes the \"image heads\" in LVLMs, masking them to construct contrastive\nsamples for contrastive decoding. We evaluated MaskCD on LLaVA-1.5-7b and\nQwen-VL-7b, using various benchmarks such as CHAIR, POPE, AMBER and MME. The\nresults demonstrate that MaskCD effectively alleviates the phenomenon of\nhallucinations and retains the general capabilities of LVLMs. Corresponding\nresources could be found at: https://github.com/Deng-Jingyuan/MaskCD .",
        "url": "http://arxiv.org/abs/2510.02790v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02790v1",
        "arxiv_id": "2510.02790v1",
        "authors": [
            "Jingyuan Deng",
            "Yujiu Yang"
        ],
        "submitted": "2025-10-03 07:59:16",
        "source": "arxiv",
        "comment": "accepted to emnlp2025 findings"
    },
    {
        "title": "XTRA: Cross-Lingual Topic Modeling with Topic and Representation Alignments",
        "abstract": "Cross-lingual topic modeling aims to uncover shared semantic themes across\nlanguages. Several methods have been proposed to address this problem,\nleveraging both traditional and neural approaches. While previous methods have\nachieved some improvements in topic diversity, they often struggle to ensure\nhigh topic coherence and consistent alignment across languages. We propose XTRA\n(Cross-Lingual Topic Modeling with Topic and Representation Alignments), a\nnovel framework that unifies Bag-of-Words modeling with multilingual\nembeddings. XTRA introduces two core components: (1) representation alignment,\naligning document-topic distributions via contrastive learning in a shared\nsemantic space; and (2) topic alignment, projecting topic-word distributions\ninto the same space to enforce crosslingual consistency. This dual mechanism\nenables XTRA to learn topics that are interpretable (coherent and diverse) and\nwell-aligned across languages. Experiments on multilingual corpora confirm that\nXTRA significantly outperforms strong baselines in topic coherence, diversity,\nand alignment quality. Code and reproducible scripts are available at https:\n//github.com/tienphat140205/XTRA.",
        "url": "http://arxiv.org/abs/2510.02788v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02788v1",
        "arxiv_id": "2510.02788v1",
        "authors": [
            "Tien Phat Nguyen",
            "Vu Minh Ngo",
            "Tung Nguyen",
            "Linh Van Ngo",
            "Duc Anh Nguyen",
            "Sang Dinh",
            "Trung Le"
        ],
        "submitted": "2025-10-03 07:46:23",
        "source": "arxiv",
        "comment": "2025 EMNLP Findings"
    },
    {
        "title": "A Granular Study of Safety Pretraining under Model Abliteration",
        "abstract": "Open-weight LLMs can be modified at inference time with simple activation\nedits, which raises a practical question for safety: do common safety\ninterventions like refusal training or metatag training survive such edits? We\nstudy model abliteration, a lightweight projection technique designed to remove\nrefusal-sensitive directions, and conduct a controlled evaluation across a\ngranular sequence of Safety Pretraining checkpoints for SmolLM2-1.7B, alongside\nwidely used open baselines. For each of 20 systems, original and abliterated,\nwe issue 100 prompts with balanced harmful and harmless cases, classify\nresponses as **Refusal** or **Non-Refusal** using multiple judges, and validate\njudge fidelity on a small human-labeled subset. We also probe whether models\ncan identify refusal in their own outputs. Our study produces a\ncheckpoint-level characterization of which data-centric safety components\nremain robust under abliteration, quantifies how judge selection influences\nevaluation outcomes, and outlines a practical protocol for integrating\ninference-time edits into safety assessments. Code:\nhttps://github.com/shashankskagnihotri/safety_pretraining.",
        "url": "http://arxiv.org/abs/2510.02768v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02768v1",
        "arxiv_id": "2510.02768v1",
        "authors": [
            "Shashank Agnihotri",
            "Jonas Jakubassa",
            "Priyam Dey",
            "Sachin Goyal",
            "Bernt Schiele",
            "Venkatesh Babu Radhakrishnan",
            "Margret Keuper"
        ],
        "submitted": "2025-10-03 07:01:45",
        "source": "arxiv",
        "comment": "Accepted at NeurIPS 2025 bWorkshop Lock-LLM. *Equal Contribution"
    },
    {
        "title": "The Path of Self-Evolving Large Language Models: Achieving Data-Efficient Learning via Intrinsic Feedback",
        "abstract": "Reinforcement learning (RL) has demonstrated potential in enhancing the\nreasoning capabilities of large language models (LLMs), but such training\ntypically demands substantial efforts in creating and annotating data. In this\nwork, we explore improving LLMs through RL with minimal data. Our approach\nalternates between the LLM proposing a task and then attempting to solve it. To\nminimize data dependency, we introduce two novel mechanisms grounded in\nself-awareness: (1) self-aware difficulty prediction, where the model learns to\nassess task difficulty relative to its own abilities and prioritize challenging\nyet solvable tasks, and (2) self-aware limit breaking, where the model\nrecognizes when a task is beyond its capability boundary and proactively\nrequests external data to break through that limit. Extensive experiments on\nnine benchmarks showing a 53.8% relative improvement with less than 1.2% extra\ndata demonstrate the efficacy of self-aware RL and underscore the promise of\nself-evolving agent training.",
        "url": "http://arxiv.org/abs/2510.02752v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02752v1",
        "arxiv_id": "2510.02752v1",
        "authors": [
            "Hangfan Zhang",
            "Siyuan Xu",
            "Zhimeng Guo",
            "Huaisheng Zhu",
            "Shicheng Liu",
            "Xinrun Wang",
            "Qiaosheng Zhang",
            "Yang Chen",
            "Peng Ye",
            "Lei Bai",
            "Shuyue Hu"
        ],
        "submitted": "2025-10-03 06:32:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "IndiCASA: A Dataset and Bias Evaluation Framework in LLMs Using Contrastive Embedding Similarity in the Indian Context",
        "abstract": "Large Language Models (LLMs) have gained significant traction across critical\ndomains owing to their impressive contextual understanding and generative\ncapabilities. However, their increasing deployment in high stakes applications\nnecessitates rigorous evaluation of embedded biases, particularly in culturally\ndiverse contexts like India where existing embedding-based bias assessment\nmethods often fall short in capturing nuanced stereotypes. We propose an\nevaluation framework based on a encoder trained using contrastive learning that\ncaptures fine-grained bias through embedding similarity. We also introduce a\nnovel dataset - IndiCASA (IndiBias-based Contextually Aligned Stereotypes and\nAnti-stereotypes) comprising 2,575 human-validated sentences spanning five\ndemographic axes: caste, gender, religion, disability, and socioeconomic\nstatus. Our evaluation of multiple open-weight LLMs reveals that all models\nexhibit some degree of stereotypical bias, with disability related biases being\nnotably persistent, and religion bias generally lower likely due to global\ndebiasing efforts demonstrating the need for fairer model development.",
        "url": "http://arxiv.org/abs/2510.02742v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02742v1",
        "arxiv_id": "2510.02742v1",
        "authors": [
            "Santhosh G S",
            "Akshay Govind S",
            "Gokul S Krishnan",
            "Balaraman Ravindran",
            "Sriraam Natarajan"
        ],
        "submitted": "2025-10-03 06:03:26",
        "source": "arxiv",
        "comment": "Accepted at 8th AAAI/ACM Conference on AI, Ethics, and Society (AIES)\n  2025"
    },
    {
        "title": "PGMEL: Policy Gradient-based Generative Adversarial Network for Multimodal Entity Linking",
        "abstract": "The task of entity linking, which involves associating mentions with their\nrespective entities in a knowledge graph, has received significant attention\ndue to its numerous potential applications. Recently, various multimodal entity\nlinking (MEL) techniques have been proposed, targeted to learn comprehensive\nembeddings by leveraging both text and vision modalities. The selection of\nhigh-quality negative samples can potentially play a crucial role in\nmetric/representation learning. However, to the best of our knowledge, this\npossibility remains unexplored in existing literature within the framework of\nMEL. To fill this gap, we address the multimodal entity linking problem in a\ngenerative adversarial setting where the generator is responsible for\ngenerating high-quality negative samples, and the discriminator is assigned the\nresponsibility for the metric learning tasks. Since the generator is involved\nin generating samples, which is a discrete process, we optimize it using policy\ngradient techniques and propose a policy gradient-based generative adversarial\nnetwork for multimodal entity linking (PGMEL). Experimental results based on\nWiki-MEL, Richpedia-MEL and WikiDiverse datasets demonstrate that PGMEL learns\nmeaningful representation by selecting challenging negative samples and\noutperforms state-of-the-art methods.",
        "url": "http://arxiv.org/abs/2510.02726v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02726v1",
        "arxiv_id": "2510.02726v1",
        "authors": [
            "KM Pooja",
            "Cheng Long",
            "Aixin Sun"
        ],
        "submitted": "2025-10-03 05:09:47",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Hyperparameter Loss Surfaces Are Simple Near their Optima",
        "abstract": "Hyperparameters greatly impact models' capabilities; however, modern models\nare too large for extensive search. Instead, researchers design recipes that\ntrain well across scales based on their understanding of the hyperparameters.\nDespite this importance, few tools exist for understanding the hyperparameter\nloss surface. We discover novel structure in it and propose a new theory\nyielding such tools. The loss surface is complex, but as you approach the\noptimum simple structure emerges. It becomes characterized by a few basic\nfeatures, like its effective dimension and the best possible loss. To uncover\nthis asymptotic regime, we develop a novel technique based on random search.\nWithin this regime, the best scores from random search take on a new\ndistribution we discover. Its parameters are exactly the features defining the\nloss surface in the asymptotic regime. From these features, we derive a new\nasymptotic law for random search that can explain and extrapolate its\nconvergence. These new tools enable new analyses, such as confidence intervals\nfor the best possible performance or determining the effective number of\nhyperparameters. We make these tools available at\nhttps://github.com/nicholaslourie/opda .",
        "url": "http://arxiv.org/abs/2510.02721v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02721v1",
        "arxiv_id": "2510.02721v1",
        "authors": [
            "Nicholas Lourie",
            "He He",
            "Kyunghyun Cho"
        ],
        "submitted": "2025-10-03 04:52:27",
        "source": "arxiv",
        "comment": "Accepted to COLM 2025. 23 pages, 8 figures"
    },
    {
        "title": "TravelBench : Exploring LLM Performance in Low-Resource Domains",
        "abstract": "Results on existing LLM benchmarks capture little information over the model\ncapabilities in low-resource tasks, making it difficult to develop effective\nsolutions in these domains. To address these challenges, we curated 14\ntravel-domain datasets spanning 7 common NLP tasks using anonymised data from\nreal-world scenarios, and analysed the performance across LLMs. We report on\nthe accuracy, scaling behaviour, and reasoning capabilities of LLMs in a\nvariety of tasks. Our results confirm that general benchmarking results are\ninsufficient for understanding model performance in low-resource tasks. Despite\nthe amount of training FLOPs, out-of-the-box LLMs hit performance bottlenecks\nin complex, domain-specific scenarios. Furthermore, reasoning provides a more\nsignificant boost for smaller LLMs by making the model a better judge on\ncertain tasks.",
        "url": "http://arxiv.org/abs/2510.02719v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02719v1",
        "arxiv_id": "2510.02719v1",
        "authors": [
            "Srinivas Billa",
            "Xiaonan Jing"
        ],
        "submitted": "2025-10-03 04:44:34",
        "source": "arxiv",
        "comment": "10 pages, 3 figures"
    },
    {
        "title": "Time-To-Inconsistency: A Survival Analysis of Large Language Model Robustness to Adversarial Attacks",
        "abstract": "Large Language Models (LLMs) have revolutionized conversational AI, yet their\nrobustness in extended multi-turn dialogues remains poorly understood. Existing\nevaluation frameworks focus on static benchmarks and single-turn assessments,\nfailing to capture the temporal dynamics of conversational degradation that\ncharacterize real-world interactions. In this work, we present the first\ncomprehensive survival analysis of conversational AI robustness, analyzing\n36,951 conversation turns across 9 state-of-the-art LLMs to model failure as a\ntime-to-event process. Our survival modeling framework-employing Cox\nproportional hazards, Accelerated Failure Time, and Random Survival Forest\napproaches-reveals extraordinary temporal dynamics. We find that abrupt,\nprompt-to-prompt(P2P) semantic drift is catastrophic, dramatically increasing\nthe hazard of conversational failure. In stark contrast, gradual, cumulative\ndrift is highly protective, vastly reducing the failure hazard and enabling\nsignificantly longer dialogues. AFT models with interactions demonstrate\nsuperior performance, achieving excellent discrimination and exceptional\ncalibration. These findings establish survival analysis as a powerful paradigm\nfor evaluating LLM robustness, offer concrete insights for designing resilient\nconversational agents, and challenge prevailing assumptions about the necessity\nof semantic consistency in conversational AI Systems.",
        "url": "http://arxiv.org/abs/2510.02712v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02712v1",
        "arxiv_id": "2510.02712v1",
        "authors": [
            "Yubo Li",
            "Ramayya Krishnan",
            "Rema Padman"
        ],
        "submitted": "2025-10-03 04:26:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Uncertainty as Feature Gaps: Epistemic Uncertainty Quantification of LLMs in Contextual Question-Answering",
        "abstract": "Uncertainty Quantification (UQ) research has primarily focused on closed-book\nfactual question answering (QA), while contextual QA remains unexplored,\ndespite its importance in real-world applications. In this work, we focus on UQ\nfor the contextual QA task and propose a theoretically grounded approach to\nquantify epistemic uncertainty. We begin by introducing a task-agnostic,\ntoken-level uncertainty measure defined as the cross-entropy between the\npredictive distribution of the given model and the unknown true distribution.\nBy decomposing this measure, we isolate the epistemic component and approximate\nthe true distribution by a perfectly prompted, idealized model. We then derive\nan upper bound for epistemic uncertainty and show that it can be interpreted as\nsemantic feature gaps in the given model's hidden representations relative to\nthe ideal model. We further apply this generic framework to the contextual QA\ntask and hypothesize that three features approximate this gap: context-reliance\n(using the provided context rather than parametric knowledge), context\ncomprehension (extracting relevant information from context), and honesty\n(avoiding intentional lies). Using a top-down interpretability approach, we\nextract these features by using only a small number of labeled samples and\nensemble them to form a robust uncertainty score. Experiments on multiple QA\nbenchmarks in both in-distribution and out-of-distribution settings show that\nour method substantially outperforms state-of-the-art unsupervised\n(sampling-free and sampling-based) and supervised UQ methods, achieving up to a\n13-point PRR improvement while incurring a negligible inference overhead.",
        "url": "http://arxiv.org/abs/2510.02671v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02671v1",
        "arxiv_id": "2510.02671v1",
        "authors": [
            "Yavuz Bakman",
            "Sungmin Kang",
            "Zhiqi Huang",
            "Duygu Nur Yaldiz",
            "Catarina G. Belém",
            "Chenyang Zhu",
            "Anoop Kumar",
            "Alfy Samuel",
            "Salman Avestimehr",
            "Daben Liu",
            "Sai Praneeth Karimireddy"
        ],
        "submitted": "2025-10-03 02:09:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AutoMaAS: Self-Evolving Multi-Agent Architecture Search for Large Language Models",
        "abstract": "Multi-agent systems powered by large language models have demonstrated\nremarkable capabilities across diverse domains, yet existing automated design\napproaches seek monolithic solutions that fail to adapt resource allocation\nbased on query complexity and domain requirements. This paper introduces\nAutoMaAS, a self-evolving multi-agent architecture search framework that\nleverages neural architecture search principles to automatically discover\noptimal agent configurations through dynamic operator lifecycle management and\nautomated machine learning techniques. Our approach incorporates four key\ninnovations: (1) automatic operator generation, fusion, and elimination based\non performance-cost analysis, (2) dynamic cost-aware optimization with\nreal-time parameter adjustment, (3) online feedback integration for continuous\narchitecture refinement, and (4) enhanced interpretability through decision\ntracing mechanisms. Extensive experiments across six benchmarks demonstrate\nthat AutoMaAS achieves 1.0-7.1\\% performance improvement while reducing\ninference costs by 3-5\\% compared to state-of-the-art methods. The framework\nshows superior transferability across datasets and LLM backbones, establishing\na new paradigm for automated multi-agent system design in the era of large\nlanguage models.",
        "url": "http://arxiv.org/abs/2510.02669v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02669v1",
        "arxiv_id": "2510.02669v1",
        "authors": [
            "Bo Ma",
            "Hang Li",
            "ZeHua Hu",
            "XiaoFan Gui",
            "LuYao Liu",
            "Simon Liu"
        ],
        "submitted": "2025-10-03 01:57:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AgenticRAG: Tool-Augmented Foundation Models for Zero-Shot Explainable Recommender Systems",
        "abstract": "Foundation models have revolutionized artificial intelligence, yet their\napplication in recommender systems remains limited by reasoning opacity and\nknowledge constraints. This paper introduces AgenticRAG, a novel framework that\ncombines tool-augmented foundation models with retrieval-augmented generation\nfor zero-shot explainable recommendations. Our approach integrates external\ntool invocation, knowledge retrieval, and chain-of-thought reasoning to create\nautonomous recommendation agents capable of transparent decision-making without\ntask-specific training. Experimental results on three real-world datasets\ndemonstrate that AgenticRAG achieves consistent improvements over\nstate-of-the-art baselines, with NDCG@10 improvements of 0.4\\% on Amazon\nElectronics, 0.8\\% on MovieLens-1M, and 1.6\\% on Yelp datasets. The framework\nexhibits superior explainability while maintaining computational efficiency\ncomparable to traditional methods.",
        "url": "http://arxiv.org/abs/2510.02668v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02668v1",
        "arxiv_id": "2510.02668v1",
        "authors": [
            "Bo Ma",
            "Hang Li",
            "ZeHua Hu",
            "XiaoFan Gui",
            "LuYao Liu",
            "Simon Liu"
        ],
        "submitted": "2025-10-03 01:52:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Self-Improvement in Multimodal Large Language Models: A Survey",
        "abstract": "Recent advancements in self-improvement for Large Language Models (LLMs) have\nefficiently enhanced model capabilities without significantly increasing costs,\nparticularly in terms of human effort. While this area is still relatively\nyoung, its extension to the multimodal domain holds immense potential for\nleveraging diverse data sources and developing more general self-improving\nmodels. This survey is the first to provide a comprehensive overview of\nself-improvement in Multimodal LLMs (MLLMs). We provide a structured overview\nof the current literature and discuss methods from three perspectives: 1) data\ncollection, 2) data organization, and 3) model optimization, to facilitate the\nfurther development of self-improvement in MLLMs. We also include commonly used\nevaluations and downstream applications. Finally, we conclude by outlining open\nchallenges and future research directions.",
        "url": "http://arxiv.org/abs/2510.02665v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02665v1",
        "arxiv_id": "2510.02665v1",
        "authors": [
            "Shijian Deng",
            "Kai Wang",
            "Tianyu Yang",
            "Harsh Singh",
            "Yapeng Tian"
        ],
        "submitted": "2025-10-03 01:48:26",
        "source": "arxiv",
        "comment": "EMNLP 2025"
    },
    {
        "title": "Less LLM, More Documents: Searching for Improved RAG",
        "abstract": "Retrieval-Augmented Generation (RAG) couples document retrieval with large\nlanguage models (LLMs). While scaling generators improves accuracy, it also\nraises cost and limits deployability. We explore an orthogonal axis: enlarging\nthe retriever's corpus to reduce reliance on large LLMs. Experimental results\nshow that corpus scaling consistently strengthens RAG and can often serve as a\nsubstitute for increasing model size, though with diminishing returns at larger\nscales. Small- and mid-sized generators paired with larger corpora often rival\nmuch larger models with smaller corpora; mid-sized models tend to gain the\nmost, while tiny and large models benefit less. Our analysis shows that\nimprovements arise primarily from increased coverage of answer-bearing\npassages, while utilization efficiency remains largely unchanged. These\nfindings establish a principled corpus-generator trade-off: investing in larger\ncorpora offers an effective path to stronger RAG, often comparable to enlarging\nthe LLM itself.",
        "url": "http://arxiv.org/abs/2510.02657v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02657v1",
        "arxiv_id": "2510.02657v1",
        "authors": [
            "Jingjie Ning",
            "Yibo Kong",
            "Yunfan Long",
            "Jamie Callan"
        ],
        "submitted": "2025-10-03 01:26:13",
        "source": "arxiv",
        "comment": "16 pages. Submitted to ECIR 2026"
    },
    {
        "title": "A Simple but Effective Elaborative Query Reformulation Approach for Natural Language Recommendation",
        "abstract": "Natural Language (NL) recommender systems aim to retrieve relevant items from\nfree-form user queries and item descriptions. Existing systems often rely on\ndense retrieval (DR), which struggles to interpret challenging queries that\nexpress broad (e.g., \"cities for youth friendly activities\") or indirect (e.g.,\n\"cities for a high school graduation trip\") user intents. While query\nreformulation (QR) has been widely adopted to improve such systems, existing QR\nmethods tend to focus only on expanding the range of query subtopics (breadth)\nor elaborating on the potential meaning of a query (depth), but not both. In\nthis paper, we propose EQR (Elaborative Subtopic Query Reformulation), a large\nlanguage model-based QR method that combines both breadth and depth by\ngenerating potential query subtopics with information-rich elaborations. We\nalso introduce three new natural language recommendation benchmarks in travel,\nhotel, and restaurant domains to establish evaluation of NL recommendation with\nchallenging queries. Experiments show EQR substantially outperforms\nstate-of-the-art QR methods in various evaluation metrics, highlighting that a\nsimple yet effective QR approach can significantly improve NL recommender\nsystems for queries with broad and indirect user intents.",
        "url": "http://arxiv.org/abs/2510.02656v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02656v1",
        "arxiv_id": "2510.02656v1",
        "authors": [
            "Qianfeng Wen",
            "Yifan Liu",
            "Justin Cui",
            "Joshua Zhang",
            "Anton Korikov",
            "George-Kirollos Saad",
            "Scott Sanner"
        ],
        "submitted": "2025-10-03 01:21:55",
        "source": "arxiv",
        "comment": "11 pages, 5 figures"
    },
    {
        "title": "Geolog-IA: Conversational System for Academic Theses",
        "abstract": "This study presents the development of Geolog-IA, a novel conversational\nsystem based on artificial intelligence that responds naturally to questions\nabout geology theses from the Central University of Ecuador. Our proposal uses\nthe Llama 3.1 and Gemini 2.5 language models, which are complemented by a\nRetrieval Augmented Generation (RAG) architecture and an SQLite database. This\nstrategy allows us to overcome problems such as hallucinations and outdated\nknowledge. The evaluation of Geolog-IA's performance with the BLEU metric\nreaches an average of 0.87, indicating high consistency and accuracy in the\nresponses generated. The system offers an intuitive, web-based interface that\nfacilitates interaction and information retrieval for directors, teachers,\nstudents, and administrative staff at the institution. This tool can be a key\nsupport in education, training, and research and establishes a basis for future\napplications in other disciplines.",
        "url": "http://arxiv.org/abs/2510.02653v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02653v1",
        "arxiv_id": "2510.02653v1",
        "authors": [
            "Micaela Fuel Pozo",
            "Andrea Guatumillo Saltos",
            "Yeseña Tipan Llumiquinga",
            "Kelly Lascano Aguirre",
            "Marilyn Castillo Jara",
            "Christian Mejia-Escobar"
        ],
        "submitted": "2025-10-03 01:11:47",
        "source": "arxiv",
        "comment": "17 pages, in Spanish language"
    },
    {
        "title": "SoT: Structured-of-Thought Prompting Guides Multilingual Reasoning in Large Language Models",
        "abstract": "Recent developments have enabled Large Language Models (LLMs) to engage in\ncomplex reasoning tasks through deep thinking. However, the capacity of\nreasoning has not been successfully transferred to non-high-resource languages\ndue to resource constraints, which struggles with multilingual reasoning tasks.\nTo this end, we propose Structured-of-Thought (SoT), a training-free method\nthat improves the performance on multilingual reasoning through a multi-step\ntransformation: Language Thinking Transformation and Structured Knowledge\nTransformation. The SoT method converts language-specific semantic information\ninto language-agnostic structured representations, enabling the models to\nunderstand the query in different languages more sophisticated. Besides, SoT\neffectively guides LLMs toward more concentrated reasoning to maintain\nconsistent underlying reasoning pathways when handling cross-lingual variations\nin expression. Experimental results demonstrate that SoT outperforms several\nstrong baselines on multiple multilingual reasoning benchmarks when adapting to\nvarious backbones of LLMs. It can also be integrated with other training-free\nstrategies for further improvements. Our code is available at\nhttps://github.com/Cherry-qwq/SoT.",
        "url": "http://arxiv.org/abs/2510.02648v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02648v1",
        "arxiv_id": "2510.02648v1",
        "authors": [
            "Rui Qi",
            "Zhibo Man",
            "Yufeng Chen",
            "Fengran Mo",
            "Jinan Xu",
            "Kaiyu Huang"
        ],
        "submitted": "2025-10-03 01:02:14",
        "source": "arxiv",
        "comment": "EMNLP 2025 (findings)"
    },
    {
        "title": "Mind the Gap: Linguistic Divergence and Adaptation Strategies in Human-LLM Assistant vs. Human-Human Interactions",
        "abstract": "As Large Language Models (LLMs) are increasingly deployed in customer-facing\napplications, a critical yet underexplored question is how users communicate\ndifferently with LLM chatbots compared to human agent. In this study, we\npresent empirical evidence that users adopt distinct communication styles when\nusers interact with chatbots versus human agents. Our analysis reveals\nsignificant differences in grammatical fluency, politeness, and lexical\ndiversity in user language between the two settings. These findings suggest\nthat models trained exclusively on human-human interaction data may not\nadequately accommodate the communication style shift that occurs once an LLM\nchatbot is deployed. To enhance LLM robustness to post-launch communication\nstyle changes, we experimented with two strategies: (1) data augmentation\nduring the post-training phase and (2) inference-time user message\nreformulation. Our results indicate that models trained on stylistically\ndiverse datasets significantly outperform those trained exclusively on original\nor stylistically uniform datasets, while inference-time reformulation proved\nless effective. These insights help us to better adapt our models for improved\nLLM-user interaction experiences.",
        "url": "http://arxiv.org/abs/2510.02645v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02645v1",
        "arxiv_id": "2510.02645v1",
        "authors": [
            "Fulei Zhang",
            "Zhou Yu"
        ],
        "submitted": "2025-10-03 00:45:37",
        "source": "arxiv",
        "comment": "Accepted to The Second Workshop on Generative AI for E-commerce\n  (GenAIECommerce '25), held September 22, 2025, in Prague, Czech Republic"
    },
    {
        "title": "HyperAdaLoRA: Accelerating LoRA Rank Allocation During Training via Hypernetworks without Sacrificing Performance",
        "abstract": "Parameter-Efficient Fine-Tuning (PEFT), especially Low-Rank Adaptation\n(LoRA), has emerged as a promising approach to fine-tuning large language\nmodels(LLMs) while reducing computational and memory overhead. However, LoRA\nassumes a uniform rank \\textit{r} for each incremental matrix, not accounting\nfor the varying significance of weight matrices across different modules and\nlayers. AdaLoRA leverages Singular Value Decomposition (SVD) to parameterize\nupdates and employs pruning of singular values to introduce dynamic rank\nallocation, thereby enhancing adaptability. However, during the training\nprocess, it often encounters issues of slow convergence speed and high\ncomputational overhead. To address this issue, we propose HyperAdaLoRA, a novel\nframework that accelerates the convergence of AdaLoRA by leveraging a\nhypernetwork. Instead of directly optimizing the components of Singular Value\nDecomposition $(P, \\Lambda, Q)$, HyperAdaLoRA employs a hypernetwork based on\nattention mechanisms to dynamically generate these parameters. By pruning the\noutputs of the hypernetwork that generates the singular values, dynamic rank\nallocation is achieved. Comprehensive experiments on various datasets and\nmodels demonstrate that our method achieves faster convergence without\nsacrificing performance. Additionally, further extension experiments on other\nLoRA-based approaches validate the broad applicability of our method.",
        "url": "http://arxiv.org/abs/2510.02630v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02630v1",
        "arxiv_id": "2510.02630v1",
        "authors": [
            "Hao Zhang",
            "Zhenjia Li",
            "Runfeng Bao",
            "Yifan Gao",
            "Xi Xiao",
            "Bo Huang",
            "Yuhang Wu",
            "Tianyang Wang",
            "Hao Xu"
        ],
        "submitted": "2025-10-03 00:15:59",
        "source": "arxiv",
        "comment": "13 pages"
    },
    {
        "title": "Evaluation Framework for Highlight Explanations of Context Utilisation in Language Models",
        "abstract": "Context utilisation, the ability of Language Models (LMs) to incorporate\nrelevant information from the provided context when generating responses,\nremains largely opaque to users, who cannot determine whether models draw from\nparametric memory or provided context, nor identify which specific context\npieces inform the response. Highlight explanations (HEs) offer a natural\nsolution as they can point the exact context pieces and tokens that influenced\nmodel outputs. However, no existing work evaluates their effectiveness in\naccurately explaining context utilisation. We address this gap by introducing\nthe first gold standard HE evaluation framework for context attribution, using\ncontrolled test cases with known ground-truth context usage, which avoids the\nlimitations of existing indirect proxy evaluations. To demonstrate the\nframework's broad applicability, we evaluate four HE methods -- three\nestablished techniques and MechLight, a mechanistic interpretability approach\nwe adapt for this task -- across four context scenarios, four datasets, and\nfive LMs. Overall, we find that MechLight performs best across all context\nscenarios. However, all methods struggle with longer contexts and exhibit\npositional biases, pointing to fundamental challenges in explanation accuracy\nthat require new approaches to deliver reliable context utilisation\nexplanations at scale.",
        "url": "http://arxiv.org/abs/2510.02629v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02629v1",
        "arxiv_id": "2510.02629v1",
        "authors": [
            "Jingyi Sun",
            "Pepa Atanasova",
            "Sagnik Ray Choudhury",
            "Sekh Mainul Islam",
            "Isabelle Augenstein"
        ],
        "submitted": "2025-10-03 00:15:36",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "On the Role of Temperature Sampling in Test-Time Scaling",
        "abstract": "Large language models (LLMs) can improve reasoning at inference time through\ntest-time scaling (TTS), where multiple reasoning traces are generated and the\nbest one is selected. Prior work shows that increasing the number of samples K\nsteadily improves accuracy. In this paper, we demonstrate that this trend does\nnot hold indefinitely: at large K, further scaling yields no gains, and certain\nhard questions remain unsolved regardless of the number of traces.\nInterestingly, we find that different sampling temperatures solve different\nsubsets of problems, implying that single-temperature scaling explores only\npart of a model's potential. We therefore propose scaling along the temperature\ndimension, which enlarges the reasoning boundary of LLMs. Averaged over Qwen3\n(0.6B, 1.7B, 4B, 8B) and five representative reasoning benchmarks (AIME\n2024/2025, MATH500, LiveCodeBench, Hi-ToM), temperature scaling yields an\nadditional 7.3 points over single-temperature TTS. Temperature scaling also\nenables base models to reach performance comparable to reinforcement learning\n(RL)-trained counterparts, without additional post-training. We further provide\na comprehensive analysis of this phenomenon and design a multi-temperature\nvoting method that reduces the overhead of temperature scaling. Overall, our\nfindings suggest that TTS is more powerful than previously thought, and that\ntemperature scaling offers a simple and effective way to unlock the latent\npotential of base models.",
        "url": "http://arxiv.org/abs/2510.02611v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02611v1",
        "arxiv_id": "2510.02611v1",
        "authors": [
            "Yuheng Wu",
            "Azalia Mirhoseini",
            "Thierry Tambe"
        ],
        "submitted": "2025-10-02 23:09:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "How Confident are Video Models? Empowering Video Models to Express their Uncertainty",
        "abstract": "Generative video models demonstrate impressive text-to-video capabilities,\nspurring widespread adoption in many real-world applications. However, like\nlarge language models (LLMs), video generation models tend to hallucinate,\nproducing plausible videos even when they are factually wrong. Although\nuncertainty quantification (UQ) of LLMs has been extensively studied in prior\nwork, no UQ method for video models exists, raising critical safety concerns.\nTo our knowledge, this paper represents the first work towards quantifying the\nuncertainty of video models. We present a framework for uncertainty\nquantification of generative video models, consisting of: (i) a metric for\nevaluating the calibration of video models based on robust rank correlation\nestimation with no stringent modeling assumptions; (ii) a black-box UQ method\nfor video models (termed S-QUBED), which leverages latent modeling to\nrigorously decompose predictive uncertainty into its aleatoric and epistemic\ncomponents; and (iii) a UQ dataset to facilitate benchmarking calibration in\nvideo models. By conditioning the generation task in the latent space, we\ndisentangle uncertainty arising due to vague task specifications from that\narising from lack of knowledge. Through extensive experiments on benchmark\nvideo datasets, we demonstrate that S-QUBED computes calibrated total\nuncertainty estimates that are negatively correlated with the task accuracy and\neffectively computes the aleatoric and epistemic constituents.",
        "url": "http://arxiv.org/abs/2510.02571v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02571v1",
        "arxiv_id": "2510.02571v1",
        "authors": [
            "Zhiting Mei",
            "Ola Shorinwa",
            "Anirudha Majumdar"
        ],
        "submitted": "2025-10-02 21:20:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Transcribe, Translate, or Transliterate: An Investigation of Intermediate Representations in Spoken Language Models",
        "abstract": "Spoken language models (SLMs) that integrate speech with large language\nmodels (LMs) rely on modality adapters (MAs) to map the output of speech\nencoders to a representation that is understandable to the decoder LM. Yet we\nknow very little about how these crucial MAs transform representations. Here we\nexamine the MA output representation in three SLMs (SALMONN, Qwen2-Audio and\nPhi-4-Multimodal-Instruct). By finding the nearest decoder LM token to an MA\nrepresentation, we uncover two strategies for MA representations. For models\nusing a Whisper encoder, MAs appear to represent the meaning of the input using\nan English-based interlingua, allowing them to handle languages unseen in\ninstruction tuning. For models that don't, like Phi-4-Multimodal-Instruct, MAs\ninstead represent the phonetics of the input, but expressed with English words.\nWe hypothesise that which arises depends on whether the speech encoder is\ntrained only for speech recognition or also for translation.",
        "url": "http://arxiv.org/abs/2510.02569v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02569v1",
        "arxiv_id": "2510.02569v1",
        "authors": [
            "Tolúl\\d{o}pé Ògúnrèmí",
            "Christopher D. Manning",
            "Dan Jurafsky",
            "Karen Livescu"
        ],
        "submitted": "2025-10-02 21:19:40",
        "source": "arxiv",
        "comment": "ASRU 2025"
    },
    {
        "title": "Knowledge-Graph Based RAG System Evaluation Framework",
        "abstract": "Large language models (LLMs) has become a significant research focus and is\nutilized in various fields, such as text generation and dialog systems. One of\nthe most essential applications of LLM is Retrieval Augmented Generation (RAG),\nwhich greatly enhances generated content's reliability and relevance. However,\nevaluating RAG systems remains a challenging task. Traditional evaluation\nmetrics struggle to effectively capture the key features of modern\nLLM-generated content that often exhibits high fluency and naturalness.\nInspired by the RAGAS tool, a well-known RAG evaluation framework, we extended\nthis framework into a KG-based evaluation paradigm, enabling multi-hop\nreasoning and semantic community clustering to derive more comprehensive\nscoring metrics. By incorporating these comprehensive evaluation criteria, we\ngain a deeper understanding of RAG systems and a more nuanced perspective on\ntheir performance. To validate the effectiveness of our approach, we compare\nits performance with RAGAS scores and construct a human-annotated subset to\nassess the correlation between human judgments and automated metrics. In\naddition, we conduct targeted experiments to demonstrate that our KG-based\nevaluation method is more sensitive to subtle semantic differences in generated\noutputs. Finally, we discuss the key challenges in evaluating RAG systems and\nhighlight potential directions for future research.",
        "url": "http://arxiv.org/abs/2510.02549v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02549v1",
        "arxiv_id": "2510.02549v1",
        "authors": [
            "Sicheng Dong",
            "Vahid Zolfaghari",
            "Nenad Petrovic",
            "Alois Knoll"
        ],
        "submitted": "2025-10-02 20:36:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Hierarchical Semantic Retrieval with Cobweb",
        "abstract": "Neural document retrieval often treats a corpus as a flat cloud of vectors\nscored at a single granularity, leaving corpus structure underused and\nexplanations opaque. We use Cobweb--a hierarchy-aware framework--to organize\nsentence embeddings into a prototype tree and rank documents via coarse-to-fine\ntraversal. Internal nodes act as concept prototypes, providing multi-granular\nrelevance signals and a transparent rationale through retrieval paths. We\ninstantiate two inference approaches: a generalized best-first search and a\nlightweight path-sum ranker. We evaluate our approaches on MS MARCO and QQP\nwith encoder (e.g., BERT/T5) and decoder (GPT-2) representations. Our results\nshow that our retrieval approaches match the dot product search on strong\nencoder embeddings while remaining robust when kNN degrades: with GPT-2\nvectors, dot product performance collapses whereas our approaches still\nretrieve relevant results. Overall, our experiments suggest that Cobweb\nprovides competitive effectiveness, improved robustness to embedding quality,\nscalability, and interpretable retrieval via hierarchical prototypes.",
        "url": "http://arxiv.org/abs/2510.02539v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02539v1",
        "arxiv_id": "2510.02539v1",
        "authors": [
            "Anant Gupta",
            "Karthik Singaravadivelan",
            "Zekun Wang"
        ],
        "submitted": "2025-10-02 20:14:52",
        "source": "arxiv",
        "comment": "20 pages, 7 tables, 4 figures"
    },
    {
        "title": "Unraveling Syntax: How Language Models Learn Context-Free Grammars",
        "abstract": "We introduce a new framework for understanding how language models acquire\nsyntax. While large models achieve impressive results, little is known about\ntheir learning dynamics. Our approach starts with the observation that most\ndomains of interest, such as natural language syntax, coding languages,\narithmetic problems, are captured by probabilistic context-free grammars\n(PCFGs). We study the learning dynamics of small models trained on synthetic\nlanguages generated from PCFGs, enabling precise control over grammar\ncomplexity, recursion depth, and subgrammar structure. We prove several\ngeneral, recursive formulae for the training loss and Kullback-Leibler\ndivergence over the subgrammar structure of a PCFG. Empirically, we find that\nunlike children, who first master simple substructures before progressing to\nmore complex constructions, transformers reduce loss across all subgrammars in\nparallel. We further show that subgrammar pretraining can improve the final\nloss for smaller models, and that pretrained models develop internal\nrepresentations more aligned with the grammar's substructure. Finally, we\ndemonstrate that models struggle with deeper recursive structures (a limitation\neven of large language models), revealing fundamental challenges in how neural\nnetworks represent hierarchical syntax. Overall, our work initiates the study\nof the learning dynamics of transformers on PCFGs as a versatile testbed for\nprobing learning in language models, opening a research direction with many\nopen questions.",
        "url": "http://arxiv.org/abs/2510.02524v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02524v1",
        "arxiv_id": "2510.02524v1",
        "authors": [
            "Laura Ying Schulz",
            "Daniel Mitropolsky",
            "Tomaso Poggio"
        ],
        "submitted": "2025-10-02 19:52:19",
        "source": "arxiv",
        "comment": "Equal contribution by LYS and DM"
    },
    {
        "title": "Revisiting Query Variants: The Advantage of Retrieval Over Generation of Query Variants for Effective QPP",
        "abstract": "Leveraging query variants (QVs), i.e., queries with potentially similar\ninformation needs to the target query, has been shown to improve the\neffectiveness of query performance prediction (QPP) approaches. Existing\nQV-based QPP methods generate QVs facilitated by either query expansion or\nnon-contextual embeddings, which may introduce topical drifts and\nhallucinations. In this paper, we propose a method that retrieves QVs from a\ntraining set (e.g., MS MARCO) for a given target query of QPP. To achieve a\nhigh recall in retrieving queries with the most similar information needs as\nthe target query from a training set, we extend the directly retrieved QVs\n(1-hop QVs) by a second retrieval using their denoted relevant documents (which\nyields 2-hop QVs). Our experiments, conducted on TREC DL'19 and DL'20, show\nthat the QPP methods with QVs retrieved by our method outperform the\nbest-performing existing generated-QV-based QPP approaches by as much as around\n20\\%, on neural ranking models like MonoT5.",
        "url": "http://arxiv.org/abs/2510.02512v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02512v1",
        "arxiv_id": "2510.02512v1",
        "authors": [
            "Fangzheng Tian",
            "Debasis Ganguly",
            "Craig Macdonald"
        ],
        "submitted": "2025-10-02 19:36:58",
        "source": "arxiv",
        "comment": "11 pages, 4 figures"
    },
    {
        "title": "Beyond Imitation: Recovering Dense Rewards from Demonstrations",
        "abstract": "Conventionally, supervised fine-tuning (SFT) is treated as a simple imitation\nlearning process that only trains a policy to imitate expert behavior on\ndemonstration datasets. In this work, we challenge this view by establishing a\nfundamental equivalence between SFT and Inverse Reinforcement Learning. We\nprove that the SFT objective is a special case of Inverse Q-Learning, which\nimplies that the SFT process does not just learn a policy, but also an\nimplicit, dense, token-level reward model that explains the expert\ndemonstrations. We then show how to recover this dense reward signal directly\nfrom the SFT model by formulating a baseline-relative reward function. The\navailability of such a dense reward model offers numerous benefits, providing\ngranular credit assignment for each token generated. We demonstrate one key\napplication by using these recovered rewards to further improve the policy with\nreinforcement learning. Our method, Dense-Path REINFORCE, consistently\noutperforms the original SFT models on instruction-following benchmarks. This\nwork reframes SFT not merely as policy imitation but as a powerful reward\nlearning mechanism, opening new possibilities for leveraging expert\ndemonstrations.",
        "url": "http://arxiv.org/abs/2510.02493v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02493v1",
        "arxiv_id": "2510.02493v1",
        "authors": [
            "Jiangnan Li",
            "Thuy-Trang Vu",
            "Ehsan Abbasnejad",
            "Gholamreza Haffari"
        ],
        "submitted": "2025-10-02 18:58:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Litespark Technical Report: High-Throughput, Energy-Efficient LLM Training Framework",
        "abstract": "Training Large Language Models (LLMs) is plagued by long training times and\nmassive energy consumption, with modern models requiring months of computation\nand gigawatt-hours of electricity. In light of these challenges,we introduce\nLitespark, a novel pre-training framework that addresses these inefficiencies\nthrough targeted optimizations to transformer attention and MLP layers. Our\napproach combines architectural improvements with algorithmic enhancements to\nmaximize Model FLOPs Utilization (MFU) while maintaining compatibility with\nstandard transformer implementations. Comprehensive benchmarking on 3B and 30B\nparameter Llama models using the SlimPajama-627B dataset demonstrates\nsubstantial performance gains: 2x-6x training throughput improvement and\n$55\\%-83$% energy consumption reduction across multi-node H200 GPU clusters.\nThese optimizations are model- and hardware-agnostic, enabling broad\napplicability across transformer architectures and extending to post-training\nphases including supervised fine-tuning and direct preference optimization.",
        "url": "http://arxiv.org/abs/2510.02483v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02483v1",
        "arxiv_id": "2510.02483v1",
        "authors": [
            "Nii Osae Osae Dade",
            "Moinul Hossain Rahat"
        ],
        "submitted": "2025-10-02 18:42:07",
        "source": "arxiv",
        "comment": "14 pages"
    },
    {
        "title": "SIMSplat: Predictive Driving Scene Editing with Language-aligned 4D Gaussian Splatting",
        "abstract": "Driving scene manipulation with sensor data is emerging as a promising\nalternative to traditional virtual driving simulators. However, existing\nframeworks struggle to generate realistic scenarios efficiently due to limited\nediting capabilities. To address these challenges, we present SIMSplat, a\npredictive driving scene editor with language-aligned Gaussian splatting. As a\nlanguage-controlled editor, SIMSplat enables intuitive manipulation using\nnatural language prompts. By aligning language with Gaussian-reconstructed\nscenes, it further supports direct querying of road objects, allowing precise\nand flexible editing. Our method provides detailed object-level editing,\nincluding adding new objects and modifying the trajectories of both vehicles\nand pedestrians, while also incorporating predictive path refinement through\nmulti-agent motion prediction to generate realistic interactions among all\nagents in the scene. Experiments on the Waymo dataset demonstrate SIMSplat's\nextensive editing capabilities and adaptability across a wide range of\nscenarios. Project page: https://sungyeonparkk.github.io/simsplat/",
        "url": "http://arxiv.org/abs/2510.02469v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02469v1",
        "arxiv_id": "2510.02469v1",
        "authors": [
            "Sung-Yeon Park",
            "Adam Lee",
            "Juanwu Lu",
            "Can Cui",
            "Luyang Jiang",
            "Rohit Gupta",
            "Kyungtae Han",
            "Ahmadreza Moradipari",
            "Ziran Wang"
        ],
        "submitted": "2025-10-02 18:22:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CLARITY: Clinical Assistant for Routing, Inference, and Triage",
        "abstract": "We present CLARITY (Clinical Assistant for Routing, Inference, and Triage),\nan AI-driven platform designed to facilitate patient-to-specialist routing,\nclinical consultations, and severity assessment of patients' conditions. Its\nhybrid architecture combines a Finite State Machine (FSM) for structured\ndialogue flows with collaborative agents that employ Large Language Model (LLM)\nto analyze symptoms and prioritize referrals to appropriate specialists. Built\non a modular microservices framework, CLARITY ensures safe, efficient, and\nrobust performance, flexible and readily scalable to meet the demands of\nexisting workflows and IT solutions in healthcare.\n  We report integration of our clinical assistant into a large-scale\nnation-wide inter-hospital IT platform, with over 55,000 content-rich user\ndialogues completed within the two months of deployment, 2,500 of which were\nexpert-annotated for a consequent validation. The validation results show that\nCLARITY surpasses human-level performance in terms of the first-attempt routing\nprecision, naturally requiring up to 3 times shorter duration of the\nconsultation than with a human.",
        "url": "http://arxiv.org/abs/2510.02463v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02463v1",
        "arxiv_id": "2510.02463v1",
        "authors": [
            "Vladimir Shaposhnikov",
            "Aleksandr Nesterov",
            "Ilia Kopanichuk",
            "Ivan Bakulin",
            "Egor Zhelvakov",
            "Ruslan Abramov",
            "Ekaterina Tsapieva",
            "Dmitry V. Dylov",
            "Ivan Oseledets"
        ],
        "submitted": "2025-10-02 18:18:41",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 (Industrial Track)"
    },
    {
        "title": "How to Train Your Advisor: Steering Black-Box LLMs with Advisor Models",
        "abstract": "Foundation models are increasingly deployed as black-box services, where\nmodel weights cannot be modified and customization is limited to prompting.\nWhile static prompt optimization has shown promise, it produces a single fixed\nprompt that fails to adapt to different inputs, users, or environments. We\nintroduce Advisor Models, lightweight parametric policies trained with\nreinforcement learning to reactively issue natural language steering\ninstructions in-context to black-box models. The advisor is a second small\nmodel that sits between the input and the model, shaping behavior on a\nper-instance basis using reward signals from the environment. Across multiple\ndomains involving reasoning and personalization, we show that Advisor Models\noutperform static prompt optimizers, discovering environment dynamics and\nimproving downstream task performance. We also demonstrate the generalizability\nof advisors by transferring them across black-box models, as well as the\nframework's ability to achieve specialization while retaining robustness to\nout-of-distribution inputs. Viewed more broadly, Advisor Models provide a\nlearnable interface to black-box systems where the advisor acts as a\nparametric, environment-specific memory. We argue that dynamic optimization of\nblack-box models via Advisor Models is a promising direction for enabling\npersonalization and environment-adaptable AI with frontier-level capabilities.",
        "url": "http://arxiv.org/abs/2510.02453v1",
        "pdf_url": "http://arxiv.org/pdf/2510.02453v1",
        "arxiv_id": "2510.02453v1",
        "authors": [
            "Parth Asawa",
            "Alan Zhu",
            "Matei Zaharia",
            "Alexandros G. Dimakis",
            "Joseph E. Gonzalez"
        ],
        "submitted": "2025-10-02 18:02:39",
        "source": "arxiv",
        "comment": null
    }
]