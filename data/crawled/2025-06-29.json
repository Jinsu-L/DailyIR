[
    {
        "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements",
        "abstract": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.",
        "url": "http://arxiv.org/abs/2506.22419v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22419v1",
        "arxiv_id": "2506.22419v1",
        "authors": [
            "Bingchen Zhao",
            "Despoina Magka",
            "Minqi Jiang",
            "Xian Li",
            "Roberta Raileanu",
            "Tatiana Shavrina",
            "Jean-Christophe Gagnon-Audet",
            "Kelvin Niu",
            "Shagun Sodhani",
            "Michael Shvartsman",
            "Andrei Lupu",
            "Alisia Lupidi",
            "Edan Toledo",
            "Karen Hambardzumyan",
            "Martin Josifoski",
            "Thomas Foster",
            "Lucia Cipolina-Kun",
            "Abhishek Charnalia",
            "Derek Dunfield",
            "Alexander H. Miller",
            "Oisin Mac Aodha",
            "Jakob Foerster",
            "Yoram Bachrach"
        ],
        "submitted": "2025-06-27 17:44:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Sequential Diagnosis with Language Models",
        "abstract": "Artificial intelligence holds great promise for expanding access to expert\nmedical knowledge and reasoning. However, most evaluations of language models\nrely on static vignettes and multiple-choice questions that fail to reflect the\ncomplexity and nuance of evidence-based medicine in real-world settings. In\nclinical practice, physicians iteratively formulate and revise diagnostic\nhypotheses, adapting each subsequent question and test to what they've just\nlearned, and weigh the evolving evidence before committing to a final\ndiagnosis. To emulate this iterative process, we introduce the Sequential\nDiagnosis Benchmark, which transforms 304 diagnostically challenging New\nEngland Journal of Medicine clinicopathological conference (NEJM-CPC) cases\ninto stepwise diagnostic encounters. A physician or AI begins with a short case\nabstract and must iteratively request additional details from a gatekeeper\nmodel that reveals findings only when explicitly queried. Performance is\nassessed not just by diagnostic accuracy but also by the cost of physician\nvisits and tests performed. We also present the MAI Diagnostic Orchestrator\n(MAI-DxO), a model-agnostic orchestrator that simulates a panel of physicians,\nproposes likely differential diagnoses and strategically selects high-value,\ncost-effective tests. When paired with OpenAI's o3 model, MAI-DxO achieves 80%\ndiagnostic accuracy--four times higher than the 20% average of generalist\nphysicians. MAI-DxO also reduces diagnostic costs by 20% compared to\nphysicians, and 70% compared to off-the-shelf o3. When configured for maximum\naccuracy, MAI-DxO achieves 85.5% accuracy. These performance gains with MAI-DxO\ngeneralize across models from the OpenAI, Gemini, Claude, Grok, DeepSeek, and\nLlama families. We highlight how AI systems, when guided to think iteratively\nand act judiciously, can advance diagnostic precision and cost-effectiveness in\nclinical care.",
        "url": "http://arxiv.org/abs/2506.22405v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22405v1",
        "arxiv_id": "2506.22405v1",
        "authors": [
            "Harsha Nori",
            "Mayank Daswani",
            "Christopher Kelly",
            "Scott Lundberg",
            "Marco Tulio Ribeiro",
            "Marc Wilson",
            "Xiaoxuan Liu",
            "Viknesh Sounderajah",
            "Jonathan Carlson",
            "Matthew P Lungren",
            "Bay Gross",
            "Peter Hames",
            "Mustafa Suleyman",
            "Dominic King",
            "Eric Horvitz"
        ],
        "submitted": "2025-06-27 17:27:26",
        "source": "arxiv",
        "comment": "23 pages, 10 figures"
    },
    {
        "title": "HyperCLOVA X THINK Technical Report",
        "abstract": "We introduce HyperCLOVA X THINK, the first reasoning-focused large language\nmodel in the HyperCLOVA X family, pre-trained on roughly $6$ trillion\nhigh-quality Korean, and English tokens, augmented with targeted synthetic\nKorean data. It was implemented as a compute-memory-balanced Peri-LN\nTransformer scaled with $\\mu$P, pre-trained through a three-stage curriculum\nthat expands the context window to $128$K tokens, and post-trained via\nsupervised fine-tuning with Reinforcement Learning from Verifiable Rewards\nsupports both detailed rationale and concise-answer modes. It delivers\ncompetitive performance against similarly sized models on Korea-focused\nbenchmarks such as KMMLU, CSAT, KoBALT-700, HAERAE-1.0, and KoBigBench, while\npreserving robust bilingual consistency and translation quality. In addition, a\nvision-augmented variant matches or exceeds GPT-4.1 on the KCSAT STEM\nbenchmark, all of which are achieved with substantially lower training compute\nthan existing models of similar sizes. We also present a pruning and\ndistillation technique that will soon be applied to HyperCLOVA X THINK for an\nopen-source and business-friendly foundation model. Altogether, these\ncapabilities position HyperCLOVA X THINK as a robust foundation for Korean AI\ninnovation and a valuable resource for the global research community.",
        "url": "http://arxiv.org/abs/2506.22403v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22403v1",
        "arxiv_id": "2506.22403v1",
        "authors": [
            "NAVER Cloud HyperCLOVA X Team"
        ],
        "submitted": "2025-06-27 17:23:12",
        "source": "arxiv",
        "comment": "49 pages, 13 figures"
    },
    {
        "title": "Refining Czech GEC: Insights from a Multi-Experiment Approach",
        "abstract": "We present a grammar error correction (GEC) system that achieves state of the\nart for the Czech language. Our system is based on a neural network translation\napproach with the Transformer architecture, and its key feature is its\nreal-time synthetic generation pipeline, which dynamically augments sentences\nwith artificial errors by introducing both language-agnostic and Czech-specific\nerrors. We conduct a comprehensive series of experiments, investigating the\nCzech GEC corpora as bases for synthetic error introduction, several error\ngeneration strategies, domain balancing, tokenization granularity, model size,\nand data scaling during fine-tuning. Additionally, we evaluate the performance\nof large language models (LLMs) on Czech GEC in both end-user and expert\nfine-tuning scenarios. Our best-performing model is superior both in\nperformance and computational efficiency. The source code and the trained model\nlinks are available on https://github.com/ufal/tsd2025-gec.",
        "url": "http://arxiv.org/abs/2506.22402v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22402v1",
        "arxiv_id": "2506.22402v1",
        "authors": [
            "Petr Pechman",
            "Milan Straka",
            "Jana Straková",
            "Jakub Náplava"
        ],
        "submitted": "2025-06-27 17:21:40",
        "source": "arxiv",
        "comment": "Accepted to TSD 2025"
    },
    {
        "title": "QuickSilver -- Speeding up LLM Inference through Dynamic Token Halting, KV Skipping, Contextual Token Fusion, and Adaptive Matryoshka Quantization",
        "abstract": "Inference accounts for the majority of latency and energy consumption in\nlarge language model (LLM) deployments, often exceeding 90% of total cost.\nWhile training-time efficiency has seen extensive progress, runtime\noptimization remains a key bottleneck, particularly under autoregressive\ndecoding. Existing approaches -- such as pruning, quantization, early exits,\nand speculative decoding -- often require retraining, architectural changes, or\ndisrupt decoding compatibility. We introduce QuickSilver, a modular,\ntoken-level framework that enables semantic adaptivity at inference time\nwithout altering model weights or structure. QuickSilver integrates four\nsynergistic mechanisms:\n  (i) Dynamic Token Halting, which halts computation for tokens with converged\nrepresentations; (ii) KV Cache Skipping, which selectively suppresses memory\nwrites to reduce attention overhead; and (iii) Contextual Token Fusion, which\ncollapses redundant tokens into shared paths to shrink sequence length.\n  Unlike speculative decoding or MoE routing, QuickSilver operates entirely on\nfrozen, dense models and requires no auxiliary networks. Applied to GPT-2 and\nLlama-2 across WikiText-103 and C4, QuickSilver achieves up to 39.6% FLOP\nreduction with negligible perplexity degradation (<=0.2).",
        "url": "http://arxiv.org/abs/2506.22396v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22396v1",
        "arxiv_id": "2506.22396v1",
        "authors": [
            "Danush Khanna",
            "Aditya Kumar Guru",
            "Srivarshinee Sridhar",
            "Zidan Ahmed",
            "Rubhav Bahirwani",
            "Meetu Malhotra",
            "Vinija Jain",
            "Aman Chadha",
            "Amitava Das",
            "Kripabandhu Ghosh"
        ],
        "submitted": "2025-06-27 17:10:32",
        "source": "arxiv",
        "comment": "Preprint. Under submission"
    },
    {
        "title": "Can Video Large Multimodal Models Think Like Doubters-or Double-Down: A Study on Defeasible Video Entailment",
        "abstract": "Video Large Multimodal Models (VLMMs) have made impressive strides in\nunderstanding video content, but they often struggle with abstract and adaptive\nreasoning-the ability to revise their interpretations when new information\nemerges. In reality, conclusions are rarely set in stone; additional context\ncan strengthen or weaken an initial inference. To address this, we introduce\nDefeasible Video Entailment (DVidE), a new task that challenges models to think\nlike doubters, constantly updating their reasoning based on evolving evidence.\nIn DVidE, given a video premise and a textual hypothesis, models must determine\nwhether a new update strengthens or weakens the hypothesis (classification\nversion) or generate a coherent update that modifies the entailment\nrelationship (generation version). For solving the classification task, we\npropose the Chain of Counterfactual Thought framework, utilizing counterfactual\nreasoning, ASR-enhanced video content, and rationale refinement to reduce\ninference bias. For the generation task, we develop a framework that combines\nASR output with a Large Language Model (LLM) to produce coherent, contextually\nrelevant updates aligned with the intended strengthener or weakener goals.\nAdditionally, we introduce a novel benchmark dataset, with\nstrengthener/weakener annotations and an LLM-based evaluation metric\nspecifically designed for assessing generative performance. Experimental\nresults demonstrate significant improvements, highlighting our proposed method\nin enhancing dynamic reasoning capabilities of VLMMs.",
        "url": "http://arxiv.org/abs/2506.22385v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22385v1",
        "arxiv_id": "2506.22385v1",
        "authors": [
            "Yue Zhang",
            "Jilei Sun",
            "Yunhui Guo",
            "Vibhav Gogate"
        ],
        "submitted": "2025-06-27 16:51:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Probabilistic Optimality for Inference-time Scaling",
        "abstract": "Inference-time scaling has emerged as a powerful technique for enhancing the\nreasoning performance of Large Language Models (LLMs). However, existing\napproaches often rely on heuristic strategies for parallel sampling, lacking a\nprincipled foundation. To address this gap, we propose a probabilistic\nframework that formalizes the optimality of inference-time scaling under the\nassumption that parallel samples are independently and identically distributed\n(i.i.d.), and where the Best-of-N selection strategy follows a probability\ndistribution that can be estimated. Within this framework, we derive a\ntheoretical lower bound on the required number of samples to achieve a target\nperformance level, providing the first principled guidance for\ncompute-efficient scaling. Leveraging this insight, we develop\n\\textsc{OptScale}, a practical algorithm that dynamically determines the\noptimal number of sampled responses. \\textsc{OptScale} employs a language\nmodel-based predictor to estimate probabilistic prior parameters, enabling the\ndecision of the minimal number of samples needed that satisfy predefined\nperformance thresholds and confidence levels. Extensive experiments on\nmathematical reasoning benchmarks (including MATH-500, GSM8K, AIME, and AMC)\ndemonstrate that \\textsc{OptScale} significantly reduces sampling overhead\nwhile remaining better or on par with state-of-the-art reasoning performance.\nOur work offers both a theoretical foundation and a practical solution for\nprincipled inference-time scaling, addressing a critical gap in the efficient\ndeployment of LLMs for complex reasoning.",
        "url": "http://arxiv.org/abs/2506.22376v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22376v1",
        "arxiv_id": "2506.22376v1",
        "authors": [
            "Youkang Wang",
            "Jian Wang",
            "Rubing Chen",
            "Xiao-Yong Wei",
            "Qing Li"
        ],
        "submitted": "2025-06-27 16:44:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Towards Fair Rankings: Leveraging LLMs for Gender Bias Detection and Measurement",
        "abstract": "The presence of social biases in Natural Language Processing (NLP) and\nInformation Retrieval (IR) systems is an ongoing challenge, which underlines\nthe importance of developing robust approaches to identifying and evaluating\nsuch biases. In this paper, we aim to address this issue by leveraging Large\nLanguage Models (LLMs) to detect and measure gender bias in passage ranking.\nExisting gender fairness metrics rely on lexical- and frequency-based measures,\nleading to various limitations, e.g., missing subtle gender disparities.\nBuilding on our LLM-based gender bias detection method, we introduce a novel\ngender fairness metric, named Class-wise Weighted Exposure (CWEx), aiming to\naddress existing limitations. To measure the effectiveness of our proposed\nmetric and study LLMs' effectiveness in detecting gender bias, we annotate a\nsubset of the MS MARCO Passage Ranking collection and release our new gender\nbias collection, called MSMGenderBias, to foster future research in this area.\nOur extensive experimental results on various ranking models show that our\nproposed metric offers a more detailed evaluation of fairness compared to\nprevious metrics, with improved alignment to human labels (58.77% for\nGrep-BiasIR, and 18.51% for MSMGenderBias, measured using Cohen's Kappa\nagreement), effectively distinguishing gender bias in ranking. By integrating\nLLM-driven bias detection, an improved fairness metric, and gender bias\nannotations for an established dataset, this work provides a more robust\nframework for analyzing and mitigating bias in IR systems.",
        "url": "http://arxiv.org/abs/2506.22372v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22372v1",
        "arxiv_id": "2506.22372v1",
        "authors": [
            "Maryam Mousavian",
            "Zahra Abbasiantaeb",
            "Mohammad Aliannejadi",
            "Fabio Crestani"
        ],
        "submitted": "2025-06-27 16:39:12",
        "source": "arxiv",
        "comment": "Accepted by ACM SIGIR Conference on Innovative Concepts and Theories\n  in Information Retrieval (ICTIR 2025)"
    },
    {
        "title": "Why Are Parsing Actions for Understanding Message Hierarchies Not Random?",
        "abstract": "If humans understood language by randomly selecting parsing actions, it might\nhave been necessary to construct a robust symbolic system capable of being\ninterpreted under any hierarchical structure. However, human parsing strategies\ndo not seem to follow such a random pattern. Why is that the case? In fact, a\nprevious study on emergent communication using models with hierarchical biases\nhave reported that agents adopting random parsing\nstrategies$\\unicode{x2013}$ones that deviate significantly from human language\ncomprehension$\\unicode{x2013}$can achieve high communication accuracy. In this\nstudy, we investigate this issue by making two simple and natural modifications\nto the experimental setup: (I) we use more complex inputs that have\nhierarchical structures, such that random parsing makes semantic interpretation\nmore difficult, and (II) we incorporate a surprisal-related term, which is\nknown to influence the order of words and characters in natural language, into\nthe objective function. With these changes, we evaluate whether agents\nemploying random parsing strategies still maintain high communication accuracy.",
        "url": "http://arxiv.org/abs/2506.22366v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22366v1",
        "arxiv_id": "2506.22366v1",
        "authors": [
            "Daichi Kato",
            "Ryo Ueda",
            "Yusuke Miyao"
        ],
        "submitted": "2025-06-27 16:27:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "HLTCOE at LiveRAG: GPT-Researcher using ColBERT retrieval",
        "abstract": "The HLTCOE LiveRAG submission utilized the GPT-researcher framework for\nresearching the context of the question, filtering the returned results, and\ngenerating the final answer. The retrieval system was a ColBERT bi-encoder\narchitecture, which represents a passage with many dense tokens. Retrieval used\na local, compressed index of the FineWeb10-BT collection created with PLAID-X,\nusing a model fine-tuned for multilingual retrieval. Query generation from\ncontext was done with Qwen2.5-7B-Instruct, while filtering was accomplished\nwith m2-bert-80M-8k-retrieval. Up to nine passages were used as context to\ngenerate an answer using Falcon3-10B. This system placed 5th in the LiveRAG\nautomatic evaluation for correctness with a score of 1.07.",
        "url": "http://arxiv.org/abs/2506.22356v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22356v1",
        "arxiv_id": "2506.22356v1",
        "authors": [
            "Kevin Duh",
            "Eugene Yang",
            "Orion Weller",
            "Andrew Yates",
            "Dawn Lawrie"
        ],
        "submitted": "2025-06-27 16:08:39",
        "source": "arxiv",
        "comment": "5 pages, 1 figure"
    },
    {
        "title": "Optimal Estimation of Watermark Proportions in Hybrid AI-Human Texts",
        "abstract": "Text watermarks in large language models (LLMs) are an increasingly important\ntool for detecting synthetic text and distinguishing human-written content from\nLLM-generated text. While most existing studies focus on determining whether\nentire texts are watermarked, many real-world scenarios involve mixed-source\ntexts, which blend human-written and watermarked content. In this paper, we\naddress the problem of optimally estimating the watermark proportion in\nmixed-source texts. We cast this problem as estimating the proportion parameter\nin a mixture model based on \\emph{pivotal statistics}. First, we show that this\nparameter is not even identifiable in certain watermarking schemes, let alone\nconsistently estimable. In stark contrast, for watermarking methods that employ\ncontinuous pivotal statistics for detection, we demonstrate that the proportion\nparameter is identifiable under mild conditions. We propose efficient\nestimators for this class of methods, which include several popular unbiased\nwatermarks as examples, and derive minimax lower bounds for any measurable\nestimator based on pivotal statistics, showing that our estimators achieve\nthese lower bounds. Through evaluations on both synthetic data and mixed-source\ntext generated by open-source models, we demonstrate that our proposed\nestimators consistently achieve high estimation accuracy.",
        "url": "http://arxiv.org/abs/2506.22343v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22343v1",
        "arxiv_id": "2506.22343v1",
        "authors": [
            "Xiang Li",
            "Garrett Wen",
            "Weiqing He",
            "Jiayuan Wu",
            "Qi Long",
            "Weijie J. Su"
        ],
        "submitted": "2025-06-27 15:53:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Evaluating Scoring Bias in LLM-as-a-Judge",
        "abstract": "The remarkable performance of Large Language Models (LLMs) gives rise\nto``LLM-as-a-Judge'', where LLMs are employed as evaluators for complex tasks.\nMoreover, it has been widely adopted across fields such as Natural Language\nProcessing (NLP), preference learning, and various specific domains. However,\nthere are various biases within LLM-as-a-Judge, which adversely affect the\nfairness and reliability of judgments. Current research on evaluating or\nmitigating bias in LLM-as-a-Judge predominantly focuses on comparison-based\nevaluations, while systematic investigations into bias in scoring-based\nevaluations remain limited. Therefore, we define scoring bias in LLM-as-a-Judge\nas the scores differ when scoring judge models are bias-related perturbed, and\nprovide a well-designed framework to comprehensively evaluate scoring bias. We\naugment existing LLM-as-a-Judge benchmarks through data synthesis to construct\nour evaluation dataset and design multi-faceted evaluation metrics. Our\nexperimental results demonstrate that the scoring stability of existing judge\nmodels is disrupted by scoring biases. Further exploratory experiments and\ndiscussions provide valuable insights into the design of scoring prompt\ntemplates and the mitigation of scoring biases on aspects such as score\nrubrics, score IDs, and reference answer selection.",
        "url": "http://arxiv.org/abs/2506.22316v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22316v1",
        "arxiv_id": "2506.22316v1",
        "authors": [
            "Qingquan Li",
            "Shaoyu Dou",
            "Kailai Shao",
            "Chao Chen",
            "Haixiang Hu"
        ],
        "submitted": "2025-06-27 15:25:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Conceptual Topic Aggregation",
        "abstract": "The vast growth of data has rendered traditional manual inspection\ninfeasible, necessitating the adoption of computational methods for efficient\ndata exploration. Topic modeling has emerged as a powerful tool for analyzing\nlarge-scale textual datasets, enabling the extraction of latent semantic\nstructures. However, existing methods for topic modeling often struggle to\nprovide interpretable representations that facilitate deeper insights into data\nstructure and content. In this paper, we propose FAT-CAT, an approach based on\nFormal Concept Analysis (FCA) to enhance meaningful topic aggregation and\nvisualization of discovered topics. Our approach can handle diverse topics and\nfile types -- grouped by directories -- to construct a concept lattice that\noffers a structured, hierarchical representation of their topic distribution.\nIn a case study on the ETYNTKE dataset, we evaluate the effectiveness of our\napproach against other representation methods to demonstrate that FCA-based\naggregation provides more meaningful and interpretable insights into dataset\ncomposition than existing topic modeling techniques.",
        "url": "http://arxiv.org/abs/2506.22309v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22309v1",
        "arxiv_id": "2506.22309v1",
        "authors": [
            "Klara M. Gutekunst",
            "Dominik Dürrschnabel",
            "Johannes Hirth",
            "Gerd Stumme"
        ],
        "submitted": "2025-06-27 15:19:38",
        "source": "arxiv",
        "comment": "16 pages, 4 tables, 11 figures, International Joint Conference on\n  Conceptual Knowledge Structures"
    },
    {
        "title": "Detection of Personal Data in Structured Datasets Using a Large Language Model",
        "abstract": "We propose a novel approach for detecting personal data in structured\ndatasets, leveraging GPT-4o, a state-of-the-art Large Language Model. A key\ninnovation of our method is the incorporation of contextual information: in\naddition to a feature's name and values, we utilize information from other\nfeature names within the dataset as well as the dataset description. We compare\nour approach to alternative methods, including Microsoft Presidio and CASSED,\nevaluating them on multiple datasets: DeSSI, a large synthetic dataset,\ndatasets we collected from Kaggle and OpenML as well as MIMIC-Demo-Ext, a\nreal-world dataset containing patient information from critical care units.\n  Our findings reveal that detection performance varies significantly depending\non the dataset used for evaluation. CASSED excels on DeSSI, the dataset on\nwhich it was trained. Performance on the medical dataset MIMIC-Demo-Ext is\ncomparable across all models, with our GPT-4o-based approach clearly\noutperforming the others. Notably, personal data detection in the Kaggle and\nOpenML datasets appears to benefit from contextual information. This is\nevidenced by the poor performance of CASSED and Presidio (both of which do not\nutilize the context of the dataset) compared to the strong results of our\nGPT-4o-based approach.\n  We conclude that further progress in this field would greatly benefit from\nthe availability of more real-world datasets containing personal information.",
        "url": "http://arxiv.org/abs/2506.22305v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22305v1",
        "arxiv_id": "2506.22305v1",
        "authors": [
            "Albert Agisha Ntwali",
            "Luca Rück",
            "Martin Heckmann"
        ],
        "submitted": "2025-06-27 15:16:43",
        "source": "arxiv",
        "comment": "10 pages"
    },
    {
        "title": "Education-Oriented Graph Retrieval-Augmented Generation for Learning Path Recommendation",
        "abstract": "Learning path recommendation seeks to provide learners with a structured\nsequence of learning items (e.g., knowledge concepts or exercises) to optimize\ntheir learning efficiency. Despite significant efforts in this area, most\nexisting methods primarily rely on prerequisite relationships, which present\ntwo major limitations: 1) Many educational datasets do not explicitly provide\nprerequisite relationships between knowledge concepts, hindering the\napplication of current learning path recommendation methods. 2) Relying solely\non prerequisite relationships as the sole knowledge structure can impede\nlearning progress and negatively impact student outcomes. To address these\nchallenges, we propose a novel approach, Discrimination Learning Enhances\nLearning Path Recommendation (DLELP), which enhances learning path\nrecommendations by incorporating both prerequisite and similarity relationships\nbetween knowledge concepts. Specifically, we introduce a knowledge concept\nstructure graph generation module that adaptively constructs knowledge concept\nstructure graphs for different educational datasets, significantly improving\nthe generalizability of learning path recommendation methods. We then propose a\nDiscrimination Learning-driven Reinforcement Learning (DLRL) framework, which\nmitigates the issue of blocked learning paths, further enhancing the efficacy\nof learning path recommendations. Finally, we conduct extensive experiments on\nthree benchmark datasets, demonstrating that our method not only achieves\nstate-of-the-art performance but also provides interpretable reasoning for the\nrecommended learning paths.",
        "url": "http://arxiv.org/abs/2506.22303v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22303v1",
        "arxiv_id": "2506.22303v1",
        "authors": [
            "Xinghe Cheng",
            "Zihan Zhang",
            "Jiapu Wang",
            "Liangda Fang",
            "Chaobo He",
            "Quanlong Guan",
            "Shirui Pan",
            "Weiqi Luo"
        ],
        "submitted": "2025-06-27 15:15:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "COOCO -- Common Objects Out-of-Context -- Semantic Violation in Scenes: Investigating Multimodal Context in Referential Communication",
        "abstract": "Natural scenes provide us with rich contexts for object recognition and\nreference. In particular, knowing what type of scene one is looking at\ngenerates expectations about which objects will occur, and what their spatial\nconfiguration should be. Do Vision-Language Models (VLMs) learn to rely on\nscene contexts in a similar way, when generating references to objects? To\naddress this question, we introduce the \\textit{Common Objects Out-of-Context\n(COOCO)} dataset and test to what extent VLMs rely on scene context to refer to\nobjects under different degrees of scene-object congruency, and different\nperturbations. Our findings show that models leverage scene context adaptively,\ndepending on both the semantic relatedness between object and scene and the\nlevel of noise. In particular, models rely more on context under high\ntarget-scene congruence or when objects are degraded. Attention analysis\nreveals that successful object categorisation involves increased focus on the\ntarget in mid-level layers, especially under moderate noise, suggesting that\nVLMs dynamically balance local and contextual information for reference\ngeneration. We make our dataset, code and models available at\n\\href{https://github.com/cs-nlp-uu/scenereg}{https://github.com/cs-nlp-uu/scenereg}.",
        "url": "http://arxiv.org/abs/2506.22274v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22274v1",
        "arxiv_id": "2506.22274v1",
        "authors": [
            "Filippo Merlo",
            "Ece Takmaz",
            "Wenkai Chen",
            "Albert Gatt"
        ],
        "submitted": "2025-06-27 14:44:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "JointRank: Rank Large Set with Single Pass",
        "abstract": "Efficiently ranking relevant items from large candidate pools is a\ncornerstone of modern information retrieval systems -- such as web search,\nrecommendation, and retrieval-augmented generation. Listwise rerankers, which\nimprove relevance by jointly considering multiple candidates, are often limited\nin practice: either by model input size constraints, or by degraded quality\nwhen processing large sets. We propose a model-agnostic method for fast\nreranking large sets that exceed a model input limits. The method first\npartitions candidate items into overlapping blocks, each of which is ranked\nindependently in parallel. Implicit pairwise comparisons are then derived from\nthese local rankings. Finally, these comparisons are aggregated to construct a\nglobal ranking using algorithms such as Winrate or PageRank. Experiments on\nTREC DL-2019 show that our method achieves an nDCG@10 of 70.88 compared to the\n57.68 for full-context listwise approach using gpt-4.1-mini as long-context\nmodel, while reducing latency from 21 to 8 seconds.\n  The implementation of the algorithm and the experiments is available in the\nrepository: https://github.com/V3RGANz/jointrank",
        "url": "http://arxiv.org/abs/2506.22262v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22262v1",
        "arxiv_id": "2506.22262v1",
        "authors": [
            "Evgeny Dedov"
        ],
        "submitted": "2025-06-27 14:30:12",
        "source": "arxiv",
        "comment": "ICTIR'25 Accepted"
    },
    {
        "title": "Projected Compression: Trainable Projection for Efficient Transformer Compression",
        "abstract": "Large language models have steadily increased in size to achieve improved\nperformance; however, this growth has also led to greater inference time and\ncomputational demands. Consequently, there is rising interest in model size\nreduction methods. To address this issue, we propose Projected Compression, a\nnovel model compression technique, that reduces model weights by utilizing\nprojection modules. Specifically, we first train additional trainable\nprojections weights and preserve access to all the original model parameters.\nSubsequently, these projections are merged into a lower-dimensional product\nmatrix, resulting in a reduced-size standard Transformer-based model. Unlike\nalternative approaches that require additional computational overhead, our\nmethod matches the base model's per-token computation step in FLOPs.\nExperimental results show that Projected Compression outperforms the comparable\nhard pruning and retraining approach on higher quality models. Moreover, the\nperformance margin scales well with the number of tokens.",
        "url": "http://arxiv.org/abs/2506.22255v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22255v1",
        "arxiv_id": "2506.22255v1",
        "authors": [
            "Maciej Stefaniak",
            "Michał Krutul",
            "Jan Małaśnicki",
            "Maciej Pióro",
            "Jakub Krajewski",
            "Sebastian Jaszczur",
            "Marek Cygan",
            "Kamil Adamczewski",
            "Jan Ludziejewski"
        ],
        "submitted": "2025-06-27 14:24:01",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Fine-Tuning MIDI-to-Audio Alignment using a Neural Network on Piano Roll and CQT Representations",
        "abstract": "In this paper, we present a neural network approach for synchronizing audio\nrecordings of human piano performances with their corresponding loosely aligned\nMIDI files. The task is addressed using a Convolutional Recurrent Neural\nNetwork (CRNN) architecture, which effectively captures spectral and temporal\nfeatures by processing an unaligned piano roll and a spectrogram as inputs to\nestimate the aligned piano roll. To train the network, we create a dataset of\npiano pieces with augmented MIDI files that simulate common human timing\nerrors. The proposed model achieves up to 20% higher alignment accuracy than\nthe industry-standard Dynamic Time Warping (DTW) method across various\ntolerance windows. Furthermore, integrating DTW with the CRNN yields additional\nimprovements, offering enhanced robustness and consistency. These findings\ndemonstrate the potential of neural networks in advancing state-of-the-art\nMIDI-to-audio alignment.",
        "url": "http://arxiv.org/abs/2506.22237v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22237v1",
        "arxiv_id": "2506.22237v1",
        "authors": [
            "Sebastian Murgul",
            "Moritz Reiser",
            "Michael Heizmann",
            "Christoph Seibert"
        ],
        "submitted": "2025-06-27 13:59:50",
        "source": "arxiv",
        "comment": "9 pages, 3 figures, 6 tables"
    },
    {
        "title": "Leveraging In-Context Learning for Political Bias Testing of LLMs",
        "abstract": "A growing body of work has been querying LLMs with political questions to\nevaluate their potential biases. However, this probing method has limited\nstability, making comparisons between models unreliable. In this paper, we\nargue that LLMs need more context. We propose a new probing task, Questionnaire\nModeling (QM), that uses human survey data as in-context examples. We show that\nQM improves the stability of question-based bias evaluation, and demonstrate\nthat it may be used to compare instruction-tuned models to their base versions.\nExperiments with LLMs of various sizes indicate that instruction tuning can\nindeed change the direction of bias. Furthermore, we observe a trend that\nlarger models are able to leverage in-context examples more effectively, and\ngenerally exhibit smaller bias scores in QM. Data and code are publicly\navailable.",
        "url": "http://arxiv.org/abs/2506.22232v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22232v1",
        "arxiv_id": "2506.22232v1",
        "authors": [
            "Patrick Haller",
            "Jannis Vamvas",
            "Rico Sennrich",
            "Lena A. Jäger"
        ],
        "submitted": "2025-06-27 13:49:37",
        "source": "arxiv",
        "comment": "ACL 2025"
    },
    {
        "title": "UiS-IAI@LiveRAG: Retrieval-Augmented Information Nugget-Based Generation of Responses",
        "abstract": "Retrieval-augmented generation (RAG) faces challenges related to factual\ncorrectness, source attribution, and response completeness. The LiveRAG\nChallenge hosted at SIGIR'25 aims to advance RAG research using a fixed corpus\nand a shared, open-source LLM. We propose a modular pipeline that operates on\ninformation nuggets-minimal, atomic units of relevant information extracted\nfrom retrieved documents. This multistage pipeline encompasses query rewriting,\npassage retrieval and reranking, nugget detection and clustering, cluster\nranking and summarization, and response fluency enhancement. This design\ninherently promotes grounding in specific facts, facilitates source\nattribution, and ensures maximum information inclusion within length\nconstraints. In this challenge, we extend our focus to also address the\nretrieval component of RAG, building upon our prior work on multi-faceted query\nrewriting. Furthermore, for augmented generation, we concentrate on improving\ncontext curation capabilities, maximizing the breadth of information covered in\nthe response while ensuring pipeline efficiency. Our results show that\ncombining original queries with a few sub-query rewrites boosts recall, while\nincreasing the number of documents used for reranking and generation beyond a\ncertain point reduces effectiveness, without improving response quality.",
        "url": "http://arxiv.org/abs/2506.22210v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22210v1",
        "arxiv_id": "2506.22210v1",
        "authors": [
            "Weronika Łajewska",
            "Ivica Kostric",
            "Gabriel Iturra-Bocaz",
            "Mariam Arustashvili",
            "Krisztian Balog"
        ],
        "submitted": "2025-06-27 13:29:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Exploring Modularity of Agentic Systems for Drug Discovery",
        "abstract": "Large-language models (LLMs) and agentic systems present exciting\nopportunities to accelerate drug discovery and design. In this study, we\ncritically examine the modularity of LLM-based agentic systems for drug\ndiscovery, i.e., whether parts of the agentic system such as the LLM are\ninterchangeable, a topic that has received limited attention in drug discovery\napplications. We compare the performance of different large language models\n(LLMs) and the effectiveness of tool-calling agents versus code-generating\nagents in this domain. Our case study, comparing performance in orchestrating\ntools for chemistry and drug discovery using an LLM-as-a-judge score, shows\nthat Claude-3.5-Sonnet, Claude-3.7-Sonnet and GPT-4o outperform alternative\nlanguage models such as Llama-3.1-8B, Llama-3.1-70B, GPT-3.5-Turbo, and\nNova-Micro. Although we confirm that code-generating agents outperform the\ntool-calling ones on average, we show that this is highly question and model\ndependent. Furthermore, the impact of replacing system prompts is dependent on\nthe specific question asked and the model used, underscoring that -- even in\nthis particular domain -- one cannot just replace language models without\nconsidering prompt re-engineering. Our study highlights the necessity of\nfurther research into the modularity of agentic systems to enable the\ndevelopment of stable and scalable solutions for real-world problems.",
        "url": "http://arxiv.org/abs/2506.22189v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22189v1",
        "arxiv_id": "2506.22189v1",
        "authors": [
            "Laura van Weesep",
            "Samuel Genheden",
            "Ola Engkvist",
            "Jens Sjölund"
        ],
        "submitted": "2025-06-27 12:57:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Missing Link: Joint Legal Citation Prediction using Heterogeneous Graph Enrichment",
        "abstract": "Legal systems heavily rely on cross-citations of legal norms as well as\nprevious court decisions. Practitioners, novices and legal AI systems need\naccess to these relevant data to inform appraisals and judgments. We propose a\nGraph-Neural-Network (GNN) link prediction model that can identify Case-Law and\nCase-Case citations with high proficiency through fusion of semantic and\ntopological information. We introduce adapted relational graph convolutions\noperating on an extended and enriched version of the original citation graph\nthat allow the topological integration of semantic meta-information. This\nfurther improves prediction by 3.1 points of average precision and by 8.5\npoints in data sparsity as well as showing robust performance over time and in\nchallenging fully inductive prediction. Jointly learning and predicting case\nand norm citations achieves a large synergistic effect that improves case\ncitation prediction by up to 4.7 points, at almost doubled efficiency.",
        "url": "http://arxiv.org/abs/2506.22165v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22165v1",
        "arxiv_id": "2506.22165v1",
        "authors": [
            "Lorenz Wendlinger",
            "Simon Alexander Nonn",
            "Abdullah Al Zubaer",
            "Michael Granitzer"
        ],
        "submitted": "2025-06-27 12:21:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Training Language Model to Critique for Better Refinement",
        "abstract": "Large language models (LLMs) have demonstrated remarkable evaluation and\ncritique capabilities, providing insightful feedback and identifying flaws in\nvarious tasks. However, limited research has explored which types of critiques\nare most effective for improving model responses or how to generate such\ncritiques. To address this gap, we introduce \\textbf{R}efinement-oriented\n\\textbf{C}ritique \\textbf{O}ptimization (RCO), a novel framework designed to\ntrain critic models using refinement signals. RCO uses a feedback loop where\ncritiques, generated by the critic model, guide the actor model in refining its\nresponses. The critique utility (CU) quantifies the effectiveness of these\nrefinements, serving as the reward signal for training the critic model. By\nfocusing on critiques that lead to better refinements, RCO eliminates the need\nfor direct critique preference assessment, ensuring that critiques driving\nmeaningful improvements are rewarded. We evaluate RCO across five tasks, i.e.,\ndialog generation, summarization, question answering, mathematical reasoning,\nand code generation, and show that it significantly outperforms traditional\nmethods and open-source models in terms of critique quality and refinement\noutcomes. Our contributions include the introduction of RCO, a novel\nsupervision scheme based on refined response preferences, and comprehensive\nexperimental results that highlight the method's effectiveness in enhancing LLM\ncritique-refinement loops.",
        "url": "http://arxiv.org/abs/2506.22157v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22157v1",
        "arxiv_id": "2506.22157v1",
        "authors": [
            "Tianshu Yu",
            "Chao Xiang",
            "Mingchuan Yang",
            "Pei Ke",
            "Bosi Wen",
            "Cunxiang Wang",
            "Jiale Cheng",
            "Li Zhang",
            "Xinyu Mu",
            "Chuxiong Sun",
            "Minlie Huang"
        ],
        "submitted": "2025-06-27 12:10:57",
        "source": "arxiv",
        "comment": "Accepted to ACL 2025 Findings"
    },
    {
        "title": "SAGE: Spliced-Audio Generated Data for Enhancing Foundational Models in Low-Resource Arabic-English Code-Switched Speech Recognition",
        "abstract": "This paper investigates the performance of various speech SSL models on\ndialectal Arabic (DA) and Arabic-English code-switched (CS) speech. To address\ndata scarcity, a modified audio-splicing approach is introduced to generate\nartificial CS speech data. Fine-tuning an already fine-tuned SSL model with the\nproposed Spliced-Audio Generated (SAGE) data results in an absolute improvement\non Word Error Rate (WER) of 7.8% on Arabic and English CS benchmarks.\nAdditionally, an Experience Replay (ER) inspired approach is proposed to\nenhance generalisation across DA and CS speech while mitigating catastrophic\nforgetting. Integrating an out-of-domain 3-gram language model reduces the\noverall mean WER from 31.7% to 26.6%. Few-shot fine-tuning for code-switching\nbenchmarks further improves WER by 4.9%. A WER of 31.1% on Arabic-English CS\nbenchmarks surpasses large-scale multilingual models, including USM and\nWhisper-large-v2 (both over ten times larger) by an absolute margin of 5.5% and\n8.4%, respectively.",
        "url": "http://arxiv.org/abs/2506.22143v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22143v1",
        "arxiv_id": "2506.22143v1",
        "authors": [
            "Muhammad Umar Farooq",
            "Oscar Saz"
        ],
        "submitted": "2025-06-27 11:42:43",
        "source": "arxiv",
        "comment": "Accepted for IEEE MLSP 2025"
    },
    {
        "title": "DAPFAM: A Domain-Aware Patent Retrieval Dataset Aggregated at the Family Level",
        "abstract": "In the landscape of publicly available patent retrieval datasets, the need\nfor explicit indomain and out-of-domain labeling, multi-jurisdiction coverage,\nbalanced query domain representation and manageable sizes that support sub\ndocument level experiments on moderate computational resources is often\noverlooked. To address these gaps, we propose DAPFAM, a new open access\ndomain-aware patent retrieval dataset constructed at the simple-family level.\nThe dataset contains 1,247 domain balanced full text query families and 45,336\nfull text target families. The dataset is enriched by clear relevance judgments\n(forward/backward citations as positive links, random negatives), as well as\nexplicit in-domain or out-of-domain relationships via a novel proposed\nlabelling scheme based on via International Patent Classification (IPC) codes,\nresulting in 49,869 evaluation pairs. The dataset is multi jurisdictional,\nrequires little to no preprocessing for retrieval evaluation, and remains of a\nsize manageable for entities with limited ressources allowing for sub document\nlevel retrieval experiments without excessive computational costs. We describe\nour three-step data-curation pipeline, present comprehensive dataset\nstatistics, and provide baseline experiments using lexical and neural retrieval\nmethods. Our baseline experiments highlight significant challenges in\ncrossdomain patent retrieval. The dataset will be publicly available (for now\nthe access link is this repository:\nhttps://osf.io/vbyzd/?view_only=1a40242e0d1941a58aa854af3e50cf6b).",
        "url": "http://arxiv.org/abs/2506.22141v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22141v1",
        "arxiv_id": "2506.22141v1",
        "authors": [
            "Iliass Ayaou",
            "Denis Cavallucci",
            "Hicham Chibane"
        ],
        "submitted": "2025-06-27 11:34:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Reward Balancing Revisited: Enhancing Offline Reinforcement Learning for Recommender Systems",
        "abstract": "Offline reinforcement learning (RL) has emerged as a prevalent and effective\nmethodology for real-world recommender systems, enabling learning policies from\nhistorical data and capturing user preferences. In offline RL, reward shaping\nencounters significant challenges, with past efforts to incorporate prior\nstrategies for uncertainty to improve world models or penalize underexplored\nstate-action pairs. Despite these efforts, a critical gap remains: the\nsimultaneous balancing of intrinsic biases in world models and the diversity of\npolicy recommendations. To address this limitation, we present an innovative\noffline RL framework termed Reallocated Reward for Recommender Systems (R3S).\nBy integrating inherent model uncertainty to tackle the intrinsic fluctuations\nin reward predictions, we boost diversity for decision-making to align with a\nmore interactive paradigm, incorporating extra penalizers with decay that deter\nactions leading to diminished state variety at both local and global scales.\nThe experimental results demonstrate that R3S improves the accuracy of world\nmodels and efficiently harmonizes the heterogeneous preferences of the users.",
        "url": "http://arxiv.org/abs/2506.22112v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22112v1",
        "arxiv_id": "2506.22112v1",
        "authors": [
            "Wenzheng Shu",
            "Yanxiang Zeng",
            "Yongxiang Tang",
            "Teng Sha",
            "Ning Luo",
            "Yanhua Cheng",
            "Xialong Liu",
            "Fan Zhou",
            "Peng Jiang"
        ],
        "submitted": "2025-06-27 10:46:41",
        "source": "arxiv",
        "comment": "Accepted in Companion Proceedings of the ACM Web Conference 2025"
    },
    {
        "title": "Identifying a Circuit for Verb Conjugation in GPT-2",
        "abstract": "I implement a procedure to isolate and interpret the sub-network (or\n\"circuit\") responsible for subject-verb agreement in GPT-2 Small. In this\nstudy, the model is given prompts where the subject is either singular (e.g.\n\"Alice\") or plural (e.g. \"Alice and Bob\"), and the task is to correctly predict\nthe appropriate verb form (\"walks\" for singular subjects, \"walk\" for plural\nsubjects). Using a series of techniques-including performance verification\nautomatic circuit discovery via direct path patching, and direct logit\nattribution- I isolate a candidate circuit that contributes significantly to\nthe model's correct verb conjugation. The results suggest that only a small\nfraction of the network's component-token pairs is needed to achieve near-model\nperformance on the base task but substantially more for more complex settings.",
        "url": "http://arxiv.org/abs/2506.22105v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22105v1",
        "arxiv_id": "2506.22105v1",
        "authors": [
            "David Demitri Africa"
        ],
        "submitted": "2025-06-27 10:35:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Involvement drives complexity of language in online debates",
        "abstract": "Language is a fundamental aspect of human societies, continuously evolving in\nresponse to various stimuli, including societal changes and intercultural\ninteractions. Technological advancements have profoundly transformed\ncommunication, with social media emerging as a pivotal force that merges\nentertainment-driven content with complex social dynamics. As these platforms\nreshape public discourse, analyzing the linguistic features of user-generated\ncontent is essential to understanding their broader societal impact. In this\npaper, we examine the linguistic complexity of content produced by influential\nusers on Twitter across three globally significant and contested topics:\nCOVID-19, COP26, and the Russia-Ukraine war. By combining multiple measures of\ntextual complexity, we assess how language use varies along four key\ndimensions: account type, political leaning, content reliability, and\nsentiment. Our analysis reveals significant differences across all four axes,\nincluding variations in language complexity between individuals and\norganizations, between profiles with sided versus moderate political views, and\nbetween those associated with higher versus lower reliability scores.\nAdditionally, profiles producing more negative and offensive content tend to\nuse more complex language, with users sharing similar political stances and\nreliability levels converging toward a common jargon. Our findings offer new\ninsights into the sociolinguistic dynamics of digital platforms and contribute\nto a deeper understanding of how language reflects ideological and social\nstructures in online spaces.",
        "url": "http://arxiv.org/abs/2506.22098v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22098v1",
        "arxiv_id": "2506.22098v1",
        "authors": [
            "Eleonora Amadori",
            "Daniele Cirulli",
            "Edoardo Di Martino",
            "Jacopo Nudo",
            "Maria Sahakyan",
            "Emanuele Sangiorgio",
            "Arnaldo Santoro",
            "Simon Zollo",
            "Alessandro Galeazzi",
            "Niccolò Di Marco"
        ],
        "submitted": "2025-06-27 10:27:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MDC-R: The Minecraft Dialogue Corpus with Reference",
        "abstract": "We introduce the Minecraft Dialogue Corpus with Reference (MDC-R). MDC-R is a\nnew language resource that supplements the original Minecraft Dialogue Corpus\n(MDC) with expert annotations of anaphoric and deictic reference. MDC's\ntask-orientated, multi-turn, situated dialogue in a dynamic environment has\nmotivated multiple annotation efforts, owing to the interesting linguistic\nphenomena that this setting gives rise to. We believe it can serve as a\nvaluable resource when annotated with reference, too. Here, we discuss our\nmethod of annotation and the resulting corpus, and provide both a quantitative\nand a qualitative analysis of the data. Furthermore, we carry out a short\nexperiment demonstrating the usefulness of our corpus for referring expression\ncomprehension.",
        "url": "http://arxiv.org/abs/2506.22062v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22062v1",
        "arxiv_id": "2506.22062v1",
        "authors": [
            "Chris Madge",
            "Maris Camilleri",
            "Paloma Carretero Garcia",
            "Mladen Karan",
            "Juexi Shao",
            "Prashant Jayannavar",
            "Julian Hough",
            "Benjamin Roth",
            "Massimo Poesio"
        ],
        "submitted": "2025-06-27 09:56:40",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Lost at the Beginning of Reasoning",
        "abstract": "Recent advancements in large language models (LLMs) have significantly\nadvanced complex reasoning capabilities, particularly through extended\nchain-of-thought (CoT) reasoning that incorporates mechanisms such as\nbacktracking, self-reflection and self-correction. Despite these developments,\nthe self-correction abilities of LLMs during long CoT reasoning remain\nunderexplored. And recent findings on overthinking suggest that such models\noften engage in unnecessarily redundant reasoning. In this work, we empirically\nshow that the first reasoning step exerts a disproportionately large influence\non the final prediction - errors introduced at this stage can substantially\ndegrade subsequent reasoning quality. This phenomenon is consistently observed\nacross two state-of-the-art open-source reasoning model families: DeepSeek-R1\nand Qwen3. To address this, we propose an efficient sampling strategy that\nleverages a reward model to identify and retain high-quality first reasoning\nsteps while discarding suboptimal ones, achieving up to a 70% reduction in\ninference cost without sacrificing accuracy. Finally, we introduce a new\nbenchmark specifically constructed with deliberately flawed first reasoning\nsteps to systematically evaluate model self-correction capabilities, offering a\nfoundation for future research on robust reasoning in LLMs.",
        "url": "http://arxiv.org/abs/2506.22058v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22058v1",
        "arxiv_id": "2506.22058v1",
        "authors": [
            "Baohao Liao",
            "Xinyi Chen",
            "Sara Rajaee",
            "Yuhui Xu",
            "Christian Herold",
            "Anders Søgaard",
            "Maarten de Rijke",
            "Christof Monz"
        ],
        "submitted": "2025-06-27 09:53:57",
        "source": "arxiv",
        "comment": "9 pages, 5 figures, 2 tables"
    },
    {
        "title": "Decoding Machine Translationese in English-Chinese News: LLMs vs. NMTs",
        "abstract": "This study explores Machine Translationese (MTese) -- the linguistic\npeculiarities of machine translation outputs -- focusing on the\nunder-researched English-to-Chinese language pair in news texts. We construct a\nlarge dataset consisting of 4 sub-corpora and employ a comprehensive five-layer\nfeature set. Then, a chi-square ranking algorithm is applied for feature\nselection in both classification and clustering tasks. Our findings confirm the\npresence of MTese in both Neural Machine Translation systems (NMTs) and Large\nLanguage Models (LLMs). Original Chinese texts are nearly perfectly\ndistinguishable from both LLM and NMT outputs. Notable linguistic patterns in\nMT outputs are shorter sentence lengths and increased use of adversative\nconjunctions. Comparing LLMs and NMTs, we achieve approximately 70%\nclassification accuracy, with LLMs exhibiting greater lexical diversity and\nNMTs using more brackets. Additionally, translation-specific LLMs show lower\nlexical diversity but higher usage of causal conjunctions compared to generic\nLLMs. Lastly, we find no significant differences between LLMs developed by\nChinese firms and their foreign counterparts.",
        "url": "http://arxiv.org/abs/2506.22050v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22050v1",
        "arxiv_id": "2506.22050v1",
        "authors": [
            "Delu Kong",
            "Lieve Macken"
        ],
        "submitted": "2025-06-27 09:45:37",
        "source": "arxiv",
        "comment": "14 pages, 5 figures, 6 tables. Accpeted in MT Summit 2025, Research:\n  Technical track. Official version may be accessed later in the ACL Anthology"
    },
    {
        "title": "GPAS: Accelerating Convergence of LLM Pretraining via Gradient-Preserving Activation Scaling",
        "abstract": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the residual path to dominate over sub-layer outputs and limiting the\nlearning capacity of deeper layers. To mitigate this issue, we propose\nGradient-Preserving Activation Scaling (GPAS), a simple technique that can be\nused in combination with existing approaches. GPAS works by scaling down the\nintermediate activations while keeping their gradients unchanged. This leaves\ninformation in the activations intact, and avoids the gradient vanishing\nproblem associated with gradient downscaling. Extensive experiments across\nvarious model sizes from 71M to 1B show that GPAS achieves consistent\nperformance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows\npromise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings.",
        "url": "http://arxiv.org/abs/2506.22049v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22049v1",
        "arxiv_id": "2506.22049v1",
        "authors": [
            "Tianhao Chen",
            "Xin Xu",
            "Zijing Liu",
            "Pengxiang Li",
            "Xinyuan Song",
            "Ajay Kumar Jaiswal",
            "Fan Zhang",
            "Jishan Hu",
            "Yang Wang",
            "Hao Chen",
            "Shizhe Diao",
            "Shiwei Liu",
            "Yu Li",
            "Yin Lu",
            "Can Yang"
        ],
        "submitted": "2025-06-27 09:45:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Can Peter Pan Survive MT? A Stylometric Study of LLMs, NMTs, and HTs in Children's Literature Translation",
        "abstract": "This study focuses on evaluating the performance of machine translations\n(MTs) compared to human translations (HTs) in English-to-Chinese children's\nliterature translation (CLT) from a stylometric perspective. The research\nconstructs a Peter Pan corpus, comprising 21 translations: 7 human translations\n(HTs), 7 large language model translations (LLMs), and 7 neural machine\ntranslation outputs (NMTs). The analysis employs a generic feature set\n(including lexical, syntactic, readability, and n-gram features) and a creative\ntext translation (CTT-specific) feature set, which captures repetition, rhythm,\ntranslatability, and miscellaneous levels, yielding 447 linguistic features in\ntotal.\n  Using classification and clustering techniques in machine learning, we\nconduct a stylometric analysis of these translations. Results reveal that in\ngeneric features, HTs and MTs exhibit significant differences in conjunction\nword distributions and the ratio of 1-word-gram-YiYang, while NMTs and LLMs\nshow significant variation in descriptive words usage and adverb ratios.\nRegarding CTT-specific features, LLMs outperform NMTs in distribution, aligning\nmore closely with HTs in stylistic characteristics, demonstrating the potential\nof LLMs in CLT.",
        "url": "http://arxiv.org/abs/2506.22038v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22038v1",
        "arxiv_id": "2506.22038v1",
        "authors": [
            "Delu Kong",
            "Lieve Macken"
        ],
        "submitted": "2025-06-27 09:34:40",
        "source": "arxiv",
        "comment": "19 pages, 8 figures, 4 tables. Accepted in 2nd Workshop on\n  Creative-text Translation and Technology Co-located with MT Summit 2025.\n  Official paper may later be accessed from ACL Anthology"
    },
    {
        "title": "Literature-Grounded Novelty Assessment of Scientific Ideas",
        "abstract": "Automated scientific idea generation systems have made remarkable progress,\nyet the automatic evaluation of idea novelty remains a critical and\nunderexplored challenge. Manual evaluation of novelty through literature review\nis labor-intensive, prone to error due to subjectivity, and impractical at\nscale. To address these issues, we propose the Idea Novelty Checker, an\nLLM-based retrieval-augmented generation (RAG) framework that leverages a\ntwo-stage retrieve-then-rerank approach. The Idea Novelty Checker first\ncollects a broad set of relevant papers using keyword and snippet-based\nretrieval, then refines this collection through embedding-based filtering\nfollowed by facet-based LLM re-ranking. It incorporates expert-labeled examples\nto guide the system in comparing papers for novelty evaluation and in\ngenerating literature-grounded reasoning. Our extensive experiments demonstrate\nthat our novelty checker achieves approximately 13% higher agreement than\nexisting approaches. Ablation studies further showcases the importance of the\nfacet-based re-ranker in identifying the most relevant literature for novelty\nevaluation.",
        "url": "http://arxiv.org/abs/2506.22026v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22026v1",
        "arxiv_id": "2506.22026v1",
        "authors": [
            "Simra Shahid",
            "Marissa Radensky",
            "Raymond Fok",
            "Pao Siangliulue",
            "Daniel S. Weld",
            "Tom Hope"
        ],
        "submitted": "2025-06-27 08:47:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Robust and Efficient Autoregressive Speech Synthesis with Dynamic Chunk-wise Prediction Policy",
        "abstract": "Recently, autoregressive (AR) language models have emerged as a dominant\napproach in speech synthesis, offering expressive generation and scalable\ntraining. However, conventional AR speech synthesis models relying on the\nnext-token prediction paradigm often encounter significant challenges when\nhandling long speech sequences. These models often struggle to construct stable\nframe-to-frame attention, leading to increased latency and degraded synthesis\nquality, thereby limiting their feasibility for real-time applications. To\naddress these limitations, we introduce a novel dynamic chunk-wise\nautoregressive synthesis framework, termed DCAR, designed to enhance both\nefficiency and intelligibility robustness in AR speech generation. DCAR\nintroduces a chunk-to-frame attention mechanism through training with\nmulti-token prediction, enabling dynamic chunk prediction in variable speech\ncontexts using a lightweight module trained on-policy. DCAR dynamically adjusts\nthe token prediction span, significantly reducing the sequence length\ndependency while obtaining high synthesis quality. Comprehensive empirical\nevaluations demonstrate that DCAR substantially outperforms traditional\nnext-token prediction models, achieving up to 72.27% intelligibility\nimprovement and 2.61x inference speedup simultaneously on the test set.\nFurthermore, we conduct comprehensive analysis to support it as a versatile\nfoundation for next-generation speech synthesis systems.",
        "url": "http://arxiv.org/abs/2506.22023v1",
        "pdf_url": "http://arxiv.org/pdf/2506.22023v1",
        "arxiv_id": "2506.22023v1",
        "authors": [
            "Bohan Li",
            "Zhihan Li",
            "Haoran Wang",
            "Hanglei Zhang",
            "Yiwei Guo",
            "Hankun Wang",
            "Xie Chen",
            "Kai Yu"
        ],
        "submitted": "2025-06-27 08:45:21",
        "source": "arxiv",
        "comment": "17 pages, 8 figures, 5 tables"
    },
    {
        "title": "Analyzing and Fine-Tuning Whisper Models for Multilingual Pilot Speech Transcription in the Cockpit",
        "abstract": "The developments in transformer encoder-decoder architectures have led to\nsignificant breakthroughs in machine translation, Automatic Speech Recognition\n(ASR), and instruction-based chat machines, among other applications. The\npre-trained models were trained on vast amounts of generic data over a few\nepochs (fewer than five in most cases), resulting in their strong\ngeneralization capabilities. Nevertheless, the performance of these models does\nsuffer when applied to niche domains like transcribing pilot speech in the\ncockpit, which involves a lot of specific vocabulary and multilingual\nconversations. This paper investigates and improves the transcription accuracy\nof cockpit conversations with Whisper models. We have collected around 85\nminutes of cockpit simulator recordings and 130 minutes of interview recordings\nwith pilots and manually labeled them. The speakers are middle aged men\nspeaking both German and English. To improve the accuracy of transcriptions, we\npropose multiple normalization schemes to refine the transcripts and improve\nWord Error Rate (WER). We then employ fine-tuning to enhance ASR performance,\nutilizing performance-efficient fine-tuning with Low-Rank Adaptation (LoRA).\nHereby, WER decreased from 68.49 \\% (pretrained whisper Large model without\nnormalization baseline) to 26.26\\% (finetuned whisper Large model with the\nproposed normalization scheme).",
        "url": "http://arxiv.org/abs/2506.21990v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21990v1",
        "arxiv_id": "2506.21990v1",
        "authors": [
            "Kartheek Kumar Reddy Nareddy",
            "Sarah Ternus",
            "Julia Niebling"
        ],
        "submitted": "2025-06-27 07:57:13",
        "source": "arxiv",
        "comment": "Computer Vision and Pattern Recognition (CVPR) 2025 Workshops"
    },
    {
        "title": "Don't Trust Generative Agents to Mimic Communication on Social Networks Unless You Benchmarked their Empirical Realism",
        "abstract": "The ability of Large Language Models (LLMs) to mimic human behavior triggered\na plethora of computational social science research, assuming that empirical\nstudies of humans can be conducted with AI agents instead. Since there have\nbeen conflicting research findings on whether and when this hypothesis holds,\nthere is a need to better understand the differences in their experimental\ndesigns. We focus on replicating the behavior of social network users with the\nuse of LLMs for the analysis of communication on social networks. First, we\nprovide a formal framework for the simulation of social networks, before\nfocusing on the sub-task of imitating user communication. We empirically test\ndifferent approaches to imitate user behavior on X in English and German. Our\nfindings suggest that social simulations should be validated by their empirical\nrealism measured in the setting in which the simulation components were fitted.\nWith this paper, we argue for more rigor when applying generative-agent-based\nmodeling for social simulation.",
        "url": "http://arxiv.org/abs/2506.21974v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21974v1",
        "arxiv_id": "2506.21974v1",
        "authors": [
            "Simon Münker",
            "Nils Schwager",
            "Achim Rettinger"
        ],
        "submitted": "2025-06-27 07:32:16",
        "source": "arxiv",
        "comment": "11 pages, 1 figure, 3 tables"
    },
    {
        "title": "Advancing Jailbreak Strategies: A Hybrid Approach to Exploiting LLM Vulnerabilities and Bypassing Modern Defenses",
        "abstract": "The advancement of Pre-Trained Language Models (PTLMs) and Large Language\nModels (LLMs) has led to their widespread adoption across diverse applications.\nDespite their success, these models remain vulnerable to attacks that exploit\ntheir inherent weaknesses to bypass safety measures. Two primary\ninference-phase threats are token-level and prompt-level jailbreaks.\nToken-level attacks embed adversarial sequences that transfer well to black-box\nmodels like GPT but leave detectable patterns and rely on gradient-based token\noptimization, whereas prompt-level attacks use semantically structured inputs\nto elicit harmful responses yet depend on iterative feedback that can be\nunreliable. To address the complementary limitations of these methods, we\npropose two hybrid approaches that integrate token- and prompt-level techniques\nto enhance jailbreak effectiveness across diverse PTLMs. GCG + PAIR and the\nnewly explored GCG + WordGame hybrids were evaluated across multiple Vicuna and\nLlama models. GCG + PAIR consistently raised attack-success rates over its\nconstituent techniques on undefended models; for instance, on Llama-3, its\nAttack Success Rate (ASR) reached 91.6%, a substantial increase from PAIR's\n58.4% baseline. Meanwhile, GCG + WordGame matched the raw performance of\nWordGame maintaining a high ASR of over 80% even under stricter evaluators like\nMistral-Sorry-Bench. Crucially, both hybrids retained transferability and\nreliably pierced advanced defenses such as Gradient Cuff and JBShield, which\nfully blocked single-mode attacks. These findings expose previously unreported\nvulnerabilities in current safety stacks, highlight trade-offs between raw\nsuccess and defensive robustness, and underscore the need for holistic\nsafeguards against adaptive adversaries.",
        "url": "http://arxiv.org/abs/2506.21972v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21972v1",
        "arxiv_id": "2506.21972v1",
        "authors": [
            "Mohamed Ahmed",
            "Mohamed Abdelmouty",
            "Mingyu Kim",
            "Gunvanth Kandula",
            "Alex Park",
            "James C. Davis"
        ],
        "submitted": "2025-06-27 07:26:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "More Vulnerable than You Think: On the Stability of Tool-Integrated LLM Agents",
        "abstract": "Current evaluations of tool-integrated LLM agents typically focus on\nend-to-end tool-usage evaluation while neglecting their stability. This limits\ntheir real-world applicability, as various internal or external factors can\ncause agents to crash or behave abnormally. Our research addresses this by\ninvestigating whether agents are vulnerable to errors throughout the entire\ntool invocation process, including reading tool documentation, selecting tools\nand generating parameters, and processing the tool's response. Through\nextensive experiments, we observe that agents are highly susceptible to errors\nat each stage and agents based on open-source models are more vulnerable than\nthose based on proprietary models. We also find that increasing the model size\ndoes not significantly improve tool invocation reasoning and may make agents\nmore vulnerable to attacks resembling normal user instructions. This highlights\nthe importance of evaluating agent stability and offers valuable insights for\nfuture LLM development and evaluation.",
        "url": "http://arxiv.org/abs/2506.21967v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21967v1",
        "arxiv_id": "2506.21967v1",
        "authors": [
            "Weimin Xiong",
            "Ke Wang",
            "Yifan Song",
            "Hanchao Liu",
            "Sai Zhou",
            "Wei Peng",
            "Sujian Li"
        ],
        "submitted": "2025-06-27 07:13:29",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Using Large Language Models to Suggest Informative Prior Distributions in Bayesian Statistics",
        "abstract": "Selecting prior distributions in Bayesian statistics is challenging,\nresource-intensive, and subjective. We analyze using large-language models\n(LLMs) to suggest suitable, knowledge-based informative priors. We developed an\nextensive prompt asking LLMs not only to suggest priors but also to verify and\nreflect on their choices.\n  We evaluated Claude Opus, Gemini 2.5 Pro, and ChatGPT-4o-mini on two real\ndatasets: heart disease risk and concrete strength. All LLMs correctly\nidentified the direction for all associations (e.g., that heart disease risk is\nhigher for males). The quality of suggested priors was measured by their\nKullback-Leibler divergence from the maximum likelihood estimator's\ndistribution.\n  The LLMs suggested both moderately and weakly informative priors. The\nmoderate priors were often overconfident, resulting in distributions misaligned\nwith the data. In our experiments, Claude and Gemini provided better priors\nthan ChatGPT. For weakly informative priors, a key performance difference\nemerged: ChatGPT and Gemini defaulted to an \"unnecessarily vague\" mean of 0,\nwhile Claude did not, demonstrating a significant advantage.\n  The ability of LLMs to identify correct associations shows their great\npotential as an efficient, objective method for developing informative priors.\nHowever, the primary challenge remains in calibrating the width of these priors\nto avoid over- and under-confidence.",
        "url": "http://arxiv.org/abs/2506.21964v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21964v1",
        "arxiv_id": "2506.21964v1",
        "authors": [
            "Michael A. Riegler",
            "Kristoffer Herland Hellton",
            "Vajira Thambawita",
            "Hugo L. Hammer"
        ],
        "submitted": "2025-06-27 07:11:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PapersPlease: A Benchmark for Evaluating Motivational Values of Large Language Models Based on ERG Theory",
        "abstract": "Evaluating the performance and biases of large language models (LLMs) through\nrole-playing scenarios is becoming increasingly common, as LLMs often exhibit\nbiased behaviors in these contexts. Building on this line of research, we\nintroduce PapersPlease, a benchmark consisting of 3,700 moral dilemmas designed\nto investigate LLMs' decision-making in prioritizing various levels of human\nneeds. In our setup, LLMs act as immigration inspectors deciding whether to\napprove or deny entry based on the short narratives of people. These narratives\nare constructed using the Existence, Relatedness, and Growth (ERG) theory,\nwhich categorizes human needs into three hierarchical levels. Our analysis of\nsix LLMs reveals statistically significant patterns in decision-making,\nsuggesting that LLMs encode implicit preferences. Additionally, our evaluation\nof the impact of incorporating social identities into the narratives shows\nvarying responsiveness based on both motivational needs and identity cues, with\nsome models exhibiting higher denial rates for marginalized identities. All\ndata is publicly available at https://github.com/yeonsuuuu28/papers-please.",
        "url": "http://arxiv.org/abs/2506.21961v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21961v1",
        "arxiv_id": "2506.21961v1",
        "authors": [
            "Junho Myung",
            "Yeon Su Park",
            "Sunwoo Kim",
            "Shin Yoo",
            "Alice Oh"
        ],
        "submitted": "2025-06-27 07:09:11",
        "source": "arxiv",
        "comment": "Accepted to GEM2 Workshop: Generation, Evaluation & Metrics - ACL\n  2025"
    },
    {
        "title": "CAL-RAG: Retrieval-Augmented Multi-Agent Generation for Content-Aware Layout Design",
        "abstract": "Automated content-aware layout generation -- the task of arranging visual\nelements such as text, logos, and underlays on a background canvas -- remains a\nfundamental yet under-explored problem in intelligent design systems. While\nrecent advances in deep generative models and large language models (LLMs) have\nshown promise in structured content generation, most existing approaches lack\ngrounding in contextual design exemplars and fall short in handling semantic\nalignment and visual coherence. In this work we introduce CAL-RAG, a\nretrieval-augmented, agentic framework for content-aware layout generation that\nintegrates multimodal retrieval, large language models, and collaborative\nagentic reasoning. Our system retrieves relevant layout examples from a\nstructured knowledge base and invokes an LLM-based layout recommender to\npropose structured element placements. A vision-language grader agent evaluates\nthe layout with visual metrics, and a feedback agent provides targeted\nrefinements, enabling iterative improvement. We implement our framework using\nLangGraph and evaluate it on the PKU PosterLayout dataset, a benchmark rich in\nsemantic and structural variability. CAL-RAG achieves state-of-the-art\nperformance across multiple layout metrics -- including underlay effectiveness,\nelement alignment, and overlap -- substantially outperforming strong baselines\nsuch as LayoutPrompter. These results demonstrate that combining retrieval\naugmentation with agentic multi-step reasoning yields a scalable,\ninterpretable, and high-fidelity solution for automated layout generation.",
        "url": "http://arxiv.org/abs/2506.21934v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21934v1",
        "arxiv_id": "2506.21934v1",
        "authors": [
            "Najmeh Forouzandehmehr",
            "Reza Yousefi Maragheh",
            "Sriram Kollipara",
            "Kai Zhao",
            "Topojoy Biswas",
            "Evren Korpeoglu",
            "Kannan Achan"
        ],
        "submitted": "2025-06-27 06:09:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ARAG: Agentic Retrieval Augmented Generation for Personalized Recommendation",
        "abstract": "Retrieval-Augmented Generation (RAG) has shown promise in enhancing\nrecommendation systems by incorporating external context into large language\nmodel prompts. However, existing RAG-based approaches often rely on static\nretrieval heuristics and fail to capture nuanced user preferences in dynamic\nrecommendation scenarios. In this work, we introduce ARAG, an Agentic\nRetrieval-Augmented Generation framework for Personalized Recommendation, which\nintegrates a multi-agent collaboration mechanism into the RAG pipeline. To\nbetter understand the long-term and session behavior of the user, ARAG\nleverages four specialized LLM-based agents: a User Understanding Agent that\nsummarizes user preferences from long-term and session contexts, a Natural\nLanguage Inference (NLI) Agent that evaluates semantic alignment between\ncandidate items retrieved by RAG and inferred intent, a context summary agent\nthat summarizes the findings of NLI agent, and an Item Ranker Agent that\ngenerates a ranked list of recommendations based on contextual fit. We evaluate\nARAG accross three datasets. Experimental results demonstrate that ARAG\nsignificantly outperforms standard RAG and recency-based baselines, achieving\nup to 42.1% improvement in NDCG@5 and 35.5% in Hit@5. We also, conduct an\nablation study to analyse the effect by different components of ARAG. Our\nfindings highlight the effectiveness of integrating agentic reasoning into\nretrieval-augmented recommendation and provide new directions for LLM-based\npersonalization.",
        "url": "http://arxiv.org/abs/2506.21931v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21931v1",
        "arxiv_id": "2506.21931v1",
        "authors": [
            "Reza Yousefi Maragheh",
            "Pratheek Vadla",
            "Priyank Gupta",
            "Kai Zhao",
            "Aysenur Inan",
            "Kehui Yao",
            "Jianpeng Xu",
            "Praveen Kanumala",
            "Jason Cho",
            "Sushant Kumar"
        ],
        "submitted": "2025-06-27 05:45:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "HyReC: Exploring Hybrid-based Retriever for Chinese",
        "abstract": "Hybrid-based retrieval methods, which unify dense-vector and lexicon-based\nretrieval, have garnered considerable attention in the industry due to\nperformance enhancement. However, despite their promising results, the\napplication of these hybrid paradigms in Chinese retrieval contexts has\nremained largely underexplored. In this paper, we introduce HyReC, an\ninnovative end-to-end optimization method tailored specifically for\nhybrid-based retrieval in Chinese. HyReC enhances performance by integrating\nthe semantic union of terms into the representation model. Additionally, it\nfeatures the Global-Local-Aware Encoder (GLAE) to promote consistent semantic\nsharing between lexicon-based and dense retrieval while minimizing the\ninterference between them. To further refine alignment, we incorporate a\nNormalization Module (NM) that fosters mutual benefits between the retrieval\napproaches. Finally, we evaluate HyReC on the C-MTEB retrieval benchmark to\ndemonstrate its effectiveness.",
        "url": "http://arxiv.org/abs/2506.21913v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21913v1",
        "arxiv_id": "2506.21913v1",
        "authors": [
            "Zunran Wang",
            "Zheng Shenpeng",
            "Wang Shenglan",
            "Minghui Zhao",
            "Zhonghua Li"
        ],
        "submitted": "2025-06-27 04:57:01",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AutoMixer: Checkpoint Artifacts as Automatic Data Mixers",
        "abstract": "In language model training, it is desirable to equip models with capabilities\nfrom various tasks. However, it is not clear how to directly obtain the right\ndata mixtures for these capabilities as the relationship between data and tasks\nis difficult to be modeled. In this work, we observe that checkpoint models\nexhibit emerging capabilities at different points in the training trajectory.\nOften, the training process saves checkpoints as artifacts that are\nunder-utilized as a source of in-training data signals. We identify these\nartifact models based on their respective capabilities on the benchmarks and\nleverage them as data mixers by using their aggregated first-order influence\napproximation over source data. We demonstrated on eight reasoning benchmarks\nthat the proposed framework shows significant improvements in the pretraining\nsetting, with performance improvements of up to 1.93%. Overall, this shows the\npotential of checkpoint models to enhance data quality and optimize data\nmixtures.",
        "url": "http://arxiv.org/abs/2506.21910v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21910v1",
        "arxiv_id": "2506.21910v1",
        "authors": [
            "Ernie Chang",
            "Yang Li",
            "Patrick Huber",
            "David Kant",
            "Yangyang Shi",
            "Vikas Chandra"
        ],
        "submitted": "2025-06-27 04:53:07",
        "source": "arxiv",
        "comment": "Accepted at ACL 2025"
    },
    {
        "title": "A Dual-Layered Evaluation of Geopolitical and Cultural Bias in LLMs",
        "abstract": "As large language models (LLMs) are increasingly deployed across diverse\nlinguistic and cultural contexts, understanding their behavior in both factual\nand disputable scenarios is essential, especially when their outputs may shape\npublic opinion or reinforce dominant narratives. In this paper, we define two\ntypes of bias in LLMs: model bias (bias stemming from model training) and\ninference bias (bias induced by the language of the query), through a two-phase\nevaluation. Phase 1 evaluates LLMs on factual questions where a single\nverifiable answer exists, assessing whether models maintain consistency across\ndifferent query languages. Phase 2 expands the scope by probing geopolitically\nsensitive disputes, where responses may reflect culturally embedded or\nideologically aligned perspectives. We construct a manually curated dataset\nspanning both factual and disputable QA, across four languages and question\ntypes. The results show that Phase 1 exhibits query language induced alignment,\nwhile Phase 2 reflects an interplay between the model's training context and\nquery language. This paper offers a structured framework for evaluating LLM\nbehavior across neutral and sensitive topics, providing insights for future LLM\ndeployment and culturally aware evaluation practices in multilingual contexts.",
        "url": "http://arxiv.org/abs/2506.21881v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21881v1",
        "arxiv_id": "2506.21881v1",
        "authors": [
            "Sean Kim",
            "Hyuhng Joon Kim"
        ],
        "submitted": "2025-06-27 03:37:15",
        "source": "arxiv",
        "comment": "This paper is accepted to ACL Student Research Workshop (SRW) 2025"
    },
    {
        "title": "Do Vision-Language Models Have Internal World Models? Towards an Atomic Evaluation",
        "abstract": "Internal world models (WMs) enable agents to understand the world's state and\npredict transitions, serving as the basis for advanced deliberative reasoning.\nRecent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and\nGemini, exhibit potential as general-purpose WMs. While the latest studies have\nevaluated and shown limitations in specific capabilities such as visual\nunderstanding, a systematic evaluation of VLMs' fundamental WM abilities\nremains absent. Drawing on comparative psychology and cognitive science, we\npropose a two-stage framework that assesses Perception (visual, spatial,\ntemporal, quantitative, and motion) and Prediction (mechanistic simulation,\ntransitive inference, compositional inference) to provide an atomic evaluation\nof VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale\nbenchmark comprising 23 fine-grained evaluation dimensions across 6 diverse\nsimulated environments with controlled counterfactual simulations. Through 660\nexperiments on 15 latest commercial and open-source VLMs, we find that these\nmodels exhibit striking limitations in basic world modeling abilities. For\ninstance, almost all models perform at near-random accuracy when distinguishing\nmotion trajectories. Additionally, they lack disentangled understanding --\ne.g., some models tend to believe blue objects move faster than green ones.\nMore rich results and analyses reveal significant gaps between VLMs and\nhuman-level world modeling.",
        "url": "http://arxiv.org/abs/2506.21876v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21876v1",
        "arxiv_id": "2506.21876v1",
        "authors": [
            "Qiyue Gao",
            "Xinyu Pi",
            "Kevin Liu",
            "Junrong Chen",
            "Ruolan Yang",
            "Xinqi Huang",
            "Xinyu Fang",
            "Lu Sun",
            "Gautham Kishore",
            "Bo Ai",
            "Stone Tao",
            "Mengyang Liu",
            "Jiaxi Yang",
            "Chao-Jung Lai",
            "Chuanyang Jin",
            "Jiannan Xiang",
            "Benhao Huang",
            "Zeming Chen",
            "David Danks",
            "Hao Su",
            "Tianmin Shu",
            "Ziqiao Ma",
            "Lianhui Qin",
            "Zhiting Hu"
        ],
        "submitted": "2025-06-27 03:24:29",
        "source": "arxiv",
        "comment": "ACL 2025 (Findings)"
    },
    {
        "title": "WildSpeech-Bench: Benchmarking Audio LLMs in Natural Speech Conversation",
        "abstract": "Recent multi-modal Large Language Models (LLMs) such as GPT-4o have\ndemonstrated strong capabilities of direct speech interaction. However, the\nlack of specialized and comprehensive benchmarks for end-to-end speech LLM\nevaluation hinders optimizing the user experience of Audio LLMs in real-world\napplications. Existing evaluation methods often adapt text-based benchmarks,\noverlooking speech's unique characteristics and challenges, including prosody,\nhomophones, stuttering, and differing user expectations. Here, we present a\nnovel approach to thoroughly evaluate LLMs in practical speech conversations.\nWe systematically curate real-world chat data relevant to spoken scenarios,\nintroduce diversity in speaker attributes and acoustic conditions, and augment\nthe dataset with speech-specific phenomena. We further design a query-aware\nevaluation method to use customized evaluation checklists and prompts to\nenhance the accuracy of automatic evaluation. We conduct comprehensive testing\nand detailed analysis of various mainstream speech models, revealing\nsignificant differences in model performance across different speech scenarios.\nThe use of query-aware evaluation further enables a finer-grained assessment\nunder various speech-specific scenarios. Our benchmark can provide valuable\ninsights for speech model development and evaluation.",
        "url": "http://arxiv.org/abs/2506.21875v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21875v1",
        "arxiv_id": "2506.21875v1",
        "authors": [
            "Jian Zhang",
            "Linhao Zhang",
            "Bokai Lei",
            "Chuhan Wu",
            "Wei Jia",
            "Xiao Zhou"
        ],
        "submitted": "2025-06-27 03:18:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "RiverEcho: Real-Time Interactive Digital System for Ancient Yellow River Culture",
        "abstract": "The Yellow River is China's mother river and a cradle of human civilization.\nThe ancient Yellow River culture is, moreover, an indispensable part of human\nart history. To conserve and inherit the ancient Yellow River culture, we\ndesigned RiverEcho, a real-time interactive system that responds to voice\nqueries using a large language model and a cultural knowledge dataset,\ndelivering explanations through a talking-head digital human. Specifically, we\nbuilt a knowledge database focused on the ancient Yellow River culture,\nincluding the collection of historical texts and the processing pipeline.\nExperimental results demonstrate that leveraging Retrieval-Augmented Generation\n(RAG) on the proposed dataset enhances the response quality of the Large\nLanguage Model(LLM), enabling the system to generate more professional and\ninformative responses. Our work not only diversifies the means of promoting\nYellow River culture but also provides users with deeper cultural insights.",
        "url": "http://arxiv.org/abs/2506.21865v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21865v1",
        "arxiv_id": "2506.21865v1",
        "authors": [
            "Haofeng Wang",
            "Yilin Guo",
            "Zehao Li",
            "Tong Yue",
            "Yizong Wang",
            "Enci Zhang",
            "Rongqun Lin",
            "Feng Gao",
            "Shiqi Wang",
            "Siwei Ma"
        ],
        "submitted": "2025-06-27 02:40:00",
        "source": "arxiv",
        "comment": "IEEE International Conference on Multimedia and Expo Workshop,\n  2025.(Accepted)"
    },
    {
        "title": "DeepTalk: Towards Seamless and Smart Speech Interaction with Adaptive Modality-Specific MoE",
        "abstract": "Native multimodal large language models (MLLMs) restructure a single large\nlanguage model (LLM) into a spoken language model (SLM) capable of both speech\nand text generation. Compared to modular and aligned MLLMs, native MLLMs\npreserve richer paralinguistic features such as emotion and prosody, and\ngenerate speech responses directly within the backbone LLM rather than using a\nseparate speech decoder. This integration also results in lower response\nlatency and smoother interaction. However, native MLLMs suffer from\ncatastrophic forgetting and performance degradation because the available\npaired speech-text data is insufficient to support the pretraining of MLLMs\ncompared to the vast amount of text data required to pretrain text LLMs. To\naddress this issue, we propose DeepTalk, a framework for adaptive modality\nexpert learning based on a Mixture of Experts (MoE) architecture. DeepTalk\nfirst adaptively distinguishes modality experts according to their modality\nload within the LLM. Each modality expert then undergoes specialized\nsingle-modality training, followed by joint multimodal collaborative training.\nAs a result, DeepTalk incurs only a 5.5% performance drop compared to the\noriginal LLM, which is significantly lower than the average performance drop of\nover 20% typically seen in native MLLMs (such as GLM-4-Voice), and is on par\nwith modular MLLMs. Meanwhile, the end-to-end dialogue latency remains within\n0.5 seconds, ensuring a seamless and intelligent speech interaction experience.\nCode and models are released at https://github.com/talkking/DeepTalk.",
        "url": "http://arxiv.org/abs/2506.21864v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21864v1",
        "arxiv_id": "2506.21864v1",
        "authors": [
            "Hang Shao",
            "Heting Gao",
            "Yunhang Shen",
            "Jiawei Chen",
            "Lijiang Li",
            "Zuwei Long",
            "Bo Tong",
            "Ke Li",
            "Xing Sun"
        ],
        "submitted": "2025-06-27 02:32:04",
        "source": "arxiv",
        "comment": "Under Review"
    },
    {
        "title": "Derivational Probing: Unveiling the Layer-wise Derivation of Syntactic Structures in Neural Language Models",
        "abstract": "Recent work has demonstrated that neural language models encode syntactic\nstructures in their internal representations, yet the derivations by which\nthese structures are constructed across layers remain poorly understood. In\nthis paper, we propose Derivational Probing to investigate how micro-syntactic\nstructures (e.g., subject noun phrases) and macro-syntactic structures (e.g.,\nthe relationship between the root verbs and their direct dependents) are\nconstructed as word embeddings propagate upward across layers. Our experiments\non BERT reveal a clear bottom-up derivation: micro-syntactic structures emerge\nin lower layers and are gradually integrated into a coherent macro-syntactic\nstructure in higher layers. Furthermore, a targeted evaluation on subject-verb\nnumber agreement shows that the timing of constructing macro-syntactic\nstructures is critical for downstream performance, suggesting an optimal timing\nfor integrating global syntactic information.",
        "url": "http://arxiv.org/abs/2506.21861v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21861v1",
        "arxiv_id": "2506.21861v1",
        "authors": [
            "Taiga Someya",
            "Ryo Yoshida",
            "Hitomi Yanaka",
            "Yohei Oseki"
        ],
        "submitted": "2025-06-27 02:29:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Consistency Hypothesis in Uncertainty Quantification for Large Language Models",
        "abstract": "Estimating the confidence of large language model (LLM) outputs is essential\nfor real-world applications requiring high user trust. Black-box uncertainty\nquantification (UQ) methods, relying solely on model API access, have gained\npopularity due to their practical benefits. In this paper, we examine the\nimplicit assumption behind several UQ methods, which use generation consistency\nas a proxy for confidence, an idea we formalize as the consistency hypothesis.\nWe introduce three mathematical statements with corresponding statistical tests\nto capture variations of this hypothesis and metrics to evaluate LLM output\nconformity across tasks. Our empirical investigation, spanning 8 benchmark\ndatasets and 3 tasks (question answering, text summarization, and text-to-SQL),\nhighlights the prevalence of the hypothesis under different settings. Among the\nstatements, we highlight the `Sim-Any' hypothesis as the most actionable, and\ndemonstrate how it can be leveraged by proposing data-free black-box UQ methods\nthat aggregate similarities between generations for confidence estimation.\nThese approaches can outperform the closest baselines, showcasing the practical\nvalue of the empirically observed consistency hypothesis.",
        "url": "http://arxiv.org/abs/2506.21849v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21849v1",
        "arxiv_id": "2506.21849v1",
        "authors": [
            "Quan Xiao",
            "Debarun Bhattacharjya",
            "Balaji Ganesan",
            "Radu Marinescu",
            "Katsiaryna Mirylenka",
            "Nhan H Pham",
            "Michael Glass",
            "Junkyu Lee"
        ],
        "submitted": "2025-06-27 01:53:15",
        "source": "arxiv",
        "comment": "Accepted by The Conference on Uncertainty in Artificial Intelligence\n  (UAI) 2025"
    },
    {
        "title": "LinguaSynth: Heterogeneous Linguistic Signals for News Classification",
        "abstract": "Deep learning has significantly advanced NLP, but its reliance on large\nblack-box models introduces critical interpretability and computational\nefficiency concerns. This paper proposes LinguaSynth, a novel text\nclassification framework that strategically integrates five complementary\nlinguistic feature types: lexical, syntactic, entity-level, word-level\nsemantics, and document-level semantics within a transparent logistic\nregression model. Unlike transformer-based architectures, LinguaSynth maintains\ninterpretability and computational efficiency, achieving an accuracy of 84.89\npercent on the 20 Newsgroups dataset and surpassing a robust TF-IDF baseline by\n3.32 percent. Through rigorous feature interaction analysis, we show that\nsyntactic and entity-level signals provide essential disambiguation and\neffectively complement distributional semantics. LinguaSynth sets a new\nbenchmark for interpretable, resource-efficient NLP models and challenges the\nprevailing assumption that deep neural networks are necessary for\nhigh-performing text classification.",
        "url": "http://arxiv.org/abs/2506.21848v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21848v1",
        "arxiv_id": "2506.21848v1",
        "authors": [
            "Duo Zhang",
            "Junyi Mo"
        ],
        "submitted": "2025-06-27 01:45:20",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "3Description: An Intuitive Human-AI Collaborative 3D Modeling Approach",
        "abstract": "This paper presents 3Description, an experimental human-AI collaborative\napproach for intuitive 3D modeling. 3Description aims to address accessibility\nand usability challenges in traditional 3D modeling by enabling\nnon-professional individuals to co-create 3D models using verbal and gesture\ndescriptions. Through a combination of qualitative research, product analysis,\nand user testing, 3Description integrates AI technologies such as Natural\nLanguage Processing and Computer Vision, powered by OpenAI and MediaPipe.\nRecognizing the web has wide cross-platform capabilities, 3Description is\nweb-based, allowing users to describe the desired model and subsequently adjust\nits components using verbal and gestural inputs. In the era of AI and emerging\nmedia, 3Description not only contributes to a more inclusive and user-friendly\ndesign process, empowering more people to participate in the construction of\nthe future 3D world, but also strives to increase human engagement in\nco-creation with AI, thereby avoiding undue surrender to technology and\npreserving human creativity.",
        "url": "http://arxiv.org/abs/2506.21845v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21845v1",
        "arxiv_id": "2506.21845v1",
        "authors": [
            "Zhuodi Cai"
        ],
        "submitted": "2025-06-27 01:33:46",
        "source": "arxiv",
        "comment": "5 pages, 2 figures, 3 tables (containing 21 subfigures)"
    },
    {
        "title": "PARSI: Persian Authorship Recognition via Stylometric Integration",
        "abstract": "The intricate linguistic, stylistic, and metrical aspects of Persian\nclassical poetry pose a challenge for computational authorship attribution. In\nthis work, we present a versatile framework to determine authorship among 67\nprominent poets. We employ a multi-input neural framework consisting of a\ntransformer-based language encoder complemented by features addressing the\nsemantic, stylometric, and metrical dimensions of Persian poetry. Our feature\nset encompasses 100-dimensional Word2Vec embeddings, seven stylometric\nmeasures, and categorical encodings of poetic form and meter. We compiled a\nvast corpus of 647,653 verses of the Ganjoor digital collection, validating the\ndata through strict preprocessing and author verification while preserving\npoem-level splitting to prevent overlap. This work employs verse-level\nclassification and majority and weighted voting schemes in evaluation,\nrevealing that weighted voting yields 71% accuracy. We further investigate\nthreshold-based decision filtering, allowing the model to generate highly\nconfident predictions, achieving 97% accuracy at a 0.9 threshold, though at\nlower coverage. Our work focuses on the integration of deep representational\nforms with domain-specific features for improved authorship attribution. The\nresults illustrate the potential of our approach for automated classification\nand the contribution to stylistic analysis, authorship disputes, and general\ncomputational literature research. This research will facilitate further\nresearch on multilingual author attribution, style shift, and generative\nmodeling of Persian poetry.",
        "url": "http://arxiv.org/abs/2506.21840v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21840v1",
        "arxiv_id": "2506.21840v1",
        "authors": [
            "Kourosh Shahnazari",
            "Mohammadali Keshtparvar",
            "Seyed Moein Ayyoubzadeh"
        ],
        "submitted": "2025-06-27 01:08:52",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "GenEscape: Hierarchical Multi-Agent Generation of Escape Room Puzzles",
        "abstract": "We challenge text-to-image models with generating escape room puzzle images\nthat are visually appealing, logically solid, and intellectually stimulating.\nWhile base image models struggle with spatial relationships and affordance\nreasoning, we propose a hierarchical multi-agent framework that decomposes this\ntask into structured stages: functional design, symbolic scene graph reasoning,\nlayout synthesis, and local image editing. Specialized agents collaborate\nthrough iterative feedback to ensure the scene is visually coherent and\nfunctionally solvable. Experiments show that agent collaboration improves\noutput quality in terms of solvability, shortcut avoidance, and affordance\nclarity, while maintaining visual quality.",
        "url": "http://arxiv.org/abs/2506.21839v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21839v1",
        "arxiv_id": "2506.21839v1",
        "authors": [
            "Mengyi Shan",
            "Brian Curless",
            "Ira Kemelmacher-Shlizerman",
            "Steve Seitz"
        ],
        "submitted": "2025-06-27 01:08:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Exploring the change in scientific readability following the release of ChatGPT",
        "abstract": "The rise and growing popularity of accessible large language models have\nraised questions about their impact on various aspects of life, including how\nscientists write and publish their research. The primary objective of this\npaper is to analyze a dataset consisting of all abstracts posted on arXiv.org\nbetween 2010 and June 7th, 2024, to assess the evolution of their readability\nand determine whether significant shifts occurred following the release of\nChatGPT in November 2022. Four standard readability formulas are used to\ncalculate individual readability scores for each paper, classifying their level\nof readability. These scores are then aggregated by year and across the eight\nprimary categories covered by the platform. The results show a steady annual\ndecrease in readability, suggesting that abstracts are likely becoming\nincreasingly complex. Additionally, following the release of ChatGPT, a\nsignificant change in readability is observed for 2023 and the analyzed months\nof 2024. Similar trends are found across categories, with most experiencing a\nnotable change in readability during 2023 and 2024. These findings offer\ninsights into the broader changes in readability and point to the likely\ninfluence of AI on scientific writing.",
        "url": "http://arxiv.org/abs/2506.21825v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21825v1",
        "arxiv_id": "2506.21825v1",
        "authors": [
            "Abdulkareem Alsudais"
        ],
        "submitted": "2025-06-26 23:57:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Exploring the Structure of AI-Induced Language Change in Scientific English",
        "abstract": "Scientific English has undergone rapid and unprecedented changes in recent\nyears, with words such as \"delve,\" \"intricate,\" and \"crucial\" showing\nsignificant spikes in frequency since around 2022. These changes are widely\nattributed to the growing influence of Large Language Models like ChatGPT in\nthe discourse surrounding bias and misalignment. However, apart from changes in\nfrequency, the exact structure of these linguistic shifts has remained unclear.\nThe present study addresses this and investigates whether these changes involve\nthe replacement of synonyms by suddenly 'spiking words,' for example, \"crucial\"\nreplacing \"essential\" and \"key,\" or whether they reflect broader semantic and\npragmatic qualifications. To further investigate structural changes, we include\npart of speech tagging in our analysis to quantify linguistic shifts over\ngrammatical categories and differentiate between word forms, like \"potential\"\nas a noun vs. as an adjective. We systematically analyze synonym groups for\nwidely discussed 'spiking words' based on frequency trends in scientific\nabstracts from PubMed. We find that entire semantic clusters often shift\ntogether, with most or all words in a group increasing in usage. This pattern\nsuggests that changes induced by Large Language Models are primarily semantic\nand pragmatic rather than purely lexical. Notably, the adjective \"important\"\nshows a significant decline, which prompted us to systematically analyze\ndecreasing lexical items. Our analysis of \"collapsing\" words reveals a more\ncomplex picture, which is consistent with organic language change and contrasts\nwith the patterns of the abrupt spikes. These insights into the structure of\nlanguage change contribute to our understanding of how language technology\ncontinues to shape human language.",
        "url": "http://arxiv.org/abs/2506.21817v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21817v1",
        "arxiv_id": "2506.21817v1",
        "authors": [
            "Riley Galpin",
            "Bryce Anderson",
            "Tom S. Juzek"
        ],
        "submitted": "2025-06-26 23:44:24",
        "source": "arxiv",
        "comment": "Accepted and published at FLAIRS 38. 8 pages, 4 figures, 1 table.\n  Licensed under CC BY-NC-SA 4.0"
    },
    {
        "title": "Towards Transparent AI: A Survey on Explainable Large Language Models",
        "abstract": "Large Language Models (LLMs) have played a pivotal role in advancing\nArtificial Intelligence (AI). However, despite their achievements, LLMs often\nstruggle to explain their decision-making processes, making them a 'black box'\nand presenting a substantial challenge to explainability. This lack of\ntransparency poses a significant obstacle to the adoption of LLMs in\nhigh-stakes domain applications, where interpretability is particularly\nessential. To overcome these limitations, researchers have developed various\nexplainable artificial intelligence (XAI) methods that provide\nhuman-interpretable explanations for LLMs. However, a systematic understanding\nof these methods remains limited. To address this gap, this survey provides a\ncomprehensive review of explainability techniques by categorizing XAI methods\nbased on the underlying transformer architectures of LLMs: encoder-only,\ndecoder-only, and encoder-decoder models. Then these techniques are examined in\nterms of their evaluation for assessing explainability, and the survey further\nexplores how these explanations are leveraged in practical applications.\nFinally, it discusses available resources, ongoing research challenges, and\nfuture directions, aiming to guide continued efforts toward developing\ntransparent and responsible LLMs.",
        "url": "http://arxiv.org/abs/2506.21812v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21812v1",
        "arxiv_id": "2506.21812v1",
        "authors": [
            "Avash Palikhe",
            "Zhenyu Yu",
            "Zichong Wang",
            "Wenbin Zhang"
        ],
        "submitted": "2025-06-26 23:25:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A suite of allotaxonometric tools for the comparison of complex systems using rank-turbulence divergence",
        "abstract": "Describing and comparing complex systems requires principled, theoretically\ngrounded tools. Built around the phenomenon of type turbulence,\nallotaxonographs provide map-and-list visual comparisons of pairs of\nheavy-tailed distributions. Allotaxonographs are designed to accommodate a wide\nrange of instruments including rank- and probability-turbulence divergences,\nJenson-Shannon divergence, and generalized entropy divergences. Here, we\ndescribe a suite of programmatic tools for rendering allotaxonographs for\nrank-turbulence divergence in Matlab, Javascript, and Python, all of which have\ndifferent use cases.",
        "url": "http://arxiv.org/abs/2506.21808v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21808v1",
        "arxiv_id": "2506.21808v1",
        "authors": [
            "Jonathan St-Onge",
            "Ashley M. A. Fehr",
            "Carter Ward",
            "Calla G. Beauregard",
            "Michael V. Arnold",
            "Samuel F. Rosenblatt",
            "Benjamin Cooley",
            "Christopher M. Danforth",
            "Peter Sheridan Dodds"
        ],
        "submitted": "2025-06-26 23:17:29",
        "source": "arxiv",
        "comment": "4 pages, 2 figures"
    },
    {
        "title": "CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation",
        "abstract": "Modeling human behavior in urban environments is fundamental for social\nscience, behavioral studies, and urban planning. Prior work often rely on\nrigid, hand-crafted rules, limiting their ability to simulate nuanced\nintentions, plans, and adaptive behaviors. Addressing these challenges, we\nenvision an urban simulator (CitySim), capitalizing on breakthroughs in\nhuman-level intelligence exhibited by large language models. In CitySim, agents\ngenerate realistic daily schedules using a recursive value-driven approach that\nbalances mandatory activities, personal habits, and situational factors. To\nenable long-term, lifelike simulations, we endow agents with beliefs, long-term\ngoals, and spatial memory for navigation. CitySim exhibits closer alignment\nwith real humans than prior work, both at micro and macro levels. Additionally,\nwe conduct insightful experiments by modeling tens of thousands of agents and\nevaluating their collective behaviors under various real-world scenarios,\nincluding estimating crowd density, predicting place popularity, and assessing\nwell-being. Our results highlight CitySim as a scalable, flexible testbed for\nunderstanding and forecasting urban phenomena.",
        "url": "http://arxiv.org/abs/2506.21805v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21805v1",
        "arxiv_id": "2506.21805v1",
        "authors": [
            "Nicolas Bougie",
            "Narimasa Watanabe"
        ],
        "submitted": "2025-06-26 23:11:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Offensive Language Detection on Social Media Using XLNet",
        "abstract": "The widespread use of text-based communication on social media-through chats,\ncomments, and microblogs-has improved user interaction but has also led to an\nincrease in offensive content, including hate speech, racism, and other forms\nof abuse. Due to the enormous volume of user-generated content, manual\nmoderation is impractical, which creates a need for automated systems that can\ndetect offensive language. Deep learning models, particularly those using\ntransfer learning, have demonstrated significant success in understanding\nnatural language through large-scale pretraining. In this study, we propose an\nautomatic offensive language detection model based on XLNet, a generalized\nautoregressive pretraining method, and compare its performance with BERT\n(Bidirectional Encoder Representations from Transformers), which is a widely\nused baseline in natural language processing (NLP). Both models are evaluated\nusing the Offensive Language Identification Dataset (OLID), a benchmark Twitter\ndataset that includes hierarchical annotations. Our experimental results show\nthat XLNet outperforms BERT in detecting offensive content and in categorizing\nthe types of offenses, while BERT performs slightly better in identifying the\ntargets of the offenses. Additionally, we find that oversampling and\nundersampling strategies are effective in addressing class imbalance and\nimproving classification performance. These findings highlight the potential of\ntransfer learning and XLNet-based architectures to create robust systems for\ndetecting offensive language on social media platforms.",
        "url": "http://arxiv.org/abs/2506.21795v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21795v1",
        "arxiv_id": "2506.21795v1",
        "authors": [
            "Reem Alothman",
            "Hafida Benhidour",
            "Said Kerrache"
        ],
        "submitted": "2025-06-26 22:37:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Evaluating List Construction and Temporal Understanding capabilities of Large Language Models",
        "abstract": "Large Language Models (LLMs) have demonstrated immense advances in a wide\nrange of natural language tasks. However, these models are susceptible to\nhallucinations and errors on particularly temporal understanding tasks\ninvolving multiple entities in answers. In such tasks, they fail to associate\nentities with accurate time intervals, generate a complete list of entities in\nanswers or reason about events associated with specific temporal bounds.\nExisting works do not extensively evaluate the abilities of the model to\nperform implicit and explicit temporal understanding in a list answer\nconstruction setup. To bridge this gap, we propose the Time referenced List\nbased Question Answering or TLQA benchmark that requires structured answers in\nlist format aligned with corresponding time periods. Our TLQA benchmark,\nrequires both list construction and temporal understanding simultaneously,\nwhich to the best of our knowledge has not been explored in prior benchmarks.\nWe investigate the temporal understanding and list construction capabilities of\nstate-of-the-art generative models on TLQA in closed-book and open-domain\nsettings. Our findings reveal significant shortcomings in current models,\nparticularly their inability to provide complete answers and temporally align\nfacts in a closed-book setup and the need to improve retrieval in open-domain\nsetup, providing clear future directions for research on TLQA. The benchmark\nand code at https://github.com/elixir-research-group/TLQA.",
        "url": "http://arxiv.org/abs/2506.21783v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21783v1",
        "arxiv_id": "2506.21783v1",
        "authors": [
            "Alexandru Dumitru",
            "V Venktesh",
            "Adam Jatowt",
            "Avishek Anand"
        ],
        "submitted": "2025-06-26 21:40:58",
        "source": "arxiv",
        "comment": "Accepted at ICTIR 2025 co-located with SIGIR 2025, 11 pages"
    },
    {
        "title": "(Fact) Check Your Bias",
        "abstract": "Automatic fact verification systems increasingly rely on large language\nmodels (LLMs). We investigate how parametric knowledge biases in these models\naffect fact-checking outcomes of the HerO system (baseline for FEVER-25). We\nexamine how the system is affected by: (1) potential bias in Llama 3.1's\nparametric knowledge and (2) intentionally injected bias. When prompted\ndirectly to perform fact-verification, Llama 3.1 labels nearly half the claims\nas \"Not Enough Evidence\". Using only its parametric knowledge it is able to\nreach a verdict on the remaining half of the claims. In the second experiment,\nwe prompt the model to generate supporting, refuting, or neutral fact-checking\ndocuments. These prompts significantly influence retrieval outcomes, with\napproximately 50\\% of retrieved evidence being unique to each perspective.\nNotably, the model sometimes refuses to generate supporting documents for\nclaims it believes to be false, creating an inherent negative bias. Despite\ndifferences in retrieved evidence, final verdict predictions show stability\nacross prompting strategies. The code is available at:\nhttps://github.com/eibakke/FEVER-8-Shared-Task",
        "url": "http://arxiv.org/abs/2506.21745v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21745v1",
        "arxiv_id": "2506.21745v1",
        "authors": [
            "Eivind Morris Bakke",
            "Nora Winger Heggelund"
        ],
        "submitted": "2025-06-26 20:03:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Identifying Speaker Information in Feed-Forward Layers of Self-Supervised Speech Transformers",
        "abstract": "In recent years, the impact of self-supervised speech Transformers has\nextended to speaker-related applications. However, little research has explored\nhow these models encode speaker information. In this work, we address this gap\nby identifying neurons in the feed-forward layers that are correlated with\nspeaker information. Specifically, we analyze neurons associated with k-means\nclusters of self-supervised features and i-vectors. Our analysis reveals that\nthese clusters correspond to broad phonetic and gender classes, making them\nsuitable for identifying neurons that represent speakers. By protecting these\nneurons during pruning, we can significantly preserve performance on\nspeaker-related task, demonstrating their crucial role in encoding speaker\ninformation.",
        "url": "http://arxiv.org/abs/2506.21712v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21712v1",
        "arxiv_id": "2506.21712v1",
        "authors": [
            "Tzu-Quan Lin",
            "Hsi-Chun Cheng",
            "Hung-yi Lee",
            "Hao Tang"
        ],
        "submitted": "2025-06-26 18:54:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ANUBHUTI: A Comprehensive Corpus For Sentiment Analysis In Bangla Regional Languages",
        "abstract": "Sentiment analysis for regional dialects of Bangla remains an underexplored\narea due to linguistic diversity and limited annotated data. This paper\nintroduces ANUBHUTI, a comprehensive dataset consisting of 2000 sentences\nmanually translated from standard Bangla into four major regional dialects\nMymensingh, Noakhali, Sylhet, and Chittagong. The dataset predominantly\nfeatures political and religious content, reflecting the contemporary socio\npolitical landscape of Bangladesh, alongside neutral texts to maintain balance.\nEach sentence is annotated using a dual annotation scheme: multiclass thematic\nlabeling categorizes sentences as Political, Religious, or Neutral, and\nmultilabel emotion annotation assigns one or more emotions from Anger,\nContempt, Disgust, Enjoyment, Fear, Sadness, and Surprise. Expert native\ntranslators conducted the translation and annotation, with quality assurance\nperformed via Cohens Kappa inter annotator agreement, achieving strong\nconsistency across dialects. The dataset was further refined through systematic\nchecks for missing data, anomalies, and inconsistencies. ANUBHUTI fills a\ncritical gap in resources for sentiment analysis in low resource Bangla\ndialects, enabling more accurate and context aware natural language processing.",
        "url": "http://arxiv.org/abs/2506.21686v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21686v1",
        "arxiv_id": "2506.21686v1",
        "authors": [
            "Swastika Kundu",
            "Autoshi Ibrahim",
            "Mithila Rahman",
            "Tanvir Ahmed"
        ],
        "submitted": "2025-06-26 18:13:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Do We Really Need GNNs with Explicit Structural Modeling? MLPs Suffice for Language Model Representations",
        "abstract": "Explicit structural information has been proven to be encoded by Graph Neural\nNetworks (GNNs), serving as auxiliary knowledge to enhance model capabilities\nand improve performance in downstream NLP tasks. However, recent studies\nindicate that GNNs fail to fully utilize structural information, whereas\nMulti-Layer Perceptrons (MLPs), despite lacking the message-passing mechanisms\ninherent to GNNs, exhibit a surprising ability in structure-aware tasks.\nMotivated by these findings, this paper introduces a comprehensive probing\nframework from an information-theoretic perspective. The framework is designed\nto systematically assess the role of explicit structural modeling in enhancing\nlanguage model (LM) representations and to investigate the potential of MLPs as\nefficient and scalable alternatives to GNNs. We extend traditional probing\nclassifiers by incorporating a control module that allows for selective use of\neither the full GNN model or its decoupled components, specifically, the\nmessage-passing and feature-transformation operations.This modular approach\nisolates and assesses the individual contributions of these operations,\navoiding confounding effects from the complete GNN architecture. Using the Edge\nProbing Suite, a diagnostic tool for evaluating the linguistic knowledge\nencoded in LMs, we find that MLPs, when used as feature-transformation modules,\nconsistently improve the linguistic knowledge captured in LM representations\nacross different architectures. They effectively encode both syntactic and\nsemantic patterns. Similarly, GNNs that incorporate feature-transformation\noperations show beneficial effects. In contrast, models that rely solely on\nmessage-passing operations tend to underperform, often leading to negative\nimpacts on probing task performance.",
        "url": "http://arxiv.org/abs/2506.21682v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21682v1",
        "arxiv_id": "2506.21682v1",
        "authors": [
            "Li Zhou",
            "Hao Jiang",
            "Junjie Li",
            "Zefeng Zhao",
            "Feng Jiang",
            "Wenyu Chen",
            "Haizhou Li"
        ],
        "submitted": "2025-06-26 18:10:28",
        "source": "arxiv",
        "comment": "Graph Neural Networks, Multi-Layer Perceptrons, Explicit Structural\n  Modeling, Probing Classifier"
    },
    {
        "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs",
        "abstract": "Current Vision-Language Models (VLMs) struggle with fine-grained spatial\nreasoning, particularly when multi-step logic and precise spatial alignment are\nrequired. In this work, we introduce SpatialReasoner-R1, a vision-language\nreasoning model designed to address these limitations. To construct\nhigh-quality supervision for spatial reasoning, we design a Multi-Model Monte\nCarlo Tree Search (M3CTS) method that generates diverse, logically consistent\nLong Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose\nfine-grained Direct Preference Optimization (fDPO), which introduces\nsegment-specific preference granularity for descriptive grounding and logical\nreasoning, guided by a spatial reward mechanism that evaluates candidate\nresponses based on visual consistency, spatial grounding, and logical\ncoherence. Experimental results demonstrate that fDPO achieves an average\nimprovement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%\ngain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a\nnew SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in\naverage accuracy, while maintaining competitive performance on general\nvision-language tasks.",
        "url": "http://arxiv.org/abs/2506.21656v1",
        "pdf_url": "http://arxiv.org/pdf/2506.21656v1",
        "arxiv_id": "2506.21656v1",
        "authors": [
            "Yifan Shen",
            "Yuanzhe Liu",
            "Jingyuan Zhu",
            "Xu Cao",
            "Xiaofeng Zhang",
            "Yixiao He",
            "Wenming Ye",
            "James Matthew Rehg",
            "Ismini Lourentzou"
        ],
        "submitted": "2025-06-26 18:00:00",
        "source": "arxiv",
        "comment": "29 pages"
    }
]