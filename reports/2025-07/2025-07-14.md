# Daily Papers Report - 2025-07-14

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Balancing Semantic Relevance and Engagement in Related Video Recommendations

- **LLM Score**: 8
- **Keyword Score**: 6
- **Authors**: Amit Jaspal, Feng Zhang, Wei Chang, Sumit Kumar, Yubo Wang, Roni Mittleman, Qifan Wang, Weize Mao
- **URL**: <http://arxiv.org/abs/2507.09403v1>
- **Submitted**: 2025-07-12 21:04:25
- **Topic Keywords**: relevance, retrieval, recommend
- **Reason**: The paper's focus on related video recommendations, query understanding, and ranking models aligns with your interests in Information Retrieval and Search technologies. The use of multi-objective retrieval framework, multi-task learning, and off-policy correction to balance semantic relevance and user engagement is also relevant to your research themes. However, the paper's specific application to video recommendations and the use of multimodal content features may not be directly applicable to your e-commerce background.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Multi-Objective Retrieval Framework for Related Video Recommendations
- **Aim**: Balance semantic relevance and user engagement in video recommendations
- **Rationale**: Combining multi-task learning, multimodal content features, and off-policy correction to mitigate popularity bias and enhance semantic representation
- **Ground**: Conventional item-to-item retrieval framework with co-engagement prediction and semantic relevance tasks
- **Experiment**: Evaluation on a large-scale video recommendation dataset with four models and an online A/B test
- **Takeaway**: The proposed framework achieves significant improvements in semantic relevance, reduction in popular item distribution, and increased user engagement, demonstrating its effectiveness in enhancing semantic coherence and relevance

#### Abstract
> Related video recommendations commonly use collaborative filtering (CF)
driven by co-engagement signals, often resulting in recommendations lacking
semantic coherence and exhibiting strong popularity bias. This paper introduces
a novel multi-objective retrieval framework, enhancing standard two-tower
models to explicitly balance semantic relevance and user engagement. Our
approach uniquely combines: (a) multi-task learning (MTL) to jointly optimize
co-engagement and semantic relevance, explicitly prioritizing topical
coherence; (b) fusion of multimodal content features (textual and visual
embeddings) for richer semantic understanding; and (c) off-policy correction
(OPC) via inverse propensity weighting to effectively mitigate popularity bias.
Evaluation on industrial-scale data and a two-week live A/B test reveals our
framework's efficacy. We observed significant improvements in semantic
relevance (from 51% to 63% topic match rate), a reduction in popular item
distribution (-13.8% popular video recommendations), and a +0.04% improvement
in our topline user engagement metric. Our method successfully achieves better
semantic coherence, balanced engagement, and practical scalability for
real-world deployment.

---

### 2. PRISM: Fine-Grained Paper-to-Paper Retrieval with Multi-Aspect-Aware Query Optimization

- **LLM Score**: 7
- **Keyword Score**: 10
- **Authors**: Sangwoo Park, Jinheon Baek, Soyeong Jeong, Sung Ju Hwang
- **URL**: <http://arxiv.org/abs/2507.10057v1>
- **Submitted**: 2025-07-14 08:41:53
- **Topic Keywords**: query, queries, rag, retrieval
- **Reason**: The paper is relevant to Information Retrieval (IR) and Search technologies, specifically in the area of document-to-document retrieval, which aligns with the user's interests. The use of multi-aspect-aware query optimization and fine-grained representations for both query and candidate papers is also related to the user's focus on query understanding and ranking models. However, the paper's focus on scientific paper retrieval and its application to a specific domain (SciFullBench) limits its relevance to the user's broader interests in e-commerce and NLP.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Scientific Paper Retrieval
- **Aim**: To develop a novel document-to-document retrieval method that improves scientific paper retrieval by considering multiple, fine-grained representations of both query and candidate papers
- **Rationale**: Previous approaches focused on abstract-level representations are suboptimal for capturing the full context of papers and achieving better performance on tasks such as identifying complementary work and generating literature reviews
- **Ground**: PRISM decomposes the query paper into multiple aspect-specific views, which are then embedded and matched against candidate papers segmented into multifaceted dimensions
- **Experiment**: The authors evaluate PRISM on SCIFULLBENCH and demonstrate that it outperforms existing abstract-level retrieval approaches and those specific to paper retrieval domains substantially
- **Takeaway**: PRISM achieves a significant average improvement of 7% even when using an off-the-shelf retriever, and its adaptive generation of multiple queries predicted on various aspects of scientific papers results in retrieving more contextually diverse papers

#### Abstract
> Scientific paper retrieval, particularly framed as document-to-document
retrieval, aims to identify relevant papers in response to a long-form query
paper, rather than a short query string. Previous approaches to this task have
focused on abstracts, embedding them into dense vectors as surrogates for full
documents and calculating similarity across them, although abstracts provide
only sparse and high-level summaries. To address this, we propose PRISM, a
novel document-to-document retrieval method that introduces multiple,
fine-grained representations for both the query and candidate papers. In
particular, each query paper is decomposed into multiple aspect-specific views
and individually embedded, which are then matched against candidate papers
similarity segmented to consider their multifaceted dimensions. Moreover, we
present SciFullBench, a novel benchmark in which the complete and segmented
context of full papers for both queries and candidates is available. Then,
experimental results show that PRISM improves performance by an average of 4.3%
over existing retrieval baselines.

---

### 3. Criteria-Based LLM Relevance Judgments

- **LLM Score**: 6
- **Keyword Score**: 17
- **Authors**: Naghmeh Farzi, Laura Dietz
- **URL**: <http://arxiv.org/abs/2507.09488v1>
- **Submitted**: 2025-07-13 04:21:21
- **Comment**: 10 pages, 3 figures, accepted to ICTIR 2025
- **Topic Keywords**: information retrieval, ranking, relevance, rag, retrieval, rank, search, trec
- **Reason**: The paper explores the use of Large Language Models (LLMs) for relevance judgments in information retrieval, which aligns with your interest in search technologies and query understanding. However, the focus on LLMs and relevance judgments is more specific and doesn't directly relate to your interests in ranking models, user behavior modeling, and deep semantic understanding.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Multi-Criteria framework for Large Language Model (LLM)-based relevance judgments
- **Aim**: To develop an approach that evaluates information retrieval systems and predicts relevance labels similar to human-annotated judgments
- **Rationale**: Decomposing the notion of relevance into four criteria: exactness, coverage, topicality, and contextual fit, and combining them to derive a final relevance label
- **Ground**: Validated on three datasets: TREC Deep Learning tracks from 2019 and 2020, and LLMJudge (based on TREC DL 2023)
- **Experiment**: Compared the proposed approach with established LLM-evaluation methods such as UMBRELA and RUBRIC, and explored variations using sum-based and Naive Bayes aggregations
- **Takeaway**: The Multi-Criteria approach outperforms many large-scale LLMs and established baselines, achieving high-quality judgments using smaller language models and demonstrating its effectiveness in evaluating the relevance of passages to queries

#### Abstract
> Relevance judgments are crucial for evaluating information retrieval systems,
but traditional human-annotated labels are time-consuming and expensive. As a
result, many researchers turn to automatic alternatives to accelerate method
development. Among these, Large Language Models (LLMs) provide a scalable
solution by generating relevance labels directly through prompting. However,
prompting an LLM for a relevance label without constraints often results in not
only incorrect predictions but also outputs that are difficult for humans to
interpret. We propose the Multi-Criteria framework for LLM-based relevance
judgments, decomposing the notion of relevance into multiple criteria--such as
exactness, coverage, topicality, and contextual fit--to improve the robustness
and interpretability of retrieval evaluations compared to direct grading
methods. We validate this approach on three datasets: the TREC Deep Learning
tracks from 2019 and 2020, as well as LLMJudge (based on TREC DL 2023). Our
results demonstrate that Multi-Criteria judgments enhance the system
ranking/leaderboard performance. Moreover, we highlight the strengths and
limitations of this approach relative to direct grading approaches, offering
insights that can guide the development of future automatic evaluation
frameworks in information retrieval.

---

### 4. Am I on the Right Track? What Can Predicted Query Performance Tell Us about the Search Behaviour of Agentic RAG

- **LLM Score**: 6
- **Keyword Score**: 14
- **Authors**: Fangzheng Tian, Jinyuan Fang, Debasis Ganguly, Zaiqiao Meng, Craig Macdonald
- **URL**: <http://arxiv.org/abs/2507.10411v1>
- **Submitted**: 2025-07-14 15:54:50
- **Topic Keywords**: retriever, query, queries, rag, retrieval, search
- **Reason**: The paper explores the application of query performance prediction (QPP) in Agentic Retrieval-Augmented Generation (RAG) models, which is related to ranking models and query understanding in Information Retrieval. However, the focus on RAG and its specific applications in question answering and adaptive retrieval is not directly aligned with the user's primary interests in e-commerce and real-time relevance optimization.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Agentic Retrieval-Augmented Generation (RAG) models
- **Aim**: To introduce a novel paradigm that enables a reasoning model to dynamically decide when to invoke a retriever to obtain external information when answering a question
- **Rationale**: To address the limitations of traditional RAG models, which apply a uniform retrieval strategy to all input questions, leading to suboptimal performance in tasks that require complex or multi-step information gathering
- **Ground**: The authors investigate the utility of Query Performance Prediction (QPP) methods in the querying behavior of two Agentic RAG models, Search-R1 and R1-Searcher, and explore different retrieval strategies
- **Experiment**: The experiments are conducted on the Natural Questions dataset, and the results are evaluated using F1 and Exact Match metrics
- **Takeaway**: The paper demonstrates the potential of Agentic RAG models in improving answer quality and reducing the length of the reasoning process, and highlights the importance of QPP methods in enhancing the performance of these models

#### Abstract
> Agentic Retrieval-Augmented Generation (RAG) is a new paradigm where the
reasoning model decides when to invoke a retriever (as a "tool") when answering
a question. This paradigm, exemplified by recent research works such as
Search-R1, enables the model to decide when to search and obtain external
information. However, the queries generated by such Agentic RAG models and the
role of the retriever in obtaining high-quality answers remain understudied. To
this end, this initial study examines the applicability of query performance
prediction (QPP) within the recent Agentic RAG models Search-R1 and
R1-Searcher. We find that applying effective retrievers can achieve higher
answer quality within a shorter reasoning process. Moreover, the QPP estimates
of the generated queries, used as an approximation of their retrieval quality,
are positively correlated with the quality of the final answer. Ultimately, our
work is a step towards adaptive retrieval within Agentic RAG, where QPP is used
to inform the model if the retrieved results are likely to be useful.

---

### 5. RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking

- **LLM Score**: 6
- **Keyword Score**: 12
- **Authors**: Shuo Yang, Zijian Yu, Zhenzhe Ying, Yuqin Dai, Guoqing Wang, Jun Lan, Jinfeng Xu, Jinze Li, Edith C. H. Ngai
- **URL**: <http://arxiv.org/abs/2507.09174v1>
- **Submitted**: 2025-07-12 07:46:51
- **Topic Keywords**: query, queries, rag, retrieval, web search, search
- **Reason**: The paper introduces a novel framework for misinformation detection in multimodal fact-checking, leveraging web search queries and multi-agent reasoning. While it touches on query formulation and evidence aggregation, the focus is more on fact-checking and verification rather than traditional information retrieval and search technologies. The paper's relevance to the user's interests is somewhat related, but not a central match.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Multimodal Misinformation Detection
- **Aim**: To develop a novel Retrieval-Augmented Multi-Agent Framework (RAMA) for verifying multimedia misinformation
- **Rationale**: To address the challenges of automated fact-checking systems, particularly when claims are ambiguous or lack sufficient context
- **Ground**: Integrating web-based evidence retrieval with multi-agent reasoning, comprising three core components: WebRetriever, VLJudge, and DecisionFuser
- **Experiment**: Evaluating RAMA using the official datasets provided by the Multimedia Verification Challenge, achieving competitive results without any training or fine-tuning
- **Takeaway**: RAMA achieves superior performance on benchmark datasets, especially in resolving ambiguous or improbable claims by grounding verification in retrieved factual evidence

#### Abstract
> The rapid proliferation of multimodal misinformation presents significant
challenges for automated fact-checking systems, especially when claims are
ambiguous or lack sufficient context. We introduce RAMA, a novel
retrieval-augmented multi-agent framework designed for verifying multimedia
misinformation. RAMA incorporates three core innovations: (1) strategic query
formulation that transforms multimodal claims into precise web search queries;
(2) cross-verification evidence aggregation from diverse, authoritative
sources; and (3) a multi-agent ensemble architecture that leverages the
complementary strengths of multiple multimodal large language models and prompt
variants. Extensive experiments demonstrate that RAMA achieves superior
performance on benchmark datasets, particularly excelling in resolving
ambiguous or improbable claims by grounding verification in retrieved factual
evidence. Our findings underscore the necessity of integrating web-based
evidence and multi-agent reasoning for trustworthy multimedia verification,
paving the way for more reliable and scalable fact-checking solutions. RAMA
will be publicly available at https://github.com/kalendsyang/RAMA.git.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking

- **LLM Score**: 6
- **Keyword Score**: 6
- **Authors**: Hai Toan Nguyen, Tien Dat Nguyen, Viet Ha Nguyen
- **URL**: <http://arxiv.org/abs/2507.09935v1>
- **Submitted**: 2025-07-14 05:21:58
- **Topic Keywords**: rag, retrieval augmented generation, retrieval
- **Reason**: The paper discusses Retrieval-Augmented Generation (RAG) and proposes a novel framework that integrates hierarchical text segmentation and clustering to generate more meaningful chunks. While it's related to information retrieval and natural language processing, the focus is on enhancing RAG rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest for you. The paper's relevance is somewhat diminished by its focus on large language models and chunking strategies, which are not directly aligned with your research themes.

#### Abstract
> Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies
for retrieval, which enhance large language models (LLMs) by enabling them to
access external knowledge, ensuring that the retrieved information is
up-to-date and domain-specific. However, traditional methods often fail to
create chunks that capture sufficient semantic meaning, as they do not account
for the underlying textual structure. This paper proposes a novel framework
that enhances RAG by integrating hierarchical text segmentation and clustering
to generate more meaningful and semantically coherent chunks. During inference,
the framework retrieves information by leveraging both segment-level and
cluster-level vector representations, thereby increasing the likelihood of
retrieving more precise and contextually relevant information. Evaluations on
the NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method
achieved improved results compared to traditional chunking techniques.

### 7. Retrieval-Augmented Recommendation Explanation Generation with Hierarchical Aggregation

- **LLM Score**: 6
- **Keyword Score**: 6
- **Authors**: Bangcheng Sun, Yazhe Chen, Jilin Yang, Xiaodong Li, Hui Li
- **URL**: <http://arxiv.org/abs/2507.09188v1>
- **Submitted**: 2025-07-12 08:15:05
- **Topic Keywords**: queries, retrieval, recommend
- **Reason**: The paper explores Explainable Recommender Systems, which is related to Information Retrieval and Search technologies. The use of hierarchical aggregation and pseudo-document queries is innovative, but the focus on recommender systems and explanation generation is not directly aligned with the user's primary interest in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Explainable Recommender System (ExRec) provides transparency to the
recommendation process, increasing users' trust and boosting the operation of
online services. With the rise of large language models (LLMs), whose extensive
world knowledge and nuanced language understanding enable the generation of
human-like, contextually grounded explanations, LLM-powered ExRec has gained
great momentum. However, existing LLM-based ExRec models suffer from profile
deviation and high retrieval overhead, hindering their deployment. To address
these issues, we propose Retrieval-Augmented Recommendation Explanation
Generation with Hierarchical Aggregation (REXHA). Specifically, we design a
hierarchical aggregation based profiling module that comprehensively considers
user and item review information, hierarchically summarizing and constructing
holistic profiles. Furthermore, we introduce an efficient retrieval module
using two types of pseudo-document queries to retrieve relevant reviews to
enhance the generation of recommendation explanations, effectively reducing
retrieval latency and improving the recall of relevant reviews. Extensive
experiments demonstrate that our method outperforms existing approaches by up
to 12.6% w.r.t. the explanation quality while achieving high retrieval
efficiency.

### 8. DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology

- **LLM Score**: 6
- **Keyword Score**: 3
- **Authors**: Jennifer D'Souza, Endres Keno Sander, Andrei Aioanei
- **URL**: <http://arxiv.org/abs/2507.10522v1>
- **Submitted**: 2025-07-14 17:47:28
- **Comment**: 12 pages, 3 figures
- **Topic Keywords**: retrieval, search
- **Reason**: The paper introduces a novel system for automated scientific synthesis, which is related to information retrieval and search technologies. While it doesn't directly focus on query understanding, ranking models, or user behavior modeling, it explores the integration of domain-specific evidence and maintains analytical rigor, which are relevant to information retrieval. However, the paper's focus on ecology and scientific question answering is not directly aligned with the user's primary research interests in e-commerce and deep semantic understanding.

#### Abstract
> We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system
for automated scientific synthesis that supports recursive, depth- and
breadth-controlled exploration of original research questions -- enhancing
search diversity and nuance in the retrieval of relevant scientific literature.
Unlike conventional retrieval-augmented generation pipelines, DeepResearch
enables user-controllable synthesis with transparent reasoning and
parameter-driven configurability, facilitating high-throughput integration of
domain-specific evidence while maintaining analytical rigor. Applied to 49
ecological research questions, DeepResearch achieves up to a 21-fold increase
in source integration and a 14.9-fold rise in sources integrated per 1,000
words. High-parameter settings yield expert-level analytical depth and
contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.

### 9. Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces

- **LLM Score**: 6
- **Keyword Score**: 2
- **Authors**: Baturay Saglam, Paul Kassianik, Blaine Nelson, Sajana Weerawardhena, Yaron Singer, Amin Karbasi
- **URL**: <http://arxiv.org/abs/2507.09709v1>
- **Submitted**: 2025-07-13 17:03:25
- **Topic Keywords**: rag
- **Reason**: The paper explores the latent space geometry of large language models, which is somewhat related to my interests in query understanding and ranking models. However, the focus on language models and their internal organization is not directly aligned with my primary focus on information retrieval and search technologies. The paper's findings on separability and causal interventions in hidden space could potentially be applied to IR, but the connection is not immediately clear.

#### Abstract
> Understanding the latent space geometry of large language models (LLMs) is
key to interpreting their behavior and improving alignment. \baturay{However,
it remains unclear to what extent LLMs internally organize representations
related to semantic understanding. To investigate this, we conduct a
large-scale empirical study of hidden states in transformer-based LLMs,
analyzing 11 decoder-only models across 6 scientific topics and 12 layers each.
We find that high-level semantic information consistently lies in
low-dimensional subspaces that form linearly separable representations across
distinct domains. This separability becomes more pronounced in deeper layers
and under prompts that trigger structured reasoning or alignment
behaviors$\unicode{x2013}$even when surface content is unchanged. This geometry
enables simple yet effective causal interventions in hidden space; for example,
reasoning patterns like chain-of-thought can be captured by a single vector
direction. Together, these findings support the development of geometry-aware
tools that operate directly on latent representations to detect and mitigate
harmful or adversarial content, using methods such as transport-based defenses
that leverage this separability. As a proof of concept, we demonstrate this
potential by training a simple MLP classifier as a lightweight latent-space
guardrail, which detects adversarial and malicious prompts with high precision.

### 10. User Long-Term Multi-Interest Retrieval Model for Recommendation

- **LLM Score**: 4
- **Keyword Score**: 16
- **Authors**: Yue Meng, Cheng Guo, Xiaohui Hu, Honghu Deng, Yi Cao, Tong Liu, Bo Zheng
- **URL**: <http://arxiv.org/abs/2507.10097v1>
- **Submitted**: 2025-07-14 09:32:26
- **Topic Keywords**: ranking, ltr, rag, user behavior, click, retrieval, recommend, rank
- **Reason**: The paper focuses on user behavior sequence modeling and recommendation systems, which is related to information retrieval and user behavior modeling. However, the specific approach and techniques used, such as hierarchical dual-interest learning and pointer-enhanced cascaded category-to-item retrieval, are not directly aligned with the user's interests in query understanding, ranking models, and deep semantic understanding.

#### Abstract
> User behavior sequence modeling, which captures user interest from rich
historical interactions, is pivotal for industrial recommendation systems.
Despite breakthroughs in ranking-stage models capable of leveraging ultra-long
behavior sequences with length scaling up to thousands, existing retrieval
models remain constrained to sequences of hundreds of behaviors due to two main
challenges. One is strict latency budget imposed by real-time service over
large-scale candidate pool. The other is the absence of target-aware mechanisms
and cross-interaction architectures, which prevent utilizing ranking-like
techniques to simplify long sequence modeling. To address these limitations, we
propose a new framework named User Long-term Multi-Interest Retrieval
Model(ULIM), which enables thousand-scale behavior modeling in retrieval
stages. ULIM includes two novel components: 1)Category-Aware Hierarchical
Dual-Interest Learning partitions long behavior sequences into multiple
category-aware subsequences representing multi-interest and jointly optimizes
long-term and short-term interests within specific interest cluster.
2)Pointer-Enhanced Cascaded Category-to-Item Retrieval introduces
Pointer-Generator Interest Network(PGIN) for next-category prediction, followed
by next-item retrieval upon the top-K predicted categories. Comprehensive
experiments on Taobao dataset show that ULIM achieves substantial improvement
over state-of-the-art methods, and brings 5.54% clicks, 11.01% orders and 4.03%
GMV lift for Taobaomiaosha, a notable mini-app of Taobao.

### 11. CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Hongchao Jiang, Yiming Chen, Yushi Cao, Hung-yi Lee, Robby T. Tan
- **URL**: <http://arxiv.org/abs/2507.10535v1>
- **Submitted**: 2025-07-14 17:56:29
- **Comment**: Dataset is available at
  https://huggingface.co/datasets/mattymchen/codejudgebench
- **Topic Keywords**: queries, ranking, pairwise, rank
- **Reason**: The paper explores the use of Large Language Models (LLMs) as judges for coding tasks, which is related to information retrieval and search technologies. However, the focus is on LLMs' ability to assess and compare code quality, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's relevance is somewhat limited, but it does touch on aspects of natural language processing and data mining.

#### Abstract
> Large Language Models (LLMs) have significantly advanced the state-of-the-art
in various coding tasks. Beyond directly answering user queries, LLMs can also
serve as judges, assessing and comparing the quality of responses generated by
other models. Such an evaluation capability is crucial both for benchmarking
different LLMs and for improving response quality through response ranking.
However, despite the growing adoption of the LLM-as-a-Judge paradigm, its
effectiveness in coding scenarios remains underexplored due to the absence of
dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a
benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge
models across three critical coding tasks: code generation, code repair, and
unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge
models, we find that recent thinking models significantly outperform
non-thinking models on our carefully designed code judging tasks. Notably, even
relatively small thinking models, such as Qwen3-8B, can outperform specially
trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still
exhibit significant randomness in their judgment of coding tasks. For pairwise
judging tasks, simply changing the order in which responses are presented can
substantially impact accuracy. In addition, when judging code and unit tests
written by different LLMs, LLM-as-a-Judge models also show variance in
performance. This sensitivity raises concerns about the reliability and
consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal
prompting strategies for LLM-as-a-Judge. We find that using pair-wise
comparison outperforms scalar point-wise judging. Furthermore, retaining
comments and reasoning in the full, unprocessed LLM response leads to improved
judge performance.

### 12. Non-parametric Graph Convolution for Re-ranking in Recommendation Systems

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Zhongyu Ouyang, Mingxuan Ju, Soroush Vosoughi, Yanfang Ye
- **URL**: <http://arxiv.org/abs/2507.09969v1>
- **Submitted**: 2025-07-14 06:35:18
- **Comment**: Accepted to RecSys2025 Main
- **Topic Keywords**: ranking, rag, retrieval, recommend, rank, recsys
- **Reason**: The paper focuses on recommender systems, which is a related topic, but it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The use of graph convolution for re-ranking is an innovative approach, but it is not directly applicable to information retrieval or search technologies.

#### Abstract
> Graph knowledge has been proven effective in enhancing item rankings in
recommender systems (RecSys), particularly during the retrieval stage. However,
its application in the ranking stage, especially when richer contextual
information in user-item interactions is available, remains underexplored. A
major challenge lies in the substantial computational cost associated with
repeatedly retrieving neighborhood information from billions of items stored in
distributed systems. This resource-intensive requirement makes it difficult to
scale graph-based methods in practical RecSys. To bridge this gap, we first
demonstrate that incorporating graphs in the ranking stage improves ranking
qualities. Notably, while the improvement is evident, we show that the
substantial computational overheads entailed by graphs are prohibitively
expensive for real-world recommendations. In light of this, we propose a
non-parametric strategy that utilizes graph convolution for re-ranking only
during test time. Our strategy circumvents the notorious computational
overheads from graph convolution during training, and utilizes structural
knowledge hidden in graphs on-the-fly during testing. It can be used as a
plug-and-play module and easily employed to enhance the ranking ability of
various ranking layers of a real-world RecSys with significantly reduced
computational overhead. Through comprehensive experiments across four benchmark
datasets with varying levels of sparsity, we demonstrate that our strategy
yields noticeable improvements (i.e., 8.1% on average) during testing time with
little to no additional computational overheads (i.e., 0.5 on average). Code:
https://github.com/zyouyang/RecSys2025_NonParamGC.git

### 13. EventHunter: Dynamic Clustering and Ranking of Security Events from Hacker Forum Discussions

- **LLM Score**: 4
- **Keyword Score**: 9
- **Authors**: Yasir Ech-Chammakhy, Anas Motii, Anass Rabii, Jaafar Chbili
- **URL**: <http://arxiv.org/abs/2507.09762v1>
- **Submitted**: 2025-07-13 19:40:36
- **Comment**: Accepted for publication at the 28th International Symposium on
  Research in Attacks, Intrusions, and Defenses (RAID 2025)
- **Topic Keywords**: ranking, relevance, rag, rank
- **Reason**: The paper focuses on event detection and ranking in hacker forum discussions, which is related to information retrieval and search technologies. However, the specific context of cybersecurity and threat detection is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Hacker forums provide critical early warning signals for emerging
cybersecurity threats, but extracting actionable intelligence from their
unstructured and noisy content remains a significant challenge. This paper
presents an unsupervised framework that automatically detects, clusters, and
prioritizes security events discussed across hacker forum posts. Our approach
leverages Transformer-based embeddings fine-tuned with contrastive learning to
group related discussions into distinct security event clusters, identifying
incidents like zero-day disclosures or malware releases without relying on
predefined keywords. The framework incorporates a daily ranking mechanism that
prioritizes identified events using quantifiable metrics reflecting timeliness,
source credibility, information completeness, and relevance. Experimental
evaluation on real-world hacker forum data demonstrates that our method
effectively reduces noise and surfaces high-priority threats, enabling security
analysts to mount proactive responses. By transforming disparate hacker forum
discussions into structured, actionable intelligence, our work addresses
fundamental challenges in automated threat detection and analysis.

### 14. Riding the Carousel: The First Extensive Eye Tracking Analysis of Browsing Behavior in Carousel Recommenders

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Santiago de Leon-Martinez, Robert Moro, Branislav Kveton, Maria Bielikova
- **URL**: <http://arxiv.org/abs/2507.10135v1>
- **Submitted**: 2025-07-14 10:26:27
- **Topic Keywords**: rag, user behavior, recommend, rank, search
- **Reason**: The paper focuses on carousel recommenders, which is a specific type of interface, and explores user behavior through eye tracking. While it touches on recommender systems, it doesn't directly relate to query understanding, ranking models, or user behavior modeling in the context of information retrieval. The paper's scope is more focused on user experience and interface design, which is somewhat relevant to the user's interests in search technologies and NLP.

#### Abstract
> Carousels have become the de-facto interface in online services. However,
there is a lack of research in carousels, particularly examining how
recommender systems may be designed differently than the traditional
single-list interfaces. One of the key elements for understanding how to design
a system for a particular interface is understanding how users browse. For
carousels, users may browse in a number of different ways due to the added
complexity of multiple topic defined-lists and swiping to see more items.
  Eye tracking is the key to understanding user behavior by providing valuable,
direct information on how users see and navigate. In this work, we provide the
first extensive analysis of the eye tracking behavior in carousel recommenders
under the free-browsing setting. To understand how users browse, we examine the
following research questions : 1) where do users start browsing, 2) how do
users transition from item to item within the same carousel and across
carousels, and 3) how does genre preference impact transitions?
  This work addresses a gap in the field and provides the first extensive
empirical results of eye tracked browsing behavior in carousels for improving
recommenders. Taking into account the insights learned from the above
questions, our final contribution is to provide suggestions to help carousel
recommender system designers optimize their systems for user browsing behavior.
The most important suggestion being to reorder the ranked item positions to
account for browsing after swiping.These contributions aim not only to help
improve current systems, but also to encourage and allow the design of new user
models, systems, and metrics that are better suited to the complexity of
carousel interfaces.

### 15. Knowledge Conceptualization Impacts RAG Efficacy

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Chris Davis Jaldi, Anmol Saini, Elham Ghiasi, O. Divine Eziolise, Cogan Shimizu
- **URL**: <http://arxiv.org/abs/2507.09389v1>
- **Submitted**: 2025-07-12 20:10:26
- **Topic Keywords**: query, rag, retrieval
- **Reason**: The paper explores the design of transferable and interpretable neurosymbolic AI systems, which is a related topic in NLP. However, the focus on knowledge conceptualization and its impact on RAG systems is not directly aligned with my research interests in Information Retrieval, query understanding, and ranking models.

#### Abstract
> Explainability and interpretability are cornerstones of frontier and
next-generation artificial intelligence (AI) systems. This is especially true
in recent systems, such as large language models (LLMs), and more broadly,
generative AI. On the other hand, adaptability to new domains, contexts, or
scenarios is also an important aspect for a successful system. As such, we are
particularly interested in how we can merge these two efforts, that is,
investigating the design of transferable and interpretable neurosymbolic AI
systems. Specifically, we focus on a class of systems referred to as ''Agentic
Retrieval-Augmented Generation'' systems, which actively select, interpret, and
query knowledge sources in response to natural language prompts. In this paper,
we systematically evaluate how different conceptualizations and representations
of knowledge, particularly the structure and complexity, impact an AI agent (in
this case, an LLM) in effectively querying a triplestore. We report our
results, which show that there are impacts from both approaches, and we discuss
their impact and implications.

### 16. Multiple Choice Learning of Low Rank Adapters for Language Modeling

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Victor Letzelter, Hugo Malard, Mathieu Fontaine, Ga√´l Richard, Slim Essid, Andrei Bursuc, Patrick P√©rez
- **URL**: <http://arxiv.org/abs/2507.10419v1>
- **Submitted**: 2025-07-14 16:00:51
- **Topic Keywords**: relevance, rag, rank
- **Reason**: The paper proposes a novel approach to language modeling, leveraging Multiple Choice Learning and Low-Rank Adaptation to handle ambiguity in sentence continuations. While it touches on some aspects of query understanding and ranking models, it is primarily focused on language modeling and does not directly relate to user behavior modeling or real-time relevance optimization. The paper's relevance to information retrieval is limited, but it may be of interest to those working in NLP.

#### Abstract
> We propose LoRA-MCL, a training scheme that extends next-token prediction in
language models with a method designed to decode diverse, plausible sentence
continuations at inference time. Traditional language modeling is an
intrinsically ill-posed problem: given a context, multiple futures may be
equally plausible. Our approach leverages Multiple Choice Learning (MCL) and
the Winner-Takes-All (WTA) loss to efficiently handle ambiguity through
Low-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying
Multiple Choice Learning to Language Modeling, assuming the data is generated
from a mixture of distributions. To illustrate the proposed approach, we use
data sampled from mixtures of Markov chains. We then demonstrate with extensive
experiments on real-world visual and audio captioning tasks that our method
achieves high diversity and relevance in generated outputs.

### 17. GraphRunner: A Multi-Stage Framework for Efficient and Accurate Graph-Based Retrieval

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Savini Kashmira, Jayanaka L. Dantanarayana, Kriszti√°n Flautner, Lingjia Tang, Jason Mars
- **URL**: <http://arxiv.org/abs/2507.08945v1>
- **Submitted**: 2025-07-11 18:10:01
- **Topic Keywords**: rag, retrieval augmented generation, retrieval
- **Reason**: The paper focuses on graph-based retrieval, which is not directly related to the user's primary interest in Information Retrieval (IR) and Search technologies. While the paper mentions query understanding and ranking models, it does not explicitly address these topics. The user's background in e-commerce and interest in real-time relevance optimization are not directly relevant to this paper.

#### Abstract
> Conventional Retrieval Augmented Generation (RAG) approaches are common in
text-based applications. However, they struggle with structured, interconnected
datasets like knowledge graphs, where understanding underlying relationships is
crucial for accurate retrieval. A common direction in graph-based retrieval
employs iterative, rule-based traversal guided by Large Language Models (LLMs).
Such existing iterative methods typically combine reasoning with single hop
traversal at each step, making them vulnerable to LLM reasoning errors and
hallucinations that ultimately hinder the retrieval of relevant information.
  To address these limitations, we propose GraphRunner, a novel graph-based
retrieval framework that operates in three distinct stages: planning,
verification, and execution. This introduces high-level traversal actions that
enable multi-hop exploration in a single step. It also generates a holistic
traversal plan, which is verified against the graph structure and pre-defined
traversal actions, reducing reasoning errors and detecting hallucinations
before execution. GraphRunner significantly reduces LLM reasoning errors and
detects hallucinations through validation. Our evaluation using the GRBench
dataset shows that GraphRunner consistently outperforms existing approaches,
achieving 10-50% performance improvements over the strongest baseline while
reducing inference cost by 3.0-12.9x and response generation time by 2.5-7.1x,
making it significantly more robust and efficient for graph-based retrieval
tasks.

### 18. Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Yangning Li, Weizhi Zhang, Yuyao Yang, Wei-Chieh Huang, Yaozu Wu, Junyu Luo, Yuanchen Bei, Henry Peng Zou, Xiao Luo, Yusheng Zhao, Chunkit Chan, Yankai Chen, Zhongfen Deng, Yinghui Li, Hai-Tao Zheng, Dongyuan Li, Renhe Jiang, Ming Zhang, Yangqiu Song, Philip S. Yu
- **URL**: <http://arxiv.org/abs/2507.09477v1>
- **Submitted**: 2025-07-13 03:29:41
- **Comment**: submitted to ARR May
- **Topic Keywords**: rag, retrieval, search
- **Reason**: The paper discusses Retrieval-Augmented Generation (RAG) and its relation to Large Language Models (LLMs), which is a topic in Natural Language Processing (NLP). While it touches on reasoning and retrieval, it does not directly address query understanding, ranking models, or user behavior modeling, which are core interests in Information Retrieval (IR). The paper's focus on LLMs and knowledge-intensive benchmarks is somewhat related to the user's interests, but it does not align with their primary focus on IR and search technologies.

#### Abstract
> Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language
Models (LLMs) by injecting external knowledge, yet it falls short on problems
that demand multi-step inference; conversely, purely reasoning-oriented
approaches often hallucinate or mis-ground facts. This survey synthesizes both
strands under a unified reasoning-retrieval perspective. We first map how
advanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,
we show how retrieved knowledge of different type supply missing premises and
expand context for complex inference (RAG-Enhanced Reasoning). Finally, we
spotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs
iteratively interleave search and reasoning to achieve state-of-the-art
performance across knowledge-intensive benchmarks. We categorize methods,
datasets, and open challenges, and outline research avenues toward deeper
RAG-Reasoning systems that are more effective, multimodally-adaptive,
trustworthy, and human-centric. The collection is available at
https://github.com/DavidZWZ/Awesome-RAG-Reasoning.

### 19. Self-Improving Model Steering

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Rongyi Zhu, Yuhui Wang, Tanqiu Jiang, Jiacheng Liang, Ting Wang
- **URL**: <http://arxiv.org/abs/2507.08967v1>
- **Submitted**: 2025-07-11 18:52:32
- **Comment**: 16 pages, 9 figures
- **Topic Keywords**: ranking, rank, search
- **Reason**: The paper presents a self-improving model-steering framework for large language models, which is related to information retrieval and search technologies. However, the focus is on language models and inference-time alignment, which is not directly aligned with the user's primary research interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat limited to the user's broader interests in NLP and data mining.

#### Abstract
> Model steering represents a powerful technique that dynamically aligns large
language models (LLMs) with human preferences during inference. However,
conventional model-steering methods rely heavily on externally annotated data,
not only limiting their adaptability to varying contexts but also tethering
their effectiveness to annotation quality. In this paper, we present SIMS, the
first self-improving model-steering framework that operates without relying on
external supervision. At its core, SIMS autonomously generates and refines
contrastive samples through iterative self-improvement cycles, enabling
adaptive, context-specific steering. Additionally, SIMS employs novel
strategies, including prompt ranking and contrast sampling, to further enhance
steering efficacy. Extensive evaluation across diverse LLMs and benchmarks
demonstrates that SIMS substantially outperforms existing methods in steering
effectiveness and adaptability, highlighting self-improving model steering as a
promising direction for future research on inference-time LLM alignment.

### 20. Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Daniele Rege Cambrin, Lorenzo Vaiani, Giuseppe Gallipoli, Luca Cagliero, Paolo Garza
- **URL**: <http://arxiv.org/abs/2507.10403v1>
- **Submitted**: 2025-07-14 15:46:56
- **Topic Keywords**: ctr, retrieval
- **Reason**: The paper focuses on text-to-image retrieval, but not in the context of query understanding, ranking models, or user behavior modeling, which are core aspects of your research interests. While it involves information retrieval and uses a novel framework, the application is specific to remote sensing and does not seem to require deep semantic understanding or real-time relevance optimization.

#### Abstract
> Retrieving relevant imagery from vast satellite archives is crucial for
applications like disaster response and long-term climate monitoring. However,
most text-to-image retrieval systems are limited to RGB data, failing to
exploit the unique physical information captured by other sensors, such as the
all-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the
spectral signatures in optical multispectral data. To bridge this gap, we
introduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1
SAR and Sentinel-2 multispectral images paired with structured textual
annotations for land cover, land use, and crisis events harmonized from
authoritative land cover systems (CORINE and Dynamic World) and crisis-specific
sources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),
a novel framework that uses text as a bridge to align unpaired optical and SAR
images into a unified embedding space. Our experiments show that CLOSP achieves
a new state-of-the-art, improving retrieval nDGC by 54% over existing models.
Additionally, we find that the unified training strategy overcomes the inherent
difficulty of interpreting SAR imagery by transferring rich semantic knowledge
from the optical domain with indirect interaction. Furthermore, GeoCLOSP, which
integrates geographic coordinates into our framework, creates a powerful
trade-off between generality and specificity: while the CLOSP excels at general
semantic tasks, the GeoCLOSP becomes a specialized expert for retrieving
location-dependent crisis events and rare geographic features. This work
highlights that the integration of diverse sensor data and geographic context
is essential for unlocking the full potential of remote sensing archives.

### 21. NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Hanwool Lee, Sara Yu, Yewon Hwang, Jonghyun Choi, Heejae Ahn, Sungbum Jung, Youngjae Yu
- **URL**: <http://arxiv.org/abs/2507.09601v1>
- **Submitted**: 2025-07-13 12:14:57
- **Comment**: Under Review
- **Topic Keywords**: rag, search, korea
- **Reason**: The paper focuses on domain-adapted neural embeddings for cross-lingual exploration of finance, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on topics like semantic-shift typology and representation learning, the primary focus on finance and low-resource languages makes it only loosely relevant to the user's research interests.

#### Abstract
> General-purpose sentence embedding models often struggle to capture
specialized financial semantics, especially in low-resource languages like
Korean, due to domain-specific jargon, temporal meaning shifts, and misaligned
bilingual vocabularies. To address these gaps, we introduce NMIXX (Neural
eMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual
embedding models fine-tuned with 18.8K high-confidence triplets that pair
in-domain paraphrases, hard negatives derived from a semantic-shift typology,
and exact Korean-English translations. Concurrently, we release KorFinSTS, a
1,921-pair Korean financial STS benchmark spanning news, disclosures, research
reports, and regulations, designed to expose nuances that general benchmarks
miss.
  When evaluated against seven open-license baselines, NMIXX's multilingual
bge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and
+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing
other models by the largest margin, while revealing a modest trade-off in
general STS performance. Our analysis further shows that models with richer
Korean token coverage adapt more effectively, underscoring the importance of
tokenizer design in low-resource, cross-lingual settings. By making both models
and the benchmark publicly available, we provide the community with robust
tools for domain-adapted, multilingual representation learning in finance.

### 22. Infinite Video Understanding

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Dell Zhang, Xiangyu Chen, Jixiang Luo, Mengxi Jia, Changzhi Sun, Ruilong Ren, Jingren Liu, Hao Sun, Xuelong Li
- **URL**: <http://arxiv.org/abs/2507.09068v1>
- **Submitted**: 2025-07-11 23:07:04
- **Topic Keywords**: ltr, search
- **Reason**: The paper focuses on video understanding, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on topics like query understanding and ranking models, the context is different and the paper's scope is broader, making it only loosely relevant to the user's interests.

#### Abstract
> The rapid advancements in Large Language Models (LLMs) and their multimodal
extensions (MLLMs) have ushered in remarkable progress in video understanding.
However, a fundamental challenge persists: effectively processing and
comprehending video content that extends beyond minutes or hours. While recent
efforts like Video-XL-2 have demonstrated novel architectural solutions for
extreme efficiency, and advancements in positional encoding such as HoPE and
VideoRoPE++ aim to improve spatio-temporal understanding over extensive
contexts, current state-of-the-art models still encounter significant
computational and memory constraints when faced with the sheer volume of visual
tokens from lengthy sequences. Furthermore, maintaining temporal coherence,
tracking complex events, and preserving fine-grained details over extended
periods remain formidable hurdles, despite progress in agentic reasoning
systems like Deep Video Discovery. This position paper posits that a logical,
albeit ambitious, next frontier for multimedia research is Infinite Video
Understanding -- the capability for models to continuously process, understand,
and reason about video data of arbitrary, potentially never-ending duration. We
argue that framing Infinite Video Understanding as a blue-sky research
objective provides a vital north star for the multimedia, and the wider AI,
research communities, driving innovation in areas such as streaming
architectures, persistent memory mechanisms, hierarchical and adaptive
representations, event-centric reasoning, and novel evaluation paradigms.
Drawing inspiration from recent work on long/ultra-long video understanding and
several closely related fields, we outline the core challenges and key research
directions towards achieving this transformative capability.

### 23. Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Muzhaffar Hazman, Minh-Khoi Pham, Shweta Soundararajan, Goncalo Mordido, Leonardo Custode, David Lynch, Giorgio Cruciata, Yucheng Shi, Hongmeng Song, Wang Chao, Pan Yue, Aleksandar Milenovic, Alexandros Agapitos
- **URL**: <http://arxiv.org/abs/2507.10326v1>
- **Submitted**: 2025-07-14 14:34:15
- **Comment**: Accepted for Publication at ECAI 2025
- **Topic Keywords**: rag, search
- **Reason**: The paper explores prompt engineering for large language models, which is related to information retrieval and search technologies. However, the focus on discrete prompt optimisation and grammar-guided evolutionary search is not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Prompt engineering has proven to be a crucial step in leveraging pretrained
large language models (LLMs) in solving various real-world tasks. Numerous
solutions have been proposed that seek to automate prompt engineering by using
the model itself to edit prompts. However, the majority of state-of-the-art
approaches are evaluated on tasks that require minimal prompt templates and on
very large and highly capable LLMs. In contrast, solving complex tasks that
require detailed information to be included in the prompt increases the amount
of text that needs to be optimised. Furthermore, smaller models have been shown
to be more sensitive to prompt design. To address these challenges, we propose
an evolutionary search approach to automated discrete prompt optimisation
consisting of two phases. In the first phase, grammar-guided genetic
programming is invoked to synthesise prompt-creating programmes by searching
the space of programmes populated by function compositions of syntactic,
dictionary-based and LLM-based prompt-editing functions. In the second phase,
local search is applied to explore the neighbourhoods of best-performing
programmes in an attempt to further fine-tune their performance. Our approach
outperforms three state-of-the-art prompt optimisation approaches,
PromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose
LLMs in four domain-specific challenging tasks. We also illustrate several
examples where these benchmark methods suffer relatively severe performance
degradation, while our approach improves performance in almost all task-model
combinations, only incurring minimal degradation when it does not.

### 24. MixLoRA-DSI: Dynamically Expandable Mixture-of-LoRA Experts for Rehearsal-Free Generative Retrieval over Dynamic Corpora

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Tuan-Luc Huynh, Thuy-Trang Vu, Weiqing Wang, Trung Le, Dragan Ga≈°eviƒá, Yuan-Fang Li, Thanh-Toan Do
- **URL**: <http://arxiv.org/abs/2507.09924v1>
- **Submitted**: 2025-07-14 05:04:32
- **Topic Keywords**: retrieval, rank
- **Reason**: The paper proposes a novel framework for generative retrieval over dynamic corpora, which is somewhat related to information retrieval and search technologies. However, the focus on generative retrieval and model-based indexes is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is limited to the broader field of information retrieval, but it does not address the user's specific areas of interest.

#### Abstract
> Continually updating model-based indexes in generative retrieval with new
documents remains challenging, as full retraining is computationally expensive
and impractical under resource constraints. We propose MixLoRA-DSI, a novel
framework that combines an expandable mixture of Low-Rank Adaptation experts
with a layer-wise out-of-distribution (OOD)-driven expansion strategy. Instead
of allocating new experts for each new corpus, our proposed expansion strategy
enables sublinear parameter growth by selectively introducing new experts only
when significant number of OOD documents are detected. Experiments on NQ320k
and MS MARCO Passage demonstrate that MixLoRA-DSI outperforms full-model update
baselines, with minimal parameter overhead and substantially lower training
costs.

### 25. Correcting the LogQ Correction: Revisiting Sampled Softmax for Large-Scale Retrieval

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Kirill Khrylchenko, Vladimir Baikalov, Sergei Makeev, Artem Matveev, Sergei Liamaev
- **URL**: <http://arxiv.org/abs/2507.09331v1>
- **Submitted**: 2025-07-12 16:16:11
- **Comment**: Accepted at ACM RecSys 2025. Author's version. To appear in the
  Proceedings of the 18th ACM Conference on Recommender Systems
- **Topic Keywords**: retrieval, recommend
- **Reason**: The paper focuses on improving the performance of two-tower neural networks for large-scale retrieval, specifically addressing the issue of bias in sampled softmax. While it touches on the topic of ranking models, it does not directly relate to query understanding, user behavior modeling, or deep semantic understanding, which are core aspects of your research interests.

#### Abstract
> Two-tower neural networks are a popular architecture for the retrieval stage
in recommender systems. These models are typically trained with a softmax loss
over the item catalog. However, in web-scale settings, the item catalog is
often prohibitively large, making full softmax infeasible. A common solution is
sampled softmax, which approximates the full softmax using a small number of
sampled negatives.
  One practical and widely adopted approach is to use in-batch negatives, where
negatives are drawn from items in the current mini-batch. However, this
introduces a bias: items that appear more frequently in the batch (i.e.,
popular items) are penalized more heavily.
  To mitigate this issue, a popular industry technique known as logQ correction
adjusts the logits during training by subtracting the log-probability of an
item appearing in the batch. This correction is derived by analyzing the bias
in the gradient and applying importance sampling, effectively twice, using the
in-batch distribution as a proposal distribution. While this approach improves
model quality, it does not fully eliminate the bias.
  In this work, we revisit the derivation of logQ correction and show that it
overlooks a subtle but important detail: the positive item in the denominator
is not Monte Carlo-sampled - it is always present with probability 1. We
propose a refined correction formula that accounts for this. Notably, our loss
introduces an interpretable sample weight that reflects the model's uncertainty
- the probability of misclassification under the current parameters. We
evaluate our method on both public and proprietary datasets, demonstrating
consistent improvements over the standard logQ correction.

### 26. Identifying Offline Metrics that Predict Online Impact: A Pragmatic Strategy for Real-World Recommender Systems

- **LLM Score**: 2
- **Keyword Score**: 11
- **Authors**: Timo Wilm, Philipp Normann
- **URL**: <http://arxiv.org/abs/2507.09566v1>
- **Submitted**: 2025-07-13 10:24:41
- **Comment**: This work was accepted for publication in the 19th ACM Conference on
  Recommender Systems (RecSys 2025). The final published version will be
  available at the ACM Digital Library
- **Topic Keywords**: rag, click, click-through rate, conversion rate, recommend, commerce, e-commerce
- **Reason**: The paper focuses on recommender systems, which is a related topic, but it does not address query understanding, ranking models, or user behavior modeling, which are core interests. The paper's emphasis on offline-to-online metric relationships and its application to session-based recommender systems on an e-commerce platform also do not align with the user's background in IR and NLP.

#### Abstract
> A critical challenge in recommender systems is to establish reliable
relationships between offline and online metrics that predict real-world
performance. Motivated by recent advances in Pareto front approximation, we
introduce a pragmatic strategy for identifying offline metrics that align with
online impact. A key advantage of this approach is its ability to
simultaneously serve multiple test groups, each with distinct offline
performance metrics, in an online experiment controlled by a single model. The
method is model-agnostic for systems with a neural network backbone, enabling
broad applicability across architectures and domains. We validate the strategy
through a large-scale online experiment in the field of session-based
recommender systems on the OTTO e-commerce platform. The online experiment
identifies significant alignments between offline metrics and real-word
click-through rate, post-click conversion rate and units sold. Our strategy
provides industry practitioners with a valuable tool for understanding
offline-to-online metric relationships and making informed, data-driven
decisions.

### 27. Automating SPARQL Query Translations between DBpedia and Wikidata

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Malte Christian Bartels, Debayan Banerjee, Ricardo Usbeck
- **URL**: <http://arxiv.org/abs/2507.10045v1>
- **Submitted**: 2025-07-14 08:23:25
- **Comment**: 18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference
  happening on September 2025
- **Topic Keywords**: query, queries, search
- **Reason**: The paper focuses on translating SPARQL queries between Knowledge Graph schemas, which is not directly related to Information Retrieval, Search technologies, or query understanding. Although it involves Natural Language Processing, the context is different from the user's primary interests.

#### Abstract
> This paper investigates whether state-of-the-art Large Language Models (LLMs)
can automatically translate SPARQL between popular Knowledge Graph (KG)
schemas. We focus on translations between the DBpedia and Wikidata KG, and
later on DBLP and OpenAlex KG. This study addresses a notable gap in KG
interoperability research by rigorously evaluating LLM performance on
SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first
align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100
DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic
KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and
Mistral-Large-Instruct-2407 are selected based on their sizes and architectures
and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs
were compared with gold answers, and resulting errors were categorized. We find
that the performance varies markedly across models and prompting strategies,
and that translations for Wikidata to DBpedia work far better than translations
for DBpedia to Wikidata.

### 28. Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Jason Zhu, Hongyu Li
- **URL**: <http://arxiv.org/abs/2507.09662v1>
- **Submitted**: 2025-07-13 14:51:59
- **Topic Keywords**: queries, search
- **Reason**: The paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. The focus is on large reasoning models and concise and adaptive thinking, which is outside the scope of the user's research interests.

#### Abstract
> Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have
demonstrated impressive performance on complex reasoning tasks like mathematics
and programming with long Chain-of-Thought (CoT) reasoning sequences
(slow-thinking), compared with traditional large language models
(fast-thinking). However, these reasoning models also face a huge challenge
that generating unnecessarily lengthy and redundant reasoning chains even for
trivial questions. This phenomenon leads to a significant waste of inference
resources, increases the response time for simple queries, and hinders the
practical application of LRMs in real-world products. To this end, it is
crucial to shorten lengthy reasoning chains and learn adaptive reasoning
between fast and slow thinking based on input difficulty. In this survey, we
provide a comprehensive overview of recent progress in concise and adaptive
thinking for efficient reasoning of LRMs, including methodologies, benchmarks,
and challenges for future exploration. We hope this survey can help researchers
quickly understand the landscape of this field and inspire novel adaptive
thinking ideas to facilitate better usage of LRMs.

### 29. Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Pawitsapak Akarajaradwong, Chompakorn Chaksangchaichot, Pirat Pothavorn, Attapol Thamrongrattanarit-Rutherford, Ekapol Chuangsuwanich, Sarana Nutanong
- **URL**: <http://arxiv.org/abs/2507.09638v1>
- **Submitted**: 2025-07-13 14:05:48
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on Retrieval-Augmented Generation systems for Thai legal question answering, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and query understanding. While it mentions embeddings and semantic-similarity rewards, the context is specific to legal reasoning and question answering, making it an off-topic paper.

#### Abstract
> The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal
question answering is still limited, especially for questions requiring
extensive, complex legal reasoning. To address these limitations, we introduce
an approach aligning LLMs toward improved law citation accuracy and better
response quality using Group-Relative Policy Optimization (GRPO). Our approach
leverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,
significantly reducing computational expenses up to 2.5x compared to large
language model judges. Experiments on the NitiBench benchmark demonstrate
substantial improvements: GRPO achieves up to 90% citation-F1 gains from the
base model and a 31% increase in joint quality metrics over instruction tuning.
Crucially, our method shows enhanced robustness on complex legal reasoning
tasks compared to instruction tuning, providing an effective and
resource-efficient solution for enhancing Thai legal LLMs.

### 30. Does UMBRELA Work on Other LLMs?

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Naghmeh Farzi, Laura Dietz
- **URL**: <http://arxiv.org/abs/2507.09483v1>
- **Submitted**: 2025-07-13 04:05:25
- **Comment**: 9 pages, 2 figures, accepted to SIGIR 2025
- **Topic Keywords**: relevance, rank
- **Reason**: The paper focuses on evaluating the generalizability of a specific evaluation framework (UMBRELA) across different large language models (LLMs), which is not directly related to information retrieval, search technologies, or query understanding. Although it touches on relevance assessment accuracy, the context is not relevant to the user's primary research interests.

#### Abstract
> We reproduce the UMBRELA LLM Judge evaluation framework across a range of
large language models (LLMs) to assess its generalizability beyond the original
study. Our investigation evaluates how LLM choice affects relevance assessment
accuracy, focusing on leaderboard rank correlation and per-label agreement
metrics. Results demonstrate that UMBRELA with DeepSeek V3 obtains very
comparable performance to GPT-4o (used in original work). For LLaMA-3.3-70B we
obtain slightly lower performance, which further degrades with smaller LLMs.

### 31. AInsight: Augmenting Expert Decision-Making with On-the-Fly Insights Grounded in Historical Data

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Mohammad Abolnejadian, Shakiba Amirshahi, Matthew Brehmer, Anamaria Crisan
- **URL**: <http://arxiv.org/abs/2507.09100v1>
- **Submitted**: 2025-07-12 00:59:41
- **Comment**: 7 pages and 4 figures. Proceedings of the 7th ACM Conference on
  Conversational User Interfaces (CUI '25)
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper focuses on augmenting expert decision-making with on-the-fly insights grounded in historical data, which is not directly related to information retrieval, search technologies, or query understanding. Although it involves a conversational user interface and a retrieval-based Large Language Model (LLM) agent, the context and application are quite different from the user's research interests.

#### Abstract
> In decision-making conversations, experts must navigate complex choices and
make on-the-spot decisions while engaged in conversation. Although extensive
historical data often exists, the real-time nature of these scenarios makes it
infeasible for decision-makers to review and leverage relevant information.
This raises an interesting question: What if experts could utilize relevant
past data in real-time decision-making through insights derived from past data?
To explore this, we implemented a conversational user interface, taking
doctor-patient interactions as an example use case. Our system continuously
listens to the conversation, identifies patient problems and doctor-suggested
solutions, and retrieves related data from an embedded dataset, generating
concise insights using a pipeline built around a retrieval-based Large Language
Model (LLM) agent. We evaluated the prototype by embedding Health Canada
datasets into a vector database and conducting simulated studies using sample
doctor-patient dialogues, showing effectiveness but also challenges, setting
directions for the next steps of our work.

### 32. From BERT to Qwen: Hate Detection across architectures

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Ariadna Mon, Sa√∫l Fenollosa, Jon Lecumberri
- **URL**: <http://arxiv.org/abs/2507.10468v1>
- **Submitted**: 2025-07-14 16:46:30
- **Comment**: 4 pages, 5 figures. EE-559 Deep Learning course project (Group 11)
- **Topic Keywords**: ltr
- **Reason**: The paper focuses on hate speech detection using BERT and LLMs, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and query understanding. While it touches on Natural Language Processing, the specific application and methodology are not aligned with the user's primary focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> Online platforms struggle to curb hate speech without over-censoring
legitimate discourse. Early bidirectional transformer encoders made big
strides, but the arrival of ultra-large autoregressive LLMs promises deeper
context-awareness. Whether this extra scale actually improves practical
hate-speech detection on real-world text remains unverified. Our study puts
this question to the test by benchmarking both model families, classic encoders
and next-generation LLMs, on curated corpora of online interactions for
hate-speech detection (Hate or No Hate).

### 33. SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Meriem Zerkouk, Miloud Mihoubi, Belkacem Chikhaoui
- **URL**: <http://arxiv.org/abs/2507.10421v1>
- **Submitted**: 2025-07-14 16:04:34
- **Comment**: International Conference on Education and New Learning Technologies
  (2025)
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on predicting dropout in distance learning using machine learning models, which is not directly related to information retrieval, search technologies, or natural language processing. While it involves sentiment analysis using BERT, the context and application are distinct from the user's research interests.

#### Abstract
> School dropout is a serious problem in distance learning, where early
detection is crucial for effective intervention and student perseverance.
Predicting student dropout using available educational data is a widely
researched topic in learning analytics. Our partner's distance learning
platform highlights the importance of integrating diverse data sources,
including socio-demographic data, behavioral data, and sentiment analysis, to
accurately predict dropout risks. In this paper, we introduce a novel model
that combines sentiment analysis of student comments using the Bidirectional
Encoder Representations from Transformers (BERT) model with socio-demographic
and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We
fine-tuned BERT on student comments to capture nuanced sentiments, which were
then merged with key features selected using feature importance techniques in
XGBoost. Our model was tested on unseen data from the next academic year,
achieving an accuracy of 84\%, compared to 82\% for the baseline model.
Additionally, the model demonstrated superior performance in other metrics,
such as precision and F1-score. The proposed method could be a vital tool in
developing personalized strategies to reduce dropout rates and encourage
student perseverance

### 34. Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Simon M√ºnker
- **URL**: <http://arxiv.org/abs/2507.10073v1>
- **Submitted**: 2025-07-14 08:59:26
- **Comment**: 15pages, 1 figure, 2 tables
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on cultural bias in large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on AI systems, it does not address ranking models, user behavior modeling, or real-time relevance optimization, which are core aspects of your research interests.

#### Abstract
> Are AI systems truly representing human values, or merely averaging across
them? Our study suggests a concerning reality: Large Language Models (LLMs)
fail to represent diverse cultural moral frameworks despite their linguistic
capabilities. We expose significant gaps between AI-generated and human moral
intuitions by applying the Moral Foundations Questionnaire across 19 cultural
contexts. Comparing multiple state-of-the-art LLMs' origins against human
baseline data, we find these models systematically homogenize moral diversity.
Surprisingly, increased model size doesn't consistently improve cultural
representation fidelity. Our findings challenge the growing use of LLMs as
synthetic populations in social science research and highlight a fundamental
limitation in current AI alignment approaches. Without data-driven alignment
beyond prompting, these systems cannot capture the nuanced, culturally-specific
moral intuitions. Our results call for more grounded alignment objectives and
evaluation metrics to ensure AI systems represent diverse human values rather
than flattening the moral landscape.

### 35. SLIF-MR: Self-loop Iterative Fusion of Heterogeneous Auxiliary Information for Multimodal Recommendation

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Jie Guo, Jiahao Jiang, Ziyuan Guo, Bin Song, Yue Sun
- **URL**: <http://arxiv.org/abs/2507.09998v1>
- **Submitted**: 2025-07-14 07:32:16
- **Comment**: 10 pages,7 figures
- **Topic Keywords**: rag, recommend
- **Reason**: The paper focuses on multimodal recommendation systems, leveraging knowledge graphs and item representations, which is not directly related to information retrieval, query understanding, or ranking models. While it touches on graph-based methods, the primary focus is on recommender systems, which is a secondary interest for the user.

#### Abstract
> Knowledge graphs (KGs) and multimodal item information, which respectively
capture relational and attribute features, play a crucial role in improving
recommender system accuracy. Recent studies have attempted to integrate them
via multimodal knowledge graphs (MKGs) to further enhance recommendation
performance. However, existing methods typically freeze the MKG structure
during training, which limits the full integration of structural information
from heterogeneous graphs (e.g., KG and user-item interaction graph), and
results in sub-optimal performance. To address this challenge, we propose a
novel framework, termed Self-loop Iterative Fusion of Heterogeneous Auxiliary
Information for Multimodal Recommendation (SLIF-MR), which leverages item
representations from previous training epoch as feedback signals to dynamically
optimize the heterogeneous graph structures composed of KG, multimodal item
feature graph, and user-item interaction graph. Through this iterative fusion
mechanism, both user and item representations are refined, thus improving the
final recommendation performance. Specifically, based on the feedback item
representations, SLIF-MR constructs an item-item correlation graph, then
integrated into the establishment process of heterogeneous graphs as additional
new structural information in a self-loop manner. Consequently, the internal
structures of heterogeneous graphs are updated with the feedback item
representations during training. Moreover, a semantic consistency learning
strategy is proposed to align heterogeneous item representations across
modalities. The experimental results show that SLIF-MR significantly
outperforms existing methods, particularly in terms of accuracy and robustness.

### 36. ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Changli Wang, Rui Wu, Fang Yin
- **URL**: <http://arxiv.org/abs/2507.09482v1>
- **Submitted**: 2025-07-13 04:03:05
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on sarcasm generation with multimodal input, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the primary goal is not to improve search or retrieval, but rather to generate sarcastic content. The techniques and concepts presented are not relevant to the user's core research themes.

#### Abstract
> Human emotions are complex, with sarcasm being a subtle and distinctive form.
Despite progress in sarcasm research, sarcasm generation remains underexplored,
primarily due to the overreliance on textual modalities and the neglect of
visual cues, as well as the mismatch between image content and sarcastic intent
in existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm
generation dataset with 4,970 samples, each containing an image, a sarcastic
text, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation
framework that integrates Proximal Policy Optimization (PPO) and contrastive
learning. PPO utilizes reward scores from DIP to steer the generation of
sarcastic texts, while contrastive learning encourages the model to favor
outputs with higher reward scores. These strategies improve overall generation
quality and produce texts with more pronounced sarcastic intent. We evaluate
ViSP across five metric sets and find it surpasses all baselines, including
large language models, underscoring their limitations in sarcasm generation.
Furthermore, we analyze the distributions of Sarcasm Scores and Factual
Incongruity for both M2SaG and the texts generated by ViSP. The generated texts
exhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity
(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic
content than the original dataset. % The dataset and code will be publicly
available. Our dataset and code will be released at
\textit{https://github.com/wclapply/ViSP}.

### 37. Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Mingchuan Yang, Ziyuan Huang
- **URL**: <http://arxiv.org/abs/2507.09470v1>
- **Submitted**: 2025-07-13 03:10:19
- **Comment**: 29 pages, 5 tables
- **Topic Keywords**: rag, search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on clinical text classification using a pre-trained language model, which is outside your primary areas of interest. While it touches on natural language processing, it is not directly related to your work in IR and search technologies.

#### Abstract
> This study explores the optimization of the DRAGON Longformer base model for
clinical text classification, specifically targeting the binary classification
of medical case descriptions. A dataset of 500 clinical cases containing
structured medical observations was used, with 400 cases for training and 100
for validation. Enhancements to the pre-trained
joeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter
tuning, domain-specific preprocessing, and architectural adjustments. Key
modifications involved increasing sequence length from 512 to 1024 tokens,
adjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5
to 8, and incorporating specialized medical terminology. The optimized model
achieved notable performance gains: accuracy improved from 72.0% to 85.2%,
precision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from
71.0% to 85.2%. Statistical analysis confirmed the significance of these
improvements (p < .001). The model demonstrated enhanced capability in
interpreting medical terminology, anatomical measurements, and clinical
observations. These findings contribute to domain-specific language model
research and offer practical implications for clinical natural language
processing applications. The optimized model's strong performance across
diverse medical conditions underscores its potential for broad use in
healthcare settings.

### 38. MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Biagio Scalingi, Chiara Barattieri di San Pietro, Paolo Canal, Valentina Bambini
- **URL**: <http://arxiv.org/abs/2507.09225v1>
- **Submitted**: 2025-07-12 09:49:30
- **Comment**: 27 pages, 5 figures
- **Topic Keywords**: search, acl
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on a specific domain (climate change) and uses Natural Language Processing for semantic and emotion analysis, but it does not address your core research themes.

#### Abstract
> Visual metaphors of climate change (e.g., melting glaciers depicted as a
melting ice grenade) are regarded as valuable tools for addressing the
complexity of environmental challenges. However, few studies have examined
their impact on communication, also due to scattered availability of material.
Here, we present a novel database of Metaphors of Climate Change in Images
(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal
images and enriched with human ratings. For each image, we collected values of
difficulty, efficacy, artistic quality, and emotional arousal from human
rating, as well as number of tags generated by participants to summarize the
message. Semantic and emotion variables were further derived from the tags via
Natural Language Processing. Visual metaphors were rated as more difficult to
understand, yet more aesthetically pleasant than literal images, but did not
differ in efficacy and arousal. The latter for visual metaphors, however, was
higher in participants with higher Need For Cognition. Furthermore, visual
metaphors received more tags, often referring to entities not depicted in the
image, and elicited words with more positive valence and greater dominance than
literal images. These results evidence the greater cognitive load of visual
metaphors, which nevertheless might induce positive effects such as deeper
cognitive elaboration and abstraction compared to literal stimuli. Furthermore,
while they are not deemed as more effective and arousing, visual metaphors seem
to generate superior aesthetic appreciation and a more positively valenced
experience. Overall, this study contributes to understanding the impact of
visual metaphors of climate change both by offering a database for future
research and by elucidating a cost-benefit trade-off to take into account when
shaping environmental communication.

### 39. Semantic Source Code Segmentation using Small and Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Abdelhalim Dahou, Ansgar Scherp, Sebastian Kurten, Brigitte Mathiak, Madhu Chauhan
- **URL**: <http://arxiv.org/abs/2507.08992v1>
- **Submitted**: 2025-07-11 19:49:59
- **Comment**: 18 pages, 4 figures
- **Topic Keywords**: retrieval, search
- **Reason**: The paper focuses on source code segmentation using language models, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves natural language processing, the context is specific to software development and code analysis, which is not a primary interest area.

#### Abstract
> Source code segmentation, dividing code into functionally coherent segments,
is crucial for knowledge retrieval and maintenance in software development.
While enabling efficient navigation and comprehension of large codebases,
manual and syntactic analysis approaches have become impractical as
repositories grow, especially for low-resource languages like R and their
research domains (e.g., social sciences, psychology).This paper introduces an
automated, domain-specific approach for research R code segmentation using
Large and Small Language Models (LLMs/SLMs). It presents two novel approaches
and a human-annotated dataset, StatCodeSeg. We explore two distinct approaches:
line-by-line analysis with context and range-based segment determination. We
experiment with LLMs and fine-tuned SLMs. To support the generalizability of
our approaches, we also include experiments on Python code from the computer
science domain.Our results show that context-based line-by-line analysis is
superior over range-based segmentation.Using smaller language models like
CodeBERT and an encoder-only version of CodeT5+ are better than their LLM
counterparts. Most notably, these two best-performing models did not see R code
during pre-training versus the LLMs but were only fine-tuned on 4,130 lines of
manually annotated code.

### 40. MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Mohamed T. Younes, Omar Walid, Mai Hassan, Ali Hamdi
- **URL**: <http://arxiv.org/abs/2507.10472v1>
- **Submitted**: 2025-07-14 16:53:19
- **Topic Keywords**: rag
- **Reason**: The paper focuses on Robotic Process Automation (RPA) and Applicant Tracking Systems (ATS), which is unrelated to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it employs Large Language Models (LLMs), the application is limited to resume screening and candidate shortlisting, which is not a central match for the user's research themes.

#### Abstract
> This paper introduces an innovative Applicant Tracking System (ATS) enhanced
by a novel Robotic process automation (RPA) framework or as further referred to
as MLAR. Traditional recruitment processes often encounter bottlenecks in
resume screening and candidate shortlisting due to time and resource
constraints. MLAR addresses these challenges employing Large Language Models
(LLMs) in three distinct layers: extracting key characteristics from job
postings in the first layer, parsing applicant resume to identify education,
experience, skills in the second layer, and similarity matching in the third
layer. These features are then matched through advanced semantic algorithms to
identify the best candidates efficiently. Our approach integrates seamlessly
into existing RPA pipelines, automating resume parsing, job matching, and
candidate notifications. Extensive performance benchmarking shows that MLAR
outperforms the leading RPA platforms, including UiPath and Automation
Anywhere, in high-volume resume-processing tasks. When processing 2,400
resumes, MLAR achieved an average processing time of 5.4 seconds per resume,
reducing processing time by approximately 16.9% compared to Automation Anywhere
and 17.1% compared to UiPath. These results highlight the potential of MLAR to
transform recruitment workflows by providing an efficient, accurate, and
scalable solution tailored to modern hiring needs.

### 41. Devanagari Handwritten Character Recognition using Convolutional Neural Network

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Diksha Mehta, Prateek Mehta
- **URL**: <http://arxiv.org/abs/2507.10398v1>
- **Submitted**: 2025-07-14 15:38:42
- **Comment**: 9 pages, 6 figures
- **Topic Keywords**: recommend, search
- **Reason**: This paper is not relevant to your research interests as it focuses on handwritten character recognition using convolutional neural networks, which is not related to information retrieval, search technologies, or natural language processing. The paper's application in recommender systems is also not a primary focus of your research.

#### Abstract
> Handwritten character recognition is getting popular among researchers
because of its possible applications in facilitating technological search
engines, social media, recommender systems, etc. The Devanagari script is one
of the oldest language scripts in India that does not have proper digitization
tools. With the advancement of computing and technology, the task of this
research is to extract handwritten Hindi characters from an image of Devanagari
script with an automated approach to save time and obsolete data. In this
paper, we present a technique to recognize handwritten Devanagari characters
using two deep convolutional neural network layers. This work employs a
methodology that is useful to enhance the recognition rate and configures a
convolutional neural network for effective Devanagari handwritten text
recognition (DHTR). This approach uses the Devanagari handwritten character
dataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each
of these classes has 1700 images for training and testing purposes. This
approach obtains promising results in terms of accuracy by achieving 96.36%
accuracy in testing and 99.55% in training time.

### 42. Meanings are like Onions: a Layered Approach to Metaphor Processing

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Silvia Cappa, Anna Sofia Lippolis, Stefano Zoia
- **URL**: <http://arxiv.org/abs/2507.10354v1>
- **Submitted**: 2025-07-14 14:56:46
- **Topic Keywords**: rag
- **Reason**: The paper focuses on metaphor processing, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper mentions computational systems, it does not address query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user.

#### Abstract
> Metaphorical meaning is not a flat mapping between concepts, but a complex
cognitive phenomenon that integrates multiple levels of interpretation. In this
paper, we propose a stratified model of metaphor processing that treats meaning
as an onion: a multi-layered structure comprising (1) content analysis, (2)
conceptual blending, and (3) pragmatic intentionality. This three-dimensional
framework allows for a richer and more cognitively grounded approach to
metaphor interpretation in computational systems. At the first level, metaphors
are annotated through basic conceptual elements. At the second level, we model
conceptual combinations, linking components to emergent meanings. Finally, at
the third level, we introduce a pragmatic vocabulary to capture speaker intent,
communicative function, and contextual effects, aligning metaphor understanding
with pragmatic theories. By unifying these layers into a single formal
framework, our model lays the groundwork for computational methods capable of
representing metaphorical meaning beyond surface associations, toward deeper,
more context-sensitive reasoning.

### 43. Task-Based Flexible Feature Distillation for LLMs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Khouloud Saadi, Di Wang
- **URL**: <http://arxiv.org/abs/2507.10155v1>
- **Submitted**: 2025-07-14 11:10:02
- **Topic Keywords**: rag
- **Reason**: This paper focuses on knowledge distillation in large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions downstream tasks, it does not involve ranking models, user behavior modeling, or real-time relevance optimization, which are key areas of interest for your research.

#### Abstract
> Knowledge Distillation (KD) in general and feature distillation in particular
are promising techniques for reducing the high computational demand of large
language models (LLMs). However, traditional feature KD methods typically
assume that the teacher and the student share the same hidden size, limiting
the flexibility of the student's architecture. A common solution to this
problem involves training a linear projector to align their feature spaces, but
this introduces additional parameters that must be learned from scratch and
often degrades performance on downstream tasks, especially in generative
settings. To address this issue, in this work, we propose a novel task-based
feature distillation method that enables knowledge transfer between teacher and
student models with different hidden layer dimensions, without introducing any
new parameters. Leveraging the insight that only a subset of LLM components
contribute significantly to a specific downstream task, our approach identifies
the most task-relevant hidden units in the teacher and directly distills their
activations to the student. Our method is flexible and easily integrates with
other distillation frameworks. Empirical results show consistent improvements
over prior approaches across diverse tasks, including classification,
instruction-following, and summarization, achieving up to a 3\% performance
gain over the linear projection baseline.

### 44. Fusing Large Language Models with Temporal Transformers for Time Series Forecasting

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Chen Su, Yuanhe Tian, Qinyu Liu, Jun Zhang, Yan Song
- **URL**: <http://arxiv.org/abs/2507.10098v1>
- **Submitted**: 2025-07-14 09:33:40
- **Topic Keywords**: rag
- **Reason**: This paper focuses on time series forecasting using large language models and temporal transformers, which is not directly related to information retrieval, search technologies, or query understanding. While it involves transformer-based architectures, the application is in a different domain and does not address ranking models or user behavior modeling.

#### Abstract
> Recently, large language models (LLMs) have demonstrated powerful
capabilities in performing various tasks and thus are applied by recent studies
to time series forecasting (TSF) tasks, which predict future values with the
given historical time series. Existing LLM-based approaches transfer knowledge
learned from text data to time series prediction using prompting or fine-tuning
strategies. However, LLMs are proficient at reasoning over discrete tokens and
semantic patterns but are not initially designed to model continuous numerical
time series data. The gaps between text and time series data lead LLMs to
achieve inferior performance to a vanilla Transformer model that is directly
trained on TSF data. However, the vanilla Transformers often struggle to learn
high-level semantic patterns. In this paper, we design a novel
Transformer-based architecture that complementarily leverages LLMs and vanilla
Transformers, so as to integrate the high-level semantic representations
learned by LLMs into the temporal information encoded by time series
Transformers, where a hybrid representation is obtained by fusing the
representations from the LLM and the Transformer. The resulting fused
representation contains both historical temporal dynamics and semantic
variation patterns, allowing our model to predict more accurate future values.
Experiments on benchmark datasets demonstrate the effectiveness of the proposed
approach.

### 45. On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Mark Burgess
- **URL**: <http://arxiv.org/abs/2507.10000v1>
- **Submitted**: 2025-07-14 07:34:58
- **Topic Keywords**: rag
- **Reason**: This paper appears to be unrelated to Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling, which are the core areas of your research interests. The focus on intentionality, knowledge representation, and cognitive agents is outside the scope of your expertise.

#### Abstract
> Since Searle's work deconstructing intent and intentionality in the realm of
philosophy, the practical meaning of intent has received little attention in
science and technology. Intentionality and context are both central to the
scope of Promise Theory's model of Semantic Spacetime, used as an effective
Tiny Language Model. One can identify themes and concepts from a text, on a low
level (without knowledge of the specific language) by using process coherence
as a guide. Any agent process can assess superficially a degree of latent
`intentionality' in data by looking for anomalous multi-scale anomalies and
assessing the work done to form them. Scale separation can be used to sort
parts into `intended' content and `ambient context', using the spacetime
coherence as a measure. This offers an elementary but pragmatic interpretation
of latent intentionality for very low computational cost, and without reference
to extensive training or reasoning capabilities. The process is well within the
reach of basic organisms as it does not require large scale artificial
probabilistic batch processing. The level of concept formation depends,
however, on the memory capacity of the agent.

### 46. TextOmics-Guided Diffusion for Hit-like Molecular Generation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Hang Yuan, Chen Li, Wenjun Ma, Yuncheng Jiang
- **URL**: <http://arxiv.org/abs/2507.09982v1>
- **Submitted**: 2025-07-14 06:56:37
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on molecular generation and drug discovery, which is outside the user's primary focus.

#### Abstract
> Hit-like molecular generation with therapeutic potential is essential for
target-specific drug discovery. However, the field lacks heterogeneous data and
unified frameworks for integrating diverse molecular representations. To bridge
this gap, we introduce TextOmics, a pioneering benchmark that establishes
one-to-one correspondences between omics expressions and molecular textual
descriptions. TextOmics provides a heterogeneous dataset that facilitates
molecular generation through representations alignment. Built upon this
foundation, we propose ToDi, a generative framework that jointly conditions on
omics expressions and molecular textual descriptions to produce biologically
relevant, chemically valid, hit-like molecules. ToDi leverages two encoders
(OmicsEn and TextEn) to capture multi-level biological and semantic
associations, and develops conditional diffusion (DiffGen) for controllable
generation. Extensive experiments confirm the effectiveness of TextOmics and
demonstrate ToDi outperforms existing state-of-the-art approaches, while also
showcasing remarkable potential in zero-shot therapeutic molecular generation.
Sources are available at: https://github.com/hala-ToDi.

### 47. Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Qinyuan Ye, Robin Jia, Xiang Ren
- **URL**: <http://arxiv.org/abs/2507.09875v1>
- **Submitted**: 2025-07-14 03:20:55
- **Comment**: Code: https://github.com/INK-USC/function-induction
- **Topic Keywords**: rag
- **Reason**: The paper focuses on understanding the internal mechanisms of large language models, specifically through the lens of off-by-one addition, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on topics like attention heads and function induction, the paper's primary focus is on interpretability and task generalization, which is not a central match for the user's research interests.

#### Abstract
> Large language models demonstrate the intriguing ability to perform unseen
tasks via in-context learning. However, it remains unclear what mechanisms
inside the model drive such task-level generalization. In this work, we
approach this question through the lens of off-by-one addition (i.e., 1+1=3,
2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function
as a second step. Leveraging circuit-style interpretability techniques such as
path patching, we analyze the models' internal computations behind their
notable performance and present three key findings. First, we uncover a
function induction mechanism that explains the model's generalization from
standard addition to off-by-one addition. This mechanism resembles the
structure of the induction head mechanism found in prior work and elevates it
to a higher level of abstraction. Second, we show that the induction of the +1
function is governed by multiple attention heads in parallel, each of which
emits a distinct piece of the +1 function. Finally, we find that this function
induction mechanism is reused in a broader range of tasks, including synthetic
tasks such as shifted multiple-choice QA and algorithmic tasks such as base-8
addition. Overall, our findings offer deeper insights into how reusable and
composable structures within language models enable task-level generalization.

### 48. Te Ahorr√© Un Click: A Revised Definition of Clickbait and Detection in Spanish News

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Gabriel Mordecki, Guillermo Moncecchi, Javier Couto
- **URL**: <http://arxiv.org/abs/2507.09777v1>
- **Submitted**: 2025-07-13 20:19:08
- **Topic Keywords**: click
- **Reason**: The paper focuses on clickbait detection in Spanish news, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on user behavior modeling, it is not in the context of search or e-commerce, and the techniques and datasets proposed are specific to the clickbait detection problem.

#### Abstract
> We revise the definition of clickbait, which lacks current consensus, and
argue that the creation of a curiosity gap is the key concept that
distinguishes clickbait from other related phenomena such as sensationalism and
headlines that do not deliver what they promise or diverge from the article.
Therefore, we propose a new definition: clickbait is a technique for generating
headlines and teasers that deliberately omit part of the information with the
goal of raising the readers' curiosity, capturing their attention and enticing
them to click. We introduce a new approach to clickbait detection datasets
creation, by refining the concept limits and annotations criteria, minimizing
the subjectivity in the decision as much as possible. Following it, we created
and release TA1C (for Te Ahorr\'e Un Click, Spanish for Saved You A Click), the
first open source dataset for clickbait detection in Spanish. It consists of
3,500 tweets coming from 18 well known media sources, manually annotated and
reaching a 0.825 Fleiss' K inter annotator agreement. We implement strong
baselines that achieve 0.84 in F1-score.

### 49. Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Bradley P. Allen, Prateek Chhikara, Thomas Macaulay Ferguson, Filip Ilievski, Paul Groth
- **URL**: <http://arxiv.org/abs/2507.09751v1>
- **Submitted**: 2025-07-13 19:05:43
- **Comment**: 29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on
  Neurosymbolic Learning and Reasoning (NeSy 2025)
- **Topic Keywords**: rag
- **Reason**: The paper focuses on integrating a large language model into a paraconsistent logic, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on natural language processing, the primary focus is on formal reasoning and logical consistency, which is not a central match for the user's research interests.

#### Abstract
> Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but they exhibit problems with
logical consistency in the output they generate. How can we harness LLMs'
broad-coverage parametric knowledge in formal reasoning despite their
inconsistency? We present a method for directly integrating an LLM into the
interpretation function of the formal semantics for a paraconsistent logic. We
provide experimental evidence for the feasibility of the method by evaluating
the function using datasets created from several short-form factuality
benchmarks. Unlike prior work, our method offers a theoretical framework for
neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the
underlying logic's soundness and completeness properties.

### 50. Adapting Definition Modeling for New Languages: A Case Study on Belarusian

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Daniela Kazakouskaya, Timothee Mickus, Janine Siewert
- **URL**: <http://arxiv.org/abs/2507.09536v1>
- **Submitted**: 2025-07-13 08:35:23
- **Comment**: To appear at SlavicNLP 2025
- **Topic Keywords**: rag
- **Reason**: The paper focuses on adapting definition modeling for a new language, Belarusian, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on language-related topics, it does not align with the user's primary research interests in IR, NLP, and data mining.

#### Abstract
> Definition modeling, the task of generating new definitions for words in
context, holds great prospect as a means to assist the work of lexicographers
in documenting a broader variety of lects and languages, yet much remains to be
done in order to assess how we can leverage pre-existing models for as-of-yet
unsupported languages. In this work, we focus on adapting existing models to
Belarusian, for which we propose a novel dataset of 43,150 definitions. Our
experiments demonstrate that adapting a definition modeling systems requires
minimal amounts of data, but that there currently are gaps in what automatic
metrics do capture.

---

