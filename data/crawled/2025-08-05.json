[
    {
        "title": "Test Set Quality in Multilingual LLM Evaluation",
        "abstract": "Several multilingual benchmark datasets have been developed in a\nsemi-automatic manner in the recent past to measure progress and understand the\nstate-of-the-art in the multilingual capabilities of Large Language Models.\nHowever, there is not a lot of attention paid to the quality of the datasets\nthemselves, despite the existence of previous work in identifying errors in\neven fully human-annotated test sets. In this paper, we manually analyze recent\nmultilingual evaluation sets in two languages - French and Telugu, identifying\nseveral errors in the process. We compare the performance difference across\nseveral LLMs with the original and revised versions of the datasets and\nidentify large differences (almost 10% in some cases) in both languages). Based\non these results, we argue that test sets should not be considered immutable\nand should be revisited, checked for correctness, and potentially versioned. We\nend with some recommendations for both the dataset creators as well as\nconsumers on addressing the dataset quality issues.",
        "url": "http://arxiv.org/abs/2508.02635v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02635v1",
        "arxiv_id": "2508.02635v1",
        "authors": [
            "Kranti Chalamalasetti",
            "Gabriel Bernier-Colborne",
            "Yvan Gauthier",
            "Sowmya Vajjala"
        ],
        "submitted": "2025-08-04 17:22:08",
        "source": "arxiv",
        "comment": "Accepted at the 1st Workshop on Multilingual Data Quality Signals,\n  COLM 2025, Short paper. 10 pages in total"
    },
    {
        "title": "Pointer: Linear-Complexity Long-Range Modeling without Pre-training",
        "abstract": "We introduce Pointer, a novel architecture that achieves linear $O(NK)$\ncomplexity for long-range sequence modeling while maintaining superior\nperformance without requiring pre-training. Unlike standard attention\nmechanisms that compute $O(N^2)$ pairwise interactions, our approach uses\nlayer-wise pointer chaining where each layer's pointer selection depends on\nprevious layer's pointer positions, creating explicit long-distance connections\nthrough pointer chains. We demonstrate that this architecture achieves\n$2$--$10\\times$ speedup on long sequences compared to standard transformers,\nmaintains $>95\\%$ accuracy on copy tasks at distances up to 2048 tokens, and\nlearns interpretable pointer patterns that reveal structured dependency\nmodeling. Our experiments on efficiency benchmarks, long-range dependency\ntasks, and interpretability analysis show that Pointer offers a compelling\nalternative to attention mechanisms for scenarios requiring efficient\nlong-range modeling without pre-training dependencies.",
        "url": "http://arxiv.org/abs/2508.02631v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02631v1",
        "arxiv_id": "2508.02631v1",
        "authors": [
            "Zixi Li"
        ],
        "submitted": "2025-08-04 17:19:56",
        "source": "arxiv",
        "comment": "Submitted to Nordic AI Meet 2025"
    },
    {
        "title": "HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and Decision in Embodied Agents",
        "abstract": "Recent advances in multimodal large language models (MLLMs) have enabled\nricher perceptual grounding for code policy generation in embodied agents.\nHowever, most existing systems lack effective mechanisms to adaptively monitor\npolicy execution and repair codes during task completion. In this work, we\nintroduce HyCodePolicy, a hybrid language-based control framework that\nsystematically integrates code synthesis, geometric grounding, perceptual\nmonitoring, and iterative repair into a closed-loop programming cycle for\nembodied agents. Technically, given a natural language instruction, our system\nfirst decomposes it into subgoals and generates an initial executable program\ngrounded in object-centric geometric primitives. The program is then executed\nin simulation, while a vision-language model (VLM) observes selected\ncheckpoints to detect and localize execution failures and infer failure\nreasons. By fusing structured execution traces capturing program-level events\nwith VLM-based perceptual feedback, HyCodePolicy infers failure causes and\nrepairs programs. This hybrid dual feedback mechanism enables self-correcting\nprogram synthesis with minimal human supervision. Our results demonstrate that\nHyCodePolicy significantly improves the robustness and sample efficiency of\nrobot manipulation policies, offering a scalable strategy for integrating\nmultimodal reasoning into autonomous decision-making pipelines.",
        "url": "http://arxiv.org/abs/2508.02629v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02629v1",
        "arxiv_id": "2508.02629v1",
        "authors": [
            "Yibin Liu",
            "Zhixuan Liang",
            "Zanxin Chen",
            "Tianxing Chen",
            "Mengkang Hu",
            "Wanxi Dong",
            "Congsheng Xu",
            "Zhaoming Han",
            "Yusen Qin",
            "Yao Mu"
        ],
        "submitted": "2025-08-04 17:18:14",
        "source": "arxiv",
        "comment": "Accepted to ICCV 2025 Workshop on Multi-Modal Reasoning for Agentic\n  Intelligence"
    },
    {
        "title": "Noosemia: toward a Cognitive and Phenomenological Account of Intentionality Attribution in Human-Generative AI Interaction",
        "abstract": "This paper introduces and formalizes Noosemia, a novel\ncognitive-phenomenological phenomenon emerging from human interaction with\ngenerative AI systems, particularly those enabling dialogic or multimodal\nexchanges. We propose a multidisciplinary framework to explain how, under\ncertain conditions, users attribute intentionality, agency, and even\ninteriority to these systems - a process grounded not in physical resemblance,\nbut in linguistic performance, epistemic opacity, and emergent technological\ncomplexity. By linking an LLM declination of meaning holism to our technical\nnotion of the LLM Contextual Cognitive Field, we clarify how LLMs construct\nmeaning relationally and how coherence and a simulacrum of agency arise at the\nhuman-AI interface. The analysis situates noosemia alongside pareidolia,\nanimism, the intentional stance and the uncanny valley, distinguishing its\nunique characteristics. We also introduce a-noosemia to describe the\nphenomenological withdrawal of such projections. The paper concludes with\nreflections on the broader philosophical, epistemological, and social\nimplications of noosemic dynamics and directions for future research.",
        "url": "http://arxiv.org/abs/2508.02622v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02622v1",
        "arxiv_id": "2508.02622v1",
        "authors": [
            "Enrico De Santis",
            "Antonello Rizzi"
        ],
        "submitted": "2025-08-04 17:10:08",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "HealthFlow: A Self-Evolving AI Agent with Meta Planning for Autonomous Healthcare Research",
        "abstract": "The efficacy of AI agents in healthcare research is hindered by their\nreliance on static, predefined strategies. This creates a critical limitation:\nagents can become better tool-users but cannot learn to become better strategic\nplanners, a crucial skill for complex domains like healthcare. We introduce\nHealthFlow, a self-evolving AI agent that overcomes this limitation through a\nnovel meta-level evolution mechanism. HealthFlow autonomously refines its own\nhigh-level problem-solving policies by distilling procedural successes and\nfailures into a durable, strategic knowledge base. To anchor our research and\nfacilitate reproducible evaluation, we introduce EHRFlowBench, a new benchmark\nfeaturing complex, realistic health data analysis tasks derived from\npeer-reviewed clinical research. Our comprehensive experiments demonstrate that\nHealthFlow's self-evolving approach significantly outperforms state-of-the-art\nagent frameworks. This work marks a necessary shift from building better\ntool-users to designing smarter, self-evolving task-managers, paving the way\nfor more autonomous and effective AI for scientific discovery.",
        "url": "http://arxiv.org/abs/2508.02621v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02621v1",
        "arxiv_id": "2508.02621v1",
        "authors": [
            "Yinghao Zhu",
            "Yifan Qi",
            "Zixiang Wang",
            "Lei Gu",
            "Dehao Sui",
            "Haoran Hu",
            "Xichen Zhang",
            "Ziyi He",
            "Liantao Ma",
            "Lequan Yu"
        ],
        "submitted": "2025-08-04 17:08:47",
        "source": "arxiv",
        "comment": "Code: https://github.com/yhzhu99/HealthFlow"
    },
    {
        "title": "Mitigating Attention Hacking in Preference-Based Reward Modeling via Interaction Distillation",
        "abstract": "The reward model (RM), as the core component of reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs), responsible for\nproviding reward signals to generated responses. However, mainstream preference\nmodeling in RM is inadequate in terms of token-level interaction, making its\njudgment signals vulnerable to being hacked by misallocated attention to\ncontext. This stems from two fundamental limitations: (1) Current preference\nmodeling employs decoder-only architectures, where the unidirectional causal\nattention mechanism leads to forward-decaying intra-sequence attention within\nthe prompt-response sequence. (2) The independent Siamese-encoding paradigm\ninduces the absence of token-level inter-sequence attention between chosen and\nrejected sequences. To address this \"attention hacking\", we propose\n\"Interaction Distillation\", a novel training framework for more adequate\npreference modeling through attention-level optimization. The method introduces\nan interaction-based natural language understanding model as the teacher to\nprovide sophisticated token interaction patterns via comprehensive attention,\nand guides the preference modeling to simulate teacher model's interaction\npattern through an attentional alignment objective. Through extensive\nexperiments, interaction distillation has demonstrated its ability to provide\nmore stable and generalizable reward signals compared to state-of-the-art RM\noptimization methods that target data noise, highlighting the attention hacking\nconstitute a more fundamental limitation in RM.",
        "url": "http://arxiv.org/abs/2508.02618v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02618v1",
        "arxiv_id": "2508.02618v1",
        "authors": [
            "Jianxiang Zang",
            "Meiling Ning",
            "Shihan Dou",
            "Jiazheng Zhang",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "submitted": "2025-08-04 17:06:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CharBench: Evaluating the Role of Tokenization in Character-Level Tasks",
        "abstract": "Tasks that require character-level reasoning, such as counting or locating\ncharacters within words, remain challenging for contemporary language models. A\ncommon conjecture is that language models' reliance on subword units, rather\nthan characters, contributes to their struggles with character-level tasks, yet\nrecent studies offer conflicting conclusions about the role of tokenization,\nleaving its impact unclear. To address this gap, we introduce CharBench, a\ncomprehensive benchmark of character-level tasks that is two orders of\nmagnitude larger than existing alternatives. We evaluate a diverse range of\nleading open-weight and proprietary models on CharBench and find that it\npresents a significant challenge to modern LLMs, with an average accuracy of\n43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic\nproperties of words and their segmentations into tokens correspond to model\nperformance. For counting tasks, we find that tokenization properties are\nweakly correlated with correctness, while the length of the queried word and\nthe actual character count play a more significant part. In contrast, for tasks\nrequiring intra-word positional understanding, performance is negatively\ncorrelated with the length of the token containing the queried character,\nsuggesting that longer tokens obscure character position information for LLMs.\nWe encourage future work to build on the benchmark and evaluation methodology\nintroduced here as tools for improving model performance on such tasks.",
        "url": "http://arxiv.org/abs/2508.02591v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02591v1",
        "arxiv_id": "2508.02591v1",
        "authors": [
            "Omri Uzan",
            "Yuval Pinter"
        ],
        "submitted": "2025-08-04 16:46:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Parameter-Efficient Routed Fine-Tuning: Mixture-of-Experts Demands Mixture of Adaptation Modules",
        "abstract": "Mixture-of-Experts (MoE) benefits from a dynamic routing mechanism among\ntheir specialized experts, which existing Parameter- Efficient Fine-Tuning\n(PEFT) strategies fail to leverage. This motivates us to investigate whether\nadaptation modules themselves should incorporate routing mechanisms to align\nwith MoE's multi-expert architecture. We analyze dynamics of core components\nwhen applying PEFT to MoE language models and examine how different routing\nstrategies affect adaptation effectiveness. Extensive experiments adapting\nOLMoE-1B-7B and Mixtral-8x7B on various commonsense and math reasoning tasks\nvalidate the performance and efficiency of our routed approach. We identify the\noptimal configurations for different scenarios and provide empirical analyses\nwith practical insights to facilitate better PEFT and MoE applications.",
        "url": "http://arxiv.org/abs/2508.02587v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02587v1",
        "arxiv_id": "2508.02587v1",
        "authors": [
            "Yilun Liu",
            "Yunpu Ma",
            "Yuetian Lu",
            "Shuo Chen",
            "Zifeng Ding",
            "Volker Tresp"
        ],
        "submitted": "2025-08-04 16:43:09",
        "source": "arxiv",
        "comment": "This paper is a preprint under review. arXiv admin note: text overlap\n  with arXiv:2411.08212"
    },
    {
        "title": "MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification",
        "abstract": "Leveraging outputs from multiple large language models (LLMs) is emerging as\na method for harnessing their power across a wide range of tasks while\nmitigating their capacity for making errors, e.g., hallucinations. However,\ncurrent approaches to combining insights from multiple LLMs often involve\nunstructured interactions (e.g., free debate), resulting in model generations\nthat are not faithfully justifiable. In this work, we introduce MArgE, a novel\nframework to provide formal structure to the evidence from each LLM, in the\nform of a tree of extracted arguments, for the task of claim verification. We\nuse a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks\nand semantics from the field of computational argumentation, to construct\nstructured argument trees for given claims. This process creates an inspectable\npathway from the initial arguments to the final claim verification decisions,\nproviding a faithful justification thereof. We show experimentally that MArgE\ncan significantly outperform single LLMs, including three open-source models\n(4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior\nmethods for unstructured multi-LLM debates. We thus demonstrate the advantages\nof incorporating formal, argumentative reasoning mechanisms when combining\nmultiple LLM outputs.",
        "url": "http://arxiv.org/abs/2508.02584v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02584v1",
        "arxiv_id": "2508.02584v1",
        "authors": [
            "Ming Pok Ng",
            "Junqi Jiang",
            "Gabriel Freedman",
            "Antonio Rago",
            "Francesca Toni"
        ],
        "submitted": "2025-08-04 16:40:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare",
        "abstract": "Arabic-language patient feedback remains under-analysed because dialect\ndiversity and scarce aspect-level sentiment labels hinder automated assessment.\nTo address this gap, we introduce EHSAN, a data-centric hybrid pipeline that\nmerges ChatGPT pseudo-labelling with targeted human review to build the first\nexplainable Arabic aspect-based sentiment dataset for healthcare. Each sentence\nis annotated with an aspect and sentiment label (positive, negative, or\nneutral), forming a pioneering Arabic dataset aligned with healthcare themes,\nwith ChatGPT-generated rationales provided for each label to enhance\ntransparency. To evaluate the impact of annotation quality on model\nperformance, we created three versions of the training data: a fully supervised\nset with all labels reviewed by humans, a semi-supervised set with 50% human\nreview, and an unsupervised set with only machine-generated labels. We\nfine-tuned two transformer models on these datasets for both aspect and\nsentiment classification. Experimental results show that our Arabic-specific\nmodel achieved high accuracy even with minimal human supervision, reflecting\nonly a minor performance drop when using ChatGPT-only labels. Reducing the\nnumber of aspect classes notably improved classification metrics across the\nboard. These findings demonstrate an effective, scalable approach to Arabic\naspect-based sentiment analysis (SA) in healthcare, combining large language\nmodel annotation with human expertise to produce a robust and explainable\ndataset. Future directions include generalisation across hospitals, prompt\nrefinement, and interpretable data-driven modelling.",
        "url": "http://arxiv.org/abs/2508.02574v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02574v1",
        "arxiv_id": "2508.02574v1",
        "authors": [
            "Eman Alamoudi",
            "Ellis Solaiman"
        ],
        "submitted": "2025-08-04 16:28:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Guess or Recall? Training CNNs to Classify and Localize Memorization in LLMs",
        "abstract": "Verbatim memorization in Large Language Models (LLMs) is a multifaceted\nphenomenon involving distinct underlying mechanisms. We introduce a novel\nmethod to analyze the different forms of memorization described by the existing\ntaxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the\nattention weights of the LLM and evaluate the alignment between this taxonomy\nand the attention weights involved in decoding.\n  We find that the existing taxonomy performs poorly and fails to reflect\ndistinct mechanisms within the attention blocks. We propose a new taxonomy that\nmaximizes alignment with the attention weights, consisting of three categories:\nmemorized samples that are guessed using language modeling abilities, memorized\nsamples that are recalled due to high duplication in the training set, and\nnon-memorized samples. Our results reveal that few-shot verbatim memorization\ndoes not correspond to a distinct attention mechanism. We also show that a\nsignificant proportion of extractable samples are in fact guessed by the model\nand should therefore be studied separately. Finally, we develop a custom visual\ninterpretability technique to localize the regions of the attention weights\ninvolved in each form of memorization.",
        "url": "http://arxiv.org/abs/2508.02573v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02573v1",
        "arxiv_id": "2508.02573v1",
        "authors": [
            "Jérémie Dentan",
            "Davide Buscaldi",
            "Sonia Vanier"
        ],
        "submitted": "2025-08-04 16:27:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
        "abstract": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness.",
        "url": "http://arxiv.org/abs/2508.02558v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02558v1",
        "arxiv_id": "2508.02558v1",
        "authors": [
            "Yuerong Song",
            "Xiaoran Liu",
            "Ruixiao Li",
            "Zhigeng Liu",
            "Zengfeng Huang",
            "Qipeng Guo",
            "Ziwei He",
            "Xipeng Qiu"
        ],
        "submitted": "2025-08-04 16:14:03",
        "source": "arxiv",
        "comment": "11 pages, 6 figures"
    },
    {
        "title": "Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks",
        "abstract": "Automated annotation of clinical text with standardized medical concepts is\ncritical for enabling structured data extraction and decision support. SNOMED\nCT provides a rich ontology for labeling clinical entities, but manual\nannotation is labor-intensive and impractical at scale. This study introduces a\nneural sequence labeling approach for SNOMED CT concept recognition using a\nBidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text\nwith domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences\ninto overlapping 19-token chunks enriched with contextual, syntactic, and\nmorphological features. The Bi-GRU model assigns IOB tags to identify concept\nspans and achieves strong performance with a 90 percent F1-score on the\nvalidation set. These results surpass traditional rule-based systems and match\nor exceed existing neural models. Qualitative analysis shows effective handling\nof ambiguous terms and misspellings. Our findings highlight that lightweight\nRNN-based architectures can deliver high-quality clinical concept annotation\nwith significantly lower computational cost than transformer-based models,\nmaking them well-suited for real-world deployment.",
        "url": "http://arxiv.org/abs/2508.02556v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02556v1",
        "arxiv_id": "2508.02556v1",
        "authors": [
            "Ali Noori",
            "Pratik Devkota",
            "Somya Mohanty",
            "Prashanti Manda"
        ],
        "submitted": "2025-08-04 16:08:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Building and Aligning Comparable Corpora",
        "abstract": "Comparable corpus is a set of topic aligned documents in multiple languages,\nwhich are not necessarily translations of each other. These documents are\nuseful for multilingual natural language processing when there is no parallel\ntext available in some domains or languages. In addition, comparable documents\nare informative because they can tell what is being said about a topic in\ndifferent languages. In this paper, we present a method to build comparable\ncorpora from Wikipedia encyclopedia and EURONEWS website in English, French and\nArabic languages. We further experiment a method to automatically align\ncomparable documents using cross-lingual similarity measures. We investigate\ntwo cross-lingual similarity measures to align comparable documents. The first\nmeasure is based on bilingual dictionary, and the second measure is based on\nLatent Semantic Indexing (LSI). Experiments on several corpora show that the\nCross-Lingual LSI (CL-LSI) measure outperforms the dictionary based measure.\nFinally, we collect English and Arabic news documents from the British\nBroadcast Corporation (BBC) and from ALJAZEERA (JSC) news website respectively.\nThen we use the CL-LSI similarity measure to automatically align comparable\ndocuments of BBC and JSC. The evaluation of the alignment shows that CL-LSI is\nnot only able to align cross-lingual documents at the topic level, but also it\nis able to do this at the event level.",
        "url": "http://arxiv.org/abs/2508.02555v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02555v1",
        "arxiv_id": "2508.02555v1",
        "authors": [
            "Motaz Saad",
            "David Langlois",
            "Kamel Smaili"
        ],
        "submitted": "2025-08-04 16:05:36",
        "source": "arxiv",
        "comment": "27 pages, 11 figures"
    },
    {
        "title": "What are you sinking? A geometric approach on attention sink",
        "abstract": "Attention sink (AS) is a consistent pattern in transformer attention maps\nwhere certain tokens (often special tokens or positional anchors)\ndisproportionately attract attention from other tokens. We show that in\ntransformers, AS is not an architectural artifact, but it is the manifestation\nof a fundamental geometric principle: the establishment of reference frames\nthat anchor representational spaces. We analyze several architectures and\nidentify three distinct reference frame types, centralized, distributed, and\nbidirectional, that correlate with the attention sink phenomenon. We show that\nthey emerge during the earliest stages of training as optimal solutions to the\nproblem of establishing stable coordinate systems in high-dimensional spaces.\nWe show the influence of architecture components, particularly position\nencoding implementations, on the specific type of reference frame. This\nperspective transforms our understanding of transformer attention mechanisms\nand provides insights for both architecture design and the relationship with\nAS.",
        "url": "http://arxiv.org/abs/2508.02546v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02546v1",
        "arxiv_id": "2508.02546v1",
        "authors": [
            "Valeria Ruscio",
            "Umberto Nanni",
            "Fabrizio Silvestri"
        ],
        "submitted": "2025-08-04 15:59:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "What's in the News? Towards Identification of Bias by Commission, Omission, and Source Selection (COSS)",
        "abstract": "In a world overwhelmed with news, determining which information comes from\nreliable sources or how neutral is the reported information in the news\narticles poses a challenge to news readers. In this paper, we propose a\nmethodology for automatically identifying bias by commission, omission, and\nsource selection (COSS) as a joint three-fold objective, as opposed to the\nprevious work separately addressing these types of bias. In a pipeline concept,\nwe describe the goals and tasks of its steps toward bias identification and\nprovide an example of a visualization that leverages the extracted features and\npatterns of text reuse.",
        "url": "http://arxiv.org/abs/2508.02540v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02540v1",
        "arxiv_id": "2508.02540v1",
        "authors": [
            "Anastasia Zhukova",
            "Terry Ruas",
            "Felix Hamborg",
            "Karsten Donnay",
            "Bela Gipp"
        ],
        "submitted": "2025-08-04 15:47:17",
        "source": "arxiv",
        "comment": "published in the Proceedings of the 2023 ACM/IEEE Joint Conference on\n  Digital Libraries"
    },
    {
        "title": "Hubness Reduction with Dual Bank Sinkhorn Normalization for Cross-Modal Retrieval",
        "abstract": "The past decade has witnessed rapid advancements in cross-modal retrieval,\nwith significant progress made in accurately measuring the similarity between\ncross-modal pairs. However, the persistent hubness problem, a phenomenon where\na small number of targets frequently appear as nearest neighbors to numerous\nqueries, continues to hinder the precision of similarity measurements. Despite\nseveral proposed methods to reduce hubness, their underlying mechanisms remain\npoorly understood. To bridge this gap, we analyze the widely-adopted Inverted\nSoftmax approach and demonstrate its effectiveness in balancing target\nprobabilities during retrieval. Building on these insights, we propose a\nprobability-balancing framework for more effective hubness reduction. We\ncontend that balancing target probabilities alone is inadequate and, therefore,\nextend the framework to balance both query and target probabilities by\nintroducing Sinkhorn Normalization (SN). Notably, we extend SN to scenarios\nwhere the true query distribution is unknown, showing that current methods,\nwhich rely solely on a query bank to estimate target hubness, produce\nsuboptimal results due to a significant distributional gap between the query\nbank and targets. To mitigate this issue, we introduce Dual Bank Sinkhorn\nNormalization (DBSN), incorporating a corresponding target bank alongside the\nquery bank to narrow this distributional gap. Our comprehensive evaluation\nacross various cross-modal retrieval tasks, including image-text retrieval,\nvideo-text retrieval, and audio-text retrieval, demonstrates consistent\nperformance improvements, validating the effectiveness of both SN and DBSN. All\ncodes are publicly available at https://github.com/ppanzx/DBSN.",
        "url": "http://arxiv.org/abs/2508.02538v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02538v1",
        "arxiv_id": "2508.02538v1",
        "authors": [
            "Zhengxin Pan",
            "Haishuai Wang",
            "Fangyu Wu",
            "Peng Zhang",
            "Jiajun Bu"
        ],
        "submitted": "2025-08-04 15:45:48",
        "source": "arxiv",
        "comment": "ACMMM 2025"
    },
    {
        "title": "Contextual Graph Transformer: A Small Language Model for Enhanced Engineering Document Information Extraction",
        "abstract": "Standard transformer-based language models, while powerful for general text,\noften struggle with the fine-grained syntax and entity relationships in complex\ntechnical, engineering documents. To address this, we propose the Contextual\nGraph Transformer (CGT), a hybrid neural architecture that combines Graph\nNeural Networks (GNNs) and Transformers for domain-specific question answering.\nCGT constructs a dynamic graph over input tokens using sequential, skip-gram,\nand semantic similarity edges, which is processed by GATv2Conv layers for local\nstructure learning. These enriched embeddings are then passed to a Transformer\nencoder to capture global dependencies. Unlike generic large models, technical\ndomains often require specialized language models with stronger\ncontextualization and structure awareness. CGT offers a parameter-efficient\nsolution for such use cases. Integrated into a Retrieval-Augmented Generation\n(RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7%\nhigher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems from\nCGTs ability to jointly model structural token interactions and long-range\nsemantic coherence. The model is trained from scratch using a two-phase\napproach: pretraining on general text followed by fine-tuning on\ndomain-specific manuals. This highlights CGTs adaptability to technical\nlanguage, enabling better grounding, entity tracking, and retrieval-augmented\nresponses in real-world applications.",
        "url": "http://arxiv.org/abs/2508.02532v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02532v1",
        "arxiv_id": "2508.02532v1",
        "authors": [
            "Karan Reddy",
            "Mayukha Pal"
        ],
        "submitted": "2025-08-04 15:41:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "I Have No Mouth, and I Must Rhyme: Uncovering Internal Phonetic Representations in LLaMA 3.2",
        "abstract": "Large language models demonstrate proficiency on phonetic tasks, such as\nrhyming, without explicit phonetic or auditory grounding. In this work, we\ninvestigate how \\verb|Llama-3.2-1B-Instruct| represents token-level phonetic\ninformation. Our results suggest that Llama uses a rich internal model of\nphonemes to complete phonetic tasks. We provide evidence for high-level\norganization of phoneme representations in its latent space. In doing so, we\nalso identify a ``phoneme mover head\" which promotes phonetic information\nduring rhyming tasks. We visualize the output space of this head and find that,\nwhile notable differences exist, Llama learns a model of vowels similar to the\nstandard IPA vowel chart for humans, despite receiving no direct supervision to\ndo so.",
        "url": "http://arxiv.org/abs/2508.02527v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02527v1",
        "arxiv_id": "2508.02527v1",
        "authors": [
            "Jack Merullo",
            "Arjun Khurana",
            "Oliver McLaughlin"
        ],
        "submitted": "2025-08-04 15:36:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs",
        "abstract": "This paper presents a systematic investigation into the constrained\ngeneration capabilities of large language models (LLMs) in producing Songci, a\nclassical Chinese poetry form characterized by strict structural, tonal, and\nrhyme constraints defined by Cipai templates. We first develop a comprehensive,\nmulti-faceted evaluation framework that includes: (i) a formal conformity\nscore, (ii) automated quality assessment using LLMs, (iii) human evaluation,\nand (iv) classification-based probing tasks. Using this framework, we evaluate\nthe generative performance of 18 LLMs, including 3 proprietary models and 15\nopen-source models across four families, under five prompting strategies:\nzero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought.\nFinally, we propose a Generate-Critic architecture in which the evaluation\nframework functions as an automated critic. Leveraging the critic's feedback as\na reward signal, we fine-tune three lightweight open-source LLMs via supervised\nfine-tuning (SFT), resulting in improvements of up to 5.88% in formal\nconformity. Our findings offer new insights into the generative strengths and\nlimitations of LLMs in producing culturally significant and formally\nconstrained literary texts.",
        "url": "http://arxiv.org/abs/2508.02515v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02515v1",
        "arxiv_id": "2508.02515v1",
        "authors": [
            "Zhan Qu",
            "Shuzhou Yuan",
            "Michael Färber"
        ],
        "submitted": "2025-08-04 15:19:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Modular Arithmetic: Language Models Solve Math Digit by Digit",
        "abstract": "While recent work has begun to uncover the internal strategies that Large\nLanguage Models (LLMs) employ for simple arithmetic tasks, a unified\nunderstanding of their underlying mechanisms is still lacking. We extend recent\nfindings showing that LLMs represent numbers in a digit-wise manner and present\nevidence for the existence of digit-position-specific circuits that LLMs use to\nperform simple arithmetic tasks, i.e. modular subgroups of MLP neurons that\noperate independently on different digit positions (units, tens, hundreds).\nNotably, such circuits exist independently of model size and of tokenization\nstrategy, i.e. both for models that encode longer numbers digit-by-digit and as\none token. Using Feature Importance and Causal Interventions, we identify and\nvalidate the digit-position-specific circuits, revealing a compositional and\ninterpretable structure underlying the solving of arithmetic problems in LLMs.\nOur interventions selectively alter the model's prediction at targeted digit\npositions, demonstrating the causal role of digit-position circuits in solving\narithmetic tasks.",
        "url": "http://arxiv.org/abs/2508.02513v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02513v1",
        "arxiv_id": "2508.02513v1",
        "authors": [
            "Tanja Baeumel",
            "Daniil Gurgurov",
            "Yusser al Ghussin",
            "Josef van Genabith",
            "Simon Ostermann"
        ],
        "submitted": "2025-08-04 15:18:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Test-time Prompt Intervention",
        "abstract": "Test-time compute has led to remarkable success in the large language model\n(LLM) community, particularly for complex tasks, where longer chains of thought\n(CoTs) are generated to enhance reasoning capabilities. However, growing\nevidence reveals that such reasoning models often produce CoTs plagued by\nexcessive redundancy, including unnecessary verification steps and repetitive\nreasoning shifts. The root cause lies in post-training of them that overly rely\non outcome reward paradigms, as the data of process reward paradigms, which\nregulate intermediate reasoning steps, is difficult to construct at scale. To\naddress this, we propose PI, a novel framework for Test-time Prompt\nIntervention. PI provides an interface to dynamically guide and regulate\nreasoning paths during inference through timely (When module) and proper (How\nmodule) interventions and post-intervention sampling (Which module). This\nallows human problem-solving expertise and cognitive science principles to be\nseamlessly integrated into LLMs' reasoning processes, enhancing controllability\nand interpretability. Extensive experiments across multiple models and datasets\ndemonstrate that PI significantly shortens CoTs while reducing hallucination,\nyielding more concise and reliable reasoning.",
        "url": "http://arxiv.org/abs/2508.02511v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02511v1",
        "arxiv_id": "2508.02511v1",
        "authors": [
            "Chenxu Yang",
            "Qingyi Si",
            "Mz Dai",
            "Dingyu Yao",
            "Mingyu Zheng",
            "Minghui Chen",
            "Zheng Lin",
            "Weiping Wang"
        ],
        "submitted": "2025-08-04 15:17:13",
        "source": "arxiv",
        "comment": "23 pages, 16 figures, under review"
    },
    {
        "title": "Decomposed Reasoning with Reinforcement Learning for Relevance Assessment in UGC Platforms",
        "abstract": "Retrieval-augmented generation (RAG) plays a critical role in user-generated\ncontent (UGC) platforms, but its effectiveness depends heavily on accurate\nrelevance assessment of query-document pairs. Despite recent advances in\napplying large language models (LLMs) to relevance modeling, UGC platforms\npresent unique challenges: 1) ambiguous user intent due to sparse user feedback\nin RAG scenarios, and 2) substantial noise introduced by informal and\nunstructured language. To address these issues, we propose the Reinforced\nReasoning Model for Relevance Assessment (R3A), which introduces a decomposed\nreasoning framework over queries and candidate documents before scoring. R3A\nfirst leverages auxiliary high-ranked documents within the platform to infer\nlatent query intent. It then performs verbatim fragment extraction to justify\nrelevance decisions, thereby reducing errors caused by noisy UGC. Based on a\nreinforcement learning framework, R3A is optimized to mitigate distortions\narising from ambiguous queries and unstructured content. Experimental results\nshow that R3A significantly outperforms existing baseline methods in terms of\nrelevance accuracy, across both offline benchmarks and online experiments.",
        "url": "http://arxiv.org/abs/2508.02506v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02506v1",
        "arxiv_id": "2508.02506v1",
        "authors": [
            "Xiaowei Yuan",
            "Lei Jin",
            "Haoxin Zhang",
            "Yan Gao",
            "Yi Wu",
            "Yao Hu",
            "Ziyang Huang",
            "Jun Zhao",
            "Kang Liu"
        ],
        "submitted": "2025-08-04 15:14:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical Modeling",
        "abstract": "LLM-based solvers have emerged as a promising means of automating problem\nmodeling and solving. However, they remain unreliable and often depend on\niterative repair loops that result in significant latency. We introduce\nOptiHive, an LLM-based framework that produces high-quality solvers for\noptimization problems from natural-language descriptions without iterative\nself-correction. OptiHive uses a single batched LLM query to generate diverse\ncomponents (solvers, problem instances, and validation tests) and filters out\nerroneous components to ensure fully interpretable outputs. Taking into account\nthe imperfection of the generated components, we employ a statistical model to\ninfer their true performance, enabling principled uncertainty quantification\nand solver selection. On tasks ranging from traditional optimization problems\nto challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive\nsignificantly outperforms baselines, increasing the optimality rate from 5\\% to\n92\\% on the most complex problems.",
        "url": "http://arxiv.org/abs/2508.02503v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02503v1",
        "arxiv_id": "2508.02503v1",
        "authors": [
            "Maxime Bouscary",
            "Saurabh Amin"
        ],
        "submitted": "2025-08-04 15:11:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "From Monolingual to Bilingual: Investigating Language Conditioning in Large Language Models for Psycholinguistic Tasks",
        "abstract": "Large Language Models (LLMs) exhibit strong linguistic capabilities, but\nlittle is known about how they encode psycholinguistic knowledge across\nlanguages. We investigate whether and how LLMs exhibit human-like\npsycholinguistic responses under different linguistic identities using two\ntasks: sound symbolism and word valence. We evaluate two models,\nLlama-3.3-70B-Instruct and Qwen2.5-72B-Instruct, under monolingual and\nbilingual prompting in English, Dutch, and Chinese. Behaviorally, both models\nadjust their outputs based on prompted language identity, with Qwen showing\ngreater sensitivity and sharper distinctions between Dutch and Chinese. Probing\nanalysis reveals that psycholinguistic signals become more decodable in deeper\nlayers, with Chinese prompts yielding stronger and more stable valence\nrepresentations than Dutch. Our results demonstrate that language identity\nconditions both output behavior and internal representations in LLMs, providing\nnew insights into their application as models of cross-linguistic cognition.",
        "url": "http://arxiv.org/abs/2508.02502v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02502v1",
        "arxiv_id": "2508.02502v1",
        "authors": [
            "Shuzhou Yuan",
            "Zhan Qu",
            "Mario Tawfelis",
            "Michael Färber"
        ],
        "submitted": "2025-08-04 15:10:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Monsoon Uprising in Bangladesh: How Facebook Shaped Collective Identity",
        "abstract": "This study investigates how Facebook shaped collective identity during the\nJuly 2024 pro-democracy uprising in Bangladesh, known as the Monsoon Uprising.\nDuring government repression, protesters turned to Facebook as a central space\nfor resistance, where multimodal expressions, images, memes, videos, hashtags,\nand satirical posts played an important role in unifying participants. Using a\nqualitative approach, this research analyzes visual rhetoric, verbal discourse,\nand digital irony to reveal how shared symbols, protest art, and slogans built\na sense of solidarity. Key elements included the symbolic use of red, the\nironic metaphorical use of the term \"Razakar\", and the widespread sharing of\nvisuals representing courage, injustice, and resistance. The findings show that\nthe combination of visual and verbal strategies on Facebook not only mobilized\npublic sentiment, but also built a strong collective identity that challenged\nauthoritarian narratives. This study tries to demonstrate how online platforms\ncan serve as powerful tools for identity construction and political\nmobilization in the digital age.",
        "url": "http://arxiv.org/abs/2508.02498v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02498v1",
        "arxiv_id": "2508.02498v1",
        "authors": [
            "Md Tasin Abir",
            "Arpita Chowdhury",
            "Ashfia Rahman"
        ],
        "submitted": "2025-08-04 15:07:38",
        "source": "arxiv",
        "comment": "10 pages, 9 figures"
    },
    {
        "title": "AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language and Multi-Agent Collaboration",
        "abstract": "While many tools are available for designing AI, non-experts still face\nchallenges in clearly expressing their intent and managing system complexity.\nWe introduce AIAP, a no-code platform that integrates natural language input\nwith visual workflows. AIAP leverages a coordinated multi-agent system to\ndecompose ambiguous user instructions into modular, actionable steps, hidden\nfrom users behind a unified interface. A user study involving 32 participants\nshowed that AIAP's AI-generated suggestions, modular workflows, and automatic\nidentification of data, actions, and context significantly improved\nparticipants' ability to develop services intuitively. These findings highlight\nthat natural language-based visual programming significantly reduces barriers\nand enhances user experience in AI service design.",
        "url": "http://arxiv.org/abs/2508.02470v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02470v1",
        "arxiv_id": "2508.02470v1",
        "authors": [
            "Hyunjn An",
            "Yongwon Kim",
            "Wonduk Seo",
            "Joonil Park",
            "Daye Kang",
            "Changhoon Oh",
            "Dokyun Kim",
            "Seunghyun Lee"
        ],
        "submitted": "2025-08-04 14:36:31",
        "source": "arxiv",
        "comment": "14 pages, 6 figures"
    },
    {
        "title": "TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions in IDEs",
        "abstract": "Token-level code completion is one of the most critical features in modern\nIntegrated Development Environments (IDEs). It assists developers by suggesting\nrelevant identifiers and APIs during coding. While completions are typically\nderived from static analysis, their usefulness depends heavily on how they are\nranked, as correct predictions buried deep in the list are rarely seen by\nusers. Most current systems rely on hand-crafted heuristics or lightweight\nmachine learning models trained on user logs, which can be further improved to\ncapture context information and generalize across projects and coding styles.\nIn this work, we propose a new scoring approach to ranking static completions\nusing language models in a lightweight and model-agnostic way. Our method\norganizes all valid completions into a prefix tree and performs a single greedy\ndecoding pass to collect token-level scores across the tree. This enables a\nprecise token-aware ranking without needing beam search, prompt engineering, or\nmodel adaptations. The approach is fast, architecture-agnostic, and compatible\nwith already deployed models for code completion. These findings highlight a\npractical and effective pathway for integrating language models into already\nexisting tools within IDEs, and ultimately providing smarter and more\nresponsive developer assistance.",
        "url": "http://arxiv.org/abs/2508.02455v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02455v1",
        "arxiv_id": "2508.02455v1",
        "authors": [
            "Daniele Cipollone",
            "Egor Bogomolov",
            "Arie van Deursen",
            "Maliheh Izadi"
        ],
        "submitted": "2025-08-04 14:20:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LatentPrompt: Optimizing Promts in Latent Space",
        "abstract": "Recent advances have shown that optimizing prompts for Large Language Models\n(LLMs) can significantly improve task performance, yet many optimization\ntechniques rely on heuristics or manual exploration. We present LatentPrompt, a\nmodel-agnostic framework for prompt optimization that leverages latent semantic\nspace to automatically generate, evaluate, and refine candidate prompts without\nrequiring hand-crafted rules. Beginning with a set of seed prompts, our method\nembeds them in a continuous latent space and systematically explores this space\nto identify prompts that maximize task-specific performance. In a\nproof-of-concept study on the Financial PhraseBank sentiment classification\nbenchmark, LatentPrompt increased classification accuracy by approximately 3\npercent after a single optimization cycle. The framework is broadly applicable,\nrequiring only black-box access to an LLM and an automatic evaluation metric,\nmaking it suitable for diverse domains and tasks.",
        "url": "http://arxiv.org/abs/2508.02452v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02452v1",
        "arxiv_id": "2508.02452v1",
        "authors": [
            "Mateusz Bystroński",
            "Grzegorz Piotrowski",
            "Nitesh V. Chawla",
            "Tomasz Kajdanowicz"
        ],
        "submitted": "2025-08-04 14:17:29",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Dynamic Forgetting and Spatio-Temporal Periodic Interest Modeling for Local-Life Service Recommendation",
        "abstract": "In the context of the booming digital economy, recommendation systems, as a\nkey link connecting users and numerous services, face challenges in modeling\nuser behavior sequences on local-life service platforms, including the sparsity\nof long sequences and strong spatio-temporal dependence. Such challenges can be\naddressed by drawing an analogy to the forgetting process in human memory. This\nis because users' responses to recommended content follow the recency effect\nand the cyclicality of memory. By exploring this, this paper introduces the\nforgetting curve and proposes Spatio-Temporal periodic Interest Modeling (STIM)\nwith long sequences for local-life service recommendation. STIM integrates\nthree key components: a dynamic masking module based on the forgetting curve,\nwhich is used to extract both recent spatiotemporal features and periodic\nspatiotemporal features; a query-based mixture of experts (MoE) approach that\ncan adaptively activate expert networks under different dynamic masks, enabling\nthe collaborative modeling of time, location, and items; and a hierarchical\nmulti-interest network unit, which captures multi-interest representations by\nmodeling the hierarchical interactions between the shallow and deep semantics\nof users' recent behaviors. By introducing the STIM method, we conducted online\nA/B tests and achieved a 1.54\\% improvement in gross transaction volume (GTV).\nIn addition, extended offline experiments also showed improvements. STIM has\nbeen deployed in a large-scale local-life service recommendation system,\nserving hundreds of millions of daily active users in core application\nscenarios.",
        "url": "http://arxiv.org/abs/2508.02451v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02451v1",
        "arxiv_id": "2508.02451v1",
        "authors": [
            "Zhaoyu Hu",
            "Hao Guo",
            "Yuan Tian",
            "Erpeng Xue",
            "Jianyang Wang",
            "Xianyang Qi",
            "Hongxiang Lin",
            "Lei Wang",
            "Sheng Chen"
        ],
        "submitted": "2025-08-04 14:16:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond Chunks and Graphs: Retrieval-Augmented Generation through Triplet-Driven Thinking",
        "abstract": "Retrieval-augmented generation (RAG) is critical for reducing hallucinations\nand incorporating external knowledge into Large Language Models (LLMs).\nHowever, advanced RAG systems face a trade-off between performance and\nefficiency. Multi-round RAG approaches achieve strong reasoning but incur\nexcessive LLM calls and token costs, while Graph RAG methods suffer from\ncomputationally expensive, error-prone graph construction and retrieval\nredundancy. To address these challenges, we propose T$^2$RAG, a novel framework\nthat operates on a simple, graph-free knowledge base of atomic triplets.\nT$^2$RAG leverages an LLM to decompose questions into searchable triplets with\nplaceholders, which it then iteratively resolves by retrieving evidence from\nthe triplet database. Empirical results show that T$^2$RAG significantly\noutperforms state-of-the-art multi-round and Graph RAG methods, achieving an\naverage performance gain of up to 11\\% across six datasets while reducing\nretrieval costs by up to 45\\%. Our code is available at\nhttps://github.com/rockcor/T2RAG",
        "url": "http://arxiv.org/abs/2508.02435v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02435v1",
        "arxiv_id": "2508.02435v1",
        "authors": [
            "Shengbo Gong",
            "Xianfeng Tang",
            "Carl Yang",
            "Wei jin"
        ],
        "submitted": "2025-08-04 13:50:44",
        "source": "arxiv",
        "comment": "19 pages"
    },
    {
        "title": "AI-Based Measurement of Innovation: Mapping Expert Insight into Large Language Model Applications",
        "abstract": "Measuring innovation often relies on context-specific proxies and on expert\nevaluation. Hence, empirical innovation research is often limited to settings\nwhere such data is available. We investigate how large language models (LLMs)\ncan be leveraged to overcome the constraints of manual expert evaluations and\nassist researchers in measuring innovation. We design an LLM framework that\nreliably approximates domain experts' assessment of innovation from\nunstructured text data. We demonstrate the performance and broad applicability\nof this framework through two studies in different contexts: (1) the\ninnovativeness of software application updates and (2) the originality of\nuser-generated feedback and improvement ideas in product reviews. We compared\nthe performance (F1-score) and reliability (consistency rate) of our LLM\nframework against alternative measures used in prior innovation studies, and to\nstate-of-the-art machine learning- and deep learning-based models. The LLM\nframework achieved higher F1-scores than the other approaches, and its results\nare highly consistent (i.e., results do not change across runs). This article\nequips R&D personnel in firms, as well as researchers, reviewers, and editors,\nwith the knowledge and tools to effectively use LLMs for measuring innovation\nand evaluating the performance of LLM-based innovation measures. In doing so,\nwe discuss, the impact of important design decisions-including model selection,\nprompt engineering, training data size, training data distribution, and\nparameter settings-on performance and reliability. Given the challenges\ninherent in using human expert evaluation and existing text-based measures, our\nframework has important implications for harnessing LLMs as reliable,\nincreasingly accessible, and broadly applicable research tools for measuring\ninnovation.",
        "url": "http://arxiv.org/abs/2508.02430v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02430v1",
        "arxiv_id": "2508.02430v1",
        "authors": [
            "Robin Nowak",
            "Patrick Figge",
            "Carolin Haeussler"
        ],
        "submitted": "2025-08-04 13:49:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Learning to Evolve: Bayesian-Guided Continual Knowledge Graph Embedding",
        "abstract": "Since knowledge graphs (KG) will continue to evolve in real scenarios,\ntraditional KGE models are only suitable for static knowledge graphs.\nTherefore, continual knowledge graph embedding (CKGE) has attracted the\nattention of researchers. Currently, a key challenge facing CKGE is that the\nmodel is prone to \"catastrophic forgetting\", resulting in the loss of\npreviously learned knowledge. In order to effectively alleviate this problem,\nwe propose a new CKGE model BAKE. First, we note that the Bayesian posterior\nupdate principle provides a natural continual learning strategy that is\ninsensitive to data order and can theoretically effectively resist the\nforgetting of previous knowledge during data evolution. Different from the\nexisting CKGE method, BAKE regards each batch of new data as a Bayesian update\nof the model prior. Under this framework, as long as the posterior distribution\nof the model is maintained, the model can better preserve the knowledge of\nearly snapshots even after evolving through multiple time snapshots. Secondly,\nwe propose a continual clustering method for CKGE, which further directly\ncombats knowledge forgetting by constraining the evolution difference (or\nchange amplitude) between new and old knowledge between different snapshots. We\nconduct extensive experiments on BAKE on multiple datasets, and the results\nshow that BAKE significantly outperforms existing baseline models.",
        "url": "http://arxiv.org/abs/2508.02426v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02426v1",
        "arxiv_id": "2508.02426v1",
        "authors": [
            "Linyu Li",
            "Zhi Jin",
            "Yuanpeng He",
            "Dongming Jin",
            "Yichi Zhang",
            "Haoran Duan",
            "Nyima Tash"
        ],
        "submitted": "2025-08-04 13:46:33",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens",
        "abstract": "Large vision-language models (LVLMs) have demonstrated remarkable multimodal\ncomprehension and reasoning capabilities, but they still suffer from severe\nobject hallucination. Previous studies primarily attribute the flaw to\nlinguistic prior caused by the scale mismatch between visual encoders and large\nlanguage models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon\nLLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs,\ngenerating descriptions inconsistent with visual cues. However, through an\nin-depth investigation of the hallucinated mechanisms, we empirically reveal a\npreviously overlooked phenomenon: LVLMs may ignore not only visual information\nbut also textual modality during hallucination, a behavior termed as modality\nbias, which indicates that LVLMs struggle to simultaneously attend to both\nvisual and textual modalities, leading to fragmented understanding of\nuser-provided instructions. Based on this observation, we propose a simple yet\neffective training-free method to mitigate object hallucination. Concretely, we\nintervene and adjust the attention weights of textual and visual tokens,\nbalancing cross-modal compatibility for better alignment with user intentions.\nFurthermore, we adopt a contrastive decoding strategy to reduce the LVLM's\noverreliance on its parametric knowledge, synergistically enhancing our\nattention manipulation. Extensive experiments confirm the widespread presence\nof modality bias in LVLMs. Notably, our method effectively mitigates\nhallucination across multiple open-source LVLMs and benchmarks, highlighting\nits generalizability and efficacy.",
        "url": "http://arxiv.org/abs/2508.02419v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02419v1",
        "arxiv_id": "2508.02419v1",
        "authors": [
            "Haohan Zheng",
            "Zhenguo Zhang"
        ],
        "submitted": "2025-08-04 13:40:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation",
        "abstract": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.",
        "url": "http://arxiv.org/abs/2508.02401v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02401v1",
        "arxiv_id": "2508.02401v1",
        "authors": [
            "Xiaolin Lin",
            "Jingcun Wang",
            "Olga Kondrateva",
            "Yiyu Shi",
            "Bing Li",
            "Grace Li Zhang"
        ],
        "submitted": "2025-08-04 13:26:16",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Graph Embedding in the Graph Fractional Fourier Transform Domain",
        "abstract": "Spectral graph embedding plays a critical role in graph representation\nlearning by generating low-dimensional vector representations from graph\nspectral information. However, the embedding space of traditional spectral\nembedding methods often exhibit limited expressiveness, failing to exhaustively\ncapture latent structural features across alternative transform domains. To\naddress this issue, we use the graph fractional Fourier transform to extend the\nexisting state-of-the-art generalized frequency filtering embedding (GEFFE)\ninto fractional domains, giving birth to the generalized fractional filtering\nembedding (GEFRFE), which enhances embedding informativeness via the graph\nfractional domain. The GEFRFE leverages graph fractional domain filtering and a\nnonlinear composition of eigenvector components derived from a fractionalized\ngraph Laplacian. To dynamically determine the fractional order, two parallel\nstrategies are introduced: search-based optimization and a ResNet18-based\nadaptive learning. Extensive experiments on six benchmark datasets demonstrate\nthat the GEFRFE captures richer structural features and significantly enhance\nclassification performance. Notably, the proposed method retains computational\ncomplexity comparable to GEFFE approaches.",
        "url": "http://arxiv.org/abs/2508.02383v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02383v1",
        "arxiv_id": "2508.02383v1",
        "authors": [
            "Changjie Sheng",
            "Zhichao Zhang",
            "Wei Yao"
        ],
        "submitted": "2025-08-04 13:09:47",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Uni-Layout: Integrating Human Feedback in Unified Layout Generation and Evaluation",
        "abstract": "Layout generation plays a crucial role in enhancing both user experience and\ndesign efficiency. However, current approaches suffer from task-specific\ngeneration capabilities and perceptually misaligned evaluation metrics, leading\nto limited applicability and ineffective measurement. In this paper, we propose\n\\textit{Uni-Layout}, a novel framework that achieves unified generation,\nhuman-mimicking evaluation and alignment between the two. For universal\ngeneration, we incorporate various layout tasks into a single taxonomy and\ndevelop a unified generator that handles background or element contents\nconstrained tasks via natural language prompts. To introduce human feedback for\nthe effective evaluation of layouts, we build \\textit{Layout-HF100k}, the first\nlarge-scale human feedback dataset with 100,000 expertly annotated layouts.\nBased on \\textit{Layout-HF100k}, we introduce a human-mimicking evaluator that\nintegrates visual and geometric information, employing a Chain-of-Thought\nmechanism to conduct qualitative assessments alongside a confidence estimation\nmodule to yield quantitative measurements. For better alignment between the\ngenerator and the evaluator, we integrate them into a cohesive system by\nadopting Dynamic-Margin Preference Optimization (DMPO), which dynamically\nadjusts margins based on preference strength to better align with human\njudgments. Extensive experiments show that \\textit{Uni-Layout} significantly\noutperforms both task-specific and general-purpose methods. Our code is\npublicly available at https://github.com/JD-GenX/Uni-Layout.",
        "url": "http://arxiv.org/abs/2508.02374v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02374v1",
        "arxiv_id": "2508.02374v1",
        "authors": [
            "Shuo Lu",
            "Yanyin Chen",
            "Wei Feng",
            "Jiahao Fan",
            "Fengheng Li",
            "Zheng Zhang",
            "Jingjing Lv",
            "Junjie Shen",
            "Ching Law",
            "Jian Liang"
        ],
        "submitted": "2025-08-04 13:02:23",
        "source": "arxiv",
        "comment": "Accepted to ACM MM 2025"
    },
    {
        "title": "Six Guidelines for Trustworthy, Ethical and Responsible Automation Design",
        "abstract": "Calibrated trust in automated systems (Lee and See 2004) is critical for\ntheir safe and seamless integration into society. Users should only rely on a\nsystem recommendation when it is actually correct and reject it when it is\nfactually wrong. One requirement to achieve this goal is an accurate\ntrustworthiness assessment, ensuring that the user's perception of the system's\ntrustworthiness aligns with its actual trustworthiness, allowing users to make\ninformed decisions about the extent to which they can rely on the system\n(Schlicker et al. 2022). We propose six design guidelines to help designers\noptimize for accurate trustworthiness assessments, thus fostering ethical and\nresponsible human-automation interactions. The proposed guidelines are derived\nfrom existing literature in various fields, such as human-computer interaction,\ncognitive psychology, automation research, user-experience design, and ethics.\nWe are incorporating key principles from the field of pragmatics, specifically\nthe cultivation of common ground (H. H. Clark 1996) and Gricean communication\nmaxims (Grice 1975). These principles are essential for the design of automated\nsystems because the user's perception of the system's trustworthiness is shaped\nby both environmental contexts, such as organizational culture or societal\nnorms, and by situational context, including the specific circumstances or\nscenarios in which the interaction occurs (Hoff and Bashir 2015). Our proposed\nguidelines provide actionable insights for designers to create automated\nsystems that make relevant trustworthiness cues available. This would ideally\nfoster calibrated trust and more satisfactory, productive, and safe\ninteractions between humans and automated systems. Furthermore, the proposed\nheuristics might work as a tool for evaluating to what extent existing systems\nenable users to accurately assess a system's trustworthiness.",
        "url": "http://arxiv.org/abs/2508.02371v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02371v1",
        "arxiv_id": "2508.02371v1",
        "authors": [
            "Matouš Jelínek",
            "Nadine Schlicker",
            "Ewart de Visser"
        ],
        "submitted": "2025-08-04 13:01:09",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Language Model Guided Reinforcement Learning in Quantitative Trading",
        "abstract": "Algorithmic trading requires short-term decisions aligned with long-term\nfinancial goals. While reinforcement learning (RL) has been explored for such\ntactical decisions, its adoption remains limited by myopic behavior and opaque\npolicy rationale. In contrast, large language models (LLMs) have recently\ndemonstrated strategic reasoning and multi-modal financial signal\ninterpretation when guided by well-designed prompts.\n  We propose a hybrid system where LLMs generate high-level trading strategies\nto guide RL agents in their actions. We evaluate (i) the rationale of\nLLM-generated strategies via expert review, and (ii) the Sharpe Ratio (SR) and\nMaximum Drawdown (MDD) of LLM-guided agents versus unguided baselines. Results\nshow improved return and risk metrics over standard RL.",
        "url": "http://arxiv.org/abs/2508.02366v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02366v1",
        "arxiv_id": "2508.02366v1",
        "authors": [
            "Adam Darmanin",
            "Vince Vella"
        ],
        "submitted": "2025-08-04 12:52:11",
        "source": "arxiv",
        "comment": "12 pages (4 pages appendix and references), 6 figures, preprint under\n  review for FLLM 2025 conference"
    },
    {
        "title": "Understanding and Mitigating Political Stance Cross-topic Generalization in Large Language Models",
        "abstract": "Fine-tuning Large Language Models on a political topic will significantly\nmanipulate their political stance on various issues and unintentionally affect\ntheir stance on unrelated topics. While previous studies have proposed this\nissue, there is still a lack of understanding regarding the internal\nrepresentations of these stances and the mechanisms that lead to unintended\ncross-topic generalization. In this paper, we systematically explore the\ninternal mechanisms underlying this phenomenon from a neuron-level perspective\nand how to mitigate the cross-topic generalization of political fine-tuning.\nFirstly, we propose Political Neuron Localization through Activation\nContrasting (PNLAC) to identify two distinct types of political neurons:\ngeneral political neurons, which govern stance across multiple political\ntopics, and topic-specific neurons} that affect the model's political stance on\nindividual topics. We find the existence of these political neuron types across\nfour models and datasets through activation patching experiments. Leveraging\nthese insights, we introduce InhibitFT, an inhibition-based fine-tuning method,\neffectively mitigating the cross-topic stance generalization. Experimental\nresults demonstrate the robustness of identified neuron types across various\nmodels and datasets, and show that InhibitFT significantly reduces the\ncross-topic stance generalization by 20% on average, while preserving\ntopic-specific performance. Moreover, we demonstrate that selectively\ninhibiting only 5% of neurons is sufficient to effectively mitigate the\ncross-topic stance generalization.",
        "url": "http://arxiv.org/abs/2508.02360v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02360v1",
        "arxiv_id": "2508.02360v1",
        "authors": [
            "Jiayi Zhang",
            "Shu Yang",
            "Junchao Wu",
            "Derek F. Wong",
            "Di Wang"
        ],
        "submitted": "2025-08-04 12:49:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Agentic Personalized Fashion Recommendation in the Age of Generative AI: Challenges, Opportunities, and Evaluation",
        "abstract": "Fashion recommender systems (FaRS) face distinct challenges due to rapid\ntrend shifts, nuanced user preferences, intricate item-item compatibility, and\nthe complex interplay among consumers, brands, and influencers. Traditional\nrecommendation approaches, largely static and retrieval-focused, struggle to\neffectively capture these dynamic elements, leading to decreased user\nsatisfaction and elevated return rates. This paper synthesizes both academic\nand industrial viewpoints to map the distinctive output space and stakeholder\necosystem of modern FaRS, identifying the complex interplay among users,\nbrands, platforms, and influencers, and highlighting the unique data and\nmodeling challenges that arise.\n  We outline a research agenda for industrial FaRS, centered on five\nrepresentative scenarios spanning static queries, outfit composition, and\nmulti-turn dialogue, and argue that mixed-modality refinement-the ability to\ncombine image-based references (anchors) with nuanced textual constraints-is a\nparticularly critical task for real-world deployment. To this end, we propose\nan Agentic Mixed-Modality Refinement (AMMR) pipeline, which fuses multimodal\nencoders with agentic LLM planners and dynamic retrieval, bridging the gap\nbetween expressive user intent and fast-changing fashion inventories. Our work\nshows that moving beyond static retrieval toward adaptive, generative, and\nstakeholder-aware systems is essential to satisfy the evolving expectations of\nfashion consumers and brands.",
        "url": "http://arxiv.org/abs/2508.02342v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02342v1",
        "arxiv_id": "2508.02342v1",
        "authors": [
            "Yashar Deldjoo",
            "Nima Rafiee",
            "Mahdyar Ravanbakhsh"
        ],
        "submitted": "2025-08-04 12:22:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search",
        "abstract": "Ad-hoc Video Search (AVS) involves using a textual query to search for\nmultiple relevant videos in a large collection of unlabeled short videos. The\nmain challenge of AVS is the visual diversity of relevant videos. A simple\nquery such as \"Find shots of a man and a woman dancing together indoors\" can\nspan a multitude of environments, from brightly lit halls and shadowy bars to\ndance scenes in black-and-white animations. It is therefore essential to\nretrieve relevant videos as comprehensively as possible. Current solutions for\nthe AVS task primarily fuse multiple features into one or more common spaces,\nyet overlook the need for diverse spaces. To fully exploit the expressive\ncapability of individual features, we propose LPD, short for Learning Partially\nDecorrelated common spaces. LPD incorporates two key innovations:\nfeature-specific common space construction and the de-correlation loss.\nSpecifically, LPD learns a separate common space for each video and text\nfeature, and employs de-correlation loss to diversify the ordering of negative\nsamples across different spaces. To enhance the consistency of multi-space\nconvergence, we designed an entropy-based fair multi-space triplet ranking\nloss. Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify\nthe effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces\nhighlight its ability to enhance result diversity.",
        "url": "http://arxiv.org/abs/2508.02340v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02340v1",
        "arxiv_id": "2508.02340v1",
        "authors": [
            "Fan Hu",
            "Zijie Xin",
            "Xirong Li"
        ],
        "submitted": "2025-08-04 12:21:16",
        "source": "arxiv",
        "comment": "Accepted by ACMMM2025"
    },
    {
        "title": "Understanding User Preferences for Interaction Styles in Conversational Recommender Systems: The Predictive Role of System Qualities, User Experience, and Traits",
        "abstract": "Conversational Recommender Systems (CRSs) deliver personalised\nrecommendations through multi-turn natural language dialogue and increasingly\nsupport both task-oriented and exploratory interactions. Yet, the factors\nshaping user interaction preferences remain underexplored. In this\nwithin-subjects study (\\(N = 139\\)), participants experienced two scripted CRS\ndialogues, rated their experiences, and indicated the importance of eight\nsystem qualities. Logistic regression revealed that preference for the\nexploratory interaction was predicted by enjoyment, usefulness, novelty, and\nconversational quality. Unexpectedly, perceived effectiveness was also\nassociated with exploratory preference. Clustering uncovered five latent user\nprofiles with distinct dialogue style preferences. Moderation analyses\nindicated that age, gender, and control preference significantly influenced\nthese choices. These findings integrate affective, cognitive, and trait-level\npredictors into CRS user modelling and inform autonomy-sensitive,\nvalue-adaptive dialogue design. The proposed predictive and adaptive framework\napplies broadly to conversational AI systems seeking to align dynamically with\nevolving user needs.",
        "url": "http://arxiv.org/abs/2508.02328v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02328v1",
        "arxiv_id": "2508.02328v1",
        "authors": [
            "Raj Mahmud",
            "Shlomo Berkovsky",
            "Mukesh Prasad",
            "A. Baki Kocaballi"
        ],
        "submitted": "2025-08-04 11:56:47",
        "source": "arxiv",
        "comment": "Accepted at OZCHI 2025. 21 pages, 9 figures, 8 tables"
    },
    {
        "title": "CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis",
        "abstract": "Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are\ndistinguished by their strong performance scaling with increasing parameters\nacross a wide range of tasks, yet they also suffer from substantial\ncomputational and storage overheads. Notably, the performance gains of MoE\nmodels do not scale proportionally with the growth in expert parameters. While\nprior works attempt to reduce parameters via expert-level pruning, merging, or\ndecomposition, they still suffer from challenges in both performance and\ncomputational efficiency. In this paper, we address these challenges by\nintroducing micro-expert as a finer-grained compression unit that spans across\nmatrices. We first establish a more fundamental perspective, viewing MoE layers\nas mixtures of micro-experts, and present CAMERA, a lightweight and\ntraining-free framework for identifying micro-expert redundancy. Our analysis\nuncovers significant variance in micro-expert contributions during decoding.\nBased on this insight, we further propose CAMERA-P, a structured micro-expert\npruning framework, and CAMERA-Q, a mixed-precision quantization idea designed\nfor micro-experts. Extensive experiments on nine downstream tasks show that\nCAMERA-P consistently outperforms strong baselines under pruning ratios ranging\nfrom 20% to 60%. Furthermore, CAMERA-Q achieves superior results under\naggressive 2-bit quantization, surpassing existing matrix- and channel-level\nideas. Notably, our method enables complete micro-expert analysis of\nQwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.",
        "url": "http://arxiv.org/abs/2508.02322v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02322v1",
        "arxiv_id": "2508.02322v1",
        "authors": [
            "Yuzhuang Xu",
            "Xu Han",
            "Yuanchi Zhang",
            "Yixuan Wang",
            "Yijun Liu",
            "Shiyu Ji",
            "Qingfu Zhu",
            "Wanxiang Che"
        ],
        "submitted": "2025-08-04 11:42:48",
        "source": "arxiv",
        "comment": "16 pages, 9 figures, 7 tables"
    },
    {
        "title": "VeOmni: Scaling Any Modality Model Training with Model-Centric Distributed Recipe Zoo",
        "abstract": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. We present VeOmni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. VeOmni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. VeOmni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. Using VeOmni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs.",
        "url": "http://arxiv.org/abs/2508.02317v2",
        "pdf_url": "http://arxiv.org/pdf/2508.02317v2",
        "arxiv_id": "2508.02317v2",
        "authors": [
            "Qianli Ma",
            "Yaowei Zheng",
            "Zhelun Shi",
            "Zhongkai Zhao",
            "Bin Jia",
            "Ziyue Huang",
            "Zhiqi Lin",
            "Youjie Li",
            "Jiacheng Yang",
            "Yanghua Peng",
            "Zhi Zhang",
            "Xin Liu"
        ],
        "submitted": "2025-08-04 11:33:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LaMPE: Length-aware Multi-grained Positional Encoding for Adaptive Long-context Scaling Without Training",
        "abstract": "Large language models (LLMs) experience significant performance degradation\nwhen the input exceeds the pretraining context window, primarily due to the\nout-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent\nstudies mitigate this problem by remapping OOD positions into the\nin-distribution range with fixed mapping strategies, ignoring the dynamic\nrelationship between input length and the model's effective context window. To\nthis end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a\ntraining-free method that fully utilizes the model's effective context window\nfor adaptive long-context scaling in LLMs. Motivated by the left-skewed\nfrequency distribution of relative positions, LaMPE establishes a dynamic\nrelationship between mapping length and input length through a parametric\nscaled sigmoid function to adaptively allocate positional capacity across\nvarying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention\nmechanism that strategically allocates positional resolution across different\nsequence regions to capture both fine-grained locality and long-range\ndependencies. Our method can be seamlessly applied to a wide range of\nRoPE-based LLMs without training. Extensive experiments on three representative\nLLMs across five mainstream long-context benchmarks demonstrate that LaMPE\nachieves significant performance improvements compared to existing length\nextrapolation methods. The code will be released at\nhttps://github.com/scar-on/LaMPE.",
        "url": "http://arxiv.org/abs/2508.02308v2",
        "pdf_url": "http://arxiv.org/pdf/2508.02308v2",
        "arxiv_id": "2508.02308v2",
        "authors": [
            "Sikui Zhang",
            "Guangze Gao",
            "Ziyun Gan",
            "Chunfeng Yuan",
            "Zefeng Lin",
            "Houwen Peng",
            "Bing Li",
            "Weiming Hu"
        ],
        "submitted": "2025-08-04 11:22:13",
        "source": "arxiv",
        "comment": "13 pages, 9 figures"
    },
    {
        "title": "Research Knowledge Graphs in NFDI4DataScience: Key Activities, Achievements, and Future Directions",
        "abstract": "As research in Artificial Intelligence and Data Science continues to grow in\nvolume and complexity, it becomes increasingly difficult to ensure\ntransparency, reproducibility, and discoverability. To address these\nchallenges, as research artifacts should be understandable and usable by\nmachines, the NFDI4DataScience consortium is developing and providing Research\nKnowledge Graphs (RKGs). Building upon earlier works, this paper presents\nrecent progress in creating semantically rich RKGs using standardized\nontologies, shared vocabularies, and automated Information Extraction\ntechniques. Key achievements include the development of the NFDI4DS ontology,\nmetadata standards, tools, and services designed to support the FAIR\nprinciples, as well as community-led projects and various implementations of\nRKGs. Together, these efforts aim to capture and connect the complex\nrelationships between datasets, models, software, and scientific publications.",
        "url": "http://arxiv.org/abs/2508.02300v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02300v1",
        "arxiv_id": "2508.02300v1",
        "authors": [
            "Kanishka Silva",
            "Marcel R. Ackermann",
            "Heike Fliegl",
            "Genet-Asefa Gesese",
            "Fidan Limani",
            "Philipp Mayr",
            "Peter Mutschke",
            "Allard Oelen",
            "Muhammad Asif Suryani",
            "Sharmila Upadhyaya",
            "Benjamin Zapilko",
            "Harald Sack",
            "Stefan Dietze"
        ],
        "submitted": "2025-08-04 11:11:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative Credit Assignment",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has improved the\nreasoning abilities of Large Language Models (LLMs) by using rule-based binary\nfeedback, helping to mitigate reward hacking. However, current RLVR methods\ntypically treat whole responses as single actions, assigning the same reward to\nevery token. This coarse-grained feedback hampers precise credit assignment,\nmaking it hard for models to identify which reasoning steps lead to success or\nfailure, and often results in suboptimal policies and inefficient learning.\nMethods like PPO provide credit assignment through value estimation, but often\nyield inaccurate and unverifiable signals due to limited sampling. On the other\nhand, methods using Process Reward Models can provide step-by-step judgments\nfor each reasoning step, but they require high-quality process supervision\nlabels and are time-consuming when applied in online reinforcement learning\n(RL). To overcome these limitations, we introduce a simple but efficient method\nCredit Assignment Policy Optimization (CAPO). Given a reasoning response\nrollout from the policy model, CAPO directly leverages an off-the-shelf,\ngeneral-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to\ngenerate all step-wise critique by one pass, thereby providing verifiable\ntoken-level rewards to refine the tokens that were originally assigned\nidentical rule-based rewards. This enables more fine-grained credit assignment\nin an effective way. Furthermore, to enhance the accuracy and robustness of\nCAPO, we employ voting mechanisms that scale with the number of generated\ncritiques. Extensive experiments using different backbones like Llama and Qwen\nmodels and in different sizes show that CAPO consistently outperforms\nsupervised learning-based and RL-based fine-tuning methods across six\nchallenging mathematical benchmarks and three out-of-domain benchmarks.",
        "url": "http://arxiv.org/abs/2508.02298v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02298v1",
        "arxiv_id": "2508.02298v1",
        "authors": [
            "Guofu Xie",
            "Yunsheng Shi",
            "Hongtao Tian",
            "Ting Yao",
            "Xiao Zhang"
        ],
        "submitted": "2025-08-04 11:06:08",
        "source": "arxiv",
        "comment": "Work in progress"
    },
    {
        "title": "Simple Methods Defend RAG Systems Well Against Real-World Attacks",
        "abstract": "Ensuring safety and in-domain responses for Retrieval-Augmented Generation\n(RAG) systems is paramount in safety-critical applications, yet remains a\nsignificant challenge. To address this, we evaluate four methodologies for\nOut-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal\nComponent Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG\nsystem only responds to queries confined to the system's knowledge base.\nSpecifically, our evaluation explores two novel dimensionality reduction and\nfeature separation strategies: \\textit{PCA}, where top components are selected\nusing explained variance or OOD separability, and an adaptation of\n\\textit{Neural Collapse Feature Separation}. We validate our approach on\nstandard datasets (StackExchange and MSMARCO) and real-world applications\n(Substance Use and COVID-19), including tests against LLM-simulated and actual\nattacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations\nof response correctness and relevance, we confirm that an external OOD detector\nis crucial for maintaining response relevance.",
        "url": "http://arxiv.org/abs/2508.02296v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02296v1",
        "arxiv_id": "2508.02296v1",
        "authors": [
            "Ilias Triantafyllopoulos",
            "Renyi Qu",
            "Salvatore Giorgi",
            "Brenda Curtis",
            "Lyle H. Ungar",
            "João Sedoc"
        ],
        "submitted": "2025-08-04 11:04:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A French Version of the OLDI Seed Corpus",
        "abstract": "We present the first French partition of the OLDI Seed Corpus, our submission\nto the WMT 2025 Open Language Data Initiative (OLDI) shared task. We detail its\ncreation process, which involved using multiple machine translation systems and\na custom-built interface for post-editing by qualified native speakers. We also\nhighlight the unique translation challenges presented by the source data, which\ncombines highly technical, encyclopedic terminology with the stylistic\nirregularities characteristic of user-generated content taken from Wikipedia.\nThis French corpus is not an end in itself, but is intended as a crucial pivot\nresource to facilitate the collection of parallel corpora for the\nunder-resourced regional languages of France.",
        "url": "http://arxiv.org/abs/2508.02290v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02290v1",
        "arxiv_id": "2508.02290v1",
        "authors": [
            "Malik Marmonier",
            "Benoît Sagot",
            "Rachel Bawden"
        ],
        "submitted": "2025-08-04 10:57:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Dialogue Systems Engineering: A Survey and Future Directions",
        "abstract": "This paper proposes to refer to the field of software engineering related to\nthe life cycle of dialogue systems as Dialogue Systems Engineering, and surveys\nthis field while also discussing its future directions. With the advancement of\nlarge language models, the core technologies underlying dialogue systems have\nsignificantly progressed. As a result, dialogue system technology is now\nexpected to be applied to solving various societal issues and in business\ncontexts. To achieve this, it is important to build, operate, and continuously\nimprove dialogue systems correctly and efficiently. Accordingly, in addition to\napplying existing software engineering knowledge, it is becoming increasingly\nimportant to evolve software engineering tailored specifically to dialogue\nsystems. In this paper, we enumerate the knowledge areas of dialogue systems\nengineering based on those of software engineering, as defined in the Software\nEngineering Body of Knowledge (SWEBOK) Version 4.0, and survey each area. Based\non this survey, we identify unexplored topics in each area and discuss the\nfuture direction of dialogue systems engineering.",
        "url": "http://arxiv.org/abs/2508.02279v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02279v1",
        "arxiv_id": "2508.02279v1",
        "authors": [
            "Mikio Nakano",
            "Hironori Takeuchi",
            "Sadahiro Yoshikawa",
            "Yoichi Matsuyama",
            "Kazunori Komatani"
        ],
        "submitted": "2025-08-04 10:49:01",
        "source": "arxiv",
        "comment": "18 pages, 2 figures"
    },
    {
        "title": "CellForge: Agentic Design of Virtual Cell Models",
        "abstract": "Virtual cell modeling represents an emerging frontier at the intersection of\nartificial intelligence and biology, aiming to predict quantities such as\nresponses to diverse perturbations quantitatively. However, autonomously\nbuilding computational models for virtual cells is challenging due to the\ncomplexity of biological systems, the heterogeneity of data modalities, and the\nneed for domain-specific expertise across multiple disciplines. Here, we\nintroduce CellForge, an agentic system that leverages a multi-agent framework\nthat transforms presented biological datasets and research objectives directly\ninto optimized computational models for virtual cells. More specifically, given\nonly raw single-cell multi-omics data and task descriptions as input, CellForge\noutputs both an optimized model architecture and executable code for training\nvirtual cell models and inference. The framework integrates three core modules:\nTask Analysis for presented dataset characterization and relevant literature\nretrieval, Method Design, where specialized agents collaboratively develop\noptimized modeling strategies, and Experiment Execution for automated\ngeneration of code. The agents in the Design module are separated into experts\nwith differing perspectives and a central moderator, and have to\ncollaboratively exchange solutions until they achieve a reasonable consensus.\nWe demonstrate CellForge's capabilities in single-cell perturbation prediction,\nusing six diverse datasets that encompass gene knockouts, drug treatments, and\ncytokine stimulations across multiple modalities. CellForge consistently\noutperforms task-specific state-of-the-art methods. Overall, CellForge\ndemonstrates how iterative interaction between LLM agents with differing\nperspectives provides better solutions than directly addressing a modeling\nchallenge. Our code is publicly available at\nhttps://github.com/gersteinlab/CellForge.",
        "url": "http://arxiv.org/abs/2508.02276v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02276v1",
        "arxiv_id": "2508.02276v1",
        "authors": [
            "Xiangru Tang",
            "Zhuoyun Yu",
            "Jiapeng Chen",
            "Yan Cui",
            "Daniel Shao",
            "Weixu Wang",
            "Fang Wu",
            "Yuchen Zhuang",
            "Wenqi Shi",
            "Zhi Huang",
            "Arman Cohan",
            "Xihong Lin",
            "Fabian Theis",
            "Smita Krishnaswamy",
            "Mark Gerstein"
        ],
        "submitted": "2025-08-04 10:43:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Dynaword: From One-shot to Continuously Developed Datasets",
        "abstract": "Large-scale datasets are foundational for research and development in natural\nlanguage processing. However, current approaches face three key challenges: (1)\nreliance on ambiguously licensed sources restricting use, sharing, and\nderivative works; (2) static dataset releases that prevent community\ncontributions and diminish longevity; and (3) quality assurance processes\nrestricted to publishing teams rather than leveraging community expertise.\n  To address these limitations, we introduce two contributions: the Dynaword\napproach and Danish Dynaword. The Dynaword approach is a framework for creating\nlarge-scale, open datasets that can be continuously updated through community\ncollaboration. Danish Dynaword is a concrete implementation that validates this\napproach and demonstrates its potential. Danish Dynaword contains over four\ntimes as many tokens as comparable releases, is exclusively openly licensed,\nand has received multiple contributions across industry and research. The\nrepository includes light-weight tests to ensure data formatting, quality, and\ndocumentation, establishing a sustainable framework for ongoing community\ncontributions and dataset evolution.",
        "url": "http://arxiv.org/abs/2508.02271v2",
        "pdf_url": "http://arxiv.org/pdf/2508.02271v2",
        "arxiv_id": "2508.02271v2",
        "authors": [
            "Kenneth Enevoldsen",
            "Kristian Nørgaard Jensen",
            "Jan Kostkan",
            "Balázs Szabó",
            "Márton Kardos",
            "Kirten Vad",
            "Johan Heinsen",
            "Andrea Blasi Núñez",
            "Gianluca Barmina",
            "Jacob Nielsen",
            "Rasmus Larsen",
            "Peter Vahlstrup",
            "Per Møldrup Dalum",
            "Desmond Elliott",
            "Lukas Galke",
            "Peter Schneider-Kamp",
            "Kristoffer Nielbo"
        ],
        "submitted": "2025-08-04 10:30:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic Bidirectional Machine Translation System",
        "abstract": "The rich linguistic landscape of the Arab world is characterized by a\nsignificant gap between Modern Standard Arabic (MSA), the language of formal\ncommunication, and the diverse regional dialects used in everyday life. This\ndiglossia presents a formidable challenge for natural language processing,\nparticularly machine translation. This paper introduces \\textbf{SHAMI-MT}, a\nbidirectional machine translation system specifically engineered to bridge the\ncommunication gap between MSA and the Syrian dialect. We present two\nspecialized models, one for MSA-to-Shami and another for Shami-to-MSA\ntranslation, both built upon the state-of-the-art AraT5v2-base-1024\narchitecture. The models were fine-tuned on the comprehensive Nabra dataset and\nrigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami\nmodel achieved an outstanding average quality score of \\textbf{4.01 out of 5.0}\nwhen judged by OPENAI model GPT-4.1, demonstrating its ability to produce\ntranslations that are not only accurate but also dialectally authentic. This\nwork provides a crucial, high-fidelity tool for a previously underserved\nlanguage pair, advancing the field of dialectal Arabic translation and offering\nsignificant applications in content localization, cultural heritage, and\nintercultural communication.",
        "url": "http://arxiv.org/abs/2508.02268v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02268v1",
        "arxiv_id": "2508.02268v1",
        "authors": [
            "Serry Sibaee",
            "Omer Nacar",
            "Yasser Al-Habashi",
            "Adel Ammar",
            "Wadii Boulila"
        ],
        "submitted": "2025-08-04 10:21:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Voronoi Diagram Encoded Hashing",
        "abstract": "The goal of learning to hash (L2H) is to derive data-dependent hash functions\nfrom a given data distribution in order to map data from the input space to a\nbinary coding space. Despite the success of L2H, two observations have cast\ndoubt on the source of the power of L2H, i.e., learning. First, a recent study\nshows that even using a version of locality sensitive hashing functions without\nlearning achieves binary representations that have comparable accuracy as those\nof L2H, but with less time cost. Second, existing L2H methods are constrained\nto three types of hash functions: thresholding, hyperspheres, and hyperplanes\nonly. In this paper, we unveil the potential of Voronoi diagrams in hashing.\nVoronoi diagram is a suitable candidate because of its three properties. This\ndiscovery has led us to propose a simple and efficient no-learning binary\nhashing method, called Voronoi Diagram Encoded Hashing (VDeH), which constructs\na set of hash functions through a data-dependent similarity measure and\nproduces independent binary bits through encoded hashing. We demonstrate\nthrough experiments on several benchmark datasets that VDeH achieves superior\nperformance and lower computational cost compared to existing state-of-the-art\nmethods under the same bit length.",
        "url": "http://arxiv.org/abs/2508.02266v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02266v1",
        "arxiv_id": "2508.02266v1",
        "authors": [
            "Yang Xu",
            "Kai Ming Ting"
        ],
        "submitted": "2025-08-04 10:16:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Decomposing the Entropy-Performance Exchange: The Missing Keys to Unlocking Effective Reinforcement Learning",
        "abstract": "Recently, reinforcement learning with verifiable rewards (RLVR) has been\nwidely used for enhancing the reasoning abilities of large language models\n(LLMs). A core challenge in RLVR involves managing the exchange between entropy\nand performance of policies. Despite the importance of this exchange, a\nfine-grained understanding of when and how this exchange operates most\neffectively remains limited. To bridge this gap, we conduct a systematic\nempirical analysis of the entropy-performance exchange mechanism of RLVR across\ndifferent levels of granularity. Specifically, we first divide the training\nprocess into two distinct stages based on entropy dynamics, i.e., rising stage\nand plateau stage, and then systematically investigate how this mechanism\nvaries across stage-level, instance-level, and token-level granularitiess. Our\nanalysis reveals that, in the rising stage, entropy reduction in negative\nsamples facilitates the learning of effective reasoning patterns, which in turn\ndrives rapid performance gains. Moreover, in the plateau stage, learning\nefficiency strongly correlates with high-entropy tokens present in\nlow-perplexity samples and those located at the end of sequences. Motivated by\nthese findings, we propose two methods that dynamically adjust the reward\nsignal using perplexity and positional information to focus RL updates on\ntokens that exhibit high learning potential, achieving improvements compared to\nthe baseline methods on various LLMs.",
        "url": "http://arxiv.org/abs/2508.02260v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02260v1",
        "arxiv_id": "2508.02260v1",
        "authors": [
            "Jia Deng",
            "Jie Chen",
            "Zhipeng Chen",
            "Wayne Xin Zhao",
            "Ji-Rong Wen"
        ],
        "submitted": "2025-08-04 10:08:10",
        "source": "arxiv",
        "comment": "7 pages, 20 figures"
    },
    {
        "title": "Interference Matrix: Quantifying Cross-Lingual Interference in Transformer Encoders",
        "abstract": "In this paper, we present a comprehensive study of language interference in\nencoder-only Transformer models across 83 languages. We construct an\ninterference matrix by training and evaluating small BERT-like models on all\npossible language pairs, providing a large-scale quantification of\ncross-lingual interference. Our analysis reveals that interference between\nlanguages is asymmetrical and that its patterns do not align with traditional\nlinguistic characteristics, such as language family, nor with proxies like\nembedding similarity, but instead better relate to script. Finally, we\ndemonstrate that the interference matrix effectively predicts performance on\ndownstream tasks, serving as a tool to better design multilingual models to\nobtain optimal performance.",
        "url": "http://arxiv.org/abs/2508.02256v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02256v1",
        "arxiv_id": "2508.02256v1",
        "authors": [
            "Belen Alastruey",
            "João Maria Janeiro",
            "Alexandre Allauzen",
            "Maha Elbayad",
            "Loïc Barrault",
            "Marta R. Costa-jussà"
        ],
        "submitted": "2025-08-04 10:02:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking",
        "abstract": "Multimodal entity linking plays a crucial role in a wide range of\napplications. Recent advances in large language model-based methods have become\nthe dominant paradigm for this task, effectively leveraging both textual and\nvisual modalities to enhance performance. Despite their success, these methods\nstill face two challenges, including unnecessary incorporation of image data in\ncertain scenarios and the reliance only on a one-time extraction of visual\nfeatures, which can undermine their effectiveness and accuracy. To address\nthese challenges, we propose a novel LLM-based framework for the multimodal\nentity linking task, called Intra- and Inter-modal Collaborative Reflections.\nThis framework prioritizes leveraging text information to address the task.\nWhen text alone is insufficient to link the correct entity through intra- and\ninter-modality evaluations, it employs a multi-round iterative strategy that\nintegrates key visual clues from various aspects of the image to support\nreasoning and enhance matching accuracy. Extensive experiments on three widely\nused public datasets demonstrate that our framework consistently outperforms\ncurrent state-of-the-art methods in the task, achieving improvements of 3.2%,\n5.1%, and 1.6%, respectively. Our code is available at\nhttps://github.com/ziyan-xiaoyu/I2CR/.",
        "url": "http://arxiv.org/abs/2508.02243v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02243v1",
        "arxiv_id": "2508.02243v1",
        "authors": [
            "Ziyan Liu",
            "Junwen Li",
            "Kaiwen Li",
            "Tong Ruan",
            "Chao Wang",
            "Xinyan He",
            "Zongyu Wang",
            "Xuezhi Cao",
            "Jingping Liu"
        ],
        "submitted": "2025-08-04 09:43:54",
        "source": "arxiv",
        "comment": "10 pages, 6 figures, accepted by ACMMM 2025"
    },
    {
        "title": "From Generation to Consumption: Personalized List Value Estimation for Re-ranking",
        "abstract": "Re-ranking is critical in recommender systems for optimizing the order of\nrecommendation lists, thus improving user satisfaction and platform revenue.\nMost existing methods follow a generator-evaluator paradigm, where the\nevaluator estimates the overall value of each candidate list. However, they\noften ignore the fact that users may exit before consuming the full list,\nleading to a mismatch between estimated generation value and actual consumption\nvalue. To bridge this gap, we propose CAVE, a personalized Consumption-Aware\nlist Value Estimation framework. CAVE formulates the list value as the\nexpectation over sub-list values, weighted by user-specific exit probabilities\nat each position. The exit probability is decomposed into an interest-driven\ncomponent and a stochastic component, the latter modeled via a Weibull\ndistribution to capture random external factors such as fatigue. By jointly\nmodeling sub-list values and user exit behavior, CAVE yields a more faithful\nestimate of actual list consumption value. We further contribute three\nlarge-scale real-world list-wise benchmarks from the Kuaishou platform, varying\nin size and user activity patterns. Extensive experiments on these benchmarks,\ntwo Amazon datasets, and online A/B testing on Kuaishou show that CAVE\nconsistently outperforms strong baselines, highlighting the benefit of\nexplicitly modeling user exits in re-ranking.",
        "url": "http://arxiv.org/abs/2508.02242v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02242v1",
        "arxiv_id": "2508.02242v1",
        "authors": [
            "Kaike Zhang",
            "Xiaobei Wang",
            "Xiaoyu Liu",
            "Shuchang Liu",
            "Hailan Yang",
            "Xiang Li",
            "Fei Sun",
            "Qi Cao"
        ],
        "submitted": "2025-08-04 09:43:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Isolating Culture Neurons in Multilingual Large Language Models",
        "abstract": "Language and culture are deeply intertwined, yet it is so far unclear how and\nwhere multilingual large language models encode culture. Here, we extend upon\nan established methodology for identifying language-specific neurons and extend\nit to localize and isolate culture-specific neurons, carefully disentangling\ntheir overlap and interaction with language-specific neurons. To facilitate our\nexperiments, we introduce MUREL, a curated dataset of 85.2 million tokens\nspanning six different cultures. Our localization and intervention experiments\nshow that LLMs encode different cultures in distinct neuron populations,\npredominantly in upper layers, and that these culture neurons can be modulated\nindependently from language-specific neurons or those specific to other\ncultures. These findings suggest that cultural knowledge and propensities in\nmultilingual language models can be selectively isolated and edited - promoting\nfairness, inclusivity, and alignment. Code and data is available at\nhttps://github.com/namazifard/Culture_Neurons .",
        "url": "http://arxiv.org/abs/2508.02241v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02241v1",
        "arxiv_id": "2508.02241v1",
        "authors": [
            "Danial Namazifard",
            "Lukas Galke"
        ],
        "submitted": "2025-08-04 09:41:10",
        "source": "arxiv",
        "comment": "18 pages, 13 figures"
    },
    {
        "title": "FinCPRG: A Bidirectional Generation Pipeline for Hierarchical Queries and Rich Relevance in Financial Chinese Passage Retrieval",
        "abstract": "In recent years, large language models (LLMs) have demonstrated significant\npotential in constructing passage retrieval datasets. However, existing methods\nstill face limitations in expressing cross-doc query needs and controlling\nannotation quality. To address these issues, this paper proposes a\nbidirectional generation pipeline, which aims to generate 3-level hierarchical\nqueries for both intra-doc and cross-doc scenarios and mine additional\nrelevance labels on top of direct mapping annotation. The pipeline introduces\ntwo query generation methods: bottom-up from single-doc text and top-down from\nmulti-doc titles. The bottom-up method uses LLMs to disassemble and generate\nstructured queries at both sentence-level and passage-level simultaneously from\nintra-doc passages. The top-down approach incorporates three key financial\nelements--industry, topic, and time--to divide report titles into clusters and\nprompts LLMs to generate topic-level queries from each cluster. For relevance\nannotation, our pipeline not only relies on direct mapping annotation from the\ngeneration relationship but also implements an indirect positives mining method\nto enrich the relevant query-passage pairs. Using this pipeline, we constructed\na Financial Passage Retrieval Generated dataset (FinCPRG) from almost 1.3k\nChinese financial research reports, which includes hierarchical queries and\nrich relevance labels. Through evaluations of mined relevance labels,\nbenchmarking and training experiments, we assessed the quality of FinCPRG and\nvalidated its effectiveness as a passage retrieval dataset for both training\nand benchmarking.",
        "url": "http://arxiv.org/abs/2508.02222v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02222v1",
        "arxiv_id": "2508.02222v1",
        "authors": [
            "Xuan Xu",
            "Beilin Chu",
            "Qinhong Lin",
            "Yixiao Zhong",
            "Fufang Wen",
            "Jiaqi Liu",
            "Binjie Fei",
            "Yu Li",
            "Zhongliang Yang",
            "Linna Zhou"
        ],
        "submitted": "2025-08-04 09:12:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
        "abstract": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.",
        "url": "http://arxiv.org/abs/2508.02215v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02215v1",
        "arxiv_id": "2508.02215v1",
        "authors": [
            "Yike Zhang",
            "Zhiyuan He",
            "Huiqiang Jiang",
            "Chengruidong Zhang",
            "Yuqing Yang",
            "Jianyong Wang",
            "Lili Qiu"
        ],
        "submitted": "2025-08-04 09:08:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Proof2Hybrid: Automatic Mathematical Benchmark Synthesis for Proof-Centric Problems",
        "abstract": "Evaluating the mathematical capability of Large Language Models (LLMs) is a\ncritical yet challenging frontier. Existing benchmarks fall short, particularly\nfor proof-centric problems, as manual creation is unscalable and costly,\nleaving the true mathematical abilities of LLMs largely unassessed. To overcome\nthese barriers, we propose Proof2Hybrid, the first fully automated framework\nthat synthesizes high-quality, proof-centric benchmarks from natural language\nmathematical corpora. The key novelty of our solution is Proof2X, a roadmap of\nconverting mathematical proofs into various kinds of questions that are easy to\nverify. Instructed by this roadmap, we propose a new type of hybrid-formatted\nquestions, named ``$m$-out-of-$n$ multiple judge questions'', specifically\ndesigned to enable robust, automatic evaluation while being resilient to\nguessing and superficial pattern matching inherent in traditional formats. As a\ndemonstration of our framework, we introduce AlgGeoTest, a benchmark for\nalgebraic geometry--a frontier domain of modern mathematics--comprising 456\nchallenging items. Our extensive evaluations on state-of-the-art LLMs using\nAlgGeoTest reveal profound deficits in their comprehension of algebraic\ngeometry, providing a more precise measure of their true mathematical\ncapabilities. Our framework and benchmark pave the way for a new wave of\nin-depth research into the mathematical intelligence of AI systems.",
        "url": "http://arxiv.org/abs/2508.02208v2",
        "pdf_url": "http://arxiv.org/pdf/2508.02208v2",
        "arxiv_id": "2508.02208v2",
        "authors": [
            "Yebo Peng",
            "Zixiang Liu",
            "Yaoming Li",
            "Zhizhuo Yang",
            "Xinye Xu",
            "Bowen Ye",
            "Weijun Yuan",
            "Zihan Wang",
            "Tong Yang"
        ],
        "submitted": "2025-08-04 08:59:36",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed Inference",
        "abstract": "We present Seed Diffusion Preview, a large-scale language model based on\ndiscrete-state diffusion, offering remarkably fast inference speed. Thanks to\nnon-sequential, parallel generation, discrete diffusion models provide a\nnotable speedup to mitigate the inherent latency of token-by-token decoding, as\ndemonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion\nPreview achieves an inference speed of 2,146 token/s over H20 GPUs while\nmaintaining competitive performance across a sweep of standard code evaluation\nbenchmarks, significantly faster than contemporary Mercury and Gemini\nDiffusion, establishing new state of the art on the speed-quality Pareto\nfrontier for code models.",
        "url": "http://arxiv.org/abs/2508.02193v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02193v1",
        "arxiv_id": "2508.02193v1",
        "authors": [
            "Yuxuan Song",
            "Zheng Zhang",
            "Cheng Luo",
            "Pengyang Gao",
            "Fan Xia",
            "Hao Luo",
            "Zheng Li",
            "Yuehang Yang",
            "Hongli Yu",
            "Xingwei Qu",
            "Yuwei Fu",
            "Jing Su",
            "Ge Zhang",
            "Wenhao Huang",
            "Mingxuan Wang",
            "Lin Yan",
            "Xiaoying Jia",
            "Jingjing Liu",
            "Wei-Ying Ma",
            "Ya-Qin Zhang",
            "Yonghui Wu",
            "Hao Zhou"
        ],
        "submitted": "2025-08-04 08:43:01",
        "source": "arxiv",
        "comment": "Demo is available at https://studio.seed.ai/exp/seed_diffusion/;\n  Project page is https://seed.bytedance.com/seed_diffusion"
    },
    {
        "title": "Learning Dynamics of Meta-Learning in Small Model Pretraining",
        "abstract": "Large language models are powerful but costly. We ask whether meta-learning\ncan make the pretraining of small language models not only better but also more\ninterpretable. We integrate first-order MAML with subset-masked LM pretraining,\nproducing four LLama-style decoder-only models (11M-570M params), and evaluate\nit on a fundamental NLP task with many settings and real-world applications.\nCompared with vanilla training, our model (i) reaches the same loss up to 1.6x\nsooner, (ii) improves F1 on multilingual Universal NER under equal compute, and\n(iii) makes the training dynamics easy to read: first the network's\nrepresentations fan out (\"diversify\") and later they collapse into a smaller,\nshared subspace (\"compress\"). This two-stage shift shows up as a rise-and-fall\nin both effective-rank curves and attention-head entropy. The same curves\npinpoint which layers specialise earliest and which later reconverge, giving a\ncompact, interpretable signature of meta-adaptation. Code, checkpoints and\nWandB logs are released.",
        "url": "http://arxiv.org/abs/2508.02189v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02189v1",
        "arxiv_id": "2508.02189v1",
        "authors": [
            "David Demitri Africa",
            "Yuval Weiss",
            "Paula Buttery",
            "Richard Diehl Martinez"
        ],
        "submitted": "2025-08-04 08:34:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through Latent Acoustic Pattern Triggers",
        "abstract": "As Audio Large Language Models (ALLMs) emerge as powerful tools for speech\nprocessing, their safety implications demand urgent attention. While\nconsiderable research has explored textual and vision safety, audio's distinct\ncharacteristics present significant challenges. This paper first investigates:\nIs ALLM vulnerable to backdoor attacks exploiting acoustic triggers? In\nresponse to this issue, we introduce Hidden in the Noise (HIN), a novel\nbackdoor attack framework designed to exploit subtle, audio-specific features.\nHIN applies acoustic modifications to raw audio waveforms, such as alterations\nto temporal dynamics and strategic injection of spectrally tailored noise.\nThese changes introduce consistent patterns that an ALLM's acoustic feature\nencoder captures, embedding robust triggers within the audio stream. To\nevaluate ALLM robustness against audio-feature-based triggers, we develop the\nAudioSafe benchmark, assessing nine distinct risk types. Extensive experiments\non AudioSafe and three established safety datasets reveal critical\nvulnerabilities in existing ALLMs: (I) audio features like environment noise\nand speech rate variations achieve over 90% average attack success rate. (II)\nALLMs exhibit significant sensitivity differences across acoustic features,\nparticularly showing minimal response to volume as a trigger, and (III)\npoisoned sample inclusion causes only marginal loss curve fluctuations,\nhighlighting the attack's stealth.",
        "url": "http://arxiv.org/abs/2508.02175v2",
        "pdf_url": "http://arxiv.org/pdf/2508.02175v2",
        "arxiv_id": "2508.02175v2",
        "authors": [
            "Liang Lin",
            "Miao Yu",
            "Kaiwen Luo",
            "Yibo Zhang",
            "Lilan Peng",
            "Dexian Wang",
            "Xuehai Tang",
            "Yuanhe Zhang",
            "Xikang Yang",
            "Zhenhong Zhou",
            "Kun Wang",
            "Yang Liu"
        ],
        "submitted": "2025-08-04 08:15:16",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Subject or Style: Adaptive and Training-Free Mixture of LoRAs",
        "abstract": "Fine-tuning models via Low-Rank Adaptation (LoRA) demonstrates remarkable\nperformance in subject-driven or style-driven generation tasks. Studies have\nexplored combinations of different LoRAs to jointly generate learned styles and\ncontent. However, current methods struggle to balance the original subject and\nstyle, and often require additional training. Recently, K-LoRA proposed a\ntraining-free LoRA fusion method. But it involves multiple hyperparameters,\nmaking it difficult to adapt to all styles and subjects. In this paper, we\npropose EST-LoRA, a training-free adaptive LoRA fusion method. It\ncomprehensively considers three critical factors: \\underline{E}nergy of matrix,\n\\underline{S}tyle discrepancy scores and \\underline{T}ime steps. Analogous to\nthe Mixture of Experts (MoE) architecture, the model adaptively selects between\nsubject LoRA and style LoRA within each attention layer. This integrated\nselection mechanism ensures balanced contributions from both components during\nthe generation process. Experimental results show that EST-LoRA outperforms\nstate-of-the-art methods in both qualitative and quantitative evaluations and\nachieves faster generation speed compared to other efficient fusion approaches.\nOur code is publicly available at:\nhttps://anonymous.4open.science/r/EST-LoRA-F318.",
        "url": "http://arxiv.org/abs/2508.02165v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02165v1",
        "arxiv_id": "2508.02165v1",
        "authors": [
            "Jia-Chen Zhang",
            "Yu-Jie Xiong"
        ],
        "submitted": "2025-08-04 08:05:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Trainable Dynamic Mask Sparse Attention",
        "abstract": "In large language models, the demand for modeling long contexts is constantly\nincreasing, but the quadratic complexity of the standard self-attention\nmechanism often becomes a bottleneck. Although existing sparse attention\nmechanisms have improved efficiency, they may still encounter issues such as\nstatic patterns or information loss. We introduce a trainable dynamic mask\nsparse attention mechanism, Dynamic Mask Attention, which effectively utilizes\ncontent-aware and position-aware sparsity. DMA achieves this through two key\ninnovations: First, it dynamically generates content-aware sparse masks from\nvalue representations, enabling the model to identify and focus on critical\ninformation adaptively. Second, it implements position-aware sparse attention\ncomputation that effectively skips unnecessary calculation regions. This\ndual-sparsity design allows the model to significantly reduce the computational\ncomplexity of important information while retaining complete information,\nachieving an excellent balance between information fidelity and computational\nefficiency. We have verified the performance of DMA through comprehensive\nexperiments. Comparative studies show that DMA outperforms multi-head\nattention, sliding window attention, multi-head latent attention, and native\nsparse attention in terms of perplexity under Chinchilla Scaling Law settings.\nMoreover, in challenging multi-query associative recall tasks, DMA also\ndemonstrates superior performance and efficiency compared to these methods.\nCrucially, in the evaluation of a 1.7B parameter model, DMA significantly\noutperforms multi-head attention in both standard benchmark performance and the\nchallenging needle-in-a-haystack task. These experimental results highlight its\ncapability to balance model efficiency and long-context modeling ability\neffectively.",
        "url": "http://arxiv.org/abs/2508.02124v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02124v1",
        "arxiv_id": "2508.02124v1",
        "authors": [
            "Jingze Shi",
            "Yifan Wu",
            "Bingheng Wu",
            "Yiran Peng",
            "Liangdong Wang",
            "Guang Liu",
            "Yuyu Luo"
        ],
        "submitted": "2025-08-04 07:05:15",
        "source": "arxiv",
        "comment": "8 figures, 4 tables"
    },
    {
        "title": "Evaluating User Experience in Conversational Recommender Systems: A Systematic Review Across Classical and LLM-Powered Approaches",
        "abstract": "Conversational Recommender Systems (CRSs) are receiving growing research\nattention across domains, yet their user experience (UX) evaluation remains\nlimited. Existing reviews largely overlook empirical UX studies, particularly\nin adaptive and large language model (LLM)-based CRSs. To address this gap, we\nconducted a systematic review following PRISMA guidelines, synthesising 23\nempirical studies published between 2017 and 2025. We analysed how UX has been\nconceptualised, measured, and shaped by domain, adaptivity, and LLM.\n  Our findings reveal persistent limitations: post hoc surveys dominate,\nturn-level affective UX constructs are rarely assessed, and adaptive behaviours\nare seldom linked to UX outcomes. LLM-based CRSs introduce further challenges,\nincluding epistemic opacity and verbosity, yet evaluations infrequently address\nthese issues. We contribute a structured synthesis of UX metrics, a comparative\nanalysis of adaptive and nonadaptive systems, and a forward-looking agenda for\nLLM-aware UX evaluation. These findings support the development of more\ntransparent, engaging, and user-centred CRS evaluation practices.",
        "url": "http://arxiv.org/abs/2508.02096v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02096v1",
        "arxiv_id": "2508.02096v1",
        "authors": [
            "Raj Mahmud",
            "Yufeng Wu",
            "Abdullah Bin Sawad",
            "Shlomo Berkovsky",
            "Mukesh Prasad",
            "A. Baki Kocaballi"
        ],
        "submitted": "2025-08-04 06:07:33",
        "source": "arxiv",
        "comment": "Accepted at OZCHI 2025. 23 pages, 1 figure, 5 tables"
    },
    {
        "title": "\"Harmless to You, Hurtful to Me!\": Investigating the Detection of Toxic Languages Grounded in the Perspective of Youth",
        "abstract": "Risk perception is subjective, and youth's understanding of toxic content\ndiffers from that of adults. Although previous research has conducted extensive\nstudies on toxicity detection in social media, the investigation of youth's\nunique toxicity, i.e., languages perceived as nontoxic by adults but toxic as\nyouth, is ignored. To address this gap, we aim to explore: 1) What are the\nfeatures of ``youth-toxicity'' languages in social media (RQ1); 2) Can existing\ntoxicity detection techniques accurately detect these languages (RQ2). For\nthese questions, we took Chinese youth as the research target, constructed the\nfirst Chinese ``youth-toxicity'' dataset, and then conducted extensive\nanalysis. Our results suggest that youth's perception of these is associated\nwith several contextual factors, like the source of an utterance and\ntext-related features. Incorporating these meta information into current\ntoxicity detection methods significantly improves accuracy overall. Finally, we\npropose several insights into future research on youth-centered toxicity\ndetection.",
        "url": "http://arxiv.org/abs/2508.02094v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02094v1",
        "arxiv_id": "2508.02094v1",
        "authors": [
            "Yaqiong Li",
            "Peng Zhang",
            "Lin Wang",
            "Hansu Gu",
            "Siyuan Qiao",
            "Ning Gu",
            "Tun Lu"
        ],
        "submitted": "2025-08-04 06:05:36",
        "source": "arxiv",
        "comment": "Accepted at the 20th International AAAI Conference on Web and Social\n  Media (ICWSM 2026)"
    },
    {
        "title": "CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search",
        "abstract": "Approximate nearest-neighbor search (ANNS) algorithms have become\nincreasingly critical for recent AI applications, particularly in\nretrieval-augmented generation (RAG) and agent-based LLM applications. In this\npaper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS\noptimization as a reinforcement learning problem where execution speed serves\nas the reward signal. This approach enables the automatic generation of\nprogressively faster ANNS implementations while maintaining accuracy\nconstraints. Our experimental evaluation demonstrates CRINN's effectiveness\nacross six widely-used NNS benchmark datasets. When compared against\nstate-of-the-art open-source ANNS algorithms, CRINN achieves best performance\non three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and\nGloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean\nand GloVe-25-angular). The implications of CRINN's success reach well beyond\nANNS optimization: It validates that LLMs augmented with reinforcement learning\ncan function as an effective tool for automating sophisticated algorithmic\noptimizations that demand specialized knowledge and labor-intensive manual\nrefinement.Code can be found at https://github.com/deepreinforce-ai/CRINN",
        "url": "http://arxiv.org/abs/2508.02091v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02091v1",
        "arxiv_id": "2508.02091v1",
        "authors": [
            "Xiaoya Li",
            "Xiaofei Sun",
            "Albert Wang",
            "Chris Shum",
            "Jiwei Li"
        ],
        "submitted": "2025-08-04 05:57:46",
        "source": "arxiv",
        "comment": "Preprint Version"
    },
    {
        "title": "When Truth Is Overridden: Uncovering the Internal Origins of Sycophancy in Large Language Models",
        "abstract": "Large Language Models (LLMs) often exhibit sycophantic behavior, agreeing\nwith user-stated opinions even when those contradict factual knowledge. While\nprior work has documented this tendency, the internal mechanisms that enable\nsuch behavior remain poorly understood. In this paper, we provide a mechanistic\naccount of how sycophancy arises within LLMs. We first systematically study how\nuser opinions induce sycophancy across different model families. We find that\nsimple opinion statements reliably induce sycophancy, whereas user expertise\nframing has a negligible impact. Through logit-lens analysis and causal\nactivation patching, we identify a two-stage emergence of sycophancy: (1) a\nlate-layer output preference shift and (2) deeper representational divergence.\nWe also verify that user authority fails to influence behavior because models\ndo not encode it internally. In addition, we examine how grammatical\nperspective affects sycophantic behavior, finding that first-person prompts\n(``I believe...'') consistently induce higher sycophancy rates than\nthird-person framings (``They believe...'') by creating stronger\nrepresentational perturbations in deeper layers. These findings highlight that\nsycophancy is not a surface-level artifact but emerges from a structural\noverride of learned knowledge in deeper layers, with implications for alignment\nand truthful AI systems.",
        "url": "http://arxiv.org/abs/2508.02087v2",
        "pdf_url": "http://arxiv.org/pdf/2508.02087v2",
        "arxiv_id": "2508.02087v2",
        "authors": [
            "Keyu Wang",
            "Jin Li",
            "Shu Yang",
            "Zhuoran Zhang",
            "Di Wang"
        ],
        "submitted": "2025-08-04 05:55:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Human Capital Visualization using Speech Amount during Meetings",
        "abstract": "In recent years, many companies have recognized the importance of human\nresources and are investing in human capital to revitalize their organizations\nand enhance internal communication, thereby fostering innovation. However,\nconventional quantification methods have mainly focused on readily measurable\nindicators without addressing the fundamental role of conversations in human\ncapital. This study focuses on routine meetings and proposes strategies to\nvisualize human capital by analyzing speech amount during these meetings. We\nemploy conversation visualization technology, which operates effectively, to\nquantify speech. We then measure differences in speech amount by attributes\nsuch as gender and job post, changes in speech amount depending on whether\ncertain participants are present, and correlations between speech amount and\ncontinuous attributes. To verify the effectiveness of our proposed methods, we\nanalyzed speech amounts by departmental affiliation during weekly meetings at\nsmall to medium enterprises.",
        "url": "http://arxiv.org/abs/2508.02075v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02075v1",
        "arxiv_id": "2508.02075v1",
        "authors": [
            "Ekai Hashimoto",
            "Takeshi Mizumoto",
            "Kohei Nagira",
            "Shun Shiramatsu"
        ],
        "submitted": "2025-08-04 05:31:09",
        "source": "arxiv",
        "comment": "This paper has been accepted for presentation at the 26th Annual\n  Meeting of the Special Interest Group on Discourse and Dialogue(SIGDIAL\n  2025). It represents the author's version of the work"
    },
    {
        "title": "The SMeL Test: A simple benchmark for media literacy in language models",
        "abstract": "The internet is rife with unattributed, deliberately misleading, or otherwise\nuntrustworthy content. Though large language models (LLMs) are often tasked\nwith autonomous web browsing, the extent to which they have learned the simple\nheuristics human researchers use to navigate this noisy environment is not\ncurrently known. In this paper, we introduce the Synthetic Media Literacy Test\n(SMeL Test), a minimal benchmark that tests the ability of language models to\nactively filter out untrustworthy information in context. We benchmark a\nvariety of commonly used instruction-tuned LLMs, including reasoning models,\nand find that no model consistently trusts more reliable sources; while\nreasoning in particular is associated with higher scores, even the best API\nmodel we test hallucinates up to 70% of the time. Remarkably, larger and more\ncapable models do not necessarily outperform their smaller counterparts. We\nhope our work sheds more light on this important form of hallucination and\nguides the development of new methods to combat it.",
        "url": "http://arxiv.org/abs/2508.02074v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02074v1",
        "arxiv_id": "2508.02074v1",
        "authors": [
            "Gustaf Ahdritz",
            "Anat Kleiman"
        ],
        "submitted": "2025-08-04 05:29:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MolReasoner: Toward Effective and Interpretable Reasoning for Molecular LLMs",
        "abstract": "Large Language Models(LLMs) have demonstrated remarkable performance across\nvarious domains, yet their capabilities in molecular reasoning remain\ninsufficiently explored. Current approaches tend to rely heavily on\ngeneral-purpose prompting, which lacks domain-specific molecular semantics,\nwhile those that use fine-tuning strategies often face challenges with\ninterpretability and reasoning depth. To address these issues, we introduce\nMolReasoner, a two-stage framework designed to transition LLMs from\nmemorization towards chemical reasoning. First, we propose Mol-SFT, which\ninitializes the model's reasoning abilities via synthetic Chain-of-Thought(CoT)\nsamples generated by GPT-4o and verified for chemical accuracy. Subsequently,\nMol-RL applies reinforcement learning with specialized reward functions\ndesigned explicitly to align chemical structures with linguistic descriptions,\nthereby enhancing molecular reasoning capabilities. Our approach notably\nenhances interpretability, improving the model 's molecular understanding and\nenabling better generalization. Extensive experiments demonstrate that\nMolReasoner outperforms existing methods, and marking a significant shift from\nmemorization-based outputs to robust chemical reasoning.",
        "url": "http://arxiv.org/abs/2508.02066v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02066v1",
        "arxiv_id": "2508.02066v1",
        "authors": [
            "Guojiang Zhao",
            "Sihang Li",
            "Zixiang Lu",
            "Zheng Cheng",
            "Haitao Lin",
            "Lirong Wu",
            "Hanchen Xia",
            "Hengxing Cai",
            "Wentao Guo",
            "Hongshuai Wang",
            "Mingjun Xu",
            "Siyu Zhu",
            "Guolin Ke",
            "Linfeng Zhang",
            "Zhifeng Gao"
        ],
        "submitted": "2025-08-04 05:10:11",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ProCut: LLM Prompt Compression via Attribution Estimation",
        "abstract": "In large-scale industrial LLM systems, prompt templates often expand to\nthousands of tokens as teams iteratively incorporate sections such as task\ninstructions, few-shot examples, and heuristic rules to enhance robustness and\ncoverage. This expansion leads to bloated prompts that are difficult to\nmaintain and incur significant inference latency and serving costs. To address\nthis, we introduce Prompt Compression via Attribution Estimation (ProCut), a\nflexible, LLM-agnostic, training-free framework that compresses prompts through\nattribution analysis. ProCut segments prompt templates into semantically\nmeaningful units, quantifies their impact on task performance, and prunes\nlow-utility components. Through extensive experiments on five public benchmark\ndatasets and real-world industrial prompts, we show that ProCut achieves\nsubstantial prompt size reductions (78% fewer tokens in production) while\nmaintaining or even slightly improving task performance (up to 62% better than\nalternative methods). We further introduce an LLM-driven attribution estimator\nthat reduces compression latency by over 50%, and demonstrate that ProCut\nintegrates seamlessly with existing prompt-optimization frameworks to produce\nconcise, high-performing prompts.",
        "url": "http://arxiv.org/abs/2508.02053v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02053v1",
        "arxiv_id": "2508.02053v1",
        "authors": [
            "Zhentao Xu",
            "Fengyi Li",
            "Albert Chen",
            "Xiaofeng Wang"
        ],
        "submitted": "2025-08-04 04:44:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Why Generate When You Can Transform? Unleashing Generative Attention for Dynamic Recommendation",
        "abstract": "Sequential Recommendation (SR) focuses on personalizing user experiences by\npredicting future preferences based on historical interactions. Transformer\nmodels, with their attention mechanisms, have become the dominant architecture\nin SR tasks due to their ability to capture dependencies in user behavior\nsequences. However, traditional attention mechanisms, where attention weights\nare computed through query-key transformations, are inherently linear and\ndeterministic. This fixed approach limits their ability to account for the\ndynamic and non-linear nature of user preferences, leading to challenges in\ncapturing evolving interests and subtle behavioral patterns. Given that\ngenerative models excel at capturing non-linearity and probabilistic\nvariability, we argue that generating attention distributions offers a more\nflexible and expressive alternative compared to traditional attention\nmechanisms. To support this claim, we present a theoretical proof demonstrating\nthat generative attention mechanisms offer greater expressiveness and\nstochasticity than traditional deterministic approaches. Building upon this\ntheoretical foundation, we introduce two generative attention models for SR,\neach grounded in the principles of Variational Autoencoders (VAE) and Diffusion\nModels (DMs), respectively. These models are designed specifically to generate\nadaptive attention distributions that better align with variable user\npreferences. Extensive experiments on real-world datasets show our models\nsignificantly outperform state-of-the-art in both accuracy and diversity.",
        "url": "http://arxiv.org/abs/2508.02050v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02050v1",
        "arxiv_id": "2508.02050v1",
        "authors": [
            "Yuli Liu",
            "Wenjun Kong",
            "Cheng Luo",
            "Weizhi Ma"
        ],
        "submitted": "2025-08-04 04:33:26",
        "source": "arxiv",
        "comment": "Accepted at ACMMM 2025"
    },
    {
        "title": "Harnessing Temporal Databases for Systematic Evaluation of Factual Time-Sensitive Question-Answering in Large Language Models",
        "abstract": "Facts evolve over time, making it essential for Large Language Models (LLMs)\nto handle time-sensitive factual knowledge accurately and reliably. While\nfactual Time-Sensitive Question-Answering (TSQA) tasks have been widely\nstudied, existing benchmarks often rely on manual curation or a small, fixed\nset of predefined templates, which restricts scalable and comprehensive TSQA\nevaluation. To address these challenges, we propose TDBench, a new benchmark\nthat systematically constructs TSQA pairs by harnessing temporal databases and\ndatabase techniques such as temporal SQL and functional dependencies. We also\nintroduce a fine-grained evaluation metric called time accuracy, which assesses\nthe validity of time references in model explanations alongside traditional\nanswer accuracy to enable a more reliable TSQA evaluation. Extensive\nexperiments on contemporary LLMs show how \\ours{} enables scalable and\ncomprehensive TSQA evaluation while reducing the reliance on human labor,\ncomplementing existing Wikipedia/Wikidata-based TSQA evaluation approaches by\nenabling LLM evaluation on application-specific data and seamless multi-hop\nquestion generation. Code and data are publicly available at:\nhttps://github.com/ssoy0701/tdbench.git.",
        "url": "http://arxiv.org/abs/2508.02045v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02045v1",
        "arxiv_id": "2508.02045v1",
        "authors": [
            "Soyeon Kim",
            "Jindong Wang",
            "Xing Xie",
            "Steven Euijong Whang"
        ],
        "submitted": "2025-08-04 04:27:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Marco-Voice Technical Report",
        "abstract": "This paper presents a multifunctional speech synthesis system that integrates\nvoice cloning and emotion control speech synthesis within a unified framework.\nThe goal of this work is to address longstanding challenges in achieving highly\nexpressive, controllable, and natural speech generation that faithfully\npreserves speaker identity across diverse linguistic and emotional contexts.\nOur approach introduces an effective speaker-emotion disentanglement mechanism\nwith in-batch contrastive learning, enabling independent manipulation of\nspeaker identity and eemotional style, as well as rotational emotional\nembedding integration method for smooth emotion control. To support\ncomprehensive training and evaluation, we construct CSEMOTIONS, a high-quality\nemotional speech dataset containing 10 hours of Mandarin speech from six\nprofessional speakers across seven emotional categories. Extensive experiments\ndemonstrate that our system, Marco-Voice, achieves substantial improvements in\nboth objective and subjective metrics. Comprehensive evaluations and analysis\nwere conducted, results show that MarcoVoice delivers competitive performance\nin terms of speech clarity and emotional richness, representing a substantial\nadvance in the field of expressive neural speech synthesis.",
        "url": "http://arxiv.org/abs/2508.02038v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02038v1",
        "arxiv_id": "2508.02038v1",
        "authors": [
            "Fengping Tian",
            "Chenyang Lyu",
            "Xuanfan Ni",
            "Haoqin Sun",
            "Qingjuan Li",
            "Zhiqiang Qian",
            "Haijun Li",
            "Longyue Wang",
            "Zhao Xu",
            "Weihua Luo",
            "Kaifu Zhang"
        ],
        "submitted": "2025-08-04 04:08:22",
        "source": "arxiv",
        "comment": "Technical Report"
    },
    {
        "title": "Diagnosing Memorization in Chain-of-Thought Reasoning, One Token at a Time",
        "abstract": "Large Language Models (LLMs) perform well on reasoning benchmarks but often\nfail when inputs alter slightly, raising concerns about the extent to which\ntheir success relies on memorization. This issue is especially acute in\nChain-of-Thought (CoT) reasoning, where spurious memorized patterns can trigger\nintermediate errors that cascade into incorrect final answers. We introduce\nSTIM, a novel framework for Source-aware Token-level Identification of\nMemorization, which attributes each token in a reasoning chain to one of\nmultiple memorization sources - local, mid-range, or long-range - based on\ntheir statistical co-occurrence with the token in the pretraining corpus. Our\ntoken-level analysis across tasks and distributional settings reveals that\nmodels rely more on memorization in complex or long-tail cases, and that local\nmemorization is often the dominant driver of errors, leading to up to 67% of\nwrong tokens. We also show that memorization scores from STIM can be effective\nin predicting the wrong tokens in the wrong reasoning step. STIM offers a\npowerful tool for diagnosing and improving model reasoning and can generalize\nto other structured step-wise generation tasks.",
        "url": "http://arxiv.org/abs/2508.02037v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02037v1",
        "arxiv_id": "2508.02037v1",
        "authors": [
            "Huihan Li",
            "You Chen",
            "Siyuan Wang",
            "Yixin He",
            "Ninareh Mehrabi",
            "Rahul Gupta",
            "Xiang Ren"
        ],
        "submitted": "2025-08-04 04:06:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Evaluating Position Bias in Large Language Model Recommendations",
        "abstract": "Large Language Models (LLMs) are being increasingly explored as\ngeneral-purpose tools for recommendation tasks, enabling zero-shot and\ninstruction-following capabilities without the need for task-specific training.\nWhile the research community is enthusiastically embracing LLMs, there are\nimportant caveats to directly adapting them for recommendation tasks. In this\npaper, we show that LLM-based recommendation models suffer from position bias,\nwhere the order of candidate items in a prompt can disproportionately influence\nthe recommendations produced by LLMs. First, we analyse the position bias of\nLLM-based recommendations on real-world datasets, where results uncover\nsystemic biases of LLMs with high sensitivity to input orders. Furthermore, we\nintroduce a new prompting strategy to mitigate the position bias of LLM\nrecommendation models called Ranking via Iterative SElection (RISE). We compare\nour proposed method against various baselines on key benchmark datasets.\nExperiment results show that our method reduces sensitivity to input ordering\nand improves stability without requiring model fine-tuning or post-processing.",
        "url": "http://arxiv.org/abs/2508.02020v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02020v1",
        "arxiv_id": "2508.02020v1",
        "authors": [
            "Ethan Bito",
            "Yongli Ren",
            "Estrid He"
        ],
        "submitted": "2025-08-04 03:30:26",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models",
        "abstract": "Large audio-language models (LALMs) have achieved near-human performance in\nsentence-level transcription and emotion recognition. However, existing\nevaluations focus mainly on surface-level perception, leaving the capacity of\nmodels for contextual and inference-driven reasoning in speech-based scenarios\ninsufficiently examined. To address this gap, we introduce SpeechR, a unified\nbenchmark for evaluating reasoning over speech in large audio-language models.\nSpeechR evaluates models along three key dimensions: factual retrieval,\nprocedural inference, and normative judgment. It includes three distinct\nevaluation formats. The multiple-choice version measures answer selection\naccuracy. The generative version assesses the coherence and logical consistency\nof reasoning chains. The acoustic-feature version investigates whether\nvariations in stress and emotion affect reasoning performance. Evaluations on\neleven state-of-the-art LALMs reveal that high transcription accuracy does not\ntranslate into strong reasoning capabilities. SpeechR establishes a structured\nbenchmark for evaluating reasoning in spoken language, enabling more targeted\nanalysis of model capabilities across diverse dialogue-based tasks.",
        "url": "http://arxiv.org/abs/2508.02018v1",
        "pdf_url": "http://arxiv.org/pdf/2508.02018v1",
        "arxiv_id": "2508.02018v1",
        "authors": [
            "Wanqi Yang",
            "Yanda Li",
            "Yunchao Wei",
            "Meng Fang",
            "Ling Chen"
        ],
        "submitted": "2025-08-04 03:28:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SpeechRole: A Large-Scale Dataset and Benchmark for Evaluating Speech Role-Playing Agents",
        "abstract": "Recently, role-playing agents have emerged as a promising paradigm for\nachieving personalized interaction and emotional resonance. Existing research\nprimarily focuses on the textual modality, neglecting the critical dimension of\nspeech in realistic interactive scenarios. In particular, there is a lack of\nsystematic evaluation for Speech Role-Playing Agents (SRPAs). To address this\ngap, we construct SpeechRole-Data, a large-scale, high-quality dataset that\ncomprises 98 diverse roles and 112k speech-based single-turn and multi-turn\nconversations. Each role demonstrates distinct vocal characteristics, including\ntimbre and prosody, thereby enabling more sophisticated speech role-playing.\nFurthermore, we propose SpeechRole-Eval, a multidimensional evaluation\nbenchmark that systematically assesses SRPAs performance in key aspects such as\nfundamental interaction ability, speech expressiveness, and role-playing\nfidelity. Experimental results reveal the advantages and challenges of both\ncascaded and end-to-end speech role-playing agents in maintaining vocal style\nconsistency and role coherence. We release all data, code, and baseline models\nto provide a solid foundation for speech-driven multimodal role-playing\nresearch and to foster further developments in this field.",
        "url": "http://arxiv.org/abs/2508.02013v2",
        "pdf_url": "http://arxiv.org/pdf/2508.02013v2",
        "arxiv_id": "2508.02013v2",
        "authors": [
            "Changhao Jiang",
            "Jiajun Sun",
            "Yifei Cao",
            "Jiabao Zhuang",
            "Hui Li",
            "Xiaoran Fan",
            "Ming Zhang",
            "Junjie Ye",
            "Shihan Dou",
            "Zhiheng Xi",
            "Jingqi Tong",
            "Yilong Wu",
            "Baoyu Fan",
            "Zhen Wang",
            "Tao Liang",
            "Zhihui Fei",
            "Mingyang Wan",
            "Guojun Ma",
            "Tao Ji",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "submitted": "2025-08-04 03:18:36",
        "source": "arxiv",
        "comment": "We request withdrawal of this paper due to an error in the\n  experimental results reported in Table 2 on page 8. Specifically, the results\n  for the Qwen2.5-Omni model are incorrect. We are currently conducting further\n  verification and plan to resubmit with corrected results"
    },
    {
        "title": "Prompting Large Language Models to Detect Dementia Family Caregivers",
        "abstract": "Social media, such as Twitter, provides opportunities for caregivers of\ndementia patients to share their experiences and seek support for a variety of\nreasons. Availability of this information online also paves the way for the\ndevelopment of internet-based interventions in their support. However, for this\npurpose, tweets written by caregivers of dementia patients must first be\nidentified. This paper demonstrates our system for the SMM4H 2025 shared task\n3, which focuses on detecting tweets posted by individuals who have a family\nmember with dementia. The task is outlined as a binary classification problem,\ndifferentiating between tweets that mention dementia in the context of a family\nmember and those that do not. Our solution to this problem explores large\nlanguage models (LLMs) with various prompting methods. Our results show that a\nsimple zero-shot prompt on a fine-tuned model yielded the best results. Our\nfinal system achieved a macro F1-score of 0.95 on the validation set and the\ntest set. Our full code is available on GitHub.",
        "url": "http://arxiv.org/abs/2508.01999v1",
        "pdf_url": "http://arxiv.org/pdf/2508.01999v1",
        "arxiv_id": "2508.01999v1",
        "authors": [
            "Md Badsha Biswas",
            "Özlem Uzuner"
        ],
        "submitted": "2025-08-04 02:39:16",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Contextually Aware E-Commerce Product Question Answering using RAG",
        "abstract": "E-commerce product pages contain a mix of structured specifications,\nunstructured reviews, and contextual elements like personalized offers or\nregional variants. Although informative, this volume can lead to cognitive\noverload, making it difficult for users to quickly and accurately find the\ninformation they need. Existing Product Question Answering (PQA) systems often\nfail to utilize rich user context and diverse product information effectively.\nWe propose a scalable, end-to-end framework for e-commerce PQA using Retrieval\nAugmented Generation (RAG) that deeply integrates contextual understanding. Our\nsystem leverages conversational history, user profiles, and product attributes\nto deliver relevant and personalized answers. It adeptly handles objective,\nsubjective, and multi-intent queries across heterogeneous sources, while also\nidentifying information gaps in the catalog to support ongoing content\nimprovement. We also introduce novel metrics to measure the framework's\nperformance which are broadly applicable for RAG system evaluations.",
        "url": "http://arxiv.org/abs/2508.01990v1",
        "pdf_url": "http://arxiv.org/pdf/2508.01990v1",
        "arxiv_id": "2508.01990v1",
        "authors": [
            "Praveen Tangarajan",
            "Anand A. Rajasekar",
            "Manish Rathi",
            "Vinay Rao Dandin",
            "Ozan Ersoy"
        ],
        "submitted": "2025-08-04 02:14:07",
        "source": "arxiv",
        "comment": "6 pages, 1 figure, 5 tables. Preprint under review"
    },
    {
        "title": "Controllable and Stealthy Shilling Attacks via Dispersive Latent Diffusion",
        "abstract": "Recommender systems (RSs) are now fundamental to various online platforms,\nbut their dependence on user-contributed data leaves them vulnerable to\nshilling attacks that can manipulate item rankings by injecting fake users.\nAlthough widely studied, most existing attack models fail to meet two critical\nobjectives simultaneously: achieving strong adversarial promotion of target\nitems while maintaining realistic behavior to evade detection. As a result, the\ntrue severity of shilling threats that manage to reconcile the two objectives\nremains underappreciated. To expose this overlooked vulnerability, we present\nDLDA, a diffusion-based attack framework that can generate highly effective yet\nindistinguishable fake users by enabling fine-grained control over target\npromotion. Specifically, DLDA operates in a pre-aligned collaborative embedding\nspace, where it employs a conditional latent diffusion process to iteratively\nsynthesize fake user profiles with precise target item control. To evade\ndetection, DLDA introduces a dispersive regularization mechanism that promotes\nvariability and realism in generated behavioral patterns. Extensive experiments\non three real-world datasets and five popular RS models demonstrate that,\ncompared to prior attacks, DLDA consistently achieves stronger item promotion\nwhile remaining harder to detect. These results highlight that modern RSs are\nmore vulnerable than previously recognized, underscoring the urgent need for\nmore robust defenses.",
        "url": "http://arxiv.org/abs/2508.01987v1",
        "pdf_url": "http://arxiv.org/pdf/2508.01987v1",
        "arxiv_id": "2508.01987v1",
        "authors": [
            "Shutong Qiao",
            "Wei Yuan",
            "Junliang Yu",
            "Tong Chen",
            "Quoc Viet Hung Nguyen",
            "Hongzhi Yin"
        ],
        "submitted": "2025-08-04 01:54:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TIBSTC-CoT: A Multi-Domain Instruction Dataset for Chain-of-Thought Reasoning in Language Models",
        "abstract": "To address the severe data scarcity in Tibetan, a low-resource language\nspoken by over six million people, we introduce TIBSTC-CoT, the large-scale,\nmulti-domain Tibetan dataset automatically constructed via chain-of-thought\nprompting with large language models (LLMs). TIBSTC-CoT establishes a scalable\nand reproducible framework for dataset creation in low-resource settings,\ncovering diverse domains and reasoning patterns essential for language\nunderstanding and generation. Building on this dataset, we develop the\nSunshine-thinking LLM family, a series of Tibetan-centric LLMs equipped with\nchain-of-thought capabilities. Trained entirely on TIBSTC-CoT,\nSunshine-thinking has demonstrated strong reasoning and generation performance,\ncomparable to state-of-the-art (SOTA) multilingual LLMs. Our work marks a\nsignificant step toward inclusive AI by enabling high-quality Tibetan language\nprocessing through both resource creation and model innovation. All data are\navailable: https://github.com/Vicentvankor/sun-shine.",
        "url": "http://arxiv.org/abs/2508.01977v1",
        "pdf_url": "http://arxiv.org/pdf/2508.01977v1",
        "arxiv_id": "2508.01977v1",
        "authors": [
            "Fan Gao",
            "Cheng Huang",
            "Nyima Tashi",
            "Yutong Liu",
            "Xiangxiang Wang",
            "Thupten Tsering",
            "Ban Ma-bao",
            "Renzeg Duojie",
            "Gadeng Luosang",
            "Rinchen Dongrub",
            "Dorje Tashi",
            "Xiao Feng",
            "Hao Wang",
            "Yongbin Yu"
        ],
        "submitted": "2025-08-04 01:32:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Non-Verbal Vocalisations and their Challenges: Emotion, Privacy, Sparseness, and Real Life",
        "abstract": "Non-Verbal Vocalisations (NVVs) are short `non-word' utterances without\nproper linguistic (semantic) meaning but conveying connotations -- be this\nemotions/affects or other paralinguistic information. We start this\ncontribution with a historic sketch: how they were addressed in psychology and\nlinguistics in the last two centuries, how they were neglected later on, and\nhow they came to the fore with the advent of emotion research. We then give an\noverview of types of NVVs (formal aspects) and functions of NVVs, exemplified\nwith the typical NVV \\textit{ah}. Interesting as they are, NVVs come, however,\nwith a bunch of challenges that should be accounted for: Privacy and general\nethical considerations prevent them of being recorded in real-life (private)\nscenarios to a sufficient extent. Isolated, prompted (acted) exemplars do not\nnecessarily model NVVs in context; yet, this is the preferred strategy so far\nwhen modelling NVVs, especially in AI. To overcome these problems, we argue in\nfavour of corpus-based approaches. This guarantees a more realistic modelling;\nhowever, we are still faced with privacy and sparse data problems.",
        "url": "http://arxiv.org/abs/2508.01960v1",
        "pdf_url": "http://arxiv.org/pdf/2508.01960v1",
        "arxiv_id": "2508.01960v1",
        "authors": [
            "Anton Batliner",
            "Shahin Amiriparian",
            "Björn W. Schuller"
        ],
        "submitted": "2025-08-03 23:59:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension",
        "abstract": "Retrieval-augmented generation (RAG) over long documents typically involves\nsplitting the text into smaller chunks, which serve as the basic units for\nretrieval. However, due to dependencies across the original document,\ncontextual information is often essential for accurately interpreting each\nchunk. To address this, prior work has explored encoding longer context windows\nto produce embeddings for longer chunks. Despite these efforts, gains in\nretrieval and downstream tasks remain limited. This is because (1) longer\nchunks strain the capacity of embedding models due to the increased amount of\ninformation they must encode, and (2) many real-world applications still\nrequire returning localized evidence due to constraints on model or human\nbandwidth.\n  We propose an alternative approach to this challenge by representing short\nchunks in a way that is conditioned on a broader context window to enhance\nretrieval performance -- i.e., situating a chunk's meaning within its context.\nWe further show that existing embedding models are not well-equipped to encode\nsuch situated context effectively, and thus introduce a new training paradigm\nand develop the situated embedding models (SitEmb). To evaluate our method, we\ncurate a book-plot retrieval dataset specifically designed to assess situated\nretrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3\nsubstantially outperforms state-of-the-art embedding models, including several\nwith up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model\nfurther improves performance by over 10% and shows strong results across\ndifferent languages and several downstream applications.",
        "url": "http://arxiv.org/abs/2508.01959v1",
        "pdf_url": "http://arxiv.org/pdf/2508.01959v1",
        "arxiv_id": "2508.01959v1",
        "authors": [
            "Junjie Wu",
            "Jiangnan Li",
            "Yuqing Li",
            "Lemao Liu",
            "Liyan Xu",
            "Jiwei Li",
            "Dit-Yan Yeung",
            "Jie Zhou",
            "Mo Yu"
        ],
        "submitted": "2025-08-03 23:59:31",
        "source": "arxiv",
        "comment": "Our trained models can be downloaded from:\n  https://huggingface.co/SituatedEmbedding"
    },
    {
        "title": "ROVER: Recursive Reasoning Over Videos with Vision-Language Models for Embodied Tasks",
        "abstract": "Vision-language models (VLMs) have exhibited impressive capabilities across\ndiverse image understanding tasks, but still struggle in settings that require\nreasoning over extended sequences of camera frames from a video. This limits\ntheir utility in embodied settings, which require reasoning over long frame\nsequences from a continuous stream of visual input at each moment of a task\nattempt. To address this limitation, we propose ROVER (Reasoning Over VidEo\nRecursively), a framework that enables the model to recursively decompose\nlong-horizon video trajectories into segments corresponding to shorter subtasks\nwithin the trajectory. In doing so, ROVER facilitates more focused and accurate\nreasoning over temporally localized frame sequences without losing global\ncontext. We evaluate ROVER, implemented using an in-context learning approach,\non diverse OpenX Embodiment videos and on a new dataset derived from RoboCasa\nthat consists of 543 videos showing both expert and perturbed non-expert\ntrajectories across 27 robotic manipulation tasks. ROVER outperforms strong\nbaselines across three video reasoning tasks: task progress estimation,\nframe-level natural language reasoning, and video question answering. We\nobserve that, by reducing the number of frames the model reasons over at each\ntimestep, ROVER mitigates hallucinations, especially during unexpected or\nnon-optimal moments of a trajectory. In addition, by enabling the\nimplementation of a subtask-specific sliding context window, ROVER's time\ncomplexity scales linearly with video length, an asymptotic improvement over\nbaselines. Demos, code, and data available at: https://rover-vlm.github.io",
        "url": "http://arxiv.org/abs/2508.01943v1",
        "pdf_url": "http://arxiv.org/pdf/2508.01943v1",
        "arxiv_id": "2508.01943v1",
        "authors": [
            "Philip Schroeder",
            "Ondrej Biza",
            "Thomas Weng",
            "Hongyin Luo",
            "James Glass"
        ],
        "submitted": "2025-08-03 22:33:43",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Word Overuse and Alignment in Large Language Models: The Influence of Learning from Human Feedback",
        "abstract": "Large Language Models (LLMs) are known to overuse certain terms like \"delve\"\nand \"intricate.\" The exact reasons for these lexical choices, however, have\nbeen unclear. Using Meta's Llama model, this study investigates the\ncontribution of Learning from Human Feedback (LHF), under which we subsume\nReinforcement Learning from Human Feedback and Direct Preference Optimization.\nWe present a straightforward procedure for detecting the lexical preferences of\nLLMs that are potentially LHF-induced. Next, we more conclusively link LHF to\nlexical overuse by experimentally emulating the LHF procedure and demonstrating\nthat participants systematically prefer text variants that include certain\nwords. This lexical overuse can be seen as a sort of misalignment, though our\nstudy highlights the potential divergence between the lexical expectations of\ndifferent populations -- namely LHF workers versus LLM users. Our work\ncontributes to the growing body of research on explainable artificial\nintelligence and emphasizes the importance of both data and procedural\ntransparency in alignment research.",
        "url": "http://arxiv.org/abs/2508.01930v1",
        "pdf_url": "http://arxiv.org/pdf/2508.01930v1",
        "arxiv_id": "2508.01930v1",
        "authors": [
            "Tom S. Juzek",
            "Zina B. Ward"
        ],
        "submitted": "2025-08-03 21:45:37",
        "source": "arxiv",
        "comment": "Accepted for publication in the Proceedings of the 5th Workshop on\n  Bias and Fairness in AI (BIAS 2025) at ECML PKDD"
    },
    {
        "title": "Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language",
        "abstract": "Despite the rapid advancement of large language models (LLMs), low-resource\nlanguages remain largely excluded from the NLP landscape. We present PunGPT2,\nthe first fully open-source suite of Punjabi large language models, trained\nfrom scratch on a 35GB domain-diverse corpus encompassing literature, religious\ntexts, news, and social discourse. Unlike prior multilingual approaches,\nPunGPT2 captures rich syntactic and morphological features unique to Punjabi\nthrough a tokenizer optimised with byte pair encoding and linguistically\naligned pretraining objectives. To improve factual grounding and domain recall,\nwe introduce Pun-RAG, a retrieval-augmented generation framework combining\nPunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We\nfurther develop Pun-Instruct, a parameter-efficient, instruction-tuned variant\nusing QLoRA, enabling robust zero-shot and instruction-following performance\nwith significantly reduced compute needs.\n  As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system\nthat fuses sparse (BM25) and dense methods with quantum-inspired semantic\nmatching. By encoding queries using amplitude-based embeddings and retrieving\nvia quantum kernel similarity, Quantum-RAG achieves improved contextual\nrelevance with minimal memory overhead marking the first practical integration\nof quantum representations in low-resource language generation. Our models\nsignificantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in\nperplexity, factuality, and fluency. This work provides a scalable,\nreproducible blueprint for extending LLM capabilities to underrepresented\nlanguages and pioneers quantum-aware retrieval in low-resource NLP",
        "url": "http://arxiv.org/abs/2508.01918v1",
        "pdf_url": "http://arxiv.org/pdf/2508.01918v1",
        "arxiv_id": "2508.01918v1",
        "authors": [
            "Jaskaranjeet Singh",
            "Rakesh Thakur"
        ],
        "submitted": "2025-08-03 21:03:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Decomposing Representation Space into Interpretable Subspaces with Unsupervised Learning",
        "abstract": "Understanding internal representations of neural models is a core interest of\nmechanistic interpretability. Due to its large dimensionality, the\nrepresentation space can encode various aspects about inputs. To what extent\nare different aspects organized and encoded in separate subspaces? Is it\npossible to find these ``natural'' subspaces in a purely unsupervised way?\nSomewhat surprisingly, we can indeed achieve this and find interpretable\nsubspaces by a seemingly unrelated training objective. Our method, neighbor\ndistance minimization (NDM), learns non-basis-aligned subspaces in an\nunsupervised manner. Qualitative analysis shows subspaces are interpretable in\nmany cases, and encoded information in obtained subspaces tends to share the\nsame abstract concept across different inputs, making such subspaces similar to\n``variables'' used by the model. We also conduct quantitative experiments using\nknown circuits in GPT-2; results show a strong connection between subspaces and\ncircuit variables. We also provide evidence showing scalability to 2B models by\nfinding separate subspaces mediating context and parametric knowledge routing.\nViewed more broadly, our findings offer a new perspective on understanding\nmodel internals and building circuits.",
        "url": "http://arxiv.org/abs/2508.01916v1",
        "pdf_url": "http://arxiv.org/pdf/2508.01916v1",
        "arxiv_id": "2508.01916v1",
        "authors": [
            "Xinting Huang",
            "Michael Hahn"
        ],
        "submitted": "2025-08-03 20:59:29",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "A Decentralized Framework for Ethical Authorship Validation in Academic Publishing: Leveraging Self-Sovereign Identity and Blockchain Technology",
        "abstract": "Academic publishing, integral to knowledge dissemination and scientific\nadvancement, increasingly faces threats from unethical practices such as\nunconsented authorship, gift authorship, author ambiguity, and undisclosed\nconflicts of interest. While existing infrastructures like ORCID effectively\ndisambiguate researcher identities, they fall short in enforcing explicit\nauthorship consent, accurately verifying contributor roles, and robustly\ndetecting conflicts of interest during peer review. To address these\nshortcomings, this paper introduces a decentralized framework leveraging\nSelf-Sovereign Identity (SSI) and blockchain technology. The proposed model\nuses Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs) to\nsecurely verify author identities and contributions, reducing ambiguity and\nensuring accurate attribution. A blockchain-based trust registry records\nauthorship consent and peer-review activity immutably. Privacy-preserving\ncryptographic techniques, especially Zero-Knowledge Proofs (ZKPs), support\nconflict-of-interest detection without revealing sensitive data. Verified\nauthorship metadata and consent records are embedded in publications,\nincreasing transparency. A stakeholder survey of researchers, editors, and\nreviewers suggests the framework improves ethical compliance and confidence in\nscholarly communication. This work represents a step toward a more transparent,\naccountable, and trustworthy academic publishing ecosystem.",
        "url": "http://arxiv.org/abs/2508.01913v1",
        "pdf_url": "http://arxiv.org/pdf/2508.01913v1",
        "arxiv_id": "2508.01913v1",
        "authors": [
            "Kamal Al-Sabahi",
            "Yousuf Khamis Al Mabsali"
        ],
        "submitted": "2025-08-03 20:26:19",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Revisiting Replay and Gradient Alignment for Continual Pre-Training of Large Language Models",
        "abstract": "Training large language models (LLMs) typically involves pre-training on\nmassive corpora, only to restart the process entirely when new data becomes\navailable. A more efficient and resource-conserving approach would be continual\npre-training, where models are updated with new data rather than retraining\nfrom scratch. However, the introduction of new data often causes distribution\nshifts, leading to performance degradation on previously learned tasks. In this\npaper, we take a deeper look at two popular proposals for addressing this\ndistribution shift within the continual learning literature: experience replay\nand gradient alignment. We consider continual pre-training of models within the\nLlama family of architectures at a large scale across languages with 100\nbillion tokens of training data in each language, finding that both replay and\ngradient alignment lead to more stable learning without forgetting. This\nconclusion holds both as we vary the model scale and as we vary the number and\ndiversity of tasks. Moreover, we are the first to demonstrate the effectiveness\nof gradient alignment techniques in the context of LLM pre-training and propose\nan efficient implementation of meta-experience replay (MER) that imbues\nexperience replay with the benefits of gradient alignment despite negligible\ncompute and memory overhead. Our scaling analysis across model sizes and replay\nrates indicates that small rates of replaying old examples are definitely a\nmore valuable use of compute than investing in model size, but that it is more\ncompute efficient to scale the size of the model than invest in high rates of\nreplaying old examples.",
        "url": "http://arxiv.org/abs/2508.01908v1",
        "pdf_url": "http://arxiv.org/pdf/2508.01908v1",
        "arxiv_id": "2508.01908v1",
        "authors": [
            "Istabrak Abbes",
            "Gopeshh Subbaraj",
            "Matthew Riemer",
            "Nizar Islah",
            "Benjamin Therien",
            "Tsuguchika Tabaru",
            "Hiroaki Kingetsu",
            "Sarath Chandar",
            "Irina Rish"
        ],
        "submitted": "2025-08-03 20:07:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Complete Evasion, Zero Modification: PDF Attacks on AI Text Detection",
        "abstract": "AI-generated text detectors have become essential tools for maintaining\ncontent authenticity, yet their robustness against evasion attacks remains\nquestionable. We present PDFuzz, a novel attack that exploits the discrepancy\nbetween visual text layout and extraction order in PDF documents. Our method\npreserves exact textual content while manipulating character positioning to\nscramble extraction sequences. We evaluate this approach against the ArguGPT\ndetector using a dataset of human and AI-generated text. Our results\ndemonstrate complete evasion: detector performance drops from (93.6 $\\pm$ 1.4)\n% accuracy and 0.938 $\\pm$ 0.014 F1 score to random-level performance ((50.4\n$\\pm$ 3.2) % accuracy, 0.0 F1 score) while maintaining perfect visual fidelity.\nOur work reveals a vulnerability in current detection systems that is inherent\nto PDF document structures and underscores the need for implementing sturdy\nsafeguards against such attacks. We make our code publicly available at\nhttps://github.com/ACMCMC/PDFuzz.",
        "url": "http://arxiv.org/abs/2508.01887v1",
        "pdf_url": "http://arxiv.org/pdf/2508.01887v1",
        "arxiv_id": "2508.01887v1",
        "authors": [
            "Aldan Creo"
        ],
        "submitted": "2025-08-03 18:43:41",
        "source": "arxiv",
        "comment": "Code: https://github.com/ACMCMC/PDFuzz"
    }
]