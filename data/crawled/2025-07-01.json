[
    {
        "title": "SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning",
        "abstract": "Recent advances in reinforcement learning have shown that language models can\ndevelop sophisticated reasoning through training on tasks with verifiable\nrewards, but these approaches depend on human-curated problem-answer pairs and\ndomain-specific reward engineering. We introduce SPIRAL, a self-play framework\nwhere models learn by playing multi-turn, zero-sum games against continuously\nimproving versions of themselves, eliminating the need for human supervision.\nThrough self-play, SPIRAL generates an infinite curriculum of progressively\nchallenging problems as models must constantly adapt to stronger opponents. To\nenable this self-play training at scale, We implement a fully online,\nmulti-turn, multi-agent reinforcement learning system for LLMs and propose\nrole-conditioned advantage estimation (RAE) to stabilize multi-agent training.\nUsing SPIRAL, self-play on zero-sum games produces reasoning capabilities that\ntransfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%\nimprovement on math and 8.4% on general reasoning, outperforming SFT on 25,000\nexpert game trajectories. Analysis reveals that this transfer occurs through\nthree cognitive patterns: systematic decomposition, expected value calculation,\nand case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple\nNegotiation) further enhances performance as each game develops distinct\nreasoning strengths. Applying SPIRAL to a strong reasoning model\n(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These\nresults demonstrate that zero-sum games naturally develop transferable\nreasoning capabilities, highlighting a promising direction for autonomous\nreasoning development.",
        "url": "http://arxiv.org/abs/2506.24119v2",
        "pdf_url": "http://arxiv.org/pdf/2506.24119v2",
        "arxiv_id": "2506.24119v2",
        "authors": [
            "Bo Liu",
            "Leon Guertler",
            "Simon Yu",
            "Zichen Liu",
            "Penghui Qi",
            "Daniel Balcells",
            "Mickel Liu",
            "Cheston Tan",
            "Weiyan Shi",
            "Min Lin",
            "Wee Sun Lee",
            "Natasha Jaques"
        ],
        "submitted": "2025-06-30 17:58:13",
        "source": "arxiv",
        "comment": "Work in Progress"
    },
    {
        "title": "Intertextual Parallel Detection in Biblical Hebrew: A Transformer-Based Benchmark",
        "abstract": "Identifying parallel passages in biblical Hebrew (BH) is central to biblical\nscholarship for understanding intertextual relationships. Traditional methods\nrely on manual comparison, a labor-intensive process prone to human error. This\nstudy evaluates the potential of pre-trained transformer-based language models,\nincluding E5, AlephBERT, MPNet, and LaBSE, for detecting textual parallels in\nthe Hebrew Bible. Focusing on known parallels between Samuel/Kings and\nChronicles, I assessed each model's capability to generate word embeddings\ndistinguishing parallel from non-parallel passages. Using cosine similarity and\nWasserstein Distance measures, I found that E5 and AlephBERT show promise; E5\nexcels in parallel detection, while AlephBERT demonstrates stronger\nnon-parallel differentiation. These findings indicate that pre-trained models\ncan enhance the efficiency and accuracy of detecting intertextual parallels in\nancient texts, suggesting broader applications for ancient language studies.",
        "url": "http://arxiv.org/abs/2506.24117v2",
        "pdf_url": "http://arxiv.org/pdf/2506.24117v2",
        "arxiv_id": "2506.24117v2",
        "authors": [
            "David M. Smiley"
        ],
        "submitted": "2025-06-30 17:57:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "On the Predictive Power of Representation Dispersion in Language Models",
        "abstract": "We show that a language model's ability to predict text is tightly linked to\nthe breadth of its embedding space: models that spread their contextual\nrepresentations more widely tend to achieve lower perplexity. Concretely, we\nfind that representation dispersion - the average pairwise cosine distance\namong hidden vectors - strongly and negatively correlates with perplexity\nacross diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia,\nnews, scientific abstracts). Beyond illustrating this link, we show how\ndispersion can be leveraged for a range of practical tasks without requiring\nlabeled data. First, measuring dispersion on unlabeled text allows us to\npredict downstream accuracy in new domains, offering a data-efficient tool for\nmodel selection. Next, we find that identifying layers with higher dispersion\npinpoints the best representations for retrieval-based methods such as kNN-LM,\nbypassing exhaustive layer-by-layer searches. Finally, we integrate a simple\npush-away objective into training, which increases dispersion in both\nsingle-domain and cross-domain scenarios and directly improves perplexity in\neach.",
        "url": "http://arxiv.org/abs/2506.24106v1",
        "pdf_url": "http://arxiv.org/pdf/2506.24106v1",
        "arxiv_id": "2506.24106v1",
        "authors": [
            "Yanhong Li",
            "Ming Li",
            "Karen Livescu",
            "Jiawei Zhou"
        ],
        "submitted": "2025-06-30 17:53:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MotionGPT3: Human Motion as a Second Modality",
        "abstract": "Though recent advances in multimodal models have demonstrated strong\ncapabilities and opportunities in unified understanding and generation, the\ndevelopment of unified motion-language models remains underexplored. To enable\nsuch models with high-fidelity human motion, two core challenges must be\naddressed. The first is the reconstruction gap between the continuous motion\nmodality and discrete representation in an autoregressive manner, and the\nsecond is the degradation of language intelligence during unified training.\nInspired by the mixture of experts, we propose MotionGPT3, a bimodal\nmotion-language model that treats human motion as a second modality, decoupling\nmotion modeling via separate model parameters and enabling both effective\ncross-modal interaction and efficient multimodal scaling training. To preserve\nlanguage intelligence, the text branch retains the original structure and\nparameters of the pretrained language model, while a new motion branch is\nintegrated via a shared attention mechanism, enabling bidirectional information\nflow between two modalities. We first employ a motion Variational Autoencoder\n(VAE) to encode raw human motion into latent representations. Based on this\ncontinuous latent space, the motion branch predicts motion latents directly\nfrom intermediate hidden states using a diffusion head, bypassing discrete\ntokenization. Extensive experiments show that our approach achieves competitive\nperformance on both motion understanding and generation tasks while preserving\nstrong language capabilities, establishing a unified bimodal motion diffusion\nframework within an autoregressive manner.",
        "url": "http://arxiv.org/abs/2506.24086v1",
        "pdf_url": "http://arxiv.org/pdf/2506.24086v1",
        "arxiv_id": "2506.24086v1",
        "authors": [
            "Bingfan Zhu",
            "Biao Jiang",
            "Sunyi Wang",
            "Shixiang Tang",
            "Tao Chen",
            "Linjie Luo",
            "Youyi Zheng",
            "Xin Chen"
        ],
        "submitted": "2025-06-30 17:42:22",
        "source": "arxiv",
        "comment": "21 pages, 8 figures"
    },
    {
        "title": "STACK: Adversarial Attacks on LLM Safeguard Pipelines",
        "abstract": "Frontier AI developers are relying on layers of safeguards to protect against\ncatastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus\nmodel using one such defense pipeline, and other frontier developers including\nGoogle DeepMind and OpenAI pledge to soon deploy similar defenses. However, the\nsecurity of such pipelines is unclear, with limited prior work evaluating or\nattacking these pipelines. We address this gap by developing and red-teaming an\nopen-source defense pipeline. First, we find that a novel few-shot-prompted\ninput and output classifier outperforms state-of-the-art open-weight safeguard\nmodel ShieldGemma across three attacks and two datasets, reducing the attack\nsuccess rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second,\nwe introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on\nClearHarm in a black-box attack against the few-shot-prompted classifier\npipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33%\nASR, providing initial evidence that it is feasible to design attacks with no\naccess to the target pipeline. We conclude by suggesting specific mitigations\nthat developers could use to thwart staged attacks.",
        "url": "http://arxiv.org/abs/2506.24068v1",
        "pdf_url": "http://arxiv.org/pdf/2506.24068v1",
        "arxiv_id": "2506.24068v1",
        "authors": [
            "Ian R. McKenzie",
            "Oskar J. Hollinsworth",
            "Tom Tseng",
            "Xander Davies",
            "Stephen Casper",
            "Aaron D. Tucker",
            "Robert Kirk",
            "Adam Gleave"
        ],
        "submitted": "2025-06-30 17:21:08",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Logit-Gap Steering: Efficient Short-Suffix Jailbreaks for Aligned Large Language Models",
        "abstract": "We introduce logit-gap steering, a fast jailbreak framework that casts the\nrefusal-affirmation gap of RLHF-aligned language models as a single pass over\nthe vocabulary. A forward-computable score blends gap reduction with\nlightweight proxies for KL penalty and reward shift, allowing a \"sort-sum-stop\"\nsweep to complete in under a second and return a short suffix--two orders of\nmagnitude fewer model calls than beam or gradient attacks. The same suffix\ngeneralises to unseen prompts and scales from 0.5 B to 70 B checkpoints,\nlifting one-shot attack success from baseline levels to 80-100% while\npreserving topical coherence. Beyond efficiency, these suffixes expose\nsentence-boundary reward cliffs and other alignment artefacts, offering a\nlightweight probe into how safety tuning reshapes internal representations.",
        "url": "http://arxiv.org/abs/2506.24056v1",
        "pdf_url": "http://arxiv.org/pdf/2506.24056v1",
        "arxiv_id": "2506.24056v1",
        "authors": [
            "Tung-Ling Li",
            "Hongliang Liu"
        ],
        "submitted": "2025-06-30 17:01:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Ella: Embodied Social Agents with Lifelong Memory",
        "abstract": "We introduce Ella, an embodied social agent capable of lifelong learning\nwithin a community in a 3D open world, where agents accumulate experiences and\nacquire knowledge through everyday visual observations and social interactions.\nAt the core of Ella's capabilities is a structured, long-term multimodal memory\nsystem that stores, updates, and retrieves information effectively. It consists\nof a name-centric semantic memory for organizing acquired knowledge and a\nspatiotemporal episodic memory for capturing multimodal experiences. By\nintegrating this lifelong memory system with foundation models, Ella retrieves\nrelevant information for decision-making, plans daily activities, builds social\nrelationships, and evolves autonomously while coexisting with other intelligent\nbeings in the open world. We conduct capability-oriented evaluations in a\ndynamic 3D open world where 15 agents engage in social activities for days and\nare assessed with a suite of unseen controlled evaluations. Experimental\nresults show that Ella can influence, lead, and cooperate with other agents\nwell to achieve goals, showcasing its ability to learn effectively through\nobservation and social interaction. Our findings highlight the transformative\npotential of combining structured memory systems with foundation models for\nadvancing embodied intelligence. More videos can be found at\nhttps://umass-embodied-agi.github.io/Ella/.",
        "url": "http://arxiv.org/abs/2506.24019v1",
        "pdf_url": "http://arxiv.org/pdf/2506.24019v1",
        "arxiv_id": "2506.24019v1",
        "authors": [
            "Hongxin Zhang",
            "Zheyuan Zhang",
            "Zeyuan Wang",
            "Zunzhe Zhang",
            "Lixing Fang",
            "Qinhong Zhou",
            "Chuang Gan"
        ],
        "submitted": "2025-06-30 16:22:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations",
        "abstract": "Recent advances in large language models and vision-language models have led\nto growing interest in explainable evaluation metrics for image captioning.\nHowever, these metrics generate explanations without standardized criteria, and\nthe overall quality of the generated explanations remains unverified. In this\npaper, we propose EXPERT, a reference-free evaluation metric that provides\nstructured explanations based on three fundamental criteria: fluency,\nrelevance, and descriptiveness. By constructing large-scale datasets of\nhigh-quality structured explanations, we develop a two-stage evaluation\ntemplate to effectively supervise a vision-language model for both scoring and\nexplanation generation. EXPERT achieves state-of-the-art results on benchmark\ndatasets while providing significantly higher-quality explanations than\nexisting metrics, as validated through comprehensive human evaluation. Our code\nand datasets are available at https://github.com/hjkim811/EXPERT.",
        "url": "http://arxiv.org/abs/2506.24016v1",
        "pdf_url": "http://arxiv.org/pdf/2506.24016v1",
        "arxiv_id": "2506.24016v1",
        "authors": [
            "Hyunjong Kim",
            "Sangyeop Kim",
            "Jongheon Jeong",
            "Yeongjae Cho",
            "Sungzoon Cho"
        ],
        "submitted": "2025-06-30 16:20:51",
        "source": "arxiv",
        "comment": "Accepted at ACL 2025 Findings"
    },
    {
        "title": "Large Language Models Don't Make Sense of Word Problems. A Scoping Review from a Mathematics Education Perspective",
        "abstract": "The progress of Large Language Models (LLMs) like ChatGPT raises the question\nof how they can be integrated into education. One hope is that they can support\nmathematics learning, including word-problem solving. Since LLMs can handle\ntextual input with ease, they appear well-suited for solving mathematical word\nproblems. Yet their real competence, whether they can make sense of the\nreal-world context, and the implications for classrooms remain unclear. We\nconducted a scoping review from a mathematics-education perspective, including\nthree parts: a technical overview, a systematic review of word problems used in\nresearch, and a state-of-the-art empirical evaluation of LLMs on mathematical\nword problems. First, in the technical overview, we contrast the\nconceptualization of word problems and their solution processes between LLMs\nand students. In computer-science research this is typically labeled\nmathematical reasoning, a term that does not align with usage in mathematics\neducation. Second, our literature review of 213 studies shows that the most\npopular word-problem corpora are dominated by s-problems, which do not require\na consideration of realities of their real-world context. Finally, our\nevaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems\nshows that most recent LLMs solve these s-problems with near-perfect accuracy,\nincluding a perfect score on 20 problems from PISA. LLMs still showed\nweaknesses in tackling problems where the real-world context is problematic or\nnon-sensical. In sum, we argue based on all three aspects that LLMs have\nmastered a superficial solution process but do not make sense of word problems,\nwhich potentially limits their value as instructional tools in mathematics\nclassrooms.",
        "url": "http://arxiv.org/abs/2506.24006v1",
        "pdf_url": "http://arxiv.org/pdf/2506.24006v1",
        "arxiv_id": "2506.24006v1",
        "authors": [
            "Anselm R. Strohmaier",
            "Wim Van Dooren",
            "Kathrin Seßler",
            "Brian Greer",
            "Lieven Verschaffel"
        ],
        "submitted": "2025-06-30 16:10:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning",
        "abstract": "Congenital heart disease (CHD) presents complex, lifelong challenges often\nunderrepresented in traditional clinical metrics. While unstructured narratives\noffer rich insights into patient and caregiver experiences, manual thematic\nanalysis (TA) remains labor-intensive and unscalable. We propose a fully\nautomated large language model (LLM) pipeline that performs end-to-end TA on\nclinical narratives, which eliminates the need for manual coding or full\ntranscript review. Our system employs a novel multi-agent framework, where\nspecialized LLM agents assume roles to enhance theme quality and alignment with\nhuman analysis. To further improve thematic relevance, we optionally integrate\nreinforcement learning from human feedback (RLHF). This supports scalable,\npatient-centered analysis of large qualitative datasets and allows LLMs to be\nfine-tuned for specific clinical contexts.",
        "url": "http://arxiv.org/abs/2506.23998v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23998v1",
        "arxiv_id": "2506.23998v1",
        "authors": [
            "Seungjun Yi",
            "Joakim Nguyen",
            "Huimin Xu",
            "Terence Lim",
            "Andrew Well",
            "Mia Markey",
            "Ying Ding"
        ],
        "submitted": "2025-06-30 16:02:28",
        "source": "arxiv",
        "comment": "Presented at ACL 2025 SRW"
    },
    {
        "title": "Machine Understanding of Scientific Language",
        "abstract": "Scientific information expresses human understanding of nature. This\nknowledge is largely disseminated in different forms of text, including\nscientific papers, news articles, and discourse among people on social media.\nWhile important for accelerating our pursuit of knowledge, not all scientific\ntext is faithful to the underlying science. As the volume of this text has\nburgeoned online in recent years, it has become a problem of societal\nimportance to be able to identify the faithfulness of a given piece of\nscientific text automatically. This thesis is concerned with the cultivation of\ndatasets, methods, and tools for machine understanding of scientific language,\nin order to analyze and understand science communication at scale. To arrive at\nthis, I present several contributions in three areas of natural language\nprocessing and machine learning: automatic fact checking, learning with limited\ndata, and scientific text processing. These contributions include new methods\nand resources for identifying check-worthy claims, adversarial claim\ngeneration, multi-source domain adaptation, learning from crowd-sourced labels,\ncite-worthiness detection, zero-shot scientific fact checking, detecting\nexaggerated scientific claims, and modeling degrees of information change in\nscience communication. Critically, I demonstrate how the research outputs of\nthis thesis are useful for effectively learning from limited amounts of\nscientific text in order to identify misinformative scientific statements and\ngenerate new insights into the science communication process",
        "url": "http://arxiv.org/abs/2506.23990v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23990v1",
        "arxiv_id": "2506.23990v1",
        "authors": [
            "Dustin Wright"
        ],
        "submitted": "2025-06-30 15:55:10",
        "source": "arxiv",
        "comment": "PhD Thesis, 210 pages"
    },
    {
        "title": "TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation",
        "abstract": "Conducting supervised fine-tuning and preference fine-tuning on large\nlanguage models (LLMs) requires high-quality datasets to improve their ability\nto follow instructions and align with human preferences and values. However,\nconstructing such datasets is resource-intensive, and most available datasets\nfor supervised and preference fine-tuning are in English. To address these\nchallenges, we propose the \\underline{\\textbf{Ta}}xonomy-Guided\n\\underline{\\textbf{P}}reference Data Generation (TaP) framework, which\nfacilitates automated and scalable construction of preference datasets across\nvarious languages. TaP is grounded in a structured taxonomy that allows\nfine-grained control over dataset composition, thereby ensuring both diversity\nand comprehensive coverage. We employ TaP-generated datasets to perform\nsupervised and preference fine-tuning on various LLMs. Experimental results\ndemonstrate that LLMs trained on TaP-generated datasets outperform those\ntrained on existing open-source datasets. Remarkably, LLMs trained on\nTaP-generated datasets surpass the performance of those trained on an\nopen-source dataset that is 180 times larger.",
        "url": "http://arxiv.org/abs/2506.23979v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23979v1",
        "arxiv_id": "2506.23979v1",
        "authors": [
            "Renren Jin",
            "Tianhao Shen",
            "Xinwei Wu",
            "Dan Shi",
            "Haoran Sun",
            "Wuwei Huang",
            "Quandong Wang",
            "Wei Liu",
            "Jian Luan",
            "Bin Wang",
            "Deyi Xiong"
        ],
        "submitted": "2025-06-30 15:45:28",
        "source": "arxiv",
        "comment": "33 pages, 15 tables, 11 figures"
    },
    {
        "title": "LLM Agents Are the Antidote to Walled Gardens",
        "abstract": "While the Internet's core infrastructure was designed to be open and\nuniversal, today's application layer is dominated by closed, proprietary\nplatforms. Open and interoperable APIs require significant investment, and\nmarket leaders have little incentive to enable data exchange that could erode\ntheir user lock-in. We argue that LLM-based agents fundamentally disrupt this\nstatus quo. Agents can automatically translate between data formats and\ninteract with interfaces designed for humans: this makes interoperability\ndramatically cheaper and effectively unavoidable. We name this shift universal\ninteroperability: the ability for any two digital services to exchange data\nseamlessly using AI-mediated adapters. Universal interoperability undermines\nmonopolistic behaviours and promotes data portability. However, it can also\nlead to new security risks and technical debt. Our position is that the ML\ncommunity should embrace this development while building the appropriate\nframeworks to mitigate the downsides. By acting now, we can harness AI to\nrestore user freedom and competitive markets without sacrificing security.",
        "url": "http://arxiv.org/abs/2506.23978v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23978v1",
        "arxiv_id": "2506.23978v1",
        "authors": [
            "Samuele Marro",
            "Philip Torr"
        ],
        "submitted": "2025-06-30 15:45:17",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders",
        "abstract": "Sparse Autoencoders (SAEs) have been successfully used to probe Large\nLanguage Models (LLMs) and extract interpretable concepts from their internal\nrepresentations. These concepts are linear combinations of neuron activations\nthat correspond to human-interpretable features. In this paper, we investigate\nthe effectiveness of SAE-based explainability approaches for sentence\nclassification, a domain where such methods have not been extensively explored.\nWe present a novel SAE-based architecture tailored for text classification,\nleveraging a specialized classifier head and incorporating an activation rate\nsparsity loss. We benchmark this architecture against established methods such\nas ConceptShap, Independent Component Analysis, and other SAE-based concept\nextraction techniques. Our evaluation covers two classification benchmarks and\nfour fine-tuned LLMs from the Pythia family. We further enrich our analysis\nwith two novel metrics for measuring the precision of concept-based\nexplanations, using an external sentence encoder. Our empirical results show\nthat our architecture improves both the causality and interpretability of the\nextracted features.",
        "url": "http://arxiv.org/abs/2506.23951v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23951v1",
        "arxiv_id": "2506.23951v1",
        "authors": [
            "Mathis Le Bail",
            "Jérémie Dentan",
            "Davide Buscaldi",
            "Sonia Vanier"
        ],
        "submitted": "2025-06-30 15:18:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs",
        "abstract": "Multimodal Large Language Models (MLLMs) have achieved success across various\ndomains. However, their applicability tends to degrade when confronted with\ndifferent types of data inputs, especially for MLLMs that have been fine-tuned\nfor specific tasks. Despite its importance, the study of knowledge sharing\namong domain-specific MLLMs--such as those trained for mathematics or\ncode--remains largely underexplored. To address the fragmentation of knowledge\nacross domain-specialized MLLMs, we propose a unified parameter integration\nframework that enables modular composition of expert capabilities. Our method\nis grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy,\nwhich leverages both local functional attribution and global\ninformation-theoretic signals to guide selective parameter fusion. By extending\nthis mechanism to the low-rank adaptation layer granularity, we ensure\nefficient integration with minimal inference overhead. Furthermore, we\nintroduce a domain compatibility scoring mechanism that quantifies inter-expert\nalignment at the activation level and correlates with downstream task utility.\nThis principled fusion protocol allows the final model to synergize\nheterogeneous expertise while preserving structural modularity. Extensive\nevaluations across diverse multimodal benchmarks validate the effectiveness of\nour framework, offering a scalable path toward compositional, domain-adaptive\nMLLMs.",
        "url": "http://arxiv.org/abs/2506.23940v2",
        "pdf_url": "http://arxiv.org/pdf/2506.23940v2",
        "arxiv_id": "2506.23940v2",
        "authors": [
            "Yang Dai",
            "Jianxiang An",
            "Tianwei Lin",
            "Hongyang He",
            "Hongzhe Huang",
            "Wenqiao Zhang",
            "Zheqi Lv",
            "Siliang Tang",
            "Yueting Zhuang"
        ],
        "submitted": "2025-06-30 15:07:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages",
        "abstract": "The rapid expansion of social media leads to a marked increase in hate\nspeech, which threatens personal lives and results in numerous hate crimes.\nDetecting hate speech presents several challenges: diverse dialects, frequent\ncode-mixing, and the prevalence of misspelled words in user-generated content\non social media platforms. Recent progress in hate speech detection is\ntypically concentrated on high-resource languages. However, low-resource\nlanguages still face significant challenges due to the lack of large-scale,\nhigh-quality datasets. This paper investigates how we can overcome this\nlimitation via prompt engineering on large language models (LLMs) focusing on\nlow-resource Bengali language. We investigate six prompting strategies -\nzero-shot prompting, refusal suppression, flattering the classifier, multi-shot\nprompting, role prompting, and finally our innovative metaphor prompting to\ndetect hate speech effectively in low-resource languages. We pioneer the\nmetaphor prompting to circumvent the built-in safety mechanisms of LLMs that\nmarks a significant departure from existing jailbreaking methods. We\ninvestigate all six different prompting strategies on the Llama2-7B model and\ncompare the results extensively with three pre-trained word embeddings - GloVe,\nWord2Vec, and FastText for three different deep learning models - multilayer\nperceptron (MLP), convolutional neural network (CNN), and bidirectional gated\nrecurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in\nthe low-resource Bengali language, we also evaluate it in another low-resource\nlanguage - Hindi, and two high-resource languages - English and German. The\nperformance of all prompting techniques is evaluated using the F1 score, and\nenvironmental impact factor (IF), which measures CO$_2$ emissions, electricity\nusage, and computational time.",
        "url": "http://arxiv.org/abs/2506.23930v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23930v1",
        "arxiv_id": "2506.23930v1",
        "authors": [
            "Ruhina Tabasshum Prome",
            "Tarikul Islam Tamiti",
            "Anomadarshi Barua"
        ],
        "submitted": "2025-06-30 14:59:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "IMPACT: Inflectional Morphology Probes Across Complex Typologies",
        "abstract": "Large Language Models (LLMs) have shown significant progress on various\nmultilingual benchmarks and are increasingly used to generate and evaluate text\nin non-English languages. However, while they may produce fluent outputs, it\nremains unclear to what extent these models truly grasp the underlying\nlinguistic complexity of those languages, particularly in morphology. To\ninvestigate this, we introduce IMPACT, a synthetically generated evaluation\nframework focused on inflectional morphology, which we publicly release,\ndesigned to evaluate LLM performance across five morphologically rich\nlanguages: Arabic, Russian, Finnish, Turkish, and Hebrew. IMPACT includes\nunit-test-style cases covering both shared and language-specific phenomena,\nfrom basic verb inflections (e.g., tense, number, gender) to unique features\nlike Arabic's reverse gender agreement and vowel harmony in Finnish and\nTurkish. We assess eight multilingual LLMs that, despite strong English\nperformance, struggle with other languages and uncommon morphological patterns,\nespecially when judging ungrammatical examples. We also show that Chain of\nThought and Thinking Models can degrade performance. Our work exposes gaps in\nLLMs' handling of linguistic complexity, pointing to clear room for\nimprovement. To support further research, we publicly release the IMPACT\nframework.",
        "url": "http://arxiv.org/abs/2506.23929v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23929v1",
        "arxiv_id": "2506.23929v1",
        "authors": [
            "Mohammed J. Saeed",
            "Tommi Vehvilainen",
            "Evgeny Fedoseev",
            "Sevil Caliskan",
            "Tatiana Vodolazova"
        ],
        "submitted": "2025-06-30 14:58:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Trilemma of Truth in Large Language Models",
        "abstract": "We often attribute human characteristics to large language models (LLMs) and\nclaim that they \"know\" certain things. LLMs have an internal probabilistic\nknowledge that represents information retained during training. How can we\nassess the veracity of this knowledge? We examine two common methods for\nprobing the veracity of LLMs and discover several assumptions that are flawed.\nTo address these flawed assumptions, we introduce sAwMIL (short for Sparse\nAware Multiple-Instance Learning), a probing method that utilizes the internal\nactivations of LLMs to separate statements into true, false, and neither.\nsAwMIL is based on multiple-instance learning and conformal prediction. We\nevaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including\nboth default and chat-based variants, as well as on 3 new datasets. Among the\ninsights we provide are: (1) the veracity signal is often concentrated in the\nthird quarter of an LLM's depth; (2) truth and falsehood signals are not always\nsymmetric; (3) linear probes perform better on chat models than on default\nmodels; (4) nonlinear probes may be required to capture veracity signals for\nsome LLMs with reinforcement learning from human feedback or knowledge\ndistillation; and (5) LLMs capture a third type of signal that is distinct from\ntrue and false and is neither true nor false. These findings provide a reliable\nmethod for verifying what LLMs \"know\" and how certain they are of their\nprobabilistic internal knowledge.",
        "url": "http://arxiv.org/abs/2506.23921v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23921v1",
        "arxiv_id": "2506.23921v1",
        "authors": [
            "Germans Savcisens",
            "Tina Eliassi-Rad"
        ],
        "submitted": "2025-06-30 14:49:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting",
        "abstract": "Recent advancements in Large Language Models (LLMs) have significantly\nimproved their problem-solving capabilities. However, these models still\nstruggle when faced with complex multi-step reasoning tasks. In this paper, we\npropose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework,\na novel approach designed to enhance multi-step mathematical reasoning in LLMs\nby integrating techniques such as Chain of Thought (CoT), Self-Reflection, and\nAuto-Prompting. Unlike traditional static prompting methods, MAPS employs an\niterative refinement process. Initially, the model generates a solution using\nCoT prompting. When errors are detected, an adaptive self-reflection mechanism\nidentifies and analyzes them, generating tailored prompts to guide corrections.\nThese dynamically adjusted prompts enable the model to iteratively refine its\nreasoning. Experiments on four well-established benchmarks across multiple LLMs\nshow that MAPS significantly outperforms standard CoT and achieves competitive\nresults with reasoning-optimized models. In addition, MAPS enables\ngeneral-purpose LLMs to reach performance levels comparable to specialized\nreasoning models. While deeper reflection layers improve accuracy, they also\nincrease token usage and costs. To balance this trade-off, MAPS strategically\nlimits reflection depth, ensuring an optimal balance between cost and reasoning\nperformance.",
        "url": "http://arxiv.org/abs/2506.23888v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23888v1",
        "arxiv_id": "2506.23888v1",
        "authors": [
            "André de Souza Loureiro",
            "Jorge Valverde-Rebaza",
            "Julieta Noguez",
            "David Escarcega",
            "Ricardo Marcacini"
        ],
        "submitted": "2025-06-30 14:18:35",
        "source": "arxiv",
        "comment": "Accepted for publication in: European Conference on Machine Learning\n  and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD\n  2025). Research Track"
    },
    {
        "title": "Emergent musical properties of a transformer under contrastive self-supervised learning",
        "abstract": "In music information retrieval (MIR), contrastive self-supervised learning\nfor general-purpose representation models is effective for global tasks such as\nautomatic tagging. However, for local tasks such as chord estimation, it is\nwidely assumed that contrastively trained general-purpose self-supervised\nmodels are inadequate and that more sophisticated SSL is necessary; e.g.,\nmasked modeling. Our paper challenges this assumption by revealing the\npotential of contrastive SSL paired with a transformer in local MIR tasks. We\nconsider a lightweight vision transformer with one-dimensional patches in the\ntime--frequency domain (ViT-1D) and train it with simple contrastive SSL\nthrough normalized temperature-scaled cross-entropy loss (NT-Xent). Although\nNT-Xent operates only over the class token, we observe that, potentially thanks\nto weight sharing, informative musical properties emerge in ViT-1D's sequence\ntokens. On global tasks, the temporal average of class and sequence tokens\noffers a performance increase compared to the class token alone, showing useful\nproperties in the sequence tokens. On local tasks, sequence tokens perform\nunexpectedly well, despite not being specifically trained for. Furthermore,\nhigh-level musical features such as onsets emerge from layer-wise attention\nmaps and self-similarity matrices show different layers capture different\nmusical dimensions. Our paper does not focus on improving performance but\nadvances the musical interpretation of transformers and sheds light on some\noverlooked abilities of contrastive SSL paired with transformers for sequence\nmodeling in MIR.",
        "url": "http://arxiv.org/abs/2506.23873v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23873v1",
        "arxiv_id": "2506.23873v1",
        "authors": [
            "Yuexuan Kong",
            "Gabriel Meseguer-Brocal",
            "Vincent Lostanlen",
            "Mathieu Lagrange",
            "Romain Hennequin"
        ],
        "submitted": "2025-06-30 14:04:59",
        "source": "arxiv",
        "comment": "Accepted at ISMIR 2025"
    },
    {
        "title": "Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What to Do About It",
        "abstract": "We conduct a systematic audit of three widely used reasoning benchmarks,\nSocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark\nitems and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and\nLLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic\nissues in benchmark design (e.g., duplicated items, ambiguous wording, and\nimplausible answers), as well as scoring procedures that prioritize output form\nover reasoning process. Through systematic human annotation and re-evaluation\non cleaned benchmark subsets, we find that model scores often improve not due\nto due to erratic surface wording variations and not to improved reasoning.\nInfact, further analyses show that model performance is highly sensitive to\nminor input variations such as context availability and phrasing, revealing\nthat high scores may reflect alignment with format-specific cues rather than\nconsistent inference based on the input. These findings challenge the validity\nof current benchmark-based claims about reasoning in LLMs, and highlight the\nneed for evaluation protocols that assess reasoning as a process of drawing\ninference from available information, rather than as static output selection.\nWe release audited data and evaluation tools to support more interpretable and\ndiagnostic assessments of model reasoning.",
        "url": "http://arxiv.org/abs/2506.23864v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23864v1",
        "arxiv_id": "2506.23864v1",
        "authors": [
            "Seyed Mahed Mousavi",
            "Edoardo Cecchinato",
            "Lucia Hornikova",
            "Giuseppe Riccardi"
        ],
        "submitted": "2025-06-30 13:57:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Use Sparse Autoencoders to Discover Unknown Concepts, Not to Act on Known Concepts",
        "abstract": "While sparse autoencoders (SAEs) have generated significant excitement, a\nseries of negative results have added to skepticism about their usefulness.\nHere, we establish a conceptual distinction that reconciles competing\nnarratives surrounding SAEs. We argue that while SAEs may be less effective for\nacting on known concepts, SAEs are powerful tools for discovering unknown\nconcepts. This distinction cleanly separates existing negative and positive\nresults, and suggests several classes of SAE applications. Specifically, we\noutline use cases for SAEs in (i) ML interpretability, explainability,\nfairness, auditing, and safety, and (ii) social and health sciences.",
        "url": "http://arxiv.org/abs/2506.23845v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23845v1",
        "arxiv_id": "2506.23845v1",
        "authors": [
            "Kenny Peng",
            "Rajiv Movva",
            "Jon Kleinberg",
            "Emma Pierson",
            "Nikhil Garg"
        ],
        "submitted": "2025-06-30 13:35:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model",
        "abstract": "Large Reasoning Models (LRMs) excel at solving complex problems but face an\noverthinking dilemma. When handling simple tasks, they often produce verbose\nresponses overloaded with thinking tokens (e.g., wait, however). These tokens\ntrigger unnecessary high-level reasoning behaviors like reflection and\nbacktracking, reducing efficiency. In this work, our pilot study reveals that\nthese thinking-token-induced behaviors are not essential for effective\nproblem-solving and may even hinder correct reasoning within constrained token\nbudgets. We identify this phenomenon as the thinking trap. To mitigate this\nissue, we propose Dual Policy Preference Optimization (DuP-PO), a novel\nalgorithm featuring: (1) A rollout sampling strategy that guarantees balanced\nexposure to responses with and without thinking tokens; (2) A fine-grained\nadvantage control technique to dynamically regulate the prediction of target\ntokens; (3) A policy shaping method ensuring stable gradient contributions from\nthinking tokens. Experimental results on five popular math reasoning benchmarks\nshow that DuP-PO performs well on the popular LRM, which significantly improves\ntheir token efficiency during reasoning, while achieving superior performance\nof the base model.",
        "url": "http://arxiv.org/abs/2506.23840v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23840v1",
        "arxiv_id": "2506.23840v1",
        "authors": [
            "Bowen Ding",
            "Yuhan Chen",
            "Futing Wang",
            "Lingfeng Ming",
            "Tao Lin"
        ],
        "submitted": "2025-06-30 13:30:33",
        "source": "arxiv",
        "comment": "13 pages, 5 figures"
    },
    {
        "title": "Towards the \"Digital Me\": A vision of authentic Conversational Agents powered by personal Human Digital Twins",
        "abstract": "Human Digital Twins (HDTs) have traditionally been conceptualized as\ndata-driven models designed to support decision-making across various domains.\nHowever, recent advancements in conversational AI open new possibilities for\nHDTs to function as authentic, interactive digital counterparts of individuals.\nThis paper introduces a novel HDT system architecture that integrates large\nlanguage models with dynamically updated personal data, enabling it to mirror\nan individual's conversational style, memories, and behaviors. To achieve this,\nour approach implements context-aware memory retrieval, neural\nplasticity-inspired consolidation, and adaptive learning mechanisms, creating a\nmore natural and evolving digital persona. The resulting system does not only\nreplicate an individual's unique conversational style depending on who they are\nspeaking with, but also enriches responses with dynamically captured personal\nexperiences, opinions, and memories. While this marks a significant step toward\ndeveloping authentic virtual counterparts, it also raises critical ethical\nconcerns regarding privacy, accountability, and the long-term implications of\npersistent digital identities. This study contributes to the field of HDTs by\ndescribing our novel system architecture, demonstrating its capabilities, and\ndiscussing future directions and emerging challenges to ensure the responsible\nand ethical development of HDTs.",
        "url": "http://arxiv.org/abs/2506.23826v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23826v1",
        "arxiv_id": "2506.23826v1",
        "authors": [
            "Lluís C. Coll",
            "Martin W. Lauer-Schmaltz",
            "Philip Cash",
            "John P. Hansen",
            "Anja Maier"
        ],
        "submitted": "2025-06-30 13:18:31",
        "source": "arxiv",
        "comment": "24 pages, 9 figures"
    },
    {
        "title": "Positional Bias in Binary Question Answering: How Uncertainty Shapes Model Preferences",
        "abstract": "Positional bias in binary question answering occurs when a model\nsystematically favors one choice over another based solely on the ordering of\npresented options. In this study, we quantify and analyze positional bias\nacross five large language models under varying degrees of answer uncertainty.\nWe re-adapted the SQuAD-it dataset by adding an extra incorrect answer option\nand then created multiple versions with progressively less context and more\nout-of-context answers, yielding datasets that range from low to high\nuncertainty. Additionally, we evaluate two naturally higher-uncertainty\nbenchmarks: (1) WebGPT - question pairs with unequal human-assigned quality\nscores, and (2) Winning Arguments - where models predict the more persuasive\nargument in Reddit's r/ChangeMyView exchanges. Across each dataset, the order\nof the \"correct\" (or higher-quality/persuasive) option is systematically\nflipped (first placed in position 1, then in position 2) to compute both\nPreference Fairness and Position Consistency. We observe that positional bias\nis nearly absent under low-uncertainty conditions, but grows exponentially when\nit becomes doubtful to decide which option is correct.",
        "url": "http://arxiv.org/abs/2506.23743v2",
        "pdf_url": "http://arxiv.org/pdf/2506.23743v2",
        "arxiv_id": "2506.23743v2",
        "authors": [
            "Tiziano Labruna",
            "Simone Gallo",
            "Giovanni Da San Martino"
        ],
        "submitted": "2025-06-30 11:30:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data",
        "abstract": "Large language models (LLMs) have shown remarkable performance on various\ntasks, but existing evaluation benchmarks are often static and insufficient to\nfully assess their robustness and generalization in realistic scenarios. Prior\nwork using evolutionary or adversarial data augmentation has improved\nevaluation diversity but lacks systematic control over perturbation types and\nmulti-step complexity, limiting comprehensive robustness analysis. To address\nthese gaps, we propose AutoEvoEval, an evolution-based evaluation framework for\nclose-ended tasks such as multi-choice question answering. AutoEvoEval\nintroduces 22 interpretable atomic evolution operations and supports\nmulti-round compositions, enabling controlled generation of diverse,\nchallenging, and realistic test samples. We conduct extensive experiments\naddressing four research questions on a broad set of open- and closed-source\nLLMs. Our results show that atomic operations cause an average accuracy drop of\n7.283\\%, with structure-disrupting or misleading semantic edits causing the\nlargest declines. Model sensitivities vary significantly for the same\nperturbation, and combining multiple evolution steps amplifies adversarial\neffects by up to 52.932\\%. These findings suggest current benchmarks may\noverestimate true model generalization and emphasize the need for\nevolution-aware robustness evaluation. Code and resources are available at:\nhttps://github.com/SYSUSELab/AutoEvoEval.",
        "url": "http://arxiv.org/abs/2506.23735v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23735v1",
        "arxiv_id": "2506.23735v1",
        "authors": [
            "JiaRu Wu",
            "Mingwei Liu"
        ],
        "submitted": "2025-06-30 11:18:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Towards an Automated Multimodal Approach for Video Summarization: Building a Bridge Between Text, Audio and Facial Cue-Based Summarization",
        "abstract": "The increasing volume of video content in educational, professional, and\nsocial domains necessitates effective summarization techniques that go beyond\ntraditional unimodal approaches. This paper proposes a behaviour-aware\nmultimodal video summarization framework that integrates textual, audio, and\nvisual cues to generate timestamp-aligned summaries. By extracting prosodic\nfeatures, textual cues and visual indicators, the framework identifies\nsemantically and emotionally important moments. A key contribution is the\nidentification of bonus words, which are terms emphasized across multiple\nmodalities and used to improve the semantic relevance and expressive clarity of\nthe summaries. The approach is evaluated against pseudo-ground truth (pGT)\nsummaries generated using LLM-based extractive method. Experimental results\ndemonstrate significant improvements over traditional extractive method, such\nas the Edmundson method, in both text and video-based evaluation metrics.\nText-based metrics show ROUGE-1 increasing from 0.4769 to 0.7929 and BERTScore\nfrom 0.9152 to 0.9536, while in video-based evaluation, our proposed framework\nimproves F1-Score by almost 23%. The findings underscore the potential of\nmultimodal integration in producing comprehensive and behaviourally informed\nvideo summaries.",
        "url": "http://arxiv.org/abs/2506.23714v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23714v1",
        "arxiv_id": "2506.23714v1",
        "authors": [
            "Md Moinul Islam",
            "Sofoklis Kakouros",
            "Janne Heikkilä",
            "Mourad Oussalah"
        ],
        "submitted": "2025-06-30 10:41:33",
        "source": "arxiv",
        "comment": "Accepted to HHAI WS 2025: Workshops at the Fourth International\n  Conference on Hybrid Human-Artificial Intelligence (HHAI)"
    },
    {
        "title": "Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments",
        "abstract": "Benchmarks are important measures to evaluate safety and compliance of AI\nmodels at scale. However, they typically do not offer verifiable results and\nlack confidentiality for model IP and benchmark datasets. We propose Attestable\nAudits, which run inside Trusted Execution Environments and enable users to\nverify interaction with a compliant AI model. Our work protects sensitive data\neven when model provider and auditor do not trust each other. This addresses\nverification challenges raised in recent AI governance frameworks. We build a\nprototype demonstrating feasibility on typical audit benchmarks against\nLlama-3.1.",
        "url": "http://arxiv.org/abs/2506.23706v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23706v1",
        "arxiv_id": "2506.23706v1",
        "authors": [
            "Christoph Schnabl",
            "Daniel Hugenroth",
            "Bill Marino",
            "Alastair R. Beresford"
        ],
        "submitted": "2025-06-30 10:29:42",
        "source": "arxiv",
        "comment": "ICML 2024 Workshop TAIG"
    },
    {
        "title": "Efficient Interleaved Speech Modeling through Knowledge Distillation",
        "abstract": "Current speech language models exceed the size and latency constraints of\nmany deployment environments. We build compact, expressive speech generation\nmodels through layer-aligned distillation, matching hidden states, attention\nmaps, and softened logits to compress large multimodal transformers by 3x with\nminimal loss in performance. We introduce TinyWave, a family of 2B-parameter\nmodels for speech-to-speech and interleaved speech-text generation, trained on\n50,000 hours of public audio. TinyWave supports (i) speech-only generation\nusing phonetic or expressive tokens and (ii) mixed speech-text continuations.\nEvaluation on Libri-Light shows TinyWave within 1.4 normalized perplexity\npoints of its teacher. Accuracy on spoken StoryCloze and SALMon reaches 93-97%\nof the teacher's performance, outperforming size-matched baselines. These\nmodels are optimized for deployment on commodity hardware, enabling\napplications in real-time conversational agents, assistive technologies, and\nlow-resource environments. We release models, training code, and evaluation\nscripts to support reproducible research on compact, expressive speech\ngeneration.",
        "url": "http://arxiv.org/abs/2506.23670v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23670v1",
        "arxiv_id": "2506.23670v1",
        "authors": [
            "Mohammadmahdi Nouriborji",
            "Morteza Rohanian"
        ],
        "submitted": "2025-06-30 09:47:37",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "L0: Reinforcement Learning to Become General Agents",
        "abstract": "Training large language models (LLMs) to act as autonomous agents for\nmulti-turn, long-horizon tasks remains significant challenges in scalability\nand training efficiency. To address this, we introduce L-Zero (L0), a scalable,\nend-to-end training pipeline for general-purpose agents. Featuring a low-cost,\nextensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier\nfor applying reinforcement learning in complex environments. We also introduce\nNB-Agent, the agent scaffold within L0, which operates in a \"code-as-action\"\nfashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality\nquestion-answering benchmarks. Our experiments demonstrate that a base model\ncan develop robust problem-solving skills using solely Reinforcement Learning\nwith Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method\nboosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41\n%. We have open-sourced the entire L0 system, including our L0 series models,\nthe NB-Agent, a complete training pipeline, and the corresponding training\nrecipes on (https://github.com/cmriat/l0).",
        "url": "http://arxiv.org/abs/2506.23667v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23667v1",
        "arxiv_id": "2506.23667v1",
        "authors": [
            "Junjie Zhang",
            "Jingyi Xi",
            "Zhuoyang Song",
            "Junyu Lu",
            "Yuhua Ke",
            "Ting Sun",
            "Yukun Yang",
            "Jiaxing Zhang",
            "Songxin Zhang",
            "Zejian Xie"
        ],
        "submitted": "2025-06-30 09:44:32",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation",
        "abstract": "Context-aware embedding methods boost retrieval accuracy by conditioning on\ncorpus statistics (e.g., term co-occurrence and topical patterns) extracted\nfrom neighboring documents. However, this context-aware approach requires\naccess to the target corpus or requires domain-specific finetuning, posing\npractical barriers in privacy-sensitive or resource-constrained settings. We\npresent ZEST, a zero-shot contextual adaptation framework that replaces real\ncorpus access with a one-time offline synthesis of a compact proxy. Given only\na handful exemplar documents representative of the general target domain, we\nuse a multi-step hierarchical procedure to generate a synthetic context corpus\nof several hundred documents that aims to emulate key domain-specific\ndistributions. At inference, the frozen context-aware encoder uses this proxy\ncorpus -- without any finetuning or target corpus access -- to produce\ndomain-adapted embeddings. Across the MTEB benchmark, ZEST's zero-shot\nsynthetic context adaptation using only five example documents performs within\n0.5% of models leveraging full target corpus access -- demonstrating remarkable\nefficacy without any retraining. ZEST thus provides a practical method for\ndeploying high-performance, adaptable embeddings in constrained environments.",
        "url": "http://arxiv.org/abs/2506.23662v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23662v1",
        "arxiv_id": "2506.23662v1",
        "authors": [
            "Philip Lippmann",
            "Jie Yang"
        ],
        "submitted": "2025-06-30 09:38:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Robustness of Misinformation Classification Systems to Adversarial Examples Through BeamAttack",
        "abstract": "We extend BeamAttack, an adversarial attack algorithm designed to evaluate\nthe robustness of text classification systems through word-level modifications\nguided by beam search. Our extensions include support for word deletions and\nthe option to skip substitutions, enabling the discovery of minimal\nmodifications that alter model predictions. We also integrate LIME to better\nprioritize word replacements. Evaluated across multiple datasets and victim\nmodels (BiLSTM, BERT, and adversarially trained RoBERTa) within the BODEGA\nframework, our approach achieves over a 99\\% attack success rate while\npreserving the semantic and lexical similarity of the original texts. Through\nboth quantitative and qualitative analysis, we highlight BeamAttack's\neffectiveness and its limitations. Our implementation is available at\nhttps://github.com/LucK1Y/BeamAttack",
        "url": "http://arxiv.org/abs/2506.23661v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23661v1",
        "arxiv_id": "2506.23661v1",
        "authors": [
            "Arnisa Fazla",
            "Lucas Krauter",
            "David Guzman Piedrahita",
            "Andrianos Michail"
        ],
        "submitted": "2025-06-30 09:37:19",
        "source": "arxiv",
        "comment": "12 pages main text, 27 pages total including references and\n  appendices. 13 figures, 10 tables. Accepted for publication in the LNCS\n  proceedings of CLEF 2025 (Best-of-Labs track)"
    },
    {
        "title": "Act-With-Think: Chunk Auto-Regressive Modeling for Generative Recommendation",
        "abstract": "Generative recommendation (GR) typically encodes behavioral or semantic\naspects of item information into discrete tokens, leveraging the standard\nautoregressive (AR) generation paradigm to make predictions. However, existing\nmethods tend to overlook their intrinsic relationship, that is, the semantic\nusually provides some reasonable explainability \"$\\textbf{why}$\" for the\nbehavior \"$\\textbf{what}$\", which may constrain the full potential of GR. To\nthis end, we present Chunk AutoRegressive Modeling (CAR), a new generation\nparadigm following the decision pattern that users usually think semantic\naspects of items (e.g. brand) and then take actions on target items (e.g.\npurchase). Our CAR, for the $\\textit{first time}$, incorporates semantics\n(SIDs) and behavior (UID) into a single autoregressive transformer from an\n``act-with-think'' dual perspective via chunk-level autoregression.\nSpecifically, CAR packs SIDs and UID into a conceptual chunk for item unified\nrepresentation, allowing each decoding step to make a holistic prediction.\nExperiments show that our CAR significantly outperforms existing methods based\non traditional AR, improving Recall@5 by 7.93% to 22.30%. Furthermore, we\nverify the scaling effect between model performance and SIDs bit number,\ndemonstrating that CAR preliminary emulates a kind of slow-thinking style\nmechanism akin to the reasoning processes observed in large language models\n(LLMs).",
        "url": "http://arxiv.org/abs/2506.23643v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23643v1",
        "arxiv_id": "2506.23643v1",
        "authors": [
            "Yifan Wang",
            "Weinan Gan",
            "Longtao Xiao",
            "Jieming Zhu",
            "Heng Chang",
            "Haozhao Wang",
            "Rui Zhang",
            "Zhenhua Dong",
            "Ruiming Tang",
            "Ruixuan Li"
        ],
        "submitted": "2025-06-30 09:13:54",
        "source": "arxiv",
        "comment": "9 pages, 2 figures"
    },
    {
        "title": "Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs",
        "abstract": "Large language models (LLMs) make it possible to generate synthetic\nbehavioural data at scale, offering an ethical and low-cost alternative to\nhuman experiments. Whether such data can faithfully capture psychological\ndifferences driven by personality traits, however, remains an open question. We\nevaluate the capacity of LLM agents, conditioned on Big-Five profiles, to\nreproduce personality-based variation in susceptibility to misinformation,\nfocusing on news discernment, the ability to judge true headlines as true and\nfalse headlines as false. Leveraging published datasets in which human\nparticipants with known personality profiles rated headline accuracy, we create\nmatching LLM agents and compare their responses to the original human patterns.\nCertain trait-misinformation associations, notably those involving\nAgreeableness and Conscientiousness, are reliably replicated, whereas others\ndiverge, revealing systematic biases in how LLMs internalize and express\npersonality. The results underscore both the promise and the limits of\npersonality-aligned LLMs for behavioral simulation, and offer new insight into\nmodeling cognitive diversity in artificial agents.",
        "url": "http://arxiv.org/abs/2506.23610v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23610v1",
        "arxiv_id": "2506.23610v1",
        "authors": [
            "Manuel Pratelli",
            "Marinella Petrocchi"
        ],
        "submitted": "2025-06-30 08:16:07",
        "source": "arxiv",
        "comment": "pre-print version - paper actually under submission"
    },
    {
        "title": "Semantic-guided Diverse Decoding for Large Language Model",
        "abstract": "Diverse decoding of large language models is crucial for applications\nrequiring multiple semantically distinct responses, yet existing methods\nprimarily achieve lexical rather than semantic diversity. This limitation\nsignificantly constrains Best-of-N strategies, group-based reinforcement\nlearning, and data synthesis. While temperature sampling and diverse beam\nsearch modify token distributions or apply n-gram penalties, they fail to\nensure meaningful semantic differentiation. We introduce Semantic-guided\nDiverse Decoding (SemDiD), operating directly in embedding space that balances\nquality with diversity through three complementary mechanisms: orthogonal\ndirectional guidance, dynamic inter-group repulsion, and position-debiased\nprobability assessment. SemDiD harmonizes these competing objectives using\nadaptive gain functions and constraint optimization, ensuring both quality\nthresholds and maximal semantic differentiation. Experiments show SemDiD\nconsistently outperforms existing methods, improving Best-of-N coverage by\n1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15%\nwhile increasing accuracy by up to 2.1%.",
        "url": "http://arxiv.org/abs/2506.23601v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23601v1",
        "arxiv_id": "2506.23601v1",
        "authors": [
            "Weijie Shi",
            "Yue Cui",
            "Yaguang Wu",
            "Jingzhi Fang",
            "Shibo Zhang",
            "Mengze Li",
            "Sirui Han",
            "Jia Zhu",
            "Jiajie Xu",
            "Xiaofang Zhou"
        ],
        "submitted": "2025-06-30 08:06:49",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Reachability in symmetric VASS",
        "abstract": "We investigate the reachability problem in symmetric vector addition systems\nwith states (VASS), where transitions are invariant under a group of\npermutations of coordinates. One extremal case, the trivial groups, yields\ngeneral VASS. In another extremal case, the symmetric groups, we show that the\nreachability problem can be solved in PSPACE, regardless of the dimension of\ninput VASS (to be contrasted with Ackermannian complexity in general VASS). We\nalso consider other groups, in particular alternating and cyclic ones.\nFurthermore, motivated by the open status of the reachability problem in data\nVASS, we estimate the gain in complexity when the group arises as a combination\nof the trivial and symmetric groups.",
        "url": "http://arxiv.org/abs/2506.23578v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23578v1",
        "arxiv_id": "2506.23578v1",
        "authors": [
            "Łukasz Kamiński",
            "Sławomir Lasota"
        ],
        "submitted": "2025-06-30 07:33:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI",
        "abstract": "Reasoning plays a crucial role in advancing Multimodal Large Language Models\n(MLLMs) toward Artificial General Intelligence. However, existing MLLM\nbenchmarks often fall short in precisely and comprehensively evaluating\nlong-chain reasoning abilities from three key aspects: (1) lack of difficulty\nand diversity, (2) susceptibility to guessability and memorization, (3)\ninadequate assessment of intermediate reasoning steps. To fill this gap, we\nintroduce MMReason, a new benchmark designed to precisely and comprehensively\nevaluate MLLM long-chain reasoning capability with diverse, open-ended,\nchallenging questions. First, we curate challenging questions requiring\nmulti-step reasoning from various fields (i.e., 6 disciplines) and multiple\ndifficulty levels (i.e., from pre-university to university, and from\nfoundational to competition tiers). Second, these questions are reformulated\ninto an open-ended format and filtered using a multi-model voting technique to\neliminate shortcut cases related to guessing and memorization, ensuring robust\nreasoning evaluations. Third, we annotate the questions with detailed\nstep-by-step solutions, and design a reference-based ternary scoring mechanism\nto reliably assess intermediate reasoning steps. With MMReason, we benchmark\npopular leading MLLMs and provide an in-depth analysis of their reasoning\ncapabilities. We hope MMReason will serve as a valuable resource for advancing\nMLLM reasoning research. Code will be available at\nhttps://github.com/HJYao00/MMReason.",
        "url": "http://arxiv.org/abs/2506.23563v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23563v1",
        "arxiv_id": "2506.23563v1",
        "authors": [
            "Huanjin Yao",
            "Jiaxing Huang",
            "Yawen Qiu",
            "Michael K. Chen",
            "Wenzheng Liu",
            "Wei Zhang",
            "Wenjie Zeng",
            "Xikun Zhang",
            "Jingyi Zhang",
            "Yuxin Song",
            "Wenhao Wu",
            "Dacheng Tao"
        ],
        "submitted": "2025-06-30 07:14:38",
        "source": "arxiv",
        "comment": "Technical report"
    },
    {
        "title": "On Recipe Memorization and Creativity in Large Language Models: Is Your Model a Creative Cook, a Bad Cook, or Merely a Plagiator?",
        "abstract": "This work-in-progress investigates the memorization, creativity, and nonsense\nfound in cooking recipes generated from Large Language Models (LLMs).\nPrecisely, we aim (i) to analyze memorization, creativity, and non-sense in\nLLMs using a small, high-quality set of human judgments and (ii) to evaluate\npotential approaches to automate such a human annotation in order to scale our\nstudy to hundreds of recipes. To achieve (i), we conduct a detailed human\nannotation on 20 preselected recipes generated by LLM (Mixtral), extracting\neach recipe's ingredients and step-by-step actions to assess which elements are\nmemorized--i.e., directly traceable to online sources possibly seen during\ntraining--and which arise from genuine creative synthesis or outright nonsense.\nWe find that Mixtral consistently reuses ingredients that can be found in\nonline documents, potentially seen during model training, suggesting strong\nreliance on memorized content. To achieve aim (ii) and scale our analysis\nbeyond small sample sizes and single LLM validation, we design an\n``LLM-as-judge'' pipeline that automates recipe generation, nonsense detection,\nparsing ingredients and recipe steps, and their annotation. For instance,\ncomparing its output against human annotations, the best ingredient extractor\nand annotator is Llama 3.1+Gemma 2 9B, achieving up to 78% accuracy on\ningredient matching. This automated framework enables large-scale\nquantification of memorization, creativity, and nonsense in generated recipes,\nproviding rigorous evidence of the models' creative capacities.",
        "url": "http://arxiv.org/abs/2506.23527v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23527v1",
        "arxiv_id": "2506.23527v1",
        "authors": [
            "Jan Kvapil",
            "Martin Fajcik"
        ],
        "submitted": "2025-06-30 05:27:11",
        "source": "arxiv",
        "comment": "13 pages, 5 figures"
    },
    {
        "title": "NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning",
        "abstract": "In the field of education, understanding students' opinions through their\ncomments is crucial, especially in the Vietnamese language, where resources\nremain limited. Existing educational datasets often lack domain relevance and\nstudent slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese\ndataset for Educational Sentiment Classification and Topic Classification,\ncurated from university forums, which offers more samples, richer class\ndiversity, longer texts, and broader vocabulary. In addition, we explore\nmultitask learning using encoder-only language models (BERT), in which we\nshowed that it achieves performance up to 83.7% and 79.8% accuracy for\nsentiment and topic classification tasks. We also benchmark our dataset and\nmodel with other datasets and models, including Large Language Models, and\ndiscuss these benchmarks. The dataset is publicly available at:\nhttps://huggingface.co/datasets/hung20gg/NEU-ESC.",
        "url": "http://arxiv.org/abs/2506.23524v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23524v1",
        "arxiv_id": "2506.23524v1",
        "authors": [
            "Phan Quoc Hung Mai",
            "Quang Hung Nguyen",
            "Phuong Giang Duong",
            "Hong Hanh Nguyen",
            "Nguyen Tuan Long"
        ],
        "submitted": "2025-06-30 05:19:04",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays",
        "abstract": "As the use of AI tools by students has become more prevalent, instructors\nhave started using AI detection tools like GPTZero and QuillBot to detect AI\nwritten text. However, the reliability of these detectors remains uncertain. In\nour study, we focused mostly on the success rate of GPTZero, the most-used AI\ndetector, in identifying AI-generated texts based on different lengths of\nrandomly submitted essays: short (40-100 word count), medium (100-350 word\ncount), and long (350-800 word count). We gathered a data set consisting of\ntwenty-eight AI-generated papers and fifty human-written papers. With this\nrandomized essay data, papers were individually plugged into GPTZero and\nmeasured for percentage of AI generation and confidence. A vast majority of the\nAI-generated papers were detected accurately (ranging from 91-100% AI believed\ngeneration), while the human generated essays fluctuated; there were a handful\nof false positives. These findings suggest that although GPTZero is effective\nat detecting purely AI-generated content, its reliability in distinguishing\nhuman-authored texts is limited. Educators should therefore exercise caution\nwhen relying solely on AI detection tools.",
        "url": "http://arxiv.org/abs/2506.23517v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23517v1",
        "arxiv_id": "2506.23517v1",
        "authors": [
            "Selin Dik",
            "Osman Erdem",
            "Mehmet Dik"
        ],
        "submitted": "2025-06-30 04:53:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably",
        "abstract": "Post-training algorithms such as Supervised Fine-Tuning (SFT) and\nReinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large\nlanguage models to downstream tasks. While effective at task adaptation, their\nimpact on prior knowledge remains unclear. In this paper, we introduce jigsaw\npuzzles as a novel task absent from existing pretraining corpora and\nsystematically study the behavior of SFT and RFT on an open-source multimodal\nmodel, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid\ntask acquisition but leads to catastrophic forgetting, whereas RFT learns more\nslowly on novel tasks but maintains prior knowledge. We analyze this phenomenon\nthrough the lens of learning dynamics, showing that RFT reinforces correct\nsamples that are naturally aligned with the base model's probability landscape,\nmitigating interference with prior knowledge. Moreover, supervised training on\ncorrect RFT-simulated rollouts allows SFT to preserve knowledge while rapidly\nlearning new tasks. These findings suggest that data distribution, rather than\nalgorithmic differences, plays a central role in forgetting, and highlight\nRFT's potential for stable continual learning in multimodal large language\nmodels.",
        "url": "http://arxiv.org/abs/2506.23508v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23508v1",
        "arxiv_id": "2506.23508v1",
        "authors": [
            "Zhihao Zhang",
            "Qiaole Dong",
            "Qi Zhang",
            "Jun Zhao",
            "Enyu Zhou",
            "Zhiheng Xi",
            "Senjie Jin",
            "Xiaoran Fan",
            "Yuhao Zhou",
            "Yanwei Fu",
            "Tao Ji",
            "Tao Gui",
            "Xuanjing Huang"
        ],
        "submitted": "2025-06-30 04:15:01",
        "source": "arxiv",
        "comment": "18 pages (Preprint. Work in progress)"
    },
    {
        "title": "Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent",
        "abstract": "Interactive recommendation is a typical information-seeking task that allows\nusers to interactively express their needs through natural language and obtain\npersonalized recommendations. Large language model-powered (LLM-powered) agents\nhave become a new paradigm in interactive recommendations, effectively\ncapturing users' real-time needs and enhancing personalized experiences.\nHowever, due to limited planning and generalization capabilities, existing\nformulations of LLM-powered interactive recommender agents struggle to\neffectively address diverse and complex user intents, such as intuitive,\nunrefined, or occasionally ambiguous requests. To tackle this challenge, we\npropose a novel thought-augmented interactive recommender agent system (TAIRA)\nthat addresses complex user intents through distilled thought patterns.\nSpecifically, TAIRA is designed as an LLM-powered multi-agent system featuring\na manager agent that orchestrates recommendation tasks by decomposing user\nneeds and planning subtasks, with its planning capacity strengthened through\nThought Pattern Distillation (TPD), a thought-augmentation method that extracts\nhigh-level thoughts from the agent's and human experts' experiences. Moreover,\nwe designed a set of user simulation schemes to generate personalized queries\nof different difficulties and evaluate the recommendations based on specific\ndatasets. Through comprehensive experiments conducted across multiple datasets,\nTAIRA exhibits significantly enhanced performance compared to existing methods.\nNotably, TAIRA shows a greater advantage on more challenging tasks while\ngeneralizing effectively on novel tasks, further validating its superiority in\nmanaging complex user intents within interactive recommendation systems. The\ncode is publicly available at:https://github.com/Alcein/TAIRA.",
        "url": "http://arxiv.org/abs/2506.23485v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23485v1",
        "arxiv_id": "2506.23485v1",
        "authors": [
            "Haocheng Yu",
            "Yaxiong Wu",
            "Hao Wang",
            "Wei Guo",
            "Yong Liu",
            "Yawen Li",
            "Yuyang Ye",
            "Junping Du",
            "Enhong Chen"
        ],
        "submitted": "2025-06-30 03:15:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "KiseKloset: Comprehensive System For Outfit Retrieval, Recommendation, And Try-On",
        "abstract": "The global fashion e-commerce industry has become integral to people's daily\nlives, leveraging technological advancements to offer personalized shopping\nexperiences, primarily through recommendation systems that enhance customer\nengagement through personalized suggestions. To improve customers' experience\nin online shopping, we propose a novel comprehensive KiseKloset system for\noutfit retrieval, recommendation, and try-on. We explore two approaches for\noutfit retrieval: similar item retrieval and text feedback-guided item\nretrieval. Notably, we introduce a novel transformer architecture designed to\nrecommend complementary items from diverse categories. Furthermore, we enhance\nthe overall performance of the search pipeline by integrating approximate\nalgorithms to optimize the search process. Additionally, addressing the crucial\nneeds of online shoppers, we employ a lightweight yet efficient virtual try-on\nframework capable of real-time operation, memory efficiency, and maintaining\nrealistic outputs compared to its predecessors. This virtual try-on module\nempowers users to visualize specific garments on themselves, enhancing the\ncustomers' experience and reducing costs associated with damaged items for\nretailers. We deployed our end-to-end system for online users to test and\nprovide feedback, enabling us to measure their satisfaction levels. The results\nof our user study revealed that 84% of participants found our comprehensive\nsystem highly useful, significantly improving their online shopping experience.",
        "url": "http://arxiv.org/abs/2506.23471v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23471v1",
        "arxiv_id": "2506.23471v1",
        "authors": [
            "Thanh-Tung Phan-Nguyen",
            "Khoi-Nguyen Nguyen-Ngoc",
            "Tam V. Nguyen",
            "Minh-Triet Tran",
            "Trung-Nghia Le"
        ],
        "submitted": "2025-06-30 02:25:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "What to Keep and What to Drop: Adaptive Table Filtering Framework",
        "abstract": "Large language models (LLMs) for table-based reasoning often struggle with\nlarge tables due to input length limits. We propose ATF (Adaptive Table\nFiltering Framework), a modular and question-aware filtering pipeline that\nprunes uninformative columns and rows using LLM-generated column descriptions,\nclustering, and sparse-dense alignment scores. ATF integrates seamlessly with\nexisting models (e.g., TAPAS, TAPEX) without retraining. Experiments show that\nATF reduces table cells by ~70\\%, boosting performance on out-of-domain TableQA\ntasks while causing slight performance drops on Table Fact Verification, where\nfull-table context is more critical. These results highlight ATF's ability to\nadaptively balance informativeness and minimalism across tasks.",
        "url": "http://arxiv.org/abs/2506.23463v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23463v1",
        "arxiv_id": "2506.23463v1",
        "authors": [
            "Jang Won June"
        ],
        "submitted": "2025-06-30 02:03:23",
        "source": "arxiv",
        "comment": "26 pages, 9 figures"
    },
    {
        "title": "Pipelined Decoder for Efficient Context-Aware Text Generation",
        "abstract": "As the basis of generative AI, an autoregressive model requires the\ngeneration of a new token depending on all the previously generated tokens,\nwhich brings high quality but also restricts the model to generate tokens one\nby one, forming a bottleneck limiting the generation speed. In this paper, we\npropose a new decoder architecture that efficiently generates text in parallel\nfor context-aware generation tasks. Our proposed pipelined decoder initiates\nthe generation of multiple subsequences simultaneously, and, at each time-step,\nit generates a new token for each subsequence to realize parallelism.\nExperiments on multiple text generation tasks, including question answering,\ntext summarization, and keyphrase generation, show that our pipelined decoder\nsignificantly improves the generation speed without a significant loss of\ngeneration quality or additional memory consumption.",
        "url": "http://arxiv.org/abs/2506.23431v2",
        "pdf_url": "http://arxiv.org/pdf/2506.23431v2",
        "arxiv_id": "2506.23431v2",
        "authors": [
            "Zixian Huang",
            "Chenxu Niu",
            "Yu Gu",
            "Gengyang Xiao",
            "Xinwei Huang",
            "Gong Cheng"
        ],
        "submitted": "2025-06-29 23:37:24",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs",
        "abstract": "Past work has studied the effects of fine-tuning on large language models'\n(LLMs) overall performance on certain tasks. However, a quantitative and\nsystematic method for analyzing its effect on individual outputs is still\nlacking. Here, we propose a new method for measuring the contribution that\nfine-tuning makes to individual LLM responses, assuming access to the original\npre-trained model. Our method tracks the model's intermediate hidden states,\nproviding a more fine-grained insight into the effects of fine-tuning than a\nsimple comparison of final outputs from pre-trained and fine-tuned models. We\nintroduce and theoretically analyze an exact decomposition of any fine-tuned\nLLM into a pre-training component and a fine-tuning component. Empirically, we\nfind that model behavior and performance can be steered by up- or down-scaling\nthe fine-tuning component during the forward pass. Motivated by this finding\nand our theoretical analysis, we define the Tuning Contribution (TuCo) as the\nratio of the magnitudes of the fine-tuning component to the pre-training\ncomponent. We observe that three prominent adversarial attacks on LLMs\ncircumvent safety measures in a way that reduces TuCo, and that TuCo is\nconsistently lower on prompts where these attacks succeed compared to those\nwhere they do not. This suggests that attenuating the effect of fine-tuning on\nmodel outputs plays a role in the success of such attacks. In summary, TuCo\nenables the quantitative study of how fine-tuning influences model behavior and\nsafety, and vice versa.",
        "url": "http://arxiv.org/abs/2506.23423v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23423v1",
        "arxiv_id": "2506.23423v1",
        "authors": [
            "Felipe Nuti",
            "Tim Franzmeyer",
            "João Henriques"
        ],
        "submitted": "2025-06-29 23:08:36",
        "source": "arxiv",
        "comment": "ICML 2025"
    },
    {
        "title": "Datasets for Fairness in Language Models: An In-Depth Survey",
        "abstract": "Fairness benchmarks play a central role in shaping how we evaluate language\nmodels, yet surprisingly little attention has been given to examining the\ndatasets that these benchmarks rely on. This survey addresses that gap by\npresenting a broad and careful review of the most widely used fairness datasets\nin current language model research, characterizing them along several key\ndimensions including their origin, scope, content, and intended use to help\nresearchers better appreciate the assumptions and limitations embedded in these\nresources. To support more meaningful comparisons and analyses, we introduce a\nunified evaluation framework that reveals consistent patterns of demographic\ndisparities across datasets and scoring methods. Applying this framework to\ntwenty four common benchmarks, we highlight the often overlooked biases that\ncan influence conclusions about model fairness and offer practical guidance for\nselecting, combining, and interpreting these datasets. We also point to\nopportunities for creating new fairness benchmarks that reflect more diverse\nsocial contexts and encourage more thoughtful use of these tools going forward.\nAll code, data, and detailed results are publicly available at\nhttps://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets\nto promote transparency and reproducibility across the research community.",
        "url": "http://arxiv.org/abs/2506.23411v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23411v1",
        "arxiv_id": "2506.23411v1",
        "authors": [
            "Jiale Zhang",
            "Zichong Wang",
            "Avash Palikhe",
            "Zhipeng Yin",
            "Wenbin Zhang"
        ],
        "submitted": "2025-06-29 22:11:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "NaviX: A Native Vector Index Design for Graph DBMSs With Robust Predicate-Agnostic Search Performance",
        "abstract": "There is an increasing demand for extending existing DBMSs with vector\nindices so that they become unified systems capable of supporting modern\npredictive applications, which require joint querying of vector embeddings\ntogether with the structured properties and connections of objects. We present\nNaviX, a native vector index for graph DBMSs (GDBMSs) that has two main design\ngoals. First, we aim to implement a disk-based vector index that leverages the\ncore storage and query-processing capabilities of the underlying GDBMS. To this\nend, NaviX is built on the Hierarchical Navigable Small-World (HNSW) graph,\nwhich itself is a graph-based structure. Second, we aim to support\npredicate-agnostic filtered vector search queries, in which the k nearest\nneighbors (kNNs) of a query vector vQ are searched only within an arbitrary\nsubset S of vectors defined by an ad-hoc selection sub-query QS. We adopt a\nprefiltering approach that evaluates QS first and passes the full description\nof subset S to the kNN search operator. We study how to design a prefiltering\nsearch algorithm that remains robust under varying selectivities and under\ndifferent correlations between subset S and query vector vQ. We propose an\nadaptive algorithm that uses the local selectivity of each vector in the HNSW\ngraph to choose an appropriate heuristic at every iteration of the kNN search.\nFinally, We demonstrate NaviX's robustness and efficiency through extensive\nexperiments against both existing prefiltering- and postfiltering-based\nbaselines.",
        "url": "http://arxiv.org/abs/2506.23397v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23397v1",
        "arxiv_id": "2506.23397v1",
        "authors": [
            "Gaurav Sehgal",
            "Semih Salihoglu"
        ],
        "submitted": "2025-06-29 21:16:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Teaching a Language Model to Speak the Language of Tools",
        "abstract": "External tool integration through function-calling is essential for practical\nlanguage model applications, yet most multilingual models lack reliable\ntool-use capabilities in non-English languages. Even state-of-the-art\nmultilingual models struggle with determining when to use tools and generating\nthe structured outputs required for function calls, often exhibiting language\nconfusion when prompted in lower-resource languages. This work presents a\nmethodology for adapting existing language models to enable robust tool use in\nany target language, using Bulgarian as a case study. The approach involves\ncontinued training of the BgGPT model series (2.6B, 9B, 27B parameters) on a\nnovel bilingual dataset of 10,035 function-calling examples designed to support\nstandardized protocols like MCP (Model Context Protocol). The research\nintroduces TUCAN (Tool-Using Capable Assistant Navigator), which achieves up to\n28.75% improvement in function-calling accuracy over base models while\npreserving core language understanding, as verified on established Bulgarian\nbenchmarks. Beyond accuracy gains, TUCAN models demonstrate production-ready\nresponse formatting with clean, parsable function calls, contrasting with the\nverbose and inconsistent outputs of base models. The models, evaluation\nframework, and dataset are released to enable replication for other languages.\nThis work demonstrates a practical approach for extending tool-augmented\ncapabilities beyond English-centric systems.",
        "url": "http://arxiv.org/abs/2506.23394v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23394v1",
        "arxiv_id": "2506.23394v1",
        "authors": [
            "Simeon Emanuilov"
        ],
        "submitted": "2025-06-29 20:47:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Hierarchical Memory Organization for Wikipedia Generation",
        "abstract": "Generating Wikipedia articles autonomously is a challenging task requiring\nthe integration of accurate, comprehensive, and well-structured information\nfrom diverse sources. This paper introduces the Memory Organization-based\nGeneration (MOG) framework, a novel approach to address these challenges by\nleveraging a hierarchical memory architecture. MOG extracts fine-grained memory\nunits from web documents, recursively organizes them into a Wikipedia-style\nhierarchical structure, and uses this structure to guide the generation\nprocess. This ensures alignment between memory and the article outline,\nimproving both informativeness and verifiability while minimizing\nhallucinations. Additionally, a citation module is implemented to enhance\ntraceability by linking every generated sentence to specific memory units.\nEvaluations on our newly created WikiStart dataset demonstrate that MOG\noutperforms baseline methods in producing informative and reliable articles,\nmaking it particularly robust in real-world scenarios.",
        "url": "http://arxiv.org/abs/2506.23393v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23393v1",
        "arxiv_id": "2506.23393v1",
        "authors": [
            "Eugene J. Yu",
            "Dawei Zhu",
            "Yifan Song",
            "Xiangyu Wong",
            "Jiebin Zhang",
            "Wenxuan Shi",
            "Xiaoguang Li",
            "Qun Liu",
            "Sujian Li"
        ],
        "submitted": "2025-06-29 20:22:49",
        "source": "arxiv",
        "comment": "ACL 2025 Main Conference"
    },
    {
        "title": "Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs",
        "abstract": "Large language models (LLMs) are used in a variety of mission-critical roles.\nDue to the rapidly developing nature of LLMs, there is a lack of quantifiable\nunderstanding of the bias and perspective associated with LLM output. Inspired\nby this need, this paper considers the broader issue of perspective or\nviewpoint of general text and perspective control of large-language model (LLM)\noutput. Perspective-Dial consists of two main components: a (1) metric space,\ndubbed Perspective Space, that enables quantitative measurements of different\nperspectives regarding a topic, and the use of (2) Systematic Prompt\nEngineering that utilizes greedy-coordinate descent to control LLM output\nperspective based on measurement feedback from the Perspective Space. The\nempirical nature of the approach allows progress to side step a principled\nunderstanding of perspective or bias -- effectively quantifying and adjusting\noutputs for a variety of topics. Potential applications include detection,\ntracking and mitigation of LLM bias, narrative detection, sense making and\ntracking in public discourse, and debate bot advocating given perspective.",
        "url": "http://arxiv.org/abs/2506.23377v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23377v1",
        "arxiv_id": "2506.23377v1",
        "authors": [
            "Taejin Kim",
            "Siun-Chuon Mau",
            "Konrad Vesey"
        ],
        "submitted": "2025-06-29 19:26:37",
        "source": "arxiv",
        "comment": "7 pages, 5 main pages of text, 5 figures, 2 tables. Research work\n  performed at CACI INTL INC"
    },
    {
        "title": "You Sound a Little Tense: L2 Tailored Clear TTS Using Durational Vowel Properties",
        "abstract": "We present the first text-to-speech (TTS) system tailored to second language\n(L2) speakers. We use duration differences between American English tense\n(longer) and lax (shorter) vowels to create a \"clarity mode\" for Matcha-TTS.\nOur perception studies showed that French-L1, English-L2 listeners had fewer\n(at least 9.15%) transcription errors when using our clarity mode, and found it\nmore encouraging and respectful than overall slowed down speech. Remarkably,\nlisteners were not aware of these effects: despite the decreased word error\nrate in clarity mode, listeners still believed that slowing all target words\nwas the most intelligible, suggesting that actual intelligibility does not\ncorrelate with perceived intelligibility. Additionally, we found that\nWhisper-ASR did not use the same cues as L2 speakers to differentiate difficult\nvowels and is not sufficient to assess the intelligibility of TTS systems for\nthese individuals.",
        "url": "http://arxiv.org/abs/2506.23367v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23367v1",
        "arxiv_id": "2506.23367v1",
        "authors": [
            "Paige Tuttösí",
            "H. Henny Yeung",
            "Yue Wang",
            "Jean-Julien Aucouturier",
            "Angelica Lim"
        ],
        "submitted": "2025-06-29 18:55:05",
        "source": "arxiv",
        "comment": "Accepted to ISCA Speech Synthesis Workshop, 2025"
    },
    {
        "title": "Density, asymmetry and citation dynamics in scientific literature",
        "abstract": "Scientific behavior is often characterized by a tension between building upon\nestablished knowledge and introducing novel ideas. Here, we investigate whether\nthis tension is reflected in the relationship between the similarity of a\nscientific paper to previous research and its eventual citation rate. To\noperationalize similarity to previous research, we introduce two complementary\nmetrics to characterize the local geometry of a publication's semantic\nneighborhood: (1) \\emph{density} ($\\rho$), defined as the ratio between a fixed\nnumber of previously-published papers and the minimum distance enclosing those\npapers in a semantic embedding space, and (2) asymmetry ($\\alpha$), defined as\nthe average directional difference between a paper and its nearest neighbors.\nWe tested the predictive relationship between these two metrics and its\nsubsequent citation rate using a Bayesian hierarchical regression approach,\nsurveying $\\sim 53,000$ publications across nine academic disciplines and five\ndifferent document embeddings. While the individual effects of $\\rho$ on\ncitation count are small and variable, incorporating density-based predictors\nconsistently improves out-of-sample prediction when added to baseline models.\nThese results suggest that the density of a paper's surrounding scientific\nliterature may carry modest but informative signals about its eventual impact.\nMeanwhile, we find no evidence that publication asymmetry improves model\npredictions of citation rates. Our work provides a scalable framework for\nlinking document embeddings to scientometric outcomes and highlights new\nquestions regarding the role that semantic similarity plays in shaping the\ndynamics of scientific reward.",
        "url": "http://arxiv.org/abs/2506.23366v1",
        "pdf_url": "http://arxiv.org/pdf/2506.23366v1",
        "arxiv_id": "2506.23366v1",
        "authors": [
            "Nathaniel Imel",
            "Zachary Hafen"
        ],
        "submitted": "2025-06-29 18:55:04",
        "source": "arxiv",
        "comment": null
    }
]