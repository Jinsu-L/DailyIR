[
    {
        "title": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search",
        "abstract": "Multimodal Large Language Models (MLLMs) in real-world applications require\naccess to external knowledge sources and must remain responsive to the dynamic\nand ever-changing real-world information in order to address\ninformation-seeking and knowledge-intensive user queries. Existing approaches,\nsuch as retrieval augmented generation (RAG) methods, search agents, and search\nequipped MLLMs, often suffer from rigid pipelines, excessive search calls, and\npoorly constructed search queries, which result in inefficiencies and\nsuboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,\nthe first multimodal LLM capable of performing on-demand, multi-turn web\nsearches and dynamically crafting queries for both image and text search tools.\nSpecifically, DeepMMSearch-R1 can initiate web searches based on relevant crops\nof the input image making the image search more effective, and can iteratively\nadapt text search queries based on retrieved information, thereby enabling\nself-reflection and self-correction. Our approach relies on a two-stage\ntraining pipeline: a cold start supervised finetuning phase followed by an\nonline reinforcement learning optimization. For training, we introduce\nDeepMMSearchVQA, a novel multimodal VQA dataset created through an automated\npipeline intermixed with real-world information from web search tools. This\ndataset contains diverse, multi-hop queries that integrate textual and visual\ninformation, teaching the model when to search, what to search for, which\nsearch tool to use and how to reason over the retrieved information. We conduct\nextensive experiments across a range of knowledge-intensive benchmarks to\ndemonstrate the superiority of our approach. Finally, we analyze the results\nand provide insights that are valuable for advancing multimodal web-search.",
        "url": "http://arxiv.org/abs/2510.12801v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12801v1",
        "arxiv_id": "2510.12801v1",
        "authors": [
            "Kartik Narayan",
            "Yang Xu",
            "Tian Cao",
            "Kavya Nerella",
            "Vishal M. Patel",
            "Navid Shiee",
            "Peter Grasch",
            "Chao Jia",
            "Yinfei Yang",
            "Zhe Gan"
        ],
        "submitted": "2025-10-14 17:59:58",
        "source": "arxiv",
        "comment": null,
        "score": 11,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'web search' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper aligns well with your research interests in Information Retrieval, particularly in query understanding and ranking models. The focus on multimodal web search, query crafting, and real-time relevance optimization also resonates with your background in e-commerce and NLP."
    },
    {
        "title": "SMILE: SeMantic Ids Enhanced CoLd Item Representation for Click-through Rate Prediction in E-commerce SEarch",
        "abstract": "With the rise of modern search and recommendation platforms, insufficient\ncollaborative information of cold-start items exacerbates the Matthew effect of\nexisting platform items, challenging platform diversity and becoming a\nlongstanding issue. Existing methods align items' side content with\ncollaborative information to transfer collaborative signals from\nhigh-popularity items to cold-start items. However, these methods fail to\naccount for the asymmetry between collaboration and content, nor the\nfine-grained differences among items. To address these issues, we propose\nSMILE, an item representation enhancement approach based on fused alignment of\nsemantic IDs. Specifically, we use RQ-OPQ encoding to quantize item content and\ncollaborative information, followed by a two-step alignment: RQ encoding\ntransfers shared collaborative signals across items, while OPQ encoding learns\ndifferentiated information of items. Comprehensive offline experiments on\nlarge-scale industrial datasets demonstrate superiority of SMILE, and rigorous\nonline A/B tests confirm statistically significant improvements: item CTR\n+1.66%, buyers +1.57%, and order volume +2.17%.",
        "url": "http://arxiv.org/abs/2510.12604v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12604v1",
        "arxiv_id": "2510.12604v1",
        "authors": [
            "Qihang Zhao",
            "Zhongbo Sun",
            "Xiaoyang Zheng",
            "Xian Guo",
            "Siyuan Wang",
            "Zihan Liang",
            "Mingcan Peng",
            "Ben Chen",
            "Chenyi Lei"
        ],
        "submitted": "2025-10-14 14:58:50",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'click' (score: +2)",
            "Found 'ctr' (score: +2)",
            "Found 'click-through rate' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 7,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of e-commerce search and click-through rate prediction. However, it focuses more on item representation and collaborative information transfer rather than query understanding, ranking models, or user behavior modeling, which are your core areas of interest."
    },
    {
        "title": "Reinforced Preference Optimization for Recommendation",
        "abstract": "Recent breakthroughs in large language models (LLMs) have fundamentally\nshifted recommender systems from discriminative to generative paradigms, where\nuser behavior modeling is achieved by generating target items conditioned on\nhistorical interactions. Yet current generative recommenders still suffer from\ntwo core limitations: the lack of high-quality negative modeling and the\nreliance on implicit rewards. Reinforcement learning with verifiable rewards\n(RLVR) offers a natural solution by enabling on-policy sampling of harder\nnegatives and grounding optimization in explicit reward signals. However,\napplying RLVR to generative recommenders remains non-trivial. Its unique\ngeneration space often leads to invalid or repetitive items that undermine\nsampling efficiency, and ranking supervision is sparse since most items receive\nidentical zero rewards. To address these challenges, we propose Reinforced\nPreference Optimization for Recommendation (ReRe), a reinforcement-based\nparadigm tailored to LLM-based recommenders, an important direction in\ngenerative recommendation. ReRe incorporates constrained beam search to improve\nsampling efficiency and diversify hard negatives, while augmenting rule-based\naccuracy rewards with auxiliary ranking rewards for finer-grained supervision.\nExtensive experiments on three real-world datasets demonstrate that ReRe\nconsistently outperforms both traditional and LLM-based recommenders in ranking\nperformance. Further analysis shows that ReRe not only enhances performance\nacross both base and SFT-initialized models but also generalizes robustly\nacross different backbone families and scales. Beyond empirical gains, we\nsystematically investigate the design space of RLVR in recommendation across\ngeneration, sampling strategy, reward modeling, and optimization algorithm,\noffering insights for future research.",
        "url": "http://arxiv.org/abs/2510.12211v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12211v1",
        "arxiv_id": "2510.12211v1",
        "authors": [
            "Junfei Tan",
            "Yuxin Chen",
            "An Zhang",
            "Junguang Jiang",
            "Bin Liu",
            "Ziru Xu",
            "Han Zhu",
            "Jian Xu",
            "Bo Zheng",
            "Xiang Wang"
        ],
        "submitted": "2025-10-14 07:04:33",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'user behavior' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper focuses on recommender systems, which is a related area to information retrieval, but it does not directly address query understanding, ranking models, or user behavior modeling. The use of reinforcement learning and large language models is relevant to some of your interests in NLP and data mining, but the specific application to recommender systems is not a central match to your research themes."
    },
    {
        "title": "Embedding the Teacher: Distilling vLLM Preferences for Scalable Image Retrieval",
        "abstract": "Text--image retrieval is necessary for applications such as product\nrecommendation. Embedding-based approaches like CLIP enable efficient\nlarge-scale retrieval via vector similarity search, but they are primarily\ntrained on literal caption-like text--image pairs and often fail to capture\nabstract or persona-driven attributes common in product recommendation\napplications (e.g., ``a gift for a mother who loves gardening''). In contrast,\nstate-of-the-art vision--language models (vLLMs) can align text with images in\na flexible manner, but their limited context window prevents them from directly\nhandling retrieval over large catalogs. We propose a framework that distills\nthe preference rankings of a powerful vLLM into an embedding-based system,\ntransferring its nuanced alignment abilities while maintaining the\ninference-time scalability of an embedding-based approach. Experiments on\npersona-driven product recommendation tasks demonstrate that our method\nsignificantly outperforms existing embedding-based baselines, providing an\nefficient solution for personalized text--image retrieval.",
        "url": "http://arxiv.org/abs/2510.12014v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12014v1",
        "arxiv_id": "2510.12014v1",
        "authors": [
            "Eric He",
            "Akash Gupta",
            "Adian Liusie",
            "Vatsal Raina",
            "Piotr Molenda",
            "Shirom Chabra",
            "Vyas Raina"
        ],
        "submitted": "2025-10-13 23:30:07",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 7,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of image retrieval and query understanding. Although it focuses on a specific application (product recommendation) and uses a different approach (distilling vLLM preferences), it explores the intersection of NLP and IR, which aligns with your interests."
    },
    {
        "title": "Simple Projection Variants Improve ColBERT Performance",
        "abstract": "Multi-vector dense retrieval methods like ColBERT systematically use a\nsingle-layer linear projection to reduce the dimensionality of individual\nvectors. In this study, we explore the implications of the MaxSim operator on\nthe gradient flows of the training of multi-vector models and show that such a\nsimple linear projection has inherent, if non-critical, limitations in this\nsetting. We then discuss the theoretical improvements that could result from\nreplacing this single-layer projection with well-studied alternative\nfeedforward linear networks (FFN), such as deeper, non-linear FFN blocks, GLU\nblocks, and skip-connections, could alleviate these limitations. Through the\ndesign and systematic evaluation of alternate projection blocks, we show that\nbetter-designed final projections positively impact the downstream performance\nof ColBERT models. We highlight that many projection variants outperform the\noriginal linear projections, with the best-performing variants increasing\naverage performance on a range of retrieval benchmarks across domains by over 2\nNDCG@10 points. We then conduct further exploration on the individual\nparameters of these projections block in order to understand what drives this\nempirical performance, highlighting the particular importance of upscaled\nintermediate projections and residual connections. As part of these ablation\nstudies, we show that numerous suboptimal projection variants still outperform\nthe traditional single-layer projection across multiple benchmarks, confirming\nour hypothesis. Finally, we observe that this effect is consistent across\nrandom seeds, further confirming that replacing the linear layer of ColBERT\nmodels is a robust, drop-in upgrade.",
        "url": "http://arxiv.org/abs/2510.12327v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12327v1",
        "arxiv_id": "2510.12327v1",
        "authors": [
            "Benjamin ClaviÃ©",
            "Sean Lee",
            "Rikiya Takehi",
            "Aamir Shakir",
            "Makoto P. Kato"
        ],
        "submitted": "2025-10-14 09:34:05",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'dense retrieval' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to your research interests in Information Retrieval, specifically in the context of ColBERT, a dense retrieval method. The study explores alternative projection variants to improve ColBERT's performance, which aligns with your focus on query understanding and ranking models. While the paper's domain is not explicitly e-commerce, the techniques and findings are applicable to various retrieval benchmarks, making it a useful contribution to the field."
    },
    {
        "title": "Chinese ModernBERT with Whole-Word Masking",
        "abstract": "Encoder-only Transformers have advanced along three axes -- architecture,\ndata, and systems -- yielding Pareto gains in accuracy, speed, and memory\nefficiency. Yet these improvements have not fully transferred to Chinese, where\ntokenization and morphology differ markedly from English. We introduce Chinese\nModernBERT, a from-scratch Chinese encoder that couples: (i) a hardware-aware\n32k BPE vocabulary tailored to frequent Chinese affixes/compounds, lowering the\nembedding budget; (ii) whole-word masking (WWM) with a dynamic masking\ncurriculum (30% -> 15%) to align task difficulty with training progress; (iii)\na two-stage pre-training pipeline that extends the native context from 1,024 to\n8,192 tokens using RoPE and alternating local/global attention; and (iv) a\ndamped-cosine learning-rate schedule for stable long-horizon optimization. We\npre-train on ~1.2T Chinese tokens from CCI3-HQ, CCI4 (Chinese), and\nCosmopedia-Chinese. On CLUE, Chinese ModernBERT is competitive with strong\nChinese encoders under a unified fine-tuning protocol. Under bf16 it achieves\nhigh long-sequence throughput while maintaining strong short-sequence speed,\nreflecting benefits from budget allocation and attention design. To probe\nretrieval-oriented quality, we add a small amount of open contrastive data:\nfine-tuning on SimCLUE (~3M pairs) improves further when adding T2Ranking\n(~2M), reaching 0.505 (Pearson) / 0.537 (Spearman) on the SimCLUE test set.\nUnder this open-data setting, Chinese ModernBERT surpasses Qwen-0.6B-embedding\non SimCLUE, suggesting a clear scaling path for STS with additional curated\npairs. We will release tokenizer and weights to facilitate reproducible\nresearch.",
        "url": "http://arxiv.org/abs/2510.12285v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12285v1",
        "arxiv_id": "2510.12285v1",
        "authors": [
            "Zeyu Zhao",
            "Ningtao Wang",
            "Xing Fu",
            "Yu Cheng"
        ],
        "submitted": "2025-10-14 08:41:22",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on advancements in Chinese language processing using BERT, specifically on tokenization, pre-training, and fine-tuning. While it touches on the topic of retrieval-oriented quality, it does not directly relate to the user's core research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Deep Associations, High Creativity: A Simple yet Effective Metric for Evaluating Large Language Models",
        "abstract": "The evaluation of LLMs' creativity represents a crucial research domain,\nthough challenges such as data contamination and costly human assessments often\nimpede progress. Drawing inspiration from human creativity assessment, we\npropose PACE, asking LLMs to generate Parallel Association Chains to Evaluate\ntheir creativity. PACE minimizes the risk of data contamination and offers a\nstraightforward, highly efficient evaluation, as evidenced by its strong\ncorrelation with Chatbot Arena Creative Writing rankings (Spearman's $\\rho =\n0.739$, $p < 0.001$) across various proprietary and open-source models. A\ncomparative analysis of associative creativity between LLMs and humans reveals\nthat while high-performing LLMs achieve scores comparable to average human\nperformance, professional humans consistently outperform LLMs. Furthermore,\nlinguistic analysis reveals that both humans and LLMs exhibit a trend of\ndecreasing concreteness in their associations, and humans demonstrating a\ngreater diversity of associative patterns.",
        "url": "http://arxiv.org/abs/2510.12110v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12110v1",
        "arxiv_id": "2510.12110v1",
        "authors": [
            "Ziliang Qiu",
            "Renfen Hu"
        ],
        "submitted": "2025-10-14 03:26:28",
        "source": "arxiv",
        "comment": "14 pages",
        "score": 7,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on evaluating the creativity of Large Language Models (LLMs), which is a topic in Natural Language Processing (NLP). However, it does not directly relate to the user's core research interests in Information Retrieval (IR), query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "Evaluating Retrieval-Augmented Generation Systems on Unanswerable, Uncheatable, Realistic, Multi-hop Queries",
        "abstract": "Real-world use cases often present RAG systems with complex queries for which\nrelevant information is missing from the corpus or is incomplete. In these\nsettings, RAG systems must be able to reject unanswerable, out-of-scope queries\nand identify failures of retrieval and multi-hop reasoning. Despite this,\nexisting RAG benchmarks rarely reflect realistic task complexity for multi-hop\nor out-of-scope questions, which often can be cheated via disconnected\nreasoning (i.e., solved without genuine multi-hop inference) or require only\nsimple factual recall. This limits the ability for such benchmarks to uncover\nlimitations of existing RAG systems. To address this gap, we present the first\npipeline for automatic, difficulty-controlled creation of\nun$\\underline{c}$heatable, $\\underline{r}$ealistic, $\\underline{u}$nanswerable,\nand $\\underline{m}$ulti-hop $\\underline{q}$uerie$\\underline{s}$ (CRUMQs),\nadaptable to any corpus and domain. We use our pipeline to create CRUMQs over\ntwo popular RAG datasets and demonstrate its effectiveness via benchmark\nexperiments on leading retrieval-augmented LLMs. Results show that compared to\nprior RAG benchmarks, CRUMQs are highly challenging for RAG systems and achieve\nup to 81.0\\% reduction in cheatability scores. More broadly, our pipeline\noffers a simple way to enhance benchmark difficulty and realism and drive\ndevelopment of more capable RAG systems.",
        "url": "http://arxiv.org/abs/2510.11956v1",
        "pdf_url": "http://arxiv.org/pdf/2510.11956v1",
        "arxiv_id": "2510.11956v1",
        "authors": [
            "Gabrielle Kaili-May Liu",
            "Bryan Li",
            "Arman Cohan",
            "William Gantt Walden",
            "Eugene Yang"
        ],
        "submitted": "2025-10-13 21:38:04",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of query understanding and ranking models. However, the focus on Retrieval-Augmented Generation Systems and multi-hop queries is not a central match to your primary interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Deep Research Brings Deeper Harm",
        "abstract": "Deep Research (DR) agents built on Large Language Models (LLMs) can perform\ncomplex, multi-step research by decomposing tasks, retrieving online\ninformation, and synthesizing detailed reports. However, the misuse of LLMs\nwith such powerful capabilities can lead to even greater risks. This is\nespecially concerning in high-stakes and knowledge-intensive domains such as\nbiosecurity, where DR can generate a professional report containing detailed\nforbidden knowledge. Unfortunately, we have found such risks in practice:\nsimply submitting a harmful query, which a standalone LLM directly rejects, can\nelicit a detailed and dangerous report from DR agents. This highlights the\nelevated risks and underscores the need for a deeper safety analysis. Yet,\njailbreak methods designed for LLMs fall short in exposing such unique risks,\nas they do not target the research ability of DR agents. To address this gap,\nwe propose two novel jailbreak strategies: Plan Injection, which injects\nmalicious sub-goals into the agent's plan; and Intent Hijack, which reframes\nharmful queries as academic research questions. We conducted extensive\nexperiments across different LLMs and various safety benchmarks, including\ngeneral and biosecurity forbidden prompts. These experiments reveal 3 key\nfindings: (1) Alignment of the LLMs often fail in DR agents, where harmful\nprompts framed in academic terms can hijack agent intent; (2) Multi-step\nplanning and execution weaken the alignment, revealing systemic vulnerabilities\nthat prompt-level safeguards cannot address; (3) DR agents not only bypass\nrefusals but also produce more coherent, professional, and dangerous content,\ncompared with standalone LLMs. These results demonstrate a fundamental\nmisalignment in DR agents and call for better alignment techniques tailored to\nDR agents. Code and datasets are available at\nhttps://chenxshuo.github.io/deeper-harm.",
        "url": "http://arxiv.org/abs/2510.11851v1",
        "pdf_url": "http://arxiv.org/pdf/2510.11851v1",
        "arxiv_id": "2510.11851v1",
        "authors": [
            "Shuo Chen",
            "Zonggen Li",
            "Zhen Han",
            "Bailan He",
            "Tong Liu",
            "Haokun Chen",
            "Georg Groh",
            "Philip Torr",
            "Volker Tresp",
            "Jindong Gu"
        ],
        "submitted": "2025-10-13 19:05:00",
        "source": "arxiv",
        "comment": "Accepted to Reliable ML from Unreliable Data Workshop @ NeurIPS 2025",
        "score": 7,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves Large Language Models and query understanding, its focus is on the misuse of these models for generating harmful content, which is not a central theme in your research."
    },
    {
        "title": "Dr.LLM: Dynamic Layer Routing in LLMs",
        "abstract": "Large Language Models (LLMs) process every token through all layers of a\ntransformer stack, causing wasted computation on simple queries and\ninsufficient flexibility for harder ones that need deeper reasoning.\nAdaptive-depth methods can improve efficiency, but prior approaches rely on\ncostly inference-time search, architectural changes, or large-scale retraining,\nand in practice often degrade accuracy despite efficiency gains. We introduce\nDr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that\nequips pretrained models with lightweight per-layer routers deciding to skip,\nexecute, or repeat a block. Routers are trained with explicit supervision:\nusing Monte Carlo Tree Search (MCTS), we derive high-quality layer\nconfigurations that preserve or improve accuracy under a compute budget. Our\ndesign, windowed pooling for stable routing, focal loss with class balancing,\nand bottleneck MLP routers, ensures robustness under class imbalance and long\nsequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to\n+3.4%p while saving 5 layers per example on average. Routers generalize to\nout-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,\nAGIEval) with only 0.85% accuracy drop while retaining efficiency, and\noutperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that\nexplicitly supervised routers retrofit frozen LLMs for budget-aware,\naccuracy-driven inference without altering base weights.",
        "url": "http://arxiv.org/abs/2510.12773v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12773v1",
        "arxiv_id": "2510.12773v1",
        "authors": [
            "Ahmed Heakl",
            "Martin Gubri",
            "Salman Khan",
            "Sangdoo Yun",
            "Seong Joon Oh"
        ],
        "submitted": "2025-10-14 17:51:26",
        "source": "arxiv",
        "comment": "17 pages, Under submission",
        "score": 6,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on improving the efficiency of Large Language Models (LLMs) through dynamic routing, which is somewhat related to the user's interest in query understanding and ranking models. However, the paper's primary focus on LLMs and their efficiency gains does not directly align with the user's core research themes in Information Retrieval and Search technologies."
    },
    {
        "title": "Fine-grained Analysis of Brain-LLM Alignment through Input Attribution",
        "abstract": "Understanding the alignment between large language models (LLMs) and human\nbrain activity can reveal computational principles underlying language\nprocessing. We introduce a fine-grained input attribution method to identify\nthe specific words most important for brain-LLM alignment, and leverage it to\nstudy a contentious research question about brain-LLM alignment: the\nrelationship between brain alignment (BA) and next-word prediction (NWP). Our\nfindings reveal that BA and NWP rely on largely distinct word subsets: NWP\nexhibits recency and primacy biases with a focus on syntax, while BA\nprioritizes semantic and discourse-level information with a more targeted\nrecency effect. This work advances our understanding of how LLMs relate to\nhuman language processing and highlights differences in feature reliance\nbetween BA and NWP. Beyond this study, our attribution method can be broadly\napplied to explore the cognitive relevance of model predictions in diverse\nlanguage processing tasks.",
        "url": "http://arxiv.org/abs/2510.12355v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12355v1",
        "arxiv_id": "2510.12355v1",
        "authors": [
            "Michela Proietti",
            "Roberto Capobianco",
            "Mariya Toneva"
        ],
        "submitted": "2025-10-14 10:19:01",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on the alignment between large language models and human brain activity, which is outside the scope of Information Retrieval and Search technologies. Although it involves Natural Language Processing, the topic is more aligned with cognitive psychology and neuroscience rather than your primary focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Task-Aware Reduction for Scalable LLM-Database Systems",
        "abstract": "Large Language Models (LLMs) are increasingly applied to data-intensive\nworkflows, from database querying to developer observability. Yet the\neffectiveness of these systems is constrained by the volume, verbosity, and\nnoise of real-world text-rich data such as logs, telemetry, and monitoring\nstreams. Feeding such data directly into LLMs is costly, environmentally\nunsustainable, and often misaligned with task objectives. Parallel efforts in\nLLM efficiency have focused on model- or architecture-level optimizations, but\nthe challenge of reducing upstream input verbosity remains underexplored. In\nthis paper, we argue for treating the token budget of an LLM as an attention\nbudget and elevating task-aware text reduction as a first-class design\nprinciple for language -- data systems. We position input-side reduction not as\ncompression, but as attention allocation: prioritizing information most\nrelevant to downstream tasks. We outline open research challenges for building\nbenchmarks, designing adaptive reduction pipelines, and integrating\ntoken-budget--aware preprocessing into database and retrieval systems. Our\nvision is to channel scarce attention resources toward meaningful signals in\nnoisy, data-intensive workflows, enabling scalable, accurate, and sustainable\nLLM--data integration.",
        "url": "http://arxiv.org/abs/2510.11813v1",
        "pdf_url": "http://arxiv.org/pdf/2510.11813v1",
        "arxiv_id": "2510.11813v1",
        "authors": [
            "Marcus Emmanuel Barnes",
            "Taher A. Ghaleb",
            "Safwat Hassan"
        ],
        "submitted": "2025-10-13 18:10:03",
        "source": "arxiv",
        "comment": "Preprint. Accepted for presentation at the Workshop on Language\n  Models and Databases (LMD), co-located with CASCON 2025 (IEEE). The final\n  version will appear in IEEE Xplore",
        "score": 6,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the efficiency of Large Language Models (LLMs) in data-intensive workflows, but its focus on reducing input verbosity and attention allocation does not directly align with your core research themes in Information Retrieval and Search technologies. While it touches on related topics like data systems and retrieval, the paper's primary focus on LLM efficiency and text reduction makes it somewhat relevant but not a central match for your interests."
    },
    {
        "title": "The Role of Parametric Injection-A Systematic Study of Parametric Retrieval-Augmented Generation",
        "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by\nretrieving external documents. As an emerging form of RAG, parametric\nretrieval-augmented generation (PRAG) encodes documents as model parameters\n(i.e., LoRA modules) and injects these representations into the model during\ninference, enabling interaction between the LLM and documents at parametric\nlevel. Compared with directly placing documents in the input context, PRAG is\nmore efficient and has the potential to offer deeper model-document\ninteraction. Despite its growing attention, the mechanism underlying parametric\ninjection remains poorly understood. In this work, we present a systematic\nstudy of PRAG to clarify the role of parametric injection, showing that\nparameterized documents capture only partial semantic information of documents,\nand relying on them alone yields inferior performance compared to interaction\nat text level. However, these parametric representations encode high-level\ndocument information that can enhance the model's understanding of documents\nwithin the input context. When combined parameterized documents with textual\ndocuments, the model can leverage relevant information more effectively and\nbecome more robust to noisy inputs, achieving better performance than either\nsource alone. We recommend jointly using parameterized and textual documents\nand advocate for increasing the information content of parametric\nrepresentations to advance PRAG.",
        "url": "http://arxiv.org/abs/2510.12668v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12668v1",
        "arxiv_id": "2510.12668v1",
        "authors": [
            "Minghao Tang",
            "Shiyu Ni",
            "Jingtong Wu",
            "Zengxin Han",
            "Keping Bi"
        ],
        "submitted": "2025-10-14 16:05:01",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores Retrieval-Augmented Generation (RAG) and its parametric variant, PRAG, which involves encoding documents as model parameters. While it touches on the interaction between models and documents, it does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest. The connection to information retrieval is indirect, but the paper's focus on deep semantic understanding and real-time relevance optimization is somewhat relevant to your broader research interests in NLP and data mining."
    },
    {
        "title": "VISaGE: Understanding Visual Generics and Exceptions",
        "abstract": "While Vision Language Models (VLMs) learn conceptual representations, in the\nform of generalized knowledge, during training, they are typically used to\nanalyze individual instances. When evaluation instances are atypical, this\nparadigm results in tension between two priors in the model. The first is a\npragmatic prior that the textual and visual input are both relevant, arising\nfrom VLM finetuning on congruent inputs; the second is a semantic prior that\nthe conceptual representation is generally true for instances of the category.\nIn order to understand how VLMs trade off these priors, we introduce a new\nevaluation dataset, VISaGE, consisting of both typical and exceptional images.\nIn carefully balanced experiments, we show that conceptual understanding\ndegrades when the assumption of congruency underlying the pragmatic prior is\nviolated with incongruent images. This effect is stronger than the effect of\nthe semantic prior when querying about individual instances.",
        "url": "http://arxiv.org/abs/2510.12548v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12548v1",
        "arxiv_id": "2510.12548v1",
        "authors": [
            "Stella Frank",
            "Emily Allaway"
        ],
        "submitted": "2025-10-14 14:13:06",
        "source": "arxiv",
        "comment": "EMNLP 2025",
        "score": 5,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the limitations of Vision Language Models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on visual generics and exceptions is not directly aligned with the user's core research themes, and the connection to real-time relevance optimization is not clear."
    },
    {
        "title": "SMEC: Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression",
        "abstract": "Large language models (LLMs) generate high-dimensional embeddings that\ncapture rich semantic and syntactic information. However, high-dimensional\nembeddings exacerbate computational complexity and storage requirements,\nthereby hindering practical deployment. To address these challenges, we propose\na novel training framework named Sequential Matryoshka Embedding Compression\n(SMEC). This framework introduces the Sequential Matryoshka Representation\nLearning(SMRL) method to mitigate gradient variance during training, the\nAdaptive Dimension Selection (ADS) module to reduce information degradation\nduring dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module\nto enhance unsupervised learning between high- and low-dimensional embeddings.\nExperiments on image, text, and multimodal datasets demonstrate that SMEC\nachieves significant dimensionality reduction while maintaining performance.\nFor instance, on the BEIR dataset, our approach improves the performance of\ncompressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points\ncompared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.",
        "url": "http://arxiv.org/abs/2510.12474v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12474v1",
        "arxiv_id": "2510.12474v1",
        "authors": [
            "Biao Zhang",
            "Lixin Chen",
            "Tong Liu",
            "Bo Zheng"
        ],
        "submitted": "2025-10-14 13:04:22",
        "source": "arxiv",
        "comment": "Accepted by EMNLP2025",
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper focuses on embedding compression for large language models, which is somewhat related to information retrieval and search technologies. However, the primary focus is on representation learning and compression, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's relevance to IR is indirect, but it may be of interest for researchers exploring the intersection of NLP and IR."
    },
    {
        "title": "Causal Inspired Multi Modal Recommendation",
        "abstract": "Multimodal recommender systems enhance personalized recommendations in\ne-commerce and online advertising by integrating visual, textual, and user-item\ninteraction data. However, existing methods often overlook two critical biases:\n(i) modal confounding, where latent factors (e.g., brand style or product\ncategory) simultaneously drive multiple modalities and influence user\npreference, leading to spurious feature-preference associations; (ii)\ninteraction bias, where genuine user preferences are mixed with noise from\nexposure effects and accidental clicks. To address these challenges, we propose\na Causal-inspired multimodal Recommendation framework. Specifically, we\nintroduce a dual-channel cross-modal diffusion module to identify hidden modal\nconfounders, utilize back-door adjustment with hierarchical matching and\nvector-quantized codebooks to block confounding paths, and apply front-door\nadjustment combined with causal topology reconstruction to build a deconfounded\ncausal subgraph. Extensive experiments on three real-world e-commerce datasets\ndemonstrate that our method significantly outperforms state-of-the-art\nbaselines while maintaining strong interpretability.",
        "url": "http://arxiv.org/abs/2510.12325v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12325v1",
        "arxiv_id": "2510.12325v1",
        "authors": [
            "Jie Yang",
            "Chenyang Gu",
            "Zixuan Liu"
        ],
        "submitted": "2025-10-14 09:29:07",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'click' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on multimodal recommendation systems, which is somewhat related to information retrieval, but its primary focus on recommender systems and causal analysis is not a central match to the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "DSAS: A Universal Plug-and-Play Framework for Attention Optimization in Multi-Document Question Answering",
        "abstract": "While large language models (LLMs) show considerable promise across various\nfields, they have notable limitations in handling multi-document question\nanswering (Multi-doc QA) tasks. The first challenge is long-range dependency\nmodeling, where LLMs struggle to focus on key information in long texts, which\nweakens important semantic connections. Second, most LLMs suffer from the\n''lost-in-the-middle'' issue, where they have difficulty processing information\nin the middle of long inputs. Current solutions either truncate global\ndependencies or demand costly finetuning, ultimately lacking a universal and\nsimple solution for these challenges. To resolve these limitations, we propose\nDual-Stage Adaptive Sharpening (DSAS) containing two modules. (i) The\nContextual Gate Weighting (CGW) module alleviates ''lost-in-the-middle'' by\nassessing paragraph relevance through layer-wise attention tracking and\nposition-aware weighting. (ii) The Reciprocal Attention Suppression (RAS)\nmodule enhances focus on critical paragraphs by suppressing information\nexchange between key and irrelevant texts, thus mitigating the limitations in\nlong-range dependency modeling. Notably, DSAS functions as a plug-and-play\nsolution requiring no architectural modifications or extra training parameters.\nExtensive experiments on four benchmarks demonstrate DSAS's efficacy across\nmainstream LLMs (Llama, Qwen, Mistral, and Deepseek), with an average F1-score\nimprovement of 4.2% in Multi-doc QA tasks on Llama-3.1-8B-Instruct and\nQwen2.5-14B-Instruct. Ablation studies confirm the essential contributions of\nboth the CGW and RAS modules. In addition, detailed discussions in the Appendix\nfurther validate the robustness and scalability of DSAS.",
        "url": "http://arxiv.org/abs/2510.12251v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12251v1",
        "arxiv_id": "2510.12251v1",
        "authors": [
            "Jiakai Li",
            "Rongzheng Wang",
            "Yizhuo Ma",
            "Shuang Liang",
            "Guangchun Luo",
            "Ke Qin"
        ],
        "submitted": "2025-10-14 08:01:59",
        "source": "arxiv",
        "comment": "27 pages, has been accepted by NeurIPS 2025",
        "score": 5,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper proposes a framework for attention optimization in multi-document question answering, which is somewhat related to information retrieval. However, the focus on large language models and multi-document question answering is not directly aligned with the user's core research themes in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "SafeMT: Multi-turn Safety for Multimodal Language Models",
        "abstract": "With the widespread use of multi-modal Large Language models (MLLMs), safety\nissues have become a growing concern. Multi-turn dialogues, which are more\ncommon in everyday interactions, pose a greater risk than single prompts;\nhowever, existing benchmarks do not adequately consider this situation. To\nencourage the community to focus on the safety issues of these models in\nmulti-turn dialogues, we introduce SafeMT, a benchmark that features dialogues\nof varying lengths generated from harmful queries accompanied by images. This\nbenchmark consists of 10,000 samples in total, encompassing 17 different\nscenarios and four jailbreak methods. Additionally, we propose Safety Index\n(SI) to evaluate the general safety of MLLMs during conversations. We assess\nthe safety of 17 models using this benchmark and discover that the risk of\nsuccessful attacks on these models increases as the number of turns in harmful\ndialogues rises. This observation indicates that the safety mechanisms of these\nmodels are inadequate for recognizing the hazard in dialogue interactions. We\npropose a dialogue safety moderator capable of detecting malicious intent\nconcealed within conversations and providing MLLMs with relevant safety\npolicies. Experimental results from several open-source models indicate that\nthis moderator is more effective in reducing multi-turn ASR compared to existed\nguard models.",
        "url": "http://arxiv.org/abs/2510.12133v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12133v1",
        "arxiv_id": "2510.12133v1",
        "authors": [
            "Han Zhu",
            "Juntao Dai",
            "Jiaming Ji",
            "Haoran Li",
            "Chengkun Cai",
            "Pengcheng Wen",
            "Chi-Min Chan",
            "Boyuan Chen",
            "Yaodong Yang",
            "Sirui Han",
            "Yike Guo"
        ],
        "submitted": "2025-10-14 04:24:07",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on safety issues in multimodal language models, specifically in multi-turn dialogues, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on the concept of query understanding and user behavior modeling, the context is safety and security rather than relevance and ranking."
    },
    {
        "title": "SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model",
        "abstract": "Multimodal embedding models aim to yield informative unified representations\nthat empower diverse cross-modal tasks. Despite promising developments in the\nevolution from CLIP-based dual-tower architectures to large vision-language\nmodels, prior works still face unavoidable challenges in real-world\napplications and business scenarios, such as the limited modality support,\nunstable training mechanisms, and industrial domain gaps. In this work, we\nintroduce SAIL-Embedding, an omni-modal embedding foundation model that\naddresses these issues through tailored training strategies and architectural\ndesign. In the optimization procedure, we propose a multi-stage training scheme\nto boost the multifaceted effectiveness of representation learning.\nSpecifically, the content-aware progressive training aims to enhance the\nmodel's adaptability to diverse downstream tasks and master enriched\ncross-modal proficiency. The collaboration-aware recommendation enhancement\ntraining further adapts multimodal representations for recommendation scenarios\nby distilling knowledge from sequence-to-item and ID-to-item embeddings while\nmining user historical interests. Concurrently, we develop the stochastic\nspecialization and dataset-driven pattern matching to strengthen model training\nflexibility and generalizability. Experimental results show that SAIL-Embedding\nachieves SOTA performance compared to other methods in different retrieval\ntasks. In online experiments across various real-world scenarios integrated\nwith our model, we observe a significant increase in Lifetime (LT), which is a\ncrucial indicator for the recommendation experience. For instance, the model\ndelivers the 7-day LT gain of +0.5% in the Douyin-Selected scenario. For the\nDouyin feed rank model, the match features produced by SAIL-Embedding yield a\n+0.1% AUC gain.",
        "url": "http://arxiv.org/abs/2510.12709v2",
        "pdf_url": "http://arxiv.org/pdf/2510.12709v2",
        "arxiv_id": "2510.12709v2",
        "authors": [
            "Lin Lin",
            "Jiefeng Long",
            "Zhihe Wan",
            "Yuchi Wang",
            "Dingkang Yang",
            "Shuang Yang",
            "Yueyang Yao",
            "Xu Chen",
            "Zirui Guo",
            "Shengqiang Li",
            "Weiran Li",
            "Hanyu Li",
            "Yaling Mou",
            "Yan Qiu",
            "Haiyang Yu",
            "Xiao Liang",
            "Hongsheng Li",
            "Chao Feng"
        ],
        "submitted": "2025-10-14 16:43:22",
        "source": "arxiv",
        "comment": "Technical Report",
        "score": 4,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on multimodal embedding models, which is somewhat related to information retrieval and search technologies. However, the primary focus on recommendation systems and real-world applications in the e-commerce domain is not a central match to the user's core research themes."
    },
    {
        "title": "When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection",
        "abstract": "Large language models (LLMs) have grown more powerful in language generation,\nproducing fluent text and even imitating personal style. Yet, this ability also\nheightens the risk of identity impersonation. To the best of our knowledge, no\nprior work has examined personalized machine-generated text (MGT) detection. In\nthis paper, we introduce \\dataset, the first benchmark for evaluating detector\nrobustness in personalized settings, built from literary and blog texts paired\nwith their LLM-generated imitations. Our experimental results demonstrate large\nperformance gaps across detectors in personalized settings: some\nstate-of-the-art models suffer significant drops. We attribute this limitation\nto the \\textit{feature-inversion trap}, where features that are discriminative\nin general domains become inverted and misleading when applied to personalized\ntext. Based on this finding, we propose \\method, a simple and reliable way to\npredict detector performance changes in personalized settings. \\method\nidentifies latent directions corresponding to inverted features and constructs\nprobe datasets that differ primarily along these features to evaluate detector\ndependence. Our experiments show that \\method can accurately predict both the\ndirection and the magnitude of post-transfer changes, showing 85\\% correlation\nwith the actual performance gaps. We hope that this work will encourage further\nresearch on personalized text detection.",
        "url": "http://arxiv.org/abs/2510.12476v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12476v1",
        "arxiv_id": "2510.12476v1",
        "authors": [
            "Lang Gao",
            "Xuhui Li",
            "Chenxi Wang",
            "Mingzhe Li",
            "Wei Liu",
            "Zirui Song",
            "Jinghui Zhang",
            "Rui Yan",
            "Preslav Nakov",
            "Xiuying Chen"
        ],
        "submitted": "2025-10-14 13:10:23",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'personalization' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on machine-generated text detection and personalization, which is somewhat related to your interests in Natural Language Processing (NLP) and related topics. However, it does not directly align with your primary focus on Information Retrieval, particularly query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation",
        "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to\nenhance the factuality of Large Language Models (LLMs). However, existing RAG\nsystems often suffer from an unfaithfulness issue, where the model's response\ncontradicts evidence from the retrieved context. Existing approaches to\nimproving contextual faithfulness largely rely on external interventions, such\nas prompt engineering, decoding constraints, or reward-based fine-tuning. These\nworks treat the LLM as a black box and overlook a crucial question: how does\nthe LLM internally integrate retrieved evidence with its parametric memory,\nparticularly under knowledge conflicts? To address this gap, we conduct a\nprobing-based analysis of hidden-state representations in LLMs and observe\nthree findings: knowledge integration occurs hierarchically, conflicts manifest\nas latent signals at the sentence level, and irrelevant context is often\namplified when aligned with parametric knowledge. Building on these findings,\nwe propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a\nframework that (i) decomposes context into fine-grained sentence-level\nknowledge, (ii) employs hidden-state probing to localize conflicting knowledge,\nand (iii) introduces conflict-aware fine-tuning to guide the model to\naccurately integrate retrieved evidence. Extensive experiments across three\nbenchmarks demonstrate that CLEAR substantially improves both accuracy and\ncontextual faithfulness, consistently outperforming strong baselines under\ndiverse conflict conditions. The related resources are available at\nhttps://github.com/LinfengGao/CLEAR.",
        "url": "http://arxiv.org/abs/2510.12460v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12460v1",
        "arxiv_id": "2510.12460v1",
        "authors": [
            "Linfeng Gao",
            "Baolong Bi",
            "Zheng Yuan",
            "Le Wang",
            "Zerui Chen",
            "Zhimin Wei",
            "Shenghua Liu",
            "Qinggang Zhang",
            "Jinsong Su"
        ],
        "submitted": "2025-10-14 12:48:24",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores Retrieval-Augmented Generation (RAG) and proposes a framework to improve contextual faithfulness in Large Language Models (LLMs). While it touches on aspects of information retrieval and knowledge integration, its primary focus is on improving the faithfulness of LLMs, which is somewhat related to your interests in query understanding and ranking models, but not a central match."
    },
    {
        "title": "PRoH: Dynamic Planning and Reasoning over Knowledge Hypergraphs for Retrieval-Augmented Generation",
        "abstract": "Knowledge Hypergraphs (KHs) have recently emerged as a knowledge\nrepresentation for retrieval-augmented generation (RAG), offering a paradigm to\nmodel multi-entity relations into a structured form. However, existing KH-based\nRAG methods suffer from three major limitations: static retrieval planning,\nnon-adaptive retrieval execution, and superficial use of KH structure and\nsemantics, which constrain their ability to perform effective multi-hop\nquestion answering. To overcome these limitations, we propose PRoH, a dynamic\nPlanning and Reasoning over Knowledge Hypergraphs framework. PRoH incorporates\nthree core innovations: (i) a context-aware planning module that sketches the\nlocal KH neighborhood to guide structurally grounded reasoning plan generation;\n(ii) a structured question decomposition process that organizes subquestions as\na dynamically evolving Directed Acyclic Graph (DAG) to enable adaptive,\nmulti-trajectory exploration; and (iii) an Entity-Weighted Overlap (EWO)-guided\nreasoning path retrieval algorithm that prioritizes semantically coherent\nhyperedge traversals. Experiments across multiple domains demonstrate that PRoH\nachieves state-of-the-art performance, surpassing the prior SOTA model\nHyperGraphRAG by an average of 19.73% in F1 and 8.41% in Generation Evaluation\n(G-E) score, while maintaining strong robustness in long-range multi-hop\nreasoning tasks.",
        "url": "http://arxiv.org/abs/2510.12434v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12434v1",
        "arxiv_id": "2510.12434v1",
        "authors": [
            "Xiangjun Zai",
            "Xingyu Tan",
            "Xiaoyang Wang",
            "Qing Liu",
            "Xiwei Xu",
            "Wenjie Zhang"
        ],
        "submitted": "2025-10-14 12:13:23",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper proposes a framework for retrieval-augmented generation, which is related to information retrieval and natural language processing. However, the focus on knowledge hypergraphs and question answering does not directly align with the user's primary interests in query understanding, ranking models, and user behavior modeling. While it touches on semantic understanding, it is more focused on generation and reasoning rather than retrieval and search."
    },
    {
        "title": "Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation",
        "abstract": "Counter-speech generation is at the core of many expert activities, such as\nfact-checking and hate speech, to counter harmful content. Yet, existing work\ntreats counter-speech generation as pure text generation task, mainly based on\nLarge Language Models or NGO experts. These approaches show severe drawbacks\ndue to the limited reliability and coherence in the generated countering text,\nand in scalability, respectively. To close this gap, we introduce a novel\nframework to model counter-speech generation as knowledge-wise text generation\nprocess. Our framework integrates advanced Retrieval-Augmented Generation (RAG)\npipelines to ensure the generation of trustworthy counter-speech for 8 main\ntarget groups identified in the hate speech literature, including women, people\nof colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons,\nand other. We built a knowledge base over the United Nations Digital Library,\nEUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792\ntexts. We use the MultiTarget-CONAN dataset to empirically assess the quality\nof the generated counter-speech, both through standard metrics (i.e., JudgeLM)\nand a human evaluation. Results show that our framework outperforms standard\nLLM baselines and competitive approach, on both assessments. The resulting\nframework and the knowledge base pave the way for studying trustworthy and\nsound counter-speech generation, in hate speech and beyond.",
        "url": "http://arxiv.org/abs/2510.12316v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12316v1",
        "arxiv_id": "2510.12316v1",
        "authors": [
            "Greta Damo",
            "Elena Cabrio",
            "Serena Villata"
        ],
        "submitted": "2025-10-14 09:20:01",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on counter-speech generation using Retrieval-Augmented Generation (RAG) pipelines, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve text generation and knowledge base construction, the specific application to counter-speech generation and hate speech is not aligned with your primary focus on e-commerce, query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "DPO-Tuned Large Language Models for Segmentation in Simultaneous Speech Translation",
        "abstract": "Simultaneous speech translation requires accurate segmentation to balance\ntranslation quality and latency. Recent studies such as SHAS have introduced\npretrained segmentation models, achieving stronger performance than heuristic\nrules. However, segmentation models such as SHAS, though pretrained and more\nrobust than heuristic methods, are still constrained by supervised learning\nobjectives and do not incorporate human preference alignment, which is crucial\nfor natural real-time interpretation. In this work, we propose a segmentation\nframework based on large language models (LLMs) trained with Direct Preference\nOptimization (DPO). By leveraging preference alignment, our method enables LLMs\nto predict natural segmentation points that better meet the demands of\nreal-time translation. We evaluate the system on the ACL 60/60 corpus across\nthree language pairs (English-Japanese, Chinese, German), using SeamlessM4T v2\nas the translation backbone. Experimental results show that our DPO-tuned LLM\nachieves higher segmentation accuracy than SHAS and yields consistent\nimprovements in translation quality (BLEU, COMET) as well as latency (Average\nLagging). Furthermore, our system benefits from IWSLT baselines for direct\ncomparison. These findings highlight the potential of preference-tuned LLMs to\nsurpass existing pretrained segmentation models and advance adaptive,\nhuman-aligned simultaneous interpretation.",
        "url": "http://arxiv.org/abs/2510.12195v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12195v1",
        "arxiv_id": "2510.12195v1",
        "authors": [
            "Zeyu Yang",
            "Satoshi Nakamura"
        ],
        "submitted": "2025-10-14 06:41:36",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on segmentation in simultaneous speech translation, leveraging large language models and Direct Preference Optimization. While it involves NLP and potentially some IR aspects, it is not directly related to the user's core research themes of query understanding, ranking models, or user behavior modeling in the context of search technologies."
    },
    {
        "title": "Towards Inference-time Scaling for Continuous Space Reasoning",
        "abstract": "Inference-time scaling through multiple sample generation in combination with\nProcess- or Outcome-Reward Model (PRM or ORM) re-ranking has proven effective\nfor text-based reasoning in large language models. This paper investigates\nwhether such established techniques can be successfully adapted to reasoning in\nthe continuous space, using COCONUT (Hao et al. 2024) continuous space\nreasoning LM as the backbone. We demonstrate the feasibility of generating\ndiverse reasoning paths through dropout-based sampling. Our Pass@N analysis on\nthe generated samples reveals the potential that could enable a significant\ngain in performance akin to observed gain in the discrete space. However, we\nhighlight unique challenges faced for materializing this gain in the continuous\nthought space. In particular, working recipes for data generation and training\nPRM and ORM models in the discrete space unlocks only marginal improvements in\nthe continuous space. Through probing various aspects including geometric\nproperties and trajectory dynamics we identify the underlying reasons that\nprevent effective discrimination between correct and incorrect reasoning\n(essential for the functioning of PRM and ORM). Our findings reveal that\ncurrent limitations stem from the absence of key inductive biases in continuous\nthought representations. We argue that the training frameworks for continuous\nreasoning LMs require not only to optimize for accuracy but also to explicitly\nincorporate inductive biases that could be utilized during inference-time for\ndiscrimination of correct and incorrect thoughts.\\footnote{Our code and data\nwill be publicly available.}",
        "url": "http://arxiv.org/abs/2510.12167v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12167v1",
        "arxiv_id": "2510.12167v1",
        "authors": [
            "Minghan Wang",
            "Thuy-Trang Vu",
            "Ehsan Shareghi",
            "Gholamreza Haffari"
        ],
        "submitted": "2025-10-14 05:53:41",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper explores the application of established techniques in text-based reasoning to continuous space reasoning, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on continuous space reasoning and the absence of explicit mention of user behavior modeling or click models limit its direct relevance to your core research themes."
    },
    {
        "title": "One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration",
        "abstract": "Symbolic world modeling requires inferring and representing an environment's\ntransitional dynamics as an executable program. Prior work has focused on\nlargely deterministic environments with abundant interaction data, simple\nmechanics, and human guidance. We address a more realistic and challenging\nsetting, learning in a complex, stochastic environment where the agent has only\n\"one life\" to explore a hostile environment without human guidance. We\nintroduce OneLife, a framework that models world dynamics through\nconditionally-activated programmatic laws within a probabilistic programming\nframework. Each law operates through a precondition-effect structure,\nactivating in relevant world states. This creates a dynamic computation graph\nthat routes inference and optimization only through relevant laws, avoiding\nscaling challenges when all laws contribute to predictions about a complex,\nhierarchical state, and enabling the learning of stochastic dynamics even with\nsparse rule activation. To evaluate our approach under these demanding\nconstraints, we introduce a new evaluation protocol that measures (a) state\nranking, the ability to distinguish plausible future states from implausible\nones, and (b) state fidelity, the ability to generate future states that\nclosely resemble reality. We develop and evaluate our framework on Crafter-OO,\nour reimplementation of the Crafter environment that exposes a structured,\nobject-oriented symbolic state and a pure transition function that operates on\nthat state alone. OneLife can successfully learn key environment dynamics from\nminimal, unguided interaction, outperforming a strong baseline on 16 out of 23\nscenarios tested. We also test OneLife's planning ability, with simulated\nrollouts successfully identifying superior strategies. Our work establishes a\nfoundation for autonomously constructing programmatic world models of unknown,\ncomplex environments.",
        "url": "http://arxiv.org/abs/2510.12088v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12088v1",
        "arxiv_id": "2510.12088v1",
        "authors": [
            "Zaid Khan",
            "Archiki Prasad",
            "Elias Stengel-Eskin",
            "Jaemin Cho",
            "Mohit Bansal"
        ],
        "submitted": "2025-10-14 02:49:32",
        "source": "arxiv",
        "comment": "Project page: https://onelife-worldmodel.github.io/; 39 pages",
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The paper focuses on symbolic world modeling and probabilistic programming for stochastic environments, which is outside your primary areas of interest."
    },
    {
        "title": "APCE: Adaptive Progressive Context Expansion for Long Context Processing",
        "abstract": "Deploying useful Long-Context Transformer Models (LCTMs) requires addressing\ntwo key challenges: (1) A growing memory footprint due to quadratic\nself-attention and linear KV-cache scaling in memory as sequence length\nincreases; (2) the ContextRot phenomena where empirical evidence suggests that\ntransformer architecture's performance degrades with increasing context length.\nGiven the shared dependency on the input, a natural question arises: Can we\nsurgically select the most important input chunks for processing to\nsynergistically (a) reduce the memory footprint, and (b) mitigate the\nContextRot effects? In this paper, we answer this question in the affirmative\nfor long-context summarization tasks. We propose APCE as a context-aware\nsolution to select the most important input chunks through low-dimensional\nsemantic similarity matching with the current query. By directly operating on\nthe input, APCE decouples from strict dependency on underlying hardware or CUDA\nenvironments, promising a compatible solution scalable to different deployment\nsystems. Our empirical evaluations have demonstrated superior or on-par\nsummarization performance for APCE compared to the full dense baseline using a\nfraction (50%-70%) of the input sequence resulting in KV-cache and\nself-attention memory efficiency improvements. We hope our findings inspire\nfurther research on context-aware efficiency solutions for LCTMs geared towards\nother relevant long-context tasks.",
        "url": "http://arxiv.org/abs/2510.12051v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12051v1",
        "arxiv_id": "2510.12051v1",
        "authors": [
            "Baisub Lee",
            "Sanghyun Byun",
            "Mohanad Odema",
            "Jung Guack",
            "Jacob Song",
            "Woo Seong Chung"
        ],
        "submitted": "2025-10-14 01:26:36",
        "source": "arxiv",
        "comment": "NeurIPS 2025 Workshop: ML For Systems",
        "score": 4,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper explores a context-aware solution for long-context processing, specifically addressing memory efficiency and performance degradation issues in transformer models. While it touches on query understanding and ranking models, its primary focus is on optimizing the processing of long contexts, which is somewhat related to the user's interests in information retrieval and deep semantic understanding. However, the paper's emphasis on scalability and deployment systems is more aligned with the e-commerce domain, making it only somewhat relevant to the user's broader research interests."
    },
    {
        "title": "Multi-stage Prompt Refinement for Mitigating Hallucinations in Large Language Models",
        "abstract": "Recent advancements in large language models (LLMs) have shown strong\nperformance in natural language understanding and generation tasks. However,\nLLMs continue to encounter challenges with hallucinations, where models\ngenerate plausible but incorrect information. While several factors contribute\nto hallucinations, the impact of ill-formed prompts, prompts with ambiguous\nwording, incorrect grammar, or incomplete information, was relatively under\nexplored. To address this, we introduce Multi-stage Prompt Refinement (MPR), a\nframework designed to systematically improve these ill-formed prompts across\nmultiple stages. Each stage addresses specific errors such as punctuation,\ntypographical mistakes, and misuse of key terms, using small language models\n(SLMs) fine-tuned for these tasks. MPR iteratively enhances the clarity of\nprompts with additional context and employs a self-reflection mechanism with\nranking to prioritize the most relevant input. Experimental results on\nhallucination benchmarks show that prompts refined by MPR achieve over an 85~\\%\nwin rate compared to their original forms, demonstrating its effectiveness in\nreducing hallucinations and improving LLM output accuracy. Interestingly, we\nreveal that MPR can be combined with existing post-hoc hallucination mitigation\nframeworks, further enhancing its versatility. MPR provides a lightweight and\nadaptable solution for enhancing LLM reliability across various domains.",
        "url": "http://arxiv.org/abs/2510.12032v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12032v1",
        "arxiv_id": "2510.12032v1",
        "authors": [
            "Jung-Woo Shim",
            "Yeong-Joon Ju",
            "Ji-Hoon Park",
            "Seong-Whan Lee"
        ],
        "submitted": "2025-10-14 00:31:36",
        "source": "arxiv",
        "comment": "22 pages, 6 figures",
        "score": 4,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on large language models and hallucinations, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the primary focus on natural language generation and post-hoc mitigation frameworks does not directly align with the user's core research themes in Information Retrieval and Search technologies."
    },
    {
        "title": "CTRL-Rec: Controlling Recommender Systems With Natural Language",
        "abstract": "When users are dissatisfied with recommendations from a recommender system,\nthey often lack fine-grained controls for changing them. Large language models\n(LLMs) offer a solution by allowing users to guide their recommendations\nthrough natural language requests (e.g., \"I want to see respectful posts with a\ndifferent perspective than mine\"). We propose a method, CTRL-Rec, that allows\nfor natural language control of traditional recommender systems in real-time\nwith computational efficiency. Specifically, at training time, we use an LLM to\nsimulate whether users would approve of items based on their language requests,\nand we train embedding models that approximate such simulated judgments. We\nthen integrate these user-request-based predictions into the standard weighting\nof signals that traditional recommender systems optimize. At deployment time,\nwe require only a single LLM embedding computation per user request, allowing\nfor real-time control of recommendations. In experiments with the MovieLens\ndataset, our method consistently allows for fine-grained control across a\ndiversity of requests. In a study with 19 Letterboxd users, we find that\nCTRL-Rec was positively received by users and significantly enhanced users'\nsense of control and satisfaction with recommendations compared to traditional\ncontrols.",
        "url": "http://arxiv.org/abs/2510.12742v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12742v1",
        "arxiv_id": "2510.12742v1",
        "authors": [
            "Micah Carroll",
            "Adeline Foote",
            "Kevin Feng",
            "Marcus Williams",
            "Anca Dragan",
            "W. Bradley Knox",
            "Smitha Milli"
        ],
        "submitted": "2025-10-14 17:20:04",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper CTRL-Rec: Controlling Recommender Systems With Natural Language is somewhat related to the user's research interests in Information Retrieval and Natural Language Processing. Although it focuses on recommender systems, it leverages large language models and natural language requests, which aligns with the user's interests in query understanding and deep semantic understanding. However, the primary focus on recommender systems and user control limits its relevance to the user's core research themes."
    },
    {
        "title": "ACADATA: Parallel Dataset of Academic Data for Machine Translation",
        "abstract": "We present ACADATA, a high-quality parallel dataset for academic translation,\nthat consists of two subsets: ACAD-TRAIN, which contains approximately 1.5\nmillion author-generated paragraph pairs across 96 language directions and\nACAD-BENCH, a curated evaluation set of almost 6,000 translations covering 12\ndirections. To validate its utility, we fine-tune two Large Language Models\n(LLMs) on ACAD-TRAIN and benchmark them on ACAD-BENCH against specialized\nmachine-translation systems, general-purpose, open-weight LLMs, and several\nlarge-scale proprietary models. Experimental results demonstrate that\nfine-tuning on ACAD-TRAIN leads to improvements in academic translation quality\nby +6.1 and +12.4 d-BLEU points on average for 7B and 2B models respectively,\nwhile also improving long-context translation in a general domain by up to\n24.9% when translating out of English. The fine-tuned top-performing model\nsurpasses the best propietary and open-weight models on academic translation\ndomain. By releasing ACAD-TRAIN, ACAD-BENCH and the fine-tuned models, we\nprovide the community with a valuable resource to advance research in academic\ndomain and long-context translation.",
        "url": "http://arxiv.org/abs/2510.12621v2",
        "pdf_url": "http://arxiv.org/pdf/2510.12621v2",
        "arxiv_id": "2510.12621v2",
        "authors": [
            "IÃ±aki Lacunza",
            "Javier Garcia Gilabert",
            "Francesca De Luca Fornaciari",
            "Javier Aula-Blasco",
            "Aitor Gonzalez-Agirre",
            "Maite Melero",
            "Marta Villegas"
        ],
        "submitted": "2025-10-14 15:20:06",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on machine translation and dataset creation for academic translation, which is outside your primary focus on Information Retrieval and Search technologies. Although it involves Natural Language Processing, the specific application and context are not aligned with your core research themes."
    },
    {
        "title": "Teaching Language Models to Faithfully Express their Uncertainty",
        "abstract": "Large language models (LLMs) often miscommunicate their uncertainty: repeated\nqueries can produce divergent answers, yet generated responses are typically\nunhedged or hedged in ways that do not reflect this variability. This conveys\nunfaithful information about the uncertain state of the LLMs' knowledge,\ncreating a faithfulness gap that affects even strong LLMs. We introduce\nFaithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches\ninstruction-tuned LLMs to express uncertainty faithfully without altering their\nunderlying answer distribution. We construct training data by augmenting model\nsamples with uncertainty hedges (i.e. verbal cues such as 'possibly' or\n'likely') aligned with sample consistency, requiring no supervision beyond the\nmodel and a set of prompts. We evaluate FUT on open-domain question answering\n(QA) across multiple models and datasets. Our results show that FUT\nsubstantially reduces the faithfulness gap, while preserving QA accuracy and\nintroducing minimal semantic distribution shift. Further analyses demonstrate\nrobustness across decoding strategies, choice of hedgers, and other forms of\nuncertainty expression (i.e. numerical). These findings establish FUT as a\nsimple and effective way to teach LLMs to communicate uncertainty faithfully.",
        "url": "http://arxiv.org/abs/2510.12587v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12587v1",
        "arxiv_id": "2510.12587v1",
        "authors": [
            "Bryan Eikema",
            "Evgenia Ilia",
            "JosÃ© G. C. de Souza",
            "Chrysoula Zerva",
            "Wilker Aziz"
        ],
        "submitted": "2025-10-14 14:42:40",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores uncertainty expression in language models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on language models and uncertainty expression in NLP is not a central match to the user's primary research interests in IR and search technologies."
    },
    {
        "title": "Leveraging Language Semantics for Collaborative Filtering with TextGCN and TextGCN-MLP: Zero-Shot vs In-Domain Performance",
        "abstract": "In recent years, various approaches have been proposed to leverage large\nlanguage models (LLMs) for incorporating textual information about items into\nrecommender systems. Existing methods primarily focus on either fine-tuning\nLLMs to generate recommendations or integrating LLM-based embeddings into\ndownstream models. In this work, we follow the latter direction and propose\n\\textbf{TextGCN}, which applies parameter-free graph convolution layers\ndirectly over LLM-based item-title embeddings, instead of learning ID-based\nembeddings as in traditional methods. By combining language semantics with\ngraph message passing, this architecture achieves state-of-the-art zero-shot\nperformance, significantly outperforming prior approaches. Furthermore, we\nintroduce \\textbf{TextGCN-MLP}, which extends TextGCN with a trainable\nmultilayer perceptron trained using a contrastive loss, achieving\nstate-of-the-art in-domain performance on recommendation benchmarks. However,\nthe zero-shot performance of TextGCN-MLP remains lower than that of TextGCN,\nhighlighting the trade-off between in-domain specialization and zero-shot\ngeneralization. We release our code on github at\n\\href{https://github.com/ChernovAndrey/TFCE}{github.com/ChernovAndrey/TFCE}.",
        "url": "http://arxiv.org/abs/2510.12461v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12461v1",
        "arxiv_id": "2510.12461v1",
        "authors": [
            "Andrei Chernov",
            "Haroon Wahab",
            "Oleg Novitskij"
        ],
        "submitted": "2025-10-14 12:50:11",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores recommender systems, specifically leveraging language semantics for collaborative filtering. While it touches on NLP and deep semantic understanding, its primary focus on recommender systems and zero-shot vs in-domain performance makes it somewhat related to your interests, but not a central match."
    },
    {
        "title": "A Survey on Parallel Reasoning",
        "abstract": "With the increasing capabilities of Large Language Models (LLMs), parallel\nreasoning has emerged as a new inference paradigm that enhances reasoning\nrobustness by concurrently exploring multiple lines of thought before\nconverging on a final answer. It has become a significant trend to explore\nparallel reasoning to overcome the fragility of standard sequential methods and\nimprove practical performance. In this paper, we aim to survey and summarize\nthe progress and challenges of parallel reasoning. We first present a formal\ndefinition of parallel reasoning and clarify its distinction from related\nconcepts like Chain-of-Thought. Then, we organize and discuss advanced\ntechniques based on a novel taxonomy, including non-interactive reasoning,\ninteractive reasoning, and efficiency-focused decoding strategies.\nAdditionally, we explore various application scenarios, such as solving complex\nproblems and enhancing the reliability of LLM outputs.Finally, we highlight the\ncore challenges of parallel reasoning and suggest potential directions for\nfuture research. We hope that our work can provide a useful roadmap for\nbeginners and encourage more research on improving parallel reasoning methods.\nRelated source can be avaliable in\nhttps://github.com/PPPP-kaqiu/Awesome-Parallel-Reasoning.",
        "url": "http://arxiv.org/abs/2510.12164v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12164v1",
        "arxiv_id": "2510.12164v1",
        "authors": [
            "Ziqi Wang",
            "Boye Niu",
            "Zipeng Gao",
            "Zhi Zheng",
            "Tong Xu",
            "Linghui Meng",
            "Zhongli Li",
            "Jing Liu",
            "Yilong Chen",
            "Chen Zhu",
            "Hua Wu",
            "Haifeng Wang",
            "Enhong Chen"
        ],
        "submitted": "2025-10-14 05:42:19",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses parallel reasoning, a new inference paradigm that enhances reasoning robustness, but it does not directly relate to information retrieval, search technologies, or query understanding. While it touches on Large Language Models (LLMs), which are relevant to NLP, the connection is not strong enough to make it a central match for your research interests."
    },
    {
        "title": "Uncertainty Quantification for Hallucination Detection in Large Language Models: Foundations, Methodology, and Future Directions",
        "abstract": "The rapid advancement of large language models (LLMs) has transformed the\nlandscape of natural language processing, enabling breakthroughs across a wide\nrange of areas including question answering, machine translation, and text\nsummarization. Yet, their deployment in real-world applications has raised\nconcerns over reliability and trustworthiness, as LLMs remain prone to\nhallucinations that produce plausible but factually incorrect outputs.\nUncertainty quantification (UQ) has emerged as a central research direction to\naddress this issue, offering principled measures for assessing the\ntrustworthiness of model generations. We begin by introducing the foundations\nof UQ, from its formal definition to the traditional distinction between\nepistemic and aleatoric uncertainty, and then highlight how these concepts have\nbeen adapted to the context of LLMs. Building on this, we examine the role of\nUQ in hallucination detection, where quantifying uncertainty provides a\nmechanism for identifying unreliable generations and improving reliability. We\nsystematically categorize a wide spectrum of existing methods along multiple\ndimensions and present empirical results for several representative approaches.\nFinally, we discuss current limitations and outline promising future research\ndirections, providing a clearer picture of the current landscape of LLM UQ for\nhallucination detection.",
        "url": "http://arxiv.org/abs/2510.12040v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12040v1",
        "arxiv_id": "2510.12040v1",
        "authors": [
            "Sungmin Kang",
            "Yavuz Faruk Bakman",
            "Duygu Nur Yaldiz",
            "Baturalp Buyukates",
            "Salman Avestimehr"
        ],
        "submitted": "2025-10-14 00:49:04",
        "source": "arxiv",
        "comment": "24 pages, 3 figures, magazine",
        "score": 3,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on uncertainty quantification in large language models, which is a topic related to NLP, but it does not directly address information retrieval, search technologies, or query understanding. While it touches on the reliability and trustworthiness of model outputs, it does not seem to be a central match for your research interests."
    },
    {
        "title": "Scaling Long-Horizon LLM Agent via Context-Folding",
        "abstract": "Large language model (LLM) agents are fundamentally constrained by context\nlength on long-horizon tasks. We introduce Context-Folding, a framework that\nempowers agents to actively manage their working context. An agent can\nprocedurally branch into a sub-trajectory to handle a subtask and then fold it\nupon completion, collapsing the intermediate steps while retaining a concise\nsummary of the outcome. To make this behavior learnable, we develop an\nend-to-end reinforcement learning framework FoldGRPO with specific process\nrewards to encourage effective task decomposition and context management. On\ncomplex long-horizon tasks (Deep Research and SWE), our folding agent matches\nor outperforms the ReAct baselines while using an active context 10$\\times$\nsmaller and significantly outperforms models that rely on summarization-based\ncontext management.",
        "url": "http://arxiv.org/abs/2510.11967v1",
        "pdf_url": "http://arxiv.org/pdf/2510.11967v1",
        "arxiv_id": "2510.11967v1",
        "authors": [
            "Weiwei Sun",
            "Miao Lu",
            "Zhan Ling",
            "Kang Liu",
            "Xuesong Yao",
            "Yiming Yang",
            "Jiecao Chen"
        ],
        "submitted": "2025-10-13 22:00:58",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be related to Large Language Models (LLMs) and reinforcement learning, but it does not directly address information retrieval, search technologies, or query understanding. While it touches on context management, it is more focused on task decomposition and long-horizon tasks, which is not a primary area of interest for the user."
    },
    {
        "title": "Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question Answering",
        "abstract": "Multilingual question answering (QA) systems must ensure factual consistency\nacross languages, especially for objective queries such as What is jaundice?,\nwhile also accounting for cultural variation in subjective responses. We\npropose MIND, a user-in-the-loop fact-checking pipeline to detect factual and\ncultural discrepancies in multilingual QA knowledge bases. MIND highlights\ndivergent answers to culturally sensitive questions (e.g., Who assists in\nchildbirth?) that vary by region and context. We evaluate MIND on a bilingual\nQA system in the maternal and infant health domain and release a dataset of\nbilingual questions annotated for factual and cultural inconsistencies. We\nfurther test MIND on datasets from other domains to assess generalization. In\nall cases, MIND reliably identifies inconsistencies, supporting the development\nof more culturally aware and factually consistent QA systems.",
        "url": "http://arxiv.org/abs/2510.11928v1",
        "pdf_url": "http://arxiv.org/pdf/2510.11928v1",
        "arxiv_id": "2510.11928v1",
        "authors": [
            "Lorena Calvo-BartolomÃ©",
            "ValÃ©rie Aldana",
            "Karla Cantarero",
            "Alonso MadroÃ±al de Mesa",
            "JerÃ³nimo Arenas-GarcÃ­a",
            "Jordan Boyd-Graber"
        ],
        "submitted": "2025-10-13 20:48:26",
        "source": "arxiv",
        "comment": "Long paper accepted at EMNLP 2025",
        "score": 3,
        "keyword_reasons": [
            "Found 'queries' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on multilingual question answering and fact-checking, which is somewhat related to information retrieval, but it does not directly address query understanding, ranking models, or user behavior modeling. The paper's emphasis on cultural variation and fact-checking does not align with the user's primary research interests in deep semantic understanding and real-time relevance optimization."
    },
    {
        "title": "LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens",
        "abstract": "Large reasoning models (LRMs) have led to new possibilities in terms of\nproblem-solving, through the devising of a natural language thought process\nprior to answering a query. While their capabilities are well known across\nmathematics and coding tasks, their impact on the task of machine translation\n(MT) remains underexplored. In this work, we explore the benefits of the\ngeneration of intermediate tokens when performing MT across multiple language\npairs of different levels of resourcedness and multiple setups. We find that\n\"thinking tokens\" do not help LRMs better perform MT. This result generalizes\nto models fine-tuned to reason before translating using distilled chain of\nthought (CoT) inspired by human translators' practices. Specifically,\nfine-tuning a model with synthetic CoT explanations detailing how to translate\nstep-by-step does not outperform standard input-output fine-tuning. However,\nconstructing the intermediate tokens by combining the outputs of modular\ntranslation-specific prompting strategies results in improvements. Our findings\nunderscore that the contribution of intermediate tokens during fine-tuning\nhighly depends on the presence of translation attempts within them. More\nbroadly, our results suggest that using a teacher to refine target translations\nor to expand parallel corpora is more impactful than distilling their CoT\nexplanations into \"thinking\" MT models.",
        "url": "http://arxiv.org/abs/2510.11919v1",
        "pdf_url": "http://arxiv.org/pdf/2510.11919v1",
        "arxiv_id": "2510.11919v1",
        "authors": [
            "Armel Zebaze",
            "Rachel Bawden",
            "BenoÃ®t Sagot"
        ],
        "submitted": "2025-10-13 20:41:01",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'query' (score: +3)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on machine translation and the use of large reasoning models, which is outside your primary area of interest in Information Retrieval and Search technologies. While it touches on Natural Language Processing, the specific application and methodology are not aligned with your core research themes."
    },
    {
        "title": "Cost Analysis of Human-corrected Transcription for Predominately Oral Languages",
        "abstract": "Creating speech datasets for low-resource languages is a critical yet poorly\nunderstood challenge, particularly regarding the actual cost in human labor.\nThis paper investigates the time and complexity required to produce\nhigh-quality annotated speech data for a subset of low-resource languages, low\nliteracy Predominately Oral Languages, focusing on Bambara, a Manding language\nof Mali. Through a one-month field study involving ten transcribers with native\nproficiency, we analyze the correction of ASR-generated transcriptions of 53\nhours of Bambara voice data. We report that it takes, on average, 30 hours of\nhuman labor to accurately transcribe one hour of speech data under laboratory\nconditions and 36 hours under field conditions. The study provides a baseline\nand practical insights for a large class of languages with comparable profiles\nundertaking the creation of NLP resources.",
        "url": "http://arxiv.org/abs/2510.12781v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12781v1",
        "arxiv_id": "2510.12781v1",
        "authors": [
            "Yacouba Diarra",
            "Nouhoum Souleymane Coulibaly",
            "Michael Leventhal"
        ],
        "submitted": "2025-10-14 17:53:11",
        "source": "arxiv",
        "comment": "6 pages, 1 figure",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it touches on NLP resources, its focus on transcription costs for low-resource languages is not a central match for your core research themes."
    },
    {
        "title": "Hey, wait a minute: on at-issue sensitivity in Language Models",
        "abstract": "Evaluating the naturalness of dialogue in language models (LMs) is not\ntrivial: notions of 'naturalness' vary, and scalable quantitative metrics\nremain limited. This study leverages the linguistic notion of 'at-issueness' to\nassess dialogue naturalness and introduces a new method: Divide, Generate,\nRecombine, and Compare (DGRC). DGRC (i) divides a dialogue as a prompt, (ii)\ngenerates continuations for subparts using LMs, (iii) recombines the dialogue\nand continuations, and (iv) compares the likelihoods of the recombined\nsequences. This approach mitigates bias in linguistic analyses of LMs and\nenables systematic testing of discourse-sensitive behavior. Applying DGRC, we\nfind that LMs prefer to continue dialogue on at-issue content, with this effect\nenhanced in instruct-tuned models. They also reduce their at-issue preference\nwhen relevant cues (e.g., \"Hey, wait a minute\") are present. Although\ninstruct-tuning does not further amplify this modulation, the pattern reflects\na hallmark of successful dialogue dynamics.",
        "url": "http://arxiv.org/abs/2510.12740v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12740v1",
        "arxiv_id": "2510.12740v1",
        "authors": [
            "Sanghee J. Kim",
            "Kanishka Misra"
        ],
        "submitted": "2025-10-14 17:17:20",
        "source": "arxiv",
        "comment": "10 pages, 5 figures, 3 tables. See\n  https://github.com/sangheek16/hey-wait-a-minute for code and data",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on evaluating the naturalness of dialogue in language models, introducing a new method called DGRC. While it touches on aspects of language understanding, it does not directly relate to query understanding, ranking models, or user behavior modeling in the context of information retrieval, which are core areas of your research interests."
    },
    {
        "title": "Who is a Better Matchmaker? Human vs. Algorithmic Judge Assignment in a High-Stakes Startup Competition",
        "abstract": "There is growing interest in applying artificial intelligence (AI) to\nautomate and support complex decision-making tasks. However, it remains unclear\nhow algorithms compare to human judgment in contexts requiring semantic\nunderstanding and domain expertise. We examine this in the context of the judge\nassignment problem, matching submissions to suitably qualified judges.\nSpecifically, we tackled this problem at the Harvard President's Innovation\nChallenge, the university's premier venture competition awarding over \\$500,000\nto student and alumni startups. This represents a real-world environment where\nhigh-quality judge assignment is essential. We developed an AI-based\njudge-assignment algorithm, Hybrid Lexical-Semantic Similarity Ensemble (HLSE),\nand deployed it at the competition. We then evaluated its performance against\nhuman expert assignments using blinded match-quality scores from judges on\n$309$ judge-venture pairs. Using a Mann-Whitney U statistic based test, we\nfound no statistically significant difference in assignment quality between the\ntwo approaches ($AUC=0.48, p=0.40$); on average, algorithmic matches are rated\n$3.90$ and manual matches $3.94$ on a 5-point scale, where 5 indicates an\nexcellent match. Furthermore, manual assignments that previously required a\nfull week could be automated in several hours by the algorithm during\ndeployment. These results demonstrate that HLSE achieves human-expert-level\nmatching quality while offering greater scalability and efficiency,\nunderscoring the potential of AI-driven solutions to support and enhance human\ndecision-making for judge assignment in high-stakes settings.",
        "url": "http://arxiv.org/abs/2510.12692v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12692v1",
        "arxiv_id": "2510.12692v1",
        "authors": [
            "Sarina Xi",
            "Orelia Pi",
            "Miaomiao Zhang",
            "Becca Xiong",
            "Jacqueline Ng Lane",
            "Nihar B. Shah"
        ],
        "submitted": "2025-10-14 16:25:09",
        "source": "arxiv",
        "comment": "17 Pages, 2 figures",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the application of AI in a high-stakes decision-making task, specifically judge assignment in a startup competition. While it touches on semantic understanding and domain expertise, its focus is on algorithmic vs. human performance rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest for your research."
    },
    {
        "title": "Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space",
        "abstract": "Multimodal reasoning aims to enhance the capabilities of MLLMs by\nincorporating intermediate reasoning steps before reaching the final answer. It\nhas evolved from text-only reasoning to the integration of visual information,\nenabling the thought process to be conveyed through both images and text.\nDespite its effectiveness, current multimodal reasoning methods depend on\nexplicit reasoning steps that require labor-intensive vision-text annotations\nand inherently introduce significant inference latency. To address these\nissues, we introduce multimodal latent reasoning with the advantages of\nmultimodal representation, reduced annotation, and inference efficiency. To\nfacilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR),\nwhich injects both visual and textual information in the reasoning process\nwithin the latent space. Specifically, IVT-LR represents each reasoning step by\ncombining two implicit parts: latent text (the hidden states from the previous\nstep) and latent vision (a set of selected image embeddings). We further\nintroduce a progressive multi-stage training strategy to enable MLLMs to\nperform the above multimodal latent reasoning steps. Experiments on M3CoT and\nScienceQA demonstrate that our IVT-LR method achieves an average performance\nincrease of 5.45% in accuracy, while simultaneously achieving a speed increase\nof over 5 times compared to existing approaches. Code available at\nhttps://github.com/FYYDCC/IVT-LR.",
        "url": "http://arxiv.org/abs/2510.12603v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12603v1",
        "arxiv_id": "2510.12603v1",
        "authors": [
            "Chao Chen",
            "Zhixin Ma",
            "Yongqi Li",
            "Yupeng Hu",
            "Yinwei Wei",
            "Wenjie Li",
            "Liqiang Nie"
        ],
        "submitted": "2025-10-14 14:58:25",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores multimodal reasoning in latent space, which is somewhat related to information retrieval and NLP. However, the focus on vision-text reasoning and multimodal representation is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling. While the paper's emphasis on real-time relevance optimization is relevant, the connection is not strong enough to warrant a higher score."
    },
    {
        "title": "BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach Annotation Disagreements (Yet)",
        "abstract": "Test-time scaling is a family of techniques to improve LLM outputs at\ninference time by performing extra computation. To the best of our knowledge,\ntest-time scaling has been limited to domains with verifiably correct answers,\nlike mathematics and coding. We transfer test-time scaling to the LeWiDi-2025\ntasks to evaluate annotation disagreements. We experiment with three test-time\nscaling methods: two benchmark algorithms (Model Averaging and Majority\nVoting), and a Best-of-N sampling method. The two benchmark methods improve LLM\nperformance consistently on the LeWiDi tasks, but the Best-of-N method does\nnot. Our experiments suggest that the Best-of-N method does not currently\ntransfer from mathematics to LeWiDi tasks, and we analyze potential reasons for\nthis gap.",
        "url": "http://arxiv.org/abs/2510.12516v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12516v1",
        "arxiv_id": "2510.12516v1",
        "authors": [
            "Tomas Ruiz",
            "Siyao Peng",
            "Barbara Plank",
            "Carsten Schwemmer"
        ],
        "submitted": "2025-10-14 13:43:08",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be focused on test-time scaling techniques for Large Language Models (LLMs), which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on LLMs, the context and application domain seem unrelated to the user's e-commerce background and interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey",
        "abstract": "This review surveys the rapid evolution of Meta AI's LLaMA (Large Language\nModel Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized\nparameter-efficient fine-tuning (PEFT) methods developed for these models. We\nfirst describe the LLaMA family of foundation models (7B-65B to 288B\nparameters), their architectures (including native multimodal and\nMixtureof-Experts variants), and key performance characteristics. We then\ndescribe and discuss the concept of PEFT, which adapts large pre-trained models\nby updating only a small subset of parameters, and review five PEFT methods\nthat have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1\nand V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method's\nmechanism, parameter savings, and example application to LLaMA (e.g.,\ninstruction tuning, multimodal tasks). We provide structured discussion and\nanalysis of model and adapter architectures, parameter counts, and benchmark\nresults (including examples where fine-tuned LLaMA models outperform larger\nbaselines). Finally, we examine real-world use cases where LLaMA-based models\nand PEFT have been successfully applied (e.g., legal and medical domains), and\nwe discuss ongoing challenges and future research directions (such as scaling\nto even larger contexts and improving robustness). This survey paper provides a\none-stop resource for ML researchers and practitioners interested in LLaMA\nmodels and efficient fine-tuning strategies.",
        "url": "http://arxiv.org/abs/2510.12178v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12178v1",
        "arxiv_id": "2510.12178v1",
        "authors": [
            "Abdulhady Abas Abdullah",
            "Arkaitz Zubiaga",
            "Seyedali Mirjalili",
            "Amir H. Gandomi",
            "Fatemeh Daneshfar",
            "Mohammadsadra Amini",
            "Alan Salam Mohammed",
            "Hadi Veisi"
        ],
        "submitted": "2025-10-14 06:12:44",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper is somewhat related to the user's interests in Natural Language Processing (NLP) and large language models, but it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The focus on parameter-efficient fine-tuning of large language models is relevant, but the context is more aligned with NLP and ML research rather than information retrieval."
    },
    {
        "title": "Tracing Multilingual Knowledge Acquisition Dynamics in Domain Adaptation: A Case Study of English-Japanese Biomedical Adaptation",
        "abstract": "Multilingual domain adaptation (ML-DA) is widely used to learn new domain\nknowledge across languages into large language models (LLMs). Although many\nmethods have been proposed to improve domain adaptation, the mechanisms of\nmultilingual knowledge acquisition, how domain knowledge is learned within a\nlanguage and transferred across languages, remain underexplored. This gap leads\nto suboptimal performance, particularly in low-resource settings. This work\nexamines the learning dynamics of LLMs during ML-DA. Because prior ML-DA\nstudies often train and evaluate on datasets with mismatched knowledge\ncoverage, we propose AdaXEval, an adaptive evaluation method that builds\nmultiple-choice QA datasets from the same bilingual domain corpus used for\ntraining, thereby directly studying multilingual knowledge acquisition. Through\ncontinual training of LLMs with diverse data recipes, we track how LLMs acquire\ndomain facts and pinpoint the mechanism behind the transformation process from\ndomain training data to knowledge. Our experiments on a 13B English-Japanese\nbilingual LLM reveal that cross-lingual transfer remains challenging despite a\nhigh-quality bilingual corpus. The code has been released.",
        "url": "http://arxiv.org/abs/2510.12115v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12115v1",
        "arxiv_id": "2510.12115v1",
        "authors": [
            "Xin Zhao",
            "Naoki Yoshinaga",
            "Yuma Tsuta",
            "Akiko Aizawa"
        ],
        "submitted": "2025-10-14 03:34:17",
        "source": "arxiv",
        "comment": "22 Pages, Submitted to ARR 2025 Oct",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on multilingual domain adaptation and large language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on knowledge acquisition and transfer, the context is domain adaptation and bilingual language models, which is somewhat tangential to your primary focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Improving Text-to-Image Generation with Input-Side Inference-Time Scaling",
        "abstract": "Recent advances in text-to-image (T2I) generation have achieved impressive\nresults, yet existing models often struggle with simple or underspecified\nprompts, leading to suboptimal image-text alignment, aesthetics, and quality.\nWe propose a prompt rewriting framework that leverages large language models\n(LLMs) to refine user inputs before feeding them into T2I backbones. Our\napproach introduces a carefully designed reward system and an iterative direct\npreference optimization (DPO) training pipeline, enabling the rewriter to\nenhance prompts without requiring supervised fine-tuning data. We evaluate our\nmethod across diverse T2I models and benchmarks. Results show that our prompt\nrewriter consistently improves image-text alignment, visual quality, and\naesthetics, outperforming strong baselines. Furthermore, we demonstrate strong\ntransferability by showing that a prompt rewriter trained on one T2I backbone\ngeneralizes effectively to others without needing to be retrained. We also\nsystematically study scalability, evaluating how performance gains scale with\nthe capacity of the large LLM used as the rewriter. These findings highlight\nthat prompt rewriting is an effective, scalable, and practical model-agnostic\nstrategy for improving T2I systems. We plan to release the code and trained\nprompt rewriters soon.",
        "url": "http://arxiv.org/abs/2510.12041v2",
        "pdf_url": "http://arxiv.org/pdf/2510.12041v2",
        "arxiv_id": "2510.12041v2",
        "authors": [
            "Ruibo Chen",
            "Jiacheng Pan",
            "Heng Huang",
            "Zhenheng Yang"
        ],
        "submitted": "2025-10-14 00:51:39",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves text processing, the focus is on text-to-image generation, which is outside your primary areas of interest."
    },
    {
        "title": "SAGE: A Top-Down Bottom-Up Knowledge-Grounded User Simulator for Multi-turn AGent Evaluation",
        "abstract": "Evaluating multi-turn interactive agents is challenging due to the need for\nhuman assessment. Evaluation with simulated users has been introduced as an\nalternative, however existing approaches typically model generic users and\noverlook the domain-specific principles required to capture realistic behavior.\nWe propose SAGE, a novel user Simulation framework for multi-turn AGent\nEvaluation that integrates knowledge from business contexts. SAGE incorporates\ntop-down knowledge rooted in business logic, such as ideal customer profiles,\ngrounding user behavior in realistic customer personas. We further integrate\nbottom-up knowledge taken from business agent infrastructure (e.g., product\ncatalogs, FAQs, and knowledge bases), allowing the simulator to generate\ninteractions that reflect users' information needs and expectations in a\ncompany's target market. Through empirical evaluation, we find that this\napproach produces interactions that are more realistic and diverse, while also\nidentifying up to 33% more agent errors, highlighting its effectiveness as an\nevaluation tool to support bug-finding and iterative agent improvement.",
        "url": "http://arxiv.org/abs/2510.11997v1",
        "pdf_url": "http://arxiv.org/pdf/2510.11997v1",
        "arxiv_id": "2510.11997v1",
        "authors": [
            "Ryan Shea",
            "Yunan Lu",
            "Liang Qiu",
            "Zhou Yu"
        ],
        "submitted": "2025-10-13 22:52:17",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'user behavior' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to information retrieval, but its focus on user simulation for multi-turn agent evaluation and business contexts is not directly aligned with your core research themes. While it touches on user behavior modeling, it's more focused on agent evaluation and business logic, which may be of interest in a broader sense, but not a central match for your research interests."
    },
    {
        "title": "R-WoM: Retrieval-augmented World Model For Computer-use Agents",
        "abstract": "Large Language Models (LLMs) can serve as world models to enhance agent\ndecision-making in digital environments by simulating future states and\npredicting action outcomes, potentially eliminating costly trial-and-error\nexploration. However, this capability is fundamentally limited by LLMs'\ntendency toward hallucination and their reliance on static training knowledge,\nwhich can lead to compounding errors that inhibit long-horizon simulations. To\nsystematically investigate whether LLMs are appropriate for world modeling, we\nprobe two core capabilities of world models--future state prediction and reward\nestimation--through three tasks: next-state identification, full-procedure\nplanning alignment, and milestone transition recognition. Our analysis shows\nthat while LLMs effectively capture immediate next states and identify\nmeaningful state transitions, their performance rapidly degrades in\nfull-procedure planning. This highlights LLMs' limitations in reliably modeling\nenvironment dynamics over long horizons. To address these limitations, we\npropose the Retrieval-augmented World Model (R-WoM), which grounds LLM\nsimulations by incorporating factual, up-to-date knowledge retrieved from\nexternal tutorials. Experiments show that R-WoM achieves substantial\nimprovements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to\nbaselines, with particular advantages in longer-horizon simulations.",
        "url": "http://arxiv.org/abs/2510.11892v1",
        "pdf_url": "http://arxiv.org/pdf/2510.11892v1",
        "arxiv_id": "2510.11892v1",
        "authors": [
            "Kai Mei",
            "Jiang Guo",
            "Shuaichen Chang",
            "Mingwen Dong",
            "Dongkyu Lee",
            "Xing Niu",
            "Jiarong Jiang"
        ],
        "submitted": "2025-10-13 19:52:04",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses the limitations of Large Language Models (LLMs) in world modeling and proposes a retrieval-augmented approach to improve their performance. While it touches on the concept of simulation and prediction, which are related to ranking models, it does not directly address query understanding, ranking models, or user behavior modeling. The focus on world modeling and agent decision-making is somewhat relevant to information retrieval, but not a central match."
    },
    {
        "title": "PHANTOM RECALL: When Familiar Puzzles Fool Smart Models",
        "abstract": "Large language models (LLMs) such as GPT, Gemini, and Claude often appear\nadept at solving classic logic puzzles--but how much genuine reasoning\nunderlies their answers? Recent evidence suggests that these models frequently\nrely on memorized templates rather than reasoning from first principles. When\npuzzles are slightly modified, their performance collapses, revealing a\nstriking fragility. In particular, we asked: Have LLMs addressed these issues?\nTo what extent? How about perturbations to other puzzles? Is there a general\nway of reformulating the prompt so that the models do better? To examine these\nthings systematically, we introduce PHANTOM RECALL, a benchmark comprising 25\nwell-known logic puzzles and 149 carefully designed perturbations that preserve\nreasoning structure but alter superficial details and solutions. We evaluate\neleven leading LLMs and identify a recurring failure mode--phantom\nrecall--where models confidently reproduce memorized solutions or spurious\nrationales that no longer fit the altered scenario. To probe and mitigate this\nissue, we contribute three tools: (i) an automated logical-equivalence judge to\ndetect reasoning mismatches, (ii) a taxonomy of fine-grained reasoning error\ncategories, and (iii) a prompting-based mitigation framework guided by these\ncategories. Despite near-perfect accuracy on unmodified puzzles, models\nsignificantly underperform humans on perturbed ones, exhibiting both phantom\nrecall and over-elaboration. Our findings reveal a crucial limitation: LLMs\noften fail to re-reason when contextual cues shift--highlighting the gap\nbetween linguistic fluency and logical understanding.",
        "url": "http://arxiv.org/abs/2510.11812v1",
        "pdf_url": "http://arxiv.org/pdf/2510.11812v1",
        "arxiv_id": "2510.11812v1",
        "authors": [
            "Souradeep Mukhopadhyay",
            "Rishabh Baral",
            "Nimeesh Mahajan",
            "Samhitha Harish",
            "Aswin RRV",
            "Mihir Parmar",
            "Mutsumi Nakamura",
            "Chitta Baral"
        ],
        "submitted": "2025-10-13 18:09:50",
        "source": "arxiv",
        "comment": "22 Pages",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. It focuses on the limitations of large language models in solving logic puzzles, which is more relevant to the field of Artificial Intelligence and Cognitive Science."
    },
    {
        "title": "Content Anonymization for Privacy in Long-form Audio",
        "abstract": "Voice anonymization techniques have been found to successfully obscure a\nspeaker's acoustic identity in short, isolated utterances in benchmarks such as\nthe VoicePrivacy Challenge. In practice, however, utterances seldom occur in\nisolation: long-form audio is commonplace in domains such as interviews, phone\ncalls, and meetings. In these cases, many utterances from the same speaker are\navailable, which pose a significantly greater privacy risk: given multiple\nutterances from the same speaker, an attacker could exploit an individual's\nvocabulary, syntax, and turns of phrase to re-identify them, even when their\nvoice is completely disguised. To address this risk, we propose new content\nanonymization approaches. Our approach performs a contextual rewriting of the\ntranscripts in an ASR-TTS pipeline to eliminate speaker-specific style while\npreserving meaning. We present results in a long-form telephone conversation\nsetting demonstrating the effectiveness of a content-based attack on\nvoice-anonymized speech. Then we show how the proposed content-based\nanonymization methods can mitigate this risk while preserving speech utility.\nOverall, we find that paraphrasing is an effective defense against\ncontent-based attacks and recommend that stakeholders adopt this step to ensure\nanonymity in long-form audio.",
        "url": "http://arxiv.org/abs/2510.12780v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12780v1",
        "arxiv_id": "2510.12780v1",
        "authors": [
            "Cristina Aggazzotti",
            "Ashi Garg",
            "Zexin Cai",
            "Nicholas Andrews"
        ],
        "submitted": "2025-10-14 17:52:50",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing, as it focuses on voice anonymization and content-based attacks in long-form audio, which does not align with your core themes."
    },
    {
        "title": "Resource-sensitive but language-blind: Community size and not grammatical complexity better predicts the accuracy of Large Language Models in a novel Wug Test",
        "abstract": "The linguistic abilities of Large Language Models are a matter of ongoing\ndebate. This study contributes to this discussion by investigating model\nperformance in a morphological generalization task that involves novel words.\nUsing a multilingual adaptation of the Wug Test, six models were tested across\nfour partially unrelated languages (Catalan, English, Greek, and Spanish) and\ncompared with human speakers. The aim is to determine whether model accuracy\napproximates human competence and whether it is shaped primarily by linguistic\ncomplexity or by the quantity of available training data. Consistent with\nprevious research, the results show that the models are able to generalize\nmorphological processes to unseen words with human-like accuracy. However,\naccuracy patterns align more closely with community size and data availability\nthan with structural complexity, refining earlier claims in the literature. In\nparticular, languages with larger speaker communities and stronger digital\nrepresentation, such as Spanish and English, revealed higher accuracy than\nless-resourced ones like Catalan and Greek. Overall, our findings suggest that\nmodel behavior is mainly driven by the richness of linguistic resources rather\nthan by sensitivity to grammatical complexity, reflecting a form of performance\nthat resembles human linguistic competence only superficially.",
        "url": "http://arxiv.org/abs/2510.12463v1",
        "pdf_url": "http://arxiv.org/pdf/2510.12463v1",
        "arxiv_id": "2510.12463v1",
        "authors": [
            "Nikoleta Pantelidou",
            "Evelina Leivada",
            "Paolo Morosi"
        ],
        "submitted": "2025-10-14 12:52:57",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The study focuses on the performance of Large Language Models in a morphological generalization task, which is more related to NLP and linguistics, but does not align with your core research themes."
    }
]