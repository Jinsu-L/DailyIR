[
    {
        "title": "Unified Interactive Multimodal Moment Retrieval via Cascaded Embedding-Reranking and Temporal-Aware Score Fusion",
        "abstract": "The exponential growth of video content has created an urgent need for efficient multimodal moment retrieval systems. However, existing approaches face three critical challenges: (1) fixed-weight fusion strategies fail across cross modal noise and ambiguous queries, (2) temporal modeling struggles to capture coherent event sequences while penalizing unrealistic gaps, and (3) systems require manual modality selection, reducing usability. We propose a unified multimodal moment retrieval system with three key innovations. First, a cascaded dual-embedding pipeline combines BEIT-3 and SigLIP for broad retrieval, refined by BLIP-2 based reranking to balance recall and precision. Second, a temporal-aware scoring mechanism applies exponential decay penalties to large temporal gaps via beam search, constructing coherent event sequences rather than isolated frames. Third, Agent-guided query decomposition (GPT-4o) automatically interprets ambiguous queries, decomposes them into modality specific sub-queries (visual/OCR/ASR), and performs adaptive score fusion eliminating manual modality selection. Qualitative analysis demonstrates that our system effectively handles ambiguous queries, retrieves temporally coherent sequences, and dynamically adapts fusion strategies, advancing interactive moment search capabilities.",
        "url": "http://arxiv.org/abs/2512.12935v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12935v1",
        "arxiv_id": "2512.12935v1",
        "authors": [
            "Toan Le Ngo Thanh",
            "Phat Ha Huu",
            "Tan Nguyen Dang Duy",
            "Thong Nguyen Le Minh",
            "Anh Nguyen Nhu Tinh"
        ],
        "submitted": "2025-12-15 02:50:43",
        "source": "arxiv",
        "comment": "Accepted at AAAI Workshop 2026",
        "score": 16,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to information retrieval, but its focus on multimodal moment retrieval in video content and the use of specific models (e.g., BEIT-3, SigLIP, BLIP-2) does not directly align with the user's core research themes in query understanding, ranking models, and user behavior modeling. While it touches on aspects of real-time relevance optimization, the paper's primary contributions are in the area of multimodal retrieval, which is a related but distinct field."
    },
    {
        "title": "A Simple and Effective Framework for Symmetric Consistent Indexing in Large-Scale Dense Retrieval",
        "abstract": "Dense retrieval has become the industry standard in large-scale information retrieval systems due to its high efficiency and competitive accuracy. Its core relies on a coarse-to-fine hierarchical architecture that enables rapid candidate selection and precise semantic matching, achieving millisecond-level response over billion-scale corpora. This capability makes it essential not only in traditional search and recommendation scenarios but also in the emerging paradigm of generative recommendation driven by large language models, where semantic IDs-themselves a form of coarse-to-fine representation-play a foundational role. However, the widely adopted dual-tower encoding architecture introduces inherent challenges, primarily representational space misalignment and retrieval index inconsistency, which degrade matching accuracy, retrieval stability, and performance on long-tail queries. These issues are further magnified in semantic ID generation, ultimately limiting the performance ceiling of downstream generative models.\n  To address these challenges, this paper proposes a simple and effective framework named SCI comprising two synergistic modules: a symmetric representation alignment module that employs an innovative input-swapping mechanism to unify the dual-tower representation space without adding parameters, and an consistent indexing with dual-tower synergy module that redesigns retrieval paths using a dual-view indexing strategy to maintain consistency from training to inference. The framework is systematic, lightweight, and engineering-friendly, requiring minimal overhead while fully supporting billion-scale deployment. We provide theoretical guarantees for our approach, with its effectiveness validated by results across public datasets and real-world e-commerce datasets.",
        "url": "http://arxiv.org/abs/2512.13074v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13074v1",
        "arxiv_id": "2512.13074v1",
        "authors": [
            "Huimu Wang",
            "Yiming Qiu",
            "Xingzhi Yao",
            "Zhiguo Chen",
            "Guoyu Tang",
            "Songlin Wang",
            "Sulong Xu",
            "Mingming Li"
        ],
        "submitted": "2025-12-15 08:11:24",
        "source": "arxiv",
        "comment": null,
        "score": 15,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'dense retrieval' (score: +3)",
            "Found 'queries' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on dense retrieval and proposes a framework to address challenges in dual-tower encoding architecture. While it touches on semantic matching and retrieval, it doesn't directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's relevance to IR and NLP is somewhat tangential, but it may be of interest for those working on large-scale retrieval systems."
    },
    {
        "title": "Reveal Hidden Pitfalls and Navigate Next Generation of Vector Similarity Search from Task-Centric Views",
        "abstract": "Vector Similarity Search (VSS) in high-dimensional spaces is rapidly emerging as core functionality in next-generation database systems for numerous data-intensive services -- from embedding lookups in large language models (LLMs), to semantic information retrieval and recommendation engines. Current benchmarks, however, evaluate VSS primarily on the recall-latency trade-off against a ground truth defined solely by distance metrics, neglecting how retrieval quality ultimately impacts downstream tasks. This disconnect can mislead both academic research and industrial practice.\n  We present Iceberg, a holistic benchmark suite for end-to-end evaluation of VSS methods in realistic application contexts. From a task-centric view, Iceberg uncovers the Information Loss Funnel, which identifies three principal sources of end-to-end performance degradation: (1) Embedding Loss during feature extraction; (2) Metric Misuse, where distances poorly reflect task relevance; (3) Data Distribution Sensitivity, highlighting index robustness across skews and modalities. For a more comprehensive assessment, Iceberg spans eight diverse datasets across key domains such as image classification, face recognition, text retrieval, and recommendation systems. Each dataset, ranging from 1M to 100M vectors, includes rich, task-specific labels and evaluation metrics, enabling assessment of retrieval algorithms within the full application pipeline rather than in isolation. Iceberg benchmarks 13 state-of-the-art VSS methods and re-ranks them based on application-level metrics, revealing substantial deviations from traditional rankings derived purely from recall-latency evaluations. Building on these insights, we define a set of task-centric meta-features and derive an interpretable decision tree to guide practitioners in selecting and tuning VSS methods for their specific workloads.",
        "url": "http://arxiv.org/abs/2512.12980v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12980v1",
        "arxiv_id": "2512.12980v1",
        "authors": [
            "Tingyang Chen",
            "Cong Fu",
            "Jiahua Wu",
            "Haotian Wu",
            "Hua Fan",
            "Xiangyu Ke",
            "Yunjun Gao",
            "Yabo Ni",
            "Anxiang Zeng"
        ],
        "submitted": "2025-12-15 04:49:33",
        "source": "arxiv",
        "comment": "SIGMOD2026",
        "score": 14,
        "keyword_reasons": [
            "Found 'information retrieval' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'relevance' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 9,
        "llm_reason": "The paper focuses on vector similarity search, a core component of semantic IR and retrieval systems, and introduces a task‑centric benchmark that directly impacts ranking quality and downstream applications such as recommendation and text retrieval. Its emphasis on embeddings, metric relevance, and end‑to‑end evaluation aligns well with the user’s interest in deep semantic understanding and real‑time relevance optimization."
    },
    {
        "title": "Learning to Retrieve with Weakened Labels: Robust Training under Label Noise",
        "abstract": "Neural Encoders are frequently used in the NLP domain to perform dense retrieval tasks, for instance, to generate the candidate documents for a given query in question-answering tasks. However, sparse annotation and label noise in the training data make it challenging to train or fine-tune such retrieval models. Although existing works have attempted to mitigate these problems by incorporating modified loss functions or data cleaning, these approaches either require some hyperparameters to tune during training or add substantial complexity to the training setup. In this work, we consider a label weakening approach to generate robust retrieval models in the presence of label noise. Instead of enforcing a single, potentially erroneous label for each query document pair, we allow for a set of plausible labels derived from both the observed supervision and the model's confidence scores. We perform an extensive evaluation considering two retrieval models, one re-ranking model, considering four diverse ranking datasets. To this end, we also consider a realistic noisy setting by using a semantic-aware noise generation technique to generate different ratios of noise. Our initial results show that label weakening can improve the performance of the retrieval tasks in comparison to 10 different state-of-the-art loss functions.",
        "url": "http://arxiv.org/abs/2512.13237v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13237v1",
        "arxiv_id": "2512.13237v1",
        "authors": [
            "Arnab Sharma"
        ],
        "submitted": "2025-12-15 11:52:13",
        "source": "arxiv",
        "comment": null,
        "score": 12,
        "keyword_reasons": [
            "Found 'dense retrieval' (score: +3)",
            "Found 'query' (score: +3)",
            "Found 'ranking' (score: +3)",
            "Found 'retrieval' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "This paper is highly relevant to the field of Information Retrieval, particularly in the context of neural encoders and dense retrieval tasks. The focus on robust training under label noise is also aligned with the user's interest in query understanding and ranking models. While the paper's emphasis on NLP is a secondary match, its core themes are closely related to the user's primary research interests."
    },
    {
        "title": "No One Left Behind: How to Exploit the Incomplete and Skewed Multi-Label Data for Conversion Rate Prediction",
        "abstract": "In most real-world online advertising systems, advertisers typically have diverse customer acquisition goals. A common solution is to use multi-task learning (MTL) to train a unified model on post-click data to estimate the conversion rate (CVR) for these diverse targets. In practice, CVR prediction often encounters missing conversion data as many advertisers submit only a subset of user conversion actions due to privacy or other constraints, making the labels of multi-task data incomplete. If the model is trained on all available samples where advertisers submit user conversion actions, it may struggle when deployed to serve a subset of advertisers targeting specific conversion actions, as the training and deployment data distributions are mismatched. While considerable MTL efforts have been made, a long-standing challenge is how to effectively train a unified model with the incomplete and skewed multi-label data. In this paper, we propose a fine-grained Knowledge transfer framework for Asymmetric Multi-Label data (KAML). We introduce an attribution-driven masking strategy (ADM) to better utilize data with asymmetric multi-label data in training. However, the more relaxed masking in ADM is a double-edged sword: it provides additional training signals but also introduces noise due to skewed data. To address this, we propose a hierarchical knowledge extraction mechanism (HKE) to model the sample discrepancy within the target task tower. Finally, to maximize the utility of unlabeled samples, we incorporate ranking loss strategy to further enhance our model. The effectiveness of KAML has been demonstrated through comprehensive evaluations on offline industry datasets and online A/B tests, which show significant performance improvements over existing MTL baselines.",
        "url": "http://arxiv.org/abs/2512.13300v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13300v1",
        "arxiv_id": "2512.13300v1",
        "authors": [
            "Qinglin Jia",
            "Zhaocheng Du",
            "Chuhan Wu",
            "Huifeng Guo",
            "Ruiming Tang",
            "Shuting Shi",
            "Muyu Zhang"
        ],
        "submitted": "2025-12-15 13:14:20",
        "source": "arxiv",
        "comment": null,
        "score": 10,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'click' (score: +2)",
            "Found 'cvr' (score: +2)",
            "Found 'conversion rate' (score: +2)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "The paper focuses on multi‑task learning for conversion‑rate prediction in advertising, which is only tangentially related to ranking models. It does not address query understanding, search ranking, or click‑through modeling in an IR context, making it only loosely relevant to the user’s core interests."
    },
    {
        "title": "Authors Should Annotate",
        "abstract": "The status quo for labeling text is third-party annotation, but there are many cases where information directly from the document's source would be preferable over a third-person proxy, especially for egocentric features like sentiment and belief. We introduce author labeling, an annotation technique where the writer of the document itself annotates the data at the moment of creation. We collaborate with a commercial chatbot with over 10,000 users to deploy an author labeling annotation system for subjective features related to product recommendation. This system identifies task-relevant queries, generates on-the-fly labeling questions, and records authors' answers in real time. We train and deploy an online-learning model architecture for product recommendation that continuously improves from author labeling and find it achieved a 534% increase in click-through rate compared to an industry advertising baseline running concurrently. We then compare the quality and practicality of author labeling to three traditional annotation approaches for sentiment analysis and find author labeling to be higher quality, faster to acquire, and cheaper. These findings reinforce existing literature that annotations, especially for egocentric and subjective beliefs, are significantly higher quality when labeled by the author rather than a third party. To facilitate broader scientific adoption, we release an author labeling service for the research community at academic.echollm.io.",
        "url": "http://arxiv.org/abs/2512.12976v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12976v1",
        "arxiv_id": "2512.12976v1",
        "authors": [
            "Marcus Ma",
            "Cole Johnson",
            "Nolan Bridges",
            "Jackson Trager",
            "Georgios Chochlakis",
            "Shrikanth Narayanan"
        ],
        "submitted": "2025-12-15 04:45:09",
        "source": "arxiv",
        "comment": null,
        "score": 9,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'click' (score: +2)",
            "Found 'click-through rate' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper discusses author labeling for text annotation, which is somewhat related to query understanding and user behavior modeling in Information Retrieval. However, the focus on annotation quality and practicality for sentiment analysis and product recommendation is not directly aligned with the user's core research themes in IR and Search technologies."
    },
    {
        "title": "Automated Information Flow Selection for Multi-scenario Multi-task Recommendation",
        "abstract": "Multi-scenario multi-task recommendation (MSMTR) systems must address recommendation demands across diverse scenarios while simultaneously optimizing multiple objectives, such as click-through rate and conversion rate. Existing MSMTR models typically consist of four information units: scenario-shared, scenario-specific, task-shared, and task-specific networks. These units interact to generate four types of relationship information flows, directed from scenario-shared or scenario-specific networks to task-shared or task-specific networks. However, these models face two main limitations: 1) They often rely on complex architectures, such as mixture-of-experts (MoE) networks, which increase the complexity of information fusion, model size, and training cost. 2) They extract all available information flows without filtering out irrelevant or even harmful content, introducing potential noise. Regarding these challenges, we propose a lightweight Automated Information Flow Selection (AutoIFS) framework for MSMTR. To tackle the first issue, AutoIFS incorporates low-rank adaptation (LoRA) to decouple the four information units, enabling more flexible and efficient information fusion with minimal parameter overhead. To address the second issue, AutoIFS introduces an information flow selection network that automatically filters out invalid scenario-task information flows based on model performance feedback. It employs a simple yet effective pruning function to eliminate useless information flows, thereby enhancing the impact of key relationships and improving model performance. Finally, we evaluate AutoIFS and confirm its effectiveness through extensive experiments on two public benchmark datasets and an online A/B test.",
        "url": "http://arxiv.org/abs/2512.13396v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13396v1",
        "arxiv_id": "2512.13396v1",
        "authors": [
            "Chaohua Yang",
            "Dugang Liu",
            "Shiwei Li",
            "Yuwen Fu",
            "Xing Tang",
            "Weihong Luo",
            "Xiangyu Zhao",
            "Xiuqiang He",
            "Zhong Ming"
        ],
        "submitted": "2025-12-15 14:48:59",
        "source": "arxiv",
        "comment": "10 Pages, 6 Figures, WSDM 2026 Accepted",
        "score": 8,
        "keyword_reasons": [
            "Found 'click' (score: +2)",
            "Found 'click-through rate' (score: +2)",
            "Found 'conversion rate' (score: +2)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 3,
        "llm_reason": "The paper focuses on multi-scenario multi-task recommendation, which is somewhat related to information retrieval. However, the emphasis is on recommender systems and the proposed framework, AutoIFS, is designed to address specific challenges in this area, rather than deep semantic understanding or real-time relevance optimization, which are core aspects of your research interests."
    },
    {
        "title": "Progressive Refinement of E-commerce Search Ranking Based on Short-Term Activities of the Buyer",
        "abstract": "In e-commerce shopping, aligning search results with a buyer's immediate needs and preferences presents a significant challenge, particularly in adapting search results throughout the buyer's shopping journey as they move from the initial stages of browsing to making a purchase decision or shift from one intent to another. This study presents a systematic approach to adapting e-commerce search results based on the current context. We start with basic methods and incrementally incorporate more contextual information and state-of-the-art techniques to improve the search outcomes. By applying this evolving contextual framework to items displayed on the search engine results page (SERP), we progressively align search outcomes more closely with the buyer's interests and current search intentions. Our findings demonstrate that this incremental enhancement, from simple heuristic autoregressive features to advanced sequence models, significantly improves ranker performance. The integration of contextual techniques enhances the performance of our production ranker, leading to improved search results in both offline and online A/B testing in terms of Mean Reciprocal Rank (MRR). Overall, the paper details iterative methodologies and their substantial contributions to search result contextualization on e-commerce platforms.",
        "url": "http://arxiv.org/abs/2512.13037v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13037v1",
        "arxiv_id": "2512.13037v1",
        "authors": [
            "Taoran Sheng",
            "Sathappan Muthiah",
            "Atiq Islam",
            "Jinming Feng"
        ],
        "submitted": "2025-12-15 07:07:32",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'shopping' (score: +1)",
            "Found 'commerce' (score: +1)",
            "Found 'e-commerce' (score: +1)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 8,
        "llm_reason": "The paper focuses on e-commerce search ranking, which aligns with your background and interests in IR and search technologies. The use of contextual information and sequence models to improve search outcomes is also relevant to your work on query understanding and ranking models."
    },
    {
        "title": "Are Large Language Models Really Effective for Training-Free Cold-Start Recommendation?",
        "abstract": "Recommender systems usually rely on large-scale interaction data to learn from users' past behaviors and make accurate predictions. However, real-world applications often face situations where no training data is available, such as when launching new services or handling entirely new users. In such cases, conventional approaches cannot be applied. This study focuses on training-free recommendation, where no task-specific training is performed, and particularly on \\textit{training-free cold-start recommendation} (TFCSR), the more challenging case where the target user has no interactions. Large language models (LLMs) have recently been explored as a promising solution, and numerous studies have been proposed. As the ability of text embedding models (TEMs) increases, they are increasingly recognized as applicable to training-free recommendation, but no prior work has directly compared LLMs and TEMs under identical conditions. We present the first controlled experiments that systematically evaluate these two approaches in the same setting. The results show that TEMs outperform LLM rerankers, and this trend holds not only in cold-start settings but also in warm-start settings with rich interactions. These findings indicate that direct LLM ranking is not the only viable option, contrary to the commonly shared belief, and TEM-based approaches provide a stronger and more scalable basis for training-free recommendation.",
        "url": "http://arxiv.org/abs/2512.13001v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13001v1",
        "arxiv_id": "2512.13001v1",
        "authors": [
            "Genki Kusano",
            "Kenya Abe",
            "Kunihiro Takeoka"
        ],
        "submitted": "2025-12-15 05:47:07",
        "source": "arxiv",
        "comment": null,
        "score": 8,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rerank' (score: +3)",
            "Found 'recommend' (score: +1)",
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores recommender systems, a related topic to Information Retrieval, but its focus on training-free cold-start recommendation and the comparison of large language models and text embedding models is not directly aligned with the user's core research themes of query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Memory in the Age of AI Agents",
        "abstract": "Memory has emerged, and will continue to remain, a core capability of foundation model-based agents. As research on agent memory rapidly expands and attracts unprecedented attention, the field has also become increasingly fragmented. Existing works that fall under the umbrella of agent memory often differ substantially in their motivations, implementations, and evaluation protocols, while the proliferation of loosely defined memory terminologies has further obscured conceptual clarity. Traditional taxonomies such as long/short-term memory have proven insufficient to capture the diversity of contemporary agent memory systems. This work aims to provide an up-to-date landscape of current agent memory research. We begin by clearly delineating the scope of agent memory and distinguishing it from related concepts such as LLM memory, retrieval augmented generation (RAG), and context engineering. We then examine agent memory through the unified lenses of forms, functions, and dynamics. From the perspective of forms, we identify three dominant realizations of agent memory, namely token-level, parametric, and latent memory. From the perspective of functions, we propose a finer-grained taxonomy that distinguishes factual, experiential, and working memory. From the perspective of dynamics, we analyze how memory is formed, evolved, and retrieved over time. To support practical development, we compile a comprehensive summary of memory benchmarks and open-source frameworks. Beyond consolidation, we articulate a forward-looking perspective on emerging research frontiers, including memory automation, reinforcement learning integration, multimodal memory, multi-agent memory, and trustworthiness issues. We hope this survey serves not only as a reference for existing work, but also as a conceptual foundation for rethinking memory as a first-class primitive in the design of future agentic intelligence.",
        "url": "http://arxiv.org/abs/2512.13564v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13564v1",
        "arxiv_id": "2512.13564v1",
        "authors": [
            "Yuyang Hu",
            "Shichun Liu",
            "Yanwei Yue",
            "Guibin Zhang",
            "Boyang Liu",
            "Fangyi Zhu",
            "Jiahang Lin",
            "Honglin Guo",
            "Shihan Dou",
            "Zhiheng Xi",
            "Senjie Jin",
            "Jiejun Tan",
            "Yanbin Yin",
            "Jiongnan Liu",
            "Zeyu Zhang",
            "Zhongxiang Sun",
            "Yutao Zhu",
            "Hao Sun",
            "Boci Peng",
            "Zhenrong Cheng",
            "Xuanbo Fan",
            "Jiaxin Guo",
            "Xinlei Yu",
            "Zhenhong Zhou",
            "Zewen Hu",
            "Jiahao Huo",
            "Junhao Wang",
            "Yuwei Niu",
            "Yu Wang",
            "Zhenfei Yin",
            "Xiaobin Hu",
            "Yue Liao",
            "Qiankun Li",
            "Kun Wang",
            "Wangchunshu Zhou",
            "Yixin Liu",
            "Dawei Cheng",
            "Qi Zhang",
            "Tao Gui",
            "Shirui Pan",
            "Yan Zhang",
            "Philip Torr",
            "Zhicheng Dou",
            "Ji-Rong Wen",
            "Xuanjing Huang",
            "Yu-Gang Jiang",
            "Shuicheng Yan"
        ],
        "submitted": "2025-12-15 17:22:34",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval augmented generation' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to the user's interests in Information Retrieval and Natural Language Processing, as it discusses agent memory and its applications in AI agents. However, the focus on agent memory and its forms, functions, and dynamics is not directly aligned with the user's core research themes in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "AutoTool: Dynamic Tool Selection and Integration for Agentic Reasoning",
        "abstract": "Agentic reinforcement learning has advanced large language models (LLMs) to reason through long chain-of-thought trajectories while interleaving external tool use. Existing approaches assume a fixed inventory of tools, limiting LLM agents' adaptability to new or evolving toolsets. We present AutoTool, a framework that equips LLM agents with dynamic tool-selection capabilities throughout their reasoning trajectories. We first construct a 200k dataset with explicit tool-selection rationales across 1,000+ tools and 100+ tasks spanning mathematics, science, code generation, and multimodal reasoning. Building on this data foundation, AutoTool employs a dual-phase optimization pipeline: (i) supervised and RL-based trajectory stabilization for coherent reasoning, and (ii) KL-regularized Plackett-Luce ranking to refine consistent multi-step tool selection. Across ten diverse benchmarks, we train two base models, Qwen3-8B and Qwen2.5-VL-7B, with AutoTool. With fewer parameters, AutoTool consistently outperforms advanced LLM agents and tool-integration methods, yielding average gains of 6.4% in math & science reasoning, 4.5% in search-based QA, 7.7% in code generation, and 6.9% in multimodal understanding. In addition, AutoTool exhibits stronger generalization by dynamically leveraging unseen tools from evolving toolsets during inference.",
        "url": "http://arxiv.org/abs/2512.13278v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13278v1",
        "arxiv_id": "2512.13278v1",
        "authors": [
            "Jiaru Zou",
            "Ling Yang",
            "Yunzhe Qi",
            "Sirui Chen",
            "Mengting Ai",
            "Ke Shen",
            "Jingrui He",
            "Mengdi Wang"
        ],
        "submitted": "2025-12-15 12:38:04",
        "source": "arxiv",
        "comment": "Best Paper Award at ICCV 2025 Workshop on Multi-Modal Reasoning for Agentic Intelligence",
        "score": 7,
        "keyword_reasons": [
            "Found 'ranking' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'rank' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on agentic reasoning and tool selection for large language models, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves ranking and optimization, the context is different from the user's core research themes."
    },
    {
        "title": "An Open and Reproducible Deep Research Agent for Long-Form Question Answering",
        "abstract": "We present an open deep research system for long-form question answering, selected as a winning system in the text-to-text track of the MMU-RAG competition at NeurIPS 2025. The system combines an open-source large language model (LLM) with an open web search API to perform iterative retrieval, reasoning, and synthesis in real-world open-domain settings. To enhance reasoning quality, we apply preference tuning based on LLM-as-a-judge feedback that evaluates multiple aspects, including clarity, insightfulness, and factuality. Our experimental results show that the proposed method consistently improves answer quality across all three aspects. Our source code is publicly available at https://github.com/efficient-deep-research/efficient-deep-research.",
        "url": "http://arxiv.org/abs/2512.13059v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13059v1",
        "arxiv_id": "2512.13059v1",
        "authors": [
            "Ikuya Yamada",
            "Wataru Ikeda",
            "Ko Yoshida",
            "Mengyu Ye",
            "Hinata Sugimoto",
            "Masatoshi Suzuki",
            "Hisanori Ozaki",
            "Jun Suzuki"
        ],
        "submitted": "2025-12-15 07:37:53",
        "source": "arxiv",
        "comment": "Technical report of a winning system in the NeurIPS MMU-RAG competition",
        "score": 7,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)",
            "Found 'web search' (score: +1)",
            "Found 'search' (score: +1)",
            "Found 'neurips' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "The paper is somewhat related to the user's interests in Information Retrieval, particularly in the context of long-form question answering and real-world open-domain settings. However, the focus on large language models and preference tuning for reasoning quality is more aligned with NLP and deep learning, rather than the user's primary focus on query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "QwenLong-L1.5: Post-Training Recipe for Long-Context Reasoning and Memory Management",
        "abstract": "We introduce QwenLong-L1.5, a model that achieves superior long-context reasoning capabilities through systematic post-training innovations. The key technical breakthroughs of QwenLong-L1.5 are as follows: (1) Long-Context Data Synthesis Pipeline: We develop a systematic synthesis framework that generates challenging reasoning tasks requiring multi-hop grounding over globally distributed evidence. By deconstructing documents into atomic facts and their underlying relationships, and then programmatically composing verifiable reasoning questions, our approach creates high-quality training data at scale, moving substantially beyond simple retrieval tasks to enable genuine long-range reasoning capabilities. (2) Stabilized Reinforcement Learning for Long-Context Training: To overcome the critical instability in long-context RL, we introduce task-balanced sampling with task-specific advantage estimation to mitigate reward bias, and propose Adaptive Entropy-Controlled Policy Optimization (AEPO) that dynamically regulates exploration-exploitation trade-offs. (3) Memory-Augmented Architecture for Ultra-Long Contexts: Recognizing that even extended context windows cannot accommodate arbitrarily long sequences, we develop a memory management framework with multi-stage fusion RL training that seamlessly integrates single-pass reasoning with iterative memory-based processing for tasks exceeding 4M tokens. Based on Qwen3-30B-A3B-Thinking, QwenLong-L1.5 achieves performance comparable to GPT-5 and Gemini-2.5-Pro on long-context reasoning benchmarks, surpassing its baseline by 9.90 points on average. On ultra-long tasks (1M~4M tokens), QwenLong-L1.5's memory-agent framework yields a 9.48-point gain over the agent baseline. Additionally, the acquired long-context reasoning ability translates to enhanced performance in general domains like scientific reasoning, memory tool using, and extended dialogue.",
        "url": "http://arxiv.org/abs/2512.12967v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12967v1",
        "arxiv_id": "2512.12967v1",
        "authors": [
            "Weizhou Shen",
            "Ziyi Yang",
            "Chenliang Li",
            "Zhiyuan Lu",
            "Miao Peng",
            "Huashan Sun",
            "Yingcheng Shi",
            "Shengyi Liao",
            "Shaopeng Lai",
            "Bo Zhang",
            "Dayiheng Liu",
            "Fei Huang",
            "Jingren Zhou",
            "Ming Yan"
        ],
        "submitted": "2025-12-15 04:11:11",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'ltr' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper focuses on long-context reasoning and memory management, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the paper's emphasis on natural language processing and deep semantic understanding is not directly aligned with the user's primary research interests in IR and search technologies."
    },
    {
        "title": "SPAR: Session-based Pipeline for Adaptive Retrieval on Legacy File Systems",
        "abstract": "The ability to extract value from historical data is essential for enterprise decision-making. However, much of this information remains inaccessible within large legacy file systems that lack structured organization and semantic indexing, making retrieval and analysis inefficient and error-prone. We introduce SPAR (Session-based Pipeline for Adaptive Retrieval), a conceptual framework that integrates Large Language Models (LLMs) into a Retrieval-Augmented Generation (RAG) architecture specifically designed for legacy enterprise environments. Unlike conventional RAG pipelines, which require costly construction and maintenance of full-scale vector databases that mirror the entire file system, SPAR employs a lightweight two-stage process: a semantic Metadata Index is first created, after which session-specific vector databases are dynamically generated on demand. This design reduces computational overhead while improving transparency, controllability, and relevance in retrieval. We provide a theoretical complexity analysis comparing SPAR with standard LLM-based RAG pipelines, demonstrating its computational advantages. To validate the framework, we apply SPAR to a synthesized enterprise-scale file system containing a large corpus of biomedical literature, showing improvements in both retrieval effectiveness and downstream model accuracy. Finally, we discuss design trade-offs and outline open challenges for deploying SPAR across diverse enterprise settings.",
        "url": "http://arxiv.org/abs/2512.12938v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12938v1",
        "arxiv_id": "2512.12938v1",
        "authors": [
            "Duy A. Nguyen",
            "Hai H. Do",
            "Minh Doan",
            "Minh N. Do"
        ],
        "submitted": "2025-12-15 02:54:10",
        "source": "arxiv",
        "comment": null,
        "score": 7,
        "keyword_reasons": [
            "Found 'relevance' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores a novel approach to retrieval on legacy file systems using Large Language Models and Retrieval-Augmented Generation architecture. While it touches on information retrieval and NLP, the focus is more on the technical implementation and computational efficiency rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest."
    },
    {
        "title": "Does Tone Change the Answer? Evaluating Prompt Politeness Effects on Modern LLMs: GPT, Gemini, LLaMA",
        "abstract": "Prompt engineering has emerged as a critical factor influencing large language model (LLM) performance, yet the impact of pragmatic elements such as linguistic tone and politeness remains underexplored, particularly across different model families. In this work, we propose a systematic evaluation framework to examine how interaction tone affects model accuracy and apply it to three recently released and widely available LLMs: GPT-4o mini (OpenAI), Gemini 2.0 Flash (Google DeepMind), and Llama 4 Scout (Meta). Using the MMMLU benchmark, we evaluate model performance under Very Friendly, Neutral, and Very Rude prompt variants across six tasks spanning STEM and Humanities domains, and analyze pairwise accuracy differences with statistical significance testing.\n  Our results show that tone sensitivity is both model-dependent and domain-specific. Neutral or Very Friendly prompts generally yield higher accuracy than Very Rude prompts, but statistically significant effects appear only in a subset of Humanities tasks, where rude tone reduces accuracy for GPT and Llama, while Gemini remains comparatively tone-insensitive. When performance is aggregated across tasks within each domain, tone effects diminish and largely lose statistical significance. Compared with earlier researches, these findings suggest that dataset scale and coverage materially influence the detection of tone effects. Overall, our study indicates that while interaction tone can matter in specific interpretive settings, modern LLMs are broadly robust to tonal variation in typical mixed-domain use, providing practical guidance for prompt design and model selection in real-world deployments.",
        "url": "http://arxiv.org/abs/2512.12812v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12812v1",
        "arxiv_id": "2512.12812v1",
        "authors": [
            "Hanyu Cai",
            "Binqi Shen",
            "Lier Jin",
            "Lan Hu",
            "Xiaojing Fan"
        ],
        "submitted": "2025-12-14 19:25:20",
        "source": "arxiv",
        "comment": null,
        "score": 6,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)",
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the impact of linguistic tone on large language model performance, which is related to query understanding and ranking models in Information Retrieval. However, the focus on tone and politeness in prompt engineering is more aligned with NLP and not directly related to the user's core research themes in IR and search technologies."
    },
    {
        "title": "Fine-tuned LLM-based Code Migration Framework",
        "abstract": "The study presents the outcomes of research and experimental validation in the domain of automated codebase migration, with a focus on addressing challenges in transitioning SQL-based systems. The proposed method for migration essentially appears as a framework that leverages the best aspects of traditional software engineering techniques and provides an iterative, scalable, precise and efficient solution for modern database transformations. The central piece of the approach is the integration of a fine-tuned Large Language Model to address critical issues in SQL code conversion, such as syntax mapping, resolving discrepancies between Oracle PL/SQL and PostgreSQL, and optimising database elements such as stored procedures, triggers, views, and overall database logic. Thus, the method involves a trade-off between fine-tuning and prompt engineering. Special attention is given to a fine-tuning approach, which enhances the adaptability and compatibility with migration requirements across the entire database. According to the achieved results, fine-tuning plays a very important role. The study employs targeted evaluation methodologies along with computational metrics to measure the success of iterative conversion cycles. Core innovations include automated SQL feature detection, semi-supervised error analysis and integration of Subject Matter Experts feedback within a systematic migration workflow. The methodology achieves significant reductions in Syntax Error Rates, enhances feature alignment throughout migration iterations, and leverages dataset sampling to ensure continual improvement. By embedding GAI into the migration process, the framework facilitates precise feature mapping, semi-automated error resolution, and data-driven optimisation loops, improving workflow efficiency.",
        "url": "http://arxiv.org/abs/2512.13515v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13515v1",
        "arxiv_id": "2512.13515v1",
        "authors": [
            "Oleg Grynets",
            "Vasyl Lyashkevych",
            "Dmytro Baran",
            "Maksym Orliansky",
            "Taras Zelenyy",
            "Markiian Leshchyshyn"
        ],
        "submitted": "2025-12-15 16:42:51",
        "source": "arxiv",
        "comment": "16 pages, 27 figures, 7 references",
        "score": 5,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not relevant to your research interests as it focuses on code migration and database transformations, which is outside your core areas of Information Retrieval, Search technologies, and Natural Language Processing. Although it involves fine-tuning a Large Language Model, the context and application are not aligned with your research themes."
    },
    {
        "title": "MAC: A Multi-Agent Framework for Interactive User Clarification in Multi-turn Conversations",
        "abstract": "Conversational agents often encounter ambiguous user requests, requiring an effective clarification to successfully complete tasks. While recent advancements in real-world applications favor multi-agent architectures to manage complex conversational scenarios efficiently, ambiguity resolution remains a critical and underexplored challenge--particularly due to the difficulty of determining which agent should initiate a clarification and how agents should coordinate their actions when faced with uncertain or incomplete user input. The fundamental questions of when to interrupt a user and how to formulate the optimal clarification query within the most optimal multi-agent settings remain open. In this paper, we propose MAC (Multi-Agent Clarification), an interactive multi-agent framework specifically optimized to resolve user ambiguities by strategically managing clarification dialogues. We first introduce a novel taxonomy categorizing user ambiguities to systematically guide clarification strategies. Then, we present MAC that autonomously coordinates multiple agents to interact synergistically with users. Empirical evaluations on MultiWOZ 2.4 demonstrate that enabling clarification at both levels increases task success rate 7.8\\% (54.5 to 62.3) and reduces the average number of dialogue turns (6.53 to 4.86) by eliciting all required user information up front and minimizing repetition. Our findings highlight the importance of active user interaction and role-aware clarification for more reliable human-agent communication.",
        "url": "http://arxiv.org/abs/2512.13154v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13154v1",
        "arxiv_id": "2512.13154v1",
        "authors": [
            "Emre Can Acikgoz",
            "Jinoh Oh",
            "Joo Hyuk Jeon",
            "Jie Hao",
            "Heng Ji",
            "Dilek Hakkani-Tür",
            "Gokhan Tur",
            "Xiang Li",
            "Chengyuan Ma",
            "Xing Fan"
        ],
        "submitted": "2025-12-15 10:02:50",
        "source": "arxiv",
        "comment": null,
        "score": 5,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores conversational agents and user clarification in multi-turn conversations, which is related to query understanding and user behavior modeling in Information Retrieval. However, the focus on multi-agent architectures and conversational dialogue management is somewhat tangential to the core themes of IR and Search technologies. While there are some connections to NLP, the paper's primary emphasis on conversational agents and dialogue management does not strongly align with the user's research interests."
    },
    {
        "title": "Towards Effective Model Editing for LLM Personalization",
        "abstract": "Personalization is becoming indispensable for LLMs to align with individual user preferences and needs. Yet current approaches are often computationally expensive, data-intensive, susceptible to catastrophic forgetting, and prone to performance degradation in multi-turn interactions or when handling implicit queries. To address these challenges, we conceptualize personalization as a model editing task and introduce Personalization Editing, a framework that applies localized edits guided by clustered preference representations. This design enables precise preference-aligned updates while preserving overall model capabilities. In addition, existing personalization benchmarks frequently rely on persona-based dialogs between LLMs rather than user-LLM interactions, or focus primarily on stylistic imitation while neglecting information-seeking tasks that require accurate recall of user-specific preferences. We introduce User Preference Question Answering (UPQA), a short-answer QA dataset constructed from in-situ user queries with varying levels of difficulty. Unlike prior benchmarks, UPQA directly evaluates a model's ability to recall and apply specific user preferences. Across experimental settings, Personalization Editing achieves higher editing accuracy and greater computational efficiency than fine-tuning, while outperforming prompting-based baselines in multi-turn conversations and implicit preference questions settings.",
        "url": "http://arxiv.org/abs/2512.13676v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13676v1",
        "arxiv_id": "2512.13676v1",
        "authors": [
            "Baixiang Huang",
            "Limeng Cui",
            "Jiapeng Liu",
            "Haoran Wang",
            "Jiawei Xu",
            "Zhuiyue Tan",
            "Yutong Chen",
            "Chen Luo",
            "Yi Liu",
            "Kai Shu"
        ],
        "submitted": "2025-12-15 18:58:15",
        "source": "arxiv",
        "comment": "15 pages (including appendix), 7 figures. Code, data, results, and additional resources are available at: https://model-editing.github.io",
        "score": 4,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'personalization' (score: +1)"
        ],
        "llm_score": 7,
        "llm_reason": "This paper is somewhat related to the user's interests in Information Retrieval, particularly in the area of query understanding and ranking models, as it focuses on personalization and preference-aligned updates for Large Language Models (LLMs). While it does not directly address the user's core research themes, it explores a relevant topic in the context of NLP and deep semantic understanding. However, the paper's primary focus on LLM personalization and editing may not be directly applicable to the user's interests in e-commerce or recommender systems."
    },
    {
        "title": "Comparative Analysis of LLM Abliteration Methods: A Cross-Architecture Evaluation",
        "abstract": "Safety alignment mechanisms in large language models prevent responses to harmful queries through learned refusal behavior, yet these same mechanisms impede legitimate research applications including cognitive modeling, adversarial testing, and security analysis. While abliteration techniques enable surgical removal of refusal representations through directional orthogonalization, the relative effectiveness of available implementations remains uncharacterized. This study evaluates four abliteration tools (Heretic, DECCP, ErisForge, FailSpy) across sixteen instruction-tuned models (7B-14B parameters), reporting tool compatibility on all 16 models and quantitative metrics on subsets dictated by tool support. Single-pass methods demonstrated superior capability preservation on the benchmarked subset (avg GSM8K change across three models: ErisForge -0.28 pp; DECCP -0.13 pp), while Bayesian-optimized abliteration produced variable distribution shift (KL divergence: 0.043-1.646) with model-dependent capability impact. These findings provide researchers with evidence-based selection criteria for abliteration tool deployment across diverse model architectures. The principal finding indicates that mathematical reasoning capabilities exhibit the highest sensitivity to abliteration interventions, with GSM8K change ranging from +1.51 pp to -18.81 pp (-26.5% relative) depending on tool selection and model architecture.",
        "url": "http://arxiv.org/abs/2512.13655v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13655v1",
        "arxiv_id": "2512.13655v1",
        "authors": [
            "Richard J. Young"
        ],
        "submitted": "2025-12-15 18:48:42",
        "source": "arxiv",
        "comment": "25 pages, 6 figures, 8 tables",
        "score": 4,
        "keyword_reasons": [
            "Found 'queries' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on abliteration methods for large language models, which is a topic in Natural Language Processing (NLP). However, it does not appear to be directly related to Information Retrieval (IR), query understanding, ranking models, or user behavior modeling, which are the core areas of interest for your research."
    },
    {
        "title": "Advancing Bangla Machine Translation Through Informal Datasets",
        "abstract": "Bangla is the sixth most widely spoken language globally, with approximately 234 million native speakers. However, progress in open-source Bangla machine translation remains limited. Most online resources are in English and often remain untranslated into Bangla, excluding millions from accessing essential information. Existing research in Bangla translation primarily focuses on formal language, neglecting the more commonly used informal language. This is largely due to the lack of pairwise Bangla-English data and advanced translation models. If datasets and models can be enhanced to better handle natural, informal Bangla, millions of people will benefit from improved online information access. In this research, we explore current state-of-the-art models and propose improvements to Bangla translation by developing a dataset from informal sources like social media and conversational texts. This work aims to advance Bangla machine translation by focusing on informal language translation and improving accessibility for Bangla speakers in the digital world.",
        "url": "http://arxiv.org/abs/2512.13487v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13487v1",
        "arxiv_id": "2512.13487v1",
        "authors": [
            "Ayon Roy",
            "Risat Rahaman",
            "Sadat Shibly",
            "Udoy Saha Joy",
            "Abdulla Al Kafi",
            "Farig Yousuf Sadeque"
        ],
        "submitted": "2025-12-15 16:22:45",
        "source": "arxiv",
        "comment": "33 pages, 13 figures",
        "score": 4,
        "keyword_reasons": [
            "Found 'pairwise' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on Bangla machine translation, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. Although it involves NLP, the specific application and goals are quite different from the user's interests."
    },
    {
        "title": "On the Effectiveness of Membership Inference in Targeted Data Extraction from Large Language Models",
        "abstract": "Large Language Models (LLMs) are prone to memorizing training data, which poses serious privacy risks. Two of the most prominent concerns are training data extraction and Membership Inference Attacks (MIAs). Prior research has shown that these threats are interconnected: adversaries can extract training data from an LLM by querying the model to generate a large volume of text and subsequently applying MIAs to verify whether a particular data point was included in the training set. In this study, we integrate multiple MIA techniques into the data extraction pipeline to systematically benchmark their effectiveness. We then compare their performance in this integrated setting against results from conventional MIA benchmarks, allowing us to evaluate their practical utility in real-world extraction scenarios.",
        "url": "http://arxiv.org/abs/2512.13352v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13352v1",
        "arxiv_id": "2512.13352v1",
        "authors": [
            "Ali Al Sahili",
            "Ali Chehab",
            "Razane Tajeddine"
        ],
        "submitted": "2025-12-15 14:05:49",
        "source": "arxiv",
        "comment": "Accepted to IEEE Conference on Secure and Trustworthy Machine Learning (SaTML) 2026",
        "score": 4,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your core research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves Large Language Models, the focus is on privacy risks and data extraction, which is not a central theme in your research."
    },
    {
        "title": "AIR: Post-training Data Selection for Reasoning via Attention Head Influence",
        "abstract": "LLMs achieve remarkable multi-step reasoning capabilities, yet effectively transferring these skills via post-training distillation remains challenging. Existing data selection methods, ranging from manual curation to heuristics based on length, entropy, or overall loss, fail to capture the causal importance of individual reasoning steps, limiting distillation efficiency. To address this, we propose Attention Influence for Reasoning (AIR), a principled, unsupervised and training-free framework that leverages mechanistic insights of the retrieval head to select high-value post-training data. AIR first identifies reasoning-critical attention heads of an off-the-shelf model, then constructs a weakened reference model with disabled head influence, and finally quantifies the resulting loss divergence as the Attention Influence Score. This score enables fine-grained assessment at both the step and sample levels, supporting step-level weighted fine-tuning and global sample selection. Experiments across multiple reasoning benchmarks show that AIR consistently improves reasoning accuracy, surpassing heuristic baselines and effectively isolating the most critical steps and samples. Our work establishes a mechanism-driven, data-efficient approach for reasoning distillation in LLMs.",
        "url": "http://arxiv.org/abs/2512.13279v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13279v1",
        "arxiv_id": "2512.13279v1",
        "authors": [
            "Jinrui Liu",
            "Jeff Wu",
            "Xuanguang Pan",
            "Gavin Cheung",
            "Shuai Ma",
            "Chongyang Tao"
        ],
        "submitted": "2025-12-15 12:38:24",
        "source": "arxiv",
        "comment": "19 pages",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper focuses on post-training data selection for reasoning in Large Language Models (LLMs), using attention head influence as a mechanism. While it touches on retrieval head mechanisms, it is primarily concerned with reasoning and distillation, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the paper's focus on LLMs and reasoning distillation is not a central match for your research themes."
    },
    {
        "title": "Finch: Benchmarking Finance & Accounting across Spreadsheet-Centric Enterprise Workflows",
        "abstract": "We introduce a finance & accounting benchmark (Finch) for evaluating AI agents on real-world, enterprise-grade professional workflows -- interleaving data entry, structuring, formatting, web search, cross-file retrieval, calculation, modeling, validation, translation, visualization, and reporting. Finch is sourced from authentic enterprise workspaces at Enron (15,000 spreadsheets and 500,000 emails from 150 employees) and other financial institutions, preserving in-the-wild messiness across multimodal artifacts (text, tables, formulas, charts, code, and images) and spanning diverse domains such as budgeting, trading, and asset management.\n  We propose a workflow construction process that combines LLM-assisted discovery with expert annotation: (1) LLM-assisted, expert-verified derivation of workflows from real-world email threads and version histories of spreadsheet files, and (2) meticulous expert annotation for workflows, requiring over 700 hours of domain-expert effort. This yields 172 composite workflows with 384 tasks, involving 1,710 spreadsheets with 27 million cells, along with PDFs and other artifacts, capturing the intrinsically messy, long-horizon, knowledge-intensive, and collaborative nature of real-world enterprise work.\n  We conduct both human and automated evaluations of frontier AI systems including GPT 5.1, Claude Sonnet 4.5, Gemini 3 Pro, Grok 4, and Qwen 3 Max, and GPT 5.1 Pro spends 48 hours in total yet passes only 38.4% of workflows, while Claude Sonnet 4.5 passes just 25.0%. Comprehensive case studies further surface the challenges that real-world enterprise workflows pose for AI agents.",
        "url": "http://arxiv.org/abs/2512.13168v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13168v1",
        "arxiv_id": "2512.13168v1",
        "authors": [
            "Haoyu Dong",
            "Pengkun Zhang",
            "Yan Gao",
            "Xuanyu Dong",
            "Yilin Cheng",
            "Mingzhe Lu",
            "Adina Yakefu",
            "Shuxin Zheng"
        ],
        "submitted": "2025-12-15 10:28:45",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'web search' (score: +1)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves AI agents and workflow construction, its focus on finance & accounting and spreadsheet-centric enterprise workflows is not aligned with your primary research themes."
    },
    {
        "title": "Understanding Structured Financial Data with LLMs: A Case Study on Fraud Detection",
        "abstract": "Detecting fraud in financial transactions typically relies on tabular models that demand heavy feature engineering to handle high-dimensional data and offer limited interpretability, making it difficult for humans to understand predictions. Large Language Models (LLMs), in contrast, can produce human-readable explanations and facilitate feature analysis, potentially reducing the manual workload of fraud analysts and informing system refinements. However, they perform poorly when applied directly to tabular fraud detection due to the difficulty of reasoning over many features, the extreme class imbalance, and the absence of contextual information. To bridge this gap, we introduce FinFRE-RAG, a two-stage approach that applies importance-guided feature reduction to serialize a compact subset of numeric/categorical attributes into natural language and performs retrieval-augmented in-context learning over label-aware, instance-level exemplars. Across four public fraud datasets and three families of open-weight LLMs, FinFRE-RAG substantially improves F1/MCC over direct prompting and is competitive with strong tabular baselines in several settings. Although these LLMs still lag behind specialized classifiers, they narrow the performance gap and provide interpretable rationales, highlighting their value as assistive tools in fraud analysis.",
        "url": "http://arxiv.org/abs/2512.13040v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13040v1",
        "arxiv_id": "2512.13040v1",
        "authors": [
            "Xuwei Tan",
            "Yao Ma",
            "Xueru Zhang"
        ],
        "submitted": "2025-12-15 07:09:11",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper explores the application of Large Language Models (LLMs) in fraud detection, which is somewhat related to information retrieval and search technologies. However, the focus on financial data and fraud detection is not a central match to the user's core research themes, and the paper does not explicitly address query understanding, ranking models, or user behavior modeling."
    },
    {
        "title": "SignRAG: A Retrieval-Augmented System for Scalable Zero-Shot Road Sign Recognition",
        "abstract": "Automated road sign recognition is a critical task for intelligent transportation systems, but traditional deep learning methods struggle with the sheer number of sign classes and the impracticality of creating exhaustive labeled datasets. This paper introduces a novel zero-shot recognition framework that adapts the Retrieval-Augmented Generation (RAG) paradigm to address this challenge. Our method first uses a Vision Language Model (VLM) to generate a textual description of a sign from an input image. This description is used to retrieve a small set of the most relevant sign candidates from a vector database of reference designs. Subsequently, a Large Language Model (LLM) reasons over the retrieved candidates to make a final, fine-grained recognition. We validate this approach on a comprehensive set of 303 regulatory signs from the Ohio MUTCD. Experimental results demonstrate the framework's effectiveness, achieving 95.58% accuracy on ideal reference images and 82.45% on challenging real-world road data. This work demonstrates the viability of RAG-based architectures for creating scalable and accurate systems for road sign recognition without task-specific training.",
        "url": "http://arxiv.org/abs/2512.12885v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12885v1",
        "arxiv_id": "2512.12885v1",
        "authors": [
            "Minghao Zhu",
            "Zhihao Zhang",
            "Anmol Sidhu",
            "Keith Redmill"
        ],
        "submitted": "2025-12-14 23:56:34",
        "source": "arxiv",
        "comment": "Submitted to IV 2026",
        "score": 4,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is not directly related to the user's core research themes in Information Retrieval, Search technologies, or Natural Language Processing. While it involves a Retrieval-Augmented Generation (RAG) paradigm, the focus is on computer vision and road sign recognition, which is outside the user's primary areas of interest."
    },
    {
        "title": "Hindsight is 20/20: Building Agent Memory that Retains, Recalls, and Reflects",
        "abstract": "Agent memory has been touted as a dimension of growth for LLM-based applications, enabling agents that can accumulate experience, adapt across sessions, and move beyond single-shot question answering. The current generation of agent memory systems treats memory as an external layer that extracts salient snippets from conversations, stores them in vector or graph-based stores, and retrieves top-k items into the prompt of an otherwise stateless model. While these systems improve personalization and context carry-over, they still blur the line between evidence and inference, struggle to organize information over long horizons, and offer limited support for agents that must explain their reasoning. We present Hindsight, a memory architecture that treats agent memory as a structured, first-class substrate for reasoning by organizing it into four logical networks that distinguish world facts, agent experiences, synthesized entity summaries, and evolving beliefs. This framework supports three core operations -- retain, recall, and reflect -- that govern how information is added, accessed, and updated. Under this abstraction, a temporal, entity aware memory layer incrementally turns conversational streams into a structured, queryable memory bank, while a reflection layer reasons over this bank to produce answers and to update information in a traceable way. On key long-horizon conversational memory benchmarks like LongMemEval and LoCoMo, Hindsight with an open-source 20B model lifts overall accuracy from 39% to 83.6% over a full-context baseline with the same backbone and outperforms full context GPT-4o. Scaling the backbone further pushes Hindsight to 91.4% on LongMemEval and up to 89.61% on LoCoMo (vs. 75.78% for the strongest prior open system), consistently outperforming existing memory architectures on multi-session and open-domain questions.",
        "url": "http://arxiv.org/abs/2512.12818v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12818v1",
        "arxiv_id": "2512.12818v1",
        "authors": [
            "Chris Latimer",
            "Nicoló Boschi",
            "Andrew Neeser",
            "Chris Bartholomew",
            "Gaurav Srivastava",
            "Xuan Wang",
            "Naren Ramakrishnan"
        ],
        "submitted": "2025-12-14 19:47:23",
        "source": "arxiv",
        "comment": null,
        "score": 4,
        "keyword_reasons": [
            "Found 'query' (score: +3)",
            "Found 'personalization' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper presents a novel memory architecture for LLM-based applications, focusing on agent memory and conversational reasoning. While it touches on aspects of query understanding and ranking models, its primary contribution is in the area of agent memory and reasoning, which is somewhat related to the user's interests in Information Retrieval and NLP. However, the paper's focus on conversational memory and reasoning does not directly align with the user's core research themes."
    },
    {
        "title": "Know Your Users! Estimating User Domain Knowledge in Conversational Recommenders",
        "abstract": "The ideal conversational recommender system (CRS) acts like a savvy salesperson, adapting its language and suggestions to each user's level of expertise. However, most current systems treat all users as experts, leading to frustrating and inefficient interactions when users are unfamiliar with a domain. Systems that can adapt their conversational strategies to a user's knowledge level stand to offer a much more natural and effective experience. To make a step toward such adaptive systems, we introduce a new task: estimating user domain knowledge from conversations, enabling a CRS to better understand user needs and personalize interactions. A key obstacle to developing such adaptive systems is the lack of suitable data; to our knowledge, no existing dataset captures the conversational behaviors of users with varying levels of domain knowledge. Furthermore, in most dialogue collection protocols, users are free to express their own preferences, which tends to concentrate on popular items and well-known features, offering little insight into how novices explore or learn about unfamiliar features. To address this, we design a game-based data collection protocol that elicits varied expressions of knowledge, release the resulting dataset, and provide an initial analysis to highlight its potential for future work on user-knowledge-aware CRS.",
        "url": "http://arxiv.org/abs/2512.13173v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13173v1",
        "arxiv_id": "2512.13173v1",
        "authors": [
            "Ivica Kostric",
            "Ujwal Gadiraju",
            "Krisztian Balog"
        ],
        "submitted": "2025-12-15 10:35:05",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)",
            "Found 'acl' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, particularly in conversational recommenders and user behavior modeling. However, the focus on user domain knowledge estimation and conversational strategies is not directly aligned with your primary interests in query understanding, ranking models, and real-time relevance optimization."
    },
    {
        "title": "Towards Practical Large-scale Dynamical Heterogeneous Graph Embedding: Cold-start Resilient Recommendation",
        "abstract": "Deploying dynamic heterogeneous graph embeddings in production faces key challenges of scalability, data freshness, and cold-start. This paper introduces a practical, two-stage solution that balances deep graph representation with low-latency incremental updates. Our framework combines HetSGFormer, a scalable graph transformer for static learning, with Incremental Locally Linear Embedding (ILLE), a lightweight, CPU-based algorithm for real-time updates. HetSGFormer captures global structure with linear scalability, while ILLE provides rapid, targeted updates to incorporate new data, thus avoiding costly full retraining. This dual approach is cold-start resilient, leveraging the graph to create meaningful embeddings from sparse data. On billion-scale graphs, A/B tests show HetSGFormer achieved up to a 6.11% lift in Advertiser Value over previous methods, while the ILLE module added another 3.22% lift and improved embedding refresh timeliness by 83.2%. Our work provides a validated framework for deploying dynamic graph learning in production environments.",
        "url": "http://arxiv.org/abs/2512.13120v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13120v1",
        "arxiv_id": "2512.13120v1",
        "authors": [
            "Mabiao Long",
            "Jiaxi Liu",
            "Yufeng Li",
            "Hao Xiong",
            "Junchi Yan",
            "Kefan Wang",
            "Yi Cao",
            "Jiandong Ding"
        ],
        "submitted": "2025-12-15 09:19:23",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper focuses on recommendation systems using graph embeddings, which is somewhat related to the user's interests in Information Retrieval and Search technologies. However, the primary focus on recommender systems and graph embeddings is not a central match to the user's core research themes, which include query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Uncovering the Role of Initial Saliency in U-Shaped Attention Bias: Scaling Initial Token Weight for Enhanced Long-Text Processing",
        "abstract": "Large language models (LLMs) have demonstrated strong performance on a variety of natural language processing (NLP) tasks. However, they often struggle with long-text sequences due to the ``lost in the middle'' phenomenon. This issue has been shown to arise from a U-shaped attention bias, where attention is disproportionately focused on the beginning and end of a text, leaving the middle section underrepresented. While previous studies have attributed this bias to position encoding, our research first identifies an additional factor: initial saliency. It means that in the attention computation for each token, tokens with higher attention weights relative to the initial token tend to receive more attention in the prediction of the next token. We further find that utilizing this property by scaling attention weight between the initial token and others improves the model's ability to process long contexts, achieving a maximum improvement of 3.6\\% in MDQA dataset. Moreover, combining this approach with existing methods to reduce position encoding bias further enhances performance, achieving a maximum improvement of 3.4\\% in KV-Retrieval tasks.",
        "url": "http://arxiv.org/abs/2512.13109v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13109v1",
        "arxiv_id": "2512.13109v1",
        "authors": [
            "Zewen Qiang",
            "Sendong Zhao",
            "Haochun Wang",
            "Bing Qin",
            "Ting Liu"
        ],
        "submitted": "2025-12-15 09:04:06",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper is somewhat related to your research interests in Natural Language Processing (NLP) and deep semantic understanding. However, it focuses on a specific issue in large language models (LLMs) and attention bias, which, while relevant to your broader interests, is not a central match. The paper's emphasis on long-text processing and attention mechanisms is somewhat aligned with your research goals."
    },
    {
        "title": "Do Reviews Matter for Recommendations in the Era of Large Language Models?",
        "abstract": "With the advent of large language models (LLMs), the landscape of recommender systems is undergoing a significant transformation. Traditionally, user reviews have served as a critical source of rich, contextual information for enhancing recommendation quality. However, as LLMs demonstrate an unprecedented ability to understand and generate human-like text, this raises the question of whether explicit user reviews remain essential in the era of LLMs. In this paper, we provide a systematic investigation of the evolving role of text reviews in recommendation by comparing deep learning methods and LLM approaches. Particularly, we conduct extensive experiments on eight public datasets with LLMs and evaluate their performance in zero-shot, few-shot, and fine-tuning scenarios. We further introduce a benchmarking evaluation framework for review-aware recommender systems, RAREval, to comprehensively assess the contribution of textual reviews to the recommendation performance of review-aware recommender systems. Our framework examines various scenarios, including the removal of some or all textual reviews, random distortion, as well as recommendation performance in data sparsity and cold-start user settings. Our findings demonstrate that LLMs are capable of functioning as effective review-aware recommendation engines, generally outperforming traditional deep learning approaches, particularly in scenarios characterized by data sparsity and cold-start conditions. In addition, the removal of some or all textual reviews and random distortion does not necessarily lead to declines in recommendation accuracy. These findings motivate a rethinking of how user preference from text reviews can be more effectively leveraged. All code and supplementary materials are available at: https://github.com/zhytk/RAREval-data-processing.",
        "url": "http://arxiv.org/abs/2512.12978v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12978v1",
        "arxiv_id": "2512.12978v1",
        "authors": [
            "Chee Heng Tan",
            "Huiying Zheng",
            "Jing Wang",
            "Zhuoyi Lin",
            "Shaodi Feng",
            "Huijing Zhan",
            "Xiaoli Li",
            "J. Senthilnath"
        ],
        "submitted": "2025-12-15 04:46:48",
        "source": "arxiv",
        "comment": "11 pages, 9 figures, 3 tables",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper explores the role of user reviews in recommender systems, particularly in the context of large language models (LLMs). While it touches on topics related to information retrieval, such as text understanding and recommendation quality, its primary focus is on recommender systems, which is a secondary interest of yours. The paper's emphasis on LLMs and their potential to replace traditional review-based approaches is somewhat relevant to your interests in deep semantic understanding and real-time relevance optimization, but it is not a central match."
    },
    {
        "title": "BLADE: A Behavior-Level Data Augmentation Framework with Dual Fusion Modeling for Multi-Behavior Sequential Recommendation",
        "abstract": "Multi-behavior sequential recommendation aims to capture users' dynamic interests by modeling diverse types of user interactions over time. Although several studies have explored this setting, the recommendation performance remains suboptimal, mainly due to two fundamental challenges: the heterogeneity of user behaviors and data sparsity. To address these challenges, we propose BLADE, a framework that enhances multi-behavior modeling while mitigating data sparsity. Specifically, to handle behavior heterogeneity, we introduce a dual item-behavior fusion architecture that incorporates behavior information at both the input and intermediate levels, enabling preference modeling from multiple perspectives. To mitigate data sparsity, we design three behavior-level data augmentation methods that operate directly on behavior sequences rather than core item sequences. These methods generate diverse augmented views while preserving the semantic consistency of item sequences. These augmented views further enhance representation learning and generalization via contrastive learning. Experiments on three real-world datasets demonstrate the effectiveness of our approach.",
        "url": "http://arxiv.org/abs/2512.12964v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12964v1",
        "arxiv_id": "2512.12964v1",
        "authors": [
            "Yupeng Li",
            "Mingyue Cheng",
            "Yucong Luo",
            "Yitong Zhou",
            "Qingyang Mao",
            "Shijin Wang"
        ],
        "submitted": "2025-12-15 04:02:53",
        "source": "arxiv",
        "comment": null,
        "score": 3,
        "keyword_reasons": [
            "Found 'user behavior' (score: +2)",
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on recommender systems, specifically multi-behavior sequential recommendation, which is somewhat related to your interests in Information Retrieval and Search technologies. However, the paper's emphasis on recommender systems and user behavior modeling in a different context (sequential recommendation) makes it less relevant to your core research themes."
    },
    {
        "title": "What Matters in Evaluating Book-Length Stories? A Systematic Study of Long Story Evaluation",
        "abstract": "In this work, we conduct systematic research in a challenging area: the automatic evaluation of book-length stories (>100K tokens). Our study focuses on two key questions: (1) understanding which evaluation aspects matter most to readers, and (2) exploring effective methods for evaluating lengthy stories. We introduce the first large-scale benchmark, LongStoryEval, comprising 600 newly published books with an average length of 121K tokens (maximum 397K). Each book includes its average rating and multiple reader reviews, presented as critiques organized by evaluation aspects. By analyzing all user-mentioned aspects, we propose an evaluation criteria structure and conduct experiments to identify the most significant aspects among the 8 top-level criteria. For evaluation methods, we compare the effectiveness of three types: aggregation-based, incremental-updated, and summary-based evaluations. Our findings reveal that aggregation- and summary-based evaluations perform better, with the former excelling in detail assessment and the latter offering greater efficiency. Building on these insights, we further propose NovelCritique, an 8B model that leverages the efficient summary-based framework to review and score stories across specified aspects. NovelCritique outperforms commercial models like GPT-4o in aligning with human evaluations. Our datasets and codes are available at https://github.com/DingyiYang/LongStoryEval.",
        "url": "http://arxiv.org/abs/2512.12839v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12839v1",
        "arxiv_id": "2512.12839v1",
        "authors": [
            "Dingyi Yang",
            "Qin Jin"
        ],
        "submitted": "2025-12-14 20:53:29",
        "source": "arxiv",
        "comment": "24 pages, 7 figures, Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics",
        "score": 3,
        "keyword_reasons": [
            "Found 'rag' (score: +2)",
            "Found 'search' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on the evaluation of book-length stories, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it involves Natural Language Processing and deep semantic understanding, the context is limited to story evaluation, making it somewhat tangential to the user's core research interests."
    },
    {
        "title": "Large-Language Memorization During the Classification of United States Supreme Court Cases",
        "abstract": "Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called \"hallucinations\" since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.",
        "url": "http://arxiv.org/abs/2512.13654v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13654v1",
        "arxiv_id": "2512.13654v1",
        "authors": [
            "John E. Ortega",
            "Dhruv D. Joshi",
            "Matt P. Borkowski"
        ],
        "submitted": "2025-12-15 18:47:48",
        "source": "arxiv",
        "comment": "7 pages, 1 figure, Appendix of Prompts",
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on large-language models and their memorization strategies, which is somewhat related to your interests in Natural Language Processing (NLP). However, the specific application to United States Supreme Court cases and classification tasks is not directly aligned with your core research themes in Information Retrieval and Search technologies."
    },
    {
        "title": "ReFusion: A Diffusion Large Language Model with Parallel Autoregressive Decoding",
        "abstract": "Autoregressive models (ARMs) are hindered by slow sequential inference. While masked diffusion models (MDMs) offer a parallel alternative, they suffer from critical drawbacks: high computational overhead from precluding Key-Value (KV) caching, and incoherent generation arising from learning dependencies over an intractable space of token combinations. To address these limitations, we introduce ReFusion, a novel masked diffusion model that achieves superior performance and efficiency by elevating parallel decoding from the token level to a higher slot level, where each slot is a fixed-length, contiguous sub-sequence. This is achieved through an iterative ``plan-and-infill'' decoding process: a diffusion-based planning step first identifies a set of weakly dependent slots, and an autoregressive infilling step then decodes these selected slots in parallel. The slot-based design simultaneously unlocks full KV cache reuse with a unified causal framework and reduces the learning complexity from the token combination space to a manageable slot-level permutation space. Extensive experiments on seven diverse benchmarks show that ReFusion not only overwhelmingly surpasses prior MDMs with 34% performance gains and an over 18$\\times$ speedup on average, but also bridges the performance gap to strong ARMs while maintaining a 2.33$\\times$ average speedup.",
        "url": "http://arxiv.org/abs/2512.13586v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13586v1",
        "arxiv_id": "2512.13586v1",
        "authors": [
            "Jia-Nan Li",
            "Jian Guan",
            "Wei Wu",
            "Chongxuan Li"
        ],
        "submitted": "2025-12-15 17:41:19",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on improving the efficiency of masked diffusion models for language generation, but it does not directly relate to information retrieval, search technologies, or user behavior modeling, which are the core areas of your research interests."
    },
    {
        "title": "TARA: Simple and Efficient Time Aware Retrieval Adaptation of MLLMs for Video Understanding",
        "abstract": "Our objective is to build a general time-aware video-text embedding model for retrieval. To that end, we propose a simple and efficient recipe, dubbed TARA (Time Aware Retrieval Adaptation), to adapt Multimodal LLMs (MLLMs) to a time-aware video-text embedding model without using any video data at all. For evaluating time-awareness in retrieval, we propose a new benchmark with temporally opposite (chiral) actions as hard negatives and curated splits for chiral and non-chiral actions. We show that TARA outperforms all existing video-text models on this chiral benchmark while also achieving strong results on standard benchmarks. Furthermore, we discover additional benefits of TARA beyond time-awareness: (i) TARA embeddings are negation-aware as shown in NegBench benchmark that evaluates negation in video retrieval, (ii) TARA achieves state of the art performance on verb and adverb understanding in videos. Overall, TARA yields a strong, versatile, time-aware video-text embedding model with state of the art zero-shot performance.",
        "url": "http://arxiv.org/abs/2512.13511v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13511v1",
        "arxiv_id": "2512.13511v1",
        "authors": [
            "Piyush Bagad",
            "Andrew Zisserman"
        ],
        "submitted": "2025-12-15 16:38:59",
        "source": "arxiv",
        "comment": "18 Pages. Project page at http://bpiyush.github.io/tara-website",
        "score": 2,
        "keyword_reasons": [
            "Found 'retrieval' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "The paper focuses on video understanding and retrieval, which is somewhat related to information retrieval and search technologies. However, the specific application domain (video understanding) and the use of multimodal LLMs are not directly aligned with the user's core research themes. Additionally, the paper's emphasis on zero-shot performance and video-text embedding models does not strongly overlap with the user's interests in query understanding, ranking models, and user behavior modeling."
    },
    {
        "title": "Scaling Laws for Code: Every Programming Language Matters",
        "abstract": "Code large language models (Code LLMs) are powerful but costly to train, with scaling laws predicting performance from model size, data, and compute. However, different programming languages (PLs) have varying impacts during pre-training that significantly affect base model performance, leading to inaccurate performance prediction. Besides, existing works focus on language-agnostic settings, neglecting the inherently multilingual nature of modern software development. Therefore, it is first necessary to investigate the scaling laws of different PLs, and then consider their mutual influences to arrive at the final multilingual scaling law. In this paper, we present the first systematic exploration of scaling laws for multilingual code pre-training, conducting over 1000+ experiments (Equivalent to 336,000+ H800 hours) across multiple PLs, model sizes (0.2B to 14B parameters), and dataset sizes (1T tokens). We establish comprehensive scaling laws for code LLMs across multiple PLs, revealing that interpreted languages (e.g., Python) benefit more from increased model size and data than compiled languages (e.g., Rust). The study demonstrates that multilingual pre-training provides synergistic benefits, particularly between syntactically similar PLs. Further, the pre-training strategy of the parallel pairing (concatenating code snippets with their translations) significantly enhances cross-lingual abilities with favorable scaling properties. Finally, a proportion-dependent multilingual scaling law is proposed to optimally allocate training tokens by prioritizing high-utility PLs (e.g., Python), balancing high-synergy pairs (e.g., JavaScript-TypeScript), and reducing allocation to fast-saturating languages (Rust), achieving superior average performance across all PLs compared to uniform distribution under the same compute budget.",
        "url": "http://arxiv.org/abs/2512.13472v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13472v1",
        "arxiv_id": "2512.13472v1",
        "authors": [
            "Jian Yang",
            "Shawn Guo",
            "Lin Jing",
            "Wei Zhang",
            "Aishan Liu",
            "Chuan Hao",
            "Zhoujun Li",
            "Wayne Xin Zhao",
            "Xianglong Liu",
            "Weifeng Lv",
            "Bryan Dai"
        ],
        "submitted": "2025-12-15 16:07:34",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on scaling laws for code large language models, exploring the impact of different programming languages on model performance. While it touches on the concept of multilingual pre-training, it does not directly relate to information retrieval, search technologies, or user behavior modeling, which are core areas of your research interests."
    },
    {
        "title": "MiniLingua: A Small Open-Source LLM for European Languages",
        "abstract": "Large language models are powerful but often limited by high computational cost, privacy concerns, and English-centric training. Recent progress demonstrates that small, efficient models with around one billion parameters can deliver strong results and enable on-device use. This paper introduces MiniLingua, a multilingual open-source LLM of one billion parameters trained from scratch for 13 European languages, designed to balance coverage and instruction-following capabilities. Based on evaluation results, the instruction-tuned version of MiniLingua outperforms EuroLLM, a model with a similar training approach but a larger training budget, on summarization, classification and both open- and closed-book question answering. Moreover, it remains competitive with more advanced state-of-the-art models on open-ended generation tasks. We release model weights, tokenizer and source code used for data processing and model training.",
        "url": "http://arxiv.org/abs/2512.13298v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13298v1",
        "arxiv_id": "2512.13298v1",
        "authors": [
            "Anna Aksenova",
            "Boris Zverkov",
            "Nicola Dainese",
            "Alexander Nikitin",
            "Pekka Marttinen"
        ],
        "submitted": "2025-12-15 13:12:42",
        "source": "arxiv",
        "comment": "9+6 pages, 6 figures and 3 tables in the main text. Code at https://github.com/MiniLingua-ai/training_artifacts",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper is primarily focused on developing a multilingual language model, which is related to Natural Language Processing (NLP), but it does not directly address information retrieval, search technologies, or query understanding, which are the core areas of your research interests."
    },
    {
        "title": "M-GRPO: Stabilizing Self-Supervised Reinforcement Learning for Large Language Models with Momentum-Anchored Policy Optimization",
        "abstract": "Self-supervised reinforcement learning (RL) presents a promising approach for enhancing the reasoning capabilities of Large Language Models (LLMs) without reliance on expensive human-annotated data. However, we find that existing methods suffer from a critical failure mode under long-horizon training: a \"policy collapse\" where performance precipitously degrades. We diagnose this instability and demonstrate that simply scaling the number of rollouts -- a common strategy to improve performance -- only delays, but does not prevent, this collapse. To counteract this instability, we first introduce M-GRPO (Momentum-Anchored Group Relative Policy Optimization), a framework that leverages a slowly evolving momentum model to provide a stable training target. In addition, we identify that this process is often accompanied by a rapid collapse in policy entropy, resulting in a prematurely confident and suboptimal policy. To specifically address this issue, we propose a second contribution: an adaptive filtering method based on the interquartile range (IQR) that dynamically prunes low-entropy trajectories, preserving essential policy diversity. Our extensive experiments on multiple reasoning benchmarks demonstrate that M-GRPO stabilizes the training process while the IQR filter prevents premature convergence. The combination of these two innovations leads to superior training stability and state-of-the-art performance.",
        "url": "http://arxiv.org/abs/2512.13070v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13070v1",
        "arxiv_id": "2512.13070v1",
        "authors": [
            "Bizhe Bai",
            "Hongming Wu",
            "Peng Ye",
            "Tao Chen"
        ],
        "submitted": "2025-12-15 08:07:23",
        "source": "arxiv",
        "comment": "7 pages, 5 figures,Accepted NeurIPS 2025 Workshop on Efficient Reasoning",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on self-supervised reinforcement learning for Large Language Models, which is outside the user's primary research interests in Information Retrieval and Search technologies. While it touches on optimization techniques, the context and application are not directly related to the user's core themes."
    },
    {
        "title": "LLM Rationalis? Measuring Bargaining Capabilities of AI Negotiators",
        "abstract": "Bilateral negotiation is a complex, context-sensitive task in which human negotiators dynamically adjust anchors, pacing, and flexibility to exploit power asymmetries and informal cues. We introduce a unified mathematical framework for modeling concession dynamics based on a hyperbolic tangent curve, and propose two metrics burstiness tau and the Concession-Rigidity Index (CRI) to quantify the timing and rigidity of offer trajectories. We conduct a large-scale empirical comparison between human negotiators and four state-of-the-art large language models (LLMs) across natural-language and numeric-offers settings, with and without rich market context, as well as six controlled power-asymmetry scenarios. Our results reveal that, unlike humans who smoothly adapt to situations and infer the opponents position and strategies, LLMs systematically anchor at extremes of the possible agreement zone for negotiations and optimize for fixed points irrespective of leverage or context. Qualitative analysis further shows limited strategy diversity and occasional deceptive tactics used by LLMs. Moreover the ability of LLMs to negotiate does not improve with better models. These findings highlight fundamental limitations in current LLM negotiation capabilities and point to the need for models that better internalize opponent reasoning and context-dependent strategy.",
        "url": "http://arxiv.org/abs/2512.13063v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13063v1",
        "arxiv_id": "2512.13063v1",
        "authors": [
            "Cheril Shah",
            "Akshit Agarwal",
            "Kanak Garg",
            "Mourad Heddaya"
        ],
        "submitted": "2025-12-15 07:50:09",
        "source": "arxiv",
        "comment": "Published in the First Workshop on Multi-Turn Interactions in Large Language Models at Neurips 2025",
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on AI negotiation capabilities and the limitations of large language models (LLMs) in bilateral negotiation tasks. While it touches on the topic of modeling human behavior, it does not directly relate to information retrieval, search technologies, or query understanding, which are core areas of your research interests."
    },
    {
        "title": "Scaling Bidirectional Spans and Span Violations in Attention Mechanism",
        "abstract": "The canonical $O(N^2)$ Transformer remains the empirical performance frontier in sequence modeling, and its training can be further optimized by addressing geometric inefficiency. We propose an optimization framework that leverages an asymmetric projection to decompose the backward-pass gradients into parallel spans and orthogonal violations, while keeping the canonical forward-pass $QKV$ structure intact. Through consistent experimental validation across various decomposition and projection setups, we provide strong theoretical evidence: the standard attention gradient is suboptimal. We demonstrated that selectively scaling these components, focusing primarily on $0^{th}$ order bidirectional parallel spans, yields the most effective learning signal. On the limited WikiText-2 dataset, and using a crude configuration, this method achieved a $0.56\\%$ reduction in validation loss, confirming the framework's fundamental validity and suggesting significant potential gains on larger datasets and deeper training regimes",
        "url": "http://arxiv.org/abs/2512.13033v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13033v1",
        "arxiv_id": "2512.13033v1",
        "authors": [
            "Jongwook Kim",
            "Sangheon Yun",
            "Sukjin Yoon"
        ],
        "submitted": "2025-12-15 07:03:24",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper appears to be focused on optimizing the Transformer architecture for sequence modeling, which is not directly related to your core research themes in Information Retrieval and Search technologies. While it touches on attention mechanisms, it does not seem to address query understanding, ranking models, or user behavior modeling, making it only loosely relevant to your interests."
    },
    {
        "title": "Meta-GPT: Decoding the Metasurface Genome with Generative Artificial Intelligence",
        "abstract": "Advancing artificial intelligence for physical sciences requires representations that are both interpretable and compatible with the underlying laws of nature. We introduce METASTRINGS, a symbolic language for photonics that expresses nanostructures as textual sequences encoding materials, geometries, and lattice configurations. Analogous to molecular textual representations in chemistry, METASTRINGS provides a framework connecting human interpretability with computational design by capturing the structural hierarchy of photonic metasurfaces. Building on this representation, we develop Meta-GPT, a foundation transformer model trained on METASTRINGS and finetuned with physics-informed supervised, reinforcement, and chain-of-thought learning. Across various design tasks, the model achieves <3% mean-squared spectral error and maintains >98% syntactic validity, generating diverse metasurface prototypes whose experimentally measured optical responses match their target spectra. These results demonstrate that Meta-GPT can learn the compositional rules of light-matter interactions through METASTRINGS, laying a rigorous foundation for AI-driven photonics and representing an important step toward a metasurface genome project.",
        "url": "http://arxiv.org/abs/2512.12888v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12888v1",
        "arxiv_id": "2512.12888v1",
        "authors": [
            "David Dang",
            "Stuart Love",
            "Meena Salib",
            "Quynh Dang",
            "Samuel Rothfarb",
            "Mysk Alnatour",
            "Andrew Salij",
            "Hou-Tong Chen",
            "Ho Wai",
            "Lee",
            "Wilton J. M. Kort-Kamp"
        ],
        "submitted": "2025-12-15 00:09:14",
        "source": "arxiv",
        "comment": "Keywords: Physics-informed machine learning; Transformer models; Reinforcement learning; Chain-of-thought reasoning; Metasurfaces; Nanophotonics; Inverse design",
        "score": 2,
        "keyword_reasons": [
            "Found 'ctr' (score: +2)"
        ],
        "llm_score": 0,
        "llm_reason": "This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. It focuses on applying Generative Artificial Intelligence to the physical sciences, specifically photonics, which is unrelated to your areas of expertise."
    },
    {
        "title": "ERA-IT: Aligning Semantic Models with Revealed Economic Preference for Real-Time and Explainable Patent Valuation",
        "abstract": "Valuing intangible assets under uncertainty remains a critical challenge in the strategic management of technological innovation due to the information asymmetry inherent in high-dimensional technical specifications. Traditional bibliometric indicators, such as citation counts, fail to address this friction in a timely manner due to the systemic latency inherent in data accumulation. To bridge this gap, this study proposes the Economic Reasoning Alignment via Instruction Tuning (ERA-IT) framework. We theoretically conceptualize patent renewal history as a revealed economic preference and leverage it as an objective supervisory signal to align the generative reasoning of Large Language Models (LLMs) with market realities, a process we term Eco-Semantic Alignment. Using a randomly sampled dataset of 10,000 European Patent Office patents across diverse technological domains, we trained the model not only to predict value tiers but also to reverse-engineer the Economic Chain-of-Thought from unstructured text. Empirical results demonstrate that ERA-IT significantly outperforms both conventional econometric models and zero-shot LLMs in predictive accuracy. More importantly, by generating explicit, logically grounded rationales for valuation, the framework serves as a transparent cognitive scaffold for decision-makers, reducing the opacity of black-box AI in high-stakes intellectual property management.",
        "url": "http://arxiv.org/abs/2512.12869v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12869v1",
        "arxiv_id": "2512.12869v1",
        "authors": [
            "Yoo Yongmin",
            "Kim Seungwoo",
            "Liu Jingjiang"
        ],
        "submitted": "2025-12-14 23:04:07",
        "source": "arxiv",
        "comment": null,
        "score": 2,
        "keyword_reasons": [
            "Found 'rag' (score: +2)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your interests in Information Retrieval and Natural Language Processing, as it involves Large Language Models (LLMs) and semantic understanding. However, the focus on patent valuation and economic preference is not a central match to your primary research themes. The paper's emphasis on explainability and transparency is also relevant, but not a key aspect of your work."
    },
    {
        "title": "SkipCat: Rank-Maximized Low-Rank Compression of Large Language Models via Shared Projection and Block Skipping",
        "abstract": "Large language models (LLM) have achieved remarkable performance across a wide range of tasks. However, their substantial parameter sizes pose significant challenges for deployment on edge devices with limited computational and memory resources. Low-rank compression is a promising approach to address this issue, as it reduces both computational and memory costs, making LLM more suitable for resource-constrained environments. Nonetheless, naïve low-rank compression methods require a significant reduction in the retained rank to achieve meaningful memory and computation savings. For a low-rank model, the ranks need to be reduced by more than half to yield efficiency gains. Such aggressive truncation, however, typically results in substantial performance degradation. To address this trade-off, we propose SkipCat, a novel low-rank compression framework that enables the use of higher ranks while achieving the same compression rates. First, we introduce an intra-layer shared low-rank projection method, where multiple matrices that share the same input use a common projection. This reduces redundancy and improves compression efficiency. Second, we propose a block skipping technique that omits computations and memory transfers for selected sub-blocks within the low-rank decomposition. These two techniques jointly enable our compressed model to retain more effective ranks under the same compression budget. Experimental results show that, without any additional fine-tuning, our method outperforms previous low-rank compression approaches by 7% accuracy improvement on zero-shot tasks under the same compression rate. These results highlight the effectiveness of our rank-maximized compression strategy in preserving model performance under tight resource constraints.",
        "url": "http://arxiv.org/abs/2512.13494v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13494v1",
        "arxiv_id": "2512.13494v1",
        "authors": [
            "Yu-Chen Lu",
            "Sheng-Feng Yu",
            "Hui-Hsien Weng",
            "Pei-Shuo Wang",
            "Yu-Fang Hu",
            "Liang Hung-Chun",
            "Hung-Yueh Chiang",
            "Kai-Chiang Wu"
        ],
        "submitted": "2025-12-15 16:25:55",
        "source": "arxiv",
        "comment": "Accepted by AAAI 2026",
        "score": 1,
        "keyword_reasons": [
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 4,
        "llm_reason": "This paper is somewhat related to your research interests in Information Retrieval and Search technologies, but it focuses on low-rank compression of large language models, which is not a central match. The paper's emphasis on real-time relevance optimization and deep semantic understanding is not directly applicable to the topic of model compression."
    },
    {
        "title": "BlossomRec: Block-level Fused Sparse Attention Mechanism for Sequential Recommendations",
        "abstract": "Transformer structures have been widely used in sequential recommender systems (SRS). However, as user interaction histories increase, computational time and memory requirements also grow. This is mainly caused by the standard attention mechanism. Although there exist many methods employing efficient attention and SSM-based models, these approaches struggle to effectively model long sequences and may exhibit unstable performance on short sequences. To address these challenges, we design a sparse attention mechanism, BlossomRec, which models both long-term and short-term user interests through attention computation to achieve stable performance across sequences of varying lengths. Specifically, we categorize user interests in recommendation systems into long-term and short-term interests, and compute them using two distinct sparse attention patterns, with the results combined through a learnable gated output. Theoretically, it significantly reduces the number of interactions participating in attention computation. Extensive experiments on four public datasets demonstrate that BlossomRec, when integrated with state-of-the-art Transformer-based models, achieves comparable or even superior performance while significantly reducing memory usage, providing strong evidence of BlossomRec's efficiency and effectiveness.The code is available at https://github.com/ronineume/BlossomRec.",
        "url": "http://arxiv.org/abs/2512.13368v1",
        "pdf_url": "https://arxiv.org/pdf/2512.13368v1",
        "arxiv_id": "2512.13368v1",
        "authors": [
            "Mengyang Ma",
            "Xiaopeng Li",
            "Wanyu Wang",
            "Zhaocheng Du",
            "Jingtong Gao",
            "Pengyue Jia",
            "Yuyang Ye",
            "Yiqi Wang",
            "Yunpeng Weng",
            "Weihong Luo",
            "Xiao Han",
            "Xiangyu Zhao"
        ],
        "submitted": "2025-12-15 14:23:57",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'recommend' (score: +1)"
        ],
        "llm_score": 2,
        "llm_reason": "This paper focuses on sequential recommender systems, specifically proposing a new attention mechanism to improve performance and efficiency. While it touches on aspects of user behavior modeling, it is primarily concerned with recommender systems, which is a related but secondary interest of yours. The paper does not address query understanding, ranking models, or deep semantic understanding in information retrieval."
    },
    {
        "title": "Counting Clues: A Lightweight Probabilistic Baseline Can Match an LLM",
        "abstract": "Large language models (LLMs) excel on multiple-choice clinical diagnosis benchmarks, yet it is unclear how much of this performance reflects underlying probabilistic reasoning. We study this through questions from MedQA, where the task is to select the most likely diagnosis. We introduce the Frequency-Based Probabilistic Ranker (FBPR), a lightweight method that scores options with a smoothed Naive Bayes over concept-diagnosis co-occurrence statistics from a large corpus. When co-occurrence statistics were sourced from the pretraining corpora for OLMo and Llama, FBPR achieves comparable performance to the corresponding LLMs pretrained on that same corpus. Direct LLM inference and FBPR largely get different questions correct, with an overlap only slightly above random chance, indicating complementary strengths of each method. These findings highlight the continued value of explicit probabilistic baselines: they provide a meaningful performance reference point and a complementary signal for potential hybridization. While the performance of LLMs seems to be driven by a mechanism other than simple frequency aggregation, we show that an approach similar to the historically grounded, low-complexity expert systems still accounts for a substantial portion of benchmark performance.",
        "url": "http://arxiv.org/abs/2512.12868v1",
        "pdf_url": "https://arxiv.org/pdf/2512.12868v1",
        "arxiv_id": "2512.12868v1",
        "authors": [
            "Furong Jia",
            "Yuan Pu",
            "Finn Guo",
            "Monica Agrawal"
        ],
        "submitted": "2025-12-14 23:00:10",
        "source": "arxiv",
        "comment": null,
        "score": 1,
        "keyword_reasons": [
            "Found 'rank' (score: +1)"
        ],
        "llm_score": 6,
        "llm_reason": "This paper is somewhat related to the user's research interests in Information Retrieval, particularly in the area of query understanding and ranking models. However, the focus on Large Language Models (LLMs) and their probabilistic reasoning is more aligned with NLP and related topics, rather than the user's primary focus on IR and deep semantic understanding."
    }
]