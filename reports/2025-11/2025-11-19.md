# Daily Papers Report - 2025-11-19

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Hint-Augmented Re-ranking: Efficient Product Search using LLM-Based Query Decomposition

- **LLM Score**: 9
- **Keyword Score**: 18
- **Authors**: Yilun Zhu, Nikhita Vedula, Shervin Malmasi
- **URL**: <http://arxiv.org/abs/2511.13994v1>
- **Submitted**: 2025-11-17 23:53:25
- **Comment**: AACL 2025
- **Topic Keywords**: query, queries, ranking, rerank, retrieval, commerce, e-commerce, rank, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in query understanding and ranking models. The use of LLMs for query decomposition and efficient re-ranking aligns with your focus on deep semantic understanding and real-time relevance optimization. The e-commerce domain is also a relevant area of application.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Superlative‚ÄëAware Retrieval for E‚Äëcommerce Search
- **Aim**: Explicitly model superlative semantics, inject them into sparse and dense retrieval, and transfer LLM knowledge to low‚Äëlatency production models.
- **Rationale**: Superlatives encode implicit attribute preferences that keyword‚Äëbased systems miss, leading to misinterpretation, feature mis‚Äëmatching, and relevance errors.
- **Ground**: Dataset of 21k superlative queries, 470k products, 1.07M labeled pairs; LLM‚Äëgenerated hints (brands, features, alternative queries); QE‚ÄëBM25, pointwise rerankers.
- **Experiment**: QE‚ÄëBM25 boosts P@1 from 14.9% to 25.9%; pointwise finetuned 0.5B model with hints reaches 64.4% P@1, outperforming 72B listwise rerankers; latency reduced 14√ó; human eval win rate 65%.
- **Takeaway**: Structured hint decomposition and knowledge transfer to lightweight models achieve large relevance gains and low latency, providing a practical framework for superlative‚Äëaware e‚Äëcommerce search.

#### Abstract
> Search queries with superlatives (e.g., best, most popular) require comparing candidates across multiple dimensions, demanding linguistic understanding and domain knowledge. We show that LLMs can uncover latent intent behind these expressions in e-commerce queries through a framework that extracts structured interpretations or hints. Our approach decomposes queries into attribute-value hints generated concurrently with retrieval, enabling efficient integration into the ranking pipeline. Our method improves search performanc eby 10.9 points in MAP and ranking by 5.9 points in MRR over baselines. Since direct LLM-based reranking faces prohibitive latency, we develop an efficient approach transferring superlative interpretations to lightweight models. Our findings provide insights into how superlative semantics can be represented and transferred between models, advancing linguistic interpretation in retrieval systems while addressing practical deployment constraints.

---

### 2. TaoSearchEmb: A Multi-Objective Reinforcement Learning Framework for Dense Retrieval in Taobao Search

- **LLM Score**: 8
- **Keyword Score**: 17
- **Authors**: Xingxian Liu, Dongshuai Li, Tao Wen, Jiahui Wan, Gui Ling, Fuyu Lv, Dan Ou, Haihong Tang
- **URL**: <http://arxiv.org/abs/2511.13885v1>
- **Submitted**: 2025-11-17 20:16:52
- **Topic Keywords**: dense retrieval, query, queries, relevance, retrieval, commerce, e-commerce, search
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of dense retrieval and query understanding. The use of multi-objective reinforcement learning and real-time relevance optimization aligns with your focus on deep semantic understanding and real-time relevance optimization. However, the specific focus on e-commerce search engines and Taobao Search may limit its generalizability to other domains.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Retrieval‚ÄëGRPO ‚Äì A Multi‚ÄëObjective Reinforcement‚ÄëLearning Framework for Dense Retrieval in Taobao Search
- **Aim**: Address the limitations of static hard‚Äënegative mining, limited semantic generalization, and seesaw effect in multi‚Äëtask dense retrieval by introducing a dynamic, RL‚Äëdriven framework that incorporates LLM‚Äëbased rewards and multi‚Äëobjective optimization.
- **Rationale**: Existing BERT‚Äëstyle pipelines rely on offline hard‚Äënegative construction, ignore unseen items, and suffer from objective imbalance. A real‚Äëtime, dynamic sampling approach combined with LLM‚Äëderived relevance scores and a principled reward fusion can overcome these bottlenecks and enable rapid, scalable training.
- **Ground**: 1) Dynamic RL‚Äëdriven dense retrieval replacing static mining; 2) LLM‚Äëbased reward model for real‚Äëtime relevance; 3) Multi‚Äëobjective reward (relevance, quality, exclusivity) to mitigate seesaw effect; 4) Dual‚Äësampling InfoNCE in SFT to expose all products; 5) Real‚Äëtime error correction via on‚Äëthe‚Äëfly candidate sampling; 6) Large‚Äëscale production deployment on Taobao/Tmall with offline and online validation.
- **Experiment**: Offline: Recall@10 +3.5%, MRR +2.1% over static baseline; Online A/B: CTR +1.8%; Long‚Äëtail queries: Recall@10 +5.2%; Training time: 33% faster (8 vs 12 epochs). Deployment handles billions of items with real‚Äëtime ANN search, eliminating offline pipeline overhead.
- **Takeaway**: Dynamic negative sampling eliminates offline bottlenecks; LLM rewards bring zero‚Äëshot generalization; multi‚Äëobjective fusion balances user satisfaction, product quality, and diversity; GRPO provides lightweight, group‚Äëbased advantage estimation suitable for dense retrieval; dual‚Äësampling InfoNCE ensures comprehensive encoder training.

#### Abstract
> Dense retrieval, as the core component of e-commerce search engines, maps user queries and items into a unified semantic space through pre-trained embedding models to enable large-scale real-time semantic retrieval. Despite the rapid advancement of LLMs gradually replacing traditional BERT architectures for embedding, their training paradigms still adhere to BERT-like supervised fine-tuning and hard negative mining strategies. This approach relies on complex offline hard negative sample construction pipelines, which constrain model iteration efficiency and hinder the evolutionary potential of semantic representation capabilities. Besides, existing multi-task learning frameworks face the seesaw effect when simultaneously optimizing semantic relevance and non-relevance objectives. In this paper, we propose Retrieval-GRPO, a multi-objective reinforcement learning-based dense retrieval framework designed to address these challenges. The method eliminates offline hard negative sample construction by dynamically retrieving Top-K candidate products for each query during training, while introducing a relevance LLM as a reward model to generate real-time feedback. Specifically, the retrieval model dynamically optimizes embedding representations through reinforcement learning, with reward signals combining LLM-generated relevance scores, product quality scores, and multi-way exclusivity metrics to achieve multi-objective user preference alignment and real-time error correction. This mechanism not only removes dependency on hard negatives but also mitigates the seesaw effect through collaborative multi-objective optimization, significantly enhancing the model's semantic generalization capability for complex long-tail queries. Extensive offline and online experiments validate the effectiveness of Retrieval-GRPO, which has been deployed on China's largest e-commerce platform.

---

### 3. Infer As You Train: A Symmetric Paradigm of Masked Generative for Click-Through Rate Prediction

- **LLM Score**: 8
- **Keyword Score**: 8
- **Authors**: Moyu Zhang, Yujun Jin, Yun Chen, Jinxin Hu, Yu Zhang, Xiaoyi Zeng
- **URL**: <http://arxiv.org/abs/2511.14403v1>
- **Submitted**: 2025-11-18 12:07:56
- **Comment**: 4 pages, 4 tables, 1 figure
- **Topic Keywords**: rag, click, ctr, click-through rate
- **Reason**: The paper addresses click-through rate prediction, a topic closely related to user behavior modeling in Information Retrieval. The proposed symmetric paradigm for masked generative models is innovative and relevant to query understanding and ranking models, although it's more focused on CTR prediction than general IR. The connection to deep semantic understanding is indirect, but the use of generative models is a promising area of research.

#### Abstract
> Generative models are increasingly being explored in click-through rate (CTR) prediction field to overcome the limitations of the conventional discriminative paradigm, which rely on a simple binary classification objective. However, existing generative models typically confine the generative paradigm to the training phase, primarily for representation learning. During online inference, they revert to a standard discriminative paradigm, failing to leverage their powerful generative capabilities to further improve prediction accuracy. This fundamental asymmetry between the training and inference phases prevents the generative paradigm from realizing its full potential. To address this limitation, we propose the Symmetric Masked Generative Paradigm for CTR prediction (SGCTR), a novel framework that establishes symmetry between the training and inference phases. Specifically, after acquiring generative capabilities by learning feature dependencies during training, SGCTR applies the generative capabilities during online inference to iteratively redefine the features of input samples, which mitigates the impact of noisy features and enhances prediction accuracy. Extensive experiments validate the superiority of SGCTR, demonstrating that applying the generative paradigm symmetrically across both training and inference significantly unlocks its power in CTR prediction.

---

### 4. NeuroPath: Neurobiology-Inspired Path Tracking and Reflection for Semantically Coherent Retrieval

- **LLM Score**: 8
- **Keyword Score**: 7
- **Authors**: Junchen Li, Rongzheng Wang, Yihong Huang, Qizhi Chen, Jiasheng Zhang, Shuang Liang
- **URL**: <http://arxiv.org/abs/2511.14096v1>
- **Submitted**: 2025-11-18 03:28:23
- **Comment**: Accepted by NeurIPS 2025
- **Topic Keywords**: query, rag, retrieval
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, particularly in the area of query understanding and ranking models. The proposed NeuroPath framework addresses challenges in multi-hop question answering and semantic coherence, which aligns with your focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> Retrieval-augmented generation (RAG) greatly enhances large language models (LLMs) performance in knowledge-intensive tasks. However, naive RAG methods struggle with multi-hop question answering due to their limited capacity to capture complex dependencies across documents. Recent studies employ graph-based RAG to capture document connections. However, these approaches often result in a loss of semantic coherence and introduce irrelevant noise during node matching and subgraph construction. To address these limitations, we propose NeuroPath, an LLM-driven semantic path tracking RAG framework inspired by the path navigational planning of place cells in neurobiology. It consists of two steps: Dynamic Path Tracking and Post-retrieval Completion. Dynamic Path Tracking performs goal-directed semantic path tracking and pruning over the constructed knowledge graph (KG), improving noise reduction and semantic coherence. Post-retrieval Completion further reinforces these benefits by conducting second-stage retrieval using intermediate reasoning and the original query to refine the query goal and complete missing information in the reasoning path. NeuroPath surpasses current state-of-the-art baselines on three multi-hop QA datasets, achieving average improvements of 16.3% on recall@2 and 13.5% on recall@5 over advanced graph-based RAG methods. Moreover, compared to existing iter-based RAG methods, NeuroPath achieves higher accuracy and reduces token consumption by 22.8%. Finally, we demonstrate the robustness of NeuroPath across four smaller LLMs (Llama3.1, GLM4, Mistral0.3, and Gemma3), and further validate its scalability across tasks of varying complexity. Code is available at https://github.com/KennyCaty/NeuroPath.

---

### 5. Don't Miss the Forest for the Trees: In-Depth Confidence Estimation for LLMs via Reasoning over the Answer Space

- **LLM Score**: 8
- **Keyword Score**: 3
- **Authors**: Ante Wang, Weizhi Ma, Yang Liu
- **URL**: <http://arxiv.org/abs/2511.14275v1>
- **Submitted**: 2025-11-18 09:09:23
- **Topic Keywords**: rag, search
- **Reason**: This paper aligns with your research interests in query understanding and ranking models, as it explores a novel approach to confidence estimation in Large Language Models (LLMs). The focus on reasoning strategies and their impact on estimated confidence is relevant to your work on Learning to Rank and user behavior modeling. However, the specific domain of LLMs and confidence estimation is somewhat outside your primary focus on e-commerce and real-time relevance optimization.

#### Abstract
> Knowing the reliability of a model's response is essential in application. With the strong generation capabilities of LLMs, research has focused on generating verbalized confidence. This is further enhanced by combining chain-of-thought reasoning, which provides logical and transparent estimation. However, how reasoning strategies affect the estimated confidence is still under-explored. In this work, we demonstrate that predicting a verbalized probability distribution can effectively encourage in-depth reasoning for confidence estimation. Intuitively, it requires an LLM to consider all candidates within the answer space instead of basing on a single guess, and to carefully assign confidence scores to meet the requirements of a distribution. This method shows an advantage across different models and various tasks, regardless of whether the answer space is known. Its advantage is maintained even after reinforcement learning, and further analysis shows its reasoning patterns are aligned with human expectations.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. WebRec: Enhancing LLM-based Recommendations with Attention-guided RAG from Web

- **LLM Score**: 7
- **Keyword Score**: 10
- **Authors**: Zihuai Zhao, Yujuan Ding, Wenqi Fan, Qing Li
- **URL**: <http://arxiv.org/abs/2511.14182v1>
- **Submitted**: 2025-11-18 06:35:33
- **Topic Keywords**: queries, rag, retrieval, recommend, web search, search
- **Reason**: The paper explores the application of retrieval-augmented generation (RAG) in recommender systems, leveraging large language models (LLMs) and web-based information. While it touches on query understanding and ranking models, its primary focus is on recommender systems, which is somewhat related to your interests in information retrieval. However, the emphasis on web-based RAG and noisy content handling is not a central match with your core research themes.

#### Abstract
> Recommender systems play a vital role in alleviating information overload and enriching users' online experience. In the era of large language models (LLMs), LLM-based recommender systems have emerged as a prevalent paradigm for advancing personalized recommendations. Recently, retrieval-augmented generation (RAG) has drawn growing interest to facilitate the recommendation capability of LLMs, incorporating useful information retrieved from external knowledge bases. However, as a rich source of up-to-date information, the web remains under-explored by existing RAG-based recommendations. In particular, unique challenges are posed from two perspectives: one is to generate effective queries for web retrieval, considering the inherent knowledge gap between web search and recommendations; another challenge lies in harnessing online websites that contain substantial noisy content. To tackle these limitations, we propose WebRec, a novel web-based RAG framework, which takes advantage of the reasoning capability of LLMs to interpret recommendation tasks into queries of user preferences that cater to web retrieval. Moreover, given noisy web-retrieved information, where relevant pieces of evidence are scattered far apart, an insightful MP-Head is designed to enhance LLM attentions between distant tokens of relevant information via message passing. Extensive experiments have been conducted to demonstrate the effectiveness of our proposed web-based RAG methods in recommendation scenarios.

### 7. PRISM: Prompt-Refined In-Context System Modelling for Financial Retrieval

- **LLM Score**: 7
- **Keyword Score**: 9
- **Authors**: Chun Chet Ng, Jia Yu Lim, Wei Zeng Low
- **URL**: <http://arxiv.org/abs/2511.14130v1>
- **Submitted**: 2025-11-18 04:30:52
- **Comment**: 3rd-place solution for the ACM ICAIF 2025 Agentic Retrieval Grand Challenge
- **Topic Keywords**: information retrieval, ranking, retrieval, rank
- **Reason**: The paper PRISM focuses on financial information retrieval, which is a specific application of Information Retrieval. Although it does not directly address query understanding, ranking models, or user behavior modeling, it does involve deep semantic understanding and ranking tasks, making it somewhat related to the user's core research themes. The use of large language models and in-context learning also touches on related topics in Natural Language Processing.

#### Abstract
> With the rapid progress of large language models (LLMs), financial information retrieval has become a critical industrial application. Extracting task-relevant information from lengthy financial filings is essential for both operational and analytical decision-making. The FinAgentBench dataset formalizes this problem through two tasks: document ranking and chunk ranking. We present PRISM, a training-free framework that integrates refined system prompting, in-context learning (ICL), and a lightweight multi-agent system. Each component is examined extensively to reveal their synergies: prompt engineering provides precise task instructions, ICL supplies semantically relevant few-shot examples, and the multi-agent system models coordinated scoring behaviour. Our best configuration achieves an NDCG@5 of 0.71818 on the restricted validation split. We further demonstrate that PRISM is feasible and robust for production-scale financial retrieval. Its modular, inference-only design makes it practical for real-world use cases. The source code is released at https://bit.ly/prism-ailens.

### 8. PathMind: A Retrieve-Prioritize-Reason Framework for Knowledge Graph Reasoning with Large Language Models

- **LLM Score**: 7
- **Keyword Score**: 7
- **Authors**: Yu Liu, Xixun Lin, Yanmin Shang, Yangxi Li, Shi Wang, Yanan Cao
- **URL**: <http://arxiv.org/abs/2511.14256v1>
- **Submitted**: 2025-11-18 08:45:16
- **Comment**: AAAI 2026, Long Paper, Oral
- **Topic Keywords**: query, rag, retrieval
- **Reason**: This paper on PathMind, a framework for knowledge graph reasoning with large language models, is somewhat related to your research interests in Information Retrieval, particularly in areas requiring deep semantic understanding. While it doesn't directly focus on query understanding, ranking models, or user behavior modeling, it does involve complex reasoning tasks and real-time relevance optimization, which aligns with your broader interests in NLP and data mining.

#### Abstract
> Knowledge graph reasoning (KGR) is the task of inferring new knowledge by performing logical deductions on knowledge graphs. Recently, large language models (LLMs) have demonstrated remarkable performance in complex reasoning tasks. Despite promising success, current LLM-based KGR methods still face two critical limitations. First, existing methods often extract reasoning paths indiscriminately, without assessing their different importance, which may introduce irrelevant noise that misleads LLMs. Second, while many methods leverage LLMs to dynamically explore potential reasoning paths, they require high retrieval demands and frequent LLM calls. To address these limitations, we propose PathMind, a novel framework designed to enhance faithful and interpretable reasoning by selectively guiding LLMs with important reasoning paths. Specifically, PathMind follows a "Retrieve-Prioritize-Reason" paradigm. First, it retrieves a query subgraph from KG through the retrieval module. Next, it introduces a path prioritization mechanism that identifies important reasoning paths using a semantic-aware path priority function, which simultaneously considers the accumulative cost and the estimated future cost for reaching the target. Finally, PathMind generates accurate and logically consistent responses via a dual-phase training strategy, including task-specific instruction tuning and path-wise preference alignment. Extensive experiments on benchmark datasets demonstrate that PathMind consistently outperforms competitive baselines, particularly on complex reasoning tasks with fewer input tokens, by identifying essential reasoning paths.

### 9. NeuCLIRBench: A Modern Evaluation Collection for Monolingual, Cross-Language, and Multilingual Information Retrieval

- **LLM Score**: 6
- **Keyword Score**: 26
- **Authors**: Dawn Lawrie, James Mayfield, Eugene Yang, Andrew Yates, Sean MacAvaney, Ronak Pradeep, Scott Miller, Paul McNamee, Luca Soldani
- **URL**: <http://arxiv.org/abs/2511.14758v1>
- **Submitted**: 2025-11-18 18:58:19
- **Comment**: 14 pages, 1 figure
- **Topic Keywords**: information retrieval, retriever, query, queries, ranking, rerank, relevance, retrieval, rank, trec
- **Reason**: This paper presents a new evaluation collection for information retrieval, specifically focusing on cross-language and multilingual retrieval. While it aligns with the broader field of IR, it doesn't directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The collection's focus on multilingual retrieval and machine translation is relevant to the e-commerce domain, but not a central match for the user's primary research themes.

#### Abstract
> To measure advances in retrieval, test collections with relevance judgments that can faithfully distinguish systems are required. This paper presents NeuCLIRBench, an evaluation collection for cross-language and multilingual retrieval. The collection consists of documents written natively in Chinese, Persian, and Russian, as well as those same documents machine translated into English. The collection supports several retrieval scenarios including: monolingual retrieval in English, Chinese, Persian, or Russian; cross-language retrieval with English as the query language and one of the other three languages as the document language; and multilingual retrieval, again with English as the query language and relevant documents in all three languages. NeuCLIRBench combines the TREC NeuCLIR track topics of 2022, 2023, and 2024. The 250,128 judgments across approximately 150 queries for the monolingual and cross-language tasks and 100 queries for multilingual retrieval provide strong statistical discriminatory power to distinguish retrieval approaches. A fusion baseline of strong neural retrieval systems is included with the collection so that developers of reranking algorithms are no longer reliant on BM25 as their first-stage retriever. NeuCLIRBench is publicly available.

### 10. LiveRAG: A diverse Q&A dataset with varying difficulty level for RAG evaluation

- **LLM Score**: 6
- **Keyword Score**: 9
- **Authors**: David Carmel, Simone Filice, Guy Horowitz, Yoelle Maarek, Alex Shtoff, Oren Somekh, Ran Tavory
- **URL**: <http://arxiv.org/abs/2511.14531v1>
- **Submitted**: 2025-11-18 14:34:35
- **Comment**: 14 pages, 4 figures, 5 tables
- **Topic Keywords**: rag, retrieval augmented generation, retrieval, search, sigir
- **Reason**: The paper introduces a new dataset for evaluating Retrieval Augmented Generation (RAG) systems, which is relevant to the field of Information Retrieval. However, the focus is on RAG evaluation rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest. The connection to IR is indirect, but the paper's contribution to the field of RAG could be useful for researchers interested in IR.

#### Abstract
> With Retrieval Augmented Generation (RAG) becoming more and more prominent in generative AI solutions, there is an emerging need for systematically evaluating their effectiveness. We introduce the LiveRAG benchmark, a publicly available dataset of 895 synthetic questions and answers designed to support systematic evaluation of RAG-based Q&A systems. This synthetic benchmark is derived from the one used during the SIGIR'2025 LiveRAG Challenge, where competitors were evaluated under strict time constraints. It is augmented with information that was not made available to competitors during the Challenge, such as the ground-truth answers, together with their associated supporting claims which were used for evaluating competitors' answers. In addition, each question is associated with estimated difficulty and discriminability scores, derived from applying an Item Response Theory model to competitors' responses. Our analysis highlights the benchmark's questions diversity, the wide range of their difficulty levels, and their usefulness in differentiating between system capabilities. The LiveRAG benchmark will hopefully help the community advance RAG research, conduct systematic evaluation, and develop more robust Q&A systems.

### 11. SciRAG: Adaptive, Citation-Aware, and Outline-Guided Retrieval and Synthesis for Scientific Literature

- **LLM Score**: 6
- **Keyword Score**: 7
- **Authors**: Hang Ding, Yilun Zhao, Tiansheng Hu, Manasi Patwardhan, Arman Cohan
- **URL**: <http://arxiv.org/abs/2511.14362v1>
- **Submitted**: 2025-11-18 11:09:19
- **Topic Keywords**: queries, rag, retrieval
- **Reason**: The paper focuses on scientific literature retrieval and synthesis, leveraging citation graphs and outline-guided synthesis. While it touches on retrieval and generation, the primary focus is on scientific knowledge aggregation, which is somewhat related to your interests in information retrieval and NLP. However, the specific domain and application are not directly aligned with your core research themes.

#### Abstract
> The accelerating growth of scientific publications has intensified the need for scalable, trustworthy systems to synthesize knowledge across diverse literature. While recent retrieval-augmented generation (RAG) methods have improved access to scientific information, they often overlook citation graph structure, adapt poorly to complex queries, and yield fragmented, hard-to-verify syntheses. We introduce SciRAG, an open-source framework for scientific literature exploration that addresses these gaps through three key innovations: (1) adaptive retrieval that flexibly alternates between sequential and parallel evidence gathering; (2) citation-aware symbolic reasoning that leverages citation graphs to organize and filter supporting documents; and (3) outline-guided synthesis that plans, critiques, and refines answers to ensure coherence and transparent attribution. Extensive experiments across multiple benchmarks such as QASA and ScholarQA demonstrate that SciRAG outperforms prior systems in factual accuracy and synthesis quality, establishing a new foundation for reliable, large-scale scientific knowledge aggregation.

### 12. Harnessing Deep LLM Participation for Robust Entity Linking

- **LLM Score**: 6
- **Keyword Score**: 3
- **Authors**: Jiajun Hou, Chenyu Zhang, Rui Meng
- **URL**: <http://arxiv.org/abs/2511.14181v1>
- **Submitted**: 2025-11-18 06:35:26
- **Topic Keywords**: rag, search
- **Reason**: The paper is somewhat related to the user's interests in Natural Language Processing (NLP) and deep semantic understanding, but it is not directly focused on Information Retrieval (IR) or query understanding. The use of Large Language Models (LLMs) for entity linking is an interesting application, but it does not directly relate to the user's primary focus on IR and search technologies.

#### Abstract
> Entity Linking (EL), the task of mapping textual entity mentions to their corresponding entries in knowledge bases, constitutes a fundamental component of natural language understanding. Recent advancements in Large Language Models (LLMs) have demonstrated remarkable potential for enhancing EL performance. Prior research has leveraged LLMs to improve entity disambiguation and input representation, yielding significant gains in accuracy and robustness. However, these approaches typically apply LLMs to isolated stages of the EL task, failing to fully integrate their capabilities throughout the entire process.
  In this work, we introduce DeepEL, a comprehensive framework that incorporates LLMs into every stage of the entity linking task. Furthermore, we identify that disambiguating entities in isolation is insufficient for optimal performance. To address this limitation, we propose a novel self-validation mechanism that utilizes global contextual information, enabling LLMs to rectify their own predictions and better recognize cohesive relationships among entities within the same sentence.
  Extensive empirical evaluation across ten benchmark datasets demonstrates that DeepEL substantially outperforms existing state-of-the-art methods, achieving an average improvement of 2.6\% in overall F1 score and a remarkable 4% gain on out-of-domain datasets. These results underscore the efficacy of deep LLM integration in advancing the state-of-the-art in entity linking.

### 13. What Works for 'Lost-in-the-Middle' in LLMs? A Study on GM-Extract and Mitigations

- **LLM Score**: 6
- **Keyword Score**: 2
- **Authors**: Mihir Gupte, Eshan Dixit, Muhammad Tayyab, Arun Adiththan
- **URL**: <http://arxiv.org/abs/2511.13900v1>
- **Submitted**: 2025-11-17 20:50:50
- **Comment**: To be submitted for publication
- **Topic Keywords**: retrieval
- **Reason**: This paper is somewhat related to the user's interests in Information Retrieval, particularly in the context of large language models and retrieval-based applications. However, its focus on the 'lost-in-the-middle' phenomenon and mitigation methods is not a central match to the user's core research themes, which include query understanding, ranking models, and user behavior modeling.

#### Abstract
> The diminishing ability of large language models (LLMs) to effectively utilize long-range context-the "lost-in-the-middle" phenomenon-poses a significant challenge in retrieval-based LLM applications. To study the impact of this phenomenon in a real-world application setting, we introduce GM-Extract, a novel benchmark dataset meticulously designed to evaluate LLM performance on retrieval of control variables. To accurately diagnose failure modes, we propose a simple yet elegant evaluation system using two distinct metrics: one for spatial retrieval capability (Document Metric) and the other for semantic retrieval capability (Variable Extraction Metric). We conduct a systematic evaluation of 7-8B parameter models on two multi-document tasks (key-value extraction and question-answering), demonstrating a significant change in retrieval performance simply by altering how the data is represented in the context window. While a distinct U-shaped curve was not consistently observed, our analysis reveals a clear pattern of performance across models, which we further correlate with perplexity scores. Furthermore, we perform a literature survey of mitigation methods, which we categorize into two distinct approaches: black-box and white-box methods. We then apply these techniques to our benchmark, finding that their efficacy is highly nuanced. Our evaluation highlights scenarios where these strategies successfully improve performance, as well as surprising cases where they lead to a negative impact, providing a comprehensive understanding of their utility in a practical context.

### 14. Knowledge-Grounded Agentic Large Language Models for Multi-Hazard Understanding from Reconnaissance Reports

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Chenchen Kuai, Zihao Li, Braden Rosen, Stephanie Paan, Navid Jafari, Jean-Louis Briaud, Yunlong Zhang, Youssef M. A. Hashash, Yang Zhou
- **URL**: <http://arxiv.org/abs/2511.14010v1>
- **Submitted**: 2025-11-18 00:36:31
- **Comment**: 17 pages, 5 figures
- **Topic Keywords**: queries, rag, retrieval, search
- **Reason**: While this paper explores the application of large language models in a domain related to information retrieval, its focus on multi-hazard understanding and post-disaster reconnaissance reports is somewhat tangential to the user's core research interests in query understanding, ranking models, and user behavior modeling. The paper's use of retrieval and question-answering mechanisms is somewhat relevant, but the specific context and domain are not directly aligned with the user's expertise in e-commerce and IR.

#### Abstract
> Post-disaster reconnaissance reports contain critical evidence for understanding multi-hazard interactions, yet their unstructured narratives make systematic knowledge transfer difficult. Large language models (LLMs) offer new potential for analyzing these reports, but often generate unreliable or hallucinated outputs when domain grounding is absent. This study introduces the Mixture-of-Retrieval Agentic RAG (MoRA-RAG), a knowledge-grounded LLM framework that transforms reconnaissance reports into a structured foundation for multi-hazard reasoning. The framework integrates a Mixture-of-Retrieval mechanism that dynamically routes queries across hazard-specific databases while using agentic chunking to preserve contextual coherence during retrieval. It also includes a verification loop that assesses evidence sufficiency, refines queries, and initiates targeted searches when information remains incomplete. We construct HazardRecQA by deriving question-answer pairs from GEER reconnaissance reports, which document 90 global events across seven major hazard types. MoRA-RAG achieves up to 94.5 percent accuracy, outperforming zero-shot LLMs by 30 percent and state-of-the-art RAG systems by 10 percent, while reducing hallucinations across diverse LLM architectures. MoRA-RAG also enables open-weight LLMs to achieve performance comparable to proprietary models. It establishes a new paradigm for transforming post-disaster documentation into actionable, trustworthy intelligence for hazard resilience.

### 15. ConInstruct: Evaluating Large Language Models on Conflict Detection and Resolution in Instructions

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Xingwei He, Qianru Zhang, Pengfei Chen, Guanhua Chen, Linlin Yu, Yuan Yuan, Siu-Ming Yiu
- **URL**: <http://arxiv.org/abs/2511.14342v2>
- **Submitted**: 2025-11-18 10:49:37
- **Comment**: Accepted to AAAI 2026
- **Topic Keywords**: ranking, rag, rank
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and Large Language Models, but it does not directly focus on Information Retrieval (IR) or Search technologies. The paper's emphasis on conflict detection and resolution in instructions is an interesting aspect, but it does not align with your primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Instruction-following is a critical capability of Large Language Models (LLMs). While existing works primarily focus on assessing how well LLMs adhere to user instructions, they often overlook scenarios where instructions contain conflicting constraints-a common occurrence in complex prompts. The behavior of LLMs under such conditions remains under-explored. To bridge this gap, we introduce ConInstruct, a benchmark specifically designed to assess LLMs' ability to detect and resolve conflicts within user instructions. Using this dataset, we evaluate LLMs' conflict detection performance and analyze their conflict resolution behavior. Our experiments reveal two key findings: (1) Most proprietary LLMs exhibit strong conflict detection capabilities, whereas among open-source models, only DeepSeek-R1 demonstrates similarly strong performance. DeepSeek-R1 and Claude-4.5-Sonnet achieve the highest average F1-scores at 91.5% and 87.3%, respectively, ranking first and second overall. (2) Despite their strong conflict detection abilities, LLMs rarely explicitly notify users about the conflicts or request clarification when faced with conflicting constraints. These results underscore a critical shortcoming in current LLMs and highlight an important area for future improvement when designing instruction-following LLMs.

### 16. Ground Truth Generation for Multilingual Historical NLP using LLMs

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Clovis Gladstone, Zhao Fang, Spencer Dean Stewart
- **URL**: <http://arxiv.org/abs/2511.14688v1>
- **Submitted**: 2025-11-18 17:25:43
- **Comment**: 13 pages, 5 tables, 1 figure
- **Topic Keywords**: rag, search
- **Reason**: This paper is somewhat related to your research interests in Natural Language Processing (NLP) and data mining, but it does not directly align with your primary focus on Information Retrieval (IR), especially query understanding, ranking models, and user behavior modeling. The paper explores the application of large language models (LLMs) in historical NLP, which is a niche area that may not be central to your research themes.

#### Abstract
> Historical and low-resource NLP remains challenging due to limited annotated data and domain mismatches with modern, web-sourced corpora. This paper outlines our work in using large language models (LLMs) to create ground-truth annotations for historical French (16th-20th centuries) and Chinese (1900-1950) texts. By leveraging LLM-generated ground truth on a subset of our corpus, we were able to fine-tune spaCy to achieve significant gains on period-specific tests for part-of-speech (POS) annotations, lemmatization, and named entity recognition (NER). Our results underscore the importance of domain-specific models and demonstrate that even relatively limited amounts of synthetic data can improve NLP tools for under-resourced corpora in computational humanities research.

### 17. From Graphs to Hypergraphs: Enhancing Aspect-Based Sentiment Analysis via Multi-Level Relational Modeling

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Omkar Mahesh Kashyap, Padegal Amit, Madhav Kashyap, Ashwini M Joshi, Shylaja SS
- **URL**: <http://arxiv.org/abs/2511.14142v1>
- **Submitted**: 2025-11-18 05:01:25
- **Topic Keywords**: pairwise
- **Reason**: The paper focuses on Aspect-Based Sentiment Analysis, which is a related topic to query understanding and ranking models in Information Retrieval. However, the primary contribution of the paper is in the application of a novel hypergraph framework, which, while interesting, does not directly align with the user's core research themes in IR and Search technologies.

#### Abstract
> Aspect-Based Sentiment Analysis (ABSA) predicts sentiment polarity for specific aspect terms, a task made difficult by conflicting sentiments across aspects and the sparse context of short texts. Prior graph-based approaches model only pairwise dependencies, forcing them to construct multiple graphs for different relational views. These introduce redundancy, parameter overhead, and error propagation during fusion, limiting robustness in short-text, low-resource settings. We present HyperABSA, a dynamic hypergraph framework that induces aspect-opinion structures through sample-specific hierarchical clustering. To construct these hyperedges, we introduce a novel acceleration-fallback cutoff for hierarchical clustering, which adaptively determines the level of granularity. Experiments on three benchmarks (Lap14, Rest14, MAMS) show consistent improvements over strong graph baselines, with substantial gains when paired with RoBERTa backbones. These results position dynamic hypergraph construction as an efficient, powerful alternative for ABSA, with potential extensions to other short-text NLP tasks.

### 18. AISAC: An Integrated multi-agent System for Transparent, Retrieval-Grounded Scientific Assistance

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Chandrachur Bhattacharya, Sibendu Som
- **URL**: <http://arxiv.org/abs/2511.14043v1>
- **Submitted**: 2025-11-18 01:51:05
- **Topic Keywords**: retrieval, search
- **Reason**: The paper AISAC: An Integrated multi-agent System for Transparent, Retrieval-Grounded Scientific Assistance is somewhat related to the user's interests in Information Retrieval and Natural Language Processing, as it involves retrieval-grounded scientific assistance and a hybrid memory approach. However, the focus on scientific workflows and multi-agent systems is not a central match for the user's primary research themes, which include query understanding, ranking models, and user behavior modeling.

#### Abstract
> AI Scientific Assistant Core (AISAC) is an integrated multi-agent system developed at Argonne National Laboratory for scientific and engineering workflows. AISAC builds on established technologies - LangGraph for orchestration, FAISS for vector search, and SQLite for persistence - and integrates them into a unified system prototype focused on transparency, provenance tracking, and scientific adaptability.
  The system implements a Router-Planner-Coordinator workflow and an optional Evaluator role, using prompt-engineered agents coordinated via LangGraph's StateGraph and supported by helper agents such as a Researcher. Each role is defined through custom system prompts that enforce structured JSON outputs. A hybrid memory approach (FAISS + SQLite) enables both semantic retrieval and structured conversation history. An incremental indexing strategy based on file hashing minimizes redundant re-embedding when scientific corpora evolve. A configuration-driven project bootstrap layer allows research teams to customize tools, prompts, and data sources without modifying core code.
  All agent decisions, tool invocations, and retrievals are logged and visualized through a custom Gradio interface, providing step-by-step transparency for each reasoning episode. The authors have applied AISAC to multiple research areas at Argonne, including specialized deployments for waste-to-products research and energy process safety, as well as general-purpose scientific assistance, demonstrating its cross-domain applicability.

### 19. Talk, Snap, Complain: Validation-Aware Multimodal Expert Framework for Fine-Grained Customer Grievances

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Rishu Kumar Singh, Navneet Shreya, Sarmistha Das, Apoorva Singh, Sriparna Saha
- **URL**: <http://arxiv.org/abs/2511.14693v1>
- **Submitted**: 2025-11-18 17:29:28
- **Comment**: To be published in the Proceedings of the 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026 Special Track on AI for Social Impact )
- **Topic Keywords**: rag
- **Reason**: The paper explores multimodal complaint analysis, which is somewhat related to information retrieval and user behavior modeling, but it doesn't directly address query understanding, ranking models, or real-time relevance optimization. The focus on complaint understanding and customer support dialogues is also somewhat tangential to the user's primary research interests in e-commerce and deep semantic understanding.

#### Abstract
> Existing approaches to complaint analysis largely rely on unimodal, short-form content such as tweets or product reviews. This work advances the field by leveraging multimodal, multi-turn customer support dialogues, where users often share both textual complaints and visual evidence (e.g., screenshots, product photos) to enable fine-grained classification of complaint aspects and severity. We introduce VALOR, a Validation-Aware Learner with Expert Routing, tailored for this multimodal setting. It employs a multi-expert reasoning setup using large-scale generative models with Chain-of-Thought (CoT) prompting for nuanced decision-making. To ensure coherence between modalities, a semantic alignment score is computed and integrated into the final classification through a meta-fusion strategy. In alignment with the United Nations Sustainable Development Goals (UN SDGs), the proposed framework supports SDG 9 (Industry, Innovation and Infrastructure) by advancing AI-driven tools for robust, scalable, and context-aware service infrastructure. Further, by enabling structured analysis of complaint narratives and visual context, it contributes to SDG 12 (Responsible Consumption and Production) by promoting more responsive product design and improved accountability in consumer services. We evaluate VALOR on a curated multimodal complaint dataset annotated with fine-grained aspect and severity labels, showing that it consistently outperforms baseline models, especially in complex complaint scenarios where information is distributed across text and images. This study underscores the value of multimodal interaction and expert validation in practical complaint understanding systems. Resources related to data and codes are available here: https://github.com/sarmistha-D/VALOR

### 20. SymLoc: Symbolic Localization of Hallucination across HaluEval and TruthfulQA

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Naveen Lamba, Sanju Tiwari, Manas Gaur
- **URL**: <http://arxiv.org/abs/2511.14172v1>
- **Submitted**: 2025-11-18 06:16:58
- **Topic Keywords**: rag
- **Reason**: This paper explores the concept of hallucination in LLMs, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on symbolic linguistic knowledge and its impact on hallucination is not directly aligned with the user's primary research themes. While it touches on deep semantic understanding, the context is more specific to LLMs and NLP, rather than general IR and search technologies.

#### Abstract
> LLMs still struggle with hallucination, especially when confronted with symbolic triggers like modifiers, negation, numbers, exceptions, and named entities. Yet, we lack a clear understanding of where these symbolic hallucinations originate, making it crucial to systematically handle such triggers and localize the emergence of hallucination inside the model. While prior work explored localization using statistical techniques like LSC and activation variance analysis, these methods treat all tokens equally and overlook the role symbolic linguistic knowledge plays in triggering hallucinations. So far, no approach has investigated how symbolic elements specifically drive hallucination failures across model layers, nor has symbolic linguistic knowledge been used as the foundation for a localization framework. We propose the first symbolic localization framework that leverages symbolic linguistic and semantic knowledge to meaningfully trace the development of hallucinations across all model layers. By focusing on how models process symbolic triggers, we analyze five models using HaluEval and TruthfulQA. Our symbolic knowledge approach reveals that attention variance for these linguistic elements explodes to critical instability in early layers (2-4), with negation triggering catastrophic variance levels, demonstrating that symbolic semantic processing breaks down from the very beginning. Through the lens of symbolic linguistic knowledge, despite larger model sizes, hallucination rates remain consistently high (78.3%-83.7% across Gemma variants), with steep attention drops for symbolic semantic triggers throughout deeper layers. Our findings demonstrate that hallucination is fundamentally a symbolic linguistic processing failure, not a general generation problem, revealing that symbolic semantic knowledge provides the key to understanding and localizing hallucination mechanisms in LLMs.

### 21. Synthetic Clinical Notes for Rare ICD Codes: A Data-Centric Framework for Long-Tail Medical Coding

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Truong Vo, Weiyi Wu, Kaize Ding
- **URL**: <http://arxiv.org/abs/2511.14112v1>
- **Submitted**: 2025-11-18 03:52:12
- **Comment**: 4 page-short paper
- **Topic Keywords**: rag
- **Reason**: This paper is somewhat related to the user's interests in Natural Language Processing (NLP) and data mining, but it is not directly focused on Information Retrieval (IR) or search technologies. The paper's emphasis on medical coding and long-tail code prediction is not a central match for the user's research themes, but it does involve the use of transformer-based models, which is a related topic.

#### Abstract
> Automatic ICD coding from clinical text is a critical task in medical NLP but remains hindered by the extreme long-tail distribution of diagnostic codes. Thousands of rare and zero-shot ICD codes are severely underrepresented in datasets like MIMIC-III, leading to low macro-F1 scores. In this work, we propose a data-centric framework that generates high-quality synthetic discharge summaries to mitigate this imbalance. Our method constructs realistic multi-label code sets anchored on rare codes by leveraging real-world co-occurrence patterns, ICD descriptions, synonyms, taxonomy, and similar clinical notes. Using these structured prompts, we generate 90,000 synthetic notes covering 7,902 ICD codes, significantly expanding the training distribution. We fine-tune two state-of-the-art transformer-based models, PLM-ICD and GKI-ICD, on both the original and extended datasets. Experiments show that our approach modestly improves macro-F1 while maintaining strong micro-F1, outperforming prior SOTA. While the gain may seem marginal relative to the computational cost, our results demonstrate that carefully crafted synthetic data can enhance equity in long-tail ICD code prediction.

### 22. Effective Diversification of Multi-Carousel Book Recommendation

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Dani√´l Wilten, Gideon Maillette de Buy Wenniger, Arjen Hommersom, Paul Lucassen, Emiel Poortman
- **URL**: <http://arxiv.org/abs/2511.14461v1>
- **Submitted**: 2025-11-18 13:03:16
- **Comment**: Accepted as a conference paper at BNAIC/BeNeLearn 2025; The 37th Benelux Conference on Artificial Intelligence and the 34th Belgian Dutch Conference on Machine Learning
- **Topic Keywords**: recommend
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Search technologies, but it focuses on recommender systems rather than query understanding or ranking models. The use of carousels and diversity metrics is an interesting aspect, but it's not directly related to your primary focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> Using multiple carousels, lists that wrap around and can be scrolled, is the basis for offering content in most contemporary movie streaming platforms. Carousels allow for highlighting different aspects of users' taste, that fall in categories such as genres and authors. However, while carousels offer structure and greater ease of navigation, they alone do not increase diversity in recommendations, while this is essential to keep users engaged. In this work we propose several approaches to effectively increase item diversity within the domain of book recommendations, on top of a collaborative filtering algorithm. These approaches are intended to improve book recommendations in the web catalogs of public libraries. Furthermore, we introduce metrics to evaluate the resulting strategies, and show that the proposed system finds a suitable balance between accuracy and beyond-accuracy aspects.

### 23. LLM-Aligned Geographic Item Tokenization for Local-Life Recommendation

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Hao Jiang, Guoquan Wang, Donglin Zhou, Sheng Yu, Yang Zeng, Wencong Zeng, Kun Gai, Guorui Zhou
- **URL**: <http://arxiv.org/abs/2511.14221v1>
- **Submitted**: 2025-11-18 07:54:32
- **Topic Keywords**: recommend
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval and Natural Language Processing, as it involves text-based recommendation and Large Language Models. However, the focus on geographic item tokenization and local-life recommendation is not directly aligned with your primary research themes, which include query understanding, ranking models, and user behavior modeling.

#### Abstract
> Recent advances in Large Language Models (LLMs) have enhanced text-based recommendation by enriching traditional ID-based methods with semantic generalization capabilities. Text-based methods typically encode item textual information via prompt design and generate discrete semantic IDs through item tokenization. However, in domain-specific tasks such as local-life services, simply injecting location information into prompts fails to capture fine-grained spatial characteristics and real-world distance awareness among items. To address this, we propose LGSID, an LLM-Aligned Geographic Item Tokenization Framework for Local-life Recommendation. This framework consists of two key components: (1) RL-based Geographic LLM Alignment, and (2) Hierarchical Geographic Item Tokenization. In the RL-based alignment module, we initially train a list-wise reward model to capture real-world spatial relationships among items. We then introduce a novel G-DPO algorithm that uses pre-trained reward model to inject generalized spatial knowledge and collaborative signals into LLMs while preserving their semantic understanding. Furthermore, we propose a hierarchical geographic item tokenization strategy, where primary tokens are derived from discrete spatial and content attributes, and residual tokens are refined using the aligned LLM's geographic representation vectors. Extensive experiments on real-world Kuaishou industry datasets show that LGSID consistently outperforms state-of-the-art discriminative and generative recommendation models. Ablation studies, visualizations, and case studies further validate its effectiveness.

### 24. Applying Relation Extraction and Graph Matching to Answering Multiple Choice Questions

- **LLM Score**: 4
- **Keyword Score**: 1
- **Authors**: Naoki Shimoda, Akihiro Yamamoto
- **URL**: <http://arxiv.org/abs/2511.14144v1>
- **Submitted**: 2025-11-18 05:03:27
- **Comment**: Presented at NeLaMKRR@KR, 2025 (arXiv:2511.09575)
- **Topic Keywords**: search
- **Reason**: This paper explores relation extraction and graph matching for answering multiple-choice questions, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on relation extraction and knowledge graphs is not directly aligned with the user's primary interests in deep semantic understanding and real-time relevance optimization in IR.

#### Abstract
> In this research, we combine Transformer-based relation extraction with matching of knowledge graphs (KGs) and apply them to answering multiple-choice questions (MCQs) while maintaining the traceability of the output process. KGs are structured representations of factual knowledge consisting of entities and relations. Due to the high construction cost, they had been regarded as static databases with validated links. However, the recent development of Transformer-based relation extraction (RE) methods has enabled us to generate KGs dynamically by giving them natural language texts, and thereby opened the possibility for representing the meaning of the input sentences with the created KGs. Using this effect, we propose a method that answers MCQs in the "fill-in-the-blank" format, taking care of the point that RE methods generate KGs that represent false information if provided with factually incorrect texts. We measure the truthfulness of each question sentence by (i) converting the sentence into a relational graph using an RE method and (ii) verifying it against factually correct KGs under the closed-world assumption. The experimental results demonstrate that our method correctly answers up to around 70% of the questions, while providing traceability of the procedure. We also highlight that the question category has a vast influence on the accuracy.

### 25. Streamlining Industrial Contract Management with Retrieval-Augmented LLMs

- **LLM Score**: 3
- **Keyword Score**: 4
- **Authors**: Kristi Topollai, Tolga Dimlioglu, Anna Choromanska, Simon Odie, Reginald Hui
- **URL**: <http://arxiv.org/abs/2511.14671v1>
- **Submitted**: 2025-11-18 17:10:57
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper is loosely relevant to your research interests in Information Retrieval, particularly in the context of retrieval-augmented generation (RAG) pipelines. However, the focus on contract management and industrial applications is not directly aligned with your primary interests in query understanding, ranking models, and user behavior modeling. The use of NLP techniques is relevant, but the specific application is not central to your research themes.

#### Abstract
> Contract management involves reviewing and negotiating provisions, individual clauses that define rights, obligations, and terms of agreement. During this process, revisions to provisions are proposed and iteratively refined, some of which may be problematic or unacceptable. Automating this workflow is challenging due to the scarcity of labeled data and the abundance of unstructured legacy contracts. In this paper, we present a modular framework designed to streamline contract management through a retrieval-augmented generation (RAG) pipeline. Our system integrates synthetic data generation, semantic clause retrieval, acceptability classification, and reward-based alignment to flag problematic revisions and generate improved alternatives. Developed and evaluated in collaboration with an industry partner, our system achieves over 80% accuracy in both identifying and optimizing problematic revisions, demonstrating strong performance under real-world, low-resource conditions and offering a practical means of accelerating contract revision workflows.

### 26. HiEAG: Evidence-Augmented Generation for Out-of-Context Misinformation Detection

- **LLM Score**: 2
- **Keyword Score**: 11
- **Authors**: Junjie Wu, Yumeng Fu, Nan Yu, Guohong Fu
- **URL**: <http://arxiv.org/abs/2511.14027v1>
- **Submitted**: 2025-11-18 01:11:48
- **Topic Keywords**: ranking, rerank, rag, retrieval, rank
- **Reason**: This paper focuses on misinformation detection, which is not a core area of interest for you. While it involves multimodal information retrieval and uses large language models, the primary goal is not query understanding, ranking models, or user behavior modeling, making it only loosely relevant to your research interests.

#### Abstract
> Recent advancements in multimodal out-of-context (OOC) misinformation detection have made remarkable progress in checking the consistencies between different modalities for supporting or refuting image-text pairs. However, existing OOC misinformation detection methods tend to emphasize the role of internal consistency, ignoring the significant of external consistency between image-text pairs and external evidence. In this paper, we propose HiEAG, a novel Hierarchical Evidence-Augmented Generation framework to refine external consistency checking through leveraging the extensive knowledge of multimodal large language models (MLLMs). Our approach decomposes external consistency checking into a comprehensive engine pipeline, which integrates reranking and rewriting, apart from retrieval. Evidence reranking module utilizes Automatic Evidence Selection Prompting (AESP) that acquires the relevant evidence item from the products of evidence retrieval. Subsequently, evidence rewriting module leverages Automatic Evidence Generation Prompting (AEGP) to improve task adaptation on MLLM-based OOC misinformation detectors. Furthermore, our approach enables explanation for judgment, and achieves impressive performance with instruction tuning. Experimental results on different benchmark datasets demonstrate that our proposed HiEAG surpasses previous state-of-the-art (SOTA) methods in the accuracy over all samples.

### 27. Steganographic Backdoor Attacks in NLP: Ultra-Low Poisoning and Defense Evasion

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Eric Xue, Ruiyi Zhang, Zijun Zhang, Pengtao Xie
- **URL**: <http://arxiv.org/abs/2511.14301v1>
- **Submitted**: 2025-11-18 09:56:16
- **Topic Keywords**: ltr, rag, search
- **Reason**: This paper focuses on backdoor attacks in NLP, which is a related topic, but it doesn't align with your primary research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling. The paper's emphasis on steganography and data poisoning doesn't seem to be directly relevant to your areas of focus.

#### Abstract
> Transformer models are foundational to natural language processing (NLP) applications, yet remain vulnerable to backdoor attacks introduced through poisoned data, which implant hidden behaviors during training. To strengthen the ability to prevent such compromises, recent research has focused on designing increasingly stealthy attacks to stress-test existing defenses, pairing backdoor behaviors with stylized artifact or token-level perturbation triggers. However, this trend diverts attention from the harder and more realistic case: making the model respond to semantic triggers such as specific names or entities, where a successful backdoor could manipulate outputs tied to real people or events in deployed systems. Motivated by this growing disconnect, we introduce SteganoBackdoor, bringing stealth techniques back into line with practical threat models. Leveraging innocuous properties from natural-language steganography, SteganoBackdoor applies a gradient-guided data optimization process to transform semantic trigger seeds into steganographic carriers that embed a high backdoor payload, remain fluent, and exhibit no representational resemblance to the trigger. Across diverse experimental settings, SteganoBackdoor achieves over 99% attack success at an order-of-magnitude lower data-poisoning rate than prior approaches while maintaining unparalleled evasion against a comprehensive suite of data-level defenses. By revealing this practical and covert attack, SteganoBackdoor highlights an urgent blind spot in current defenses and demands immediate attention to adversarial data defenses and real-world threat modeling.

### 28. EchoAgent: Guideline-Centric Reasoning Agent for Echocardiography Measurement and Interpretation

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Matin Daghyani, Lyuyang Wang, Nima Hashemi, Bassant Medhat, Baraa Abdelsamad, Eros Rojas Velez, XiaoXiao Li, Michael Y. C. Tsang, Christina Luong, Teresa S. M. Tsang, Purang Abolmaesumi
- **URL**: <http://arxiv.org/abs/2511.13948v1>
- **Submitted**: 2025-11-17 22:06:12
- **Comment**: 12 pages, Under Review
- **Topic Keywords**: query, ltr
- **Reason**: This paper appears to be focused on echocardiography measurement and interpretation, which is outside the primary scope of your research interests in Information Retrieval and Search technologies. While it involves some aspects of deep learning and model control, the context and application are not directly related to your core areas of interest.

#### Abstract
> Purpose: Echocardiographic interpretation requires video-level reasoning and guideline-based measurement analysis, which current deep learning models for cardiac ultrasound do not support. We present EchoAgent, a framework that enables structured, interpretable automation for this domain. Methods: EchoAgent orchestrates specialized vision tools under Large Language Model (LLM) control to perform temporal localization, spatial measurement, and clinical interpretation. A key contribution is a measurement-feasibility prediction model that determines whether anatomical structures are reliably measurable in each frame, enabling autonomous tool selection. We curated a benchmark of diverse, clinically validated video-query pairs for evaluation. Results: EchoAgent achieves accurate, interpretable results despite added complexity of spatiotemporal video analysis. Outputs are grounded in visual evidence and clinical guidelines, supporting transparency and traceability. Conclusion: This work demonstrates the feasibility of agentic, guideline-aligned reasoning for echocardiographic video analysis, enabled by task-specific tools and full video-level automation. EchoAgent sets a new direction for trustworthy AI in cardiac ultrasound.

### 29. Tell Me: An LLM-powered Mental Well-being Assistant with RAG, Synthetic Dialogue Generation, and Agentic Planning

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Trishala Jayesh Ahalpara
- **URL**: <http://arxiv.org/abs/2511.14445v1>
- **Submitted**: 2025-11-18 12:43:04
- **Comment**: 8 pages, 2 figures, 1 Table. Submitted to the Computation and Language (cs.CL) category. Uses the ACL-style template. Code and demo will be released at: https://github.com/trystine/Tell_Me_Mental_Wellbeing_System
- **Topic Keywords**: rag, retrieval, search
- **Reason**: This paper is not directly related to your core research interests in Information Retrieval, Search technologies, or Natural Language Processing, as it focuses on mental well-being systems and conversational assistants. While it does involve NLP and large language models, the context and application are quite different from your primary areas of interest.

#### Abstract
> We present Tell Me, a mental well-being system that leverages advances in large language models to provide accessible, context-aware support for users and researchers. The system integrates three components: (i) a retrieval-augmented generation (RAG) assistant for personalized, knowledge-grounded dialogue; (ii) a synthetic client-therapist dialogue generator conditioned on client profiles to facilitate research on therapeutic language and data augmentation; and (iii) a Well-being AI crew, implemented with CrewAI, that produces weekly self-care plans and guided meditation audio. The system is designed as a reflective space for emotional processing rather than a substitute for professional therapy. It illustrates how conversational assistants can lower barriers to support, complement existing care, and broaden access to mental health resources. To address the shortage of confidential therapeutic data, we introduce synthetic client-therapist dialogue generation conditioned on client profiles. Finally, the planner demonstrates an innovative agentic workflow for dynamically adaptive, personalized self-care, bridging the limitations of static well-being tools. We describe the architecture, demonstrate its functionalities, and report evaluation of the RAG assistant in curated well-being scenarios using both automatic LLM-based judgments and a human-user study. This work highlights opportunities for interdisciplinary collaboration between NLP researchers and mental health professionals to advance responsible innovation in human-AI interaction for well-being.

### 30. A Specialized Large Language Model for Clinical Reasoning and Diagnosis in Rare Diseases

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Tao Yang, Dandan Huang, Yunting Lin, Pengfei Wu, Zhikun Wu, Gangyuan Ma, Yulan Lu, Xinran Dong, Dingpeng Li, Junshuang Ge, Zhiyan Zhang, Xuanzhao Huang, Wenyan Nong, Yao Zhou, Hui Tang, Hongxi Yang, Shijie Zhang, Juan Li, Xiaojun Cao, Lin Yang, Xia Gao, Kaishou Xu, Xiaoqiong Gu, Wen Zhang, Huimin Xia, Li Liu, Wenhao Zhou, Mulin Jun Li
- **URL**: <http://arxiv.org/abs/2511.14638v1>
- **Submitted**: 2025-11-18 16:29:19
- **Comment**: 50 pages, 5 figures
- **Topic Keywords**: ctr, retrieval
- **Reason**: This paper focuses on developing a specialized large language model for clinical reasoning and diagnosis in rare diseases, which is outside your primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves large language models and retrieval, the context and application are quite different from your areas of focus.

#### Abstract
> Rare diseases affect hundreds of millions worldwide, yet diagnosis often spans years. Convectional pipelines decouple noisy evidence extraction from downstream inferential diagnosis, and general/medical large language models (LLMs) face scarce real world electronic health records (EHRs), stale domain knowledge, and hallucinations. We assemble a large, domain specialized clinical corpus and a clinician validated reasoning set, and develop RareSeek R1 via staged instruction tuning, chain of thought learning, and graph grounded retrieval. Across multicenter EHR narratives and public benchmarks, RareSeek R1 attains state of the art accuracy, robust generalization, and stability under noisy or overlapping phenotypes. Augmented retrieval yields the largest gains when narratives pair with prioritized variants by resolving ambiguity and aligning candidates to mechanisms. Human studies show performance on par with experienced physicians and consistent gains in assistive use. Notably, transparent reasoning highlights decisive non phenotypic evidence (median 23.1%, such as imaging, interventions, functional tests) underpinning many correct diagnoses. This work advances a narrative first, knowledge integrated reasoning paradigm that shortens the diagnostic odyssey and enables auditable, clinically translatable decision support.

### 31. SMRC: Aligning Large Language Models with Student Reasoning for Mathematical Error Correction

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Biaojie Zeng, Min Zhang, Juan Zhou, Fengrui Liu, Ruiyang Huang, Xin Lin
- **URL**: <http://arxiv.org/abs/2511.14684v1>
- **Submitted**: 2025-11-18 17:22:37
- **Comment**: 13 pages, 3 figures
- **Topic Keywords**: rag, search
- **Reason**: This paper focuses on mathematical error correction using large language models, which is not directly related to information retrieval, search technologies, or user behavior modeling. While it involves deep semantic understanding, the context is educational and not aligned with the user's primary research interests.

#### Abstract
> Large language models (LLMs) often make reasoning errors when solving mathematical problems, and how to automatically detect and correct these errors has become an important research direction. However, existing approaches \textit{mainly focus on self-correction within the model}, which falls short of the ``teacher-style`` correction required in educational settings, \textit{i.e.}, systematically guiding and revising a student's problem-solving process. To address this gap, we propose \texttt{SMRC} (\textit{\underline{S}tudent \underline{M}athematical \underline{R}easoning \underline{C}orrection}), a novel method that aligns LLMs with student reasoning. Specifically, \texttt{SMRC} formulates student reasoning as a multi-step sequential decision problem and introduces Monte Carlo Tree Search (MCTS) to explore optimal correction paths. To reduce the cost of the annotating process-level rewards, we leverage breadth-first search (BFS) guided by LLMs and final-answer evaluation to generate reward signals, which are then distributed across intermediate reasoning steps via a back-propagation mechanism, enabling fine-grained process supervision. Additionally, we construct a benchmark for high school mathematics, MSEB (Multi-Solution Error Benchmark), consisting of 158 instances that include problem statements, student solutions, and correct reasoning steps. We further propose a dual evaluation protocol centered on \textbf{solution accuracy} and \textbf{correct-step retention}, offering a comprehensive measure of educational applicability. Experiments demonstrate that \texttt{SMRC} significantly outperforms existing methods on two public datasets (ProcessBench and MR-GSM8K) and our MSEB in terms of effectiveness and overall performance. The code and data are available at https://github.com/Mind-Lab-ECNU/SMRC.

### 32. Unified Defense for Large Language Models against Jailbreak and Fine-Tuning Attacks in Education

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Xin Yi, Yue Li, Dongsheng Shi, Linlin Wang, Xiaoling Wang, Liang He
- **URL**: <http://arxiv.org/abs/2511.14423v1>
- **Submitted**: 2025-11-18 12:27:51
- **Topic Keywords**: queries
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing, as it focuses on the safety of Large Language Models in educational applications and proposes a framework to mitigate attacks. The paper's scope is limited to the educational domain and does not address query understanding, ranking models, or user behavior modeling, which are central to your research interests.

#### Abstract
> Large Language Models (LLMs) are increasingly integrated into educational applications. However, they remain vulnerable to jailbreak and fine-tuning attacks, which can compromise safety alignment and lead to harmful outputs. Existing studies mainly focus on general safety evaluations, with limited attention to the unique safety requirements of educational scenarios. To address this gap, we construct EduHarm, a benchmark containing safe-unsafe instruction pairs across five representative educational scenarios, enabling systematic safety evaluation of educational LLMs. Furthermore, we propose a three-stage shield framework (TSSF) for educational LLMs that simultaneously mitigates both jailbreak and fine-tuning attacks. First, safety-aware attention realignment redirects attention toward critical unsafe tokens, thereby restoring the harmfulness feature that discriminates between unsafe and safe inputs. Second, layer-wise safety judgment identifies harmfulness features by aggregating safety cues across multiple layers to detect unsafe instructions. Finally, defense-driven dual routing separates safe and unsafe queries, ensuring normal processing for benign inputs and guarded responses for harmful ones. Extensive experiments across eight jailbreak attack strategies demonstrate that TSSF effectively strengthens safety while preventing over-refusal of benign queries. Evaluations on three fine-tuning attack datasets further show that it consistently achieves robust defense against harmful queries while maintaining preserving utility gains from benign fine-tuning.

### 33. Subword Tokenization Strategies for Kurdish Word Embeddings

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ali Salehi, Cassandra L. Jacobs
- **URL**: <http://arxiv.org/abs/2511.14696v1>
- **Submitted**: 2025-11-18 17:33:32
- **Topic Keywords**: rag
- **Reason**: This paper focuses on tokenization strategies for Kurdish word embeddings, which is a topic in Natural Language Processing (NLP). However, it does not align with the user's primary research interests in Information Retrieval (IR), query understanding, ranking models, and user behavior modeling, nor does it involve deep semantic understanding or real-time relevance optimization.

#### Abstract
> We investigate tokenization strategies for Kurdish word embeddings by comparing word-level, morpheme-based, and BPE approaches on morphological similarity preservation tasks. We develop a BiLSTM-CRF morphological segmenter using bootstrapped training from minimal manual annotation and evaluate Word2Vec embeddings across comprehensive metrics including similarity preservation, clustering quality, and semantic organization. Our analysis reveals critical evaluation biases in tokenization comparison. While BPE initially appears superior in morphological similarity, it evaluates only 28.6\% of test cases compared to 68.7\% for morpheme model, creating artificial performance inflation. When assessed comprehensively, morpheme-based tokenization demonstrates superior embedding space organization, better semantic neighborhood structure, and more balanced coverage across morphological complexity levels. These findings highlight the importance of coverage-aware evaluation in low-resource language processing and offers different tokenization methods for low-resourced language processing.

### 34. Leveraging Digitized Newspapers to Collect Summarization Data in Low-Resource Languages

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Noam Dahan, Omer Kidron, Gabriel Stanovsky
- **URL**: <http://arxiv.org/abs/2511.14598v1>
- **Submitted**: 2025-11-18 15:39:48
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to your core research themes in Information Retrieval, Search technologies, or Natural Language Processing. While it involves text summarization, it focuses on collecting summarization data in low-resource languages, which is not a central match to your interests in query understanding, ranking models, or user behavior modeling.

#### Abstract
> High quality summarization data remains scarce in under-represented languages. However, historical newspapers, made available through recent digitization efforts, offer an abundant source of untapped, naturally annotated data. In this work, we present a novel method for collecting naturally occurring summaries via Front-Page Teasers, where editors summarize full length articles. We show that this phenomenon is common across seven diverse languages and supports multi-document summarization. To scale data collection, we develop an automatic process, suited to varying linguistic resource levels. Finally, we apply this process to a Hebrew newspaper title, producing HEBTEASESUM, the first dedicated multi-document summarization dataset in Hebrew.

### 35. O3SLM: Open Weight, Open Data, and Open Vocabulary Sketch-Language Model

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Rishi Gupta, Mukilan Karuppasamy, Shyam Marjit, Aditay Tripathi, Anirban Chakraborty
- **URL**: <http://arxiv.org/abs/2511.14368v1>
- **Submitted**: 2025-11-18 11:18:08
- **Comment**: Accepted to AAAI 2026
- **Topic Keywords**: retrieval
- **Reason**: This paper focuses on Large Vision Language Models (LVLMs) and their ability to interpret abstract visual inputs, specifically hand-drawn sketches. While it involves a form of information retrieval through image retrieval and visual question answering, the primary focus is on LVLMs and their applications, which is not a central match to your research interests in Information Retrieval and Search technologies.

#### Abstract
> While Large Vision Language Models (LVLMs) are increasingly deployed in real-world applications, their ability to interpret abstract visual inputs remains limited. Specifically, they struggle to comprehend hand-drawn sketches, a modality that offers an intuitive means of expressing concepts that are difficult to describe textually. We identify the primary bottleneck as the absence of a large-scale dataset that jointly models sketches, photorealistic images, and corresponding natural language instructions. To address this, we present two key contributions: (1) a new, large-scale dataset of image-sketch-instruction triplets designed to facilitate both pretraining and instruction tuning, and (2) O3SLM, an LVLM trained on this dataset. Comprehensive evaluations on multiple sketch-based tasks: (a) object localization, (b) counting, (c) image retrieval i.e., (SBIR and fine-grained SBIR), and (d) visual question answering (VQA); while incorporating the three existing sketch datasets, namely QuickDraw!, Sketchy, and Tu Berlin, along with our generated SketchVCL dataset, show that O3SLM achieves state-of-the-art performance, substantially outperforming existing LVLMs in sketch comprehension and reasoning.

### 36. The Tokenization Bottleneck: How Vocabulary Extension Improves Chemistry Representation Learning in Pretrained Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Prathamesh Kalamkar, Ned Letcher, Meissane Chami, Sahger Lad, Shayan Mohanty, Prasanna Pendse
- **URL**: <http://arxiv.org/abs/2511.14365v1>
- **Submitted**: 2025-11-18 11:12:35
- **Topic Keywords**: rag
- **Reason**: This paper focuses on chemistry representation learning in pretrained language models, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it involves language models, the context is chemistry and molecular structures, which is not a primary area of interest for the user.

#### Abstract
> The application of large language models (LLMs) to chemistry is frequently hampered by a "tokenization bottleneck", where tokenizers tuned on general-domain text tend to fragment chemical representations such as SMILES into semantically uninformative sub-tokens. This paper introduces a principled methodology to resolve this bottleneck by unifying the representation of natural language and molecular structures within a single model. Our approach involves targeted vocabulary extension-augmenting a pretrained LLM's vocabulary with chemically salient tokens, followed by continued pretraining on chemistry-domain text to integrate this new knowledge. We provide an empirical demonstration of the effectiveness of this strategy, showing that our methodology leads to superior performance on a range of downstream chemical tasks.

### 37. DataSage: Multi-agent Collaboration for Insight Discovery with External Knowledge Retrieval, Multi-role Debating, and Multi-path Reasoning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Xiaochuan Liu, Yuanfeng Song, Xiaoming Yin, Xing Chen
- **URL**: <http://arxiv.org/abs/2511.14299v1>
- **Submitted**: 2025-11-18 09:54:13
- **Topic Keywords**: retrieval
- **Reason**: This paper focuses on data analytics and insight discovery, leveraging large language models and multi-agent collaboration. While it involves knowledge retrieval and reasoning, it is primarily concerned with data analysis and code generation, which is not directly related to information retrieval, search technologies, or user behavior modeling, making it less relevant to your research interests.

#### Abstract
> In today's data-driven era, fully automated end-to-end data analytics, particularly insight discovery, is critical for discovering actionable insights that assist organizations in making effective decisions. With the rapid advancement of large language models (LLMs), LLM-driven agents have emerged as a promising paradigm for automating data analysis and insight discovery. However, existing data insight agents remain limited in several key aspects, often failing to deliver satisfactory results due to: (1) insufficient utilization of domain knowledge, (2) shallow analytical depth, and (3) error-prone code generation during insight generation. To address these issues, we propose DataSage, a novel multi-agent framework that incorporates three innovative features including external knowledge retrieval to enrich the analytical context, a multi-role debating mechanism to simulate diverse analytical perspectives and deepen analytical depth, and multi-path reasoning to improve the accuracy of the generated code and insights. Extensive experiments on InsightBench demonstrate that DataSage consistently outperforms existing data insight agents across all difficulty levels, offering an effective solution for automated data insight discovery.

### 38. Entropy-Guided Reasoning Compression

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Hourun Zhu, Yang Gao, Wenlong Fei, Jiawei Li, Huashan Sun
- **URL**: <http://arxiv.org/abs/2511.14258v1>
- **Submitted**: 2025-11-18 08:48:58
- **Comment**: 10pages, 4 figures
- **Topic Keywords**: rag
- **Reason**: This paper focuses on reasoning compression in large models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves optimization and deployability, the context is more aligned with AI model efficiency rather than semantic understanding or real-time relevance.

#### Abstract
> Large reasoning models have demonstrated remarkable performance on complex reasoning tasks, yet the excessive length of their chain-of-thought outputs remains a major practical bottleneck due to high computation cost and poor deployability. Existing compression methods have achieved partial success but overlook a crucial phenomenon in the training process -- the entropy conflict. During compression training, entropy decreases, leading to shorter reasoning but limited exploration, while accuracy-oriented objectives increase entropy, lengthening reasoning chains. This can cause the model to get stuck in a local dilemma. Our analysis further reveals the origin of the entropy conflict: many high-entropy tokens are logical connectors that receive larger gradients and are encouraged under the performance objective, while the compression objective simultaneously penalizes these potentially redundant connectors. This opposing pressure creates a direct source of entropy conflict. To address these issues, we adopt an entropy-guided training framework. As entropy descends, the model is guided toward efficient reasoning by encouraging concise thought steps; as entropy rises, exploration is reinforced under the compact reasoning mode to improve robustness. Experiments on six mathematical benchmarks show that our method compresses reasoning length to 20% of the original while maintaining or even surpassing baseline accuracy. Code and models will be released publicly.

### 39. Towards Authentic Movie Dubbing with Retrieve-Augmented Director-Actor Interaction Learning

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Rui Liu, Yuan Zhao, Zhenqi Jia
- **URL**: <http://arxiv.org/abs/2511.14249v1>
- **Submitted**: 2025-11-18 08:39:44
- **Comment**: Accepted by AAAI 2026
- **Topic Keywords**: retrieval
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The focus is on movie dubbing and multimodal interaction learning, which is outside your primary areas of interest.

#### Abstract
> The automatic movie dubbing model generates vivid speech from given scripts, replicating a speaker's timbre from a brief timbre prompt while ensuring lip-sync with the silent video. Existing approaches simulate a simplified workflow where actors dub directly without preparation, overlooking the critical director-actor interaction. In contrast, authentic workflows involve a dynamic collaboration: directors actively engage with actors, guiding them to internalize the context cues, specifically emotion, before performance. To address this issue, we propose a new Retrieve-Augmented Director-Actor Interaction Learning scheme to achieve authentic movie dubbing, termed Authentic-Dubber, which contains three novel mechanisms: (1) We construct a multimodal Reference Footage library to simulate the learning footage provided by directors. Note that we integrate Large Language Models (LLMs) to achieve deep comprehension of emotional representations across multimodal signals. (2) To emulate how actors efficiently and comprehensively internalize director-provided footage during dubbing, we propose an Emotion-Similarity-based Retrieval-Augmentation strategy. This strategy retrieves the most relevant multimodal information that aligns with the target silent video. (3) We develop a Progressive Graph-based speech generation approach that incrementally incorporates the retrieved multimodal emotional knowledge, thereby simulating the actor's final dubbing process. The above mechanisms enable the Authentic-Dubber to faithfully replicate the authentic dubbing workflow, achieving comprehensive improvements in emotional expressiveness. Both subjective and objective evaluations on the V2C Animation benchmark dataset validate the effectiveness. The code and demos are available at https://github.com/AI-S2-Lab/Authentic-Dubber.

### 40. GRPO Privacy Is at Risk: A Membership Inference Attack Against Reinforcement Learning With Verifiable Rewards

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yule Liu, Heyi Zhang, Jinyi Zheng, Zhen Sun, Zifan Peng, Tianshuo Cong, Yilong Yang, Xinlei He, Zhuo Ma
- **URL**: <http://arxiv.org/abs/2511.14045v1>
- **Submitted**: 2025-11-18 01:51:34
- **Topic Keywords**: rag
- **Reason**: This paper focuses on membership inference attacks in Reinforcement Learning with Verifiable Rewards, which is not directly related to your core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on deep learning models, the context is privacy risks and attacks, which is not a central area of interest for you.

#### Abstract
> Membership inference attacks (MIAs) on large language models (LLMs) pose significant privacy risks across various stages of model training. Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have brought a profound paradigm shift in LLM training, particularly for complex reasoning tasks. However, the on-policy nature of RLVR introduces a unique privacy leakage pattern: since training relies on self-generated responses without fixed ground-truth outputs, membership inference must now determine whether a given prompt (independent of any specific response) is used during fine-tuning. This creates a threat where leakage arises not from answer memorization.
  To audit this novel privacy risk, we propose Divergence-in-Behavior Attack (DIBA), the first membership inference framework specifically designed for RLVR. DIBA shifts the focus from memorization to behavioral change, leveraging measurable shifts in model behavior across two axes: advantage-side improvement (e.g., correctness gain) and logit-side divergence (e.g., policy drift). Through comprehensive evaluations, we demonstrate that DIBA significantly outperforms existing baselines, achieving around 0.8 AUC and an order-of-magnitude higher TPR@0.1%FPR. We validate DIBA's superiority across multiple settings--including in-distribution, cross-dataset, cross-algorithm, black-box scenarios, and extensions to vision-language models. Furthermore, our attack remains robust under moderate defensive measures.
  To the best of our knowledge, this is the first work to systematically analyze privacy vulnerabilities in RLVR, revealing that even in the absence of explicit supervision, training data exposure can be reliably inferred through behavioral traces.

### 41. Strategic Innovation Management in the Age of Large Language Models Market Intelligence, Adaptive R&D, and Ethical Governance

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Raha Aghaei, Ali A. Kiaei, Mahnaz Boush, Mahan Rofoosheh, Mohammad Zavvar
- **URL**: <http://arxiv.org/abs/2511.14709v1>
- **Submitted**: 2025-11-18 17:50:39
- **Topic Keywords**: search
- **Reason**: This paper appears to be unrelated to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. The focus on Strategic Innovation Management and Large Language Models in a business context does not align with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> This study analyzes the multiple functions of Large Language Models (LLMs) in transforming research and development (R&D) processes. By automating knowledge discovery, boosting hypothesis creation, integrating transdisciplinary insights, and enabling cooperation within innovation ecosystems, LLMs dramatically improve the efficiency and effectiveness of research processes. Through extensive analysis of scientific literature, patent databases, and experimental data, these models enable more flexible and informed R&D workflows, ultimately accelerating innovation cycles and lowering time-to-market for breakthrough ideas.

### 42. Encoding and Understanding Astrophysical Information in Large Language Model-Generated Summaries

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Kiera McCormick, Rafael Mart√≠nez-Galarza
- **URL**: <http://arxiv.org/abs/2511.14685v1>
- **Submitted**: 2025-11-18 17:23:29
- **Comment**: Accepted to the Machine Learning and the Physical Sciences Workshop at NeurIPS 2025, 11 pages, 4 figures
- **Topic Keywords**: search
- **Reason**: This paper appears to be unrelated to your primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it involves Large Language Models, the focus is on encoding astrophysical information, which is outside your domain of expertise and interest in e-commerce.

#### Abstract
> Large Language Models have demonstrated the ability to generalize well at many levels across domains, modalities, and even shown in-context learning capabilities. This enables research questions regarding how they can be used to encode physical information that is usually only available from scientific measurements, and loosely encoded in textual descriptions. Using astrophysics as a test bed, we investigate if LLM embeddings can codify physical summary statistics that are obtained from scientific measurements through two main questions: 1) Does prompting play a role on how those quantities are codified by the LLM? and 2) What aspects of language are most important in encoding the physics represented by the measurement? We investigate this using sparse autoencoders that extract interpretable features from the text.

### 43. Graded strength of comparative illusions is explained by Bayesian inference

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Yuhan Zhang, Erxiao Wang, Cory Shain
- **URL**: <http://arxiv.org/abs/2511.14642v1>
- **Submitted**: 2025-11-18 16:33:19
- **Comment**: 49 pages, 7 figures
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing, as it focuses on a specific linguistic phenomenon (comparative illusions) and its explanation through Bayesian inference, without any apparent connection to query understanding, ranking models, user behavior modeling, or real-time relevance optimization.

#### Abstract
> Like visual processing, language processing is susceptible to illusions in which people systematically misperceive stimuli. In one such case--the comparative illusion (CI), e.g., More students have been to Russia than I have--comprehenders tend to judge the sentence as acceptable despite its underlying nonsensical comparison. Prior research has argued that this phenomenon can be explained as Bayesian inference over a noisy channel: the posterior probability of an interpretation of a sentence is proportional to both the prior probability of that interpretation and the likelihood of corruption into the observed (CI) sentence. Initial behavioral work has supported this claim by evaluating a narrow set of alternative interpretations of CI sentences and showing that comprehenders favor interpretations that are more likely to have been corrupted into the illusory sentence. In this study, we replicate and go substantially beyond this earlier work by directly predicting the strength of illusion with a quantitative model of the posterior probability of plausible interpretations, which we derive through a novel synthesis of statistical language models with human behavioral data. Our model explains not only the fine gradations in the strength of CI effects, but also a previously unexplained effect caused by pronominal vs. full noun phrase than-clause subjects. These findings support a noisy-channel theory of sentence comprehension by demonstrating that the theory makes novel predictions about the comparative illusion that bear out empirically. This outcome joins related evidence of noisy channel processing in both illusory and non-illusory contexts to support noisy channel inference as a unified computational-level theory of diverse language processing phenomena.

### 44. AfriSpeech-MultiBench: A Verticalized Multidomain Multicountry Benchmark Suite for African Accented English ASR

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Gabrial Zencha Ashungafac, Mardhiyah Sanni, Busayo Awobade, Alex Gichamba, Tobi Olatunji
- **URL**: <http://arxiv.org/abs/2511.14255v1>
- **Submitted**: 2025-11-18 08:44:17
- **Comment**: Accepted As a Conference Paper IJCNLP-AACL 2025
- **Topic Keywords**: search
- **Reason**: This paper focuses on speech recognition and voice interfaces, which is outside your primary research interests in Information Retrieval and Search technologies. While it touches on Natural Language Processing, the context is specific to African accented English ASR and does not align with your core themes of query understanding, ranking models, and user behavior modeling.

#### Abstract
> Recent advances in speech-enabled AI, including Google's NotebookLM and OpenAI's speech-to-speech API, are driving widespread interest in voice interfaces globally. Despite this momentum, there exists no publicly available application-specific model evaluation that caters to Africa's linguistic diversity. We present AfriSpeech-MultiBench, the first domain-specific evaluation suite for over 100 African English accents across 10+ countries and seven application domains: Finance, Legal, Medical, General dialogue, Call Center, Named Entities and Hallucination Robustness. We benchmark a diverse range of open, closed, unimodal ASR and multimodal LLM-based speech recognition systems using both spontaneous and non-spontaneous speech conversation drawn from various open African accented English speech datasets. Our empirical analysis reveals systematic variation: open-source ASR models excels in spontaneous speech contexts but degrades on noisy, non-native dialogue; multimodal LLMs are more accent-robust yet struggle with domain-specific named entities; proprietary models deliver high accuracy on clean speech but vary significantly by country and domain. Models fine-tuned on African English achieve competitive accuracy with lower latency, a practical advantage for deployment, hallucinations still remain a big problem for most SOTA models. By releasing this comprehensive benchmark, we empower practitioners and researchers to select voice technologies suited to African use-cases, fostering inclusive voice applications for underserved communities.

### 45. When AI Does Science: Evaluating the Autonomous AI Scientist KOSMOS in Radiation Biology

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Humza Nusrat, Omar Nusrat
- **URL**: <http://arxiv.org/abs/2511.13825v1>
- **Submitted**: 2025-11-17 19:00:03
- **Comment**: 13 pages, 3 figures, preprint
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper focuses on the application of AI in radiation biology, which is outside your primary areas of interest.

#### Abstract
> Agentic AI "scientists" now use language models to search the literature, run analyses, and generate hypotheses. We evaluate KOSMOS, an autonomous AI scientist, on three problems in radiation biology using simple random-gene null benchmarks. Hypothesis 1: baseline DNA damage response (DDR) capacity across cell lines predicts the p53 transcriptional response after irradiation (GSE30240). Hypothesis 2: baseline expression of OGT and CDO1 predicts the strength of repressed and induced radiation-response modules in breast cancer cells (GSE59732). Hypothesis 3: a 12-gene expression signature predicts biochemical recurrence-free survival after prostate radiotherapy plus androgen deprivation therapy (GSE116918). The DDR-p53 hypothesis was not supported: DDR score and p53 response were weakly negatively correlated (Spearman rho = -0.40, p = 0.76), indistinguishable from random five-gene scores. OGT showed only a weak association (r = 0.23, p = 0.34), whereas CDO1 was a clear outlier (r = 0.70, empirical p = 0.0039). The 12-gene signature achieved a concordance index of 0.61 (p = 0.017) but a non-unique effect size. Overall, KOSMOS produced one well-supported discovery, one plausible but uncertain result, and one false hypothesis, illustrating that AI scientists can generate useful ideas but require rigorous auditing against appropriate null models.

### 46. CORGI: Efficient Pattern Matching With Quadratic Guarantees

- **LLM Score**: 0
- **Keyword Score**: 3
- **Authors**: Daniel Weitekamp
- **URL**: <http://arxiv.org/abs/2511.13942v1>
- **Submitted**: 2025-11-17 21:49:39
- **Topic Keywords**: query
- **Reason**: LLM scoring failed.

#### Abstract
> Rule-based systems must solve complex matching problems within tight time constraints to be effective in real-time applications, such as planning and reactive control for AI agents, as well as low-latency relational database querying. Pattern-matching systems can encounter issues where exponential time and space are required to find matches for rules with many underconstrained variables, or which produce combinatorial intermediate partial matches (but are otherwise well-constrained). When online AI systems automatically generate rules from example-driven induction or code synthesis, they can easily produce worst-case matching patterns that slow or halt program execution by exceeding available memory. In our own work with cognitive systems that learn from example, we've found that aggressive forms of anti-unification-based generalization can easily produce these circumstances. To make these systems practical without hand-engineering constraints or succumbing to unpredictable failure modes, we introduce a new matching algorithm called CORGI (Collection-Oriented Relational Graph Iteration). Unlike RETE-based approaches, CORGI offers quadratic time and space guarantees for finding single satisficing matches, and the ability to iteratively stream subsequent matches without committing entire conflict sets to memory. CORGI differs from RETE in that it does not have a traditional $Œ≤$-memory for collecting partial matches. Instead, CORGI takes a two-step approach: a graph of grounded relations is built/maintained in a forward pass, and an iterator generates matches as needed by working backward through the graph. This approach eliminates the high-latency delays and memory overflows that can result from populating full conflict sets. In a performance evaluation, we demonstrate that CORGI significantly outperforms RETE implementations from SOAR and OPS5 on a simple combinatorial matching task.

### 47. A Method for Characterizing Disease Progression from Acute Kidney Injury to Chronic Kidney Disease

- **LLM Score**: 0
- **Keyword Score**: 2
- **Authors**: Yilu Fang, Jordan G. Nestor, Casey N. Ta, Jerard Z. Kneifati-Hayek, Chunhua Weng
- **URL**: <http://arxiv.org/abs/2511.14603v1>
- **Submitted**: 2025-11-18 15:53:31
- **Topic Keywords**: ctr
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, Natural Language Processing, data mining, or recommender systems. The paper focuses on disease progression and electronic health record analysis, which is outside your primary areas of interest.

#### Abstract
> Patients with acute kidney injury (AKI) are at high risk of developing chronic kidney disease (CKD), but identifying those at greatest risk remains challenging. We used electronic health record (EHR) data to dynamically track AKI patients' clinical evolution and characterize AKI-to-CKD progression. Post-AKI clinical states were identified by clustering patient vectors derived from longitudinal medical codes and creatinine measurements. Transition probabilities between states and progression to CKD were estimated using multi-state modeling. After identifying common post-AKI trajectories, CKD risk factors in AKI subpopulations were identified through survival analysis. Of 20,699 patients with AKI at admission, 3,491 (17%) developed CKD. We identified fifteen distinct post-AKI states, each with different probabilities of CKD development. Most patients (75%, n=15,607) remained in a single state or made only one transition during the study period. Both established (e.g., AKI severity, diabetes, hypertension, heart failure, liver disease) and novel CKD risk factors, with their impact varying across these clinical states. This study demonstrates a data-driven approach for identifying high-risk AKI patients, supporting the development of decision-support tools for early CKD detection and intervention.

### 48. ArbESC+: Arabic Enhanced Edit Selection System Combination for Grammatical Error Correction Resolving conflict and improving system combination in Arabic GEC

- **LLM Score**: 0
- **Keyword Score**: 1
- **Authors**: Ahlam Alrehili, Areej Alhothali
- **URL**: <http://arxiv.org/abs/2511.14230v1>
- **Submitted**: 2025-11-18 08:06:28
- **Comment**: 26 pages
- **Topic Keywords**: search
- **Reason**: LLM scoring failed.

#### Abstract
> Grammatical Error Correction (GEC) is an important aspect of natural language processing. Arabic has a complicated morphological and syntactic structure, posing a greater challenge than other languages. Even though modern neural models have improved greatly in recent years, the majority of previous attempts used individual models without taking into account the potential benefits of combining different systems. In this paper, we present one of the first multi-system approaches for correcting grammatical errors in Arabic, the Arab Enhanced Edit Selection System Complication (ArbESC+). Several models are used to collect correction proposals, which are represented as numerical features in the framework. A classifier determines and implements the appropriate corrections based on these features. In order to improve output quality, the framework uses support techniques to filter overlapping corrections and estimate decision reliability. A combination of AraT5, ByT5, mT5, AraBART, AraBART+Morph+GEC, and Text editing systems gave better results than a single model alone, with F0.5 at 82.63% on QALB-14 test data, 84.64% on QALB-15 L1 data, and 65.55% on QALB-15 L2 data. As one of the most significant contributions of this work, it's the first Arab attempt to integrate linguistic error correction. Improving existing models provides a practical step towards developing advanced tools that will benefit users and researchers of Arabic text processing.

---

