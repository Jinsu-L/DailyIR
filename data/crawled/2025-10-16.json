[
    {
        "title": "Generative Universal Verifier as Multimodal Meta-Reasoner",
        "abstract": "We introduce Generative Universal Verifier, a novel concept and plugin\ndesigned for next-generation multimodal reasoning in vision-language models and\nunified multimodal models, providing the fundamental capability of reflection\nand refinement on visual outcomes during the reasoning and generation process.\nThis work makes three main contributions: (1) We build ViVerBench, a\ncomprehensive benchmark spanning 16 categories of critical tasks for evaluating\nvisual outcomes in multimodal reasoning. Results show that existing VLMs\nconsistently underperform across these tasks, underscoring a substantial gap\nfrom human-level capability in reliable visual verification. (2) We design two\nautomated pipelines to construct large-scale visual verification data and train\nOmniVerifier-7B, the first omni-capable generative verifier trained for\nuniversal visual verification and achieves notable gains on ViVerBench(+8.3).\nThrough training, we identify three atomic capabilities in visual verification\nand demonstrate how they generalize and interact synergistically. (3) We\npropose OmniVerifier-TTS, a sequential test-time scaling paradigm that\nleverages the universal verifier to bridge image generation and editing within\nunified models, enhancing the upper bound of generative ability through\niterative fine-grained optimization. Beyond generation, we extend universal\nverifier to broader world-modeling interleaved reasoning scenarios.\nEmpirically, OmniVerifier-TTS achieves improvements on T2I-ReasonBench(+3.7),\nand GenEval++(+4.3), outperforming existing parallel test-time scaling methods,\nsuch as Best-of-N. By endowing multimodal reasoning with reliable visual\nverification, OmniVerifier advances both reliable reflection during generation\nand scalable test-time refinement, marking a step toward more trustworthy and\ncontrollable next-generation reasoning systems.",
        "url": "http://arxiv.org/abs/2510.13804v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13804v1",
        "arxiv_id": "2510.13804v1",
        "authors": [
            "Xinchen Zhang",
            "Xiaoying Zhang",
            "Youbin Wu",
            "Yanbin Cao",
            "Renrui Zhang",
            "Ruihang Chu",
            "Ling Yang",
            "Yujiu Yang"
        ],
        "submitted": "2025-10-15 17:59:24",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "BRIEF-Pro: Universal Context Compression with Short-to-Long Synthesis for Fast and Accurate Multi-Hop Reasoning",
        "abstract": "As retrieval-augmented generation (RAG) tackles complex tasks, increasingly\nexpanded contexts offer richer information, but at the cost of higher latency\nand increased cognitive load on the model. To mitigate this bottleneck,\nespecially for intricate multi-hop questions, we introduce BRIEF-Pro. It is a\nuniversal, lightweight compressor that distills relevant evidence for a given\nquery from retrieved documents into a concise summary for seamless integration\ninto in-context RAG. Using seed data consisting of relatively short contexts\n(fewer than 1k words), BRIEF-Pro is trained to perform abstractive compression\nof extended contexts exceeding 10k words across a wide range of scenarios.\nFurthermore, BRIEF-Pro offers flexible user control over summary length by\nallowing users to specify the desired number of sentences. Experiments on four\nopen-domain multi-hop question-answering datasets show that BRIEF-Pro generates\nmore concise and relevant summaries, enhancing performance across small, large,\nand proprietary language models. With the 70B reader model, 32x compression by\nBRIEF-Pro improves QA performance by 4.67% on average over LongLLMLingua's 9x,\nwhile requiring only 23% of its computational overhead.",
        "url": "http://arxiv.org/abs/2510.13799v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13799v1",
        "arxiv_id": "2510.13799v1",
        "authors": [
            "Jia-Chen Gu",
            "Junyi Zhang",
            "Di Wu",
            "Yuankai Li",
            "Kai-Wei Chang",
            "Nanyun Peng"
        ],
        "submitted": "2025-10-15 17:57:45",
        "source": "arxiv",
        "comment": "Code and data: https://github.com/JasonForJoy/BRIEF"
    },
    {
        "title": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression Beacons",
        "abstract": "The scalability of large language models for long-context reasoning is\nseverely constrained by the linear growth of their Transformer key-value cache,\nwhich incurs significant memory and computational costs. We posit that as a\nmodel generates reasoning tokens, the informational value of past generated\ntokens diminishes, creating an opportunity for compression. In this work, we\npropose to periodically compress the generation KV cache with a learned,\nspecial-purpose token and evict compressed entries. We train the model to\nperform this compression via a modified joint distillation and reinforcement\nlearning (RL) framework. Our training method minimizes overhead over the\nconventional RL process, as it leverages RL outputs for distillation.\nEmpirically, our method achieves a superior memory-accuracy Pareto frontier\ncompared to both the model without cache compression and training-free\ncompression techniques.",
        "url": "http://arxiv.org/abs/2510.13797v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13797v1",
        "arxiv_id": "2510.13797v1",
        "authors": [
            "Giovanni Monea",
            "Yair Feldman",
            "Shankar Padmanabhan",
            "Kianté Brantley",
            "Yoav Artzi"
        ],
        "submitted": "2025-10-15 17:57:21",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Mechanistic Emergence of Symbol Grounding in Language Models",
        "abstract": "Symbol grounding (Harnad, 1990) describes how symbols such as words acquire\ntheir meanings by connecting to real-world sensorimotor experiences. Recent\nwork has shown preliminary evidence that grounding may emerge in\n(vision-)language models trained at scale without using explicit grounding\nobjectives. Yet, the specific loci of this emergence and the mechanisms that\ndrive it remain largely unexplored. To address this problem, we introduce a\ncontrolled evaluation framework that systematically traces how symbol grounding\narises within the internal computations through mechanistic and causal\nanalysis. Our findings show that grounding concentrates in middle-layer\ncomputations and is implemented through the aggregate mechanism, where\nattention heads aggregate the environmental ground to support the prediction of\nlinguistic forms. This phenomenon replicates in multimodal dialogue and across\narchitectures (Transformers and state-space models), but not in unidirectional\nLSTMs. Our results provide behavioral and mechanistic evidence that symbol\ngrounding can emerge in language models, with practical implications for\npredicting and potentially controlling the reliability of generation.",
        "url": "http://arxiv.org/abs/2510.13796v2",
        "pdf_url": "http://arxiv.org/pdf/2510.13796v2",
        "arxiv_id": "2510.13796v2",
        "authors": [
            "Shuyu Wu",
            "Ziqiao Ma",
            "Xiaoxi Luo",
            "Yidong Huang",
            "Josue Torres-Fonseca",
            "Freda Shi",
            "Joyce Chai"
        ],
        "submitted": "2025-10-15 17:56:15",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Confidence-Based Response Abstinence: Improving LLM Trustworthiness via Activation-Based Uncertainty Estimation",
        "abstract": "We propose a method for confidence estimation in retrieval-augmented\ngeneration (RAG) systems that aligns closely with the correctness of large\nlanguage model (LLM) outputs. Confidence estimation is especially critical in\nhigh-stakes domains such as finance and healthcare, where the cost of an\nincorrect answer outweighs that of not answering the question. Our approach\nextends prior uncertainty quantification methods by leveraging raw feed-forward\nnetwork (FFN) activations as auto-regressive signals, avoiding the information\nloss inherent in token logits and probabilities after projection and softmax\nnormalization. We model confidence prediction as a sequence classification\ntask, and regularize training with a Huber loss term to improve robustness\nagainst noisy supervision. Applied in a real-world financial industry\ncustomer-support setting with complex knowledge bases, our method outperforms\nstrong baselines and maintains high accuracy under strict latency constraints.\nExperiments on Llama 3.1 8B model show that using activations from only the\n16th layer preserves accuracy while reducing response latency. Our results\ndemonstrate that activation-based confidence modeling offers a scalable,\narchitecture-aware path toward trustworthy RAG deployment.",
        "url": "http://arxiv.org/abs/2510.13750v2",
        "pdf_url": "http://arxiv.org/pdf/2510.13750v2",
        "arxiv_id": "2510.13750v2",
        "authors": [
            "Zhiqi Huang",
            "Vivek Datla",
            "Chenyang Zhu",
            "Alfy Samuel",
            "Daben Liu",
            "Anoop Kumar",
            "Ritesh Soni"
        ],
        "submitted": "2025-10-15 16:55:56",
        "source": "arxiv",
        "comment": "UncertaiNLP at EMNLP 2025"
    },
    {
        "title": "Assessing Web Search Credibility and Response Groundedness in Chat Assistants",
        "abstract": "Chat assistants increasingly integrate web search functionality, enabling\nthem to retrieve and cite external sources. While this promises more reliable\nanswers, it also raises the risk of amplifying misinformation from\nlow-credibility sources. In this paper, we introduce a novel methodology for\nevaluating assistants' web search behavior, focusing on source credibility and\nthe groundedness of responses with respect to cited sources. Using 100 claims\nacross five misinformation-prone topics, we assess GPT-4o, GPT-5, Perplexity,\nand Qwen Chat. Our findings reveal differences between the assistants, with\nPerplexity achieving the highest source credibility, whereas GPT-4o exhibits\nelevated citation of non-credibility sources on sensitive topics. This work\nprovides the first systematic comparison of commonly used chat assistants for\nfact-checking behavior, offering a foundation for evaluating AI systems in\nhigh-stakes information environments.",
        "url": "http://arxiv.org/abs/2510.13749v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13749v1",
        "arxiv_id": "2510.13749v1",
        "authors": [
            "Ivan Vykopal",
            "Matúš Pikuliak",
            "Simon Ostermann",
            "Marián Šimko"
        ],
        "submitted": "2025-10-15 16:55:47",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier Math",
        "abstract": "Large language model (LLM)-based reasoning systems have recently achieved\ngold medal-level performance in the IMO 2025 competition, writing mathematical\nproofs where, to receive full credit, each step must be not only correct but\nalso sufficiently supported. To train LLM-based reasoners in such challenging,\nopen-ended settings, strong verifiers capable of catching step-level mistakes\nare necessary prerequisites. We introduce Hard2Verify, a human-annotated,\nstep-level verification benchmark produced with over 500 hours of human labor.\nHard2Verify is designed to rigorously assess step-level verifiers at the\nfrontier: Verifiers must provide step-level annotations or identify the first\nerror in responses generated by frontier LLMs for very recent, challenging, and\nopen-ended math questions. We evaluate 29 generative critics and process reward\nmodels, demonstrating that, beyond a few standouts, open-source verifiers lag\nclosed source models. We subsequently analyze what drives poor performance in\nstep-level verification, the impacts of scaling verifier compute, as well as\nfundamental questions such as self-verification and verification-generation\ndynamics.",
        "url": "http://arxiv.org/abs/2510.13744v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13744v1",
        "arxiv_id": "2510.13744v1",
        "authors": [
            "Shrey Pandit",
            "Austin Xu",
            "Xuan-Phi Nguyen",
            "Yifei Ming",
            "Caiming Xiong",
            "Shafiq Joty"
        ],
        "submitted": "2025-10-15 16:50:54",
        "source": "arxiv",
        "comment": "21 pages, 8 figures, 5 tables"
    },
    {
        "title": "HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based Sequential Recommendation",
        "abstract": "Large language models (LLMs) have recently demonstrated strong potential for\nsequential recommendation. However, current LLM-based approaches face critical\nlimitations in modeling users' long-term and diverse interests. First, due to\ninference latency and feature fetching bandwidth constraints, existing methods\ntypically truncate user behavior sequences to include only the most recent\ninteractions, resulting in the loss of valuable long-range preference signals.\nSecond, most current methods rely on next-item prediction with a single\npredicted embedding, overlooking the multifaceted nature of user interests and\nlimiting recommendation diversity. To address these challenges, we propose\nHyMiRec, a hybrid multi-interest sequential recommendation framework, which\nleverages a lightweight recommender to extracts coarse interest embeddings from\nlong user sequences and an LLM-based recommender to captures refined interest\nembeddings. To alleviate the overhead of fetching features, we introduce a\nresidual codebook based on cosine similarity, enabling efficient compression\nand reuse of user history embeddings. To model the diverse preferences of\nusers, we design a disentangled multi-interest learning module, which leverages\nmultiple interest queries to learn disentangles multiple interest signals\nadaptively, allowing the model to capture different facets of user intent.\nExtensive experiments are conducted on both benchmark datasets and a collected\nindustrial dataset, demonstrating our effectiveness over existing\nstate-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec\nbrings consistent improvements in real-world recommendation systems.",
        "url": "http://arxiv.org/abs/2510.13738v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13738v1",
        "arxiv_id": "2510.13738v1",
        "authors": [
            "Jingyi Zhou",
            "Cheng Chen",
            "Kai Zuo",
            "Manjie Xu",
            "Zhendong Fu",
            "Yibo Chen",
            "Xu Tang",
            "Yao Hu"
        ],
        "submitted": "2025-10-15 16:45:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "GAPS: A Clinically Grounded, Automated Benchmark for Evaluating AI Clinicians",
        "abstract": "Current benchmarks for AI clinician systems, often based on multiple-choice\nexams or manual rubrics, fail to capture the depth, robustness, and safety\nrequired for real-world clinical practice. To address this, we introduce the\nGAPS framework, a multidimensional paradigm for evaluating \\textbf{G}rounding\n(cognitive depth), \\textbf{A}dequacy (answer completeness),\n\\textbf{P}erturbation (robustness), and \\textbf{S}afety. Critically, we\ndeveloped a fully automated, guideline-anchored pipeline to construct a\nGAPS-aligned benchmark end-to-end, overcoming the scalability and subjectivity\nlimitations of prior work. Our pipeline assembles an evidence neighborhood,\ncreates dual graph and tree representations, and automatically generates\nquestions across G-levels. Rubrics are synthesized by a DeepResearch agent that\nmimics GRADE-consistent, PICO-driven evidence review in a ReAct loop. Scoring\nis performed by an ensemble of large language model (LLM) judges. Validation\nconfirmed our automated questions are high-quality and align with clinician\njudgment. Evaluating state-of-the-art models on the benchmark revealed key\nfailure modes: performance degrades sharply with increased reasoning depth\n(G-axis), models struggle with answer completeness (A-axis), and they are\nhighly vulnerable to adversarial perturbations (P-axis) as well as certain\nsafety issues (S-axis). This automated, clinically-grounded approach provides a\nreproducible and scalable method for rigorously evaluating AI clinician systems\nand guiding their development toward safer, more reliable clinical practice.",
        "url": "http://arxiv.org/abs/2510.13734v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13734v1",
        "arxiv_id": "2510.13734v1",
        "authors": [
            "Xiuyuan Chen",
            "Tao Sun",
            "Dexin Su",
            "Ailing Yu",
            "Junwei Liu",
            "Zhe Chen",
            "Gangzeng Jin",
            "Xin Wang",
            "Jingnan Liu",
            "Hansong Xiao",
            "Hualei Zhou",
            "Dongjie Tao",
            "Chunxiao Guo",
            "Minghui Yang",
            "Yuan Xia",
            "Jing Zhao",
            "Qianrui Fan",
            "Yanyun Wang",
            "Shuai Zhen",
            "Kezhong Chen",
            "Jun Wang",
            "Zewen Sun",
            "Heng Zhao",
            "Tian Guan",
            "Shaodong Wang",
            "Geyun Chang",
            "Jiaming Deng",
            "Hongchengcheng Chen",
            "Kexin Feng",
            "Ruzhen Li",
            "Jiayi Geng",
            "Changtai Zhao",
            "Jun Wang",
            "Guihu Lin",
            "Peihao Li",
            "Liqi Liu",
            "Peng Wei",
            "Jian Wang",
            "Jinjie Gu",
            "Ping Wang",
            "Fan Yang"
        ],
        "submitted": "2025-10-15 16:40:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "NExT-OMNI: Towards Any-to-Any Omnimodal Foundation Models with Discrete Flow Matching",
        "abstract": "Next-generation multimodal foundation models capable of any-to-any\ncross-modal generation and multi-turn interaction will serve as core components\nof artificial general intelligence systems, playing a pivotal role in\nhuman-machine interaction. However, most existing multimodal models remain\nconstrained by autoregressive architectures, whose inherent limitations prevent\na balanced integration of understanding and generation capabilities. Although\nhybrid and decoupling strategies have been explored to address these tasks\nwithin unified frameworks separately, their redundant, non-integrated designs\nlimit their applicability to broader scenarios, such as cross-modal retrieval.\nIn this work, we introduce NExT-OMNI, an open-source omnimodal foundation model\nthat achieves unified modeling through discrete flow paradigms. By leveraging\nmetric-induced probability paths and kinetic optimal velocities, NExT-OMNI\nnatively supports any-to-any understanding and generation with enhanced\nresponse efficiency, while enabling broader application scenarios through\nconcise unified representations rather than task-decoupled designs. Trained on\nlarge-scale interleaved text, image, video, and audio data, NExT-OMNI delivers\ncompetitive performance on multimodal generation and understanding benchmarks,\nwhile outperforming prior unified models in multi-turn multimodal interaction\nand cross-modal retrieval, highlighting its architectural advantages as a\nnext-generation multimodal foundation model. To advance further research, we\nrelease training details, data protocols, and open-source both the code and\nmodel checkpoints.",
        "url": "http://arxiv.org/abs/2510.13721v2",
        "pdf_url": "http://arxiv.org/pdf/2510.13721v2",
        "arxiv_id": "2510.13721v2",
        "authors": [
            "Run Luo",
            "Xiaobo Xia",
            "Lu Wang",
            "Longze Chen",
            "Renke Shan",
            "Jing Luo",
            "Min Yang",
            "Tat-Seng Chua"
        ],
        "submitted": "2025-10-15 16:25:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "How Sampling Affects the Detectability of Machine-written texts: A Comprehensive Study",
        "abstract": "As texts generated by Large Language Models (LLMs) are ever more common and\noften indistinguishable from human-written content, research on automatic text\ndetection has attracted growing attention. Many recent detectors report\nnear-perfect accuracy, often boasting AUROC scores above 99\\%. However, these\nclaims typically assume fixed generation settings, leaving open the question of\nhow robust such systems are to changes in decoding strategies. In this work, we\nsystematically examine how sampling-based decoding impacts detectability, with\na focus on how subtle variations in a model's (sub)word-level distribution\naffect detection performance. We find that even minor adjustments to decoding\nparameters - such as temperature, top-p, or nucleus sampling - can severely\nimpair detector accuracy, with AUROC dropping from near-perfect levels to 1\\%\nin some settings. Our findings expose critical blind spots in current detection\nmethods and emphasize the need for more comprehensive evaluation protocols. To\nfacilitate future research, we release a large-scale dataset encompassing 37\ndecoding configurations, along with our code and evaluation framework\nhttps://github.com/BaggerOfWords/Sampling-and-Detection",
        "url": "http://arxiv.org/abs/2510.13681v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13681v1",
        "arxiv_id": "2510.13681v1",
        "authors": [
            "Matthieu Dubois",
            "François Yvon",
            "Pablo Piantanida"
        ],
        "submitted": "2025-10-15 15:36:45",
        "source": "arxiv",
        "comment": "EMNLP 2025 Findings"
    },
    {
        "title": "Closing the Gap Between Text and Speech Understanding in LLMs",
        "abstract": "Large Language Models (LLMs) can be adapted to extend their text capabilities\nto speech inputs. However, these speech-adapted LLMs consistently underperform\ntheir text-based counterparts--and even cascaded pipelines--on language\nunderstanding tasks. We term this shortfall the text-speech understanding gap:\nthe performance drop observed when a speech-adapted LLM processes spoken inputs\nrelative to when the original text-based LLM processes the equivalent text.\nRecent approaches to narrowing this gap either rely on large-scale speech\nsynthesis of text corpora, which is costly and heavily dependent on synthetic\ndata, or on large-scale proprietary speech datasets, which are not\nreproducible. As a result, there remains a need for more data-efficient\nalternatives for closing the text-speech understanding gap. In this work, we\nanalyze the gap as driven by two factors: (i) forgetting of text capabilities\nduring adaptation, and (ii) cross-modal misalignment between speech and text.\nBased on this analysis, we introduce SALAD--Sample-efficient Alignment with\nLearning through Active selection and cross-modal Distillation--which combines\ncross-modal distillation with targeted synthetic data to improve alignment\nwhile mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves\ncompetitive performance with a strong open-weight model across broad-domain\nbenchmarks in knowledge, language understanding, and reasoning, while training\non over an order of magnitude less speech data from public corpora.",
        "url": "http://arxiv.org/abs/2510.13632v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13632v1",
        "arxiv_id": "2510.13632v1",
        "authors": [
            "Santiago Cuervo",
            "Skyler Seto",
            "Maureen de Seyssel",
            "Richard He Bai",
            "Zijin Gu",
            "Tatiana Likhomanenko",
            "Navdeep Jaitly",
            "Zakaria Aldeneh"
        ],
        "submitted": "2025-10-15 14:57:16",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LIBERO-Plus: In-depth Robustness Analysis of Vision-Language-Action Models",
        "abstract": "Visual-Language-Action (VLA) models report impressive success rates on\nrobotic manipulation benchmarks, yet these results may mask fundamental\nweaknesses in robustness. We perform a systematic vulnerability analysis by\nintroducing controlled perturbations across seven dimensions: objects layout,\ncamera viewpoints, robot initial states, language instructions, light\nconditions, background textures and sensor noise. We comprehensively analyzed\nmultiple state-of-the-art models and revealed consistent brittleness beneath\napparent competence. Our analysis exposes critical weaknesses: models exhibit\nextreme sensitivity to perturbation factors, including camera viewpoints and\nrobot initial states, with performance dropping from 95% to below 30% under\nmodest perturbations. Surprisingly, models are largely insensitive to language\nvariations, with further experiments revealing that models tend to ignore\nlanguage instructions completely. Our findings challenge the assumption that\nhigh benchmark scores equate to true competency and highlight the need for\nevaluation practices that assess reliability under realistic variation.",
        "url": "http://arxiv.org/abs/2510.13626v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13626v1",
        "arxiv_id": "2510.13626v1",
        "authors": [
            "Senyu Fei",
            "Siyin Wang",
            "Junhao Shi",
            "Zihao Dai",
            "Jikun Cai",
            "Pengfang Qian",
            "Li Ji",
            "Xinzhe He",
            "Shiduo Zhang",
            "Zhaoye Fei",
            "Jinlan Fu",
            "Jingjing Gong",
            "Xipeng Qiu"
        ],
        "submitted": "2025-10-15 14:51:36",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of German Tumor Diagnoses",
        "abstract": "Accurate coding of tumor diagnoses with ICD-10-GM and ICD-O-3 is essential\nfor structured cancer documentation in Germany. Smaller open-weight LLMs are\nappealing for privacy-preserving automation but often struggle with coding\naccuracy in German-language contexts. This study investigates whether\ninstruction-based fine-tuning on public datasets improves the coding accuracy\nof open-weight LLMs for German tumor diagnosis texts. The evaluation uses coded\ndiagnoses from the local tumor documentation system as test data. In a\nsystematic data quality assessment, the upper limit for ICD-10 coding\nperformance was estimated at 60-79% for exact and 81-94% for partial\n(three-character codes only) derivation. As training data, over 500,000\nquestion-answer pairs were created based on the ICD-10-GM, ICD-O-3, and OPS\ncatalogues. Eight open-weight models from the Qwen, Llama, and Mistral families\n(7-70 B parameters) were fine-tuned. ICD-10-GM accuracy rose from 1.4-24% to\n41-58%, and partial accuracy from 31-74% to 73-83%. The accuracy of ICD-O-3\ntopography coding also improved but started and remained considerably lower\nwith an exact accuracy of 22-40% and a partial accuracy of 56-67% after\nfine-tuning. Malformed code outputs dropped to 0% for all models.\nTumor-diagnosis recognition reached 99%. Accuracy correlated positively with\nmodel size, but gaps between small and large models narrowed after fine-tuning.\nThe reasoning mode in Qwen3 generally yielded a lower performance than\nfine-tuning and was over 100 times slower. Our findings highlight the potential\nof leveraging public catalogues to build instruction datasets that improve LLMs\nin medical documentation tasks. The complete training dataset and the\nbest-performing checkpoints of the fine-tuned models are available from\nhttps://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024.",
        "url": "http://arxiv.org/abs/2510.13624v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13624v1",
        "arxiv_id": "2510.13624v1",
        "authors": [
            "Stefan Lenz",
            "Lakisha Ortiz Rosario",
            "Georg Vollmar",
            "Arsenij Ustjanzew",
            "Fatma Alickovic",
            "Thomas Kindler",
            "Torsten Panholzer"
        ],
        "submitted": "2025-10-15 14:51:28",
        "source": "arxiv",
        "comment": "19 pages, 4 figures"
    },
    {
        "title": "MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large Language Model Reasoning",
        "abstract": "Large Language Models (LLMs) have achieved impressive reasoning abilities,\nbut struggle with temporal understanding, especially when questions involve\nmultiple entities, compound operators, and evolving event sequences. Temporal\nKnowledge Graphs (TKGs), which capture vast amounts of temporal facts in a\nstructured format, offer a reliable source for temporal reasoning. However,\nexisting TKG-based LLM reasoning methods still struggle with four major\nchallenges: maintaining temporal faithfulness in multi-hop reasoning, achieving\nmulti-entity temporal synchronization, adapting retrieval to diverse temporal\noperators, and reusing prior reasoning experience for stability and efficiency.\nTo address these issues, we propose MemoTime, a memory-augmented temporal\nknowledge graph framework that enhances LLM reasoning through structured\ngrounding, recursive reasoning, and continual experience learning. MemoTime\ndecomposes complex temporal questions into a hierarchical Tree of Time,\nenabling operator-aware reasoning that enforces monotonic timestamps and\nco-constrains multiple entities under unified temporal bounds. A dynamic\nevidence retrieval layer adaptively selects operator-specific retrieval\nstrategies, while a self-evolving experience memory stores verified reasoning\ntraces, toolkit decisions, and sub-question embeddings for cross-type reuse.\nComprehensive experiments on multiple temporal QA benchmarks show that MemoTime\nachieves overall state-of-the-art results, outperforming the strong baseline by\nup to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to\nachieve reasoning performance comparable to that of GPT-4-Turbo.",
        "url": "http://arxiv.org/abs/2510.13614v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13614v1",
        "arxiv_id": "2510.13614v1",
        "authors": [
            "Xingyu Tan",
            "Xiaoyang Wang",
            "Qing Liu",
            "Xiwei Xu",
            "Xin Yuan",
            "Liming Zhu",
            "Wenjie Zhang"
        ],
        "submitted": "2025-10-15 14:43:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "NOSA: Native and Offloadable Sparse Attention",
        "abstract": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2).",
        "url": "http://arxiv.org/abs/2510.13602v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13602v1",
        "arxiv_id": "2510.13602v1",
        "authors": [
            "Yuxiang Huang",
            "Chaojun Xiao",
            "Xu Han",
            "Zhiyuan Liu"
        ],
        "submitted": "2025-10-15 14:33:16",
        "source": "arxiv",
        "comment": "Preprint"
    },
    {
        "title": "FreshTab: Sourcing Fresh Data for Table-to-Text Generation Evaluation",
        "abstract": "Table-to-text generation (insight generation from tables) is a challenging\ntask that requires precision in analyzing the data. In addition, the evaluation\nof existing benchmarks is affected by contamination of Large Language Model\n(LLM) training data as well as domain imbalance. We introduce FreshTab, an\non-the-fly table-to-text benchmark generation from Wikipedia, to combat the LLM\ndata contamination problem and enable domain-sensitive evaluation. While\nnon-English table-to-text datasets are limited, FreshTab collects datasets in\ndifferent languages on demand (we experiment with German, Russian and French in\naddition to English). We find that insights generated by LLMs from recent\ntables collected by our method appear clearly worse by automatic metrics, but\nthis does not translate into LLM and human evaluations. Domain effects are\nvisible in all evaluations, showing that a~domain-balanced benchmark is more\nchallenging.",
        "url": "http://arxiv.org/abs/2510.13598v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13598v1",
        "arxiv_id": "2510.13598v1",
        "authors": [
            "Kristýna Onderková",
            "Ondřej Plátek",
            "Zdeněk Kasner",
            "Ondřej Dušek"
        ],
        "submitted": "2025-10-15 14:31:44",
        "source": "arxiv",
        "comment": "To be published in INLG 2025"
    },
    {
        "title": "RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for Evolving Knowledge",
        "abstract": "Knowledge is inherently time-sensitive and continuously evolves over time.\nAlthough current Retrieval-Augmented Generation (RAG) systems enrich LLMs with\nexternal knowledge, they largely ignore this temporal nature. This raises two\nchallenges for RAG. First, current RAG methods lack effective time-aware\nrepresentations. Same facts of different time are difficult to distinguish with\nvector embeddings or conventional knowledge graphs. Second, most RAG\nevaluations assume a static corpus, leaving a blind spot regarding update costs\nand retrieval stability as knowledge evolves. To make RAG time-aware, we\npropose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level\ntemporal graph consisting of a temporal knowledge graph with timestamped\nrelations and a hierarchical time graph. Multi-granularity temporal summaries\nare generated for each time node to capture both key events and broader trends\nat that time. The design supports incremental updates by extracting new\ntemporal facts from the incoming corpus and merging them into the existing\ngraph. The temporal graph explicitly represents identical facts at different\ntimes as distinct edges to avoid ambiguity, and the time hierarchy graph allows\nonly generating reports for new leaf time nodes and their ancestors, ensuring\neffective and efficient updates. During inference, TG-RAG dynamically retrieves\na subgraph within the temporal and semantic scope of the query, enabling\nprecise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive\nquestion-answering dataset featuring both specific and abstract queries, along\nwith a comprehensive evaluation protocol designed to assess incremental update\ncapabilities of RAG systems. Extensive experiments show that TG-RAG\nsignificantly outperforms existing baselines, demonstrating the effectiveness\nof our method in handling temporal knowledge and incremental updates.",
        "url": "http://arxiv.org/abs/2510.13590v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13590v1",
        "arxiv_id": "2510.13590v1",
        "authors": [
            "Jiale Han",
            "Austin Cheung",
            "Yubai Wei",
            "Zheng Yu",
            "Xusheng Wang",
            "Bing Zhu",
            "Yi Yang"
        ],
        "submitted": "2025-10-15 14:21:08",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Deflanderization for Game Dialogue: Balancing Character Authenticity with Task Execution in LLM-based NPCs",
        "abstract": "The emergence of large language models (LLMs) has opened new opportunities\nfor cre- ating dynamic non-player characters (NPCs) in gaming environments,\nenabling both func- tional task execution and persona-consistent dialogue\ngeneration. In this paper, we (Tu_Character_lab) report our participation in\nthe Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which\neval- uates agents across three tracks: task-oriented dialogue, context-aware\ndialogue, and their integration. Our approach combines two complementary\nstrategies: (i) lightweight prompting techniques in the API track, including a\nDeflanderization prompting method to suppress excessive role-play and improve\ntask fidelity, and (ii) fine-tuned large models in the GPU track, leveraging\nQwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our\nbest submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on\nTask 3 (GPU track).",
        "url": "http://arxiv.org/abs/2510.13586v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13586v1",
        "arxiv_id": "2510.13586v1",
        "authors": [
            "Pasin Buakhaw",
            "Kun Kerdthaisong",
            "Phuree Phenhiran",
            "Pitikorn Khlaisamniang",
            "Supasate Vorathammathorn",
            "Piyalitt Ittichaiwong",
            "Nutchanon Yongsatianchot"
        ],
        "submitted": "2025-10-15 14:17:23",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Sparse Subnetwork Enhancement for Underrepresented Languages in Large Language Models",
        "abstract": "Large language models exhibit uneven performance across languages, with\nsubstantial gaps between high- and low-resource languages. We present a\nframework for enhancing monolingual capabilities of LLMs in underrepresented\nlanguages while preserving their general-purpose performance through targeted\nfine-tuning of language-specific subnetworks. Our approach identifies\nlanguage-specific neurons using Language Activation Probability Entropy and\nfine-tunes only the weights associated with these neurons, a dedicated\nsubnetwork, on target-language data. Experiments on Llama-3.1-8B and\nMistral-Nemo-12B across 12 mid- and low-resource languages demonstrate that our\nmethod consistently outperforms full fine-tuning, FFN-only fine-tuning, LoRA\nadaptation, and random subset fine-tuning baselines while efficiently updating\nonly up to 1% of model parameters. Beyond performance improvements, we observe\nenhanced favorable training dynamics, cross-lingual representational alignment,\nand systematic weight update changes. To facilitate future research, we release\nlanguage-specific neuron identifications for over 100 languages as well as our\nadaptation pipeline, offering a cost-effective pathway for adapting\nstate-of-the-art models to underrepresented languages.",
        "url": "http://arxiv.org/abs/2510.13580v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13580v1",
        "arxiv_id": "2510.13580v1",
        "authors": [
            "Daniil Gurgurov",
            "Josef van Genabith",
            "Simon Ostermann"
        ],
        "submitted": "2025-10-15 14:14:49",
        "source": "arxiv",
        "comment": "preprint"
    },
    {
        "title": "Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm Enables Fine-Grained Policy Optimization",
        "abstract": "The reasoning pattern of Large language models (LLMs) remains opaque, and\nReinforcement learning (RL) typically applies uniform credit across an entire\ngeneration, blurring the distinction between pivotal and routine steps. This\nwork positions attention as a privileged substrate that renders the internal\nlogic of LLMs legible, not merely as a byproduct of computation, but as a\nmechanistic blueprint of reasoning itself. We first distinguish attention heads\nbetween locally and globally focused information processing and reveal that\nlocally focused heads produce a sawtooth pattern near the diagonal indicating\nphrasal chunks, while globally focused heads expose tokens that exert broad\ndownstream influence over future tokens. We formalize these with two metrics:\n1) Windowed Average Attention Distance, which measures the extent of backward\nattention within a clipped window; 2) Future Attention Influence, which\nquantifies a token's global importance as the average attention it receives\nfrom subsequent tokens. Taken together, these signals reveal a recurring\npreplan-and-anchor mechanism, where the model first performs a long-range\ncontextual reference to generate an introductory token, which is immediately\nfollowed by or coincides with a semantic anchor token that organizes subsequent\nreasoning. Leveraging these insights, we introduce three novel RL strategies\nthat dynamically perform targeted credit assignment to critical nodes (preplan\ntokens, anchor tokens, and their temporal coupling) and show consistent\nperformance gains across various reasoning tasks. By aligning optimization with\nthe model's intrinsic reasoning rhythm, we aim to transform opaque optimization\ninto an actionable structure-aware process, hoping to offer a potential step\ntoward more transparent and effective optimization of LLM reasoning.",
        "url": "http://arxiv.org/abs/2510.13554v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13554v1",
        "arxiv_id": "2510.13554v1",
        "authors": [
            "Yang Li",
            "Zhichen Dong",
            "Yuhan Sun",
            "Weixun Wang",
            "Shaopan Xiong",
            "Yijia Luo",
            "Jiashun Liu",
            "Han Lu",
            "Jiamang Wang",
            "Wenbo Su",
            "Bo Zheng",
            "Junchi Yan"
        ],
        "submitted": "2025-10-15 13:49:51",
        "source": "arxiv",
        "comment": "23 pages, 8 figures, 5 tables"
    },
    {
        "title": "K-Merge: Online Continual Merging of Adapters for On-device Large Language Models",
        "abstract": "On-device deployment of Large Language Models (LLMs) frequently leverages\nLow-Rank Adapters (LoRAs) to support diverse downstream tasks under tight\nresource constraints. To address the limited storage capacity of mobile\ndevices, recent works have explored model merging techniques to fuse multiple\nLoRAs into a single one. In practice, however, LoRAs are often delivered\nincrementally, as users request support for new tasks (e.g., novel problem\ntypes or languages). This scenario introduces a new challenge: on-device online\ncontinual merging, where the objective is to incorporate new LoRAs while\npreserving the performance on previously supported tasks. In this paper, we\npropose a data-free and computationally efficient strategy for selecting and\nmerging LoRAs when a new one becomes available, assuming the device can store\nonly a limited number of adapters. Extensive experiments across real-world\ntasks demonstrate the superiority of our approach compared to alternative\nstrategies while adhering to the storage budget and compute limitations of\non-device settings.",
        "url": "http://arxiv.org/abs/2510.13537v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13537v1",
        "arxiv_id": "2510.13537v1",
        "authors": [
            "Donald Shenaj",
            "Ondrej Bohdal",
            "Taha Ceritli",
            "Mete Ozay",
            "Pietro Zanuttigh",
            "Umberto Michieli"
        ],
        "submitted": "2025-10-15 13:32:25",
        "source": "arxiv",
        "comment": "15 pages, 8 figures"
    },
    {
        "title": "MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts",
        "abstract": "LLMs hold great promise for healthcare applications, but the rapid evolution\nof medical knowledge and errors in training data often cause them to generate\noutdated or inaccurate information, limiting their applicability in high-stakes\nclinical practice. Model editing has emerged as a potential remedy without full\nretraining. While parameter-based editing often compromises locality and is\nthus ill-suited for the medical domain, retrieval-based editing offers a more\nviable alternative. However, it still faces two critical challenges: (1)\nrepresentation overlap within the medical knowledge space often causes\ninaccurate retrieval and reduces editing accuracy; (2) existing methods are\nrestricted to single-sample edits, while batch-editing remains largely\nunexplored despite its importance for real-world medical applications. To\naddress these challenges, we first construct MedVersa, \\hk{an enhanced\nbenchmark with broader coverage of medical subjects, designed to evaluate both\nsingle and batch edits under strict locality constraints}. We then propose\nMedREK, a retrieval-based editing framework that integrates a shared query-key\nmodule for precise matching with an attention-based prompt encoder for\ninformative guidance. Experimental results on various medical benchmarks\ndemonstrate that our MedREK achieves superior performance across different core\nmetrics and provides the first validated solution for batch-editing in medical\nLLMs. Our code and dataset are available at\nhttps://github.com/mylittleriver/MedREK.",
        "url": "http://arxiv.org/abs/2510.13500v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13500v1",
        "arxiv_id": "2510.13500v1",
        "authors": [
            "Shujun Xia",
            "Haokun Lin",
            "Yichen Wu",
            "Yinan Zhou",
            "Zixuan Li",
            "Zhongwei Wan",
            "Xingrun Xing",
            "Yefeng Zheng",
            "Xiang Li",
            "Caifeng Shan",
            "Zhenan Sun",
            "Quanzheng Li"
        ],
        "submitted": "2025-10-15 12:50:33",
        "source": "arxiv",
        "comment": "Preprint, work in progress"
    },
    {
        "title": "ConsintBench: Evaluating Language Models on Real-World Consumer Intent Understanding",
        "abstract": "Understanding human intent is a complex, high-level task for large language\nmodels (LLMs), requiring analytical reasoning, contextual interpretation,\ndynamic information aggregation, and decision-making under uncertainty.\nReal-world public discussions, such as consumer product discussions, are rarely\nlinear or involve a single user. Instead, they are characterized by interwoven\nand often conflicting perspectives, divergent concerns, goals, emotional\ntendencies, as well as implicit assumptions and background knowledge about\nusage scenarios. To accurately understand such explicit public intent, an LLM\nmust go beyond parsing individual sentences; it must integrate multi-source\nsignals, reason over inconsistencies, and adapt to evolving discourse, similar\nto how experts in fields like politics, economics, or finance approach complex,\nuncertain environments. Despite the importance of this capability, no\nlarge-scale benchmark currently exists for evaluating LLMs on real-world human\nintent understanding, primarily due to the challenges of collecting real-world\npublic discussion data and constructing a robust evaluation pipeline. To bridge\nthis gap, we introduce \\bench, the first dynamic, live evaluation benchmark\nspecifically designed for intent understanding, particularly in the consumer\ndomain. \\bench is the largest and most diverse benchmark of its kind,\nsupporting real-time updates while preventing data contamination through an\nautomated curation pipeline.",
        "url": "http://arxiv.org/abs/2510.13499v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13499v1",
        "arxiv_id": "2510.13499v1",
        "authors": [
            "Xiaozhe Li",
            "TianYi Lyu",
            "Siyi Yang",
            "Yuxi Gong",
            "Yizhao Yang",
            "Jinxuan Huang",
            "Ligao Zhang",
            "Zhuoyi Huang",
            "Qingwen Liu"
        ],
        "submitted": "2025-10-15 12:49:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LiteraryQA: Towards Effective Evaluation of Long-document Narrative QA",
        "abstract": "Question Answering (QA) on narrative text poses a unique challenge to current\nsystems, requiring a deep understanding of long, complex documents. However,\nthe reliability of NarrativeQA, the most widely used benchmark in this domain,\nis hindered by noisy documents and flawed QA pairs. In this work, we introduce\nLiteraryQA, a high-quality subset of NarrativeQA focused on literary works.\nUsing a human- and LLM-validated pipeline, we identify and correct low-quality\nQA samples while removing extraneous text from source documents. We then carry\nout a meta-evaluation of automatic metrics to clarify how systems should be\nevaluated on LiteraryQA. This analysis reveals that all n-gram-based metrics\nhave a low system-level correlation to human judgment, while LLM-as-a-Judge\nevaluations, even with small open-weight models, can strongly agree with the\nranking identified by humans. Finally, we benchmark a set of long-context LLMs\non LiteraryQA. We release our code and data at\nhttps://github.com/SapienzaNLP/LiteraryQA.",
        "url": "http://arxiv.org/abs/2510.13494v1",
        "pdf_url": "http://arxiv.org/pdf/2510.13494v1",
        "arxiv_id": "2510.13494v1",
        "authors": [
            "Tommaso Bonomo",
            "Luca Gioffré",
            "Roberto Navigli"
        ],
        "submitted": "2025-10-15 12:43:59",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 Main Conference. 22 pages"
    }
]