# Daily Papers Report - 2025-08-05

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. Decomposed Reasoning with Reinforcement Learning for Relevance Assessment in UGC Platforms

- **LLM Score**: 8
- **Keyword Score**: 14
- **Authors**: Xiaowei Yuan, Lei Jin, Haoxin Zhang, Yan Gao, Yi Wu, Yao Hu, Ziyang Huang, Jun Zhao, Kang Liu
- **URL**: <http://arxiv.org/abs/2508.02506v1>
- **Submitted**: 2025-08-04 15:14:09
- **Topic Keywords**: query, queries, relevance, rag, retrieval, rank
- **Reason**: The paper focuses on relevance assessment in user-generated content platforms, which is closely related to information retrieval and query understanding. The use of reinforcement learning and decomposed reasoning framework is also relevant to ranking models and user behavior modeling. However, the specific application to UGC platforms and the emphasis on noisy and unstructured language may not be directly applicable to the user's e-commerce background.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Relevance Assessment in User-Generated Content (UGC) Platforms
- **Aim**: To improve the accuracy of relevance assessment in UGC platforms by reducing errors caused by noisy UGC and ambiguous user intent
- **Rationale**: The proposed R¬≥A model introduces a decomposed reasoning framework that infers latent query intent and performs verbatim fragment extraction to justify relevance decisions
- **Ground**: The R¬≥A model is optimized using a reinforcement learning framework and evaluated using a real-world industry dataset, NoteRel
- **Experiment**: The authors conduct an online experiment with 10% of online traffic for one week, comparing the performance of R¬≥A-Distill-1.5B with their previous online model
- **Takeaway**: The R¬≥A model outperforms all baseline models in relevance assessment, reduces re-query rates, and demonstrates the effectiveness of leveraging reasoning capability in relevance assessment tasks

#### Abstract
> Retrieval-augmented generation (RAG) plays a critical role in user-generated
content (UGC) platforms, but its effectiveness depends heavily on accurate
relevance assessment of query-document pairs. Despite recent advances in
applying large language models (LLMs) to relevance modeling, UGC platforms
present unique challenges: 1) ambiguous user intent due to sparse user feedback
in RAG scenarios, and 2) substantial noise introduced by informal and
unstructured language. To address these issues, we propose the Reinforced
Reasoning Model for Relevance Assessment (R3A), which introduces a decomposed
reasoning framework over queries and candidate documents before scoring. R3A
first leverages auxiliary high-ranked documents within the platform to infer
latent query intent. It then performs verbatim fragment extraction to justify
relevance decisions, thereby reducing errors caused by noisy UGC. Based on a
reinforcement learning framework, R3A is optimized to mitigate distortions
arising from ambiguous queries and unstructured content. Experimental results
show that R3A significantly outperforms existing baseline methods in terms of
relevance accuracy, across both offline benchmarks and online experiments.

---

### 2. FinCPRG: A Bidirectional Generation Pipeline for Hierarchical Queries and Rich Relevance in Financial Chinese Passage Retrieval

- **LLM Score**: 7
- **Keyword Score**: 15
- **Authors**: Xuan Xu, Beilin Chu, Qinhong Lin, Yixiao Zhong, Fufang Wen, Jiaqi Liu, Binjie Fei, Yu Li, Zhongliang Yang, Linna Zhou
- **URL**: <http://arxiv.org/abs/2508.02222v1>
- **Submitted**: 2025-08-04 09:12:45
- **Topic Keywords**: passage retrieval, query, queries, relevance, retrieval, search
- **Reason**: The paper focuses on passage retrieval, query understanding, and relevance optimization, which aligns with your interests in Information Retrieval and Search technologies. The use of hierarchical queries and rich relevance labels is also relevant to your research on query understanding and ranking models. However, the specific domain of financial Chinese passage retrieval is not directly related to your e-commerce background.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Financial Chinese Passage Retrieval
- **Aim**: To propose a bidirectional generation pipeline, FinCPRG, for constructing a passage retrieval dataset for financial Chinese passage retrieval
- **Rationale**: To address the challenges of limited context window and simplistic synthesis pipelines in using Large Language Models (LLMs) to synthesize high-quality retrieval datasets
- **Ground**: The pipeline combines bottom-up and top-down approaches to generate hierarchical queries, employing an indirect positives mining method to balance efficiency and coverage
- **Experiment**: The authors evaluate the FinCPRG dataset through multiple experiments, assessing the quality of relevance labels and validating its effectiveness as both an evaluation benchmark and a training dataset
- **Takeaway**: The resulting dataset, FinCPRG, includes hierarchical queries and rich relevance labels and is constructed from almost 1.3k Chinese financial research reports, demonstrating high consistency with true financial retrieval benchmarks and potential for generating effective training data for low-resource domains

#### Abstract
> In recent years, large language models (LLMs) have demonstrated significant
potential in constructing passage retrieval datasets. However, existing methods
still face limitations in expressing cross-doc query needs and controlling
annotation quality. To address these issues, this paper proposes a
bidirectional generation pipeline, which aims to generate 3-level hierarchical
queries for both intra-doc and cross-doc scenarios and mine additional
relevance labels on top of direct mapping annotation. The pipeline introduces
two query generation methods: bottom-up from single-doc text and top-down from
multi-doc titles. The bottom-up method uses LLMs to disassemble and generate
structured queries at both sentence-level and passage-level simultaneously from
intra-doc passages. The top-down approach incorporates three key financial
elements--industry, topic, and time--to divide report titles into clusters and
prompts LLMs to generate topic-level queries from each cluster. For relevance
annotation, our pipeline not only relies on direct mapping annotation from the
generation relationship but also implements an indirect positives mining method
to enrich the relevant query-passage pairs. Using this pipeline, we constructed
a Financial Passage Retrieval Generated dataset (FinCPRG) from almost 1.3k
Chinese financial research reports, which includes hierarchical queries and
rich relevance labels. Through evaluations of mined relevance labels,
benchmarking and training experiments, we assessed the quality of FinCPRG and
validated its effectiveness as a passage retrieval dataset for both training
and benchmarking.

---

### 3. Contextually Aware E-Commerce Product Question Answering using RAG

- **LLM Score**: 7
- **Keyword Score**: 9
- **Authors**: Praveen Tangarajan, Anand A. Rajasekar, Manish Rathi, Vinay Rao Dandin, Ozan Ersoy
- **URL**: <http://arxiv.org/abs/2508.01990v1>
- **Submitted**: 2025-08-04 02:14:07
- **Comment**: 6 pages, 1 figure, 5 tables. Preprint under review
- **Topic Keywords**: queries, rag, retrieval, commerce, e-commerce
- **Reason**: The paper explores Product Question Answering (PQA) in e-commerce, which is related to Information Retrieval and Search technologies. The use of Retrieval Augmented Generation (RAG) and contextual understanding aligns with the user's interest in query understanding and ranking models. However, the focus on e-commerce and product pages is somewhat specific and may not be directly applicable to the user's broader interests in NLP and data mining.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Scalable End-to-End Framework for E-commerce Product Question Answering (PQA) using Retrieval Augmented Generation (RAG)
- **Aim**: To develop a comprehensive solution for e-commerce PQA, addressing cognitive overload, ambiguity, and hallucinations
- **Rationale**: The proposed framework leverages conversational history, user profiles, and product attributes to deliver relevant and personalized answers, handling objective, subjective, and multi-intent queries across heterogeneous sources
- **Ground**: The framework consists of three main stages: Intent Detection, Retrieval, and Generation, and is evaluated using a robust evaluation framework assessing answer quality and alignment with context
- **Experiment**: The system is deployed in a production conversational assistant, serving over 5 million monthly active users and delivering an 8% increase in user thumbs-up rates and measurable improvements in conversion and customer satisfaction
- **Takeaway**: The proposed framework provides a comprehensive solution for e-commerce PQA, demonstrating strong performance in a production setting, but has limitations that can be addressed by developing a more adaptive, agentic architecture and automated pipelines for catalog enrichment

#### Abstract
> E-commerce product pages contain a mix of structured specifications,
unstructured reviews, and contextual elements like personalized offers or
regional variants. Although informative, this volume can lead to cognitive
overload, making it difficult for users to quickly and accurately find the
information they need. Existing Product Question Answering (PQA) systems often
fail to utilize rich user context and diverse product information effectively.
We propose a scalable, end-to-end framework for e-commerce PQA using Retrieval
Augmented Generation (RAG) that deeply integrates contextual understanding. Our
system leverages conversational history, user profiles, and product attributes
to deliver relevant and personalized answers. It adeptly handles objective,
subjective, and multi-intent queries across heterogeneous sources, while also
identifying information gaps in the catalog to support ongoing content
improvement. We also introduce novel metrics to measure the framework's
performance which are broadly applicable for RAG system evaluations.

---

### 4. SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic Association and Long Story Comprehension

- **LLM Score**: 6
- **Keyword Score**: 7
- **Authors**: Junjie Wu, Jiangnan Li, Yuqing Li, Lemao Liu, Liyan Xu, Jiwei Li, Dit-Yan Yeung, Jie Zhou, Mo Yu
- **URL**: <http://arxiv.org/abs/2508.01959v1>
- **Submitted**: 2025-08-03 23:59:31
- **Comment**: Our trained models can be downloaded from:
  https://huggingface.co/SituatedEmbedding
- **Topic Keywords**: dense retrieval, rag, retrieval
- **Reason**: The paper proposes a new approach to dense retrieval for semantic association and long story comprehension, which is related to information retrieval and query understanding. However, it does not specifically focus on ranking models or user behavior modeling, which are key areas of interest for the user. The paper's emphasis on situated context and embedding models is somewhat relevant to the user's background in e-commerce and NLP, but it does not directly address the user's primary focus on real-time relevance optimization and deep semantic understanding.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Retrieval-Augmented Generation (RAG) tasks
- **Aim**: To propose a novel approach to encoding contextual information in text embeddings for RAG tasks
- **Rationale**: Existing methods involve encoding longer context windows or increasing chunk size, leading to critical information loss and underperformance in applications
- **Ground**: Situated embedding models (SitEmb) that represent short chunks conditioned on a broader context window
- **Experiment**: Evaluation on a book-plot retrieval task, comparing SitEmb with state-of-the-art embedding models
- **Takeaway**: SitEmb models outperform state-of-the-art models, and the proposed approach provides an effective alternative to contextual retrieval

#### Abstract
> Retrieval-augmented generation (RAG) over long documents typically involves
splitting the text into smaller chunks, which serve as the basic units for
retrieval. However, due to dependencies across the original document,
contextual information is often essential for accurately interpreting each
chunk. To address this, prior work has explored encoding longer context windows
to produce embeddings for longer chunks. Despite these efforts, gains in
retrieval and downstream tasks remain limited. This is because (1) longer
chunks strain the capacity of embedding models due to the increased amount of
information they must encode, and (2) many real-world applications still
require returning localized evidence due to constraints on model or human
bandwidth.
  We propose an alternative approach to this challenge by representing short
chunks in a way that is conditioned on a broader context window to enhance
retrieval performance -- i.e., situating a chunk's meaning within its context.
We further show that existing embedding models are not well-equipped to encode
such situated context effectively, and thus introduce a new training paradigm
and develop the situated embedding models (SitEmb). To evaluate our method, we
curate a book-plot retrieval dataset specifically designed to assess situated
retrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3
substantially outperforms state-of-the-art embedding models, including several
with up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model
further improves performance by over 10% and shows strong results across
different languages and several downstream applications.

---

### 5. Evaluating Position Bias in Large Language Model Recommendations

- **LLM Score**: 6
- **Keyword Score**: 6
- **Authors**: Ethan Bito, Yongli Ren, Estrid He
- **URL**: <http://arxiv.org/abs/2508.02020v1>
- **Submitted**: 2025-08-04 03:30:26
- **Topic Keywords**: ranking, recommend, rank, search
- **Reason**: The paper explores the issue of position bias in Large Language Model recommendations, which is related to my interests in Information Retrieval and Search technologies. However, the focus on recommender systems and language models is not directly aligned with my primary focus on query understanding, ranking models, and user behavior modeling.

#### T.A.R.G.E.T. Summary (from Full Text)
- **Topic**: Position Bias in Large Language Model (LLM) Recommendations
- **Aim**: To mitigate position bias in LLM-based recommendations and improve their stability
- **Rationale**: LLMs are highly sensitive to input orders, leading to position bias, which can be alleviated by a new prompting strategy called Ranking via Iterative SElection (RISE)
- **Ground**: The recommendation problem is formalized as a learning-to-rank task, and pre-trained LLMs are employed as the recommendation model
- **Experiment**: RISE is evaluated on two popular datasets (MovieLens-1M and Amazon Books) and compared with two baselines (Standard Prompting and Bootstrapping) using metrics such as Positional Consistency, Output Similarity, Input Sensitivity, Recall@K, and NDCG@K
- **Takeaway**: RISE can effectively reduce position bias by up to 25% compared to baselines, and popularity bias may not be a primary driver of LLM prompting behavior

#### Abstract
> Large Language Models (LLMs) are being increasingly explored as
general-purpose tools for recommendation tasks, enabling zero-shot and
instruction-following capabilities without the need for task-specific training.
While the research community is enthusiastically embracing LLMs, there are
important caveats to directly adapting them for recommendation tasks. In this
paper, we show that LLM-based recommendation models suffer from position bias,
where the order of candidate items in a prompt can disproportionately influence
the recommendations produced by LLMs. First, we analyse the position bias of
LLM-based recommendations on real-world datasets, where results uncover
systemic biases of LLMs with high sensitivity to input orders. Furthermore, we
introduce a new prompting strategy to mitigate the position bias of LLM
recommendation models called Ranking via Iterative SElection (RISE). We compare
our proposed method against various baselines on key benchmark datasets.
Experiment results show that our method reduces sensitivity to input ordering
and improves stability without requiring model fine-tuning or post-processing.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Learning Partially-Decorrelated Common Spaces for Ad-hoc Video Search

- **LLM Score**: 4
- **Keyword Score**: 10
- **Authors**: Fan Hu, Zijie Xin, Xirong Li
- **URL**: <http://arxiv.org/abs/2508.02340v1>
- **Submitted**: 2025-08-04 12:21:16
- **Comment**: Accepted by ACMMM2025
- **Topic Keywords**: query, ranking, rank, search, trec
- **Reason**: The paper focuses on Ad-hoc Video Search, which is not directly related to my primary research interests in Information Retrieval and Search technologies. While it mentions query understanding and ranking models, the context is specific to video search and does not align with my expertise in text-based search. The paper's emphasis on visual features and feature-specific common spaces is also outside my area of focus.

#### Abstract
> Ad-hoc Video Search (AVS) involves using a textual query to search for
multiple relevant videos in a large collection of unlabeled short videos. The
main challenge of AVS is the visual diversity of relevant videos. A simple
query such as "Find shots of a man and a woman dancing together indoors" can
span a multitude of environments, from brightly lit halls and shadowy bars to
dance scenes in black-and-white animations. It is therefore essential to
retrieve relevant videos as comprehensively as possible. Current solutions for
the AVS task primarily fuse multiple features into one or more common spaces,
yet overlook the need for diverse spaces. To fully exploit the expressive
capability of individual features, we propose LPD, short for Learning Partially
Decorrelated common spaces. LPD incorporates two key innovations:
feature-specific common space construction and the de-correlation loss.
Specifically, LPD learns a separate common space for each video and text
feature, and employs de-correlation loss to diversify the ordering of negative
samples across different spaces. To enhance the consistency of multi-space
convergence, we designed an entropy-based fair multi-space triplet ranking
loss. Extensive experiments on the TRECVID AVS benchmarks (2016-2023) justify
the effectiveness of LPD. Moreover, diversity visualizations of LPD's spaces
highlight its ability to enhance result diversity.

### 7. Hubness Reduction with Dual Bank Sinkhorn Normalization for Cross-Modal Retrieval

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Zhengxin Pan, Haishuai Wang, Fangyu Wu, Peng Zhang, Jiajun Bu
- **URL**: <http://arxiv.org/abs/2508.02538v1>
- **Submitted**: 2025-08-04 15:45:48
- **Comment**: ACMMM 2025
- **Topic Keywords**: query, queries, retrieval
- **Reason**: The paper focuses on cross-modal retrieval, which is a related topic to information retrieval, but it does not directly address query understanding, ranking models, or user behavior modeling, which are core research themes for you. The paper's emphasis on hubness reduction and probability balancing is also not directly relevant to your interests in deep semantic understanding and real-time relevance optimization.

#### Abstract
> The past decade has witnessed rapid advancements in cross-modal retrieval,
with significant progress made in accurately measuring the similarity between
cross-modal pairs. However, the persistent hubness problem, a phenomenon where
a small number of targets frequently appear as nearest neighbors to numerous
queries, continues to hinder the precision of similarity measurements. Despite
several proposed methods to reduce hubness, their underlying mechanisms remain
poorly understood. To bridge this gap, we analyze the widely-adopted Inverted
Softmax approach and demonstrate its effectiveness in balancing target
probabilities during retrieval. Building on these insights, we propose a
probability-balancing framework for more effective hubness reduction. We
contend that balancing target probabilities alone is inadequate and, therefore,
extend the framework to balance both query and target probabilities by
introducing Sinkhorn Normalization (SN). Notably, we extend SN to scenarios
where the true query distribution is unknown, showing that current methods,
which rely solely on a query bank to estimate target hubness, produce
suboptimal results due to a significant distributional gap between the query
bank and targets. To mitigate this issue, we introduce Dual Bank Sinkhorn
Normalization (DBSN), incorporating a corresponding target bank alongside the
query bank to narrow this distributional gap. Our comprehensive evaluation
across various cross-modal retrieval tasks, including image-text retrieval,
video-text retrieval, and audio-text retrieval, demonstrates consistent
performance improvements, validating the effectiveness of both SN and DBSN. All
codes are publicly available at https://github.com/ppanzx/DBSN.

### 8. Agentic Personalized Fashion Recommendation in the Age of Generative AI: Challenges, Opportunities, and Evaluation

- **LLM Score**: 4
- **Keyword Score**: 7
- **Authors**: Yashar Deldjoo, Nima Rafiee, Mahdyar Ravanbakhsh
- **URL**: <http://arxiv.org/abs/2508.02342v1>
- **Submitted**: 2025-08-04 12:22:25
- **Topic Keywords**: queries, retrieval, recommend, search
- **Reason**: The paper focuses on fashion recommender systems, which is outside the user's primary area of interest in Information Retrieval and Search technologies. While it mentions some relevant concepts like multimodal encoders and dynamic retrieval, the application is specific to the fashion domain and does not align with the user's broader interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Fashion recommender systems (FaRS) face distinct challenges due to rapid
trend shifts, nuanced user preferences, intricate item-item compatibility, and
the complex interplay among consumers, brands, and influencers. Traditional
recommendation approaches, largely static and retrieval-focused, struggle to
effectively capture these dynamic elements, leading to decreased user
satisfaction and elevated return rates. This paper synthesizes both academic
and industrial viewpoints to map the distinctive output space and stakeholder
ecosystem of modern FaRS, identifying the complex interplay among users,
brands, platforms, and influencers, and highlighting the unique data and
modeling challenges that arise.
  We outline a research agenda for industrial FaRS, centered on five
representative scenarios spanning static queries, outfit composition, and
multi-turn dialogue, and argue that mixed-modality refinement-the ability to
combine image-based references (anchors) with nuanced textual constraints-is a
particularly critical task for real-world deployment. To this end, we propose
an Agentic Mixed-Modality Refinement (AMMR) pipeline, which fuses multimodal
encoders with agentic LLM planners and dynamic retrieval, bridging the gap
between expressive user intent and fast-changing fashion inventories. Our work
shows that moving beyond static retrieval toward adaptive, generative, and
stakeholder-aware systems is essential to satisfy the evolving expectations of
fashion consumers and brands.

### 9. Why Generate When You Can Transform? Unleashing Generative Attention for Dynamic Recommendation

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Yuli Liu, Wenjun Kong, Cheng Luo, Weizhi Ma
- **URL**: <http://arxiv.org/abs/2508.02050v1>
- **Submitted**: 2025-08-04 04:33:26
- **Comment**: Accepted at ACMMM 2025
- **Topic Keywords**: query, user behavior, recommend
- **Reason**: The paper explores sequential recommendation and transformer models, which are somewhat related to information retrieval and search technologies. However, the focus on recommendation systems and generative attention mechanisms is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Sequential Recommendation (SR) focuses on personalizing user experiences by
predicting future preferences based on historical interactions. Transformer
models, with their attention mechanisms, have become the dominant architecture
in SR tasks due to their ability to capture dependencies in user behavior
sequences. However, traditional attention mechanisms, where attention weights
are computed through query-key transformations, are inherently linear and
deterministic. This fixed approach limits their ability to account for the
dynamic and non-linear nature of user preferences, leading to challenges in
capturing evolving interests and subtle behavioral patterns. Given that
generative models excel at capturing non-linearity and probabilistic
variability, we argue that generating attention distributions offers a more
flexible and expressive alternative compared to traditional attention
mechanisms. To support this claim, we present a theoretical proof demonstrating
that generative attention mechanisms offer greater expressiveness and
stochasticity than traditional deterministic approaches. Building upon this
theoretical foundation, we introduce two generative attention models for SR,
each grounded in the principles of Variational Autoencoders (VAE) and Diffusion
Models (DMs), respectively. These models are designed specifically to generate
adaptive attention distributions that better align with variable user
preferences. Extensive experiments on real-world datasets show our models
significantly outperform state-of-the-art in both accuracy and diversity.

### 10. TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions in IDEs

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Daniele Cipollone, Egor Bogomolov, Arie van Deursen, Maliheh Izadi
- **URL**: <http://arxiv.org/abs/2508.02455v1>
- **Submitted**: 2025-08-04 14:20:39
- **Topic Keywords**: ranking, rank, search
- **Reason**: The paper focuses on code completion in IDEs, which is a specific application of information retrieval. While it uses language models and ranking, the context is different from traditional search and query understanding. The paper's emphasis on code completion and IDEs makes it somewhat relevant to the user's interests, but it does not directly align with their core research themes.

#### Abstract
> Token-level code completion is one of the most critical features in modern
Integrated Development Environments (IDEs). It assists developers by suggesting
relevant identifiers and APIs during coding. While completions are typically
derived from static analysis, their usefulness depends heavily on how they are
ranked, as correct predictions buried deep in the list are rarely seen by
users. Most current systems rely on hand-crafted heuristics or lightweight
machine learning models trained on user logs, which can be further improved to
capture context information and generalize across projects and coding styles.
In this work, we propose a new scoring approach to ranking static completions
using language models in a lightweight and model-agnostic way. Our method
organizes all valid completions into a prefix tree and performs a single greedy
decoding pass to collect token-level scores across the tree. This enables a
precise token-aware ranking without needing beam search, prompt engineering, or
model adaptations. The approach is fast, architecture-agnostic, and compatible
with already deployed models for code completion. These findings highlight a
practical and effective pathway for integrating language models into already
existing tools within IDEs, and ultimately providing smarter and more
responsive developer assistance.

### 11. Beyond Chunks and Graphs: Retrieval-Augmented Generation through Triplet-Driven Thinking

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Shengbo Gong, Xianfeng Tang, Carl Yang, Wei jin
- **URL**: <http://arxiv.org/abs/2508.02435v1>
- **Submitted**: 2025-08-04 13:50:44
- **Comment**: 19 pages
- **Topic Keywords**: rag, retrieval, search
- **Reason**: The paper proposes a novel framework for retrieval-augmented generation, focusing on reducing hallucinations and incorporating external knowledge into Large Language Models. While it's related to information retrieval and NLP, the specific context of language models and triplet-driven thinking doesn't directly align with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Retrieval-augmented generation (RAG) is critical for reducing hallucinations
and incorporating external knowledge into Large Language Models (LLMs).
However, advanced RAG systems face a trade-off between performance and
efficiency. Multi-round RAG approaches achieve strong reasoning but incur
excessive LLM calls and token costs, while Graph RAG methods suffer from
computationally expensive, error-prone graph construction and retrieval
redundancy. To address these challenges, we propose T$^2$RAG, a novel framework
that operates on a simple, graph-free knowledge base of atomic triplets.
T$^2$RAG leverages an LLM to decompose questions into searchable triplets with
placeholders, which it then iteratively resolves by retrieving evidence from
the triplet database. Empirical results show that T$^2$RAG significantly
outperforms state-of-the-art multi-round and Graph RAG methods, achieving an
average performance gain of up to 11\% across six datasets while reducing
retrieval costs by up to 45\%. Our code is available at
https://github.com/rockcor/T2RAG

### 12. CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important Before Generation

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Xiaolin Lin, Jingcun Wang, Olga Kondrateva, Yiyu Shi, Bing Li, Grace Li Zhang
- **URL**: <http://arxiv.org/abs/2508.02401v1>
- **Submitted**: 2025-08-04 13:26:16
- **Topic Keywords**: query, retrieval
- **Reason**: The paper focuses on compressing key-value caches in large language models, which is a topic related to information retrieval and search technologies. However, the specific problem and solution presented are not directly aligned with the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Recent advances in large language models (LLMs) have significantly boosted
long-context processing. However, the increasing key-value (KV) cache size
poses critical challenges to memory and execution efficiency. Most KV cache
compression methods rely on heuristic token eviction using all attention heads
in Grouped Query Attention (GQA)-based LLMs. This method ignores the different
functionalities of attention heads, leading to the eviction of critical tokens
and thus degrades the performance of LLMs.
  To address the issue above, instead of using all the attention heads in
GQA-based LLMs to determine important tokens as in the previous work, we first
identify the attention heads in each layer that are not only capable of
retrieving the initial and final tokens of a prompt, but also capable of
retrieving important tokens within the text and attending to their surrounding
semantic context. Afterwards, we exploit such heads to determine the important
tokens and retain their corresponding KV cache pairs. Furthermore, we analyze
the cache eviction error of each layer individually and introduce a
layer-adaptive KV cache allocation strategy. Experimental results demonstrate
the proposed CompressKV consistently outperforms state-of-the-art approaches
under various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.
Our code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.

### 13. From Generation to Consumption: Personalized List Value Estimation for Re-ranking

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Kaike Zhang, Xiaobei Wang, Xiaoyu Liu, Shuchang Liu, Hailan Yang, Xiang Li, Fei Sun, Qi Cao
- **URL**: <http://arxiv.org/abs/2508.02242v1>
- **Submitted**: 2025-08-04 09:43:21
- **Topic Keywords**: ranking, recommend, rank
- **Reason**: The paper focuses on recommender systems, which is a related topic, but it does not directly address query understanding, ranking models, or user behavior modeling in the context of information retrieval. The paper's emphasis on list value estimation and user exit behavior is not directly relevant to my core research interests.

#### Abstract
> Re-ranking is critical in recommender systems for optimizing the order of
recommendation lists, thus improving user satisfaction and platform revenue.
Most existing methods follow a generator-evaluator paradigm, where the
evaluator estimates the overall value of each candidate list. However, they
often ignore the fact that users may exit before consuming the full list,
leading to a mismatch between estimated generation value and actual consumption
value. To bridge this gap, we propose CAVE, a personalized Consumption-Aware
list Value Estimation framework. CAVE formulates the list value as the
expectation over sub-list values, weighted by user-specific exit probabilities
at each position. The exit probability is decomposed into an interest-driven
component and a stochastic component, the latter modeled via a Weibull
distribution to capture random external factors such as fatigue. By jointly
modeling sub-list values and user exit behavior, CAVE yields a more faithful
estimate of actual list consumption value. We further contribute three
large-scale real-world list-wise benchmarks from the Kuaishou platform, varying
in size and user activity patterns. Extensive experiments on these benchmarks,
two Amazon datasets, and online A/B testing on Kuaishou show that CAVE
consistently outperforms strong baselines, highlighting the benefit of
explicitly modeling user exits in re-ranking.

### 14. Contextual Graph Transformer: A Small Language Model for Enhanced Engineering Document Information Extraction

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Karan Reddy, Mayukha Pal
- **URL**: <http://arxiv.org/abs/2508.02532v1>
- **Submitted**: 2025-08-04 15:41:35
- **Topic Keywords**: rag, retrieval
- **Reason**: The paper proposes a novel neural architecture, Contextual Graph Transformer (CGT), for domain-specific question answering in technical documents. While it's related to Natural Language Processing (NLP) and information extraction, it doesn't directly address query understanding, ranking models, or user behavior modeling, which are core aspects of Information Retrieval (IR) and Search technologies. The paper's focus on technical language and domain-specific question answering is somewhat relevant to the user's interests, but not a central match.

#### Abstract
> Standard transformer-based language models, while powerful for general text,
often struggle with the fine-grained syntax and entity relationships in complex
technical, engineering documents. To address this, we propose the Contextual
Graph Transformer (CGT), a hybrid neural architecture that combines Graph
Neural Networks (GNNs) and Transformers for domain-specific question answering.
CGT constructs a dynamic graph over input tokens using sequential, skip-gram,
and semantic similarity edges, which is processed by GATv2Conv layers for local
structure learning. These enriched embeddings are then passed to a Transformer
encoder to capture global dependencies. Unlike generic large models, technical
domains often require specialized language models with stronger
contextualization and structure awareness. CGT offers a parameter-efficient
solution for such use cases. Integrated into a Retrieval-Augmented Generation
(RAG) pipeline, CGT outperforms baselines like GPT-2 and BERT, achieving 24.7%
higher accuracy than GPT-2 with 62.4% fewer parameters. This gain stems from
CGTs ability to jointly model structural token interactions and long-range
semantic coherence. The model is trained from scratch using a two-phase
approach: pretraining on general text followed by fine-tuning on
domain-specific manuals. This highlights CGTs adaptability to technical
language, enabling better grounding, entity tracking, and retrieval-augmented
responses in real-world applications.

### 15. Pointer: Linear-Complexity Long-Range Modeling without Pre-training

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Zixi Li
- **URL**: <http://arxiv.org/abs/2508.02631v1>
- **Submitted**: 2025-08-04 17:19:56
- **Comment**: Submitted to Nordic AI Meet 2025
- **Topic Keywords**: pairwise
- **Reason**: The paper introduces a novel architecture for long-range sequence modeling, which is relevant to the field of Information Retrieval, particularly in the context of query understanding and ranking models. However, the focus on sequence modeling and attention mechanisms is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling. The paper's relevance is somewhat limited to the user's broader interests in NLP and data mining.

#### Abstract
> We introduce Pointer, a novel architecture that achieves linear $O(NK)$
complexity for long-range sequence modeling while maintaining superior
performance without requiring pre-training. Unlike standard attention
mechanisms that compute $O(N^2)$ pairwise interactions, our approach uses
layer-wise pointer chaining where each layer's pointer selection depends on
previous layer's pointer positions, creating explicit long-distance connections
through pointer chains. We demonstrate that this architecture achieves
$2$--$10\times$ speedup on long sequences compared to standard transformers,
maintains $>95\%$ accuracy on copy tasks at distances up to 2048 tokens, and
learns interpretable pointer patterns that reveal structured dependency
modeling. Our experiments on efficiency benchmarks, long-range dependency
tasks, and interpretability analysis show that Pointer offers a compelling
alternative to attention mechanisms for scenarios requiring efficient
long-range modeling without pre-training dependencies.

### 16. Trainable Dynamic Mask Sparse Attention

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Jingze Shi, Yifan Wu, Bingheng Wu, Yiran Peng, Liangdong Wang, Guang Liu, Yuyu Luo
- **URL**: <http://arxiv.org/abs/2508.02124v1>
- **Submitted**: 2025-08-04 07:05:15
- **Comment**: 8 figures, 4 tables
- **Topic Keywords**: query
- **Reason**: The paper introduces a trainable dynamic mask sparse attention mechanism, which is a novel approach to improve the efficiency of self-attention mechanisms in large language models. While it is related to information retrieval and natural language processing, the focus is on improving the efficiency of language models rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest for the user.

#### Abstract
> In large language models, the demand for modeling long contexts is constantly
increasing, but the quadratic complexity of the standard self-attention
mechanism often becomes a bottleneck. Although existing sparse attention
mechanisms have improved efficiency, they may still encounter issues such as
static patterns or information loss. We introduce a trainable dynamic mask
sparse attention mechanism, Dynamic Mask Attention, which effectively utilizes
content-aware and position-aware sparsity. DMA achieves this through two key
innovations: First, it dynamically generates content-aware sparse masks from
value representations, enabling the model to identify and focus on critical
information adaptively. Second, it implements position-aware sparse attention
computation that effectively skips unnecessary calculation regions. This
dual-sparsity design allows the model to significantly reduce the computational
complexity of important information while retaining complete information,
achieving an excellent balance between information fidelity and computational
efficiency. We have verified the performance of DMA through comprehensive
experiments. Comparative studies show that DMA outperforms multi-head
attention, sliding window attention, multi-head latent attention, and native
sparse attention in terms of perplexity under Chinchilla Scaling Law settings.
Moreover, in challenging multi-query associative recall tasks, DMA also
demonstrates superior performance and efficiency compared to these methods.
Crucially, in the evaluation of a 1.7B parameter model, DMA significantly
outperforms multi-head attention in both standard benchmark performance and the
challenging needle-in-a-haystack task. These experimental results highlight its
capability to balance model efficiency and long-context modeling ability
effectively.

### 17. MArgE: Meshing Argumentative Evidence from Multiple Large Language Models for Justifiable Claim Verification

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Ming Pok Ng, Junqi Jiang, Gabriel Freedman, Antonio Rago, Francesca Toni
- **URL**: <http://arxiv.org/abs/2508.02584v1>
- **Submitted**: 2025-08-04 16:40:02
- **Topic Keywords**: rag
- **Reason**: The paper explores the combination of multiple large language models, which is a relevant topic in Information Retrieval. However, the focus on argumentative evidence and claim verification is not directly related to query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's connection to Natural Language Processing is more tangential, as it does not directly address search technologies or real-time relevance optimization.

#### Abstract
> Leveraging outputs from multiple large language models (LLMs) is emerging as
a method for harnessing their power across a wide range of tasks while
mitigating their capacity for making errors, e.g., hallucinations. However,
current approaches to combining insights from multiple LLMs often involve
unstructured interactions (e.g., free debate), resulting in model generations
that are not faithfully justifiable. In this work, we introduce MArgE, a novel
framework to provide formal structure to the evidence from each LLM, in the
form of a tree of extracted arguments, for the task of claim verification. We
use a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks
and semantics from the field of computational argumentation, to construct
structured argument trees for given claims. This process creates an inspectable
pathway from the initial arguments to the final claim verification decisions,
providing a faithful justification thereof. We show experimentally that MArgE
can significantly outperform single LLMs, including three open-source models
(4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior
methods for unstructured multi-LLM debates. We thus demonstrate the advantages
of incorporating formal, argumentative reasoning mechanisms when combining
multiple LLM outputs.

### 18. LatentPrompt: Optimizing Promts in Latent Space

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Mateusz Bystro≈Ñski, Grzegorz Piotrowski, Nitesh V. Chawla, Tomasz Kajdanowicz
- **URL**: <http://arxiv.org/abs/2508.02452v1>
- **Submitted**: 2025-08-04 14:17:29
- **Topic Keywords**: rag
- **Reason**: The paper presents a framework for optimizing prompts for Large Language Models, which is related to query understanding and ranking models in Information Retrieval. However, the focus on language models and sentiment classification is not directly aligned with the user's primary research interests in search technologies and user behavior modeling.

#### Abstract
> Recent advances have shown that optimizing prompts for Large Language Models
(LLMs) can significantly improve task performance, yet many optimization
techniques rely on heuristics or manual exploration. We present LatentPrompt, a
model-agnostic framework for prompt optimization that leverages latent semantic
space to automatically generate, evaluate, and refine candidate prompts without
requiring hand-crafted rules. Beginning with a set of seed prompts, our method
embeds them in a continuous latent space and systematically explores this space
to identify prompts that maximize task-specific performance. In a
proof-of-concept study on the Financial PhraseBank sentiment classification
benchmark, LatentPrompt increased classification accuracy by approximately 3
percent after a single optimization cycle. The framework is broadly applicable,
requiring only black-box access to an LLM and an automatic evaluation metric,
making it suitable for diverse domains and tasks.

### 19. Evaluating User Experience in Conversational Recommender Systems: A Systematic Review Across Classical and LLM-Powered Approaches

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Raj Mahmud, Yufeng Wu, Abdullah Bin Sawad, Shlomo Berkovsky, Mukesh Prasad, A. Baki Kocaballi
- **URL**: <http://arxiv.org/abs/2508.02096v1>
- **Submitted**: 2025-08-04 06:07:33
- **Comment**: Accepted at OZCHI 2025. 23 pages, 1 figure, 5 tables
- **Topic Keywords**: recommend, search
- **Reason**: The paper evaluates user experience in Conversational Recommender Systems, which is a related topic to Information Retrieval and Search technologies. However, the focus on conversational systems and user experience evaluation is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Conversational Recommender Systems (CRSs) are receiving growing research
attention across domains, yet their user experience (UX) evaluation remains
limited. Existing reviews largely overlook empirical UX studies, particularly
in adaptive and large language model (LLM)-based CRSs. To address this gap, we
conducted a systematic review following PRISMA guidelines, synthesising 23
empirical studies published between 2017 and 2025. We analysed how UX has been
conceptualised, measured, and shaped by domain, adaptivity, and LLM.
  Our findings reveal persistent limitations: post hoc surveys dominate,
turn-level affective UX constructs are rarely assessed, and adaptive behaviours
are seldom linked to UX outcomes. LLM-based CRSs introduce further challenges,
including epistemic opacity and verbosity, yet evaluations infrequently address
these issues. We contribute a structured synthesis of UX metrics, a comparative
analysis of adaptive and nonadaptive systems, and a forward-looking agenda for
LLM-aware UX evaluation. These findings support the development of more
transparent, engaging, and user-centred CRS evaluation practices.

### 20. Simple Methods Defend RAG Systems Well Against Real-World Attacks

- **LLM Score**: 2
- **Keyword Score**: 13
- **Authors**: Ilias Triantafyllopoulos, Renyi Qu, Salvatore Giorgi, Brenda Curtis, Lyle H. Ungar, Jo√£o Sedoc
- **URL**: <http://arxiv.org/abs/2508.02296v1>
- **Submitted**: 2025-08-04 11:04:54
- **Topic Keywords**: query, queries, relevance, rag, retrieval
- **Reason**: This paper focuses on ensuring safety and in-domain responses for Retrieval-Augmented Generation (RAG) systems, which is not directly related to my research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling. The paper's emphasis on Out-Of-Domain (OOD) query detection and dimensionality reduction strategies is also not aligned with my primary focus on deep semantic understanding and real-time relevance optimization.

#### Abstract
> Ensuring safety and in-domain responses for Retrieval-Augmented Generation
(RAG) systems is paramount in safety-critical applications, yet remains a
significant challenge. To address this, we evaluate four methodologies for
Out-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal
Component Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG
system only responds to queries confined to the system's knowledge base.
Specifically, our evaluation explores two novel dimensionality reduction and
feature separation strategies: \textit{PCA}, where top components are selected
using explained variance or OOD separability, and an adaptation of
\textit{Neural Collapse Feature Separation}. We validate our approach on
standard datasets (StackExchange and MSMARCO) and real-world applications
(Substance Use and COVID-19), including tests against LLM-simulated and actual
attacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations
of response correctness and relevance, we confirm that an external OOD detector
is crucial for maintaining response relevance.

### 21. Quantum-RAG and PunGPT2: Advancing Low-Resource Language Generation and Retrieval for the Punjabi Language

- **LLM Score**: 2
- **Keyword Score**: 13
- **Authors**: Jaskaranjeet Singh, Rakesh Thakur
- **URL**: <http://arxiv.org/abs/2508.01918v1>
- **Submitted**: 2025-08-03 21:03:22
- **Topic Keywords**: retriever, queries, relevance, rag, retrieval
- **Reason**: The paper focuses on low-resource language generation and retrieval for the Punjabi language, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. Although the paper mentions retrieval-augmented generation and quantum-inspired semantic matching, it does not address the user's specific areas of interest such as query understanding, ranking models, and user behavior modeling.

#### Abstract
> Despite the rapid advancement of large language models (LLMs), low-resource
languages remain largely excluded from the NLP landscape. We present PunGPT2,
the first fully open-source suite of Punjabi large language models, trained
from scratch on a 35GB domain-diverse corpus encompassing literature, religious
texts, news, and social discourse. Unlike prior multilingual approaches,
PunGPT2 captures rich syntactic and morphological features unique to Punjabi
through a tokenizer optimised with byte pair encoding and linguistically
aligned pretraining objectives. To improve factual grounding and domain recall,
we introduce Pun-RAG, a retrieval-augmented generation framework combining
PunGPT2 with a dense FAISS retriever over a curated Punjabi knowledge base. We
further develop Pun-Instruct, a parameter-efficient, instruction-tuned variant
using QLoRA, enabling robust zero-shot and instruction-following performance
with significantly reduced compute needs.
  As a key innovation, we propose Quantum-RAG, a novel hybrid retrieval system
that fuses sparse (BM25) and dense methods with quantum-inspired semantic
matching. By encoding queries using amplitude-based embeddings and retrieving
via quantum kernel similarity, Quantum-RAG achieves improved contextual
relevance with minimal memory overhead marking the first practical integration
of quantum representations in low-resource language generation. Our models
significantly outperform strong multilingual baselines (mBERT, mT5, MuRIL) in
perplexity, factuality, and fluency. This work provides a scalable,
reproducible blueprint for extending LLM capabilities to underrepresented
languages and pioneers quantum-aware retrieval in low-resource NLP

### 22. Dynamic Forgetting and Spatio-Temporal Periodic Interest Modeling for Local-Life Service Recommendation

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Zhaoyu Hu, Hao Guo, Yuan Tian, Erpeng Xue, Jianyang Wang, Xianyang Qi, Hongxiang Lin, Lei Wang, Sheng Chen
- **URL**: <http://arxiv.org/abs/2508.02451v1>
- **Submitted**: 2025-08-04 14:16:49
- **Topic Keywords**: query, user behavior, recommend
- **Reason**: The paper focuses on recommender systems, specifically local-life service recommendation, which is not a primary area of interest. While it mentions user behavior modeling, it does not specifically address query understanding, ranking models, or click models, which are key areas of focus in the user's research interests.

#### Abstract
> In the context of the booming digital economy, recommendation systems, as a
key link connecting users and numerous services, face challenges in modeling
user behavior sequences on local-life service platforms, including the sparsity
of long sequences and strong spatio-temporal dependence. Such challenges can be
addressed by drawing an analogy to the forgetting process in human memory. This
is because users' responses to recommended content follow the recency effect
and the cyclicality of memory. By exploring this, this paper introduces the
forgetting curve and proposes Spatio-Temporal periodic Interest Modeling (STIM)
with long sequences for local-life service recommendation. STIM integrates
three key components: a dynamic masking module based on the forgetting curve,
which is used to extract both recent spatiotemporal features and periodic
spatiotemporal features; a query-based mixture of experts (MoE) approach that
can adaptively activate expert networks under different dynamic masks, enabling
the collaborative modeling of time, location, and items; and a hierarchical
multi-interest network unit, which captures multi-interest representations by
modeling the hierarchical interactions between the shallow and deep semantics
of users' recent behaviors. By introducing the STIM method, we conducted online
A/B tests and achieved a 1.54\% improvement in gross transaction volume (GTV).
In addition, extended offline experiments also showed improvements. STIM has
been deployed in a large-scale local-life service recommendation system,
serving hundreds of millions of daily active users in core application
scenarios.

### 23. Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Yuerong Song, Xiaoran Liu, Ruixiao Li, Zhigeng Liu, Zengfeng Huang, Qipeng Guo, Ziwei He, Xipeng Qiu
- **URL**: <http://arxiv.org/abs/2508.02558v1>
- **Submitted**: 2025-08-04 16:14:03
- **Comment**: 11 pages, 6 figures
- **Topic Keywords**: relevance, rag
- **Reason**: This paper focuses on accelerating Diffusion Large Language Models (dLLMs) with dynamic cache eviction, which is not directly related to Information Retrieval, Search technologies, query understanding, ranking models, or user behavior modeling. Although it mentions attention patterns, it does not explore query understanding or ranking models, and its relevance to NLP and data mining is limited to the application of dLLMs.

#### Abstract
> Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and
parallel decoding but suffer from prohibitive quadratic computational
complexity and memory overhead during inference. Current caching techniques
accelerate decoding by storing full-layer states, yet impose substantial memory
usage that limit long-context applications. Our analysis of attention patterns
in dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining
salient across decoding steps and low-relevance tokens staying unimportant,
motivating selective cache eviction. We propose Sparse-dLLM, the first
training-free framework integrating dynamic cache eviction with sparse
attention via delayed bidirectional sparse caching. By leveraging the stability
of token saliency over steps, it retains critical tokens and dynamically evicts
unimportant prefix/suffix entries using an attention-guided strategy. Extensive
experiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to
10$\times$ higher throughput than vanilla dLLMs, with comparable performance
and similar peak memory costs, outperforming previous methods in efficiency and
effectiveness.

### 24. Graph Embedding in the Graph Fractional Fourier Transform Domain

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Changjie Sheng, Zhichao Zhang, Wei Yao
- **URL**: <http://arxiv.org/abs/2508.02383v1>
- **Submitted**: 2025-08-04 13:09:47
- **Topic Keywords**: rag, ctr, search
- **Reason**: This paper focuses on graph embedding and spectral graph embedding, which is not directly related to Information Retrieval, Search technologies, or query understanding. The techniques and domains discussed in the paper are not relevant to the user's research interests.

#### Abstract
> Spectral graph embedding plays a critical role in graph representation
learning by generating low-dimensional vector representations from graph
spectral information. However, the embedding space of traditional spectral
embedding methods often exhibit limited expressiveness, failing to exhaustively
capture latent structural features across alternative transform domains. To
address this issue, we use the graph fractional Fourier transform to extend the
existing state-of-the-art generalized frequency filtering embedding (GEFFE)
into fractional domains, giving birth to the generalized fractional filtering
embedding (GEFRFE), which enhances embedding informativeness via the graph
fractional domain. The GEFRFE leverages graph fractional domain filtering and a
nonlinear composition of eigenvector components derived from a fractionalized
graph Laplacian. To dynamically determine the fractional order, two parallel
strategies are introduced: search-based optimization and a ResNet18-based
adaptive learning. Extensive experiments on six benchmark datasets demonstrate
that the GEFRFE captures richer structural features and significantly enhance
classification performance. Notably, the proposed method retains computational
complexity comparable to GEFFE approaches.

### 25. CellForge: Agentic Design of Virtual Cell Models

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Xiangru Tang, Zhuoyun Yu, Jiapeng Chen, Yan Cui, Daniel Shao, Weixu Wang, Fang Wu, Yuchen Zhuang, Wenqi Shi, Zhi Huang, Arman Cohan, Xihong Lin, Fabian Theis, Smita Krishnaswamy, Mark Gerstein
- **URL**: <http://arxiv.org/abs/2508.02276v1>
- **Submitted**: 2025-08-04 10:43:31
- **Topic Keywords**: rag, retrieval, search
- **Reason**: The paper is not relevant to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on virtual cell modeling, artificial intelligence, and biology, which is outside the user's primary research areas.

#### Abstract
> Virtual cell modeling represents an emerging frontier at the intersection of
artificial intelligence and biology, aiming to predict quantities such as
responses to diverse perturbations quantitatively. However, autonomously
building computational models for virtual cells is challenging due to the
complexity of biological systems, the heterogeneity of data modalities, and the
need for domain-specific expertise across multiple disciplines. Here, we
introduce CellForge, an agentic system that leverages a multi-agent framework
that transforms presented biological datasets and research objectives directly
into optimized computational models for virtual cells. More specifically, given
only raw single-cell multi-omics data and task descriptions as input, CellForge
outputs both an optimized model architecture and executable code for training
virtual cell models and inference. The framework integrates three core modules:
Task Analysis for presented dataset characterization and relevant literature
retrieval, Method Design, where specialized agents collaboratively develop
optimized modeling strategies, and Experiment Execution for automated
generation of code. The agents in the Design module are separated into experts
with differing perspectives and a central moderator, and have to
collaboratively exchange solutions until they achieve a reasonable consensus.
We demonstrate CellForge's capabilities in single-cell perturbation prediction,
using six diverse datasets that encompass gene knockouts, drug treatments, and
cytokine stimulations across multiple modalities. CellForge consistently
outperforms task-specific state-of-the-art methods. Overall, CellForge
demonstrates how iterative interaction between LLM agents with differing
perspectives provides better solutions than directly addressing a modeling
challenge. Our code is publicly available at
https://github.com/gersteinlab/CellForge.

### 26. Hidden in the Noise: Unveiling Backdoors in Audio LLMs Alignment through Latent Acoustic Pattern Triggers

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Liang Lin, Miao Yu, Kaiwen Luo, Yibo Zhang, Lilan Peng, Dexian Wang, Xuehai Tang, Yuanhe Zhang, Xikang Yang, Zhenhong Zhou, Kun Wang, Yang Liu
- **URL**: <http://arxiv.org/abs/2508.02175v2>
- **Submitted**: 2025-08-04 08:15:16
- **Topic Keywords**: rag, ctr, search
- **Reason**: This paper is not relevant to your research interests as it focuses on audio large language models and backdoor attacks, which is outside the scope of information retrieval, search technologies, and natural language processing. The paper's emphasis on acoustic features and audio processing is also not aligned with your background in e-commerce and interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> As Audio Large Language Models (ALLMs) emerge as powerful tools for speech
processing, their safety implications demand urgent attention. While
considerable research has explored textual and vision safety, audio's distinct
characteristics present significant challenges. This paper first investigates:
Is ALLM vulnerable to backdoor attacks exploiting acoustic triggers? In
response to this issue, we introduce Hidden in the Noise (HIN), a novel
backdoor attack framework designed to exploit subtle, audio-specific features.
HIN applies acoustic modifications to raw audio waveforms, such as alterations
to temporal dynamics and strategic injection of spectrally tailored noise.
These changes introduce consistent patterns that an ALLM's acoustic feature
encoder captures, embedding robust triggers within the audio stream. To
evaluate ALLM robustness against audio-feature-based triggers, we develop the
AudioSafe benchmark, assessing nine distinct risk types. Extensive experiments
on AudioSafe and three established safety datasets reveal critical
vulnerabilities in existing ALLMs: (I) audio features like environment noise
and speech rate variations achieve over 90% average attack success rate. (II)
ALLMs exhibit significant sensitivity differences across acoustic features,
particularly showing minimal response to volume as a trigger, and (III)
poisoned sample inclusion causes only marginal loss curve fluctuations,
highlighting the attack's stealth.

### 27. CRINN: Contrastive Reinforcement Learning for Approximate Nearest Neighbor Search

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Xiaoya Li, Xiaofei Sun, Albert Wang, Chris Shum, Jiwei Li
- **URL**: <http://arxiv.org/abs/2508.02091v1>
- **Submitted**: 2025-08-04 05:57:46
- **Comment**: Preprint Version
- **Topic Keywords**: rag, retrieval, search
- **Reason**: The paper focuses on Approximate Nearest-Neighbor Search (ANNS) algorithms, which is not directly related to Information Retrieval (IR) or Search technologies. While it mentions reinforcement learning, which is a related topic, the application is not in the context of query understanding, ranking models, or user behavior modeling, making it only loosely relevant to your research interests.

#### Abstract
> Approximate nearest-neighbor search (ANNS) algorithms have become
increasingly critical for recent AI applications, particularly in
retrieval-augmented generation (RAG) and agent-based LLM applications. In this
paper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS
optimization as a reinforcement learning problem where execution speed serves
as the reward signal. This approach enables the automatic generation of
progressively faster ANNS implementations while maintaining accuracy
constraints. Our experimental evaluation demonstrates CRINN's effectiveness
across six widely-used NNS benchmark datasets. When compared against
state-of-the-art open-source ANNS algorithms, CRINN achieves best performance
on three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and
GloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean
and GloVe-25-angular). The implications of CRINN's success reach well beyond
ANNS optimization: It validates that LLMs augmented with reinforcement learning
can function as an effective tool for automating sophisticated algorithmic
optimizations that demand specialized knowledge and labor-intensive manual
refinement.Code can be found at https://github.com/deepreinforce-ai/CRINN

### 28. Controllable and Stealthy Shilling Attacks via Dispersive Latent Diffusion

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Shutong Qiao, Wei Yuan, Junliang Yu, Tong Chen, Quoc Viet Hung Nguyen, Hongzhi Yin
- **URL**: <http://arxiv.org/abs/2508.01987v1>
- **Submitted**: 2025-08-04 01:54:32
- **Topic Keywords**: ranking, recommend, rank
- **Reason**: This paper is not directly related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The focus is on recommender systems and shilling attacks, which is a different area of research. While the paper mentions user behavior modeling, it is not in the context of search or IR.

#### Abstract
> Recommender systems (RSs) are now fundamental to various online platforms,
but their dependence on user-contributed data leaves them vulnerable to
shilling attacks that can manipulate item rankings by injecting fake users.
Although widely studied, most existing attack models fail to meet two critical
objectives simultaneously: achieving strong adversarial promotion of target
items while maintaining realistic behavior to evade detection. As a result, the
true severity of shilling threats that manage to reconcile the two objectives
remains underappreciated. To expose this overlooked vulnerability, we present
DLDA, a diffusion-based attack framework that can generate highly effective yet
indistinguishable fake users by enabling fine-grained control over target
promotion. Specifically, DLDA operates in a pre-aligned collaborative embedding
space, where it employs a conditional latent diffusion process to iteratively
synthesize fake user profiles with precise target item control. To evade
detection, DLDA introduces a dispersive regularization mechanism that promotes
variability and realism in generated behavioral patterns. Extensive experiments
on three real-world datasets and five popular RS models demonstrate that,
compared to prior attacks, DLDA consistently achieves stronger item promotion
while remaining harder to detect. These results highlight that modern RSs are
more vulnerable than previously recognized, underscoring the urgent need for
more robust defenses.

### 29. Six Guidelines for Trustworthy, Ethical and Responsible Automation Design

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Matou≈° Jel√≠nek, Nadine Schlicker, Ewart de Visser
- **URL**: <http://arxiv.org/abs/2508.02371v1>
- **Submitted**: 2025-08-04 13:01:09
- **Topic Keywords**: rag, recommend, search
- **Reason**: The paper focuses on trustworthiness assessment and design guidelines for human-automation interactions, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on user behavior and interaction, the context is more focused on human-computer interaction and ethics rather than search and retrieval.

#### Abstract
> Calibrated trust in automated systems (Lee and See 2004) is critical for
their safe and seamless integration into society. Users should only rely on a
system recommendation when it is actually correct and reject it when it is
factually wrong. One requirement to achieve this goal is an accurate
trustworthiness assessment, ensuring that the user's perception of the system's
trustworthiness aligns with its actual trustworthiness, allowing users to make
informed decisions about the extent to which they can rely on the system
(Schlicker et al. 2022). We propose six design guidelines to help designers
optimize for accurate trustworthiness assessments, thus fostering ethical and
responsible human-automation interactions. The proposed guidelines are derived
from existing literature in various fields, such as human-computer interaction,
cognitive psychology, automation research, user-experience design, and ethics.
We are incorporating key principles from the field of pragmatics, specifically
the cultivation of common ground (H. H. Clark 1996) and Gricean communication
maxims (Grice 1975). These principles are essential for the design of automated
systems because the user's perception of the system's trustworthiness is shaped
by both environmental contexts, such as organizational culture or societal
norms, and by situational context, including the specific circumstances or
scenarios in which the interaction occurs (Hoff and Bashir 2015). Our proposed
guidelines provide actionable insights for designers to create automated
systems that make relevant trustworthiness cues available. This would ideally
foster calibrated trust and more satisfactory, productive, and safe
interactions between humans and automated systems. Furthermore, the proposed
heuristics might work as a tool for evaluating to what extent existing systems
enable users to accurately assess a system's trustworthiness.

### 30. OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical Modeling

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Maxime Bouscary, Saurabh Amin
- **URL**: <http://arxiv.org/abs/2508.02503v1>
- **Submitted**: 2025-08-04 15:11:51
- **Topic Keywords**: query
- **Reason**: The paper focuses on LLM-based optimization and solver selection, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions natural-language descriptions, the context is problem modeling and solving, rather than search or retrieval.

#### Abstract
> LLM-based solvers have emerged as a promising means of automating problem
modeling and solving. However, they remain unreliable and often depend on
iterative repair loops that result in significant latency. We introduce
OptiHive, an LLM-based framework that produces high-quality solvers for
optimization problems from natural-language descriptions without iterative
self-correction. OptiHive uses a single batched LLM query to generate diverse
components (solvers, problem instances, and validation tests) and filters out
erroneous components to ensure fully interpretable outputs. Taking into account
the imperfection of the generated components, we employ a statistical model to
infer their true performance, enabling principled uncertainty quantification
and solver selection. On tasks ranging from traditional optimization problems
to challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive
significantly outperforms baselines, increasing the optimality rate from 5\% to
92\% on the most complex problems.

### 31. AI-Based Measurement of Innovation: Mapping Expert Insight into Large Language Model Applications

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Robin Nowak, Patrick Figge, Carolin Haeussler
- **URL**: <http://arxiv.org/abs/2508.02430v1>
- **Submitted**: 2025-08-04 13:49:30
- **Topic Keywords**: rag, search
- **Reason**: The paper focuses on using large language models to measure innovation, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions machine learning and deep learning, the context is different from the user's primary research interests.

#### Abstract
> Measuring innovation often relies on context-specific proxies and on expert
evaluation. Hence, empirical innovation research is often limited to settings
where such data is available. We investigate how large language models (LLMs)
can be leveraged to overcome the constraints of manual expert evaluations and
assist researchers in measuring innovation. We design an LLM framework that
reliably approximates domain experts' assessment of innovation from
unstructured text data. We demonstrate the performance and broad applicability
of this framework through two studies in different contexts: (1) the
innovativeness of software application updates and (2) the originality of
user-generated feedback and improvement ideas in product reviews. We compared
the performance (F1-score) and reliability (consistency rate) of our LLM
framework against alternative measures used in prior innovation studies, and to
state-of-the-art machine learning- and deep learning-based models. The LLM
framework achieved higher F1-scores than the other approaches, and its results
are highly consistent (i.e., results do not change across runs). This article
equips R&D personnel in firms, as well as researchers, reviewers, and editors,
with the knowledge and tools to effectively use LLMs for measuring innovation
and evaluating the performance of LLM-based innovation measures. In doing so,
we discuss, the impact of important design decisions-including model selection,
prompt engineering, training data size, training data distribution, and
parameter settings-on performance and reliability. Given the challenges
inherent in using human expert evaluation and existing text-based measures, our
framework has important implications for harnessing LLMs as reliable,
increasingly accessible, and broadly applicable research tools for measuring
innovation.

### 32. Dynaword: From One-shot to Continuously Developed Datasets

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Kenneth Enevoldsen, Kristian N√∏rgaard Jensen, Jan Kostkan, Bal√°zs Szab√≥, M√°rton Kardos, Kirten Vad, Johan Heinsen, Andrea Blasi N√∫√±ez, Gianluca Barmina, Jacob Nielsen, Rasmus Larsen, Peter Vahlstrup, Per M√∏ldrup Dalum, Desmond Elliott, Lukas Galke, Peter Schneider-Kamp, Kristoffer Nielbo
- **URL**: <http://arxiv.org/abs/2508.02271v2>
- **Submitted**: 2025-08-04 10:30:42
- **Topic Keywords**: rag, search
- **Reason**: This paper is not directly related to Information Retrieval, Search technologies, or query understanding, ranking models, or user behavior modeling. While it touches on Natural Language Processing, the focus is on dataset creation and community collaboration, which is not a primary interest.

#### Abstract
> Large-scale datasets are foundational for research and development in natural
language processing. However, current approaches face three key challenges: (1)
reliance on ambiguously licensed sources restricting use, sharing, and
derivative works; (2) static dataset releases that prevent community
contributions and diminish longevity; and (3) quality assurance processes
restricted to publishing teams rather than leveraging community expertise.
  To address these limitations, we introduce two contributions: the Dynaword
approach and Danish Dynaword. The Dynaword approach is a framework for creating
large-scale, open datasets that can be continuously updated through community
collaboration. Danish Dynaword is a concrete implementation that validates this
approach and demonstrates its potential. Danish Dynaword contains over four
times as many tokens as comparable releases, is exclusively openly licensed,
and has received multiple contributions across industry and research. The
repository includes light-weight tests to ensure data formatting, quality, and
documentation, establishing a sustainable framework for ongoing community
contributions and dataset evolution.

### 33. A Decentralized Framework for Ethical Authorship Validation in Academic Publishing: Leveraging Self-Sovereign Identity and Blockchain Technology

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Kamal Al-Sabahi, Yousuf Khamis Al Mabsali
- **URL**: <http://arxiv.org/abs/2508.01913v1>
- **Submitted**: 2025-08-03 20:26:19
- **Topic Keywords**: rag, search
- **Reason**: This paper is not related to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The paper focuses on a specific problem in academic publishing and proposes a decentralized framework for ethical authorship validation, which is outside the user's areas of interest.

#### Abstract
> Academic publishing, integral to knowledge dissemination and scientific
advancement, increasingly faces threats from unethical practices such as
unconsented authorship, gift authorship, author ambiguity, and undisclosed
conflicts of interest. While existing infrastructures like ORCID effectively
disambiguate researcher identities, they fall short in enforcing explicit
authorship consent, accurately verifying contributor roles, and robustly
detecting conflicts of interest during peer review. To address these
shortcomings, this paper introduces a decentralized framework leveraging
Self-Sovereign Identity (SSI) and blockchain technology. The proposed model
uses Decentralized Identifiers (DIDs) and Verifiable Credentials (VCs) to
securely verify author identities and contributions, reducing ambiguity and
ensuring accurate attribution. A blockchain-based trust registry records
authorship consent and peer-review activity immutably. Privacy-preserving
cryptographic techniques, especially Zero-Knowledge Proofs (ZKPs), support
conflict-of-interest detection without revealing sensitive data. Verified
authorship metadata and consent records are embedded in publications,
increasing transparency. A stakeholder survey of researchers, editors, and
reviewers suggests the framework improves ethical compliance and confidence in
scholarly communication. This work represents a step toward a more transparent,
accountable, and trustworthy academic publishing ecosystem.

### 34. CharBench: Evaluating the Role of Tokenization in Character-Level Tasks

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Omri Uzan, Yuval Pinter
- **URL**: <http://arxiv.org/abs/2508.02591v1>
- **Submitted**: 2025-08-04 16:46:15
- **Topic Keywords**: rag
- **Reason**: The paper focuses on character-level tasks and tokenization, which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. The paper's scope is limited to language models and character-level tasks, which does not align with the user's broader interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Tasks that require character-level reasoning, such as counting or locating
characters within words, remain challenging for contemporary language models. A
common conjecture is that language models' reliance on subword units, rather
than characters, contributes to their struggles with character-level tasks, yet
recent studies offer conflicting conclusions about the role of tokenization,
leaving its impact unclear. To address this gap, we introduce CharBench, a
comprehensive benchmark of character-level tasks that is two orders of
magnitude larger than existing alternatives. We evaluate a diverse range of
leading open-weight and proprietary models on CharBench and find that it
presents a significant challenge to modern LLMs, with an average accuracy of
43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic
properties of words and their segmentations into tokens correspond to model
performance. For counting tasks, we find that tokenization properties are
weakly correlated with correctness, while the length of the queried word and
the actual character count play a more significant part. In contrast, for tasks
requiring intra-word positional understanding, performance is negatively
correlated with the length of the token containing the queried character,
suggesting that longer tokens obscure character position information for LLMs.
We encourage future work to build on the benchmark and evaluation methodology
introduced here as tools for improving model performance on such tasks.

### 35. Parameter-Efficient Routed Fine-Tuning: Mixture-of-Experts Demands Mixture of Adaptation Modules

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yilun Liu, Yunpu Ma, Yuetian Lu, Shuo Chen, Zifeng Ding, Volker Tresp
- **URL**: <http://arxiv.org/abs/2508.02587v1>
- **Submitted**: 2025-08-04 16:43:09
- **Comment**: This paper is a preprint under review. arXiv admin note: text overlap
  with arXiv:2411.08212
- **Topic Keywords**: rag
- **Reason**: The paper focuses on Mixture-of-Experts (MoE) and Parameter-Efficient Fine-Tuning (PEFT), which is not directly related to Information Retrieval, Search technologies, or query understanding. Although it involves language models, the context is not relevant to user behavior modeling or real-time relevance optimization, making it only loosely relevant to the user's research interests.

#### Abstract
> Mixture-of-Experts (MoE) benefits from a dynamic routing mechanism among
their specialized experts, which existing Parameter- Efficient Fine-Tuning
(PEFT) strategies fail to leverage. This motivates us to investigate whether
adaptation modules themselves should incorporate routing mechanisms to align
with MoE's multi-expert architecture. We analyze dynamics of core components
when applying PEFT to MoE language models and examine how different routing
strategies affect adaptation effectiveness. Extensive experiments adapting
OLMoE-1B-7B and Mixtral-8x7B on various commonsense and math reasoning tasks
validate the performance and efficiency of our routed approach. We identify the
optimal configurations for different scenarios and provide empirical analyses
with practical insights to facilitate better PEFT and MoE applications.

### 36. EHSAN: Leveraging ChatGPT in a Hybrid Framework for Arabic Aspect-Based Sentiment Analysis in Healthcare

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Eman Alamoudi, Ellis Solaiman
- **URL**: <http://arxiv.org/abs/2508.02574v1>
- **Submitted**: 2025-08-04 16:28:58
- **Topic Keywords**: rag
- **Reason**: The paper focuses on Arabic aspect-based sentiment analysis in healthcare, leveraging ChatGPT for pseudo-labelling, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper involves NLP and language models, the specific application and methodology are not aligned with the user's core research themes.

#### Abstract
> Arabic-language patient feedback remains under-analysed because dialect
diversity and scarce aspect-level sentiment labels hinder automated assessment.
To address this gap, we introduce EHSAN, a data-centric hybrid pipeline that
merges ChatGPT pseudo-labelling with targeted human review to build the first
explainable Arabic aspect-based sentiment dataset for healthcare. Each sentence
is annotated with an aspect and sentiment label (positive, negative, or
neutral), forming a pioneering Arabic dataset aligned with healthcare themes,
with ChatGPT-generated rationales provided for each label to enhance
transparency. To evaluate the impact of annotation quality on model
performance, we created three versions of the training data: a fully supervised
set with all labels reviewed by humans, a semi-supervised set with 50% human
review, and an unsupervised set with only machine-generated labels. We
fine-tuned two transformer models on these datasets for both aspect and
sentiment classification. Experimental results show that our Arabic-specific
model achieved high accuracy even with minimal human supervision, reflecting
only a minor performance drop when using ChatGPT-only labels. Reducing the
number of aspect classes notably improved classification metrics across the
board. These findings demonstrate an effective, scalable approach to Arabic
aspect-based sentiment analysis (SA) in healthcare, combining large language
model annotation with human expertise to produce a robust and explainable
dataset. Future directions include generalisation across hospitals, prompt
refinement, and interpretable data-driven modelling.

### 37. Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU Neural Networks

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ali Noori, Pratik Devkota, Somya Mohanty, Prashanti Manda
- **URL**: <http://arxiv.org/abs/2508.02556v1>
- **Submitted**: 2025-08-04 16:08:49
- **Topic Keywords**: rag
- **Reason**: This paper focuses on automated annotation of clinical text with standardized medical concepts using a neural sequence labeling approach, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it involves NLP techniques, the specific application and methodology are not aligned with the user's research interests.

#### Abstract
> Automated annotation of clinical text with standardized medical concepts is
critical for enabling structured data extraction and decision support. SNOMED
CT provides a rich ontology for labeling clinical entities, but manual
annotation is labor-intensive and impractical at scale. This study introduces a
neural sequence labeling approach for SNOMED CT concept recognition using a
Bidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text
with domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences
into overlapping 19-token chunks enriched with contextual, syntactic, and
morphological features. The Bi-GRU model assigns IOB tags to identify concept
spans and achieves strong performance with a 90 percent F1-score on the
validation set. These results surpass traditional rule-based systems and match
or exceed existing neural models. Qualitative analysis shows effective handling
of ambiguous terms and misspellings. Our findings highlight that lightweight
RNN-based architectures can deliver high-quality clinical concept annotation
with significantly lower computational cost than transformer-based models,
making them well-suited for real-world deployment.

### 38. What's in the News? Towards Identification of Bias by Commission, Omission, and Source Selection (COSS)

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Anastasia Zhukova, Terry Ruas, Felix Hamborg, Karsten Donnay, Bela Gipp
- **URL**: <http://arxiv.org/abs/2508.02540v1>
- **Submitted**: 2025-08-04 15:47:17
- **Comment**: published in the Proceedings of the 2023 ACM/IEEE Joint Conference on
  Digital Libraries
- **Topic Keywords**: rag
- **Reason**: The paper focuses on identifying bias in news articles, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on natural language processing, the scope is limited to text analysis for bias detection, which is not a primary interest in my research.

#### Abstract
> In a world overwhelmed with news, determining which information comes from
reliable sources or how neutral is the reported information in the news
articles poses a challenge to news readers. In this paper, we propose a
methodology for automatically identifying bias by commission, omission, and
source selection (COSS) as a joint three-fold objective, as opposed to the
previous work separately addressing these types of bias. In a pipeline concept,
we describe the goals and tasks of its steps toward bias identification and
provide an example of a visualization that leverages the extracted features and
patterns of text reuse.

### 39. PoeTone: A Framework for Constrained Generation of Structured Chinese Songci with LLMs

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zhan Qu, Shuzhou Yuan, Michael F√§rber
- **URL**: <http://arxiv.org/abs/2508.02515v1>
- **Submitted**: 2025-08-04 15:19:22
- **Topic Keywords**: rag
- **Reason**: The paper focuses on the constrained generation of Chinese poetry using large language models, which is not directly related to the user's research interests in Information Retrieval, Search technologies, and Natural Language Processing. While the paper explores the capabilities of language models, it does not address query understanding, ranking models, or user behavior modeling, which are key areas of interest for the user.

#### Abstract
> This paper presents a systematic investigation into the constrained
generation capabilities of large language models (LLMs) in producing Songci, a
classical Chinese poetry form characterized by strict structural, tonal, and
rhyme constraints defined by Cipai templates. We first develop a comprehensive,
multi-faceted evaluation framework that includes: (i) a formal conformity
score, (ii) automated quality assessment using LLMs, (iii) human evaluation,
and (iv) classification-based probing tasks. Using this framework, we evaluate
the generative performance of 18 LLMs, including 3 proprietary models and 15
open-source models across four families, under five prompting strategies:
zero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought.
Finally, we propose a Generate-Critic architecture in which the evaluation
framework functions as an automated critic. Leveraging the critic's feedback as
a reward signal, we fine-tune three lightweight open-source LLMs via supervised
fine-tuning (SFT), resulting in improvements of up to 5.88% in formal
conformity. Our findings offer new insights into the generative strengths and
limitations of LLMs in producing culturally significant and formally
constrained literary texts.

### 40. AIAP: A No-Code Workflow Builder for Non-Experts with Natural Language and Multi-Agent Collaboration

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Hyunjn An, Yongwon Kim, Wonduk Seo, Joonil Park, Daye Kang, Changhoon Oh, Dokyun Kim, Seunghyun Lee
- **URL**: <http://arxiv.org/abs/2508.02470v1>
- **Submitted**: 2025-08-04 14:36:31
- **Comment**: 14 pages, 6 figures
- **Topic Keywords**: rag
- **Reason**: The paper focuses on AIAP, a no-code platform for designing AI services, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it mentions natural language input, it does not explore query understanding, ranking models, or user behavior modeling. The paper's focus on visual programming and multi-agent collaboration is also not aligned with the user's interests in NLP, data mining, or related topics.

#### Abstract
> While many tools are available for designing AI, non-experts still face
challenges in clearly expressing their intent and managing system complexity.
We introduce AIAP, a no-code platform that integrates natural language input
with visual workflows. AIAP leverages a coordinated multi-agent system to
decompose ambiguous user instructions into modular, actionable steps, hidden
from users behind a unified interface. A user study involving 32 participants
showed that AIAP's AI-generated suggestions, modular workflows, and automatic
identification of data, actions, and context significantly improved
participants' ability to develop services intuitively. These findings highlight
that natural language-based visual programming significantly reduces barriers
and enhances user experience in AI service design.

### 41. Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination via Attention Lens

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Haohan Zheng, Zhenguo Zhang
- **URL**: <http://arxiv.org/abs/2508.02419v1>
- **Submitted**: 2025-08-04 13:40:59
- **Topic Keywords**: rag
- **Reason**: The paper focuses on Large vision-language models (LVLMs) and object hallucination, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on attention mechanisms, it is primarily concerned with multimodal comprehension and reasoning, which is outside the scope of the user's research interests.

#### Abstract
> Large vision-language models (LVLMs) have demonstrated remarkable multimodal
comprehension and reasoning capabilities, but they still suffer from severe
object hallucination. Previous studies primarily attribute the flaw to
linguistic prior caused by the scale mismatch between visual encoders and large
language models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon
LLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs,
generating descriptions inconsistent with visual cues. However, through an
in-depth investigation of the hallucinated mechanisms, we empirically reveal a
previously overlooked phenomenon: LVLMs may ignore not only visual information
but also textual modality during hallucination, a behavior termed as modality
bias, which indicates that LVLMs struggle to simultaneously attend to both
visual and textual modalities, leading to fragmented understanding of
user-provided instructions. Based on this observation, we propose a simple yet
effective training-free method to mitigate object hallucination. Concretely, we
intervene and adjust the attention weights of textual and visual tokens,
balancing cross-modal compatibility for better alignment with user intentions.
Furthermore, we adopt a contrastive decoding strategy to reduce the LVLM's
overreliance on its parametric knowledge, synergistically enhancing our
attention manipulation. Extensive experiments confirm the widespread presence
of modality bias in LVLMs. Notably, our method effectively mitigates
hallucination across multiple open-source LVLMs and benchmarks, highlighting
its generalizability and efficacy.

### 42. Understanding and Mitigating Political Stance Cross-topic Generalization in Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Jiayi Zhang, Shu Yang, Junchao Wu, Derek F. Wong, Di Wang
- **URL**: <http://arxiv.org/abs/2508.02360v1>
- **Submitted**: 2025-08-04 12:49:10
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to Information Retrieval or Search technologies, and does not address query understanding, ranking models, or user behavior modeling. The focus on large language models and political stance cross-topic generalization is outside the scope of the user's research interests.

#### Abstract
> Fine-tuning Large Language Models on a political topic will significantly
manipulate their political stance on various issues and unintentionally affect
their stance on unrelated topics. While previous studies have proposed this
issue, there is still a lack of understanding regarding the internal
representations of these stances and the mechanisms that lead to unintended
cross-topic generalization. In this paper, we systematically explore the
internal mechanisms underlying this phenomenon from a neuron-level perspective
and how to mitigate the cross-topic generalization of political fine-tuning.
Firstly, we propose Political Neuron Localization through Activation
Contrasting (PNLAC) to identify two distinct types of political neurons:
general political neurons, which govern stance across multiple political
topics, and topic-specific neurons} that affect the model's political stance on
individual topics. We find the existence of these political neuron types across
four models and datasets through activation patching experiments. Leveraging
these insights, we introduce InhibitFT, an inhibition-based fine-tuning method,
effectively mitigating the cross-topic stance generalization. Experimental
results demonstrate the robustness of identified neuron types across various
models and datasets, and show that InhibitFT significantly reduces the
cross-topic stance generalization by 20% on average, while preserving
topic-specific performance. Moreover, we demonstrate that selectively
inhibiting only 5% of neurons is sufficient to effectively mitigate the
cross-topic stance generalization.

### 43. CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert Redundancy Analysis

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yuzhuang Xu, Xu Han, Yuanchi Zhang, Yixuan Wang, Yijun Liu, Shiyu Ji, Qingfu Zhu, Wanxiang Che
- **URL**: <http://arxiv.org/abs/2508.02322v1>
- **Submitted**: 2025-08-04 11:42:48
- **Comment**: 16 pages, 9 figures, 7 tables
- **Topic Keywords**: rag
- **Reason**: This paper focuses on compressing Mixture-of-Experts (MoE) models for Large Language Models, which is not directly related to Information Retrieval, Search technologies, or query understanding. While it mentions some optimization techniques, the context is not relevant to the user's primary research interests.

#### Abstract
> Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are
distinguished by their strong performance scaling with increasing parameters
across a wide range of tasks, yet they also suffer from substantial
computational and storage overheads. Notably, the performance gains of MoE
models do not scale proportionally with the growth in expert parameters. While
prior works attempt to reduce parameters via expert-level pruning, merging, or
decomposition, they still suffer from challenges in both performance and
computational efficiency. In this paper, we address these challenges by
introducing micro-expert as a finer-grained compression unit that spans across
matrices. We first establish a more fundamental perspective, viewing MoE layers
as mixtures of micro-experts, and present CAMERA, a lightweight and
training-free framework for identifying micro-expert redundancy. Our analysis
uncovers significant variance in micro-expert contributions during decoding.
Based on this insight, we further propose CAMERA-P, a structured micro-expert
pruning framework, and CAMERA-Q, a mixed-precision quantization idea designed
for micro-experts. Extensive experiments on nine downstream tasks show that
CAMERA-P consistently outperforms strong baselines under pruning ratios ranging
from 20% to 60%. Furthermore, CAMERA-Q achieves superior results under
aggressive 2-bit quantization, surpassing existing matrix- and channel-level
ideas. Notably, our method enables complete micro-expert analysis of
Qwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.

### 44. CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative Credit Assignment

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Guofu Xie, Yunsheng Shi, Hongtao Tian, Ting Yao, Xiao Zhang
- **URL**: <http://arxiv.org/abs/2508.02298v1>
- **Submitted**: 2025-08-04 11:06:08
- **Comment**: Work in progress
- **Topic Keywords**: rag
- **Reason**: The paper focuses on reinforcement learning and credit assignment in large language models, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions generative models, the context is different from the user's interests in NLP and data mining.

#### Abstract
> Reinforcement Learning with Verifiable Rewards (RLVR) has improved the
reasoning abilities of Large Language Models (LLMs) by using rule-based binary
feedback, helping to mitigate reward hacking. However, current RLVR methods
typically treat whole responses as single actions, assigning the same reward to
every token. This coarse-grained feedback hampers precise credit assignment,
making it hard for models to identify which reasoning steps lead to success or
failure, and often results in suboptimal policies and inefficient learning.
Methods like PPO provide credit assignment through value estimation, but often
yield inaccurate and unverifiable signals due to limited sampling. On the other
hand, methods using Process Reward Models can provide step-by-step judgments
for each reasoning step, but they require high-quality process supervision
labels and are time-consuming when applied in online reinforcement learning
(RL). To overcome these limitations, we introduce a simple but efficient method
Credit Assignment Policy Optimization (CAPO). Given a reasoning response
rollout from the policy model, CAPO directly leverages an off-the-shelf,
general-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to
generate all step-wise critique by one pass, thereby providing verifiable
token-level rewards to refine the tokens that were originally assigned
identical rule-based rewards. This enables more fine-grained credit assignment
in an effective way. Furthermore, to enhance the accuracy and robustness of
CAPO, we employ voting mechanisms that scale with the number of generated
critiques. Extensive experiments using different backbones like Llama and Qwen
models and in different sizes show that CAPO consistently outperforms
supervised learning-based and RL-based fine-tuning methods across six
challenging mathematical benchmarks and three out-of-domain benchmarks.

### 45. SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic Bidirectional Machine Translation System

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Serry Sibaee, Omer Nacar, Yasser Al-Habashi, Adel Ammar, Wadii Boulila
- **URL**: <http://arxiv.org/abs/2508.02268v1>
- **Submitted**: 2025-08-04 10:21:11
- **Topic Keywords**: rag
- **Reason**: The paper focuses on machine translation between Modern Standard Arabic and the Syrian dialect, which is not directly related to information retrieval, search technologies, or query understanding. While it involves natural language processing, the scope is limited to dialectal Arabic translation, which is not a primary interest of yours.

#### Abstract
> The rich linguistic landscape of the Arab world is characterized by a
significant gap between Modern Standard Arabic (MSA), the language of formal
communication, and the diverse regional dialects used in everyday life. This
diglossia presents a formidable challenge for natural language processing,
particularly machine translation. This paper introduces \textbf{SHAMI-MT}, a
bidirectional machine translation system specifically engineered to bridge the
communication gap between MSA and the Syrian dialect. We present two
specialized models, one for MSA-to-Shami and another for Shami-to-MSA
translation, both built upon the state-of-the-art AraT5v2-base-1024
architecture. The models were fine-tuned on the comprehensive Nabra dataset and
rigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami
model achieved an outstanding average quality score of \textbf{4.01 out of 5.0}
when judged by OPENAI model GPT-4.1, demonstrating its ability to produce
translations that are not only accurate but also dialectally authentic. This
work provides a crucial, high-fidelity tool for a previously underserved
language pair, advancing the field of dialectal Arabic translation and offering
significant applications in content localization, cultural heritage, and
intercultural communication.

### 46. I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal Entity Linking

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ziyan Liu, Junwen Li, Kaiwen Li, Tong Ruan, Chao Wang, Xinyan He, Zongyu Wang, Xuezhi Cao, Jingping Liu
- **URL**: <http://arxiv.org/abs/2508.02243v1>
- **Submitted**: 2025-08-04 09:43:54
- **Comment**: 10 pages, 6 figures, accepted by ACMMM 2025
- **Topic Keywords**: rag
- **Reason**: The paper focuses on multimodal entity linking, which is not directly related to information retrieval, search technologies, or query understanding. While it mentions large language models, the application is not in the context of search or ranking models, and the paper's emphasis on visual features and multimodal data does not align with the user's interests in IR and NLP.

#### Abstract
> Multimodal entity linking plays a crucial role in a wide range of
applications. Recent advances in large language model-based methods have become
the dominant paradigm for this task, effectively leveraging both textual and
visual modalities to enhance performance. Despite their success, these methods
still face two challenges, including unnecessary incorporation of image data in
certain scenarios and the reliance only on a one-time extraction of visual
features, which can undermine their effectiveness and accuracy. To address
these challenges, we propose a novel LLM-based framework for the multimodal
entity linking task, called Intra- and Inter-modal Collaborative Reflections.
This framework prioritizes leveraging text information to address the task.
When text alone is insufficient to link the correct entity through intra- and
inter-modality evaluations, it employs a multi-round iterative strategy that
integrates key visual clues from various aspects of the image to support
reasoning and enhance matching accuracy. Extensive experiments on three widely
used public datasets demonstrate that our framework consistently outperforms
current state-of-the-art methods in the task, achieving improvements of 3.2%,
5.1%, and 1.6%, respectively. Our code is available at
https://github.com/ziyan-xiaoyu/I2CR/.

### 47. LeanK: Learnable K Cache Channel Pruning for Efficient Decoding

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yike Zhang, Zhiyuan He, Huiqiang Jiang, Chengruidong Zhang, Yuqing Yang, Jianyong Wang, Lili Qiu
- **URL**: <http://arxiv.org/abs/2508.02215v1>
- **Submitted**: 2025-08-04 09:08:43
- **Topic Keywords**: rag
- **Reason**: This paper is not relevant to the user's research interests in Information Retrieval, Search technologies, query understanding, ranking models, and user behavior modeling. The paper focuses on pruning key-value cache channels for efficient decoding in large language models, which is a topic in NLP, but not directly related to the user's primary interests.

#### Abstract
> Large language models (LLMs) enable long-context tasks but face efficiency
challenges due to the growing key-value (KV) cache. We propose LeanK, a
learning-based method that prunes unimportant key (K) cache channels by
leveraging static channel sparsity. With a novel two-stage training process,
LeanK learns channel-wise static mask that could satisfy specific sparsity
ratio and hardware alignment requirement. LeanK reduces GPU memory and
accelerates decoding without sacrificing accuracy. Experiments demonstrate up
to 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel
enables 1.3x speedup for attention computation. We also provide insights into
model channels and attention heads during long-context inference by analyzing
the learned importance distribution. Our code is available at
https://aka.ms/LeanK.

### 48. ProCut: LLM Prompt Compression via Attribution Estimation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Zhentao Xu, Fengyi Li, Albert Chen, Xiaofeng Wang
- **URL**: <http://arxiv.org/abs/2508.02053v1>
- **Submitted**: 2025-08-04 04:44:43
- **Topic Keywords**: rag
- **Reason**: The paper focuses on prompt compression for Large Language Models (LLMs), which is not directly related to Information Retrieval, Search technologies, or query understanding. While it touches on optimization and performance improvement, the context and methodology are distinct from the user's primary research interests.

#### Abstract
> In large-scale industrial LLM systems, prompt templates often expand to
thousands of tokens as teams iteratively incorporate sections such as task
instructions, few-shot examples, and heuristic rules to enhance robustness and
coverage. This expansion leads to bloated prompts that are difficult to
maintain and incur significant inference latency and serving costs. To address
this, we introduce Prompt Compression via Attribution Estimation (ProCut), a
flexible, LLM-agnostic, training-free framework that compresses prompts through
attribution analysis. ProCut segments prompt templates into semantically
meaningful units, quantifies their impact on task performance, and prunes
low-utility components. Through extensive experiments on five public benchmark
datasets and real-world industrial prompts, we show that ProCut achieves
substantial prompt size reductions (78% fewer tokens in production) while
maintaining or even slightly improving task performance (up to 62% better than
alternative methods). We further introduce an LLM-driven attribution estimator
that reduces compression latency by over 50%, and demonstrate that ProCut
integrates seamlessly with existing prompt-optimization frameworks to produce
concise, high-performing prompts.

### 49. SpeechR: A Benchmark for Speech Reasoning in Large Audio-Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Wanqi Yang, Yanda Li, Yunchao Wei, Meng Fang, Ling Chen
- **URL**: <http://arxiv.org/abs/2508.02018v1>
- **Submitted**: 2025-08-04 03:28:04
- **Topic Keywords**: retrieval
- **Reason**: The paper focuses on evaluating the reasoning capabilities of large audio-language models, which is not directly related to information retrieval, search technologies, or query understanding. While it touches on natural language processing, the scope is limited to spoken language and does not address ranking models or user behavior modeling.

#### Abstract
> Large audio-language models (LALMs) have achieved near-human performance in
sentence-level transcription and emotion recognition. However, existing
evaluations focus mainly on surface-level perception, leaving the capacity of
models for contextual and inference-driven reasoning in speech-based scenarios
insufficiently examined. To address this gap, we introduce SpeechR, a unified
benchmark for evaluating reasoning over speech in large audio-language models.
SpeechR evaluates models along three key dimensions: factual retrieval,
procedural inference, and normative judgment. It includes three distinct
evaluation formats. The multiple-choice version measures answer selection
accuracy. The generative version assesses the coherence and logical consistency
of reasoning chains. The acoustic-feature version investigates whether
variations in stress and emotion affect reasoning performance. Evaluations on
eleven state-of-the-art LALMs reveal that high transcription accuracy does not
translate into strong reasoning capabilities. SpeechR establishes a structured
benchmark for evaluating reasoning in spoken language, enabling more targeted
analysis of model capabilities across diverse dialogue-based tasks.

### 50. Monsoon Uprising in Bangladesh: How Facebook Shaped Collective Identity

- **LLM Score**: 0
- **Keyword Score**: 3
- **Authors**: Md Tasin Abir, Arpita Chowdhury, Ashfia Rahman
- **URL**: <http://arxiv.org/abs/2508.02498v1>
- **Submitted**: 2025-08-04 15:07:38
- **Comment**: 10 pages, 9 figures
- **Topic Keywords**: rag, search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The topic is focused on social media, collective identity, and political mobilization, which is outside your primary areas of interest.

#### Abstract
> This study investigates how Facebook shaped collective identity during the
July 2024 pro-democracy uprising in Bangladesh, known as the Monsoon Uprising.
During government repression, protesters turned to Facebook as a central space
for resistance, where multimodal expressions, images, memes, videos, hashtags,
and satirical posts played an important role in unifying participants. Using a
qualitative approach, this research analyzes visual rhetoric, verbal discourse,
and digital irony to reveal how shared symbols, protest art, and slogans built
a sense of solidarity. Key elements included the symbolic use of red, the
ironic metaphorical use of the term "Razakar", and the widespread sharing of
visuals representing courage, injustice, and resistance. The findings show that
the combination of visual and verbal strategies on Facebook not only mobilized
public sentiment, but also built a strong collective identity that challenged
authoritarian narratives. This study tries to demonstrate how online platforms
can serve as powerful tools for identity construction and political
mobilization in the digital age.

---

