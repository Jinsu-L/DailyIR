[
    {
        "title": "RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation",
        "abstract": "Large language models excel at function- and file-level code generation, yet\ngenerating complete repositories from scratch remains a fundamental challenge.\nThis process demands coherent and reliable planning across proposal- and\nimplementation-level stages, while natural language, due to its ambiguity and\nverbosity, is ill-suited for faithfully representing complex software\nstructures. To address this, we introduce the Repository Planning Graph (RPG),\na persistent representation that unifies proposal- and implementation-level\nplanning by encoding capabilities, file structures, data flows, and functions\nin one graph. RPG replaces ambiguous natural language with an explicit\nblueprint, enabling long-horizon planning and scalable repository generation.\nBuilding on RPG, we develop ZeroRepo, a graph-driven framework for repository\ngeneration from scratch. It operates in three stages: proposal-level planning\nand implementation-level refinement to construct the graph, followed by\ngraph-guided code generation with test validation. To evaluate this setting, we\nconstruct RepoCraft, a benchmark of six real-world projects with 1,052 tasks.\nOn RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly\n3.9$\\times$ the strongest baseline (Claude Code) and about 64$\\times$ other\nbaselines. It attains 81.5% functional coverage and a 69.7% pass rate,\nexceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further\nanalysis shows that RPG models complex dependencies, enables progressively more\nsophisticated planning through near-linear scaling, and enhances LLM\nunderstanding of repositories, thereby accelerating agent localization.",
        "url": "http://arxiv.org/abs/2509.16198v1",
        "pdf_url": "http://arxiv.org/pdf/2509.16198v1",
        "arxiv_id": "2509.16198v1",
        "authors": [
            "Jane Luo",
            "Xin Zhang",
            "Steven Liu",
            "Jie Wu",
            "Yiming Huang",
            "Yangyu Huang",
            "Chengyu Yin",
            "Ying Xin",
            "Jianfeng Liu",
            "Yuefeng Zhan",
            "Hao Sun",
            "Qi Chen",
            "Scarlett Li",
            "Mao Yang"
        ],
        "submitted": "2025-09-19 17:58:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MANZANO: A Simple and Scalable Unified Multimodal Model with a Hybrid Vision Tokenizer",
        "abstract": "Unified multimodal Large Language Models (LLMs) that can both understand and\ngenerate visual content hold immense potential. However, existing open-source\nmodels often suffer from a performance trade-off between these capabilities. We\npresent Manzano, a simple and scalable unified framework that substantially\nreduces this tension by coupling a hybrid image tokenizer with a well-curated\ntraining recipe. A single shared vision encoder feeds two lightweight adapters\nthat produce continuous embeddings for image-to-text understanding and discrete\ntokens for text-to-image generation within a common semantic space. A unified\nautoregressive LLM predicts high-level semantics in the form of text and image\ntokens, with an auxiliary diffusion decoder subsequently translating the image\ntokens into pixels. The architecture, together with a unified training recipe\nover understanding and generation data, enables scalable joint learning of both\ncapabilities. Manzano achieves state-of-the-art results among unified models,\nand is competitive with specialist models, particularly on text-rich\nevaluation. Our studies show minimal task conflicts and consistent gains from\nscaling model size, validating our design choice of a hybrid tokenizer.",
        "url": "http://arxiv.org/abs/2509.16197v1",
        "pdf_url": "http://arxiv.org/pdf/2509.16197v1",
        "arxiv_id": "2509.16197v1",
        "authors": [
            "Yanghao Li",
            "Rui Qian",
            "Bowen Pan",
            "Haotian Zhang",
            "Haoshuo Huang",
            "Bowen Zhang",
            "Jialing Tong",
            "Haoxuan You",
            "Xianzhi Du",
            "Zhe Gan",
            "Hyunjik Kim",
            "Chao Jia",
            "Zhenbang Wang",
            "Yinfei Yang",
            "Mingfei Gao",
            "Zi-Yi Dou",
            "Wenze Hu",
            "Chang Gao",
            "Dongxu Li",
            "Philipp Dufter",
            "Zirui Wang",
            "Guoli Yin",
            "Zhengdong Zhang",
            "Chen Chen",
            "Yang Zhao",
            "Ruoming Pang",
            "Zhifeng Chen"
        ],
        "submitted": "2025-09-19 17:58:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Latent learning: episodic memory complements parametric learning by enabling flexible reuse of experiences",
        "abstract": "When do machine learning systems fail to generalize, and what mechanisms\ncould improve their generalization? Here, we draw inspiration from cognitive\nscience to argue that one weakness of machine learning systems is their failure\nto exhibit latent learning -- learning information that is not relevant to the\ntask at hand, but that might be useful in a future task. We show how this\nperspective links failures ranging from the reversal curse in language modeling\nto new findings on agent-based navigation. We then highlight how cognitive\nscience points to episodic memory as a potential part of the solution to these\nissues. Correspondingly, we show that a system with an oracle retrieval\nmechanism can use learning experiences more flexibly to generalize better\nacross many of these challenges. We also identify some of the essential\ncomponents for effectively using retrieval, including the importance of\nwithin-example in-context learning for acquiring the ability to use information\nacross retrieved examples. In summary, our results illustrate one possible\ncontributor to the relative data inefficiency of current machine learning\nsystems compared to natural intelligence, and help to understand how retrieval\nmethods can complement parametric learning to improve generalization.",
        "url": "http://arxiv.org/abs/2509.16189v1",
        "pdf_url": "http://arxiv.org/pdf/2509.16189v1",
        "arxiv_id": "2509.16189v1",
        "authors": [
            "Andrew Kyle Lampinen",
            "Martin Engelcke",
            "Yuxuan Li",
            "Arslan Chaudhry",
            "James L. McClelland"
        ],
        "submitted": "2025-09-19 17:49:25",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CultureScope: A Dimensional Lens for Probing Cultural Understanding in LLMs",
        "abstract": "As large language models (LLMs) are increasingly deployed in diverse cultural\nenvironments, evaluating their cultural understanding capability has become\nessential for ensuring trustworthy and culturally aligned applications.\nHowever, most existing benchmarks lack comprehensiveness and are challenging to\nscale and adapt across different cultural contexts, because their frameworks\noften lack guidance from well-established cultural theories and tend to rely on\nexpert-driven manual annotations. To address these issues, we propose\nCultureScope, the most comprehensive evaluation framework to date for assessing\ncultural understanding in LLMs. Inspired by the cultural iceberg theory, we\ndesign a novel dimensional schema for cultural knowledge classification,\ncomprising 3 layers and 140 dimensions, which guides the automated construction\nof culture-specific knowledge bases and corresponding evaluation datasets for\nany given languages and cultures. Experimental results demonstrate that our\nmethod can effectively evaluate cultural understanding. They also reveal that\nexisting large language models lack comprehensive cultural competence, and\nmerely incorporating multilingual data does not necessarily enhance cultural\nunderstanding. All code and data files are available at\nhttps://github.com/HoganZinger/Culture",
        "url": "http://arxiv.org/abs/2509.16188v1",
        "pdf_url": "http://arxiv.org/pdf/2509.16188v1",
        "arxiv_id": "2509.16188v1",
        "authors": [
            "Jinghao Zhang",
            "Sihang Jiang",
            "Shiwei Guo",
            "Shisong Chen",
            "Yanghua Xiao",
            "Hongwei Feng",
            "Jiaqing Liang",
            "Minggui HE",
            "Shimin Tao",
            "Hongxia Ma"
        ],
        "submitted": "2025-09-19 17:47:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Robust Vision-Language Models via Tensor Decomposition: A Defense Against Adversarial Attacks",
        "abstract": "Vision language models (VLMs) excel in multimodal understanding but are prone\nto adversarial attacks. Existing defenses often demand costly retraining or\nsignificant architecture changes. We introduce a lightweight defense using\ntensor decomposition suitable for any pre-trained VLM, requiring no retraining.\nBy decomposing and reconstructing vision encoder representations, it filters\nadversarial noise while preserving meaning. Experiments with CLIP on COCO and\nFlickr30K show improved robustness. On Flickr30K, it restores 12.3\\%\nperformance lost to attacks, raising Recall@1 accuracy from 7.5\\% to 19.8\\%. On\nCOCO, it recovers 8.1\\% performance, improving accuracy from 3.8\\% to 11.9\\%.\nAnalysis shows Tensor Train decomposition with low rank (8-32) and low residual\nstrength ($\\alpha=0.1-0.2$) is optimal. This method is a practical,\nplug-and-play solution with minimal overhead for existing VLMs.",
        "url": "http://arxiv.org/abs/2509.16163v1",
        "pdf_url": "http://arxiv.org/pdf/2509.16163v1",
        "arxiv_id": "2509.16163v1",
        "authors": [
            "Het Patel",
            "Muzammil Allie",
            "Qian Zhang",
            "Jia Chen",
            "Evangelos E. Papalexakis"
        ],
        "submitted": "2025-09-19 17:16:32",
        "source": "arxiv",
        "comment": "To be presented as a poster at the Workshop on Safe and Trustworthy\n  Multimodal AI Systems (SafeMM-AI), 2025"
    },
    {
        "title": "CodeRAG: Finding Relevant and Necessary Knowledge for Retrieval-Augmented Repository-Level Code Completion",
        "abstract": "Repository-level code completion automatically predicts the unfinished code\nbased on the broader information from the repository. Recent strides in Code\nLarge Language Models (code LLMs) have spurred the development of\nrepository-level code completion methods, yielding promising results.\nNevertheless, they suffer from issues such as inappropriate query construction,\nsingle-path code retrieval, and misalignment between code retriever and code\nLLM. To address these problems, we introduce CodeRAG, a framework tailored to\nidentify relevant and necessary knowledge for retrieval-augmented\nrepository-level code completion. Its core components include log probability\nguided query construction, multi-path code retrieval, and preference-aligned\nBestFit reranking. Extensive experiments on benchmarks ReccEval and CCEval\ndemonstrate that CodeRAG significantly and consistently outperforms\nstate-of-the-art methods. The implementation of CodeRAG is available at\nhttps://github.com/KDEGroup/CodeRAG.",
        "url": "http://arxiv.org/abs/2509.16112v1",
        "pdf_url": "http://arxiv.org/pdf/2509.16112v1",
        "arxiv_id": "2509.16112v1",
        "authors": [
            "Sheng Zhang",
            "Yifan Ding",
            "Shuquan Lian",
            "Shun Song",
            "Hui Li"
        ],
        "submitted": "2025-09-19 15:57:40",
        "source": "arxiv",
        "comment": "EMNLP 2025"
    },
    {
        "title": "It Depends: Resolving Referential Ambiguity in Minimal Contexts with Commonsense Knowledge",
        "abstract": "Ambiguous words or underspecified references require interlocutors to resolve\nthem, often by relying on shared context and commonsense knowledge. Therefore,\nwe systematically investigate whether Large Language Models (LLMs) can leverage\ncommonsense to resolve referential ambiguity in multi-turn conversations and\nanalyze their behavior when ambiguity persists. Further, we study how requests\nfor simplified language affect this capacity. Using a novel multilingual\nevaluation dataset, we test DeepSeek v3, GPT-4o, Qwen3-32B, GPT-4o-mini, and\nLlama-3.1-8B via LLM-as-Judge and human annotations. Our findings indicate that\ncurrent LLMs struggle to resolve ambiguity effectively: they tend to commit to\na single interpretation or cover all possible references, rather than hedging\nor seeking clarification. This limitation becomes more pronounced under\nsimplification prompts, which drastically reduce the use of commonsense\nreasoning and diverse response strategies. Fine-tuning Llama-3.1-8B with Direct\nPreference Optimization substantially improves ambiguity resolution across all\nrequest types. These results underscore the need for advanced fine-tuning to\nimprove LLMs' handling of ambiguity and to ensure robust performance across\ndiverse communication styles.",
        "url": "http://arxiv.org/abs/2509.16107v1",
        "pdf_url": "http://arxiv.org/pdf/2509.16107v1",
        "arxiv_id": "2509.16107v1",
        "authors": [
            "Lukas Ellinger",
            "Georg Groh"
        ],
        "submitted": "2025-09-19 15:49:26",
        "source": "arxiv",
        "comment": "Accepted by UncertaiNLP workshop @ EMNLP 2025"
    },
    {
        "title": "DiEP: Adaptive Mixture-of-Experts Compression through Differentiable Expert Pruning",
        "abstract": "Despite the significant breakthrough of Mixture-of-Experts (MoE), the\nincreasing scale of these MoE models presents huge memory and storage\nchallenges. Existing MoE pruning methods, which involve reducing parameter size\nwith a uniform sparsity across all layers, often lead to suboptimal outcomes\nand performance degradation due to varying expert redundancy in different MoE\nlayers. To address this, we propose a non-uniform pruning strategy, dubbed\n\\textbf{Di}fferentiable \\textbf{E}xpert \\textbf{P}runing (\\textbf{DiEP}), which\nadaptively adjusts pruning rates at the layer level while jointly learning\ninter-layer importance, effectively capturing the varying redundancy across\ndifferent MoE layers. By transforming the global discrete search space into a\ncontinuous one, our method handles exponentially growing non-uniform expert\ncombinations, enabling adaptive gradient-based pruning. Extensive experiments\non five advanced MoE models demonstrate the efficacy of our method across\nvarious NLP tasks. Notably, \\textbf{DiEP} retains around 92\\% of original\nperformance on Mixtral 8$\\times$7B with only half the experts, outperforming\nother pruning methods by up to 7.1\\% on the challenging MMLU dataset.",
        "url": "http://arxiv.org/abs/2509.16105v1",
        "pdf_url": "http://arxiv.org/pdf/2509.16105v1",
        "arxiv_id": "2509.16105v1",
        "authors": [
            "Sikai Bai",
            "Haoxi Li",
            "Jie Zhang",
            "Zicong Hong",
            "Song Guo"
        ],
        "submitted": "2025-09-19 15:47:42",
        "source": "arxiv",
        "comment": "18 pages"
    },
    {
        "title": "Beyond Pointwise Scores: Decomposed Criteria-Based Evaluation of LLM Responses",
        "abstract": "Evaluating long-form answers in high-stakes domains such as law or medicine\nremains a fundamental challenge. Standard metrics like BLEU and ROUGE fail to\ncapture semantic correctness, and current LLM-based evaluators often reduce\nnuanced aspects of answer quality into a single undifferentiated score. We\nintroduce DeCE, a decomposed LLM evaluation framework that separates precision\n(factual accuracy and relevance) and recall (coverage of required concepts),\nusing instance-specific criteria automatically extracted from gold answer\nrequirements. DeCE is model-agnostic and domain-general, requiring no\npredefined taxonomies or handcrafted rubrics. We instantiate DeCE to evaluate\ndifferent LLMs on a real-world legal QA task involving multi-jurisdictional\nreasoning and citation grounding. DeCE achieves substantially stronger\ncorrelation with expert judgments ($r=0.78$), compared to traditional metrics\n($r=0.12$), pointwise LLM scoring ($r=0.35$), and modern multidimensional\nevaluators ($r=0.48$). It also reveals interpretable trade-offs: generalist\nmodels favor recall, while specialized models favor precision. Importantly,\nonly 11.95% of LLM-generated criteria required expert revision, underscoring\nDeCE's scalability. DeCE offers an interpretable and actionable LLM evaluation\nframework in expert domains.",
        "url": "http://arxiv.org/abs/2509.16093v1",
        "pdf_url": "http://arxiv.org/pdf/2509.16093v1",
        "arxiv_id": "2509.16093v1",
        "authors": [
            "Fangyi Yu",
            "Nabeel Seedat",
            "Dasha Herrmannova",
            "Frank Schilder",
            "Jonathan Richard Schwarz"
        ],
        "submitted": "2025-09-19 15:36:02",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SABER: Uncovering Vulnerabilities in Safety Alignment via Cross-Layer Residual Connection",
        "abstract": "Large Language Models (LLMs) with safe-alignment training are powerful\ninstruments with robust language comprehension capabilities. These models\ntypically undergo meticulous alignment procedures involving human feedback to\nensure the acceptance of safe inputs while rejecting harmful or unsafe ones.\nHowever, despite their massive scale and alignment efforts, LLMs remain\nvulnerable to jailbreak attacks, where malicious users manipulate the model to\nproduce harmful outputs that it was explicitly trained to avoid. In this study,\nwe find that the safety mechanisms in LLMs are predominantly embedded in the\nmiddle-to-late layers. Building on this insight, we introduce a novel white-box\njailbreak method, SABER (Safety Alignment Bypass via Extra Residuals), which\nconnects two intermediate layers $s$ and $e$ such that $s < e$, through a\nresidual connection. Our approach achieves a 51% improvement over the\nbest-performing baseline on the HarmBench test set. Furthermore, SABER induces\nonly a marginal shift in perplexity when evaluated on the HarmBench validation\nset. The source code is publicly available at\nhttps://github.com/PalGitts/SABER.",
        "url": "http://arxiv.org/abs/2509.16060v1",
        "pdf_url": "http://arxiv.org/pdf/2509.16060v1",
        "arxiv_id": "2509.16060v1",
        "authors": [
            "Maithili Joshi",
            "Palash Nandi",
            "Tanmoy Chakraborty"
        ],
        "submitted": "2025-09-19 15:10:19",
        "source": "arxiv",
        "comment": "Accepted in EMNLP'25 Main"
    },
    {
        "title": "Think, Verbalize, then Speak: Bridging Complex Thoughts and Comprehensible Speech",
        "abstract": "Spoken dialogue systems increasingly employ large language models (LLMs) to\nleverage their advanced reasoning capabilities. However, direct application of\nLLMs in spoken communication often yield suboptimal results due to mismatches\nbetween optimal textual and verbal delivery. While existing approaches adapt\nLLMs to produce speech-friendly outputs, their impact on reasoning performance\nremains underexplored. In this work, we propose Think-Verbalize-Speak, a\nframework that decouples reasoning from spoken delivery to preserve the full\nreasoning capacity of LLMs. Central to our method is verbalizing, an\nintermediate step that translates thoughts into natural, speech-ready text. We\nalso introduce ReVerT, a latency-efficient verbalizer based on incremental and\nasynchronous summarization. Experiments across multiple benchmarks show that\nour method enhances speech naturalness and conciseness with minimal impact on\nreasoning. The project page with the dataset and the source code is available\nat https://yhytoto12.github.io/TVS-ReVerT",
        "url": "http://arxiv.org/abs/2509.16028v1",
        "pdf_url": "http://arxiv.org/pdf/2509.16028v1",
        "arxiv_id": "2509.16028v1",
        "authors": [
            "Sang Hoon Woo",
            "Sehun Lee",
            "Kang-wook Kim",
            "Gunhee Kim"
        ],
        "submitted": "2025-09-19 14:34:22",
        "source": "arxiv",
        "comment": "EMNLP 2025 Main. Project page: https://yhytoto12.github.io/TVS-ReVerT"
    },
    {
        "title": "Session-Level Spoken Language Assessment with a Multimodal Foundation Model via Multi-Target Learning",
        "abstract": "Spoken Language Assessment (SLA) estimates a learner's oral proficiency from\nspontaneous speech. The growing population of L2 English speakers has\nintensified the demand for reliable SLA, a critical component of Computer\nAssisted Language Learning (CALL). Existing efforts often rely on cascaded\npipelines, which are prone to error propagation, or end-to-end models that\noften operate on a short audio window, which might miss discourse-level\nevidence. This paper introduces a novel multimodal foundation model approach\nthat performs session-level evaluation in a single pass. Our approach couples\nmulti-target learning with a frozen, Whisper ASR model-based speech prior for\nacoustic-aware calibration, allowing for jointly learning holistic and\ntrait-level objectives of SLA without resorting to handcrafted features. By\ncoherently processing the entire response session of an L2 speaker, the model\nexcels at predicting holistic oral proficiency. Experiments conducted on the\nSpeak & Improve benchmark demonstrate that our proposed approach outperforms\nthe previous state-of-the-art cascaded system and exhibits robust cross-part\ngeneralization, producing a compact deployable grader that is tailored for CALL\napplications.",
        "url": "http://arxiv.org/abs/2509.16025v1",
        "pdf_url": "http://arxiv.org/pdf/2509.16025v1",
        "arxiv_id": "2509.16025v1",
        "authors": [
            "Hong-Yun Lin",
            "Jhen-Ke Lin",
            "Chung-Chun Wang",
            "Hao-Chien Lu",
            "Berlin Chen"
        ],
        "submitted": "2025-09-19 14:33:05",
        "source": "arxiv",
        "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"
    },
    {
        "title": "EmoHeal: An End-to-End System for Personalized Therapeutic Music Retrieval from Fine-grained Emotions",
        "abstract": "Existing digital mental wellness tools often overlook the nuanced emotional\nstates underlying everyday challenges. For example, pre-sleep anxiety affects\nmore than 1.5 billion people worldwide, yet current approaches remain largely\nstatic and \"one-size-fits-all\", failing to adapt to individual needs. In this\nwork, we present EmoHeal, an end-to-end system that delivers personalized,\nthree-stage supportive narratives. EmoHeal detects 27 fine-grained emotions\nfrom user text with a fine-tuned XLM-RoBERTa model, mapping them to musical\nparameters via a knowledge graph grounded in music therapy principles (GEMS,\niso-principle). EmoHeal retrieves audiovisual content using the CLAMP3 model to\nguide users from their current state toward a calmer one\n(\"match-guide-target\"). A within-subjects study (N=40) demonstrated significant\nsupportive effects, with participants reporting substantial mood improvement\n(M=4.12, p<0.001) and high perceived emotion recognition accuracy (M=4.05,\np<0.001). A strong correlation between perceived accuracy and therapeutic\noutcome (r=0.72, p<0.001) validates our fine-grained approach. These findings\nestablish the viability of theory-driven, emotion-aware digital wellness tools\nand provides a scalable AI blueprint for operationalizing music therapy\nprinciples.",
        "url": "http://arxiv.org/abs/2509.15986v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15986v1",
        "arxiv_id": "2509.15986v1",
        "authors": [
            "Xinchen Wan",
            "Jinhua Liang",
            "Huan Zhang"
        ],
        "submitted": "2025-09-19 13:52:22",
        "source": "arxiv",
        "comment": "5 pages, 5 figures. Submitted to the 2026 IEEE International\n  Conference on Acoustics, Speech and Signal Processing (ICASSP 2026)"
    },
    {
        "title": "BEFT: Bias-Efficient Fine-Tuning of Language Models",
        "abstract": "Fine-tuning all-bias-terms stands out among various parameter-efficient\nfine-tuning (PEFT) techniques, owing to its out-of-the-box usability and\ncompetitive performance, especially in low-data regimes. Bias-only fine-tuning\nhas the potential for unprecedented parameter efficiency. However, the link\nbetween fine-tuning different bias terms (i.e., bias terms in the query, key,\nor value projections) and downstream performance remains unclear. The existing\napproaches, e.g., based on the magnitude of bias change or empirical Fisher\ninformation, provide limited guidance for selecting the particular bias term\nfor effective fine-tuning. In this paper, we propose an approach for selecting\nthe bias term to be fine-tuned, forming the foundation of our bias-efficient\nfine-tuning (BEFT). We extensively evaluate our bias-efficient approach against\nother bias-selection approaches, across a wide range of large language models\n(LLMs) spanning encoder-only and decoder-only architectures from 110M to 6.7B\nparameters. Our results demonstrate the effectiveness and superiority of our\nbias-efficient approach on diverse downstream tasks, including classification,\nmultiple-choice, and generation tasks.",
        "url": "http://arxiv.org/abs/2509.15974v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15974v1",
        "arxiv_id": "2509.15974v1",
        "authors": [
            "Baichuan Huang",
            "Ananth Balashankar",
            "Amir Aminifar"
        ],
        "submitted": "2025-09-19 13:35:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "VoXtream: Full-Stream Text-to-Speech with Extremely Low Latency",
        "abstract": "We present VoXtream, a fully autoregressive, zero-shot streaming\ntext-to-speech (TTS) system for real-time use that begins speaking from the\nfirst word. VoXtream directly maps incoming phonemes to audio tokens using a\nmonotonic alignment scheme and a dynamic look-ahead that does not delay onset.\nBuilt around an incremental phoneme transformer, a temporal transformer\npredicting semantic and duration tokens, and a depth transformer producing\nacoustic tokens, VoXtream achieves, to our knowledge, the lowest initial delay\namong publicly available streaming TTS: 102 ms on GPU. Despite being trained on\na mid-scale 9k-hour corpus, it matches or surpasses larger baselines on several\nmetrics, while delivering competitive quality in both output- and\nfull-streaming settings. Demo and code are available at\nhttps://herimor.github.io/voxtream.",
        "url": "http://arxiv.org/abs/2509.15969v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15969v1",
        "arxiv_id": "2509.15969v1",
        "authors": [
            "Nikita Torgashov",
            "Gustav Eje Henter",
            "Gabriel Skantze"
        ],
        "submitted": "2025-09-19 13:26:46",
        "source": "arxiv",
        "comment": "5 pages, 1 figure, submitted to IEEE ICASSP 2026"
    },
    {
        "title": "Localmax dynamics for attention in transformers and its asymptotic behavior",
        "abstract": "We introduce a new discrete-time attention model, termed the localmax\ndynamics, which interpolates between the classic softmax dynamics and the\nhardmax dynamics, where only the tokens that maximize the influence toward a\ngiven token have a positive weight. As in hardmax, uniform weights are\ndetermined by a parameter controlling neighbor influence, but the key extension\nlies in relaxing neighborhood interactions through an alignment-sensitivity\nparameter, which allows controlled deviations from pure hardmax behavior. As we\nprove, while the convex hull of the token states still converges to a convex\npolytope, its structure can no longer be fully described by a maximal alignment\nset, prompting the introduction of quiescent sets to capture the invariant\nbehavior of tokens near vertices. We show that these sets play a key role in\nunderstanding the asymptotic behavior of the system, even under time-varying\nalignment sensitivity parameters. We further show that localmax dynamics does\nnot exhibit finite-time convergence and provide results for vanishing, nonzero,\ntime-varying alignment-sensitivity parameters, recovering the limiting behavior\nof hardmax as a by-product. Finally, we adapt Lyapunov-based methods from\nclassical opinion dynamics, highlighting their limitations in the asymmetric\nsetting of localmax interactions and outlining directions for future research.",
        "url": "http://arxiv.org/abs/2509.15958v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15958v1",
        "arxiv_id": "2509.15958v1",
        "authors": [
            "Henri Cimetière",
            "Maria Teresa Chiri",
            "Bahman Gharesifard"
        ],
        "submitted": "2025-09-19 13:18:30",
        "source": "arxiv",
        "comment": "28 pages, 5 figures"
    },
    {
        "title": "EHR-MCP: Real-world Evaluation of Clinical Information Retrieval by Large Language Models via Model Context Protocol",
        "abstract": "Background: Large language models (LLMs) show promise in medicine, but their\ndeployment in hospitals is limited by restricted access to electronic health\nrecord (EHR) systems. The Model Context Protocol (MCP) enables integration\nbetween LLMs and external tools.\n  Objective: To evaluate whether an LLM connected to an EHR database via MCP\ncan autonomously retrieve clinically relevant information in a real hospital\nsetting.\n  Methods: We developed EHR-MCP, a framework of custom MCP tools integrated\nwith the hospital EHR database, and used GPT-4.1 through a LangGraph ReAct\nagent to interact with it. Six tasks were tested, derived from use cases of the\ninfection control team (ICT). Eight patients discussed at ICT conferences were\nretrospectively analyzed. Agreement with physician-generated gold standards was\nmeasured.\n  Results: The LLM consistently selected and executed the correct MCP tools.\nExcept for two tasks, all tasks achieved near-perfect accuracy. Performance was\nlower in the complex task requiring time-dependent calculations. Most errors\narose from incorrect arguments or misinterpretation of tool results. Responses\nfrom EHR-MCP were reliable, though long and repetitive data risked exceeding\nthe context window.\n  Conclusions: LLMs can retrieve clinical data from an EHR via MCP tools in a\nreal hospital setting, achieving near-perfect performance in simple tasks while\nhighlighting challenges in complex ones. EHR-MCP provides an infrastructure for\nsecure, consistent data access and may serve as a foundation for hospital AI\nagents. Future work should extend beyond retrieval to reasoning, generation,\nand clinical impact assessment, paving the way for effective integration of\ngenerative AI into clinical practice.",
        "url": "http://arxiv.org/abs/2509.15957v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15957v1",
        "arxiv_id": "2509.15957v1",
        "authors": [
            "Kanato Masayoshi",
            "Masahiro Hashimoto",
            "Ryoichi Yokoyama",
            "Naoki Toda",
            "Yoshifumi Uwamino",
            "Shogo Fukuda",
            "Ho Namkoong",
            "Masahiro Jinzaki"
        ],
        "submitted": "2025-09-19 13:17:16",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond the Score: Uncertainty-Calibrated LLMs for Automated Essay Assessment",
        "abstract": "Automated Essay Scoring (AES) systems now reach near human agreement on some\npublic benchmarks, yet real-world adoption, especially in high-stakes\nexaminations, remains limited. A principal obstacle is that most models output\na single score without any accompanying measure of confidence or explanation.\nWe address this gap with conformal prediction, a distribution-free wrapper that\nequips any classifier with set-valued outputs and formal coverage guarantees.\nTwo open-source large language models (Llama-3 8B and Qwen-2.5 3B) are\nfine-tuned on three diverse corpora (ASAP, TOEFL11, Cambridge-FCE) and\ncalibrated at a 90 percent risk level. Reliability is assessed with UAcc, an\nuncertainty-aware accuracy that rewards models for being both correct and\nconcise. To our knowledge, this is the first work to combine conformal\nprediction and UAcc for essay scoring. The calibrated models consistently meet\nthe coverage target while keeping prediction sets compact, indicating that\nopen-source, mid-sized LLMs can already support teacher-in-the-loop AES; we\ndiscuss scaling and broader user studies as future work.",
        "url": "http://arxiv.org/abs/2509.15926v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15926v1",
        "arxiv_id": "2509.15926v1",
        "authors": [
            "Ahmed Karim",
            "Qiao Wang",
            "Zheng Yuan"
        ],
        "submitted": "2025-09-19 12:28:50",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025 (Main Conference). Camera-ready version"
    },
    {
        "title": "Re-FRAME the Meeting Summarization SCOPE: Fact-Based Summarization and Personalization via Questions",
        "abstract": "Meeting summarization with large language models (LLMs) remains error-prone,\noften producing outputs with hallucinations, omissions, and irrelevancies. We\npresent FRAME, a modular pipeline that reframes summarization as a semantic\nenrichment task. FRAME extracts and scores salient facts, organizes them\nthematically, and uses these to enrich an outline into an abstractive summary.\nTo personalize summaries, we introduce SCOPE, a reason-out-loud protocol that\nhas the model build a reasoning trace by answering nine questions before\ncontent selection. For evaluation, we propose P-MESA, a multi-dimensional,\nreference-free evaluation framework to assess if a summary fits a target\nreader. P-MESA reliably identifies error instances, achieving >= 89% balanced\naccuracy against human annotations and strongly aligns with human severity\nratings (r >= 0.70). On QMSum and FAME, FRAME reduces hallucination and\nomission by 2 out of 5 points (measured with MESA), while SCOPE improves\nknowledge fit and goal alignment over prompt-only baselines. Our findings\nadvocate for rethinking summarization to improve control, faithfulness, and\npersonalization.",
        "url": "http://arxiv.org/abs/2509.15901v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15901v1",
        "arxiv_id": "2509.15901v1",
        "authors": [
            "Frederic Kirstein",
            "Sonu Kumar",
            "Terry Ruas",
            "Bela Gipp"
        ],
        "submitted": "2025-09-19 11:58:17",
        "source": "arxiv",
        "comment": "Accepted at EMNLP 2025"
    },
    {
        "title": "The Psychology of Falsehood: A Human-Centric Survey of Misinformation Detection",
        "abstract": "Misinformation remains one of the most significant issues in the digital age.\nWhile automated fact-checking has emerged as a viable solution, most current\nsystems are limited to evaluating factual accuracy. However, the detrimental\neffect of misinformation transcends simple falsehoods; it takes advantage of\nhow individuals perceive, interpret, and emotionally react to information. This\nunderscores the need to move beyond factuality and adopt more human-centered\ndetection frameworks. In this survey, we explore the evolving interplay between\ntraditional fact-checking approaches and psychological concepts such as\ncognitive biases, social dynamics, and emotional responses. By analyzing\nstate-of-the-art misinformation detection systems through the lens of human\npsychology and behavior, we reveal critical limitations of current methods and\nidentify opportunities for improvement. Additionally, we outline future\nresearch directions aimed at creating more robust and adaptive frameworks, such\nas neuro-behavioural models that integrate technological factors with the\ncomplexities of human cognition and social influence. These approaches offer\npromising pathways to more effectively detect and mitigate the societal harms\nof misinformation.",
        "url": "http://arxiv.org/abs/2509.15896v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15896v1",
        "arxiv_id": "2509.15896v1",
        "authors": [
            "Arghodeep Nandi",
            "Megha Sundriyal",
            "Euna Mehnaz Khan",
            "Jikai Sun",
            "Emily Vraga",
            "Jaideep Srivastava",
            "Tanmoy Chakraborty"
        ],
        "submitted": "2025-09-19 11:51:17",
        "source": "arxiv",
        "comment": "Accepted in EMNLP'25 Main"
    },
    {
        "title": "Distribution-Aligned Decoding for Efficient LLM Task Adaptation",
        "abstract": "Adapting billion-parameter language models to a downstream task is still\ncostly, even with parameter-efficient fine-tuning (PEFT). We re-cast task\nadaptation as output-distribution alignment: the objective is to steer the\noutput distribution toward the task distribution directly during decoding\nrather than indirectly through weight updates. Building on this view, we\nintroduce Steering Vector Decoding (SVD), a lightweight, PEFT-compatible, and\ntheoretically grounded method. We start with a short warm-start fine-tune and\nextract a task-aware steering vector from the Kullback-Leibler (KL) divergence\ngradient between the output distribution of the warm-started and pre-trained\nmodels. This steering vector is then used to guide the decoding process to\nsteer the model's output distribution towards the task distribution. We\ntheoretically prove that SVD is first-order equivalent to the gradient step of\nfull fine-tuning and derive a globally optimal solution for the strength of the\nsteering vector. Across three tasks and nine benchmarks, SVD paired with four\nstandard PEFT methods improves multiple-choice accuracy by up to 5 points and\nopen-ended truthfulness by 2 points, with similar gains (1-2 points) on\ncommonsense datasets without adding trainable parameters beyond the PEFT\nadapter. SVD thus offers a lightweight, theoretically grounded path to stronger\ntask adaptation for large language models.",
        "url": "http://arxiv.org/abs/2509.15888v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15888v1",
        "arxiv_id": "2509.15888v1",
        "authors": [
            "Senkang Hu",
            "Xudong Han",
            "Jinqi Jiang",
            "Yihang Tao",
            "Zihan Fang",
            "Sam Tak Wu Kwong",
            "Yuguang Fang"
        ],
        "submitted": "2025-09-19 11:35:56",
        "source": "arxiv",
        "comment": "Accepted by NeurIPS'25"
    },
    {
        "title": "Optimizing Product Deduplication in E-Commerce with Multimodal Embeddings",
        "abstract": "In large scale e-commerce marketplaces, duplicate product listings frequently\ncause consumer confusion and operational inefficiencies, degrading trust on the\nplatform and increasing costs. Traditional keyword-based search methodologies\nfalter in accurately identifying duplicates due to their reliance on exact\ntextual matches, neglecting semantic similarities inherent in product titles.\nTo address these challenges, we introduce a scalable, multimodal product\ndeduplication designed specifically for the e-commerce domain. Our approach\nemploys a domain-specific text model grounded in BERT architecture in\nconjunction with MaskedAutoEncoders for image representations. Both of these\narchitectures are augmented with dimensionality reduction techniques to produce\ncompact 128-dimensional embeddings without significant information loss.\nComplementing this, we also developed a novel decider model that leverages both\ntext and image vectors. By integrating these feature extraction mechanisms with\nMilvus, an optimized vector database, our system can facilitate efficient and\nhigh-precision similarity searches across extensive product catalogs exceeding\n200 million items with just 100GB of system RAM consumption. Empirical\nevaluations demonstrate that our matching system achieves a macro-average F1\nscore of 0.90, outperforming third-party solutions which attain an F1 score of\n0.83. Our findings show the potential of combining domain-specific adaptations\nwith state-of-the-art machine learning techniques to mitigate duplicate\nlistings in large-scale e-commerce environments.",
        "url": "http://arxiv.org/abs/2509.15858v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15858v1",
        "arxiv_id": "2509.15858v1",
        "authors": [
            "Aysenur Kulunk",
            "Berk Taskin",
            "M. Furkan Eseoglu",
            "H. Bahadir Sahin"
        ],
        "submitted": "2025-09-19 10:49:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Multi-Physics: A Comprehensive Benchmark for Multimodal LLMs Reasoning on Chinese Multi-Subject Physics Problems",
        "abstract": "While multimodal LLMs (MLLMs) demonstrate remarkable reasoning progress,\ntheir application in specialized scientific domains like physics reveals\nsignificant gaps in current evaluation benchmarks. Specifically, existing\nbenchmarks often lack fine-grained subject coverage, neglect the step-by-step\nreasoning process, and are predominantly English-centric, failing to\nsystematically evaluate the role of visual information. Therefore, we introduce\n\\textbf {Multi-Physics} for Chinese physics reasoning, a comprehensive\nbenchmark that includes 5 difficulty levels, featuring 1,412 image-associated,\nmultiple-choice questions spanning 11 high-school physics subjects. We employ a\ndual evaluation framework to evaluate 20 different MLLMs, analyzing both final\nanswer accuracy and the step-by-step integrity of their chain-of-thought.\nFurthermore, we systematically study the impact of difficulty level and visual\ninformation by comparing the model performance before and after changing the\ninput mode. Our work provides not only a fine-grained resource for the\ncommunity but also offers a robust methodology for dissecting the multimodal\nreasoning process of state-of-the-art MLLMs, and our dataset and code have been\nopen-sourced: https://github.com/luozhongze/Multi-Physics.",
        "url": "http://arxiv.org/abs/2509.15839v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15839v1",
        "arxiv_id": "2509.15839v1",
        "authors": [
            "Zhongze Luo",
            "Zhenshuai Yin",
            "Yongxin Guo",
            "Zhichao Wang",
            "Jionghao Zhu",
            "Xiaoying Tang"
        ],
        "submitted": "2025-09-19 10:18:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "The Curious Case of Visual Grounding: Different Effects for Speech- and Text-based Language Encoders",
        "abstract": "How does visual information included in training affect language processing\nin audio- and text-based deep learning models? We explore how such visual\ngrounding affects model-internal representations of words, and find\nsubstantially different effects in speech- vs. text-based language encoders.\nFirstly, global representational comparisons reveal that visual grounding\nincreases alignment between representations of spoken and written language, but\nthis effect seems mainly driven by enhanced encoding of word identity rather\nthan meaning. We then apply targeted clustering analyses to probe for phonetic\nvs. semantic discriminability in model representations. Speech-based\nrepresentations remain phonetically dominated with visual grounding, but in\ncontrast to text-based representations, visual grounding does not improve\nsemantic discriminability. Our findings could usefully inform the development\nof more efficient methods to enrich speech-based models with visually-informed\nsemantics.",
        "url": "http://arxiv.org/abs/2509.15837v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15837v1",
        "arxiv_id": "2509.15837v1",
        "authors": [
            "Adrian Sauter",
            "Willem Zuidema",
            "Marianne de Heer Kloots"
        ],
        "submitted": "2025-09-19 10:16:58",
        "source": "arxiv",
        "comment": "5 pages, 3 figures, Submitted to ICASSP 2026"
    },
    {
        "title": "Best-of-L: Cross-Lingual Reward Modeling for Mathematical Reasoning",
        "abstract": "While the reasoning abilities of large language models (LLMs) continue to\nadvance, it remains unclear how such ability varies across languages in\nmultilingual LLMs and whether different languages produce reasoning paths that\ncomplement each other. To investigate this question, we train a reward model to\nrank generated responses for a given question across languages. Our results\nshow that our cross-lingual reward model substantially improves mathematical\nreasoning performance compared to using reward modeling within a single\nlanguage, benefiting even high-resource languages. While English often exhibits\nthe highest performance in multilingual models, we find that cross-lingual\nsampling particularly benefits English under low sampling budgets. Our findings\nreveal new opportunities to improve multilingual reasoning by leveraging the\ncomplementary strengths of diverse languages.",
        "url": "http://arxiv.org/abs/2509.15811v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15811v1",
        "arxiv_id": "2509.15811v1",
        "authors": [
            "Sara Rajaee",
            "Rochelle Choenni",
            "Ekaterina Shutova",
            "Christof Monz"
        ],
        "submitted": "2025-09-19 09:38:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "RAVE: Retrieval and Scoring Aware Verifiable Claim Detection",
        "abstract": "The rapid spread of misinformation on social media underscores the need for\nscalable fact-checking tools. A key step is claim detection, which identifies\nstatements that can be objectively verified. Prior approaches often rely on\nlinguistic cues or claim check-worthiness, but these struggle with vague\npolitical discourse and diverse formats such as tweets. We present RAVE\n(Retrieval and Scoring Aware Verifiable Claim Detection), a framework that\ncombines evidence retrieval with structured signals of relevance and source\ncredibility. Experiments on CT22-test and PoliClaim-test show that RAVE\nconsistently outperforms text-only and retrieval-based baselines in both\naccuracy and F1.",
        "url": "http://arxiv.org/abs/2509.15793v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15793v1",
        "arxiv_id": "2509.15793v1",
        "authors": [
            "Yufeng Li",
            "Arkaitz Zubiaga"
        ],
        "submitted": "2025-09-19 09:23:41",
        "source": "arxiv",
        "comment": "5 pages, 1 figure"
    },
    {
        "title": "UPRPRC: Unified Pipeline for Reproducing Parallel Resources -- Corpus from the United Nations",
        "abstract": "The quality and accessibility of multilingual datasets are crucial for\nadvancing machine translation. However, previous corpora built from United\nNations documents have suffered from issues such as opaque process, difficulty\nof reproduction, and limited scale. To address these challenges, we introduce a\ncomplete end-to-end solution, from data acquisition via web scraping to text\nalignment. The entire process is fully reproducible, with a minimalist\nsingle-machine example and optional distributed computing steps for\nscalability. At its core, we propose a new Graph-Aided Paragraph Alignment\n(GAPA) algorithm for efficient and flexible paragraph-level alignment. The\nresulting corpus contains over 713 million English tokens, more than doubling\nthe scale of prior work. To the best of our knowledge, this represents the\nlargest publicly available parallel corpus composed entirely of\nhuman-translated, non-AI-generated content. Our code and corpus are accessible\nunder the MIT License.",
        "url": "http://arxiv.org/abs/2509.15789v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15789v1",
        "arxiv_id": "2509.15789v1",
        "authors": [
            "Qiuyang Lu",
            "Fangjian Shen",
            "Zhengkai Tang",
            "Qiang Liu",
            "Hexuan Cheng",
            "Hui Liu",
            "Wushao Wen"
        ],
        "submitted": "2025-09-19 09:21:13",
        "source": "arxiv",
        "comment": "5 pages, 1 figure, submitted to ICASSP2026"
    },
    {
        "title": "Building Data-Driven Occupation Taxonomies: A Bottom-Up Multi-Stage Approach via Semantic Clustering and Multi-Agent Collaboration",
        "abstract": "Creating robust occupation taxonomies, vital for applications ranging from\njob recommendation to labor market intelligence, is challenging. Manual\ncuration is slow, while existing automated methods are either not adaptive to\ndynamic regional markets (top-down) or struggle to build coherent hierarchies\nfrom noisy data (bottom-up). We introduce CLIMB (CLusterIng-based Multi-agent\ntaxonomy Builder), a framework that fully automates the creation of\nhigh-quality, data-driven taxonomies from raw job postings. CLIMB uses global\nsemantic clustering to distill core occupations, then employs a\nreflection-based multi-agent system to iteratively build a coherent hierarchy.\nOn three diverse, real-world datasets, we show that CLIMB produces taxonomies\nthat are more coherent and scalable than existing methods and successfully\ncapture unique regional characteristics. We release our code and datasets at\nhttps://anonymous.4open.science/r/CLIMB.",
        "url": "http://arxiv.org/abs/2509.15786v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15786v1",
        "arxiv_id": "2509.15786v1",
        "authors": [
            "Nan Li",
            "Bo Kang",
            "Tijl De Bie"
        ],
        "submitted": "2025-09-19 09:17:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "UniGist: Towards General and Hardware-aligned Sequence-level Long Context Compression",
        "abstract": "Large language models are increasingly capable of handling long-context\ninputs, but the memory overhead of key-value (KV) cache remains a major\nbottleneck for general-purpose deployment. While various compression strategies\nhave been explored, sequence-level compression, which drops the full KV caches\nfor certain tokens, is particularly challenging as it can lead to the loss of\nimportant contextual information. To address this, we introduce UniGist, a\nsequence-level long-context compression framework that efficiently preserves\ncontext information by replacing raw tokens with special compression tokens\n(gists) in a fine-grained manner. We adopt a chunk-free training strategy and\ndesign an efficient kernel with a gist shift trick, enabling optimized GPU\ntraining. Our scheme also supports flexible inference by allowing the actual\nremoval of compressed tokens, resulting in real-time memory savings.\nExperiments across multiple long-context tasks demonstrate that UniGist\nsignificantly improves compression quality, with especially strong performance\nin detail-recalling tasks and long-range dependency modeling.",
        "url": "http://arxiv.org/abs/2509.15763v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15763v1",
        "arxiv_id": "2509.15763v1",
        "authors": [
            "Chenlong Deng",
            "Zhisong Zhang",
            "Kelong Mao",
            "Shuaiyi Li",
            "Tianqing Fang",
            "Hongming Zhang",
            "Haitao Mi",
            "Dong Yu",
            "Zhicheng Dou"
        ],
        "submitted": "2025-09-19 08:47:37",
        "source": "arxiv",
        "comment": "15 pages, 7 figures"
    },
    {
        "title": "Can LLMs Judge Debates? Evaluating Non-Linear Reasoning via Argumentation Theory Semantics",
        "abstract": "Large Language Models (LLMs) excel at linear reasoning tasks but remain\nunderexplored on non-linear structures such as those found in natural debates,\nwhich are best expressed as argument graphs. We evaluate whether LLMs can\napproximate structured reasoning from Computational Argumentation Theory (CAT).\nSpecifically, we use Quantitative Argumentation Debate (QuAD) semantics, which\nassigns acceptability scores to arguments based on their attack and support\nrelations. Given only dialogue-formatted debates from two NoDE datasets, models\nare prompted to rank arguments without access to the underlying graph. We test\nseveral LLMs under advanced instruction strategies, including Chain-of-Thought\nand In-Context Learning. While models show moderate alignment with QuAD\nrankings, performance degrades with longer inputs or disrupted discourse flow.\nAdvanced prompting helps mitigate these effects by reducing biases related to\nargument length and position. Our findings highlight both the promise and\nlimitations of LLMs in modeling formal argumentation semantics and motivate\nfuture work on graph-aware reasoning.",
        "url": "http://arxiv.org/abs/2509.15739v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15739v1",
        "arxiv_id": "2509.15739v1",
        "authors": [
            "Reza Sanayei",
            "Srdjan Vesic",
            "Eduardo Blanco",
            "Mihai Surdeanu"
        ],
        "submitted": "2025-09-19 08:10:32",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 Findings"
    },
    {
        "title": "REFER: Mitigating Bias in Opinion Summarisation via Frequency Framed Prompting",
        "abstract": "Individuals express diverse opinions, a fair summary should represent these\nviewpoints comprehensively. Previous research on fairness in opinion\nsummarisation using large language models (LLMs) relied on hyperparameter\ntuning or providing ground truth distributional information in prompts.\nHowever, these methods face practical limitations: end-users rarely modify\ndefault model parameters, and accurate distributional information is often\nunavailable. Building upon cognitive science research demonstrating that\nfrequency-based representations reduce systematic biases in human statistical\nreasoning by making reference classes explicit and reducing cognitive load,\nthis study investigates whether frequency framed prompting (REFER) can\nsimilarly enhance fairness in LLM opinion summarisation. Through systematic\nexperimentation with different prompting frameworks, we adapted techniques\nknown to improve human reasoning to elicit more effective information\nprocessing in language models compared to abstract probabilistic\nrepresentations.Our results demonstrate that REFER enhances fairness in\nlanguage models when summarising opinions. This effect is particularly\npronounced in larger language models and using stronger reasoning instructions.",
        "url": "http://arxiv.org/abs/2509.15723v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15723v1",
        "arxiv_id": "2509.15723v1",
        "authors": [
            "Nannan Huang",
            "Haytham M. Fayek",
            "Xiuzhen Zhang"
        ],
        "submitted": "2025-09-19 07:53:51",
        "source": "arxiv",
        "comment": "Accepted to the 5th New Frontiers in Summarization Workshop\n  (NewSumm@EMNLP 2025)"
    },
    {
        "title": "Once Upon a Time: Interactive Learning for Storytelling with Small Language Models",
        "abstract": "Children efficiently acquire language not just by listening, but by\ninteracting with others in their social environment. Conversely, large language\nmodels are typically trained with next-word prediction on massive amounts of\ntext. Motivated by this contrast, we investigate whether language models can be\ntrained with less data by learning not only from next-word prediction but also\nfrom high-level, cognitively inspired feedback. We train a student model to\ngenerate stories, which a teacher model rates on readability, narrative\ncoherence, and creativity. By varying the amount of pretraining before the\nfeedback loop, we assess the impact of this interactive learning on formal and\nfunctional linguistic competence. We find that the high-level feedback is\nhighly data efficient: With just 1 M words of input in interactive learning,\nstorytelling skills can improve as much as with 410 M words of next-word\nprediction.",
        "url": "http://arxiv.org/abs/2509.15714v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15714v1",
        "arxiv_id": "2509.15714v1",
        "authors": [
            "Jonas Mayer Martins",
            "Ali Hamza Bashir",
            "Muhammad Rehan Khalid",
            "Lisa Beinborn"
        ],
        "submitted": "2025-09-19 07:45:34",
        "source": "arxiv",
        "comment": "EMNLP 2025, BabyLM Challenge; 16 pages, 6 figures"
    },
    {
        "title": "Understanding Embedding Scaling in Collaborative Filtering",
        "abstract": "Scaling recommendation models into large recommendation models has become one\nof the most widely discussed topics. Recent efforts focus on components beyond\nthe scaling embedding dimension, as it is believed that scaling embedding may\nlead to performance degradation. Although there have been some initial\nobservations on embedding, the root cause of their non-scalability remains\nunclear. Moreover, whether performance degradation occurs across different\ntypes of models and datasets is still an unexplored area. Regarding the effect\nof embedding dimensions on performance, we conduct large-scale experiments\nacross 10 datasets with varying sparsity levels and scales, using 4\nrepresentative classical architectures. We surprisingly observe two novel\nphenomenon: double-peak and logarithmic. For the former, as the embedding\ndimension increases, performance first improves, then declines, rises again,\nand eventually drops. For the latter, it exhibits a perfect logarithmic curve.\nOur contributions are threefold. First, we discover two novel phenomena when\nscaling collaborative filtering models. Second, we gain an understanding of the\nunderlying causes of the double-peak phenomenon. Lastly, we theoretically\nanalyze the noise robustness of collaborative filtering models, with results\nmatching empirical observations.",
        "url": "http://arxiv.org/abs/2509.15709v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15709v1",
        "arxiv_id": "2509.15709v1",
        "authors": [
            "Zhuangzhuang He",
            "Zhou Kaiyu",
            "Haoyue Bai",
            "Fengbin Zhu",
            "Yonghui Yang"
        ],
        "submitted": "2025-09-19 07:33:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Fine-Tuning Large Multimodal Models for Automatic Pronunciation Assessment",
        "abstract": "Automatic Pronunciation Assessment (APA) is critical for Computer-Assisted\nLanguage Learning (CALL), requiring evaluation across multiple granularities\nand aspects. Large Multimodal Models (LMMs) present new opportunities for APA,\nbut their effectiveness in fine-grained assessment remains uncertain. This work\ninvestigates fine-tuning LMMs for APA using the Speechocean762 dataset and a\nprivate corpus. Fine-tuning significantly outperforms zero-shot settings and\nachieves competitive results on single-granularity tasks compared to public and\ncommercial systems. The model performs well at word and sentence levels, while\nphoneme-level assessment remains challenging. We also observe that the Pearson\nCorrelation Coefficient (PCC) reaches 0.9, whereas Spearman's rank Correlation\nCoefficient (SCC) remains around 0.6, suggesting that SCC better reflects\nordinal consistency. These findings highlight both the promise and limitations\nof LMMs for APA and point to future work on fine-grained modeling and\nrank-aware evaluation.",
        "url": "http://arxiv.org/abs/2509.15701v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15701v1",
        "arxiv_id": "2509.15701v1",
        "authors": [
            "Ke Wang",
            "Wenning Wei",
            "Yan Deng",
            "Lei He",
            "Sheng Zhao"
        ],
        "submitted": "2025-09-19 07:23:25",
        "source": "arxiv",
        "comment": "submitted to ICASSP2026"
    },
    {
        "title": "Direct Simultaneous Translation Activation for Large Audio-Language Models",
        "abstract": "Simultaneous speech-to-text translation (Simul-S2TT) aims to translate speech\ninto target text in real time, outputting translations while receiving source\nspeech input, rather than waiting for the entire utterance to be spoken.\nSimul-S2TT research often modifies model architectures to implement read-write\nstrategies. However, with the rise of large audio-language models (LALMs), a\nkey challenge is how to directly activate Simul-S2TT capabilities in base\nmodels without additional architectural changes. In this paper, we introduce\n{\\bf Simul}taneous {\\bf S}elf-{\\bf A}ugmentation ({\\bf SimulSA}), a strategy\nthat utilizes LALMs' inherent capabilities to obtain simultaneous data by\nrandomly truncating speech and constructing partially aligned translation. By\nincorporating them into offline SFT data, SimulSA effectively bridges the\ndistribution gap between offline translation during pretraining and\nsimultaneous translation during inference. Experimental results demonstrate\nthat augmenting only about {\\bf 1\\%} of the simultaneous data, compared to the\nfull offline SFT data, can significantly activate LALMs' Simul-S2TT\ncapabilities without modifications to model architecture or decoding strategy.",
        "url": "http://arxiv.org/abs/2509.15692v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15692v1",
        "arxiv_id": "2509.15692v1",
        "authors": [
            "Pei Zhang",
            "Yiming Wang",
            "Jialong Tang",
            "Baosong Yang",
            "Rui Wang",
            "Derek F. Wong",
            "Fei Huang"
        ],
        "submitted": "2025-09-19 07:12:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "KITE: Kernelized and Information Theoretic Exemplars for In-Context Learning",
        "abstract": "In-context learning (ICL) has emerged as a powerful paradigm for adapting\nlarge language models (LLMs) to new and data-scarce tasks using only a few\ncarefully selected task-specific examples presented in the prompt. However,\ngiven the limited context size of LLMs, a fundamental question arises: Which\nexamples should be selected to maximize performance on a given user query?\nWhile nearest-neighbor-based methods like KATE have been widely adopted for\nthis purpose, they suffer from well-known drawbacks in high-dimensional\nembedding spaces, including poor generalization and a lack of diversity. In\nthis work, we study this problem of example selection in ICL from a principled,\ninformation theory-driven perspective. We first model an LLM as a linear\nfunction over input embeddings and frame the example selection task as a\nquery-specific optimization problem: selecting a subset of exemplars from a\nlarger example bank that minimizes the prediction error on a specific query.\nThis formulation departs from traditional generalization-focused learning\ntheoretic approaches by targeting accurate prediction for a specific query\ninstance. We derive a principled surrogate objective that is approximately\nsubmodular, enabling the use of a greedy algorithm with an approximation\nguarantee. We further enhance our method by (i) incorporating the kernel trick\nto operate in high-dimensional feature spaces without explicit mappings, and\n(ii) introducing an optimal design-based regularizer to encourage diversity in\nthe selected examples. Empirically, we demonstrate significant improvements\nover standard retrieval methods across a suite of classification tasks,\nhighlighting the benefits of structure-aware, diverse example selection for ICL\nin real-world, label-scarce scenarios.",
        "url": "http://arxiv.org/abs/2509.15676v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15676v1",
        "arxiv_id": "2509.15676v1",
        "authors": [
            "Vaibhav Singh",
            "Soumya Suvra Ghosal",
            "Kapu Nirmal Joshua",
            "Soumyabrata Pal",
            "Sayak Ray Chowdhury"
        ],
        "submitted": "2025-09-19 06:50:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "VOX-KRIKRI: Unifying Speech and Language through Continuous Fusion",
        "abstract": "We present a multimodal fusion framework that bridges pre-trained\ndecoder-based large language models (LLM) and acoustic encoder-decoder\narchitectures such as Whisper, with the aim of building speech-enabled LLMs.\nInstead of directly using audio embeddings, we explore an intermediate\naudio-conditioned text space as a more effective mechanism for alignment. Our\nmethod operates fully in continuous text representation spaces, fusing\nWhisper's hidden decoder states with those of an LLM through cross-modal\nattention, and supports both offline and streaming modes. We introduce\n\\textit{VoxKrikri}, the first Greek speech LLM, and show through analysis that\nour approach effectively aligns representations across modalities. These\nresults highlight continuous space fusion as a promising path for multilingual\nand low-resource speech LLMs, while achieving state-of-the-art results for\nAutomatic Speech Recognition in Greek, providing an average $\\sim20\\%$ relative\nimprovement across benchmarks.",
        "url": "http://arxiv.org/abs/2509.15667v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15667v1",
        "arxiv_id": "2509.15667v1",
        "authors": [
            "Dimitrios Damianos",
            "Leon Voukoutis",
            "Georgios Paraskevopoulos",
            "Vassilis Katsouros"
        ],
        "submitted": "2025-09-19 06:42:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SightSound-R1: Cross-Modal Reasoning Distillation from Vision to Audio Language Models",
        "abstract": "While large audio-language models (LALMs) have demonstrated state-of-the-art\naudio understanding, their reasoning capability in complex soundscapes still\nfalls behind large vision-language models (LVLMs). Compared to the visual\ndomain, one bottleneck is the lack of large-scale chain-of-thought audio data\nto teach LALM stepwise reasoning. To circumvent this data and modality gap, we\npresent SightSound-R1, a cross-modal distillation framework that transfers\nadvanced reasoning from a stronger LVLM teacher to a weaker LALM student on the\nsame audio-visual question answering (AVQA) dataset. SightSound-R1 consists of\nthree core steps: (i) test-time scaling to generate audio-focused chains of\nthought (CoT) from an LVLM teacher, (ii) audio-grounded validation to filter\nhallucinations, and (iii) a distillation pipeline with supervised fine-tuning\n(SFT) followed by Group Relative Policy Optimization (GRPO) for the LALM\nstudent. Results show that SightSound-R1 improves LALM reasoning performance\nboth in the in-domain AVQA test set as well as in unseen auditory scenes and\nquestions, outperforming both pretrained and label-only distilled baselines.\nThus, we conclude that vision reasoning can be effectively transferred to audio\nmodels and scaled with abundant audio-visual data.",
        "url": "http://arxiv.org/abs/2509.15661v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15661v1",
        "arxiv_id": "2509.15661v1",
        "authors": [
            "Qiaolin Wang",
            "Xilin Jiang",
            "Linyang He",
            "Junkai Wu",
            "Nima Mesgarani"
        ],
        "submitted": "2025-09-19 06:39:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Chunk Knowledge Generation Model for Enhanced Information Retrieval: A Multi-task Learning Approach",
        "abstract": "Traditional query expansion techniques for addressing vocabulary mismatch\nproblems in information retrieval are context-sensitive and may lead to\nperformance degradation. As an alternative, document expansion research has\ngained attention, but existing methods such as Doc2Query have limitations\nincluding excessive preprocessing costs, increased index size, and reliability\nissues with generated content. To mitigate these problems and seek more\nstructured and efficient alternatives, this study proposes a method that\ndivides documents into chunk units and generates textual data for each chunk to\nsimultaneously improve retrieval efficiency and accuracy. The proposed \"Chunk\nKnowledge Generation Model\" adopts a T5-based multi-task learning structure\nthat simultaneously generates titles and candidate questions from each document\nchunk while extracting keywords from user queries. This approach maximizes\ncomputational efficiency by generating and extracting three types of semantic\ninformation in parallel through a single encoding and two decoding processes.\nThe generated data is utilized as additional information in the retrieval\nsystem. GPT-based evaluation on 305 query-document pairs showed that retrieval\nusing the proposed model achieved 95.41% accuracy at Top@10, demonstrating\nsuperior performance compared to document chunk-level retrieval. This study\ncontributes by proposing an approach that simultaneously generates titles and\ncandidate questions from document chunks for application in retrieval\npipelines, and provides empirical evidence applicable to large-scale\ninformation retrieval systems by demonstrating improved retrieval accuracy\nthrough qualitative evaluation.",
        "url": "http://arxiv.org/abs/2509.15658v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15658v1",
        "arxiv_id": "2509.15658v1",
        "authors": [
            "Jisu Kim",
            "Jinhee Park",
            "Changhyun Jeon",
            "Jungwoo Choi",
            "Keonwoo Kim",
            "Minji Hong",
            "Sehyun Kim"
        ],
        "submitted": "2025-09-19 06:32:30",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Layer-wise Minimal Pair Probing Reveals Contextual Grammatical-Conceptual Hierarchy in Speech Representations",
        "abstract": "Transformer-based speech language models (SLMs) have significantly improved\nneural speech recognition and understanding. While existing research has\nexamined how well SLMs encode shallow acoustic and phonetic features, the\nextent to which SLMs encode nuanced syntactic and conceptual features remains\nunclear. By drawing parallels with linguistic competence assessments for large\nlanguage models, this study is the first to systematically evaluate the\npresence of contextual syntactic and semantic features across SLMs for\nself-supervised learning (S3M), automatic speech recognition (ASR), speech\ncompression (codec), and as the encoder for auditory large language models\n(AudioLLMs). Through minimal pair designs and diagnostic feature analysis\nacross 71 tasks spanning diverse linguistic levels, our layer-wise and\ntime-resolved analysis uncovers that 1) all speech encode grammatical features\nmore robustly than conceptual ones.",
        "url": "http://arxiv.org/abs/2509.15655v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15655v1",
        "arxiv_id": "2509.15655v1",
        "authors": [
            "Linyang He",
            "Qiaolin Wang",
            "Xilin Jiang",
            "Nima Mesgarani"
        ],
        "submitted": "2025-09-19 06:29:33",
        "source": "arxiv",
        "comment": "EMNLP 2025 Main Conference (Oral)"
    },
    {
        "title": "Multilingual LLM Prompting Strategies for Medical English-Vietnamese Machine Translation",
        "abstract": "Medical English-Vietnamese machine translation (En-Vi MT) is essential for\nhealthcare access and communication in Vietnam, yet Vietnamese remains a\nlow-resource and under-studied language. We systematically evaluate prompting\nstrategies for six multilingual LLMs (0.5B-9B parameters) on the MedEV dataset,\ncomparing zero-shot, few-shot, and dictionary-augmented prompting with Meddict,\nan English-Vietnamese medical lexicon. Results show that model scale is the\nprimary driver of performance: larger LLMs achieve strong zero-shot results,\nwhile few-shot prompting yields only marginal improvements. In contrast,\nterminology-aware cues and embedding-based example retrieval consistently\nimprove domain-specific translation. These findings underscore both the promise\nand the current limitations of multilingual LLMs for medical En-Vi MT.",
        "url": "http://arxiv.org/abs/2509.15640v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15640v1",
        "arxiv_id": "2509.15640v1",
        "authors": [
            "Nhu Vo",
            "Nu-Uyen-Phuong Le",
            "Dung D. Le",
            "Massimo Piccardi",
            "Wray Buntine"
        ],
        "submitted": "2025-09-19 06:06:36",
        "source": "arxiv",
        "comment": "The work is under peer review"
    },
    {
        "title": "Sparse-Autoencoder-Guided Internal Representation Unlearning for Large Language Models",
        "abstract": "As large language models (LLMs) are increasingly deployed across various\napplications, privacy and copyright concerns have heightened the need for more\neffective LLM unlearning techniques. Many existing unlearning methods aim to\nsuppress undesirable outputs through additional training (e.g., gradient\nascent), which reduces the probability of generating such outputs. While such\nsuppression-based approaches can control model outputs, they may not eliminate\nthe underlying knowledge embedded in the model's internal activations; muting a\nresponse is not the same as forgetting it. Moreover, such suppression-based\nmethods often suffer from model collapse. To address these issues, we propose a\nnovel unlearning method that directly intervenes in the model's internal\nactivations. In our formulation, forgetting is defined as a state in which the\nactivation of a forgotten target is indistinguishable from that of ``unknown''\nentities. Our method introduces an unlearning objective that modifies the\nactivation of the target entity away from those of known entities and toward\nthose of unknown entities in a sparse autoencoder latent space. By aligning the\ntarget's internal activation with those of unknown entities, we shift the\nmodel's recognition of the target entity from ``known'' to ``unknown'',\nachieving genuine forgetting while avoiding over-suppression and model\ncollapse. Empirically, we show that our method effectively aligns the internal\nactivations of the forgotten target, a result that the suppression-based\napproaches do not reliably achieve. Additionally, our method effectively\nreduces the model's recall of target knowledge in question-answering tasks\nwithout significant damage to the non-target knowledge.",
        "url": "http://arxiv.org/abs/2509.15631v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15631v1",
        "arxiv_id": "2509.15631v1",
        "authors": [
            "Tomoya Yamashita",
            "Akira Ito",
            "Yuuki Yamanaka",
            "Masanori Yamada",
            "Takayuki Miura",
            "Toshiki Shibahara"
        ],
        "submitted": "2025-09-19 05:48:12",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Concept Unlearning in Large Language Models via Self-Constructed Knowledge Triplets",
        "abstract": "Machine Unlearning (MU) has recently attracted considerable attention as a\nsolution to privacy and copyright issues in large language models (LLMs).\nExisting MU methods aim to remove specific target sentences from an LLM while\nminimizing damage to unrelated knowledge. However, these approaches require\nexplicit target sentences and do not support removing broader concepts, such as\npersons or events. To address this limitation, we introduce Concept Unlearning\n(CU) as a new requirement for LLM unlearning. We leverage knowledge graphs to\nrepresent the LLM's internal knowledge and define CU as removing the forgetting\ntarget nodes and associated edges. This graph-based formulation enables a more\nintuitive unlearning and facilitates the design of more effective methods. We\npropose a novel method that prompts the LLM to generate knowledge triplets and\nexplanatory sentences about the forgetting target and applies the unlearning\nprocess to these representations. Our approach enables more precise and\ncomprehensive concept removal by aligning the unlearning process with the LLM's\ninternal knowledge representations. Experiments on real-world and synthetic\ndatasets demonstrate that our method effectively achieves concept-level\nunlearning while preserving unrelated knowledge.",
        "url": "http://arxiv.org/abs/2509.15621v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15621v1",
        "arxiv_id": "2509.15621v1",
        "authors": [
            "Tomoya Yamashita",
            "Yuuki Yamanaka",
            "Masanori Yamada",
            "Takayuki Miura",
            "Toshiki Shibahara",
            "Tomoharu Iwata"
        ],
        "submitted": "2025-09-19 05:34:45",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SciEvent: Benchmarking Multi-domain Scientific Event Extraction",
        "abstract": "Scientific information extraction (SciIE) has primarily relied on\nentity-relation extraction in narrow domains, limiting its applicability to\ninterdisciplinary research and struggling to capture the necessary context of\nscientific information, often resulting in fragmented or conflicting\nstatements. In this paper, we introduce SciEvent, a novel multi-domain\nbenchmark of scientific abstracts annotated via a unified event extraction (EE)\nschema designed to enable structured and context-aware understanding of\nscientific content. It includes 500 abstracts across five research domains,\nwith manual annotations of event segments, triggers, and fine-grained\narguments. We define SciIE as a multi-stage EE pipeline: (1) segmenting\nabstracts into core scientific activities--Background, Method, Result, and\nConclusion; and (2) extracting the corresponding triggers and arguments.\nExperiments with fine-tuned EE models, large language models (LLMs), and human\nannotators reveal a performance gap, with current models struggling in domains\nsuch as sociology and humanities. SciEvent serves as a challenging benchmark\nand a step toward generalizable, multi-domain SciIE.",
        "url": "http://arxiv.org/abs/2509.15620v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15620v1",
        "arxiv_id": "2509.15620v1",
        "authors": [
            "Bofu Dong",
            "Pritesh Shah",
            "Sumedh Sonawane",
            "Tiyasha Banerjee",
            "Erin Brady",
            "Xinya Du",
            "Ming Jiang"
        ],
        "submitted": "2025-09-19 05:32:50",
        "source": "arxiv",
        "comment": "9 pages, 8 figures (main); 22 pages, 11 figures (appendix). Accepted\n  to EMNLP 2025 (Main Conference)"
    },
    {
        "title": "CFDA & CLIP at TREC iKAT 2025: Enhancing Personalized Conversational Search via Query Reformulation and Rank Fusion",
        "abstract": "The 2025 TREC Interactive Knowledge Assistance Track (iKAT) featured both\ninteractive and offline submission tasks. The former requires systems to\noperate under real-time constraints, making robustness and efficiency as\nimportant as accuracy, while the latter enables controlled evaluation of\npassage ranking and response generation with pre-defined datasets. To address\nthis, we explored query rewriting and retrieval fusion as core strategies. We\nbuilt our pipelines around Best-of-$N$ selection and Reciprocal Rank Fusion\n(RRF) strategies to handle different submission tasks. Results show that\nreranking and fusion improve robustness while revealing trade-offs between\neffectiveness and efficiency across both tasks.",
        "url": "http://arxiv.org/abs/2509.15588v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15588v1",
        "arxiv_id": "2509.15588v1",
        "authors": [
            "Yu-Cheng Chang",
            "Guan-Wei Yeo",
            "Quah Eugene",
            "Fan-Jie Shih",
            "Yuan-Ching Kuo",
            "Tsung-En Yu",
            "Hung-Chun Hsu",
            "Ming-Feng Tsai",
            "Chuan-Ju Wang"
        ],
        "submitted": "2025-09-19 04:42:31",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DivLogicEval: A Framework for Benchmarking Logical Reasoning Evaluation in Large Language Models",
        "abstract": "Logic reasoning in natural language has been recognized as an important\nmeasure of human intelligence for Large Language Models (LLMs). Popular\nbenchmarks may entangle multiple reasoning skills and thus provide unfaithful\nevaluations on the logic reasoning skill. Meanwhile, existing logic reasoning\nbenchmarks are limited in language diversity and their distributions are\ndeviated from the distribution of an ideal logic reasoning benchmark, which may\nlead to biased evaluation results. This paper thereby proposes a new classical\nlogic benchmark DivLogicEval, consisting of natural sentences composed of\ndiverse statements in a counterintuitive way. To ensure a more reliable\nevaluation, we also introduce a new evaluation metric that mitigates the\ninfluence of bias and randomness inherent in LLMs. Through experiments, we\ndemonstrate the extent to which logical reasoning is required to answer the\nquestions in DivLogicEval and compare the performance of different popular LLMs\nin conducting logical reasoning.",
        "url": "http://arxiv.org/abs/2509.15587v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15587v1",
        "arxiv_id": "2509.15587v1",
        "authors": [
            "Tsz Ting Chung",
            "Lemao Liu",
            "Mo Yu",
            "Dit-Yan Yeung"
        ],
        "submitted": "2025-09-19 04:40:46",
        "source": "arxiv",
        "comment": "Accepted by EMNLP 2025. Project Page:\n  https://ttchungc.github.io/projects/divlogiceval/"
    },
    {
        "title": "Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization",
        "abstract": "Low latency speech human-machine communication is becoming increasingly\nnecessary as speech technology advances quickly in the last decade. One of the\nprimary factors behind the advancement of speech technology is self-supervised\nlearning. Most self-supervised learning algorithms are designed with full\nutterance assumption and compromises have to made if partial utterances are\npresented, which are common in the streaming applications. In this work, we\npropose a chunk based self-supervised learning (Chunk SSL) algorithm as an\nunified solution for both streaming and offline speech pre-training. Chunk SSL\nis optimized with the masked prediction loss and an acoustic encoder is\nencouraged to restore indices of those masked speech frames with help from\nunmasked frames in the same chunk and preceding chunks. A copy and append data\naugmentation approach is proposed to conduct efficient chunk based\npre-training. Chunk SSL utilizes a finite scalar quantization (FSQ) module to\ndiscretize input speech features and our study shows a high resolution FSQ\ncodebook, i.e., a codebook with vocabulary size up to a few millions, is\nbeneficial to transfer knowledge from the pre-training task to the downstream\ntasks. A group masked prediction loss is employed during pre-training to\nalleviate the high memory and computation cost introduced by the large\ncodebook. The proposed approach is examined in two speech to text tasks, i.e.,\nspeech recognition and speech translation. Experimental results on the\n\\textsc{Librispeech} and \\textsc{Must-C} datasets show that the proposed method\ncould achieve very competitive results for speech to text tasks at both\nstreaming and offline modes.",
        "url": "http://arxiv.org/abs/2509.15579v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15579v1",
        "arxiv_id": "2509.15579v1",
        "authors": [
            "Yun Tang",
            "Cindy Tseng"
        ],
        "submitted": "2025-09-19 04:29:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Relevance to Utility: Process-Supervised Rewrite for RAG",
        "abstract": "Retrieval-Augmented Generation systems often suffer from a gap between\noptimizing retrieval relevance and generative utility: retrieved documents may\nbe topically relevant but still lack the content needed for effective reasoning\nduring generation. While existing \"bridge\" modules attempt to rewrite the\nretrieved text for better generation, we show how they fail to capture true\ndocument utility. In this work, we propose R2U, with a key distinction of\ndirectly optimizing to maximize the probability of generating a correct answer\nthrough process supervision. As such direct observation is expensive, we also\npropose approximating an efficient distillation pipeline by scaling the\nsupervision from LLMs, which helps the smaller rewriter model generalize\nbetter. We evaluate our method across multiple open-domain question-answering\nbenchmarks. The empirical results demonstrate consistent improvements over\nstrong bridging baselines.",
        "url": "http://arxiv.org/abs/2509.15577v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15577v1",
        "arxiv_id": "2509.15577v1",
        "authors": [
            "Jaeyoung Kim",
            "Jongho Kim",
            "Seung-won Hwang",
            "Seoho Song",
            "Young-In Song"
        ],
        "submitted": "2025-09-19 04:24:57",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LiteLong: Resource-Efficient Long-Context Data Synthesis for LLMs",
        "abstract": "High-quality long-context data is essential for training large language\nmodels (LLMs) capable of processing extensive documents, yet existing synthesis\napproaches using relevance-based aggregation face challenges of computational\nefficiency. We present LiteLong, a resource-efficient method for synthesizing\nlong-context data through structured topic organization and multi-agent debate.\nOur approach leverages the BISAC book classification system to provide a\ncomprehensive hierarchical topic organization, and then employs a debate\nmechanism with multiple LLMs to generate diverse, high-quality topics within\nthis structure. For each topic, we use lightweight BM25 retrieval to obtain\nrelevant documents and concatenate them into 128K-token training samples.\nExperiments on HELMET and Ruler benchmarks demonstrate that LiteLong achieves\ncompetitive long-context performance and can seamlessly integrate with other\nlong-dependency enhancement methods. LiteLong makes high-quality long-context\ndata synthesis more accessible by reducing both computational and data\nengineering costs, facilitating further research in long-context language\ntraining.",
        "url": "http://arxiv.org/abs/2509.15568v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15568v1",
        "arxiv_id": "2509.15568v1",
        "authors": [
            "Junlong Jia",
            "Xing Wu",
            "Chaochen Gao",
            "Ziyang Chen",
            "Zijia Lin",
            "Zhongzhi Li",
            "Weinong Wang",
            "Haotian Xu",
            "Donghui Jin",
            "Debing Zhang",
            "Binghui Guo"
        ],
        "submitted": "2025-09-19 04:07:46",
        "source": "arxiv",
        "comment": "work in progress"
    },
    {
        "title": "Small LLMs with Expert Blocks Are Good Enough for Hyperparamter Tuning",
        "abstract": "Hyper-parameter Tuning (HPT) is a necessary step in machine learning (ML)\npipelines but becomes computationally expensive and opaque with larger models.\nRecently, Large Language Models (LLMs) have been explored for HPT, yet most\nrely on models exceeding 100 billion parameters. We propose an Expert Block\nFramework for HPT using Small LLMs. At its core is the Trajectory Context\nSummarizer (TCS), a deterministic block that transforms raw training\ntrajectories into structured context, enabling small LLMs to analyze\noptimization progress with reliability comparable to larger models. Using two\nlocally-run LLMs (phi4:reasoning14B and qwen2.5-coder:32B) and a 10-trial\nbudget, our TCS-enabled HPT pipeline achieves average performance within ~0.9\npercentage points of GPT-4 across six diverse tasks.",
        "url": "http://arxiv.org/abs/2509.15561v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15561v1",
        "arxiv_id": "2509.15561v1",
        "authors": [
            "Om Naphade",
            "Saksham Bansal",
            "Parikshit Pareek"
        ],
        "submitted": "2025-09-19 03:46:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "How important is language for human-like intelligence?",
        "abstract": "We use language to communicate our thoughts. But is language merely the\nexpression of thoughts, which are themselves produced by other, nonlinguistic\nparts of our minds? Or does language play a more transformative role in human\ncognition, allowing us to have thoughts that we otherwise could (or would) not\nhave? Recent developments in artificial intelligence (AI) and cognitive science\nhave reinvigorated this old question. We argue that language may hold the key\nto the emergence of both more general AI systems and central aspects of human\nintelligence. We highlight two related properties of language that make it such\na powerful tool for developing domain--general abilities. First, language\noffers compact representations that make it easier to represent and reason\nabout many abstract concepts (e.g., exact numerosity). Second, these compressed\nrepresentations are the iterated output of collective minds. In learning a\nlanguage, we learn a treasure trove of culturally evolved abstractions. Taken\ntogether, these properties mean that a sufficiently powerful learning system\nexposed to language--whether biological or artificial--learns a compressed\nmodel of the world, reverse engineering many of the conceptual and causal\nstructures that support human (and human-like) thought.",
        "url": "http://arxiv.org/abs/2509.15560v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15560v1",
        "arxiv_id": "2509.15560v1",
        "authors": [
            "Gary Lupyan",
            "Hunter Gentry",
            "Martin Zettersten"
        ],
        "submitted": "2025-09-19 03:45:44",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Exploring Polyglot Harmony: On Multilingual Data Allocation for Large Language Models Pretraining",
        "abstract": "Large language models (LLMs) have become integral to a wide range of\napplications worldwide, driving an unprecedented global demand for effective\nmultilingual capabilities. Central to achieving robust multilingual performance\nis the strategic allocation of language proportions within training corpora.\nHowever, determining optimal language ratios is highly challenging due to\nintricate cross-lingual interactions and sensitivity to dataset scale. This\npaper introduces Climb (Cross-Lingual Interaction-aware Multilingual\nBalancing), a novel framework designed to systematically optimize multilingual\ndata allocation. At its core, Climb introduces a cross-lingual\ninteraction-aware language ratio, explicitly quantifying each language's\neffective allocation by capturing inter-language dependencies. Leveraging this\nratio, Climb proposes a principled two-step optimization procedure--first\nequalizing marginal benefits across languages, then maximizing the magnitude of\nthe resulting language allocation vectors--significantly simplifying the\ninherently complex multilingual optimization problem. Extensive experiments\nconfirm that Climb can accurately measure cross-lingual interactions across\nvarious multilingual settings. LLMs trained with Climb-derived proportions\nconsistently achieve state-of-the-art multilingual performance, even achieving\ncompetitive performance with open-sourced LLMs trained with more tokens.",
        "url": "http://arxiv.org/abs/2509.15556v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15556v1",
        "arxiv_id": "2509.15556v1",
        "authors": [
            "Ping Guo",
            "Yubing Ren",
            "Binbin Liu",
            "Fengze Liu",
            "Haobin Lin",
            "Yifan Zhang",
            "Bingni Zhang",
            "Taifeng Wang",
            "Yin Zheng"
        ],
        "submitted": "2025-09-19 03:34:34",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "DNA-DetectLLM: Unveiling AI-Generated Text via a DNA-Inspired Mutation-Repair Paradigm",
        "abstract": "The rapid advancement of large language models (LLMs) has blurred the line\nbetween AI-generated and human-written text. This progress brings societal\nrisks such as misinformation, authorship ambiguity, and intellectual property\nconcerns, highlighting the urgent need for reliable AI-generated text detection\nmethods. However, recent advances in generative language modeling have resulted\nin significant overlap between the feature distributions of human-written and\nAI-generated text, blurring classification boundaries and making accurate\ndetection increasingly challenging. To address the above challenges, we propose\na DNA-inspired perspective, leveraging a repair-based process to directly and\ninterpretably capture the intrinsic differences between human-written and\nAI-generated text. Building on this perspective, we introduce DNA-DetectLLM, a\nzero-shot detection method for distinguishing AI-generated and human-written\ntext. The method constructs an ideal AI-generated sequence for each input,\niteratively repairs non-optimal tokens, and quantifies the cumulative repair\neffort as an interpretable detection signal. Empirical evaluations demonstrate\nthat our method achieves state-of-the-art detection performance and exhibits\nstrong robustness against various adversarial attacks and input lengths.\nSpecifically, DNA-DetectLLM achieves relative improvements of 5.55% in AUROC\nand 2.08% in F1 score across multiple public benchmark datasets.",
        "url": "http://arxiv.org/abs/2509.15550v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15550v1",
        "arxiv_id": "2509.15550v1",
        "authors": [
            "Xiaowei Zhu",
            "Yubing Ren",
            "Fang Fang",
            "Qingfeng Tan",
            "Shi Wang",
            "Yanan Cao"
        ],
        "submitted": "2025-09-19 03:08:13",
        "source": "arxiv",
        "comment": "NeurIPS 2025 Spotlight"
    },
    {
        "title": "A method for improving multilingual quality and diversity of instruction fine-tuning datasets",
        "abstract": "Multilingual Instruction Fine-Tuning (IFT) is essential for enabling large\nlanguage models (LLMs) to generalize effectively across diverse linguistic and\ncultural contexts. However, the scarcity of high-quality multilingual training\ndata and corresponding building method remains a critical bottleneck. While\ndata selection has shown promise in English settings, existing methods often\nfail to generalize across languages due to reliance on simplistic heuristics or\nlanguage-specific assumptions. In this work, we introduce Multilingual Data\nQuality and Diversity (M-DaQ), a novel method for improving LLMs\nmultilinguality, by selecting high-quality and semantically diverse\nmultilingual IFT samples. We further conduct the first systematic investigation\nof the Superficial Alignment Hypothesis (SAH) in multilingual setting.\nEmpirical results across 18 languages demonstrate that models fine-tuned with\nM-DaQ method achieve significant performance gains over vanilla baselines over\n60% win rate. Human evaluations further validate these gains, highlighting the\nincrement of cultural points in the response. We release the M-DaQ code to\nsupport future research.",
        "url": "http://arxiv.org/abs/2509.15549v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15549v1",
        "arxiv_id": "2509.15549v1",
        "authors": [
            "Chunguang Zhao",
            "Yilun Liu",
            "Pufan Zeng",
            "Yuanchang Luo",
            "Shimin Tao",
            "Minggui He",
            "Weibin Meng",
            "Song Xu",
            "Ziang Chen",
            "Chen Liu",
            "Hongxia Ma",
            "Li Zhang",
            "Boxing Chen",
            "Daimeng Wei"
        ],
        "submitted": "2025-09-19 03:07:59",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond Words: Enhancing Desire, Emotion, and Sentiment Recognition with Non-Verbal Cues",
        "abstract": "Desire, as an intention that drives human behavior, is closely related to\nboth emotion and sentiment. Multimodal learning has advanced sentiment and\nemotion recognition, but multimodal approaches specially targeting human desire\nunderstanding remain underexplored. And existing methods in sentiment analysis\npredominantly emphasize verbal cues and overlook images as complementary\nnon-verbal cues. To address these gaps, we propose a Symmetrical Bidirectional\nMultimodal Learning Framework for Desire, Emotion, and Sentiment Recognition,\nwhich enforces mutual guidance between text and image modalities to effectively\ncapture intention-related representations in the image. Specifically,\nlow-resolution images are used to obtain global visual representations for\ncross-modal alignment, while high resolution images are partitioned into\nsub-images and modeled with masked image modeling to enhance the ability to\ncapture fine-grained local features. A text-guided image decoder and an\nimage-guided text decoder are introduced to facilitate deep cross-modal\ninteraction at both local and global representations of image information.\nAdditionally, to balance perceptual gains with computation cost, a mixed-scale\nimage strategy is adopted, where high-resolution images are cropped into\nsub-images for masked modeling. The proposed approach is evaluated on MSED, a\nmultimodal dataset that includes a desire understanding benchmark, as well as\nemotion and sentiment recognition. Experimental results indicate consistent\nimprovements over other state-of-the-art methods, validating the effectiveness\nof our proposed method. Specifically, our method outperforms existing\napproaches, achieving F1-score improvements of 1.1% in desire understanding,\n0.6% in emotion recognition, and 0.9% in sentiment analysis. Our code is\navailable at: https://github.com/especiallyW/SyDES.",
        "url": "http://arxiv.org/abs/2509.15540v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15540v1",
        "arxiv_id": "2509.15540v1",
        "authors": [
            "Wei Chen",
            "Tongguan Wang",
            "Feiyue Xue",
            "Junkai Li",
            "Hui Liu",
            "Ying Sha"
        ],
        "submitted": "2025-09-19 02:49:47",
        "source": "arxiv",
        "comment": "13 page, 5 figures, uploaded by Wei Chen"
    },
    {
        "title": "How do Language Models Generate Slang: A Systematic Comparison between Human and Machine-Generated Slang Usages",
        "abstract": "Slang is a commonly used type of informal language that poses a daunting\nchallenge to NLP systems. Recent advances in large language models (LLMs),\nhowever, have made the problem more approachable. While LLM agents are becoming\nmore widely applied to intermediary tasks such as slang detection and slang\ninterpretation, their generalizability and reliability are heavily dependent on\nwhether these models have captured structural knowledge about slang that align\nwell with human attested slang usages. To answer this question, we contribute a\nsystematic comparison between human and machine-generated slang usages. Our\nevaluative framework focuses on three core aspects: 1) Characteristics of the\nusages that reflect systematic biases in how machines perceive slang, 2)\nCreativity reflected by both lexical coinages and word reuses employed by the\nslang usages, and 3) Informativeness of the slang usages when used as\ngold-standard examples for model distillation. By comparing human-attested\nslang usages from the Online Slang Dictionary (OSD) and slang generated by\nGPT-4o and Llama-3, we find significant biases in how LLMs perceive slang. Our\nresults suggest that while LLMs have captured significant knowledge about the\ncreative aspects of slang, such knowledge does not align with humans\nsufficiently to enable LLMs for extrapolative tasks such as linguistic\nanalyses.",
        "url": "http://arxiv.org/abs/2509.15518v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15518v1",
        "arxiv_id": "2509.15518v1",
        "authors": [
            "Siyang Wu",
            "Zhewei Sun"
        ],
        "submitted": "2025-09-19 01:49:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LLM Cache Bandit Revisited: Addressing Query Heterogeneity for Cost-Effective LLM Inference",
        "abstract": "This paper revisits the LLM cache bandit problem, with a special focus on\naddressing the query heterogeneity for cost-effective LLM inference. Previous\nworks often assume uniform query sizes. Heterogeneous query sizes introduce a\ncombinatorial structure for cache selection, making the cache replacement\nprocess more computationally and statistically challenging. We treat optimal\ncache selection as a knapsack problem and employ an accumulation-based strategy\nto effectively balance computational overhead and cache updates. In theoretical\nanalysis, we prove that the regret of our algorithm achieves an $O(\\sqrt{MNT})$\nbound, improving the coefficient of $\\sqrt{MN}$ compared to the $O(MN\\sqrt{T})$\nresult in Berkeley, where $N$ is the total number of queries and $M$ is the\ncache size. Additionally, we also provide a problem-dependent bound, which was\nabsent in previous works. The experiment rely on real-world data show that our\nalgorithm reduces the total cost by approximately 12\\%.",
        "url": "http://arxiv.org/abs/2509.15515v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15515v1",
        "arxiv_id": "2509.15515v1",
        "authors": [
            "Hantao Yang",
            "Hong Xie",
            "Defu Lian",
            "Enhong Chen"
        ],
        "submitted": "2025-09-19 01:39:08",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "mucAI at BAREC Shared Task 2025: Towards Uncertainty Aware Arabic Readability Assessment",
        "abstract": "We present a simple, model-agnostic post-processing technique for\nfine-grained Arabic readability classification in the BAREC 2025 Shared Task\n(19 ordinal levels). Our method applies conformal prediction to generate\nprediction sets with coverage guarantees, then computes weighted averages using\nsoftmax-renormalized probabilities over the conformal sets. This\nuncertainty-aware decoding improves Quadratic Weighted Kappa (QWK) by reducing\nhigh-penalty misclassifications to nearer levels. Our approach shows consistent\nQWK improvements of 1-3 points across different base models. In the strict\ntrack, our submission achieves QWK scores of 84.9\\%(test) and 85.7\\% (blind\ntest) for sentence level, and 73.3\\% for document level. For Arabic educational\nassessment, this enables human reviewers to focus on a handful of plausible\nlevels, combining statistical guarantees with practical usability.",
        "url": "http://arxiv.org/abs/2509.15485v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15485v1",
        "arxiv_id": "2509.15485v1",
        "authors": [
            "Ahmed Abdou"
        ],
        "submitted": "2025-09-18 23:14:51",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Red Teaming Multimodal Language Models: Evaluating Harm Across Prompt Modalities and Models",
        "abstract": "Multimodal large language models (MLLMs) are increasingly used in real world\napplications, yet their safety under adversarial conditions remains\nunderexplored. This study evaluates the harmlessness of four leading MLLMs\n(GPT-4o, Claude Sonnet 3.5, Pixtral 12B, and Qwen VL Plus) when exposed to\nadversarial prompts across text-only and multimodal formats. A team of 26 red\nteamers generated 726 prompts targeting three harm categories: illegal\nactivity, disinformation, and unethical behaviour. These prompts were submitted\nto each model, and 17 annotators rated 2,904 model outputs for harmfulness\nusing a 5-point scale. Results show significant differences in vulnerability\nacross models and modalities. Pixtral 12B exhibited the highest rate of harmful\nresponses (~62%), while Claude Sonnet 3.5 was the most resistant (~10%).\nContrary to expectations, text-only prompts were slightly more effective at\nbypassing safety mechanisms than multimodal ones. Statistical analysis\nconfirmed that both model type and input modality were significant predictors\nof harmfulness. These findings underscore the urgent need for robust,\nmultimodal safety benchmarks as MLLMs are deployed more widely.",
        "url": "http://arxiv.org/abs/2509.15478v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15478v1",
        "arxiv_id": "2509.15478v1",
        "authors": [
            "Madison Van Doren",
            "Casey Ford",
            "Emily Dix"
        ],
        "submitted": "2025-09-18 22:51:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Evaluating Multimodal Large Language Models on Spoken Sarcasm Understanding",
        "abstract": "Sarcasm detection remains a challenge in natural language understanding, as\nsarcastic intent often relies on subtle cross-modal cues spanning text, speech,\nand vision. While prior work has primarily focused on textual or visual-textual\nsarcasm, comprehensive audio-visual-textual sarcasm understanding remains\nunderexplored. In this paper, we systematically evaluate large language models\n(LLMs) and multimodal LLMs for sarcasm detection on English (MUStARD++) and\nChinese (MCSD 1.0) in zero-shot, few-shot, and LoRA fine-tuning settings. In\naddition to direct classification, we explore models as feature encoders,\nintegrating their representations through a collaborative gating fusion module.\nExperimental results show that audio-based models achieve the strongest\nunimodal performance, while text-audio and audio-vision combinations outperform\nunimodal and trimodal models. Furthermore, MLLMs such as Qwen-Omni show\ncompetitive zero-shot and fine-tuned performance. Our findings highlight the\npotential of MLLMs for cross-lingual, audio-visual-textual sarcasm\nunderstanding.",
        "url": "http://arxiv.org/abs/2509.15476v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15476v1",
        "arxiv_id": "2509.15476v1",
        "authors": [
            "Zhu Li",
            "Xiyuan Gao",
            "Yuqing Zhang",
            "Shekhar Nayak",
            "Matt Coler"
        ],
        "submitted": "2025-09-18 22:44:27",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Breathing and Semantic Pause Detection and Exertion-Level Classification in Post-Exercise Speech",
        "abstract": "Post-exercise speech contains rich physiological and linguistic cues, often\nmarked by semantic pauses, breathing pauses, and combined breathing-semantic\npauses. Detecting these events enables assessment of recovery rate, lung\nfunction, and exertion-related abnormalities. However, existing works on\nidentifying and distinguishing different types of pauses in this context are\nlimited. In this work, building on a recently released dataset with\nsynchronized audio and respiration signals, we provide systematic annotations\nof pause types. Using these annotations, we systematically conduct exploratory\nbreathing and semantic pause detection and exertion-level classification across\ndeep learning models (GRU, 1D CNN-LSTM, AlexNet, VGG16), acoustic features\n(MFCC, MFB), and layer-stratified Wav2Vec2 representations. We evaluate three\nsetups-single feature, feature fusion, and a two-stage detection-classification\ncascade-under both classification and regression formulations. Results show\nper-type detection accuracy up to 89$\\%$ for semantic, 55$\\%$ for breathing,\n86$\\%$ for combined pauses, and 73$\\%$overall, while exertion-level\nclassification achieves 90.5$\\%$ accuracy, outperformin prior work.",
        "url": "http://arxiv.org/abs/2509.15473v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15473v1",
        "arxiv_id": "2509.15473v1",
        "authors": [
            "Yuyu Wang",
            "Wuyue Xia",
            "Huaxiu Yao",
            "Jingping Nie"
        ],
        "submitted": "2025-09-18 22:39:34",
        "source": "arxiv",
        "comment": "6 pages, 3rd ACM International Workshop on Intelligent Acoustic\n  Systems and Applications (IASA 25)"
    },
    {
        "title": "PILOT: Steering Synthetic Data Generation with Psychological & Linguistic Output Targeting",
        "abstract": "Generative AI applications commonly leverage user personas as a steering\nmechanism for synthetic data generation, but reliance on natural language\nrepresentations forces models to make unintended inferences about which\nattributes to emphasize, limiting precise control over outputs. We introduce\nPILOT (Psychological and Linguistic Output Targeting), a two-phase framework\nfor steering large language models with structured psycholinguistic profiles.\nIn Phase 1, PILOT translates natural language persona descriptions into\nmultidimensional profiles with normalized scores across linguistic and\npsychological dimensions. In Phase 2, these profiles guide generation along\nmeasurable axes of variation. We evaluate PILOT across three state-of-the-art\nLLMs (Mistral Large 2, Deepseek-R1, LLaMA 3.3 70B) using 25 synthetic personas\nunder three conditions: Natural-language Persona Steering (NPS), Schema-Based\nSteering (SBS), and Hybrid Persona-Schema Steering (HPS). Results demonstrate\nthat schema-based approaches significantly reduce artificial-sounding persona\nrepetition while improving output coherence, with silhouette scores increasing\nfrom 0.098 to 0.237 and topic purity from 0.773 to 0.957. Our analysis reveals\na fundamental trade-off: SBS produces more concise outputs with higher topical\nconsistency, while NPS offers greater lexical diversity but reduced\npredictability. HPS achieves a balance between these extremes, maintaining\noutput variety while preserving structural consistency. Expert linguistic\nevaluation confirms that PILOT maintains high response quality across all\nconditions, with no statistically significant differences between steering\napproaches.",
        "url": "http://arxiv.org/abs/2509.15447v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15447v1",
        "arxiv_id": "2509.15447v1",
        "authors": [
            "Caitlin Cisar",
            "Emily Sheffield",
            "Joshua Drake",
            "Alden Harrell",
            "Subramanian Chidambaram",
            "Nikita Nangia",
            "Vinayak Arannil",
            "Alex Williams"
        ],
        "submitted": "2025-09-18 21:43:28",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Dual-Mode Visual System for Brain-Computer Interfaces: Integrating SSVEP and P300 Responses",
        "abstract": "In brain-computer interface (BCI) systems, steady-state visual evoked\npotentials (SSVEP) and P300 responses have achieved widespread implementation\nowing to their superior information transfer rates (ITR) and minimal training\nrequirements. These neurophysiological signals have exhibited robust efficacy\nand versatility in external device control, demonstrating enhanced precision\nand scalability. However, conventional implementations predominantly utilise\nliquid crystal display (LCD)-based visual stimulation paradigms, which present\nlimitations in practical deployment scenarios. This investigation presents the\ndevelopment and evaluation of a novel light-emitting diode (LED)-based dual\nstimulation apparatus designed to enhance SSVEP classification accuracy through\nthe integration of both SSVEP and P300 paradigms. The system employs four\ndistinct frequencies, 7 Hz, 8 Hz, 9 Hz, and 10 Hz, corresponding to forward,\nbackward, right, and left directional controls, respectively. Oscilloscopic\nverification confirmed the precision of these stimulation frequencies.\nReal-time feature extraction was accomplished through the concurrent analysis\nof maximum Fast Fourier Transform (FFT) amplitude and P300 peak detection to\nascertain user intent. Directional control was determined by the frequency\nexhibiting maximal amplitude characteristics. The visual stimulation hardware\ndemonstrated minimal frequency deviation, with error differentials ranging from\n0.15%to 0.20%across all frequencies. The implemented signal processing\nalgorithm successfully discriminated all four stimulus frequencies whilst\ncorrelating them with their respective P300 event markers. Classification\naccuracy was evaluated based on correct task intention recognition. The\nproposed hybrid system achieved a mean classification accuracy of 86.25%,\ncoupled with an average ITR of 42.08 bits per minute (bpm).",
        "url": "http://arxiv.org/abs/2509.15439v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15439v1",
        "arxiv_id": "2509.15439v1",
        "authors": [
            "Ekgari Kasawala",
            "Surej Mouli"
        ],
        "submitted": "2025-09-18 21:25:18",
        "source": "arxiv",
        "comment": "15 Pages"
    },
    {
        "title": "SERVAL: Surprisingly Effective Zero-Shot Visual Document Retrieval Powered by Large Vision and Language Models",
        "abstract": "Visual Document Retrieval (VDR) typically operates as text-to-image retrieval\nusing specialized bi-encoders trained to directly embed document images. We\nrevisit a zero-shot generate-and-encode pipeline: a vision-language model first\nproduces a detailed textual description of each document image, which is then\nembedded by a standard text encoder. On the ViDoRe-v2 benchmark, the method\nreaches 63.4% nDCG@5, surpassing the strongest specialised multi-vector visual\ndocument encoder. It also scales better to large collections and offers broader\nmultilingual coverage. Analysis shows that modern vision-language models\ncapture complex textual and visual cues with sufficient granularity to act as a\nreusable semantic proxy. By offloading modality alignment to pretrained\nvision-language models, our approach removes the need for computationally\nintensive text-image contrastive training and establishes a strong zero-shot\nbaseline for future VDR systems.",
        "url": "http://arxiv.org/abs/2509.15432v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15432v1",
        "arxiv_id": "2509.15432v1",
        "authors": [
            "Thong Nguyen",
            "Yibin Lei",
            "Jia-Huei Ju",
            "Andrew Yates"
        ],
        "submitted": "2025-09-18 21:11:13",
        "source": "arxiv",
        "comment": "Accepted"
    },
    {
        "title": "BiRQ: Bi-Level Self-Labeling Random Quantization for Self-Supervised Speech Recognition",
        "abstract": "Speech is a rich signal, and labeled audio-text pairs are costly, making\nself-supervised learning essential for scalable representation learning. A core\nchallenge in speech SSL is generating pseudo-labels that are both informative\nand efficient: strong labels, such as those used in HuBERT, improve downstream\nperformance but rely on external encoders and multi-stage pipelines, while\nefficient methods like BEST-RQ achieve simplicity at the cost of weaker labels.\nWe propose BiRQ, a bilevel SSL framework that combines the efficiency of\nBEST-RQ with the refinement benefits of HuBERT-style label enhancement. The key\nidea is to reuse part of the model itself as a pseudo-label generator:\nintermediate representations are discretized by a random-projection quantizer\nto produce enhanced labels, while anchoring labels derived directly from the\nraw input stabilize training and prevent collapse. Training is formulated as an\nefficient first-order bilevel optimization problem, solved end-to-end with\ndifferentiable Gumbel-softmax selection. This design eliminates the need for\nexternal label encoders, reduces memory cost, and enables iterative label\nrefinement in an end-to-end fashion. BiRQ consistently improves over BEST-RQ\nwhile maintaining low complexity and computational efficiency. We validate our\nmethod on various datasets, including 960-hour LibriSpeech, 150-hour AMI\nmeetings and 5,000-hour YODAS, demonstrating consistent gains over BEST-RQ.",
        "url": "http://arxiv.org/abs/2509.15430v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15430v1",
        "arxiv_id": "2509.15430v1",
        "authors": [
            "Liuyuan Jiang",
            "Xiaodong Cui",
            "Brian Kingsbury",
            "Tianyi Chen",
            "Lisha Chen"
        ],
        "submitted": "2025-09-18 21:09:29",
        "source": "arxiv",
        "comment": "5 pages including reference"
    },
    {
        "title": "Deep learning and abstractive summarisation for radiological reports: an empirical study for adapting the PEGASUS models' family with scarce data",
        "abstract": "Regardless of the rapid development of artificial intelligence, abstractive\nsummarisation is still challenging for sensitive and data-restrictive domains\nlike medicine. With the increasing number of imaging, the relevance of\nautomated tools for complex medical text summarisation is expected to become\nhighly relevant. In this paper, we investigated the adaptation via fine-tuning\nprocess of a non-domain-specific abstractive summarisation encoder-decoder\nmodel family, and gave insights to practitioners on how to avoid over- and\nunderfitting. We used PEGASUS and PEGASUS-X, on a medium-sized radiological\nreports public dataset. For each model, we comprehensively evaluated two\ndifferent checkpoints with varying sizes of the same training data. We\nmonitored the models' performances with lexical and semantic metrics during the\ntraining history on the fixed-size validation set. PEGASUS exhibited different\nphases, which can be related to epoch-wise double-descent, or\npeak-drop-recovery behaviour. For PEGASUS-X, we found that using a larger\ncheckpoint led to a performance detriment. This work highlights the challenges\nand risks of fine-tuning models with high expressivity when dealing with scarce\ntraining data, and lays the groundwork for future investigations into more\nrobust fine-tuning strategies for summarisation models in specialised domains.",
        "url": "http://arxiv.org/abs/2509.15419v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15419v1",
        "arxiv_id": "2509.15419v1",
        "authors": [
            "Claudio Benzoni",
            "Martina Langhals",
            "Martin Boeker",
            "Luise Modersohn",
            "Máté E. Maros"
        ],
        "submitted": "2025-09-18 20:51:33",
        "source": "arxiv",
        "comment": "14 pages, 4 figures, and 3 tables"
    },
    {
        "title": "Quantifying Uncertainty in Natural Language Explanations of Large Language Models for Question Answering",
        "abstract": "Large language models (LLMs) have shown strong capabilities, enabling\nconcise, context-aware answers in question answering (QA) tasks. The lack of\ntransparency in complex LLMs has inspired extensive research aimed at\ndeveloping methods to explain large language behaviors. Among existing\nexplanation methods, natural language explanations stand out due to their\nability to explain LLMs in a self-explanatory manner and enable the\nunderstanding of model behaviors even when the models are closed-source.\nHowever, despite these promising advancements, there is no existing work\nstudying how to provide valid uncertainty guarantees for these generated\nnatural language explanations. Such uncertainty quantification is critical in\nunderstanding the confidence behind these explanations. Notably, generating\nvalid uncertainty estimates for natural language explanations is particularly\nchallenging due to the auto-regressive generation process of LLMs and the\npresence of noise in medical inquiries. To bridge this gap, in this work, we\nfirst propose a novel uncertainty estimation framework for these generated\nnatural language explanations, which provides valid uncertainty guarantees in a\npost-hoc and model-agnostic manner. Additionally, we also design a novel robust\nuncertainty estimation method that maintains valid uncertainty guarantees even\nunder noise. Extensive experiments on QA tasks demonstrate the desired\nperformance of our methods.",
        "url": "http://arxiv.org/abs/2509.15403v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15403v1",
        "arxiv_id": "2509.15403v1",
        "authors": [
            "Yangyi Li",
            "Mengdi Huai"
        ],
        "submitted": "2025-09-18 20:29:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Exploring Fine-Tuning of Large Audio Language Models for Spoken Language Understanding under Limited Speech data",
        "abstract": "Large Audio Language Models (LALMs) have emerged as powerful tools for\nspeech-related tasks but remain underexplored for fine-tuning, especially with\nlimited speech data. To bridge this gap, we systematically examine how\ndifferent fine-tuning schemes including text-only, direct mixing, and\ncurriculum learning affect spoken language understanding (SLU), focusing on\nscenarios where text-label pairs are abundant while paired speech-label data\nare limited. Results show that LALMs already achieve competitive performance\nwith text-only fine-tuning, highlighting their strong generalization ability.\nAdding even small amounts of speech data (2-5%) yields substantial further\ngains, with curriculum learning particularly effective under scarce data. In\ncross-lingual SLU, combining source-language speech data with target-language\ntext and minimal target-language speech data enables effective adaptation.\nOverall, this study provides practical insights into the LALM fine-tuning under\nrealistic data constraints.",
        "url": "http://arxiv.org/abs/2509.15389v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15389v1",
        "arxiv_id": "2509.15389v1",
        "authors": [
            "Youngwon Choi",
            "Jaeyoon Jung",
            "Hyeonyu Kim",
            "Huu-Kim Nguyen",
            "Hwayeon Kim"
        ],
        "submitted": "2025-09-18 19:54:08",
        "source": "arxiv",
        "comment": "4 pages (excluding references), 2 figures, submitted to ICASSP 2026"
    },
    {
        "title": "Efficient and Versatile Model for Multilingual Information Retrieval of Islamic Text: Development and Deployment in Real-World Scenarios",
        "abstract": "Despite recent advancements in Multilingual Information Retrieval (MLIR), a\nsignificant gap remains between research and practical deployment. Many studies\nassess MLIR performance in isolated settings, limiting their applicability to\nreal-world scenarios. In this work, we leverage the unique characteristics of\nthe Quranic multilingual corpus to examine the optimal strategies to develop an\nad-hoc IR system for the Islamic domain that is designed to satisfy users'\ninformation needs in multiple languages. We prepared eleven retrieval models\nemploying four training approaches: monolingual, cross-lingual,\ntranslate-train-all, and a novel mixed method combining cross-lingual and\nmonolingual techniques. Evaluation on an in-domain dataset demonstrates that\nthe mixed approach achieves promising results across diverse retrieval\nscenarios. Furthermore, we provide a detailed analysis of how different\ntraining configurations affect the embedding space and their implications for\nmultilingual retrieval effectiveness. Finally, we discuss deployment\nconsiderations, emphasizing the cost-efficiency of deploying a single\nversatile, lightweight model for real-world MLIR applications.",
        "url": "http://arxiv.org/abs/2509.15380v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15380v1",
        "arxiv_id": "2509.15380v1",
        "authors": [
            "Vera Pavlova",
            "Mohammed Makhlouf"
        ],
        "submitted": "2025-09-18 19:32:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Frustratingly Easy Data Augmentation for Low-Resource ASR",
        "abstract": "This paper introduces three self-contained data augmentation methods for\nlow-resource Automatic Speech Recognition (ASR). Our techniques first generate\nnovel text--using gloss-based replacement, random replacement, or an LLM-based\napproach--and then apply Text-to-Speech (TTS) to produce synthetic audio. We\napply these methods, which leverage only the original annotated data, to four\nlanguages with extremely limited resources (Vatlongos, Nashta, Shinekhen\nBuryat, and Kakabe). Fine-tuning a pretrained Wav2Vec2-XLSR-53 model on a\ncombination of the original audio and generated synthetic data yields\nsignificant performance gains, including a 14.3% absolute WER reduction for\nNashta. The methods prove effective across all four low-resource languages and\nalso show utility for high-resource languages like English, demonstrating their\nbroad applicability.",
        "url": "http://arxiv.org/abs/2509.15373v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15373v1",
        "arxiv_id": "2509.15373v1",
        "authors": [
            "Katsumi Ibaraki",
            "David Chiang"
        ],
        "submitted": "2025-09-18 19:20:37",
        "source": "arxiv",
        "comment": "5 pages, 2 figures, 2 tables, submitted to ICASSP 2026"
    },
    {
        "title": "Speech Language Models for Under-Represented Languages: Insights from Wolof",
        "abstract": "We present our journey in training a speech language model for Wolof, an\nunderrepresented language spoken in West Africa, and share key insights. We\nfirst emphasize the importance of collecting large-scale, spontaneous,\nhigh-quality speech data, and show that continued pretraining HuBERT on this\ndataset outperforms both the base model and African-centric models on ASR. We\nthen integrate this speech encoder into a Wolof LLM to train the first Speech\nLLM for this language, extending its capabilities to tasks such as speech\ntranslation. Furthermore, we explore training the Speech LLM to perform\nmulti-step Chain-of-Thought before transcribing or translating. Our results\nshow that the Speech LLM not only improves speech recognition but also performs\nwell in speech translation. The models and the code will be openly shared.",
        "url": "http://arxiv.org/abs/2509.15362v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15362v1",
        "arxiv_id": "2509.15362v1",
        "authors": [
            "Yaya Sy",
            "Dioula Doucouré",
            "Christophe Cerisara",
            "Irina Illina"
        ],
        "submitted": "2025-09-18 19:01:48",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Beyond Spurious Signals: Debiasing Multimodal Large Language Models via Counterfactual Inference and Adaptive Expert Routing",
        "abstract": "Multimodal Large Language Models (MLLMs) have shown substantial capabilities\nin integrating visual and textual information, yet frequently rely on spurious\ncorrelations, undermining their robustness and generalization in complex\nmultimodal reasoning tasks. This paper addresses the critical challenge of\nsuperficial correlation bias in MLLMs through a novel causal mediation-based\ndebiasing framework. Specially, we distinguishing core semantics from spurious\ntextual and visual contexts via counterfactual examples to activate\ntraining-stage debiasing and employ a Mixture-of-Experts (MoE) architecture\nwith dynamic routing to selectively engages modality-specific debiasing\nexperts. Empirical evaluation on multimodal sarcasm detection and sentiment\nanalysis tasks demonstrates that our framework significantly surpasses unimodal\ndebiasing strategies and existing state-of-the-art models.",
        "url": "http://arxiv.org/abs/2509.15361v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15361v1",
        "arxiv_id": "2509.15361v1",
        "authors": [
            "Zichen Wu",
            "Hsiu-Yuan Huang",
            "Yunfang Wu"
        ],
        "submitted": "2025-09-18 19:01:11",
        "source": "arxiv",
        "comment": "Accepted by EMNLP 2025 Findings"
    },
    {
        "title": "Real, Fake, or Manipulated? Detecting Machine-Influenced Text",
        "abstract": "Large Language Model (LLMs) can be used to write or modify documents,\npresenting a challenge for understanding the intent behind their use. For\nexample, benign uses may involve using LLM on a human-written document to\nimprove its grammar or to translate it into another language. However, a\ndocument entirely produced by a LLM may be more likely to be used to spread\nmisinformation than simple translation (\\eg, from use by malicious actors or\nsimply by hallucinating). Prior works in Machine Generated Text (MGT) detection\nmostly focus on simply identifying whether a document was human or machine\nwritten, ignoring these fine-grained uses. In this paper, we introduce a\nHiErarchical, length-RObust machine-influenced text detector (HERO), which\nlearns to separate text samples of varying lengths from four primary types:\nhuman-written, machine-generated, machine-polished, and machine-translated.\nHERO accomplishes this by combining predictions from length-specialist models\nthat have been trained with Subcategory Guidance. Specifically, for categories\nthat are easily confused (\\eg, different source languages), our Subcategory\nGuidance module encourages separation of the fine-grained categories, boosting\nperformance. Extensive experiments across five LLMs and six domains demonstrate\nthe benefits of our HERO, outperforming the state-of-the-art by 2.5-3 mAP on\naverage.",
        "url": "http://arxiv.org/abs/2509.15350v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15350v1",
        "arxiv_id": "2509.15350v1",
        "authors": [
            "Yitong Wang",
            "Zhongping Zhang",
            "Margherita Piana",
            "Zheng Zhou",
            "Peter Gerstoft",
            "Bryan A. Plummer"
        ],
        "submitted": "2025-09-18 18:41:57",
        "source": "arxiv",
        "comment": "Accepted to EMNLP 2025 Findings"
    },
    {
        "title": "Quantifying Self-Awareness of Knowledge in Large Language Models",
        "abstract": "Hallucination prediction in large language models (LLMs) is often interpreted\nas a sign of self-awareness. However, we argue that such performance can arise\nfrom question-side shortcuts rather than true model-side introspection. To\ndisentangle these factors, we propose the Approximate Question-side Effect\n(AQE), which quantifies the contribution of question-awareness. Our analysis\nacross multiple datasets reveals that much of the reported success stems from\nexploiting superficial patterns in questions. We further introduce SCAO\n(Semantic Compression by Answering in One word), a method that enhances the use\nof model-side signals. Experiments show that SCAO achieves strong and\nconsistent performance, particularly in settings with reduced question-side\ncues, highlighting its effectiveness in fostering genuine self-awareness in\nLLMs.",
        "url": "http://arxiv.org/abs/2509.15339v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15339v1",
        "arxiv_id": "2509.15339v1",
        "authors": [
            "Yeongbin Seo",
            "Dongha Lee",
            "Jinyoung Yeo"
        ],
        "submitted": "2025-09-18 18:29:14",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PolBiX: Detecting LLMs' Political Bias in Fact-Checking through X-phemisms",
        "abstract": "Large Language Models are increasingly used in applications requiring\nobjective assessment, which could be compromised by political bias. Many\nstudies found preferences for left-leaning positions in LLMs, but downstream\neffects on tasks like fact-checking remain underexplored. In this study, we\nsystematically investigate political bias through exchanging words with\neuphemisms or dysphemisms in German claims. We construct minimal pairs of\nfactually equivalent claims that differ in political connotation, to assess the\nconsistency of LLMs in classifying them as true or false. We evaluate six LLMs\nand find that, more than political leaning, the presence of judgmental words\nsignificantly influences truthfulness assessment. While a few models show\ntendencies of political bias, this is not mitigated by explicitly calling for\nobjectivism in prompts.",
        "url": "http://arxiv.org/abs/2509.15335v1",
        "pdf_url": "http://arxiv.org/pdf/2509.15335v1",
        "arxiv_id": "2509.15335v1",
        "authors": [
            "Charlott Jakob",
            "David Harbecke",
            "Patrick Parschan",
            "Pia Wenzel Neves",
            "Vera Schmitt"
        ],
        "submitted": "2025-09-18 18:26:53",
        "source": "arxiv",
        "comment": null
    }
]