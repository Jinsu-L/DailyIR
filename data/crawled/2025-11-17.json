[
    {
        "title": "Accepted with Minor Revisions: Value of AI-Assisted Scientific Writing",
        "abstract": "Large Language Models have seen expanding application across domains, yet their effectiveness as assistive tools for scientific writing -- an endeavor requiring precision, multimodal synthesis, and domain expertise -- remains insufficiently understood. We examine the potential of LLMs to support domain experts in scientific writing, with a focus on abstract composition. We design an incentivized randomized controlled trial with a hypothetical conference setup where participants with relevant expertise are split into an author and reviewer pool. Inspired by methods in behavioral science, our novel incentive structure encourages authors to edit the provided abstracts to an acceptable quality for a peer-reviewed submission. Our 2x2 between-subject design expands into two dimensions: the implicit source of the provided abstract and the disclosure of it. We find authors make most edits when editing human-written abstracts compared to AI-generated abstracts without source attribution, often guided by higher perceived readability in AI generation. Upon disclosure of source information, the volume of edits converges in both source treatments. Reviewer decisions remain unaffected by the source of the abstract, but bear a significant correlation with the number of edits made. Careful stylistic edits, especially in the case of AI-generated abstracts, in the presence of source information, improve the chance of acceptance. We find that AI-generated abstracts hold potential to reach comparable levels of acceptability to human-written ones with minimal revision, and that perceptions of AI authorship, rather than objective quality, drive much of the observed editing behavior. Our findings reverberate the significance of source disclosure in collaborative scientific writing.",
        "url": "http://arxiv.org/abs/2511.12529v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12529v1",
        "arxiv_id": "2511.12529v1",
        "authors": [
            "Sanchaita Hazra",
            "Doeun Lee",
            "Bodhisattwa Prasad Majumder",
            "Sachin Kumar"
        ],
        "submitted": "2025-11-16 09:49:01",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "TAdaRAG: Task Adaptive Retrieval-Augmented Generation via On-the-Fly Knowledge Graph Construction",
        "abstract": "Retrieval-Augmented Generation (RAG) improves large language models by retrieving external knowledge, often truncated into smaller chunks due to the input context window, which leads to information loss, resulting in response hallucinations and broken reasoning chains. Moreover, traditional RAG retrieves unstructured knowledge, introducing irrelevant details that hinder accurate reasoning. To address these issues, we propose TAdaRAG, a novel RAG framework for on-the-fly task-adaptive knowledge graph construction from external sources. Specifically, we design an intent-driven routing mechanism to a domain-specific extraction template, followed by supervised fine-tuning and a reinforcement learning-based implicit extraction mechanism, ensuring concise, coherent, and non-redundant knowledge integration. Evaluations on six public benchmarks and a real-world business benchmark (NowNewsQA) across three backbone models demonstrate that TAdaRAG outperforms existing methods across diverse domains and long-text tasks, highlighting its strong generalization and practical effectiveness.",
        "url": "http://arxiv.org/abs/2511.12520v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12520v1",
        "arxiv_id": "2511.12520v1",
        "authors": [
            "Jie Zhang",
            "Bo Tang",
            "Wanzi Shao",
            "Wenqiang Wei",
            "Jihao Zhao",
            "Jianqing Zhu",
            "Zhiyu li",
            "Wen Xi",
            "Zehao Lin",
            "Feiyu Xiong",
            "Yanchao Tan"
        ],
        "submitted": "2025-11-16 09:23:09",
        "source": "arxiv",
        "comment": "Accepted by AAAI 2026"
    },
    {
        "title": "DualGR: Generative Retrieval with Long and Short-Term Interests Modeling",
        "abstract": "In large-scale industrial recommendation systems, retrieval must produce high-quality candidates from massive corpora under strict latency. Recently, Generative Retrieval (GR) has emerged as a viable alternative to Embedding-Based Retrieval (EBR), which quantizes items into a finite token space and decodes candidates autoregressively, providing a scalable path that explicitly models target-history interactions via cross-attention. However, three challenges persist: 1) how to balance users' long-term and short-term interests , 2) noise interference when generating hierarchical semantic IDs (SIDs), 3) the absence of explicit modeling for negative feedback such as exposed items without clicks. To address these challenges, we propose DualGR, a generative retrieval framework that explicitly models dual horizons of user interests with selective activation. Specifically, DualGR utilizes Dual-Branch Long/Short-Term Router (DBR) to cover both stable preferences and transient intents by explicitly modeling users' long- and short-term behaviors. Meanwhile, Search-based SID Decoding (S2D) is presented to control context-induced noise and enhance computational efficiency by constraining candidate interactions to the current coarse (level-1) bucket during fine-grained (level-2/3) SID prediction. % also reinforcing intra-class consistency. Finally, we propose an Exposure-aware Next-Token Prediction Loss (ENTP-Loss) that treats \"exposed-but-unclicked\" items as hard negatives at level-1, enabling timely interest fade-out. On the large-scale Kuaishou short-video recommendation system, DualGR has achieved outstanding performance. Online A/B testing shows +0.527% video views and +0.432% watch time lifts, validating DualGR as a practical and effective paradigm for industrial generative retrieval.",
        "url": "http://arxiv.org/abs/2511.12518v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12518v1",
        "arxiv_id": "2511.12518v1",
        "authors": [
            "Zhongchao Yi",
            "Kai Feng",
            "Xiaojian Ma",
            "Yalong Wang",
            "Yongqi Liu",
            "Han Li",
            "Zhengyang Zhou",
            "Yang Wang"
        ],
        "submitted": "2025-11-16 09:20:54",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "QA-Noun: Representing Nominal Semantics via Natural Language Question-Answer Pairs",
        "abstract": "Decomposing sentences into fine-grained meaning units is increasingly used to model semantic alignment. While QA-based semantic approaches have shown effectiveness for representing predicate-argument relations, they have so far left noun-centered semantics largely unaddressed. We introduce QA-Noun, a QA-based framework for capturing noun-centered semantic relations. QA-Noun defines nine question templates that cover both explicit syntactical and implicit contextual roles for nouns, producing interpretable QA pairs that complement verbal QA-SRL. We release detailed guidelines, a dataset of over 2,000 annotated noun mentions, and a trained model integrated with QA-SRL to yield a unified decomposition of sentence meaning into individual, highly fine-grained, facts. Evaluation shows that QA-Noun achieves near-complete coverage of AMR's noun arguments while surfacing additional contextually implied relations, and that combining QA-Noun with QA-SRL yields over 130\\% higher granularity than recent fact-based decomposition methods such as FactScore and DecompScore. QA-Noun thus complements the broader QA-based semantic framework, forming a comprehensive and scalable approach to fine-grained semantic decomposition for cross-text alignment.",
        "url": "http://arxiv.org/abs/2511.12504v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12504v1",
        "arxiv_id": "2511.12504v1",
        "authors": [
            "Maria Tseytlin",
            "Paul Roit",
            "Omri Abend",
            "Ido Dagan",
            "Ayal Klein"
        ],
        "submitted": "2025-11-16 08:32:38",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "SGuard-v1: Safety Guardrail for Large Language Models",
        "abstract": "We present SGuard-v1, a lightweight safety guardrail for Large Language Models (LLMs), which comprises two specialized models to detect harmful content and screen adversarial prompts in human-AI conversational settings. The first component, ContentFilter, is trained to identify safety risks in LLM prompts and responses in accordance with the MLCommons hazard taxonomy, a comprehensive framework for trust and safety assessment of AI. The second component, JailbreakFilter, is trained with a carefully designed curriculum over integrated datasets and findings from prior work on adversarial prompting, covering 60 major attack types while mitigating false-unsafe classification. SGuard-v1 is built on the 2B-parameter Granite-3.3-2B-Instruct model that supports 12 languages. We curate approximately 1.4 million training instances from both collected and synthesized data and perform instruction tuning on the base model, distributing the curated data across the two component according to their designated functions. Through extensive evaluation on public and proprietary safety benchmarks, SGuard-v1 achieves state-of-the-art safety performance while remaining lightweight, thereby reducing deployment overhead. SGuard-v1 also improves interpretability for downstream use by providing multi-class safety predictions and their binary confidence scores. We release the SGuard-v1 under the Apache-2.0 License to enable further research and practical deployment in AI safety.",
        "url": "http://arxiv.org/abs/2511.12497v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12497v1",
        "arxiv_id": "2511.12497v1",
        "authors": [
            "JoonHo Lee",
            "HyeonMin Cho",
            "Jaewoong Yun",
            "Hyunjae Lee",
            "JunKyu Lee",
            "Juree Seok"
        ],
        "submitted": "2025-11-16 08:15:54",
        "source": "arxiv",
        "comment": "Technical Report"
    },
    {
        "title": "Task-Aware Retrieval Augmentation for Dynamic Recommendation",
        "abstract": "Dynamic recommendation systems aim to provide personalized suggestions by modeling temporal user-item interactions across time-series behavioral data. Recent studies have leveraged pre-trained dynamic graph neural networks (GNNs) to learn user-item representations over temporal snapshot graphs. However, fine-tuning GNNs on these graphs often results in generalization issues due to temporal discrepancies between pre-training and fine-tuning stages, limiting the model's ability to capture evolving user preferences. To address this, we propose TarDGR, a task-aware retrieval-augmented framework designed to enhance generalization capability by incorporating task-aware model and retrieval-augmentation. Specifically, TarDGR introduces a Task-Aware Evaluation Mechanism to identify semantically relevant historical subgraphs, enabling the construction of task-specific datasets without manual labeling. It also presents a Graph Transformer-based Task-Aware Model that integrates semantic and structural encodings to assess subgraph relevance. During inference, TarDGR retrieves and fuses task-aware subgraphs with the query subgraph, enriching its representation and mitigating temporal generalization issues. Experiments on multiple large-scale dynamic graph datasets demonstrate that TarDGR consistently outperforms state-of-the-art methods, with extensive empirical evidence underscoring its superior accuracy and generalization capabilities.",
        "url": "http://arxiv.org/abs/2511.12495v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12495v1",
        "arxiv_id": "2511.12495v1",
        "authors": [
            "Zhen Tao",
            "Xinke Jiang",
            "Qingshuai Feng",
            "Haoyu Zhang",
            "Lun Du",
            "Yuchen Fang",
            "Hao Miao",
            "Bangquan Xie",
            "Qingqiang Sun"
        ],
        "submitted": "2025-11-16 08:14:52",
        "source": "arxiv",
        "comment": "AAAI 2026"
    },
    {
        "title": "Evolving Prompts for Toxicity Search in Large Language Models",
        "abstract": "Large Language Models remain vulnerable to adversarial prompts that elicit toxic content even after safety alignment. We present ToxSearch, a black-box evolutionary framework that tests model safety by evolving prompts in a synchronous steady-state loop. The system employs a diverse set of operators, including lexical substitutions, negation, back-translation, paraphrasing, and two semantic crossover operators, while a moderation oracle provides fitness guidance. Operator-level analysis shows heterogeneous behavior: lexical substitutions offer the best yield-variance trade-off, semantic-similarity crossover acts as a precise low-throughput inserter, and global rewrites exhibit high variance with elevated refusal costs. Using elite prompts evolved on LLaMA 3.1 8B, we observe practically meaningful but attenuated cross-model transfer, with toxicity roughly halving on most targets, smaller LLaMA 3.2 variants showing the strongest resistance, and some cross-architecture models retaining higher toxicity. These results suggest that small, controllable perturbations are effective vehicles for systematic red-teaming and that defenses should anticipate cross-model reuse of adversarial prompts rather than focusing only on single-model hardening.",
        "url": "http://arxiv.org/abs/2511.12487v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12487v1",
        "arxiv_id": "2511.12487v1",
        "authors": [
            "Onkar Shelar",
            "Travis Desell"
        ],
        "submitted": "2025-11-16 07:47:31",
        "source": "arxiv",
        "comment": "pre-print"
    },
    {
        "title": "Co-Layout: LLM-driven Co-optimization for Interior Layout",
        "abstract": "We present a novel framework for automated interior design that combines large language models (LLMs) with grid-based integer programming to jointly optimize room layout and furniture placement. Given a textual prompt, the LLM-driven agent workflow extracts structured design constraints related to room configurations and furniture arrangements. These constraints are encoded into a unified grid-based representation inspired by ``Modulor\". Our formulation accounts for key design requirements, including corridor connectivity, room accessibility, spatial exclusivity, and user-specified preferences. To improve computational efficiency, we adopt a coarse-to-fine optimization strategy that begins with a low-resolution grid to solve a simplified problem and guides the solution at the full resolution. Experimental results across diverse scenarios demonstrate that our joint optimization approach significantly outperforms existing two-stage design pipelines in solution quality, and achieves notable computational efficiency through the coarse-to-fine strategy.",
        "url": "http://arxiv.org/abs/2511.12474v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12474v1",
        "arxiv_id": "2511.12474v1",
        "authors": [
            "Chucheng Xiang",
            "Ruchao Bao",
            "Biyin Feng",
            "Wenzheng Wu",
            "Zhongyuan Liu",
            "Yirui Guan",
            "Ligang Liu"
        ],
        "submitted": "2025-11-16 06:20:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Assessing LLMs for Serendipity Discovery in Knowledge Graphs: A Case for Drug Repurposing",
        "abstract": "Large Language Models (LLMs) have greatly advanced knowledge graph question answering (KGQA), yet existing systems are typically optimized for returning highly relevant but predictable answers. A missing yet desired capacity is to exploit LLMs to suggest surprise and novel (\"serendipitious\") answers. In this paper, we formally define the serendipity-aware KGQA task and propose the SerenQA framework to evaluate LLMs' ability to uncover unexpected insights in scientific KGQA tasks. SerenQA includes a rigorous serendipity metric based on relevance, novelty, and surprise, along with an expert-annotated benchmark derived from the Clinical Knowledge Graph, focused on drug repurposing. Additionally, it features a structured evaluation pipeline encompassing three subtasks: knowledge retrieval, subgraph reasoning, and serendipity exploration. Our experiments reveal that while state-of-the-art LLMs perform well on retrieval, they still struggle to identify genuinely surprising and valuable discoveries, underscoring a significant room for future improvements. Our curated resources and extended version are released at: https://cwru-db-group.github.io/serenQA.",
        "url": "http://arxiv.org/abs/2511.12472v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12472v1",
        "arxiv_id": "2511.12472v1",
        "authors": [
            "Mengying Wang",
            "Chenhui Ma",
            "Ao Jiao",
            "Tuo Liang",
            "Pengjun Lu",
            "Shrinidhi Hegde",
            "Yu Yin",
            "Evren Gurkan-Cavusoglu",
            "Yinghui Wu"
        ],
        "submitted": "2025-11-16 06:19:53",
        "source": "arxiv",
        "comment": "The 40th AAAI Conference on Artificial Intelligence (AAAI-26)"
    },
    {
        "title": "Probing Preference Representations: A Multi-Dimensional Evaluation and Analysis Method for Reward Models",
        "abstract": "Previous methods evaluate reward models by testing them on a fixed pairwise ranking test set, but they typically do not provide performance information on each preference dimension. In this work, we address the evaluation challenge of reward models by probing preference representations. To confirm the effectiveness of this evaluation method, we construct a Multi-dimensional Reward Model Benchmark (MRMBench), a collection of six probing tasks for different preference dimensions. We design it to favor and encourage reward models that better capture preferences across different dimensions. Furthermore, we introduce an analysis method, inference-time probing, which identifies the dimensions used during the reward prediction and enhances its interpretability. Through extensive experiments, we find that MRMBench strongly correlates with the alignment performance of large language models (LLMs), making it a reliable reference for developing advanced reward models. Our analysis of MRMBench evaluation results reveals that reward models often struggle to capture preferences across multiple dimensions, highlighting the potential of multi-objective optimization in reward modeling. Additionally, our findings show that the proposed inference-time probing method offers a reliable metric for assessing the confidence of reward predictions, which ultimately improves the alignment of LLMs.",
        "url": "http://arxiv.org/abs/2511.12464v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12464v1",
        "arxiv_id": "2511.12464v1",
        "authors": [
            "Chenglong Wang",
            "Yifu Huo",
            "Yang Gan",
            "Yongyu Mu",
            "Qiaozhi He",
            "Murun Yang",
            "Bei Li",
            "Chunliang Zhang",
            "Tongran Liu",
            "Anxiang Ma",
            "Zhengtao Yu",
            "Jingbo Zhu",
            "Tong Xiao"
        ],
        "submitted": "2025-11-16 05:29:29",
        "source": "arxiv",
        "comment": "Accepted by AAAI 2026"
    },
    {
        "title": "DenseAnnotate: Enabling Scalable Dense Caption Collection for Images and 3D Scenes via Spoken Descriptions",
        "abstract": "With the rapid adoption of multimodal large language models (MLLMs) across diverse applications, there is a pressing need for task-centered, high-quality training data. A key limitation of current training datasets is their reliance on sparse annotations mined from the Internet or entered via manual typing that capture only a fraction of an image's visual content. Dense annotations are more valuable but remain scarce. Traditional text-based annotation pipelines are poorly suited for creating dense annotations: typing limits expressiveness, slows annotation speed, and underrepresents nuanced visual features, especially in specialized areas such as multicultural imagery and 3D asset annotation. In this paper, we present DenseAnnotate, an audio-driven online annotation platform that enables efficient creation of dense, fine-grained annotations for images and 3D assets. Annotators narrate observations aloud while synchronously linking spoken phrases to image regions or 3D scene parts. Our platform incorporates speech-to-text transcription and region-of-attention marking. To demonstrate the effectiveness of DenseAnnotate, we conducted case studies involving over 1,000 annotators across two domains: culturally diverse images and 3D scenes. We curate a human-annotated multi-modal dataset of 3,531 images, 898 3D scenes, and 7,460 3D objects, with audio-aligned dense annotations in 20 languages, including 8,746 image captions, 2,000 scene captions, and 19,000 object captions. Models trained on this dataset exhibit improvements of 5% in multilingual, 47% in cultural alignment, and 54% in 3D spatial capabilities. Our results show that our platform offers a feasible approach for future vision-language research and can be applied to various tasks and diverse types of data.",
        "url": "http://arxiv.org/abs/2511.12452v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12452v1",
        "arxiv_id": "2511.12452v1",
        "authors": [
            "Xiaoyu Lin",
            "Aniket Ghorpade",
            "Hansheng Zhu",
            "Justin Qiu",
            "Dea Rrozhani",
            "Monica Lama",
            "Mick Yang",
            "Zixuan Bian",
            "Ruohan Ren",
            "Alan B. Hong",
            "Jiatao Gu",
            "Chris Callison-Burch"
        ],
        "submitted": "2025-11-16 04:46:06",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MOON2.0: Dynamic Modality-balanced Multimodal Representation Learning for E-commerce Product Understanding",
        "abstract": "The rapid growth of e-commerce calls for multimodal models that comprehend rich visual and textual product information. Although recent multimodal large language models (MLLMs) for product understanding exhibit strong capability in representation learning for e-commerce, they still face three challenges: (i) the modality imbalance induced by modality mixed training; (ii) underutilization of the intrinsic alignment relationships among visual and textual information within a product; and (iii) limited handling of noise in e-commerce multimodal data. To address these, we propose MOON2.0, a dynamic modality-balanced multimodal representation learning framework for e-commerce product understanding. MOON2.0 comprises: (1) a Modality-driven Mixture-of-Experts (MoE) module that adaptively processes input samples by their modality composition, enabling Multimodal Joint Learning to mitigate the modality imbalance; (2) a Dual-level Alignment method to better leverage semantic alignment properties inside individual products; and (3) an MLLM-based Image-text Co-augmentation strategy that integrates textual enrichment with visual expansion, coupled with Dynamic Sample Filtering to improve training data quality. We further introduce MBE2.0, a co-augmented multimodal representation benchmark for e-commerce representation learning and evaluation. Experiments show that MOON2.0 delivers state-of-the-art zero-shot performance on MBE2.0 and multiple public datasets. Furthermore, attention-based heatmap visualization provides qualitative evidence of improved multimodal alignment of MOON2.0.",
        "url": "http://arxiv.org/abs/2511.12449v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12449v1",
        "arxiv_id": "2511.12449v1",
        "authors": [
            "Zhanheng Nie",
            "Chenghan Fu",
            "Daoze Zhang",
            "Junxian Wu",
            "Wanxian Guan",
            "Pengjie Wang",
            "Jian Xu",
            "Bo Zheng"
        ],
        "submitted": "2025-11-16 04:29:35",
        "source": "arxiv",
        "comment": "11 pages, 7 figures"
    },
    {
        "title": "From Phonemes to Meaning: Evaluating Large Language Models on Tamil",
        "abstract": "Large Language Models (LLMs) have shown strong generalization across tasks in high-resource languages; however, their linguistic competence in low-resource and morphologically rich languages such as Tamil remains largely unexplored. Existing multilingual benchmarks often rely on translated English datasets, failing to capture the linguistic and cultural nuances of the target language. To address this gap, we introduce ILAKKANAM, the first Tamil-specific linguistic evaluation benchmark manually curated using 820 questions from Sri Lankan school-level Tamil subject examination papers. Each question is annotated by trained linguists under five linguistic categories and a factual knowledge category, spanning Grades 1--13 to ensure broad linguistic coverage. We evaluate both closed-source and open-source LLMs using a standardized evaluation framework. Our results show that Gemini 2.5 achieves the highest overall performance, while open-source models lag behind, highlighting the gap in linguistic grounding. Category- and grade-wise analyses reveal that all models perform well on lower-grade questions but show a clear decline as linguistic complexity increases. Further, no strong correlation is observed between a model's overall performance and its ability to identify linguistic categories, suggesting that performance may be driven by exposure rather than genuine understanding.",
        "url": "http://arxiv.org/abs/2511.12387v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12387v1",
        "arxiv_id": "2511.12387v1",
        "authors": [
            "Jeyarajalingam Varsha",
            "Menan Velayuthan",
            "Sumirtha Karunakaran",
            "Rasan Nivethiga",
            "Kengatharaiyer Sarveswaran"
        ],
        "submitted": "2025-11-15 23:41:16",
        "source": "arxiv",
        "comment": "11 pages"
    },
    {
        "title": "Don't Think of the White Bear: Ironic Negation in Transformer Models Under Cognitive Load",
        "abstract": "Negation instructions such as 'do not mention $X$' can paradoxically increase the accessibility of $X$ in human thought, a phenomenon known as ironic rebound. Large language models (LLMs) face the same challenge: suppressing a concept requires internally activating it, which may prime rebound instead of avoidance. We investigated this tension with two experiments. \\textbf{(1) Load \\& content}: after a negation instruction, we vary distractor text (semantic, syntactic, repetition) and measure rebound strength. \\textbf{(2) Polarity separation}: We test whether models distinguish neutral from negative framings of the same concept and whether this separation predicts rebound persistence. Results show that rebound consistently arises immediately after negation and intensifies with longer or semantic distractors, while repetition supports suppression. Stronger polarity separation correlates with more persistent rebound. Together, these findings, complemented by a circuit tracing analysis that identifies sparse middle-layer attention heads amplifying forbidden tokens while early layers suppress, link cognitive predictions of ironic rebound with mechanistic insights into long-context interference. To support future work, we release ReboundBench, a dataset of $5,000$ systematically varied negation prompts designed to probe rebound in LLMs.",
        "url": "http://arxiv.org/abs/2511.12381v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12381v1",
        "arxiv_id": "2511.12381v1",
        "authors": [
            "Logan Mann",
            "Nayan Saxena",
            "Sarah Tandon",
            "Chenhao Sun",
            "Savar Toteja",
            "Kevin Zhu"
        ],
        "submitted": "2025-11-15 23:00:56",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "VoiceCraft-X: Unifying Multilingual, Voice-Cloning Speech Synthesis and Speech Editing",
        "abstract": "We introduce VoiceCraft-X, an autoregressive neural codec language model which unifies multilingual speech editing and zero-shot Text-to-Speech (TTS) synthesis across 11 languages: English, Mandarin, Korean, Japanese, Spanish, French, German, Dutch, Italian, Portuguese, and Polish. VoiceCraft-X utilizes the Qwen3 large language model for phoneme-free cross-lingual text processing and a novel token reordering mechanism with time-aligned text and speech tokens to handle both tasks as a single sequence generation problem. The model generates high-quality, natural-sounding speech, seamlessly creating new audio or editing existing recordings within one framework. VoiceCraft-X shows robust performance in diverse linguistic settings, even with limited per-language data, underscoring the power of unified autoregressive approaches for advancing complex, real-world multilingual speech applications. Audio samples are available at https://zhishengzheng.com/voicecraft-x/.",
        "url": "http://arxiv.org/abs/2511.12347v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12347v1",
        "arxiv_id": "2511.12347v1",
        "authors": [
            "Zhisheng Zheng",
            "Puyuan Peng",
            "Anuj Diwan",
            "Cong Phuoc Huynh",
            "Xiaohang Sun",
            "Zhu Liu",
            "Vimal Bhat",
            "David Harwath"
        ],
        "submitted": "2025-11-15 20:27:25",
        "source": "arxiv",
        "comment": "EMNLP 2025. Demo and code are available at https://zhishengzheng.com/voicecraft-x/"
    },
    {
        "title": "Do LLMs and Humans Find the Same Questions Difficult? A Case Study on Japanese Quiz Answering",
        "abstract": "LLMs have achieved performance that surpasses humans in many NLP tasks. However, it remains unclear whether problems that are difficult for humans are also difficult for LLMs. This study investigates how the difficulty of quizzes in a buzzer setting differs between LLMs and humans. Specifically, we first collect Japanese quiz data including questions, answers, and correct response rate of humans, then prompted LLMs to answer the quizzes under several settings, and compare their correct answer rate to that of humans from two analytical perspectives. The experimental results showed that, compared to humans, LLMs struggle more with quizzes whose correct answers are not covered by Wikipedia entries, and also have difficulty with questions that require numerical answers.",
        "url": "http://arxiv.org/abs/2511.12300v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12300v1",
        "arxiv_id": "2511.12300v1",
        "authors": [
            "Naoya Sugiura",
            "Kosuke Yamada",
            "Yasuhiro Ogawa",
            "Katsuhiko Toyama",
            "Ryohei Sasano"
        ],
        "submitted": "2025-11-15 17:23:00",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AugAbEx : Way Forward for Extractive Case Summarization",
        "abstract": "Summarization of legal judgments poses a heavy cognitive burden on law practitioners due to the complexity of the language, context-sensitive legal jargon, and the length of the document. Therefore, the automatic summarization of legal documents has attracted serious attention from natural language processing researchers. Since the abstractive summaries of legal documents generated by deep neural methods remain prone to the risk of misrepresenting nuanced legal jargon or overlooking key contextual details, we envisage a rising trend toward the use of extractive case summarizers.\n  Given the high cost of human annotation for gold standard extractive summaries, we engineer a light and transparent pipeline that leverages existing abstractive gold standard summaries to create the corresponding extractive gold standard versions. The approach ensures that the experts` opinions ensconced in the original gold standard abstractive summaries are carried over to the transformed extractive summaries. We aim to augment seven existing case summarization datasets, which include abstractive summaries, by incorporating corresponding extractive summaries and create an enriched data resource for case summarization research community. To ensure the quality of the augmented extractive summaries, we perform an extensive comparative evaluation with the original abstractive gold standard summaries covering structural, lexical, and semantic dimensions. We also compare the domain-level information of the two summaries. We commit to release the augmented datasets in the public domain for use by the research community and believe that the resource will offer opportunities to advance the field of automatic summarization of legal documents.",
        "url": "http://arxiv.org/abs/2511.12290v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12290v1",
        "arxiv_id": "2511.12290v1",
        "authors": [
            "Purnima Bindal",
            "Vikas Kumar",
            "Sagar Rathore",
            "Vasudha Bhatnagar"
        ],
        "submitted": "2025-11-15 16:49:42",
        "source": "arxiv",
        "comment": "30 pages, under review in a Journal"
    },
    {
        "title": "How Far Do SSL Speech Models Listen for Tone? Temporal Focus of Tone Representation under Low-resource Transfer",
        "abstract": "Lexical tone is central to many languages but remains underexplored in self-supervised learning (SSL) speech models, especially beyond Mandarin. We study four languages with complex and diverse tone systems: Burmese, Thai, Lao, and Vietnamese, to examine how far such models listen for tone and how transfer operates in low-resource conditions. As a baseline reference, we estimate the temporal span of tone cues to be about 100 ms in Burmese and Thai, and about 180 ms in Lao and Vietnamese. Probes and gradient analyses on fine-tuned SSL models reveal that tone transfer varies by downstream task: automatic speech recognition fine-tuning aligns spans with language-specific tone cues, while prosody- and voice-related tasks bias the model toward overly long spans. These findings indicate that tone transfer is shaped by downstream task, highlighting task effects on temporal focus in tone modeling.",
        "url": "http://arxiv.org/abs/2511.12285v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12285v1",
        "arxiv_id": "2511.12285v1",
        "authors": [
            "Minu Kim",
            "Ji Sub Um",
            "Hoirin Kim"
        ],
        "submitted": "2025-11-15 16:38:09",
        "source": "arxiv",
        "comment": "5 pages, 7 figures, submitted to ICASSP 2026"
    },
    {
        "title": "Cmprsr: Abstractive Token-Level Question-Agnostic Prompt Compressor",
        "abstract": "Motivated by the high costs of using black-box Large Language Models (LLMs), we introduce a novel prompt compression paradigm, under which we use smaller LLMs to compress inputs for the larger ones. We present the first comprehensive LLM-as-a-compressor benchmark spanning 25 open- and closed-source models, which reveals significant disparity in models' compression ability in terms of (i) preserving semantically important information (ii) following the user-provided compression rate (CR). We further improve the performance of gpt-4.1-mini, the best overall vanilla compressor, with Textgrad-based compression meta-prompt optimization. We also identify the most promising open-source vanilla LLM - Qwen3-4B - and post-train it with a combination of supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO), pursuing the dual objective of CR adherence and maximizing the downstream task performance. We call the resulting model Cmprsr and demonstrate its superiority over both extractive and vanilla abstractive compression across the entire range of compression rates on lengthy inputs from MeetingBank and LongBench as well as short prompts from GSM8k. The latter highlights Cmprsr's generalizability across varying input lengths and domains. Moreover, Cmprsr closely follows the requested compression rate, offering fine control over the cost-quality trade-off.",
        "url": "http://arxiv.org/abs/2511.12281v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12281v1",
        "arxiv_id": "2511.12281v1",
        "authors": [
            "Ivan Zakazov",
            "Alexander Sharipov",
            "Berke Argin",
            "Oussama Gabouj",
            "Kamel Charaf",
            "Alexi Semiz",
            "Lorenzo Drudi",
            "Nicolas Baldwin",
            "Robert West"
        ],
        "submitted": "2025-11-15 16:28:03",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "D$^{3}$ToM: Decider-Guided Dynamic Token Merging for Accelerating Diffusion MLLMs",
        "abstract": "Diffusion-based multimodal large language models (Diffusion MLLMs) have recently demonstrated impressive non-autoregressive generative capabilities across vision-and-language tasks. However, Diffusion MLLMs exhibit substantially slower inference than autoregressive models: Each denoising step employs full bidirectional self-attention over the entire sequence, resulting in cubic decoding complexity that becomes computationally impractical with thousands of visual tokens. To address this challenge, we propose D$^{3}$ToM, a Decider-guided dynamic token merging method that dynamically merges redundant visual tokens at different denoising steps to accelerate inference in Diffusion MLLMs. At each denoising step, D$^{3}$ToM uses decider tokens-the tokens generated in the previous denoising step-to build an importance map over all visual tokens. Then it maintains a proportion of the most salient tokens and merges the remainder through similarity-based aggregation. This plug-and-play module integrates into a single transformer layer, physically shortening the visual token sequence for all subsequent layers without altering model parameters. Moreover, D$^{3}$ToM employs a merge ratio that dynamically varies with each denoising step, aligns with the native decoding process of Diffusion MLLMs, achieving superior performance under equivalent computational budgets. Extensive experiments show that D$^{3}$ToM accelerates inference while preserving competitive performance. The code is released at https://github.com/bcmi/D3ToM-Diffusion-MLLM.",
        "url": "http://arxiv.org/abs/2511.12280v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12280v1",
        "arxiv_id": "2511.12280v1",
        "authors": [
            "Shuochen Chang",
            "Xiaofeng Zhang",
            "Qingyang Liu",
            "Li Niu"
        ],
        "submitted": "2025-11-15 16:24:12",
        "source": "arxiv",
        "comment": "Accepted by AAAI Conference on Artificial Intelligence (AAAI) 2026. Code available at https://github.com/bcmi/D3ToM-Diffusion-MLLM"
    },
    {
        "title": "Mobile-Agent-RAG: Driving Smart Multi-Agent Coordination with Contextual Knowledge Empowerment for Long-Horizon Mobile Automation",
        "abstract": "Mobile agents show immense potential, yet current state-of-the-art (SoTA) agents exhibit inadequate success rates on real-world, long-horizon, cross-application tasks. We attribute this bottleneck to the agents' excessive reliance on static, internal knowledge within MLLMs, which leads to two critical failure points: 1) strategic hallucinations in high-level planning and 2) operational errors during low-level execution on user interfaces (UI). The core insight of this paper is that high-level planning and low-level UI operations require fundamentally distinct types of knowledge. Planning demands high-level, strategy-oriented experiences, whereas operations necessitate low-level, precise instructions closely tied to specific app UIs. Motivated by these insights, we propose Mobile-Agent-RAG, a novel hierarchical multi-agent framework that innovatively integrates dual-level retrieval augmentation. At the planning stage, we introduce Manager-RAG to reduce strategic hallucinations by retrieving human-validated comprehensive task plans that provide high-level guidance. At the execution stage, we develop Operator-RAG to improve execution accuracy by retrieving the most precise low-level guidance for accurate atomic actions, aligned with the current app and subtask. To accurately deliver these knowledge types, we construct two specialized retrieval-oriented knowledge bases. Furthermore, we introduce Mobile-Eval-RAG, a challenging benchmark for evaluating such agents on realistic multi-app, long-horizon tasks. Extensive experiments demonstrate that Mobile-Agent-RAG significantly outperforms SoTA baselines, improving task completion rate by 11.0% and step efficiency by 10.2%, establishing a robust paradigm for context-aware, reliable multi-agent mobile automation.",
        "url": "http://arxiv.org/abs/2511.12254v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12254v1",
        "arxiv_id": "2511.12254v1",
        "authors": [
            "Yuxiang Zhou",
            "Jichang Li",
            "Yanhao Zhang",
            "Haonan Lu",
            "Guanbin Li"
        ],
        "submitted": "2025-11-15 15:22:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ViConBERT: Context-Gloss Aligned Vietnamese Word Embedding for Polysemous and Sense-Aware Representations",
        "abstract": "Recent advances in contextualized word embeddings have greatly improved semantic tasks such as Word Sense Disambiguation (WSD) and contextual similarity, but most progress has been limited to high-resource languages like English. Vietnamese, in contrast, still lacks robust models and evaluation resources for fine-grained semantic understanding. In this paper, we present ViConBERT, a novel framework for learning Vietnamese contextualized embeddings that integrates contrastive learning (SimCLR) and gloss-based distillation to better capture word meaning. We also introduce ViConWSD, the first large-scale synthetic dataset for evaluating semantic understanding in Vietnamese, covering both WSD and contextual similarity. Experimental results show that ViConBERT outperforms strong baselines on WSD (F1 = 0.87) and achieves competitive performance on ViCon (AP = 0.88) and ViSim-400 (Spearman's rho = 0.60), demonstrating its effectiveness in modeling both discrete senses and graded semantic relations. Our code, models, and data are available at https://github.com/tkhangg0910/ViConBERT",
        "url": "http://arxiv.org/abs/2511.12249v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12249v1",
        "arxiv_id": "2511.12249v1",
        "authors": [
            "Khang T. Huynh",
            "Dung H. Nguyen",
            "Binh T. Nguyen"
        ],
        "submitted": "2025-11-15 15:11:52",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Consistency Is the Key: Detecting Hallucinations in LLM Generated Text By Checking Inconsistencies About Key Facts",
        "abstract": "Large language models (LLMs), despite their remarkable text generation capabilities, often hallucinate and generate text that is factually incorrect and not grounded in real-world knowledge. This poses serious risks in domains like healthcare, finance, and customer support. A typical way to use LLMs is via the APIs provided by LLM vendors where there is no access to model weights or options to fine-tune the model. Existing methods to detect hallucinations in such settings where the model access is restricted or constrained by resources typically require making multiple LLM API calls, increasing latency and API cost. We introduce CONFACTCHECK, an efficient hallucination detection approach that does not leverage any external knowledge base and works on the simple intuition that responses to factual probes within the generated text should be consistent within a single LLM and across different LLMs. Rigorous empirical evaluation on multiple datasets that cover both the generation of factual texts and the open generation shows that CONFACTCHECK can detect hallucinated facts efficiently using fewer resources and achieves higher accuracy scores compared to existing baselines that operate under similar conditions. Our code is available here.",
        "url": "http://arxiv.org/abs/2511.12236v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12236v1",
        "arxiv_id": "2511.12236v1",
        "authors": [
            "Raavi Gupta",
            "Pranav Hari Panicker",
            "Sumit Bhatia",
            "Ganesh Ramakrishnan"
        ],
        "submitted": "2025-11-15 14:33:02",
        "source": "arxiv",
        "comment": "To appear at International Joint Conference on Natural Language Processing & Asia-Pacific Chapter of the Association for Computational Linguistics (IJCNLP-AACL), 2025"
    },
    {
        "title": "MME-RAG: Multi-Manager-Expert Retrieval-Augmented Generation for Fine-Grained Entity Recognition in Task-Oriented Dialogues",
        "abstract": "Fine-grained entity recognition is crucial for reasoning and decision-making in task-oriented dialogues, yet current large language models (LLMs) continue to face challenges in domain adaptation and retrieval controllability. We introduce MME-RAG, a Multi-Manager-Expert Retrieval-Augmented Generation framework that decomposes entity recognition into two coordinated stages: type-level judgment by lightweight managers and span-level extraction by specialized experts. Each expert is supported by a KeyInfo retriever that injects semantically aligned, few-shot exemplars during inference, enabling precise and domain-adaptive extraction without additional training. Experiments on CrossNER, MIT-Movie, MIT-Restaurant, and our newly constructed multi-domain customer-service dataset demonstrate that MME-RAG performs better than recent baselines in most domains. Ablation studies further show that both the hierarchical decomposition and KeyInfo-guided retrieval are key drivers of robustness and cross-domain generalization, establishing MME-RAG as a scalable and interpretable solution for adaptive dialogue understanding.",
        "url": "http://arxiv.org/abs/2511.12213v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12213v1",
        "arxiv_id": "2511.12213v1",
        "authors": [
            "Liang Xue",
            "Haoyu Liu",
            "Yajun Tian",
            "Xinyu Zhong",
            "Yang Liu"
        ],
        "submitted": "2025-11-15 13:35:55",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CriticSearch: Fine-Grained Credit Assignment for Search Agents via a Retrospective Critic",
        "abstract": "Tool-Integrated Reasoning (TIR) with search engines enables large language models to iteratively retrieve up-to-date external knowledge, enhancing adaptability and generalization in complex question-answering tasks. However, existing search agent pipelines typically depend on reinforcement learning based optimization, which often suffers from sparse outcome rewards, leading to inefficient exploration and unstable training. We introduce CriticSearch, a fine-grained credit-assignment framework that supplies dense, turn-level feedback via a retrospective critic mechanism. During training, a frozen, asymmetric critique LLM retrospectively evaluates each turn using privileged information from the full trajectory and gold answers, converting these assessments into stable, dense rewards that guide policy improvement. Experimental results across diverse multi-hop reasoning benchmarks demonstrate that CriticSearch consistently outperforms existing baselines, achieving faster convergence, improved training stability, and higher performance.",
        "url": "http://arxiv.org/abs/2511.12159v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12159v1",
        "arxiv_id": "2511.12159v1",
        "authors": [
            "Yaocheng Zhang",
            "Haohuan Huang",
            "Zijun Song",
            "Yuanheng Zhu",
            "Qichao Zhang",
            "Zijie Zhao",
            "Dongbin Zhao"
        ],
        "submitted": "2025-11-15 11:06:57",
        "source": "arxiv",
        "comment": "17 pages, 10 figures"
    },
    {
        "title": "Seeing is Believing: Rich-Context Hallucination Detection for MLLMs via Backward Visual Grounding",
        "abstract": "Multimodal Large Language Models (MLLMs) have unlocked powerful cross-modal capabilities, but still significantly suffer from hallucinations. As such, accurate detection of hallucinations in MLLMs is imperative for ensuring their reliability in practical applications. To this end, guided by the principle of \"Seeing is Believing\", we introduce VBackChecker, a novel reference-free hallucination detection framework that verifies the consistency of MLLMgenerated responses with visual inputs, by leveraging a pixellevel Grounding LLM equipped with reasoning and referring segmentation capabilities. This reference-free framework not only effectively handles rich-context scenarios, but also offers interpretability. To facilitate this, an innovative pipeline is accordingly designed for generating instruction-tuning data (R-Instruct), featuring rich-context descriptions, grounding masks, and hard negative samples. We further establish R^2 -HalBench, a new hallucination benchmark for MLLMs, which, unlike previous benchmarks, encompasses real-world, rich-context descriptions from 18 MLLMs with high-quality annotations, spanning diverse object-, attribute, and relationship-level details. VBackChecker outperforms prior complex frameworks and achieves state-of-the-art performance on R^2 -HalBench, even rivaling GPT-4o's capabilities in hallucination detection. It also surpasses prior methods in the pixel-level grounding task, achieving over a 10% improvement. All codes, data, and models are available at https://github.com/PinxueGuo/VBackChecker.",
        "url": "http://arxiv.org/abs/2511.12140v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12140v1",
        "arxiv_id": "2511.12140v1",
        "authors": [
            "Pinxue Guo",
            "Chongruo Wu",
            "Xinyu Zhou",
            "Lingyi Hong",
            "Zhaoyu Chen",
            "Jinglun Li",
            "Kaixun Jiang",
            "Sen-ching Samson Cheung",
            "Wei Zhang",
            "Wenqiang Zhang"
        ],
        "submitted": "2025-11-15 10:11:13",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "AI-Salesman: Towards Reliable Large Language Model Driven Telemarketing",
        "abstract": "Goal-driven persuasive dialogue, exemplified by applications like telemarketing, requires sophisticated multi-turn planning and strict factual faithfulness, which remains a significant challenge for even state-of-the-art Large Language Models (LLMs). A lack of task-specific data often limits previous works, and direct LLM application suffers from strategic brittleness and factual hallucination. In this paper, we first construct and release TeleSalesCorpus, the first real-world-grounded dialogue dataset for this domain. We then propose AI-Salesman, a novel framework featuring a dual-stage architecture. For the training stage, we design a Bayesian-supervised reinforcement learning algorithm that learns robust sales strategies from noisy dialogues. For the inference stage, we introduce the Dynamic Outline-Guided Agent (DOGA), which leverages a pre-built script library to provide dynamic, turn-by-turn strategic guidance. Moreover, we design a comprehensive evaluation framework that combines fine-grained metrics for key sales skills with the LLM-as-a-Judge paradigm. Experimental results demonstrate that our proposed AI-Salesman significantly outperforms baseline models in both automatic metrics and comprehensive human evaluations, showcasing its effectiveness in complex persuasive scenarios.",
        "url": "http://arxiv.org/abs/2511.12133v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12133v1",
        "arxiv_id": "2511.12133v1",
        "authors": [
            "Qingyu Zhang",
            "Chunlei Xin",
            "Xuanang Chen",
            "Yaojie Lu",
            "Hongyu Lin",
            "Xianpei Han",
            "Le Sun",
            "Qing Ye",
            "Qianlong Xie",
            "Xingxing Wang"
        ],
        "submitted": "2025-11-15 09:44:42",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "PRISM of Opinions: A Persona-Reasoned Multimodal Framework for User-centric Conversational Stance Detection",
        "abstract": "The rapid proliferation of multimodal social media content has driven research in Multimodal Conversational Stance Detection (MCSD), which aims to interpret users' attitudes toward specific targets within complex discussions. However, existing studies remain limited by: **1) pseudo-multimodality**, where visual cues appear only in source posts while comments are treated as text-only, misaligning with real-world multimodal interactions; and **2) user homogeneity**, where diverse users are treated uniformly, neglecting personal traits that shape stance expression. To address these issues, we introduce **U-MStance**, the first user-centric MCSD dataset, containing over 40k annotated comments across six real-world targets. We further propose **PRISM**, a **P**ersona-**R**easoned mult**I**modal **S**tance **M**odel for MCSD. PRISM first derives longitudinal user personas from historical posts and comments to capture individual traits, then aligns textual and visual cues within conversational context via Chain-of-Thought to bridge semantic and pragmatic gaps across modalities. Finally, a mutual task reinforcement mechanism is employed to jointly optimize stance detection and stance-aware response generation for bidirectional knowledge transfer. Experiments on U-MStance demonstrate that PRISM yields significant gains over strong baselines, underscoring the effectiveness of user-centric and context-grounded multimodal reasoning for realistic stance understanding.",
        "url": "http://arxiv.org/abs/2511.12130v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12130v1",
        "arxiv_id": "2511.12130v1",
        "authors": [
            "Bingbing Wang",
            "Zhixin Bai",
            "Zhengda Jin",
            "Zihan Wang",
            "Xintong Song",
            "Jingjie Lin",
            "Sixuan Li",
            "Jing Li",
            "Ruifeng Xu"
        ],
        "submitted": "2025-11-15 09:35:58",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "LLMLagBench: Identifying Temporal Training Boundaries in Large Language Models",
        "abstract": "Large Language Models (LLMs) are pretrained on textual data up to a specific temporal cutoff. This creates a strict knowledge boundary beyond which models cannot provide accurate information without querying external sources. More subtly, when this limitation is unknown or ignored, LLMs may inadvertently blend outdated time-sensitive information with general knowledge during reasoning tasks, potentially compromising response accuracy. We introduce LLMLagBench, an LLM freshness benchmark, as a systematic approach for identifying the earliest probable temporal boundaries of an LLM's training data by evaluating its knowledge of recent events. We then apply this benchmark to evaluate a large set of LLMs, including models with both explicitly declared and undeclared training cutoffs. The reliability of the benchmark is assessed by manual validation and comparison with publicly released information about LLM pretraining.",
        "url": "http://arxiv.org/abs/2511.12116v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12116v1",
        "arxiv_id": "2511.12116v1",
        "authors": [
            "Piotr Pzik",
            "Konrad Kaczyski",
            "Maria Szymaska",
            "Filip arnecki",
            "Zuzanna Deckert",
            "Jakub Kwiatkowski",
            "Wojciech Janowski"
        ],
        "submitted": "2025-11-15 09:08:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Continuous-time Discrete-space Diffusion Model for Recommendation",
        "abstract": "In the era of information explosion, Recommender Systems (RS) are essential for alleviating information overload and providing personalized user experiences. Recent advances in diffusion-based generative recommenders have shown promise in capturing the dynamic nature of user preferences. These approaches explore a broader range of user interests by progressively perturbing the distribution of user-item interactions and recovering potential preferences from noise, enabling nuanced behavioral understanding. However, existing diffusion-based approaches predominantly operate in continuous space through encoded graph-based historical interactions, which may compromise potential information loss and suffer from computational inefficiency. As such, we propose CDRec, a novel Continuous-time Discrete-space Diffusion Recommendation framework, which models user behavior patterns through discrete diffusion on historical interactions over continuous time. The discrete diffusion algorithm operates via discrete element operations (e.g., masking) while incorporating domain knowledge through transition matrices, producing more meaningful diffusion trajectories. Furthermore, the continuous-time formulation enables flexible adaptive sampling. To better adapt discrete diffusion models to recommendations, CDRec introduces: (1) a novel popularity-aware noise schedule that generates semantically meaningful diffusion trajectories, and (2) an efficient training framework combining consistency parameterization for fast sampling and a contrastive learning objective guided by multi-hop collaborative signals for personalized recommendation. Extensive experiments on real-world datasets demonstrate CDRec's superior performance in both recommendation accuracy and computational efficiency.",
        "url": "http://arxiv.org/abs/2511.12114v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12114v1",
        "arxiv_id": "2511.12114v1",
        "authors": [
            "Chengyi Liu",
            "Xiao Chen",
            "Shijie Wang",
            "Wenqi Fan",
            "Qing Li"
        ],
        "submitted": "2025-11-15 09:06:57",
        "source": "arxiv",
        "comment": "Accepted by WSDM 2026"
    },
    {
        "title": "Exploring Parameter-Efficient Fine-Tuning and Backtranslation for the WMT 25 General Translation Task",
        "abstract": "In this paper, we explore the effectiveness of combining fine-tuning and backtranslation on a small Japanese corpus for neural machine translation. Starting from a baseline English{\\textrightarrow}Japanese model (COMET = 0.460), we first apply backtranslation (BT) using synthetic data generated from monolingual Japanese corpora, yielding a modest increase (COMET = 0.468). Next, we fine-tune (FT) the model on a genuine small parallel dataset drawn from diverse Japanese news and literary corpora, achieving a substantial jump to COMET = 0.589 when using Mistral 7B. Finally, we integrate both backtranslation and fine-tuning{ -- }first augmenting the small dataset with BT generated examples, then adapting via FT{ -- }which further boosts performance to COMET = 0.597. These results demonstrate that, even with limited training data, the synergistic use of backtranslation and targeted fine-tuning on Japanese corpora can significantly enhance translation quality, outperforming each technique in isolation. This approach offers a lightweight yet powerful strategy for improving low-resource language pairs.",
        "url": "http://arxiv.org/abs/2511.12109v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12109v1",
        "arxiv_id": "2511.12109v1",
        "authors": [
            "Felipe Fujita",
            "Hideyuki Takada"
        ],
        "submitted": "2025-11-15 08:59:18",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "From Scaling to Structured Expressivity: Rethinking Transformers for CTR Prediction",
        "abstract": "Despite massive investments in scale, deep models for click-through rate (CTR) prediction often exhibit rapidly diminishing returns - a stark contrast to the smooth, predictable gains seen in large language models. We identify the root cause as a structural misalignment: Transformers assume sequential compositionality, while CTR data demand combinatorial reasoning over high-cardinality semantic fields. Unstructured attention spreads capacity indiscriminately, amplifying noise under extreme sparsity and breaking scalable learning. To restore alignment, we introduce the Field-Aware Transformer (FAT), which embeds field-based interaction priors into attention through decomposed content alignment and cross-field modulation. This design ensures model complexity scales with the number of fields F, not the total vocabulary size n >> F, leading to tighter generalization and, critically, observed power-law scaling in AUC as model width increases. We present the first formal scaling law for CTR models, grounded in Rademacher complexity, that explains and predicts this behavior. On large-scale benchmarks, FAT improves AUC by up to +0.51% over state-of-the-art methods. Deployed online, it delivers +2.33% CTR and +0.66% RPM. Our work establishes that effective scaling in recommendation arises not from size, but from structured expressivity-architectural coherence with data semantics.",
        "url": "http://arxiv.org/abs/2511.12081v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12081v1",
        "arxiv_id": "2511.12081v1",
        "authors": [
            "Bencheng Yan",
            "Yuejie Lei",
            "Zhiyuan Zeng",
            "Di Wang",
            "Kaiyi Lin",
            "Pengjie Wang",
            "Jian Xu",
            "Bo Zheng"
        ],
        "submitted": "2025-11-15 07:55:50",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Preference Learning from Physics-Based Feedback: Tuning Language Models to Design BCC/B2 Superalloys",
        "abstract": "We apply preference learning to the task of language model-guided design of novel structural alloys. In contrast to prior work that focuses on generating stable inorganic crystals, our approach targets the synthesizeability of a specific structural class: BCC/B2 superalloys, an underexplored family of materials with potential applications in extreme environments. Using three open-weight models (LLaMA-3.1, Gemma-2, and OLMo-2), we demonstrate that language models can be optimized for multiple design objectives using a single, unified reward signal through Direct Preference Optimization (DPO). Unlike prior approaches that rely on heuristic or human-in-the-loop feedback (costly), our reward signal is derived from thermodynamic phase calculations, offering a scientifically grounded criterion for model tuning. To our knowledge, this is the first demonstration of preference-tuning a language model using physics-grounded feedback for structural alloy design. The resulting framework is general and extensible, providing a path forward for intelligent design-space exploration across a range of physical science domains.",
        "url": "http://arxiv.org/abs/2511.12036v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12036v1",
        "arxiv_id": "2511.12036v1",
        "authors": [
            "Satanu Ghosh",
            "Collin Holgate",
            "Neal R. Brodnik",
            "Doug Downey",
            "Samantha Daly",
            "Tresa M. Pollock",
            "Samuel Carton"
        ],
        "submitted": "2025-11-15 05:08:22",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "CURE: Cultural Understanding and Reasoning Evaluation - A Framework for \"Thick\" Culture Alignment Evaluation in LLMs",
        "abstract": "Large language models (LLMs) are increasingly deployed in culturally diverse environments, yet existing evaluations of cultural competence remain limited. Existing methods focus on de-contextualized correctness or forced-choice judgments, overlooking the need for cultural understanding and reasoning required for appropriate responses. To address this gap, we introduce a set of benchmarks that, instead of directly probing abstract norms or isolated statements, present models with realistic situational contexts that require culturally grounded reasoning. In addition to the standard Exact Match metric, we introduce four complementary metrics (Coverage, Specificity, Connotation, and Coherence) to capture different dimensions of model's response quality. Empirical analysis across frontier models reveals that thin evaluation systematically overestimates cultural competence and produces unstable assessments with high variance. In contrast, thick evaluation exposes differences in reasoning depth, reduces variance, and provides more stable, interpretable signals of cultural understanding.",
        "url": "http://arxiv.org/abs/2511.12014v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12014v1",
        "arxiv_id": "2511.12014v1",
        "authors": [
            "Truong Vo",
            "Sanmi Koyejo"
        ],
        "submitted": "2025-11-15 03:39:13",
        "source": "arxiv",
        "comment": "7 pages, 5 figures"
    },
    {
        "title": "Leveraging Large Language Models for Career Mobility Analysis: A Study of Gender, Race, and Job Change Using U.S. Online Resume Profiles",
        "abstract": "We present a large-scale analysis of career mobility of college-educated U.S. workers using online resume profiles to investigate how gender, race, and job change options are associated with upward mobility. This study addresses key research questions of how the job changes affect their upward career mobility, and how the outcomes of upward career mobility differ by gender and race. We address data challenges -- such as missing demographic attributes, missing wage data, and noisy occupation labels -- through various data processing and Artificial Intelligence (AI) methods. In particular, we develop a large language models (LLMs) based occupation classification method known as FewSOC that achieves accuracy significantly higher than the original occupation labels in the resume dataset. Analysis of 228,710 career trajectories reveals that intra-firm occupation change has been found to facilitate upward mobility most strongly, followed by inter-firm occupation change and inter-firm lateral move. Women and Black college graduates experience significantly lower returns from job changes than men and White peers. Multilevel sensitivity analyses confirm that these disparities are robust to cluster-level heterogeneity and reveal additional intersectional patterns.",
        "url": "http://arxiv.org/abs/2511.12010v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12010v1",
        "arxiv_id": "2511.12010v1",
        "authors": [
            "Palakorn Achananuparp",
            "Connie Xu",
            "Yao Lu",
            "Xavier Jayaraj Siddarth Ashok",
            "Ee-Peng Lim"
        ],
        "submitted": "2025-11-15 03:26:57",
        "source": "arxiv",
        "comment": "Submitted to EPJ Data Science"
    },
    {
        "title": "ComLQ: Benchmarking Complex Logical Queries in Information Retrieval",
        "abstract": "Information retrieval (IR) systems play a critical role in navigating information overload across various applications. Existing IR benchmarks primarily focus on simple queries that are semantically analogous to single- and multi-hop relations, overlooking \\emph{complex logical queries} involving first-order logic operations such as conjunction ($\\land$), disjunction ($\\lor$), and negation ($\\lnot$). Thus, these benchmarks can not be used to sufficiently evaluate the performance of IR models on complex queries in real-world scenarios. To address this problem, we propose a novel method leveraging large language models (LLMs) to construct a new IR dataset \\textbf{ComLQ} for \\textbf{Com}plex \\textbf{L}ogical \\textbf{Q}ueries, which comprises 2,909 queries and 11,251 candidate passages. A key challenge in constructing the dataset lies in capturing the underlying logical structures within unstructured text. Therefore, by designing the subgraph-guided prompt with the subgraph indicator, an LLM (such as GPT-4o) is guided to generate queries with specific logical structures based on selected passages. All query-passage pairs in ComLQ are ensured \\emph{structure conformity} and \\emph{evidence distribution} through expert annotation. To better evaluate whether retrievers can handle queries with negation, we further propose a new evaluation metric, \\textbf{Log-Scaled Negation Consistency} (\\textbf{LSNC@$K$}). As a supplement to standard relevance-based metrics (such as nDCG and mAP), LSNC@$K$ measures whether top-$K$ retrieved passages violate negation conditions in queries. Our experimental results under zero-shot settings demonstrate existing retrieval models' limited performance on complex logical queries, especially on queries with negation, exposing their inferior capabilities of modeling exclusion.",
        "url": "http://arxiv.org/abs/2511.12004v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12004v1",
        "arxiv_id": "2511.12004v1",
        "authors": [
            "Ganlin Xu",
            "Zhitao Yin",
            "Linghao Zhang",
            "Jiaqing Liang",
            "Weijia Lu",
            "Xiaodong Zhang",
            "Zhifei Yang",
            "Sihang Jiang",
            "Deqing Yang"
        ],
        "submitted": "2025-11-15 02:58:21",
        "source": "arxiv",
        "comment": "Accepted by AAAI 2026"
    },
    {
        "title": "Critical or Compliant? The Double-Edged Sword of Reasoning in Chain-of-Thought Explanations",
        "abstract": "Explanations are often promoted as tools for transparency, but they can also foster confirmation bias; users may assume reasoning is correct whenever outputs appear acceptable. We study this double-edged role of Chain-of-Thought (CoT) explanations in multimodal moral scenarios by systematically perturbing reasoning chains and manipulating delivery tones. Specifically, we analyze reasoning errors in vision language models (VLMs) and how they impact user trust and the ability to detect errors. Our findings reveal two key effects: (1) users often equate trust with outcome agreement, sustaining reliance even when reasoning is flawed, and (2) the confident tone suppresses error detection while maintaining reliance, showing that delivery styles can override correctness. These results highlight how CoT explanations can simultaneously clarify and mislead, underscoring the need for NLP systems to provide explanations that encourage scrutiny and critical thinking rather than blind trust. All code will be released publicly.",
        "url": "http://arxiv.org/abs/2511.12001v1",
        "pdf_url": "https://arxiv.org/pdf/2511.12001v1",
        "arxiv_id": "2511.12001v1",
        "authors": [
            "Eunkyu Park",
            "Wesley Hanwen Deng",
            "Vasudha Varadarajan",
            "Mingxi Yan",
            "Gunhee Kim",
            "Maarten Sap",
            "Motahhare Eslami"
        ],
        "submitted": "2025-11-15 02:38:49",
        "source": "arxiv",
        "comment": "Under review; 16 pages, 15 figures"
    },
    {
        "title": "A Reasoning Paradigm for Named Entity Recognition",
        "abstract": "Generative LLMs typically improve Named Entity Recognition (NER) performance through instruction tuning. They excel at generating entities by semantic pattern matching but lack an explicit, verifiable reasoning mechanism. This \"cognitive shortcutting\" leads to suboptimal performance and brittle generalization, especially in zero-shot and lowresource scenarios where reasoning from limited contextual cues is crucial. To address this issue, a reasoning framework is proposed for NER, which shifts the extraction paradigm from implicit pattern matching to explicit reasoning. This framework consists of three stages: Chain of Thought (CoT) generation, CoT tuning, and reasoning enhancement. First, a dataset annotated with NER-oriented CoTs is generated, which contain task-relevant reasoning chains. Then, they are used to tune the NER model to generate coherent rationales before deriving the final answer. Finally, a reasoning enhancement stage is implemented to optimize the reasoning process using a comprehensive reward signal. This stage ensures explicit and verifiable extractions. Experiments show that ReasoningNER demonstrates impressive cognitive ability in the NER task, achieving competitive performance. In zero-shot settings, it achieves state-of-the-art (SOTA) performance, outperforming GPT-4 by 12.3 percentage points on the F1 score. Analytical results also demonstrate its great potential to advance research in reasoningoriented information extraction. Our codes are available at https://github.com/HuiResearch/ReasoningIE.",
        "url": "http://arxiv.org/abs/2511.11978v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11978v1",
        "arxiv_id": "2511.11978v1",
        "authors": [
            "Hui Huang",
            "Yanping Chen",
            "Ruizhang Huang",
            "Chuan Lin",
            "Yongbin Qin"
        ],
        "submitted": "2025-11-15 01:31:43",
        "source": "arxiv",
        "comment": "Accepted at AAAI 2026"
    },
    {
        "title": "On the Entropy Calibration of Language Models",
        "abstract": "We study the problem of entropy calibration, which asks whether a language model's entropy over generations matches its log loss on human text. Past work found that models are miscalibrated, with entropy per step increasing (and text quality decreasing) as generations grow longer. This error accumulation is a fundamental problem in autoregressive models, and the standard solution is to truncate the distribution, which improves text quality at the cost of diversity. In this paper, we ask: is miscalibration likely to improve with scale, and is it theoretically possible to calibrate without tradeoffs? To build intuition, we first study a simplified theoretical setting to characterize the scaling behavior of miscalibration with respect to dataset size. We find that the scaling behavior depends on the power law exponent of the data distribution -- in particular, for a power law exponent close to 1, the scaling exponent is close to 0, meaning that miscalibration improves very slowly with scale. Next, we measure miscalibration empirically in language models ranging from 0.5B to 70B parameters. We find that the observed scaling behavior is similar to what is predicted by the simplified setting: our fitted scaling exponents for text are close to 0, meaning that larger models accumulate error at a similar rate as smaller ones. This scaling (or, lack thereof) provides one explanation for why we sample from larger models with similar amounts of truncation as smaller models, even though the larger models are of higher quality. However, truncation is not a satisfying solution because it comes at the cost of increased log loss. In theory, is it even possible to reduce entropy while preserving log loss? We prove that it is possible, if we assume access to a black box which can fit models to predict the future entropy of text.",
        "url": "http://arxiv.org/abs/2511.11966v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11966v1",
        "arxiv_id": "2511.11966v1",
        "authors": [
            "Steven Cao",
            "Gregory Valiant",
            "Percy Liang"
        ],
        "submitted": "2025-11-15 00:33:03",
        "source": "arxiv",
        "comment": "Neurips 2025"
    },
    {
        "title": "Improving LLM's Attachment to External Knowledge In Dialogue Generation Tasks Through Entity Anonymization",
        "abstract": "Knowledge graph-based dialogue generation (KG-DG) is a challenging task requiring models to effectively incorporate external knowledge into conversational responses. While large language models (LLMs) have achieved impressive results across various NLP tasks, their ability to utilize external knowledge in KG-DG remains under-explored. We observe that LLMs often rely on internal knowledge, leading to detachment from provided knowledge graphs, even when they are given a flawlessly retrieved knowledge graph. First, we introduce LLM-KAT, an evaluation procedure for measuring knowledge attachment in generated responses. Second, we propose a simple yet effective entity anonymization technique to encourage LLMs to better leverage external knowledge. Experiments on the OpenDialKG dataset demonstrate that our approach improves LLMs' attachment on external knowledge.",
        "url": "http://arxiv.org/abs/2511.11946v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11946v1",
        "arxiv_id": "2511.11946v1",
        "authors": [
            "Hadi Sheikhi",
            "Chenyang Huang",
            "Osmar R. Zaane"
        ],
        "submitted": "2025-11-14 23:37:35",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "InData: Towards Secure Multi-Step, Tool-Based Data Analysis",
        "abstract": "Large language model agents for data analysis typically generate and execute code directly on databases. However, when applied to sensitive data, this approach poses significant security risks. To address this issue, we propose a security-motivated alternative: restrict LLMs from direct code generation and data access, and require them to interact with data exclusively through a predefined set of secure, verified tools. Although recent tool-use benchmarks exist, they primarily target tool selection and simple execution rather than the compositional, multi-step reasoning needed for complex data analysis. To reduce this gap, we introduce Indirect Data Engagement (InData), a dataset designed to assess LLMs' multi-step tool-based reasoning ability. InData includes data analysis questions at three difficulty levels--Easy, Medium, and Hard--capturing increasing reasoning complexity. We benchmark 15 open-source LLMs on InData and find that while large models (e.g., gpt-oss-120b) achieve high accuracy on Easy tasks (97.3%), performance drops sharply on Hard tasks (69.6%). These results show that current LLMs still lack robust multi-step tool-based reasoning ability. With InData, we take a step toward enabling the development and evaluation of LLMs with stronger multi-step tool-use capabilities. We will publicly release the dataset and code.",
        "url": "http://arxiv.org/abs/2511.11933v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11933v1",
        "arxiv_id": "2511.11933v1",
        "authors": [
            "Karthikeyan K",
            "Raghuveer Thirukovalluru",
            "Bhuwan Dhingra",
            "David Edwin Carlson"
        ],
        "submitted": "2025-11-14 23:15:41",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Additive Large Language Models for Semi-Structured Text",
        "abstract": "Large Language Models have advanced clinical text classification, but their opaque predictions remain a critical barrier to practical adoption in research and clinical settings where investigators and physicians need to understand which parts of a patient's record drive risk signals. To address this challenge, we introduce \\textbf{CALM}, short for \\textbf{Classification with Additive Large Language Models}, an interpretable framework for semi-structured text where inputs are composed of semantically meaningful components, such as sections of an admission note or question-answer fields from an intake form. CALM predicts outcomes as the additive sum of each component's contribution, making these contributions part of the forward computation itself and enabling faithful explanations at both the patient and population level. The additive structure also enables clear visualizations, such as component-level risk curves similar to those used in generalized additive models, making the learned relationships easier to inspect and communicate. Although CALM expects semi-structured inputs, many clinical documents already have this form, and similar structure can often be automatically extracted from free-text notes. CALM achieves performance comparable to conventional LLM classifiers while improving trust, supporting quality-assurance checks, and revealing clinically meaningful patterns during model development and auditing.",
        "url": "http://arxiv.org/abs/2511.11922v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11922v1",
        "arxiv_id": "2511.11922v1",
        "authors": [
            "Karthikeyan K",
            "Raghuveer Thirukovalluru",
            "David Carlson"
        ],
        "submitted": "2025-11-14 23:06:16",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Forgetting-MarI: LLM Unlearning via Marginal Information Regularization",
        "abstract": "As AI models are trained on ever-expanding datasets, the ability to remove the influence of specific data from trained models has become essential for privacy protection and regulatory compliance. Unlearning addresses this challenge by selectively removing parametric knowledge from the trained models without retraining from scratch, which is critical for resource-intensive models such as Large Language Models (LLMs). Existing unlearning methods often degrade model performance by removing more information than necessary when attempting to ''forget'' specific data. We introduce Forgetting-MarI, an LLM unlearning framework that provably removes only the additional (marginal) information contributed by the data to be unlearned, while preserving the information supported by the data to be retained. By penalizing marginal information, our method yields an explicit upper bound on the unlearn dataset's residual influence in the trained models, providing provable undetectability. Extensive experiments confirm that our approach outperforms current state-of-the-art unlearning methods, delivering reliable forgetting and better preserved general model performance across diverse benchmarks. This advancement represents an important step toward making AI systems more controllable and compliant with privacy and copyright regulations without compromising their effectiveness.",
        "url": "http://arxiv.org/abs/2511.11914v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11914v1",
        "arxiv_id": "2511.11914v1",
        "authors": [
            "Shizhou Xu",
            "Yuan Ni",
            "Stefan Broecker",
            "Thomas Strohmer"
        ],
        "submitted": "2025-11-14 22:48:39",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Context-Emotion Aware Therapeutic Dialogue Generation: A Multi-component Reinforcement Learning Approach to Language Models for Mental Health Support",
        "abstract": "Mental health illness represents a substantial global socioeconomic burden, with COVID-19 further exacerbating accessibility challenges and driving increased demand for telehealth mental health support. While large language models (LLMs) offer promising solutions through 24/7 availability and non-judgmental interactions, pre-trained models often lack the contextual and emotional awareness necessary for appropriate therapeutic responses. This paper investigated the application of supervised fine-tuning (SFT) and reinforcement learning (RL) techniques to enhance GPT-2's capacity for therapeutic dialogue generation. The methodology restructured input formats to enable simultaneous processing of contextual information and emotional states alongside user input, employing a multi-component reward function that aligned model outputs with professional therapist responses and annotated emotions. Results demonstrated improvements through reinforcement learning over baseline GPT-2 across multiple evaluation metrics: BLEU (0.0111), ROUGE-1 (0.1397), ROUGE-2 (0.0213), ROUGE-L (0.1317), and METEOR (0.0581). LLM evaluation confirmed high contextual relevance and professionalism, while reinforcement learning achieved 99.34% emotion accuracy compared to 66.96% for baseline GPT-2. These findings demonstrate reinforcement learning's effectiveness in developing therapeutic dialogue systems that can serve as valuable assistive tools for therapists while maintaining essential human clinical oversight.",
        "url": "http://arxiv.org/abs/2511.11884v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11884v1",
        "arxiv_id": "2511.11884v1",
        "authors": [
            "Eric Hua Qing Zhang",
            "Julia Ive"
        ],
        "submitted": "2025-11-14 21:32:10",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "ClinStructor: AI-Powered Structuring of Unstructured Clinical Texts",
        "abstract": "Clinical notes contain valuable, context-rich information, but their unstructured format introduces several challenges, including unintended biases (e.g., gender or racial bias), and poor generalization across clinical settings (e.g., models trained on one EHR system may perform poorly on another due to format differences) and poor interpretability. To address these issues, we present ClinStructor, a pipeline that leverages large language models (LLMs) to convert clinical free-text into structured, task-specific question-answer pairs prior to predictive modeling. Our method substantially enhances transparency and controllability and only leads to a modest reduction in predictive performance (a 2-3% drop in AUC), compared to direct fine-tuning, on the ICU mortality prediction task. ClinStructor lays a strong foundation for building reliable, interpretable, and generalizable machine learning models in clinical environments.",
        "url": "http://arxiv.org/abs/2511.11883v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11883v1",
        "arxiv_id": "2511.11883v1",
        "authors": [
            "Karthikeyan K",
            "Raghuveer Thirukovalluru",
            "David Carlson"
        ],
        "submitted": "2025-11-14 21:21:16",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "Better LLM Reasoning via Dual-Play",
        "abstract": "Large Language Models (LLMs) have achieved remarkable progress through Reinforcement Learning with Verifiable Rewards (RLVR), yet still rely heavily on external supervision (e.g., curated labels). Adversarial learning, particularly through self-play, offers a promising alternative that enables models to iteratively learn from themselves - thus reducing reliance on external supervision. Dual-play extends adversarial learning by assigning specialized roles to two models and training them against each other, fostering sustained competition and mutual evolution. Despite its promise, adapting dual-play training to LLMs remains limited, largely due to their susceptibility to reward hacking and training instability. In this paper, we introduce PasoDoble, a novel LLM dual-play framework. PasoDoble adversarially trains two models initialized from the same base model: a Proposer, which generates challenging questions with ground-truth answers, and a Solver, which attempts to solve them. We enrich the Proposer with knowledge from a pre-training dataset to ensure the questions' quality and diversity. To avoid reward hacking, the Proposer is rewarded for producing only valid questions that push the Solver's limit, while the Solver is rewarded for solving them correctly, and both are updated jointly. To further enhance training stability, we introduce an optional offline paradigm that decouples Proposer and Solver updates, alternately updating each for several steps while holding the other fixed. Notably, PasoDoble operates without supervision during training. Experimental results show that PasoDoble can improve the reasoning performance of LLMs. Our project page is available at https://hcy123902.github.io/PasoDoble.",
        "url": "http://arxiv.org/abs/2511.11881v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11881v1",
        "arxiv_id": "2511.11881v1",
        "authors": [
            "Zhengxin Zhang",
            "Chengyu Huang",
            "Aochong Oliver Li",
            "Claire Cardie"
        ],
        "submitted": "2025-11-14 21:19:07",
        "source": "arxiv",
        "comment": null
    },
    {
        "title": "MedPT: A Massive Medical Question Answering Dataset for Brazilian-Portuguese Speakers",
        "abstract": "While large language models (LLMs) show transformative potential in healthcare, their development remains focused on high-resource languages, creating a critical barrier for others as simple translation fails to capture unique clinical and cultural nuances, such as endemic diseases. To address this, we introduce MedPT, the first large-scale, real-world corpus for Brazilian Portuguese, comprising 384,095 authentic question-answer pairs from patient-doctor interactions. The dataset underwent a meticulous multi-stage curation protocol, using a hybrid quantitative-qualitative analysis to filter noise and contextually enrich thousands of ambiguous queries. We further augmented the corpus via LLM-driven annotation, classifying questions into seven semantic types to capture user intent. Our analysis reveals its thematic breadth (3,200 topics) and unique linguistic properties, like the natural asymmetry in patient-doctor communication. To validate its utility, we benchmark a medical specialty routing task: fine-tuning a 1.7B parameter model achieves an outstanding 94\\% F1-score on a 20-class setup. Furthermore, our qualitative error analysis shows misclassifications are not random but reflect genuine clinical ambiguities (e.g., between comorbid conditions), proving the dataset's deep semantic richness. We publicly release MedPT to foster the development of more equitable, accurate, and culturally-aware medical technologies for the Portuguese-speaking world.",
        "url": "http://arxiv.org/abs/2511.11878v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11878v1",
        "arxiv_id": "2511.11878v1",
        "authors": [
            "Fernanda Bufon Frber",
            "Iago Alves Brito",
            "Julia Soares Dollis",
            "Pedro Schindler Freire Brasil Ribeiro",
            "Rafael Teixeira Sousa",
            "Arlindo Rodrigues Galvo Filho"
        ],
        "submitted": "2025-11-14 21:13:28",
        "source": "arxiv",
        "comment": "11 pages, 3 tables, 2 figures"
    },
    {
        "title": "Identifying Imaging Follow-Up in Radiology Reports: A Comparative Analysis of Traditional ML and LLM Approaches",
        "abstract": "Large language models (LLMs) have shown considerable promise in clinical natural language processing, yet few domain-specific datasets exist to rigorously evaluate their performance on radiology tasks. In this work, we introduce an annotated corpus of 6,393 radiology reports from 586 patients, each labeled for follow-up imaging status, to support the development and benchmarking of follow-up adherence detection systems. Using this corpus, we systematically compared traditional machine-learning classifiers, including logistic regression (LR), support vector machines (SVM), Longformer, and a fully fine-tuned Llama3-8B-Instruct, with recent generative LLMs. To evaluate generative LLMs, we tested GPT-4o and the open-source GPT-OSS-20B under two configurations: a baseline (Base) and a task-optimized (Advanced) setting that focused inputs on metadata, recommendation sentences, and their surrounding context. A refined prompt for GPT-OSS-20B further improved reasoning accuracy. Performance was assessed using precision, recall, and F1 scores with 95% confidence intervals estimated via non-parametric bootstrapping. Inter-annotator agreement was high (F1 = 0.846). GPT-4o (Advanced) achieved the best performance (F1 = 0.832), followed closely by GPT-OSS-20B (Advanced; F1 = 0.828). LR and SVM also performed strongly (F1 = 0.776 and 0.775), underscoring that while LLMs approach human-level agreement through prompt optimization, interpretable and resource-efficient models remain valuable baselines.",
        "url": "http://arxiv.org/abs/2511.11867v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11867v1",
        "arxiv_id": "2511.11867v1",
        "authors": [
            "Namu Park",
            "Giridhar Kaushik Ramachandran",
            "Kevin Lybarger",
            "Fei Xia",
            "Ozlem Uzuner",
            "Meliha Yetisgen",
            "Martin Gunn"
        ],
        "submitted": "2025-11-14 20:55:44",
        "source": "arxiv",
        "comment": "Submitted to LREC 2026"
    },
    {
        "title": "Three Stage Narrative Analysis; Plot-Sentiment Breakdown, Structure Learning and Concept Detection",
        "abstract": "Story understanding and analysis have long been challenging areas within Natural Language Understanding. Automated narrative analysis requires deep computational semantic representations along with syntactic processing. Moreover, the large volume of narrative data demands automated semantic analysis and computational learning rather than manual analytical approaches. In this paper, we propose a framework that analyzes the sentiment arcs of movie scripts and performs extended analysis related to the context of the characters involved. The framework enables the extraction of high-level and low-level concepts conveyed through the narrative. Using dictionary-based sentiment analysis, our approach applies a custom lexicon built with the LabMTsimple storylab module. The custom lexicon is based on the Valence, Arousal, and Dominance scores from the NRC-VAD dataset. Furthermore, the framework advances the analysis by clustering similar sentiment plots using Wards hierarchical clustering technique. Experimental evaluation on a movie dataset shows that the resulting analysis is helpful to consumers and readers when selecting a narrative or story.",
        "url": "http://arxiv.org/abs/2511.11857v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11857v1",
        "arxiv_id": "2511.11857v1",
        "authors": [
            "Taimur Khan",
            "Ramoza Ahsan",
            "Mohib Hameed"
        ],
        "submitted": "2025-11-14 20:30:18",
        "source": "arxiv",
        "comment": "18 pages"
    },
    {
        "title": "A Multimodal Manufacturing Safety Chatbot: Knowledge Base Design, Benchmark Development, and Evaluation of Multiple RAG Approaches",
        "abstract": "Ensuring worker safety remains a critical challenge in modern manufacturing environments. Industry 5.0 reorients the prevailing manufacturing paradigm toward more human-centric operations. Using a design science research methodology, we identify three essential requirements for next-generation safety training systems: high accuracy, low latency, and low cost. We introduce a multimodal chatbot powered by large language models that meets these design requirements. The chatbot uses retrieval-augmented generation to ground its responses in curated regulatory and technical documentation. To evaluate our solution, we developed a domain-specific benchmark of expert-validated question and answer pairs for three representative machines: a Bridgeport manual mill, a Haas TL-1 CNC lathe, and a Universal Robots UR5e collaborative robot. We tested 24 RAG configurations using a full-factorial design and assessed them with automated evaluations of correctness, latency, and cost. Our top 2 configurations were then evaluated by ten industry experts and academic researchers. Our results show that retrieval strategy and model configuration have a significant impact on performance. The top configuration (selected for chatbot deployment) achieved an accuracy of 86.66%, an average latency of 10.04 seconds, and an average cost of $0.005 per query. Overall, our work provides three contributions: an open-source, domain-grounded safety training chatbot; a validated benchmark for evaluating AI-assisted safety instruction; and a systematic methodology for designing and assessing AI-enabled instructional and immersive safety training systems for Industry 5.0 environments.",
        "url": "http://arxiv.org/abs/2511.11847v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11847v1",
        "arxiv_id": "2511.11847v1",
        "authors": [
            "Ryan Singh",
            "Austin Hamilton",
            "Amanda White",
            "Michael Wise",
            "Ibrahim Yousif",
            "Arthur Carvalho",
            "Zhe Shan",
            "Reza Abrisham Baf",
            "Mohammad Mayyas",
            "Lora A. Cavuoto",
            "Fadel M. Megahed"
        ],
        "submitted": "2025-11-14 20:10:23",
        "source": "arxiv",
        "comment": "25 pages, 5 figures"
    },
    {
        "title": "Towards Autoformalization of LLM-generated Outputs for Requirement Verification",
        "abstract": "Autoformalization, the process of translating informal statements into formal logic, has gained renewed interest with the emergence of powerful Large Language Models (LLMs). While LLMs show promise in generating structured outputs from natural language (NL), such as Gherkin Scenarios from NL feature requirements, there's currently no formal method to verify if these outputs are accurate. This paper takes a preliminary step toward addressing this gap by exploring the use of a simple LLM-based autoformalizer to verify LLM-generated outputs against a small set of natural language requirements. We conducted two distinct experiments. In the first one, the autoformalizer successfully identified that two differently-worded NL requirements were logically equivalent, demonstrating the pipeline's potential for consistency checks. In the second, the autoformalizer was used to identify a logical inconsistency between a given NL requirement and an LLM-generated output, highlighting its utility as a formal verification tool. Our findings, while limited, suggest that autoformalization holds significant potential for ensuring the fidelity and logical consistency of LLM-generated outputs, laying a crucial foundation for future, more extensive studies into this novel application.",
        "url": "http://arxiv.org/abs/2511.11829v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11829v1",
        "arxiv_id": "2511.11829v1",
        "authors": [
            "Mihir Gupte",
            "Ramesh S"
        ],
        "submitted": "2025-11-14 19:45:17",
        "source": "arxiv",
        "comment": "To be submitted for publication"
    },
    {
        "title": "Scaling Open-Weight Large Language Models for Hydropower Regulatory Information Extraction: A Systematic Analysis",
        "abstract": "Information extraction from regulatory documents using large language models presents critical trade-offs between performance and computational resources. We evaluated seven open-weight models (0.6B-70B parameters) on hydropower licensing documentation to provide empirical deployment guidance.\n  Our analysis identified a pronounced 14B parameter threshold where validation methods transition from ineffective (F1 $<$ 0.15) to viable (F1 = 0.64). Consumer-deployable models achieve 64\\% F1 through appropriate validation, while smaller models plateau at 51\\%. Large-scale models approach 77\\% F1 but require enterprise infrastructure.\n  We identified systematic hallucination patterns where perfect recall indicates extraction failure rather than success in smaller models. Our findings establish the first comprehensive resource-performance mapping for open-weight information extraction in regulatory contexts, enabling evidence-based model selection.\n  These results provide immediate value for hydropower compliance while contributing insights into parameter scaling effects that generalize across information extraction tasks.",
        "url": "http://arxiv.org/abs/2511.11821v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11821v1",
        "arxiv_id": "2511.11821v1",
        "authors": [
            "Hong-Jun Yoon",
            "Faisal Ashraf",
            "Thomas A. Ruggles",
            "Debjani Singh"
        ],
        "submitted": "2025-11-14 19:23:25",
        "source": "arxiv",
        "comment": "18 pages, zero figures, Preprint submitted to Environmental Modeling and Software"
    },
    {
        "title": "Do LLMs Really Struggle at NL-FOL Translation? Revealing their Strengths via a Novel Benchmarking Strategy",
        "abstract": "Due to its expressiveness and unambiguous nature, First-Order Logic (FOL) is a powerful formalism for representing concepts expressed in natural language (NL). This is useful, e.g., for specifying and verifying desired system properties. While translating FOL into human-readable English is relatively straightforward, the inverse problem, converting NL to FOL (NL-FOL translation), has remained a longstanding challenge, for both humans and machines. Although the emergence of Large Language Models (LLMs) promised a breakthrough, recent literature provides contrasting results on their ability to perform NL-FOL translation. In this work, we provide a threefold contribution. First, we critically examine existing datasets and protocols for evaluating NL-FOL translation performance, revealing key limitations that may cause a misrepresentation of LLMs' actual capabilities. Second, to overcome these shortcomings, we propose a novel evaluation protocol explicitly designed to distinguish genuine semantic-level logical understanding from superficial pattern recognition, memorization, and dataset contamination. Third, using this new approach, we show that state-of-the-art, dialogue-oriented LLMs demonstrate strong NL-FOL translation skills and a genuine grasp of sentence-level logic, whereas embedding-centric models perform markedly worse.",
        "url": "http://arxiv.org/abs/2511.11816v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11816v1",
        "arxiv_id": "2511.11816v1",
        "authors": [
            "Andrea Brunello",
            "Luca Geatti",
            "Michele Mignani",
            "Angelo Montanari",
            "Nicola Saccomanno"
        ],
        "submitted": "2025-11-14 19:11:41",
        "source": "arxiv",
        "comment": "Full version of the paper accepted for publication at The 40th Annual AAAI Conference on Artificial Intelligence (AAAI 2026)"
    },
    {
        "title": "On the Notion that Language Models Reason",
        "abstract": "Language models (LMs) are said to be exhibiting reasoning, but what does this entail? We assess definitions of reasoning and how key papers in the field of natural language processing (NLP) use the notion and argue that the definitions provided are not consistent with how LMs are trained, process information, and generate new tokens. To illustrate this incommensurability we assume the view that transformer-based LMs implement an \\textit{implicit} finite-order Markov kernel mapping contexts to conditional token distributions. In this view, reasoning-like outputs correspond to statistical regularities and approximate statistical invariances in the learned kernel rather than the implementation of explicit logical mechanisms. This view is illustrative of the claim that LMs are \"statistical pattern matchers\"\" and not genuine reasoners and provides a perspective that clarifies why reasoning-like outputs arise in LMs without any guarantees of logical consistency. This distinction is fundamental to how epistemic uncertainty is evaluated in LMs. We invite a discussion on the importance of how the computational processes of the systems we build and analyze in NLP research are described.",
        "url": "http://arxiv.org/abs/2511.11810v1",
        "pdf_url": "https://arxiv.org/pdf/2511.11810v1",
        "arxiv_id": "2511.11810v1",
        "authors": [
            "Bertram Hjer"
        ],
        "submitted": "2025-11-14 19:04:24",
        "source": "arxiv",
        "comment": "Accepted at the 1st Workshop on Epistemic Intelligence in Machine Learning, EurIPS 2025"
    }
]