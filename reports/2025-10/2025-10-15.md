# Daily Papers Report - 2025-10-15

## üåü Top 5 Papers with Summaries

ÏÑ†Ï†ïÎêú Top 5Í∞ú ÎÖºÎ¨∏Ïóê ÎåÄÌïú ÏÉÅÏÑ∏ ÏöîÏïΩÏûÖÎãàÎã§.

### 1. DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search

- **LLM Score**: 8
- **Keyword Score**: 11
- **Authors**: Kartik Narayan, Yang Xu, Tian Cao, Kavya Nerella, Vishal M. Patel, Navid Shiee, Peter Grasch, Chao Jia, Yinfei Yang, Zhe Gan
- **URL**: <http://arxiv.org/abs/2510.12801v1>
- **Submitted**: 2025-10-14 17:59:58
- **Topic Keywords**: queries, rag, retrieval augmented generation, retrieval, web search, search
- **Reason**: This paper aligns well with your research interests in Information Retrieval, particularly in query understanding and ranking models. The focus on multimodal web search, query crafting, and real-time relevance optimization also resonates with your background in e-commerce and NLP.

#### Abstract
> Multimodal Large Language Models (MLLMs) in real-world applications require
access to external knowledge sources and must remain responsive to the dynamic
and ever-changing real-world information in order to address
information-seeking and knowledge-intensive user queries. Existing approaches,
such as retrieval augmented generation (RAG) methods, search agents, and search
equipped MLLMs, often suffer from rigid pipelines, excessive search calls, and
poorly constructed search queries, which result in inefficiencies and
suboptimal outcomes. To address these limitations, we present DeepMMSearch-R1,
the first multimodal LLM capable of performing on-demand, multi-turn web
searches and dynamically crafting queries for both image and text search tools.
Specifically, DeepMMSearch-R1 can initiate web searches based on relevant crops
of the input image making the image search more effective, and can iteratively
adapt text search queries based on retrieved information, thereby enabling
self-reflection and self-correction. Our approach relies on a two-stage
training pipeline: a cold start supervised finetuning phase followed by an
online reinforcement learning optimization. For training, we introduce
DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated
pipeline intermixed with real-world information from web search tools. This
dataset contains diverse, multi-hop queries that integrate textual and visual
information, teaching the model when to search, what to search for, which
search tool to use and how to reason over the retrieved information. We conduct
extensive experiments across a range of knowledge-intensive benchmarks to
demonstrate the superiority of our approach. Finally, we analyze the results
and provide insights that are valuable for advancing multimodal web-search.

---

### 2. Simple Projection Variants Improve ColBERT Performance

- **LLM Score**: 8
- **Keyword Score**: 7
- **Authors**: Benjamin Clavi√©, Sean Lee, Rikiya Takehi, Aamir Shakir, Makoto P. Kato
- **URL**: <http://arxiv.org/abs/2510.12327v1>
- **Submitted**: 2025-10-14 09:34:05
- **Topic Keywords**: dense retrieval, rag, retrieval
- **Reason**: This paper is highly relevant to your research interests in Information Retrieval, specifically in the context of ColBERT, a dense retrieval method. The study explores alternative projection variants to improve ColBERT's performance, which aligns with your focus on query understanding and ranking models. While the paper's domain is not explicitly e-commerce, the techniques and findings are applicable to various retrieval benchmarks, making it a useful contribution to the field.

#### Abstract
> Multi-vector dense retrieval methods like ColBERT systematically use a
single-layer linear projection to reduce the dimensionality of individual
vectors. In this study, we explore the implications of the MaxSim operator on
the gradient flows of the training of multi-vector models and show that such a
simple linear projection has inherent, if non-critical, limitations in this
setting. We then discuss the theoretical improvements that could result from
replacing this single-layer projection with well-studied alternative
feedforward linear networks (FFN), such as deeper, non-linear FFN blocks, GLU
blocks, and skip-connections, could alleviate these limitations. Through the
design and systematic evaluation of alternate projection blocks, we show that
better-designed final projections positively impact the downstream performance
of ColBERT models. We highlight that many projection variants outperform the
original linear projections, with the best-performing variants increasing
average performance on a range of retrieval benchmarks across domains by over 2
NDCG@10 points. We then conduct further exploration on the individual
parameters of these projections block in order to understand what drives this
empirical performance, highlighting the particular importance of upscaled
intermediate projections and residual connections. As part of these ablation
studies, we show that numerous suboptimal projection variants still outperform
the traditional single-layer projection across multiple benchmarks, confirming
our hypothesis. Finally, we observe that this effect is consistent across
random seeds, further confirming that replacing the linear layer of ColBERT
models is a robust, drop-in upgrade.

---

### 3. SMILE: SeMantic Ids Enhanced CoLd Item Representation for Click-through Rate Prediction in E-commerce SEarch

- **LLM Score**: 7
- **Keyword Score**: 10
- **Authors**: Qihang Zhao, Zhongbo Sun, Xiaoyang Zheng, Xian Guo, Siyuan Wang, Zihan Liang, Mingcan Peng, Ben Chen, Chenyi Lei
- **URL**: <http://arxiv.org/abs/2510.12604v1>
- **Submitted**: 2025-10-14 14:58:50
- **Topic Keywords**: click, ctr, click-through rate, recommend, commerce, e-commerce, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of e-commerce search and click-through rate prediction. However, it focuses more on item representation and collaborative information transfer rather than query understanding, ranking models, or user behavior modeling, which are your core areas of interest.

#### Abstract
> With the rise of modern search and recommendation platforms, insufficient
collaborative information of cold-start items exacerbates the Matthew effect of
existing platform items, challenging platform diversity and becoming a
longstanding issue. Existing methods align items' side content with
collaborative information to transfer collaborative signals from
high-popularity items to cold-start items. However, these methods fail to
account for the asymmetry between collaboration and content, nor the
fine-grained differences among items. To address these issues, we propose
SMILE, an item representation enhancement approach based on fused alignment of
semantic IDs. Specifically, we use RQ-OPQ encoding to quantize item content and
collaborative information, followed by a two-step alignment: RQ encoding
transfers shared collaborative signals across items, while OPQ encoding learns
differentiated information of items. Comprehensive offline experiments on
large-scale industrial datasets demonstrate superiority of SMILE, and rigorous
online A/B tests confirm statistically significant improvements: item CTR
+1.66%, buyers +1.57%, and order volume +2.17%.

---

### 4. Embedding the Teacher: Distilling vLLM Preferences for Scalable Image Retrieval

- **LLM Score**: 7
- **Keyword Score**: 8
- **Authors**: Eric He, Akash Gupta, Adian Liusie, Vatsal Raina, Piotr Molenda, Shirom Chabra, Vyas Raina
- **URL**: <http://arxiv.org/abs/2510.12014v1>
- **Submitted**: 2025-10-13 23:30:07
- **Topic Keywords**: ranking, retrieval, recommend, rank, search
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of image retrieval and query understanding. Although it focuses on a specific application (product recommendation) and uses a different approach (distilling vLLM preferences), it explores the intersection of NLP and IR, which aligns with your interests.

#### Abstract
> Text--image retrieval is necessary for applications such as product
recommendation. Embedding-based approaches like CLIP enable efficient
large-scale retrieval via vector similarity search, but they are primarily
trained on literal caption-like text--image pairs and often fail to capture
abstract or persona-driven attributes common in product recommendation
applications (e.g., ``a gift for a mother who loves gardening''). In contrast,
state-of-the-art vision--language models (vLLMs) can align text with images in
a flexible manner, but their limited context window prevents them from directly
handling retrieval over large catalogs. We propose a framework that distills
the preference rankings of a powerful vLLM into an embedding-based system,
transferring its nuanced alignment abilities while maintaining the
inference-time scalability of an embedding-based approach. Experiments on
persona-driven product recommendation tasks demonstrate that our method
significantly outperforms existing embedding-based baselines, providing an
efficient solution for personalized text--image retrieval.

---

### 5. Evaluating Retrieval-Augmented Generation Systems on Unanswerable, Uncheatable, Realistic, Multi-hop Queries

- **LLM Score**: 6
- **Keyword Score**: 7
- **Authors**: Gabrielle Kaili-May Liu, Bryan Li, Arman Cohan, William Gantt Walden, Eugene Yang
- **URL**: <http://arxiv.org/abs/2510.11956v1>
- **Submitted**: 2025-10-13 21:38:04
- **Topic Keywords**: queries, rag, retrieval
- **Reason**: This paper is somewhat related to your research interests in Information Retrieval, particularly in the context of query understanding and ranking models. However, the focus on Retrieval-Augmented Generation Systems and multi-hop queries is not a central match to your primary interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Real-world use cases often present RAG systems with complex queries for which
relevant information is missing from the corpus or is incomplete. In these
settings, RAG systems must be able to reject unanswerable, out-of-scope queries
and identify failures of retrieval and multi-hop reasoning. Despite this,
existing RAG benchmarks rarely reflect realistic task complexity for multi-hop
or out-of-scope questions, which often can be cheated via disconnected
reasoning (i.e., solved without genuine multi-hop inference) or require only
simple factual recall. This limits the ability for such benchmarks to uncover
limitations of existing RAG systems. To address this gap, we present the first
pipeline for automatic, difficulty-controlled creation of
un$\underline{c}$heatable, $\underline{r}$ealistic, $\underline{u}$nanswerable,
and $\underline{m}$ulti-hop $\underline{q}$uerie$\underline{s}$ (CRUMQs),
adaptable to any corpus and domain. We use our pipeline to create CRUMQs over
two popular RAG datasets and demonstrate its effectiveness via benchmark
experiments on leading retrieval-augmented LLMs. Results show that compared to
prior RAG benchmarks, CRUMQs are highly challenging for RAG systems and achieve
up to 81.0\% reduction in cheatability scores. More broadly, our pipeline
offers a simple way to enhance benchmark difficulty and realism and drive
development of more capable RAG systems.

---

## üìù Other Noteworthy Papers

LLMÏù¥ Ïä§ÏΩîÏñ¥ÎßÅÌñàÏßÄÎßå, Top 5Ïóê Ìè¨Ìï®ÎêòÏßÄ ÏïäÏùÄ ÎÇòÎ®∏ÏßÄ ÎÖºÎ¨∏Îì§ÏûÖÎãàÎã§.

### 6. Towards Inference-time Scaling for Continuous Space Reasoning

- **LLM Score**: 6
- **Keyword Score**: 4
- **Authors**: Minghan Wang, Thuy-Trang Vu, Ehsan Shareghi, Gholamreza Haffari
- **URL**: <http://arxiv.org/abs/2510.12167v1>
- **Submitted**: 2025-10-14 05:53:41
- **Topic Keywords**: ranking, rank
- **Reason**: This paper explores the application of established techniques in text-based reasoning to continuous space reasoning, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on continuous space reasoning and the absence of explicit mention of user behavior modeling or click models limit its direct relevance to your core research themes.

#### Abstract
> Inference-time scaling through multiple sample generation in combination with
Process- or Outcome-Reward Model (PRM or ORM) re-ranking has proven effective
for text-based reasoning in large language models. This paper investigates
whether such established techniques can be successfully adapted to reasoning in
the continuous space, using COCONUT (Hao et al. 2024) continuous space
reasoning LM as the backbone. We demonstrate the feasibility of generating
diverse reasoning paths through dropout-based sampling. Our Pass@N analysis on
the generated samples reveals the potential that could enable a significant
gain in performance akin to observed gain in the discrete space. However, we
highlight unique challenges faced for materializing this gain in the continuous
thought space. In particular, working recipes for data generation and training
PRM and ORM models in the discrete space unlocks only marginal improvements in
the continuous space. Through probing various aspects including geometric
properties and trajectory dynamics we identify the underlying reasons that
prevent effective discrimination between correct and incorrect reasoning
(essential for the functioning of PRM and ORM). Our findings reveal that
current limitations stem from the absence of key inductive biases in continuous
thought representations. We argue that the training frameworks for continuous
reasoning LMs require not only to optimize for accuracy but also to explicitly
incorporate inductive biases that could be utilized during inference-time for
discrimination of correct and incorrect thoughts.\footnote{Our code and data
will be publicly available.}

### 7. APCE: Adaptive Progressive Context Expansion for Long Context Processing

- **LLM Score**: 6
- **Keyword Score**: 4
- **Authors**: Baisub Lee, Sanghyun Byun, Mohanad Odema, Jung Guack, Jacob Song, Woo Seong Chung
- **URL**: <http://arxiv.org/abs/2510.12051v1>
- **Submitted**: 2025-10-14 01:26:36
- **Comment**: NeurIPS 2025 Workshop: ML For Systems
- **Topic Keywords**: query, search
- **Reason**: This paper explores a context-aware solution for long-context processing, specifically addressing memory efficiency and performance degradation issues in transformer models. While it touches on query understanding and ranking models, its primary focus is on optimizing the processing of long contexts, which is somewhat related to the user's interests in information retrieval and deep semantic understanding. However, the paper's emphasis on scalability and deployment systems is more aligned with the e-commerce domain, making it only somewhat relevant to the user's broader research interests.

#### Abstract
> Deploying useful Long-Context Transformer Models (LCTMs) requires addressing
two key challenges: (1) A growing memory footprint due to quadratic
self-attention and linear KV-cache scaling in memory as sequence length
increases; (2) the ContextRot phenomena where empirical evidence suggests that
transformer architecture's performance degrades with increasing context length.
Given the shared dependency on the input, a natural question arises: Can we
surgically select the most important input chunks for processing to
synergistically (a) reduce the memory footprint, and (b) mitigate the
ContextRot effects? In this paper, we answer this question in the affirmative
for long-context summarization tasks. We propose APCE as a context-aware
solution to select the most important input chunks through low-dimensional
semantic similarity matching with the current query. By directly operating on
the input, APCE decouples from strict dependency on underlying hardware or CUDA
environments, promising a compatible solution scalable to different deployment
systems. Our empirical evaluations have demonstrated superior or on-par
summarization performance for APCE compared to the full dense baseline using a
fraction (50%-70%) of the input sequence resulting in KV-cache and
self-attention memory efficiency improvements. We hope our findings inspire
further research on context-aware efficiency solutions for LCTMs geared towards
other relevant long-context tasks.

### 8. Reinforced Preference Optimization for Recommendation

- **LLM Score**: 4
- **Keyword Score**: 8
- **Authors**: Junfei Tan, Yuxin Chen, An Zhang, Junguang Jiang, Bin Liu, Ziru Xu, Han Zhu, Jian Xu, Bo Zheng, Xiang Wang
- **URL**: <http://arxiv.org/abs/2510.12211v1>
- **Submitted**: 2025-10-14 07:04:33
- **Topic Keywords**: ranking, user behavior, recommend, rank, search
- **Reason**: This paper focuses on recommender systems, which is a related area to information retrieval, but it does not directly address query understanding, ranking models, or user behavior modeling. The use of reinforcement learning and large language models is relevant to some of your interests in NLP and data mining, but the specific application to recommender systems is not a central match to your research themes.

#### Abstract
> Recent breakthroughs in large language models (LLMs) have fundamentally
shifted recommender systems from discriminative to generative paradigms, where
user behavior modeling is achieved by generating target items conditioned on
historical interactions. Yet current generative recommenders still suffer from
two core limitations: the lack of high-quality negative modeling and the
reliance on implicit rewards. Reinforcement learning with verifiable rewards
(RLVR) offers a natural solution by enabling on-policy sampling of harder
negatives and grounding optimization in explicit reward signals. However,
applying RLVR to generative recommenders remains non-trivial. Its unique
generation space often leads to invalid or repetitive items that undermine
sampling efficiency, and ranking supervision is sparse since most items receive
identical zero rewards. To address these challenges, we propose Reinforced
Preference Optimization for Recommendation (ReRe), a reinforcement-based
paradigm tailored to LLM-based recommenders, an important direction in
generative recommendation. ReRe incorporates constrained beam search to improve
sampling efficiency and diversify hard negatives, while augmenting rule-based
accuracy rewards with auxiliary ranking rewards for finer-grained supervision.
Extensive experiments on three real-world datasets demonstrate that ReRe
consistently outperforms both traditional and LLM-based recommenders in ranking
performance. Further analysis shows that ReRe not only enhances performance
across both base and SFT-initialized models but also generalizes robustly
across different backbone families and scales. Beyond empirical gains, we
systematically investigate the design space of RLVR in recommendation across
generation, sampling strategy, reward modeling, and optimization algorithm,
offering insights for future research.

### 9. Dr.LLM: Dynamic Layer Routing in LLMs

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Ahmed Heakl, Martin Gubri, Salman Khan, Sangdoo Yun, Seong Joon Oh
- **URL**: <http://arxiv.org/abs/2510.12773v1>
- **Submitted**: 2025-10-14 17:51:26
- **Comment**: 17 pages, Under submission
- **Topic Keywords**: queries, rag, search
- **Reason**: The paper focuses on improving the efficiency of Large Language Models (LLMs) through dynamic routing, which is somewhat related to the user's interest in query understanding and ranking models. However, the paper's primary focus on LLMs and their efficiency gains does not directly align with the user's core research themes in Information Retrieval and Search technologies.

#### Abstract
> Large Language Models (LLMs) process every token through all layers of a
transformer stack, causing wasted computation on simple queries and
insufficient flexibility for harder ones that need deeper reasoning.
Adaptive-depth methods can improve efficiency, but prior approaches rely on
costly inference-time search, architectural changes, or large-scale retraining,
and in practice often degrade accuracy despite efficiency gains. We introduce
Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that
equips pretrained models with lightweight per-layer routers deciding to skip,
execute, or repeat a block. Routers are trained with explicit supervision:
using Monte Carlo Tree Search (MCTS), we derive high-quality layer
configurations that preserve or improve accuracy under a compute budget. Our
design, windowed pooling for stable routing, focal loss with class balancing,
and bottleneck MLP routers, ensures robustness under class imbalance and long
sequences. On ARC (logic) and DART (math), Dr.LLM improves accuracy by up to
+3.4%p while saving 5 layers per example on average. Routers generalize to
out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA,
AGIEval) with only 0.85% accuracy drop while retaining efficiency, and
outperform prior routing methods by up to +7.7%p. Overall, Dr.LLM shows that
explicitly supervised routers retrofit frozen LLMs for budget-aware,
accuracy-driven inference without altering base weights.

### 10. Task-Aware Reduction for Scalable LLM-Database Systems

- **LLM Score**: 4
- **Keyword Score**: 6
- **Authors**: Marcus Emmanuel Barnes, Taher A. Ghaleb, Safwat Hassan
- **URL**: <http://arxiv.org/abs/2510.11813v1>
- **Submitted**: 2025-10-13 18:10:03
- **Comment**: Preprint. Accepted for presentation at the Workshop on Language
  Models and Databases (LMD), co-located with CASCON 2025 (IEEE). The final
  version will appear in IEEE Xplore
- **Topic Keywords**: query, retrieval, search
- **Reason**: The paper explores the efficiency of Large Language Models (LLMs) in data-intensive workflows, but its focus on reducing input verbosity and attention allocation does not directly align with your core research themes in Information Retrieval and Search technologies. While it touches on related topics like data systems and retrieval, the paper's primary focus on LLM efficiency and text reduction makes it somewhat relevant but not a central match for your interests.

#### Abstract
> Large Language Models (LLMs) are increasingly applied to data-intensive
workflows, from database querying to developer observability. Yet the
effectiveness of these systems is constrained by the volume, verbosity, and
noise of real-world text-rich data such as logs, telemetry, and monitoring
streams. Feeding such data directly into LLMs is costly, environmentally
unsustainable, and often misaligned with task objectives. Parallel efforts in
LLM efficiency have focused on model- or architecture-level optimizations, but
the challenge of reducing upstream input verbosity remains underexplored. In
this paper, we argue for treating the token budget of an LLM as an attention
budget and elevating task-aware text reduction as a first-class design
principle for language -- data systems. We position input-side reduction not as
compression, but as attention allocation: prioritizing information most
relevant to downstream tasks. We outline open research challenges for building
benchmarks, designing adaptive reduction pipelines, and integrating
token-budget--aware preprocessing into database and retrieval systems. Our
vision is to channel scarce attention resources toward meaningful signals in
noisy, data-intensive workflows, enabling scalable, accurate, and sustainable
LLM--data integration.

### 11. The Role of Parametric Injection-A Systematic Study of Parametric Retrieval-Augmented Generation

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Minghao Tang, Shiyu Ni, Jingtong Wu, Zengxin Han, Keping Bi
- **URL**: <http://arxiv.org/abs/2510.12668v1>
- **Submitted**: 2025-10-14 16:05:01
- **Topic Keywords**: rag, retrieval, recommend
- **Reason**: The paper explores Retrieval-Augmented Generation (RAG) and its parametric variant, PRAG, which involves encoding documents as model parameters. While it touches on the interaction between models and documents, it does not directly relate to query understanding, ranking models, or user behavior modeling, which are core areas of interest. The connection to information retrieval is indirect, but the paper's focus on deep semantic understanding and real-time relevance optimization is somewhat relevant to your broader research interests in NLP and data mining.

#### Abstract
> Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
retrieving external documents. As an emerging form of RAG, parametric
retrieval-augmented generation (PRAG) encodes documents as model parameters
(i.e., LoRA modules) and injects these representations into the model during
inference, enabling interaction between the LLM and documents at parametric
level. Compared with directly placing documents in the input context, PRAG is
more efficient and has the potential to offer deeper model-document
interaction. Despite its growing attention, the mechanism underlying parametric
injection remains poorly understood. In this work, we present a systematic
study of PRAG to clarify the role of parametric injection, showing that
parameterized documents capture only partial semantic information of documents,
and relying on them alone yields inferior performance compared to interaction
at text level. However, these parametric representations encode high-level
document information that can enhance the model's understanding of documents
within the input context. When combined parameterized documents with textual
documents, the model can leverage relevant information more effectively and
become more robust to noisy inputs, achieving better performance than either
source alone. We recommend jointly using parameterized and textual documents
and advocate for increasing the information content of parametric
representations to advance PRAG.

### 12. VISaGE: Understanding Visual Generics and Exceptions

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Stella Frank, Emily Allaway
- **URL**: <http://arxiv.org/abs/2510.12548v1>
- **Submitted**: 2025-10-14 14:13:06
- **Comment**: EMNLP 2025
- **Topic Keywords**: query, rag
- **Reason**: The paper explores the limitations of Vision Language Models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on visual generics and exceptions is not directly aligned with the user's core research themes, and the connection to real-time relevance optimization is not clear.

#### Abstract
> While Vision Language Models (VLMs) learn conceptual representations, in the
form of generalized knowledge, during training, they are typically used to
analyze individual instances. When evaluation instances are atypical, this
paradigm results in tension between two priors in the model. The first is a
pragmatic prior that the textual and visual input are both relevant, arising
from VLM finetuning on congruent inputs; the second is a semantic prior that
the conceptual representation is generally true for instances of the category.
In order to understand how VLMs trade off these priors, we introduce a new
evaluation dataset, VISaGE, consisting of both typical and exceptional images.
In carefully balanced experiments, we show that conceptual understanding
degrades when the assumption of congruency underlying the pragmatic prior is
violated with incongruent images. This effect is stronger than the effect of
the semantic prior when querying about individual instances.

### 13. SMEC: Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Biao Zhang, Lixin Chen, Tong Liu, Bo Zheng
- **URL**: <http://arxiv.org/abs/2510.12474v1>
- **Submitted**: 2025-10-14 13:04:22
- **Comment**: Accepted by EMNLP2025
- **Topic Keywords**: rag, retrieval, search
- **Reason**: This paper focuses on embedding compression for large language models, which is somewhat related to information retrieval and search technologies. However, the primary focus is on representation learning and compression, rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest. The paper's relevance to IR is indirect, but it may be of interest for researchers exploring the intersection of NLP and IR.

#### Abstract
> Large language models (LLMs) generate high-dimensional embeddings that
capture rich semantic and syntactic information. However, high-dimensional
embeddings exacerbate computational complexity and storage requirements,
thereby hindering practical deployment. To address these challenges, we propose
a novel training framework named Sequential Matryoshka Embedding Compression
(SMEC). This framework introduces the Sequential Matryoshka Representation
Learning(SMRL) method to mitigate gradient variance during training, the
Adaptive Dimension Selection (ADS) module to reduce information degradation
during dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module
to enhance unsupervised learning between high- and low-dimensional embeddings.
Experiments on image, text, and multimodal datasets demonstrate that SMEC
achieves significant dimensionality reduction while maintaining performance.
For instance, on the BEIR dataset, our approach improves the performance of
compressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points
compared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.

### 14. Causal Inspired Multi Modal Recommendation

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Jie Yang, Chenyang Gu, Zixuan Liu
- **URL**: <http://arxiv.org/abs/2510.12325v1>
- **Submitted**: 2025-10-14 09:29:07
- **Topic Keywords**: click, recommend, commerce, e-commerce
- **Reason**: The paper focuses on multimodal recommendation systems, which is somewhat related to information retrieval, but its primary focus on recommender systems and causal analysis is not a central match to the user's interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Multimodal recommender systems enhance personalized recommendations in
e-commerce and online advertising by integrating visual, textual, and user-item
interaction data. However, existing methods often overlook two critical biases:
(i) modal confounding, where latent factors (e.g., brand style or product
category) simultaneously drive multiple modalities and influence user
preference, leading to spurious feature-preference associations; (ii)
interaction bias, where genuine user preferences are mixed with noise from
exposure effects and accidental clicks. To address these challenges, we propose
a Causal-inspired multimodal Recommendation framework. Specifically, we
introduce a dual-channel cross-modal diffusion module to identify hidden modal
confounders, utilize back-door adjustment with hierarchical matching and
vector-quantized codebooks to block confounding paths, and apply front-door
adjustment combined with causal topology reconstruction to build a deconfounded
causal subgraph. Extensive experiments on three real-world e-commerce datasets
demonstrate that our method significantly outperforms state-of-the-art
baselines while maintaining strong interpretability.

### 15. DSAS: A Universal Plug-and-Play Framework for Attention Optimization in Multi-Document Question Answering

- **LLM Score**: 4
- **Keyword Score**: 5
- **Authors**: Jiakai Li, Rongzheng Wang, Yizhuo Ma, Shuang Liang, Guangchun Luo, Ke Qin
- **URL**: <http://arxiv.org/abs/2510.12251v1>
- **Submitted**: 2025-10-14 08:01:59
- **Comment**: 27 pages, has been accepted by NeurIPS 2025
- **Topic Keywords**: relevance, rag
- **Reason**: This paper proposes a framework for attention optimization in multi-document question answering, which is somewhat related to information retrieval. However, the focus on large language models and multi-document question answering is not directly aligned with the user's core research themes in query understanding, ranking models, and user behavior modeling.

#### Abstract
> While large language models (LLMs) show considerable promise across various
fields, they have notable limitations in handling multi-document question
answering (Multi-doc QA) tasks. The first challenge is long-range dependency
modeling, where LLMs struggle to focus on key information in long texts, which
weakens important semantic connections. Second, most LLMs suffer from the
''lost-in-the-middle'' issue, where they have difficulty processing information
in the middle of long inputs. Current solutions either truncate global
dependencies or demand costly finetuning, ultimately lacking a universal and
simple solution for these challenges. To resolve these limitations, we propose
Dual-Stage Adaptive Sharpening (DSAS) containing two modules. (i) The
Contextual Gate Weighting (CGW) module alleviates ''lost-in-the-middle'' by
assessing paragraph relevance through layer-wise attention tracking and
position-aware weighting. (ii) The Reciprocal Attention Suppression (RAS)
module enhances focus on critical paragraphs by suppressing information
exchange between key and irrelevant texts, thus mitigating the limitations in
long-range dependency modeling. Notably, DSAS functions as a plug-and-play
solution requiring no architectural modifications or extra training parameters.
Extensive experiments on four benchmarks demonstrate DSAS's efficacy across
mainstream LLMs (Llama, Qwen, Mistral, and Deepseek), with an average F1-score
improvement of 4.2% in Multi-doc QA tasks on Llama-3.1-8B-Instruct and
Qwen2.5-14B-Instruct. Ablation studies confirm the essential contributions of
both the CGW and RAS modules. In addition, detailed discussions in the Appendix
further validate the robustness and scalability of DSAS.

### 16. SAIL-Embedding Technical Report: Omni-modal Embedding Foundation Model

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Lin Lin, Jiefeng Long, Zhihe Wan, Yuchi Wang, Dingkang Yang, Shuang Yang, Yueyang Yao, Xu Chen, Zirui Guo, Shengqiang Li, Weiran Li, Hanyu Li, Yaling Mou, Yan Qiu, Haiyang Yu, Xiao Liang, Hongsheng Li, Chao Feng
- **URL**: <http://arxiv.org/abs/2510.12709v2>
- **Submitted**: 2025-10-14 16:43:22
- **Comment**: Technical Report
- **Topic Keywords**: retrieval, recommend, rank
- **Reason**: The paper focuses on multimodal embedding models, which is somewhat related to information retrieval and search technologies. However, the primary focus on recommendation systems and real-world applications in the e-commerce domain is not a central match to the user's core research themes.

#### Abstract
> Multimodal embedding models aim to yield informative unified representations
that empower diverse cross-modal tasks. Despite promising developments in the
evolution from CLIP-based dual-tower architectures to large vision-language
models, prior works still face unavoidable challenges in real-world
applications and business scenarios, such as the limited modality support,
unstable training mechanisms, and industrial domain gaps. In this work, we
introduce SAIL-Embedding, an omni-modal embedding foundation model that
addresses these issues through tailored training strategies and architectural
design. In the optimization procedure, we propose a multi-stage training scheme
to boost the multifaceted effectiveness of representation learning.
Specifically, the content-aware progressive training aims to enhance the
model's adaptability to diverse downstream tasks and master enriched
cross-modal proficiency. The collaboration-aware recommendation enhancement
training further adapts multimodal representations for recommendation scenarios
by distilling knowledge from sequence-to-item and ID-to-item embeddings while
mining user historical interests. Concurrently, we develop the stochastic
specialization and dataset-driven pattern matching to strengthen model training
flexibility and generalizability. Experimental results show that SAIL-Embedding
achieves SOTA performance compared to other methods in different retrieval
tasks. In online experiments across various real-world scenarios integrated
with our model, we observe a significant increase in Lifetime (LT), which is a
crucial indicator for the recommendation experience. For instance, the model
delivers the 7-day LT gain of +0.5% in the Douyin-Selected scenario. For the
Douyin feed rank model, the match features produced by SAIL-Embedding yield a
+0.1% AUC gain.

### 17. Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Linfeng Gao, Baolong Bi, Zheng Yuan, Le Wang, Zerui Chen, Zhimin Wei, Shenghua Liu, Qinggang Zhang, Jinsong Su
- **URL**: <http://arxiv.org/abs/2510.12460v1>
- **Submitted**: 2025-10-14 12:48:24
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper explores Retrieval-Augmented Generation (RAG) and proposes a framework to improve contextual faithfulness in Large Language Models (LLMs). While it touches on aspects of information retrieval and knowledge integration, its primary focus is on improving the faithfulness of LLMs, which is somewhat related to your interests in query understanding and ranking models, but not a central match.

#### Abstract
> Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to
enhance the factuality of Large Language Models (LLMs). However, existing RAG
systems often suffer from an unfaithfulness issue, where the model's response
contradicts evidence from the retrieved context. Existing approaches to
improving contextual faithfulness largely rely on external interventions, such
as prompt engineering, decoding constraints, or reward-based fine-tuning. These
works treat the LLM as a black box and overlook a crucial question: how does
the LLM internally integrate retrieved evidence with its parametric memory,
particularly under knowledge conflicts? To address this gap, we conduct a
probing-based analysis of hidden-state representations in LLMs and observe
three findings: knowledge integration occurs hierarchically, conflicts manifest
as latent signals at the sentence level, and irrelevant context is often
amplified when aligned with parametric knowledge. Building on these findings,
we propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a
framework that (i) decomposes context into fine-grained sentence-level
knowledge, (ii) employs hidden-state probing to localize conflicting knowledge,
and (iii) introduces conflict-aware fine-tuning to guide the model to
accurately integrate retrieved evidence. Extensive experiments across three
benchmarks demonstrate that CLEAR substantially improves both accuracy and
contextual faithfulness, consistently outperforming strong baselines under
diverse conflict conditions. The related resources are available at
https://github.com/LinfengGao/CLEAR.

### 18. PRoH: Dynamic Planning and Reasoning over Knowledge Hypergraphs for Retrieval-Augmented Generation

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Xiangjun Zai, Xingyu Tan, Xiaoyang Wang, Qing Liu, Xiwei Xu, Wenjie Zhang
- **URL**: <http://arxiv.org/abs/2510.12434v1>
- **Submitted**: 2025-10-14 12:13:23
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper proposes a framework for retrieval-augmented generation, which is related to information retrieval and natural language processing. However, the focus on knowledge hypergraphs and question answering does not directly align with the user's primary interests in query understanding, ranking models, and user behavior modeling. While it touches on semantic understanding, it is more focused on generation and reasoning rather than retrieval and search.

#### Abstract
> Knowledge Hypergraphs (KHs) have recently emerged as a knowledge
representation for retrieval-augmented generation (RAG), offering a paradigm to
model multi-entity relations into a structured form. However, existing KH-based
RAG methods suffer from three major limitations: static retrieval planning,
non-adaptive retrieval execution, and superficial use of KH structure and
semantics, which constrain their ability to perform effective multi-hop
question answering. To overcome these limitations, we propose PRoH, a dynamic
Planning and Reasoning over Knowledge Hypergraphs framework. PRoH incorporates
three core innovations: (i) a context-aware planning module that sketches the
local KH neighborhood to guide structurally grounded reasoning plan generation;
(ii) a structured question decomposition process that organizes subquestions as
a dynamically evolving Directed Acyclic Graph (DAG) to enable adaptive,
multi-trajectory exploration; and (iii) an Entity-Weighted Overlap (EWO)-guided
reasoning path retrieval algorithm that prioritizes semantically coherent
hyperedge traversals. Experiments across multiple domains demonstrate that PRoH
achieves state-of-the-art performance, surpassing the prior SOTA model
HyperGraphRAG by an average of 19.73% in F1 and 8.41% in Generation Evaluation
(G-E) score, while maintaining strong robustness in long-range multi-hop
reasoning tasks.

### 19. Multi-stage Prompt Refinement for Mitigating Hallucinations in Large Language Models

- **LLM Score**: 4
- **Keyword Score**: 4
- **Authors**: Jung-Woo Shim, Yeong-Joon Ju, Ji-Hoon Park, Seong-Whan Lee
- **URL**: <http://arxiv.org/abs/2510.12032v1>
- **Submitted**: 2025-10-14 00:31:36
- **Comment**: 22 pages, 6 figures
- **Topic Keywords**: ranking, rank
- **Reason**: The paper focuses on large language models and hallucinations, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the primary focus on natural language generation and post-hoc mitigation frameworks does not directly align with the user's core research themes in Information Retrieval and Search technologies.

#### Abstract
> Recent advancements in large language models (LLMs) have shown strong
performance in natural language understanding and generation tasks. However,
LLMs continue to encounter challenges with hallucinations, where models
generate plausible but incorrect information. While several factors contribute
to hallucinations, the impact of ill-formed prompts, prompts with ambiguous
wording, incorrect grammar, or incomplete information, was relatively under
explored. To address this, we introduce Multi-stage Prompt Refinement (MPR), a
framework designed to systematically improve these ill-formed prompts across
multiple stages. Each stage addresses specific errors such as punctuation,
typographical mistakes, and misuse of key terms, using small language models
(SLMs) fine-tuned for these tasks. MPR iteratively enhances the clarity of
prompts with additional context and employs a self-reflection mechanism with
ranking to prioritize the most relevant input. Experimental results on
hallucination benchmarks show that prompts refined by MPR achieve over an 85~\%
win rate compared to their original forms, demonstrating its effectiveness in
reducing hallucinations and improving LLM output accuracy. Interestingly, we
reveal that MPR can be combined with existing post-hoc hallucination mitigation
frameworks, further enhancing its versatility. MPR provides a lightweight and
adaptable solution for enhancing LLM reliability across various domains.

### 20. CTRL-Rec: Controlling Recommender Systems With Natural Language

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Micah Carroll, Adeline Foote, Kevin Feng, Marcus Williams, Anca Dragan, W. Bradley Knox, Smitha Milli
- **URL**: <http://arxiv.org/abs/2510.12742v1>
- **Submitted**: 2025-10-14 17:20:04
- **Topic Keywords**: ctr, recommend
- **Reason**: The paper CTRL-Rec: Controlling Recommender Systems With Natural Language is somewhat related to the user's research interests in Information Retrieval and Natural Language Processing. Although it focuses on recommender systems, it leverages large language models and natural language requests, which aligns with the user's interests in query understanding and deep semantic understanding. However, the primary focus on recommender systems and user control limits its relevance to the user's core research themes.

#### Abstract
> When users are dissatisfied with recommendations from a recommender system,
they often lack fine-grained controls for changing them. Large language models
(LLMs) offer a solution by allowing users to guide their recommendations
through natural language requests (e.g., "I want to see respectful posts with a
different perspective than mine"). We propose a method, CTRL-Rec, that allows
for natural language control of traditional recommender systems in real-time
with computational efficiency. Specifically, at training time, we use an LLM to
simulate whether users would approve of items based on their language requests,
and we train embedding models that approximate such simulated judgments. We
then integrate these user-request-based predictions into the standard weighting
of signals that traditional recommender systems optimize. At deployment time,
we require only a single LLM embedding computation per user request, allowing
for real-time control of recommendations. In experiments with the MovieLens
dataset, our method consistently allows for fine-grained control across a
diversity of requests. In a study with 19 Letterboxd users, we find that
CTRL-Rec was positively received by users and significantly enhanced users'
sense of control and satisfaction with recommendations compared to traditional
controls.

### 21. Teaching Language Models to Faithfully Express their Uncertainty

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Bryan Eikema, Evgenia Ilia, Jos√© G. C. de Souza, Chrysoula Zerva, Wilker Aziz
- **URL**: <http://arxiv.org/abs/2510.12587v1>
- **Submitted**: 2025-10-14 14:42:40
- **Topic Keywords**: queries
- **Reason**: The paper explores uncertainty expression in language models, which is somewhat related to query understanding and ranking models in Information Retrieval. However, the focus on language models and uncertainty expression in NLP is not a central match to the user's primary research interests in IR and search technologies.

#### Abstract
> Large language models (LLMs) often miscommunicate their uncertainty: repeated
queries can produce divergent answers, yet generated responses are typically
unhedged or hedged in ways that do not reflect this variability. This conveys
unfaithful information about the uncertain state of the LLMs' knowledge,
creating a faithfulness gap that affects even strong LLMs. We introduce
Faithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches
instruction-tuned LLMs to express uncertainty faithfully without altering their
underlying answer distribution. We construct training data by augmenting model
samples with uncertainty hedges (i.e. verbal cues such as 'possibly' or
'likely') aligned with sample consistency, requiring no supervision beyond the
model and a set of prompts. We evaluate FUT on open-domain question answering
(QA) across multiple models and datasets. Our results show that FUT
substantially reduces the faithfulness gap, while preserving QA accuracy and
introducing minimal semantic distribution shift. Further analyses demonstrate
robustness across decoding strategies, choice of hedgers, and other forms of
uncertainty expression (i.e. numerical). These findings establish FUT as a
simple and effective way to teach LLMs to communicate uncertainty faithfully.

### 22. Leveraging Language Semantics for Collaborative Filtering with TextGCN and TextGCN-MLP: Zero-Shot vs In-Domain Performance

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Andrei Chernov, Haroon Wahab, Oleg Novitskij
- **URL**: <http://arxiv.org/abs/2510.12461v1>
- **Submitted**: 2025-10-14 12:50:11
- **Topic Keywords**: rag, recommend
- **Reason**: This paper explores recommender systems, specifically leveraging language semantics for collaborative filtering. While it touches on NLP and deep semantic understanding, its primary focus on recommender systems and zero-shot vs in-domain performance makes it somewhat related to your interests, but not a central match.

#### Abstract
> In recent years, various approaches have been proposed to leverage large
language models (LLMs) for incorporating textual information about items into
recommender systems. Existing methods primarily focus on either fine-tuning
LLMs to generate recommendations or integrating LLM-based embeddings into
downstream models. In this work, we follow the latter direction and propose
\textbf{TextGCN}, which applies parameter-free graph convolution layers
directly over LLM-based item-title embeddings, instead of learning ID-based
embeddings as in traditional methods. By combining language semantics with
graph message passing, this architecture achieves state-of-the-art zero-shot
performance, significantly outperforming prior approaches. Furthermore, we
introduce \textbf{TextGCN-MLP}, which extends TextGCN with a trainable
multilayer perceptron trained using a contrastive loss, achieving
state-of-the-art in-domain performance on recommendation benchmarks. However,
the zero-shot performance of TextGCN-MLP remains lower than that of TextGCN,
highlighting the trade-off between in-domain specialization and zero-shot
generalization. We release our code on github at
\href{https://github.com/ChernovAndrey/TFCE}{github.com/ChernovAndrey/TFCE}.

### 23. A Survey on Parallel Reasoning

- **LLM Score**: 4
- **Keyword Score**: 3
- **Authors**: Ziqi Wang, Boye Niu, Zipeng Gao, Zhi Zheng, Tong Xu, Linghui Meng, Zhongli Li, Jing Liu, Yilong Chen, Chen Zhu, Hua Wu, Haifeng Wang, Enhong Chen
- **URL**: <http://arxiv.org/abs/2510.12164v1>
- **Submitted**: 2025-10-14 05:42:19
- **Topic Keywords**: rag, search
- **Reason**: The paper discusses parallel reasoning, a new inference paradigm that enhances reasoning robustness, but it does not directly relate to information retrieval, search technologies, or query understanding. While it touches on Large Language Models (LLMs), which are relevant to NLP, the connection is not strong enough to make it a central match for your research interests.

#### Abstract
> With the increasing capabilities of Large Language Models (LLMs), parallel
reasoning has emerged as a new inference paradigm that enhances reasoning
robustness by concurrently exploring multiple lines of thought before
converging on a final answer. It has become a significant trend to explore
parallel reasoning to overcome the fragility of standard sequential methods and
improve practical performance. In this paper, we aim to survey and summarize
the progress and challenges of parallel reasoning. We first present a formal
definition of parallel reasoning and clarify its distinction from related
concepts like Chain-of-Thought. Then, we organize and discuss advanced
techniques based on a novel taxonomy, including non-interactive reasoning,
interactive reasoning, and efficiency-focused decoding strategies.
Additionally, we explore various application scenarios, such as solving complex
problems and enhancing the reliability of LLM outputs.Finally, we highlight the
core challenges of parallel reasoning and suggest potential directions for
future research. We hope that our work can provide a useful roadmap for
beginners and encourage more research on improving parallel reasoning methods.
Related source can be avaliable in
https://github.com/PPPP-kaqiu/Awesome-Parallel-Reasoning.

### 24. Who is a Better Matchmaker? Human vs. Algorithmic Judge Assignment in a High-Stakes Startup Competition

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Sarina Xi, Orelia Pi, Miaomiao Zhang, Becca Xiong, Jacqueline Ng Lane, Nihar B. Shah
- **URL**: <http://arxiv.org/abs/2510.12692v1>
- **Submitted**: 2025-10-14 16:25:09
- **Comment**: 17 Pages, 2 figures
- **Topic Keywords**: rag
- **Reason**: This paper explores the application of AI in a high-stakes decision-making task, specifically judge assignment in a startup competition. While it touches on semantic understanding and domain expertise, its focus is on algorithmic vs. human performance rather than query understanding, ranking models, or user behavior modeling, which are core areas of interest for your research.

#### Abstract
> There is growing interest in applying artificial intelligence (AI) to
automate and support complex decision-making tasks. However, it remains unclear
how algorithms compare to human judgment in contexts requiring semantic
understanding and domain expertise. We examine this in the context of the judge
assignment problem, matching submissions to suitably qualified judges.
Specifically, we tackled this problem at the Harvard President's Innovation
Challenge, the university's premier venture competition awarding over \$500,000
to student and alumni startups. This represents a real-world environment where
high-quality judge assignment is essential. We developed an AI-based
judge-assignment algorithm, Hybrid Lexical-Semantic Similarity Ensemble (HLSE),
and deployed it at the competition. We then evaluated its performance against
human expert assignments using blinded match-quality scores from judges on
$309$ judge-venture pairs. Using a Mann-Whitney U statistic based test, we
found no statistically significant difference in assignment quality between the
two approaches ($AUC=0.48, p=0.40$); on average, algorithmic matches are rated
$3.90$ and manual matches $3.94$ on a 5-point scale, where 5 indicates an
excellent match. Furthermore, manual assignments that previously required a
full week could be automated in several hours by the algorithm during
deployment. These results demonstrate that HLSE achieves human-expert-level
matching quality while offering greater scalability and efficiency,
underscoring the potential of AI-driven solutions to support and enhance human
decision-making for judge assignment in high-stakes settings.

### 25. Reasoning in the Dark: Interleaved Vision-Text Reasoning in Latent Space

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Chao Chen, Zhixin Ma, Yongqi Li, Yupeng Hu, Yinwei Wei, Wenjie Li, Liqiang Nie
- **URL**: <http://arxiv.org/abs/2510.12603v1>
- **Submitted**: 2025-10-14 14:58:25
- **Topic Keywords**: rag
- **Reason**: The paper explores multimodal reasoning in latent space, which is somewhat related to information retrieval and NLP. However, the focus on vision-text reasoning and multimodal representation is not directly aligned with the user's primary interests in query understanding, ranking models, and user behavior modeling. While the paper's emphasis on real-time relevance optimization is relevant, the connection is not strong enough to warrant a higher score.

#### Abstract
> Multimodal reasoning aims to enhance the capabilities of MLLMs by
incorporating intermediate reasoning steps before reaching the final answer. It
has evolved from text-only reasoning to the integration of visual information,
enabling the thought process to be conveyed through both images and text.
Despite its effectiveness, current multimodal reasoning methods depend on
explicit reasoning steps that require labor-intensive vision-text annotations
and inherently introduce significant inference latency. To address these
issues, we introduce multimodal latent reasoning with the advantages of
multimodal representation, reduced annotation, and inference efficiency. To
facilicate it, we propose Interleaved Vision-Text Latent Reasoning (IVT-LR),
which injects both visual and textual information in the reasoning process
within the latent space. Specifically, IVT-LR represents each reasoning step by
combining two implicit parts: latent text (the hidden states from the previous
step) and latent vision (a set of selected image embeddings). We further
introduce a progressive multi-stage training strategy to enable MLLMs to
perform the above multimodal latent reasoning steps. Experiments on M3CoT and
ScienceQA demonstrate that our IVT-LR method achieves an average performance
increase of 5.45% in accuracy, while simultaneously achieving a speed increase
of over 5 times compared to existing approaches. Code available at
https://github.com/FYYDCC/IVT-LR.

### 26. Evolution of meta's llama models and parameter-efficient fine-tuning of large language models: a survey

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Abdulhady Abas Abdullah, Arkaitz Zubiaga, Seyedali Mirjalili, Amir H. Gandomi, Fatemeh Daneshfar, Mohammadsadra Amini, Alan Salam Mohammed, Hadi Veisi
- **URL**: <http://arxiv.org/abs/2510.12178v1>
- **Submitted**: 2025-10-14 06:12:44
- **Topic Keywords**: rank, search
- **Reason**: The paper is somewhat related to the user's interests in Natural Language Processing (NLP) and large language models, but it does not directly address query understanding, ranking models, or user behavior modeling, which are core areas of interest. The focus on parameter-efficient fine-tuning of large language models is relevant, but the context is more aligned with NLP and ML research rather than information retrieval.

#### Abstract
> This review surveys the rapid evolution of Meta AI's LLaMA (Large Language
Model Meta AI) series - from LLaMA 1 through LLaMA 4 and the specialized
parameter-efficient fine-tuning (PEFT) methods developed for these models. We
first describe the LLaMA family of foundation models (7B-65B to 288B
parameters), their architectures (including native multimodal and
Mixtureof-Experts variants), and key performance characteristics. We then
describe and discuss the concept of PEFT, which adapts large pre-trained models
by updating only a small subset of parameters, and review five PEFT methods
that have been applied to LLaMA: LoRA (Low-Rank Adaptation), LLaMA-Adapter V1
and V2, LLaMA-Excitor, and QLoRA (Quantized LoRA). We discuss each method's
mechanism, parameter savings, and example application to LLaMA (e.g.,
instruction tuning, multimodal tasks). We provide structured discussion and
analysis of model and adapter architectures, parameter counts, and benchmark
results (including examples where fine-tuned LLaMA models outperform larger
baselines). Finally, we examine real-world use cases where LLaMA-based models
and PEFT have been successfully applied (e.g., legal and medical domains), and
we discuss ongoing challenges and future research directions (such as scaling
to even larger contexts and improving robustness). This survey paper provides a
one-stop resource for ML researchers and practitioners interested in LLaMA
models and efficient fine-tuning strategies.

### 27. SAGE: A Top-Down Bottom-Up Knowledge-Grounded User Simulator for Multi-turn AGent Evaluation

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Ryan Shea, Yunan Lu, Liang Qiu, Zhou Yu
- **URL**: <http://arxiv.org/abs/2510.11997v1>
- **Submitted**: 2025-10-13 22:52:17
- **Topic Keywords**: user behavior
- **Reason**: This paper is somewhat related to information retrieval, but its focus on user simulation for multi-turn agent evaluation and business contexts is not directly aligned with your core research themes. While it touches on user behavior modeling, it's more focused on agent evaluation and business logic, which may be of interest in a broader sense, but not a central match for your research interests.

#### Abstract
> Evaluating multi-turn interactive agents is challenging due to the need for
human assessment. Evaluation with simulated users has been introduced as an
alternative, however existing approaches typically model generic users and
overlook the domain-specific principles required to capture realistic behavior.
We propose SAGE, a novel user Simulation framework for multi-turn AGent
Evaluation that integrates knowledge from business contexts. SAGE incorporates
top-down knowledge rooted in business logic, such as ideal customer profiles,
grounding user behavior in realistic customer personas. We further integrate
bottom-up knowledge taken from business agent infrastructure (e.g., product
catalogs, FAQs, and knowledge bases), allowing the simulator to generate
interactions that reflect users' information needs and expectations in a
company's target market. Through empirical evaluation, we find that this
approach produces interactions that are more realistic and diverse, while also
identifying up to 33% more agent errors, highlighting its effectiveness as an
evaluation tool to support bug-finding and iterative agent improvement.

### 28. R-WoM: Retrieval-augmented World Model For Computer-use Agents

- **LLM Score**: 4
- **Keyword Score**: 2
- **Authors**: Kai Mei, Jiang Guo, Shuaichen Chang, Mingwen Dong, Dongkyu Lee, Xing Niu, Jiarong Jiang
- **URL**: <http://arxiv.org/abs/2510.11892v1>
- **Submitted**: 2025-10-13 19:52:04
- **Topic Keywords**: retrieval
- **Reason**: The paper discusses the limitations of Large Language Models (LLMs) in world modeling and proposes a retrieval-augmented approach to improve their performance. While it touches on the concept of simulation and prediction, which are related to ranking models, it does not directly address query understanding, ranking models, or user behavior modeling. The focus on world modeling and agent decision-making is somewhat relevant to information retrieval, but not a central match.

#### Abstract
> Large Language Models (LLMs) can serve as world models to enhance agent
decision-making in digital environments by simulating future states and
predicting action outcomes, potentially eliminating costly trial-and-error
exploration. However, this capability is fundamentally limited by LLMs'
tendency toward hallucination and their reliance on static training knowledge,
which can lead to compounding errors that inhibit long-horizon simulations. To
systematically investigate whether LLMs are appropriate for world modeling, we
probe two core capabilities of world models--future state prediction and reward
estimation--through three tasks: next-state identification, full-procedure
planning alignment, and milestone transition recognition. Our analysis shows
that while LLMs effectively capture immediate next states and identify
meaningful state transitions, their performance rapidly degrades in
full-procedure planning. This highlights LLMs' limitations in reliably modeling
environment dynamics over long horizons. To address these limitations, we
propose the Retrieval-augmented World Model (R-WoM), which grounds LLM
simulations by incorporating factual, up-to-date knowledge retrieved from
external tutorials. Experiments show that R-WoM achieves substantial
improvements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to
baselines, with particular advantages in longer-horizon simulations.

### 29. Chinese ModernBERT with Whole-Word Masking

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Zeyu Zhao, Ningtao Wang, Xing Fu, Yu Cheng
- **URL**: <http://arxiv.org/abs/2510.12285v1>
- **Submitted**: 2025-10-14 08:41:22
- **Topic Keywords**: ranking, retrieval, rank, search
- **Reason**: This paper focuses on advancements in Chinese language processing using BERT, specifically on tokenization, pre-training, and fine-tuning. While it touches on the topic of retrieval-oriented quality, it does not directly relate to the user's core research interests in Information Retrieval, query understanding, ranking models, and user behavior modeling.

#### Abstract
> Encoder-only Transformers have advanced along three axes -- architecture,
data, and systems -- yielding Pareto gains in accuracy, speed, and memory
efficiency. Yet these improvements have not fully transferred to Chinese, where
tokenization and morphology differ markedly from English. We introduce Chinese
ModernBERT, a from-scratch Chinese encoder that couples: (i) a hardware-aware
32k BPE vocabulary tailored to frequent Chinese affixes/compounds, lowering the
embedding budget; (ii) whole-word masking (WWM) with a dynamic masking
curriculum (30% -> 15%) to align task difficulty with training progress; (iii)
a two-stage pre-training pipeline that extends the native context from 1,024 to
8,192 tokens using RoPE and alternating local/global attention; and (iv) a
damped-cosine learning-rate schedule for stable long-horizon optimization. We
pre-train on ~1.2T Chinese tokens from CCI3-HQ, CCI4 (Chinese), and
Cosmopedia-Chinese. On CLUE, Chinese ModernBERT is competitive with strong
Chinese encoders under a unified fine-tuning protocol. Under bf16 it achieves
high long-sequence throughput while maintaining strong short-sequence speed,
reflecting benefits from budget allocation and attention design. To probe
retrieval-oriented quality, we add a small amount of open contrastive data:
fine-tuning on SimCLUE (~3M pairs) improves further when adding T2Ranking
(~2M), reaching 0.505 (Pearson) / 0.537 (Spearman) on the SimCLUE test set.
Under this open-data setting, Chinese ModernBERT surpasses Qwen-0.6B-embedding
on SimCLUE, suggesting a clear scaling path for STS with additional curated
pairs. We will release tokenizer and weights to facilitate reproducible
research.

### 30. Deep Associations, High Creativity: A Simple yet Effective Metric for Evaluating Large Language Models

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Ziliang Qiu, Renfen Hu
- **URL**: <http://arxiv.org/abs/2510.12110v1>
- **Submitted**: 2025-10-14 03:26:28
- **Comment**: 14 pages
- **Topic Keywords**: ranking, rag, rank, search
- **Reason**: This paper focuses on evaluating the creativity of Large Language Models (LLMs), which is a topic in Natural Language Processing (NLP). However, it does not directly relate to the user's core research interests in Information Retrieval (IR), query understanding, ranking models, or user behavior modeling.

#### Abstract
> The evaluation of LLMs' creativity represents a crucial research domain,
though challenges such as data contamination and costly human assessments often
impede progress. Drawing inspiration from human creativity assessment, we
propose PACE, asking LLMs to generate Parallel Association Chains to Evaluate
their creativity. PACE minimizes the risk of data contamination and offers a
straightforward, highly efficient evaluation, as evidenced by its strong
correlation with Chatbot Arena Creative Writing rankings (Spearman's $\rho =
0.739$, $p < 0.001$) across various proprietary and open-source models. A
comparative analysis of associative creativity between LLMs and humans reveals
that while high-performing LLMs achieve scores comparable to average human
performance, professional humans consistently outperform LLMs. Furthermore,
linguistic analysis reveals that both humans and LLMs exhibit a trend of
decreasing concreteness in their associations, and humans demonstrating a
greater diversity of associative patterns.

### 31. Deep Research Brings Deeper Harm

- **LLM Score**: 2
- **Keyword Score**: 7
- **Authors**: Shuo Chen, Zonggen Li, Zhen Han, Bailan He, Tong Liu, Haokun Chen, Georg Groh, Philip Torr, Volker Tresp, Jindong Gu
- **URL**: <http://arxiv.org/abs/2510.11851v1>
- **Submitted**: 2025-10-13 19:05:00
- **Comment**: Accepted to Reliable ML from Unreliable Data Workshop @ NeurIPS 2025
- **Topic Keywords**: query, queries, search
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves Large Language Models and query understanding, its focus is on the misuse of these models for generating harmful content, which is not a central theme in your research.

#### Abstract
> Deep Research (DR) agents built on Large Language Models (LLMs) can perform
complex, multi-step research by decomposing tasks, retrieving online
information, and synthesizing detailed reports. However, the misuse of LLMs
with such powerful capabilities can lead to even greater risks. This is
especially concerning in high-stakes and knowledge-intensive domains such as
biosecurity, where DR can generate a professional report containing detailed
forbidden knowledge. Unfortunately, we have found such risks in practice:
simply submitting a harmful query, which a standalone LLM directly rejects, can
elicit a detailed and dangerous report from DR agents. This highlights the
elevated risks and underscores the need for a deeper safety analysis. Yet,
jailbreak methods designed for LLMs fall short in exposing such unique risks,
as they do not target the research ability of DR agents. To address this gap,
we propose two novel jailbreak strategies: Plan Injection, which injects
malicious sub-goals into the agent's plan; and Intent Hijack, which reframes
harmful queries as academic research questions. We conducted extensive
experiments across different LLMs and various safety benchmarks, including
general and biosecurity forbidden prompts. These experiments reveal 3 key
findings: (1) Alignment of the LLMs often fail in DR agents, where harmful
prompts framed in academic terms can hijack agent intent; (2) Multi-step
planning and execution weaken the alignment, revealing systemic vulnerabilities
that prompt-level safeguards cannot address; (3) DR agents not only bypass
refusals but also produce more coherent, professional, and dangerous content,
compared with standalone LLMs. These results demonstrate a fundamental
misalignment in DR agents and call for better alignment techniques tailored to
DR agents. Code and datasets are available at
https://chenxshuo.github.io/deeper-harm.

### 32. Fine-grained Analysis of Brain-LLM Alignment through Input Attribution

- **LLM Score**: 2
- **Keyword Score**: 6
- **Authors**: Michela Proietti, Roberto Capobianco, Mariya Toneva
- **URL**: <http://arxiv.org/abs/2510.12355v1>
- **Submitted**: 2025-10-14 10:19:01
- **Topic Keywords**: relevance, rag, search
- **Reason**: This paper is not relevant to your research interests as it focuses on the alignment between large language models and human brain activity, which is outside the scope of Information Retrieval and Search technologies. Although it involves Natural Language Processing, the topic is more aligned with cognitive psychology and neuroscience rather than your primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Understanding the alignment between large language models (LLMs) and human
brain activity can reveal computational principles underlying language
processing. We introduce a fine-grained input attribution method to identify
the specific words most important for brain-LLM alignment, and leverage it to
study a contentious research question about brain-LLM alignment: the
relationship between brain alignment (BA) and next-word prediction (NWP). Our
findings reveal that BA and NWP rely on largely distinct word subsets: NWP
exhibits recency and primacy biases with a focus on syntax, while BA
prioritizes semantic and discourse-level information with a more targeted
recency effect. This work advances our understanding of how LLMs relate to
human language processing and highlights differences in feature reliance
between BA and NWP. Beyond this study, our attribution method can be broadly
applied to explore the cognitive relevance of model predictions in diverse
language processing tasks.

### 33. SafeMT: Multi-turn Safety for Multimodal Language Models

- **LLM Score**: 2
- **Keyword Score**: 5
- **Authors**: Han Zhu, Juntao Dai, Jiaming Ji, Haoran Li, Chengkun Cai, Pengcheng Wen, Chi-Min Chan, Boyuan Chen, Yaodong Yang, Sirui Han, Yike Guo
- **URL**: <http://arxiv.org/abs/2510.12133v1>
- **Submitted**: 2025-10-14 04:24:07
- **Topic Keywords**: queries, rag
- **Reason**: This paper focuses on safety issues in multimodal language models, specifically in multi-turn dialogues, which is not directly related to the user's core research themes in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on the concept of query understanding and user behavior modeling, the context is safety and security rather than relevance and ranking.

#### Abstract
> With the widespread use of multi-modal Large Language models (MLLMs), safety
issues have become a growing concern. Multi-turn dialogues, which are more
common in everyday interactions, pose a greater risk than single prompts;
however, existing benchmarks do not adequately consider this situation. To
encourage the community to focus on the safety issues of these models in
multi-turn dialogues, we introduce SafeMT, a benchmark that features dialogues
of varying lengths generated from harmful queries accompanied by images. This
benchmark consists of 10,000 samples in total, encompassing 17 different
scenarios and four jailbreak methods. Additionally, we propose Safety Index
(SI) to evaluate the general safety of MLLMs during conversations. We assess
the safety of 17 models using this benchmark and discover that the risk of
successful attacks on these models increases as the number of turns in harmful
dialogues rises. This observation indicates that the safety mechanisms of these
models are inadequate for recognizing the hazard in dialogue interactions. We
propose a dialogue safety moderator capable of detecting malicious intent
concealed within conversations and providing MLLMs with relevant safety
policies. Experimental results from several open-source models indicate that
this moderator is more effective in reducing multi-turn ASR compared to existed
guard models.

### 34. When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Lang Gao, Xuhui Li, Chenxi Wang, Mingzhe Li, Wei Liu, Zirui Song, Jinghui Zhang, Rui Yan, Preslav Nakov, Xiuying Chen
- **URL**: <http://arxiv.org/abs/2510.12476v1>
- **Submitted**: 2025-10-14 13:10:23
- **Topic Keywords**: rag, personalization, search
- **Reason**: This paper focuses on machine-generated text detection and personalization, which is somewhat related to your interests in Natural Language Processing (NLP) and related topics. However, it does not directly align with your primary focus on Information Retrieval, particularly query understanding, ranking models, and user behavior modeling.

#### Abstract
> Large language models (LLMs) have grown more powerful in language generation,
producing fluent text and even imitating personal style. Yet, this ability also
heightens the risk of identity impersonation. To the best of our knowledge, no
prior work has examined personalized machine-generated text (MGT) detection. In
this paper, we introduce \dataset, the first benchmark for evaluating detector
robustness in personalized settings, built from literary and blog texts paired
with their LLM-generated imitations. Our experimental results demonstrate large
performance gaps across detectors in personalized settings: some
state-of-the-art models suffer significant drops. We attribute this limitation
to the \textit{feature-inversion trap}, where features that are discriminative
in general domains become inverted and misleading when applied to personalized
text. Based on this finding, we propose \method, a simple and reliable way to
predict detector performance changes in personalized settings. \method
identifies latent directions corresponding to inverted features and constructs
probe datasets that differ primarily along these features to evaluate detector
dependence. Our experiments show that \method can accurately predict both the
direction and the magnitude of post-transfer changes, showing 85\% correlation
with the actual performance gaps. We hope that this work will encourage further
research on personalized text detection.

### 35. Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Greta Damo, Elena Cabrio, Serena Villata
- **URL**: <http://arxiv.org/abs/2510.12316v1>
- **Submitted**: 2025-10-14 09:20:01
- **Topic Keywords**: rag, retrieval
- **Reason**: This paper focuses on counter-speech generation using Retrieval-Augmented Generation (RAG) pipelines, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it does involve text generation and knowledge base construction, the specific application to counter-speech generation and hate speech is not aligned with your primary focus on e-commerce, query understanding, ranking models, and user behavior modeling.

#### Abstract
> Counter-speech generation is at the core of many expert activities, such as
fact-checking and hate speech, to counter harmful content. Yet, existing work
treats counter-speech generation as pure text generation task, mainly based on
Large Language Models or NGO experts. These approaches show severe drawbacks
due to the limited reliability and coherence in the generated countering text,
and in scalability, respectively. To close this gap, we introduce a novel
framework to model counter-speech generation as knowledge-wise text generation
process. Our framework integrates advanced Retrieval-Augmented Generation (RAG)
pipelines to ensure the generation of trustworthy counter-speech for 8 main
target groups identified in the hate speech literature, including women, people
of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons,
and other. We built a knowledge base over the United Nations Digital Library,
EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792
texts. We use the MultiTarget-CONAN dataset to empirically assess the quality
of the generated counter-speech, both through standard metrics (i.e., JudgeLM)
and a human evaluation. Results show that our framework outperforms standard
LLM baselines and competitive approach, on both assessments. The resulting
framework and the knowledge base pave the way for studying trustworthy and
sound counter-speech generation, in hate speech and beyond.

### 36. DPO-Tuned Large Language Models for Segmentation in Simultaneous Speech Translation

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Zeyu Yang, Satoshi Nakamura
- **URL**: <http://arxiv.org/abs/2510.12195v1>
- **Submitted**: 2025-10-14 06:41:36
- **Topic Keywords**: rag, acl
- **Reason**: This paper focuses on segmentation in simultaneous speech translation, leveraging large language models and Direct Preference Optimization. While it involves NLP and potentially some IR aspects, it is not directly related to the user's core research themes of query understanding, ranking models, or user behavior modeling in the context of search technologies.

#### Abstract
> Simultaneous speech translation requires accurate segmentation to balance
translation quality and latency. Recent studies such as SHAS have introduced
pretrained segmentation models, achieving stronger performance than heuristic
rules. However, segmentation models such as SHAS, though pretrained and more
robust than heuristic methods, are still constrained by supervised learning
objectives and do not incorporate human preference alignment, which is crucial
for natural real-time interpretation. In this work, we propose a segmentation
framework based on large language models (LLMs) trained with Direct Preference
Optimization (DPO). By leveraging preference alignment, our method enables LLMs
to predict natural segmentation points that better meet the demands of
real-time translation. We evaluate the system on the ACL 60/60 corpus across
three language pairs (English-Japanese, Chinese, German), using SeamlessM4T v2
as the translation backbone. Experimental results show that our DPO-tuned LLM
achieves higher segmentation accuracy than SHAS and yields consistent
improvements in translation quality (BLEU, COMET) as well as latency (Average
Lagging). Furthermore, our system benefits from IWSLT baselines for direct
comparison. These findings highlight the potential of preference-tuned LLMs to
surpass existing pretrained segmentation models and advance adaptive,
human-aligned simultaneous interpretation.

### 37. One Life to Learn: Inferring Symbolic World Models for Stochastic Environments from Unguided Exploration

- **LLM Score**: 2
- **Keyword Score**: 4
- **Authors**: Zaid Khan, Archiki Prasad, Elias Stengel-Eskin, Jaemin Cho, Mohit Bansal
- **URL**: <http://arxiv.org/abs/2510.12088v1>
- **Submitted**: 2025-10-14 02:49:32
- **Comment**: Project page: https://onelife-worldmodel.github.io/; 39 pages
- **Topic Keywords**: ranking, rank
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. The paper focuses on symbolic world modeling and probabilistic programming for stochastic environments, which is outside your primary areas of interest.

#### Abstract
> Symbolic world modeling requires inferring and representing an environment's
transitional dynamics as an executable program. Prior work has focused on
largely deterministic environments with abundant interaction data, simple
mechanics, and human guidance. We address a more realistic and challenging
setting, learning in a complex, stochastic environment where the agent has only
"one life" to explore a hostile environment without human guidance. We
introduce OneLife, a framework that models world dynamics through
conditionally-activated programmatic laws within a probabilistic programming
framework. Each law operates through a precondition-effect structure,
activating in relevant world states. This creates a dynamic computation graph
that routes inference and optimization only through relevant laws, avoiding
scaling challenges when all laws contribute to predictions about a complex,
hierarchical state, and enabling the learning of stochastic dynamics even with
sparse rule activation. To evaluate our approach under these demanding
constraints, we introduce a new evaluation protocol that measures (a) state
ranking, the ability to distinguish plausible future states from implausible
ones, and (b) state fidelity, the ability to generate future states that
closely resemble reality. We develop and evaluate our framework on Crafter-OO,
our reimplementation of the Crafter environment that exposes a structured,
object-oriented symbolic state and a pure transition function that operates on
that state alone. OneLife can successfully learn key environment dynamics from
minimal, unguided interaction, outperforming a strong baseline on 16 out of 23
scenarios tested. We also test OneLife's planning ability, with simulated
rollouts successfully identifying superior strategies. Our work establishes a
foundation for autonomously constructing programmatic world models of unknown,
complex environments.

### 38. ACADATA: Parallel Dataset of Academic Data for Machine Translation

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: I√±aki Lacunza, Javier Garcia Gilabert, Francesca De Luca Fornaciari, Javier Aula-Blasco, Aitor Gonzalez-Agirre, Maite Melero, Marta Villegas
- **URL**: <http://arxiv.org/abs/2510.12621v2>
- **Submitted**: 2025-10-14 15:20:06
- **Topic Keywords**: rag, search
- **Reason**: This paper is not relevant to your research interests as it focuses on machine translation and dataset creation for academic translation, which is outside your primary focus on Information Retrieval and Search technologies. Although it involves Natural Language Processing, the specific application and context are not aligned with your core research themes.

#### Abstract
> We present ACADATA, a high-quality parallel dataset for academic translation,
that consists of two subsets: ACAD-TRAIN, which contains approximately 1.5
million author-generated paragraph pairs across 96 language directions and
ACAD-BENCH, a curated evaluation set of almost 6,000 translations covering 12
directions. To validate its utility, we fine-tune two Large Language Models
(LLMs) on ACAD-TRAIN and benchmark them on ACAD-BENCH against specialized
machine-translation systems, general-purpose, open-weight LLMs, and several
large-scale proprietary models. Experimental results demonstrate that
fine-tuning on ACAD-TRAIN leads to improvements in academic translation quality
by +6.1 and +12.4 d-BLEU points on average for 7B and 2B models respectively,
while also improving long-context translation in a general domain by up to
24.9% when translating out of English. The fine-tuned top-performing model
surpasses the best propietary and open-weight models on academic translation
domain. By releasing ACAD-TRAIN, ACAD-BENCH and the fine-tuned models, we
provide the community with a valuable resource to advance research in academic
domain and long-context translation.

### 39. Uncertainty Quantification for Hallucination Detection in Large Language Models: Foundations, Methodology, and Future Directions

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Sungmin Kang, Yavuz Faruk Bakman, Duygu Nur Yaldiz, Baturalp Buyukates, Salman Avestimehr
- **URL**: <http://arxiv.org/abs/2510.12040v1>
- **Submitted**: 2025-10-14 00:49:04
- **Comment**: 24 pages, 3 figures, magazine
- **Topic Keywords**: ctr, search
- **Reason**: This paper focuses on uncertainty quantification in large language models, which is a topic related to NLP, but it does not directly address information retrieval, search technologies, or query understanding. While it touches on the reliability and trustworthiness of model outputs, it does not seem to be a central match for your research interests.

#### Abstract
> The rapid advancement of large language models (LLMs) has transformed the
landscape of natural language processing, enabling breakthroughs across a wide
range of areas including question answering, machine translation, and text
summarization. Yet, their deployment in real-world applications has raised
concerns over reliability and trustworthiness, as LLMs remain prone to
hallucinations that produce plausible but factually incorrect outputs.
Uncertainty quantification (UQ) has emerged as a central research direction to
address this issue, offering principled measures for assessing the
trustworthiness of model generations. We begin by introducing the foundations
of UQ, from its formal definition to the traditional distinction between
epistemic and aleatoric uncertainty, and then highlight how these concepts have
been adapted to the context of LLMs. Building on this, we examine the role of
UQ in hallucination detection, where quantifying uncertainty provides a
mechanism for identifying unreliable generations and improving reliability. We
systematically categorize a wide spectrum of existing methods along multiple
dimensions and present empirical results for several representative approaches.
Finally, we discuss current limitations and outline promising future research
directions, providing a clearer picture of the current landscape of LLM UQ for
hallucination detection.

### 40. Scaling Long-Horizon LLM Agent via Context-Folding

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Weiwei Sun, Miao Lu, Zhan Ling, Kang Liu, Xuesong Yao, Yiming Yang, Jiecao Chen
- **URL**: <http://arxiv.org/abs/2510.11967v1>
- **Submitted**: 2025-10-13 22:00:58
- **Topic Keywords**: rag, search
- **Reason**: This paper appears to be related to Large Language Models (LLMs) and reinforcement learning, but it does not directly address information retrieval, search technologies, or query understanding. While it touches on context management, it is more focused on task decomposition and long-horizon tasks, which is not a primary area of interest for the user.

#### Abstract
> Large language model (LLM) agents are fundamentally constrained by context
length on long-horizon tasks. We introduce Context-Folding, a framework that
empowers agents to actively manage their working context. An agent can
procedurally branch into a sub-trajectory to handle a subtask and then fold it
upon completion, collapsing the intermediate steps while retaining a concise
summary of the outcome. To make this behavior learnable, we develop an
end-to-end reinforcement learning framework FoldGRPO with specific process
rewards to encourage effective task decomposition and context management. On
complex long-horizon tasks (Deep Research and SWE), our folding agent matches
or outperforms the ReAct baselines while using an active context 10$\times$
smaller and significantly outperforms models that rely on summarization-based
context management.

### 41. Discrepancy Detection at the Data Level: Toward Consistent Multilingual Question Answering

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Lorena Calvo-Bartolom√©, Val√©rie Aldana, Karla Cantarero, Alonso Madro√±al de Mesa, Jer√≥nimo Arenas-Garc√≠a, Jordan Boyd-Graber
- **URL**: <http://arxiv.org/abs/2510.11928v1>
- **Submitted**: 2025-10-13 20:48:26
- **Comment**: Long paper accepted at EMNLP 2025
- **Topic Keywords**: queries
- **Reason**: This paper focuses on multilingual question answering and fact-checking, which is somewhat related to information retrieval, but it does not directly address query understanding, ranking models, or user behavior modeling. The paper's emphasis on cultural variation and fact-checking does not align with the user's primary research interests in deep semantic understanding and real-time relevance optimization.

#### Abstract
> Multilingual question answering (QA) systems must ensure factual consistency
across languages, especially for objective queries such as What is jaundice?,
while also accounting for cultural variation in subjective responses. We
propose MIND, a user-in-the-loop fact-checking pipeline to detect factual and
cultural discrepancies in multilingual QA knowledge bases. MIND highlights
divergent answers to culturally sensitive questions (e.g., Who assists in
childbirth?) that vary by region and context. We evaluate MIND on a bilingual
QA system in the maternal and infant health domain and release a dataset of
bilingual questions annotated for factual and cultural inconsistencies. We
further test MIND on datasets from other domains to assess generalization. In
all cases, MIND reliably identifies inconsistencies, supporting the development
of more culturally aware and factually consistent QA systems.

### 42. LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens

- **LLM Score**: 2
- **Keyword Score**: 3
- **Authors**: Armel Zebaze, Rachel Bawden, Beno√Æt Sagot
- **URL**: <http://arxiv.org/abs/2510.11919v1>
- **Submitted**: 2025-10-13 20:41:01
- **Topic Keywords**: query
- **Reason**: This paper is not relevant to your research interests as it focuses on machine translation and the use of large reasoning models, which is outside your primary area of interest in Information Retrieval and Search technologies. While it touches on Natural Language Processing, the specific application and methodology are not aligned with your core research themes.

#### Abstract
> Large reasoning models (LRMs) have led to new possibilities in terms of
problem-solving, through the devising of a natural language thought process
prior to answering a query. While their capabilities are well known across
mathematics and coding tasks, their impact on the task of machine translation
(MT) remains underexplored. In this work, we explore the benefits of the
generation of intermediate tokens when performing MT across multiple language
pairs of different levels of resourcedness and multiple setups. We find that
"thinking tokens" do not help LRMs better perform MT. This result generalizes
to models fine-tuned to reason before translating using distilled chain of
thought (CoT) inspired by human translators' practices. Specifically,
fine-tuning a model with synthetic CoT explanations detailing how to translate
step-by-step does not outperform standard input-output fine-tuning. However,
constructing the intermediate tokens by combining the outputs of modular
translation-specific prompting strategies results in improvements. Our findings
underscore that the contribution of intermediate tokens during fine-tuning
highly depends on the presence of translation attempts within them. More
broadly, our results suggest that using a teacher to refine target translations
or to expand parallel corpora is more impactful than distilling their CoT
explanations into "thinking" MT models.

### 43. Cost Analysis of Human-corrected Transcription for Predominately Oral Languages

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Yacouba Diarra, Nouhoum Souleymane Coulibaly, Michael Leventhal
- **URL**: <http://arxiv.org/abs/2510.12781v1>
- **Submitted**: 2025-10-14 17:53:11
- **Comment**: 6 pages, 1 figure
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it touches on NLP resources, its focus on transcription costs for low-resource languages is not a central match for your core research themes.

#### Abstract
> Creating speech datasets for low-resource languages is a critical yet poorly
understood challenge, particularly regarding the actual cost in human labor.
This paper investigates the time and complexity required to produce
high-quality annotated speech data for a subset of low-resource languages, low
literacy Predominately Oral Languages, focusing on Bambara, a Manding language
of Mali. Through a one-month field study involving ten transcribers with native
proficiency, we analyze the correction of ASR-generated transcriptions of 53
hours of Bambara voice data. We report that it takes, on average, 30 hours of
human labor to accurately transcribe one hour of speech data under laboratory
conditions and 36 hours under field conditions. The study provides a baseline
and practical insights for a large class of languages with comparable profiles
undertaking the creation of NLP resources.

### 44. Hey, wait a minute: on at-issue sensitivity in Language Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Sanghee J. Kim, Kanishka Misra
- **URL**: <http://arxiv.org/abs/2510.12740v1>
- **Submitted**: 2025-10-14 17:17:20
- **Comment**: 10 pages, 5 figures, 3 tables. See
  https://github.com/sangheek16/hey-wait-a-minute for code and data
- **Topic Keywords**: rag
- **Reason**: This paper focuses on evaluating the naturalness of dialogue in language models, introducing a new method called DGRC. While it touches on aspects of language understanding, it does not directly relate to query understanding, ranking models, or user behavior modeling in the context of information retrieval, which are core areas of your research interests.

#### Abstract
> Evaluating the naturalness of dialogue in language models (LMs) is not
trivial: notions of 'naturalness' vary, and scalable quantitative metrics
remain limited. This study leverages the linguistic notion of 'at-issueness' to
assess dialogue naturalness and introduces a new method: Divide, Generate,
Recombine, and Compare (DGRC). DGRC (i) divides a dialogue as a prompt, (ii)
generates continuations for subparts using LMs, (iii) recombines the dialogue
and continuations, and (iv) compares the likelihoods of the recombined
sequences. This approach mitigates bias in linguistic analyses of LMs and
enables systematic testing of discourse-sensitive behavior. Applying DGRC, we
find that LMs prefer to continue dialogue on at-issue content, with this effect
enhanced in instruct-tuned models. They also reduce their at-issue preference
when relevant cues (e.g., "Hey, wait a minute") are present. Although
instruct-tuning does not further amplify this modulation, the pattern reflects
a hallmark of successful dialogue dynamics.

### 45. BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach Annotation Disagreements (Yet)

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Tomas Ruiz, Siyao Peng, Barbara Plank, Carsten Schwemmer
- **URL**: <http://arxiv.org/abs/2510.12516v1>
- **Submitted**: 2025-10-14 13:43:08
- **Topic Keywords**: rag
- **Reason**: This paper appears to be focused on test-time scaling techniques for Large Language Models (LLMs), which is not directly related to the user's primary research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on LLMs, the context and application domain seem unrelated to the user's e-commerce background and interests in query understanding, ranking models, and user behavior modeling.

#### Abstract
> Test-time scaling is a family of techniques to improve LLM outputs at
inference time by performing extra computation. To the best of our knowledge,
test-time scaling has been limited to domains with verifiably correct answers,
like mathematics and coding. We transfer test-time scaling to the LeWiDi-2025
tasks to evaluate annotation disagreements. We experiment with three test-time
scaling methods: two benchmark algorithms (Model Averaging and Majority
Voting), and a Best-of-N sampling method. The two benchmark methods improve LLM
performance consistently on the LeWiDi tasks, but the Best-of-N method does
not. Our experiments suggest that the Best-of-N method does not currently
transfer from mathematics to LeWiDi tasks, and we analyze potential reasons for
this gap.

### 46. Tracing Multilingual Knowledge Acquisition Dynamics in Domain Adaptation: A Case Study of English-Japanese Biomedical Adaptation

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Xin Zhao, Naoki Yoshinaga, Yuma Tsuta, Akiko Aizawa
- **URL**: <http://arxiv.org/abs/2510.12115v1>
- **Submitted**: 2025-10-14 03:34:17
- **Comment**: 22 Pages, Submitted to ARR 2025 Oct
- **Topic Keywords**: rag
- **Reason**: This paper focuses on multilingual domain adaptation and large language models, which is not directly related to your core research interests in Information Retrieval, Search technologies, and Natural Language Processing. While it touches on knowledge acquisition and transfer, the context is domain adaptation and bilingual language models, which is somewhat tangential to your primary focus on query understanding, ranking models, and user behavior modeling.

#### Abstract
> Multilingual domain adaptation (ML-DA) is widely used to learn new domain
knowledge across languages into large language models (LLMs). Although many
methods have been proposed to improve domain adaptation, the mechanisms of
multilingual knowledge acquisition, how domain knowledge is learned within a
language and transferred across languages, remain underexplored. This gap leads
to suboptimal performance, particularly in low-resource settings. This work
examines the learning dynamics of LLMs during ML-DA. Because prior ML-DA
studies often train and evaluate on datasets with mismatched knowledge
coverage, we propose AdaXEval, an adaptive evaluation method that builds
multiple-choice QA datasets from the same bilingual domain corpus used for
training, thereby directly studying multilingual knowledge acquisition. Through
continual training of LLMs with diverse data recipes, we track how LLMs acquire
domain facts and pinpoint the mechanism behind the transformation process from
domain training data to knowledge. Our experiments on a 13B English-Japanese
bilingual LLM reveal that cross-lingual transfer remains challenging despite a
high-quality bilingual corpus. The code has been released.

### 47. Improving Text-to-Image Generation with Input-Side Inference-Time Scaling

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Ruibo Chen, Jiacheng Pan, Heng Huang, Zhenheng Yang
- **URL**: <http://arxiv.org/abs/2510.12041v2>
- **Submitted**: 2025-10-14 00:51:39
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. While it involves text processing, the focus is on text-to-image generation, which is outside your primary areas of interest.

#### Abstract
> Recent advances in text-to-image (T2I) generation have achieved impressive
results, yet existing models often struggle with simple or underspecified
prompts, leading to suboptimal image-text alignment, aesthetics, and quality.
We propose a prompt rewriting framework that leverages large language models
(LLMs) to refine user inputs before feeding them into T2I backbones. Our
approach introduces a carefully designed reward system and an iterative direct
preference optimization (DPO) training pipeline, enabling the rewriter to
enhance prompts without requiring supervised fine-tuning data. We evaluate our
method across diverse T2I models and benchmarks. Results show that our prompt
rewriter consistently improves image-text alignment, visual quality, and
aesthetics, outperforming strong baselines. Furthermore, we demonstrate strong
transferability by showing that a prompt rewriter trained on one T2I backbone
generalizes effectively to others without needing to be retrained. We also
systematically study scalability, evaluating how performance gains scale with
the capacity of the large LLM used as the rewriter. These findings highlight
that prompt rewriting is an effective, scalable, and practical model-agnostic
strategy for improving T2I systems. We plan to release the code and trained
prompt rewriters soon.

### 48. PHANTOM RECALL: When Familiar Puzzles Fool Smart Models

- **LLM Score**: 2
- **Keyword Score**: 2
- **Authors**: Souradeep Mukhopadhyay, Rishabh Baral, Nimeesh Mahajan, Samhitha Harish, Aswin RRV, Mihir Parmar, Mutsumi Nakamura, Chitta Baral
- **URL**: <http://arxiv.org/abs/2510.11812v1>
- **Submitted**: 2025-10-13 18:09:50
- **Comment**: 22 Pages
- **Topic Keywords**: rag
- **Reason**: This paper is not directly related to your research interests in Information Retrieval, Search technologies, or Natural Language Processing. It focuses on the limitations of large language models in solving logic puzzles, which is more relevant to the field of Artificial Intelligence and Cognitive Science.

#### Abstract
> Large language models (LLMs) such as GPT, Gemini, and Claude often appear
adept at solving classic logic puzzles--but how much genuine reasoning
underlies their answers? Recent evidence suggests that these models frequently
rely on memorized templates rather than reasoning from first principles. When
puzzles are slightly modified, their performance collapses, revealing a
striking fragility. In particular, we asked: Have LLMs addressed these issues?
To what extent? How about perturbations to other puzzles? Is there a general
way of reformulating the prompt so that the models do better? To examine these
things systematically, we introduce PHANTOM RECALL, a benchmark comprising 25
well-known logic puzzles and 149 carefully designed perturbations that preserve
reasoning structure but alter superficial details and solutions. We evaluate
eleven leading LLMs and identify a recurring failure mode--phantom
recall--where models confidently reproduce memorized solutions or spurious
rationales that no longer fit the altered scenario. To probe and mitigate this
issue, we contribute three tools: (i) an automated logical-equivalence judge to
detect reasoning mismatches, (ii) a taxonomy of fine-grained reasoning error
categories, and (iii) a prompting-based mitigation framework guided by these
categories. Despite near-perfect accuracy on unmodified puzzles, models
significantly underperform humans on perturbed ones, exhibiting both phantom
recall and over-elaboration. Our findings reveal a crucial limitation: LLMs
often fail to re-reason when contextual cues shift--highlighting the gap
between linguistic fluency and logical understanding.

### 49. Content Anonymization for Privacy in Long-form Audio

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Cristina Aggazzotti, Ashi Garg, Zexin Cai, Nicholas Andrews
- **URL**: <http://arxiv.org/abs/2510.12780v1>
- **Submitted**: 2025-10-14 17:52:50
- **Topic Keywords**: recommend
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, or Natural Language Processing, as it focuses on voice anonymization and content-based attacks in long-form audio, which does not align with your core themes.

#### Abstract
> Voice anonymization techniques have been found to successfully obscure a
speaker's acoustic identity in short, isolated utterances in benchmarks such as
the VoicePrivacy Challenge. In practice, however, utterances seldom occur in
isolation: long-form audio is commonplace in domains such as interviews, phone
calls, and meetings. In these cases, many utterances from the same speaker are
available, which pose a significantly greater privacy risk: given multiple
utterances from the same speaker, an attacker could exploit an individual's
vocabulary, syntax, and turns of phrase to re-identify them, even when their
voice is completely disguised. To address this risk, we propose new content
anonymization approaches. Our approach performs a contextual rewriting of the
transcripts in an ASR-TTS pipeline to eliminate speaker-specific style while
preserving meaning. We present results in a long-form telephone conversation
setting demonstrating the effectiveness of a content-based attack on
voice-anonymized speech. Then we show how the proposed content-based
anonymization methods can mitigate this risk while preserving speech utility.
Overall, we find that paraphrasing is an effective defense against
content-based attacks and recommend that stakeholders adopt this step to ensure
anonymity in long-form audio.

### 50. Resource-sensitive but language-blind: Community size and not grammatical complexity better predicts the accuracy of Large Language Models in a novel Wug Test

- **LLM Score**: 2
- **Keyword Score**: 1
- **Authors**: Nikoleta Pantelidou, Evelina Leivada, Paolo Morosi
- **URL**: <http://arxiv.org/abs/2510.12463v1>
- **Submitted**: 2025-10-14 12:52:57
- **Topic Keywords**: search
- **Reason**: This paper is not relevant to your research interests in Information Retrieval, Search technologies, query understanding, ranking models, user behavior modeling, or Natural Language Processing. The study focuses on the performance of Large Language Models in a morphological generalization task, which is more related to NLP and linguistics, but does not align with your core research themes.

#### Abstract
> The linguistic abilities of Large Language Models are a matter of ongoing
debate. This study contributes to this discussion by investigating model
performance in a morphological generalization task that involves novel words.
Using a multilingual adaptation of the Wug Test, six models were tested across
four partially unrelated languages (Catalan, English, Greek, and Spanish) and
compared with human speakers. The aim is to determine whether model accuracy
approximates human competence and whether it is shaped primarily by linguistic
complexity or by the quantity of available training data. Consistent with
previous research, the results show that the models are able to generalize
morphological processes to unseen words with human-like accuracy. However,
accuracy patterns align more closely with community size and data availability
than with structural complexity, refining earlier claims in the literature. In
particular, languages with larger speaker communities and stronger digital
representation, such as Spanish and English, revealed higher accuracy than
less-resourced ones like Catalan and Greek. Overall, our findings suggest that
model behavior is mainly driven by the richness of linguistic resources rather
than by sensitivity to grammatical complexity, reflecting a form of performance
that resembles human linguistic competence only superficially.

---

